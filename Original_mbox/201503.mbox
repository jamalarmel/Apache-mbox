From dev-return-11819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  1 23:00:25 2015
Return-Path: <dev-return-11819-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1727817924
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Mar 2015 23:00:25 +0000 (UTC)
Received: (qmail 87877 invoked by uid 500); 1 Mar 2015 23:00:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87789 invoked by uid 500); 1 Mar 2015 23:00:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87776 invoked by uid 99); 1 Mar 2015 23:00:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Mar 2015 23:00:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.160.170 as permitted sender)
Received: from [209.85.160.170] (HELO mail-yk0-f170.google.com) (209.85.160.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Mar 2015 22:59:58 +0000
Received: by ykq19 with SMTP id 19so11820823ykq.9
        for <dev@spark.apache.org>; Sun, 01 Mar 2015 14:59:57 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=G1LBWmqhKM7d1WV1QDIJHRiBHH8CGbqXEmkYheEatZs=;
        b=HrvVxoio1JMv05Wf0h4FqDi5kq2sXgmP25488hD8lD1Q+we2lDHU1Qk/MdrnOkK+Q3
         MnF7ReEHFvqU5QVC87FQZcG6N7ee+X4/ChlVrWGSyJIOHeqDQe2FfviFYbxLl0+GVgZM
         EBiWGRCyXQ9I49fHydAmgSerLqbx7GInIfWjihHkqzukxY6c5LN3vbipgVqfuyd4trh8
         6f2Pn+ThyeDAbYtY54OX+UBHH0lv8hCe0BYhfrVRpk+6LQfhsvRnCq8sn7oxiMSgX6YE
         aUci5gUolhy2w9YNsN+9/EdpFFNGZv8HoO1CcT0Ifq+7vMpvIcPdRv2KTjAjKUpZPxn6
         +Bag==
X-Received: by 10.170.210.214 with SMTP id b205mr25323350ykf.0.1425250796988;
 Sun, 01 Mar 2015 14:59:56 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 01 Mar 2015 22:59:56 +0000
Message-ID: <CAOhmDzeRAkkJp9a2o1WV09y5xZ8GFVJwie9m+tGnT51rjuJtZQ@mail.gmail.com>
Subject: spark-ec2 default to Hadoop 2
To: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1139ae587870060510420fb6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139ae587870060510420fb6
Content-Type: text/plain; charset=UTF-8

https://github.com/apache/spark/blob/fd8d283eeb98e310b1e85ef8c3a8af9e547ab5e0/ec2/spark_ec2.py#L162-L164

Is there any reason we shouldn't update the default Hadoop major version in
spark-ec2 to 2?

Nick

--001a1139ae587870060510420fb6--

From dev-return-11820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  1 23:17:07 2015
Return-Path: <dev-return-11820-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 095EF17956
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Mar 2015 23:17:07 +0000 (UTC)
Received: (qmail 99930 invoked by uid 500); 1 Mar 2015 23:17:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99855 invoked by uid 500); 1 Mar 2015 23:17:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99415 invoked by uid 99); 1 Mar 2015 23:17:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Mar 2015 23:17:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 209.85.217.180 as permitted sender)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Mar 2015 23:17:00 +0000
Received: by lbdu14 with SMTP id u14so5601393lbd.0
        for <dev@spark.apache.org>; Sun, 01 Mar 2015 15:14:24 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=PZI9e3EsADxF2ddeQ4uzDnN4XEwBEdvddTZLQA782tw=;
        b=POhQR6netMzs5xPrBU/K3wyZYIhuEPLIT4WbxVgyf87ZhcARsbFASjifO0hQLVE99w
         NTTuW8/wZDycsNqTsmZ5s/3Ye5Gs+kCbOjO8ennZvtl5Arn+lA+2nP4a1lACuPIRyU/V
         q7fE2Rmwq13LgsqO01NgIYQzmBXoy1SG6lfKz1sg3ndA9BG4CtPmfPSjB1euTXGIA6tB
         42M5T9R0uEWTxxBHHQLsoD8otRJ7dO7Tn//m04/SxwNLJuJ9sKUcL6Pa5vaqQwXLafBm
         OJv4pY/Oyr/uq1CzBzhBD6XHxBoOvL2W5N+sS9a84FEvGO8ZkBviL0UtNntx3OiQqdOm
         yQrw==
X-Gm-Message-State: ALoCoQn/7c1YM4cMrKwB68p20lQJR+pWM22h8kfcjtIfUaaFdjj73gZS3IPvkxMiUFcphwF+bgLI
MIME-Version: 1.0
X-Received: by 10.152.42.133 with SMTP id o5mr21852984lal.21.1425251663919;
 Sun, 01 Mar 2015 15:14:23 -0800 (PST)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.25.160.202 with HTTP; Sun, 1 Mar 2015 15:14:23 -0800 (PST)
In-Reply-To: <CAOhmDzeRAkkJp9a2o1WV09y5xZ8GFVJwie9m+tGnT51rjuJtZQ@mail.gmail.com>
References: <CAOhmDzeRAkkJp9a2o1WV09y5xZ8GFVJwie9m+tGnT51rjuJtZQ@mail.gmail.com>
Date: Sun, 1 Mar 2015 15:14:23 -0800
Message-ID: <CAKx7Bf8SV4tVF+d=qHw1hzCR=eKSWTCoF+DBrpU-2f0j7KijJQ@mail.gmail.com>
Subject: Re: spark-ec2 default to Hadoop 2
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c366e624d442051042435e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c366e624d442051042435e
Content-Type: text/plain; charset=UTF-8

One reason I wouldn't change the default is that the Hadoop 2 launched by
spark-ec2 is not a full Hadoop 2 distribution -- Its more of a hybrid
Hadoop version built using CDH4 (it uses HDFS 2, but not YARN AFAIK).

Also our default Hadoop version in the Spark build is still 1.0.4 [1], so
it makes sense to stick to that in spark-ec2 as well ?

[1] https://github.com/apache/spark/blob/master/pom.xml#L122

Thanks
Shivaram

On Sun, Mar 1, 2015 at 2:59 PM, Nicholas Chammas <nicholas.chammas@gmail.com
> wrote:

>
> https://github.com/apache/spark/blob/fd8d283eeb98e310b1e85ef8c3a8af9e547ab5e0/ec2/spark_ec2.py#L162-L164
>
> Is there any reason we shouldn't update the default Hadoop major version in
> spark-ec2 to 2?
>
> Nick
>

--001a11c366e624d442051042435e--

From dev-return-11821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  1 23:42:25 2015
Return-Path: <dev-return-11821-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CAA54179B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  1 Mar 2015 23:42:25 +0000 (UTC)
Received: (qmail 29563 invoked by uid 500); 1 Mar 2015 23:42:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29490 invoked by uid 500); 1 Mar 2015 23:42:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29478 invoked by uid 99); 1 Mar 2015 23:42:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Mar 2015 23:42:24 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 01 Mar 2015 23:42:20 +0000
Received: by mail-oi0-f41.google.com with SMTP id z81so24242862oif.0
        for <dev@spark.apache.org>; Sun, 01 Mar 2015 15:40:29 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=tnomKjMUVkjgJZY2hOg4ylY+uINbu2W16m6GuSk+tqI=;
        b=EjVvn3ZHNclBP4APTEAP6gsZYfbRI07F09I8AZ14wxxzzCVdees7BSZTq2jY74Ccmo
         CLUSTsein17erGz3i4kiCsMUXXOktIL4BHegR1AaNQuWALqi5TNpBe6LDfbHT2C7VPoY
         /92R5Dk/nOMmvqDx7K9li9jb1iGMdV3nj7SBUC5IVX/RBb0WX/U24xcRVg9UW06pcoBT
         BDLfliEmfuJZI1K+1022uyoPrzmCVitmlp4pAOTRRIILTm30hv+RyxHxJ77Wr00lN3C6
         4wQZkFe2mTkGN0O44/qqpvWVn0RdSKISGa10pbIWnHLlzr3sRKWDNsydBWtvGYlx0A5z
         IN1g==
MIME-Version: 1.0
X-Received: by 10.60.115.99 with SMTP id jn3mr17539700oeb.68.1425253229516;
 Sun, 01 Mar 2015 15:40:29 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Sun, 1 Mar 2015 15:40:29 -0800 (PST)
In-Reply-To: <CAKx7Bf8SV4tVF+d=qHw1hzCR=eKSWTCoF+DBrpU-2f0j7KijJQ@mail.gmail.com>
References: <CAOhmDzeRAkkJp9a2o1WV09y5xZ8GFVJwie9m+tGnT51rjuJtZQ@mail.gmail.com>
	<CAKx7Bf8SV4tVF+d=qHw1hzCR=eKSWTCoF+DBrpU-2f0j7KijJQ@mail.gmail.com>
Date: Sun, 1 Mar 2015 15:40:29 -0800
Message-ID: <CABPQxsuVc4HwPCQF5W-TbAC+MFwyMHWxOOXdoiq+mR-BocLEFQ@mail.gmail.com>
Subject: Re: spark-ec2 default to Hadoop 2
From: Patrick Wendell <pwendell@gmail.com>
To: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah calling it Hadoop 2 was a very bad naming choice (of mine!), this
was back when CDH4 was the only real distribution available with some
of the newer Hadoop API's and packaging.

I think to not surprise people using this, it's best to keep v1 as the
default. Overall, we try not to change default values too often to
make upgrading easy for people.

- Patrick

On Sun, Mar 1, 2015 at 3:14 PM, Shivaram Venkataraman
<shivaram@eecs.berkeley.edu> wrote:
> One reason I wouldn't change the default is that the Hadoop 2 launched by
> spark-ec2 is not a full Hadoop 2 distribution -- Its more of a hybrid
> Hadoop version built using CDH4 (it uses HDFS 2, but not YARN AFAIK).
>
> Also our default Hadoop version in the Spark build is still 1.0.4 [1], so
> it makes sense to stick to that in spark-ec2 as well ?
>
> [1] https://github.com/apache/spark/blob/master/pom.xml#L122
>
> Thanks
> Shivaram
>
> On Sun, Mar 1, 2015 at 2:59 PM, Nicholas Chammas <nicholas.chammas@gmail.com
>> wrote:
>
>>
>> https://github.com/apache/spark/blob/fd8d283eeb98e310b1e85ef8c3a8af9e547ab5e0/ec2/spark_ec2.py#L162-L164
>>
>> Is there any reason we shouldn't update the default Hadoop major version in
>> spark-ec2 to 2?
>>
>> Nick
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 07:52:59 2015
Return-Path: <dev-return-11822-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92D2310362
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 07:52:59 +0000 (UTC)
Received: (qmail 38577 invoked by uid 500); 2 Mar 2015 07:52:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38496 invoked by uid 500); 2 Mar 2015 07:52:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38480 invoked by uid 99); 2 Mar 2015 07:52:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 07:52:48 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 07:52:24 +0000
Received: by wghl2 with SMTP id l2so31658698wgh.9
        for <dev@spark.apache.org>; Sun, 01 Mar 2015 23:50:06 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=bbF1ORGKVoPAB+QhFYf8uP5owiYLf5+UcAwsLddH6yg=;
        b=auqazB7M1gYK7NwXAadz/Py3uiDWj2l/rp5qDWQ19xrCRRamS80hzq5Nyugi4DSzub
         /juOtbV6+UuIKlQbKf+Mb2iNuwgOsr1rHCzYWlsEaQpw02nAOjwIlquKoYTV7K8xU+CW
         HYK+XDuZuBSvirmVzyk805nHeXsrxKCyLftms3NW19Q19x1A4qQjGHiaYqHDog0mTGKN
         bjoAouTgAiwvsli500OpS3W+4Mid1839UY+pdeH80vx625mIexjheAOVIAh4z80FA7hp
         DlBbQFbgrckKR2LZJkA7WpTV833rED6Caxqy7wbNhZdOWkUHJDmYm+gOLS0z404IGIb9
         0LiQ==
X-Gm-Message-State: ALoCoQkyO4wNYw7TM2ndLrDVPFspYOR9Tuef+ZlOZRNQMRrgSuaKVZkBrcQPDSJxahmQ3uQfUQU3
X-Received: by 10.180.106.70 with SMTP id gs6mr33154767wib.39.1425282606758;
 Sun, 01 Mar 2015 23:50:06 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.83.209 with HTTP; Sun, 1 Mar 2015 23:49:46 -0800 (PST)
In-Reply-To: <CAOhmDzeRAkkJp9a2o1WV09y5xZ8GFVJwie9m+tGnT51rjuJtZQ@mail.gmail.com>
References: <CAOhmDzeRAkkJp9a2o1WV09y5xZ8GFVJwie9m+tGnT51rjuJtZQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 2 Mar 2015 07:49:46 +0000
Message-ID: <CAMAsSdJ+WhVEGR5rd1zWvA_mrF74syDDkJg2qRAi48KQBc8m8A@mail.gmail.com>
Subject: Re: spark-ec2 default to Hadoop 2
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I agree with that. My anecdotal impression is that Hadoop 1.x usage
out there is maybe a couple percent, and so we should shift towards
2.x at least as defaults.

On Sun, Mar 1, 2015 at 10:59 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> https://github.com/apache/spark/blob/fd8d283eeb98e310b1e85ef8c3a8af9e547ab5e0/ec2/spark_ec2.py#L162-L164
>
> Is there any reason we shouldn't update the default Hadoop major version in
> spark-ec2 to 2?
>
> Nick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 09:58:15 2015
Return-Path: <dev-return-11823-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D3C31086C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 09:58:15 +0000 (UTC)
Received: (qmail 87939 invoked by uid 500); 2 Mar 2015 09:58:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87859 invoked by uid 500); 2 Mar 2015 09:58:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87848 invoked by uid 99); 2 Mar 2015 09:58:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 09:58:12 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 09:58:07 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 5951A1596B99
	for <dev@spark.apache.org>; Mon,  2 Mar 2015 01:55:57 -0800 (PST)
Date: Mon, 2 Mar 2015 02:55:56 -0700 (MST)
From: Wail <w.alkowaileet@cces-kacst-mit.org>
To: dev@spark.apache.org
Message-ID: <1425290156896-10835.post@n3.nabble.com>
Subject: Is SparkSQL optimizer aware of the needed data after the query?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Dears,

I'm just curious about the complexity of the query optimizer. Can the
optimizer evaluates what after the SQL? maybe it's a stupid question ,, but
here is an example to show the case:

>From the Spark SQL example:
val teenagers = sqlContext.sql("SELECT * FROM people WHERE age >= 13 AND age
<= 19")

if(condition)
{
    teenagers.map(t => "Name: " + t(0)).collect().foreach(println)
}
else
{
    teenagers.map(t => "Age: " + t(1)).collect().foreach(println)
}

As for instance ... is the optimizer aware that I need only one column and
pushes down the projection to bring only one  as needed?

Thanks!




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Is-SparkSQL-optimizer-aware-of-the-needed-data-after-the-query-tp10835.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 13:18:00 2015
Return-Path: <dev-return-11824-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 400E810EA5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 13:18:00 +0000 (UTC)
Received: (qmail 52997 invoked by uid 500); 2 Mar 2015 13:17:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52924 invoked by uid 500); 2 Mar 2015 13:17:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52906 invoked by uid 99); 2 Mar 2015 13:17:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 13:17:58 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dirceu.semighini@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 13:17:32 +0000
Received: by mail-ob0-f176.google.com with SMTP id wo20so31049964obc.7
        for <dev@spark.apache.org>; Mon, 02 Mar 2015 05:17:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=aHcCPHJoP2lUUFB+9T7do+i3YY+E6Ra7HMk7FLdRTc0=;
        b=XrL+dOo52fq3evJmISYBG5H5FqD3R/Xw5ek9psu5lO25NC51+RbZBZwbHNpfJQGcJ0
         y/eVIw6Xi4mAnZayI3y8gNtuLnxm+H3k2UR4Uw/qo/qnfx6NbkQ4LmtKpPuunRvFEVz3
         VYcwuemQH6xm+uIhr2GhPDnC0AZju3gIyDH7Cq311hBhooQZ2e6KidUh+v4Qn2MrA1s/
         t6PeNlLusLEBpTueAUEU4ZbFekbyCI/ESm6yNkqDClN2ArPB47Sjk/NQkFHcIjgB+UKS
         WnD4/OokZ7qvI8uz6osk0E/pl0pJjAnLIXqiRlLXcQMKwZMN2l9HIVZSFviEBwpzJq/Z
         SV7Q==
X-Received: by 10.202.105.211 with SMTP id e202mr18266724oic.134.1425302250920;
 Mon, 02 Mar 2015 05:17:30 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.88.215 with HTTP; Mon, 2 Mar 2015 05:16:50 -0800 (PST)
In-Reply-To: <CADkoF-qJLKGYRnRvTSei8O7q1re6YVeQmEjq6wmVfqmpbgVOwg@mail.gmail.com>
References: <201502281558413298157@163.com> <CADkoF-qJLKGYRnRvTSei8O7q1re6YVeQmEjq6wmVfqmpbgVOwg@mail.gmail.com>
From: Dirceu Semighini Filho <dirceu.semighini@gmail.com>
Date: Mon, 2 Mar 2015 10:16:50 -0300
Message-ID: <CAO4-Pq883P9RuJLG6sLFTjR813=tq8ayLcuhh6OX3rh9q6fM4w@mail.gmail.com>
Subject: Re: How to create a Row from a List or Array in Spark using Scala
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a114034e05d1f9105104e0a40
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114034e05d1f9105104e0a40
Content-Type: text/plain; charset=UTF-8

You can use the parallelize method:

val data = List(
  Row(1, 5, "vlr1", 10.5),
  Row(2, 1, "vl3", 0.1),
  Row(3, 8, "vl3", 10.0),
  Row(4, 1, "vl4", 1.0))
val rdd = sc.parallelize(data)

Here I'm using a list of Rows, but you could use it with a list of
other kind of object, like this:


val x = sc.parallelize(List("a","b","c"))

Where x is an RDD[String] and sc is the spark context.


Regards,

Dirceu


2015-02-28 5:37 GMT-03:00 DEVAN M.S. <msdevanms@gmail.com>:

>   In scala API its there, Row.fromSeq(ARRAY), I dnt know much more
> about java api
>
>
>
> Devan M.S. | Research Associate | Cyber Security | AMRITA VISHWA
> VIDYAPEETHAM | Amritapuri | Cell +919946535290 |
>
>
> On Sat, Feb 28, 2015 at 1:28 PM, r7raul1984@163.com <r7raul1984@163.com>
> wrote:
>
> > import org.apache.spark.sql.catalyst.expressions._
> >
> > val values: JavaArrayList[Any] = new JavaArrayList()
> > computedValues = Row(values.get(0),values.get(1)) //It is not good by use
> > get(index).  How to create a Row from a List or Array in Spark using
> Scala .
> >
> >
> >
> > r7raul1984@163.com
> >
>

--001a114034e05d1f9105104e0a40--

From dev-return-11825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 17:23:15 2015
Return-Path: <dev-return-11825-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1ED2217BD8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 17:23:15 +0000 (UTC)
Received: (qmail 37836 invoked by uid 500); 2 Mar 2015 17:23:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37756 invoked by uid 500); 2 Mar 2015 17:23:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37745 invoked by uid 99); 2 Mar 2015 17:23:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 17:23:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 209.85.217.174 as permitted sender)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 17:22:48 +0000
Received: by lbjf15 with SMTP id f15so3628338lbj.2
        for <dev@spark.apache.org>; Mon, 02 Mar 2015 09:22:00 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=u9Nzf7I7W1bW2H3rHRHYoI9+dcQd5d32caTw8plYW+c=;
        b=AaKsHZ1TeCJjfjwZEiyt+Uz2NF70urQJSMUdmv9W0AI5N5OUZCSJTrAX6ysF1btma5
         ZxwBMReojpxfDZ3ZBrRONXORZDtk1kD2XsOOQPNA2b1ZVjFmMRNWjMbNCtg4S69ydJ1n
         bC3qKUK24gPMeii+OFoRs71yZ+JLl2+vDFAA8hbWbxVWsfqoW0tG3L8SZ/XWOSPre/s5
         Fpvw0mznBw6gc3tNNUFrHn/osT6uBlIMeNX4PC+LsNCDCHePTi02BNIredUdPbY8fm4m
         kHPgDHm0cFdJZkxviSNqJkfDD7nqtkYTiGeqezbnd9xAj0c75uQcRdeju34jN3jIA8hm
         cSqw==
X-Gm-Message-State: ALoCoQlguaDod7w9mmhs2cB1IMi7jZQMWAXWug1gay9QpoeCLWw/u59ZKwHKQRcJyQb/HrE9clzM
MIME-Version: 1.0
X-Received: by 10.152.36.167 with SMTP id r7mr22580884laj.7.1425316920781;
 Mon, 02 Mar 2015 09:22:00 -0800 (PST)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.25.160.202 with HTTP; Mon, 2 Mar 2015 09:22:00 -0800 (PST)
In-Reply-To: <CAMAsSdJ+WhVEGR5rd1zWvA_mrF74syDDkJg2qRAi48KQBc8m8A@mail.gmail.com>
References: <CAOhmDzeRAkkJp9a2o1WV09y5xZ8GFVJwie9m+tGnT51rjuJtZQ@mail.gmail.com>
	<CAMAsSdJ+WhVEGR5rd1zWvA_mrF74syDDkJg2qRAi48KQBc8m8A@mail.gmail.com>
Date: Mon, 2 Mar 2015 09:22:00 -0800
Message-ID: <CAKx7Bf904ZfAicV5N9Ocw3KLAUPsVFptZoSmJ52602Hc8Pkj1Q@mail.gmail.com>
Subject: Re: spark-ec2 default to Hadoop 2
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: Sean Owen <sowen@cloudera.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160b8a8c17ff105105174d0
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b8a8c17ff105105174d0
Content-Type: text/plain; charset=UTF-8

FWIW there is a PR open to add support for Hadoop 2.4 to spark-ec2 scripts
at https://github.com/mesos/spark-ec2/pull/77 -- But it hasnt' received
much review or testing to be merged.

Thanks
Shivaram

On Sun, Mar 1, 2015 at 11:49 PM, Sean Owen <sowen@cloudera.com> wrote:

> I agree with that. My anecdotal impression is that Hadoop 1.x usage
> out there is maybe a couple percent, and so we should shift towards
> 2.x at least as defaults.
>
> On Sun, Mar 1, 2015 at 10:59 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> >
> https://github.com/apache/spark/blob/fd8d283eeb98e310b1e85ef8c3a8af9e547ab5e0/ec2/spark_ec2.py#L162-L164
> >
> > Is there any reason we shouldn't update the default Hadoop major version
> in
> > spark-ec2 to 2?
> >
> > Nick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e0160b8a8c17ff105105174d0--

From dev-return-11826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 19:43:02 2015
Return-Path: <dev-return-11826-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC4CC103EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 19:43:02 +0000 (UTC)
Received: (qmail 25884 invoked by uid 500); 2 Mar 2015 19:42:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25813 invoked by uid 500); 2 Mar 2015 19:42:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25481 invoked by uid 99); 2 Mar 2015 19:42:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 19:42:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 19:42:08 +0000
Received: by igdh15 with SMTP id h15so20361226igd.3
        for <dev@spark.apache.org>; Mon, 02 Mar 2015 11:42:06 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=BotTN5bescTSJnWf3uIFsDylmBKT65Dpo8e4HRG9xE0=;
        b=rmi54JOcn1MV7LW3hFybA9dyyVD/hgcpmR1Cyc35cCnfxfMz6G+MQfBm6ByWlxw1X8
         4LYAz3Yft2jBDjW1UPysi4S16Su+feY8jyLSucsrVqt1s+4C2Feg8GiH1SKfM02c4CXD
         B4SYMY+O0p75t+Hv/USdlcEpUrJK6jM3pZCYjzhywBSdWy1k1/YuUud73YqgxrhKY7If
         RvTNJu43Hw3x4PWSjcOppdk/+YtIv5fklXBsKuVTlpaxkjSFmLcN9fZAqj50FLGodw0p
         wkVzyEsrskEDJaA0p/1FHaG7QiWYr84UTPJn9MH/x4s8lFbojAGwicAPvl00rNWuqNqN
         jXPg==
MIME-Version: 1.0
X-Received: by 10.50.25.225 with SMTP id f1mr24463046igg.29.1425325326665;
 Mon, 02 Mar 2015 11:42:06 -0800 (PST)
Received: by 10.36.99.76 with HTTP; Mon, 2 Mar 2015 11:42:06 -0800 (PST)
In-Reply-To: <CALR_T9A8ukiDZ4K+uaMvSSR+wFL9a5yHwEe4AV1JCG6GEG7qmQ@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<CALR_T9BJQZTP1jo98BrS3MicX+hXXmvUvDNhNUefO=AMXyALLA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE0314C@G9W0737.americas.hpqcorp.net>
	<87ioeo5n6e.fsf@gmail.com>
	<CAJgQjQ9q2wEu-URc6OkNf+rVriX+FDcViSBM-die2HyCpRC=-A@mail.gmail.com>
	<CALR_T9BsNT9SBAveH7z+Aw-CYB+NFPxGH0Rm_JNsjHu+RhMqsQ@mail.gmail.com>
	<CAJgQjQ8e8S4sdbZ+bPnDS7TsOthX2zv89707W2r7FVDsf4n9ZQ@mail.gmail.com>
	<CALR_T9A8ukiDZ4K+uaMvSSR+wFL9a5yHwEe4AV1JCG6GEG7qmQ@mail.gmail.com>
Date: Mon, 2 Mar 2015 11:42:06 -0800
Message-ID: <CAJgQjQ-nGwEmoPhv5nLiv1HRRzmiUXeaDJAtakzPk5urbR7BSQ@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Xiangrui Meng <mengxr@gmail.com>
To: Sam Halliday <sam.halliday@gmail.com>
Cc: Joseph Bradley <joseph@databricks.com>, Alexander Ulanov <alexander.ulanov@hp.com>, 
	dev <dev@spark.apache.org>, "Evan R. Sparks" <evan.sparks@gmail.com>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

On Fri, Feb 27, 2015 at 12:33 PM, Sam Halliday <sam.halliday@gmail.com> wro=
te:
> Also, check the JNILoader output.
>
> Remember, for netlib-java to use your system libblas all you need to do i=
s
> setup libblas.so.3 like any native application would expect.
>
> I haven't ever used the cublas "real BLAS"  implementation, so I'd be
> interested to hear about this. Do an 'ldd /usr/lib/libblas.so.3' to check
> that all the runtime links are in order.
>

There are two shared libraries in this hybrid setup. nvblas.so must be
loaded before libblas.so to intercept level 3 routines using GPU. More
details are at: http://docs.nvidia.com/cuda/nvblas/index.html#Usage

> Btw, I have some DGEMM wrappers in my netlib-java performance module... a=
nd
> I also planned to write more in MultiBLAS (until I mothballed the project
> for the hardware to catch up, which is probably has and now I just need a
> reason to look at it)
>
> On 27 Feb 2015 20:26, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>>
>> Hey Sam,
>>
>> The running times are not "big O" estimates:
>>
>> > The CPU version finished in 12 seconds.
>> > The CPU->GPU->CPU version finished in 2.2 seconds.
>> > The GPU version finished in 1.7 seconds.
>>
>> I think there is something wrong with the netlib/cublas combination.
>> Sam already mentioned that cuBLAS doesn't implement the CPU BLAS
>> interfaces. I checked the CUDA doc and it seems that to use GPU BLAS
>> through the CPU BLAS interface we need to use NVBLAS, which intercepts
>> some Level 3 CPU BLAS calls (including GEMM). So we need to load
>> nvblas.so first and then some CPU BLAS library in JNI. I wonder
>> whether the setup was correct.
>>
>> Alexander, could you check whether GPU is used in the netlib-cublas
>> experiments? You can tell it by watching CPU/GPU usage.
>>
>> Best,
>> Xiangrui
>>
>> On Thu, Feb 26, 2015 at 10:47 PM, Sam Halliday <sam.halliday@gmail.com>
>> wrote:
>> > Don't use "big O" estimates, always measure. It used to work back in t=
he
>> > days when double multiplication was a bottleneck. The computation cost
>> > is
>> > effectively free on both the CPU and GPU and you're seeing pure copyin=
g
>> > costs. Also, I'm dubious that cublas is doing what you think it is. Ca=
n
>> > you
>> > link me to the source code for DGEMM?
>> >
>> > I show all of this in my talk, with explanations, I can't stress enoug=
h
>> > how
>> > much I recommend that you watch it if you want to understand high
>> > performance hardware acceleration for linear algebra :-)
>> >
>> > On 27 Feb 2015 01:42, "Xiangrui Meng" <mengxr@gmail.com> wrote:
>> >>
>> >> The copying overhead should be quadratic on n, while the computation
>> >> cost is cubic on n. I can understand that netlib-cublas is slower tha=
n
>> >> netlib-openblas on small problems. But I'm surprised to see that it i=
s
>> >> still 20x slower on 10000x10000. I did the following on a g2.2xlarge
>> >> instance with BIDMat:
>> >>
>> >> val n =3D 10000
>> >>
>> >> val f =3D rand(n, n)
>> >> flip; f*f; val rf =3D flop
>> >>
>> >> flip; val g =3D GMat(n, n); g.copyFrom(f); (g*g).toFMat(null); val rg=
 =3D
>> >> flop
>> >>
>> >> flip; g*g; val rgg =3D flop
>> >>
>> >> The CPU version finished in 12 seconds.
>> >> The CPU->GPU->CPU version finished in 2.2 seconds.
>> >> The GPU version finished in 1.7 seconds.
>> >>
>> >> I'm not sure whether my CPU->GPU->CPU code simulates the netlib-cubla=
s
>> >> path. But based on the result, the data copying overhead is definitel=
y
>> >> not as big as 20x at n =3D 10000.
>> >>
>> >> Best,
>> >> Xiangrui
>> >>
>> >>
>> >> On Thu, Feb 26, 2015 at 2:21 PM, Sam Halliday <sam.halliday@gmail.com=
>
>> >> wrote:
>> >> > I've had some email exchanges with the author of BIDMat: it does
>> >> > exactly
>> >> > what you need to get the GPU benefit and writes higher level
>> >> > algorithms
>> >> > entirely in the GPU kernels so that the memory stays there as long =
as
>> >> > possible. The restriction with this approach is that it is only
>> >> > offering
>> >> > high-level algorithms so is not a toolkit for applied mathematics
>> >> > research and development --- but it works well as a toolkit for
>> >> > higher
>> >> > level analysis (e.g. for analysts and practitioners).
>> >> >
>> >> > I believe BIDMat's approach is the best way to get performance out =
of
>> >> > GPU hardware at the moment but I also have strong evidence to sugge=
st
>> >> > that the hardware will catch up and the memory transfer costs betwe=
en
>> >> > CPU/GPU will disappear meaning that there will be no need for custo=
m
>> >> > GPU
>> >> > kernel implementations. i.e. please continue to use BLAS primitives
>> >> > when
>> >> > writing new algorithms and only go to the GPU for an alternative
>> >> > optimised implementation.
>> >> >
>> >> > Note that CUDA and cuBLAS are *not* BLAS. They are BLAS-like, and
>> >> > offer
>> >> > an API that looks like BLAS but takes pointers to special regions i=
n
>> >> > the
>> >> > GPU memory region. Somebody has written a wrapper around CUDA to
>> >> > create
>> >> > a proper BLAS library but it only gives marginal performance over t=
he
>> >> > CPU because of the memory transfer overhead.
>> >> >
>> >> > This slide from my talk
>> >> >
>> >> >   http://fommil.github.io/scalax14/#/11/2
>> >> >
>> >> > says it all. X axis is matrix size, Y axis is logarithmic time to d=
o
>> >> > DGEMM. Black line is the "cheating" time for the GPU and the green
>> >> > line
>> >> > is after copying the memory to/from the GPU memory. APUs have the
>> >> > potential to eliminate the green line.
>> >> >
>> >> > Best regards,
>> >> > Sam
>> >> >
>> >> >
>> >> >
>> >> > "Ulanov, Alexander" <alexander.ulanov@hp.com> writes:
>> >> >
>> >> >> Evan, thank you for the summary. I would like to add some more
>> >> >> observations. The GPU that I used is 2.5 times cheaper than the CP=
U
>> >> >> ($250 vs
>> >> >> $100). They both are 3 years old. I've also did a small test with
>> >> >> modern
>> >> >> hardware, and the new GPU nVidia Titan was slightly more than 1
>> >> >> order of
>> >> >> magnitude faster than Intel E5-2650 v2 for the same tests. However=
,
>> >> >> it costs
>> >> >> as much as CPU ($1200). My takeaway is that GPU is making a better
>> >> >> price/value progress.
>> >> >>
>> >> >>
>> >> >>
>> >> >> Xiangrui, I was also surprised that BIDMat-cuda was faster than
>> >> >> netlib-cuda and the most reasonable explanation is that it holds t=
he
>> >> >> result
>> >> >> in GPU memory, as Sam suggested. At the same time, it is OK becaus=
e
>> >> >> you can
>> >> >> copy the result back from GPU only when needed. However, to be sur=
e,
>> >> >> I am
>> >> >> going to ask the developer of BIDMat on his upcoming talk.
>> >> >>
>> >> >>
>> >> >>
>> >> >> Best regards, Alexander
>> >> >>
>> >> >>
>> >> >> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>> >> >> Sent: Thursday, February 26, 2015 1:56 PM
>> >> >> To: Xiangrui Meng
>> >> >> Cc: dev@spark.apache.org; Joseph Bradley; Ulanov, Alexander; Evan =
R.
>> >> >> Sparks
>> >> >> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >> >>
>> >> >>
>> >> >> Btw, I wish people would stop cheating when comparing CPU and GPU
>> >> >> timings for things like matrix multiply :-P
>> >> >>
>> >> >> Please always compare apples with apples and include the time it
>> >> >> takes
>> >> >> to set up the matrices, send it to the processing unit, doing the
>> >> >> calculation AND copying it back to where you need to see the
>> >> >> results.
>> >> >>
>> >> >> Ignoring this method will make you believe that your GPU is
>> >> >> thousands
>> >> >> of times faster than it really is. Again, jump to the end of my ta=
lk
>> >> >> for
>> >> >> graphs and more discussion....  especially the bit about me being
>> >> >> keen on
>> >> >> funding to investigate APU hardware further ;-) (I believe it will
>> >> >> solve the
>> >> >> problem)
>> >> >> On 26 Feb 2015 21:16, "Xiangrui Meng"
>> >> >> <mengxr@gmail.com<mailto:mengxr@gmail.com>> wrote:
>> >> >> Hey Alexander,
>> >> >>
>> >> >> I don't quite understand the part where netlib-cublas is about 20x
>> >> >> slower than netlib-openblas. What is the overhead of using a GPU
>> >> >> BLAS
>> >> >> with netlib-java?
>> >> >>
>> >> >> CC'ed Sam, the author of netlib-java.
>> >> >>
>> >> >> Best,
>> >> >> Xiangrui
>> >> >>
>> >> >> On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley
>> >> >> <joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
>> >> >>> Better documentation for linking would be very helpful!  Here's a
>> >> >>> JIRA:
>> >> >>> https://issues.apache.org/jira/browse/SPARK-6019
>> >> >>>
>> >> >>>
>> >> >>> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> >> >>> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>> >> >>> wrote:
>> >> >>>
>> >> >>>> Thanks for compiling all the data and running these benchmarks,
>> >> >>>> Alex.
>> >> >>>> The
>> >> >>>> big takeaways here can be seen with this chart:
>> >> >>>>
>> >> >>>>
>> >> >>>>
>> >> >>>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF=
50uZHl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>> >> >>>>
>> >> >>>> 1) A properly configured GPU matrix multiply implementation (e.g=
.
>> >> >>>> BIDMat+GPU) can provide substantial (but less than an order of
>> >> >>>> magnitude)
>> >> >>>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>> >> >>>> netlib-java+openblas-compiled).
>> >> >>>> 2) A poorly tuned CPU implementation can be 1-2 orders of
>> >> >>>> magnitude
>> >> >>>> worse
>> >> >>>> than a well-tuned CPU implementation, particularly for larger
>> >> >>>> matrices.
>> >> >>>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib -
>> >> >>>> this
>> >> >>>> basically agrees with the authors own benchmarks (
>> >> >>>> https://github.com/fommil/netlib-java)
>> >> >>>>
>> >> >>>> I think that most of our users are in a situation where using GP=
Us
>> >> >>>> may not
>> >> >>>> be practical - although we could consider having a good GPU
>> >> >>>> backend
>> >> >>>> available as an option. However, *ALL* users of MLlib could
>> >> >>>> benefit
>> >> >>>> (potentially tremendously) from using a well-tuned CPU-based BLA=
S
>> >> >>>> implementation. Perhaps we should consider updating the mllib
>> >> >>>> guide
>> >> >>>> with a
>> >> >>>> more complete section for enabling high performance binaries on
>> >> >>>> OSX
>> >> >>>> and
>> >> >>>> Linux? Or better, figure out a way for the system to fetch these
>> >> >>>> automatically.
>> >> >>>>
>> >> >>>> - Evan
>> >> >>>>
>> >> >>>>
>> >> >>>>
>> >> >>>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>> >> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>> >> >>>>
>> >> >>>>> Just to summarize this thread, I was finally able to make all
>> >> >>>>> performance
>> >> >>>>> comparisons that we discussed. It turns out that:
>> >> >>>>> BIDMat-cublas>>BIDMat
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openbl=
as-yum-repo=3D=3Dnetlib-cublas>netlib-blas>f2jblas
>> >> >>>>>
>> >> >>>>> Below is the link to the spreadsheet with full results.
>> >> >>>>>
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQg=
HUMx378T9J5r7kwKSPkY/edit?usp=3Dsharing
>> >> >>>>>
>> >> >>>>> One thing still needs exploration: does BIDMat-cublas perform
>> >> >>>>> copying
>> >> >>>>> to/from machine=E2=80=99s RAM?
>> >> >>>>>
>> >> >>>>> -----Original Message-----
>> >> >>>>> From: Ulanov, Alexander
>> >> >>>>> Sent: Tuesday, February 10, 2015 2:12 PM
>> >> >>>>> To: Evan R. Sparks
>> >> >>>>> Cc: Joseph Bradley;
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >> >>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>> >> >>>>>
>> >> >>>>> Thanks, Evan! It seems that ticket was marked as duplicate thou=
gh
>> >> >>>>> the
>> >> >>>>> original one discusses slightly different topic. I was able to
>> >> >>>>> link
>> >> >>>>> netlib
>> >> >>>>> with MKL from BIDMat binaries. Indeed, MKL is statically linked
>> >> >>>>> inside a
>> >> >>>>> 60MB library.
>> >> >>>>>
>> >> >>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>> >> >>>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> +--------------------------------------------------------------=
---------+
>> >> >>>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,00255=
6
>> >> >>>>> |
>> >> >>>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>> >> >>>>> |1,638475459 |
>> >> >>>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,09352=
11
>> >> >>>>> |
>> >> >>>>> 1569,233228 |
>> >> >>>>>
>> >> >>>>> It turn out that pre-compiled MKL is faster than precompiled
>> >> >>>>> OpenBlas on
>> >> >>>>> my machine. Probably, I=E2=80=99ll add two more columns with lo=
cally
>> >> >>>>> compiled
>> >> >>>>> openblas and cuda.
>> >> >>>>>
>> >> >>>>> Alexander
>> >> >>>>>
>> >> >>>>> From: Evan R. Sparks
>> >> >>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>> >> >>>>> Sent: Monday, February 09, 2015 6:06 PM
>> >> >>>>> To: Ulanov, Alexander
>> >> >>>>> Cc: Joseph Bradley;
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >> >>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >> >>>>>
>> >> >>>>> Great - perhaps we can move this discussion off-list and onto a
>> >> >>>>> JIRA
>> >> >>>>> ticket? (Here's one:
>> >> >>>>> https://issues.apache.org/jira/browse/SPARK-5705)
>> >> >>>>>
>> >> >>>>> It seems like this is going to be somewhat exploratory for a
>> >> >>>>> while
>> >> >>>>> (and
>> >> >>>>> there's probably only a handful of us who really care about fas=
t
>> >> >>>>> linear
>> >> >>>>> algebra!)
>> >> >>>>>
>> >> >>>>> - Evan
>> >> >>>>>
>> >> >>>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
>> >> >>>>> wrote:
>> >> >>>>> Hi Evan,
>> >> >>>>>
>> >> >>>>> Thank you for explanation and useful link. I am going to build
>> >> >>>>> OpenBLAS,
>> >> >>>>> link it with Netlib-java and perform benchmark again.
>> >> >>>>>
>> >> >>>>> Do I understand correctly that BIDMat binaries contain statical=
ly
>> >> >>>>> linked
>> >> >>>>> Intel MKL BLAS? It might be the reason why I am able to run
>> >> >>>>> BIDMat
>> >> >>>>> not
>> >> >>>>> having MKL BLAS installed on my server. If it is true, I wonder
>> >> >>>>> if
>> >> >>>>> it is OK
>> >> >>>>> because Intel sells this library. Nevertheless, it seems that i=
n
>> >> >>>>> my
>> >> >>>>> case
>> >> >>>>> precompiled MKL BLAS performs better than precompiled OpenBLAS
>> >> >>>>> given
>> >> >>>>> that
>> >> >>>>> BIDMat and Netlib-java are supposed to be on par with JNI
>> >> >>>>> overheads.
>> >> >>>>>
>> >> >>>>> Though, it might be interesting to link Netlib-java with Intel
>> >> >>>>> MKL,
>> >> >>>>> as
>> >> >>>>> you suggested. I wonder, are John Canny (BIDMat) and Sam Hallid=
ay
>> >> >>>>> (Netlib-java) interested to compare their libraries.
>> >> >>>>>
>> >> >>>>> Best regards, Alexander
>> >> >>>>>
>> >> >>>>> From: Evan R. Sparks
>> >> >>>>>
>> >> >>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mai=
lto:
>> >> >>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >> >>>>> Sent: Friday, February 06, 2015 5:58 PM
>> >> >>>>>
>> >> >>>>> To: Ulanov, Alexander
>> >> >>>>> Cc: Joseph Bradley;
>> >> >>>>>
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@sp=
ark.apache.org<mailto:dev@spark.apache.org>>
>> >> >>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >> >>>>>
>> >> >>>>> I would build OpenBLAS yourself, since good BLAS performance
>> >> >>>>> comes
>> >> >>>>> from
>> >> >>>>> getting cache sizes, etc. set up correctly for your particular
>> >> >>>>> hardware -
>> >> >>>>> this is often a very tricky process (see, e.g. ATLAS), but we
>> >> >>>>> found
>> >> >>>>> that on
>> >> >>>>> relatively modern Xeon chips, OpenBLAS builds quickly and yield=
s
>> >> >>>>> performance competitive with MKL.
>> >> >>>>>
>> >> >>>>> To make sure the right library is getting used, you have to mak=
e
>> >> >>>>> sure
>> >> >>>>> it's first on the search path - export
>> >> >>>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick he=
re.
>> >> >>>>>
>> >> >>>>> For some examples of getting netlib-java setup on an ec2 node a=
nd
>> >> >>>>> some
>> >> >>>>> example benchmarking code we ran a while back, see:
>> >> >>>>> https://github.com/shivaram/matrix-bench
>> >> >>>>>
>> >> >>>>> In particular - build-openblas-ec2.sh shows you how to build th=
e
>> >> >>>>> library
>> >> >>>>> and set up symlinks correctly, and scala/run-netlib.sh shows yo=
u
>> >> >>>>> how
>> >> >>>>> to get
>> >> >>>>> the path setup and get that library picked up by netlib-java.
>> >> >>>>>
>> >> >>>>> In this way - you could probably get cuBLAS set up to be used b=
y
>> >> >>>>> netlib-java as well.
>> >> >>>>>
>> >> >>>>> - Evan
>> >> >>>>>
>> >> >>>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
>> >> >>>>> wrote:
>> >> >>>>> Evan, could you elaborate on how to force BIDMat and netlib-jav=
a
>> >> >>>>> to
>> >> >>>>> force
>> >> >>>>> loading the right blas? For netlib, I there are few JVM flags,
>> >> >>>>> such
>> >> >>>>> as
>> >> >>>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jB=
LAS,
>> >> >>>>> so
>> >> >>>>> I can
>> >> >>>>> force it to use Java implementation. Not sure I understand how =
to
>> >> >>>>> force use
>> >> >>>>> a specific blas (not specific wrapper for blas).
>> >> >>>>>
>> >> >>>>> Btw. I have installed openblas (yum install openblas), so I
>> >> >>>>> suppose
>> >> >>>>> that
>> >> >>>>> netlib is using it.
>> >> >>>>>
>> >> >>>>> From: Evan R. Sparks
>> >> >>>>>
>> >> >>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mai=
lto:
>> >> >>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >> >>>>> Sent: Friday, February 06, 2015 5:19 PM
>> >> >>>>> To: Ulanov, Alexander
>> >> >>>>> Cc: Joseph Bradley;
>> >> >>>>>
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@sp=
ark.apache.org<mailto:dev@spark.apache.org>>
>> >> >>>>>
>> >> >>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >> >>>>>
>> >> >>>>> Getting breeze to pick up the right blas library is critical fo=
r
>> >> >>>>> performance. I recommend using OpenBLAS (or MKL, if you already
>> >> >>>>> have
>> >> >>>>> it).
>> >> >>>>> It might make sense to force BIDMat to use the same underlying
>> >> >>>>> BLAS
>> >> >>>>> library
>> >> >>>>> as well.
>> >> >>>>>
>> >> >>>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
>> >> >>>>> wrote:
>> >> >>>>> Hi Evan, Joseph
>> >> >>>>>
>> >> >>>>> I did few matrix multiplication test and BIDMat seems to be ~10=
x
>> >> >>>>> faster
>> >> >>>>> than netlib-java+breeze (sorry for weird table formatting):
>> >> >>>>>
>> >> >>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>> >> >>>>> native_system_linux_x86-64|
>> >> >>>>> Breeze+Netlib-java f2jblas |
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> +--------------------------------------------------------------=
---------+
>> >> >>>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>> >> >>>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>> >> >>>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 |
>> >> >>>>> 1569,233228 |
>> >> >>>>>
>> >> >>>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM,
>> >> >>>>> Fedora
>> >> >>>>> 19
>> >> >>>>> Linux, Scala 2.11.
>> >> >>>>>
>> >> >>>>> Later I will make tests with Cuda. I need to install new Cuda
>> >> >>>>> version for
>> >> >>>>> this purpose.
>> >> >>>>>
>> >> >>>>> Do you have any ideas why breeze-netlib with native blas is so
>> >> >>>>> much
>> >> >>>>> slower than BIDMat MKL?
>> >> >>>>>
>> >> >>>>> Best regards, Alexander
>> >> >>>>>
>> >> >>>>> From: Joseph Bradley
>> >> >>>>>
>> >> >>>>> [mailto:joseph@databricks.com<mailto:joseph@databricks.com><mai=
lto:
>> >> >>>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>> >> >>>>> Sent: Thursday, February 05, 2015 5:29 PM
>> >> >>>>> To: Ulanov, Alexander
>> >> >>>>> Cc: Evan R. Sparks;
>> >> >>>>>
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@sp=
ark.apache.org<mailto:dev@spark.apache.org>>
>> >> >>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >> >>>>>
>> >> >>>>> Hi Alexander,
>> >> >>>>>
>> >> >>>>> Using GPUs with Spark would be very exciting.  Small comment:
>> >> >>>>> Concerning
>> >> >>>>> your question earlier about keeping data stored on the GPU rath=
er
>> >> >>>>> than
>> >> >>>>> having to move it between main memory and GPU memory on each
>> >> >>>>> iteration, I
>> >> >>>>> would guess this would be critical to getting good performance.
>> >> >>>>> If
>> >> >>>>> you
>> >> >>>>> could do multiple local iterations before aggregating results,
>> >> >>>>> then
>> >> >>>>> the
>> >> >>>>> cost of data movement to the GPU could be amortized (and I
>> >> >>>>> believe
>> >> >>>>> that is
>> >> >>>>> done in practice).  Having Spark be aware of the GPU and using =
it
>> >> >>>>> as
>> >> >>>>> another part of memory sounds like a much bigger undertaking.
>> >> >>>>>
>> >> >>>>> Joseph
>> >> >>>>>
>> >> >>>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>
>> >> >>>>> wrote:
>> >> >>>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach pre=
sentation
>> >> >>>>> by
>> >> >>>>> John
>> >> >>>>> Canny and I am really inspired by his talk and comparisons with
>> >> >>>>> Spark MLlib.
>> >> >>>>>
>> >> >>>>> I am very interested to find out what will be better within
>> >> >>>>> Spark:
>> >> >>>>> BIDMat
>> >> >>>>> or netlib-java with CPU or GPU natives. Could you suggest a fai=
r
>> >> >>>>> way
>> >> >>>>> to
>> >> >>>>> benchmark them? Currently I do benchmarks on artificial neural
>> >> >>>>> networks in
>> >> >>>>> batch mode. While it is not a =E2=80=9Cpure=E2=80=9D test of li=
near algebra, it
>> >> >>>>> involves
>> >> >>>>> some other things that are essential to machine learning.
>> >> >>>>>
>> >> >>>>> From: Evan R. Sparks
>> >> >>>>>
>> >> >>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mai=
lto:
>> >> >>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >> >>>>> Sent: Thursday, February 05, 2015 1:29 PM
>> >> >>>>> To: Ulanov, Alexander
>> >> >>>>> Cc:
>> >> >>>>>
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@sp=
ark.apache.org<mailto:dev@spark.apache.org>>
>> >> >>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >> >>>>>
>> >> >>>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster th=
an
>> >> >>>>> netlib-java+OpenBLAS, but if it is much faster it's probably du=
e
>> >> >>>>> to
>> >> >>>>> data
>> >> >>>>> layout and fewer levels of indirection - it's definitely a
>> >> >>>>> worthwhile
>> >> >>>>> experiment to run. The main speedups I've seen from using it co=
me
>> >> >>>>> from
>> >> >>>>> highly optimized GPU code for linear algebra. I know that in th=
e
>> >> >>>>> past Canny
>> >> >>>>> has gone as far as to write custom GPU kernels for
>> >> >>>>> performance-critical
>> >> >>>>> regions of code.[1]
>> >> >>>>>
>> >> >>>>> BIDMach is highly optimized for single node performance or
>> >> >>>>> performance on
>> >> >>>>> small clusters.[2] Once data doesn't fit easily in GPU memory (=
or
>> >> >>>>> can be
>> >> >>>>> batched in that way) the performance tends to fall off. Canny
>> >> >>>>> argues
>> >> >>>>> for
>> >> >>>>> hardware/software codesign and as such prefers machine
>> >> >>>>> configurations that
>> >> >>>>> are quite different than what we find in most commodity cluster
>> >> >>>>> nodes -
>> >> >>>>> e.g. 10 disk cahnnels and 4 GPUs.
>> >> >>>>>
>> >> >>>>> In contrast, MLlib was designed for horizontal scalability on
>> >> >>>>> commodity
>> >> >>>>> clusters and works best on very big datasets - order of
>> >> >>>>> terabytes.
>> >> >>>>>
>> >> >>>>> For the most part, these projects developed concurrently to
>> >> >>>>> address
>> >> >>>>> slightly different use cases. That said, there may be bits of
>> >> >>>>> BIDMach we
>> >> >>>>> could repurpose for MLlib - keep in mind we need to be careful
>> >> >>>>> about
>> >> >>>>> maintaining cross-language compatibility for our Java and
>> >> >>>>> Python-users,
>> >> >>>>> though.
>> >> >>>>>
>> >> >>>>> - Evan
>> >> >>>>>
>> >> >>>>> [1] - http://arxiv.org/abs/1409.5402
>> >> >>>>> [2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>> >> >>>>>
>> >> >>>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>>
>> >> >>>>> wrote:
>> >> >>>>> Hi Evan,
>> >> >>>>>
>> >> >>>>> Thank you for suggestion! BIDMat seems to have terrific speed. =
Do
>> >> >>>>> you
>> >> >>>>> know what makes them faster than netlib-java?
>> >> >>>>>
>> >> >>>>> The same group has BIDMach library that implements machine
>> >> >>>>> learning.
>> >> >>>>> For
>> >> >>>>> some examples they use Caffe convolutional neural network libra=
ry
>> >> >>>>> owned by
>> >> >>>>> another group in Berkeley. Could you elaborate on how these all
>> >> >>>>> might be
>> >> >>>>> connected with Spark Mllib? If you take BIDMat for linear algeb=
ra
>> >> >>>>> why don=E2=80=99t
>> >> >>>>> you take BIDMach for optimization and learning?
>> >> >>>>>
>> >> >>>>> Best regards, Alexander
>> >> >>>>>
>> >> >>>>> From: Evan R. Sparks
>> >> >>>>>
>> >> >>>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mai=
lto:
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:eva=
n.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>> >> >>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>> >> >>>>> Sent: Thursday, February 05, 2015 12:09 PM
>> >> >>>>> To: Ulanov, Alexander
>> >> >>>>> Cc:
>> >> >>>>>
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@sp=
ark.apache.org<mailto:dev@spark.apache.org>><mailto:
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@sp=
ark.apache.org<mailto:dev@spark.apache.org>>>
>> >> >>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >> >>>>>
>> >> >>>>> I'd expect that we can make GPU-accelerated BLAS faster than CP=
U
>> >> >>>>> blas in
>> >> >>>>> many cases.
>> >> >>>>>
>> >> >>>>> You might consider taking a look at the codepaths that BIDMat (
>> >> >>>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>> >> >>>>> netlib-java/breeze. John Canny et. al. have done a bunch of wor=
k
>> >> >>>>> optimizing
>> >> >>>>> to make this work really fast from Scala. I've run it on my
>> >> >>>>> laptop
>> >> >>>>> and
>> >> >>>>> compared to MKL and in certain cases it's 10x faster at matrix
>> >> >>>>> multiply.
>> >> >>>>> There are a lot of layers of indirection here and you really wa=
nt
>> >> >>>>> to
>> >> >>>>> avoid
>> >> >>>>> data copying as much as possible.
>> >> >>>>>
>> >> >>>>> We could also consider swapping out BIDMat for Breeze, but that
>> >> >>>>> would be
>> >> >>>>> a big project and if we can figure out how to get breeze+cublas
>> >> >>>>> to
>> >> >>>>> comparable performance that would be a big win.
>> >> >>>>>
>> >> >>>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:=
alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>>
>> >> >>>>> wrote:
>> >> >>>>> Dear Spark developers,
>> >> >>>>>
>> >> >>>>> I am exploring how to make linear algebra operations faster
>> >> >>>>> within
>> >> >>>>> Spark.
>> >> >>>>> One way of doing this is to use Scala Breeze library that is
>> >> >>>>> bundled
>> >> >>>>> with
>> >> >>>>> Spark. For matrix operations, it employs Netlib-java that has a
>> >> >>>>> Java
>> >> >>>>> wrapper for BLAS (basic linear algebra subprograms) and LAPACK
>> >> >>>>> native
>> >> >>>>> binaries if they are available on the worker node. It also has
>> >> >>>>> its
>> >> >>>>> own
>> >> >>>>> optimized Java implementation of BLAS. It is worth mentioning,
>> >> >>>>> that
>> >> >>>>> native
>> >> >>>>> binaries provide better performance only for BLAS level 3, i.e.
>> >> >>>>> matrix-matrix operations or general matrix multiplication (GEMM=
).
>> >> >>>>> This is
>> >> >>>>> confirmed by GEMM test on Netlib-java page
>> >> >>>>> https://github.com/fommil/netlib-java. I also confirmed it with
>> >> >>>>> my
>> >> >>>>> experiments with training of artificial neural network
>> >> >>>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952=
.
>> >> >>>>> However, I would like to boost performance more.
>> >> >>>>>
>> >> >>>>> GPU is supposed to work fast with linear algebra and there is
>> >> >>>>> Nvidia
>> >> >>>>> CUDA
>> >> >>>>> implementation of BLAS, called cublas. I have one Linux server
>> >> >>>>> with
>> >> >>>>> Nvidia
>> >> >>>>> GPU and I was able to do the following. I linked cublas (instea=
d
>> >> >>>>> of
>> >> >>>>> cpu-based blas) with Netlib-java wrapper and put it into Spark,
>> >> >>>>> so
>> >> >>>>> Breeze/Netlib is using it. Then I did some performance
>> >> >>>>> measurements
>> >> >>>>> with
>> >> >>>>> regards to artificial neural network batch learning in Spark
>> >> >>>>> MLlib
>> >> >>>>> that
>> >> >>>>> involves matrix-matrix multiplications. It turns out that for
>> >> >>>>> matrices of
>> >> >>>>> size less than ~1000x780 GPU cublas has the same speed as CPU
>> >> >>>>> blas.
>> >> >>>>> Cublas
>> >> >>>>> becomes slower for bigger matrices. It worth mentioning that it
>> >> >>>>> is
>> >> >>>>> was not
>> >> >>>>> a test for ONLY multiplication since there are other operations
>> >> >>>>> involved.
>> >> >>>>> One of the reasons for slowdown might be the overhead of copyin=
g
>> >> >>>>> the
>> >> >>>>> matrices from computer memory to graphic card memory and back.
>> >> >>>>>
>> >> >>>>> So, few questions:
>> >> >>>>> 1) Do these results with CUDA make sense?
>> >> >>>>> 2) If the problem is with copy overhead, are there any librarie=
s
>> >> >>>>> that
>> >> >>>>> allow to force intermediate results to stay in graphic card
>> >> >>>>> memory
>> >> >>>>> thus
>> >> >>>>> removing the overhead?
>> >> >>>>> 3) Any other options to speed-up linear algebra in Spark?
>> >> >>>>>
>> >> >>>>> Thank you, Alexander
>> >> >>>>>
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> ---------------------------------------------------------------=
------
>> >> >>>>> To unsubscribe, e-mail:
>> >> >>>>>
>> >> >>>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.a=
pache.org><mailto:
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.a=
pache.org>><mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@=
spark.apache.org>
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe=
@spark.apache.org>>>
>> >> >>>>> For additional commands, e-mail:
>> >> >>>>>
>> >> >>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mai=
lto:
>> >> >>>>>
>> >> >>>>>
>> >> >>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><ma=
ilto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >> >>>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>> >> >>>>>
>> >> >>>>>
>> >> >>>>>
>> >> >>>>>
>> >> >>>>
>> >> >
>> >> > --
>> >> > Best regards,
>> >> > Sam
>> >> >

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 21:06:34 2015
Return-Path: <dev-return-11827-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 25B7710764
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 21:06:34 +0000 (UTC)
Received: (qmail 51175 invoked by uid 500); 2 Mar 2015 21:06:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51096 invoked by uid 500); 2 Mar 2015 21:06:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51085 invoked by uid 99); 2 Mar 2015 21:06:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 21:06:31 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.240.92.67] (HELO g9t5009.houston.hp.com) (15.240.92.67)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 21:06:05 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5009.houston.hp.com (Postfix) with ESMTPS id ACCF5A5C;
	Mon,  2 Mar 2015 21:06:02 +0000 (UTC)
Received: from G4W6306.americas.hpqcorp.net (16.210.26.231) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 2 Mar 2015 21:04:57 +0000
Received: from G9W0737.americas.hpqcorp.net ([169.254.9.160]) by
 G4W6306.americas.hpqcorp.net ([16.210.26.231]) with mapi id 14.03.0169.001;
 Mon, 2 Mar 2015 21:04:57 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Xiangrui Meng <mengxr@gmail.com>, Sam Halliday <sam.halliday@gmail.com>
CC: Joseph Bradley <joseph@databricks.com>, dev <dev@spark.apache.org>, "Evan
 R. Sparks" <evan.sparks@gmail.com>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAAFf0QAAAAlNIAAA3mqAAAcBgYAACqowgAAcmbAAAAA7TgAAlRi7AAACzupA
Date: Mon, 2 Mar 2015 21:04:55 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE05B42@G9W0737.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<CALR_T9BJQZTP1jo98BrS3MicX+hXXmvUvDNhNUefO=AMXyALLA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE0314C@G9W0737.americas.hpqcorp.net>
	<87ioeo5n6e.fsf@gmail.com>
	<CAJgQjQ9q2wEu-URc6OkNf+rVriX+FDcViSBM-die2HyCpRC=-A@mail.gmail.com>
	<CALR_T9BsNT9SBAveH7z+Aw-CYB+NFPxGH0Rm_JNsjHu+RhMqsQ@mail.gmail.com>
	<CAJgQjQ8e8S4sdbZ+bPnDS7TsOthX2zv89707W2r7FVDsf4n9ZQ@mail.gmail.com>
	<CALR_T9A8ukiDZ4K+uaMvSSR+wFL9a5yHwEe4AV1JCG6GEG7qmQ@mail.gmail.com>
 <CAJgQjQ-nGwEmoPhv5nLiv1HRRzmiUXeaDJAtakzPk5urbR7BSQ@mail.gmail.com>
In-Reply-To: <CAJgQjQ-nGwEmoPhv5nLiv1HRRzmiUXeaDJAtakzPk5urbR7BSQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 22:29:36 2015
Return-Path: <dev-return-11828-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 809AB10BEE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 22:29:36 +0000 (UTC)
Received: (qmail 63890 invoked by uid 500); 2 Mar 2015 22:29:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63815 invoked by uid 500); 2 Mar 2015 22:29:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63803 invoked by uid 99); 2 Mar 2015 22:29:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 22:29:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.160.181 as permitted sender)
Received: from [209.85.160.181] (HELO mail-yk0-f181.google.com) (209.85.160.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 22:29:31 +0000
Received: by ykq19 with SMTP id 19so14844181ykq.9
        for <dev@spark.apache.org>; Mon, 02 Mar 2015 14:28:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:cc:content-type;
        bh=4/RTN4nqSQ/20OqPJnsuJT50pJEvDVGhmfxqIjQ7ktI=;
        b=uXW5Sb58SUbeqmRjSXrTV1vJl6GK6qzHPXxzJBLov5PslRQL7AmlDbvmBs708WM8hr
         Iw0wYZHVAIoyUSeUhS3mnhU3hvDmlWkYR2t/WwLzUwpzRnRwVX76sAU+ek58b1aWdo/p
         m/fUup6glv0bPnIqrIaAoNMP+VRd13vZMVHF71XGPKr+IHFTc9/1HC5OjzpyOyc9Xkii
         l6LoJRjoZ1zHvPjSCd47j05TxWQrQwp6nN+Rhy4z8IgFCVNvTTh4ZUho/101lMY0dy4H
         NAD93hfynzSLdRlnX1KkOfOBKVIEnBNOno1VEv6UWoVlZmb8xdVtW63kkjM+6KiQwEmA
         z8kg==
X-Received: by 10.236.62.136 with SMTP id y8mr27767107yhc.13.1425335305188;
 Mon, 02 Mar 2015 14:28:25 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 02 Mar 2015 22:28:24 +0000
Message-ID: <CAOhmDzeytskHwS3oSqtWe1-w4arc5gn6=KDEbTtFzr0d_CV6Vw@mail.gmail.com>
Subject: Re: spark-ec2 default to Hadoop 2
To: shivaram@eecs.berkeley.edu, Sean Owen <sowen@cloudera.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0122ed8a8d3d4e051055bcb8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122ed8a8d3d4e051055bcb8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I might take a look at that pr if we get around to doing some perf testing
of Spark on various resource managers.

2015=EB=85=84 3=EC=9B=94 2=EC=9D=BC (=EC=9B=94) =EC=98=A4=ED=9B=84 12:22, S=
hivaram Venkataraman <shivaram@eecs.berkeley.edu>=EB=8B=98=EC=9D=B4
=EC=9E=91=EC=84=B1:

FWIW there is a PR open to add support for Hadoop 2.4 to spark-ec2 scripts
> at https://github.com/mesos/spark-ec2/pull/77 -- But it hasnt' received
> much review or testing to be merged.
>
> Thanks
> Shivaram
>
> On Sun, Mar 1, 2015 at 11:49 PM, Sean Owen <sowen@cloudera.com> wrote:
>
>> I agree with that. My anecdotal impression is that Hadoop 1.x usage
>> out there is maybe a couple percent, and so we should shift towards
>> 2.x at least as defaults.
>>
>> On Sun, Mar 1, 2015 at 10:59 PM, Nicholas Chammas
>> <nicholas.chammas@gmail.com> wrote:
>> > https://github.com/apache/spark/blob/fd8d283eeb98e310b1e85ef8c3a8af
>> 9e547ab5e0/ec2/spark_ec2.py#L162-L164
>> >
>> > Is there any reason we shouldn't update the default Hadoop major
>> version in
>> > spark-ec2 to 2?
>> >
>> > Nick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--089e0122ed8a8d3d4e051055bcb8--

From dev-return-11829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 22:41:12 2015
Return-Path: <dev-return-11829-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BEDC410C94
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 22:41:12 +0000 (UTC)
Received: (qmail 83755 invoked by uid 500); 2 Mar 2015 22:40:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83679 invoked by uid 500); 2 Mar 2015 22:40:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83654 invoked by uid 99); 2 Mar 2015 22:40:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 22:40:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.160.178 as permitted sender)
Received: from [209.85.160.178] (HELO mail-yk0-f178.google.com) (209.85.160.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 22:40:32 +0000
Received: by ykr79 with SMTP id 79so14876789ykr.0
        for <dev@spark.apache.org>; Mon, 02 Mar 2015 14:39:27 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=81kjzryhWeM2EjVuVfpirMl2hIwxoV9rKLt5Z9eyxMo=;
        b=YVi01kgBeGAFgv7z75EjOWbFwJo8Bn5gvyMrSS1M75FrEzWug0Xbj7TquWEL7Iej1q
         AoGsTkL/b2HKr0e076DXn6YnDKSr5q+T1qkesFxzlw+1Vc5hm6T6mNb9P81gVV90A36K
         /BLuAzayaZbe57ZVz6d1VaeAVCRCAuasUF9A5O8vwKliZeCo2Iwk3nPqM0CFw+9C9e3B
         +koRbLMIjl/tO5nPfA33WAfIcg0sUBwlUBz92rc6tYGAUGCSTAXwN+hSUpjJr7NkeGbs
         rOT6Q5QObcJcB2RR16u9n37Em9V6LS6pamLs22MABKASnjEq0Tv8wFAT0gjMiwHd9zKg
         D9ig==
X-Received: by 10.236.53.65 with SMTP id f41mr27801240yhc.182.1425335967065;
 Mon, 02 Mar 2015 14:39:27 -0800 (PST)
MIME-Version: 1.0
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 02 Mar 2015 22:39:26 +0000
Message-ID: <CAOhmDzcgm=7k5MTDmN7y9tEZ8prAMofu4=8HwZaBSv3mo=Q-tg@mail.gmail.com>
Subject: PSA: Link to files at fixed version
To: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0116057a00ac9f051055e4c1
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0116057a00ac9f051055e4c1
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

*TL;DR*: Hit y on any file page on GitHub to update the URL to a permanent
link.

Many of you probably already know this. Here=E2=80=99s a handy tip for the =
rest.

So you=E2=80=99re on Github and you want to link to a file in an email, PR,=
 or JIRA
report. Or better yet, you want to link to some specific lines in a file.

So you do this:

https://github.com/apache/spark/blob/master/ec2/spark_ec2.py#L805-L841

This is fine until someone changes the file. Now the link doesn=E2=80=99t m=
ake
sense because the line numbers are all different.

Instead, you want to do this:

https://github.com/apache/spark/blob/582e5a24c55e8c876733537c9910001affc8b2=
9b/ec2/spark_ec2.py#L805-L841

This link will always make sense.

To get a link like this, you can just press y while you have the file open
on github.com, and GitHub will update the address in your browser bar to a
perma-link (i.e. from the former link to the latter).

Nick
=E2=80=8B

--089e0116057a00ac9f051055e4c1--

From dev-return-11830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  2 22:47:13 2015
Return-Path: <dev-return-11830-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 146CC10CF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  2 Mar 2015 22:47:13 +0000 (UTC)
Received: (qmail 16312 invoked by uid 500); 2 Mar 2015 22:46:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16232 invoked by uid 500); 2 Mar 2015 22:46:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16216 invoked by uid 99); 2 Mar 2015 22:46:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 22:46:37 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 02 Mar 2015 22:46:10 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id B093E78C;
	Mon,  2 Mar 2015 22:45:07 +0000 (UTC)
Received: from G4W6306.americas.hpqcorp.net (16.210.26.231) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 2 Mar 2015 22:43:50 +0000
Received: from G9W0737.americas.hpqcorp.net ([169.254.9.160]) by
 G4W6306.americas.hpqcorp.net ([16.210.26.231]) with mapi id 14.03.0169.001;
 Mon, 2 Mar 2015 22:43:50 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Sam Halliday <sam.halliday@gmail.com>
CC: Joseph Bradley <joseph@databricks.com>, dev <dev@spark.apache.org>, "Evan
 R. Sparks" <evan.sparks@gmail.com>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAAFf0QAAAAlNIAAA3mqAAAcBgYAACqowgAAcmbAAAAA7TgAAlRi7AAACzupAAADDHwAAAr60YA==
Date: Mon, 2 Mar 2015 22:43:49 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE05BB0@G9W0737.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<CALR_T9BJQZTP1jo98BrS3MicX+hXXmvUvDNhNUefO=AMXyALLA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE0314C@G9W0737.americas.hpqcorp.net>
	<87ioeo5n6e.fsf@gmail.com>
	<CAJgQjQ9q2wEu-URc6OkNf+rVriX+FDcViSBM-die2HyCpRC=-A@mail.gmail.com>
	<CALR_T9BsNT9SBAveH7z+Aw-CYB+NFPxGH0Rm_JNsjHu+RhMqsQ@mail.gmail.com>
	<CAJgQjQ8e8S4sdbZ+bPnDS7TsOthX2zv89707W2r7FVDsf4n9ZQ@mail.gmail.com>
	<CALR_T9A8ukiDZ4K+uaMvSSR+wFL9a5yHwEe4AV1JCG6GEG7qmQ@mail.gmail.com>
	<CAJgQjQ-nGwEmoPhv5nLiv1HRRzmiUXeaDJAtakzPk5urbR7BSQ@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE05B42@G9W0737.americas.hpqcorp.net>
 <CALR_T9Dzf4ikzNM1_TyQo7QM4yWCT4cFDiYWrPygkAjiu4jWKA@mail.gmail.com>
In-Reply-To: <CALR_T9Dzf4ikzNM1_TyQo7QM4yWCT4cFDiYWrPygkAjiu4jWKA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar  3 10:43:41 2015
Return-Path: <dev-return-11831-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A429B10D7B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Mar 2015 10:43:41 +0000 (UTC)
Received: (qmail 82314 invoked by uid 500); 3 Mar 2015 10:43:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82232 invoked by uid 500); 3 Mar 2015 10:43:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82219 invoked by uid 99); 3 Mar 2015 10:43:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Mar 2015 10:43:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of niranda.perera@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Mar 2015 10:43:34 +0000
Received: by iecrd18 with SMTP id rd18so56104593iec.5
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 02:43:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=8YTxUqzAT8zzHJtxnu28lt7CePGE9/6n11gIu6h+QxE=;
        b=Z/jNtwY7MV93WfHNG4gtf6N8chvdljcBAHAWe3ecaYILSBft0Ol97Edj5oGBkp71hT
         un+o3MKOszXafCIElIeDZevJEJ/lghQLqMIeJj9CN2q2jgGT4BL/Kyb84oivW/1yqa3H
         nnZgoL0Pbec6NaKUwqZUBlQDHNhCf+B75tq7LyFyyXBPSxf/94QTtNy3jzhEC5G3pwOl
         FnrT9QhYdUdY8CTwHNTYA6gwYMMnlNADAzzktpkRUDBh5ug3RaU9Gd+swpnB3+K9G7mp
         tQO4lw7MeafWzpgwPoCcPu8Vu9b7wNFde3ue/xDITu39CuGC8F6f6864Us8l2O/57DPB
         wobg==
MIME-Version: 1.0
X-Received: by 10.202.54.67 with SMTP id d64mr21234334oia.55.1425379393510;
 Tue, 03 Mar 2015 02:43:13 -0800 (PST)
Received: by 10.182.117.228 with HTTP; Tue, 3 Mar 2015 02:43:13 -0800 (PST)
Date: Tue, 3 Mar 2015 16:13:13 +0530
Message-ID: <CANCoaU6W_6joGjzVGmNMVicxu+WgXK16p9TWR1bwchEKHVp6Zg@mail.gmail.com>
Subject: Deploying master and worker programatically in java
From: Niranda Perera <niranda.perera@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ce0066bb4ff0510600058
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ce0066bb4ff0510600058
Content-Type: text/plain; charset=UTF-8

Hi,

I want to start a Spark standalone cluster programatically in java.

I have been checking these classes,
- org.apache.spark.deploy.master.Master
- org.apache.spark.deploy.worker.Worker

I successfully started a master with this simple main class.

 public static void main(String[] args) {
        SparkConf conf = new SparkConf();
        Master.startSystemAndActor("localhost", 4500, 8080, conf);
}


but I'm finding it hard to carry out a similar approach for the worker.

can anyone give an example of how to pass a value to the workerNumber field
in the Worker.startSystemAndActor constructor (in the java env)?

Cheers
-- 
Niranda

--001a113ce0066bb4ff0510600058--

From dev-return-11832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar  3 21:54:40 2015
Return-Path: <dev-return-11832-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 34F8910FD6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  3 Mar 2015 21:54:40 +0000 (UTC)
Received: (qmail 64164 invoked by uid 500); 3 Mar 2015 21:54:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64088 invoked by uid 500); 3 Mar 2015 21:54:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64070 invoked by uid 99); 3 Mar 2015 21:54:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Mar 2015 21:54:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sam.halliday@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 03 Mar 2015 21:54:24 +0000
Received: by wiwl15 with SMTP id l15so26354103wiw.5
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 13:54:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:to:cc:subject:in-reply-to:references:user-agent:date
         :message-id:mime-version:content-type:content-transfer-encoding;
        bh=8hV/e3gopNDqSUMZx+FzAhNByRexZmbSEaH5LHGmdpo=;
        b=Jse2lH32Z2PrPm0+iufNElI0FY1LEjqP1MKY1mrJKe7Fpk/D/xZMOn+aLnBvmyn2r4
         P0LdJLzx/qaQMvLx4eVsRTcCiM1w6pWW0c3SkO3TVb/LtsptU114z4ttkeIAATaVVfEX
         cZOhON78oyTnlp7fOssez+P/zXr37jU4QbNg1EgD0W0HoJfXVvsKG03+0/lO7SB71T/T
         EG7crJwArsCAR//I6pJghZafn+ZV8jPMfVkP/a9mwxg5UFVyNkRJIV42f8Za2DqsfH3T
         1NMuSKYeop3OnWTTVU0ARJK7YRgoV5RYgfNYhzkYRt32arvId/FGXhGuR7DdkpirIFnG
         d//w==
X-Received: by 10.194.62.74 with SMTP id w10mr1480308wjr.95.1425419643704;
        Tue, 03 Mar 2015 13:54:03 -0800 (PST)
Received: from Samskara (host86-176-86-184.range86-176.btcentralplus.com. [86.176.86.184])
        by mx.google.com with ESMTPSA id fo9sm22340785wib.16.2015.03.03.13.54.01
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Tue, 03 Mar 2015 13:54:02 -0800 (PST)
From: Sam Halliday <sam.halliday@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>
Cc: "Evan R. Sparks" <evan.sparks@gmail.com>, "Ulanov\, Alexander" <alexander.ulanov@hp.com>, "dev\@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Using CUDA within Spark / boosting linear algebra
In-Reply-To: <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
 <CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
 <CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
 <CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
 <CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com> <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
User-Agent: Notmuch/0.18.2 (http://notmuchmail.org) Emacs/24.4.1 (x86_64-pc-linux-gnu)
Date: Tue, 03 Mar 2015 21:54:00 +0000
Message-ID: <87ioehu4qv.fsf@gmail.com>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

BTW, is anybody on this list going to the London Meetup in a few weeks?

https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduc=
e-world#community

Would be nice to meet other people working on the guts of Spark! :-)


Xiangrui Meng <mengxr@gmail.com> writes:

> Hey Alexander,
>
> I don't quite understand the part where netlib-cublas is about 20x
> slower than netlib-openblas. What is the overhead of using a GPU BLAS
> with netlib-java?
>
> CC'ed Sam, the author of netlib-java.
>
> Best,
> Xiangrui
>
> On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com> w=
rote:
>> Better documentation for linking would be very helpful!  Here's a JIRA:
>> https://issues.apache.org/jira/browse/SPARK-6019
>>
>>
>> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks <evan.sparks@gmail.com>
>> wrote:
>>
>>> Thanks for compiling all the data and running these benchmarks, Alex. T=
he
>>> big takeaways here can be seen with this chart:
>>>
>>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZHl6=
kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>>>
>>> 1) A properly configured GPU matrix multiply implementation (e.g.
>>> BIDMat+GPU) can provide substantial (but less than an order of magnitud=
e)
>>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>>> netlib-java+openblas-compiled).
>>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude wor=
se
>>> than a well-tuned CPU implementation, particularly for larger matrices.
>>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>>> basically agrees with the authors own benchmarks (
>>> https://github.com/fommil/netlib-java)
>>>
>>> I think that most of our users are in a situation where using GPUs may =
not
>>> be practical - although we could consider having a good GPU backend
>>> available as an option. However, *ALL* users of MLlib could benefit
>>> (potentially tremendously) from using a well-tuned CPU-based BLAS
>>> implementation. Perhaps we should consider updating the mllib guide wit=
h a
>>> more complete section for enabling high performance binaries on OSX and
>>> Linux? Or better, figure out a way for the system to fetch these
>>> automatically.
>>>
>>> - Evan
>>>
>>>
>>>
>>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com> wrote:
>>>
>>>> Just to summarize this thread, I was finally able to make all performa=
nce
>>>> comparisons that we discussed. It turns out that:
>>>> BIDMat-cublas>>BIDMat
>>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yum-=
repo=3D=3Dnetlib-cublas>netlib-blas>f2jblas
>>>>
>>>> Below is the link to the spreadsheet with full results.
>>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378=
T9J5r7kwKSPkY/edit?usp=3Dsharing
>>>>
>>>> One thing still needs exploration: does BIDMat-cublas perform copying
>>>> to/from machine=E2=80=99s RAM?
>>>>
>>>> -----Original Message-----
>>>> From: Ulanov, Alexander
>>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>>> To: Evan R. Sparks
>>>> Cc: Joseph Bradley; dev@spark.apache.org
>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Thanks, Evan! It seems that ticket was marked as duplicate though the
>>>> original one discusses slightly different topic. I was able to link ne=
tlib
>>>> with MKL from BIDMat binaries. Indeed, MKL is statically linked inside=
 a
>>>> 60MB library.
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>>> +---------------------------------------------------------------------=
--+
>>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>>> |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>>>> 1569,233228 |
>>>>
>>>> It turn out that pre-compiled MKL is faster than precompiled OpenBlas =
on
>>>> my machine. Probably, I=E2=80=99ll add two more columns with locally c=
ompiled
>>>> openblas and cuda.
>>>>
>>>> Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
>>>> Sent: Monday, February 09, 2015 6:06 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley; dev@spark.apache.org
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Great - perhaps we can move this discussion off-list and onto a JIRA
>>>> ticket? (Here's one: https://issues.apache.org/jira/browse/SPARK-5705)
>>>>
>>>> It seems like this is going to be somewhat exploratory for a while (and
>>>> there's probably only a handful of us who really care about fast linear
>>>> algebra!)
>>>>
>>>> - Evan
>>>>
>>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for explanation and useful link. I am going to build OpenBLA=
S,
>>>> link it with Netlib-java and perform benchmark again.
>>>>
>>>> Do I understand correctly that BIDMat binaries contain statically link=
ed
>>>> Intel MKL BLAS? It might be the reason why I am able to run BIDMat not
>>>> having MKL BLAS installed on my server. If it is true, I wonder if it =
is OK
>>>> because Intel sells this library. Nevertheless, it seems that in my ca=
se
>>>> precompiled MKL BLAS performs better than precompiled OpenBLAS given t=
hat
>>>> BIDMat and Netlib-java are supposed to be on par with JNI overheads.
>>>>
>>>> Though, it might be interesting to link Netlib-java with Intel MKL, as
>>>> you suggested. I wonder, are John Canny (BIDMat) and Sam Halliday
>>>> (Netlib-java) interested to compare their libraries.
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com>]
>>>> Sent: Friday, February 06, 2015 5:58 PM
>>>>
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I would build OpenBLAS yourself, since good BLAS performance comes from
>>>> getting cache sizes, etc. set up correctly for your particular hardwar=
e -
>>>> this is often a very tricky process (see, e.g. ATLAS), but we found th=
at on
>>>> relatively modern Xeon chips, OpenBLAS builds quickly and yields
>>>> performance competitive with MKL.
>>>>
>>>> To make sure the right library is getting used, you have to make sure
>>>> it's first on the search path - export
>>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
>>>>
>>>> For some examples of getting netlib-java setup on an ec2 node and some
>>>> example benchmarking code we ran a while back, see:
>>>> https://github.com/shivaram/matrix-bench
>>>>
>>>> In particular - build-openblas-ec2.sh shows you how to build the libra=
ry
>>>> and set up symlinks correctly, and scala/run-netlib.sh shows you how t=
o get
>>>> the path setup and get that library picked up by netlib-java.
>>>>
>>>> In this way - you could probably get cuBLAS set up to be used by
>>>> netlib-java as well.
>>>>
>>>> - Evan
>>>>
>>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>>> Evan, could you elaborate on how to force BIDMat and netlib-java to fo=
rce
>>>> loading the right blas? For netlib, I there are few JVM flags, such as
>>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS, so=
 I can
>>>> force it to use Java implementation. Not sure I understand how to forc=
e use
>>>> a specific blas (not specific wrapper for blas).
>>>>
>>>> Btw. I have installed openblas (yum install openblas), so I suppose th=
at
>>>> netlib is using it.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com>]
>>>> Sent: Friday, February 06, 2015 5:19 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Getting breeze to pick up the right blas library is critical for
>>>> performance. I recommend using OpenBLAS (or MKL, if you already have i=
t).
>>>> It might make sense to force BIDMat to use the same underlying BLAS li=
brary
>>>> as well.
>>>>
>>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>>> Hi Evan, Joseph
>>>>
>>>> I did few matrix multiplication test and BIDMat seems to be ~10x faster
>>>> than netlib-java+breeze (sorry for weird table formatting):
>>>>
>>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java native_system_linux_x86-6=
4|
>>>> Breeze+Netlib-java f2jblas |
>>>> +---------------------------------------------------------------------=
--+
>>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228 |
>>>>
>>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora 19
>>>> Linux, Scala 2.11.
>>>>
>>>> Later I will make tests with Cuda. I need to install new Cuda version =
for
>>>> this purpose.
>>>>
>>>> Do you have any ideas why breeze-netlib with native blas is so much
>>>> slower than BIDMat MKL?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>>>> joseph@databricks.com>]
>>>> Sent: Thursday, February 05, 2015 5:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc: Evan R. Sparks; dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Hi Alexander,
>>>>
>>>> Using GPUs with Spark would be very exciting.  Small comment: Concerni=
ng
>>>> your question earlier about keeping data stored on the GPU rather than
>>>> having to move it between main memory and GPU memory on each iteration=
, I
>>>> would guess this would be critical to getting good performance.  If you
>>>> could do multiple local iterations before aggregating results, then the
>>>> cost of data movement to the GPU could be amortized (and I believe tha=
t is
>>>> done in practice).  Having Spark be aware of the GPU and using it as
>>>> another part of memory sounds like a much bigger undertaking.
>>>>
>>>> Joseph
>>>>
>>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presentati=
on by John
>>>> Canny and I am really inspired by his talk and comparisons with Spark =
MLlib.
>>>>
>>>> I am very interested to find out what will be better within Spark: BID=
Mat
>>>> or netlib-java with CPU or GPU natives. Could you suggest a fair way to
>>>> benchmark them? Currently I do benchmarks on artificial neural network=
s in
>>>> batch mode. While it is not a =E2=80=9Cpure=E2=80=9D test of linear al=
gebra, it involves
>>>> some other things that are essential to machine learning.
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com>]
>>>> Sent: Thursday, February 05, 2015 1:29 PM
>>>> To: Ulanov, Alexander
>>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to da=
ta
>>>> layout and fewer levels of indirection - it's definitely a worthwhile
>>>> experiment to run. The main speedups I've seen from using it come from
>>>> highly optimized GPU code for linear algebra. I know that in the past =
Canny
>>>> has gone as far as to write custom GPU kernels for performance-critical
>>>> regions of code.[1]
>>>>
>>>> BIDMach is highly optimized for single node performance or performance=
 on
>>>> small clusters.[2] Once data doesn't fit easily in GPU memory (or can =
be
>>>> batched in that way) the performance tends to fall off. Canny argues f=
or
>>>> hardware/software codesign and as such prefers machine configurations =
that
>>>> are quite different than what we find in most commodity cluster nodes -
>>>> e.g. 10 disk cahnnels and 4 GPUs.
>>>>
>>>> In contrast, MLlib was designed for horizontal scalability on commodity
>>>> clusters and works best on very big datasets - order of terabytes.
>>>>
>>>> For the most part, these projects developed concurrently to address
>>>> slightly different use cases. That said, there may be bits of BIDMach =
we
>>>> could repurpose for MLlib - keep in mind we need to be careful about
>>>> maintaining cross-language compatibility for our Java and Python-users,
>>>> though.
>>>>
>>>> - Evan
>>>>
>>>> [1] - http://arxiv.org/abs/1409.5402
>>>> [2] - http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>>>
>>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Hi Evan,
>>>>
>>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do you
>>>> know what makes them faster than netlib-java?
>>>>
>>>> The same group has BIDMach library that implements machine learning. F=
or
>>>> some examples they use Caffe convolutional neural network library owne=
d by
>>>> another group in Berkeley. Could you elaborate on how these all might =
be
>>>> connected with Spark Mllib? If you take BIDMat for linear algebra why =
don=E2=80=99t
>>>> you take BIDMach for optimization and learning?
>>>>
>>>> Best regards, Alexander
>>>>
>>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com>>]
>>>> Sent: Thursday, February 05, 2015 12:09 PM
>>>> To: Ulanov, Alexander
>>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU blas =
in
>>>> many cases.
>>>>
>>>> You might consider taking a look at the codepaths that BIDMat (
>>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>>> netlib-java/breeze. John Canny et. al. have done a bunch of work optim=
izing
>>>> to make this work really fast from Scala. I've run it on my laptop and
>>>> compared to MKL and in certain cases it's 10x faster at matrix multipl=
y.
>>>> There are a lot of layers of indirection here and you really want to a=
void
>>>> data copying as much as possible.
>>>>
>>>> We could also consider swapping out BIDMat for Breeze, but that would =
be
>>>> a big project and if we can figure out how to get breeze+cublas to
>>>> comparable performance that would be a big win.
>>>>
>>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> Dear Spark developers,
>>>>
>>>> I am exploring how to make linear algebra operations faster within Spa=
rk.
>>>> One way of doing this is to use Scala Breeze library that is bundled w=
ith
>>>> Spark. For matrix operations, it employs Netlib-java that has a Java
>>>> wrapper for BLAS (basic linear algebra subprograms) and LAPACK native
>>>> binaries if they are available on the worker node. It also has its own
>>>> optimized Java implementation of BLAS. It is worth mentioning, that na=
tive
>>>> binaries provide better performance only for BLAS level 3, i.e.
>>>> matrix-matrix operations or general matrix multiplication (GEMM). This=
 is
>>>> confirmed by GEMM test on Netlib-java page
>>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>>>> experiments with training of artificial neural network
>>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>>> However, I would like to boost performance more.
>>>>
>>>> GPU is supposed to work fast with linear algebra and there is Nvidia C=
UDA
>>>> implementation of BLAS, called cublas. I have one Linux server with Nv=
idia
>>>> GPU and I was able to do the following. I linked cublas (instead of
>>>> cpu-based blas) with Netlib-java wrapper and put it into Spark, so
>>>> Breeze/Netlib is using it. Then I did some performance measurements wi=
th
>>>> regards to artificial neural network batch learning in Spark MLlib that
>>>> involves matrix-matrix multiplications. It turns out that for matrices=
 of
>>>> size less than ~1000x780 GPU cublas has the same speed as CPU blas. Cu=
blas
>>>> becomes slower for bigger matrices. It worth mentioning that it is was=
 not
>>>> a test for ONLY multiplication since there are other operations involv=
ed.
>>>> One of the reasons for slowdown might be the overhead of copying the
>>>> matrices from computer memory to graphic card memory and back.
>>>>
>>>> So, few questions:
>>>> 1) Do these results with CUDA make sense?
>>>> 2) If the problem is with copy overhead, are there any libraries that
>>>> allow to force intermediate results to stay in graphic card memory thus
>>>> removing the overhead?
>>>> 3) Any other options to speed-up linear algebra in Spark?
>>>>
>>>> Thank you, Alexander
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
>>>> dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.=
org
>>>> <mailto:dev-unsubscribe@spark.apache.org>>
>>>> For additional commands, e-mail: dev-help@spark.apache.org<mailto:
>>>> dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:
>>>> dev-help@spark.apache.org>>
>>>>
>>>>
>>>>
>>>>
>>>

--=20
Best regards,
Sam

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 01:52:38 2015
Return-Path: <dev-return-11833-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 328EB17AD6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 01:52:38 +0000 (UTC)
Received: (qmail 2907 invoked by uid 500); 4 Mar 2015 01:52:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2830 invoked by uid 500); 4 Mar 2015 01:52:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2818 invoked by uid 99); 4 Mar 2015 01:52:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 01:52:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of robert.dodier@gmail.com designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 01:52:31 +0000
Received: by lbiz12 with SMTP id z12so13272003lbi.12
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 17:51:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=7EetLAZ77fj3JkKpy8GEkK3OJ+x92bKHWUNNJVQjAtQ=;
        b=CMNtcFnSxwJHC66mV9aZFgMZOfxBjDRChO1jNgz+Xp1tHa0dbzFSK0T281noeJHR+A
         AfTg9/eV0oI/L7AORLU8qkoKzMXt4QkWZUXDu1/s7fRHnab0gXso6YKXFvE7bYH8ydpi
         qTICfUs1pa6Mr+0rbsVD7yathj13Ni7PBJgbXlH2rK50pens5raPdhmjtmapP1tTiN2g
         nNAJDpthFb2g8ep9Vo0BaaEyMiaI0xT0iE0lc1GoFhEmKtLvA082BPoj03XuOsO6Ha/j
         3DdfyKdMfO/J2QXA0TvfIedhgXRXi6vLmv2q3JZdATdbs8Ut2KXz6jJb7zcEyV7J5L1v
         +uYQ==
MIME-Version: 1.0
X-Received: by 10.152.9.200 with SMTP id c8mr1278106lab.67.1425433885069; Tue,
 03 Mar 2015 17:51:25 -0800 (PST)
Received: by 10.112.239.35 with HTTP; Tue, 3 Mar 2015 17:51:25 -0800 (PST)
Date: Tue, 3 Mar 2015 17:51:25 -0800
Message-ID: <CAAsY_sTDG9G6i8NsD517bwtzTp2cZbsXvQY0Zmu15-8gxjnT+g@mail.gmail.com>
Subject: ideas for MLlib development
From: Robert Dodier <robert.dodier@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I have some ideas for MLlib that I think might be of general interest
so I'd like to see what people think and maybe find some collaborators.

(1) Some form of Markov chain Monte Carlo such as Gibbs sampling
or Metropolis-Hastings. Any kind of Monte Carlo method is readily
parallelized so Spark seems like a natural platform for them.
MCMC plays an important role in computational implementations
of Bayesian inference.

(2) A function to compute the calibration of a probabilistic classifier.
The question this answers is, if the classifier outputs 0.x for some
group of examples, is the actual proportion approximately 0.x ?
This is useful to know if the classifier outputs are used to compute
expected loss in some decision procedure.

Of course (1) is much bigger than (2). Perhaps (2) is a one-person
job but (1) will take a lot of teamwork. I am thinking that in the short
term, we could at least make some progress on an outline or
framework for (1).

I am a newcomer to Scala and Spark but I have a lot of experience
in statistical computing. I am thinking that maybe one or the other
of these projects will be a good way for me to learn more about
Spark and make a useful contribution. Thanks for your interest
and I look forward to your comments.

Robert Dodier

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 02:36:57 2015
Return-Path: <dev-return-11834-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9752117C01
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 02:36:57 +0000 (UTC)
Received: (qmail 84080 invoked by uid 500); 4 Mar 2015 02:36:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83997 invoked by uid 500); 4 Mar 2015 02:36:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83982 invoked by uid 99); 4 Mar 2015 02:36:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 02:36:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.220.172 as permitted sender)
Received: from [209.85.220.172] (HELO mail-vc0-f172.google.com) (209.85.220.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 02:35:55 +0000
Received: by mail-vc0-f172.google.com with SMTP id kv19so5832657vcb.3
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 18:34:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Fw703c5fsUy+Fq0WHrPJsbN4I6rYBIj39k+DKiAPMv8=;
        b=qTL4Xiaog5/SWvicB3bo/AEnkhcAUUkfbwjbr2JRoqV0pIkB6FAwLg9eE0H365/ffR
         gxrF8FGCEFhME+GfYK6QM69PWW03u5I8zoUeqwYo6x7orxfIO9EWNzit0kM/EW0pdi9T
         gZK8AHqxFYdJuT1n7TjCN4P4Sw8nM3nAy0BkNcOjkA/LCeVwQtML1EDzvbxPcaxyp1Xv
         8uhdzTnIkHqF4QnIJreEf/6KGiWIw/052C30mCiqtB3ut2TXaPtC/52v8ckSPcHpGjpO
         H0ltD6/XGrGcL5PPE7bZJBEwROj/o+zFighpGqPhQYRmU827eMzxSRNXWlJdXrnMQHxD
         q6zQ==
X-Received: by 10.52.108.71 with SMTP id hi7mr2972359vdb.87.1425436463377;
 Tue, 03 Mar 2015 18:34:23 -0800 (PST)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Tue, 3 Mar 2015 18:34:03 -0800 (PST)
In-Reply-To: <CAAsY_sTDG9G6i8NsD517bwtzTp2cZbsXvQY0Zmu15-8gxjnT+g@mail.gmail.com>
References: <CAAsY_sTDG9G6i8NsD517bwtzTp2cZbsXvQY0Zmu15-8gxjnT+g@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Tue, 3 Mar 2015 18:34:03 -0800
Message-ID: <CABjXkq65ygjH2da_bSSc7Y3vpgKh1-H-ME938s3Qb1gv22p3Rg@mail.gmail.com>
Subject: Re: ideas for MLlib development
To: Robert Dodier <robert.dodier@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec548a0550cb30305106d4a5e
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec548a0550cb30305106d4a5e
Content-Type: text/plain; charset=UTF-8

Hi Robert,

There's some work to do LDA via Gibbs sampling in this JIRA:
https://issues.apache.org/jira/browse/SPARK-1405 as well as this one:
https://issues.apache.org/jira/browse/SPARK-5556

It may make sense to have a more general Gibbs sampling framework, but it
might be good to have a few desired applications in mind (e.g. higher level
models that rely on Gibbs) to help API design, parallelization strategy,
etc.

See the guide (
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ContributingNewAlgorithmstoMLLib)
for information about contributing to MLlib.

- Evan

On Tue, Mar 3, 2015 at 5:51 PM, Robert Dodier <robert.dodier@gmail.com>
wrote:

> Hi,
>
> I have some ideas for MLlib that I think might be of general interest
> so I'd like to see what people think and maybe find some collaborators.
>
> (1) Some form of Markov chain Monte Carlo such as Gibbs sampling
> or Metropolis-Hastings. Any kind of Monte Carlo method is readily
> parallelized so Spark seems like a natural platform for them.
> MCMC plays an important role in computational implementations
> of Bayesian inference.


> (2) A function to compute the calibration of a probabilistic classifier.
> The question this answers is, if the classifier outputs 0.x for some
> group of examples, is the actual proportion approximately 0.x ?
> This is useful to know if the classifier outputs are used to compute
> expected loss in some decision procedure.
>
> Of course (1) is much bigger than (2). Perhaps (2) is a one-person
> job but (1) will take a lot of teamwork. I am thinking that in the short
> term, we could at least make some progress on an outline or
> framework for (1).
>
> I am a newcomer to Scala and Spark but I have a lot of experience
> in statistical computing. I am thinking that maybe one or the other
> of these projects will be a good way for me to learn more about
> Spark and make a useful contribution. Thanks for your interest
> and I look forward to your comments.
>
> Robert Dodier
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--bcaec548a0550cb30305106d4a5e--

From dev-return-11835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 04:16:31 2015
Return-Path: <dev-return-11835-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A129417E75
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 04:16:31 +0000 (UTC)
Received: (qmail 97997 invoked by uid 500); 4 Mar 2015 04:16:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97922 invoked by uid 500); 4 Mar 2015 04:16:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97903 invoked by uid 99); 4 Mar 2015 04:16:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 04:16:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 04:16:25 +0000
Received: by obcva2 with SMTP id va2so4439260obc.6
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 20:14:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=gwI55+ujVtKRRqykc6cX/Oh/8Lk7VXLb41rravfphY0=;
        b=M6UNYqHIiBJMS27aEvhuSKxRe+paTPay5jeruJcCBRI+9cvfS/SsS6zDKgrBiKpIVM
         LaGtyIvZLFGnby3XGPca0NCZKOB1e+Kad9zwm3I7qTeVHnDauhH3UclLl49fQSVYB0VQ
         bdTo7TOBUTVL31zH92UAJGf/uRrZytTgBUjvthBhm8JkvMIH4F28AOVQ1FjyLfiIzu9H
         5Zjqfppz/jlKVFYthN0XGoxllqYqDk7/wA3M36UQTfIvMUo1wRh/BrcjwPg+4nL3njOF
         BlbNFgnP/pHYEDoggovHo33VHeI938ToIo7SN3z2o3XhTDvnk9b+ispe0YnpjFauH2sv
         w0xQ==
MIME-Version: 1.0
X-Received: by 10.202.80.72 with SMTP id e69mr1465952oib.75.1425442474630;
 Tue, 03 Mar 2015 20:14:34 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Tue, 3 Mar 2015 20:14:34 -0800 (PST)
Date: Tue, 3 Mar 2015 20:14:34 -0800
Message-ID: <CABPQxsucPXO4tZSAP8OqfVBNU08Ctnue2L3nUrG84twmyTe1Gg@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.3.0 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote is cancelled in favor of RC2.

On Thu, Feb 26, 2015 at 9:50 AM, Sandor Van Wassenhove
<sandorw@palantir.com> wrote:
> FWIW, I tested the first rc and saw no regressions. I ran our benchmarks
> built against spark 1.3 and saw results consistent with spark 1.2/1.2.1.
>
> On 2/25/15, 5:51 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>
>>Hey All,
>>
>>Just a quick updated on this thread. Issues have continued to trickle
>>in. Not all of them are blocker level but enough to warrant another
>>RC:
>>
>>I've been keeping the JIRA dashboard up and running with the latest
>>status (sorry, long link):
>>https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jir
>>a_issues_-3Fjql-3Dproject-2520-253D-2520SPARK-2520AND-2520-2522Target-2520
>>Version-252Fs-2522-2520-253D-25201.3.0-2520AND-2520-28fixVersion-2520IS-25
>>20EMPTY-2520OR-2520fixVersion-2520-21-253D-25201.3.0-29-2520AND-2520-28Res
>>olution-2520IS-2520EMPTY-2520OR-2520Resolution-2520IN-2520-28Done-252C-252
>>0Fixed-252C-2520Implemented-29-29-2520ORDER-2520BY-2520priority-252C-2520c
>>omponent&d=AwIFAw&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=cyguR-hd
>>uPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2JUA&m=frmHzwi9qJcMu2udAW6MBS4NWwKmHCBBpCG9
>>zeuaRhA&s=SEjc91m9Dpx8QLLWlMK_5G0ORYtTHlLR2r3091n9qU0&e=
>>
>>One these are in I will cut another RC. Thanks everyone for the
>>continued voting!
>>
>>- Patrick
>>
>>On Mon, Feb 23, 2015 at 10:52 PM, Tathagata Das
>><tathagata.das1565@gmail.com> wrote:
>>> Hey all,
>>>
>>> I found a major issue where JobProgressListener (a listener used to keep
>>> track of jobs for the web UI) never forgets stages in one of its data
>>> structures. This is a blocker for long running applications.
>>>
>>>https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_ji
>>>ra_browse_SPARK-2D5967&d=AwIFAw&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oO
>>>nmz8&r=cyguR-hduPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2JUA&m=frmHzwi9qJcMu2udAW6M
>>>BS4NWwKmHCBBpCG9zeuaRhA&s=06QttEOx2YqhPQ2sWdQmOElwog_cJ5iT2Mqa1_5jnl4&e=
>>>
>>> I am testing a fix for this right now.
>>>
>>> TD
>>>
>>> On Mon, Feb 23, 2015 at 7:23 PM, Soumitra Kumar
>>><kumar.soumitra@gmail.com>
>>> wrote:
>>>
>>>> +1 (non-binding)
>>>>
>>>> For:
>>>>https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_j
>>>>ira_browse_SPARK-2D3660&d=AwIFAw&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6
>>>>oOnmz8&r=cyguR-hduPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2JUA&m=frmHzwi9qJcMu2udA
>>>>W6MBS4NWwKmHCBBpCG9zeuaRhA&s=0sBvf0vWgAski9HweupKdPZwWdYH0Mimda14oHnNVDA
>>>>&e=
>>>>
>>>> . Docs OK
>>>> . Example code is good
>>>>
>>>> -Soumitra.
>>>>
>>>>
>>>> On Mon, Feb 23, 2015 at 10:33 AM, Marcelo Vanzin <vanzin@cloudera.com>
>>>> wrote:
>>>>
>>>> > Hi Tom, are you using an sbt-built assembly by any chance? If so,
>>>>take
>>>> > a look at SPARK-5808.
>>>> >
>>>> > I haven't had any problems with the maven-built assembly. Setting
>>>> > SPARK_HOME on the executors is a workaround if you want to use the
>>>>sbt
>>>> > assembly.
>>>> >
>>>> > On Fri, Feb 20, 2015 at 2:56 PM, Tom Graves
>>>> > <tgraves_cs@yahoo.com.invalid> wrote:
>>>> > > Trying to run pyspark on yarn in client mode with basic wordcount
>>>> > example I see the following error when doing the collect:
>>>> > > Error from python worker:  /usr/bin/python: No module named
>>>> > sqlPYTHONPATH was:
>>>> >
>>>>
>>>>/grid/3/tmp/yarn-local/usercache/tgraves/filecache/20/spark-assembly-1.3
>>>>.0-hadoop2.6.0.1.1411101121.jarjava.io.EOFException
>>>> >       at java.io.DataInputStream.readInt(DataInputStream.java:392)
>>>> > at
>>>> >
>>>>
>>>>org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorker
>>>>Factory.scala:163)
>>>> >       at
>>>> >
>>>>
>>>>org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(Pyth
>>>>onWorkerFactory.scala:86)
>>>> >       at
>>>> >
>>>>
>>>>org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFacto
>>>>ry.scala:62)
>>>> >       at
>>>>org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:105)
>>>> >       at
>>>> org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:69)
>>>> >       at
>>>>org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
>>>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)        at
>>>> > org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:308)
>>>> > at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
>>>> > at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)        at
>>>> >
>>>>
>>>>org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:6
>>>>8)
>>>> >       at
>>>> >
>>>>
>>>>org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:4
>>>>1)
>>>> >       at org.apache.spark.scheduler.Task.run(Task.scala:64)        at
>>>> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
>>>> >   at
>>>> >
>>>>
>>>>java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.jav
>>>>a:1145)
>>>> >       at
>>>> >
>>>>
>>>>java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.ja
>>>>va:615)
>>>> >       at java.lang.Thread.run(Thread.java:722)
>>>> > > any ideas on this?
>>>> > > Tom
>>>> > >
>>>> > >      On Wednesday, February 18, 2015 2:14 AM, Patrick Wendell <
>>>> > pwendell@gmail.com> wrote:
>>>> > >
>>>> > >
>>>> > >  Please vote on releasing the following candidate as Apache Spark
>>>> > version 1.3.0!
>>>> > >
>>>> > > The tag to be voted on is v1.3.0-rc1 (commit f97b0d4a):
>>>> > >
>>>> >
>>>>
>>>>https://urldefense.proofpoint.com/v2/url?u=https-3A__git-2Dwip-2Dus.apac
>>>>he.org_repos_asf-3Fp-3Dspark.git-3Ba-3Dcommit-3Bh-3Df97b0d4a6b2650491681
>>>>6d7aefcf3132cd1da6c2&d=AwIFAw&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOn
>>>>mz8&r=cyguR-hduPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2JUA&m=frmHzwi9qJcMu2udAW6M
>>>>BS4NWwKmHCBBpCG9zeuaRhA&s=DF8Cc8QmI354neHBHJ0HGyQtKL4yOIX2SDDwc0-hshw&e=
>>>>
>>>> > >
>>>> > > The release files, including signatures, digests, etc. can be
>>>>found at:
>>>> > >
>>>>https://urldefense.proofpoint.com/v2/url?u=http-3A__people.apache.org_-7
>>>>Epwendell_spark-2D1.3.0-2Drc1_&d=AwIFAw&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXB
>>>>rZ4tFb6oOnmz8&r=cyguR-hduPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2JUA&m=frmHzwi9qJ
>>>>cMu2udAW6MBS4NWwKmHCBBpCG9zeuaRhA&s=SHWRgoK3UcmmnWVXU0LWjArD2PdG9RYWnO2f
>>>>lVC8nMQ&e=
>>>> > >
>>>> > > Release artifacts are signed with the following key:
>>>> > >
>>>>https://urldefense.proofpoint.com/v2/url?u=https-3A__people.apache.org_k
>>>>eys_committer_pwendell.asc&d=AwIFAw&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4t
>>>>Fb6oOnmz8&r=cyguR-hduPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2JUA&m=frmHzwi9qJcMu2
>>>>udAW6MBS4NWwKmHCBBpCG9zeuaRhA&s=lAnGa6hXGkJQp14UV7lB1zQqOcCeMS3hYG0scwXh
>>>>OFw&e=
>>>> > >
>>>> > > The staging repository for this release can be found at:
>>>> > >
>>>>
>>>>https://urldefense.proofpoint.com/v2/url?u=https-3A__repository.apache.o
>>>>rg_content_repositories_orgapachespark-2D1069_&d=AwIFAw&c=izlc9mHr637UR4
>>>>lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=cyguR-hduPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2
>>>>JUA&m=frmHzwi9qJcMu2udAW6MBS4NWwKmHCBBpCG9zeuaRhA&s=TOEI0htKa2cktRFNdRiM
>>>>owZerFsTz44EPFC3qpzDzs8&e=
>>>> > >
>>>> > > The documentation corresponding to this release can be found at:
>>>> > >
>>>>https://urldefense.proofpoint.com/v2/url?u=http-3A__people.apache.org_-7
>>>>Epwendell_spark-2D1.3.0-2Drc1-2Ddocs_&d=AwIFAw&c=izlc9mHr637UR4lpLEZLFFS
>>>>3Vn2UXBrZ4tFb6oOnmz8&r=cyguR-hduPXP87jeUDbz1NGOZ18iIQjDTb_C1-_2JUA&m=frm
>>>>Hzwi9qJcMu2udAW6MBS4NWwKmHCBBpCG9zeuaRhA&s=iduBlV7hay0TwWj6-Gwto3ZBElN4k
>>>>0frDTIn0Ce8B8E&e=
>>>> > >
>>>> > > Please vote on releasing this package as Apache Spark 1.3.0!
>>>> > >
>>>> > > The vote is open until Saturday, February 21, at 08:03 UTC and
>>>>passes
>>>> > > if a majority of at least 3 +1 PMC votes are cast.
>>>> > >
>>>> > > [ ] +1 Release this package as Apache Spark 1.3.0
>>>> > > [ ] -1 Do not release this package because ...
>>>> > >
>>>> > > To learn more about Apache Spark, please see
>>>> > >
>>>>https://urldefense.proofpoint.com/v2/url?u=http-3A__spark.apache.org_&d=
>>>>AwIFAw&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=cyguR-hduPXP87jeU
>>>>Dbz1NGOZ18iIQjDTb_C1-_2JUA&m=frmHzwi9qJcMu2udAW6MBS4NWwKmHCBBpCG9zeuaRhA
>>>>&s=UPGEOKzVMEZ-8CqDq6dkvwzKpkF6fmBgy9ZVXanQOcE&e=
>>>> > >
>>>> > > == How can I help test this release? ==
>>>> > > If you are a Spark user, you can help us test this release by
>>>> > > taking a Spark 1.2 workload and running on this release candidate,
>>>> > > then reporting any regressions.
>>>> > >
>>>> > > == What justifies a -1 vote for this release? ==
>>>> > > This vote is happening towards the end of the 1.3 QA period,
>>>> > > so -1 votes should only occur for significant regressions from
>>>>1.2.1.
>>>> > > Bugs already present in 1.2.X, minor regressions, or bugs related
>>>> > > to new features will not block this release.
>>>> > >
>>>> > > - Patrick
>>>> > >
>>>> > >
>>>>---------------------------------------------------------------------
>>>> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> > > For additional commands, e-mail: dev-help@spark.apache.org
>>>> > >
>>>> > >
>>>> > >
>>>> > >
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > Marcelo
>>>> >
>>>> > ---------------------------------------------------------------------
>>>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> > For additional commands, e-mail: dev-help@spark.apache.org
>>>> >
>>>> >
>>>>
>>
>>---------------------------------------------------------------------
>>To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>For additional commands, e-mail: dev-help@spark.apache.org
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 04:22:02 2015
Return-Path: <dev-return-11836-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B897E17E95
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 04:22:02 +0000 (UTC)
Received: (qmail 9359 invoked by uid 500); 4 Mar 2015 04:22:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9267 invoked by uid 500); 4 Mar 2015 04:22:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9223 invoked by uid 99); 4 Mar 2015 04:22:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 04:22:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 04:21:54 +0000
Received: by oigi138 with SMTP id i138so3574455oig.4
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 20:19:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=HUEtJJnSHy1wREb34wnBHv2G9kPhSqlxphcliedrKtg=;
        b=neEdifLW3W0j9PxwgruBbNgdXSP52qyh85GEFA7/AJrCPgTUl+pS7P/Y72Q1EVvWW4
         FB9A/Aoo3P1m6Usj7oXnkLbhFhgPeDF9SnMgLsoLUQjjhBsr6xNT0jxuuhXTivx5InA1
         e0+FwDkdlSSKLEtIYoX1hSohyanYpy0r/Xc7jYRgwMJrxIKCATq3E7GJfeWM3E00BgX6
         pxBGs2J+dWOx9ok6JrdHKuSYJQgcbKyhsQUMByWEjKGZ5SNg8H7jaRqyS6UsLk0JC0eB
         ree4PHB+Z22b6NedRjnU0gb2SmddHDJkBRT9kTjmCtKoBP1HOnCS6pZIQCG5EjKmS8rv
         M7Ow==
MIME-Version: 1.0
X-Received: by 10.60.155.180 with SMTP id vx20mr1707249oeb.30.1425442759377;
 Tue, 03 Mar 2015 20:19:19 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Tue, 3 Mar 2015 20:19:19 -0800 (PST)
Date: Tue, 3 Mar 2015 20:19:19 -0800
Message-ID: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.3.0!

The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

Staging repositories for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1074/
(published with version '1.3.0')
https://repository.apache.org/content/repositories/orgapachespark-1075/
(published with version '1.3.0-rc2')

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/

Please vote on releasing this package as Apache Spark 1.3.0!

The vote is open until Saturday, March 07, at 04:17 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== How does this compare to RC1 ==
This patch includes a variety of bug fixes found in RC1.

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.2 workload and running on this release candidate,
then reporting any regressions.

If you are happy with this release based on your own testing, give a +1 vote.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.3 QA period,
so -1 votes should only occur for significant regressions from 1.2.1.
Bugs already present in 1.2.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 06:16:34 2015
Return-Path: <dev-return-11837-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0AB651011C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 06:16:34 +0000 (UTC)
Received: (qmail 16879 invoked by uid 500); 4 Mar 2015 06:16:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16802 invoked by uid 500); 4 Mar 2015 06:16:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16788 invoked by uid 99); 4 Mar 2015 06:16:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 06:16:32 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of spotvenky@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 06:16:26 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 3806915D435D
	for <dev@spark.apache.org>; Tue,  3 Mar 2015 22:15:38 -0800 (PST)
Date: Tue, 3 Mar 2015 23:15:36 -0700 (MST)
From: spotvenky <spotvenky@gmail.com>
To: dev@spark.apache.org
Message-ID: <1425449736459-10859.post@n3.nabble.com>
Subject: Sharing SparkContext across multiple Unit Test Scala files
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Can someone show me a code snippet on how I can create one SparkContext and
share it across multiple Unit Test files? I want the tests to run in
parallel as well. (i.e. parallelExecution in Test := true) 

I looked up SharedSparkContext, doesnt seem to work when tests are run in
parallel. Can someone guide me here.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Sharing-SparkContext-across-multiple-Unit-Test-Scala-files-tp10859.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 07:17:15 2015
Return-Path: <dev-return-11838-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B331102C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 07:17:15 +0000 (UTC)
Received: (qmail 28887 invoked by uid 500); 4 Mar 2015 07:17:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28804 invoked by uid 500); 4 Mar 2015 07:17:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28792 invoked by uid 99); 4 Mar 2015 07:17:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 07:17:06 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.220.53 as permitted sender)
Received: from [209.85.220.53] (HELO mail-pa0-f53.google.com) (209.85.220.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 07:17:01 +0000
Received: by paceu11 with SMTP id eu11so17213930pac.1
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 23:15:56 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=65i6JZP4kjzy3LbFTukMQZ2mwJvGwxOVn/OBXT8Kx60=;
        b=tVv9JJD1NjoG55mkfDR9O3ibD1H7bNL1GWUWwLIBRnuHfrH3Iyyz5yAW+o/Ey9ERJv
         gWF6E7kPLu6im8U8mptukgs9Ia0p7QtETvVGkdHqkxcqLW/iDQEL0qiAsKCtPd3eoShR
         ArfP2pDkeuQMO+MS5quBAm1PVwPvTtDkibUsKOepPHdLaoq9o4ATjDCfhyBKAnNSHB0p
         k+YpFLT9IiJBoDG9h92ETN6VNd8c3vbj2oJzrssCKnk/YAEmbpVkqPPlgacfcYUT4jAZ
         MjffwEsxNEj4GENMArVljFW8zUjIQhhAV39ifIoj4pVUfLWhoAX6rUFTOy+5S0wa4zUD
         J9zA==
MIME-Version: 1.0
X-Received: by 10.68.218.202 with SMTP id pi10mr4437924pbc.13.1425453355944;
 Tue, 03 Mar 2015 23:15:55 -0800 (PST)
Received: by 10.70.19.130 with HTTP; Tue, 3 Mar 2015 23:15:55 -0800 (PST)
In-Reply-To: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
Date: Tue, 3 Mar 2015 23:15:55 -0800
Message-ID: <CAOTBr2ndx_eKAqnpx8y8T5ej-+gjweE4Kiyp879wneQ8wDu6UQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Krishna Sankar <ksankar42@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b10cbedecd27e05107138d8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10cbedecd27e05107138d8
Content-Type: text/plain; charset=UTF-8

+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 13:53 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests -Dscala-2.11
2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
1.2.x
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
But MSE has increased from 40.81 to 105.86. Has some refactoring happened
on SGD/Linear Models ? Or do we have some extra parameters ? or change of
defaults ?
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
       Center And Scale OK
       WSSSE has come down slightly
2.5. rdd operations OK
      State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
       Model evaluation/optimization (rank, numIter, lmbda) with itertools
OK
3. Scala - MLlib
3.1. statistics (min,max,mean,Pearson,Spearman) OK
3.2. LinearRegressionWIthSGD OK
3.3. Decision Tree OK
3.4. KMeans OK
3.5. Recommendation (Movielens medium dataset ~1 M ratings) OK
4.0. SQL from Python
4.1. result = sqlContext.sql("SELECT * from Employees WHERE State = 'WA'")
OK

Cheers
<k/>

On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1074/
> (published with version '1.3.0')
> https://repository.apache.org/content/repositories/orgapachespark-1075/
> (published with version '1.3.0-rc2')
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == How does this compare to RC1 ==
> This patch includes a variety of bug fixes found in RC1.
>
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1
> vote.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b10cbedecd27e05107138d8--

From dev-return-11839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 07:29:19 2015
Return-Path: <dev-return-11839-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0EA8C1032E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 07:29:19 +0000 (UTC)
Received: (qmail 59326 invoked by uid 500); 4 Mar 2015 07:29:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59249 invoked by uid 500); 4 Mar 2015 07:29:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59237 invoked by uid 99); 4 Mar 2015 07:29:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 07:29:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 07:29:10 +0000
Received: by mail-qa0-f52.google.com with SMTP id v10so31974417qac.11
        for <dev@spark.apache.org>; Tue, 03 Mar 2015 23:28:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=iKRypbLMl/urLK/Zyqaub6ZVpzwWAdG5LXccQa0tFig=;
        b=AhGKy+ht7qtYSbOnoSlHZ/LbaPaEFCDrCMMImUptZynzR848WGeocKRMTpseoQXVvq
         c0N+z4VHfQETEGjbiNZwE/ofdN5hXBQXrEpCuo0OXnSPfG7BUzsgBNe/aYlsKKcCokcd
         2wrKp1u6V+IluYLID/c8vCp1nSii621+ifBeqSsIJz7ESmQs+CzAjyIg+zgebR6Or4Z0
         J6xp4d7ESN2RC5S9KGGsF3PIgIRVf0UkLP1cu7jlRSOSrvsJ91kfwRxgoiihrqCLdoix
         5Td7DENsQYwWZvkONlpM+VTX694XkzELHM+RoZ7cGrUieC53qZIKcOVHCEcFzeALabLN
         YkLg==
MIME-Version: 1.0
X-Received: by 10.55.41.99 with SMTP id p96mr4815995qkh.98.1425454085396; Tue,
 03 Mar 2015 23:28:05 -0800 (PST)
Received: by 10.140.97.182 with HTTP; Tue, 3 Mar 2015 23:28:05 -0800 (PST)
In-Reply-To: <CAOTBr2ndx_eKAqnpx8y8T5ej-+gjweE4Kiyp879wneQ8wDu6UQ@mail.gmail.com>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
	<CAOTBr2ndx_eKAqnpx8y8T5ej-+gjweE4Kiyp879wneQ8wDu6UQ@mail.gmail.com>
Date: Tue, 3 Mar 2015 23:28:05 -0800
Message-ID: <CAJgQjQ84O20r9p-Rg2mYY_EWcmkrShSBAQWcShB6VxbBr8MwJA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Xiangrui Meng <mengxr@gmail.com>
To: Krishna Sankar <ksankar42@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Tue, Mar 3, 2015 at 11:15 PM, Krishna Sankar <ksankar42@gmail.com> wrote:
> +1 (non-binding, of course)
>
> 1. Compiled OSX 10.10 (Yosemite) OK Total time: 13:53 min
>      mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
> -Dhadoop.version=2.6.0 -Phive -DskipTests -Dscala-2.11
> 2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
> 1.2.x
> 2.1. statistics (min,max,mean,Pearson,Spearman) OK
> 2.2. Linear/Ridge/Laso Regression OK
> But MSE has increased from 40.81 to 105.86. Has some refactoring happened
> on SGD/Linear Models ? Or do we have some extra parameters ? or change of
> defaults ?

Could you share the code you used? I don't remember any changes in
linear regression. Thanks! -Xiangrui

> 2.3. Decision Tree, Naive Bayes OK
> 2.4. KMeans OK
>        Center And Scale OK
>        WSSSE has come down slightly
> 2.5. rdd operations OK
>       State of the Union Texts - MapReduce, Filter,sortByKey (word count)
> 2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
>        Model evaluation/optimization (rank, numIter, lmbda) with itertools
> OK
> 3. Scala - MLlib
> 3.1. statistics (min,max,mean,Pearson,Spearman) OK
> 3.2. LinearRegressionWIthSGD OK
> 3.3. Decision Tree OK
> 3.4. KMeans OK
> 3.5. Recommendation (Movielens medium dataset ~1 M ratings) OK
> 4.0. SQL from Python
> 4.1. result = sqlContext.sql("SELECT * from Employees WHERE State = 'WA'")
> OK
>
> Cheers
> <k/>
>
> On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.3.0!
>>
>> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> Staging repositories for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1074/
>> (published with version '1.3.0')
>> https://repository.apache.org/content/repositories/orgapachespark-1075/
>> (published with version '1.3.0-rc2')
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.3.0!
>>
>> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.3.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == How does this compare to RC1 ==
>> This patch includes a variety of bug fixes found in RC1.
>>
>> == How can I help test this release? ==
>> If you are a Spark user, you can help us test this release by
>> taking a Spark 1.2 workload and running on this release candidate,
>> then reporting any regressions.
>>
>> If you are happy with this release based on your own testing, give a +1
>> vote.
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening towards the end of the 1.3 QA period,
>> so -1 votes should only occur for significant regressions from 1.2.1.
>> Bugs already present in 1.2.X, minor regressions, or bugs related
>> to new features will not block this release.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 09:18:27 2015
Return-Path: <dev-return-11840-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C34C107B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 09:18:27 +0000 (UTC)
Received: (qmail 9775 invoked by uid 500); 4 Mar 2015 09:18:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9694 invoked by uid 500); 4 Mar 2015 09:18:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9681 invoked by uid 99); 4 Mar 2015 09:18:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 09:18:12 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ksankar42@gmail.com designates 209.85.220.53 as permitted sender)
Received: from [209.85.220.53] (HELO mail-pa0-f53.google.com) (209.85.220.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 09:17:46 +0000
Received: by pablj1 with SMTP id lj1so25767691pab.8
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 01:17:44 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=KAZ2LaxsuwFLEBHBonkcAeQwmCVJqLenpCxeo3jg9GE=;
        b=VGYPMsXUon+lXD0enTgGZRrGg3gAuISrZ2fYfrVXav70VJMNy5svxpLtcHpCEg0HNt
         yQiaye1z30w0y2FZEClD3RbrIsOW1Bliyut7+TVoE/++nBVK42mJrWy/8KvLfwNdQkz/
         +l+xCPyNGuPPXJPGuJKJnxjfJNeh+HOD69JxdXfB/vL7KZUqo+Js4TvC54ZSMjgsCFx0
         qosQkGYXWnmXM03kYOGw646mNcP5xcHnVOgO8BMUp1GFfWGzWOhb1t3kiwCG3y8Vk033
         lg4cXWH9FkUA+GZ6EVJRxWIdmK7ZRmeB46GtNrIObSmj63Uvtjh4RyOhWeveM6VIlk8w
         VD/A==
MIME-Version: 1.0
X-Received: by 10.68.234.164 with SMTP id uf4mr5072956pbc.37.1425460663913;
 Wed, 04 Mar 2015 01:17:43 -0800 (PST)
Received: by 10.70.19.130 with HTTP; Wed, 4 Mar 2015 01:17:43 -0800 (PST)
In-Reply-To: <CAJgQjQ84O20r9p-Rg2mYY_EWcmkrShSBAQWcShB6VxbBr8MwJA@mail.gmail.com>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
	<CAOTBr2ndx_eKAqnpx8y8T5ej-+gjweE4Kiyp879wneQ8wDu6UQ@mail.gmail.com>
	<CAJgQjQ84O20r9p-Rg2mYY_EWcmkrShSBAQWcShB6VxbBr8MwJA@mail.gmail.com>
Date: Wed, 4 Mar 2015 01:17:43 -0800
Message-ID: <CAOTBr2=bQ=mT=FiMUBfWB-6N1=HMSO-vpnY4uhFPjeztCh5hsA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Krishna Sankar <ksankar42@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b33960b83920c051072ecd8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33960b83920c051072ecd8
Content-Type: text/plain; charset=UTF-8

It is the LR over car-data at https://github.com/xsankar/cloaked-ironman.
1.2.0 gives Mean Squared Error = 40.8130551358
1.3.0 gives Mean Squared Error = 105.857603953

I will verify it one more time tomorrow.

Cheers
<k/>

On Tue, Mar 3, 2015 at 11:28 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> On Tue, Mar 3, 2015 at 11:15 PM, Krishna Sankar <ksankar42@gmail.com>
> wrote:
> > +1 (non-binding, of course)
> >
> > 1. Compiled OSX 10.10 (Yosemite) OK Total time: 13:53 min
> >      mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
> > -Dhadoop.version=2.6.0 -Phive -DskipTests -Dscala-2.11
> > 2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
> > 1.2.x
> > 2.1. statistics (min,max,mean,Pearson,Spearman) OK
> > 2.2. Linear/Ridge/Laso Regression OK
> > But MSE has increased from 40.81 to 105.86. Has some refactoring happened
> > on SGD/Linear Models ? Or do we have some extra parameters ? or change of
> > defaults ?
>
> Could you share the code you used? I don't remember any changes in
> linear regression. Thanks! -Xiangrui
>
> > 2.3. Decision Tree, Naive Bayes OK
> > 2.4. KMeans OK
> >        Center And Scale OK
> >        WSSSE has come down slightly
> > 2.5. rdd operations OK
> >       State of the Union Texts - MapReduce, Filter,sortByKey (word count)
> > 2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
> >        Model evaluation/optimization (rank, numIter, lmbda) with
> itertools
> > OK
> > 3. Scala - MLlib
> > 3.1. statistics (min,max,mean,Pearson,Spearman) OK
> > 3.2. LinearRegressionWIthSGD OK
> > 3.3. Decision Tree OK
> > 3.4. KMeans OK
> > 3.5. Recommendation (Movielens medium dataset ~1 M ratings) OK
> > 4.0. SQL from Python
> > 4.1. result = sqlContext.sql("SELECT * from Employees WHERE State =
> 'WA'")
> > OK
> >
> > Cheers
> > <k/>
> >
> > On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> Please vote on releasing the following candidate as Apache Spark version
> >> 1.3.0!
> >>
> >> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> Staging repositories for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1074/
> >> (published with version '1.3.0')
> >> https://repository.apache.org/content/repositories/orgapachespark-1075/
> >> (published with version '1.3.0-rc2')
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.3.0!
> >>
> >> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.3.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == How does this compare to RC1 ==
> >> This patch includes a variety of bug fixes found in RC1.
> >>
> >> == How can I help test this release? ==
> >> If you are a Spark user, you can help us test this release by
> >> taking a Spark 1.2 workload and running on this release candidate,
> >> then reporting any regressions.
> >>
> >> If you are happy with this release based on your own testing, give a +1
> >> vote.
> >>
> >> == What justifies a -1 vote for this release? ==
> >> This vote is happening towards the end of the 1.3 QA period,
> >> so -1 votes should only occur for significant regressions from 1.2.1.
> >> Bugs already present in 1.2.X, minor regressions, or bugs related
> >> to new features will not block this release.
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>

--047d7b33960b83920c051072ecd8--

From dev-return-11841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 10:12:56 2015
Return-Path: <dev-return-11841-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2070E10961
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 10:12:56 +0000 (UTC)
Received: (qmail 80182 invoked by uid 500); 4 Mar 2015 10:04:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80050 invoked by uid 500); 4 Mar 2015 10:04:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79240 invoked by uid 99); 4 Mar 2015 10:04:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 10:04:52 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [203.81.22.165] (HELO mail1.qilinsoft.com) (203.81.22.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 10:04:48 +0000
X-MimeOLE: Produced By Microsoft Exchange V6.5
Content-class: urn:content-classes:message
MIME-Version: 1.0
Content-Type: text/plain;
	charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
Subject: Spark Streaming and SchemaRDD usage
Date: Wed, 4 Mar 2015 18:04:24 +0800
Message-ID: <2EB23AF5EEEA2140946B8F292EB2EB9F1AD839@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
Thread-Topic: Spark Streaming and SchemaRDD usage
Thread-Index: AdBWYpdhCPbpIBipRV6AV5rPB/a+jQ==
From: "Haopu Wang" <HWang@qilinsoft.com>
To: "user" <user@spark.apache.org>,
	<dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, in the roadmap of Spark in 2015 (link:
http://files.meetup.com/3138542/Spark%20in%202015%20Talk%20-%20Wendell.p
ptx), I saw SchemaRDD is designed to be the basis of BOTH Spark
Streaming and Spark SQL.

My question is: what's the typical usage of SchemaRDD in a Spark
Streaming application? Thank you very much!


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 11:50:14 2015
Return-Path: <dev-return-11842-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 519B910BD1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 11:50:14 +0000 (UTC)
Received: (qmail 96862 invoked by uid 500); 4 Mar 2015 11:49:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96807 invoked by uid 500); 4 Mar 2015 11:49:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96796 invoked by uid 99); 4 Mar 2015 11:49:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 11:49:56 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [212.124.192.214] (HELO nicola.2020media.net.uk) (212.124.192.214)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 11:49:52 +0000
Received: from [86.7.249.6] (helo=[192.168.0.8])
	by nicola.2020media.net.uk with esmtpsa (TLSv1:AES256-SHA:256)
	(Exim 4.72)
	(envelope-from <robin.east@xense.co.uk>)
	id 1YT7ni-0001TW-AZ; Wed, 04 Mar 2015 11:49:18 +0000
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Robin East <robin.east@xense.co.uk>
In-Reply-To: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
Date: Wed, 4 Mar 2015 11:49:30 +0000
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <6AB2623D-49D6-4124-B96B-36B51159A3F3@xense.co.uk>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
X-Mailer: Apple Mail (2.2070.6)
X-2020-Relay: Sent using 2020MEDIA.net.uk relay with auth code: xense
 Send Abuse reports to abuse@2020media.net.uk
X-Virus-Checked: Checked by ClamAV on apache.org

+1 (subject to comments on ec2 issues below)

machine 1: Macbook Air, OSX 10.10.2 (Yosemite), Java 8
machine 2: iMac, OSX 10.8.4, Java 7

1. mvn clean package -DskipTests  (33min/13min)
2. ran SVM benchmark https://github.com/insidedctm/spark-mllib-benchmark


EC2 issues:

1) Unable to successfully run servers on ec2 using the following =
command:
./spark-ec2 -k <key> -i <pathtokey> -s 2  launch test

Error was:
  Initializing Spark
  ERROR: Unknown Spark Version

This seems to be due to init.sh in =
https://github.com/mesos/spark-ec2/tree/branch-1.3/spark not being =
updated for version 1.3.0.=20
Question: Is it expected that this repository is updated in tandem with =
the Spark release?

2) One of my machines fails to authenticate when running the ec2 =
scripts. Using spark-ec2 I get the following error:
  ssl.SSLError: [Errno 1] _ssl.c:504: error:14090086:SSL =
routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed

It seems this machine has a problem where openssl can=92t verify server =
certificates. Not been a problem for me before but spark1.3.0 has =
updated the boto repository it uses for EC2 functionality to 2.34.0. =
This new version has the capability to verify server certificates when =
authenticating to AWS servers and the default behaviour is to check =
certificates. Clearly this is not a Spark issue per se however if the =
problem I have is reasonably widespread then there maybe quite a few =
issues coming up on the mailing lists. There is a workaround option that =
could be put into spark_ec2.py script to turn off certificate validation =
if necessary.

> On 4 Mar 2015, at 04:19, Patrick Wendell <pwendell@gmail.com> wrote:
>=20
> Please vote on releasing the following candidate as Apache Spark =
version 1.3.0!
>=20
> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D3af26=
870e5163438868c4eb2df88380a533bb232
>=20
> The release files, including signatures, digests, etc. can be found =
at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>=20
> Staging repositories for this release can be found at:
> =
https://repository.apache.org/content/repositories/orgapachespark-1074/
> (published with version '1.3.0')
> =
https://repository.apache.org/content/repositories/orgapachespark-1075/
> (published with version '1.3.0-rc2')
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.3.0!
>=20
> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>=20
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>=20
> =3D=3D How does this compare to RC1 =3D=3D
> This patch includes a variety of bug fixes found in RC1.
>=20
> =3D=3D How can I help test this release? =3D=3D
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>=20
> If you are happy with this release based on your own testing, give a =
+1 vote.
>=20
> =3D=3D What justifies a -1 vote for this release? =3D=3D
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 12:11:01 2015
Return-Path: <dev-return-11843-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B71E10CAC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 12:11:01 +0000 (UTC)
Received: (qmail 38826 invoked by uid 500); 4 Mar 2015 12:11:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38736 invoked by uid 500); 4 Mar 2015 12:11:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38725 invoked by uid 99); 4 Mar 2015 12:10:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 12:10:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of uli@spielviel.de designates 83.223.83.232 as permitted sender)
Received: from [83.223.83.232] (HELO uli.spielviel.de) (83.223.83.232)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 12:10:53 +0000
Received: from STAUL-AMBP.local (unknown [213.20.244.134])
	by uli.spielviel.de (Postfix) with ESMTPSA id 7DFE759F051;
	Wed,  4 Mar 2015 13:10:32 +0100 (CET)
Message-ID: <54F6F638.9050103@spielviel.de>
Date: Wed, 04 Mar 2015 13:10:32 +0100
From: =?UTF-8?B?VWxyaWNoIFN0w6Ryaw==?= <uli@spielviel.de>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: manojkumarsivaraj334@gmail.com, dev@spark.apache.org
CC: dev@community.apache.org, Xiangrui Meng <mengxr@gmail.com>
Subject: Re: Google Summer of Code - Quick Query
References: <CAFQAd-nvbjph0RH52yPbVf+f=3XF+otPMCzDd6f_=3eeZDuvRQ@mail.gmail.com>
In-Reply-To: <CAFQAd-nvbjph0RH52yPbVf+f=3XF+otPMCzDd6f_=3eeZDuvRQ@mail.gmail.com>
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Manoj,

this question is best asked on the Spark mailing lists (copied). From a formal point of view all
that counts is your proposal in Melange once applications start but your mentor or the project you
wish to contribute to may have additional requirements.

Cheers,

Uli

On 2015-03-03 08:54, Manoj Kumar wrote:
> Hello,
> 
> I am Manoj, a prospective student from the Apache Spark project. I have
> been contributing to Spark and discussing the project idea with my mentor
> for some time now. The tentative project has a number of JIRA's associated
> with it. I would still like to know if it is necessary to create an
> umbrella JIRA with all the other JIRA's  linked to it (and tagged with
> gsoc) or is it sufficient to just upload a proposal when the registration
> opens.
> 
> 
> Regards
> 

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 17:29:56 2015
Return-Path: <dev-return-11844-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3775A17CE0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 17:29:56 +0000 (UTC)
Received: (qmail 64206 invoked by uid 500); 4 Mar 2015 17:28:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64134 invoked by uid 500); 4 Mar 2015 17:28:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64123 invoked by uid 99); 4 Mar 2015 17:28:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 17:28:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 17:28:30 +0000
Received: by igdh15 with SMTP id h15so38376946igd.4
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 09:26:13 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=oM/SH8Vc53EifIedgAY7FTc9VK8UHfQ/A7KOasb6so0=;
        b=dJoCwX0UhDvH8rhl4LUFwVLLIUNR0lN8jW2uVxHPx31t5usinlGYBZm/CbWYv8GNW4
         FADuM6gR1ofowz4i7W1K/d9wgVXkpecfrkclCyR1ow5AtRWC1QYFnDfeTvy48B3dftF/
         InV1d7XQ9f7cuTWgiAvHHzbDyrqzlnAl7XGHBM75er5lQa/HFHVmaMxiLwEdRyOgU4K2
         lLvR3ZxFKmjfe1KPpSPuYJO4rJzCB536C/hP5kbxjRcGrdX8F2QKt6jEoUrAB+c0c+UH
         CjqQ5Fdhoa5B2k0K977l80f3fGCm4mFDg+8CgjYzcQstGhKu7kCHZ94Vlz9UerP5WXAq
         2iow==
X-Gm-Message-State: ALoCoQkcaYOLRJxEvPuD6ia6h5yrii0RawppWjgn5XoBC1aNuBR0e58D+OPgxwiT7idcFaDPKk4v
MIME-Version: 1.0
X-Received: by 10.107.149.212 with SMTP id x203mr13481176iod.12.1425489973105;
 Wed, 04 Mar 2015 09:26:13 -0800 (PST)
Received: by 10.36.88.8 with HTTP; Wed, 4 Mar 2015 09:26:13 -0800 (PST)
In-Reply-To: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
Date: Wed, 4 Mar 2015 09:26:13 -0800
Message-ID: <CAAOnQ7vxhK2MLh_TQu6OXupgx0MBoz04r_DRUcdEgaO5aUbG7Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I haven't tested the rc2 bits yet, but I'd consider
https://issues.apache.org/jira/browse/SPARK-6144 a serious regression
from 1.2 (since it affects existing "addFile()" functionality if the
URL is "hdfs:...").

Will test other parts separately.

On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1074/
> (published with version '1.3.0')
> https://repository.apache.org/content/repositories/orgapachespark-1075/
> (published with version '1.3.0-rc2')
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == How does this compare to RC1 ==
> This patch includes a variety of bug fixes found in RC1.
>
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1 vote.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 17:36:53 2015
Return-Path: <dev-return-11845-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EDD4617D1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 17:36:52 +0000 (UTC)
Received: (qmail 84223 invoked by uid 500); 4 Mar 2015 17:36:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84133 invoked by uid 500); 4 Mar 2015 17:36:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84116 invoked by uid 99); 4 Mar 2015 17:36:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 17:36:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 17:36:41 +0000
Received: by oiav1 with SMTP id v1so6964162oia.9
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 09:36:20 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=2YOdmq49t0MgHggDFB82aDuCD4taAkRONOc/vxV7bXw=;
        b=Frg+0MejPRaQE4dgHooMDGbdff9fDE68S+IBhVgYW6BSuuBlhW+c4qIg3TV/piN0FB
         x7kOEAOum6BZfvLkQzvWEdIqLM3S5Qw+Ab72JfHL2ywyShzGnnFg9WnB20u+lAGDs55p
         hqC43ZC7KxejuMOacPkJRu+Kj+Rvsst+rU6LY7QDD7XPz9WbFbJ4GNyFnnHuex9Q2KzL
         gtlPtuQ1p7hsbDHPjC0esHTJMm1K0kURpmwpg1Ugt9SbQypcaP2Pn5tPZAtIJIIsyWFP
         jH3SbOCkoaLUItVcKpfz2a+hA/Lo/P4K20L0GgUDN780mAQ+jZAhwm/ao2LIpHCwZhpJ
         bJ/A==
MIME-Version: 1.0
X-Received: by 10.182.215.163 with SMTP id oj3mr3888798obc.49.1425490580844;
 Wed, 04 Mar 2015 09:36:20 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Wed, 4 Mar 2015 09:36:20 -0800 (PST)
In-Reply-To: <CAAOnQ7vxhK2MLh_TQu6OXupgx0MBoz04r_DRUcdEgaO5aUbG7Q@mail.gmail.com>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
	<CAAOnQ7vxhK2MLh_TQu6OXupgx0MBoz04r_DRUcdEgaO5aUbG7Q@mail.gmail.com>
Date: Wed, 4 Mar 2015 09:36:20 -0800
Message-ID: <CABPQxsvvce2FUMLpp2mAFUVQ2TEek2HaPCMS1d-PUZ2w2en5LA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Marcelo,

Yes - I agree. That one trickled in just as I was packaging this RC.
However, I still put this out here to allow people to test the
existing fixes, etc.

- Patrick

On Wed, Mar 4, 2015 at 9:26 AM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> I haven't tested the rc2 bits yet, but I'd consider
> https://issues.apache.org/jira/browse/SPARK-6144 a serious regression
> from 1.2 (since it affects existing "addFile()" functionality if the
> URL is "hdfs:...").
>
> Will test other parts separately.
>
> On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>>
>> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> Staging repositories for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1074/
>> (published with version '1.3.0')
>> https://repository.apache.org/content/repositories/orgapachespark-1075/
>> (published with version '1.3.0-rc2')
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.3.0!
>>
>> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.3.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == How does this compare to RC1 ==
>> This patch includes a variety of bug fixes found in RC1.
>>
>> == How can I help test this release? ==
>> If you are a Spark user, you can help us test this release by
>> taking a Spark 1.2 workload and running on this release candidate,
>> then reporting any regressions.
>>
>> If you are happy with this release based on your own testing, give a +1 vote.
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening towards the end of the 1.3 QA period,
>> so -1 votes should only occur for significant regressions from 1.2.1.
>> Bugs already present in 1.2.X, minor regressions, or bugs related
>> to new features will not block this release.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>
>
>
> --
> Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 19:10:53 2015
Return-Path: <dev-return-11846-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5637C10243
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 19:10:53 +0000 (UTC)
Received: (qmail 27106 invoked by uid 500); 4 Mar 2015 19:10:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27031 invoked by uid 500); 4 Mar 2015 19:10:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27018 invoked by uid 99); 4 Mar 2015 19:10:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 19:10:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 19:10:19 +0000
Received: by igkb16 with SMTP id b16so39277889igk.1
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 11:09:32 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=MWPj7OIY60uSFYncrVI2ozyEysphkMx7cq9M4oUafxU=;
        b=Imm7xM21ZCzUxNdGwGoJL/RUEYAAdZ6RgnWhxQ6rcwh4GLXFG/yoikahgMTzPZJlEI
         u4rQmY6RLXx5I8pyb2utqHKfEO5ebk2RaMvB9vpwh+CBaRaLrlNmGK+WwXzKKnF+iPxI
         pntzcusEl1om0qA58XdoEbzku4OcJmIj+1r7NnvxYJfxpsk+oWBllbkBuozgsKUUm1xC
         y/Dvp65f3PdwcTfmoo4MU8yiQiKYy0jVnxnHPiERf61Aj/ClP6sgYu/kMSIBOR4Jjets
         VUSAdPzTmTHp1rgTpBlf7JGg1GQ36gKkW2vSTZFXvvpciN0zKU1cKvOnqXox5F9MNyxC
         zQ3Q==
X-Gm-Message-State: ALoCoQnQDzbrHMdAFlQ3y0XYQhvsis5w62o55okNEutSnEmBmAPYBW5rJzOsCQ6D1Lc/0pVeoEgD
MIME-Version: 1.0
X-Received: by 10.50.253.12 with SMTP id zw12mr40120837igc.24.1425496172584;
 Wed, 04 Mar 2015 11:09:32 -0800 (PST)
Received: by 10.36.88.8 with HTTP; Wed, 4 Mar 2015 11:09:32 -0800 (PST)
In-Reply-To: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
Date: Wed, 4 Mar 2015 11:09:32 -0800
Message-ID: <CAAOnQ7suK+cAzdr-st=Oe0Fnf-rHHcCD-k7dFA5O=tKWP-nxFA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

-1 (non-binding) because of SPARK-6144.

But aside from that I ran a set of tests on top of standalone and yarn
and things look good.

On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1074/
> (published with version '1.3.0')
> https://repository.apache.org/content/repositories/orgapachespark-1075/
> (published with version '1.3.0-rc2')
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == How does this compare to RC1 ==
> This patch includes a variety of bug fixes found in RC1.
>
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1 vote.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 21:08:24 2015
Return-Path: <dev-return-11847-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0616410988
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 21:08:24 +0000 (UTC)
Received: (qmail 41271 invoked by uid 500); 4 Mar 2015 21:08:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41190 invoked by uid 500); 4 Mar 2015 21:08:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41179 invoked by uid 99); 4 Mar 2015 21:08:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 21:08:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.178 as permitted sender)
Received: from [209.85.217.178] (HELO mail-lb0-f178.google.com) (209.85.217.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 21:08:17 +0000
Received: by lbvp9 with SMTP id p9so24143783lbv.8
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 13:07:10 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=2dzn+JJthBiU9VALdfTk5fR5PAXMqREsiFz4kdHsr/Q=;
        b=GmxwU9dY4z/9vBfywqEtspusnwvA7sK1gobNTF5GO13sfxhg1S3YujmWpLxs16yfkW
         bQ++MfF9255Xpigy2dsONY/VWOgr25e7hMzZlyeOrTi8MNsnnhqQ4XyQ4nBK4iCSlJaw
         GAWNQM+LKq2Fvav3vAvYYs2/7Etbdp2Z5Ws7cdJzUEgbe5qYPgXWrTydaKYq0AeB7/4y
         2EIPRANey3PrDCSvchqeshq7zQ9TB8gIscpmJ2I17YOXngW7DlNr4o/H5ItpH3YcXhYi
         1Y2pz3cWpwe/ZmQUhcjAMEYM/L74s+hUt6m8ExMJiI2O+Bi+ajkCioLTRM4cA/7bh9BV
         HS8w==
X-Gm-Message-State: ALoCoQnGng5p/lJ2F4TKJERO9nEzplvoVOMaWim83FZkb03/Jl31WB0th4QMVrqWiRIXLtczNnA8
X-Received: by 10.152.28.233 with SMTP id e9mr5142094lah.3.1425503230369; Wed,
 04 Mar 2015 13:07:10 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Wed, 4 Mar 2015 13:06:50 -0800 (PST)
From: shane knapp <sknapp@berkeley.edu>
Date: Wed, 4 Mar 2015 13:06:50 -0800
Message-ID: <CACdU-dQxLyEc7HVA9YONPkkfk5aEX+7Xwo7AWngvGYr+FOsuPQ@mail.gmail.com>
Subject: short jenkins 7am downtime tomorrow morning (3-5-15)
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158c7ccac312805107cd54d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c7ccac312805107cd54d
Content-Type: text/plain; charset=UTF-8

the master and workers need some system and package updates, and i'll also
be rebooting the machines as well.

this shouldn't take very long to perform, and i expect jenkins to be back
up and building by 9am at the *latest*.

important note:  i will NOT be updating jenkins or any of the plugins
during this maintenance!

as always, please let me know if you have any questions or concerns.

danke shane

--089e0158c7ccac312805107cd54d--

From dev-return-11848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar  4 23:23:49 2015
Return-Path: <dev-return-11848-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6DCB117313
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  4 Mar 2015 23:23:49 +0000 (UTC)
Received: (qmail 29955 invoked by uid 500); 4 Mar 2015 23:23:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29876 invoked by uid 500); 4 Mar 2015 23:23:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29864 invoked by uid 99); 4 Mar 2015 23:23:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 23:23:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 04 Mar 2015 23:23:19 +0000
Received: by widex7 with SMTP id ex7so34428637wid.0
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 15:23:18 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=cE4buH9VbSrMtz7yUInfypX15Z8SLN/EKzy2NmlF0ec=;
        b=kF3isxPc/soqb4CLvk4Oj0vxGRqVgT9Zc0Pucww48KfjEVgWcvYEy1hj44L5cVHJhd
         3PVoY4vHuv8GNtsDeNVHNbIaXCyJyaz0Ee4WogS0Y0CM7Bd7sEYl+OFoDOVH0dptz4z6
         DfvbKcj8W7QlMRilmBE7KUt5yIrIYv5yIRs122rjrl6l+ug5QtRMC3ryHbbyAf7LLvOM
         I6QKgrT+1YHmHzv7MwJjBXubm32YUVbekWtn+geOm1AlhPCoZ9izVX5jTUe4jyQHf28Z
         sxEGBEbk3rZf6ofk4TAa3+WVwiRUEGuAvJJ3ul9MT/S4irtrDgFsVFYXrK1PCN0ISdGa
         BM5g==
X-Gm-Message-State: ALoCoQlq5VP7tXR956b7wjxl6WRb0xSYOcO5NToe+D1+C39jETcKVPbJFC0OkoESdXVesNNYNEp4
X-Received: by 10.180.37.110 with SMTP id x14mr17054956wij.45.1425511397988;
 Wed, 04 Mar 2015 15:23:17 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Wed, 4 Mar 2015 15:22:57 -0800 (PST)
In-Reply-To: <CAAOnQ7suK+cAzdr-st=Oe0Fnf-rHHcCD-k7dFA5O=tKWP-nxFA@mail.gmail.com>
References: <CABPQxss=gOecdXY=2hdWy3WfYaUvP3GjzJSwGPzAkeoBPXd7zg@mail.gmail.com>
 <CAAOnQ7suK+cAzdr-st=Oe0Fnf-rHHcCD-k7dFA5O=tKWP-nxFA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 4 Mar 2015 23:22:57 +0000
Message-ID: <CAMAsSdL+1SXhRdrUgMuuTGBiLTNhbNq+aFwVEtGU43Jac4frkw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC2)
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I think we will have to fix
https://issues.apache.org/jira/browse/SPARK-5143 as well before the
final 1.3.x.

But yes everything else checks out for me, including sigs and hashes
and building the source release.

I have been following JIRA closely and am not aware of other blockers
besides the ones already identified.

On Wed, Mar 4, 2015 at 7:09 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> -1 (non-binding) because of SPARK-6144.
>
> But aside from that I ran a set of tests on top of standalone and yarn
> and things look good.
>
> On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>>
>> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> Staging repositories for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1074/
>> (published with version '1.3.0')
>> https://repository.apache.org/content/repositories/orgapachespark-1075/
>> (published with version '1.3.0-rc2')
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.3.0!
>>
>> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.3.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == How does this compare to RC1 ==
>> This patch includes a variety of bug fixes found in RC1.
>>
>> == How can I help test this release? ==
>> If you are a Spark user, you can help us test this release by
>> taking a Spark 1.2 workload and running on this release candidate,
>> then reporting any regressions.
>>
>> If you are happy with this release based on your own testing, give a +1 vote.
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening towards the end of the 1.3 QA period,
>> so -1 votes should only occur for significant regressions from 1.2.1.
>> Bugs already present in 1.2.X, minor regressions, or bugs related
>> to new features will not block this release.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>
>
>
> --
> Marcelo
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 00:03:16 2015
Return-Path: <dev-return-11849-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 34EBD17499
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 00:03:16 +0000 (UTC)
Received: (qmail 37845 invoked by uid 500); 5 Mar 2015 00:02:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37768 invoked by uid 500); 5 Mar 2015 00:02:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37756 invoked by uid 99); 5 Mar 2015 00:02:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 00:02:52 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 00:02:27 +0000
Received: from EX02-WEST.YOJOE.local ([169.254.1.145]) by
 EX03-WEST.YOJOE.local ([169.254.2.196]) with mapi id 14.03.0195.001; Wed, 4
 Mar 2015 16:01:17 -0800
From: Mingyu Kim <mkim@palantir.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
CC: Matt Cheah <mcheah@palantir.com>, Eric Lin <elin@palantir.com>
Subject: Task result is serialized twice by serializer and closure serializer
Thread-Topic: Task result is serialized twice by serializer and closure
 serializer
Thread-Index: AQHQVteA9qc4PgIBuE+tJrqAQ1mKPg==
Date: Thu, 5 Mar 2015 00:01:16 +0000
Message-ID: <D11CDCCB.1F5C5%mkim@palantir.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.100.91.90]
Content-Type: multipart/alternative;
	boundary="_000_D11CDCCB1F5C5mkimpalantircom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D11CDCCB1F5C5mkimpalantircom_
Content-Type: text/plain; charset="Windows-1252"
Content-Transfer-Encoding: quoted-printable

Hi all,

It looks like the result of task is serialized twice, once by serializer (I=
.e. Java/Kryo depending on configuration) and once again by closure seriali=
zer (I.e. Java). To link the actual code,

The first one: https://github.com/apache/spark/blob/master/core/src/main/sc=
ala/org/apache/spark/executor/Executor.scala#L213
The second one: https://github.com/apache/spark/blob/master/core/src/main/s=
cala/org/apache/spark/executor/Executor.scala#L226

This serializes the =93value=94, which is the result of task run twice, whi=
ch affects things like collect(), takeSample(), and toLocalIterator(). Woul=
d it make sense to simply serialize the DirectTaskResult once using the reg=
ular =93serializer=94 (as opposed to closure serializer)? Would it cause pr=
oblems when the Accumulator values are not Kryo-serializable?

Alternatively, if we can assume that Accumator values are small, we can clo=
sure-serialize those, put the serialized byte array in DirectTaskResult wit=
h the raw task result =93value=94, and serialize DirectTaskResult.

What do people think?

Thanks,
Mingyu

--_000_D11CDCCB1F5C5mkimpalantircom_--

From dev-return-11850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 01:10:11 2015
Return-Path: <dev-return-11850-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB6E617619
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 01:10:11 +0000 (UTC)
Received: (qmail 75039 invoked by uid 500); 5 Mar 2015 01:09:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74958 invoked by uid 500); 5 Mar 2015 01:09:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74946 invoked by uid 99); 5 Mar 2015 01:09:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:09:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.54 as permitted sender)
Received: from [209.85.218.54] (HELO mail-oi0-f54.google.com) (209.85.218.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:09:49 +0000
Received: by oifu20 with SMTP id u20so9064188oif.12
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 17:07:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=XmRyKOosEJQhXS+D4bA1x7/sO0LYfCUg40PuB0UXBmM=;
        b=f6o+F2sUiSsdFt1dP6TZFa4wLd+L15j7AmX4QFmIKKXB3QQKkEsI6Lghwn5frOtrI0
         A9+FGxeqyYJHMJiuGP+XBLMpYKTrRSj1dVBoUC0h9Cttec+wMrI2rfld7379F/xU6ipu
         vkVJ4fvUT9WgzzECdFNXu2KKJBhFWG4DKauzFryjoKsBNuCBD+hZZkrzZCYh4UWPoa4t
         DPc78RxpnAdm7hhKUaOUOACasq9YL0ST1La/kKPgm6SRKSEisGKFl6YJFn17HQc86B2E
         YSDE6048EGNSk5BgGR5DBGfhuiwHbmuM3thmotU5kfhGz27mKdcJQ+zD6jESU69j+TFK
         jW4A==
MIME-Version: 1.0
X-Received: by 10.202.1.200 with SMTP id 191mr4884478oib.82.1425517633555;
 Wed, 04 Mar 2015 17:07:13 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Wed, 4 Mar 2015 17:07:13 -0800 (PST)
In-Reply-To: <D11CDCCB.1F5C5%mkim@palantir.com>
References: <D11CDCCB.1F5C5%mkim@palantir.com>
Date: Wed, 4 Mar 2015 17:07:13 -0800
Message-ID: <CABPQxssNDwd4P2YvqFf4sHL9pEy8zzhwUnSdg-hvEzwYK+ZRaQ@mail.gmail.com>
Subject: Re: Task result is serialized twice by serializer and closure serializer
From: Patrick Wendell <pwendell@gmail.com>
To: Mingyu Kim <mkim@palantir.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Matt Cheah <mcheah@palantir.com>, 
	Eric Lin <elin@palantir.com>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Mingyu,

I think it's broken out separately so we can record the time taken to
serialize the result. Once we serializing it once, the second
serialization should be really simple since it's just wrapping
something that has already been turned into a byte buffer. Do you see
a specific issue with serializing it twice?

I think you need to have two steps if you want to record the time
taken to serialize the result, since that needs to be sent back to the
driver when the task completes.

- Patrick

On Wed, Mar 4, 2015 at 4:01 PM, Mingyu Kim <mkim@palantir.com> wrote:
> Hi all,
>
> It looks like the result of task is serialized twice, once by serializer =
(I.e. Java/Kryo depending on configuration) and once again by closure seria=
lizer (I.e. Java). To link the actual code,
>
> The first one: https://github.com/apache/spark/blob/master/core/src/main/=
scala/org/apache/spark/executor/Executor.scala#L213
> The second one: https://github.com/apache/spark/blob/master/core/src/main=
/scala/org/apache/spark/executor/Executor.scala#L226
>
> This serializes the "value", which is the result of task run twice, which=
 affects things like collect(), takeSample(), and toLocalIterator(). Would =
it make sense to simply serialize the DirectTaskResult once using the regul=
ar "serializer" (as opposed to closure serializer)? Would it cause problems=
 when the Accumulator values are not Kryo-serializable?
>
> Alternatively, if we can assume that Accumator values are small, we can c=
losure-serialize those, put the serialized byte array in DirectTaskResult w=
ith the raw task result "value", and serialize DirectTaskResult.
>
> What do people think?
>
> Thanks,
> Mingyu

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 01:11:53 2015
Return-Path: <dev-return-11851-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2F50F17629
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 01:11:53 +0000 (UTC)
Received: (qmail 78628 invoked by uid 500); 5 Mar 2015 01:11:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78551 invoked by uid 500); 5 Mar 2015 01:11:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78539 invoked by uid 99); 5 Mar 2015 01:11:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:11:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:11:03 +0000
Received: by igal13 with SMTP id l13so42015499iga.0
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 17:11:01 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type
         :content-transfer-encoding;
        bh=I0VM2clEY5MNMBIjrQQL2MuXkXXOJvn92Uzy2BzoA48=;
        b=0/fj+lC4BeZAWMtxsqR9v9lC/YZqBlz1aBIZKe3IuG0S1hb0vgoFctTsa6rhVA26fl
         n8TtWXD76sbZs6wTMgRF16gmzh+vNZoAo+VsgpObUPuekSaMmQiyDz6VQGqgWQbTqWZ0
         b3CYVfgKrnNam9czbmQ5FVYK/NnXQS2IdE7AKrolwLd2aGeXMszRkNBZ4zhNcF2hclr1
         AmnZ1NmOx8riXs/YV3Uz972hdyKYKZMwf7NOhAaG5Nbc9SU82WZtvbmLg4uvopnwuCw+
         cQhPLlSqMnS2HwEekwvR+RVL7jfhyke95IRl045NTGDniVBFZI5Zg9e8LCfuPDFSoONS
         Be8A==
MIME-Version: 1.0
X-Received: by 10.42.167.8 with SMTP id q8mr736312icy.94.1425517861378; Wed,
 04 Mar 2015 17:11:01 -0800 (PST)
Received: by 10.36.99.76 with HTTP; Wed, 4 Mar 2015 17:11:01 -0800 (PST)
Date: Wed, 4 Mar 2015 17:11:01 -0800
Message-ID: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
Subject: enum-like types in Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

There are many places where we use enum-like types in Spark, but in
different ways. Every approach has both pros and cons. I wonder
whether there should be an =E2=80=9Cofficial=E2=80=9D approach for enum-lik=
e types in
Spark.

1. Scala=E2=80=99s Enumeration (e.g., SchedulingMode, WorkerState, etc)

* All types show up as Enumeration.Value in Java.
http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/Sch=
edulingMode.html

2. Java=E2=80=99s Enum (e.g., SaveMode, IOMode)

* Implementation must be in a Java file.
* Values doesn=E2=80=99t show up in the ScalaDoc:
http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.uti=
l.IOMode

3. Static fields in Java (e.g., TripletFields)

* Implementation must be in a Java file.
* Doesn=E2=80=99t need =E2=80=9C()=E2=80=9D in Java code.
* Values don't show up in the ScalaDoc:
http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.Trip=
letFields

4. Objects in Scala. (e.g., StorageLevel)

* Needs =E2=80=9C()=E2=80=9D in Java code.
* Values show up in both ScalaDoc and JavaDoc:
  http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.S=
torageLevel$
  http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/Sto=
rageLevel.html

It would be great if we have an =E2=80=9Cofficial=E2=80=9D approach for thi=
s as well
as the naming convention for enum-like values (=E2=80=9CMEMORY_ONLY=E2=80=
=9D or
=E2=80=9CMemoryOnly=E2=80=9D). Personally, I like 4) with =E2=80=9CMEMORY_O=
NLY=E2=80=9D. Any thoughts?

Best,
Xiangrui

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 01:15:24 2015
Return-Path: <dev-return-11852-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 248211763B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 01:15:24 +0000 (UTC)
Received: (qmail 85500 invoked by uid 500); 5 Mar 2015 01:15:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85418 invoked by uid 500); 5 Mar 2015 01:15:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85406 invoked by uid 99); 5 Mar 2015 01:15:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:15:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.223.172 as permitted sender)
Received: from [209.85.223.172] (HELO mail-ie0-f172.google.com) (209.85.223.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:15:08 +0000
Received: by iecvy18 with SMTP id vy18so8331159iec.1
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 17:14:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=jS4VFPWMnCm1w9pxEqSrYz1bwdw0+A9VPU2BRSUa4rU=;
        b=O9un3emWqB7rm/kZrc6XIPFPTsh5PHGw1OfwiXHcsEZPhTdUQpaOJb2lBC/pEkC5hq
         XyTlrnW2frjlWE54oT39FQADpiXZMmeo8ZI9NDY/ZVFoIm9WXTxNX0gWv59ZPXX2Eg0C
         d7AoLY7u5/uQpxZ2B0UB7WnNOSXr8IMe5CSR4pa9fgEATIAlecaXgXMaIjV1IasQShYn
         XNtbkq3cT4GzHS1MKdIU3PHSJv1xxNxD0XQQZYl1V1oneeevQu+FdFvFgtUZpZaJ2WbG
         yhKo4ETTMtnRqGH/Yo9MZPSI/3LiLgdTVDXooAZL9ACUQTSYJdi5kxuAMGJGBnq7RyW8
         rUAg==
MIME-Version: 1.0
X-Received: by 10.107.13.144 with SMTP id 138mr16594996ion.24.1425518043322;
 Wed, 04 Mar 2015 17:14:03 -0800 (PST)
Received: by 10.107.155.143 with HTTP; Wed, 4 Mar 2015 17:14:03 -0800 (PST)
In-Reply-To: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
Date: Wed, 4 Mar 2015 17:14:03 -0800
Message-ID: <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Stephen Boesch <javadba@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1140a1469846870510804811
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1140a1469846870510804811
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

#4 but with MemoryOnly (more scala-like)

http://docs.scala-lang.org/style/naming-conventions.html

Constants, Values, Variable and Methods

Constant names should be in upper camel case. That is, if the member is
final, immutable and it belongs to a package object or an object, it may be
considered a constant (similar to Java=E2=80=99sstatic final members):


   1. object Container {
   2.     val MyConstant =3D ...
   3. }


2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:

> Hi all,
>
> There are many places where we use enum-like types in Spark, but in
> different ways. Every approach has both pros and cons. I wonder
> whether there should be an =E2=80=9Cofficial=E2=80=9D approach for enum-l=
ike types in
> Spark.
>
> 1. Scala=E2=80=99s Enumeration (e.g., SchedulingMode, WorkerState, etc)
>
> * All types show up as Enumeration.Value in Java.
>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/S=
chedulingMode.html
>
> 2. Java=E2=80=99s Enum (e.g., SaveMode, IOMode)
>
> * Implementation must be in a Java file.
> * Values doesn=E2=80=99t show up in the ScalaDoc:
>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.u=
til.IOMode
>
> 3. Static fields in Java (e.g., TripletFields)
>
> * Implementation must be in a Java file.
> * Doesn=E2=80=99t need =E2=80=9C()=E2=80=9D in Java code.
> * Values don't show up in the ScalaDoc:
>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.Tr=
ipletFields
>
> 4. Objects in Scala. (e.g., StorageLevel)
>
> * Needs =E2=80=9C()=E2=80=9D in Java code.
> * Values show up in both ScalaDoc and JavaDoc:
>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.S=
torageLevel$
>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/Sto=
rageLevel.html
>
> It would be great if we have an =E2=80=9Cofficial=E2=80=9D approach for t=
his as well
> as the naming convention for enum-like values (=E2=80=9CMEMORY_ONLY=E2=80=
=9D or
> =E2=80=9CMemoryOnly=E2=80=9D). Personally, I like 4) with =E2=80=9CMEMORY=
_ONLY=E2=80=9D. Any thoughts?
>
> Best,
> Xiangrui
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1140a1469846870510804811--

From dev-return-11853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 01:30:50 2015
Return-Path: <dev-return-11853-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BAF85176B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 01:30:50 +0000 (UTC)
Received: (qmail 13311 invoked by uid 500); 5 Mar 2015 01:30:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13224 invoked by uid 500); 5 Mar 2015 01:30:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13211 invoked by uid 99); 5 Mar 2015 01:30:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:30:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:30:32 +0000
Received: by iecrp18 with SMTP id rp18so4631265iec.10
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 17:29:51 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=L4CNhV3Wp6HG5jBk0+ZSfoNTI8YgdhiQiNA7aHm3pOA=;
        b=jB/Y0kyZlfB5jkKWQXnsLE+ewlWO7wocgBKk5d7sIpDAViKABUlhmmyvwEKeeWXTrZ
         W/wU47ODB2k4zbkl5VLhUbNqucMgOf+JTGrPIrL/GoD0m3q7IvhTTNIhOyC+nNCTOjrz
         /K+3Boj4A2CeKkJG4lbLF2vm1EmSgD/5gVPOJDUF3pQit0xh8szp9rrqXerLoNBDbjDU
         ZrkA2vdRjtyuAP96zgDJQ2dc8BD1AwTf6DNCVf9jYrYxoZz04+0P4wCot0t9EKaCGbWl
         ULlfqI1G/1n61aFTL7Tuitwt4hm76k6V/PaRpa0iapYT9HyRPtwsD8MPXhRXqa3uspxy
         a2xQ==
X-Gm-Message-State: ALoCoQkgryC0Sg4SSVHbyvAnqAw6Xm1NVsJe4r5JwDmHvwPrSs3wZ4EITz7WWOJYVz6tOUYsCF1b
MIME-Version: 1.0
X-Received: by 10.50.87.42 with SMTP id u10mr9439920igz.31.1425518991169; Wed,
 04 Mar 2015 17:29:51 -0800 (PST)
Received: by 10.36.118.18 with HTTP; Wed, 4 Mar 2015 17:29:51 -0800 (PST)
In-Reply-To: <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
Date: Wed, 4 Mar 2015 17:29:51 -0800
Message-ID: <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Joseph Bradley <joseph@databricks.com>
To: Stephen Boesch <javadba@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfeab1c1708f105108081e7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfeab1c1708f105108081e7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

another vote for #4
People are already used to adding "()" in Java.


On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com> wrote:

> #4 but with MemoryOnly (more scala-like)
>
> http://docs.scala-lang.org/style/naming-conventions.html
>
> Constants, Values, Variable and Methods
>
> Constant names should be in upper camel case. That is, if the member is
> final, immutable and it belongs to a package object or an object, it may =
be
> considered a constant (similar to Java=E2=80=99sstatic final members):
>
>
>    1. object Container {
>    2.     val MyConstant =3D ...
>    3. }
>
>
> 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>
> > Hi all,
> >
> > There are many places where we use enum-like types in Spark, but in
> > different ways. Every approach has both pros and cons. I wonder
> > whether there should be an =E2=80=9Cofficial=E2=80=9D approach for enum=
-like types in
> > Spark.
> >
> > 1. Scala=E2=80=99s Enumeration (e.g., SchedulingMode, WorkerState, etc)
> >
> > * All types show up as Enumeration.Value in Java.
> >
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/S=
chedulingMode.html
> >
> > 2. Java=E2=80=99s Enum (e.g., SaveMode, IOMode)
> >
> > * Implementation must be in a Java file.
> > * Values doesn=E2=80=99t show up in the ScalaDoc:
> >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.u=
til.IOMode
> >
> > 3. Static fields in Java (e.g., TripletFields)
> >
> > * Implementation must be in a Java file.
> > * Doesn=E2=80=99t need =E2=80=9C()=E2=80=9D in Java code.
> > * Values don't show up in the ScalaDoc:
> >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.Tr=
ipletFields
> >
> > 4. Objects in Scala. (e.g., StorageLevel)
> >
> > * Needs =E2=80=9C()=E2=80=9D in Java code.
> > * Values show up in both ScalaDoc and JavaDoc:
> >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.S=
torageLevel$
> >
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/Sto=
rageLevel.html
> >
> > It would be great if we have an =E2=80=9Cofficial=E2=80=9D approach for=
 this as well
> > as the naming convention for enum-like values (=E2=80=9CMEMORY_ONLY=E2=
=80=9D or
> > =E2=80=9CMemoryOnly=E2=80=9D). Personally, I like 4) with =E2=80=9CMEMO=
RY_ONLY=E2=80=9D. Any thoughts?
> >
> > Best,
> > Xiangrui
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--047d7bfeab1c1708f105108081e7--

From dev-return-11854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 01:39:21 2015
Return-Path: <dev-return-11854-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 70691176E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 01:39:21 +0000 (UTC)
Received: (qmail 31184 invoked by uid 500); 5 Mar 2015 01:39:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31106 invoked by uid 500); 5 Mar 2015 01:39:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31095 invoked by uid 99); 5 Mar 2015 01:39:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:39:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:39:10 +0000
Received: by mail-qa0-f48.google.com with SMTP id dc16so36775957qab.7
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 17:37:44 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=gGsJNP63qPRg+AqY6LM/GQovL1aLN7mww6lBaCRFpE4=;
        b=O/0LyKPUUhZlK7y5IS1MbIZ41kfKizKGEc4Ko5uG1BhW87KtxjVy28LOETAZsfE0bZ
         FYXr37v5qtKp+7V95NYIidtMP3yFN/d8A0vcdAGz84SVcra4TfguyONJYupkHHOIfRMz
         GTt+SnCnp/Mmu8T36B+Lj7ZhghFKm7Ry+4aEpZuKddMlB3fzNCHy+9nrU1+Q0eQyKd52
         AVwcJQMY+f1UD2T0IVpMuUygJDUcTtqe0O297XmDm5i77HNQNOnaIzs+KPLV2HczJj3W
         mmKxO0sME+F5OD0lt7nJbVzjZX9HixPc3BSyWbGPWVv61I5vxkMUGOkDrFhyerTmuGIo
         cL0g==
X-Gm-Message-State: ALoCoQmXp/IKChyzKUR6xd+GSavnKUilLc0xQlgZDVZvR7TPCMcIfsTpDYmba9cQJVZRX10Yuk00
X-Received: by 10.140.38.100 with SMTP id s91mr9399415qgs.37.1425519464660;
 Wed, 04 Mar 2015 17:37:44 -0800 (PST)
MIME-Version: 1.0
Received: by 10.140.20.14 with HTTP; Wed, 4 Mar 2015 17:37:24 -0800 (PST)
In-Reply-To: <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com> <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Wed, 4 Mar 2015 17:37:24 -0800
Message-ID: <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Joseph Bradley <joseph@databricks.com>
Cc: Stephen Boesch <javadba@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c133344fe8db0510809dc6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c133344fe8db0510809dc6
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

#4 with a preference for CamelCaseEnums

On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
wrote:

> another vote for #4
> People are already used to adding "()" in Java.
>
>
> On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
> > #4 but with MemoryOnly (more scala-like)
> >
> > http://docs.scala-lang.org/style/naming-conventions.html
> >
> > Constants, Values, Variable and Methods
> >
> > Constant names should be in upper camel case. That is, if the member is
> > final, immutable and it belongs to a package object or an object, it ma=
y
> be
> > considered a constant (similar to Java=E2=80=99sstatic final members):
> >
> >
> >    1. object Container {
> >    2.     val MyConstant =3D ...
> >    3. }
> >
> >
> > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> >
> > > Hi all,
> > >
> > > There are many places where we use enum-like types in Spark, but in
> > > different ways. Every approach has both pros and cons. I wonder
> > > whether there should be an =E2=80=9Cofficial=E2=80=9D approach for en=
um-like types in
> > > Spark.
> > >
> > > 1. Scala=E2=80=99s Enumeration (e.g., SchedulingMode, WorkerState, et=
c)
> > >
> > > * All types show up as Enumeration.Value in Java.
> > >
> > >
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/S=
chedulingMode.html
> > >
> > > 2. Java=E2=80=99s Enum (e.g., SaveMode, IOMode)
> > >
> > > * Implementation must be in a Java file.
> > > * Values doesn=E2=80=99t show up in the ScalaDoc:
> > >
> > >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.u=
til.IOMode
> > >
> > > 3. Static fields in Java (e.g., TripletFields)
> > >
> > > * Implementation must be in a Java file.
> > > * Doesn=E2=80=99t need =E2=80=9C()=E2=80=9D in Java code.
> > > * Values don't show up in the ScalaDoc:
> > >
> > >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.Tr=
ipletFields
> > >
> > > 4. Objects in Scala. (e.g., StorageLevel)
> > >
> > > * Needs =E2=80=9C()=E2=80=9D in Java code.
> > > * Values show up in both ScalaDoc and JavaDoc:
> > >
> > >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.S=
torageLevel$
> > >
> > >
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/Sto=
rageLevel.html
> > >
> > > It would be great if we have an =E2=80=9Cofficial=E2=80=9D approach f=
or this as well
> > > as the naming convention for enum-like values (=E2=80=9CMEMORY_ONLY=
=E2=80=9D or
> > > =E2=80=9CMemoryOnly=E2=80=9D). Personally, I like 4) with =E2=80=9CME=
MORY_ONLY=E2=80=9D. Any thoughts?
> > >
> > > Best,
> > > Xiangrui
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> > >
> >
>

--001a11c133344fe8db0510809dc6--

From dev-return-11855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 01:47:47 2015
Return-Path: <dev-return-11855-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C60FB1771A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 01:47:47 +0000 (UTC)
Received: (qmail 48683 invoked by uid 500); 5 Mar 2015 01:47:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48601 invoked by uid 500); 5 Mar 2015 01:47:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48585 invoked by uid 99); 5 Mar 2015 01:47:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:47:46 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 01:47:21 +0000
Received: from EXDR01-WEST.YOJOE.local (10.160.10.135) by
 ex02-west.YOJOE.local (10.160.10.131) with Microsoft SMTP Server (TLS) id
 14.3.195.1; Wed, 4 Mar 2015 17:47:19 -0800
Received: from EX02-WEST.YOJOE.local ([169.254.1.145]) by
 EXDR01-WEST.YOJOE.local ([169.254.3.4]) with mapi id 14.03.0195.001; Wed, 4
 Mar 2015 17:47:18 -0800
From: Mingyu Kim <mkim@palantir.com>
To: Patrick Wendell <pwendell@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, Matt Cheah
	<mcheah@palantir.com>, Eric Lin <elin@palantir.com>
Subject: Re: Task result is serialized twice by serializer and closure
 serializer
Thread-Topic: Task result is serialized twice by serializer and closure
 serializer
Thread-Index: AQHQVteA9qc4PgIBuE+tJrqAQ1mKPp0NmdqA//+FE4A=
Date: Thu, 5 Mar 2015 01:47:17 +0000
Message-ID: <D11CF28A.1F603%mkim@palantir.com>
References: <D11CDCCB.1F5C5%mkim@palantir.com>
 <CABPQxssNDwd4P2YvqFf4sHL9pEy8zzhwUnSdg-hvEzwYK+ZRaQ@mail.gmail.com>
In-Reply-To: <CABPQxssNDwd4P2YvqFf4sHL9pEy8zzhwUnSdg-hvEzwYK+ZRaQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.100.91.90]
Content-Type: text/plain; charset="iso-8859-1"
Content-ID: <3E55439062C19E4E8C9696E7D0A5F241@palantir.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

The concern is really just the runtime overhead and memory footprint of
Java-serializing an already-serialized byte array again. We originally
noticed this when we were using RDD.toLocalIterator() which serializes the
entire 64MB partition. We worked around this issue by kryo-serializing and
snappy-compressing the partition on the executor side before returning it
back to the driver, but this operation just felt redundant.

Your explanation about reporting the time taken makes it clearer why it=B9s
designed this way. Since the byte array for the serialized task result
shouldn=B9t account for the majority of memory footprint anyways, I=B9m oka=
y
with leaving it as is, then.

Thanks,
Mingyu





On 3/4/15, 5:07 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:

>Hey Mingyu,
>
>I think it's broken out separately so we can record the time taken to
>serialize the result. Once we serializing it once, the second
>serialization should be really simple since it's just wrapping
>something that has already been turned into a byte buffer. Do you see
>a specific issue with serializing it twice?
>
>I think you need to have two steps if you want to record the time
>taken to serialize the result, since that needs to be sent back to the
>driver when the task completes.
>
>- Patrick
>
>On Wed, Mar 4, 2015 at 4:01 PM, Mingyu Kim <mkim@palantir.com> wrote:
>> Hi all,
>>
>> It looks like the result of task is serialized twice, once by
>>serializer (I.e. Java/Kryo depending on configuration) and once again by
>>closure serializer (I.e. Java). To link the actual code,
>>
>> The first one:=20
>>https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__github.com_apache_=
sp
>>ark_blob_master_core_src_main_scala_org_apache_spark_executor_Executor.sc
>>ala-23L213&d=3DAwIFAw&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=
=3DennQJ
>>q47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3Ddw_fNvxBZ1DixNDGBTXRZBKn36QFyH=
-9
>>WMY_2Z07ulA&s=3DcSKekTNmnB0g54h6-FaF-zOL46UZC_1_LdKK3p9Q0aA&e=3D
>> The second one:=20
>>https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__github.com_apache_=
sp
>>ark_blob_master_core_src_main_scala_org_apache_spark_executor_Executor.sc
>>ala-23L226&d=3DAwIFAw&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=
=3DennQJ
>>q47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3Ddw_fNvxBZ1DixNDGBTXRZBKn36QFyH=
-9
>>WMY_2Z07ulA&s=3DPFoz0HyINd2XuiqkHPgyMsOh9eFkCwXOdl9zdxfBwxM&e=3D
>>
>> This serializes the "value", which is the result of task run twice,
>>which affects things like collect(), takeSample(), and
>>toLocalIterator(). Would it make sense to simply serialize the
>>DirectTaskResult once using the regular "serializer" (as opposed to
>>closure serializer)? Would it cause problems when the Accumulator values
>>are not Kryo-serializable?
>>
>> Alternatively, if we can assume that Accumator values are small, we can
>>closure-serialize those, put the serialized byte array in
>>DirectTaskResult with the raw task result "value", and serialize
>>DirectTaskResult.
>>
>> What do people think?
>>
>> Thanks,
>> Mingyu


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 02:09:29 2015
Return-Path: <dev-return-11856-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4733177E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 02:09:29 +0000 (UTC)
Received: (qmail 449 invoked by uid 500); 5 Mar 2015 02:09:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 367 invoked by uid 500); 5 Mar 2015 02:09:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 349 invoked by uid 99); 5 Mar 2015 02:09:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 02:09:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 02:08:53 +0000
Received: by wghn12 with SMTP id n12so10284222wgh.3
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 18:07:22 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=wx0qOaMiNN3gnYBq7rxnT3BdzYCpuVrNHhHKCETZlJc=;
        b=hwtbXAecdtig799NH0iy8ORPuB7c4HHnDitZYLlZbV1VBrxmVDDT4q/YghHH0otzuy
         Mfohm63i5PH+Nxd4TxIjVPjavu8iCsi0Rcq5bCwdwn5pdrwds0HOc5Jc6othHbCo/uXC
         X3Ogc0PsAlJhUW4j3DGwy/cN8FNYgKbZVQEYofHhuRdZ07xB+Va9bZv0qV4h6VYDJ2v4
         U76KoIlXmoKk8L0+lomuYXuPrU9B8AnjTJPhEK1EznspsdaXb+uTDziUiVyQM31wzs0n
         fTgzD03KvhgeOjDFttwufjpHQGlFoTNEo9KoXTBqzWEFqLHPRgOaTqA/GKBUMguzu34A
         m2JA==
X-Received: by 10.180.77.48 with SMTP id p16mr59824901wiw.89.1425521242550;
 Wed, 04 Mar 2015 18:07:22 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Wed, 4 Mar 2015 18:07:02 -0800 (PST)
In-Reply-To: <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com> <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Wed, 4 Mar 2015 18:07:02 -0800
Message-ID: <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Michael Armbrust <michael@databricks.com>
Cc: Joseph Bradley <joseph@databricks.com>, Stephen Boesch <javadba@gmail.com>, 
	Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043c7ef24836ad05108107ed
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c7ef24836ad05108107ed
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm cool with #4 as well, but make sure we dictate that the values should
be defined within an object with the same name as the enumeration (like we
do for StorageLevel). Otherwise we may pollute a higher namespace.

e.g. we SHOULD do:

trait StorageLevel
object StorageLevel {
  case object MemoryOnly extends StorageLevel
  case object DiskOnly extends StorageLevel
}

On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <michael@databricks.com>
wrote:

> #4 with a preference for CamelCaseEnums
>
> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
> wrote:
>
> > another vote for #4
> > People are already used to adding "()" in Java.
> >
> >
> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
> wrote:
> >
> > > #4 but with MemoryOnly (more scala-like)
> > >
> > > http://docs.scala-lang.org/style/naming-conventions.html
> > >
> > > Constants, Values, Variable and Methods
> > >
> > > Constant names should be in upper camel case. That is, if the member =
is
> > > final, immutable and it belongs to a package object or an object, it
> may
> > be
> > > considered a constant (similar to Java=E2=80=99sstatic final members)=
:
> > >
> > >
> > >    1. object Container {
> > >    2.     val MyConstant =3D ...
> > >    3. }
> > >
> > >
> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> > >
> > > > Hi all,
> > > >
> > > > There are many places where we use enum-like types in Spark, but in
> > > > different ways. Every approach has both pros and cons. I wonder
> > > > whether there should be an =E2=80=9Cofficial=E2=80=9D approach for =
enum-like types in
> > > > Spark.
> > > >
> > > > 1. Scala=E2=80=99s Enumeration (e.g., SchedulingMode, WorkerState, =
etc)
> > > >
> > > > * All types show up as Enumeration.Value in Java.
> > > >
> > > >
> > >
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/S=
chedulingMode.html
> > > >
> > > > 2. Java=E2=80=99s Enum (e.g., SaveMode, IOMode)
> > > >
> > > > * Implementation must be in a Java file.
> > > > * Values doesn=E2=80=99t show up in the ScalaDoc:
> > > >
> > > >
> > >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.u=
til.IOMode
> > > >
> > > > 3. Static fields in Java (e.g., TripletFields)
> > > >
> > > > * Implementation must be in a Java file.
> > > > * Doesn=E2=80=99t need =E2=80=9C()=E2=80=9D in Java code.
> > > > * Values don't show up in the ScalaDoc:
> > > >
> > > >
> > >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.Tr=
ipletFields
> > > >
> > > > 4. Objects in Scala. (e.g., StorageLevel)
> > > >
> > > > * Needs =E2=80=9C()=E2=80=9D in Java code.
> > > > * Values show up in both ScalaDoc and JavaDoc:
> > > >
> > > >
> > >
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.S=
torageLevel$
> > > >
> > > >
> > >
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/Sto=
rageLevel.html
> > > >
> > > > It would be great if we have an =E2=80=9Cofficial=E2=80=9D approach=
 for this as well
> > > > as the naming convention for enum-like values (=E2=80=9CMEMORY_ONLY=
=E2=80=9D or
> > > > =E2=80=9CMemoryOnly=E2=80=9D). Personally, I like 4) with =E2=80=9C=
MEMORY_ONLY=E2=80=9D. Any
> thoughts?
> > > >
> > > > Best,
> > > > Xiangrui
> > > >
> > > > -------------------------------------------------------------------=
--
> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > > For additional commands, e-mail: dev-help@spark.apache.org
> > > >
> > > >
> > >
> >
>

--f46d043c7ef24836ad05108107ed--

From dev-return-11857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 04:05:56 2015
Return-Path: <dev-return-11857-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E8AF17A82
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 04:05:56 +0000 (UTC)
Received: (qmail 77517 invoked by uid 500); 5 Mar 2015 04:05:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77434 invoked by uid 500); 5 Mar 2015 04:05:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77416 invoked by uid 99); 5 Mar 2015 04:05:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:05:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:05:30 +0000
Received: by oiax69 with SMTP id x69so9638472oia.5
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 20:05:28 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=pNDcGLD9VR51etSfLiKg0QFv0qe+TCHcKekG4VFeXqY=;
        b=vdCJmt8tnzhHdghnZplafMvEkmkBrB6YYXjBLlZeeS5qFLka0JekEe+KSry1QcXoH3
         Ycqg0x+9DM8A4p+r2YQd4mNdX9UMpf31C4BIbnaCBZqd5mKhYbzt7EJCCnGCvqzsrGbc
         nyABSKgKgTJIDBG4uvA/EWrxBGSK+vR8nHbsMaG5eF2GjR6qCan/w4befFk4Br9Li2dC
         PqPux9ZDlEdgDFAdts2Az7RJCzkPt17kLDL/ZcS911WDr0nbx2ngsZ9bx+MmhPiMFPun
         Vr15O/mjK0ackhrGHB0huCDPyIhvnuNri/AKXCDPTTUAG2paqKSMJWpWI7hy5HWU9BXw
         hsUA==
MIME-Version: 1.0
X-Received: by 10.182.215.163 with SMTP id oj3mr5592776obc.49.1425528328540;
 Wed, 04 Mar 2015 20:05:28 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Wed, 4 Mar 2015 20:05:28 -0800 (PST)
In-Reply-To: <D11CF28A.1F603%mkim@palantir.com>
References: <D11CDCCB.1F5C5%mkim@palantir.com>
	<CABPQxssNDwd4P2YvqFf4sHL9pEy8zzhwUnSdg-hvEzwYK+ZRaQ@mail.gmail.com>
	<D11CF28A.1F603%mkim@palantir.com>
Date: Wed, 4 Mar 2015 20:05:28 -0800
Message-ID: <CABPQxss=xBFkPn0bTzNZf2v-wnrmPnfu17QnsWq_w0kmEe-dyg@mail.gmail.com>
Subject: Re: Task result is serialized twice by serializer and closure serializer
From: Patrick Wendell <pwendell@gmail.com>
To: Mingyu Kim <mkim@palantir.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Matt Cheah <mcheah@palantir.com>, 
	Eric Lin <elin@palantir.com>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, it will result in a second serialized copy of the array (costing
some memory). But the computational overhead should be very small. The
absolute worst case here will be when doing a collect() or something
similar that just bundles the entire partition.

- Patrick

On Wed, Mar 4, 2015 at 5:47 PM, Mingyu Kim <mkim@palantir.com> wrote:
> The concern is really just the runtime overhead and memory footprint of
> Java-serializing an already-serialized byte array again. We originally
> noticed this when we were using RDD.toLocalIterator() which serializes th=
e
> entire 64MB partition. We worked around this issue by kryo-serializing an=
d
> snappy-compressing the partition on the executor side before returning it
> back to the driver, but this operation just felt redundant.
>
> Your explanation about reporting the time taken makes it clearer why it=
=B9s
> designed this way. Since the byte array for the serialized task result
> shouldn=B9t account for the majority of memory footprint anyways, I=B9m o=
kay
> with leaving it as is, then.
>
> Thanks,
> Mingyu
>
>
>
>
>
> On 3/4/15, 5:07 PM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>
>>Hey Mingyu,
>>
>>I think it's broken out separately so we can record the time taken to
>>serialize the result. Once we serializing it once, the second
>>serialization should be really simple since it's just wrapping
>>something that has already been turned into a byte buffer. Do you see
>>a specific issue with serializing it twice?
>>
>>I think you need to have two steps if you want to record the time
>>taken to serialize the result, since that needs to be sent back to the
>>driver when the task completes.
>>
>>- Patrick
>>
>>On Wed, Mar 4, 2015 at 4:01 PM, Mingyu Kim <mkim@palantir.com> wrote:
>>> Hi all,
>>>
>>> It looks like the result of task is serialized twice, once by
>>>serializer (I.e. Java/Kryo depending on configuration) and once again by
>>>closure serializer (I.e. Java). To link the actual code,
>>>
>>> The first one:
>>>https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__github.com_apache=
_sp
>>>ark_blob_master_core_src_main_scala_org_apache_spark_executor_Executor.s=
c
>>>ala-23L213&d=3DAwIFAw&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=
=3DennQJ
>>>q47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3Ddw_fNvxBZ1DixNDGBTXRZBKn36QFy=
H-9
>>>WMY_2Z07ulA&s=3DcSKekTNmnB0g54h6-FaF-zOL46UZC_1_LdKK3p9Q0aA&e=3D
>>> The second one:
>>>https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__github.com_apache=
_sp
>>>ark_blob_master_core_src_main_scala_org_apache_spark_executor_Executor.s=
c
>>>ala-23L226&d=3DAwIFAw&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnmz8&r=
=3DennQJ
>>>q47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3Ddw_fNvxBZ1DixNDGBTXRZBKn36QFy=
H-9
>>>WMY_2Z07ulA&s=3DPFoz0HyINd2XuiqkHPgyMsOh9eFkCwXOdl9zdxfBwxM&e=3D
>>>
>>> This serializes the "value", which is the result of task run twice,
>>>which affects things like collect(), takeSample(), and
>>>toLocalIterator(). Would it make sense to simply serialize the
>>>DirectTaskResult once using the regular "serializer" (as opposed to
>>>closure serializer)? Would it cause problems when the Accumulator values
>>>are not Kryo-serializable?
>>>
>>> Alternatively, if we can assume that Accumator values are small, we can
>>>closure-serialize those, put the serialized byte array in
>>>DirectTaskResult with the raw task result "value", and serialize
>>>DirectTaskResult.
>>>
>>> What do people think?
>>>
>>> Thanks,
>>> Mingyu
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 04:11:27 2015
Return-Path: <dev-return-11858-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9AFE517AA8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 04:11:27 +0000 (UTC)
Received: (qmail 85831 invoked by uid 500); 5 Mar 2015 04:11:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85745 invoked by uid 500); 5 Mar 2015 04:11:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85712 invoked by uid 99); 5 Mar 2015 04:11:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:11:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:11:21 +0000
Received: by oibg201 with SMTP id g201so9661646oib.10
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 20:10:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DZ1HuEWApzkHdthmLIt1bvyPaEgbBDUwM7GDhLdBZ3s=;
        b=A0NBk6BjmgqNLaICwXdMMi+R9uycLqb85A6Qtj6qAlXv5TYLrzkCWc6zlZB/gbZkCX
         WDPir7B9Jui0mY1SIRK7X968ektl8pavA0N8xk50RhIgfhBJ/cnDrLslwLw9RBKod/CQ
         aDEyIjyO5HGrYblqo+15CfVdLzNPFHAIoxjtGzmo3KhYhnDe+QkSiUmuMSCNJTwRP9t9
         UfNf138KsO2qeDuXQVSTJAu/uaJi/q8aHxT+ldXjXscrtcDeGQEPjxiv8BctRzTCLqWH
         BQCaLDhkCorOhO4NnBqHXiaSQ5CabxriZW7TlhAm//P4RE4x8RsNVjgPFogeDg5ErZz3
         +NXQ==
MIME-Version: 1.0
X-Received: by 10.202.1.200 with SMTP id 191mr5274185oib.82.1425528615921;
 Wed, 04 Mar 2015 20:10:15 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Wed, 4 Mar 2015 20:10:15 -0800 (PST)
In-Reply-To: <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
Date: Wed, 4 Mar 2015 20:10:15 -0800
Message-ID: <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Aaron Davidson <ilikerps@gmail.com>
Cc: Michael Armbrust <michael@databricks.com>, Joseph Bradley <joseph@databricks.com>, 
	Stephen Boesch <javadba@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I like #4 as well and agree with Aaron's suggestion.

- Patrick

On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> I'm cool with #4 as well, but make sure we dictate that the values should
> be defined within an object with the same name as the enumeration (like we
> do for StorageLevel). Otherwise we may pollute a higher namespace.
>
> e.g. we SHOULD do:
>
> trait StorageLevel
> object StorageLevel {
>   case object MemoryOnly extends StorageLevel
>   case object DiskOnly extends StorageLevel
> }
>
> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
>> #4 with a preference for CamelCaseEnums
>>
>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
>> wrote:
>>
>> > another vote for #4
>> > People are already used to adding "()" in Java.
>> >
>> >
>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>> wrote:
>> >
>> > > #4 but with MemoryOnly (more scala-like)
>> > >
>> > > http://docs.scala-lang.org/style/naming-conventions.html
>> > >
>> > > Constants, Values, Variable and Methods
>> > >
>> > > Constant names should be in upper camel case. That is, if the member is
>> > > final, immutable and it belongs to a package object or an object, it
>> may
>> > be
>> > > considered a constant (similar to Java'sstatic final members):
>> > >
>> > >
>> > >    1. object Container {
>> > >    2.     val MyConstant = ...
>> > >    3. }
>> > >
>> > >
>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>> > >
>> > > > Hi all,
>> > > >
>> > > > There are many places where we use enum-like types in Spark, but in
>> > > > different ways. Every approach has both pros and cons. I wonder
>> > > > whether there should be an "official" approach for enum-like types in
>> > > > Spark.
>> > > >
>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>> > > >
>> > > > * All types show up as Enumeration.Value in Java.
>> > > >
>> > > >
>> > >
>> >
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>> > > >
>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>> > > >
>> > > > * Implementation must be in a Java file.
>> > > > * Values doesn't show up in the ScalaDoc:
>> > > >
>> > > >
>> > >
>> >
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>> > > >
>> > > > 3. Static fields in Java (e.g., TripletFields)
>> > > >
>> > > > * Implementation must be in a Java file.
>> > > > * Doesn't need "()" in Java code.
>> > > > * Values don't show up in the ScalaDoc:
>> > > >
>> > > >
>> > >
>> >
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>> > > >
>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>> > > >
>> > > > * Needs "()" in Java code.
>> > > > * Values show up in both ScalaDoc and JavaDoc:
>> > > >
>> > > >
>> > >
>> >
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>> > > >
>> > > >
>> > >
>> >
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>> > > >
>> > > > It would be great if we have an "official" approach for this as well
>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>> thoughts?
>> > > >
>> > > > Best,
>> > > > Xiangrui
>> > > >
>> > > > ---------------------------------------------------------------------
>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>> > > >
>> > > >
>> > >
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 04:42:50 2015
Return-Path: <dev-return-11859-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4D4D17B27
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 04:42:50 +0000 (UTC)
Received: (qmail 26349 invoked by uid 500); 5 Mar 2015 04:42:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26268 invoked by uid 500); 5 Mar 2015 04:42:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26256 invoked by uid 99); 5 Mar 2015 04:42:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:42:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of robert.dodier@gmail.com designates 209.85.215.45 as permitted sender)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:42:17 +0000
Received: by lamq1 with SMTP id q1so25898482lam.0
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 20:40:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ht3JKo2VKuOZhWg3r/x0jb3iDa4I9SIR8mtcKvu94U4=;
        b=jvEr32LJ8LyO5jHPynD9MTU2VI0Rn5jQWdU/NmiWT6tjAi+MyIIc+N4Cu/V5cjuOj6
         Q49iwVrCNmlv5eFYO9t8WeucHi504TP61oJEb2xDymXPSG6S4+lI1nbM40PtaqkZBJ/B
         3nWUYnCzXBgfo/rU8g3ogZV5AKo/UYS8udx5HLH748AH58fX37a2QLaZYv/rEXwnZdkw
         VNQ0AM37P2ztqfZLmdRP1/KXE7i3rWyLGA86CkDjBhsxPI3mp8iam61s/VztNE9AJ9ZD
         0Er75+OxebZFJC2sAjJcYi/WtRZTwkvX12T5xPpDZ3nK2BJ13uukUYp8ZDTudXtb+uQo
         +gRQ==
MIME-Version: 1.0
X-Received: by 10.152.115.169 with SMTP id jp9mr6136868lab.121.1425530446102;
 Wed, 04 Mar 2015 20:40:46 -0800 (PST)
Received: by 10.112.239.35 with HTTP; Wed, 4 Mar 2015 20:40:46 -0800 (PST)
In-Reply-To: <CABjXkq65ygjH2da_bSSc7Y3vpgKh1-H-ME938s3Qb1gv22p3Rg@mail.gmail.com>
References: <CAAsY_sTDG9G6i8NsD517bwtzTp2cZbsXvQY0Zmu15-8gxjnT+g@mail.gmail.com>
	<CABjXkq65ygjH2da_bSSc7Y3vpgKh1-H-ME938s3Qb1gv22p3Rg@mail.gmail.com>
Date: Wed, 4 Mar 2015 20:40:46 -0800
Message-ID: <CAAsY_sRFviTXDgiy-ubCjjNaH==aegkYLVfAcDtPrSgFOMPRAQ@mail.gmail.com>
Subject: Re: ideas for MLlib development
From: Robert Dodier <robert.dodier@gmail.com>
To: "Evan R. Sparks" <evan.sparks@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for your reply, Evan.

> It may make sense to have a more general Gibbs sampling
> framework, but it might be good to have a few desired applications
> in mind (e.g. higher level models that rely on Gibbs) to help API
> design, parallelization strategy, etc.

I think I'm more interested in a general framework which could
be applied to a variety of models, as opposed to an implementation
tailored to a specific model such as LDA. I'm thinking that such
a framework could be used in model exploration, either as an
end in itself or perhaps to identify promising models that could
then be given optimized, custom implementations. This would
be very much in the spirit of existing packages such as BUGS.
In fact, if we were to go down this road, I would propose that
models be specified in the BUGS modeling language -- no need
to reinvent that wheel, I would say.

At a very high level, the API for this framework would specify
methods to compute conditional distributions, marginalizing
as necessary via MCMC. Other operations could include
computing the expected value of a variable or function.
All this is very reminiscent of BUGS, of course.

best,

Robert Dodier

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 04:59:23 2015
Return-Path: <dev-return-11860-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5CA917B96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 04:59:23 +0000 (UTC)
Received: (qmail 44961 invoked by uid 500); 5 Mar 2015 04:59:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44888 invoked by uid 500); 5 Mar 2015 04:59:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44875 invoked by uid 99); 5 Mar 2015 04:59:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:59:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of deepujain@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 04:58:56 +0000
Received: by igjz20 with SMTP id z20so42915996igj.4
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 20:57:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=H+c6qhGWo0V4KqByq3xk9u6pFxBf9LXR8/WxrhWi4J8=;
        b=ohWdWSvXQcgh274MiFfJ3FJzwzjkw3vyb74hVugOqQEZ47V6OqZVi26CfSU7p10t+L
         6Totqa13QBbGcAjkYlrCRkzYREfSw8gCdGOm4R7RMXejaPwCadOY5n+7wszx2jfqamCu
         r81EtmBE0yfcwuwgbJoTgEfBXYdv4bvZZ9bmAOFLdVX2WIM7Lq3CrUrYY8KIW5jEePKs
         Ln2qMrnwBQzyl2Ew0m6weGUNxp9SIuaVUfQI/fy79knWNQq5zKZXmys2KpVEO9YjKS9e
         BS8g8v1aHkdPyO3r4Utoumo/UUZDg6kqwN2m3dHjjc7Q4pUZW84bIHl6g0HJV2wXx/Zq
         +Nbg==
X-Received: by 10.107.167.145 with SMTP id q139mr17932760ioe.16.1425531444336;
 Wed, 04 Mar 2015 20:57:24 -0800 (PST)
MIME-Version: 1.0
Received: by 10.36.61.148 with HTTP; Wed, 4 Mar 2015 20:57:04 -0800 (PST)
In-Reply-To: <CAPKgQ9o8ZzWspE_qXhbobM330s7xaskUoti78Gu73Qg3rNER=g@mail.gmail.com>
References: <CAPKgQ9o8ZzWspE_qXhbobM330s7xaskUoti78Gu73Qg3rNER=g@mail.gmail.com>
From: =?UTF-8?B?w5DOnuKCrM+BQNKcICjguY/Mr82h4LmPKQ==?= <deepujain@gmail.com>
Date: Thu, 5 Mar 2015 10:27:04 +0530
Message-ID: <CAPKgQ9p+mo8NfF2CK8=ZVCo4wjekh0VVjAwdTdnihVooMhmTJg@mail.gmail.com>
Subject: Fwd: Unable to Read/Write Avro RDD on cluster.
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a114299905b1f3c0510836774
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114299905b1f3c0510836774
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I am trying to read RDD avro, transform and write.
I am able to run it locally fine but when i run onto cluster, i see issues
with Avro.


export SPARK_HOME=3D/home/dvasthimal/spark/spark-1.0.2-bin-2.4.1
export SPARK_YARN_USER_ENV=3D"CLASSPATH=3D/apache/hadoop/conf"
export HADOOP_CONF_DIR=3D/apache/hadoop/conf
export YARN_CONF_DIR=3D/apache/hadoop/conf
export SPARK_JAR=3D$SPARK_HOME/lib/spark-assembly-1.0.2-hadoop2.4.1.jar
export SPARK_LIBRARY_PATH=3D/apache/hadoop/lib/native
export SPARK_YARN_USER_ENV=3D"CLASSPATH=3D/apache/hadoop/conf"
export SPARK_YARN_USER_ENV=3D"CLASSPATH=3D/apache/hadoop/conf"
export
SPARK_CLASSPATH=3D/apache/hadoop/share/hadoop/common/hadoop-common-2.4.1-co=
mpany-2.jar:/apache/hadoop/lib/hadoop-lzo-0.6.0.jar:/home/dvasthimal/spark/=
avro-mapred-1.7.7-hadoop2.jar:/home/dvasthimal/spark/avro-1.7.7.jar
export SPARK_LIBRARY_PATH=3D"/apache/hadoop/lib/native"
export YARN_CONF_DIR=3D/apache/hadoop/conf/

cd $SPARK_HOME

./bin/spark-submit --master yarn-cluster --jars
/home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar,/home/dvasthimal/spark=
/avro-1.7.7.jar
--num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores
1  --queue hdmi-spark --class com.company.ep.poc.spark.reporting.SparkApp
/home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar
startDate=3D2015-02-16 endDate=3D2015-02-16
epoutputdirectory=3D/user/dvasthimal/epdatasets_small/exptsession
subcommand=3Dsuccessevents
outputdir=3D/user/dvasthimal/epdatasets/successdetail

Spark assembly has been built with Hive, including Datanucleus jars on
classpath
15/03/04 03:20:29 INFO client.ConfiguredRMFailoverProxyProvider: Failing
over to rm2
15/03/04 03:20:30 INFO yarn.Client: Got Cluster metric info from
ApplicationsManager (ASM), number of NodeManagers: 2221
15/03/04 03:20:30 INFO yarn.Client: Queue info ... queueName: hdmi-spark,
queueCurrentCapacity: 0.7162806, queueMaxCapacity: 0.08,
      queueApplicationCount =3D 7, queueChildQueueCount =3D 0
15/03/04 03:20:30 INFO yarn.Client: Max mem capabililty of a single
resource in this cluster 16384
15/03/04 03:20:30 INFO yarn.Client: Preparing Local resources
15/03/04 03:20:30 WARN util.NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable
15/03/04 03:20:30 WARN hdfs.BlockReaderLocal: The short-circuit local reads
feature cannot be used because libhadoop cannot be loaded.


15/03/04 03:20:46 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token
7780745 for dvasthimal on 10.115.206.112:8020
15/03/04 03:20:46 INFO yarn.Client: Uploading
file:/home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar to hdfs://
apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_14=
25075571333_61948/spark_reporting-1.0-SNAPSHOT.jar
15/03/04 03:20:47 INFO yarn.Client: Uploading
file:/home/dvasthimal/spark/spark-1.0.2-bin-2.4.1/lib/spark-assembly-1.0.2-=
hadoop2.4.1.jar
to hdfs://
apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_14=
25075571333_61948/spark-assembly-1.0.2-hadoop2.4.1.jar
15/03/04 03:20:52 INFO yarn.Client: Uploading
file:/home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar to hdfs://
apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_14=
25075571333_61948/avro-mapred-1.7.7-hadoop2.jar
15/03/04 03:20:52 INFO yarn.Client: Uploading
file:/home/dvasthimal/spark/avro-1.7.7.jar to hdfs://
apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_14=
25075571333_61948/avro-1.7.7.jar
15/03/04 03:20:54 INFO yarn.Client: Setting up the launch environment
15/03/04 03:20:54 INFO yarn.Client: Setting up container launch context
15/03/04 03:20:54 INFO yarn.Client: Command for starting the Spark
ApplicationMaster: List($JAVA_HOME/bin/java, -server, -Xmx4096m,
-Djava.io.tmpdir=3D$PWD/tmp,
-Dspark.app.name=3D\"com.company.ep.poc.spark.reporting.SparkApp\",
 -Dlog4j.configuration=3Dlog4j-spark-container.properties,
org.apache.spark.deploy.yarn.ApplicationMaster, --class,
com.company.ep.poc.spark.reporting.SparkApp, --jar ,
file:/home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar,  --args
 'startDate=3D2015-02-16'  --args  'endDate=3D2015-02-16'  --args
 'epoutputdirectory=3D/user/dvasthimal/epdatasets_small/exptsession'  --arg=
s
 'subcommand=3Dsuccessevents'  --args
 'outputdir=3D/user/dvasthimal/epdatasets/successdetail' , --executor-memor=
y,
2048, --executor-cores, 1, --num-executors , 3, 1>, <LOG_DIR>/stdout, 2>,
<LOG_DIR>/stderr)
15/03/04 03:20:54 INFO yarn.Client: Submitting application to ASM
15/03/04 03:20:54 INFO impl.YarnClientImpl: Submitted application
application_1425075571333_61948
15/03/04 03:20:56 INFO yarn.Client: Application report from ASM:
 application identifier: application_1425075571333_61948
 appId: 61948
 clientToAMToken: null
 appDiagnostics:
 appMasterHost: N/A
 appQueue: hdmi-spark
 appMasterRpcPort: -1
 appStartTime: 1425464454263
 yarnAppState: ACCEPTED
 distributedFinalState: UNDEFINED
 appTrackingUrl:
https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333_6=
1948/
 appUser: dvasthimal
15/03/04 03:21:18 INFO yarn.Client: Application report from ASM:
 application identifier: application_1425075571333_61948
 appId: 61948
 clientToAMToken: Token { kind: YARN_CLIENT_TOKEN, service:  }
 appDiagnostics:
 appMasterHost: phxaishdc9dn0169.phx.company.com
 appQueue: hdmi-spark
 appMasterRpcPort: 0
 appStartTime: 1425464454263
 yarnAppState: RUNNING
 distributedFinalState: UNDEFINED
 appTrackingUrl:
https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333_6=
1948/
 appUser: dvasthimal
=E2=80=A6.
=E2=80=A6.
15/03/04 03:21:22 INFO yarn.Client: Application report from ASM:
 application identifier: application_1425075571333_61948
 appId: 61948
 clientToAMToken: Token { kind: YARN_CLIENT_TOKEN, service:  }
 appDiagnostics:
 appMasterHost: phxaishdc9dn0169.phx.company.com
 appQueue: hdmi-spark
 appMasterRpcPort: 0
 appStartTime: 1425464454263
 yarnAppState: FINISHED
 distributedFinalState: FAILED
 appTrackingUrl:
https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333_6=
1948/A
 appUser: dvasthimal



AM failed with following exception

/apache/hadoop/bin/yarn logs -applicationId application_1425075571333_61948
15/03/04 03:21:22 INFO NewHadoopRDD: Input split: hdfs://
apollo-phx-nn.company.com:8020/user/dvasthimal/epdatasets_small/exptsession=
/2015/02/16/part-r-00000.avro:0+13890
15/03/04 03:21:22 ERROR Executor: Exception in task ID 3
java.lang.IncompatibleClassChangeError: Found interface
org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
at
org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyInpu=
tFormat.java:47)
at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:111)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
at org.apache.spark.scheduler.Task.run(Task.scala:51)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1=
145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:=
615)
at java.lang.Thread.run(Thread.java:745)



1) Having figured out the error the fix would be to put the right version
of avro libs into AM JVM classpath. Hence i included --jars
/home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar,/home/dvasthimal/spark=
/avro-1.7.7.jar
in spark-submit command. However i still see the same exception.
2) I tried to include these libs in SPARK_CLASSPATH. However i see the same
exception.


--=20
Deepak

--001a114299905b1f3c0510836774--

From dev-return-11861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 06:08:35 2015
Return-Path: <dev-return-11861-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A0CA17D71
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 06:08:35 +0000 (UTC)
Received: (qmail 61455 invoked by uid 500); 5 Mar 2015 06:08:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61374 invoked by uid 500); 5 Mar 2015 06:08:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61362 invoked by uid 99); 5 Mar 2015 06:08:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 06:08:33 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 06:08:29 +0000
Received: from EXDR01-WEST.YOJOE.local (10.160.10.135) by
 EX03-WEST.YOJOE.local (10.160.10.136) with Microsoft SMTP Server (TLS) id
 14.3.195.1; Wed, 4 Mar 2015 22:08:07 -0800
Received: from EX02-WEST.YOJOE.local ([169.254.1.145]) by
 EXDR01-WEST.YOJOE.local ([169.254.3.4]) with mapi id 14.03.0195.001; Wed, 4
 Mar 2015 22:08:07 -0800
From: Mingyu Kim <mkim@palantir.com>
To: Patrick Wendell <pwendell@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, Matt Cheah
	<mcheah@palantir.com>, Eric Lin <elin@palantir.com>
Subject: Re: Task result is serialized twice by serializer and closure
 serializer
Thread-Topic: Task result is serialized twice by serializer and closure
 serializer
Thread-Index: AQHQVteA9qc4PgIBuE+tJrqAQ1mKPp0NmdqA//+FE4CAAKy6AP//nCMA
Date: Thu, 5 Mar 2015 06:08:06 +0000
Message-ID: <D11D32B4.1F64D%mkim@palantir.com>
References: <D11CDCCB.1F5C5%mkim@palantir.com>
 <CABPQxssNDwd4P2YvqFf4sHL9pEy8zzhwUnSdg-hvEzwYK+ZRaQ@mail.gmail.com>
 <D11CF28A.1F603%mkim@palantir.com>
 <CABPQxss=xBFkPn0bTzNZf2v-wnrmPnfu17QnsWq_w0kmEe-dyg@mail.gmail.com>
In-Reply-To: <CABPQxss=xBFkPn0bTzNZf2v-wnrmPnfu17QnsWq_w0kmEe-dyg@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.160.122.85]
Content-Type: text/plain; charset="euc-kr"
Content-ID: <FAAC276A997A864BB9CACDEDCB74A5B0@palantir.com>
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 07:37:56 2015
Return-Path: <dev-return-11862-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7EA2100C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 07:37:56 +0000 (UTC)
Received: (qmail 64233 invoked by uid 500); 5 Mar 2015 07:37:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64149 invoked by uid 500); 5 Mar 2015 07:37:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64134 invoked by uid 99); 5 Mar 2015 07:37:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 07:37:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 07:37:30 +0000
Received: by igdh15 with SMTP id h15so43489913igd.4
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 23:35:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=hwakaieqTEyYhex9SrV3KcvvVi4GkhSJe9rN6KPdoh0=;
        b=dv6IRVs0sIO28uo1T/mg1/YF9MzF1n79cWmTi26YlMolEvChBv5hR/Ze9fsKxUg5gD
         5Qx4RGNMeNe/J6aXsJG35yNIpeev5ibUAwfFgLexhSDNqxaO+nZYoQdxFaysOn4h/WoK
         sfIkQ7rbSclKXo2PA3gArmepqMil+SCq0xfF1ehVYJFvHiljkudnaeGuJB0jsBqvQgEr
         djULCPsl5v21dTJjnXxr2XsFYhLaaAj01N0+o0ln2bMh2oKV4g26C89a03pkGZgp94IQ
         grGLGs7EreqBMq3rIEDQaSgRFdissyelxOg2Db8c0C3GeAQGclftvW9aS9b7H0OH/XQl
         BvPQ==
MIME-Version: 1.0
X-Received: by 10.50.137.99 with SMTP id qh3mr40517069igb.9.1425540913772;
 Wed, 04 Mar 2015 23:35:13 -0800 (PST)
Received: by 10.36.99.76 with HTTP; Wed, 4 Mar 2015 23:35:13 -0800 (PST)
In-Reply-To: <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
Date: Wed, 4 Mar 2015 23:35:13 -0800
Message-ID: <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Aaron Davidson <ilikerps@gmail.com>, Michael Armbrust <michael@databricks.com>, 
	Joseph Bradley <joseph@databricks.com>, Stephen Boesch <javadba@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

`case object` inside an `object` doesn't show up in Java. This is the
minimal code I found to make everything show up correctly in both
Scala and Java:

sealed abstract class StorageLevel // cannot be a trait

object StorageLevel {
  private[this] case object _MemoryOnly extends StorageLevel
  final val MemoryOnly: StorageLevel = _MemoryOnly

  private[this] case object _DiskOnly extends StorageLevel
  final val DiskOnly: StorageLevel = _DiskOnly
}

On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> I like #4 as well and agree with Aaron's suggestion.
>
> - Patrick
>
> On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
>> I'm cool with #4 as well, but make sure we dictate that the values should
>> be defined within an object with the same name as the enumeration (like we
>> do for StorageLevel). Otherwise we may pollute a higher namespace.
>>
>> e.g. we SHOULD do:
>>
>> trait StorageLevel
>> object StorageLevel {
>>   case object MemoryOnly extends StorageLevel
>>   case object DiskOnly extends StorageLevel
>> }
>>
>> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <michael@databricks.com>
>> wrote:
>>
>>> #4 with a preference for CamelCaseEnums
>>>
>>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
>>> wrote:
>>>
>>> > another vote for #4
>>> > People are already used to adding "()" in Java.
>>> >
>>> >
>>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>>> wrote:
>>> >
>>> > > #4 but with MemoryOnly (more scala-like)
>>> > >
>>> > > http://docs.scala-lang.org/style/naming-conventions.html
>>> > >
>>> > > Constants, Values, Variable and Methods
>>> > >
>>> > > Constant names should be in upper camel case. That is, if the member is
>>> > > final, immutable and it belongs to a package object or an object, it
>>> may
>>> > be
>>> > > considered a constant (similar to Java'sstatic final members):
>>> > >
>>> > >
>>> > >    1. object Container {
>>> > >    2.     val MyConstant = ...
>>> > >    3. }
>>> > >
>>> > >
>>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>>> > >
>>> > > > Hi all,
>>> > > >
>>> > > > There are many places where we use enum-like types in Spark, but in
>>> > > > different ways. Every approach has both pros and cons. I wonder
>>> > > > whether there should be an "official" approach for enum-like types in
>>> > > > Spark.
>>> > > >
>>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>>> > > >
>>> > > > * All types show up as Enumeration.Value in Java.
>>> > > >
>>> > > >
>>> > >
>>> >
>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>>> > > >
>>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>>> > > >
>>> > > > * Implementation must be in a Java file.
>>> > > > * Values doesn't show up in the ScalaDoc:
>>> > > >
>>> > > >
>>> > >
>>> >
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>>> > > >
>>> > > > 3. Static fields in Java (e.g., TripletFields)
>>> > > >
>>> > > > * Implementation must be in a Java file.
>>> > > > * Doesn't need "()" in Java code.
>>> > > > * Values don't show up in the ScalaDoc:
>>> > > >
>>> > > >
>>> > >
>>> >
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>>> > > >
>>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>>> > > >
>>> > > > * Needs "()" in Java code.
>>> > > > * Values show up in both ScalaDoc and JavaDoc:
>>> > > >
>>> > > >
>>> > >
>>> >
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>>> > > >
>>> > > >
>>> > >
>>> >
>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>>> > > >
>>> > > > It would be great if we have an "official" approach for this as well
>>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
>>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>>> thoughts?
>>> > > >
>>> > > > Best,
>>> > > > Xiangrui
>>> > > >
>>> > > > ---------------------------------------------------------------------
>>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>>> > > >
>>> > > >
>>> > >
>>> >
>>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 07:46:36 2015
Return-Path: <dev-return-11863-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6BCE1015E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 07:46:36 +0000 (UTC)
Received: (qmail 90075 invoked by uid 500); 5 Mar 2015 07:46:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89994 invoked by uid 500); 5 Mar 2015 07:46:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89982 invoked by uid 99); 5 Mar 2015 07:46:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 07:46:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 07:46:09 +0000
Received: by wghb13 with SMTP id b13so51566137wgh.0
        for <dev@spark.apache.org>; Wed, 04 Mar 2015 23:45:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=K0v4QboFH1CaeS9hehzoBblVbtKmCzzqoQTiddnHPMc=;
        b=O9zyqg0ugvBNsNVsXQv92J+YJq+MA6SGqvpvXvxJ0uCJw3WFehSPdMZWVxE4hKe8QS
         Ldv1zOBObb/zRdA29ooqXoHUmlwGd3jEAHcbEJjZTUc0PK0RJuAcfxWh+8pmm2OWwcT1
         fzxL291lPhQE6cq8GYbKuLjtQ6ManojwLrTp6sWbNKtMEnBrJ+c7MLl+mKrNMxb/BpqH
         cR8XKmwiCjIjIyJ3jy/WzdKpo7pnOFxD7+nAMhQEV6CLz6ean9TZNdHmxz+j6oUD7pg2
         BRdlFgFNqbItc+6b0Gu6noYTLpYdJD8IOedvtscwz76XKFjoo/SlnQszNawVtcaa9wlo
         tYog==
X-Received: by 10.181.12.39 with SMTP id en7mr19863201wid.29.1425541523067;
 Wed, 04 Mar 2015 23:45:23 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Wed, 4 Mar 2015 23:45:01 -0800 (PST)
In-Reply-To: <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com> <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Wed, 4 Mar 2015 23:45:01 -0800
Message-ID: <CANGvG8qiJ4QL1g-pGV+DAhxpqWiAJLxiLzP+ZHZ+PybrHBZkQg@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Michael Armbrust <michael@databricks.com>, 
	Joseph Bradley <joseph@databricks.com>, Stephen Boesch <javadba@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04388e09185765051085c008
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04388e09185765051085c008
Content-Type: text/plain; charset=UTF-8

That's kinda annoying, but it's just a little extra boilerplate. Can you
call it as StorageLevel.DiskOnly() from Java? Would it also work if they
were case classes with empty constructors, without the field?

On Wed, Mar 4, 2015 at 11:35 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> `case object` inside an `object` doesn't show up in Java. This is the
> minimal code I found to make everything show up correctly in both
> Scala and Java:
>
> sealed abstract class StorageLevel // cannot be a trait
>
> object StorageLevel {
>   private[this] case object _MemoryOnly extends StorageLevel
>   final val MemoryOnly: StorageLevel = _MemoryOnly
>
>   private[this] case object _DiskOnly extends StorageLevel
>   final val DiskOnly: StorageLevel = _DiskOnly
> }
>
> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > I like #4 as well and agree with Aaron's suggestion.
> >
> > - Patrick
> >
> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >> I'm cool with #4 as well, but make sure we dictate that the values
> should
> >> be defined within an object with the same name as the enumeration (like
> we
> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
> >>
> >> e.g. we SHOULD do:
> >>
> >> trait StorageLevel
> >> object StorageLevel {
> >>   case object MemoryOnly extends StorageLevel
> >>   case object DiskOnly extends StorageLevel
> >> }
> >>
> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
> michael@databricks.com>
> >> wrote:
> >>
> >>> #4 with a preference for CamelCaseEnums
> >>>
> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
> >>> wrote:
> >>>
> >>> > another vote for #4
> >>> > People are already used to adding "()" in Java.
> >>> >
> >>> >
> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
> >>> wrote:
> >>> >
> >>> > > #4 but with MemoryOnly (more scala-like)
> >>> > >
> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
> >>> > >
> >>> > > Constants, Values, Variable and Methods
> >>> > >
> >>> > > Constant names should be in upper camel case. That is, if the
> member is
> >>> > > final, immutable and it belongs to a package object or an object,
> it
> >>> may
> >>> > be
> >>> > > considered a constant (similar to Java'sstatic final members):
> >>> > >
> >>> > >
> >>> > >    1. object Container {
> >>> > >    2.     val MyConstant = ...
> >>> > >    3. }
> >>> > >
> >>> > >
> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> >>> > >
> >>> > > > Hi all,
> >>> > > >
> >>> > > > There are many places where we use enum-like types in Spark, but
> in
> >>> > > > different ways. Every approach has both pros and cons. I wonder
> >>> > > > whether there should be an "official" approach for enum-like
> types in
> >>> > > > Spark.
> >>> > > >
> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
> >>> > > >
> >>> > > > * All types show up as Enumeration.Value in Java.
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
> >>> > > >
> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
> >>> > > >
> >>> > > > * Implementation must be in a Java file.
> >>> > > > * Values doesn't show up in the ScalaDoc:
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
> >>> > > >
> >>> > > > 3. Static fields in Java (e.g., TripletFields)
> >>> > > >
> >>> > > > * Implementation must be in a Java file.
> >>> > > > * Doesn't need "()" in Java code.
> >>> > > > * Values don't show up in the ScalaDoc:
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
> >>> > > >
> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
> >>> > > >
> >>> > > > * Needs "()" in Java code.
> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
> >>> > > >
> >>> > > > It would be great if we have an "official" approach for this as
> well
> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
> >>> thoughts?
> >>> > > >
> >>> > > > Best,
> >>> > > > Xiangrui
> >>> > > >
> >>> > > >
> ---------------------------------------------------------------------
> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
>

--f46d04388e09185765051085c008--

From dev-return-11864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 08:18:11 2015
Return-Path: <dev-return-11864-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 85F75102C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 08:18:11 +0000 (UTC)
Received: (qmail 74345 invoked by uid 500); 5 Mar 2015 08:18:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74256 invoked by uid 500); 5 Mar 2015 08:18:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74244 invoked by uid 99); 5 Mar 2015 08:18:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:18:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:17:42 +0000
Received: by qcwr17 with SMTP id r17so41804177qcw.1
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 00:16:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=pzE/3W47OmyJyH5+DddqXhVIc9EDqA0mJcQUrkgV97I=;
        b=bxUHecOzA0DglWbjQfmlAclDIQj99ke31ftawzmyJlPjglCszWkIaTblTAOL5B264N
         DLq2jBq+bP3CdwFlQcKR91nBvG50fDyjCzCfWCGPmWwvCQuyHX01uP/ANxHmnMJsmxAg
         54RmdatlrAKDYA/SdZ7MCZlTsHVq+PSET76HJ48DCXGWdsFE8wZJgjWrGg954TkqdIEg
         cvqxl5LHGx6/mw4yrfiG/QaZxPSYy9hn7yBAzuENOGNUEYKnUggQrhsXEVk9qeShhFhl
         KTlQ3Q6jDdT//fP97QFCn7dog5n2d5nax8rJIB7PKGLXyX14L9n29bQrhXZQBWoxHNz/
         FDZg==
MIME-Version: 1.0
X-Received: by 10.140.147.147 with SMTP id 141mr11328962qht.57.1425543370300;
 Thu, 05 Mar 2015 00:16:10 -0800 (PST)
Received: by 10.140.33.131 with HTTP; Thu, 5 Mar 2015 00:16:10 -0800 (PST)
In-Reply-To: <CANGvG8qiJ4QL1g-pGV+DAhxpqWiAJLxiLzP+ZHZ+PybrHBZkQg@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CANGvG8qiJ4QL1g-pGV+DAhxpqWiAJLxiLzP+ZHZ+PybrHBZkQg@mail.gmail.com>
Date: Thu, 5 Mar 2015 00:16:10 -0800
Message-ID: <CAJiQeYLpT5dRf=TVBVXo4HFnB=e43sEECsuAmXVbWoYYwG4rUg@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Mridul Muralidharan <mridul@gmail.com>
To: Aaron Davidson <ilikerps@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, Patrick Wendell <pwendell@gmail.com>, 
	Michael Armbrust <michael@databricks.com>, Joseph Bradley <joseph@databricks.com>, 
	Stephen Boesch <javadba@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

While I dont have any strong opinions about how we handle enum's
either way in spark, I assume the discussion is targetted at (new) api
being designed in spark.
Rewiring what we already have exposed will lead to incompatible api
change (StorageLevel for example, is in 1.0).

Regards,
Mridul

On Wed, Mar 4, 2015 at 11:45 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> That's kinda annoying, but it's just a little extra boilerplate. Can you
> call it as StorageLevel.DiskOnly() from Java? Would it also work if they
> were case classes with empty constructors, without the field?
>
> On Wed, Mar 4, 2015 at 11:35 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> `case object` inside an `object` doesn't show up in Java. This is the
>> minimal code I found to make everything show up correctly in both
>> Scala and Java:
>>
>> sealed abstract class StorageLevel // cannot be a trait
>>
>> object StorageLevel {
>>   private[this] case object _MemoryOnly extends StorageLevel
>>   final val MemoryOnly: StorageLevel = _MemoryOnly
>>
>>   private[this] case object _DiskOnly extends StorageLevel
>>   final val DiskOnly: StorageLevel = _DiskOnly
>> }
>>
>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > I like #4 as well and agree with Aaron's suggestion.
>> >
>> > - Patrick
>> >
>> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>> wrote:
>> >> I'm cool with #4 as well, but make sure we dictate that the values
>> should
>> >> be defined within an object with the same name as the enumeration (like
>> we
>> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
>> >>
>> >> e.g. we SHOULD do:
>> >>
>> >> trait StorageLevel
>> >> object StorageLevel {
>> >>   case object MemoryOnly extends StorageLevel
>> >>   case object DiskOnly extends StorageLevel
>> >> }
>> >>
>> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>> michael@databricks.com>
>> >> wrote:
>> >>
>> >>> #4 with a preference for CamelCaseEnums
>> >>>
>> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
>> >>> wrote:
>> >>>
>> >>> > another vote for #4
>> >>> > People are already used to adding "()" in Java.
>> >>> >
>> >>> >
>> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>> >>> wrote:
>> >>> >
>> >>> > > #4 but with MemoryOnly (more scala-like)
>> >>> > >
>> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
>> >>> > >
>> >>> > > Constants, Values, Variable and Methods
>> >>> > >
>> >>> > > Constant names should be in upper camel case. That is, if the
>> member is
>> >>> > > final, immutable and it belongs to a package object or an object,
>> it
>> >>> may
>> >>> > be
>> >>> > > considered a constant (similar to Java'sstatic final members):
>> >>> > >
>> >>> > >
>> >>> > >    1. object Container {
>> >>> > >    2.     val MyConstant = ...
>> >>> > >    3. }
>> >>> > >
>> >>> > >
>> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>> >>> > >
>> >>> > > > Hi all,
>> >>> > > >
>> >>> > > > There are many places where we use enum-like types in Spark, but
>> in
>> >>> > > > different ways. Every approach has both pros and cons. I wonder
>> >>> > > > whether there should be an "official" approach for enum-like
>> types in
>> >>> > > > Spark.
>> >>> > > >
>> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>> >>> > > >
>> >>> > > > * All types show up as Enumeration.Value in Java.
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>> >>> > > >
>> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>> >>> > > >
>> >>> > > > * Implementation must be in a Java file.
>> >>> > > > * Values doesn't show up in the ScalaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>> >>> > > >
>> >>> > > > 3. Static fields in Java (e.g., TripletFields)
>> >>> > > >
>> >>> > > > * Implementation must be in a Java file.
>> >>> > > > * Doesn't need "()" in Java code.
>> >>> > > > * Values don't show up in the ScalaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>> >>> > > >
>> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>> >>> > > >
>> >>> > > > * Needs "()" in Java code.
>> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>> >>> > > >
>> >>> > > > It would be great if we have an "official" approach for this as
>> well
>> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
>> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>> >>> thoughts?
>> >>> > > >
>> >>> > > > Best,
>> >>> > > > Xiangrui
>> >>> > > >
>> >>> > > >
>> ---------------------------------------------------------------------
>> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 08:19:48 2015
Return-Path: <dev-return-11865-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 26334102E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 08:19:48 +0000 (UTC)
Received: (qmail 78514 invoked by uid 500); 5 Mar 2015 08:19:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78433 invoked by uid 500); 5 Mar 2015 08:19:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78421 invoked by uid 99); 5 Mar 2015 08:19:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:19:46 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:19:20 +0000
Received: by obbgq1 with SMTP id gq1so12391371obb.2
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 00:19:18 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=nxhb9cqtR3+vaA/yKaI7eCYEi6H50lBIU7XCYYSWKRU=;
        b=p0k7JhL6MnB9EeP9K+Cg/WTVDvz1qADUP0jhJ0jZTa+t46/20Pkq2haOz2On3zwsKc
         rhFASxKZKvwL6bcEOOJoc8coT0ff/gMjWMQZK89PlGCyjwoQT4gAOc687zEJzYybxISR
         Bs2bVFEF3f/4N0tA//7pvkNY/7DzzZVReSNKP6oIawQnkVc8L+Aqo5TgH4DfT8Gq1jB3
         T2hDzSg0e4eqnlTHDSUAY7BMmiYY2jULlLZ0qvEmM4v+xeTzXj0R0zyOZWycIa1d8BkK
         893u3ry8ew0xAyHI/gOxl5aKfXsb0vOk2c6M9he9e9wLJ6I+VDnKxErCzUT6Jornkgr6
         Ag9g==
MIME-Version: 1.0
X-Received: by 10.202.185.198 with SMTP id j189mr5868851oif.72.1425543558768;
 Thu, 05 Mar 2015 00:19:18 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Thu, 5 Mar 2015 00:19:18 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Thu, 5 Mar 2015 00:19:18 -0800 (PST)
In-Reply-To: <CAJiQeYLpT5dRf=TVBVXo4HFnB=e43sEECsuAmXVbWoYYwG4rUg@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CANGvG8qiJ4QL1g-pGV+DAhxpqWiAJLxiLzP+ZHZ+PybrHBZkQg@mail.gmail.com>
	<CAJiQeYLpT5dRf=TVBVXo4HFnB=e43sEECsuAmXVbWoYYwG4rUg@mail.gmail.com>
Date: Thu, 5 Mar 2015 00:19:18 -0800
Message-ID: <CABPQxsuRrNXXKrsxcykOE1bcSjnSf=0E5V5ywSfTfZB2OzAzKA@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: Joseph Bradley <joseph@databricks.com>, Michael Armbrust <michael@databricks.com>, 
	Aaron Davidson <ilikerps@gmail.com>, Xiangrui Meng <mengxr@gmail.com>, dev <dev@spark.apache.org>, 
	Stephen Boesch <javadba@gmail.com>
Content-Type: multipart/alternative; boundary=001a113ce80e6ead290510863990
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ce80e6ead290510863990
Content-Type: text/plain; charset=ISO-8859-1

Yes - only new or internal API's. I doubt we'd break any exposed APIs for
the purpose of clean up.

Patrick
On Mar 5, 2015 12:16 AM, "Mridul Muralidharan" <mridul@gmail.com> wrote:

> While I dont have any strong opinions about how we handle enum's
> either way in spark, I assume the discussion is targetted at (new) api
> being designed in spark.
> Rewiring what we already have exposed will lead to incompatible api
> change (StorageLevel for example, is in 1.0).
>
> Regards,
> Mridul
>
> On Wed, Mar 4, 2015 at 11:45 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> > That's kinda annoying, but it's just a little extra boilerplate. Can you
> > call it as StorageLevel.DiskOnly() from Java? Would it also work if they
> > were case classes with empty constructors, without the field?
> >
> > On Wed, Mar 4, 2015 at 11:35 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >
> >> `case object` inside an `object` doesn't show up in Java. This is the
> >> minimal code I found to make everything show up correctly in both
> >> Scala and Java:
> >>
> >> sealed abstract class StorageLevel // cannot be a trait
> >>
> >> object StorageLevel {
> >>   private[this] case object _MemoryOnly extends StorageLevel
> >>   final val MemoryOnly: StorageLevel = _MemoryOnly
> >>
> >>   private[this] case object _DiskOnly extends StorageLevel
> >>   final val DiskOnly: StorageLevel = _DiskOnly
> >> }
> >>
> >> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >> > I like #4 as well and agree with Aaron's suggestion.
> >> >
> >> > - Patrick
> >> >
> >> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
> >> wrote:
> >> >> I'm cool with #4 as well, but make sure we dictate that the values
> >> should
> >> >> be defined within an object with the same name as the enumeration
> (like
> >> we
> >> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
> >> >>
> >> >> e.g. we SHOULD do:
> >> >>
> >> >> trait StorageLevel
> >> >> object StorageLevel {
> >> >>   case object MemoryOnly extends StorageLevel
> >> >>   case object DiskOnly extends StorageLevel
> >> >> }
> >> >>
> >> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
> >> michael@databricks.com>
> >> >> wrote:
> >> >>
> >> >>> #4 with a preference for CamelCaseEnums
> >> >>>
> >> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
> joseph@databricks.com>
> >> >>> wrote:
> >> >>>
> >> >>> > another vote for #4
> >> >>> > People are already used to adding "()" in Java.
> >> >>> >
> >> >>> >
> >> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com
> >
> >> >>> wrote:
> >> >>> >
> >> >>> > > #4 but with MemoryOnly (more scala-like)
> >> >>> > >
> >> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
> >> >>> > >
> >> >>> > > Constants, Values, Variable and Methods
> >> >>> > >
> >> >>> > > Constant names should be in upper camel case. That is, if the
> >> member is
> >> >>> > > final, immutable and it belongs to a package object or an
> object,
> >> it
> >> >>> may
> >> >>> > be
> >> >>> > > considered a constant (similar to Java'sstatic final members):
> >> >>> > >
> >> >>> > >
> >> >>> > >    1. object Container {
> >> >>> > >    2.     val MyConstant = ...
> >> >>> > >    3. }
> >> >>> > >
> >> >>> > >
> >> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> >> >>> > >
> >> >>> > > > Hi all,
> >> >>> > > >
> >> >>> > > > There are many places where we use enum-like types in Spark,
> but
> >> in
> >> >>> > > > different ways. Every approach has both pros and cons. I
> wonder
> >> >>> > > > whether there should be an "official" approach for enum-like
> >> types in
> >> >>> > > > Spark.
> >> >>> > > >
> >> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState,
> etc)
> >> >>> > > >
> >> >>> > > > * All types show up as Enumeration.Value in Java.
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
> >> >>> > > >
> >> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
> >> >>> > > >
> >> >>> > > > * Implementation must be in a Java file.
> >> >>> > > > * Values doesn't show up in the ScalaDoc:
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
> >> >>> > > >
> >> >>> > > > 3. Static fields in Java (e.g., TripletFields)
> >> >>> > > >
> >> >>> > > > * Implementation must be in a Java file.
> >> >>> > > > * Doesn't need "()" in Java code.
> >> >>> > > > * Values don't show up in the ScalaDoc:
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
> >> >>> > > >
> >> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
> >> >>> > > >
> >> >>> > > > * Needs "()" in Java code.
> >> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
> >> >>> > > >
> >> >>> > > > It would be great if we have an "official" approach for this
> as
> >> well
> >> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY"
> or
> >> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
> >> >>> thoughts?
> >> >>> > > >
> >> >>> > > > Best,
> >> >>> > > > Xiangrui
> >> >>> > > >
> >> >>> > > >
> >> ---------------------------------------------------------------------
> >> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
>

--001a113ce80e6ead290510863990--

From dev-return-11866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 08:29:08 2015
Return-Path: <dev-return-11866-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 76E2A10365
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 08:29:08 +0000 (UTC)
Received: (qmail 3520 invoked by uid 500); 5 Mar 2015 08:29:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3442 invoked by uid 500); 5 Mar 2015 08:29:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3430 invoked by uid 99); 5 Mar 2015 08:29:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:29:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pllee@appier.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:29:02 +0000
Received: by qcxn11 with SMTP id n11so19775001qcx.3
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 00:28:40 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=j4HCWSJ3v3rrxAk9mjT8fgstX3hZkLUOdPjOPT0hMP8=;
        b=JvtZEedgmO6w4t47rGDY8MuMwPCoZz4s/P/oWt85xH+ZNpzXvN7OU9xlBjNsF8XEDD
         ZtAAfNru8e/wcjte2Cc0yrc4HBWNqmSPGl5P1ifFcOcNT5P6zGaFQpQEQfkJEzukMd6r
         pGhn13CZ3ArXY2lWwhFbguXV6ODZn1Oz0rPI0=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=j4HCWSJ3v3rrxAk9mjT8fgstX3hZkLUOdPjOPT0hMP8=;
        b=WkkuHuhf/dPxplOPXgvJdH3mACbLTlP2o96PDckVrxPHSnyYUMMH3KT1Hm1vDzr5cQ
         DE094tgUjP+/QFcNvtjm0Q2DVyXUccktHdkhPsTNITzs2VAWFTJjpFoUzk69j/MSY2Ik
         3FgEn/wYOPBz2vRj04vnG6ruIFEtxvUv8GDzd94+6HWkCqIYcpxtxjUEjgE3rLN08FAi
         4FqUK8Ndr40OTeQ5P4LXCyilOKWZJdzoEWOd3n3XwBxdzmzHmK/i9LMXgmKrtPcPdXsN
         7t2IOFlG39ii32QmnYupU6KnAM+RooWoOoYBTDXuIq8WoFo50Qnc1oJ/1bQrbZsJ8zb3
         ng+g==
X-Gm-Message-State: ALoCoQm6q0uwF7luYtCOQPoeT/CkPMAp70zTL7bVqbQI9PAWMJVrs08gr+CrtxRz9CGen9wy1Qzh
MIME-Version: 1.0
X-Received: by 10.140.235.65 with SMTP id g62mr11367454qhc.30.1425544120809;
 Thu, 05 Mar 2015 00:28:40 -0800 (PST)
Received: by 10.229.233.136 with HTTP; Thu, 5 Mar 2015 00:28:40 -0800 (PST)
In-Reply-To: <CAH70K74N+hu2r5XEZOYFg0sDjurCZuuLvYeUvF8E9UyUw81GQg@mail.gmail.com>
References: <D110C7BA.1DBB7%mkim@palantir.com>
	<483876581.4513560.1424722604100.JavaMail.yahoo@mail.yahoo.com>
	<1508291672.8549572.1424726193790.JavaMail.yahoo@mail.yahoo.com>
	<CAH70K74N+hu2r5XEZOYFg0sDjurCZuuLvYeUvF8E9UyUw81GQg@mail.gmail.com>
Date: Thu, 5 Mar 2015 16:28:40 +0800
Message-ID: <CANrtgzUBgyF4WtZunieeKhPiL_rxwUEhHcQ8GDQ0EPvE4WGv1g@mail.gmail.com>
Subject: Re: Which OutputCommitter to use for S3?
From: Pei-Lun Lee <pllee@appier.com>
To: "user@spark.apache.org" <user@spark.apache.org>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11355fd2eedf320510865a07
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11355fd2eedf320510865a07
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks for the DirectOutputCommitter example.
However I found it only works for saveAsHadoopFile. What about
saveAsParquetFile?
It looks like SparkSQL is using ParquetOutputCommitter, which is subclass
of FileOutputCommitter.

On Fri, Feb 27, 2015 at 1:52 AM, Thomas Demoor <thomas.demoor@amplidata.com=
>
wrote:

> FYI. We're currently addressing this at the Hadoop level in
> https://issues.apache.org/jira/browse/HADOOP-9565
>
>
> Thomas Demoor
>
> On Mon, Feb 23, 2015 at 10:16 PM, Darin McBeath <
> ddmcbeath@yahoo.com.invalid> wrote:
>
>> Just to close the loop in case anyone runs into the same problem I had.
>>
>> By setting --hadoop-major-version=3D2 when using the ec2 scripts,
>> everything worked fine.
>>
>> Darin.
>>
>>
>> ----- Original Message -----
>> From: Darin McBeath <ddmcbeath@yahoo.com.INVALID>
>> To: Mingyu Kim <mkim@palantir.com>; Aaron Davidson <ilikerps@gmail.com>
>> Cc: "user@spark.apache.org" <user@spark.apache.org>
>> Sent: Monday, February 23, 2015 3:16 PM
>> Subject: Re: Which OutputCommitter to use for S3?
>>
>> Thanks.  I think my problem might actually be the other way around.
>>
>> I'm compiling with hadoop 2,  but when I startup Spark, using the ec2
>> scripts, I don't specify a
>> -hadoop-major-version and the default is 1.   I'm guessing that if I mak=
e
>> that a 2 that it might work correctly.  I'll try it and post a response.
>>
>>
>> ----- Original Message -----
>> From: Mingyu Kim <mkim@palantir.com>
>> To: Darin McBeath <ddmcbeath@yahoo.com>; Aaron Davidson <
>> ilikerps@gmail.com>
>> Cc: "user@spark.apache.org" <user@spark.apache.org>
>> Sent: Monday, February 23, 2015 3:06 PM
>> Subject: Re: Which OutputCommitter to use for S3?
>>
>> Cool, we will start from there. Thanks Aaron and Josh!
>>
>> Darin, it=C2=B9s likely because the DirectOutputCommitter is compiled wi=
th
>> Hadoop 1 classes and you=C2=B9re running it with Hadoop 2.
>> org.apache.hadoop.mapred.JobContext used to be a class in Hadoop 1, and =
it
>> became an interface in Hadoop 2.
>>
>> Mingyu
>>
>>
>>
>>
>>
>> On 2/23/15, 11:52 AM, "Darin McBeath" <ddmcbeath@yahoo.com.INVALID>
>> wrote:
>>
>> >Aaron.  Thanks for the class. Since I'm currently writing Java based
>> >Spark applications, I tried converting your class to Java (it seemed
>> >pretty straightforward).
>> >
>> >I set up the use of the class as follows:
>> >
>> >SparkConf conf =3D new SparkConf()
>> >.set("spark.hadoop.mapred.output.committer.class",
>> >"com.elsevier.common.DirectOutputCommitter");
>> >
>> >And I then try and save a file to S3 (which I believe should use the ol=
d
>> >hadoop apis).
>> >
>> >JavaPairRDD<Text, Text> newBaselineRDDWritable =3D
>> >reducedhsfPairRDD.mapToPair(new ConvertToWritableTypes());
>> >newBaselineRDDWritable.saveAsHadoopFile(baselineOutputBucketFile,
>> >Text.class, Text.class, SequenceFileOutputFormat.class,
>> >org.apache.hadoop.io.compress.GzipCodec.class);
>> >
>> >But, I get the following error message.
>> >
>> >Exception in thread "main" java.lang.IncompatibleClassChangeError: Foun=
d
>> >class org.apache.hadoop.mapred.JobContext, but interface was expected
>> >at
>>
>> >com.elsevier.common.DirectOutputCommitter.commitJob(DirectOutputCommitt=
er.
>> >java:68)
>> >at
>> >org.apache.spark.SparkHadoopWriter.commitJob(SparkHadoopWriter.scala:12=
7)
>> >at
>>
>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFuncti=
ons
>> >.scala:1075)
>> >at
>>
>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions=
.sc
>> >ala:940)
>> >at
>>
>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions=
.sc
>> >ala:902)
>> >at
>>
>> >org.apache.spark.api.java.JavaPairRDD.saveAsHadoopFile(JavaPairRDD.scal=
a:7
>> >71)
>> >at com.elsevier.spark.SparkSyncDedup.main(SparkSyncDedup.java:156)
>> >
>> >In my class, JobContext is an interface of  type
>> >org.apache.hadoop.mapred.JobContext.
>> >
>> >Is there something obvious that I might be doing wrong (or messed up in
>> >the translation from Scala to Java) or something I should look into?  I=
'm
>> >using Spark 1.2 with hadoop 2.4.
>> >
>> >
>> >Thanks.
>> >
>> >Darin.
>> >
>> >
>> >________________________________
>> >
>> >
>> >From: Aaron Davidson <ilikerps@gmail.com>
>> >To: Andrew Ash <andrew@andrewash.com>
>> >Cc: Josh Rosen <rosenville@gmail.com>; Mingyu Kim <mkim@palantir.com>;
>> >"user@spark.apache.org" <user@spark.apache.org>; Aaron Davidson
>> ><aaron@databricks.com>
>> >Sent: Saturday, February 21, 2015 7:01 PM
>> >Subject: Re: Which OutputCommitter to use for S3?
>> >
>> >
>> >
>> >Here is the class:
>> >
>> https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__gist.github.com_a=
aron
>>
>> >dav_c513916e72101bbe14ec&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBr=
Z4tFb6o
>>
>> >Onmz8&r=3DennQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmuK=
ZRf6sFs
>> >zOvl_-ZnxmkBPHo1K24TfGE&s=3DcwSCPKlJO-BJcz4UcGck3xOE2N-4V3eoNvgtFCdMLP8=
&e=3D
>> >
>> >You can use it by setting "mapred.output.committer.class" in the Hadoop
>> >configuration (or "spark.hadoop.mapred.output.committer.class" in the
>> >Spark configuration). Note that this only works for the old Hadoop APIs=
,
>> >I believe the new Hadoop APIs strongly tie committer to input format (s=
o
>> >FileInputFormat always uses FileOutputCommitter), which makes this fix
>> >more difficult to apply.
>> >
>> >
>> >
>> >
>> >On Sat, Feb 21, 2015 at 12:12 PM, Andrew Ash <andrew@andrewash.com>
>> wrote:
>> >
>> >Josh is that class something you guys would consider open sourcing, or
>> >would you rather the community step up and create an OutputCommitter
>> >implementation optimized for S3?
>> >>
>> >>
>> >>On Fri, Feb 20, 2015 at 4:02 PM, Josh Rosen <rosenville@gmail.com>
>> wrote:
>> >>
>> >>We (Databricks) use our own DirectOutputCommitter implementation, whic=
h
>> >>is a couple tens of lines of Scala code.  The class would almost
>> >>entirely be a no-op except we took some care to properly handle the
>> >>_SUCCESS file.
>> >>>
>> >>>
>> >>>On Fri, Feb 20, 2015 at 3:52 PM, Mingyu Kim <mkim@palantir.com> wrote=
:
>> >>>
>> >>>I didn=C2=B9t get any response. It=C2=B9d be really appreciated if an=
yone using a
>> >>>special OutputCommitter for S3 can comment on this!
>> >>>>
>> >>>>
>> >>>>Thanks,
>> >>>>Mingyu
>> >>>>
>> >>>>
>> >>>>From: Mingyu Kim <mkim@palantir.com>
>> >>>>Date: Monday, February 16, 2015 at 1:15 AM
>> >>>>To: "user@spark.apache.org" <user@spark.apache.org>
>> >>>>Subject: Which OutputCommitter to use for S3?
>> >>>>
>> >>>>
>> >>>>
>> >>>>HI all,
>> >>>>
>> >>>>
>> >>>>The default OutputCommitter used by RDD, which is FileOutputCommitte=
r,
>> >>>>seems to require moving files at the commit step, which is not a
>> >>>>constant operation in S3, as discussed in
>> >>>>
>> https://urldefense.proofpoint.com/v2/url?u=3Dhttp-3A__mail-2Darchives.ap=
a
>>
>> >>>>che.org_mod-5Fmbox_spark-2Duser_201410.mbox_-253C543E33FA.2000802-40=
ent
>>
>> >>>>ropy.be-253E&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOn=
mz8&r=3De
>>
>> >>>>nnQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmuKZRf6sFsz=
Ovl_-
>> >>>>ZnxmkBPHo1K24TfGE&s=3DEQOZaHRANJupdjXCfHSXL2t5BZ9YgMt2pRc3pht4o7o&e=
=3D .
>>
>> >>>>People seem to develop their own NullOutputCommitter implementation =
or
>> >>>>use DirectFileOutputCommitter (as mentioned in SPARK-3595), but I
>> >>>>wanted to check if there is a de facto standard, publicly available
>> >>>>OutputCommitter to use for S3 in conjunction with Spark.
>> >>>>
>> >>>>
>> >>>>Thanks,
>> >>>>Mingyu
>> >>>
>> >>
>> >
>> >---------------------------------------------------------------------
>> >To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> >For additional commands, e-mail: user-help@spark.apache.org
>>
>> >
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>>
>

--001a11355fd2eedf320510865a07--

From dev-return-11867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 08:34:00 2015
Return-Path: <dev-return-11867-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DE1510420
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 08:34:00 +0000 (UTC)
Received: (qmail 26748 invoked by uid 500); 5 Mar 2015 08:33:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26608 invoked by uid 500); 5 Mar 2015 08:33:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25802 invoked by uid 99); 5 Mar 2015 08:33:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:33:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 74.125.82.49 as permitted sender)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 08:33:39 +0000
Received: by wggx13 with SMTP id x13so4528788wgg.12;
        Thu, 05 Mar 2015 00:32:33 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=fARxKZwskVrIusNx4XiRvHUJkBKTA7B25FRwMzbvXtk=;
        b=GBrcWyMcylkWDeGcm2tasL6BrvcHgXP25j/3ys54bKJP3zLcZEzqzixoUWjBlz8TFw
         0Zo3YluZkC4STevu/UWSi5vHQm9DLLxXn4eJm+0yJDtdaL89ltUKg+EZQhCR6G3fq82c
         dWL+fCyCwnHrTs7/2Mjw8u8YK8MBkh+lMn5XJY3pLWPjGVxav4lJgXwMA6vF/5AAiyFj
         WfSkHuFlULYKNotQ3bifkoEYTn0tnp6vEsqtppbQm3+jMhIXkGk484muDf9n0dBoEduA
         cB5674SdY+4u5tOg+U96V7B0hY1JYVI4GetD97oSiBqK2XMFwRrvEXWBBPSEV4RfymQx
         A2uA==
X-Received: by 10.181.5.43 with SMTP id cj11mr62268426wid.61.1425544352976;
 Thu, 05 Mar 2015 00:32:32 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Thu, 5 Mar 2015 00:32:12 -0800 (PST)
In-Reply-To: <CANrtgzUBgyF4WtZunieeKhPiL_rxwUEhHcQ8GDQ0EPvE4WGv1g@mail.gmail.com>
References: <D110C7BA.1DBB7%mkim@palantir.com> <483876581.4513560.1424722604100.JavaMail.yahoo@mail.yahoo.com>
 <1508291672.8549572.1424726193790.JavaMail.yahoo@mail.yahoo.com>
 <CAH70K74N+hu2r5XEZOYFg0sDjurCZuuLvYeUvF8E9UyUw81GQg@mail.gmail.com> <CANrtgzUBgyF4WtZunieeKhPiL_rxwUEhHcQ8GDQ0EPvE4WGv1g@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Thu, 5 Mar 2015 00:32:12 -0800
Message-ID: <CANGvG8pQD8A84a6VOXXjo8VcGnXYBadYZr=oYAgdaQnJLY7Xnw@mail.gmail.com>
Subject: Re: Which OutputCommitter to use for S3?
To: Pei-Lun Lee <pllee@appier.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134cd62c55471051086680d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134cd62c55471051086680d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yes, unfortunately that direct dependency makes this injection much more
difficult for saveAsParquetFile.

On Thu, Mar 5, 2015 at 12:28 AM, Pei-Lun Lee <pllee@appier.com> wrote:

> Thanks for the DirectOutputCommitter example.
> However I found it only works for saveAsHadoopFile. What about
> saveAsParquetFile?
> It looks like SparkSQL is using ParquetOutputCommitter, which is subclass
> of FileOutputCommitter.
>
> On Fri, Feb 27, 2015 at 1:52 AM, Thomas Demoor <
> thomas.demoor@amplidata.com>
> wrote:
>
> > FYI. We're currently addressing this at the Hadoop level in
> > https://issues.apache.org/jira/browse/HADOOP-9565
> >
> >
> > Thomas Demoor
> >
> > On Mon, Feb 23, 2015 at 10:16 PM, Darin McBeath <
> > ddmcbeath@yahoo.com.invalid> wrote:
> >
> >> Just to close the loop in case anyone runs into the same problem I had=
.
> >>
> >> By setting --hadoop-major-version=3D2 when using the ec2 scripts,
> >> everything worked fine.
> >>
> >> Darin.
> >>
> >>
> >> ----- Original Message -----
> >> From: Darin McBeath <ddmcbeath@yahoo.com.INVALID>
> >> To: Mingyu Kim <mkim@palantir.com>; Aaron Davidson <ilikerps@gmail.com=
>
> >> Cc: "user@spark.apache.org" <user@spark.apache.org>
> >> Sent: Monday, February 23, 2015 3:16 PM
> >> Subject: Re: Which OutputCommitter to use for S3?
> >>
> >> Thanks.  I think my problem might actually be the other way around.
> >>
> >> I'm compiling with hadoop 2,  but when I startup Spark, using the ec2
> >> scripts, I don't specify a
> >> -hadoop-major-version and the default is 1.   I'm guessing that if I
> make
> >> that a 2 that it might work correctly.  I'll try it and post a respons=
e.
> >>
> >>
> >> ----- Original Message -----
> >> From: Mingyu Kim <mkim@palantir.com>
> >> To: Darin McBeath <ddmcbeath@yahoo.com>; Aaron Davidson <
> >> ilikerps@gmail.com>
> >> Cc: "user@spark.apache.org" <user@spark.apache.org>
> >> Sent: Monday, February 23, 2015 3:06 PM
> >> Subject: Re: Which OutputCommitter to use for S3?
> >>
> >> Cool, we will start from there. Thanks Aaron and Josh!
> >>
> >> Darin, it=C2=B9s likely because the DirectOutputCommitter is compiled =
with
> >> Hadoop 1 classes and you=C2=B9re running it with Hadoop 2.
> >> org.apache.hadoop.mapred.JobContext used to be a class in Hadoop 1, an=
d
> it
> >> became an interface in Hadoop 2.
> >>
> >> Mingyu
> >>
> >>
> >>
> >>
> >>
> >> On 2/23/15, 11:52 AM, "Darin McBeath" <ddmcbeath@yahoo.com.INVALID>
> >> wrote:
> >>
> >> >Aaron.  Thanks for the class. Since I'm currently writing Java based
> >> >Spark applications, I tried converting your class to Java (it seemed
> >> >pretty straightforward).
> >> >
> >> >I set up the use of the class as follows:
> >> >
> >> >SparkConf conf =3D new SparkConf()
> >> >.set("spark.hadoop.mapred.output.committer.class",
> >> >"com.elsevier.common.DirectOutputCommitter");
> >> >
> >> >And I then try and save a file to S3 (which I believe should use the
> old
> >> >hadoop apis).
> >> >
> >> >JavaPairRDD<Text, Text> newBaselineRDDWritable =3D
> >> >reducedhsfPairRDD.mapToPair(new ConvertToWritableTypes());
> >> >newBaselineRDDWritable.saveAsHadoopFile(baselineOutputBucketFile,
> >> >Text.class, Text.class, SequenceFileOutputFormat.class,
> >> >org.apache.hadoop.io.compress.GzipCodec.class);
> >> >
> >> >But, I get the following error message.
> >> >
> >> >Exception in thread "main" java.lang.IncompatibleClassChangeError:
> Found
> >> >class org.apache.hadoop.mapred.JobContext, but interface was expected
> >> >at
> >>
> >>
> >com.elsevier.common.DirectOutputCommitter.commitJob(DirectOutputCommitte=
r.
> >> >java:68)
> >> >at
> >>
> >org.apache.spark.SparkHadoopWriter.commitJob(SparkHadoopWriter.scala:127=
)
> >> >at
> >>
> >>
> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctio=
ns
> >> >.scala:1075)
> >> >at
> >>
> >>
> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.=
sc
> >> >ala:940)
> >> >at
> >>
> >>
> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.=
sc
> >> >ala:902)
> >> >at
> >>
> >>
> >org.apache.spark.api.java.JavaPairRDD.saveAsHadoopFile(JavaPairRDD.scala=
:7
> >> >71)
> >> >at com.elsevier.spark.SparkSyncDedup.main(SparkSyncDedup.java:156)
> >> >
> >> >In my class, JobContext is an interface of  type
> >> >org.apache.hadoop.mapred.JobContext.
> >> >
> >> >Is there something obvious that I might be doing wrong (or messed up =
in
> >> >the translation from Scala to Java) or something I should look into?
> I'm
> >> >using Spark 1.2 with hadoop 2.4.
> >> >
> >> >
> >> >Thanks.
> >> >
> >> >Darin.
> >> >
> >> >
> >> >________________________________
> >> >
> >> >
> >> >From: Aaron Davidson <ilikerps@gmail.com>
> >> >To: Andrew Ash <andrew@andrewash.com>
> >> >Cc: Josh Rosen <rosenville@gmail.com>; Mingyu Kim <mkim@palantir.com>=
;
> >> >"user@spark.apache.org" <user@spark.apache.org>; Aaron Davidson
> >> ><aaron@databricks.com>
> >> >Sent: Saturday, February 21, 2015 7:01 PM
> >> >Subject: Re: Which OutputCommitter to use for S3?
> >> >
> >> >
> >> >
> >> >Here is the class:
> >> >
> >>
> https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__gist.github.com_aa=
ron
> >>
> >>
> >dav_c513916e72101bbe14ec&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ=
4tFb6o
> >>
> >>
> >Onmz8&r=3DennQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmuKZ=
Rf6sFs
> >>
> >zOvl_-ZnxmkBPHo1K24TfGE&s=3DcwSCPKlJO-BJcz4UcGck3xOE2N-4V3eoNvgtFCdMLP8&=
e=3D
> >> >
> >> >You can use it by setting "mapred.output.committer.class" in the Hado=
op
> >> >configuration (or "spark.hadoop.mapred.output.committer.class" in the
> >> >Spark configuration). Note that this only works for the old Hadoop
> APIs,
> >> >I believe the new Hadoop APIs strongly tie committer to input format
> (so
> >> >FileInputFormat always uses FileOutputCommitter), which makes this fi=
x
> >> >more difficult to apply.
> >> >
> >> >
> >> >
> >> >
> >> >On Sat, Feb 21, 2015 at 12:12 PM, Andrew Ash <andrew@andrewash.com>
> >> wrote:
> >> >
> >> >Josh is that class something you guys would consider open sourcing, o=
r
> >> >would you rather the community step up and create an OutputCommitter
> >> >implementation optimized for S3?
> >> >>
> >> >>
> >> >>On Fri, Feb 20, 2015 at 4:02 PM, Josh Rosen <rosenville@gmail.com>
> >> wrote:
> >> >>
> >> >>We (Databricks) use our own DirectOutputCommitter implementation,
> which
> >> >>is a couple tens of lines of Scala code.  The class would almost
> >> >>entirely be a no-op except we took some care to properly handle the
> >> >>_SUCCESS file.
> >> >>>
> >> >>>
> >> >>>On Fri, Feb 20, 2015 at 3:52 PM, Mingyu Kim <mkim@palantir.com>
> wrote:
> >> >>>
> >> >>>I didn=C2=B9t get any response. It=C2=B9d be really appreciated if =
anyone
> using a
> >> >>>special OutputCommitter for S3 can comment on this!
> >> >>>>
> >> >>>>
> >> >>>>Thanks,
> >> >>>>Mingyu
> >> >>>>
> >> >>>>
> >> >>>>From: Mingyu Kim <mkim@palantir.com>
> >> >>>>Date: Monday, February 16, 2015 at 1:15 AM
> >> >>>>To: "user@spark.apache.org" <user@spark.apache.org>
> >> >>>>Subject: Which OutputCommitter to use for S3?
> >> >>>>
> >> >>>>
> >> >>>>
> >> >>>>HI all,
> >> >>>>
> >> >>>>
> >> >>>>The default OutputCommitter used by RDD, which is
> FileOutputCommitter,
> >> >>>>seems to require moving files at the commit step, which is not a
> >> >>>>constant operation in S3, as discussed in
> >> >>>>
> >> https://urldefense.proofpoint.com/v2/url?u=3Dhttp-3A__mail-2Darchives.=
apa
> >>
> >>
> >>>>che.org_mod-5Fmbox_spark-2Duser_201410.mbox_-253C543E33FA.2000802-40e=
nt
> >>
> >>
> >>>>ropy.be-253E&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOnm=
z8&r=3De
> >>
> >>
> >>>>nnQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmuKZRf6sFszO=
vl_-
> >> >>>>ZnxmkBPHo1K24TfGE&s=3DEQOZaHRANJupdjXCfHSXL2t5BZ9YgMt2pRc3pht4o7o&=
e=3D .
> >>
> >> >>>>People seem to develop their own NullOutputCommitter implementatio=
n
> or
> >> >>>>use DirectFileOutputCommitter (as mentioned in SPARK-3595), but I
> >> >>>>wanted to check if there is a de facto standard, publicly availabl=
e
> >> >>>>OutputCommitter to use for S3 in conjunction with Spark.
> >> >>>>
> >> >>>>
> >> >>>>Thanks,
> >> >>>>Mingyu
> >> >>>
> >> >>
> >> >
> >> >---------------------------------------------------------------------
> >> >To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> >> >For additional commands, e-mail: user-help@spark.apache.org
> >>
> >> >
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: user-help@spark.apache.org
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: user-help@spark.apache.org
> >>
> >>
> >
>

--001a1134cd62c55471051086680d--

From dev-return-11868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 10:13:30 2015
Return-Path: <dev-return-11868-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD570109D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 10:13:30 +0000 (UTC)
Received: (qmail 11331 invoked by uid 500); 5 Mar 2015 10:13:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11260 invoked by uid 500); 5 Mar 2015 10:13:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11248 invoked by uid 99); 5 Mar 2015 10:13:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 10:13:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.51] (HELO mail-la0-f51.google.com) (209.85.215.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 10:13:01 +0000
Received: by labhs14 with SMTP id hs14so50381934lab.4
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 02:11:08 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=mcMCDlwPnjdy9k+whSGWC7HkPb/4WKy/mvgviRJVD8A=;
        b=VyhCxsp+9HfsbDhdZeJWiYCYASLuERB9g++GiTeQEdky9K7eF7NY89oDvZpMa867gD
         PwE/KiBq2yVdNLylpLnIyXmVeI8OflDFm1NsAXCkd5NCqoCJBPLHLrQ9V8HgadPmZoHW
         gbsQpIh5HlQH0uXNLZ0Qy45o5m2VQxre6VyOyK/KbBvC/9ZlIKiKQ9aV21Zr2hxzEvfB
         fJ9KgvVICGu4nZ8lVNUgmeFLnquhPGvkdqsxHq1FkiSSf6mlJj5m4vgDX1v6hcCB+g1b
         hIuthoF0DoOh3S1d1HjD4UnIBdEpjnFD4AX1AU/dW2amwrAEK8CCFOTCbzhquACcUKUi
         xn0w==
X-Gm-Message-State: ALoCoQnKmfXbxfjjaMWVi04B0VYEf/57txvlIpt9kJLnFxAg8nwqq10iF6W57Ak6f1/UHdUrCNOT
MIME-Version: 1.0
X-Received: by 10.152.87.3 with SMTP id t3mr7154364laz.19.1425549847933; Thu,
 05 Mar 2015 02:04:07 -0800 (PST)
Received: by 10.152.43.234 with HTTP; Thu, 5 Mar 2015 02:04:07 -0800 (PST)
In-Reply-To: <CAPKgQ9p+mo8NfF2CK8=ZVCo4wjekh0VVjAwdTdnihVooMhmTJg@mail.gmail.com>
References: <CAPKgQ9o8ZzWspE_qXhbobM330s7xaskUoti78Gu73Qg3rNER=g@mail.gmail.com>
	<CAPKgQ9p+mo8NfF2CK8=ZVCo4wjekh0VVjAwdTdnihVooMhmTJg@mail.gmail.com>
Date: Thu, 5 Mar 2015 15:34:07 +0530
Message-ID: <CAHUQ+_Z6hngo2jZ3r2AvTehRE_JRv_UAJxKq6P6=WO7=+sutiQ@mail.gmail.com>
Subject: Re: Unable to Read/Write Avro RDD on cluster.
From: Akhil Das <akhil@sigmoidanalytics.com>
To: =?UTF-8?B?w5DOnuKCrM+BQNKcICjguY/Mr82h4LmPKQ==?= <deepujain@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c355764bcb34051087b001
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c355764bcb34051087b001
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Here's a workaround:

- Download and put this jar
<http://repo1.maven.org/maven2/org/apache/avro/avro-mapred/1.7.7/
avro-mapred-1.7.7-hadoop2.jar> in the SPARK_CLASSPATH in all workers
- Make sure that jar is present in the same path in all workers.

Thanks
Best Regards

On Thu, Mar 5, 2015 at 10:27 AM, =C3=90=CE=9E=E2=82=AC=CF=81@=D2=9C (=E0=B9=
=8F=CC=AF=CD=A1=E0=B9=8F) <deepujain@gmail.com> wrote:

> I am trying to read RDD avro, transform and write.
> I am able to run it locally fine but when i run onto cluster, i see issue=
s
> with Avro.
>
>
> export SPARK_HOME=3D/home/dvasthimal/spark/spark-1.0.2-bin-2.4.1
> export SPARK_YARN_USER_ENV=3D"CLASSPATH=3D/apache/hadoop/conf"
> export HADOOP_CONF_DIR=3D/apache/hadoop/conf
> export YARN_CONF_DIR=3D/apache/hadoop/conf
> export SPARK_JAR=3D$SPARK_HOME/lib/spark-assembly-1.0.2-hadoop2.4.1.jar
> export SPARK_LIBRARY_PATH=3D/apache/hadoop/lib/native
> export SPARK_YARN_USER_ENV=3D"CLASSPATH=3D/apache/hadoop/conf"
> export SPARK_YARN_USER_ENV=3D"CLASSPATH=3D/apache/hadoop/conf"
> export
>
> SPARK_CLASSPATH=3D/apache/hadoop/share/hadoop/common/hadoop-common-2.4.1-=
company-2.jar:/apache/hadoop/lib/hadoop-lzo-0.6.0.jar:/home/dvasthimal/spar=
k/avro-mapred-1.7.7-hadoop2.jar:/home/dvasthimal/spark/avro-1.7.7.jar
> export SPARK_LIBRARY_PATH=3D"/apache/hadoop/lib/native"
> export YARN_CONF_DIR=3D/apache/hadoop/conf/
>
> cd $SPARK_HOME
>
> ./bin/spark-submit --master yarn-cluster --jars
>
> /home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar,/home/dvasthimal/spa=
rk/avro-1.7.7.jar
> --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-core=
s
> 1  --queue hdmi-spark --class com.company.ep.poc.spark.reporting.SparkApp
> /home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar
> startDate=3D2015-02-16 endDate=3D2015-02-16
> epoutputdirectory=3D/user/dvasthimal/epdatasets_small/exptsession
> subcommand=3Dsuccessevents
> outputdir=3D/user/dvasthimal/epdatasets/successdetail
>
> Spark assembly has been built with Hive, including Datanucleus jars on
> classpath
> 15/03/04 03:20:29 INFO client.ConfiguredRMFailoverProxyProvider: Failing
> over to rm2
> 15/03/04 03:20:30 INFO yarn.Client: Got Cluster metric info from
> ApplicationsManager (ASM), number of NodeManagers: 2221
> 15/03/04 03:20:30 INFO yarn.Client: Queue info ... queueName: hdmi-spark,
> queueCurrentCapacity: 0.7162806, queueMaxCapacity: 0.08,
>       queueApplicationCount =3D 7, queueChildQueueCount =3D 0
> 15/03/04 03:20:30 INFO yarn.Client: Max mem capabililty of a single
> resource in this cluster 16384
> 15/03/04 03:20:30 INFO yarn.Client: Preparing Local resources
> 15/03/04 03:20:30 WARN util.NativeCodeLoader: Unable to load native-hadoo=
p
> library for your platform... using builtin-java classes where applicable
> 15/03/04 03:20:30 WARN hdfs.BlockReaderLocal: The short-circuit local rea=
ds
> feature cannot be used because libhadoop cannot be loaded.
>
>
> 15/03/04 03:20:46 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN toke=
n
> 7780745 for dvasthimal on 10.115.206.112:8020
> 15/03/04 03:20:46 INFO yarn.Client: Uploading
> file:/home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar to hdfs://
>
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_=
1425075571333_61948/spark_reporting-1.0-SNAPSHOT.jar
> 15/03/04 03:20:47 INFO yarn.Client: Uploading
>
> file:/home/dvasthimal/spark/spark-1.0.2-bin-2.4.1/lib/spark-assembly-1.0.=
2-hadoop2.4.1.jar
> to hdfs://
>
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_=
1425075571333_61948/spark-assembly-1.0.2-hadoop2.4.1.jar
> 15/03/04 03:20:52 INFO yarn.Client: Uploading
> file:/home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar to hdfs://
>
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_=
1425075571333_61948/avro-mapred-1.7.7-hadoop2.jar
> 15/03/04 03:20:52 INFO yarn.Client: Uploading
> file:/home/dvasthimal/spark/avro-1.7.7.jar to hdfs://
>
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_=
1425075571333_61948/avro-1.7.7.jar
> 15/03/04 03:20:54 INFO yarn.Client: Setting up the launch environment
> 15/03/04 03:20:54 INFO yarn.Client: Setting up container launch context
> 15/03/04 03:20:54 INFO yarn.Client: Command for starting the Spark
> ApplicationMaster: List($JAVA_HOME/bin/java, -server, -Xmx4096m,
> -Djava.io.tmpdir=3D$PWD/tmp,
> -Dspark.app.name=3D\"com.company.ep.poc.spark.reporting.SparkApp\",
>  -Dlog4j.configuration=3Dlog4j-spark-container.properties,
> org.apache.spark.deploy.yarn.ApplicationMaster, --class,
> com.company.ep.poc.spark.reporting.SparkApp, --jar ,
> file:/home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar,  --args
>  'startDate=3D2015-02-16'  --args  'endDate=3D2015-02-16'  --args
>  'epoutputdirectory=3D/user/dvasthimal/epdatasets_small/exptsession'  --a=
rgs
>  'subcommand=3Dsuccessevents'  --args
>  'outputdir=3D/user/dvasthimal/epdatasets/successdetail' , --executor-mem=
ory,
> 2048, --executor-cores, 1, --num-executors , 3, 1>, <LOG_DIR>/stdout, 2>,
> <LOG_DIR>/stderr)
> 15/03/04 03:20:54 INFO yarn.Client: Submitting application to ASM
> 15/03/04 03:20:54 INFO impl.YarnClientImpl: Submitted application
> application_1425075571333_61948
> 15/03/04 03:20:56 INFO yarn.Client: Application report from ASM:
>  application identifier: application_1425075571333_61948
>  appId: 61948
>  clientToAMToken: null
>  appDiagnostics:
>  appMasterHost: N/A
>  appQueue: hdmi-spark
>  appMasterRpcPort: -1
>  appStartTime: 1425464454263
>  yarnAppState: ACCEPTED
>  distributedFinalState: UNDEFINED
>  appTrackingUrl:
>
> https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333=
_61948/
>  appUser: dvasthimal
> 15/03/04 03:21:18 INFO yarn.Client: Application report from ASM:
>  application identifier: application_1425075571333_61948
>  appId: 61948
>  clientToAMToken: Token { kind: YARN_CLIENT_TOKEN, service:  }
>  appDiagnostics:
>  appMasterHost: phxaishdc9dn0169.phx.company.com
>  appQueue: hdmi-spark
>  appMasterRpcPort: 0
>  appStartTime: 1425464454263
>  yarnAppState: RUNNING
>  distributedFinalState: UNDEFINED
>  appTrackingUrl:
>
> https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333=
_61948/
>  appUser: dvasthimal
> =E2=80=A6.
> =E2=80=A6.
> 15/03/04 03:21:22 INFO yarn.Client: Application report from ASM:
>  application identifier: application_1425075571333_61948
>  appId: 61948
>  clientToAMToken: Token { kind: YARN_CLIENT_TOKEN, service:  }
>  appDiagnostics:
>  appMasterHost: phxaishdc9dn0169.phx.company.com
>  appQueue: hdmi-spark
>  appMasterRpcPort: 0
>  appStartTime: 1425464454263
>  yarnAppState: FINISHED
>  distributedFinalState: FAILED
>  appTrackingUrl:
>
> https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333=
_61948/A
>  appUser: dvasthimal
>
>
>
> AM failed with following exception
>
> /apache/hadoop/bin/yarn logs -applicationId application_1425075571333_619=
48
> 15/03/04 03:21:22 INFO NewHadoopRDD: Input split: hdfs://
>
> apollo-phx-nn.company.com:8020/user/dvasthimal/epdatasets_small/exptsessi=
on/2015/02/16/part-r-00000.avro:0+13890
> 15/03/04 03:21:22 ERROR Executor: Exception in task ID 3
> java.lang.IncompatibleClassChangeError: Found interface
> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
> at
>
> org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyIn=
putFormat.java:47)
> at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:11=
1)
> at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
> at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
> at org.apache.spark.scheduler.Task.run(Task.scala:51)
> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
> at
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java=
:1145)
> at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.jav=
a:615)
> at java.lang.Thread.run(Thread.java:745)
>
>
>
> 1) Having figured out the error the fix would be to put the right version
> of avro libs into AM JVM classpath. Hence i included --jars
>
> /home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar,/home/dvasthimal/spa=
rk/avro-1.7.7.jar
> in spark-submit command. However i still see the same exception.
> 2) I tried to include these libs in SPARK_CLASSPATH. However i see the sa=
me
> exception.
>
>
> --
> Deepak
>

--001a11c355764bcb34051087b001--

From dev-return-11869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 14:30:30 2015
Return-Path: <dev-return-11869-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 329EC1755A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 14:30:30 +0000 (UTC)
Received: (qmail 7412 invoked by uid 500); 5 Mar 2015 14:30:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7338 invoked by uid 500); 5 Mar 2015 14:30:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7327 invoked by uid 99); 5 Mar 2015 14:30:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 14:30:25 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of java8964@hotmail.com designates 65.55.90.240 as permitted sender)
Received: from [65.55.90.240] (HELO SNT004-OMC4S37.hotmail.com) (65.55.90.240)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 14:30:20 +0000
Received: from SNT149-W81 ([65.55.90.199]) by SNT004-OMC4S37.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Thu, 5 Mar 2015 06:28:55 -0800
X-TMN: [7kexYDnkhFlYq7NA/2rWcdkStpnLjkn+PfffE2PAsAo=]
X-Originating-Email: [java8964@hotmail.com]
Message-ID: <SNT149-W8180ACD5F2AF7AE5475877D01F0@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_ff075d4c-3402-46ef-bf68-ba31a884c905_"
From: java8964 <java8964@hotmail.com>
To: =?utf-8?B?w5DOnuKCrM+BQNKcIOC5j8yvzaHguY8=?= <deepujain@gmail.com>,
	"dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Unable to Read/Write Avro RDD on cluster.
Date: Thu, 5 Mar 2015 09:28:55 -0500
Importance: Normal
In-Reply-To:
 <CAPKgQ9p+mo8NfF2CK8=ZVCo4wjekh0VVjAwdTdnihVooMhmTJg@mail.gmail.com>
References:
 <CAPKgQ9o8ZzWspE_qXhbobM330s7xaskUoti78Gu73Qg3rNER=g@mail.gmail.com>,<CAPKgQ9p+mo8NfF2CK8=ZVCo4wjekh0VVjAwdTdnihVooMhmTJg@mail.gmail.com>
MIME-Version: 1.0
X-OriginalArrivalTime: 05 Mar 2015 14:28:55.0789 (UTC) FILETIME=[B619D1D0:01D05750]
X-Virus-Checked: Checked by ClamAV on apache.org

--_ff075d4c-3402-46ef-bf68-ba31a884c905_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_ff075d4c-3402-46ef-bf68-ba31a884c905_--

From dev-return-11870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 14:59:13 2015
Return-Path: <dev-return-11870-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F143917739
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 14:59:12 +0000 (UTC)
Received: (qmail 12135 invoked by uid 500); 5 Mar 2015 14:59:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12057 invoked by uid 500); 5 Mar 2015 14:59:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12046 invoked by uid 99); 5 Mar 2015 14:59:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 14:59:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.48 as permitted sender)
Received: from [209.85.215.48] (HELO mail-la0-f48.google.com) (209.85.215.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 14:58:56 +0000
Received: by lamq1 with SMTP id q1so28847936lam.0
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 06:57:50 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=wT8Z7l7/IABmelOFdpAqeOhi72h96XWxCD4rqe1LLOU=;
        b=RmT7IVAo/gmYaojZTxvRE+GnDE2xYXseS3+tXYjJU6NjVDILr4wE9139VFX4weKC/6
         t27UnB/lZ7t0OVrsRLX0a4So6MBbeYVrvGr7cvZ1NcBH5/i2mdskc7+7Qt76v5iqF0tL
         ssBBOCXuJre+BIE4H4vwIOQqhLkXgYxQ4mTm0MlxuDg+Da/lBH5HhZJz9s7xUQat8Fv2
         vNrYdd5WQwkAfieGldtrumC2gboBv7zsZRzJiVHWQ7MkU0X/ZttC5mp9/3dY6XXDzagO
         bEZu3i5YRaQOPdCruYVQGp2jAPEL8SrkSlBpF/J+mQVD4biQxAMA0xh6KqCVIFlJGmRK
         Y/mQ==
X-Gm-Message-State: ALoCoQl/KeUZehUhVPyEIRJG9LjU2z+NocgMlloKYnf+f6UB9F+EJxenhmJcbXNbZrxa8HulnPtc
X-Received: by 10.152.22.200 with SMTP id g8mr8221244laf.96.1425567470774;
 Thu, 05 Mar 2015 06:57:50 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Thu, 5 Mar 2015 06:57:30 -0800 (PST)
In-Reply-To: <CACdU-dQxLyEc7HVA9YONPkkfk5aEX+7Xwo7AWngvGYr+FOsuPQ@mail.gmail.com>
References: <CACdU-dQxLyEc7HVA9YONPkkfk5aEX+7Xwo7AWngvGYr+FOsuPQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 5 Mar 2015 06:57:30 -0800
Message-ID: <CACdU-dTSGq61P9UED5gaJNdDi=zH2TPyOJUg8Ctbw4y06kjMEg@mail.gmail.com>
Subject: Re: short jenkins 7am downtime tomorrow morning (3-5-15)
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158c31ab2fc5005108bca7e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c31ab2fc5005108bca7e
Content-Type: text/plain; charset=UTF-8

this is happening now.  i'm waiting for the pull request builders to finish
(~16 mins) before i start.

On Wed, Mar 4, 2015 at 1:06 PM, shane knapp <sknapp@berkeley.edu> wrote:

> the master and workers need some system and package updates, and i'll also
> be rebooting the machines as well.
>
> this shouldn't take very long to perform, and i expect jenkins to be back
> up and building by 9am at the *latest*.
>
> important note:  i will NOT be updating jenkins or any of the plugins
> during this maintenance!
>
> as always, please let me know if you have any questions or concerns.
>
> danke shane
>

--089e0158c31ab2fc5005108bca7e--

From dev-return-11871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 16:31:40 2015
Return-Path: <dev-return-11871-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7CA9917C18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 16:31:40 +0000 (UTC)
Received: (qmail 4974 invoked by uid 500); 5 Mar 2015 16:31:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4894 invoked by uid 500); 5 Mar 2015 16:31:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4883 invoked by uid 99); 5 Mar 2015 16:31:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 16:31:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 16:31:13 +0000
Received: by labge10 with SMTP id ge10so52538238lab.12
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 08:29:41 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=RgSdq5U4f5X8/IACMjPg/MG4MdyR89UtTgffN0f9OjQ=;
        b=S72yb1DtySDhQOc0jjDGhT123x5IWsUlmWQ2ZBqJQnn2CAQGb2tSsUATJs+3IWo7ho
         7yOOcexXgS6b+oAc2KPgDUyfwdnkNpI4/hTmYjdebEWX3Y2GrFYSBeUzWBol8ZG/iUY8
         eeWp79+cab2DffSDsltWEHWRz30j51Yys00Y6QPkYV88DkBFercn6F5/kK9/Gj58yag1
         61l7Nlgnv36aJNdDjRa0laHGLyk92ynydE/tT45OLKcdxvlkdEoywA6U4d9CCE/3219X
         CihjzTgT1rFbSHcOFtv7f/JJ74a6ropdZaI+3Wc9RQ+1GT/K9imgJZ6znevNsVFADkAX
         ngSg==
X-Gm-Message-State: ALoCoQmZL/pci2TgE8phSqtaapN3oXtv3VoRnpOgx0LFy9eV69VxrefaC63c9RmWun085cJi9iiV
X-Received: by 10.152.87.178 with SMTP id az18mr8962600lab.122.1425572981684;
 Thu, 05 Mar 2015 08:29:41 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Thu, 5 Mar 2015 08:29:19 -0800 (PST)
In-Reply-To: <CACdU-dTSGq61P9UED5gaJNdDi=zH2TPyOJUg8Ctbw4y06kjMEg@mail.gmail.com>
References: <CACdU-dQxLyEc7HVA9YONPkkfk5aEX+7Xwo7AWngvGYr+FOsuPQ@mail.gmail.com>
 <CACdU-dTSGq61P9UED5gaJNdDi=zH2TPyOJUg8Ctbw4y06kjMEg@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 5 Mar 2015 08:29:19 -0800
Message-ID: <CACdU-dRsdX03x3c+PDuC7wZUcz-LzPuEfNT+SMWeyCKJaV28sg@mail.gmail.com>
Subject: Re: short jenkins 7am downtime tomorrow morning (3-5-15)
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c354602cd5d805108d13d9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c354602cd5d805108d13d9
Content-Type: text/plain; charset=UTF-8

we're all back up and building now...  looks like the package/kernel
updates went off w/o a hitch!

On Thu, Mar 5, 2015 at 6:57 AM, shane knapp <sknapp@berkeley.edu> wrote:

> this is happening now.  i'm waiting for the pull request builders to
> finish (~16 mins) before i start.
>
> On Wed, Mar 4, 2015 at 1:06 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> the master and workers need some system and package updates, and i'll
>> also be rebooting the machines as well.
>>
>> this shouldn't take very long to perform, and i expect jenkins to be back
>> up and building by 9am at the *latest*.
>>
>> important note:  i will NOT be updating jenkins or any of the plugins
>> during this maintenance!
>>
>> as always, please let me know if you have any questions or concerns.
>>
>> danke shane
>>
>
>

--001a11c354602cd5d805108d13d9--

From dev-return-11872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 18:37:10 2015
Return-Path: <dev-return-11872-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48904101FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 18:37:10 +0000 (UTC)
Received: (qmail 45756 invoked by uid 500); 5 Mar 2015 18:37:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45689 invoked by uid 500); 5 Mar 2015 18:37:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45676 invoked by uid 99); 5 Mar 2015 18:37:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 18:37:03 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=FORGED_YAHOO_RCVD,FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_NONE,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of medale94@yahoo.com designates 98.138.91.158 as permitted sender)
Received: from [98.138.91.158] (HELO nm28-vm3.bullet.mail.ne1.yahoo.com) (98.138.91.158)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 18:36:38 +0000
Received: from [98.138.226.178] by nm28.bullet.mail.ne1.yahoo.com with NNFMP; 05 Mar 2015 18:36:35 -0000
Received: from [98.138.226.133] by tm13.bullet.mail.ne1.yahoo.com with NNFMP; 05 Mar 2015 18:36:35 -0000
Received: from [127.0.0.1] by smtp220.mail.ne1.yahoo.com with NNFMP; 05 Mar 2015 18:36:35 -0000
X-Yahoo-Newman-Id: 395772.29273.bm@smtp220.mail.ne1.yahoo.com
X-Yahoo-Newman-Property: ymail-3
X-YMail-OSG: RdFsTo4VM1lUAD7rv9EK6DG9lV5bts.A7j.zX2PALQB8APe
 a8LPkdv3xOEI5YO5bkt.2n.Y3A8tA07vNv6GYB_LD5xLYE8mE.HF.mtU9HtM
 4mbWwEtQkgLovM0Y2ldSbVht4iYP9zHVsjylxQkRwYRzcmmcNn4J0jE4qou6
 Usk9jvPCJDDwkOPntd6uMt5rVwMaDtbSz5QXQpN.pMj7qx.PWGMe86ioEUrQ
 PEoJX.noD5zdmGpgXZjsxGt2G4Vnv51dmwlRfrA7ANP7XBYFwP_wUptIAbTv
 C_L.4fVY6Tr.lbv6gBq0wWsRBTug6nYIzI1j68BEVsJtPKuswyqiQJ9gx.eP
 .tmkEiMPcxxbuDRmIo_TVfP1.R96pRDbseIha5NFW0PG3CIY5cMcxdIJ7oy2
 faC_._efyd_ZPe.U2TyCfBn8wwG7gMm5BMIQywtLf7pIhIJB879s8rhwZyov
 yUATr1AgaXhPkJxYr3ufWeolCZDsDsFQXUaj2ZnGdLOjP6puNK3oremTIdup
 XhBFUqcqHudOPqJK_PSNheP4VemVEENL.tlvzEhgC4AdKe83KbGydXIqRT4u
 80Ktgq1ritncF3.Ss0NgwyJmmMmFiAv4gtDirnb8xFka0UyW4Gu6WFDeP7oE
 txCiRQp0Fev_CqKeJcgjdgGVt94tke2rybuzDWSF92ud.ivSo2PadPpdE18.
 FXxiX.iAbcfQSuUgGMHFjJZze9I9cPspH8MrxWt_laXipRahkdqbjZ_rzu00
 47Zme7mzGckuBkPlb.rN3zKO.illaHUoYWRHaMgGOtETEm_J.mVDQpl8GSI3
 TDfX6CtmfYY3XVNR5FqXIKw4y2PuLiyM8I6bcKjsLoUvG8iNxgGIZbNY5.tS
 2tcloWoUfxW24sRBz_u73Cj9blpZkZjxfkjM43_oecDQGr9gOKckZGTAoPHp
 QiizwzCXMBdF4MMaVrQLo1rvvz2lotMt4bluDN8sS94HxEpOL9Q96TDW5_.y
 lFs8yo9q7dNmZs_4KNaKUDCrrNJwpSe32jqHvuGWMV6nBFaPYaQFdf_KoEdF
 4tfuQugIxPSBSke_yy7E-
X-Yahoo-SMTP: 3cEdYaqswBCV.ogjqp0uR59JyCnf
Message-ID: <54F8A232.9090602@yahoo.com>
Date: Thu, 05 Mar 2015 13:36:34 -0500
From: "M. Dale" <medale94@yahoo.com.INVALID>
Reply-To: medale@acm.org
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: =?UTF-8?B?IsOQzp7igqzPgUDSnCAo4LmPzK/NoeC5jyki?=
 <deepujain@gmail.com>, dev@spark.apache.org
Subject: Re: Fwd: Unable to Read/Write Avro RDD on cluster.
References: <CAPKgQ9o8ZzWspE_qXhbobM330s7xaskUoti78Gu73Qg3rNER=g@mail.gmail.com> <CAPKgQ9p+mo8NfF2CK8=ZVCo4wjekh0VVjAwdTdnihVooMhmTJg@mail.gmail.com>
In-Reply-To: <CAPKgQ9p+mo8NfF2CK8=ZVCo4wjekh0VVjAwdTdnihVooMhmTJg@mail.gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
X-Virus-Checked: Checked by ClamAV on apache.org

There was a avro-mapred version conflict described in 
https://issues.apache.org/jira/browse/SPARK-3039.
Fixed by https://github.com/apache/spark/pull/4315 for Spark 1.3.

Here is a link that describes how to fix Spark 1.2.1 for avro-mapred 
hadoop2: 
https://github.com/medale/spark-mail/blob/master/presentation/CreatingAvroMapred2Spark.md

We had built that 1.2.1 version for a demo for the February MD Apache 
Spark meetup (http://www.meetup.com/Apache-Spark-Maryland/) that was 
hosted at 
https://s3.amazonaws.com/morris-datasets/ENRON/demo/spark-1.2.1.tar.gz.

Hope this helps,
Markus

On 03/04/2015 11:57 PM, ÐΞ€ρ@Ҝ (๏̯͡๏) wrote:
> I am trying to read RDD avro, transform and write.
> I am able to run it locally fine but when i run onto cluster, i see issues
> with Avro.
>
>
> export SPARK_HOME=/home/dvasthimal/spark/spark-1.0.2-bin-2.4.1
> export SPARK_YARN_USER_ENV="CLASSPATH=/apache/hadoop/conf"
> export HADOOP_CONF_DIR=/apache/hadoop/conf
> export YARN_CONF_DIR=/apache/hadoop/conf
> export SPARK_JAR=$SPARK_HOME/lib/spark-assembly-1.0.2-hadoop2.4.1.jar
> export SPARK_LIBRARY_PATH=/apache/hadoop/lib/native
> export SPARK_YARN_USER_ENV="CLASSPATH=/apache/hadoop/conf"
> export SPARK_YARN_USER_ENV="CLASSPATH=/apache/hadoop/conf"
> export
> SPARK_CLASSPATH=/apache/hadoop/share/hadoop/common/hadoop-common-2.4.1-company-2.jar:/apache/hadoop/lib/hadoop-lzo-0.6.0.jar:/home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar:/home/dvasthimal/spark/avro-1.7.7.jar
> export SPARK_LIBRARY_PATH="/apache/hadoop/lib/native"
> export YARN_CONF_DIR=/apache/hadoop/conf/
>
> cd $SPARK_HOME
>
> ./bin/spark-submit --master yarn-cluster --jars
> /home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar,/home/dvasthimal/spark/avro-1.7.7.jar
> --num-executors 3 --driver-memory 4g --executor-memory 2g --executor-cores
> 1  --queue hdmi-spark --class com.company.ep.poc.spark.reporting.SparkApp
> /home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar
> startDate=2015-02-16 endDate=2015-02-16
> epoutputdirectory=/user/dvasthimal/epdatasets_small/exptsession
> subcommand=successevents
> outputdir=/user/dvasthimal/epdatasets/successdetail
>
> Spark assembly has been built with Hive, including Datanucleus jars on
> classpath
> 15/03/04 03:20:29 INFO client.ConfiguredRMFailoverProxyProvider: Failing
> over to rm2
> 15/03/04 03:20:30 INFO yarn.Client: Got Cluster metric info from
> ApplicationsManager (ASM), number of NodeManagers: 2221
> 15/03/04 03:20:30 INFO yarn.Client: Queue info ... queueName: hdmi-spark,
> queueCurrentCapacity: 0.7162806, queueMaxCapacity: 0.08,
>        queueApplicationCount = 7, queueChildQueueCount = 0
> 15/03/04 03:20:30 INFO yarn.Client: Max mem capabililty of a single
> resource in this cluster 16384
> 15/03/04 03:20:30 INFO yarn.Client: Preparing Local resources
> 15/03/04 03:20:30 WARN util.NativeCodeLoader: Unable to load native-hadoop
> library for your platform... using builtin-java classes where applicable
> 15/03/04 03:20:30 WARN hdfs.BlockReaderLocal: The short-circuit local reads
> feature cannot be used because libhadoop cannot be loaded.
>
>
> 15/03/04 03:20:46 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token
> 7780745 for dvasthimal on 10.115.206.112:8020
> 15/03/04 03:20:46 INFO yarn.Client: Uploading
> file:/home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar to hdfs://
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_1425075571333_61948/spark_reporting-1.0-SNAPSHOT.jar
> 15/03/04 03:20:47 INFO yarn.Client: Uploading
> file:/home/dvasthimal/spark/spark-1.0.2-bin-2.4.1/lib/spark-assembly-1.0.2-hadoop2.4.1.jar
> to hdfs://
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_1425075571333_61948/spark-assembly-1.0.2-hadoop2.4.1.jar
> 15/03/04 03:20:52 INFO yarn.Client: Uploading
> file:/home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar to hdfs://
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_1425075571333_61948/avro-mapred-1.7.7-hadoop2.jar
> 15/03/04 03:20:52 INFO yarn.Client: Uploading
> file:/home/dvasthimal/spark/avro-1.7.7.jar to hdfs://
> apollo-phx-nn.company.com:8020/user/dvasthimal/.sparkStaging/application_1425075571333_61948/avro-1.7.7.jar
> 15/03/04 03:20:54 INFO yarn.Client: Setting up the launch environment
> 15/03/04 03:20:54 INFO yarn.Client: Setting up container launch context
> 15/03/04 03:20:54 INFO yarn.Client: Command for starting the Spark
> ApplicationMaster: List($JAVA_HOME/bin/java, -server, -Xmx4096m,
> -Djava.io.tmpdir=$PWD/tmp,
> -Dspark.app.name=\"com.company.ep.poc.spark.reporting.SparkApp\",
>   -Dlog4j.configuration=log4j-spark-container.properties,
> org.apache.spark.deploy.yarn.ApplicationMaster, --class,
> com.company.ep.poc.spark.reporting.SparkApp, --jar ,
> file:/home/dvasthimal/spark/spark_reporting-1.0-SNAPSHOT.jar,  --args
>   'startDate=2015-02-16'  --args  'endDate=2015-02-16'  --args
>   'epoutputdirectory=/user/dvasthimal/epdatasets_small/exptsession'  --args
>   'subcommand=successevents'  --args
>   'outputdir=/user/dvasthimal/epdatasets/successdetail' , --executor-memory,
> 2048, --executor-cores, 1, --num-executors , 3, 1>, <LOG_DIR>/stdout, 2>,
> <LOG_DIR>/stderr)
> 15/03/04 03:20:54 INFO yarn.Client: Submitting application to ASM
> 15/03/04 03:20:54 INFO impl.YarnClientImpl: Submitted application
> application_1425075571333_61948
> 15/03/04 03:20:56 INFO yarn.Client: Application report from ASM:
>   application identifier: application_1425075571333_61948
>   appId: 61948
>   clientToAMToken: null
>   appDiagnostics:
>   appMasterHost: N/A
>   appQueue: hdmi-spark
>   appMasterRpcPort: -1
>   appStartTime: 1425464454263
>   yarnAppState: ACCEPTED
>   distributedFinalState: UNDEFINED
>   appTrackingUrl:
> https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333_61948/
>   appUser: dvasthimal
> 15/03/04 03:21:18 INFO yarn.Client: Application report from ASM:
>   application identifier: application_1425075571333_61948
>   appId: 61948
>   clientToAMToken: Token { kind: YARN_CLIENT_TOKEN, service:  }
>   appDiagnostics:
>   appMasterHost: phxaishdc9dn0169.phx.company.com
>   appQueue: hdmi-spark
>   appMasterRpcPort: 0
>   appStartTime: 1425464454263
>   yarnAppState: RUNNING
>   distributedFinalState: UNDEFINED
>   appTrackingUrl:
> https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333_61948/
>   appUser: dvasthimal
> ….
> ….
> 15/03/04 03:21:22 INFO yarn.Client: Application report from ASM:
>   application identifier: application_1425075571333_61948
>   appId: 61948
>   clientToAMToken: Token { kind: YARN_CLIENT_TOKEN, service:  }
>   appDiagnostics:
>   appMasterHost: phxaishdc9dn0169.phx.company.com
>   appQueue: hdmi-spark
>   appMasterRpcPort: 0
>   appStartTime: 1425464454263
>   yarnAppState: FINISHED
>   distributedFinalState: FAILED
>   appTrackingUrl:
> https://apollo-phx-rm-2.company.com:50030/proxy/application_1425075571333_61948/A
>   appUser: dvasthimal
>
>
>
> AM failed with following exception
>
> /apache/hadoop/bin/yarn logs -applicationId application_1425075571333_61948
> 15/03/04 03:21:22 INFO NewHadoopRDD: Input split: hdfs://
> apollo-phx-nn.company.com:8020/user/dvasthimal/epdatasets_small/exptsession/2015/02/16/part-r-00000.avro:0+13890
> 15/03/04 03:21:22 ERROR Executor: Exception in task ID 3
> java.lang.IncompatibleClassChangeError: Found interface
> org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
> at
> org.apache.avro.mapreduce.AvroKeyInputFormat.createRecordReader(AvroKeyInputFormat.java:47)
> at org.apache.spark.rdd.NewHadoopRDD$$anon$1.<init>(NewHadoopRDD.scala:111)
> at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:99)
> at org.apache.spark.rdd.NewHadoopRDD.compute(NewHadoopRDD.scala:61)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.FlatMappedRDD.compute(FlatMappedRDD.scala:33)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
> at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
> at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
> at org.apache.spark.scheduler.Task.run(Task.scala:51)
> at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> at java.lang.Thread.run(Thread.java:745)
>
>
>
> 1) Having figured out the error the fix would be to put the right version
> of avro libs into AM JVM classpath. Hence i included --jars
> /home/dvasthimal/spark/avro-mapred-1.7.7-hadoop2.jar,/home/dvasthimal/spark/avro-1.7.7.jar
> in spark-submit command. However i still see the same exception.
> 2) I tried to include these libs in SPARK_CLASSPATH. However i see the same
> exception.
>
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 21:09:45 2015
Return-Path: <dev-return-11873-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1496610A84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 21:09:45 +0000 (UTC)
Received: (qmail 79108 invoked by uid 500); 5 Mar 2015 21:09:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79040 invoked by uid 500); 5 Mar 2015 21:09:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79028 invoked by uid 99); 5 Mar 2015 21:09:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 21:09:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of irashid@cloudera.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 21:09:38 +0000
Received: by widem10 with SMTP id em10so39197906wid.0
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 13:09:17 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=lNXpz4EAtC80yeVq11L9KWeTCClYVkdqCYd3ceqPn70=;
        b=fJaVDehFtQjPyYOWj62Z6w9Pe6tU9UfvmrVdivhh2XBWv056l6udfPvXIHuZUvMY36
         nE9FWoVWsp8bmaXkNyB9VYCVq2zg5wbI/M3k98IrXw3gIgQ4JBZbdwFYOmO74h4obmq9
         ys3hFOK11+Ye3P1qlRfKwtne3m9PcOHAhsT3Ihp8/B09pxwgEEP8qDseaZYxrQCw46Zk
         cjwE0kZL2hA6vj2Kt11x33zrQrQwO4NwfPIkFNb0Qr1+OdIaNIZbW9Je0R1zqUzloCDJ
         hqPGtJA1bb+MpOUp9IfPxWZXck6mmpKT/FOXl1lAbwx2iALPNYTPgiEM6nkEUoW0edtF
         cnJg==
X-Gm-Message-State: ALoCoQndZYwV4/UxoIITweQkasoTrQwKnZyXp7/Opf5fpIQQB9KyWhWRlw47JSZ3yZ7BH7sD5Omp
X-Received: by 10.194.172.9 with SMTP id ay9mr22614828wjc.2.1425589757179;
 Thu, 05 Mar 2015 13:09:17 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.62.197 with HTTP; Thu, 5 Mar 2015 13:08:56 -0800 (PST)
In-Reply-To: <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com> <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Thu, 5 Mar 2015 15:08:56 -0600
Message-ID: <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01184dd6129071051090fb38
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184dd6129071051090fb38
Content-Type: text/plain; charset=UTF-8

I have a very strong dislike for #1 (scala enumerations).   I'm ok with #4
(with Xiangrui's final suggestion, especially making it sealed & available
in Java), but I really think #2, java enums, are the best option.

Java enums actually have some very real advantages over the other
approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There has
been endless debate in the Scala community about the problems with the
approaches in Scala.  Very smart, level-headed Scala gurus have complained
about their short-comings (Rex Kerr's name is coming to mind, though I'm
not positive about that); there have been numerous well-thought out
proposals to give Scala a better enum.  But the powers-that-be in Scala
always reject them.  IIRC the explanation for rejecting is basically that
(a) enums aren't important enough for introducing some new special feature,
scala's got bigger things to work on and (b) if you really need a good
enum, just use java's enum.

I doubt it really matters that much for Spark internals, which is why I
think #4 is fine.  But I figured I'd give my spiel, because every developer
loves language wars :)

Imran



On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> `case object` inside an `object` doesn't show up in Java. This is the
> minimal code I found to make everything show up correctly in both
> Scala and Java:
>
> sealed abstract class StorageLevel // cannot be a trait
>
> object StorageLevel {
>   private[this] case object _MemoryOnly extends StorageLevel
>   final val MemoryOnly: StorageLevel = _MemoryOnly
>
>   private[this] case object _DiskOnly extends StorageLevel
>   final val DiskOnly: StorageLevel = _DiskOnly
> }
>
> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > I like #4 as well and agree with Aaron's suggestion.
> >
> > - Patrick
> >
> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >> I'm cool with #4 as well, but make sure we dictate that the values
> should
> >> be defined within an object with the same name as the enumeration (like
> we
> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
> >>
> >> e.g. we SHOULD do:
> >>
> >> trait StorageLevel
> >> object StorageLevel {
> >>   case object MemoryOnly extends StorageLevel
> >>   case object DiskOnly extends StorageLevel
> >> }
> >>
> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
> michael@databricks.com>
> >> wrote:
> >>
> >>> #4 with a preference for CamelCaseEnums
> >>>
> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
> >>> wrote:
> >>>
> >>> > another vote for #4
> >>> > People are already used to adding "()" in Java.
> >>> >
> >>> >
> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
> >>> wrote:
> >>> >
> >>> > > #4 but with MemoryOnly (more scala-like)
> >>> > >
> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
> >>> > >
> >>> > > Constants, Values, Variable and Methods
> >>> > >
> >>> > > Constant names should be in upper camel case. That is, if the
> member is
> >>> > > final, immutable and it belongs to a package object or an object,
> it
> >>> may
> >>> > be
> >>> > > considered a constant (similar to Java'sstatic final members):
> >>> > >
> >>> > >
> >>> > >    1. object Container {
> >>> > >    2.     val MyConstant = ...
> >>> > >    3. }
> >>> > >
> >>> > >
> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> >>> > >
> >>> > > > Hi all,
> >>> > > >
> >>> > > > There are many places where we use enum-like types in Spark, but
> in
> >>> > > > different ways. Every approach has both pros and cons. I wonder
> >>> > > > whether there should be an "official" approach for enum-like
> types in
> >>> > > > Spark.
> >>> > > >
> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
> >>> > > >
> >>> > > > * All types show up as Enumeration.Value in Java.
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
> >>> > > >
> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
> >>> > > >
> >>> > > > * Implementation must be in a Java file.
> >>> > > > * Values doesn't show up in the ScalaDoc:
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
> >>> > > >
> >>> > > > 3. Static fields in Java (e.g., TripletFields)
> >>> > > >
> >>> > > > * Implementation must be in a Java file.
> >>> > > > * Doesn't need "()" in Java code.
> >>> > > > * Values don't show up in the ScalaDoc:
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
> >>> > > >
> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
> >>> > > >
> >>> > > > * Needs "()" in Java code.
> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
> >>> > > >
> >>> > > > It would be great if we have an "official" approach for this as
> well
> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
> >>> thoughts?
> >>> > > >
> >>> > > > Best,
> >>> > > > Xiangrui
> >>> > > >
> >>> > > >
> ---------------------------------------------------------------------
> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
> >>> > > >
> >>> > > >
> >>> > >
> >>> >
> >>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e01184dd6129071051090fb38--

From dev-return-11874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 21:27:55 2015
Return-Path: <dev-return-11874-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 12C5B10B24
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 21:27:55 +0000 (UTC)
Received: (qmail 21901 invoked by uid 500); 5 Mar 2015 21:27:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21827 invoked by uid 500); 5 Mar 2015 21:27:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21816 invoked by uid 99); 5 Mar 2015 21:27:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 21:27:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 21:27:28 +0000
Received: by qgdz107 with SMTP id z107so8133311qgd.4
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 13:27:06 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=3Yj9PlO+dnml/906aPEqA/N1/KmHBSIQ+YpnHMbIhZ4=;
        b=GyQFOEDng56eQrwt2TylUFZtzA/+CYccV38VDX3caQ9WiBbwQohLSdfVsf3JA+Z/PC
         IuZA42Qa9Yf7Wg5yWMlgEowBWhOmIyA7CVR3xiDAnWPviRfRqIUSCHgroKIM/FhGMZhQ
         TlxBYbLhhnvroqQV6H7G3m2gJD7wtNWxVPp/ApYP/MHmrjrEpzphDlhjEh7Ylw/jCKQT
         zagUCd22Dwll5bvgP7gDCtaqFQOZ6sR3L2P7Ge+9mlUKCaV7XVGF3YpYqzSU7ApyWrIp
         w7464j2pH4jDysnb2v86fTZGZ0jCzAKhKukyRV+r42Yh0cpX64G14eWZwFJjPDBzr6GS
         UgBA==
X-Gm-Message-State: ALoCoQnCQixMIFh/LkjHL17CVP40WmEQDPCS+0Rv4Wt6uNk39QRIlMxf7Pg5guHTQ/z4hbNgty7X
X-Received: by 10.229.95.74 with SMTP id c10mr15724559qcn.17.1425590826706;
 Thu, 05 Mar 2015 13:27:06 -0800 (PST)
MIME-Version: 1.0
Received: by 10.96.109.9 with HTTP; Thu, 5 Mar 2015 13:26:46 -0800 (PST)
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 5 Mar 2015 13:26:46 -0800
Message-ID: <CAPh_B=b3Lgu0Z-qZrrLuRGMEkEFLQ3U4zGZFjnzmZHM_dwq49w@mail.gmail.com>
Subject: over 10000 commits!
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11344c86d243320510913ae8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11344c86d243320510913ae8
Content-Type: text/plain; charset=UTF-8

We reached a new milestone today.

https://github.com/apache/spark


10,001 commits now. Congratulations to Xiangrui for making the 10000th
commit!

--001a11344c86d243320510913ae8--

From dev-return-11875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar  5 21:35:21 2015
Return-Path: <dev-return-11875-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4475A10B6E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  5 Mar 2015 21:35:21 +0000 (UTC)
Received: (qmail 42936 invoked by uid 500); 5 Mar 2015 21:35:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42857 invoked by uid 500); 5 Mar 2015 21:35:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42839 invoked by uid 99); 5 Mar 2015 21:35:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 21:35:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.180 as permitted sender)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 05 Mar 2015 21:34:55 +0000
Received: by lbvp9 with SMTP id p9so18788299lbv.10
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 13:34:53 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Ou5C/L7p5UOpauGApKyI1/7hZJ0YVhP4Ja+MaSxMy0k=;
        b=Vfw9KvWJzoXQ+0PjZ3thqP4EQzKDUvydSIBkDiqtrMlnyYOAjiGmJKgGkVh1hT2kpd
         u18ooSrru1VMDcnJzB4TwmnK1iqSIVPDGKBNL2q5enytwx3QHt3GCbehbVEXf/ID0Y+l
         DqVGIXa6OS24gGk9VZWUN7ZnpCwxkrSm+8hyE3fmW+6TSAiW0vAB4eAOjeomX5pO7E+1
         HiFeL5kW3hBpeZd0650rIFhLfRP7Rd/t4mU/39Ucd2xVUUWw9vk0laoa+rsjBfXOP5LZ
         CCxjQegA5lSUGxvk6CZUrtzvsNTx45sUHOkaNif7zv07TuhX3R9Bs+3q1xcxFMbUALhJ
         R3MQ==
X-Gm-Message-State: ALoCoQmMnwzB/6tHpxF1niD4YjB3mcjVyXVEW1+FT73zaKJnOLthhjjhIx7bj79kWpeVXw/ist9s
X-Received: by 10.152.87.199 with SMTP id ba7mr4520475lab.75.1425591293662;
 Thu, 05 Mar 2015 13:34:53 -0800 (PST)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Thu, 5 Mar 2015 13:34:33 -0800 (PST)
In-Reply-To: <CAPh_B=b3Lgu0Z-qZrrLuRGMEkEFLQ3U4zGZFjnzmZHM_dwq49w@mail.gmail.com>
References: <CAPh_B=b3Lgu0Z-qZrrLuRGMEkEFLQ3U4zGZFjnzmZHM_dwq49w@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 5 Mar 2015 13:34:33 -0800
Message-ID: <CACdU-dQ_D96jq1naeaAt-kVJ6Hi33BhbVmCBjDh58NcJskAPJQ@mail.gmail.com>
Subject: Re: over 10000 commits!
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c34e46a7677c051091565a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c34e46a7677c051091565a
Content-Type: text/plain; charset=UTF-8

WOOT!

On Thu, Mar 5, 2015 at 1:26 PM, Reynold Xin <rxin@databricks.com> wrote:

> We reached a new milestone today.
>
> https://github.com/apache/spark
>
>
> 10,001 commits now. Congratulations to Xiangrui for making the 10000th
> commit!
>

--001a11c34e46a7677c051091565a--

From dev-return-11876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 02:53:48 2015
Return-Path: <dev-return-11876-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DE31017C5B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 02:53:48 +0000 (UTC)
Received: (qmail 76608 invoked by uid 500); 6 Mar 2015 02:53:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76534 invoked by uid 500); 6 Mar 2015 02:53:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76522 invoked by uid 99); 6 Mar 2015 02:53:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 02:53:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 02:53:21 +0000
Received: by obcuz6 with SMTP id uz6so18197321obc.12
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 18:52:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Pl1wgEKMgH//DBwQfAxTiptbYBqEcNE/bIyZzr6Js74=;
        b=YUEvmjj9Bd/FBnW0xvzmSmejiqXSiIKojQx9NAJb29xG+AvW2J9XFlScLbw5gKg1uU
         Gq5bu4OSDA70Fbn+wnVZh+WJ7NudKdkH2Vp8jum9KJPA74xfqUgz/28wrJrEmT+ea5IS
         Y96SLX9dVhjYfP7Y+cspssNPUpO1y+5/NqduwDI4S42YVfMhLVW3CaX4WGHt7DhCRQvt
         S+1yUwFBK0rDKgF3c9YiYf+GEWrGbqzbRN/UrJTz2TQqxyfahvT2vop7mmfbyKOMkn7+
         qXf8tF8DlSi1mMBiNvC1AFDjwnfx3/HFKiIDPa2peYOdpX+G9HXXSMZ731H2TDrWXx23
         uDbA==
MIME-Version: 1.0
X-Received: by 10.202.1.200 with SMTP id 191mr8915829oib.82.1425610354659;
 Thu, 05 Mar 2015 18:52:34 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Thu, 5 Mar 2015 18:52:34 -0800 (PST)
Date: Thu, 5 Mar 2015 18:52:34 -0800
Message-ID: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.3.0!

The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

Staging repositories for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1078

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/

Please vote on releasing this package as Apache Spark 1.3.0!

The vote is open until Monday, March 09, at 02:52 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== How does this compare to RC2 ==
This release includes the following bug fixes:

https://issues.apache.org/jira/browse/SPARK-6144
https://issues.apache.org/jira/browse/SPARK-6171
https://issues.apache.org/jira/browse/SPARK-5143
https://issues.apache.org/jira/browse/SPARK-6182
https://issues.apache.org/jira/browse/SPARK-6175

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.2 workload and running on this release candidate,
then reporting any regressions.

If you are happy with this release based on your own testing, give a +1 vote.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.3 QA period,
so -1 votes should only occur for significant regressions from 1.2.1.
Bugs already present in 1.2.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 02:55:01 2015
Return-Path: <dev-return-11877-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B2EAB17C64
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 02:55:01 +0000 (UTC)
Received: (qmail 78795 invoked by uid 500); 6 Mar 2015 02:55:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78707 invoked by uid 500); 6 Mar 2015 02:55:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78643 invoked by uid 99); 6 Mar 2015 02:54:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 02:54:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.180 as permitted sender)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 02:54:55 +0000
Received: by obcwp4 with SMTP id wp4so11283854obc.4
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 18:52:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=LtR/Rh80DnkH6Oti5TwKxdNYS0oWT+wkjiJeDlGWnAY=;
        b=yu2akzRGmE+YwzDeE1EGK4WG2YsIqgb+Tf3sI30LB2Yi7FQgBlpW+cFx4/aDDkZXUX
         PFESCpgkllq3fJeMyZvvBsvgHqagYjdFZN1fCtAjJ9PBvXTUMt2f9rP/y15sPBY04POt
         2SF0TWNmrvc7uI53UEaO/p8Ttogae3jfjMwI7e6HijOMFOa+iiIPnw9QMVi7biBHsIDY
         rmqMjY0HIayumyTYG/y8kOaVFR+orhNHqUTvlJb3imxDlAHmFLKinLlbSaL9R1LTU7+1
         XNUey3mqvabs8+IMEbTxL6MSZUIYMaLfOVvC8W5f7N5I7z6Xx4Q4CCmoqprQT75Wwnlc
         ZRKg==
MIME-Version: 1.0
X-Received: by 10.182.117.226 with SMTP id kh2mr1057659obb.15.1425610339479;
 Thu, 05 Mar 2015 18:52:19 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Thu, 5 Mar 2015 18:52:19 -0800 (PST)
Date: Thu, 5 Mar 2015 18:52:19 -0800
Message-ID: <CABPQxstmQBrCPmFZ07dtOSSUBYf-J+6EybNz7KCo1s-6JVTNGQ@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.3.0 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote is cancelled in favor of RC3.

On Wed, Mar 4, 2015 at 3:22 PM, Sean Owen <sowen@cloudera.com> wrote:
> I think we will have to fix
> https://issues.apache.org/jira/browse/SPARK-5143 as well before the
> final 1.3.x.
>
> But yes everything else checks out for me, including sigs and hashes
> and building the source release.
>
> I have been following JIRA closely and am not aware of other blockers
> besides the ones already identified.
>
> On Wed, Mar 4, 2015 at 7:09 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
>> -1 (non-binding) because of SPARK-6144.
>>
>> But aside from that I ran a set of tests on top of standalone and yarn
>> and things look good.
>>
>> On Tue, Mar 3, 2015 at 8:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>>>
>>> The tag to be voted on is v1.3.0-rc2 (commit 3af2687):
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3af26870e5163438868c4eb2df88380a533bb232
>>>
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~pwendell/spark-1.3.0-rc2/
>>>
>>> Release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/pwendell.asc
>>>
>>> Staging repositories for this release can be found at:
>>> https://repository.apache.org/content/repositories/orgapachespark-1074/
>>> (published with version '1.3.0')
>>> https://repository.apache.org/content/repositories/orgapachespark-1075/
>>> (published with version '1.3.0-rc2')
>>>
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~pwendell/spark-1.3.0-rc2-docs/
>>>
>>> Please vote on releasing this package as Apache Spark 1.3.0!
>>>
>>> The vote is open until Saturday, March 07, at 04:17 UTC and passes if
>>> a majority of at least 3 +1 PMC votes are cast.
>>>
>>> [ ] +1 Release this package as Apache Spark 1.3.0
>>> [ ] -1 Do not release this package because ...
>>>
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>
>>> == How does this compare to RC1 ==
>>> This patch includes a variety of bug fixes found in RC1.
>>>
>>> == How can I help test this release? ==
>>> If you are a Spark user, you can help us test this release by
>>> taking a Spark 1.2 workload and running on this release candidate,
>>> then reporting any regressions.
>>>
>>> If you are happy with this release based on your own testing, give a +1 vote.
>>>
>>> == What justifies a -1 vote for this release? ==
>>> This vote is happening towards the end of the 1.3 QA period,
>>> so -1 votes should only occur for significant regressions from 1.2.1.
>>> Bugs already present in 1.2.X, minor regressions, or bugs related
>>> to new features will not block this release.
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>
>>
>>
>> --
>> Marcelo
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 04:01:16 2015
Return-Path: <dev-return-11878-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1D5717DC5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 04:01:15 +0000 (UTC)
Received: (qmail 51381 invoked by uid 500); 6 Mar 2015 04:01:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51294 invoked by uid 500); 6 Mar 2015 04:01:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51282 invoked by uid 99); 6 Mar 2015 04:01:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 04:01:13 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.223.171 as permitted sender)
Received: from [209.85.223.171] (HELO mail-ie0-f171.google.com) (209.85.223.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 04:00:49 +0000
Received: by iecar1 with SMTP id ar1so82594268iec.0
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 19:58:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=9fSTTYYNgSX3iZ+r5MYohLD6Q1MdBaFSomUAEItPFMM=;
        b=qEhh2mneudH6Y5asJ6XmgugPJ0aCgGzFXg3k+UeeBtF2tJnP5ktpXkmQqgYQfJiRc0
         TpnI2eCpsqk94gJ1uOYKP24kldGZQbS/Us3nr/xSAwrQInVMzEr0tWBN6ADlE2bCISU2
         3GcMoS3RK6fJIhhpDv/9kb5+wPZSt0Whj4UsgwOcxItK3hBUwSuXif8gs8bGJ8mMsMWk
         jpiI4mbgHSwYVEY41P9lI5FwV9oOc+Begq7m4iZKJ/iU8zXSk7r2w4n0Qy0oM7MIqbwY
         x1tbAAQLalkKheH2DzLrT/firsx7rqKBF8Ace+oen5BrYcm52WMgYT+XyKKt9LuOVNke
         0P7g==
MIME-Version: 1.0
X-Received: by 10.50.25.225 with SMTP id f1mr24886092igg.29.1425614312457;
 Thu, 05 Mar 2015 19:58:32 -0800 (PST)
Received: by 10.36.99.76 with HTTP; Thu, 5 Mar 2015 19:58:32 -0800 (PST)
In-Reply-To: <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
Date: Thu, 5 Mar 2015 19:58:32 -0800
Message-ID: <CAJgQjQ-NW2Enya+G_SFs-a-vS1jZ82bu_Ef1ALBtuLtfpWmArw@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: Imran Rashid <irashid@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

For #4, my previous proposal may confuse the IDEs with additional
types generated by the case objects, and their toString contain the
underscore. The following works better:

sealed abstract class StorageLevel

object StorageLevel {
  final val MemoryOnly: StorageLevel = {
    case object MemoryOnly extends StorageLevel
    MemoryOnly
  }

  final val DiskOnly: StorageLevel = {
    case object DiskOnly extends StorageLevel
    DiskOnly
 }
}

MemoryOnly and DiskOnly can be used in pattern matching. If people are
okay with this approach, I can add it to the code style guide.

Imran, this is not just for internal APIs, which are relatively more
flexible. It is good to use the same approach to implement public
enum-like types from now on.

Best,
Xiangrui

On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com> wrote:
> I have a very strong dislike for #1 (scala enumerations).   I'm ok with #4
> (with Xiangrui's final suggestion, especially making it sealed & available
> in Java), but I really think #2, java enums, are the best option.
>
> Java enums actually have some very real advantages over the other
> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There has
> been endless debate in the Scala community about the problems with the
> approaches in Scala.  Very smart, level-headed Scala gurus have complained
> about their short-comings (Rex Kerr's name is coming to mind, though I'm
> not positive about that); there have been numerous well-thought out
> proposals to give Scala a better enum.  But the powers-that-be in Scala
> always reject them.  IIRC the explanation for rejecting is basically that
> (a) enums aren't important enough for introducing some new special feature,
> scala's got bigger things to work on and (b) if you really need a good
> enum, just use java's enum.
>
> I doubt it really matters that much for Spark internals, which is why I
> think #4 is fine.  But I figured I'd give my spiel, because every developer
> loves language wars :)
>
> Imran
>
>
>
> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> `case object` inside an `object` doesn't show up in Java. This is the
>> minimal code I found to make everything show up correctly in both
>> Scala and Java:
>>
>> sealed abstract class StorageLevel // cannot be a trait
>>
>> object StorageLevel {
>>   private[this] case object _MemoryOnly extends StorageLevel
>>   final val MemoryOnly: StorageLevel = _MemoryOnly
>>
>>   private[this] case object _DiskOnly extends StorageLevel
>>   final val DiskOnly: StorageLevel = _DiskOnly
>> }
>>
>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > I like #4 as well and agree with Aaron's suggestion.
>> >
>> > - Patrick
>> >
>> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>> wrote:
>> >> I'm cool with #4 as well, but make sure we dictate that the values
>> should
>> >> be defined within an object with the same name as the enumeration (like
>> we
>> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
>> >>
>> >> e.g. we SHOULD do:
>> >>
>> >> trait StorageLevel
>> >> object StorageLevel {
>> >>   case object MemoryOnly extends StorageLevel
>> >>   case object DiskOnly extends StorageLevel
>> >> }
>> >>
>> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>> michael@databricks.com>
>> >> wrote:
>> >>
>> >>> #4 with a preference for CamelCaseEnums
>> >>>
>> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
>> >>> wrote:
>> >>>
>> >>> > another vote for #4
>> >>> > People are already used to adding "()" in Java.
>> >>> >
>> >>> >
>> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>> >>> wrote:
>> >>> >
>> >>> > > #4 but with MemoryOnly (more scala-like)
>> >>> > >
>> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
>> >>> > >
>> >>> > > Constants, Values, Variable and Methods
>> >>> > >
>> >>> > > Constant names should be in upper camel case. That is, if the
>> member is
>> >>> > > final, immutable and it belongs to a package object or an object,
>> it
>> >>> may
>> >>> > be
>> >>> > > considered a constant (similar to Java'sstatic final members):
>> >>> > >
>> >>> > >
>> >>> > >    1. object Container {
>> >>> > >    2.     val MyConstant = ...
>> >>> > >    3. }
>> >>> > >
>> >>> > >
>> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>> >>> > >
>> >>> > > > Hi all,
>> >>> > > >
>> >>> > > > There are many places where we use enum-like types in Spark, but
>> in
>> >>> > > > different ways. Every approach has both pros and cons. I wonder
>> >>> > > > whether there should be an "official" approach for enum-like
>> types in
>> >>> > > > Spark.
>> >>> > > >
>> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>> >>> > > >
>> >>> > > > * All types show up as Enumeration.Value in Java.
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>> >>> > > >
>> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>> >>> > > >
>> >>> > > > * Implementation must be in a Java file.
>> >>> > > > * Values doesn't show up in the ScalaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>> >>> > > >
>> >>> > > > 3. Static fields in Java (e.g., TripletFields)
>> >>> > > >
>> >>> > > > * Implementation must be in a Java file.
>> >>> > > > * Doesn't need "()" in Java code.
>> >>> > > > * Values don't show up in the ScalaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>> >>> > > >
>> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>> >>> > > >
>> >>> > > > * Needs "()" in Java code.
>> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>> >>> > > >
>> >>> > > > It would be great if we have an "official" approach for this as
>> well
>> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
>> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>> >>> thoughts?
>> >>> > > >
>> >>> > > > Best,
>> >>> > > > Xiangrui
>> >>> > > >
>> >>> > > >
>> ---------------------------------------------------------------------
>> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 04:49:43 2015
Return-Path: <dev-return-11879-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EF24117EF8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 04:49:42 +0000 (UTC)
Received: (qmail 97708 invoked by uid 500); 6 Mar 2015 04:49:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97627 invoked by uid 500); 6 Mar 2015 04:49:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97615 invoked by uid 99); 6 Mar 2015 04:49:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 04:49:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 04:49:03 +0000
Received: by qgea108 with SMTP id a108so10261975qge.8
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 20:49:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lEN3yiAClX6gXvxRln5CYhmtW3wobvw48QaOuHeMaU0=;
        b=09ojRyghHGix6PQIPO3WMJ27ST3xSycxAWA7r0ArcExyhM+dkwwXgrg6MEJh5Eim5a
         w1DdhiHT45QwfWyWE/7S4ayY5BYtofA0QUPPZDc/wDqTlHuRO2XERug/N6Qkyk+wjGaS
         57KYRXw/pJW0WVOgpDJTCW8socdgiDVGz6U/KYk96aN8auiAghCQOKcxNFX71zVc9SoP
         bMUlaPYFACGbKt+4xcFXGkUjE7Ddkj6zCJ43j2SEDPlgdFzpp8KL54hvWAs0Nf+qQnTD
         AmfuOwZ5oBkpmqa+FymFu3/SiudRPSBmpC+VSpOhloKtRaVcUymPYBFjiXikqyTDSlvI
         qGWA==
MIME-Version: 1.0
X-Received: by 10.140.38.100 with SMTP id s91mr17034564qgs.37.1425617342238;
 Thu, 05 Mar 2015 20:49:02 -0800 (PST)
Received: by 10.140.33.131 with HTTP; Thu, 5 Mar 2015 20:49:02 -0800 (PST)
In-Reply-To: <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
Date: Thu, 5 Mar 2015 20:49:02 -0800
Message-ID: <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Mridul Muralidharan <mridul@gmail.com>
To: Imran Rashid <irashid@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

  I have a strong dislike for java enum's due to the fact that they
are not stable across JVM's - if it undergoes serde, you end up with
unpredictable results at times [1].
One of the reasons why we prevent enum's from being key : though it is
highly possible users might depend on it internally and shoot
themselves in the foot.

Would be better to keep away from them in general and use something more stable.

Regards,
Mridul

[1] Having had to debug this issue for 2 weeks - I really really hate it.


On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com> wrote:
> I have a very strong dislike for #1 (scala enumerations).   I'm ok with #4
> (with Xiangrui's final suggestion, especially making it sealed & available
> in Java), but I really think #2, java enums, are the best option.
>
> Java enums actually have some very real advantages over the other
> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There has
> been endless debate in the Scala community about the problems with the
> approaches in Scala.  Very smart, level-headed Scala gurus have complained
> about their short-comings (Rex Kerr's name is coming to mind, though I'm
> not positive about that); there have been numerous well-thought out
> proposals to give Scala a better enum.  But the powers-that-be in Scala
> always reject them.  IIRC the explanation for rejecting is basically that
> (a) enums aren't important enough for introducing some new special feature,
> scala's got bigger things to work on and (b) if you really need a good
> enum, just use java's enum.
>
> I doubt it really matters that much for Spark internals, which is why I
> think #4 is fine.  But I figured I'd give my spiel, because every developer
> loves language wars :)
>
> Imran
>
>
>
> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> `case object` inside an `object` doesn't show up in Java. This is the
>> minimal code I found to make everything show up correctly in both
>> Scala and Java:
>>
>> sealed abstract class StorageLevel // cannot be a trait
>>
>> object StorageLevel {
>>   private[this] case object _MemoryOnly extends StorageLevel
>>   final val MemoryOnly: StorageLevel = _MemoryOnly
>>
>>   private[this] case object _DiskOnly extends StorageLevel
>>   final val DiskOnly: StorageLevel = _DiskOnly
>> }
>>
>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > I like #4 as well and agree with Aaron's suggestion.
>> >
>> > - Patrick
>> >
>> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>> wrote:
>> >> I'm cool with #4 as well, but make sure we dictate that the values
>> should
>> >> be defined within an object with the same name as the enumeration (like
>> we
>> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
>> >>
>> >> e.g. we SHOULD do:
>> >>
>> >> trait StorageLevel
>> >> object StorageLevel {
>> >>   case object MemoryOnly extends StorageLevel
>> >>   case object DiskOnly extends StorageLevel
>> >> }
>> >>
>> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>> michael@databricks.com>
>> >> wrote:
>> >>
>> >>> #4 with a preference for CamelCaseEnums
>> >>>
>> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
>> >>> wrote:
>> >>>
>> >>> > another vote for #4
>> >>> > People are already used to adding "()" in Java.
>> >>> >
>> >>> >
>> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>> >>> wrote:
>> >>> >
>> >>> > > #4 but with MemoryOnly (more scala-like)
>> >>> > >
>> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
>> >>> > >
>> >>> > > Constants, Values, Variable and Methods
>> >>> > >
>> >>> > > Constant names should be in upper camel case. That is, if the
>> member is
>> >>> > > final, immutable and it belongs to a package object or an object,
>> it
>> >>> may
>> >>> > be
>> >>> > > considered a constant (similar to Java'sstatic final members):
>> >>> > >
>> >>> > >
>> >>> > >    1. object Container {
>> >>> > >    2.     val MyConstant = ...
>> >>> > >    3. }
>> >>> > >
>> >>> > >
>> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>> >>> > >
>> >>> > > > Hi all,
>> >>> > > >
>> >>> > > > There are many places where we use enum-like types in Spark, but
>> in
>> >>> > > > different ways. Every approach has both pros and cons. I wonder
>> >>> > > > whether there should be an "official" approach for enum-like
>> types in
>> >>> > > > Spark.
>> >>> > > >
>> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>> >>> > > >
>> >>> > > > * All types show up as Enumeration.Value in Java.
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>> >>> > > >
>> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>> >>> > > >
>> >>> > > > * Implementation must be in a Java file.
>> >>> > > > * Values doesn't show up in the ScalaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>> >>> > > >
>> >>> > > > 3. Static fields in Java (e.g., TripletFields)
>> >>> > > >
>> >>> > > > * Implementation must be in a Java file.
>> >>> > > > * Doesn't need "()" in Java code.
>> >>> > > > * Values don't show up in the ScalaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>> >>> > > >
>> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>> >>> > > >
>> >>> > > > * Needs "()" in Java code.
>> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>> >>> > > >
>> >>> > > > It would be great if we have an "official" approach for this as
>> well
>> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
>> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>> >>> thoughts?
>> >>> > > >
>> >>> > > > Best,
>> >>> > > > Xiangrui
>> >>> > > >
>> >>> > > >
>> ---------------------------------------------------------------------
>> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>> >>> > > >
>> >>> > > >
>> >>> > >
>> >>> >
>> >>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 08:02:21 2015
Return-Path: <dev-return-11880-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 78610105BA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 08:02:21 +0000 (UTC)
Received: (qmail 70347 invoked by uid 500); 6 Mar 2015 08:02:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70277 invoked by uid 500); 6 Mar 2015 08:02:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70265 invoked by uid 99); 6 Mar 2015 08:02:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 08:02:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 08:01:54 +0000
Received: by oiba3 with SMTP id a3so16377212oib.7
        for <dev@spark.apache.org>; Thu, 05 Mar 2015 23:59:37 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=aagwGV98D88JHOtg8F3KY212gM+njcpo47YR85nUJOw=;
        b=tcHvyTvvSO9LKdyb5wEoEKZiHK4yLCPVnfUrLQUQnbk7RYCWgDp9T/kTus4MeoQNBs
         cPntzGflrz6vnrKzTDae761LjTYM285m54xbTBF/ARe4VJyfxsuy3e7zyANo9Ui5CUz+
         aLNFWMGJ1SW4JhgmRKvXQvbgpuNSg8le44IC9WpnAjWdKCpyGLeFlnwO+eUW1tKvzx6+
         meWjR3tiKG6BsBI06R6N+94wdEYEV7Pv8jClBeAOwRYOL3J5X/oTRhHcr9IMnmHMXxTw
         61fXHLrlHh368tO9taGRIHeQCPQfDHGxufBdXyAwZ64yBbIn4s7k9cxZUfX52F+Kgg/z
         a53A==
MIME-Version: 1.0
X-Received: by 10.182.66.1 with SMTP id b1mr180607obt.14.1425628777328; Thu,
 05 Mar 2015 23:59:37 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Thu, 5 Mar 2015 23:59:37 -0800 (PST)
In-Reply-To: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
Date: Thu, 5 Mar 2015 23:59:37 -0800
Message-ID: <CABPQxsv_ruAopSgubeB4aNRgiEQ-SrYwZYAYjp1Pm-UrgDq1PA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'll kick it off with a +1.

On Thu, Mar 5, 2015 at 6:52 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1078
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == How does this compare to RC2 ==
> This release includes the following bug fixes:
>
> https://issues.apache.org/jira/browse/SPARK-6144
> https://issues.apache.org/jira/browse/SPARK-6171
> https://issues.apache.org/jira/browse/SPARK-5143
> https://issues.apache.org/jira/browse/SPARK-6182
> https://issues.apache.org/jira/browse/SPARK-6175
>
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1 vote.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 09:06:09 2015
Return-Path: <dev-return-11881-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D8546107FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 09:06:09 +0000 (UTC)
Received: (qmail 84921 invoked by uid 500); 6 Mar 2015 09:06:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84841 invoked by uid 500); 6 Mar 2015 09:06:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84830 invoked by uid 99); 6 Mar 2015 09:06:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 09:06:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 09:05:40 +0000
Received: by wesx3 with SMTP id x3so4733446wes.1
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 01:04:54 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=rsmxS57lS02twmBc9l95hL8yLqWfaPg6cJ8m28lwkfE=;
        b=Od55TEUQooECeFQgpgXeIGEeGOkX8zswBCo2vn+OlprtYDnkm2ItfWASzpu9KhgZZF
         phkbJX2/0P1a7CUDZsnaFBR9ydPqv4wsYgVwaQ48EhtKzOaIfSSG8AAgeQmW75C3BJWo
         cXx/xvfjlznqyhPRr67gbIqFXGoc5pGWec2ApuQhMUEDHcFwUPX/57ZgdA5zbgGpCKCk
         Pxn6EZFNoH111qEtNc6zy2LLD9SuCssPbIXhSwLKDhSiB78tJv/92G5R0bnpusA8iFmI
         F4d8BkqqZ05et3f5lJOkU/pPPljjNK2g6NLbsV6ogcDSbq5c+JjtVRGRixEtEzDTlKYd
         z5WQ==
X-Gm-Message-State: ALoCoQl8CTaIawfh7yKWHT06us6TO6owgE8YrcsBXVrrdfk0WcbXZohM5ni1ZPrvXhWmNIoLQSpY
X-Received: by 10.194.21.193 with SMTP id x1mr26876830wje.144.1425632694347;
 Fri, 06 Mar 2015 01:04:54 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 6 Mar 2015 01:04:34 -0800 (PST)
In-Reply-To: <CAJgQjQ-NW2Enya+G_SFs-a-vS1jZ82bu_Ef1ALBtuLtfpWmArw@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com> <CAJgQjQ-NW2Enya+G_SFs-a-vS1jZ82bu_Ef1ALBtuLtfpWmArw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 6 Mar 2015 09:04:34 +0000
Message-ID: <CAMAsSdLUYVvJnqFUVqruUJ_8yP-kXiHGhOEixNjwPnpvjAUrDA@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Imran Rashid <irashid@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

This has some disadvantage for Java, I think. You can't switch on an
object defined like this, but you can with an enum. And although the
scala compiler understands that the set of values is fixed because of
'sealed' and so can warn about missing cases, the JVM won't know this,
and can't do the same.

On Fri, Mar 6, 2015 at 3:58 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> For #4, my previous proposal may confuse the IDEs with additional
> types generated by the case objects, and their toString contain the
> underscore. The following works better:
>
> sealed abstract class StorageLevel
>
> object StorageLevel {
>   final val MemoryOnly: StorageLevel = {
>     case object MemoryOnly extends StorageLevel
>     MemoryOnly
>   }
>
>   final val DiskOnly: StorageLevel = {
>     case object DiskOnly extends StorageLevel
>     DiskOnly
>  }
> }
>
> MemoryOnly and DiskOnly can be used in pattern matching. If people are
> okay with this approach, I can add it to the code style guide.
>
> Imran, this is not just for internal APIs, which are relatively more
> flexible. It is good to use the same approach to implement public
> enum-like types from now on.
>
> Best,
> Xiangrui
>
> On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com> wrote:
>> I have a very strong dislike for #1 (scala enumerations).   I'm ok with #4
>> (with Xiangrui's final suggestion, especially making it sealed & available
>> in Java), but I really think #2, java enums, are the best option.
>>
>> Java enums actually have some very real advantages over the other
>> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There has
>> been endless debate in the Scala community about the problems with the
>> approaches in Scala.  Very smart, level-headed Scala gurus have complained
>> about their short-comings (Rex Kerr's name is coming to mind, though I'm
>> not positive about that); there have been numerous well-thought out
>> proposals to give Scala a better enum.  But the powers-that-be in Scala
>> always reject them.  IIRC the explanation for rejecting is basically that
>> (a) enums aren't important enough for introducing some new special feature,
>> scala's got bigger things to work on and (b) if you really need a good
>> enum, just use java's enum.
>>
>> I doubt it really matters that much for Spark internals, which is why I
>> think #4 is fine.  But I figured I'd give my spiel, because every developer
>> loves language wars :)
>>
>> Imran
>>
>>
>>
>> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>>> `case object` inside an `object` doesn't show up in Java. This is the
>>> minimal code I found to make everything show up correctly in both
>>> Scala and Java:
>>>
>>> sealed abstract class StorageLevel // cannot be a trait
>>>
>>> object StorageLevel {
>>>   private[this] case object _MemoryOnly extends StorageLevel
>>>   final val MemoryOnly: StorageLevel = _MemoryOnly
>>>
>>>   private[this] case object _DiskOnly extends StorageLevel
>>>   final val DiskOnly: StorageLevel = _DiskOnly
>>> }
>>>
>>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>> > I like #4 as well and agree with Aaron's suggestion.
>>> >
>>> > - Patrick
>>> >
>>> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>>> wrote:
>>> >> I'm cool with #4 as well, but make sure we dictate that the values
>>> should
>>> >> be defined within an object with the same name as the enumeration (like
>>> we
>>> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
>>> >>
>>> >> e.g. we SHOULD do:
>>> >>
>>> >> trait StorageLevel
>>> >> object StorageLevel {
>>> >>   case object MemoryOnly extends StorageLevel
>>> >>   case object DiskOnly extends StorageLevel
>>> >> }
>>> >>
>>> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>>> michael@databricks.com>
>>> >> wrote:
>>> >>
>>> >>> #4 with a preference for CamelCaseEnums
>>> >>>
>>> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
>>> >>> wrote:
>>> >>>
>>> >>> > another vote for #4
>>> >>> > People are already used to adding "()" in Java.
>>> >>> >
>>> >>> >
>>> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>>> >>> wrote:
>>> >>> >
>>> >>> > > #4 but with MemoryOnly (more scala-like)
>>> >>> > >
>>> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
>>> >>> > >
>>> >>> > > Constants, Values, Variable and Methods
>>> >>> > >
>>> >>> > > Constant names should be in upper camel case. That is, if the
>>> member is
>>> >>> > > final, immutable and it belongs to a package object or an object,
>>> it
>>> >>> may
>>> >>> > be
>>> >>> > > considered a constant (similar to Java'sstatic final members):
>>> >>> > >
>>> >>> > >
>>> >>> > >    1. object Container {
>>> >>> > >    2.     val MyConstant = ...
>>> >>> > >    3. }
>>> >>> > >
>>> >>> > >
>>> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>>> >>> > >
>>> >>> > > > Hi all,
>>> >>> > > >
>>> >>> > > > There are many places where we use enum-like types in Spark, but
>>> in
>>> >>> > > > different ways. Every approach has both pros and cons. I wonder
>>> >>> > > > whether there should be an "official" approach for enum-like
>>> types in
>>> >>> > > > Spark.
>>> >>> > > >
>>> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>>> >>> > > >
>>> >>> > > > * All types show up as Enumeration.Value in Java.
>>> >>> > > >
>>> >>> > > >
>>> >>> > >
>>> >>> >
>>> >>>
>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>>> >>> > > >
>>> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>>> >>> > > >
>>> >>> > > > * Implementation must be in a Java file.
>>> >>> > > > * Values doesn't show up in the ScalaDoc:
>>> >>> > > >
>>> >>> > > >
>>> >>> > >
>>> >>> >
>>> >>>
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>>> >>> > > >
>>> >>> > > > 3. Static fields in Java (e.g., TripletFields)
>>> >>> > > >
>>> >>> > > > * Implementation must be in a Java file.
>>> >>> > > > * Doesn't need "()" in Java code.
>>> >>> > > > * Values don't show up in the ScalaDoc:
>>> >>> > > >
>>> >>> > > >
>>> >>> > >
>>> >>> >
>>> >>>
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>>> >>> > > >
>>> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>>> >>> > > >
>>> >>> > > > * Needs "()" in Java code.
>>> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
>>> >>> > > >
>>> >>> > > >
>>> >>> > >
>>> >>> >
>>> >>>
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>>> >>> > > >
>>> >>> > > >
>>> >>> > >
>>> >>> >
>>> >>>
>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>>> >>> > > >
>>> >>> > > > It would be great if we have an "official" approach for this as
>>> well
>>> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY" or
>>> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>>> >>> thoughts?
>>> >>> > > >
>>> >>> > > > Best,
>>> >>> > > > Xiangrui
>>> >>> > > >
>>> >>> > > >
>>> ---------------------------------------------------------------------
>>> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>>> >>> > > >
>>> >>> > > >
>>> >>> > >
>>> >>> >
>>> >>>
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 09:42:06 2015
Return-Path: <dev-return-11882-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 01E5D1094C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 09:42:06 +0000 (UTC)
Received: (qmail 60229 invoked by uid 500); 6 Mar 2015 09:41:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60147 invoked by uid 500); 6 Mar 2015 09:41:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60130 invoked by uid 99); 6 Mar 2015 09:41:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 09:41:42 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [157.193.49.126] (HELO smtp2.ugent.be) (157.193.49.126)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 09:41:16 +0000
Received: from localhost (mcheck2.ugent.be [157.193.49.249])
	by smtp2.ugent.be (Postfix) with ESMTP id C881712C40C
	for <dev@spark.apache.org>; Fri,  6 Mar 2015 10:41:14 +0100 (CET)
X-Virus-Scanned: by UGent DICT
Received: from smtp2.ugent.be ([IPv6:::ffff:157.193.49.126])
	by localhost (mcheck2.UGent.be [::ffff:157.193.43.11]) (amavisd-new, port 10024)
	with ESMTP id 4zu5ysigmmqf for <dev@spark.apache.org>;
	Fri,  6 Mar 2015 10:41:14 +0100 (CET)
Received: from [157.193.44.241] (gast044a.ugent.be [157.193.44.241])
	(Authenticated sender: ehiggs)
	by smtp2.ugent.be (Postfix) with ESMTPSA id 3C58912C408
	for <dev@spark.apache.org>; Fri,  6 Mar 2015 10:41:14 +0100 (CET)
Message-ID: <54F9763A.6030503@ugent.be>
Date: Fri, 06 Mar 2015 10:41:14 +0100
From: Ewan Higgs <ewan.higgs@ugent.be>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Fwd: SparkSpark-perf terasort WIP branch
References: <54B67039.9070408@ugent.be>
In-Reply-To: <54B67039.9070408@ugent.be>
X-Forwarded-Message-Id: <54B67039.9070408@ugent.be>
Content-Type: multipart/alternative;
 boundary="------------060206050102050607040008"
X-Miltered: at jchkm3 with ID 54F9763A.000 by Joe's j-chkmail (http://helpdesk.ugent.be/email/)!
X-j-chkmail-Enveloppe: 54F9763A.000 from gast044a.ugent.be/gast044a.ugent.be/157.193.44.241/[157.193.44.241]/<ewan.higgs@ugent.be>
X-j-chkmail-Score: MSGID : 54F9763A.000 on smtp2.ugent.be : j-chkmail score : . : R=. U=. O=. B=0.000 -> S=0.000
X-j-chkmail-Status: Ham
X-Virus-Checked: Checked by ClamAV on apache.org

--------------060206050102050607040008
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

Hi all,
I never heard from anyone on this and have received emails in private 
that people would like to add terasort to their spark-perf installs so 
it becomes part of their cluster validation checks.

Yours,
Ewan

-------- Forwarded Message --------
Subject: 	SparkSpark-perf terasort WIP branch
Date: 	Wed, 14 Jan 2015 14:33:45 +0100
From: 	Ewan Higgs <ewan.higgs@ugent.be>
To: 	dev@spark.apache.org <dev@spark.apache.org>



Hi all,
I'm trying to build the Spark-perf WIP code but there are some errors to
do with Hadoop APIs. I presume this is because there is some Hadoop
version set and it's referring to that. But I can't seem to find it.

The errors are as follows:

[info] Compiling 15 Scala sources and 2 Java sources to
/home/ehiggs/src/spark-perf/spark-tests/target/scala-2.10/classes...
[error]
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraInputFormat.scala:40:
object task is not a member of package org.apache.hadoop.mapreduce
[error] import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
[error]                                    ^
[error]
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraInputFormat.scala:132:
not found: type TaskAttemptContextImpl
[error]             val context = new TaskAttemptContextImpl(
[error]                               ^
[error]
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraScheduler.scala:37:
object TTConfig is not a member of package
org.apache.hadoop.mapreduce.server.tasktracker
[error] import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig
[error]        ^
[error]
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraScheduler.scala:91:
not found: value TTConfig
[error]   var slotsPerHost : Int = conf.getInt(TTConfig.TT_MAP_SLOTS, 4)
[error]                                        ^
[error]
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraSortAll.scala:7:
value run is not a member of org.apache.spark.examples.terasort.TeraGen
[error]     tg.run(Array[String]("10M", "/tmp/terasort_in"))
[error]        ^
[error]
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraSortAll.scala:9:
value run is not a member of org.apache.spark.examples.terasort.TeraSort
[error]     ts.run(Array[String]("/tmp/terasort_in", "/tmp/terasort_out"))
[error]        ^
[error] 6 errors found
[error] (compile:compile) Compilation failed
[error] Total time: 13 s, completed 05-Jan-2015 12:21:47

I can build the same code if it's in the Spark tree using the following
command:
mvn -Dhadoop.version=2.5.0 -DskipTests=true install

Is there a way I can convince spark-perf to build this code with the
appropriate Hadoop library version? I tried to apply the following to
spark-tests/project/SparkTestsBuild.scala but it didn't seem to work as
I expected:

$ git diff project/SparkTestsBuild.scala
diff --git a/spark-tests/project/SparkTestsBuild.scala
b/spark-tests/project/SparkTestsBuild.scala
index 4116326..4ed5f0c 100644
--- a/spark-tests/project/SparkTestsBuild.scala
+++ b/spark-tests/project/SparkTestsBuild.scala
@@ -16,7 +16,9 @@ object SparkTestsBuild extends Build {
           "org.scalatest" %% "scalatest" % "2.2.1" % "test",
           "com.google.guava" % "guava" % "14.0.1",
           "org.apache.spark" %% "spark-core" % "1.0.0" % "provided",
-        "org.json4s" %% "json4s-native" % "3.2.9"
+        "org.json4s" %% "json4s-native" % "3.2.9",
+        "org.apache.hadoop" % "hadoop-common" % "2.5.0",
+        "org.apache.hadoop" % "hadoop-mapreduce" % "2.5.0"
         ),
         test in assembly := {},
         outputPath in assembly :=
file("target/spark-perf-tests-assembly.jar"),
@@ -36,4 +38,4 @@ object SparkTestsBuild extends Build {
           case _ => MergeStrategy.first
         }
       ))
-}
\ No newline at end of file
+}


Yours,
Ewan




--------------060206050102050607040008--

From dev-return-11883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 14:04:06 2015
Return-Path: <dev-return-11883-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0AA1E174AC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 14:04:06 +0000 (UTC)
Received: (qmail 95805 invoked by uid 500); 6 Mar 2015 14:04:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95724 invoked by uid 500); 6 Mar 2015 14:04:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95713 invoked by uid 99); 6 Mar 2015 14:04:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 14:04:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 14:03:57 +0000
Received: by wiwl15 with SMTP id l15so3622552wiw.1
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 06:03:36 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Jn/P0oJ7JeFQTZk3HLigmM5lMfDCNXbUmVRMbvm3ONM=;
        b=lH/wtbGKivev5zQ9hn+581Pa9xLNaWK6HBT255KcaWZFEz/GjCvBSFMZhz3gSf/aLY
         gYWy2RNf/0pjIrr1rIGv6C7Y47gn3qe9IHIerEQ6BxCQoazH//BoVuGHRmBrCxw1lF4a
         7qMf43gRSOveGQemEgVLJem3GSufNJoHpvQthnh4k7jWWIPQjU51eQzCre1Hx8SdhQsk
         5vrirCyyChu4/2V5HYZtqMZ9fx3Kp3ktdUsZ0YeLhZrUGeZQE65VPVw6rFRQ0ncdkbMv
         5UPXHSA7BzFJpgcz7hORZfrLWtjzVLqX83iAiI3BOynC56hYRChXDdrauIjsGzs7Lar8
         3lgw==
X-Gm-Message-State: ALoCoQnoCW8wimfDPktK4zQPFiNZLoGLvsK6Yz7iLvF90a79ApcDd2EzO/VcX8ANiAr1wCwomNZ/
X-Received: by 10.180.91.47 with SMTP id cb15mr33304275wib.39.1425650616418;
 Fri, 06 Mar 2015 06:03:36 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 6 Mar 2015 06:03:16 -0800 (PST)
In-Reply-To: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 6 Mar 2015 14:03:16 +0000
Message-ID: <CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

There are still three JIRAs marked as blockers for 1.3.0:

SPARK-5310 Update SQL programming guide for 1.3
SPARK-5183 Document data source API
SPARK-6128 Update Spark Streaming Guide for Spark 1.3

As a matter of hygiene, let's either mark them resolved if they're
resolved, or push them / deprioritize them.


Signatures look good, source compiles with a Hadoop-2.6 + YARN +
Hive-flavored build, for me.


On OS X and Ubuntu, I still observe the same test failure as in the
first RC, but agree this isn't a blocker:

UISeleniumSuite:
*** RUN ABORTED ***
  java.lang.NoClassDefFoundError: org/w3c/dom/ElementTraversal
  ...


On both, I also see a few Hive tests fail, like the following:

- udf_std *** FAILED ***
  Results do not match for udf_std:
  DESCRIBE FUNCTION EXTENDED std
  == Parsed Logical Plan ==
  HiveNativeCommand DESCRIBE FUNCTION EXTENDED std

  == Analyzed Logical Plan ==
  HiveNativeCommand DESCRIBE FUNCTION EXTENDED std

  == Optimized Logical Plan ==
  HiveNativeCommand DESCRIBE FUNCTION EXTENDED std

  == Physical Plan ==
  ExecutedCommand (HiveNativeCommand DESCRIBE FUNCTION EXTENDED std)

  Code Generation: false
  == RDD ==
  result
  !== HIVE - 2 row(s) ==                                         ==
CATALYST - 2 row(s) ==
   std(x) - Returns the standard deviation of a set of numbers
std(x) - Returns the standard deviation of a set of numbers
  !Synonyms: stddev_pop, stddev
Synonyms: stddev, stddev_pop (HiveComparisonTest.scala:384)


Before I give a +1 I wanted to see if anyone sees these test failures
too, and/or believes they're ignorable for some reason. I also want to
resolve the open blocker JIRAs.


On Fri, Mar 6, 2015 at 2:52 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1078
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == How does this compare to RC2 ==
> This release includes the following bug fixes:
>
> https://issues.apache.org/jira/browse/SPARK-6144
> https://issues.apache.org/jira/browse/SPARK-6171
> https://issues.apache.org/jira/browse/SPARK-5143
> https://issues.apache.org/jira/browse/SPARK-6182
> https://issues.apache.org/jira/browse/SPARK-6175
>
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1 vote.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 18:07:32 2015
Return-Path: <dev-return-11884-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 447A417EF7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 18:07:32 +0000 (UTC)
Received: (qmail 28647 invoked by uid 500); 6 Mar 2015 18:07:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28577 invoked by uid 500); 6 Mar 2015 18:07:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28564 invoked by uid 99); 6 Mar 2015 18:07:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 18:07:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hector.yee@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 18:07:06 +0000
Received: by obcuz6 with SMTP id uz6so22139445obc.12
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 10:05:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=7te9LP8qHCmbn3v3mykewfSd9e6xFNm90aD67ALtpSE=;
        b=E3XOj7jPS1Cy6oP5cN6MI46rfXmOEU8gjYqz5lFIUZ1g2y5bRRYXkapPehi2s9VRwQ
         uToHjawkgNXfDp7rmQyBnXNjZkiv8RdLOcglWWYX6u1VKHXRubKqYYamYT0E4PG2nFAy
         aVshxNG9vy3atITIqH8gBtP5VMLsOgDWAIniAUgOUFyYkscapUZOtzFdHetgK8KfV7lX
         DunCqb1+tCNiOwF8yWj9Njv2r0JMTQBPxasbaWyneCvfIzacveRLXspd7jsRsAmCaDwS
         rRbcDC9+gQ5gUxNtK5p62pXF1100VeVTmNgQY3vhvv4oDwZUDBd5AWRxOjVXeRDmlJ+y
         lfLw==
X-Received: by 10.60.48.35 with SMTP id i3mr11898298oen.39.1425665134605; Fri,
 06 Mar 2015 10:05:34 -0800 (PST)
MIME-Version: 1.0
Received: by 10.202.51.139 with HTTP; Fri, 6 Mar 2015 10:05:14 -0800 (PST)
In-Reply-To: <CACdU-dQ_D96jq1naeaAt-kVJ6Hi33BhbVmCBjDh58NcJskAPJQ@mail.gmail.com>
References: <CAPh_B=b3Lgu0Z-qZrrLuRGMEkEFLQ3U4zGZFjnzmZHM_dwq49w@mail.gmail.com>
 <CACdU-dQ_D96jq1naeaAt-kVJ6Hi33BhbVmCBjDh58NcJskAPJQ@mail.gmail.com>
From: Hector Yee <hector.yee@gmail.com>
Date: Fri, 6 Mar 2015 10:05:14 -0800
Message-ID: <CAPi87hev7TYRfMHGhTN7coPqhjJ+=iQoh9XCKUhCak_k5=nbKg@mail.gmail.com>
Subject: Re: over 10000 commits!
To: shane knapp <sknapp@berkeley.edu>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135f1e6eaba490510a28724
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135f1e6eaba490510a28724
Content-Type: text/plain; charset=UTF-8

Congrats!

On Thu, Mar 5, 2015 at 1:34 PM, shane knapp <sknapp@berkeley.edu> wrote:

> WOOT!
>
> On Thu, Mar 5, 2015 at 1:26 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > We reached a new milestone today.
> >
> > https://github.com/apache/spark
> >
> >
> > 10,001 commits now. Congratulations to Xiangrui for making the 10000th
> > commit!
> >
>



-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*

--001a1135f1e6eaba490510a28724--

From dev-return-11885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 18:44:19 2015
Return-Path: <dev-return-11885-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5932B10047
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 18:44:19 +0000 (UTC)
Received: (qmail 29003 invoked by uid 500); 6 Mar 2015 18:44:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28929 invoked by uid 500); 6 Mar 2015 18:44:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28917 invoked by uid 99); 6 Mar 2015 18:44:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 18:44:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 18:44:11 +0000
Received: by obcvb8 with SMTP id vb8so18407361obc.10
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 10:43:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=QQVU4pWGuWxnY71mN7d/xt+HGE+lC78dZdZnKvGb8Uw=;
        b=pKfVz7m4HtVEA/TFviuajGBmIAorW1bZz/UpAlnhfuarPHApPdJIrxJJYuzVC5knza
         xcBW59HDkgC9y6xUjoSDcygZhytXFzjN9s6Ytg0O0GdJIzOhkCZ/P4Cc+pO0aDJwGXsw
         lk8JeG9KHhjQoDzwFyaQBfPM1t9gQO0urSMsca4xwe/L9qLi3VNKCk/10+A9ECyUpFFr
         +Vm8KUqTRhO9KBgR9Bua6xaic5+RNApnKtNEY9cngIUgQNuRddT6XDbNQaXxrrYV2ffe
         xPFbEkZ7XQOJHzWEq0MHVNXfsiEkams9JfN942vY/+h1xMBAAhnoodqaI6nRl9roRp/2
         J5Jw==
MIME-Version: 1.0
X-Received: by 10.182.66.1 with SMTP id b1mr2050578obt.14.1425667385945; Fri,
 06 Mar 2015 10:43:05 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Fri, 6 Mar 2015 10:43:05 -0800 (PST)
In-Reply-To: <CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
Date: Fri, 6 Mar 2015 10:43:05 -0800
Message-ID: <CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Sean,

> SPARK-5310 Update SQL programming guide for 1.3
> SPARK-5183 Document data source API
> SPARK-6128 Update Spark Streaming Guide for Spark 1.3

For these, the issue is that they are documentation JIRA's, which
don't need to be timed exactly with the release vote, since we can
update the documentation on the website whenever we want. In the past
I've just mentally filtered these out when considering RC's. I see a
few options here:

1. We downgrade such issues away from Blocker (more clear, but we risk
loosing them in the fray if they really are things we want to have
before the release is posted).
2. We provide a filter to the community that excludes 'Documentation'
issues and shows all other blockers for 1.3. We can put this on the
wiki, for instance.

Which do you prefer?

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 19:02:58 2015
Return-Path: <dev-return-11886-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9D5B910140
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 19:02:58 +0000 (UTC)
Received: (qmail 94843 invoked by uid 500); 6 Mar 2015 19:02:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94760 invoked by uid 500); 6 Mar 2015 19:02:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94749 invoked by uid 99); 6 Mar 2015 19:02:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 19:02:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 19:02:28 +0000
Received: by wesw55 with SMTP id w55so7965660wes.3
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 11:02:27 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Fp0lTQxXKUvuiUAf4iYQe/VvnlSKYptUBNdvGzeDuKw=;
        b=Tdc6N3eSypwDeaeLovvvbwe8ZwazD+JlNaeoB0SokNBzPeKgT0aVQcrHLJP8hRQfjr
         6R8GaTl2AeKEv50pFOqUUOOml4Vg3zMaLUKMpkZBvbd+3yFf4ZhqOV32xWpYg1GzJe/f
         JOIt9WhYumd4ptl7zT+CCkSsyX7VUVwzZIrXF60nC2F+SZqFBb8QvDCuzSMaEOziZ7+C
         9u8wHCEz+wF2gsx5GUd1cqbeaslfGSDyRILrTxjbaNsIw3zdaN91dhW9G/Ev9s6y6X/q
         enTm512F5vy48Ouxtp6lyB7dUPFGFAjNayfDI6FH2Slki6sMf7OdWzUuimF/bOHeQYNk
         PDbA==
X-Gm-Message-State: ALoCoQnYC7iO8GvfXPDl175JXo6IpsrDwYp43sHa1TL3aXfdnGJTWw1EvMxCT+lMq0Cs06HIlxbo
X-Received: by 10.194.134.169 with SMTP id pl9mr30812924wjb.67.1425668546876;
 Fri, 06 Mar 2015 11:02:26 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 6 Mar 2015 11:02:06 -0800 (PST)
In-Reply-To: <CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com> <CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 6 Mar 2015 19:02:06 +0000
Message-ID: <CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Given the title and tagging, it sounds like there could be some
must-have doc changes to go with what is being released as 1.3. It can
be finished later, and published later, but then the docs source
shipped with the release doesn't match the site, and until then, 1.3
is released without some "must-have" docs for 1.3 on the site.

The real question to me is: are there any further, absolutely
essential doc changes that need to accompany 1.3 or not?

If not, just resolve these. If there are, then it seems like the
release has to block on them. If there are some docs that should have
gone in for 1.3, but didn't, but aren't essential, well I suppose it
bears thinking about how to not slip as much work, but it doesn't
block.

I think Documentation issues certainly can be a blocker and shouldn't
be specially ignored.


BTW the UISeleniumSuite issue is a real failure, but I do not think it
is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
a regression from 1.2.x, but only affects tests, and only affects a
subset of build profiles.




On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Sean,
>
>> SPARK-5310 Update SQL programming guide for 1.3
>> SPARK-5183 Document data source API
>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
>
> For these, the issue is that they are documentation JIRA's, which
> don't need to be timed exactly with the release vote, since we can
> update the documentation on the website whenever we want. In the past
> I've just mentally filtered these out when considering RC's. I see a
> few options here:
>
> 1. We downgrade such issues away from Blocker (more clear, but we risk
> loosing them in the fray if they really are things we want to have
> before the release is posted).
> 2. We provide a filter to the community that excludes 'Documentation'
> issues and shows all other blockers for 1.3. We can put this on the
> wiki, for instance.
>
> Which do you prefer?
>
> - Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 19:24:26 2015
Return-Path: <dev-return-11887-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED1B3102D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 19:24:26 +0000 (UTC)
Received: (qmail 68471 invoked by uid 500); 6 Mar 2015 19:24:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68395 invoked by uid 500); 6 Mar 2015 19:24:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68384 invoked by uid 99); 6 Mar 2015 19:24:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 19:24:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 19:24:20 +0000
Received: by iecrl12 with SMTP id rl12so5415396iec.8
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 11:22:29 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=lgujCIHP4VwNWWqobCKJeWZPrSlYKIvqVv1Ekg5Fptk=;
        b=l6WoO2q1UsJdZr6ya52NNrXPuLLT5ZrEKx3DxlD3TzGJjV7OXCJTN9ZlDhCUYCs1xZ
         9kNPuQfm7EdlRVIl8iUYfIYLGhU4tKz8z2vTlO85+Hbl0Di4HmZf31FpsthMbaiHtw9I
         lFq3OEJ1Xd5BD6HOpijqNUNivaobD//W2SXIH3Fs3PA8BZXYtIYYtiitbbgXvMz2Nvso
         5Vq9h/zVTMhDU1x8e73NjKnjQ6Gs9RJKvMzxOebk/0cbEz8zPwxEgnsixGGp0KUysM7S
         WO9MWyzU4e2N1MhYm4uE5bTwpRPd7m8raU22nQLw0qonL9tcAIM2gVDceaykxZ8Ec4OF
         Wktg==
X-Gm-Message-State: ALoCoQn6w0mbtx4TApEky9PUWXVwm4oL3Xhyve5j0R6GM0Feuwb2F828lGhE8JDV2eqO8bPbCNaK
MIME-Version: 1.0
X-Received: by 10.107.138.232 with SMTP id c101mr3341008ioj.47.1425669749322;
 Fri, 06 Mar 2015 11:22:29 -0800 (PST)
Received: by 10.36.120.198 with HTTP; Fri, 6 Mar 2015 11:22:29 -0800 (PST)
In-Reply-To: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
Date: Fri, 6 Mar 2015 11:22:29 -0800
Message-ID: <CAAOnQ7sbcTAR51LY-RAS6Uufpm9aZgm0LmvDBrLWEyVpdE2JzA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 (non-binding, doc issues aside)

Ran batch of tests against yarn and standalone, including tests for
rc2 blockers, all looks fine.

On Thu, Mar 5, 2015 at 6:52 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1078
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == How does this compare to RC2 ==
> This release includes the following bug fixes:
>
> https://issues.apache.org/jira/browse/SPARK-6144
> https://issues.apache.org/jira/browse/SPARK-6171
> https://issues.apache.org/jira/browse/SPARK-5143
> https://issues.apache.org/jira/browse/SPARK-6182
> https://issues.apache.org/jira/browse/SPARK-6175
>
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1 vote.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 20:01:59 2015
Return-Path: <dev-return-11888-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A090710646
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 20:01:59 +0000 (UTC)
Received: (qmail 83394 invoked by uid 500); 6 Mar 2015 20:01:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83322 invoked by uid 500); 6 Mar 2015 20:01:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83310 invoked by uid 99); 6 Mar 2015 20:01:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 20:01:57 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 20:01:52 +0000
Received: by padfb1 with SMTP id fb1so44856698pad.7
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 12:01:32 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Swe1Ndaa6FDVO9EXvUd8kKJUhlAoesQiAb1pE4VL/Vc=;
        b=KBvFPnAAkFc4EsdS+yGTrEAcdTQ3pR8Q0hSFuJ7Dv4UJCP/lh8QbHdxVmE1D6DbuFa
         H1gsj9PWOlOxj0i0nKTeGHuEt5+FXum0V8l4YxHwLYPs9IQHDusMHdOiNWy/SREpsYSU
         qDP7qYN7UxM67LPYabQLYVMsUtB/T59NfLLvC6YEoeuNzbzF2gAIzYX6emgPbOwTdBAr
         7I+f5g8NfuJqFJE/D/KqMcXhV7RES+QlaO86H/51rFTs+bMrfsaMDOY9XjDNFbvhFyRp
         UWjAcLA7gmy0ZafMaoTCc3jaxO+A5XwIpSergtoP7WSjYQmBoIcMFNhjBFqBCMUQrWjc
         /81Q==
MIME-Version: 1.0
X-Received: by 10.68.234.164 with SMTP id uf4mr28095172pbc.37.1425672092127;
 Fri, 06 Mar 2015 12:01:32 -0800 (PST)
Received: by 10.70.19.130 with HTTP; Fri, 6 Mar 2015 12:01:32 -0800 (PST)
In-Reply-To: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
Date: Fri, 6 Mar 2015 12:01:32 -0800
Message-ID: <CAOTBr2=UiHFmvHh=Xf329iX7DEhxT3hcUMP4c0LrodhNXut=8A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Krishna Sankar <ksankar42@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b33960b9e193a0510a4264a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b33960b9e193a0510a4264a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 13:55 min
     mvn clean package -Pyarn -Dyarn.version=3D2.6.0 -Phadoop-2.4
-Dhadoop.version=3D2.6.0 -Phive -DskipTests -Dscala-2.11
2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
1.2.x
   pyspark works well with the new iPython 3.0.0 release
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
     Note: But MSE has increased from 40.81 (1.2.x) to 105.86 (1.3.0).
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
       Center And Scale OK
       Note : WSSSE has come down slightly
2.5. RDD operations OK
      State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
       Model evaluation/optimization (rank, numIter, lambda) with itertools
OK
3. Scala - MLlib
3.1. statistics (min,max,mean,Pearson,Spearman) OK
3.2. LinearRegressionWithSGD OK
3.3. Decision Tree OK
3.4. KMeans OK
3.5. Recommendation (Movielens medium dataset ~1 M ratings) OK
4.0. Spark SQL from Python OK
4.1. result =3D sqlContext.sql("SELECT * from people WHERE State =3D 'WA'")=
 OK
5.0  Good work on introducing DataFrames. Didn=E2=80=99t test DataFrames. W=
ill add
test cases for next release.

Cheers
<k/>

On Thu, Mar 5, 2015 at 6:52 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
>
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D4aaf=
48d46d13129f0f9bdafd771dd80fe568a7dc
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1078
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> =3D=3D How does this compare to RC2 =3D=3D
> This release includes the following bug fixes:
>
> https://issues.apache.org/jira/browse/SPARK-6144
> https://issues.apache.org/jira/browse/SPARK-6171
> https://issues.apache.org/jira/browse/SPARK-5143
> https://issues.apache.org/jira/browse/SPARK-6182
> https://issues.apache.org/jira/browse/SPARK-6175
>
> =3D=3D How can I help test this release? =3D=3D
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1
> vote.
>
> =3D=3D What justifies a -1 vote for this release? =3D=3D
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b33960b9e193a0510a4264a--

From dev-return-11889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 20:57:47 2015
Return-Path: <dev-return-11889-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58B9C1086E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 20:57:47 +0000 (UTC)
Received: (qmail 80416 invoked by uid 500); 6 Mar 2015 20:57:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80332 invoked by uid 500); 6 Mar 2015 20:57:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80320 invoked by uid 99); 6 Mar 2015 20:57:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 20:57:45 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.240.92.67] (HELO g9t5009.houston.hp.com) (15.240.92.67)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 20:57:17 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5009.houston.hp.com (Postfix) with ESMTPS id 6EF8F20F
	for <dev@spark.apache.org>; Fri,  6 Mar 2015 20:56:44 +0000 (UTC)
Received: from G9W3616.americas.hpqcorp.net (16.216.186.51) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Fri, 6 Mar 2015 20:55:19 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G9W3616.americas.hpqcorp.net ([16.216.186.51]) with mapi id 14.03.0169.001;
 Fri, 6 Mar 2015 20:55:19 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: dev <dev@spark.apache.org>
Subject: Loading previously serialized object to Spark
Thread-Topic: Loading previously serialized object to Spark
Thread-Index: AdBYT9oKXKom1WKWRnOearMR4wGkng==
Date: Fri, 6 Mar 2015 20:55:18 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE1539EG4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE1539EG4W3292americas_
Content-Type: text/plain; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable

Hi,

I've implemented class MyClass in MLlib that does some operation on Labeled=
Point. MyClass extends serializable, so I can map this operation on data of=
 RDD[LabeledPoints], such as data.map(lp =3D> MyClass.operate(lp)). I write=
 this class in file with ObjectOutputStream.writeObject. Then I stop and re=
start Spark. I load this class from file with ObjectInputStream.readObject.=
asInstanceOf[MyClass]. When I try to map the same operation of this class t=
o RDD, Spark throws not serializable exception:
org.apache.spark.SparkException: Task not serializable
        at org.apache.spark.util.ClosureCleaner$.ensureSerializable(Closure=
Cleaner.scala:166)
        at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala=
:158)
        at org.apache.spark.SparkContext.clean(SparkContext.scala:1453)
        at org.apache.spark.rdd.RDD.map(RDD.scala:273)

Could you suggest why it throws this exception while MyClass is serializabl=
e by definition?

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE1539EG4W3292americas_--

From dev-return-11890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 21:18:01 2015
Return-Path: <dev-return-11890-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D38B1090F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 21:18:01 +0000 (UTC)
Received: (qmail 19847 invoked by uid 500); 6 Mar 2015 21:17:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19774 invoked by uid 500); 6 Mar 2015 21:17:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19762 invoked by uid 99); 6 Mar 2015 21:17:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 21:17:59 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 21:17:55 +0000
Received: by obcvb8 with SMTP id vb8so19250613obc.0
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 13:17:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=OeZGswUDRYyGxdVnNJaIlqFAxYSGrf/6C/rRtG5Xt+8=;
        b=HP5AXgCz87ffSXtl6e9y0uNsOMoUCDtgpQWFgSC0UT5TRuEmifiOXMb/cpxatTDU/t
         roIOp/vECNo1iX9pj9+8GIViMxo92qzT/v1DhNWDjnN239FiEBvYpZ8k/dJr27XpWu92
         3z93/Thy9tpS2PLbAw8UGHTmldzRDrjFiLj0uWQeJmDKg/vdql4HWcYjHUDy0Jb9ii+8
         iMFBCWrOY494BEc+YzELpv0b1/skNu5sPOnRLzEGCUs3C2nW5fxGSi4vjlsqFlHp93g4
         tSK6F34iBhmf0K1wKUTshj4LO4hxjxmaEB5LfmC3YkBCXaHZnSMpD/kWsJCU3/GGASe/
         Z7hQ==
MIME-Version: 1.0
X-Received: by 10.182.213.38 with SMTP id np6mr12437663obc.34.1425676654848;
 Fri, 06 Mar 2015 13:17:34 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Fri, 6 Mar 2015 13:17:34 -0800 (PST)
In-Reply-To: <CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
	<CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
	<CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
Date: Fri, 6 Mar 2015 13:17:34 -0800
Message-ID: <CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Sean,

The docs are distributed and consumed in a fundamentally different way
than Spark code itself. So we've always considered the "deadline" for
doc changes to be when the release is finally posted.

If there are small inconsistencies with the docs present in the source
code for that release tag, IMO that doesn't matter much since we don't
even distribute the docs with Spark's binary releases and virtually no
one builds and hosts the docs on their own (that I am aware of, at
least). Perhaps we can recommend if people want to build the doc
sources that they should always grab the head of the most recent
release branch, to set expectations accordingly.

In the past we haven't considered it worth holding up the release
process for the purpose of the docs. It just doesn't make sense since
they are consumed "as a service". If we decide to change this
convention, it would mean shipping our releases later, since we
could't pipeline the doc finalization with voting.

- Patrick

On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
> Given the title and tagging, it sounds like there could be some
> must-have doc changes to go with what is being released as 1.3. It can
> be finished later, and published later, but then the docs source
> shipped with the release doesn't match the site, and until then, 1.3
> is released without some "must-have" docs for 1.3 on the site.
>
> The real question to me is: are there any further, absolutely
> essential doc changes that need to accompany 1.3 or not?
>
> If not, just resolve these. If there are, then it seems like the
> release has to block on them. If there are some docs that should have
> gone in for 1.3, but didn't, but aren't essential, well I suppose it
> bears thinking about how to not slip as much work, but it doesn't
> block.
>
> I think Documentation issues certainly can be a blocker and shouldn't
> be specially ignored.
>
>
> BTW the UISeleniumSuite issue is a real failure, but I do not think it
> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
> a regression from 1.2.x, but only affects tests, and only affects a
> subset of build profiles.
>
>
>
>
> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Hey Sean,
>>
>>> SPARK-5310 Update SQL programming guide for 1.3
>>> SPARK-5183 Document data source API
>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
>>
>> For these, the issue is that they are documentation JIRA's, which
>> don't need to be timed exactly with the release vote, since we can
>> update the documentation on the website whenever we want. In the past
>> I've just mentally filtered these out when considering RC's. I see a
>> few options here:
>>
>> 1. We downgrade such issues away from Blocker (more clear, but we risk
>> loosing them in the fray if they really are things we want to have
>> before the release is posted).
>> 2. We provide a filter to the community that excludes 'Documentation'
>> issues and shows all other blockers for 1.3. We can put this on the
>> wiki, for instance.
>>
>> Which do you prefer?
>>
>> - Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 21:23:23 2015
Return-Path: <dev-return-11891-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A7B11094D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 21:23:23 +0000 (UTC)
Received: (qmail 29068 invoked by uid 500); 6 Mar 2015 21:23:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28986 invoked by uid 500); 6 Mar 2015 21:23:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28975 invoked by uid 99); 6 Mar 2015 21:23:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 21:23:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 21:23:17 +0000
Received: by widem10 with SMTP id em10so6581876wid.1
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 13:21:51 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=GJiDEIokQFJIHdW2e+yVd0qy/lDFZODyaWVIJgMO+Ck=;
        b=Xz6O49Y2GqV3oPgdBqiVBo51ADbs4UXUWc9gzZFVKfspKiWJC88G6/c5e0BKbkOJ+b
         /rGl6jLKleD0M/KW2YbUt2cQqLWfQct2RxCok6aCsLtoP3217TWG6zxZMhPYw2f23fRF
         zsTnNciiKAImgkpUkQvHn0UmBwZS6GaVjKuzcZxnxd1fBD/+8KUHVrRWq05DHJWem24y
         9qeuqQiQfT2Tw1ACRvdLqhsJJbJyyVQ8AtiyQRyersC7fECGlb4JSx2C6KgIEZ1rSNro
         Vuck+Y9FEh/LI+2LY9GtoYO70homv7kk9dZXYsCxr4poItRjOmwdIPBRLyuc+MDh4jXc
         Y4WA==
X-Gm-Message-State: ALoCoQlAnpeg5Ks4VYJDrNyjHc3+BfvDMTpx3vFXDSy/J1DL1Jtyl7kc3zdg+pTXia4NkHC9j7Dr
X-Received: by 10.194.63.16 with SMTP id c16mr33589588wjs.117.1425676911320;
 Fri, 06 Mar 2015 13:21:51 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.93.227 with HTTP; Fri, 6 Mar 2015 13:21:21 -0800 (PST)
In-Reply-To: <CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
 <CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
 <CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com> <CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
From: Tathagata Das <tdas@databricks.com>
Date: Fri, 6 Mar 2015 13:21:21 -0800
Message-ID: <CA+AHuKm-bU10ySk8conVswDNUcYWbVgG9rhv9YpfLxOtHpTvyA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Patrick Wendell <pwendell@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b86da3add350a0510a54534
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86da3add350a0510a54534
Content-Type: text/plain; charset=UTF-8

To add to what Patrick said, the only reason that those JIRAs are marked as
Blockers (at least I can say for myself) is so that they are at the top of
the JIRA list signifying that these are more *immediate* issues than all
the Critical issues. To make it less confusing for the community voting, we
can definitely add a filter that ignores Documentation issues from the JIRA
list.


On Fri, Mar 6, 2015 at 1:17 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Sean,
>
> The docs are distributed and consumed in a fundamentally different way
> than Spark code itself. So we've always considered the "deadline" for
> doc changes to be when the release is finally posted.
>
> If there are small inconsistencies with the docs present in the source
> code for that release tag, IMO that doesn't matter much since we don't
> even distribute the docs with Spark's binary releases and virtually no
> one builds and hosts the docs on their own (that I am aware of, at
> least). Perhaps we can recommend if people want to build the doc
> sources that they should always grab the head of the most recent
> release branch, to set expectations accordingly.
>
> In the past we haven't considered it worth holding up the release
> process for the purpose of the docs. It just doesn't make sense since
> they are consumed "as a service". If we decide to change this
> convention, it would mean shipping our releases later, since we
> could't pipeline the doc finalization with voting.
>
> - Patrick
>
> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
> > Given the title and tagging, it sounds like there could be some
> > must-have doc changes to go with what is being released as 1.3. It can
> > be finished later, and published later, but then the docs source
> > shipped with the release doesn't match the site, and until then, 1.3
> > is released without some "must-have" docs for 1.3 on the site.
> >
> > The real question to me is: are there any further, absolutely
> > essential doc changes that need to accompany 1.3 or not?
> >
> > If not, just resolve these. If there are, then it seems like the
> > release has to block on them. If there are some docs that should have
> > gone in for 1.3, but didn't, but aren't essential, well I suppose it
> > bears thinking about how to not slip as much work, but it doesn't
> > block.
> >
> > I think Documentation issues certainly can be a blocker and shouldn't
> > be specially ignored.
> >
> >
> > BTW the UISeleniumSuite issue is a real failure, but I do not think it
> > is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
> > a regression from 1.2.x, but only affects tests, and only affects a
> > subset of build profiles.
> >
> >
> >
> >
> > On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >> Hey Sean,
> >>
> >>> SPARK-5310 Update SQL programming guide for 1.3
> >>> SPARK-5183 Document data source API
> >>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
> >>
> >> For these, the issue is that they are documentation JIRA's, which
> >> don't need to be timed exactly with the release vote, since we can
> >> update the documentation on the website whenever we want. In the past
> >> I've just mentally filtered these out when considering RC's. I see a
> >> few options here:
> >>
> >> 1. We downgrade such issues away from Blocker (more clear, but we risk
> >> loosing them in the fray if they really are things we want to have
> >> before the release is posted).
> >> 2. We provide a filter to the community that excludes 'Documentation'
> >> issues and shows all other blockers for 1.3. We can put this on the
> >> wiki, for instance.
> >>
> >> Which do you prefer?
> >>
> >> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b86da3add350a0510a54534--

From dev-return-11892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar  6 21:37:54 2015
Return-Path: <dev-return-11892-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D0E55109F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  6 Mar 2015 21:37:54 +0000 (UTC)
Received: (qmail 68445 invoked by uid 500); 6 Mar 2015 21:37:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68375 invoked by uid 500); 6 Mar 2015 21:37:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68363 invoked by uid 99); 6 Mar 2015 21:37:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 21:37:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 06 Mar 2015 21:37:15 +0000
Received: by wesx3 with SMTP id x3so8652530wes.1
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 13:36:29 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=U6NG/M1vV7+INuFBZ0zl6YkQz55BTqFzpVJ44V8+Nhw=;
        b=lwuiIO44oVkBme+OrVncB94jSrIxy5rV3WpRI4sS2l8dThhta99k8wjdjhCbQvicVi
         WzbiOYHqREtqLJH3xN2isy/7gfqaa6iJ9dtffCFtWgVod3XkQi05eQKKjIYykast37Gn
         ewL3ceqcjrgUmM9yx19ZwYq+XhRnCy4KR7RkBxNUfo8QYLH7cFFYY7XzyzNbiCmKEwPu
         mdTl/3IWidEqYmXeDnEPMVuoOLSPOLfbMECiRL6RuMKFgyuVlW62DUR+k8W0zGVM9RN1
         RyK1t+HN2JJlel800tE466Ls4C90P3sCaxcIhTKpB22ZkaUyvUlkYws8jdo91h8oYTjA
         mZ7g==
X-Gm-Message-State: ALoCoQm0QiaS8Z+ub10sCrbx6u01975WX+8EYOOAvKnpuQEjzb1PIMiICtCW2j2y5WL6P0hB+ac/
X-Received: by 10.194.134.169 with SMTP id pl9mr31901440wjb.67.1425677789408;
 Fri, 06 Mar 2015 13:36:29 -0800 (PST)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 6 Mar 2015 13:36:08 -0800 (PST)
In-Reply-To: <CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
 <CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
 <CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com> <CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 6 Mar 2015 21:36:08 +0000
Message-ID: <CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Although the problem is small, especially if indeed the essential docs
changes are following just a couple days behind the final release, I
mean, why the rush if they're essential? wait a couple days, finish
them, make the release.

Answer is, I think these changes aren't actually essential given the
comment from tdas, so: just mark these Critical? (although ... they do
say they're changes for the 1.3 release, so kind of funny to get to
them for 1.3.x or 1.4, but that's not important now.)

I thought that Blocker really meant Blocker in this project, as I've
been encouraged to use it to mean "don't release without this." I
think we should use it that way. Just thinking of it as "extra
Critical" doesn't add anything. I don't think Documentation should be
special-cased as less important, and I don't think there's confusion
if Blocker means what it says, so I'd 'fix' that way.

If nobody sees the Hive failure I observed, and if we can just zap
those "Blockers" one way or the other, +1


On Fri, Mar 6, 2015 at 9:17 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Sean,
>
> The docs are distributed and consumed in a fundamentally different way
> than Spark code itself. So we've always considered the "deadline" for
> doc changes to be when the release is finally posted.
>
> If there are small inconsistencies with the docs present in the source
> code for that release tag, IMO that doesn't matter much since we don't
> even distribute the docs with Spark's binary releases and virtually no
> one builds and hosts the docs on their own (that I am aware of, at
> least). Perhaps we can recommend if people want to build the doc
> sources that they should always grab the head of the most recent
> release branch, to set expectations accordingly.
>
> In the past we haven't considered it worth holding up the release
> process for the purpose of the docs. It just doesn't make sense since
> they are consumed "as a service". If we decide to change this
> convention, it would mean shipping our releases later, since we
> could't pipeline the doc finalization with voting.
>
> - Patrick
>
> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
>> Given the title and tagging, it sounds like there could be some
>> must-have doc changes to go with what is being released as 1.3. It can
>> be finished later, and published later, but then the docs source
>> shipped with the release doesn't match the site, and until then, 1.3
>> is released without some "must-have" docs for 1.3 on the site.
>>
>> The real question to me is: are there any further, absolutely
>> essential doc changes that need to accompany 1.3 or not?
>>
>> If not, just resolve these. If there are, then it seems like the
>> release has to block on them. If there are some docs that should have
>> gone in for 1.3, but didn't, but aren't essential, well I suppose it
>> bears thinking about how to not slip as much work, but it doesn't
>> block.
>>
>> I think Documentation issues certainly can be a blocker and shouldn't
>> be specially ignored.
>>
>>
>> BTW the UISeleniumSuite issue is a real failure, but I do not think it
>> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
>> a regression from 1.2.x, but only affects tests, and only affects a
>> subset of build profiles.
>>
>>
>>
>>
>> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> Hey Sean,
>>>
>>>> SPARK-5310 Update SQL programming guide for 1.3
>>>> SPARK-5183 Document data source API
>>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
>>>
>>> For these, the issue is that they are documentation JIRA's, which
>>> don't need to be timed exactly with the release vote, since we can
>>> update the documentation on the website whenever we want. In the past
>>> I've just mentally filtered these out when considering RC's. I see a
>>> few options here:
>>>
>>> 1. We downgrade such issues away from Blocker (more clear, but we risk
>>> loosing them in the fray if they really are things we want to have
>>> before the release is posted).
>>> 2. We provide a filter to the community that excludes 'Documentation'
>>> issues and shows all other blockers for 1.3. We can put this on the
>>> wiki, for instance.
>>>
>>> Which do you prefer?
>>>
>>> - Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar  7 00:20:37 2015
Return-Path: <dev-return-11893-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24BC91748C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  7 Mar 2015 00:20:37 +0000 (UTC)
Received: (qmail 69065 invoked by uid 500); 7 Mar 2015 00:20:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68989 invoked by uid 500); 7 Mar 2015 00:20:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68978 invoked by uid 99); 7 Mar 2015 00:20:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 07 Mar 2015 00:20:35 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of turp1twin@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 07 Mar 2015 00:20:29 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 4708F1632081
	for <dev@spark.apache.org>; Fri,  6 Mar 2015 16:20:13 -0800 (PST)
Date: Fri, 6 Mar 2015 17:20:09 -0700 (MST)
From: turp1twin <turp1twin@gmail.com>
To: dev@spark.apache.org
Message-ID: <1425687609663-10934.post@n3.nabble.com>
Subject: Block Transfer Service encryption support
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Is there a plan to implement SSL support for the Block Transfer Service
(specifically, the NettyBlockTransferService implementation)? I can
volunteer if needed...

Jeff




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar  7 07:20:38 2015
Return-Path: <dev-return-11894-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 758CD17C96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  7 Mar 2015 07:20:38 +0000 (UTC)
Received: (qmail 85161 invoked by uid 500); 7 Mar 2015 07:20:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85077 invoked by uid 500); 7 Mar 2015 07:20:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85065 invoked by uid 99); 7 Mar 2015 07:20:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 07 Mar 2015 07:20:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 07 Mar 2015 07:19:59 +0000
Received: by obcwo20 with SMTP id wo20so24944664obc.5
        for <dev@spark.apache.org>; Fri, 06 Mar 2015 23:19:57 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=nLoWEzkcKioHA+2Wz7TaWbm2g1D6l9BgOZE6DCMqfjY=;
        b=Y4Up+7sz5huum7bEvkRgTbfqQbCrxZ2XGeDCz76KHgYmjtpQt47r+RQBJC7G94fJE8
         NSgdBdlvwOn5gbmCfhOHjvTtXfw8b6/g0aIOLKnhnOCUKhV220MHYCbFl92FoTaTQEUS
         uKG4phyK1pi8Ix1k9N8VsBZ+tOMV7lhSE1U20TI2BH/T6EtqjnAD1HGshIqk1UBbaWDP
         FKcuXGv6LxO4Ed0Cct/JZrrC7F3rUyaCSnAX+NY4KJAyZQzJfR2Fr+u4fLT7QtLBBhwr
         LoXhGokAlH6KfEFpM0nJP+Qt+m5wHMxI7ZD/mbkdIIkrp6BltiYgZY9ZGz5RumzOmluk
         T2Ig==
MIME-Version: 1.0
X-Received: by 10.202.45.214 with SMTP id t205mr13320404oit.100.1425712797423;
 Fri, 06 Mar 2015 23:19:57 -0800 (PST)
Received: by 10.202.226.137 with HTTP; Fri, 6 Mar 2015 23:19:57 -0800 (PST)
In-Reply-To: <CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
	<CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
	<CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
	<CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
	<CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
Date: Fri, 6 Mar 2015 23:19:57 -0800
Message-ID: <CABPQxsv2D4rk5oNJJtTU6k+LcjHSMKr8b1q8xxTf6Cid5hJAkg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

For now, I'll just put this as critical. We can discuss the
documentation stuff offline or in another thread.

On Fri, Mar 6, 2015 at 1:36 PM, Sean Owen <sowen@cloudera.com> wrote:
> Although the problem is small, especially if indeed the essential docs
> changes are following just a couple days behind the final release, I
> mean, why the rush if they're essential? wait a couple days, finish
> them, make the release.
>
> Answer is, I think these changes aren't actually essential given the
> comment from tdas, so: just mark these Critical? (although ... they do
> say they're changes for the 1.3 release, so kind of funny to get to
> them for 1.3.x or 1.4, but that's not important now.)
>
> I thought that Blocker really meant Blocker in this project, as I've
> been encouraged to use it to mean "don't release without this." I
> think we should use it that way. Just thinking of it as "extra
> Critical" doesn't add anything. I don't think Documentation should be
> special-cased as less important, and I don't think there's confusion
> if Blocker means what it says, so I'd 'fix' that way.
>
> If nobody sees the Hive failure I observed, and if we can just zap
> those "Blockers" one way or the other, +1
>
>
> On Fri, Mar 6, 2015 at 9:17 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Sean,
>>
>> The docs are distributed and consumed in a fundamentally different way
>> than Spark code itself. So we've always considered the "deadline" for
>> doc changes to be when the release is finally posted.
>>
>> If there are small inconsistencies with the docs present in the source
>> code for that release tag, IMO that doesn't matter much since we don't
>> even distribute the docs with Spark's binary releases and virtually no
>> one builds and hosts the docs on their own (that I am aware of, at
>> least). Perhaps we can recommend if people want to build the doc
>> sources that they should always grab the head of the most recent
>> release branch, to set expectations accordingly.
>>
>> In the past we haven't considered it worth holding up the release
>> process for the purpose of the docs. It just doesn't make sense since
>> they are consumed "as a service". If we decide to change this
>> convention, it would mean shipping our releases later, since we
>> could't pipeline the doc finalization with voting.
>>
>> - Patrick
>>
>> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
>>> Given the title and tagging, it sounds like there could be some
>>> must-have doc changes to go with what is being released as 1.3. It can
>>> be finished later, and published later, but then the docs source
>>> shipped with the release doesn't match the site, and until then, 1.3
>>> is released without some "must-have" docs for 1.3 on the site.
>>>
>>> The real question to me is: are there any further, absolutely
>>> essential doc changes that need to accompany 1.3 or not?
>>>
>>> If not, just resolve these. If there are, then it seems like the
>>> release has to block on them. If there are some docs that should have
>>> gone in for 1.3, but didn't, but aren't essential, well I suppose it
>>> bears thinking about how to not slip as much work, but it doesn't
>>> block.
>>>
>>> I think Documentation issues certainly can be a blocker and shouldn't
>>> be specially ignored.
>>>
>>>
>>> BTW the UISeleniumSuite issue is a real failure, but I do not think it
>>> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
>>> a regression from 1.2.x, but only affects tests, and only affects a
>>> subset of build profiles.
>>>
>>>
>>>
>>>
>>> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>> Hey Sean,
>>>>
>>>>> SPARK-5310 Update SQL programming guide for 1.3
>>>>> SPARK-5183 Document data source API
>>>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
>>>>
>>>> For these, the issue is that they are documentation JIRA's, which
>>>> don't need to be timed exactly with the release vote, since we can
>>>> update the documentation on the website whenever we want. In the past
>>>> I've just mentally filtered these out when considering RC's. I see a
>>>> few options here:
>>>>
>>>> 1. We downgrade such issues away from Blocker (more clear, but we risk
>>>> loosing them in the fray if they really are things we want to have
>>>> before the release is posted).
>>>> 2. We provide a filter to the community that excludes 'Documentation'
>>>> issues and shows all other blockers for 1.3. We can put this on the
>>>> wiki, for instance.
>>>>
>>>> Which do you prefer?
>>>>
>>>> - Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 10:18:01 2015
Return-Path: <dev-return-11895-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 67A3617A36
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 10:18:01 +0000 (UTC)
Received: (qmail 28851 invoked by uid 500); 8 Mar 2015 10:18:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28766 invoked by uid 500); 8 Mar 2015 10:18:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28755 invoked by uid 99); 8 Mar 2015 10:17:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 10:17:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 10:17:54 +0000
Received: by labgq15 with SMTP id gq15so11614077lab.13
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 03:17:18 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=lKDqvs24gNGtFKBh45GyGbu7g1FGgL8vxaMUtDjcu38=;
        b=fghkieAWqG6VVMhmFSUH6EptdzmO6+ROmyAT0uH6Eltg1u14xxDKRMhTXR7fmbI+fN
         YapZTrskSIm0ZQuykvY/rdOqb1c/Rjs6wav+LoIDiORTb8JEPqiImm0Htx3r+0pDCXSD
         b6tyi4fUCLNyscGFfWk8nsSXlVJ1dZSFkHcHAJynfkv3+eBg6yMdySO7LLSd0WNSJQW6
         70GEXxOkaCHdodZHS2QF2w9B20QriSxgyFE23N9X/EH/Z585EsPVuzC/qph9j62F7rZD
         WiVfsFGLAL/7mfQvDJORki/qF5tV687iiWZZv+p3xYHq1IF6cUWqYFtAuUZG2mWNUo0k
         +Ocg==
X-Gm-Message-State: ALoCoQl/+c+p/oBkVd6aVA9uLsXj8R/MLbHIR4O3NEzlPiqbzTUjZMfv41xgAFAiDaWPcdfs2Vto
MIME-Version: 1.0
X-Received: by 10.112.17.67 with SMTP id m3mr20762505lbd.109.1425809838196;
 Sun, 08 Mar 2015 03:17:18 -0700 (PDT)
Received: by 10.152.43.234 with HTTP; Sun, 8 Mar 2015 03:17:18 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
Date: Sun, 8 Mar 2015 15:47:18 +0530
Message-ID: <CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
Subject: Re: Loading previously serialized object to Spark
From: Akhil Das <akhil@sigmoidanalytics.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3c082ec61840510c43840
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3c082ec61840510c43840
Content-Type: text/plain; charset=UTF-8

Can you paste the complete code?

Thanks
Best Regards

On Sat, Mar 7, 2015 at 2:25 AM, Ulanov, Alexander <alexander.ulanov@hp.com>
wrote:

> Hi,
>
> I've implemented class MyClass in MLlib that does some operation on
> LabeledPoint. MyClass extends serializable, so I can map this operation on
> data of RDD[LabeledPoints], such as data.map(lp => MyClass.operate(lp)). I
> write this class in file with ObjectOutputStream.writeObject. Then I stop
> and restart Spark. I load this class from file with
> ObjectInputStream.readObject.asInstanceOf[MyClass]. When I try to map the
> same operation of this class to RDD, Spark throws not serializable
> exception:
> org.apache.spark.SparkException: Task not serializable
>         at
> org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166)
>         at
> org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)
>         at org.apache.spark.SparkContext.clean(SparkContext.scala:1453)
>         at org.apache.spark.rdd.RDD.map(RDD.scala:273)
>
> Could you suggest why it throws this exception while MyClass is
> serializable by definition?
>
> Best regards, Alexander
>

--001a11c3c082ec61840510c43840--

From dev-return-11896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 19:43:47 2015
Return-Path: <dev-return-11896-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 618D510658
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 19:43:47 +0000 (UTC)
Received: (qmail 91082 invoked by uid 500); 8 Mar 2015 19:43:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91008 invoked by uid 500); 8 Mar 2015 19:43:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90995 invoked by uid 99); 8 Mar 2015 19:43:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 19:43:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.170 as permitted sender)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 19:43:39 +0000
Received: by mail-vc0-f170.google.com with SMTP id hq11so6072856vcb.1
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 12:42:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=T8CVfTFaaN5YxvW6KW/z2LFILQax8AyKib868TRJpQ8=;
        b=agfuyjNkXIVCB6ER2m7812UCEor+H7Ms/2O4i67D0W7j+x4X11tIo3us/lDE7z/Ekp
         BsE9U62M38ME5cgrsIIioL3mDrJ4MBgBaQ/xkN1wNX8Mo/e9Rt1NKMV967M2fgVlzCxJ
         fsl3IAJILqouipejn2ktK3GWKpkzjGR+2mMDi2fnfNY4b/NUvvnK3zs6WXLFrbbM71jJ
         EZQrmKOHylnniYSmoGqmFbJyl4e8KT1dXqXP9Ad6WvCgnazfW92mRLo+Jl/mNZKb3M+6
         C47GD2HGzGD504nsPGe/geq0rEp1b+ZnpWiTfI4IxgpzmPR/E7VGQTQ+cOwq7MDDnAmY
         F0qg==
X-Received: by 10.52.146.179 with SMTP id td19mr5723537vdb.8.1425843753183;
        Sun, 08 Mar 2015 12:42:33 -0700 (PDT)
Received: from mbp-4.fios-router.home (pool-108-49-55-117.bstnma.fios.verizon.net. [108.49.55.117])
        by mx.google.com with ESMTPSA id bb5sm2845666vdc.8.2015.03.08.12.42.32
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 08 Mar 2015 12:42:32 -0700 (PDT)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
Date: Sun, 8 Mar 2015 15:42:31 -0400
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <2800A0EB-8A3C-4C54-A8DB-D2BD7B62B8B0@gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
X-Mailer: Apple Mail (2.2070.6)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested it on Mac OS X.

One small issue I noticed is that the Scala 2.11 build is using Hadoop 1 =
without Hive, which is kind of weird because people will more likely =
want Hadoop 2 with Hive. So it would be good to publish a build for that =
configuration instead. We can do it if we do a new RC, or it might be =
that binary builds may not need to be voted on (I forgot the details =
there).

Matei

> On Mar 5, 2015, at 9:52 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>=20
> Please vote on releasing the following candidate as Apache Spark =
version 1.3.0!
>=20
> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D4aaf4=
8d46d13129f0f9bdafd771dd80fe568a7dc
>=20
> The release files, including signatures, digests, etc. can be found =
at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>=20
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1078
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.3.0!
>=20
> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>=20
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>=20
> =3D=3D How does this compare to RC2 =3D=3D
> This release includes the following bug fixes:
>=20
> https://issues.apache.org/jira/browse/SPARK-6144
> https://issues.apache.org/jira/browse/SPARK-6171
> https://issues.apache.org/jira/browse/SPARK-5143
> https://issues.apache.org/jira/browse/SPARK-6182
> https://issues.apache.org/jira/browse/SPARK-6175
>=20
> =3D=3D How can I help test this release? =3D=3D
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>=20
> If you are happy with this release based on your own testing, give a =
+1 vote.
>=20
> =3D=3D What justifies a -1 vote for this release? =3D=3D
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 19:47:29 2015
Return-Path: <dev-return-11897-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3827610663
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 19:47:29 +0000 (UTC)
Received: (qmail 95448 invoked by uid 500); 8 Mar 2015 19:47:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95373 invoked by uid 500); 8 Mar 2015 19:47:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95362 invoked by uid 99); 8 Mar 2015 19:47:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 19:47:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 19:47:02 +0000
Received: by wggx13 with SMTP id x13so23412291wgg.12
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 12:47:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=3Vh8itLWO+NjZLQM3FnQqvwFBqOS7LFsWk9UzH6ydic=;
        b=IYr/KgIneWVZZunIdjORXooVPJ3Nd/KAUPkT7h7sk9L1G7/PL1OR627+qGS/yDrRFc
         R2y8LGtYOuDouKRIWE8dMVkoj482giGhKqMhqaGtekCxUVMILqqDARnpUVlE7SI89gBd
         knMkJ7depP8rzTG/qPQS5oSHmVNBPDI5nzFCsB3jYe5zWW5vDoxZapn11BOLlrvbeM8r
         uINU3qmhlCLuLft14J1mNBrriq4AX8CTufB9asQdJRNIe5vEDMV6Wqd+zUdmRnivfj8P
         PKfoQOt2y1qT+Zzn6yIZjGAC/G+7MEUvA8onTY2Ri0FkzTSOMUUm383lCnaOj/gqdkaa
         IpIw==
X-Gm-Message-State: ALoCoQm4D7JdYuUxLMFqPGeYyTeMvIk6xpqeJVdA5ZrUA1qHrlHO7wvYa6y+7I5+z4fpSZ3A92Q7
X-Received: by 10.194.59.112 with SMTP id y16mr52571194wjq.36.1425844021481;
 Sun, 08 Mar 2015 12:47:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Sun, 8 Mar 2015 12:46:39 -0700 (PDT)
In-Reply-To: <2800A0EB-8A3C-4C54-A8DB-D2BD7B62B8B0@gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <2800A0EB-8A3C-4C54-A8DB-D2BD7B62B8B0@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 8 Mar 2015 19:46:39 +0000
Message-ID: <CAMAsSd+0A=hfri7W1_Vj9rM-=wGf7PHgPQ+WXVSv=VmpEaXFzQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, interesting question of what is the better default for the
single set of artifacts published to Maven. I think there's an
argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
and cons discussed more at

https://issues.apache.org/jira/browse/SPARK-5134
https://github.com/apache/spark/pull/3917

On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com> wro=
te:
> +1
>
> Tested it on Mac OS X.
>
> One small issue I noticed is that the Scala 2.11 build is using Hadoop 1 =
without Hive, which is kind of weird because people will more likely want H=
adoop 2 with Hive. So it would be good to publish a build for that configur=
ation instead. We can do it if we do a new RC, or it might be that binary b=
uilds may not need to be voted on (I forgot the details there).
>
> Matei

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 21:12:34 2015
Return-Path: <dev-return-11898-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EE93010849
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 21:12:34 +0000 (UTC)
Received: (qmail 98930 invoked by uid 500); 8 Mar 2015 21:12:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98857 invoked by uid 500); 8 Mar 2015 21:12:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98845 invoked by uid 99); 8 Mar 2015 21:12:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 21:12:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.51 as permitted sender)
Received: from [209.85.218.51] (HELO mail-oi0-f51.google.com) (209.85.218.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 21:12:02 +0000
Received: by oifu20 with SMTP id u20so26025130oif.12
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 14:11:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=7MSpu2FiDZVCs09U0KSoN6UuPC9gsQgG+bXfEAs0D78=;
        b=jKBj7OEyuyZzmfG7JKUsMt+idW35r/cxWgbAQ/3JKkyq7litfbGtuk4SKDWKI5pUW6
         /limKmTpvuG1Ksh8Ov5YoisIMNu80g1hW5kMPV5FW5FfCKiJyI6AA4Eb0UHf5EjR29Qm
         BkgJFiUjEMoy/xOxNv6vBq5PIZJEjKtDAVlNJyTl4lRYboUPBnTc/JelgDrl2CiFWs0C
         ghg6bAieM3KFUFSDhfjkQo3vIreQYfkTW75MtsdPDiTxDKSWwS0VsRR5l2+SpE9+QQlk
         svam3Cl9uOPndehKcsEwpPLlrXAcX4VCcHbD+TGelgKdq/YMk82pdS2o6f/V7T2+hqMv
         iSVQ==
MIME-Version: 1.0
X-Received: by 10.182.22.137 with SMTP id d9mr19122314obf.67.1425849076048;
 Sun, 08 Mar 2015 14:11:16 -0700 (PDT)
Received: by 10.202.226.137 with HTTP; Sun, 8 Mar 2015 14:11:15 -0700 (PDT)
In-Reply-To: <CAMAsSd+0A=hfri7W1_Vj9rM-=wGf7PHgPQ+WXVSv=VmpEaXFzQ@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<2800A0EB-8A3C-4C54-A8DB-D2BD7B62B8B0@gmail.com>
	<CAMAsSd+0A=hfri7W1_Vj9rM-=wGf7PHgPQ+WXVSv=VmpEaXFzQ@mail.gmail.com>
Date: Sun, 8 Mar 2015 14:11:15 -0700
Message-ID: <CABPQxstobDfLyiGtejaJ1+f-E8pj_OMZLygYHNiCkdjPmy2T7g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

We probably want to revisit the way we do binaries in general for
1.4+. IMO, something worth forking a separate thread for.

I've been hesitating to add new binaries because people
(understandably) complain if you ever stop packaging older ones, but
on the other hand the ASF has complained that we have too many
binaries already and that we need to pare it down because of the large
volume of files. Doubling the number of binaries we produce for Scala
2.11 seemed like it would be too much.

One solution potentially is to actually package "Hadoop provided"
binaries and encourage users to use these by simply setting
HADOOP_HOME, or have instructions for specific distros. I've heard
that our existing packages don't work well on HDP for instance, since
there are some configuration quirks that differ from the upstream
Hadoop.

If we cut down on the cross building for Hadoop versions, then it is
more tenable to cross build for Scala versions without exploding the
number of binaries.

- Patrick

On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrote:
> Yeah, interesting question of what is the better default for the
> single set of artifacts published to Maven. I think there's an
> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
> and cons discussed more at
>
> https://issues.apache.org/jira/browse/SPARK-5134
> https://github.com/apache/spark/pull/3917
>
> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com> w=
rote:
>> +1
>>
>> Tested it on Mac OS X.
>>
>> One small issue I noticed is that the Scala 2.11 build is using Hadoop 1=
 without Hive, which is kind of weird because people will more likely want =
Hadoop 2 with Hive. So it would be good to publish a build for that configu=
ration instead. We can do it if we do a new RC, or it might be that binary =
builds may not need to be voted on (I forgot the details there).
>>
>> Matei

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 21:44:01 2015
Return-Path: <dev-return-11899-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C149A108FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 21:44:01 +0000 (UTC)
Received: (qmail 49217 invoked by uid 500); 8 Mar 2015 21:43:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49141 invoked by uid 500); 8 Mar 2015 21:43:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49129 invoked by uid 99); 8 Mar 2015 21:43:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 21:43:55 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ksankar42@gmail.com designates 209.85.192.182 as permitted sender)
Received: from [209.85.192.182] (HELO mail-pd0-f182.google.com) (209.85.192.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 21:43:30 +0000
Received: by pdbfl12 with SMTP id fl12so58893741pdb.9
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 14:42:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Pjd7hRSusHfMe8qBB/OuY3T1ZWi1olKjXAUBa3kTnUU=;
        b=di9iZmGJsZbK4Ligq567Daz6LRaN2sJ5ONhDmiE3kNb/j8zOLHqSIGgpHJQ1DxO1vx
         JyzMsgPr+h2mw3asC40EiQAs2JMwxlnSUTQXjZYOYnbdVvbJlnRML19W5E1HHy+CPXEv
         skpSlLpj7IRtDaL0SpxoG68Ot9bfP4kZzV1Jb2qHgUHSSItxtDww+PwrBzvECfgQXfRg
         F51zMaOM/B5Fju3NM8h2w/LwysMElCB/+OEEcmKL1K7/GHQAPpDc70PzQElZlkuBgYfj
         DjeimHEo/iNSMRm+Kn/Btl2gRGbElgUInbxm0AideUH87BXTTao5HJQ15IeQ1Xv3MJMW
         vlTA==
MIME-Version: 1.0
X-Received: by 10.70.37.134 with SMTP id y6mr46869755pdj.13.1425850963608;
 Sun, 08 Mar 2015 14:42:43 -0700 (PDT)
Received: by 10.70.19.130 with HTTP; Sun, 8 Mar 2015 14:42:43 -0700 (PDT)
In-Reply-To: <CABPQxstobDfLyiGtejaJ1+f-E8pj_OMZLygYHNiCkdjPmy2T7g@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<2800A0EB-8A3C-4C54-A8DB-D2BD7B62B8B0@gmail.com>
	<CAMAsSd+0A=hfri7W1_Vj9rM-=wGf7PHgPQ+WXVSv=VmpEaXFzQ@mail.gmail.com>
	<CABPQxstobDfLyiGtejaJ1+f-E8pj_OMZLygYHNiCkdjPmy2T7g@mail.gmail.com>
Date: Sun, 8 Mar 2015 14:42:43 -0700
Message-ID: <CAOTBr2mjAtenG1R7rJ4-0ZL_ZENSFxvngA_nuk5zkfPcnkgF9Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Krishna Sankar <ksankar42@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfead04304b8d0510cdcc60
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfead04304b8d0510cdcc60
Content-Type: text/plain; charset=UTF-8

Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
Distributions X ...

May be one option is to have a minimum basic set (which I know is what we
are discussing) and move the rest to spark-packages.org. There the vendors
can add the latest downloads - for example when 1.4 is released, HDP can
build a release of HDP Spark 1.4 bundle.

Cheers
<k/>

On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> We probably want to revisit the way we do binaries in general for
> 1.4+. IMO, something worth forking a separate thread for.
>
> I've been hesitating to add new binaries because people
> (understandably) complain if you ever stop packaging older ones, but
> on the other hand the ASF has complained that we have too many
> binaries already and that we need to pare it down because of the large
> volume of files. Doubling the number of binaries we produce for Scala
> 2.11 seemed like it would be too much.
>
> One solution potentially is to actually package "Hadoop provided"
> binaries and encourage users to use these by simply setting
> HADOOP_HOME, or have instructions for specific distros. I've heard
> that our existing packages don't work well on HDP for instance, since
> there are some configuration quirks that differ from the upstream
> Hadoop.
>
> If we cut down on the cross building for Hadoop versions, then it is
> more tenable to cross build for Scala versions without exploding the
> number of binaries.
>
> - Patrick
>
> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrote:
> > Yeah, interesting question of what is the better default for the
> > single set of artifacts published to Maven. I think there's an
> > argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
> > and cons discussed more at
> >
> > https://issues.apache.org/jira/browse/SPARK-5134
> > https://github.com/apache/spark/pull/3917
> >
> > On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> >> +1
> >>
> >> Tested it on Mac OS X.
> >>
> >> One small issue I noticed is that the Scala 2.11 build is using Hadoop
> 1 without Hive, which is kind of weird because people will more likely want
> Hadoop 2 with Hive. So it would be good to publish a build for that
> configuration instead. We can do it if we do a new RC, or it might be that
> binary builds may not need to be voted on (I forgot the details there).
> >>
> >> Matei
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7bfead04304b8d0510cdcc60--

From dev-return-11900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 21:58:50 2015
Return-Path: <dev-return-11900-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F57C10949
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 21:58:50 +0000 (UTC)
Received: (qmail 66546 invoked by uid 500); 8 Mar 2015 21:58:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66464 invoked by uid 500); 8 Mar 2015 21:58:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66453 invoked by uid 99); 8 Mar 2015 21:58:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 21:58:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 21:58:24 +0000
Received: by wgha1 with SMTP id a1so20378815wgh.1
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 14:56:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to:cc
         :content-type;
        bh=JPcpWPWXLqo6P+nxPTLmipXLUBmGBxEIK9e+/gqkF9E=;
        b=PLUhzzx94kZQ/839M7Rg8qWyQsDeNcVN28b4fetQsy1df3fwJCUH7lMoFS5plMkQWg
         1MpPDjqi+sDkEJmZU2dzcM9ioMzYphy4whyf9gWgAQ1quClkG5PQuI1DLbEyNxSaoZp5
         qopKq533EByekWJOtVFEuu0JtuUZm2j6j2ZgpOLyS98XDJinC2HnioXZq9CMcfvDxfdF
         r8dhidOR1r5OhkfWyADM6W1898h37xoC/t3I+tkPNDRDfwpVz282E/3dfVO3aprtKnGf
         iKOc7iJjzWbSZ3jRR8Qnu8m5bCOgNk2a9whki6M9VHczZ38GZqpcA3lxJhXz2S+a6D2r
         sJrw==
X-Gm-Message-State: ALoCoQlqC4dfDxcFeEs3Mq2UtkonJnMgTK1CnRVpxXeEyF7NvH2o8KS/t3BtWo4q/h6lj10e5iBP
X-Received: by 10.180.75.233 with SMTP id f9mr25235659wiw.5.1425851811855;
 Sun, 08 Mar 2015 14:56:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Sun, 8 Mar 2015 14:56:31 -0700 (PDT)
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 8 Mar 2015 21:56:31 +0000
Message-ID: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
Subject: Release Scala version vs Hadoop version (was: [VOTE] Release Apache
 Spark 1.3.0 (RC3))
To: Krishna Sankar <ksankar42@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball
at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
Maven artifacts.

Patrick I see you just commented on SPARK-5134 and will follow up
there. Sounds like this may accidentally not be a problem.

On binary tarball releases, I wonder if anyone has an opinion on my
opinion that these shouldn't be distributed for specific Hadoop
*distributions* to begin with. (Won't repeat the argument here yet.)
That resolves this n x m explosion too.

Vendors already provide their own distribution, yes, that's their job.


On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com> wrote:
> Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
> Distributions X ...
>
> May be one option is to have a minimum basic set (which I know is what we
> are discussing) and move the rest to spark-packages.org. There the vendors
> can add the latest downloads - for example when 1.4 is released, HDP can
> build a release of HDP Spark 1.4 bundle.
>
> Cheers
> <k/>
>
> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>> We probably want to revisit the way we do binaries in general for
>> 1.4+. IMO, something worth forking a separate thread for.
>>
>> I've been hesitating to add new binaries because people
>> (understandably) complain if you ever stop packaging older ones, but
>> on the other hand the ASF has complained that we have too many
>> binaries already and that we need to pare it down because of the large
>> volume of files. Doubling the number of binaries we produce for Scala
>> 2.11 seemed like it would be too much.
>>
>> One solution potentially is to actually package "Hadoop provided"
>> binaries and encourage users to use these by simply setting
>> HADOOP_HOME, or have instructions for specific distros. I've heard
>> that our existing packages don't work well on HDP for instance, since
>> there are some configuration quirks that differ from the upstream
>> Hadoop.
>>
>> If we cut down on the cross building for Hadoop versions, then it is
>> more tenable to cross build for Scala versions without exploding the
>> number of binaries.
>>
>> - Patrick
>>
>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrote:
>> > Yeah, interesting question of what is the better default for the
>> > single set of artifacts published to Maven. I think there's an
>> > argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>> > and cons discussed more at
>> >
>> > https://issues.apache.org/jira/browse/SPARK-5134
>> > https://github.com/apache/spark/pull/3917
>> >
>> > On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> > wrote:
>> >> +1
>> >>
>> >> Tested it on Mac OS X.
>> >>
>> >> One small issue I noticed is that the Scala 2.11 build is using Hadoop
>> >> 1 without Hive, which is kind of weird because people will more likely want
>> >> Hadoop 2 with Hive. So it would be good to publish a build for that
>> >> configuration instead. We can do it if we do a new RC, or it might be that
>> >> binary builds may not need to be voted on (I forgot the details there).
>> >>
>> >> Matei
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 23:09:14 2015
Return-Path: <dev-return-11901-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5FFA610B2A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 23:09:14 +0000 (UTC)
Received: (qmail 39290 invoked by uid 500); 8 Mar 2015 23:09:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39196 invoked by uid 500); 8 Mar 2015 23:09:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39184 invoked by uid 99); 8 Mar 2015 23:09:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 23:09:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 23:09:07 +0000
Received: by mail-vc0-f180.google.com with SMTP id hq12so3822979vcb.11
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 16:08:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=wV/FE39juLilLPKfpXbCwa4pbmYpnluU8UzTNudONAM=;
        b=AJxN56sUiBedqmTec8/ZJVU4F/YBRZgM5mpZfuSprM1mciWZSVVx5xwMwZIbq+MdZ/
         4CYOZnHHVxuk5t3lxAlI06Vo6zib8F0MBHhuDcbqujcKjkpu2PwOFQ9YwdYleasKAP8d
         6lWr/Kwl0/vqIZ/wz0DmKSuuAjLBvJxJYeuvX8Hr0YFluygOk90Olp2Pd6dZBNvHCFXZ
         3mPKZuvGxpvCDR2Dhm2TBFNkU21pi0TaX9FMPG3/bWmnyXjHrD7v/eW+kE0LVvt0ZUsS
         B5abK3qiPOEOr7rzkP0VPq9HehpjpkOwHxkyKoYQbtCwZ8CkWCLOo7p5ILZL4G1ijbs5
         4x1A==
X-Received: by 10.52.175.6 with SMTP id bw6mr30236891vdc.91.1425856081594;
        Sun, 08 Mar 2015 16:08:01 -0700 (PDT)
Received: from mbp-4.fios-router.home (pool-108-49-55-117.bstnma.fios.verizon.net. [108.49.55.117])
        by mx.google.com with ESMTPSA id qm9sm2933522vdb.17.2015.03.08.16.08.00
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 08 Mar 2015 16:08:00 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
Subject: Re: Release Scala version vs Hadoop version (was: [VOTE] Release Apache Spark 1.3.0 (RC3))
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
Date: Sun, 8 Mar 2015 19:07:59 -0400
Cc: Krishna Sankar <ksankar42@gmail.com>,
 Patrick Wendell <pwendell@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com>
References: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
To: Sean Owen <sowen@cloudera.com>
X-Mailer: Apple Mail (2.2070.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Our goal is to let people use the latest Apache release even if vendors =
fall behind or don't want to package everything, so that's why we put =
out releases for vendors' versions. It's fairly low overhead.

Matei

> On Mar 8, 2015, at 5:56 PM, Sean Owen <sowen@cloudera.com> wrote:
>=20
> Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball
> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
> Maven artifacts.
>=20
> Patrick I see you just commented on SPARK-5134 and will follow up
> there. Sounds like this may accidentally not be a problem.
>=20
> On binary tarball releases, I wonder if anyone has an opinion on my
> opinion that these shouldn't be distributed for specific Hadoop
> *distributions* to begin with. (Won't repeat the argument here yet.)
> That resolves this n x m explosion too.
>=20
> Vendors already provide their own distribution, yes, that's their job.
>=20
>=20
> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com> =
wrote:
>> Yep, otherwise this will become an N^2 problem - Scala versions X =
Hadoop
>> Distributions X ...
>>=20
>> May be one option is to have a minimum basic set (which I know is =
what we
>> are discussing) and move the rest to spark-packages.org. There the =
vendors
>> can add the latest downloads - for example when 1.4 is released, HDP =
can
>> build a release of HDP Spark 1.4 bundle.
>>=20
>> Cheers
>> <k/>
>>=20
>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>>>=20
>>> We probably want to revisit the way we do binaries in general for
>>> 1.4+. IMO, something worth forking a separate thread for.
>>>=20
>>> I've been hesitating to add new binaries because people
>>> (understandably) complain if you ever stop packaging older ones, but
>>> on the other hand the ASF has complained that we have too many
>>> binaries already and that we need to pare it down because of the =
large
>>> volume of files. Doubling the number of binaries we produce for =
Scala
>>> 2.11 seemed like it would be too much.
>>>=20
>>> One solution potentially is to actually package "Hadoop provided"
>>> binaries and encourage users to use these by simply setting
>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>> that our existing packages don't work well on HDP for instance, =
since
>>> there are some configuration quirks that differ from the upstream
>>> Hadoop.
>>>=20
>>> If we cut down on the cross building for Hadoop versions, then it is
>>> more tenable to cross build for Scala versions without exploding the
>>> number of binaries.
>>>=20
>>> - Patrick
>>>=20
>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> =
wrote:
>>>> Yeah, interesting question of what is the better default for the
>>>> single set of artifacts published to Maven. I think there's an
>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>>>> and cons discussed more at
>>>>=20
>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>> https://github.com/apache/spark/pull/3917
>>>>=20
>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia =
<matei.zaharia@gmail.com>
>>>> wrote:
>>>>> +1
>>>>>=20
>>>>> Tested it on Mac OS X.
>>>>>=20
>>>>> One small issue I noticed is that the Scala 2.11 build is using =
Hadoop
>>>>> 1 without Hive, which is kind of weird because people will more =
likely want
>>>>> Hadoop 2 with Hive. So it would be good to publish a build for =
that
>>>>> configuration instead. We can do it if we do a new RC, or it might =
be that
>>>>> binary builds may not need to be voted on (I forgot the details =
there).
>>>>>=20
>>>>> Matei
>>>=20
>>> =
---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar  8 23:46:14 2015
Return-Path: <dev-return-11902-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4A4910C58
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  8 Mar 2015 23:46:14 +0000 (UTC)
Received: (qmail 99926 invoked by uid 500); 8 Mar 2015 23:46:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99843 invoked by uid 500); 8 Mar 2015 23:46:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99831 invoked by uid 99); 8 Mar 2015 23:46:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 23:46:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.50 as permitted sender)
Received: from [209.85.218.50] (HELO mail-oi0-f50.google.com) (209.85.218.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 08 Mar 2015 23:46:02 +0000
Received: by oiba3 with SMTP id a3so26369013oib.3
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 16:45:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=okVQIqbzW70Ylz7ZASM4Alx5YVDbhWoo7BzOix0riKU=;
        b=GjwtK7CWEEgr+ywVvlLtY2S6N6kkMzDqwijLX9iPBlI4iCLQbgZw8/a3ErzbaLtb38
         YTpoUmj4noixxU02w/tsMyOFyrCKt3G1Tdcp9M7Bm81kCJ9/+t48XlUC5enVMoax1puL
         K8SstdIDVhjOdBedkkkQ1jJd8I/EHRpPxXnwgNJu3+S5Eov58P1D+t15M59/FeLWOMhh
         VOf3T+DS+b/pwYfmkB1fvfbQXGURmCYbiYaSP2JNyiJsPUZzm87xvOkXJ/CaFXMKDBJE
         86aDc8Ry/FWf8mlrX+HggRA3C5E1rFkXNZ0pny3lCnLmz+6e2dZcWH2UcV/kyPCtgbVm
         gaIQ==
MIME-Version: 1.0
X-Received: by 10.60.155.225 with SMTP id vz1mr4540460oeb.52.1425858341579;
 Sun, 08 Mar 2015 16:45:41 -0700 (PDT)
Received: by 10.202.226.137 with HTTP; Sun, 8 Mar 2015 16:45:41 -0700 (PDT)
In-Reply-To: <CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com>
References: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
	<CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com>
Date: Sun, 8 Mar 2015 16:45:41 -0700
Message-ID: <CABPQxssQx8bLGDpt6557m4UEAZ7Xqb7G-MirmftoT01dr8ebxg@mail.gmail.com>
Subject: Re: Release Scala version vs Hadoop version (was: [VOTE] Release
 Apache Spark 1.3.0 (RC3))
From: Patrick Wendell <pwendell@gmail.com>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Krishna Sankar <ksankar42@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I think it's important to separate the goals from the implementation.
I agree with Matei on the goal - I think the goal needs to be to allow
people to download Apache Spark and use it with CDH, HDP, MapR,
whatever... This is the whole reason why HDFS and YARN have stable
API's, so that other projects can build on them in a way that works
across multiple versions. I wouldn't want to force users to upgrade
according only to some vendor timetable, that doesn't seem from the
ASF perspective like a good thing for the project. If users want to
get packages from Bigtop, or the vendors, that's totally fine too.

My point earlier was - I am not sure we are actually accomplishing
that goal now, because I've heard in some cases our "Hadoop 2.X"
packages actually don't work on certain distributions, even those that
are based on that Hadoop version. So one solution is to move towards
"bring your own Hadoop" binaries and have users just set HADOOP_HOME
and maybe document any vendor-specific configs that need to be set.
That also happens to solve the "too many binaries" problem, but only
incidentally.

- Patrick

On Sun, Mar 8, 2015 at 4:07 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Our goal is to let people use the latest Apache release even if vendors fall behind or don't want to package everything, so that's why we put out releases for vendors' versions. It's fairly low overhead.
>
> Matei
>
>> On Mar 8, 2015, at 5:56 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball
>> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
>> Maven artifacts.
>>
>> Patrick I see you just commented on SPARK-5134 and will follow up
>> there. Sounds like this may accidentally not be a problem.
>>
>> On binary tarball releases, I wonder if anyone has an opinion on my
>> opinion that these shouldn't be distributed for specific Hadoop
>> *distributions* to begin with. (Won't repeat the argument here yet.)
>> That resolves this n x m explosion too.
>>
>> Vendors already provide their own distribution, yes, that's their job.
>>
>>
>> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com> wrote:
>>> Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
>>> Distributions X ...
>>>
>>> May be one option is to have a minimum basic set (which I know is what we
>>> are discussing) and move the rest to spark-packages.org. There the vendors
>>> can add the latest downloads - for example when 1.4 is released, HDP can
>>> build a release of HDP Spark 1.4 bundle.
>>>
>>> Cheers
>>> <k/>
>>>
>>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>>
>>>> We probably want to revisit the way we do binaries in general for
>>>> 1.4+. IMO, something worth forking a separate thread for.
>>>>
>>>> I've been hesitating to add new binaries because people
>>>> (understandably) complain if you ever stop packaging older ones, but
>>>> on the other hand the ASF has complained that we have too many
>>>> binaries already and that we need to pare it down because of the large
>>>> volume of files. Doubling the number of binaries we produce for Scala
>>>> 2.11 seemed like it would be too much.
>>>>
>>>> One solution potentially is to actually package "Hadoop provided"
>>>> binaries and encourage users to use these by simply setting
>>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>>> that our existing packages don't work well on HDP for instance, since
>>>> there are some configuration quirks that differ from the upstream
>>>> Hadoop.
>>>>
>>>> If we cut down on the cross building for Hadoop versions, then it is
>>>> more tenable to cross build for Scala versions without exploding the
>>>> number of binaries.
>>>>
>>>> - Patrick
>>>>
>>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>>> Yeah, interesting question of what is the better default for the
>>>>> single set of artifacts published to Maven. I think there's an
>>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>>>>> and cons discussed more at
>>>>>
>>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>>> https://github.com/apache/spark/pull/3917
>>>>>
>>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com>
>>>>> wrote:
>>>>>> +1
>>>>>>
>>>>>> Tested it on Mac OS X.
>>>>>>
>>>>>> One small issue I noticed is that the Scala 2.11 build is using Hadoop
>>>>>> 1 without Hive, which is kind of weird because people will more likely want
>>>>>> Hadoop 2 with Hive. So it would be good to publish a build for that
>>>>>> configuration instead. We can do it if we do a new RC, or it might be that
>>>>>> binary builds may not need to be voted on (I forgot the details there).
>>>>>>
>>>>>> Matei
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 00:04:21 2015
Return-Path: <dev-return-11903-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4203310D5C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 00:04:21 +0000 (UTC)
Received: (qmail 24688 invoked by uid 500); 9 Mar 2015 00:04:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24613 invoked by uid 500); 9 Mar 2015 00:04:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24601 invoked by uid 99); 9 Mar 2015 00:04:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:04:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:04:15 +0000
Received: by wghl18 with SMTP id l18so17231931wgh.11
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 17:02:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/s6aIJTG2X2ILfeQV74DyB5JMKfhXKaskIJVTnyZwG8=;
        b=Ga0Jv1aYWFXwvBpWhBgBYbxo2853igOs2MdsUFMS7oHCXvfpgbybLZTkwbGpwT593x
         NU8tjGpO05o0K/2xhRGPFFVQmILBd1PfBtgugu+DQ7MAvUhox3g2j6K5254MLLfZ0RHF
         dJkvfuQVW9gNpY0QmBoEvaWLEm10k5/lk7MSZq2oXamjYDqEUir4TJjtfXMPLKKwcvT2
         xYCedNhm17Yp6UuVVZsU7kBpbGyAVVGZQw2+y+idHUsaqM7g60wkAi84rQo6j5KGQULW
         Ps1EFRm/tlh+lTvpaKGE0sEBPLbHoUWuY6NKHUB+0ka16xY4D5Ld6NRUvspuG74/TbaM
         gzxQ==
X-Gm-Message-State: ALoCoQlGHWLZwKQ6Doqf0ojbOZ8FR0W4tuso/ygyFPn+Lx2dZME2/r7d5qRwkFJUqQaLY1RlDLnV
X-Received: by 10.180.212.70 with SMTP id ni6mr40292739wic.8.1425859344553;
 Sun, 08 Mar 2015 17:02:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Sun, 8 Mar 2015 17:02:04 -0700 (PDT)
In-Reply-To: <CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com>
References: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
 <CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 9 Mar 2015 00:02:04 +0000
Message-ID: <CAMAsSdKivUZSyL4y-_FONf7beLpUR8uHrhjtE2aCAZAjHJC3jw@mail.gmail.com>
Subject: Re: Release Scala version vs Hadoop version (was: [VOTE] Release
 Apache Spark 1.3.0 (RC3))
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah it's not much overhead, but here's an example of where it causes
a little issue.

I like that reasoning. However, the released builds don't track the
later versions of Hadoop that vendors would be distributing -- there's
no Hadoop 2.6 build for example. CDH4 is here, but not the
far-more-used CDH5. HDP isn't present at all. The CDH4 build doesn't
actually work with many CDH4 versions.

I agree with the goal of maximizing the reach of Spark, but I don't
know how much these builds advance that goal.

Anyone can roll-their-own exactly-right build, and the docs and build
have been set up to make that as simple as can be expected. So these
aren't *required* to let me use latest Spark on distribution X.

I had thought these existed to sorta support 'legacy' distributions,
like CDH4, and that build was justified as a
quasi-Hadoop-2.0.x-flavored build. But then I don't understand what
the MapR profiles are for.

I think it's too much work to correctly, in parallel, maintain any
customizations necessary for any major distro, and it might be best to
do not at all than to do it incompletely. You could say it's also an
enabler for distros to vary in ways that require special
customization.

Maybe there's a concern that, if lots of people consume Spark on
Hadoop, and most people consume Hadoop through distros, and distros
alone manage Spark distributions, then you de facto 'have to' go
through a distro instead of get bits from Spark? Different
conversation but I think this sort of effect does not end up being a
negative.

Well anyway, I like the idea of seeing how far Hadoop-provided
releases can help. It might kill several birds with one stone.

On Sun, Mar 8, 2015 at 11:07 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Our goal is to let people use the latest Apache release even if vendors fall behind or don't want to package everything, so that's why we put out releases for vendors' versions. It's fairly low overhead.
>
> Matei
>
>> On Mar 8, 2015, at 5:56 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball
>> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
>> Maven artifacts.
>>
>> Patrick I see you just commented on SPARK-5134 and will follow up
>> there. Sounds like this may accidentally not be a problem.
>>
>> On binary tarball releases, I wonder if anyone has an opinion on my
>> opinion that these shouldn't be distributed for specific Hadoop
>> *distributions* to begin with. (Won't repeat the argument here yet.)
>> That resolves this n x m explosion too.
>>
>> Vendors already provide their own distribution, yes, that's their job.
>>
>>
>> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com> wrote:
>>> Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
>>> Distributions X ...
>>>
>>> May be one option is to have a minimum basic set (which I know is what we
>>> are discussing) and move the rest to spark-packages.org. There the vendors
>>> can add the latest downloads - for example when 1.4 is released, HDP can
>>> build a release of HDP Spark 1.4 bundle.
>>>
>>> Cheers
>>> <k/>
>>>
>>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>>
>>>> We probably want to revisit the way we do binaries in general for
>>>> 1.4+. IMO, something worth forking a separate thread for.
>>>>
>>>> I've been hesitating to add new binaries because people
>>>> (understandably) complain if you ever stop packaging older ones, but
>>>> on the other hand the ASF has complained that we have too many
>>>> binaries already and that we need to pare it down because of the large
>>>> volume of files. Doubling the number of binaries we produce for Scala
>>>> 2.11 seemed like it would be too much.
>>>>
>>>> One solution potentially is to actually package "Hadoop provided"
>>>> binaries and encourage users to use these by simply setting
>>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>>> that our existing packages don't work well on HDP for instance, since
>>>> there are some configuration quirks that differ from the upstream
>>>> Hadoop.
>>>>
>>>> If we cut down on the cross building for Hadoop versions, then it is
>>>> more tenable to cross build for Scala versions without exploding the
>>>> number of binaries.
>>>>
>>>> - Patrick
>>>>
>>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>>> Yeah, interesting question of what is the better default for the
>>>>> single set of artifacts published to Maven. I think there's an
>>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>>>>> and cons discussed more at
>>>>>
>>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>>> https://github.com/apache/spark/pull/3917
>>>>>
>>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com>
>>>>> wrote:
>>>>>> +1
>>>>>>
>>>>>> Tested it on Mac OS X.
>>>>>>
>>>>>> One small issue I noticed is that the Scala 2.11 build is using Hadoop
>>>>>> 1 without Hive, which is kind of weird because people will more likely want
>>>>>> Hadoop 2 with Hive. So it would be good to publish a build for that
>>>>>> configuration instead. We can do it if we do a new RC, or it might be that
>>>>>> binary builds may not need to be voted on (I forgot the details there).
>>>>>>
>>>>>> Matei
>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 00:41:05 2015
Return-Path: <dev-return-11904-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AEB0D10E73
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 00:41:05 +0000 (UTC)
Received: (qmail 61806 invoked by uid 500); 9 Mar 2015 00:40:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61727 invoked by uid 500); 9 Mar 2015 00:40:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61714 invoked by uid 99); 9 Mar 2015 00:40:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:40:58 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:40:53 +0000
Received: by obcvb8 with SMTP id vb8so26462785obc.0
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 17:39:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=4zeRwPk+p6siNKwFk+8UxUZY6vxVxuiI40GsCgJbNFY=;
        b=TjXEbU+85Y43xmr8jhEDyvkUUbwGfeBV+yu3MsiYu6XnMEe6vYW89TCIfFciVD3aAl
         2IH41WXhV1DPuSRVjzGJLnBrOngOpf8rJM9fNyIqlWbXPsQyMX03vCncu7HXYm8lCIEv
         Nv5f3tFKlt9xVCDpgFs9FCaPaCBMcKsQD0rIaVXuzBePAFkk+5Ogx+RN90PUlbSHQWGV
         ZEuUB3nallqJtSHHbxzRUBPrgu3RcYVlaqdTMl5zyEnlLDm+wg0XFXmYON1qTJMWJPu0
         2RxriWsn4SZdhNMB8L6LfcHypqAoZiX6TlKa9ui6RzgT66uTYovoKDUU7cA0im7aMngv
         0OjA==
X-Gm-Message-State: ALoCoQm9SxqiMmTDv4le+Ocr+DUm4l6HsnE+yb8wrFVoOtEK6k1yYOaHc9ej6Q97Fi4cRTwVHHet
X-Received: by 10.183.24.162 with SMTP id ij2mr19557451obd.18.1425861567118;
        Sun, 08 Mar 2015 17:39:27 -0700 (PDT)
Received: from mail-ob0-f176.google.com (mail-ob0-f176.google.com. [209.85.214.176])
        by mx.google.com with ESMTPSA id c65sm10889937oig.21.2015.03.08.17.39.25
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 08 Mar 2015 17:39:26 -0700 (PDT)
Received: by obcva2 with SMTP id va2so11902665obc.3
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 17:39:25 -0700 (PDT)
X-Received: by 10.60.139.1 with SMTP id qu1mr19308368oeb.83.1425861565254;
 Sun, 08 Mar 2015 17:39:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.182.99 with HTTP; Sun, 8 Mar 2015 17:39:04 -0700 (PDT)
In-Reply-To: <1425687609663-10934.post@n3.nabble.com>
References: <1425687609663-10934.post@n3.nabble.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Sun, 8 Mar 2015 17:39:04 -0700
Message-ID: <CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
To: turp1twin <turp1twin@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b4724361895760510d0443d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4724361895760510d0443d
Content-Type: text/plain; charset=UTF-8

I'm interested in seeing this data transfer occurring over encrypted
communication channels as well.  Many customers require that all network
transfer occur encrypted to prevent the "soft underbelly" that's often
found inside a corporate network.

On Fri, Mar 6, 2015 at 4:20 PM, turp1twin <turp1twin@gmail.com> wrote:

> Is there a plan to implement SSL support for the Block Transfer Service
> (specifically, the NettyBlockTransferService implementation)? I can
> volunteer if needed...
>
> Jeff
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b4724361895760510d0443d--

From dev-return-11905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 00:42:41 2015
Return-Path: <dev-return-11905-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6D4C10E7A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 00:42:41 +0000 (UTC)
Received: (qmail 63689 invoked by uid 500); 9 Mar 2015 00:42:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63603 invoked by uid 500); 9 Mar 2015 00:42:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63589 invoked by uid 99); 9 Mar 2015 00:42:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:42:40 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of turp1twin@gmail.com designates 209.85.213.45 as permitted sender)
Received: from [209.85.213.45] (HELO mail-yh0-f45.google.com) (209.85.213.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:42:15 +0000
Received: by yhoc41 with SMTP id c41so17648283yho.13
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 17:42:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DSUCQcJTLDyd8uQ/pRr81a4vMjzROgolzyiuvxWbsnE=;
        b=oYdQ0LMLA75lDAO/fYaR79B/nx6CfBSTqkY5x0OkmxrRCYafwa2qegl7bl4+9meA9u
         mHmtImzWVMDejHuM2cdQRoPn+f0a+HsV4dbfkrM18bpIPsPdU/jtByLY32EhAZnk50DZ
         fBFHWzVZhuyCwc3nOqmTl53qF1GKXpYsdr94Hkk5oNz1VozRKG1ze92Y+REUZVmZWmoO
         h9ObOL26cWgPHr1t4V/j4fBj6++VcWuWRseC2zdCgPOPBvisREY2sCN7QxWnCTO91YvG
         MksZ/I/INCQvcfBEBsni5OBlWTu0cHl5UpnM2txKvGw0WgjHRhEu8uqW+gAuLvYNa/oW
         mmgQ==
MIME-Version: 1.0
X-Received: by 10.236.14.168 with SMTP id d28mr24313309yhd.3.1425861733649;
 Sun, 08 Mar 2015 17:42:13 -0700 (PDT)
Received: by 10.170.128.17 with HTTP; Sun, 8 Mar 2015 17:42:13 -0700 (PDT)
In-Reply-To: <CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com>
References: <1425687609663-10934.post@n3.nabble.com>
	<CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com>
Date: Sun, 8 Mar 2015 17:42:13 -0700
Message-ID: <CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
From: Jeff Turpin <turp1twin@gmail.com>
To: Andrew Ash <andrew@andrewash.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013a180e2213160510d04e98
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a180e2213160510d04e98
Content-Type: text/plain; charset=UTF-8

I have already written most of the code, just finishing up the unit tests
right now...

Jeff


On Sun, Mar 8, 2015 at 5:39 PM, Andrew Ash <andrew@andrewash.com> wrote:

> I'm interested in seeing this data transfer occurring over encrypted
> communication channels as well.  Many customers require that all network
> transfer occur encrypted to prevent the "soft underbelly" that's often
> found inside a corporate network.
>
> On Fri, Mar 6, 2015 at 4:20 PM, turp1twin <turp1twin@gmail.com> wrote:
>
>> Is there a plan to implement SSL support for the Block Transfer Service
>> (specifically, the NettyBlockTransferService implementation)? I can
>> volunteer if needed...
>>
>> Jeff
>>
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--089e013a180e2213160510d04e98--

From dev-return-11906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 00:53:03 2015
Return-Path: <dev-return-11906-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 226AB10EBA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 00:53:03 +0000 (UTC)
Received: (qmail 71080 invoked by uid 500); 9 Mar 2015 00:53:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71014 invoked by uid 500); 9 Mar 2015 00:53:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70999 invoked by uid 99); 9 Mar 2015 00:53:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:53:01 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 00:52:37 +0000
Received: by oifz81 with SMTP id z81so26527883oif.2
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 17:51:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=h6981Zb0GG7ACZGSl4JtuFwJl5QD8NJYl8Qfa+fWsl8=;
        b=yI4IBKhYEwgtz0JOFJxVT9CHAfQGWITPdBBviBg6un9lZxwRh9uFrqe3jHDb1zpE0T
         xb2tC4ChFxiPiDrm06RiqT+hcJnTG88EPv+Ak429H480g9Cblv2XOVQCiZY414hrGR0j
         IgEHZXpJlSMrv7lPlENzM2l7VUGDT91JVoyrOpr2AiyjclVHY7+sxe7tSls8L+DrkTSW
         iWB24AiytkJQfeYBWKrwsyi1jVMy7XQmIprR25Pm6ujPaq+FucaQiwZ3kQ8QcpmVOo5C
         dtSppeulQcyy/UIbOJnXQh7x1zjLxrcATBXySwJsUwshkRcVt0Opji9+kCbsKzJv+67n
         WHlQ==
MIME-Version: 1.0
X-Received: by 10.60.139.1 with SMTP id qu1mr19330614oeb.83.1425862265464;
 Sun, 08 Mar 2015 17:51:05 -0700 (PDT)
Received: by 10.202.226.137 with HTTP; Sun, 8 Mar 2015 17:51:05 -0700 (PDT)
In-Reply-To: <CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com>
References: <1425687609663-10934.post@n3.nabble.com>
	<CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com>
	<CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com>
Date: Sun, 8 Mar 2015 17:51:05 -0700
Message-ID: <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
From: Patrick Wendell <pwendell@gmail.com>
To: Jeff Turpin <turp1twin@gmail.com>
Cc: Andrew Ash <andrew@andrewash.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I think that yes, longer term we want to have encryption of all
communicated data. However Jeff, can you open a JIRA to discuss the
design before opening a pull request (it's fine to link to a WIP
branch if you'd like)? I'd like to better understand the performance
and operational complexity of using SSL for this in comparison with
alternatives. It would also be good to look at how the Hadoop
encryption works for their shuffle service, in terms of the design
decisions made there.

- Patrick

On Sun, Mar 8, 2015 at 5:42 PM, Jeff Turpin <turp1twin@gmail.com> wrote:
> I have already written most of the code, just finishing up the unit tests
> right now...
>
> Jeff
>
>
> On Sun, Mar 8, 2015 at 5:39 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
>> I'm interested in seeing this data transfer occurring over encrypted
>> communication channels as well.  Many customers require that all network
>> transfer occur encrypted to prevent the "soft underbelly" that's often
>> found inside a corporate network.
>>
>> On Fri, Mar 6, 2015 at 4:20 PM, turp1twin <turp1twin@gmail.com> wrote:
>>
>>> Is there a plan to implement SSL support for the Block Transfer Service
>>> (specifically, the NettyBlockTransferService implementation)? I can
>>> volunteer if needed...
>>>
>>> Jeff
>>>
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934.html
>>> Sent from the Apache Spark Developers List mailing list archive at
>>> Nabble.com.
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 01:36:34 2015
Return-Path: <dev-return-11907-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E23E10F8B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 01:36:34 +0000 (UTC)
Received: (qmail 12809 invoked by uid 500); 9 Mar 2015 01:36:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12732 invoked by uid 500); 9 Mar 2015 01:36:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12720 invoked by uid 99); 9 Mar 2015 01:36:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 01:36:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 01:36:04 +0000
Received: by qcvp6 with SMTP id p6so19527069qcv.1
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 18:33:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=x2s/lnhA+gP6QQKHYjHRXglTRVEOf+EYegGHKunIFRY=;
        b=VNXwvUpf9mKwOsr3aYBLTFeG4d4NwIvR0Zl8776/3aAYMJVvvzdTSLgUh8S8DftZEg
         wsLSTaNc+O3/RPIz0WwWw6GF/ZDzWOE9DPeX6oLhMYZ3yyQ8MBO19XkgMnC3Gh2WBdwg
         A1go/HksXogYdufedoQspB1vYsUirXOU8eTxCTj+AjA5vxugQF+qWc5LTuD2MsRyNKgq
         Np1mYF1DZqM9MyfhV2G1nvF26xM79t1CJOBx1X+xCbwEh/olmjKvNDCaCBdhWWhLFm8B
         /Hhx4v3d3bYrBM+WLz8qPJ7jgoL/aRHhhZi/v8KpYoUfSPK9ctWmKHZVVwKzQB4WBWl3
         XQAw==
X-Received: by 10.140.34.204 with SMTP id l70mr31728394qgl.55.1425864828002;
        Sun, 08 Mar 2015 18:33:48 -0700 (PDT)
Received: from mbp-4.fios-router.home (pool-108-49-55-117.bstnma.fios.verizon.net. [108.49.55.117])
        by mx.google.com with ESMTPSA id 70sm10627362qhe.29.2015.03.08.18.33.47
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 08 Mar 2015 18:33:47 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
Subject: Re: Release Scala version vs Hadoop version (was: [VOTE] Release Apache Spark 1.3.0 (RC3))
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMAsSdKivUZSyL4y-_FONf7beLpUR8uHrhjtE2aCAZAjHJC3jw@mail.gmail.com>
Date: Sun, 8 Mar 2015 21:33:46 -0400
Cc: Patrick Wendell <pwendell@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <A7557B5E-1185-4EAC-859B-B399246D81A1@gmail.com>
References: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com> <CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com> <CAMAsSdKivUZSyL4y-_FONf7beLpUR8uHrhjtE2aCAZAjHJC3jw@mail.gmail.com>
To: Sean Owen <sowen@cloudera.com>
X-Mailer: Apple Mail (2.2070.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, my concern is that people should get Apache Spark from *Apache*, =
not from a vendor. It helps everyone use the latest features no matter =
where they are. In the Hadoop distro case, Hadoop made all this effort =
to have standard APIs (e.g. YARN), so it should be easy. But it is a =
problem if we're not packaging for the newest versions of some distros; =
I think we just fell behind at Hadoop 2.4.

Matei

> On Mar 8, 2015, at 8:02 PM, Sean Owen <sowen@cloudera.com> wrote:
>=20
> Yeah it's not much overhead, but here's an example of where it causes
> a little issue.
>=20
> I like that reasoning. However, the released builds don't track the
> later versions of Hadoop that vendors would be distributing -- there's
> no Hadoop 2.6 build for example. CDH4 is here, but not the
> far-more-used CDH5. HDP isn't present at all. The CDH4 build doesn't
> actually work with many CDH4 versions.
>=20
> I agree with the goal of maximizing the reach of Spark, but I don't
> know how much these builds advance that goal.
>=20
> Anyone can roll-their-own exactly-right build, and the docs and build
> have been set up to make that as simple as can be expected. So these
> aren't *required* to let me use latest Spark on distribution X.
>=20
> I had thought these existed to sorta support 'legacy' distributions,
> like CDH4, and that build was justified as a
> quasi-Hadoop-2.0.x-flavored build. But then I don't understand what
> the MapR profiles are for.
>=20
> I think it's too much work to correctly, in parallel, maintain any
> customizations necessary for any major distro, and it might be best to
> do not at all than to do it incompletely. You could say it's also an
> enabler for distros to vary in ways that require special
> customization.
>=20
> Maybe there's a concern that, if lots of people consume Spark on
> Hadoop, and most people consume Hadoop through distros, and distros
> alone manage Spark distributions, then you de facto 'have to' go
> through a distro instead of get bits from Spark? Different
> conversation but I think this sort of effect does not end up being a
> negative.
>=20
> Well anyway, I like the idea of seeing how far Hadoop-provided
> releases can help. It might kill several birds with one stone.
>=20
> On Sun, Mar 8, 2015 at 11:07 PM, Matei Zaharia =
<matei.zaharia@gmail.com> wrote:
>> Our goal is to let people use the latest Apache release even if =
vendors fall behind or don't want to package everything, so that's why =
we put out releases for vendors' versions. It's fairly low overhead.
>>=20
>> Matei
>>=20
>>> On Mar 8, 2015, at 5:56 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>=20
>>> Ah. I misunderstood that Matei was referring to the Scala 2.11 =
tarball
>>> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
>>> Maven artifacts.
>>>=20
>>> Patrick I see you just commented on SPARK-5134 and will follow up
>>> there. Sounds like this may accidentally not be a problem.
>>>=20
>>> On binary tarball releases, I wonder if anyone has an opinion on my
>>> opinion that these shouldn't be distributed for specific Hadoop
>>> *distributions* to begin with. (Won't repeat the argument here yet.)
>>> That resolves this n x m explosion too.
>>>=20
>>> Vendors already provide their own distribution, yes, that's their =
job.
>>>=20
>>>=20
>>> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com> =
wrote:
>>>> Yep, otherwise this will become an N^2 problem - Scala versions X =
Hadoop
>>>> Distributions X ...
>>>>=20
>>>> May be one option is to have a minimum basic set (which I know is =
what we
>>>> are discussing) and move the rest to spark-packages.org. There the =
vendors
>>>> can add the latest downloads - for example when 1.4 is released, =
HDP can
>>>> build a release of HDP Spark 1.4 bundle.
>>>>=20
>>>> Cheers
>>>> <k/>
>>>>=20
>>>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell =
<pwendell@gmail.com> wrote:
>>>>>=20
>>>>> We probably want to revisit the way we do binaries in general for
>>>>> 1.4+. IMO, something worth forking a separate thread for.
>>>>>=20
>>>>> I've been hesitating to add new binaries because people
>>>>> (understandably) complain if you ever stop packaging older ones, =
but
>>>>> on the other hand the ASF has complained that we have too many
>>>>> binaries already and that we need to pare it down because of the =
large
>>>>> volume of files. Doubling the number of binaries we produce for =
Scala
>>>>> 2.11 seemed like it would be too much.
>>>>>=20
>>>>> One solution potentially is to actually package "Hadoop provided"
>>>>> binaries and encourage users to use these by simply setting
>>>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>>>> that our existing packages don't work well on HDP for instance, =
since
>>>>> there are some configuration quirks that differ from the upstream
>>>>> Hadoop.
>>>>>=20
>>>>> If we cut down on the cross building for Hadoop versions, then it =
is
>>>>> more tenable to cross build for Scala versions without exploding =
the
>>>>> number of binaries.
>>>>>=20
>>>>> - Patrick
>>>>>=20
>>>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> =
wrote:
>>>>>> Yeah, interesting question of what is the better default for the
>>>>>> single set of artifacts published to Maven. I think there's an
>>>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. =
Pros
>>>>>> and cons discussed more at
>>>>>>=20
>>>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>>>> https://github.com/apache/spark/pull/3917
>>>>>>=20
>>>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia =
<matei.zaharia@gmail.com>
>>>>>> wrote:
>>>>>>> +1
>>>>>>>=20
>>>>>>> Tested it on Mac OS X.
>>>>>>>=20
>>>>>>> One small issue I noticed is that the Scala 2.11 build is using =
Hadoop
>>>>>>> 1 without Hive, which is kind of weird because people will more =
likely want
>>>>>>> Hadoop 2 with Hive. So it would be good to publish a build for =
that
>>>>>>> configuration instead. We can do it if we do a new RC, or it =
might be that
>>>>>>> binary builds may not need to be voted on (I forgot the details =
there).
>>>>>>>=20
>>>>>>> Matei
>>>>>=20
>>>>> =
---------------------------------------------------------------------
>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>=20
>>>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 03:54:26 2015
Return-Path: <dev-return-11908-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9050B17506
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 03:54:26 +0000 (UTC)
Received: (qmail 98508 invoked by uid 500); 9 Mar 2015 03:54:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98438 invoked by uid 500); 9 Mar 2015 03:54:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98420 invoked by uid 99); 9 Mar 2015 03:54:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 03:54:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.manglano@gmail.com designates 209.85.218.45 as permitted sender)
Received: from [209.85.218.45] (HELO mail-oi0-f45.google.com) (209.85.218.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 03:54:19 +0000
Received: by oiax69 with SMTP id x69so27050760oia.5
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 20:53:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=uXfwSPAl9q0RSIjNCGxBMoVqIX6dSWpfAuRKd4ZIPgI=;
        b=KyvVbTzQcWGWzeon+uE8cbu+jZxH3vZt9qktDwgaTVTyYI4Mra/UoPASPqrMCKovyT
         PI9FdwRfZeAQ+b6vQ28Xf3XinxbjBzVU2VHWTtWLJPJBmLArN7d7HlLb1RxYMZkn9O8u
         h+86ezg2J+eS9U4j+j8HU7ea0q82fLlICE8101W7sf11a3vX+hXpUFBdYNqQmmnewmwl
         0sXMh7/pvYnLSSrNAfiiAwGaj21p7JwQI1UunWfbEMeE5V0veJOUWCfmtmEhbcfwzMbJ
         Fybiq/+Jf1TOx2+ltGea+0TrCwWaI5puDrmJVPmWopvOP8WPOZaPxiDO8EclYWcEgsEp
         Qs2w==
MIME-Version: 1.0
X-Received: by 10.202.85.17 with SMTP id j17mr18701745oib.65.1425873238791;
 Sun, 08 Mar 2015 20:53:58 -0700 (PDT)
Received: by 10.76.104.233 with HTTP; Sun, 8 Mar 2015 20:53:58 -0700 (PDT)
Date: Sun, 8 Mar 2015 22:53:58 -0500
Message-ID: <CAAThnfP_0mJcbtLEU0N4z+kVqYa0q6hqMYsb+9U1tgmMrbFUmg@mail.gmail.com>
Subject: GSoC 2015
From: "David J. Manglano" <david.manglano@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113d2e2ce49ba80510d2fb70
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d2e2ce49ba80510d2fb70
Content-Type: text/plain; charset=UTF-8

Hi Spark devs!

I'm writing regarding your GSoC 2015 project idea. I'm a graduate student
with experience in Python and discrete mathematics. I'm interested in
machine learning, and understand some of its basic concepts.

I was wondering if someone might be able to elaborate upon the goals for
Spark with GSoC (it is my understanding that Manoj Kumar is the mentor),
though I may be incorrect. I have been reading the Spark codebase on GitHub
and think I may be able to help develop Spark's Python API.

To get involved, what next steps should I take?

Thanks!

David J. Manglano
Masters Program in Computer Science
University of Chicago

--001a113d2e2ce49ba80510d2fb70--

From dev-return-11909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 04:02:34 2015
Return-Path: <dev-return-11909-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62FB117559
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 04:02:34 +0000 (UTC)
Received: (qmail 14909 invoked by uid 500); 9 Mar 2015 04:02:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14716 invoked by uid 500); 9 Mar 2015 04:02:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14702 invoked by uid 99); 9 Mar 2015 04:02:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 04:02:32 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of turp1twin@gmail.com designates 209.85.213.42 as permitted sender)
Received: from [209.85.213.42] (HELO mail-yh0-f42.google.com) (209.85.213.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 04:02:07 +0000
Received: by yhai57 with SMTP id i57so6679301yha.8
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 21:00:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=iYWcENWdz5YcZWLz4ObtrSBKP7rP/nDMiZ+0rF3ohbY=;
        b=fJ65W03w5D1aV6bVYVvvEIF95AWgFELn1aEImJdtAgwT9r5w1iWSfwNGdWaXKXUetp
         bY2pRrgGE4+cg9yGRncI7/XDaOqvzV+uq2P6L2P0m5+LN0JCZ/uTAjqbr0VUo+Gg2ts+
         iuN+RPdDGnLFX0b/6U71Gh8Q2A+UBI2o9b/QMTh/JpIY0p1azCcOAufvBKP+G7iYE5A4
         20wfqLwVhKw6TbkSscY3gjR+Nk4HnQwBJYtuz39NSx8yFr3yJTFjQBbdiYuojxw5fqiO
         WOkmyzsaSVIwsKT+QDopOVgf2ryfcQrG28QNuV0J7segc2PvTcwMKYjJvMqd1SO0Qnju
         D1aw==
MIME-Version: 1.0
X-Received: by 10.170.207.13 with SMTP id y13mr28457649yke.55.1425873635904;
 Sun, 08 Mar 2015 21:00:35 -0700 (PDT)
Received: by 10.170.128.17 with HTTP; Sun, 8 Mar 2015 21:00:35 -0700 (PDT)
In-Reply-To: <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com>
References: <1425687609663-10934.post@n3.nabble.com>
	<CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com>
	<CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com>
	<CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com>
Date: Sun, 8 Mar 2015 21:00:35 -0700
Message-ID: <CACCBg5dPD48k1t=yP9fdF+xSgvfkM4KOinKE64-hvxMs8oRJKA@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
From: Jeff Turpin <turp1twin@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Andrew Ash <andrew@andrewash.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11395cb89014fe0510d3136b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11395cb89014fe0510d3136b
Content-Type: text/plain; charset=UTF-8

Hey Patrick,

Yes, I will open a Jira tomorrow... For now my implementation is a basic
SSL implementation for the TransportServer and TransportClient.. I will
type up the design and at the same time look at the Hadoop impl for
possible improvements... Cheers!

Jeff


On Sun, Mar 8, 2015 at 5:51 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> I think that yes, longer term we want to have encryption of all
> communicated data. However Jeff, can you open a JIRA to discuss the
> design before opening a pull request (it's fine to link to a WIP
> branch if you'd like)? I'd like to better understand the performance
> and operational complexity of using SSL for this in comparison with
> alternatives. It would also be good to look at how the Hadoop
> encryption works for their shuffle service, in terms of the design
> decisions made there.
>
> - Patrick
>
> On Sun, Mar 8, 2015 at 5:42 PM, Jeff Turpin <turp1twin@gmail.com> wrote:
> > I have already written most of the code, just finishing up the unit tests
> > right now...
> >
> > Jeff
> >
> >
> > On Sun, Mar 8, 2015 at 5:39 PM, Andrew Ash <andrew@andrewash.com> wrote:
> >
> >> I'm interested in seeing this data transfer occurring over encrypted
> >> communication channels as well.  Many customers require that all network
> >> transfer occur encrypted to prevent the "soft underbelly" that's often
> >> found inside a corporate network.
> >>
> >> On Fri, Mar 6, 2015 at 4:20 PM, turp1twin <turp1twin@gmail.com> wrote:
> >>
> >>> Is there a plan to implement SSL support for the Block Transfer Service
> >>> (specifically, the NettyBlockTransferService implementation)? I can
> >>> volunteer if needed...
> >>>
> >>> Jeff
> >>>
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
> >>>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934.html
> >>> Sent from the Apache Spark Developers List mailing list archive at
> >>> Nabble.com.
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>>
> >>
>

--001a11395cb89014fe0510d3136b--

From dev-return-11910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 05:51:42 2015
Return-Path: <dev-return-11910-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7FF1417790
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 05:51:42 +0000 (UTC)
Received: (qmail 30211 invoked by uid 500); 9 Mar 2015 05:51:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30124 invoked by uid 500); 9 Mar 2015 05:51:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30113 invoked by uid 99); 9 Mar 2015 05:51:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 05:51:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 05:51:36 +0000
Received: by iecvy18 with SMTP id vy18so36000212iec.9
        for <dev@spark.apache.org>; Sun, 08 Mar 2015 22:51:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=jIW+BHyjjRh+oqA1eFzTNpHKsYLFQH4OY/MfayBdAx0=;
        b=hJAX/nj/rTfXVRARMVIUpqsIqyIU5vKAx/EwmuVnlLeTxSvXN2nE+JRS5auf8iL59L
         Oo6r2uRogM8bVqGrMkpviwIiBfdhaSbSSdz8Q5ZPGHkozUvprzsC56icPvIifylg6ntL
         c2rowORTbIuxPA4JBmFcM9ks06pfCkcTnUiwSBL82b/1htcLkfOlu4RNMBfSSRXdICRk
         W6fOmLrKm+yS8DC/rlx+tpqWXK567W6jhzWIN3Sg+mrugSk5zZfFPL15BeRrqAEPG7XP
         FuV5dFqPsnosYTH0gm7TTeK05RFwCzczp1yk61YUs59Z9mRaTC6xzpMO3Q0mED+nuYLX
         QZMg==
X-Gm-Message-State: ALoCoQl3AcZ04XaHsRjzG56E9kRBAoDY6p72EPPqF9MmktNW57CeT7N1R7CLFYUyUM9kEB7uC+co
MIME-Version: 1.0
X-Received: by 10.43.70.10 with SMTP id ye10mr24579964icb.66.1425880276400;
 Sun, 08 Mar 2015 22:51:16 -0700 (PDT)
Received: by 10.36.90.208 with HTTP; Sun, 8 Mar 2015 22:51:16 -0700 (PDT)
In-Reply-To: <CAOTBr2mjAtenG1R7rJ4-0ZL_ZENSFxvngA_nuk5zkfPcnkgF9Q@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<2800A0EB-8A3C-4C54-A8DB-D2BD7B62B8B0@gmail.com>
	<CAMAsSd+0A=hfri7W1_Vj9rM-=wGf7PHgPQ+WXVSv=VmpEaXFzQ@mail.gmail.com>
	<CABPQxstobDfLyiGtejaJ1+f-E8pj_OMZLygYHNiCkdjPmy2T7g@mail.gmail.com>
	<CAOTBr2mjAtenG1R7rJ4-0ZL_ZENSFxvngA_nuk5zkfPcnkgF9Q@mail.gmail.com>
Date: Sun, 8 Mar 2015 22:51:16 -0700
Message-ID: <CACBYxKKx19cNF8jCADo0XcrJgt9DzT3i6FejB-1qMj_uZyXtnA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Krishna Sankar <ksankar42@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	Matei Zaharia <matei.zaharia@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec51b204d5e14d60510d49fea
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec51b204d5e14d60510d49fea
Content-Type: text/plain; charset=UTF-8

+1 (non-binding, doc and packaging issues aside)

Built from source, ran jobs and spark-shell against a pseudo-distributed
YARN cluster.

On Sun, Mar 8, 2015 at 2:42 PM, Krishna Sankar <ksankar42@gmail.com> wrote:

> Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
> Distributions X ...
>
> May be one option is to have a minimum basic set (which I know is what we
> are discussing) and move the rest to spark-packages.org. There the vendors
> can add the latest downloads - for example when 1.4 is released, HDP can
> build a release of HDP Spark 1.4 bundle.
>
> Cheers
> <k/>
>
> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > We probably want to revisit the way we do binaries in general for
> > 1.4+. IMO, something worth forking a separate thread for.
> >
> > I've been hesitating to add new binaries because people
> > (understandably) complain if you ever stop packaging older ones, but
> > on the other hand the ASF has complained that we have too many
> > binaries already and that we need to pare it down because of the large
> > volume of files. Doubling the number of binaries we produce for Scala
> > 2.11 seemed like it would be too much.
> >
> > One solution potentially is to actually package "Hadoop provided"
> > binaries and encourage users to use these by simply setting
> > HADOOP_HOME, or have instructions for specific distros. I've heard
> > that our existing packages don't work well on HDP for instance, since
> > there are some configuration quirks that differ from the upstream
> > Hadoop.
> >
> > If we cut down on the cross building for Hadoop versions, then it is
> > more tenable to cross build for Scala versions without exploding the
> > number of binaries.
> >
> > - Patrick
> >
> > On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrote:
> > > Yeah, interesting question of what is the better default for the
> > > single set of artifacts published to Maven. I think there's an
> > > argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
> > > and cons discussed more at
> > >
> > > https://issues.apache.org/jira/browse/SPARK-5134
> > > https://github.com/apache/spark/pull/3917
> > >
> > > On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com
> >
> > wrote:
> > >> +1
> > >>
> > >> Tested it on Mac OS X.
> > >>
> > >> One small issue I noticed is that the Scala 2.11 build is using Hadoop
> > 1 without Hive, which is kind of weird because people will more likely
> want
> > Hadoop 2 with Hive. So it would be good to publish a build for that
> > configuration instead. We can do it if we do a new RC, or it might be
> that
> > binary builds may not need to be voted on (I forgot the details there).
> > >>
> > >> Matei
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--bcaec51b204d5e14d60510d49fea--

From dev-return-11911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 07:06:15 2015
Return-Path: <dev-return-11911-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B1FFD178E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 07:06:15 +0000 (UTC)
Received: (qmail 31804 invoked by uid 500); 9 Mar 2015 07:06:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31708 invoked by uid 500); 9 Mar 2015 07:06:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31695 invoked by uid 99); 9 Mar 2015 07:06:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 07:06:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.169] (HELO mail-lb0-f169.google.com) (209.85.217.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 07:06:08 +0000
Received: by lbiw7 with SMTP id w7so29628915lbi.7
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 00:04:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=VVJBvgFhG4/H9JuUttZtR5I8Ydt0gLBOh5Zn7oZpfLI=;
        b=FwFRMcUA1WF/Lt8E1b0Q0KlhwBsItitOF+1WcguKgvjTjQdyBn+QMVHB//4/m1Sdz+
         QBJ1wsTJdE5UwOz+MoNTQBNYe9QrHrVQT4uWjCCAyYSWkuQFZh6EtcWdKiUHxngvR0GW
         hXx4PgVolNrIcm0FdbZvD/eeLmTYItl9MPwoauVEa46gMprGAYK8SeN4XHZ7YgmlVllk
         7aPIMBtqPek3k2pgpIikPfIhX/lWHsjji1Xk0+wcJxjG989hg/2UnjocV6s1W+Jldp9Z
         JqkIF57QXBrITCU14XOCLPShUHl94EhQEGsV0KJx5g2b37O7R5hCDfT/Dj+dAdDV38lP
         Yc3w==
X-Gm-Message-State: ALoCoQnjerFFwKS5cLSdjhXvAqOrJtOQh9eQEx6ZDkNmZtuWgsZUlojXF6jcTHPdoeguD/pazI+G
MIME-Version: 1.0
X-Received: by 10.112.17.67 with SMTP id m3mr24243389lbd.109.1425884682690;
 Mon, 09 Mar 2015 00:04:42 -0700 (PDT)
Received: by 10.152.43.234 with HTTP; Mon, 9 Mar 2015 00:04:42 -0700 (PDT)
In-Reply-To: <CAAThnfP_0mJcbtLEU0N4z+kVqYa0q6hqMYsb+9U1tgmMrbFUmg@mail.gmail.com>
References: <CAAThnfP_0mJcbtLEU0N4z+kVqYa0q6hqMYsb+9U1tgmMrbFUmg@mail.gmail.com>
Date: Mon, 9 Mar 2015 12:34:42 +0530
Message-ID: <CAHUQ+_Z_GMsq2tb9MGDeCCQHgbRAQRDM-wUY9X5f5m1OJi-E2A@mail.gmail.com>
Subject: Re: GSoC 2015
From: Akhil Das <akhil@sigmoidanalytics.com>
To: "David J. Manglano" <david.manglano@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3c08200a89d0510d5a60a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3c08200a89d0510d5a60a
Content-Type: text/plain; charset=UTF-8

This might help
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Thanks
Best Regards

On Mon, Mar 9, 2015 at 9:23 AM, David J. Manglano <david.manglano@gmail.com>
wrote:

> Hi Spark devs!
>
> I'm writing regarding your GSoC 2015 project idea. I'm a graduate student
> with experience in Python and discrete mathematics. I'm interested in
> machine learning, and understand some of its basic concepts.
>
> I was wondering if someone might be able to elaborate upon the goals for
> Spark with GSoC (it is my understanding that Manoj Kumar is the mentor),
> though I may be incorrect. I have been reading the Spark codebase on GitHub
> and think I may be able to help develop Spark's Python API.
>
> To get involved, what next steps should I take?
>
> Thanks!
>
> David J. Manglano
> Masters Program in Computer Science
> University of Chicago
>

--001a11c3c08200a89d0510d5a60a--

From dev-return-11912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 09:24:59 2015
Return-Path: <dev-return-11912-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 751AE17BCA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 09:24:59 +0000 (UTC)
Received: (qmail 46952 invoked by uid 500); 9 Mar 2015 09:24:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46856 invoked by uid 500); 9 Mar 2015 09:24:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46844 invoked by uid 99); 9 Mar 2015 09:24:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 09:24:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hedonplay@gmail.com designates 209.85.214.179 as permitted sender)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 09:24:27 +0000
Received: by obcva8 with SMTP id va8so20991749obc.8
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 02:24:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=fPBzAYavdPEzFcA4Z8jFCi5qBel+ugn/cdYAUn+mpbI=;
        b=b91j7uzJuO27dKqMwdqBlpprRbOhVtvoqXnY0dQevkfgvjkT1l8X9R4u9z06pRYXvT
         wq3OxSfuFNmGzwdtZ7lNT+TtL8jc093fKLiBIC5KEOswHaIZawHl/38m6mwLWaFpso2l
         /8jxo08T9NuyhslvR2tnfMa8eUWd6o9XCFMR1A6bBpaudEr/v9lwcm+GNRbzEIE5z2QN
         aYKDqBA0PgEtM0G8o3DSdRIy13eJMtxP2m+fe40OphlqISFTXczbq00otS5UXOmOlRdR
         8p6JjWMgwn+VlVtHIuah3d/oK/+Pot/pTVTX/LnTpv1zL2DPwJMlAFeggxxhvHJSzoKg
         AQXQ==
X-Received: by 10.60.56.73 with SMTP id y9mr20531837oep.53.1425893065190; Mon,
 09 Mar 2015 02:24:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.202.15.1 with HTTP; Mon, 9 Mar 2015 02:24:05 -0700 (PDT)
From: Hui WANG <hedonplay@gmail.com>
Date: Mon, 9 Mar 2015 10:24:05 +0100
Message-ID: <CADUODdnesV8TQTPG_NdyD1OvQey7a7oGbkCZvD626dbRGmLeJQ@mail.gmail.com>
Subject: missing explanation of cache in the documentation of cluster overview
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c244f2a374a50510d79988
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c244f2a374a50510d79988
Content-Type: text/plain; charset=UTF-8

Hello Guys,

I'm reading the documentation of cluster mode overview on
https://spark.apache.org/docs/latest/cluster-overview.html.

In the schema, cache is shown aside executor but no explanation is done on
it.

Can someone please help to explain it and improve this page ?

-- 
Hui WANG
Tel : +33 (0) 6 71 33 45 39
Blog : http://www.hui-wang.info

--001a11c244f2a374a50510d79988--

From dev-return-11913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 09:30:50 2015
Return-Path: <dev-return-11913-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 05B4317BE7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 09:30:50 +0000 (UTC)
Received: (qmail 60585 invoked by uid 500); 9 Mar 2015 09:30:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60513 invoked by uid 500); 9 Mar 2015 09:30:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60501 invoked by uid 99); 9 Mar 2015 09:30:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 09:30:48 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 09:30:23 +0000
Received: by wgha1 with SMTP id a1so22828650wgh.1
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 02:29:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=B1u1CnJ9ZIRbR16vaCZqfYZNnEiIOfqPhpGP9u+nADA=;
        b=K6m4A2EIRyg96h5JaA1cpHCbpSexWdhr0oTI3+0PMhqvFiHEN8Y9rxN7f4YNR9NaNU
         xDj6AeXN5XuiEyHS/jkMMx1uZqTfuJoZEcK9FqSE6heG5Vz/AAmZonjEuwuziE9D8nMR
         rbCy2dEMrmZIzXcCJQPxTfV8k8KXsdF27WEg5Y3VW30fffNmgPu9rwiklP5DfXWHpmet
         KhhxOXr1eqcinLx/WEeTgVmJY2M2I8URW27tAE0rU+83X/mwS/bi1uqQjIsv0mQDeiqm
         WHu1KxuE8zNIScEyVHrSVKC/t9bPPJLjggweol3szdqQn4ZItxu1efMICU/twN3mV6iR
         Y+oQ==
X-Gm-Message-State: ALoCoQkytkbqxs3ZJ7SxAo/bKAgA5q4/+wrAXdxfeRmPmWmg3Gb3DKfbc6TOlPoPzzLQG/940964
X-Received: by 10.194.134.169 with SMTP id pl9mr53516835wjb.67.1425893377182;
 Mon, 09 Mar 2015 02:29:37 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Mon, 9 Mar 2015 02:29:16 -0700 (PDT)
In-Reply-To: <CADUODdnesV8TQTPG_NdyD1OvQey7a7oGbkCZvD626dbRGmLeJQ@mail.gmail.com>
References: <CADUODdnesV8TQTPG_NdyD1OvQey7a7oGbkCZvD626dbRGmLeJQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 9 Mar 2015 09:29:16 +0000
Message-ID: <CAMAsSdK-yu1AwisNrHTNkj5p-jJ-NFbOQZfBDSakEjSh=eFOvQ@mail.gmail.com>
Subject: Re: missing explanation of cache in the documentation of cluster overview
To: Hui WANG <hedonplay@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

It's explained at
https://spark.apache.org/docs/latest/programming-guide.html and it's
configuration at
https://spark.apache.org/docs/latest/configuration.html  Have a read
over all the docs first.

On Mon, Mar 9, 2015 at 9:24 AM, Hui WANG <hedonplay@gmail.com> wrote:
> Hello Guys,
>
> I'm reading the documentation of cluster mode overview on
> https://spark.apache.org/docs/latest/cluster-overview.html.
>
> In the schema, cache is shown aside executor but no explanation is done on
> it.
>
> Can someone please help to explain it and improve this page ?
>
> --
> Hui WANG
> Tel : +33 (0) 6 71 33 45 39
> Blog : http://www.hui-wang.info

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 09:52:08 2015
Return-Path: <dev-return-11914-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A61B17C77
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 09:52:08 +0000 (UTC)
Received: (qmail 2305 invoked by uid 500); 9 Mar 2015 09:51:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2198 invoked by uid 500); 9 Mar 2015 09:51:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2186 invoked by uid 99); 9 Mar 2015 09:51:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 09:51:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 09:51:47 +0000
Received: by wibbs8 with SMTP id bs8so18564150wib.0
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 02:50:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=DyHkfOUP0/3ct4cNwpFlG6DCvFG0vFINwdMxFmbd2Fg=;
        b=SDsbV/7HMgyJTCIziNTJueDivbGgXOi18xxMJ9fL9Ly8o2pQGzbxXr5wLF54DU5ff4
         1gR/iX+IwvTryOs0yScRxjkEwoevkRMqdSBVlTiUPQu9+bWQycvjDavy1iuoNl/cB+Aq
         QKLThf/FpAVtA8GV4DrzMsc8TVPNP0TltCiKTOn7F72d/BKbJNJEl9pqJyui95sF1HBR
         +/slEVJZstAQ3w/uQAu/uBjgSj81uyfc2svvHiN62nPFECbb09lmCtIOEB29SKzwzFtc
         yfn2rkWldbeU0dTXBOsEuPbwPx3lpie8mmKpzqcr4ZC8I8UCz/5B+hZQwmmN6MampGmZ
         znZQ==
X-Gm-Message-State: ALoCoQncKyTdweTtpoNMHm8J9L9cVT2GmSO+NMdncIBBwcimNSFwwp0HYpX84iCjrCMorUOuqJX7
X-Received: by 10.180.198.37 with SMTP id iz5mr56078018wic.95.1425894641673;
 Mon, 09 Mar 2015 02:50:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Mon, 9 Mar 2015 02:50:20 -0700 (PDT)
In-Reply-To: <A7557B5E-1185-4EAC-859B-B399246D81A1@gmail.com>
References: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
 <CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com> <CAMAsSdKivUZSyL4y-_FONf7beLpUR8uHrhjtE2aCAZAjHJC3jw@mail.gmail.com>
 <A7557B5E-1185-4EAC-859B-B399246D81A1@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 9 Mar 2015 09:50:20 +0000
Message-ID: <CAMAsSdKXsmsbSV-Mueq_A8vSR+ZY80CnkrDjNC6iF=wGrE49Lg@mail.gmail.com>
Subject: Re: Release Scala version vs Hadoop version (was: [VOTE] Release
 Apache Spark 1.3.0 (RC3))
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, you should always find working bits at Apache no matter what --
though 'no matter what' really means 'as long as you use Hadoop distro
compatible with upstream Hadoop'. Even distros have a strong interest
in that, since the market, the 'pie', is made large by this kind of
freedom at the core.

If tso, then no vendor-specific builds are needed, only some
Hadoop-release-specific ones. So a Hadoop 2.6-specific build could be
good (although I'm not yet clear if there's something about 2.5 or 2.6
that needs a different build.)

I take it that we already believe that, say, the "Hadoop 2.4" build
works with CDH5, so no CDH5-specific build is provided by Spark.

If a distro doesn't work with stock Spark, then it's either something
Spark should fix (e.g. use of a private YARN API or something), or
it's something the distro should really fix because it's incompatible.

Could we maybe rename the "CDH4" build then, as it doesn't really work
with all CDH4, to be a "Hadoop 2.0.x build"? That's been floated
before. And can we remove the MapR builds -- or else can someone
explain why these exist separately from a Hadoop 2.3 build? I hope it
is not *because* they are somehow non-standard. And shall we first run
down why Spark doesn't fully work on HDP and see if it's something
that Spark or HDP needs to tweak, rather than contemplate another
binary? or, if so, can it simply be called a "Hadoop 2.7 + YARN
whatever" build and not made specific to a vendor, even if the project
has to field another tarball combo for a vendor?

Maybe we are saying almost the same thing.


On Mon, Mar 9, 2015 at 1:33 AM, Matei Zaharia <matei.zaharia@gmail.com> wro=
te:
> Yeah, my concern is that people should get Apache Spark from *Apache*, no=
t from a vendor. It helps everyone use the latest features no matter where =
they are. In the Hadoop distro case, Hadoop made all this effort to have st=
andard APIs (e.g. YARN), so it should be easy. But it is a problem if we're=
 not packaging for the newest versions of some distros; I think we just fel=
l behind at Hadoop 2.4.
>
> Matei
>
>> On Mar 8, 2015, at 8:02 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> Yeah it's not much overhead, but here's an example of where it causes
>> a little issue.
>>
>> I like that reasoning. However, the released builds don't track the
>> later versions of Hadoop that vendors would be distributing -- there's
>> no Hadoop 2.6 build for example. CDH4 is here, but not the
>> far-more-used CDH5. HDP isn't present at all. The CDH4 build doesn't
>> actually work with many CDH4 versions.
>>
>> I agree with the goal of maximizing the reach of Spark, but I don't
>> know how much these builds advance that goal.
>>
>> Anyone can roll-their-own exactly-right build, and the docs and build
>> have been set up to make that as simple as can be expected. So these
>> aren't *required* to let me use latest Spark on distribution X.
>>
>> I had thought these existed to sorta support 'legacy' distributions,
>> like CDH4, and that build was justified as a
>> quasi-Hadoop-2.0.x-flavored build. But then I don't understand what
>> the MapR profiles are for.
>>
>> I think it's too much work to correctly, in parallel, maintain any
>> customizations necessary for any major distro, and it might be best to
>> do not at all than to do it incompletely. You could say it's also an
>> enabler for distros to vary in ways that require special
>> customization.
>>
>> Maybe there's a concern that, if lots of people consume Spark on
>> Hadoop, and most people consume Hadoop through distros, and distros
>> alone manage Spark distributions, then you de facto 'have to' go
>> through a distro instead of get bits from Spark? Different
>> conversation but I think this sort of effect does not end up being a
>> negative.
>>
>> Well anyway, I like the idea of seeing how far Hadoop-provided
>> releases can help. It might kill several birds with one stone.
>>
>> On Sun, Mar 8, 2015 at 11:07 PM, Matei Zaharia <matei.zaharia@gmail.com>=
 wrote:
>>> Our goal is to let people use the latest Apache release even if vendors=
 fall behind or don't want to package everything, so that's why we put out =
releases for vendors' versions. It's fairly low overhead.
>>>
>>> Matei
>>>
>>>> On Mar 8, 2015, at 5:56 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>>
>>>> Ah. I misunderstood that Matei was referring to the Scala 2.11 tarball
>>>> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
>>>> Maven artifacts.
>>>>
>>>> Patrick I see you just commented on SPARK-5134 and will follow up
>>>> there. Sounds like this may accidentally not be a problem.
>>>>
>>>> On binary tarball releases, I wonder if anyone has an opinion on my
>>>> opinion that these shouldn't be distributed for specific Hadoop
>>>> *distributions* to begin with. (Won't repeat the argument here yet.)
>>>> That resolves this n x m explosion too.
>>>>
>>>> Vendors already provide their own distribution, yes, that's their job.
>>>>
>>>>
>>>> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com> w=
rote:
>>>>> Yep, otherwise this will become an N^2 problem - Scala versions X Had=
oop
>>>>> Distributions X ...
>>>>>
>>>>> May be one option is to have a minimum basic set (which I know is wha=
t we
>>>>> are discussing) and move the rest to spark-packages.org. There the ve=
ndors
>>>>> can add the latest downloads - for example when 1.4 is released, HDP =
can
>>>>> build a release of HDP Spark 1.4 bundle.
>>>>>
>>>>> Cheers
>>>>> <k/>
>>>>>
>>>>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>>>>>>
>>>>>> We probably want to revisit the way we do binaries in general for
>>>>>> 1.4+. IMO, something worth forking a separate thread for.
>>>>>>
>>>>>> I've been hesitating to add new binaries because people
>>>>>> (understandably) complain if you ever stop packaging older ones, but
>>>>>> on the other hand the ASF has complained that we have too many
>>>>>> binaries already and that we need to pare it down because of the lar=
ge
>>>>>> volume of files. Doubling the number of binaries we produce for Scal=
a
>>>>>> 2.11 seemed like it would be too much.
>>>>>>
>>>>>> One solution potentially is to actually package "Hadoop provided"
>>>>>> binaries and encourage users to use these by simply setting
>>>>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>>>>> that our existing packages don't work well on HDP for instance, sinc=
e
>>>>>> there are some configuration quirks that differ from the upstream
>>>>>> Hadoop.
>>>>>>
>>>>>> If we cut down on the cross building for Hadoop versions, then it is
>>>>>> more tenable to cross build for Scala versions without exploding the
>>>>>> number of binaries.
>>>>>>
>>>>>> - Patrick
>>>>>>
>>>>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrot=
e:
>>>>>>> Yeah, interesting question of what is the better default for the
>>>>>>> single set of artifacts published to Maven. I think there's an
>>>>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>>>>>>> and cons discussed more at
>>>>>>>
>>>>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>>>>> https://github.com/apache/spark/pull/3917
>>>>>>>
>>>>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.=
com>
>>>>>>> wrote:
>>>>>>>> +1
>>>>>>>>
>>>>>>>> Tested it on Mac OS X.
>>>>>>>>
>>>>>>>> One small issue I noticed is that the Scala 2.11 build is using Ha=
doop
>>>>>>>> 1 without Hive, which is kind of weird because people will more li=
kely want
>>>>>>>> Hadoop 2 with Hive. So it would be good to publish a build for tha=
t
>>>>>>>> configuration instead. We can do it if we do a new RC, or it might=
 be that
>>>>>>>> binary builds may not need to be voted on (I forgot the details th=
ere).
>>>>>>>>
>>>>>>>> Matei
>>>>>>
>>>>>> --------------------------------------------------------------------=
-
>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>
>>>>>
>>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 13:01:58 2015
Return-Path: <dev-return-11915-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3708217606
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 13:01:58 +0000 (UTC)
Received: (qmail 6093 invoked by uid 500); 9 Mar 2015 13:01:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6020 invoked by uid 500); 9 Mar 2015 13:01:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49160 invoked by uid 99); 9 Mar 2015 12:35:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 12:35:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.175 as permitted sender)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 12:35:11 +0000
Received: by qcwr17 with SMTP id r17so13909631qcw.2
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 05:34:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=+M6HVdMpPbBnpELZePTdY546lw1jxofUVXW6iSEXGtk=;
        b=Uwv0emRr+6QQ50rLL8s8SYhYdDVzIRgLBS8PUwwbGnCJErX5wIhJM8p6Sktem8dx6H
         Y5Qyxu60qGf5YFkMLA64bqyygidDbD/SJTmiHABxncaF8jL2kmG1ktz0RlhFjhnJdKdP
         qw+oAE5gmdF/4mf5nXAORmh7mvToaRslnkcuCw2pPR3UU+uOXx76Bkoe3DL6qnPE9dvG
         IAg0Zk1WZw8k7cr8TGEPa9qg/QSUznwfLukG+qg3mDGG1fAeGyvKmz7TWXPAkRtJFZZQ
         1eDMnk6d1D7Q5lS3cd/OFj77WqpcrVcpaOgXy6gacKcvneka1KN7plbzWDoSYzEj3/Gp
         3xog==
MIME-Version: 1.0
X-Received: by 10.140.38.100 with SMTP id s91mr34729216qgs.37.1425904491015;
 Mon, 09 Mar 2015 05:34:51 -0700 (PDT)
Received: by 10.140.33.131 with HTTP; Mon, 9 Mar 2015 05:34:50 -0700 (PDT)
In-Reply-To: <CAMAsSdKXsmsbSV-Mueq_A8vSR+ZY80CnkrDjNC6iF=wGrE49Lg@mail.gmail.com>
References: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
	<CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com>
	<CAMAsSdKivUZSyL4y-_FONf7beLpUR8uHrhjtE2aCAZAjHJC3jw@mail.gmail.com>
	<A7557B5E-1185-4EAC-859B-B399246D81A1@gmail.com>
	<CAMAsSdKXsmsbSV-Mueq_A8vSR+ZY80CnkrDjNC6iF=wGrE49Lg@mail.gmail.com>
Date: Mon, 9 Mar 2015 05:34:50 -0700
Message-ID: <CAJiQeY+Hg-XHBD6w-6DRiJvr9W3fqUAWiVRSsfZifQmDruZoQQ@mail.gmail.com>
Subject: Re: Release Scala version vs Hadoop version (was: [VOTE] Release
 Apache Spark 1.3.0 (RC3))
From: Mridul Muralidharan <mridul@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

In ideal situation, +1 on removing all vendor specific builds and
making just hadoop version specific - that is what we should depend on
anyway.
Though I hope Sean is correct in assuming that vendor specific builds
for hadoop 2.4 are just that; and not 2.4- or 2.4+ which cause
incompatibilities for us or our users !

Regards,
Mridul


On Mon, Mar 9, 2015 at 2:50 AM, Sean Owen <sowen@cloudera.com> wrote:
> Yes, you should always find working bits at Apache no matter what --
> though 'no matter what' really means 'as long as you use Hadoop distro
> compatible with upstream Hadoop'. Even distros have a strong interest
> in that, since the market, the 'pie', is made large by this kind of
> freedom at the core.
>
> If tso, then no vendor-specific builds are needed, only some
> Hadoop-release-specific ones. So a Hadoop 2.6-specific build could be
> good (although I'm not yet clear if there's something about 2.5 or 2.6
> that needs a different build.)
>
> I take it that we already believe that, say, the "Hadoop 2.4" build
> works with CDH5, so no CDH5-specific build is provided by Spark.
>
> If a distro doesn't work with stock Spark, then it's either something
> Spark should fix (e.g. use of a private YARN API or something), or
> it's something the distro should really fix because it's incompatible.
>
> Could we maybe rename the "CDH4" build then, as it doesn't really work
> with all CDH4, to be a "Hadoop 2.0.x build"? That's been floated
> before. And can we remove the MapR builds -- or else can someone
> explain why these exist separately from a Hadoop 2.3 build? I hope it
> is not *because* they are somehow non-standard. And shall we first run
> down why Spark doesn't fully work on HDP and see if it's something
> that Spark or HDP needs to tweak, rather than contemplate another
> binary? or, if so, can it simply be called a "Hadoop 2.7 + YARN
> whatever" build and not made specific to a vendor, even if the project
> has to field another tarball combo for a vendor?
>
> Maybe we are saying almost the same thing.
>
>
> On Mon, Mar 9, 2015 at 1:33 AM, Matei Zaharia <matei.zaharia@gmail.com> w=
rote:
>> Yeah, my concern is that people should get Apache Spark from *Apache*, n=
ot from a vendor. It helps everyone use the latest features no matter where=
 they are. In the Hadoop distro case, Hadoop made all this effort to have s=
tandard APIs (e.g. YARN), so it should be easy. But it is a problem if we'r=
e not packaging for the newest versions of some distros; I think we just fe=
ll behind at Hadoop 2.4.
>>
>> Matei
>>
>>> On Mar 8, 2015, at 8:02 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>
>>> Yeah it's not much overhead, but here's an example of where it causes
>>> a little issue.
>>>
>>> I like that reasoning. However, the released builds don't track the
>>> later versions of Hadoop that vendors would be distributing -- there's
>>> no Hadoop 2.6 build for example. CDH4 is here, but not the
>>> far-more-used CDH5. HDP isn't present at all. The CDH4 build doesn't
>>> actually work with many CDH4 versions.
>>>
>>> I agree with the goal of maximizing the reach of Spark, but I don't
>>> know how much these builds advance that goal.
>>>
>>> Anyone can roll-their-own exactly-right build, and the docs and build
>>> have been set up to make that as simple as can be expected. So these
>>> aren't *required* to let me use latest Spark on distribution X.
>>>
>>> I had thought these existed to sorta support 'legacy' distributions,
>>> like CDH4, and that build was justified as a
>>> quasi-Hadoop-2.0.x-flavored build. But then I don't understand what
>>> the MapR profiles are for.
>>>
>>> I think it's too much work to correctly, in parallel, maintain any
>>> customizations necessary for any major distro, and it might be best to
>>> do not at all than to do it incompletely. You could say it's also an
>>> enabler for distros to vary in ways that require special
>>> customization.
>>>
>>> Maybe there's a concern that, if lots of people consume Spark on
>>> Hadoop, and most people consume Hadoop through distros, and distros
>>> alone manage Spark distributions, then you de facto 'have to' go
>>> through a distro instead of get bits from Spark? Different
>>> conversation but I think this sort of effect does not end up being a
>>> negative.
>>>
>>> Well anyway, I like the idea of seeing how far Hadoop-provided
>>> releases can help. It might kill several birds with one stone.
>>>
>>> On Sun, Mar 8, 2015 at 11:07 PM, Matei Zaharia <matei.zaharia@gmail.com=
> wrote:
>>>> Our goal is to let people use the latest Apache release even if vendor=
s fall behind or don't want to package everything, so that's why we put out=
 releases for vendors' versions. It's fairly low overhead.
>>>>
>>>> Matei
>>>>
>>>>> On Mar 8, 2015, at 5:56 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>>>
>>>>> Ah. I misunderstood that Matei was referring to the Scala 2.11 tarbal=
l
>>>>> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
>>>>> Maven artifacts.
>>>>>
>>>>> Patrick I see you just commented on SPARK-5134 and will follow up
>>>>> there. Sounds like this may accidentally not be a problem.
>>>>>
>>>>> On binary tarball releases, I wonder if anyone has an opinion on my
>>>>> opinion that these shouldn't be distributed for specific Hadoop
>>>>> *distributions* to begin with. (Won't repeat the argument here yet.)
>>>>> That resolves this n x m explosion too.
>>>>>
>>>>> Vendors already provide their own distribution, yes, that's their job=
.
>>>>>
>>>>>
>>>>> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com> =
wrote:
>>>>>> Yep, otherwise this will become an N^2 problem - Scala versions X Ha=
doop
>>>>>> Distributions X ...
>>>>>>
>>>>>> May be one option is to have a minimum basic set (which I know is wh=
at we
>>>>>> are discussing) and move the rest to spark-packages.org. There the v=
endors
>>>>>> can add the latest downloads - for example when 1.4 is released, HDP=
 can
>>>>>> build a release of HDP Spark 1.4 bundle.
>>>>>>
>>>>>> Cheers
>>>>>> <k/>
>>>>>>
>>>>>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com>=
 wrote:
>>>>>>>
>>>>>>> We probably want to revisit the way we do binaries in general for
>>>>>>> 1.4+. IMO, something worth forking a separate thread for.
>>>>>>>
>>>>>>> I've been hesitating to add new binaries because people
>>>>>>> (understandably) complain if you ever stop packaging older ones, bu=
t
>>>>>>> on the other hand the ASF has complained that we have too many
>>>>>>> binaries already and that we need to pare it down because of the la=
rge
>>>>>>> volume of files. Doubling the number of binaries we produce for Sca=
la
>>>>>>> 2.11 seemed like it would be too much.
>>>>>>>
>>>>>>> One solution potentially is to actually package "Hadoop provided"
>>>>>>> binaries and encourage users to use these by simply setting
>>>>>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>>>>>> that our existing packages don't work well on HDP for instance, sin=
ce
>>>>>>> there are some configuration quirks that differ from the upstream
>>>>>>> Hadoop.
>>>>>>>
>>>>>>> If we cut down on the cross building for Hadoop versions, then it i=
s
>>>>>>> more tenable to cross build for Scala versions without exploding th=
e
>>>>>>> number of binaries.
>>>>>>>
>>>>>>> - Patrick
>>>>>>>
>>>>>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wro=
te:
>>>>>>>> Yeah, interesting question of what is the better default for the
>>>>>>>> single set of artifacts published to Maven. I think there's an
>>>>>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pro=
s
>>>>>>>> and cons discussed more at
>>>>>>>>
>>>>>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>>>>>> https://github.com/apache/spark/pull/3917
>>>>>>>>
>>>>>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail=
.com>
>>>>>>>> wrote:
>>>>>>>>> +1
>>>>>>>>>
>>>>>>>>> Tested it on Mac OS X.
>>>>>>>>>
>>>>>>>>> One small issue I noticed is that the Scala 2.11 build is using H=
adoop
>>>>>>>>> 1 without Hive, which is kind of weird because people will more l=
ikely want
>>>>>>>>> Hadoop 2 with Hive. So it would be good to publish a build for th=
at
>>>>>>>>> configuration instead. We can do it if we do a new RC, or it migh=
t be that
>>>>>>>>> binary builds may not need to be voted on (I forgot the details t=
here).
>>>>>>>>>
>>>>>>>>> Matei
>>>>>>>
>>>>>>> -------------------------------------------------------------------=
--
>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>
>>>>>>
>>>>
>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 13:03:50 2015
Return-Path: <dev-return-11916-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC44417625
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 13:03:50 +0000 (UTC)
Received: (qmail 16631 invoked by uid 500); 9 Mar 2015 13:03:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16548 invoked by uid 500); 9 Mar 2015 13:03:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25442 invoked by uid 99); 9 Mar 2015 12:22:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 12:22:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pahomov.egor@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 12:21:44 +0000
Received: by qgdq107 with SMTP id q107so27652622qgd.6
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 05:21:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=h3PVO8rkCsm0155v5Hb0agjJcmFnyO5bBXSe63/03P4=;
        b=DGCCMPW23jysVD+sWoAg9+Z6BGJ4v9YR7D3CqK6o70vCu+ei5lV31A1ttrcHW4R2Wp
         bxK5knPAA1qCDyJ9/QTzbVqGrwuQzuy9nX+VJ7nEBh2o4tWtlJTQj7/CWPbMXCqPem4o
         1PQIVrWq0O0XkJfYvrDIKJnZBucrlkPa3H5zkdfJ1Xng8I+v7w6PMY7ekYg2pysOWwBh
         xJAvbFpYKCSV1G8X2wKHd29iCP0K9ZwxHTtoFdfsISjQXtDUuxqXcxF43Jv9FxEIStW5
         iwxSA2i3cK2aU7uv85Rq94O1pbgD9V5Dx6pfbAKZaCYh2xA5nAT6oP80FJLz6S+DumH2
         EKHw==
MIME-Version: 1.0
X-Received: by 10.55.42.219 with SMTP id q88mr13358446qkq.3.1425903702801;
 Mon, 09 Mar 2015 05:21:42 -0700 (PDT)
Received: by 10.140.38.177 with HTTP; Mon, 9 Mar 2015 05:21:42 -0700 (PDT)
Date: Mon, 9 Mar 2015 15:21:42 +0300
Message-ID: <CAMrx5DypOztk=1AOUyPPzaRU5Nu1h4kTp00iB2FWOrfBAsd0bQ@mail.gmail.com>
Subject: How to implement unsupervised or reinforcement algorithm in new org.apache.spark.ml
From: Egor Pahomov <pahomov.egor@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1147ba8cb082b00510da1347
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1147ba8cb082b00510da1347
Content-Type: text/plain; charset=UTF-8

Hi, I'm redoing my PR <https://github.com/apache/spark/pull/2731> about
genetic algorithm in new org.apache.spark.ml architecture. Do we have
already some code about handling unsupervised or reinforcement algorithm in
new architecture? If no do we have some tickets on this matter? If no do we
have understanding when it would be doing, and how?

-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*

--001a1147ba8cb082b00510da1347--

From dev-return-11917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 16:17:38 2015
Return-Path: <dev-return-11917-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CF96B17DC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 16:17:38 +0000 (UTC)
Received: (qmail 4517 invoked by uid 500); 9 Mar 2015 16:17:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4434 invoked by uid 500); 9 Mar 2015 16:17:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4423 invoked by uid 99); 9 Mar 2015 16:17:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 16:17:37 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.23 as permitted sender)
Received: from [216.64.169.23] (HELO pdxmta02.webtrends.com) (216.64.169.23)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 16:17:33 +0000
Received: from pdxex2.webtrends.corp (Not Verified[10.61.2.221]) by pdxmta02.webtrends.com with MailMarshal (v7,2,3,6978) (using TLS: SSLv23)
	id <B54fdc78d0000>; Mon, 09 Mar 2015 16:17:17 +0000
Received: from PDXEX1.WebTrends.corp ([172.27.5.220]) by pdxex2.webtrends.corp
 ([172.27.3.221]) with mapi id 14.03.0224.002; Mon, 9 Mar 2015 16:17:12 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
CC: Krishna Sankar <ksankar42@gmail.com>, Patrick Wendell
	<pwendell@gmail.com>, Sean Owen <sowen@cloudera.com>, Matei Zaharia
	<matei.zaharia@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
Thread-Topic: [VOTE] Release Apache Spark 1.3.0 (RC3)
Thread-Index: AQHQV7jFWyDdLI9/iEKVVLEA34gFsJ0TAJaAgAABKICAABejgIAACMqAgACIgACAAK7CgA==
Date: Mon, 9 Mar 2015 16:17:12 +0000
Message-ID: <1861C724-0757-466C-8D66-A3E7ED39DC84@webtrends.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <2800A0EB-8A3C-4C54-A8DB-D2BD7B62B8B0@gmail.com>
 <CAMAsSd+0A=hfri7W1_Vj9rM-=wGf7PHgPQ+WXVSv=VmpEaXFzQ@mail.gmail.com>
 <CABPQxstobDfLyiGtejaJ1+f-E8pj_OMZLygYHNiCkdjPmy2T7g@mail.gmail.com>
 <CAOTBr2mjAtenG1R7rJ4-0ZL_ZENSFxvngA_nuk5zkfPcnkgF9Q@mail.gmail.com>
 <CACBYxKKx19cNF8jCADo0XcrJgt9DzT3i6FejB-1qMj_uZyXtnA@mail.gmail.com>
In-Reply-To: <CACBYxKKx19cNF8jCADo0XcrJgt9DzT3i6FejB-1qMj_uZyXtnA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.223]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <056DB9DA7E662548BB264DB12AAFC0FB@WebTrends.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Ran local tests and tested our spark apps on a spark+yarn cluster.

Cheers,

Sean


> On Mar 8, 2015, at 11:51 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>=20
> +1 (non-binding, doc and packaging issues aside)
>=20
> Built from source, ran jobs and spark-shell against a pseudo-distributed
> YARN cluster.
>=20
> On Sun, Mar 8, 2015 at 2:42 PM, Krishna Sankar <ksankar42@gmail.com> wrot=
e:
>=20
>> Yep, otherwise this will become an N^2 problem - Scala versions X Hadoop
>> Distributions X ...
>>=20
>> May be one option is to have a minimum basic set (which I know is what w=
e
>> are discussing) and move the rest to spark-packages.org. There the vendo=
rs
>> can add the latest downloads - for example when 1.4 is released, HDP can
>> build a release of HDP Spark 1.4 bundle.
>>=20
>> Cheers
>> <k/>
>>=20
>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>=20
>>> We probably want to revisit the way we do binaries in general for
>>> 1.4+. IMO, something worth forking a separate thread for.
>>>=20
>>> I've been hesitating to add new binaries because people
>>> (understandably) complain if you ever stop packaging older ones, but
>>> on the other hand the ASF has complained that we have too many
>>> binaries already and that we need to pare it down because of the large
>>> volume of files. Doubling the number of binaries we produce for Scala
>>> 2.11 seemed like it would be too much.
>>>=20
>>> One solution potentially is to actually package "Hadoop provided"
>>> binaries and encourage users to use these by simply setting
>>> HADOOP_HOME, or have instructions for specific distros. I've heard
>>> that our existing packages don't work well on HDP for instance, since
>>> there are some configuration quirks that differ from the upstream
>>> Hadoop.
>>>=20
>>> If we cut down on the cross building for Hadoop versions, then it is
>>> more tenable to cross build for Scala versions without exploding the
>>> number of binaries.
>>>=20
>>> - Patrick
>>>=20
>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com> wrote:
>>>> Yeah, interesting question of what is the better default for the
>>>> single set of artifacts published to Maven. I think there's an
>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too. Pros
>>>> and cons discussed more at
>>>>=20
>>>> https://issues.apache.org/jira/browse/SPARK-5134
>>>> https://github.com/apache/spark/pull/3917
>>>>=20
>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <matei.zaharia@gmail.com
>>>=20
>>> wrote:
>>>>> +1
>>>>>=20
>>>>> Tested it on Mac OS X.
>>>>>=20
>>>>> One small issue I noticed is that the Scala 2.11 build is using Hadoo=
p
>>> 1 without Hive, which is kind of weird because people will more likely
>> want
>>> Hadoop 2 with Hive. So it would be good to publish a build for that
>>> configuration instead. We can do it if we do a new RC, or it might be
>> that
>>> binary builds may not need to be voted on (I forgot the details there).
>>>>>=20
>>>>> Matei
>>>=20
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 16:18:33 2015
Return-Path: <dev-return-11918-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE28A17DCC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 16:18:33 +0000 (UTC)
Received: (qmail 11939 invoked by uid 500); 9 Mar 2015 16:18:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11871 invoked by uid 500); 9 Mar 2015 16:18:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11859 invoked by uid 99); 9 Mar 2015 16:18:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 16:18:16 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FORGED_YAHOO_RCVD,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 67.195.87.147 as permitted sender)
Received: from [67.195.87.147] (HELO nm45.bullet.mail.gq1.yahoo.com) (67.195.87.147)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 16:18:08 +0000
Received: from [127.0.0.1] by nm45.bullet.mail.gq1.yahoo.com with NNFMP; 09 Mar 2015 16:16:45 -0000
Received: from [98.137.12.61] by nm45.bullet.mail.gq1.yahoo.com with NNFMP; 09 Mar 2015 16:13:39 -0000
Received: from [98.139.215.141] by tm6.bullet.mail.gq1.yahoo.com with NNFMP; 09 Mar 2015 16:13:23 -0000
Received: from [98.139.215.229] by tm12.bullet.mail.bf1.yahoo.com with NNFMP; 09 Mar 2015 16:13:23 -0000
Received: from [127.0.0.1] by omp1069.mail.bf1.yahoo.com with NNFMP; 09 Mar 2015 16:13:23 -0000
X-Yahoo-Newman-Property: ymail-4
X-Yahoo-Newman-Id: 441739.88892.bm@omp1069.mail.bf1.yahoo.com
X-YMail-OSG: xfQu_kgVM1mP5HdecfmewPRr.4vXCFrdNXCrRqc1GSX7j5KtspZjDK5E4wOWt3z
 carft2bd6JAe5maPrvonb7UPFPChmyL1NmqVXXGTE1pe1NfGgUxF_NvYNK1eLEx1.oLN8el6PXPh
 WbA06cZowHpSKjavYWQ9KyNL9RhdYD6azlHZhWLgZWZfjF5hphX9gJ0iFnjIfeihiq.pkYRq3KZg
 C5tF7jsLnrqUEHoKKXy87DqWM1Uft3HY3RdOTpeqmbWhAw7q1PiAf6sfrrbrTPVwvReSvApMlKb0
 a0jatupXtZmIE.vBfKEQt9l27DQcsAD96TdRjiyZ9q44z9YZ4VwZkNHii75vppFJU63yW2KyPkTG
 pxkEIH9WfEqik8o7wSbGVGMQHUc_Z5TZH2iDm7fh4CfoEaNnwq1gtWggBILADWy3Ato6PCWQvbIq
 uaNNmyUHNTByl7xB2KBOWWe7U8P.5jCWYj.A2BUalIPO7YcOMXyl2OscNVSxk7Pq1_um_Ud.XFvH
 e1mb.FtDwGTeILyDc58.eZshHtutIDeFXytM76tXGc85QCNEXt1IQCK2SmCiJDX.fwrJI7cL8bzf
 Ws9Ixq9u.L19k
Received: by 66.196.81.116; Mon, 09 Mar 2015 16:13:22 +0000 
Date: Mon, 9 Mar 2015 16:13:22 +0000 (UTC)
From: Tom Graves <tgraves_cs@yahoo.com.INVALID>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
To: Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <1064497734.1624411.1425917602457.JavaMail.yahoo@mail.yahoo.com>
In-Reply-To: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_1624410_1377975904.1425917602452"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_1624410_1377975904.1425917602452
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: 7bit

+1. Built from source and ran Spark on yarn on hadoop 2.6 in cluster and client mode.
Tom 

     On Thursday, March 5, 2015 8:53 PM, Patrick Wendell <pwendell@gmail.com> wrote:
   

 Please vote on releasing the following candidate as Apache Spark version 1.3.0!

The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc3/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

Staging repositories for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1078

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/

Please vote on releasing this package as Apache Spark 1.3.0!

The vote is open until Monday, March 09, at 02:52 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.3.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

== How does this compare to RC2 ==
This release includes the following bug fixes:

https://issues.apache.org/jira/browse/SPARK-6144
https://issues.apache.org/jira/browse/SPARK-6171
https://issues.apache.org/jira/browse/SPARK-5143
https://issues.apache.org/jira/browse/SPARK-6182
https://issues.apache.org/jira/browse/SPARK-6175

== How can I help test this release? ==
If you are a Spark user, you can help us test this release by
taking a Spark 1.2 workload and running on this release candidate,
then reporting any regressions.

If you are happy with this release based on your own testing, give a +1 vote.

== What justifies a -1 vote for this release? ==
This vote is happening towards the end of the 1.3 QA period,
so -1 votes should only occur for significant regressions from 1.2.1.
Bugs already present in 1.2.X, minor regressions, or bugs related
to new features will not block this release.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org



    
------=_Part_1624410_1377975904.1425917602452--

From dev-return-11919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 16:36:40 2015
Return-Path: <dev-return-11919-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 087D017EC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 16:36:40 +0000 (UTC)
Received: (qmail 70582 invoked by uid 500); 9 Mar 2015 16:36:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70511 invoked by uid 500); 9 Mar 2015 16:36:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70499 invoked by uid 99); 9 Mar 2015 16:36:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 16:36:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of denny.g.lee@gmail.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 16:36:26 +0000
Received: by iecvy18 with SMTP id vy18so40667733iec.1
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 09:36:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :content-type;
        bh=DllbEQGbA8sJ/iBdNILwLHYfPCPLIVKHVex9gkZfRt4=;
        b=Gh82tL1qvfl6O3WH8WVOsEUqbyNGiu477Pq4WQvP95lB6Xb1fkZqASFvn40kH344mn
         YFiG+bH9u45P8s66yW9JjkejgAyp4Ej578j98jj05Kx3PFmlPcNhp4o1cmyaDVV0Pm9/
         uyFh5j9oSb2nSKJz4YEV0vafXnaK7YAr8bbl7xJh4MXrW+RHg/fi1ZuSD1oWHUM3BZaY
         B4+kFQVkaS8QuZaRbqsTBfUPAtyceIxAezpu3g5ohFejEfmlBy47zqu5chhpqnpEB+qF
         kTi6mWEOZLiSKVbOGUUlxrapvr7J7MAFLkiWa2sEKyOUDy5SWzxCpZRQ9YOwXTIkvwP6
         L4OA==
X-Received: by 10.50.43.198 with SMTP id y6mr76059052igl.16.1425918966403;
 Mon, 09 Mar 2015 09:36:06 -0700 (PDT)
MIME-Version: 1.0
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <1064497734.1624411.1425917602457.JavaMail.yahoo@mail.yahoo.com>
In-Reply-To: <1064497734.1624411.1425917602457.JavaMail.yahoo@mail.yahoo.com>
From: Denny Lee <denny.g.lee@gmail.com>
Date: Mon, 09 Mar 2015 16:36:05 +0000
Message-ID: <CABjYQ39MFFnRWAAkcZ2r-C7UiZWhJzq-8_qDH-SWokR=qX1=2g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Tom Graves <tgraves_cs@yahoo.com>, Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01537a96789c240510dda116
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01537a96789c240510dda116
Content-Type: text/plain; charset=UTF-8

+1 (non-binding)

Spark Standalone and YARN on Hadoop 2.6 on OSX plus various tests (MLLib,
SparkSQL, etc.)

On Mon, Mar 9, 2015 at 9:18 AM Tom Graves <tgraves_cs@yahoo.com.invalid>
wrote:

> +1. Built from source and ran Spark on yarn on hadoop 2.6 in cluster and
> client mode.
> Tom
>
>      On Thursday, March 5, 2015 8:53 PM, Patrick Wendell <
> pwendell@gmail.com> wrote:
>
>
>  Please vote on releasing the following candidate as Apache Spark version
> 1.3.0!
>
> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=
> 4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> Staging repositories for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1078
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>
> Please vote on releasing this package as Apache Spark 1.3.0!
>
> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.3.0
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> == How does this compare to RC2 ==
> This release includes the following bug fixes:
>
> https://issues.apache.org/jira/browse/SPARK-6144
> https://issues.apache.org/jira/browse/SPARK-6171
> https://issues.apache.org/jira/browse/SPARK-5143
> https://issues.apache.org/jira/browse/SPARK-6182
> https://issues.apache.org/jira/browse/SPARK-6175
>
> == How can I help test this release? ==
> If you are a Spark user, you can help us test this release by
> taking a Spark 1.2 workload and running on this release candidate,
> then reporting any regressions.
>
> If you are happy with this release based on your own testing, give a +1
> vote.
>
> == What justifies a -1 vote for this release? ==
> This vote is happening towards the end of the 1.3 QA period,
> so -1 votes should only occur for significant regressions from 1.2.1.
> Bugs already present in 1.2.X, minor regressions, or bugs related
> to new features will not block this release.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
>
>

--089e01537a96789c240510dda116--

From dev-return-11920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 17:39:01 2015
Return-Path: <dev-return-11920-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0561817394
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 17:39:01 +0000 (UTC)
Received: (qmail 38031 invoked by uid 500); 9 Mar 2015 17:38:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37955 invoked by uid 500); 9 Mar 2015 17:38:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37878 invoked by uid 99); 9 Mar 2015 17:38:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 17:38:59 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 17:38:51 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id 5AC37A3;
	Mon,  9 Mar 2015 17:38:00 +0000 (UTC)
Received: from G9W3613.americas.hpqcorp.net (16.216.186.48) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 9 Mar 2015 17:37:25 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G9W3613.americas.hpqcorp.net ([16.216.186.48]) with mapi id 14.03.0169.001;
 Mon, 9 Mar 2015 17:37:26 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Akhil Das <akhil@sigmoidanalytics.com>
CC: dev <dev@spark.apache.org>
Subject: RE: Loading previously serialized object to Spark
Thread-Topic: Loading previously serialized object to Spark
Thread-Index: AdBYT9oKXKom1WKWRnOearMR4wGkngBOTRQAAEFprwA=
Date: Mon, 9 Mar 2015 17:37:25 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE16976@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
 <CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
In-Reply-To: <CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE16976G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE16976G4W3292americas_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE16976G4W3292americas_--

From dev-return-11921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 17:56:56 2015
Return-Path: <dev-return-11921-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E3181746E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 17:56:56 +0000 (UTC)
Received: (qmail 95647 invoked by uid 500); 9 Mar 2015 17:56:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95572 invoked by uid 500); 9 Mar 2015 17:56:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95559 invoked by uid 99); 9 Mar 2015 17:56:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 17:56:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 17:56:50 +0000
Received: by igal13 with SMTP id l13so23625460iga.0
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 10:54:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=UUSB1z/+V3tsKDsJ9vJz5JtLlEBYAzfzgna3vMUD910=;
        b=dr+qRfXpBkUl4Z1v0ymPcUHufN8yvKSGH/V5Tj3Lp8NcVG2B2pSHjOn9CXvTuM6wec
         TVhE/RCOmPA/gOWqGKtagAh8xlnaq7lzWrm9CBUT/3Cs0MzDvYEBNq9C75xrgwOYd0s6
         tXYFD6dVVb2gkqkJQijRrHCroPLnOcO9TBpcLi7YPDTJRHwV2bi+Xl6ZqlAZ7pmm3j6k
         a+CtAKjZ1uik0RFAPqEWbgrXFjuNW2q0GURzj3es0DYMqqThzVG3CCEbh4IuxGMqR4Eh
         iRAIzXiuCQ2CnKeY5roNi1iugwU7VzJYzCyRX0mN3Q9LgcAF/r4K1w4dVrdHPx1oB8LW
         ielA==
X-Gm-Message-State: ALoCoQmrme8u8IwHQG+N2U1IT8PoSDOIs6FeD5ySQVkwWQDC0aDBc+SB7GG1ohhmgw9oiqn4gJJR
MIME-Version: 1.0
X-Received: by 10.42.110.10 with SMTP id n10mr28928576icp.65.1425923679157;
 Mon, 09 Mar 2015 10:54:39 -0700 (PDT)
Received: by 10.36.118.18 with HTTP; Mon, 9 Mar 2015 10:54:39 -0700 (PDT)
In-Reply-To: <CAMrx5DypOztk=1AOUyPPzaRU5Nu1h4kTp00iB2FWOrfBAsd0bQ@mail.gmail.com>
References: <CAMrx5DypOztk=1AOUyPPzaRU5Nu1h4kTp00iB2FWOrfBAsd0bQ@mail.gmail.com>
Date: Mon, 9 Mar 2015 10:54:39 -0700
Message-ID: <CAF7ADNqfKQdSHk60eQeYXK2-m9wOwwWU6tyvoR5yJitkyZx4jA@mail.gmail.com>
Subject: Re: How to implement unsupervised or reinforcement algorithm in new org.apache.spark.ml
From: Joseph Bradley <joseph@databricks.com>
To: Egor Pahomov <pahomov.egor@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303dd3845f956a0510debac3
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303dd3845f956a0510debac3
Content-Type: text/plain; charset=UTF-8

Hi,

There are no examples currently.  For unsupervised learning, I think the
pattern is straightforward.  It would follow the pattern from supervised
learning, but without the label input column and with a model having a
different transform() behavior.

Reinforcement learning might take a bit more design since I haven't seen
work on it so far.  I'd recommend making a Discussion JIRA to post a set of
requirements and get feedback on a design.  Reinforcement learning would be
great to have in MLlib.

Joseph

On Mon, Mar 9, 2015 at 5:21 AM, Egor Pahomov <pahomov.egor@gmail.com> wrote:

> Hi, I'm redoing my PR <https://github.com/apache/spark/pull/2731> about
> genetic algorithm in new org.apache.spark.ml architecture. Do we have
> already some code about handling unsupervised or reinforcement algorithm in
> new architecture? If no do we have some tickets on this matter? If no do we
> have understanding when it would be doing, and how?
>
> --
>
>
>
> *Sincerely yoursEgor PakhomovScala Developer, Yandex*
>

--20cf303dd3845f956a0510debac3--

From dev-return-11922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 18:27:46 2015
Return-Path: <dev-return-11922-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 948C417693
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 18:27:46 +0000 (UTC)
Received: (qmail 13504 invoked by uid 500); 9 Mar 2015 18:27:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13420 invoked by uid 500); 9 Mar 2015 18:27:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13408 invoked by uid 99); 9 Mar 2015 18:27:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 18:27:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 18:27:39 +0000
Received: by obbnt9 with SMTP id nt9so10924578obb.12
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 11:27:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=X9ITjnS9/2I+OX7POdgOagv9t3YT92T8X9T8DnNhbNU=;
        b=HK4770B7TKPHNL3BLjPh75dNnbvh7uE6c0IsMOMnBKkvU4trLiUvJAqapYiHjri/lJ
         BV/j7UIfjo8JivsKWSXmoYqEFXOxeNEyiDa70ehQqZks3vghFYFfbvt7lEoeAAQyZWwc
         dJqsWf9u8IzrBl0RgrQErO457qae1LYCO1g12v1Y/51Yt0SvTj7/E6OT4gtZ5Hocf9db
         5TM/cwZ3T3Zk3EHy58Mc93CG/8LyY3vViokpj+CBJ6A9r0pj/rGESxo/EVIukESuEI9N
         uTqsbkBJnClGIhLnPufltDrK7DZhbRLg1yMxZtEE+EdPp/8aUrF+LiR2EoQJSDrL5/lo
         bTrg==
MIME-Version: 1.0
X-Received: by 10.202.196.137 with SMTP id u131mr21443261oif.78.1425925638758;
 Mon, 09 Mar 2015 11:27:18 -0700 (PDT)
Received: by 10.202.226.137 with HTTP; Mon, 9 Mar 2015 11:27:18 -0700 (PDT)
In-Reply-To: <CABjYQ39MFFnRWAAkcZ2r-C7UiZWhJzq-8_qDH-SWokR=qX1=2g@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<1064497734.1624411.1425917602457.JavaMail.yahoo@mail.yahoo.com>
	<CABjYQ39MFFnRWAAkcZ2r-C7UiZWhJzq-8_qDH-SWokR=qX1=2g@mail.gmail.com>
Date: Mon, 9 Mar 2015 11:27:18 -0700
Message-ID: <CABPQxstJE=zbbULUvBsUtK7+X_yNhY-TS2WUiFHorxSAGTkVOw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Today there was a JIRA posted with an observed regression around Spark
Streaming during certain recovery scenarios:

https://issues.apache.org/jira/browse/SPARK-6222

My preference is to go ahead and ship this release (RC3) as-is and if
this issue is isolated resolved soon, we can make a patch release in
the next week or two.

At some point, the cost of continuing to hold the release re/vote is
so high that it's better to just ship the release. We can document
known issues and point users to a fix once it's available. We did this
in 1.2.0 as well (there were two small known issues) and I think as a
point of process, this approach is necessary given the size of the
project.

I wanted to notify this thread though, in case this change anyones
opinion on their release vote. I will leave the thread open at least
until the end of today.

Still +1 on RC3, for me.

- Patrick

On Mon, Mar 9, 2015 at 9:36 AM, Denny Lee <denny.g.lee@gmail.com> wrote:
> +1 (non-binding)
>
> Spark Standalone and YARN on Hadoop 2.6 on OSX plus various tests (MLLib,
> SparkSQL, etc.)
>
> On Mon, Mar 9, 2015 at 9:18 AM Tom Graves <tgraves_cs@yahoo.com.invalid>
> wrote:
>>
>> +1. Built from source and ran Spark on yarn on hadoop 2.6 in cluster and
>> client mode.
>> Tom
>>
>>      On Thursday, March 5, 2015 8:53 PM, Patrick Wendell
>> <pwendell@gmail.com> wrote:
>>
>>
>>  Please vote on releasing the following candidate as Apache Spark version
>> 1.3.0!
>>
>> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> Staging repositories for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1078
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.3.0!
>>
>> The vote is open until Monday, March 09, at 02:52 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.3.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> == How does this compare to RC2 ==
>> This release includes the following bug fixes:
>>
>> https://issues.apache.org/jira/browse/SPARK-6144
>> https://issues.apache.org/jira/browse/SPARK-6171
>> https://issues.apache.org/jira/browse/SPARK-5143
>> https://issues.apache.org/jira/browse/SPARK-6182
>> https://issues.apache.org/jira/browse/SPARK-6175
>>
>> == How can I help test this release? ==
>> If you are a Spark user, you can help us test this release by
>> taking a Spark 1.2 workload and running on this release candidate,
>> then reporting any regressions.
>>
>> If you are happy with this release based on your own testing, give a +1
>> vote.
>>
>> == What justifies a -1 vote for this release? ==
>> This vote is happening towards the end of the 1.3 QA period,
>> so -1 votes should only occur for significant regressions from 1.2.1.
>> Bugs already present in 1.2.X, minor regressions, or bugs related
>> to new features will not block this release.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 18:55:12 2015
Return-Path: <dev-return-11923-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 88BE4177D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 18:55:12 +0000 (UTC)
Received: (qmail 13273 invoked by uid 500); 9 Mar 2015 18:55:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13194 invoked by uid 500); 9 Mar 2015 18:55:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13183 invoked by uid 99); 9 Mar 2015 18:55:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 18:55:10 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 18:55:02 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id C2FC987;
	Mon,  9 Mar 2015 18:53:41 +0000 (UTC)
Received: from G4W6304.americas.hpqcorp.net (16.210.26.229) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 9 Mar 2015 18:52:57 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G4W6304.americas.hpqcorp.net ([16.210.26.229]) with mapi id 14.03.0169.001;
 Mon, 9 Mar 2015 18:52:57 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Akhil Das <akhil@sigmoidanalytics.com>
CC: dev <dev@spark.apache.org>
Subject: RE: Loading previously serialized object to Spark
Thread-Topic: Loading previously serialized object to Spark
Thread-Index: AdBYT9oKXKom1WKWRnOearMR4wGkngBOTRQAAEFprwAAAskQkA==
Date: Mon, 9 Mar 2015 18:52:56 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE16A9F@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
 <CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE16976@G4W3292.americas.hpqcorp.net>
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE16976@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 19:12:03 2015
Return-Path: <dev-return-11924-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A57F178A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 19:12:03 +0000 (UTC)
Received: (qmail 60057 invoked by uid 500); 9 Mar 2015 19:12:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59991 invoked by uid 500); 9 Mar 2015 19:12:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59979 invoked by uid 99); 9 Mar 2015 19:12:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:12:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.223.171 as permitted sender)
Received: from [209.85.223.171] (HELO mail-ie0-f171.google.com) (209.85.223.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:11:35 +0000
Received: by iecvy18 with SMTP id vy18so42045955iec.9
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 12:10:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=XHNyW8LOwSvNNmZXKw8op2zlNVqOtxhy7rkI9dnTJGw=;
        b=aEgiRdgnP8wnKyd6iLtxvrWxaGbq1OkkHgLMcbwyir7S9L8Q+eDLkm2w4ZCqS5/DEW
         egD5XvHFxUIaZRd+uNbQIqEcFWwljJvcfGWoydhKDpGc1BQqPWDXeq9yXs4lU73GYyJJ
         zcrpdf/93t0dek7qsiqFUgj4SDJznV4oG6Fy04KmPdn2gu6pQOJjSwarseFKI4hXIYZ0
         K/TqffvKTstK2QJxbqxZWgzxz+JyBS41DPlHToWe+mBCymxEjGBElo5sERbKX4aPFbFx
         HK1Wot0YP4IX4vaUGyLcq3PMCFnqrH8EucFzAJrkZE0vDR1D9rpUYTEEdnsay4unFoEn
         oT+w==
MIME-Version: 1.0
X-Received: by 10.107.129.141 with SMTP id l13mr41680656ioi.91.1425928203830;
 Mon, 09 Mar 2015 12:10:03 -0700 (PDT)
Received: by 10.36.99.76 with HTTP; Mon, 9 Mar 2015 12:10:03 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE16A9F@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
	<CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16976@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16A9F@G4W3292.americas.hpqcorp.net>
Date: Mon, 9 Mar 2015 12:10:03 -0700
Message-ID: <CAJgQjQ-FOtXqUcsG=W05EUzuAqaxDzV7_1ehR6x43LFL8dBzpg@mail.gmail.com>
Subject: Re: Loading previously serialized object to Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Akhil Das <akhil@sigmoidanalytics.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Could you try `sc.objectFile` instead?

sc.parallelize(Seq(model), 1).saveAsObjectFile("path")
val sameModel =3D sc.objectFile[NaiveBayesModel]("path").first()

-Xiangrui

On Mon, Mar 9, 2015 at 11:52 AM, Ulanov, Alexander
<alexander.ulanov@hp.com> wrote:
> Just tried, the same happens if I use the internal Spark serializer:
> val serializer =3D SparkEnv.get.closureSerializer.newInstance
>
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Monday, March 09, 2015 10:37 AM
> To: Akhil Das
> Cc: dev
> Subject: RE: Loading previously serialized object to Spark
>
> Below is the code with standard MLlib class. Apparently this issue can ha=
ppen in the same Spark instance.
>
> import java.io._
>
> import org.apache.spark.mllib.classification.NaiveBayes
> import org.apache.spark.mllib.classification.NaiveBayesModel
> import org.apache.spark.mllib.util.MLUtils
>
> val data =3D MLUtils.loadLibSVMFile(sc, "hdfs://myserver:9000/data/mnist.=
scale")
> val nb =3D NaiveBayes.train(data)
> // RDD map works fine
> val predictionAndLabels =3D data.map( lp =3D> (nb.classifierModel.predict=
(lp.features), lp.label))
>
> // serialize the model to file and immediately load it val oos =3D new Ob=
jectOutputStream(new FileOutputStream("/home/myuser/nb.bin"))
> oos.writeObject(nb)
> oos.close
> val ois =3D new ObjectInputStream(new FileInputStream("/home/myuser/nb.bi=
n"))
> val nbSerialized =3D ois.readObject.asInstanceOf[NaiveBayesModel]
> ois.close
> // RDD map fails
> val predictionAndLabels =3D data.map( lp =3D> (nbSerialized.predict(lp.fe=
atures), lp.label))
> org.apache.spark.SparkException: Task not serializable
>         at org.apache.spark.util.ClosureCleaner$.ensureSerializable(Closu=
reCleaner.scala:166)
>         at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.sca=
la:158)
>         at org.apache.spark.SparkContext.clean(SparkContext.scala:1453)
>         at org.apache.spark.rdd.RDD.map(RDD.scala:273)
>
>
> From: Akhil Das [mailto:akhil@sigmoidanalytics.com]
> Sent: Sunday, March 08, 2015 3:17 AM
> To: Ulanov, Alexander
> Cc: dev
> Subject: Re: Loading previously serialized object to Spark
>
> Can you paste the complete code?
>
> Thanks
> Best Regards
>
> On Sat, Mar 7, 2015 at 2:25 AM, Ulanov, Alexander <alexander.ulanov@hp.co=
m<mailto:alexander.ulanov@hp.com>> wrote:
> Hi,
>
> I've implemented class MyClass in MLlib that does some operation on Label=
edPoint. MyClass extends serializable, so I can map this operation on data =
of RDD[LabeledPoints], such as data.map(lp =3D> MyClass.operate(lp)). I wri=
te this class in file with ObjectOutputStream.writeObject. Then I stop and =
restart Spark. I load this class from file with ObjectInputStream.readObjec=
t.asInstanceOf[MyClass]. When I try to map the same operation of this class=
 to RDD, Spark throws not serializable exception:
> org.apache.spark.SparkException: Task not serializable
>         at org.apache.spark.util.ClosureCleaner$.ensureSerializable(Closu=
reCleaner.scala:166)
>         at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.sca=
la:158)
>         at org.apache.spark.SparkContext.clean(SparkContext.scala:1453)
>         at org.apache.spark.rdd.RDD.map(RDD.scala:273)
>
> Could you suggest why it throws this exception while MyClass is serializa=
ble by definition?
>
> Best regards, Alexander
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 19:27:18 2015
Return-Path: <dev-return-11925-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CF70C17934
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 19:27:18 +0000 (UTC)
Received: (qmail 3665 invoked by uid 500); 9 Mar 2015 19:27:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3588 invoked by uid 500); 9 Mar 2015 19:27:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3577 invoked by uid 99); 9 Mar 2015 19:27:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:27:16 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:26:48 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id 1341071;
	Mon,  9 Mar 2015 19:26:16 +0000 (UTC)
Received: from G4W6300.americas.hpqcorp.net (16.210.26.225) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 9 Mar 2015 19:25:36 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G4W6300.americas.hpqcorp.net ([16.210.26.225]) with mapi id 14.03.0169.001;
 Mon, 9 Mar 2015 19:25:35 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Xiangrui Meng <mengxr@gmail.com>
CC: Akhil Das <akhil@sigmoidanalytics.com>, dev <dev@spark.apache.org>
Subject: RE: Loading previously serialized object to Spark
Thread-Topic: Loading previously serialized object to Spark
Thread-Index: AdBYT9oKXKom1WKWRnOearMR4wGkngBOTRQAAEFprwAAAskQkAAAswOAAABnrQA=
Date: Mon, 9 Mar 2015 19:25:34 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE16B14@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
	<CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16976@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16A9F@G4W3292.americas.hpqcorp.net>
 <CAJgQjQ-FOtXqUcsG=W05EUzuAqaxDzV7_1ehR6x43LFL8dBzpg@mail.gmail.com>
In-Reply-To: <CAJgQjQ-FOtXqUcsG=W05EUzuAqaxDzV7_1ehR6x43LFL8dBzpg@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 19:32:12 2015
Return-Path: <dev-return-11926-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ACBDC1795A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 19:32:12 +0000 (UTC)
Received: (qmail 17941 invoked by uid 500); 9 Mar 2015 19:32:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17871 invoked by uid 500); 9 Mar 2015 19:32:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17856 invoked by uid 99); 9 Mar 2015 19:32:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:32:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:31:45 +0000
Received: by igbhl2 with SMTP id hl2so22529977igb.0
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 12:31:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type:content-transfer-encoding;
        bh=bGXX39/uME8Ro5Z/0jzsFxWnSQNBpfS/mllU+hEFjdE=;
        b=Ev4byQ7qPMxj4vTxsY0esSAjd1gY25L58WC9dgAPOpS8uGU97pTQaLS++BHNtFPfxR
         gYvPpDEat4B1ipBjrN4idiNG0XJV4ppr7cGYdZ/Jx0kcTrrJSumqJcNFiVtYOChK0dIf
         xHrv2Sk571Cge/JMh3Eo8lwu9gbteONz9uGy6nkqvGlBX6gj4ztw0YtwAVpl41zzvRon
         cuXW5bEmsB+ZBc7vlaFGW18SrKvlwqq223Bw3BAhydfuHtg63dNU1gRk3GYaEZDchPmH
         wD/j3WidpuVkuALdeD+Q3LYYXF3+h5EOtlbOI/wFZi0RmoBBI6HwQMYwqIn6pfJjUGTN
         4KnQ==
MIME-Version: 1.0
X-Received: by 10.42.95.70 with SMTP id e6mr28994334icn.14.1425929503521; Mon,
 09 Mar 2015 12:31:43 -0700 (PDT)
Received: by 10.36.99.76 with HTTP; Mon, 9 Mar 2015 12:31:43 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE16B14@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
	<CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16976@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16A9F@G4W3292.americas.hpqcorp.net>
	<CAJgQjQ-FOtXqUcsG=W05EUzuAqaxDzV7_1ehR6x43LFL8dBzpg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16B14@G4W3292.americas.hpqcorp.net>
Date: Mon, 9 Mar 2015 12:31:43 -0700
Message-ID: <CAJgQjQ9g1WseS1zxw3+0y7G=v=G+7g7oNnsS_nNV6h2k=x2GvQ@mail.gmail.com>
Subject: Re: Loading previously serialized object to Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Akhil Das <akhil@sigmoidanalytics.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Well, it is the standard "hacky" way for model save/load in MLlib. We
have SPARK-4587 and SPARK-5991 to provide save/load for all MLlib
models, in an exchangeable format. -Xiangrui

On Mon, Mar 9, 2015 at 12:25 PM, Ulanov, Alexander
<alexander.ulanov@hp.com> wrote:
> Thanks so much! It works! Is it the standard way for Mllib models to be s=
erialized?
>
> Btw. The example I pasted below works if one implements a TestSuite with =
MLlibTestSparkContext.
>
> -----Original Message-----
> From: Xiangrui Meng [mailto:mengxr@gmail.com]
> Sent: Monday, March 09, 2015 12:10 PM
> To: Ulanov, Alexander
> Cc: Akhil Das; dev
> Subject: Re: Loading previously serialized object to Spark
>
> Could you try `sc.objectFile` instead?
>
> sc.parallelize(Seq(model), 1).saveAsObjectFile("path") val sameModel =3D =
sc.objectFile[NaiveBayesModel]("path").first()
>
> -Xiangrui
>
> On Mon, Mar 9, 2015 at 11:52 AM, Ulanov, Alexander <alexander.ulanov@hp.c=
om> wrote:
>> Just tried, the same happens if I use the internal Spark serializer:
>> val serializer =3D SparkEnv.get.closureSerializer.newInstance
>>
>>
>> -----Original Message-----
>> From: Ulanov, Alexander
>> Sent: Monday, March 09, 2015 10:37 AM
>> To: Akhil Das
>> Cc: dev
>> Subject: RE: Loading previously serialized object to Spark
>>
>> Below is the code with standard MLlib class. Apparently this issue can h=
appen in the same Spark instance.
>>
>> import java.io._
>>
>> import org.apache.spark.mllib.classification.NaiveBayes
>> import org.apache.spark.mllib.classification.NaiveBayesModel
>> import org.apache.spark.mllib.util.MLUtils
>>
>> val data =3D MLUtils.loadLibSVMFile(sc,
>> "hdfs://myserver:9000/data/mnist.scale")
>> val nb =3D NaiveBayes.train(data)
>> // RDD map works fine
>> val predictionAndLabels =3D data.map( lp =3D>
>> (nb.classifierModel.predict(lp.features), lp.label))
>>
>> // serialize the model to file and immediately load it val oos =3D new
>> ObjectOutputStream(new FileOutputStream("/home/myuser/nb.bin"))
>> oos.writeObject(nb)
>> oos.close
>> val ois =3D new ObjectInputStream(new
>> FileInputStream("/home/myuser/nb.bin"))
>> val nbSerialized =3D ois.readObject.asInstanceOf[NaiveBayesModel]
>> ois.close
>> // RDD map fails
>> val predictionAndLabels =3D data.map( lp =3D>
>> (nbSerialized.predict(lp.features), lp.label))
>> org.apache.spark.SparkException: Task not serializable
>>         at org.apache.spark.util.ClosureCleaner$.ensureSerializable(Clos=
ureCleaner.scala:166)
>>         at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.sc=
ala:158)
>>         at org.apache.spark.SparkContext.clean(SparkContext.scala:1453)
>>         at org.apache.spark.rdd.RDD.map(RDD.scala:273)
>>
>>
>> From: Akhil Das [mailto:akhil@sigmoidanalytics.com]
>> Sent: Sunday, March 08, 2015 3:17 AM
>> To: Ulanov, Alexander
>> Cc: dev
>> Subject: Re: Loading previously serialized object to Spark
>>
>> Can you paste the complete code?
>>
>> Thanks
>> Best Regards
>>
>> On Sat, Mar 7, 2015 at 2:25 AM, Ulanov, Alexander <alexander.ulanov@hp.c=
om<mailto:alexander.ulanov@hp.com>> wrote:
>> Hi,
>>
>> I've implemented class MyClass in MLlib that does some operation on Labe=
ledPoint. MyClass extends serializable, so I can map this operation on data=
 of RDD[LabeledPoints], such as data.map(lp =3D> MyClass.operate(lp)). I wr=
ite this class in file with ObjectOutputStream.writeObject. Then I stop and=
 restart Spark. I load this class from file with ObjectInputStream.readObje=
ct.asInstanceOf[MyClass]. When I try to map the same operation of this clas=
s to RDD, Spark throws not serializable exception:
>> org.apache.spark.SparkException: Task not serializable
>>         at org.apache.spark.util.ClosureCleaner$.ensureSerializable(Clos=
ureCleaner.scala:166)
>>         at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.sc=
ala:158)
>>         at org.apache.spark.SparkContext.clean(SparkContext.scala:1453)
>>         at org.apache.spark.rdd.RDD.map(RDD.scala:273)
>>
>> Could you suggest why it throws this exception while MyClass is serializ=
able by definition?
>>
>> Best regards, Alexander
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 19:39:08 2015
Return-Path: <dev-return-11927-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A763179A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 19:39:08 +0000 (UTC)
Received: (qmail 33690 invoked by uid 500); 9 Mar 2015 19:39:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33615 invoked by uid 500); 9 Mar 2015 19:39:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33604 invoked by uid 99); 9 Mar 2015 19:39:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:39:06 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.240.92.67] (HELO g9t5009.houston.hp.com) (15.240.92.67)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:38:58 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5009.houston.hp.com (Postfix) with ESMTPS id 5B943189;
	Mon,  9 Mar 2015 19:38:07 +0000 (UTC)
Received: from G4W6300.americas.hpqcorp.net (16.210.26.225) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 9 Mar 2015 19:37:06 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G4W6300.americas.hpqcorp.net ([16.210.26.225]) with mapi id 14.03.0169.001;
 Mon, 9 Mar 2015 19:37:06 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Xiangrui Meng <mengxr@gmail.com>
CC: Akhil Das <akhil@sigmoidanalytics.com>, dev <dev@spark.apache.org>
Subject: RE: Loading previously serialized object to Spark
Thread-Topic: Loading previously serialized object to Spark
Thread-Index: AdBYT9oKXKom1WKWRnOearMR4wGkngBOTRQAAEFprwAAAskQkAAAswOAAABnrQAAAFoKgAAAHMmg
Date: Mon, 9 Mar 2015 19:37:05 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE16B5E@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1539E@G4W3292.americas.hpqcorp.net>
	<CAHUQ+_Zamu_OCJ7dELFCAzHd+AT+BLPJ_+=P_JaqyoUFM7oYxw@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16976@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16A9F@G4W3292.americas.hpqcorp.net>
	<CAJgQjQ-FOtXqUcsG=W05EUzuAqaxDzV7_1ehR6x43LFL8dBzpg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE16B14@G4W3292.americas.hpqcorp.net>
 <CAJgQjQ9g1WseS1zxw3+0y7G=v=G+7g7oNnsS_nNV6h2k=x2GvQ@mail.gmail.com>
In-Reply-To: <CAJgQjQ9g1WseS1zxw3+0y7G=v=G+7g7oNnsS_nNV6h2k=x2GvQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-11928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 19:55:43 2015
Return-Path: <dev-return-11928-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E839017A2A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 19:55:42 +0000 (UTC)
Received: (qmail 69588 invoked by uid 500); 9 Mar 2015 19:55:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69515 invoked by uid 500); 9 Mar 2015 19:55:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69503 invoked by uid 99); 9 Mar 2015 19:55:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:55:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.54 as permitted sender)
Received: from [209.85.218.54] (HELO mail-oi0-f54.google.com) (209.85.218.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 19:55:17 +0000
Received: by oifz81 with SMTP id z81so31496083oif.0
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 12:53:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=weSa7MuZoakbr6GDPRj1mqvq6kxZ6FYQP7cKF9hbSdg=;
        b=llisnYsNqRaSflnTQkgIPYWpEJXXqZbrP4lW7WYegoGnn1WiEMupGx8iX1uVZ8c0Zq
         O7eNm4dTVeJJxW6Q7GyG2HF8k5LZXyWPkXP3uhnDtny+7U4umbAfnlNmASwzGQzWuyGP
         v62NkIbPysdQnwQYMTJH+TpRZbvRGrapqK2i+ojNLNrlnAVGttnn1DcihnLVeXHpmKYc
         PWWH114k0NKMhxJ2j8jv1Pwd6JNDDea5GOuI/DGIgdyzmUglOz6jpcEuN1wYHKsc2LLu
         X0d4sgIKO0jl6ogGFO0Nwzlc05tQpQTKngTP9KFDDxMdKB9EyOIQDfDqQbl/kedY2fNm
         jbFQ==
MIME-Version: 1.0
X-Received: by 10.202.45.214 with SMTP id t205mr22108917oit.100.1425930780249;
 Mon, 09 Mar 2015 12:53:00 -0700 (PDT)
Received: by 10.202.226.137 with HTTP; Mon, 9 Mar 2015 12:53:00 -0700 (PDT)
Date: Mon, 9 Mar 2015 12:53:00 -0700
Message-ID: <CABPQxstgD2VMZZdYTdf8761eZHq3AzxNqcm50a3C6RM92QRpdA@mail.gmail.com>
Subject: Cross cutting internal changes to launch scripts
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Marcelo Vanzin has been working on a patch for a few months that
performs cross cutting clean-up and fixes to the way that Spark's
launch scripts work (including PySpark, spark submit, the daemon
scripts, etc.). The changes won't modify any public API's in terms of
how those scripts are invoked.

Historically, such patches have been difficult to test due to the
number of interactions between components and interactions with
external environments. I'd like to welcome people to test and/or code
review this patch in their own environment. This patch is the in the
very late stages of review and will likely be merged soon into master
(eventually 1.4).

https://github.com/apache/spark/pull/3916/files

I'll ping this thread again once it is merged and we can establish a
JIRA to encapsulate any issues. Just wanted to give a heads up as this
is one of the larger internal changes we've made to this
infrastructure since Spark 1.0

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 20:32:47 2015
Return-Path: <dev-return-11929-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2D2F517C6A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 20:32:47 +0000 (UTC)
Received: (qmail 27027 invoked by uid 500); 9 Mar 2015 20:32:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26947 invoked by uid 500); 9 Mar 2015 20:32:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26936 invoked by uid 99); 9 Mar 2015 20:32:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 20:32:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kostas@cloudera.com designates 209.85.160.169 as permitted sender)
Received: from [209.85.160.169] (HELO mail-yk0-f169.google.com) (209.85.160.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 20:32:40 +0000
Received: by ykt10 with SMTP id 10so8509551ykt.0
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 13:31:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=5dX4cXXz8jN2aTqkHggtFHyoIWsZYq7jx4hIgVUQmEE=;
        b=UXnDjVI/oeGZxUv1oMFip7zxUUIpXwRLtKPRLRxU/5BDxJJNXU2JPe+zLSPUao49Cx
         u233Nbpzsk+kWEN9JzAHhGsdMJwfFP2atnMRXijdQ79XTzOMrsblCl3bDm++TN1DLpUe
         l4+Qsl1RdN4InBLuOHz+HU+/pIxCs9lAQeL8GKrD7jNCSFt4Qxbo6t3h85RF3Ftjypi9
         EpofwuNH61Bvy83qTbOd3s355oG5mcHjlGFy5/+91ftyikpddD9hiNA0L+vLziPWKuCa
         rovfwmDsBuj694GvNJ5HK6pLaSB6oj8gMbB3lHswZn5YjLIDZF8f+QXNjUopAGL5Wj6f
         R3lg==
X-Gm-Message-State: ALoCoQmztbO07JIF3Y7/3/dm6VMu5RQ/EmpEHaDBZEoCuU1wubMjTxc/qsPk/zjeMzsG+Xqwn8CX
MIME-Version: 1.0
X-Received: by 10.236.25.196 with SMTP id z44mr28602200yhz.19.1425933094722;
 Mon, 09 Mar 2015 13:31:34 -0700 (PDT)
Received: by 10.170.44.11 with HTTP; Mon, 9 Mar 2015 13:31:34 -0700 (PDT)
In-Reply-To: <CABPQxstJE=zbbULUvBsUtK7+X_yNhY-TS2WUiFHorxSAGTkVOw@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<1064497734.1624411.1425917602457.JavaMail.yahoo@mail.yahoo.com>
	<CABjYQ39MFFnRWAAkcZ2r-C7UiZWhJzq-8_qDH-SWokR=qX1=2g@mail.gmail.com>
	<CABPQxstJE=zbbULUvBsUtK7+X_yNhY-TS2WUiFHorxSAGTkVOw@mail.gmail.com>
Date: Mon, 9 Mar 2015 13:31:34 -0700
Message-ID: <CAGrwSrO_zgP5FtyQKns7tueKbERtAM77GWK0jBOL4fu+8EGFkA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Kostas Sakellis <kostas@cloudera.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149bf5495c64f0510e0eb74
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149bf5495c64f0510e0eb74
Content-Type: text/plain; charset=UTF-8

+1 on RC3

I agree that this should not block the release. Once we have a fix for it,
putting it in a double dot release sounds like a good plan.

Kostas



On Mon, Mar 9, 2015 at 11:27 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey All,
>
> Today there was a JIRA posted with an observed regression around Spark
> Streaming during certain recovery scenarios:
>
> https://issues.apache.org/jira/browse/SPARK-6222
>
> My preference is to go ahead and ship this release (RC3) as-is and if
> this issue is isolated resolved soon, we can make a patch release in
> the next week or two.
>
> At some point, the cost of continuing to hold the release re/vote is
> so high that it's better to just ship the release. We can document
> known issues and point users to a fix once it's available. We did this
> in 1.2.0 as well (there were two small known issues) and I think as a
> point of process, this approach is necessary given the size of the
> project.
>
> I wanted to notify this thread though, in case this change anyones
> opinion on their release vote. I will leave the thread open at least
> until the end of today.
>
> Still +1 on RC3, for me.
>
> - Patrick
>
> On Mon, Mar 9, 2015 at 9:36 AM, Denny Lee <denny.g.lee@gmail.com> wrote:
> > +1 (non-binding)
> >
> > Spark Standalone and YARN on Hadoop 2.6 on OSX plus various tests (MLLib,
> > SparkSQL, etc.)
> >
> > On Mon, Mar 9, 2015 at 9:18 AM Tom Graves <tgraves_cs@yahoo.com.invalid>
> > wrote:
> >>
> >> +1. Built from source and ran Spark on yarn on hadoop 2.6 in cluster and
> >> client mode.
> >> Tom
> >>
> >>      On Thursday, March 5, 2015 8:53 PM, Patrick Wendell
> >> <pwendell@gmail.com> wrote:
> >>
> >>
> >>  Please vote on releasing the following candidate as Apache Spark
> version
> >> 1.3.0!
> >>
> >> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/pwendell.asc
> >>
> >> Staging repositories for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1078
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.3.0!
> >>
> >> The vote is open until Monday, March 09, at 02:52 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 1.3.0
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> == How does this compare to RC2 ==
> >> This release includes the following bug fixes:
> >>
> >> https://issues.apache.org/jira/browse/SPARK-6144
> >> https://issues.apache.org/jira/browse/SPARK-6171
> >> https://issues.apache.org/jira/browse/SPARK-5143
> >> https://issues.apache.org/jira/browse/SPARK-6182
> >> https://issues.apache.org/jira/browse/SPARK-6175
> >>
> >> == How can I help test this release? ==
> >> If you are a Spark user, you can help us test this release by
> >> taking a Spark 1.2 workload and running on this release candidate,
> >> then reporting any regressions.
> >>
> >> If you are happy with this release based on your own testing, give a +1
> >> vote.
> >>
> >> == What justifies a -1 vote for this release? ==
> >> This vote is happening towards the end of the 1.3 QA period,
> >> so -1 votes should only occur for significant regressions from 1.2.1.
> >> Bugs already present in 1.2.X, minor regressions, or bugs related
> >> to new features will not block this release.
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
> >>
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e0149bf5495c64f0510e0eb74--

From dev-return-11930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 20:38:56 2015
Return-Path: <dev-return-11930-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6BB0C17CD4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 20:38:56 +0000 (UTC)
Received: (qmail 60206 invoked by uid 500); 9 Mar 2015 20:38:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60129 invoked by uid 500); 9 Mar 2015 20:38:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60117 invoked by uid 99); 9 Mar 2015 20:38:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 20:38:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 20:38:50 +0000
Received: by wevk48 with SMTP id k48so31688606wev.5
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 13:38:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=a5MYQFNc9BZvlxfpIifMpd1X+Di84t7BUa+fMEhXek0=;
        b=DKGlLHKtzwiLJKl3s94AvIOmMGYzjI2Hif2fMbsLj9o/xkaMIHkwlyHRlGOyMvcrMP
         PuTdQlFHkSOfyTz6GnN8msOV8MNMtY9hqaMK0+FhS/58cCNxH59mzeHsA8IXiYYd2FSA
         QiAZ8EG7ojNYo1JmqYqedg7lFHWjZr/aSaSkYIhf84fAsin28sjKYIoitVCoXgq1vt+m
         Zw03x7HRFjy906i15+gz4mIdiVJ4DW95dVqcdkXcrzVKPgLtrLQMoumU/eF074jwanhR
         uepKIYMiKigXS2SxRBZ0IGFOPo8mihmLSKhvHg6Yi/qyA1s55kTD2BJkN1VL7mh+InPI
         lPyQ==
X-Gm-Message-State: ALoCoQmHsOwEYUEf1f+hHwOJxmaZ4tQYh6VCSs6JgX2tPcJQp5tIYne2sS2zFYKxrvs1TgHrurXo
X-Received: by 10.194.175.137 with SMTP id ca9mr515067wjc.67.1425933509555;
 Mon, 09 Mar 2015 13:38:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Mon, 9 Mar 2015 13:38:09 -0700 (PDT)
In-Reply-To: <CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
 <CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
 <CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
 <CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com> <CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 9 Mar 2015 20:38:09 +0000
Message-ID: <CAMAsSdKayuGpW2kZ-89ttzCpSpK9C=4rH5Qhr5=i+DzGB1gzMw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'm +1 as I have not heard of any one else seeing the Hive test
failure, which is likely a test issue rather than code issue anyway,
and not a blocker.

On Fri, Mar 6, 2015 at 9:36 PM, Sean Owen <sowen@cloudera.com> wrote:
> Although the problem is small, especially if indeed the essential docs
> changes are following just a couple days behind the final release, I
> mean, why the rush if they're essential? wait a couple days, finish
> them, make the release.
>
> Answer is, I think these changes aren't actually essential given the
> comment from tdas, so: just mark these Critical? (although ... they do
> say they're changes for the 1.3 release, so kind of funny to get to
> them for 1.3.x or 1.4, but that's not important now.)
>
> I thought that Blocker really meant Blocker in this project, as I've
> been encouraged to use it to mean "don't release without this." I
> think we should use it that way. Just thinking of it as "extra
> Critical" doesn't add anything. I don't think Documentation should be
> special-cased as less important, and I don't think there's confusion
> if Blocker means what it says, so I'd 'fix' that way.
>
> If nobody sees the Hive failure I observed, and if we can just zap
> those "Blockers" one way or the other, +1
>
>
> On Fri, Mar 6, 2015 at 9:17 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>> Sean,
>>
>> The docs are distributed and consumed in a fundamentally different way
>> than Spark code itself. So we've always considered the "deadline" for
>> doc changes to be when the release is finally posted.
>>
>> If there are small inconsistencies with the docs present in the source
>> code for that release tag, IMO that doesn't matter much since we don't
>> even distribute the docs with Spark's binary releases and virtually no
>> one builds and hosts the docs on their own (that I am aware of, at
>> least). Perhaps we can recommend if people want to build the doc
>> sources that they should always grab the head of the most recent
>> release branch, to set expectations accordingly.
>>
>> In the past we haven't considered it worth holding up the release
>> process for the purpose of the docs. It just doesn't make sense since
>> they are consumed "as a service". If we decide to change this
>> convention, it would mean shipping our releases later, since we
>> could't pipeline the doc finalization with voting.
>>
>> - Patrick
>>
>> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
>>> Given the title and tagging, it sounds like there could be some
>>> must-have doc changes to go with what is being released as 1.3. It can
>>> be finished later, and published later, but then the docs source
>>> shipped with the release doesn't match the site, and until then, 1.3
>>> is released without some "must-have" docs for 1.3 on the site.
>>>
>>> The real question to me is: are there any further, absolutely
>>> essential doc changes that need to accompany 1.3 or not?
>>>
>>> If not, just resolve these. If there are, then it seems like the
>>> release has to block on them. If there are some docs that should have
>>> gone in for 1.3, but didn't, but aren't essential, well I suppose it
>>> bears thinking about how to not slip as much work, but it doesn't
>>> block.
>>>
>>> I think Documentation issues certainly can be a blocker and shouldn't
>>> be specially ignored.
>>>
>>>
>>> BTW the UISeleniumSuite issue is a real failure, but I do not think it
>>> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
>>> a regression from 1.2.x, but only affects tests, and only affects a
>>> subset of build profiles.
>>>
>>>
>>>
>>>
>>> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>> Hey Sean,
>>>>
>>>>> SPARK-5310 Update SQL programming guide for 1.3
>>>>> SPARK-5183 Document data source API
>>>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
>>>>
>>>> For these, the issue is that they are documentation JIRA's, which
>>>> don't need to be timed exactly with the release vote, since we can
>>>> update the documentation on the website whenever we want. In the past
>>>> I've just mentally filtered these out when considering RC's. I see a
>>>> few options here:
>>>>
>>>> 1. We downgrade such issues away from Blocker (more clear, but we risk
>>>> loosing them in the fray if they really are things we want to have
>>>> before the release is posted).
>>>> 2. We provide a filter to the community that excludes 'Documentation'
>>>> issues and shows all other blockers for 1.3. We can put this on the
>>>> wiki, for instance.
>>>>
>>>> Which do you prefer?
>>>>
>>>> - Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 21:09:50 2015
Return-Path: <dev-return-11931-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C325D17E0A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 21:09:50 +0000 (UTC)
Received: (qmail 28319 invoked by uid 500); 9 Mar 2015 21:09:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28242 invoked by uid 500); 9 Mar 2015 21:09:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28231 invoked by uid 99); 9 Mar 2015 21:09:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 21:09:49 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 21:09:44 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id E6D54B0;
	Mon,  9 Mar 2015 21:08:53 +0000 (UTC)
Received: from G9W3612.americas.hpqcorp.net (16.216.186.47) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 9 Mar 2015 21:07:43 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G9W3612.americas.hpqcorp.net ([16.216.186.47]) with mapi id 14.03.0169.001;
 Mon, 9 Mar 2015 21:07:42 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Sam Halliday <sam.halliday@gmail.com>, Xiangrui Meng <mengxr@gmail.com>,
	Joseph Bradley <joseph@databricks.com>
CC: "Evan R. Sparks" <evan.sparks@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkA==
Date: Mon, 9 Mar 2015 21:07:41 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
 <CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
 <CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
 <CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
 <CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
 <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
 <87ioehu4qv.fsf@gmail.com>
In-Reply-To: <87ioehu4qv.fsf@gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-11932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 21:46:54 2015
Return-Path: <dev-return-11932-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B51B110015
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 21:46:54 +0000 (UTC)
Received: (qmail 84305 invoked by uid 500); 9 Mar 2015 21:46:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84226 invoked by uid 500); 9 Mar 2015 21:46:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84215 invoked by uid 99); 9 Mar 2015 21:46:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 21:46:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 21:46:48 +0000
Received: by obcvb8 with SMTP id vb8so32729928obc.10
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 14:44:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=FUzvxchPKSbtDfE92autxZaj/lA2UYp6ongjXe9rr1g=;
        b=Ns6bc9Q6Wr5+d6bxFhF62exfH/mgjzquWT9hRQXYIWoTU32zP5vXt3pvPTkeCNAeGn
         3zJ8AaqTzZirf397qnFvlxpMKFy4UMjzwDN3RDMbwTVVqwbNu7BY7ADhTJQDWVbB/oxk
         Av6kWnK3afPgpMD0t6bxXK/4CwmJb3/JS08eqAXjAxWTpkIhGeItkyLrsIS0U1cJJh8Y
         FNoFHoJDNbvQUZ7SagldGUqTtD652lzZPIBkcxgAEHDtavaGtbSs7LW+5IQAwSAbSRWg
         /IH0C6oivNPBx9RErZjXQZecDEXYw4W81c8GdXVoCi7xDpbmYS6ygf2GuuBxRGEgwHHd
         FdaQ==
X-Gm-Message-State: ALoCoQnITxZ88Kn2gPh6MbNf9GB4Rpn1D5xzUjIdi5rV2plk/Y7rtcwJ+6ZZupikx4bILofLumB5
X-Received: by 10.202.187.5 with SMTP id l5mr21897716oif.51.1425937477952;
        Mon, 09 Mar 2015 14:44:37 -0700 (PDT)
Received: from mail-ob0-f169.google.com (mail-ob0-f169.google.com. [209.85.214.169])
        by mx.google.com with ESMTPSA id w198sm12611754oiw.17.2015.03.09.14.44.35
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 09 Mar 2015 14:44:37 -0700 (PDT)
Received: by obcuy5 with SMTP id uy5so29572188obc.11
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 14:44:33 -0700 (PDT)
X-Received: by 10.202.96.69 with SMTP id u66mr22195590oib.3.1425937473063;
 Mon, 09 Mar 2015 14:44:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.182.99 with HTTP; Mon, 9 Mar 2015 14:44:12 -0700 (PDT)
In-Reply-To: <CAJiQeY+Hg-XHBD6w-6DRiJvr9W3fqUAWiVRSsfZifQmDruZoQQ@mail.gmail.com>
References: <CAMAsSd+s7sc9wHRdWgJ6Ro6019vE8+ftj6Kvn0pZW-1CABN7-w@mail.gmail.com>
 <CA0ADC5A-8AFB-42C4-934C-35428D189822@gmail.com> <CAMAsSdKivUZSyL4y-_FONf7beLpUR8uHrhjtE2aCAZAjHJC3jw@mail.gmail.com>
 <A7557B5E-1185-4EAC-859B-B399246D81A1@gmail.com> <CAMAsSdKXsmsbSV-Mueq_A8vSR+ZY80CnkrDjNC6iF=wGrE49Lg@mail.gmail.com>
 <CAJiQeY+Hg-XHBD6w-6DRiJvr9W3fqUAWiVRSsfZifQmDruZoQQ@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 9 Mar 2015 14:44:12 -0700
Message-ID: <CA+-p3AFEt24QLcXEXFrKVALG_gxjCN3EaB84XtvJoLoL+D4ijA@mail.gmail.com>
Subject: Re: Release Scala version vs Hadoop version (was: [VOTE] Release
 Apache Spark 1.3.0 (RC3))
To: Mridul Muralidharan <mridul@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Matei Zaharia <matei.zaharia@gmail.com>, 
	Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d34c08dd3a70510e1f02e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d34c08dd3a70510e1f02e
Content-Type: text/plain; charset=UTF-8

Does the Apache project team have any ability to measure download counts of
the various releases?  That data could be useful when it comes time to
sunset vendor-specific releases, like CDH4 for example.

On Mon, Mar 9, 2015 at 5:34 AM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> In ideal situation, +1 on removing all vendor specific builds and
> making just hadoop version specific - that is what we should depend on
> anyway.
> Though I hope Sean is correct in assuming that vendor specific builds
> for hadoop 2.4 are just that; and not 2.4- or 2.4+ which cause
> incompatibilities for us or our users !
>
> Regards,
> Mridul
>
>
> On Mon, Mar 9, 2015 at 2:50 AM, Sean Owen <sowen@cloudera.com> wrote:
> > Yes, you should always find working bits at Apache no matter what --
> > though 'no matter what' really means 'as long as you use Hadoop distro
> > compatible with upstream Hadoop'. Even distros have a strong interest
> > in that, since the market, the 'pie', is made large by this kind of
> > freedom at the core.
> >
> > If tso, then no vendor-specific builds are needed, only some
> > Hadoop-release-specific ones. So a Hadoop 2.6-specific build could be
> > good (although I'm not yet clear if there's something about 2.5 or 2.6
> > that needs a different build.)
> >
> > I take it that we already believe that, say, the "Hadoop 2.4" build
> > works with CDH5, so no CDH5-specific build is provided by Spark.
> >
> > If a distro doesn't work with stock Spark, then it's either something
> > Spark should fix (e.g. use of a private YARN API or something), or
> > it's something the distro should really fix because it's incompatible.
> >
> > Could we maybe rename the "CDH4" build then, as it doesn't really work
> > with all CDH4, to be a "Hadoop 2.0.x build"? That's been floated
> > before. And can we remove the MapR builds -- or else can someone
> > explain why these exist separately from a Hadoop 2.3 build? I hope it
> > is not *because* they are somehow non-standard. And shall we first run
> > down why Spark doesn't fully work on HDP and see if it's something
> > that Spark or HDP needs to tweak, rather than contemplate another
> > binary? or, if so, can it simply be called a "Hadoop 2.7 + YARN
> > whatever" build and not made specific to a vendor, even if the project
> > has to field another tarball combo for a vendor?
> >
> > Maybe we are saying almost the same thing.
> >
> >
> > On Mon, Mar 9, 2015 at 1:33 AM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> >> Yeah, my concern is that people should get Apache Spark from *Apache*,
> not from a vendor. It helps everyone use the latest features no matter
> where they are. In the Hadoop distro case, Hadoop made all this effort to
> have standard APIs (e.g. YARN), so it should be easy. But it is a problem
> if we're not packaging for the newest versions of some distros; I think we
> just fell behind at Hadoop 2.4.
> >>
> >> Matei
> >>
> >>> On Mar 8, 2015, at 8:02 PM, Sean Owen <sowen@cloudera.com> wrote:
> >>>
> >>> Yeah it's not much overhead, but here's an example of where it causes
> >>> a little issue.
> >>>
> >>> I like that reasoning. However, the released builds don't track the
> >>> later versions of Hadoop that vendors would be distributing -- there's
> >>> no Hadoop 2.6 build for example. CDH4 is here, but not the
> >>> far-more-used CDH5. HDP isn't present at all. The CDH4 build doesn't
> >>> actually work with many CDH4 versions.
> >>>
> >>> I agree with the goal of maximizing the reach of Spark, but I don't
> >>> know how much these builds advance that goal.
> >>>
> >>> Anyone can roll-their-own exactly-right build, and the docs and build
> >>> have been set up to make that as simple as can be expected. So these
> >>> aren't *required* to let me use latest Spark on distribution X.
> >>>
> >>> I had thought these existed to sorta support 'legacy' distributions,
> >>> like CDH4, and that build was justified as a
> >>> quasi-Hadoop-2.0.x-flavored build. But then I don't understand what
> >>> the MapR profiles are for.
> >>>
> >>> I think it's too much work to correctly, in parallel, maintain any
> >>> customizations necessary for any major distro, and it might be best to
> >>> do not at all than to do it incompletely. You could say it's also an
> >>> enabler for distros to vary in ways that require special
> >>> customization.
> >>>
> >>> Maybe there's a concern that, if lots of people consume Spark on
> >>> Hadoop, and most people consume Hadoop through distros, and distros
> >>> alone manage Spark distributions, then you de facto 'have to' go
> >>> through a distro instead of get bits from Spark? Different
> >>> conversation but I think this sort of effect does not end up being a
> >>> negative.
> >>>
> >>> Well anyway, I like the idea of seeing how far Hadoop-provided
> >>> releases can help. It might kill several birds with one stone.
> >>>
> >>> On Sun, Mar 8, 2015 at 11:07 PM, Matei Zaharia <
> matei.zaharia@gmail.com> wrote:
> >>>> Our goal is to let people use the latest Apache release even if
> vendors fall behind or don't want to package everything, so that's why we
> put out releases for vendors' versions. It's fairly low overhead.
> >>>>
> >>>> Matei
> >>>>
> >>>>> On Mar 8, 2015, at 5:56 PM, Sean Owen <sowen@cloudera.com> wrote:
> >>>>>
> >>>>> Ah. I misunderstood that Matei was referring to the Scala 2.11
> tarball
> >>>>> at http://people.apache.org/~pwendell/spark-1.3.0-rc3/ and not the
> >>>>> Maven artifacts.
> >>>>>
> >>>>> Patrick I see you just commented on SPARK-5134 and will follow up
> >>>>> there. Sounds like this may accidentally not be a problem.
> >>>>>
> >>>>> On binary tarball releases, I wonder if anyone has an opinion on my
> >>>>> opinion that these shouldn't be distributed for specific Hadoop
> >>>>> *distributions* to begin with. (Won't repeat the argument here yet.)
> >>>>> That resolves this n x m explosion too.
> >>>>>
> >>>>> Vendors already provide their own distribution, yes, that's their
> job.
> >>>>>
> >>>>>
> >>>>> On Sun, Mar 8, 2015 at 9:42 PM, Krishna Sankar <ksankar42@gmail.com>
> wrote:
> >>>>>> Yep, otherwise this will become an N^2 problem - Scala versions X
> Hadoop
> >>>>>> Distributions X ...
> >>>>>>
> >>>>>> May be one option is to have a minimum basic set (which I know is
> what we
> >>>>>> are discussing) and move the rest to spark-packages.org. There the
> vendors
> >>>>>> can add the latest downloads - for example when 1.4 is released,
> HDP can
> >>>>>> build a release of HDP Spark 1.4 bundle.
> >>>>>>
> >>>>>> Cheers
> >>>>>> <k/>
> >>>>>>
> >>>>>> On Sun, Mar 8, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>>>>>>
> >>>>>>> We probably want to revisit the way we do binaries in general for
> >>>>>>> 1.4+. IMO, something worth forking a separate thread for.
> >>>>>>>
> >>>>>>> I've been hesitating to add new binaries because people
> >>>>>>> (understandably) complain if you ever stop packaging older ones,
> but
> >>>>>>> on the other hand the ASF has complained that we have too many
> >>>>>>> binaries already and that we need to pare it down because of the
> large
> >>>>>>> volume of files. Doubling the number of binaries we produce for
> Scala
> >>>>>>> 2.11 seemed like it would be too much.
> >>>>>>>
> >>>>>>> One solution potentially is to actually package "Hadoop provided"
> >>>>>>> binaries and encourage users to use these by simply setting
> >>>>>>> HADOOP_HOME, or have instructions for specific distros. I've heard
> >>>>>>> that our existing packages don't work well on HDP for instance,
> since
> >>>>>>> there are some configuration quirks that differ from the upstream
> >>>>>>> Hadoop.
> >>>>>>>
> >>>>>>> If we cut down on the cross building for Hadoop versions, then it
> is
> >>>>>>> more tenable to cross build for Scala versions without exploding
> the
> >>>>>>> number of binaries.
> >>>>>>>
> >>>>>>> - Patrick
> >>>>>>>
> >>>>>>> On Sun, Mar 8, 2015 at 12:46 PM, Sean Owen <sowen@cloudera.com>
> wrote:
> >>>>>>>> Yeah, interesting question of what is the better default for the
> >>>>>>>> single set of artifacts published to Maven. I think there's an
> >>>>>>>> argument for Hadoop 2 and perhaps Hive for the 2.10 build too.
> Pros
> >>>>>>>> and cons discussed more at
> >>>>>>>>
> >>>>>>>> https://issues.apache.org/jira/browse/SPARK-5134
> >>>>>>>> https://github.com/apache/spark/pull/3917
> >>>>>>>>
> >>>>>>>> On Sun, Mar 8, 2015 at 7:42 PM, Matei Zaharia <
> matei.zaharia@gmail.com>
> >>>>>>>> wrote:
> >>>>>>>>> +1
> >>>>>>>>>
> >>>>>>>>> Tested it on Mac OS X.
> >>>>>>>>>
> >>>>>>>>> One small issue I noticed is that the Scala 2.11 build is using
> Hadoop
> >>>>>>>>> 1 without Hive, which is kind of weird because people will more
> likely want
> >>>>>>>>> Hadoop 2 with Hive. So it would be good to publish a build for
> that
> >>>>>>>>> configuration instead. We can do it if we do a new RC, or it
> might be that
> >>>>>>>>> binary builds may not need to be voted on (I forgot the details
> there).
> >>>>>>>>>
> >>>>>>>>> Matei
> >>>>>>>
> >>>>>>>
> ---------------------------------------------------------------------
> >>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>>>>
> >>>>>>
> >>>>
> >>
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113d34c08dd3a70510e1f02e--

From dev-return-11933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar  9 22:31:55 2015
Return-Path: <dev-return-11933-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 009E610326
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  9 Mar 2015 22:31:55 +0000 (UTC)
Received: (qmail 55056 invoked by uid 500); 9 Mar 2015 22:31:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54977 invoked by uid 500); 9 Mar 2015 22:31:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54964 invoked by uid 99); 9 Mar 2015 22:31:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 22:31:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 09 Mar 2015 22:31:28 +0000
Received: by igal13 with SMTP id l13so23683670iga.1
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 15:30:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=FpDn5HZpCG0yClK+4FM9TxnossAEQ4JvpvECmkjU7M8=;
        b=kdasidwgwfzHPxRUQh7m6dXNGvtXigXVdpdQwb4YtdFsgP0qyzP8scx19a8OlIHw8Q
         UWFkbJOnWqYxgogDHoP4rpyw9f+NEqP3FQQdbDArJ0+wXS0a1SjsuKD0VIOsMu271F1k
         lVB52PuHJNcQ9QjluiwAGLr0esO4sB5GDemWEvzsXO+reTRrlrs6Vs4MUh3lw+tZuojG
         0VHETxk8SulyucNX0qJHqBhJErc+bQ4zI7n3p9WXCgCUPqjz6te5jZUAxfbiNbYqGOBb
         w9ezTv4XQ2RkAYWB/WLt8sDhicmJmh2QVG6tD106XwQ9Zt4oO8MmVGce6gO64HCrqERO
         azrQ==
MIME-Version: 1.0
X-Received: by 10.43.13.200 with SMTP id pn8mr30792478icb.0.1425940241559;
 Mon, 09 Mar 2015 15:30:41 -0700 (PDT)
Received: by 10.36.99.76 with HTTP; Mon, 9 Mar 2015 15:30:41 -0700 (PDT)
In-Reply-To: <CAMAsSdKayuGpW2kZ-89ttzCpSpK9C=4rH5Qhr5=i+DzGB1gzMw@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
	<CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
	<CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
	<CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
	<CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
	<CAMAsSdKayuGpW2kZ-89ttzCpSpK9C=4rH5Qhr5=i+DzGB1gzMw@mail.gmail.com>
Date: Mon, 9 Mar 2015 15:30:41 -0700
Message-ID: <CAJgQjQ8aUa3hWjeDSuG7gBdUovZRe+g_+pd9_FNe6OZm9Ud9Bg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Xiangrui Meng <mengxr@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Krishna, I tested your linear regression example. For linear
regression, we changed its objective function from 1/n * \|A x -
b\|_2^2 to 1/(2n) * \|Ax - b\|_2^2 to be consistent with common least
squares formulations. It means you could re-produce the same result by
multiplying the step size by 2. This is not a problem if both run
until convergence (if not blow up). However, in your example, a very
small step size is chosen and it didn't converge in 100 iterations. In
this case, the step size matters. I will put a note in the migration
guide. Thanks! -Xiangrui

On Mon, Mar 9, 2015 at 1:38 PM, Sean Owen <sowen@cloudera.com> wrote:
> I'm +1 as I have not heard of any one else seeing the Hive test
> failure, which is likely a test issue rather than code issue anyway,
> and not a blocker.
>
> On Fri, Mar 6, 2015 at 9:36 PM, Sean Owen <sowen@cloudera.com> wrote:
>> Although the problem is small, especially if indeed the essential docs
>> changes are following just a couple days behind the final release, I
>> mean, why the rush if they're essential? wait a couple days, finish
>> them, make the release.
>>
>> Answer is, I think these changes aren't actually essential given the
>> comment from tdas, so: just mark these Critical? (although ... they do
>> say they're changes for the 1.3 release, so kind of funny to get to
>> them for 1.3.x or 1.4, but that's not important now.)
>>
>> I thought that Blocker really meant Blocker in this project, as I've
>> been encouraged to use it to mean "don't release without this." I
>> think we should use it that way. Just thinking of it as "extra
>> Critical" doesn't add anything. I don't think Documentation should be
>> special-cased as less important, and I don't think there's confusion
>> if Blocker means what it says, so I'd 'fix' that way.
>>
>> If nobody sees the Hive failure I observed, and if we can just zap
>> those "Blockers" one way or the other, +1
>>
>>
>> On Fri, Mar 6, 2015 at 9:17 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>> Sean,
>>>
>>> The docs are distributed and consumed in a fundamentally different way
>>> than Spark code itself. So we've always considered the "deadline" for
>>> doc changes to be when the release is finally posted.
>>>
>>> If there are small inconsistencies with the docs present in the source
>>> code for that release tag, IMO that doesn't matter much since we don't
>>> even distribute the docs with Spark's binary releases and virtually no
>>> one builds and hosts the docs on their own (that I am aware of, at
>>> least). Perhaps we can recommend if people want to build the doc
>>> sources that they should always grab the head of the most recent
>>> release branch, to set expectations accordingly.
>>>
>>> In the past we haven't considered it worth holding up the release
>>> process for the purpose of the docs. It just doesn't make sense since
>>> they are consumed "as a service". If we decide to change this
>>> convention, it would mean shipping our releases later, since we
>>> could't pipeline the doc finalization with voting.
>>>
>>> - Patrick
>>>
>>> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
>>>> Given the title and tagging, it sounds like there could be some
>>>> must-have doc changes to go with what is being released as 1.3. It can
>>>> be finished later, and published later, but then the docs source
>>>> shipped with the release doesn't match the site, and until then, 1.3
>>>> is released without some "must-have" docs for 1.3 on the site.
>>>>
>>>> The real question to me is: are there any further, absolutely
>>>> essential doc changes that need to accompany 1.3 or not?
>>>>
>>>> If not, just resolve these. If there are, then it seems like the
>>>> release has to block on them. If there are some docs that should have
>>>> gone in for 1.3, but didn't, but aren't essential, well I suppose it
>>>> bears thinking about how to not slip as much work, but it doesn't
>>>> block.
>>>>
>>>> I think Documentation issues certainly can be a blocker and shouldn't
>>>> be specially ignored.
>>>>
>>>>
>>>> BTW the UISeleniumSuite issue is a real failure, but I do not think it
>>>> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
>>>> a regression from 1.2.x, but only affects tests, and only affects a
>>>> subset of build profiles.
>>>>
>>>>
>>>>
>>>>
>>>> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>>> Hey Sean,
>>>>>
>>>>>> SPARK-5310 Update SQL programming guide for 1.3
>>>>>> SPARK-5183 Document data source API
>>>>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
>>>>>
>>>>> For these, the issue is that they are documentation JIRA's, which
>>>>> don't need to be timed exactly with the release vote, since we can
>>>>> update the documentation on the website whenever we want. In the past
>>>>> I've just mentally filtered these out when considering RC's. I see a
>>>>> few options here:
>>>>>
>>>>> 1. We downgrade such issues away from Blocker (more clear, but we risk
>>>>> loosing them in the fray if they really are things we want to have
>>>>> before the release is posted).
>>>>> 2. We provide a filter to the community that excludes 'Documentation'
>>>>> issues and shows all other blockers for 1.3. We can put this on the
>>>>> wiki, for instance.
>>>>>
>>>>> Which do you prefer?
>>>>>
>>>>> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 00:51:24 2015
Return-Path: <dev-return-11934-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF22F10B78
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 00:51:24 +0000 (UTC)
Received: (qmail 31599 invoked by uid 500); 10 Mar 2015 00:51:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31526 invoked by uid 500); 10 Mar 2015 00:51:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31509 invoked by uid 99); 10 Mar 2015 00:51:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 00:51:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 00:51:18 +0000
Received: by igbhn18 with SMTP id hn18so26126702igb.2
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 17:49:07 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Hzv9EP3raZW6VkoU3ZbhqZKj95lPuywlv5hBM6CHxnU=;
        b=PxPzYe+fm39a1lP/Zk1S3oYfcZXHIYU54cGw3M30xUFn5UhhSASSrOZc+7VJlTRLZr
         4yYreOnRqz78XvyhoN8/VmJbJpLeHQtfcJg5hEnPiUMug5toT9wOLmvymZfaegEZ9QoS
         WwGU4y1qFx6BrpNzFpRbOnIwAJEInOGO4vX9XjjG6T8j3UYRQC7W9nM0KELCKO9u+36W
         +OWVnht67P9XTfS0j+d4r6pt3Hzthm0x/sgr7PluTEYTvLS9JoXUF+A7JPAXJJgs4JTT
         rgQlCWGq/n7GLy5VOGHsg5KbBs4HfbaCMlRx+L9Bkja3X/cqWW3efQOyiffjWpAst23V
         IUyQ==
X-Gm-Message-State: ALoCoQl1lm6l+5WC4VJk+5m26TGnI29GLb+cEJHzgFDxP9auCGrliGUJ7V1JoWzgKev4+Y4lLduy
MIME-Version: 1.0
X-Received: by 10.107.46.230 with SMTP id u99mr51712272iou.21.1425948547620;
 Mon, 09 Mar 2015 17:49:07 -0700 (PDT)
Received: by 10.36.118.18 with HTTP; Mon, 9 Mar 2015 17:49:07 -0700 (PDT)
In-Reply-To: <CAJgQjQ8aUa3hWjeDSuG7gBdUovZRe+g_+pd9_FNe6OZm9Ud9Bg@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
	<CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
	<CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
	<CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
	<CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
	<CAMAsSdKayuGpW2kZ-89ttzCpSpK9C=4rH5Qhr5=i+DzGB1gzMw@mail.gmail.com>
	<CAJgQjQ8aUa3hWjeDSuG7gBdUovZRe+g_+pd9_FNe6OZm9Ud9Bg@mail.gmail.com>
Date: Mon, 9 Mar 2015 17:49:07 -0700
Message-ID: <CAF7ADNrc1M_cuebfgzLpasK-5PNT6t8p3c-z64VZGhELbCroUQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Joseph Bradley <joseph@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c16a96a692020510e48424
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c16a96a692020510e48424
Content-Type: text/plain; charset=UTF-8

+1
Tested on Mac OS X

On Mon, Mar 9, 2015 at 3:30 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Krishna, I tested your linear regression example. For linear
> regression, we changed its objective function from 1/n * \|A x -
> b\|_2^2 to 1/(2n) * \|Ax - b\|_2^2 to be consistent with common least
> squares formulations. It means you could re-produce the same result by
> multiplying the step size by 2. This is not a problem if both run
> until convergence (if not blow up). However, in your example, a very
> small step size is chosen and it didn't converge in 100 iterations. In
> this case, the step size matters. I will put a note in the migration
> guide. Thanks! -Xiangrui
>
> On Mon, Mar 9, 2015 at 1:38 PM, Sean Owen <sowen@cloudera.com> wrote:
> > I'm +1 as I have not heard of any one else seeing the Hive test
> > failure, which is likely a test issue rather than code issue anyway,
> > and not a blocker.
> >
> > On Fri, Mar 6, 2015 at 9:36 PM, Sean Owen <sowen@cloudera.com> wrote:
> >> Although the problem is small, especially if indeed the essential docs
> >> changes are following just a couple days behind the final release, I
> >> mean, why the rush if they're essential? wait a couple days, finish
> >> them, make the release.
> >>
> >> Answer is, I think these changes aren't actually essential given the
> >> comment from tdas, so: just mark these Critical? (although ... they do
> >> say they're changes for the 1.3 release, so kind of funny to get to
> >> them for 1.3.x or 1.4, but that's not important now.)
> >>
> >> I thought that Blocker really meant Blocker in this project, as I've
> >> been encouraged to use it to mean "don't release without this." I
> >> think we should use it that way. Just thinking of it as "extra
> >> Critical" doesn't add anything. I don't think Documentation should be
> >> special-cased as less important, and I don't think there's confusion
> >> if Blocker means what it says, so I'd 'fix' that way.
> >>
> >> If nobody sees the Hive failure I observed, and if we can just zap
> >> those "Blockers" one way or the other, +1
> >>
> >>
> >> On Fri, Mar 6, 2015 at 9:17 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>> Sean,
> >>>
> >>> The docs are distributed and consumed in a fundamentally different way
> >>> than Spark code itself. So we've always considered the "deadline" for
> >>> doc changes to be when the release is finally posted.
> >>>
> >>> If there are small inconsistencies with the docs present in the source
> >>> code for that release tag, IMO that doesn't matter much since we don't
> >>> even distribute the docs with Spark's binary releases and virtually no
> >>> one builds and hosts the docs on their own (that I am aware of, at
> >>> least). Perhaps we can recommend if people want to build the doc
> >>> sources that they should always grab the head of the most recent
> >>> release branch, to set expectations accordingly.
> >>>
> >>> In the past we haven't considered it worth holding up the release
> >>> process for the purpose of the docs. It just doesn't make sense since
> >>> they are consumed "as a service". If we decide to change this
> >>> convention, it would mean shipping our releases later, since we
> >>> could't pipeline the doc finalization with voting.
> >>>
> >>> - Patrick
> >>>
> >>> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
> >>>> Given the title and tagging, it sounds like there could be some
> >>>> must-have doc changes to go with what is being released as 1.3. It can
> >>>> be finished later, and published later, but then the docs source
> >>>> shipped with the release doesn't match the site, and until then, 1.3
> >>>> is released without some "must-have" docs for 1.3 on the site.
> >>>>
> >>>> The real question to me is: are there any further, absolutely
> >>>> essential doc changes that need to accompany 1.3 or not?
> >>>>
> >>>> If not, just resolve these. If there are, then it seems like the
> >>>> release has to block on them. If there are some docs that should have
> >>>> gone in for 1.3, but didn't, but aren't essential, well I suppose it
> >>>> bears thinking about how to not slip as much work, but it doesn't
> >>>> block.
> >>>>
> >>>> I think Documentation issues certainly can be a blocker and shouldn't
> >>>> be specially ignored.
> >>>>
> >>>>
> >>>> BTW the UISeleniumSuite issue is a real failure, but I do not think it
> >>>> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
> >>>> a regression from 1.2.x, but only affects tests, and only affects a
> >>>> subset of build profiles.
> >>>>
> >>>>
> >>>>
> >>>>
> >>>> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>>>> Hey Sean,
> >>>>>
> >>>>>> SPARK-5310 Update SQL programming guide for 1.3
> >>>>>> SPARK-5183 Document data source API
> >>>>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
> >>>>>
> >>>>> For these, the issue is that they are documentation JIRA's, which
> >>>>> don't need to be timed exactly with the release vote, since we can
> >>>>> update the documentation on the website whenever we want. In the past
> >>>>> I've just mentally filtered these out when considering RC's. I see a
> >>>>> few options here:
> >>>>>
> >>>>> 1. We downgrade such issues away from Blocker (more clear, but we
> risk
> >>>>> loosing them in the fray if they really are things we want to have
> >>>>> before the release is posted).
> >>>>> 2. We provide a filter to the community that excludes 'Documentation'
> >>>>> issues and shows all other blockers for 1.3. We can put this on the
> >>>>> wiki, for instance.
> >>>>>
> >>>>> Which do you prefer?
> >>>>>
> >>>>> - Patrick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11c16a96a692020510e48424--

From dev-return-11935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 01:01:15 2015
Return-Path: <dev-return-11935-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4405410BC4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 01:01:15 +0000 (UTC)
Received: (qmail 50421 invoked by uid 500); 10 Mar 2015 01:01:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50336 invoked by uid 500); 10 Mar 2015 01:01:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50323 invoked by uid 99); 10 Mar 2015 01:01:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:01:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sam.halliday@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:01:03 +0000
Received: by igbhl2 with SMTP id hl2so26600366igb.5
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 18:00:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=0OIKrDnEs2D1BUCvK19T3x0mWQTAsp/qNSu8h/TkQlA=;
        b=y7wHuwC38/tFxiMfXEBNkDBqeiQadVpHsZ2wLTATFeTtEJp9dONYgwSSng8vakO5U1
         oaY3gqysecRPev58+JMdcoAlCrZICP6WHIpuyZqJfgCblCwFX4LU79VjWI/j345hUuUt
         kG+ivyCaPZurSLySN8flyEI9o4LU5GQihgCMfprSOWzpd8u3RdKg1wR125hqxewjhyQX
         MnoI2UKUQL6/hClAxo9pQexElXAHdYbUUkKui5xJDjJzkLa7pKs0x4QYyitNLUej5dYb
         u3UgD5nDCqL+9cMY+IQQpt79XiTEocdOQKGUIhqB8CFsHcrpCXe3RoOzX7PLnkLn+LxX
         6psA==
MIME-Version: 1.0
X-Received: by 10.50.142.106 with SMTP id rv10mr53075129igb.18.1425949242637;
 Mon, 09 Mar 2015 18:00:42 -0700 (PDT)
Received: by 10.36.39.203 with HTTP; Mon, 9 Mar 2015 18:00:42 -0700 (PDT)
Received: by 10.36.39.203 with HTTP; Mon, 9 Mar 2015 18:00:42 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
Date: Tue, 10 Mar 2015 01:00:42 +0000
Message-ID: <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
Subject: RE: Using CUDA within Spark / boosting linear algebra
From: Sam Halliday <sam.halliday@gmail.com>
To: Alexander Ulanov <alexander.ulanov@hp.com>
Cc: dev@spark.apache.org, Xiangrui Meng <mengxr@gmail.com>, 
	Joseph Bradley <joseph@databricks.com>, "Evan R. Sparks" <evan.sparks@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c3d13a13455d0510e4aea7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3d13a13455d0510e4aea7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks so much for following up on this!

Hmm, I wonder if we should have a concerted effort to chart performance on
various pieces of hardware...
On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com> wrote:

> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
> support of Double in the current source code), did the test with BIDMat a=
nd
> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:sam.halliday@gmail.com]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <mengxr@gmail.com> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com>
> wrote:
> >> Better documentation for linking would be very helpful!  Here's a JIRA=
:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <evan.sparks@gmail.com>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for larger
> matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> alexander.ulanov@hp.com> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley; dev@spark.apache.org
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley; dev@spark.apache.org
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
> to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> >>>> evan.sparks@gmail.com>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked up b=
y
> netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I understand
> how to force use a specific blas (not specific wrapper for blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> >>>> evan.sparks@gmail.com>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already have
> it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
> >>>> joseph@databricks.com>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons with
> Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential to
> machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> >>>> evan.sparks@gmail.com>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and =
4
> GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> >>>> evan.sparks@gmail.com><mailto:evan.sparks@gmail.com<mailto:
> >>>> evan.sparks@gmail.com>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x faster a=
t
> matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster within
> Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better performance
> only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was not a
> test for ONLY multiplication since there are other operations involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
> >>>> dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apac
> >>>> he.org <mailto:dev-unsubscribe@spark.apache.org>>
> >>>> For additional commands, e-mail: dev-help@spark.apache.org<mailto:
> >>>> dev-help@spark.apache.org><mailto:dev-help@spark.apache.org<mailto:
> >>>> dev-help@spark.apache.org>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --
> Best regards,
> Sam
>

--001a11c3d13a13455d0510e4aea7--

From dev-return-11936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 01:18:00 2015
Return-Path: <dev-return-11936-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3DF8110C50
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 01:18:00 +0000 (UTC)
Received: (qmail 82128 invoked by uid 500); 10 Mar 2015 01:17:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82045 invoked by uid 500); 10 Mar 2015 01:17:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82034 invoked by uid 99); 10 Mar 2015 01:17:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:17:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of irashid@cloudera.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:17:54 +0000
Received: by wevl61 with SMTP id l61so14388051wev.10
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 18:16:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ErMMIs+MJuDdlthcONpDonf2B/8WddPQh6n4ZqYTVJs=;
        b=DP/KalSu947lMbtAC7AxIcvfhqk5itXUcoQDdbZW3/x0ijWXKPBJPeY5UNWOnf96MS
         S/aVdHj066qHCUuCAyyPfnwzL2+yzwNlcJX3YqdnrzZ0WN84rowaHpEiQ8J025HtB+Xn
         IUoeSJzIbItLK/q7KIAE6T+BGFOcrbwp7yQLMkrvSBMvC5L1ZuTO51CtEmts/m0SNi2n
         8+QuVNMFv1wJVBnwFJjvy8VY5hhv/JJBXJRHMPuZQzMrqCqC/Qm5RHkggpiREZCNuJPI
         DvZCGF7jnjS84V8Fcx81YOmwqYNSv+TlmDAjQNm/YAHyRlWj9dRTX4sgr0icE+CYV1Ii
         MWsw==
X-Gm-Message-State: ALoCoQkEunsy6ZL1xXyO0ZVcHc0yZ5lsUDqmBthNuQV+0o69OnBFcoT4PV51i3Nsrvg6JeInk3yl
X-Received: by 10.194.171.136 with SMTP id au8mr64369117wjc.6.1425950163951;
 Mon, 09 Mar 2015 18:16:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.62.197 with HTTP; Mon, 9 Mar 2015 18:15:43 -0700 (PDT)
In-Reply-To: <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com> <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Mon, 9 Mar 2015 20:15:43 -0500
Message-ID: <CA+3qhFQGGQnLfnFy8iXmEK_uwgA0HJJ=Nh=1fYrOTtnYJTDQ+A@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Mridul Muralidharan <mridul@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0122f58cfd7ab40510e4e438
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122f58cfd7ab40510e4e438
Content-Type: text/plain; charset=UTF-8

Can you expand on the serde issues w/ java enum's at all?  I haven't heard
of any problems specific to enums.  The java object serialization rules
seem very clear and it doesn't seem like different jvms should have a
choice on what they do:

http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serial-arch.html#6469

(in a nutshell, serialization must use enum.name())

of course there are plenty of ways the user could screw this up(eg. rename
the enums, or change their meaning, or remove them).  But then again, all
of java serialization has issues w/ serialization the user has to be aware
of.  Eg., if we go with case objects, than java serialization blows up if
you add another helper method, even if that helper method is completely
compatible.

Some prior debate in the scala community:

https://groups.google.com/d/msg/scala-internals/8RWkccSRBxQ/AN5F_ZbdKIsJ

SO post on which version to use in scala:

http://stackoverflow.com/questions/1321745/how-to-model-type-safe-enum-types

SO post about the macro-craziness people try to add to scala to make them
almost as good as a simple java enum:
(NB: the accepted answer doesn't actually work in all cases ...)

http://stackoverflow.com/questions/20089920/custom-scala-enum-most-elegant-version-searched

Another proposal to add better enums built into scala ... but seems to be
dormant:

https://groups.google.com/forum/#!topic/scala-sips/Bf82LxK02Kk



On Thu, Mar 5, 2015 at 10:49 PM, Mridul Muralidharan <mridul@gmail.com>
wrote:

>   I have a strong dislike for java enum's due to the fact that they
> are not stable across JVM's - if it undergoes serde, you end up with
> unpredictable results at times [1].
> One of the reasons why we prevent enum's from being key : though it is
> highly possible users might depend on it internally and shoot
> themselves in the foot.
>
> Would be better to keep away from them in general and use something more
> stable.
>
> Regards,
> Mridul
>
> [1] Having had to debug this issue for 2 weeks - I really really hate it.
>
>
> On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com> wrote:
> > I have a very strong dislike for #1 (scala enumerations).   I'm ok with
> #4
> > (with Xiangrui's final suggestion, especially making it sealed &
> available
> > in Java), but I really think #2, java enums, are the best option.
> >
> > Java enums actually have some very real advantages over the other
> > approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There
> has
> > been endless debate in the Scala community about the problems with the
> > approaches in Scala.  Very smart, level-headed Scala gurus have
> complained
> > about their short-comings (Rex Kerr's name is coming to mind, though I'm
> > not positive about that); there have been numerous well-thought out
> > proposals to give Scala a better enum.  But the powers-that-be in Scala
> > always reject them.  IIRC the explanation for rejecting is basically that
> > (a) enums aren't important enough for introducing some new special
> feature,
> > scala's got bigger things to work on and (b) if you really need a good
> > enum, just use java's enum.
> >
> > I doubt it really matters that much for Spark internals, which is why I
> > think #4 is fine.  But I figured I'd give my spiel, because every
> developer
> > loves language wars :)
> >
> > Imran
> >
> >
> >
> > On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >
> >> `case object` inside an `object` doesn't show up in Java. This is the
> >> minimal code I found to make everything show up correctly in both
> >> Scala and Java:
> >>
> >> sealed abstract class StorageLevel // cannot be a trait
> >>
> >> object StorageLevel {
> >>   private[this] case object _MemoryOnly extends StorageLevel
> >>   final val MemoryOnly: StorageLevel = _MemoryOnly
> >>
> >>   private[this] case object _DiskOnly extends StorageLevel
> >>   final val DiskOnly: StorageLevel = _DiskOnly
> >> }
> >>
> >> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >> > I like #4 as well and agree with Aaron's suggestion.
> >> >
> >> > - Patrick
> >> >
> >> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
> >> wrote:
> >> >> I'm cool with #4 as well, but make sure we dictate that the values
> >> should
> >> >> be defined within an object with the same name as the enumeration
> (like
> >> we
> >> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
> >> >>
> >> >> e.g. we SHOULD do:
> >> >>
> >> >> trait StorageLevel
> >> >> object StorageLevel {
> >> >>   case object MemoryOnly extends StorageLevel
> >> >>   case object DiskOnly extends StorageLevel
> >> >> }
> >> >>
> >> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
> >> michael@databricks.com>
> >> >> wrote:
> >> >>
> >> >>> #4 with a preference for CamelCaseEnums
> >> >>>
> >> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
> joseph@databricks.com>
> >> >>> wrote:
> >> >>>
> >> >>> > another vote for #4
> >> >>> > People are already used to adding "()" in Java.
> >> >>> >
> >> >>> >
> >> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com
> >
> >> >>> wrote:
> >> >>> >
> >> >>> > > #4 but with MemoryOnly (more scala-like)
> >> >>> > >
> >> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
> >> >>> > >
> >> >>> > > Constants, Values, Variable and Methods
> >> >>> > >
> >> >>> > > Constant names should be in upper camel case. That is, if the
> >> member is
> >> >>> > > final, immutable and it belongs to a package object or an
> object,
> >> it
> >> >>> may
> >> >>> > be
> >> >>> > > considered a constant (similar to Java'sstatic final members):
> >> >>> > >
> >> >>> > >
> >> >>> > >    1. object Container {
> >> >>> > >    2.     val MyConstant = ...
> >> >>> > >    3. }
> >> >>> > >
> >> >>> > >
> >> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> >> >>> > >
> >> >>> > > > Hi all,
> >> >>> > > >
> >> >>> > > > There are many places where we use enum-like types in Spark,
> but
> >> in
> >> >>> > > > different ways. Every approach has both pros and cons. I
> wonder
> >> >>> > > > whether there should be an "official" approach for enum-like
> >> types in
> >> >>> > > > Spark.
> >> >>> > > >
> >> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState,
> etc)
> >> >>> > > >
> >> >>> > > > * All types show up as Enumeration.Value in Java.
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
> >> >>> > > >
> >> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
> >> >>> > > >
> >> >>> > > > * Implementation must be in a Java file.
> >> >>> > > > * Values doesn't show up in the ScalaDoc:
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
> >> >>> > > >
> >> >>> > > > 3. Static fields in Java (e.g., TripletFields)
> >> >>> > > >
> >> >>> > > > * Implementation must be in a Java file.
> >> >>> > > > * Doesn't need "()" in Java code.
> >> >>> > > > * Values don't show up in the ScalaDoc:
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
> >> >>> > > >
> >> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
> >> >>> > > >
> >> >>> > > > * Needs "()" in Java code.
> >> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
> >> >>> > > >
> >> >>> > > > It would be great if we have an "official" approach for this
> as
> >> well
> >> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY"
> or
> >> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
> >> >>> thoughts?
> >> >>> > > >
> >> >>> > > > Best,
> >> >>> > > > Xiangrui
> >> >>> > > >
> >> >>> > > >
> >> ---------------------------------------------------------------------
> >> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
> >> >>> > > >
> >> >>> > > >
> >> >>> > >
> >> >>> >
> >> >>>
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>

--089e0122f58cfd7ab40510e4e438--

From dev-return-11937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 01:26:33 2015
Return-Path: <dev-return-11937-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C121410CB6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 01:26:33 +0000 (UTC)
Received: (qmail 847 invoked by uid 500); 10 Mar 2015 01:26:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 772 invoked by uid 500); 10 Mar 2015 01:26:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 760 invoked by uid 99); 10 Mar 2015 01:26:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:26:32 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:26:07 +0000
Received: by widex7 with SMTP id ex7so25709326wid.1
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 18:26:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Q7lNK2PiNy6mQcpyUHIm9wZuJK3I7qTJc7MZWNTsggI=;
        b=QTeDkeobGPvPaH80IoIZoDtKrv/gW8ar22bPvdBRXcaacxpiIbjJZK4Ot9q15VMRF9
         NqhtvgIRETyPhnEACfn5iprskgwSLBBOFfHptaWaBggfLkzZzOIixsIiqJYujh9ZqbUA
         uejNX1NzlVbrC0KKgaIktXVF5VrA6H8yj96z9W6d2poZxNVLxvTHUlvRVXtAW0EFJprr
         7SObZMz+/9+ZQMfyPz/2D5iM2ER8u/yzAN1BfmIFyrrs+A7NEssQ2pY28oJJAkbnzzZL
         GKNcvCUYPaHW9zymi3S3AFuvDDd0sXJu1CGkR5b/lDKv1umq1Gpsz0ea9U2UHseWGHw7
         Wy/Q==
X-Received: by 10.194.90.210 with SMTP id by18mr61317991wjb.80.1425950765971;
 Mon, 09 Mar 2015 18:26:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Mon, 9 Mar 2015 18:25:45 -0700 (PDT)
In-Reply-To: <CA+3qhFQGGQnLfnFy8iXmEK_uwgA0HJJ=Nh=1fYrOTtnYJTDQ+A@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
 <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com> <CA+3qhFQGGQnLfnFy8iXmEK_uwgA0HJJ=Nh=1fYrOTtnYJTDQ+A@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 9 Mar 2015 18:25:45 -0700
Message-ID: <CANGvG8pXqPCFs4dhEJVKkwtfmPFVYkgaaqb0SaBMN-33otVUYQ@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Imran Rashid <irashid@cloudera.com>
Cc: Mridul Muralidharan <mridul@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfcf662df7ebe0510e508d5
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf662df7ebe0510e508d5
Content-Type: text/plain; charset=UTF-8

Perhaps the problem with Java enums that was brought up was actually that
their hashCode is not stable across JVMs, as it depends on the memory
location of the enum itself.

On Mon, Mar 9, 2015 at 6:15 PM, Imran Rashid <irashid@cloudera.com> wrote:

> Can you expand on the serde issues w/ java enum's at all?  I haven't heard
> of any problems specific to enums.  The java object serialization rules
> seem very clear and it doesn't seem like different jvms should have a
> choice on what they do:
>
>
> http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serial-arch.html#6469
>
> (in a nutshell, serialization must use enum.name())
>
> of course there are plenty of ways the user could screw this up(eg. rename
> the enums, or change their meaning, or remove them).  But then again, all
> of java serialization has issues w/ serialization the user has to be aware
> of.  Eg., if we go with case objects, than java serialization blows up if
> you add another helper method, even if that helper method is completely
> compatible.
>
> Some prior debate in the scala community:
>
> https://groups.google.com/d/msg/scala-internals/8RWkccSRBxQ/AN5F_ZbdKIsJ
>
> SO post on which version to use in scala:
>
>
> http://stackoverflow.com/questions/1321745/how-to-model-type-safe-enum-types
>
> SO post about the macro-craziness people try to add to scala to make them
> almost as good as a simple java enum:
> (NB: the accepted answer doesn't actually work in all cases ...)
>
>
> http://stackoverflow.com/questions/20089920/custom-scala-enum-most-elegant-version-searched
>
> Another proposal to add better enums built into scala ... but seems to be
> dormant:
>
> https://groups.google.com/forum/#!topic/scala-sips/Bf82LxK02Kk
>
>
>
> On Thu, Mar 5, 2015 at 10:49 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>
> >   I have a strong dislike for java enum's due to the fact that they
> > are not stable across JVM's - if it undergoes serde, you end up with
> > unpredictable results at times [1].
> > One of the reasons why we prevent enum's from being key : though it is
> > highly possible users might depend on it internally and shoot
> > themselves in the foot.
> >
> > Would be better to keep away from them in general and use something more
> > stable.
> >
> > Regards,
> > Mridul
> >
> > [1] Having had to debug this issue for 2 weeks - I really really hate it.
> >
> >
> > On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
> > > I have a very strong dislike for #1 (scala enumerations).   I'm ok with
> > #4
> > > (with Xiangrui's final suggestion, especially making it sealed &
> > available
> > > in Java), but I really think #2, java enums, are the best option.
> > >
> > > Java enums actually have some very real advantages over the other
> > > approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There
> > has
> > > been endless debate in the Scala community about the problems with the
> > > approaches in Scala.  Very smart, level-headed Scala gurus have
> > complained
> > > about their short-comings (Rex Kerr's name is coming to mind, though
> I'm
> > > not positive about that); there have been numerous well-thought out
> > > proposals to give Scala a better enum.  But the powers-that-be in Scala
> > > always reject them.  IIRC the explanation for rejecting is basically
> that
> > > (a) enums aren't important enough for introducing some new special
> > feature,
> > > scala's got bigger things to work on and (b) if you really need a good
> > > enum, just use java's enum.
> > >
> > > I doubt it really matters that much for Spark internals, which is why I
> > > think #4 is fine.  But I figured I'd give my spiel, because every
> > developer
> > > loves language wars :)
> > >
> > > Imran
> > >
> > >
> > >
> > > On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> > >
> > >> `case object` inside an `object` doesn't show up in Java. This is the
> > >> minimal code I found to make everything show up correctly in both
> > >> Scala and Java:
> > >>
> > >> sealed abstract class StorageLevel // cannot be a trait
> > >>
> > >> object StorageLevel {
> > >>   private[this] case object _MemoryOnly extends StorageLevel
> > >>   final val MemoryOnly: StorageLevel = _MemoryOnly
> > >>
> > >>   private[this] case object _DiskOnly extends StorageLevel
> > >>   final val DiskOnly: StorageLevel = _DiskOnly
> > >> }
> > >>
> > >> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
> > >> wrote:
> > >> > I like #4 as well and agree with Aaron's suggestion.
> > >> >
> > >> > - Patrick
> > >> >
> > >> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
> > >> wrote:
> > >> >> I'm cool with #4 as well, but make sure we dictate that the values
> > >> should
> > >> >> be defined within an object with the same name as the enumeration
> > (like
> > >> we
> > >> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
> > >> >>
> > >> >> e.g. we SHOULD do:
> > >> >>
> > >> >> trait StorageLevel
> > >> >> object StorageLevel {
> > >> >>   case object MemoryOnly extends StorageLevel
> > >> >>   case object DiskOnly extends StorageLevel
> > >> >> }
> > >> >>
> > >> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
> > >> michael@databricks.com>
> > >> >> wrote:
> > >> >>
> > >> >>> #4 with a preference for CamelCaseEnums
> > >> >>>
> > >> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
> > joseph@databricks.com>
> > >> >>> wrote:
> > >> >>>
> > >> >>> > another vote for #4
> > >> >>> > People are already used to adding "()" in Java.
> > >> >>> >
> > >> >>> >
> > >> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <
> javadba@gmail.com
> > >
> > >> >>> wrote:
> > >> >>> >
> > >> >>> > > #4 but with MemoryOnly (more scala-like)
> > >> >>> > >
> > >> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
> > >> >>> > >
> > >> >>> > > Constants, Values, Variable and Methods
> > >> >>> > >
> > >> >>> > > Constant names should be in upper camel case. That is, if the
> > >> member is
> > >> >>> > > final, immutable and it belongs to a package object or an
> > object,
> > >> it
> > >> >>> may
> > >> >>> > be
> > >> >>> > > considered a constant (similar to Java'sstatic final members):
> > >> >>> > >
> > >> >>> > >
> > >> >>> > >    1. object Container {
> > >> >>> > >    2.     val MyConstant = ...
> > >> >>> > >    3. }
> > >> >>> > >
> > >> >>> > >
> > >> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> > >> >>> > >
> > >> >>> > > > Hi all,
> > >> >>> > > >
> > >> >>> > > > There are many places where we use enum-like types in Spark,
> > but
> > >> in
> > >> >>> > > > different ways. Every approach has both pros and cons. I
> > wonder
> > >> >>> > > > whether there should be an "official" approach for enum-like
> > >> types in
> > >> >>> > > > Spark.
> > >> >>> > > >
> > >> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState,
> > etc)
> > >> >>> > > >
> > >> >>> > > > * All types show up as Enumeration.Value in Java.
> > >> >>> > > >
> > >> >>> > > >
> > >> >>> > >
> > >> >>> >
> > >> >>>
> > >>
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
> > >> >>> > > >
> > >> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
> > >> >>> > > >
> > >> >>> > > > * Implementation must be in a Java file.
> > >> >>> > > > * Values doesn't show up in the ScalaDoc:
> > >> >>> > > >
> > >> >>> > > >
> > >> >>> > >
> > >> >>> >
> > >> >>>
> > >>
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
> > >> >>> > > >
> > >> >>> > > > 3. Static fields in Java (e.g., TripletFields)
> > >> >>> > > >
> > >> >>> > > > * Implementation must be in a Java file.
> > >> >>> > > > * Doesn't need "()" in Java code.
> > >> >>> > > > * Values don't show up in the ScalaDoc:
> > >> >>> > > >
> > >> >>> > > >
> > >> >>> > >
> > >> >>> >
> > >> >>>
> > >>
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
> > >> >>> > > >
> > >> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
> > >> >>> > > >
> > >> >>> > > > * Needs "()" in Java code.
> > >> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
> > >> >>> > > >
> > >> >>> > > >
> > >> >>> > >
> > >> >>> >
> > >> >>>
> > >>
> >
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
> > >> >>> > > >
> > >> >>> > > >
> > >> >>> > >
> > >> >>> >
> > >> >>>
> > >>
> >
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
> > >> >>> > > >
> > >> >>> > > > It would be great if we have an "official" approach for this
> > as
> > >> well
> > >> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY"
> > or
> > >> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
> > >> >>> thoughts?
> > >> >>> > > >
> > >> >>> > > > Best,
> > >> >>> > > > Xiangrui
> > >> >>> > > >
> > >> >>> > > >
> > >> ---------------------------------------------------------------------
> > >> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
> > >> >>> > > >
> > >> >>> > > >
> > >> >>> > >
> > >> >>> >
> > >> >>>
> > >>
> > >> ---------------------------------------------------------------------
> > >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > >> For additional commands, e-mail: dev-help@spark.apache.org
> > >>
> > >>
> >
>

--047d7bfcf662df7ebe0510e508d5--

From dev-return-11938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 01:40:43 2015
Return-Path: <dev-return-11938-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E3BF210D6A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 01:40:43 +0000 (UTC)
Received: (qmail 37272 invoked by uid 500); 10 Mar 2015 01:40:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37197 invoked by uid 500); 10 Mar 2015 01:40:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37185 invoked by uid 99); 10 Mar 2015 01:40:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:40:41 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 01:40:38 +0000
Received: by obcwo20 with SMTP id wo20so2579080obc.2
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 18:39:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=4Fv4dSM000ehJA3fgcHeWrfz3zzp4p6w9+01I6WEBJc=;
        b=iPv5614nG7GwoVyuLH8GVks44KvHM0KDMzQ6E0N5u+Eh6PcFKjHAZKPnJD4ESV/nHU
         7ydRGtADMo9ghA5uvd9WdPbxU3w0eX1vkXaBCmUtUhThbhmDPgA1AdYoLVIk+QDxlVkg
         qETfAPLvUPOLvl22YIKXEBndCgSPul0TSvfbi9hm+HwX58IjPdMRnkaJi0wRKJStgKyF
         qMHu9eWGyDfMxTCl/q0xUPuQHqXUUhkF4nwwymo+Tnd0g4th/896wobxIkyykrKH3PAX
         o6MvSHOGcCSRJipeayiyu2MNeD6hkba/P/NBKcu/Jq6+9kDguw1SIv6vHDdWQpW98ZKQ
         eMFg==
MIME-Version: 1.0
X-Received: by 10.202.185.198 with SMTP id j189mr22886374oif.72.1425951572748;
 Mon, 09 Mar 2015 18:39:32 -0700 (PDT)
Received: by 10.202.226.137 with HTTP; Mon, 9 Mar 2015 18:39:32 -0700 (PDT)
In-Reply-To: <CANGvG8pXqPCFs4dhEJVKkwtfmPFVYkgaaqb0SaBMN-33otVUYQ@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
	<CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
	<CA+3qhFQGGQnLfnFy8iXmEK_uwgA0HJJ=Nh=1fYrOTtnYJTDQ+A@mail.gmail.com>
	<CANGvG8pXqPCFs4dhEJVKkwtfmPFVYkgaaqb0SaBMN-33otVUYQ@mail.gmail.com>
Date: Mon, 9 Mar 2015 18:39:32 -0700
Message-ID: <CABPQxsthhRTkypYyOtdrZTT1SwX-hpD+ZAtV7vtOLdRbtAhdQA@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Aaron Davidson <ilikerps@gmail.com>
Cc: Imran Rashid <irashid@cloudera.com>, Mridul Muralidharan <mridul@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Does this matter for our own internal types in Spark? I don't think
any of these types are designed to be used in RDD records, for
instance.

On Mon, Mar 9, 2015 at 6:25 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> Perhaps the problem with Java enums that was brought up was actually that
> their hashCode is not stable across JVMs, as it depends on the memory
> location of the enum itself.
>
> On Mon, Mar 9, 2015 at 6:15 PM, Imran Rashid <irashid@cloudera.com> wrote:
>
>> Can you expand on the serde issues w/ java enum's at all?  I haven't heard
>> of any problems specific to enums.  The java object serialization rules
>> seem very clear and it doesn't seem like different jvms should have a
>> choice on what they do:
>>
>>
>> http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serial-arch.html#6469
>>
>> (in a nutshell, serialization must use enum.name())
>>
>> of course there are plenty of ways the user could screw this up(eg. rename
>> the enums, or change their meaning, or remove them).  But then again, all
>> of java serialization has issues w/ serialization the user has to be aware
>> of.  Eg., if we go with case objects, than java serialization blows up if
>> you add another helper method, even if that helper method is completely
>> compatible.
>>
>> Some prior debate in the scala community:
>>
>> https://groups.google.com/d/msg/scala-internals/8RWkccSRBxQ/AN5F_ZbdKIsJ
>>
>> SO post on which version to use in scala:
>>
>>
>> http://stackoverflow.com/questions/1321745/how-to-model-type-safe-enum-types
>>
>> SO post about the macro-craziness people try to add to scala to make them
>> almost as good as a simple java enum:
>> (NB: the accepted answer doesn't actually work in all cases ...)
>>
>>
>> http://stackoverflow.com/questions/20089920/custom-scala-enum-most-elegant-version-searched
>>
>> Another proposal to add better enums built into scala ... but seems to be
>> dormant:
>>
>> https://groups.google.com/forum/#!topic/scala-sips/Bf82LxK02Kk
>>
>>
>>
>> On Thu, Mar 5, 2015 at 10:49 PM, Mridul Muralidharan <mridul@gmail.com>
>> wrote:
>>
>> >   I have a strong dislike for java enum's due to the fact that they
>> > are not stable across JVM's - if it undergoes serde, you end up with
>> > unpredictable results at times [1].
>> > One of the reasons why we prevent enum's from being key : though it is
>> > highly possible users might depend on it internally and shoot
>> > themselves in the foot.
>> >
>> > Would be better to keep away from them in general and use something more
>> > stable.
>> >
>> > Regards,
>> > Mridul
>> >
>> > [1] Having had to debug this issue for 2 weeks - I really really hate it.
>> >
>> >
>> > On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com>
>> wrote:
>> > > I have a very strong dislike for #1 (scala enumerations).   I'm ok with
>> > #4
>> > > (with Xiangrui's final suggestion, especially making it sealed &
>> > available
>> > > in Java), but I really think #2, java enums, are the best option.
>> > >
>> > > Java enums actually have some very real advantages over the other
>> > > approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There
>> > has
>> > > been endless debate in the Scala community about the problems with the
>> > > approaches in Scala.  Very smart, level-headed Scala gurus have
>> > complained
>> > > about their short-comings (Rex Kerr's name is coming to mind, though
>> I'm
>> > > not positive about that); there have been numerous well-thought out
>> > > proposals to give Scala a better enum.  But the powers-that-be in Scala
>> > > always reject them.  IIRC the explanation for rejecting is basically
>> that
>> > > (a) enums aren't important enough for introducing some new special
>> > feature,
>> > > scala's got bigger things to work on and (b) if you really need a good
>> > > enum, just use java's enum.
>> > >
>> > > I doubt it really matters that much for Spark internals, which is why I
>> > > think #4 is fine.  But I figured I'd give my spiel, because every
>> > developer
>> > > loves language wars :)
>> > >
>> > > Imran
>> > >
>> > >
>> > >
>> > > On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> > >
>> > >> `case object` inside an `object` doesn't show up in Java. This is the
>> > >> minimal code I found to make everything show up correctly in both
>> > >> Scala and Java:
>> > >>
>> > >> sealed abstract class StorageLevel // cannot be a trait
>> > >>
>> > >> object StorageLevel {
>> > >>   private[this] case object _MemoryOnly extends StorageLevel
>> > >>   final val MemoryOnly: StorageLevel = _MemoryOnly
>> > >>
>> > >>   private[this] case object _DiskOnly extends StorageLevel
>> > >>   final val DiskOnly: StorageLevel = _DiskOnly
>> > >> }
>> > >>
>> > >> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>> > >> wrote:
>> > >> > I like #4 as well and agree with Aaron's suggestion.
>> > >> >
>> > >> > - Patrick
>> > >> >
>> > >> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>> > >> wrote:
>> > >> >> I'm cool with #4 as well, but make sure we dictate that the values
>> > >> should
>> > >> >> be defined within an object with the same name as the enumeration
>> > (like
>> > >> we
>> > >> >> do for StorageLevel). Otherwise we may pollute a higher namespace.
>> > >> >>
>> > >> >> e.g. we SHOULD do:
>> > >> >>
>> > >> >> trait StorageLevel
>> > >> >> object StorageLevel {
>> > >> >>   case object MemoryOnly extends StorageLevel
>> > >> >>   case object DiskOnly extends StorageLevel
>> > >> >> }
>> > >> >>
>> > >> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>> > >> michael@databricks.com>
>> > >> >> wrote:
>> > >> >>
>> > >> >>> #4 with a preference for CamelCaseEnums
>> > >> >>>
>> > >> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
>> > joseph@databricks.com>
>> > >> >>> wrote:
>> > >> >>>
>> > >> >>> > another vote for #4
>> > >> >>> > People are already used to adding "()" in Java.
>> > >> >>> >
>> > >> >>> >
>> > >> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <
>> javadba@gmail.com
>> > >
>> > >> >>> wrote:
>> > >> >>> >
>> > >> >>> > > #4 but with MemoryOnly (more scala-like)
>> > >> >>> > >
>> > >> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
>> > >> >>> > >
>> > >> >>> > > Constants, Values, Variable and Methods
>> > >> >>> > >
>> > >> >>> > > Constant names should be in upper camel case. That is, if the
>> > >> member is
>> > >> >>> > > final, immutable and it belongs to a package object or an
>> > object,
>> > >> it
>> > >> >>> may
>> > >> >>> > be
>> > >> >>> > > considered a constant (similar to Java'sstatic final members):
>> > >> >>> > >
>> > >> >>> > >
>> > >> >>> > >    1. object Container {
>> > >> >>> > >    2.     val MyConstant = ...
>> > >> >>> > >    3. }
>> > >> >>> > >
>> > >> >>> > >
>> > >> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>> > >> >>> > >
>> > >> >>> > > > Hi all,
>> > >> >>> > > >
>> > >> >>> > > > There are many places where we use enum-like types in Spark,
>> > but
>> > >> in
>> > >> >>> > > > different ways. Every approach has both pros and cons. I
>> > wonder
>> > >> >>> > > > whether there should be an "official" approach for enum-like
>> > >> types in
>> > >> >>> > > > Spark.
>> > >> >>> > > >
>> > >> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState,
>> > etc)
>> > >> >>> > > >
>> > >> >>> > > > * All types show up as Enumeration.Value in Java.
>> > >> >>> > > >
>> > >> >>> > > >
>> > >> >>> > >
>> > >> >>> >
>> > >> >>>
>> > >>
>> >
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>> > >> >>> > > >
>> > >> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>> > >> >>> > > >
>> > >> >>> > > > * Implementation must be in a Java file.
>> > >> >>> > > > * Values doesn't show up in the ScalaDoc:
>> > >> >>> > > >
>> > >> >>> > > >
>> > >> >>> > >
>> > >> >>> >
>> > >> >>>
>> > >>
>> >
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>> > >> >>> > > >
>> > >> >>> > > > 3. Static fields in Java (e.g., TripletFields)
>> > >> >>> > > >
>> > >> >>> > > > * Implementation must be in a Java file.
>> > >> >>> > > > * Doesn't need "()" in Java code.
>> > >> >>> > > > * Values don't show up in the ScalaDoc:
>> > >> >>> > > >
>> > >> >>> > > >
>> > >> >>> > >
>> > >> >>> >
>> > >> >>>
>> > >>
>> >
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>> > >> >>> > > >
>> > >> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>> > >> >>> > > >
>> > >> >>> > > > * Needs "()" in Java code.
>> > >> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
>> > >> >>> > > >
>> > >> >>> > > >
>> > >> >>> > >
>> > >> >>> >
>> > >> >>>
>> > >>
>> >
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>> > >> >>> > > >
>> > >> >>> > > >
>> > >> >>> > >
>> > >> >>> >
>> > >> >>>
>> > >>
>> >
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>> > >> >>> > > >
>> > >> >>> > > > It would be great if we have an "official" approach for this
>> > as
>> > >> well
>> > >> >>> > > > as the naming convention for enum-like values ("MEMORY_ONLY"
>> > or
>> > >> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>> > >> >>> thoughts?
>> > >> >>> > > >
>> > >> >>> > > > Best,
>> > >> >>> > > > Xiangrui
>> > >> >>> > > >
>> > >> >>> > > >
>> > >> ---------------------------------------------------------------------
>> > >> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > >> >>> > > > For additional commands, e-mail: dev-help@spark.apache.org
>> > >> >>> > > >
>> > >> >>> > > >
>> > >> >>> > >
>> > >> >>> >
>> > >> >>>
>> > >>
>> > >> ---------------------------------------------------------------------
>> > >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > >> For additional commands, e-mail: dev-help@spark.apache.org
>> > >>
>> > >>
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 03:03:44 2015
Return-Path: <dev-return-11939-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF35117350
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 03:03:44 +0000 (UTC)
Received: (qmail 64362 invoked by uid 500); 10 Mar 2015 03:03:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64293 invoked by uid 500); 10 Mar 2015 03:03:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64279 invoked by uid 99); 10 Mar 2015 03:03:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 03:03:42 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ksankar42@gmail.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 03:03:38 +0000
Received: by padfa1 with SMTP id fa1so71455253pad.3
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 20:01:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=FsCY84FcYKLjTwYgtTkfuU53akj86C/pmIJDrYUyiDQ=;
        b=eTqJe8Y3JfJNNYLeIM7ZONhB6hd6wPW2P5sZGRWM2uEJ0aZ/mtnFPrneY6gJVffbuU
         gCjfNGZk4hfSDqPjKWbWV8MhIR2u4/GdYi82PAwSnFkodY2ozeXKfI9XJAReGBhT90oN
         p/x77TEStCRglU4m3or+4kzctz7vVxUgdhbOGNmj2RdvFb23PRKmjdGGricv1SqcjKQn
         7F4qFsDN9Jh8asPyvCbEsEjJKbEkFDYKC5fk27e8PHWXP3nLNvalwIfFUufLhVj75Tla
         A9ewtDlvrOXgjUTnJBKhCSgU9r8L2JaYzrclvI6aW3it6ciE/+NUcolyrNfU4jOONX6g
         uasw==
MIME-Version: 1.0
X-Received: by 10.70.131.15 with SMTP id oi15mr60254439pdb.161.1425956462492;
 Mon, 09 Mar 2015 20:01:02 -0700 (PDT)
Received: by 10.70.19.130 with HTTP; Mon, 9 Mar 2015 20:01:02 -0700 (PDT)
In-Reply-To: <CAJgQjQ8aUa3hWjeDSuG7gBdUovZRe+g_+pd9_FNe6OZm9Ud9Bg@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
	<CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
	<CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
	<CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
	<CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
	<CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
	<CAMAsSdKayuGpW2kZ-89ttzCpSpK9C=4rH5Qhr5=i+DzGB1gzMw@mail.gmail.com>
	<CAJgQjQ8aUa3hWjeDSuG7gBdUovZRe+g_+pd9_FNe6OZm9Ud9Bg@mail.gmail.com>
Date: Mon, 9 Mar 2015 20:01:02 -0700
Message-ID: <CAOTBr2=NzePbFKe4eYJWdzn=uUzMaVRmkj1Z1qCb3_FjAwkhgg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Krishna Sankar <ksankar42@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Patrick Wendell <pwendell@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133da546983d10510e65c05
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133da546983d10510e65c05
Content-Type: text/plain; charset=UTF-8

Excellent, Thanks Xiangrui. The mystery is solved.
Cheers
<k/>


On Mon, Mar 9, 2015 at 3:30 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Krishna, I tested your linear regression example. For linear
> regression, we changed its objective function from 1/n * \|A x -
> b\|_2^2 to 1/(2n) * \|Ax - b\|_2^2 to be consistent with common least
> squares formulations. It means you could re-produce the same result by
> multiplying the step size by 2. This is not a problem if both run
> until convergence (if not blow up). However, in your example, a very
> small step size is chosen and it didn't converge in 100 iterations. In
> this case, the step size matters. I will put a note in the migration
> guide. Thanks! -Xiangrui
>
> On Mon, Mar 9, 2015 at 1:38 PM, Sean Owen <sowen@cloudera.com> wrote:
> > I'm +1 as I have not heard of any one else seeing the Hive test
> > failure, which is likely a test issue rather than code issue anyway,
> > and not a blocker.
> >
> > On Fri, Mar 6, 2015 at 9:36 PM, Sean Owen <sowen@cloudera.com> wrote:
> >> Although the problem is small, especially if indeed the essential docs
> >> changes are following just a couple days behind the final release, I
> >> mean, why the rush if they're essential? wait a couple days, finish
> >> them, make the release.
> >>
> >> Answer is, I think these changes aren't actually essential given the
> >> comment from tdas, so: just mark these Critical? (although ... they do
> >> say they're changes for the 1.3 release, so kind of funny to get to
> >> them for 1.3.x or 1.4, but that's not important now.)
> >>
> >> I thought that Blocker really meant Blocker in this project, as I've
> >> been encouraged to use it to mean "don't release without this." I
> >> think we should use it that way. Just thinking of it as "extra
> >> Critical" doesn't add anything. I don't think Documentation should be
> >> special-cased as less important, and I don't think there's confusion
> >> if Blocker means what it says, so I'd 'fix' that way.
> >>
> >> If nobody sees the Hive failure I observed, and if we can just zap
> >> those "Blockers" one way or the other, +1
> >>
> >>
> >> On Fri, Mar 6, 2015 at 9:17 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>> Sean,
> >>>
> >>> The docs are distributed and consumed in a fundamentally different way
> >>> than Spark code itself. So we've always considered the "deadline" for
> >>> doc changes to be when the release is finally posted.
> >>>
> >>> If there are small inconsistencies with the docs present in the source
> >>> code for that release tag, IMO that doesn't matter much since we don't
> >>> even distribute the docs with Spark's binary releases and virtually no
> >>> one builds and hosts the docs on their own (that I am aware of, at
> >>> least). Perhaps we can recommend if people want to build the doc
> >>> sources that they should always grab the head of the most recent
> >>> release branch, to set expectations accordingly.
> >>>
> >>> In the past we haven't considered it worth holding up the release
> >>> process for the purpose of the docs. It just doesn't make sense since
> >>> they are consumed "as a service". If we decide to change this
> >>> convention, it would mean shipping our releases later, since we
> >>> could't pipeline the doc finalization with voting.
> >>>
> >>> - Patrick
> >>>
> >>> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com> wrote:
> >>>> Given the title and tagging, it sounds like there could be some
> >>>> must-have doc changes to go with what is being released as 1.3. It can
> >>>> be finished later, and published later, but then the docs source
> >>>> shipped with the release doesn't match the site, and until then, 1.3
> >>>> is released without some "must-have" docs for 1.3 on the site.
> >>>>
> >>>> The real question to me is: are there any further, absolutely
> >>>> essential doc changes that need to accompany 1.3 or not?
> >>>>
> >>>> If not, just resolve these. If there are, then it seems like the
> >>>> release has to block on them. If there are some docs that should have
> >>>> gone in for 1.3, but didn't, but aren't essential, well I suppose it
> >>>> bears thinking about how to not slip as much work, but it doesn't
> >>>> block.
> >>>>
> >>>> I think Documentation issues certainly can be a blocker and shouldn't
> >>>> be specially ignored.
> >>>>
> >>>>
> >>>> BTW the UISeleniumSuite issue is a real failure, but I do not think it
> >>>> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It isn't
> >>>> a regression from 1.2.x, but only affects tests, and only affects a
> >>>> subset of build profiles.
> >>>>
> >>>>
> >>>>
> >>>>
> >>>> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>>>> Hey Sean,
> >>>>>
> >>>>>> SPARK-5310 Update SQL programming guide for 1.3
> >>>>>> SPARK-5183 Document data source API
> >>>>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
> >>>>>
> >>>>> For these, the issue is that they are documentation JIRA's, which
> >>>>> don't need to be timed exactly with the release vote, since we can
> >>>>> update the documentation on the website whenever we want. In the past
> >>>>> I've just mentally filtered these out when considering RC's. I see a
> >>>>> few options here:
> >>>>>
> >>>>> 1. We downgrade such issues away from Blocker (more clear, but we
> risk
> >>>>> loosing them in the fray if they really are things we want to have
> >>>>> before the release is posted).
> >>>>> 2. We provide a filter to the community that excludes 'Documentation'
> >>>>> issues and shows all other blockers for 1.3. We can put this on the
> >>>>> wiki, for instance.
> >>>>>
> >>>>> Which do you prefer?
> >>>>>
> >>>>> - Patrick
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1133da546983d10510e65c05--

From dev-return-11940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 03:20:17 2015
Return-Path: <dev-return-11940-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 927CD173C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 03:20:17 +0000 (UTC)
Received: (qmail 6457 invoked by uid 500); 10 Mar 2015 03:20:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6378 invoked by uid 500); 10 Mar 2015 03:20:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6366 invoked by uid 99); 10 Mar 2015 03:20:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 03:20:15 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of cjnolet@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 03:19:50 +0000
Received: by igal13 with SMTP id l13so25139826iga.1
        for <dev@spark.apache.org>; Mon, 09 Mar 2015 20:18:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=H3mK18dbzu1klfltL8I0ibiMUl8SEQRp3oVcHBokxn0=;
        b=dVcRSrrbx8XEuPziFmUh59HaB6Ah0hItPVQ+/5xvPi3VpDvNy9UV2zCD5yOCYupLXM
         tI8VeHRaVobMezTDTG8Erkyw3Kd3JzgTNTqh3CqqwXKuIxkFRvZiPswIP1OfUtFykpP9
         csh82eM3wcIRrRUOYQF3nFMzeaQYQCQM2qr8Kv2xtjj853OTWE6mB8T1hqVEzD9XAXzT
         Ak6W2AeWjOA3+DVAIaGysxnKsb0nYgDOhQpfv0gUGOry7D5UX6UEcPhXI4UHyKBiZhe0
         arcyoPAxJM1IFpwIEEBkzmArI+4nW0wwe9zzfxyJzAPrPrA6ry5TTlYI+6gHj48xyIsR
         ScYg==
X-Received: by 10.50.61.34 with SMTP id m2mr53214579igr.20.1425957498279; Mon,
 09 Mar 2015 20:18:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.55.167 with HTTP; Mon, 9 Mar 2015 20:17:56 -0700 (PDT)
In-Reply-To: <CAOTBr2=NzePbFKe4eYJWdzn=uUzMaVRmkj1Z1qCb3_FjAwkhgg@mail.gmail.com>
References: <CABPQxsurNC2fk_gh4QKDCBMOu1Ed0+nQW2ReLQPg1cpZiovdtA@mail.gmail.com>
 <CAMAsSdK1QkuQ2HRYN_7KRaq2YsQDPS5Q4qehndnjTpM+1npiFg@mail.gmail.com>
 <CABPQxsuwXOkxLLDRas+ZYL26HAUVnHpB6B0S1BQC3TWjpnAZcA@mail.gmail.com>
 <CAMAsSdK7LoSvO4O_jJGv5DSP4gdPg5VA8jV9542BUdTFVrKqGQ@mail.gmail.com>
 <CABPQxsupqpsTGB0=KgbSqdFEX+jq0_rpZM3dgw6HEfzkrTb1XQ@mail.gmail.com>
 <CAMAsSdKggLEW0NmL1OqksZ+bLziHHJgU-bWJ8S=5FPTQFgt7dg@mail.gmail.com>
 <CAMAsSdKayuGpW2kZ-89ttzCpSpK9C=4rH5Qhr5=i+DzGB1gzMw@mail.gmail.com>
 <CAJgQjQ8aUa3hWjeDSuG7gBdUovZRe+g_+pd9_FNe6OZm9Ud9Bg@mail.gmail.com> <CAOTBr2=NzePbFKe4eYJWdzn=uUzMaVRmkj1Z1qCb3_FjAwkhgg@mail.gmail.com>
From: Corey Nolet <cjnolet@gmail.com>
Date: Mon, 9 Mar 2015 23:17:56 -0400
Message-ID: <CAOHP_tER3d5ebtfiY3W2MgvZsCvC1C5ZD5Eo_Rt_6kQToL9uWA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
To: Krishna Sankar <ksankar42@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, Sean Owen <sowen@cloudera.com>, 
	Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135e0842660af0510e69aea
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135e0842660af0510e69aea
Content-Type: text/plain; charset=UTF-8

+1 (non-binding)

- Verified signatures
- Built on Mac OS X and Fedora 21.

On Mon, Mar 9, 2015 at 11:01 PM, Krishna Sankar <ksankar42@gmail.com> wrote:

> Excellent, Thanks Xiangrui. The mystery is solved.
> Cheers
> <k/>
>
>
> On Mon, Mar 9, 2015 at 3:30 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
> > Krishna, I tested your linear regression example. For linear
> > regression, we changed its objective function from 1/n * \|A x -
> > b\|_2^2 to 1/(2n) * \|Ax - b\|_2^2 to be consistent with common least
> > squares formulations. It means you could re-produce the same result by
> > multiplying the step size by 2. This is not a problem if both run
> > until convergence (if not blow up). However, in your example, a very
> > small step size is chosen and it didn't converge in 100 iterations. In
> > this case, the step size matters. I will put a note in the migration
> > guide. Thanks! -Xiangrui
> >
> > On Mon, Mar 9, 2015 at 1:38 PM, Sean Owen <sowen@cloudera.com> wrote:
> > > I'm +1 as I have not heard of any one else seeing the Hive test
> > > failure, which is likely a test issue rather than code issue anyway,
> > > and not a blocker.
> > >
> > > On Fri, Mar 6, 2015 at 9:36 PM, Sean Owen <sowen@cloudera.com> wrote:
> > >> Although the problem is small, especially if indeed the essential docs
> > >> changes are following just a couple days behind the final release, I
> > >> mean, why the rush if they're essential? wait a couple days, finish
> > >> them, make the release.
> > >>
> > >> Answer is, I think these changes aren't actually essential given the
> > >> comment from tdas, so: just mark these Critical? (although ... they do
> > >> say they're changes for the 1.3 release, so kind of funny to get to
> > >> them for 1.3.x or 1.4, but that's not important now.)
> > >>
> > >> I thought that Blocker really meant Blocker in this project, as I've
> > >> been encouraged to use it to mean "don't release without this." I
> > >> think we should use it that way. Just thinking of it as "extra
> > >> Critical" doesn't add anything. I don't think Documentation should be
> > >> special-cased as less important, and I don't think there's confusion
> > >> if Blocker means what it says, so I'd 'fix' that way.
> > >>
> > >> If nobody sees the Hive failure I observed, and if we can just zap
> > >> those "Blockers" one way or the other, +1
> > >>
> > >>
> > >> On Fri, Mar 6, 2015 at 9:17 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > >>> Sean,
> > >>>
> > >>> The docs are distributed and consumed in a fundamentally different
> way
> > >>> than Spark code itself. So we've always considered the "deadline" for
> > >>> doc changes to be when the release is finally posted.
> > >>>
> > >>> If there are small inconsistencies with the docs present in the
> source
> > >>> code for that release tag, IMO that doesn't matter much since we
> don't
> > >>> even distribute the docs with Spark's binary releases and virtually
> no
> > >>> one builds and hosts the docs on their own (that I am aware of, at
> > >>> least). Perhaps we can recommend if people want to build the doc
> > >>> sources that they should always grab the head of the most recent
> > >>> release branch, to set expectations accordingly.
> > >>>
> > >>> In the past we haven't considered it worth holding up the release
> > >>> process for the purpose of the docs. It just doesn't make sense since
> > >>> they are consumed "as a service". If we decide to change this
> > >>> convention, it would mean shipping our releases later, since we
> > >>> could't pipeline the doc finalization with voting.
> > >>>
> > >>> - Patrick
> > >>>
> > >>> On Fri, Mar 6, 2015 at 11:02 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> > >>>> Given the title and tagging, it sounds like there could be some
> > >>>> must-have doc changes to go with what is being released as 1.3. It
> can
> > >>>> be finished later, and published later, but then the docs source
> > >>>> shipped with the release doesn't match the site, and until then, 1.3
> > >>>> is released without some "must-have" docs for 1.3 on the site.
> > >>>>
> > >>>> The real question to me is: are there any further, absolutely
> > >>>> essential doc changes that need to accompany 1.3 or not?
> > >>>>
> > >>>> If not, just resolve these. If there are, then it seems like the
> > >>>> release has to block on them. If there are some docs that should
> have
> > >>>> gone in for 1.3, but didn't, but aren't essential, well I suppose it
> > >>>> bears thinking about how to not slip as much work, but it doesn't
> > >>>> block.
> > >>>>
> > >>>> I think Documentation issues certainly can be a blocker and
> shouldn't
> > >>>> be specially ignored.
> > >>>>
> > >>>>
> > >>>> BTW the UISeleniumSuite issue is a real failure, but I do not think
> it
> > >>>> is serious: http://issues.apache.org/jira/browse/SPARK-6205  It
> isn't
> > >>>> a regression from 1.2.x, but only affects tests, and only affects a
> > >>>> subset of build profiles.
> > >>>>
> > >>>>
> > >>>>
> > >>>>
> > >>>> On Fri, Mar 6, 2015 at 6:43 PM, Patrick Wendell <pwendell@gmail.com
> >
> > wrote:
> > >>>>> Hey Sean,
> > >>>>>
> > >>>>>> SPARK-5310 Update SQL programming guide for 1.3
> > >>>>>> SPARK-5183 Document data source API
> > >>>>>> SPARK-6128 Update Spark Streaming Guide for Spark 1.3
> > >>>>>
> > >>>>> For these, the issue is that they are documentation JIRA's, which
> > >>>>> don't need to be timed exactly with the release vote, since we can
> > >>>>> update the documentation on the website whenever we want. In the
> past
> > >>>>> I've just mentally filtered these out when considering RC's. I see
> a
> > >>>>> few options here:
> > >>>>>
> > >>>>> 1. We downgrade such issues away from Blocker (more clear, but we
> > risk
> > >>>>> loosing them in the fray if they really are things we want to have
> > >>>>> before the release is posted).
> > >>>>> 2. We provide a filter to the community that excludes
> 'Documentation'
> > >>>>> issues and shows all other blockers for 1.3. We can put this on the
> > >>>>> wiki, for instance.
> > >>>>>
> > >>>>> Which do you prefer?
> > >>>>>
> > >>>>> - Patrick
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a1135e0842660af0510e69aea--

From dev-return-11941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 09:09:10 2015
Return-Path: <dev-return-11941-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 625DA17E60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 09:09:10 +0000 (UTC)
Received: (qmail 74420 invoked by uid 500); 10 Mar 2015 09:09:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74353 invoked by uid 500); 10 Mar 2015 09:09:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73888 invoked by uid 99); 10 Mar 2015 09:09:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 09:09:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pllee@appier.com designates 209.85.216.173 as permitted sender)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 09:08:35 +0000
Received: by qcyl6 with SMTP id l6so110726qcy.13
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 02:06:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=/0JTs7lzKy1KmxInySYqtXc6bdE6WiZl69F+ZSZ6dsQ=;
        b=eaY9/OhbaUGwQ2KY2yPTxxlaGNaWtTbkhaK46ManKOcr9CnNfSmeAV/yxZvXwGpSWb
         SXzyIrRYe0Twc51lGHPB8BzD8Wr+OnhtSXNz16xvi7A5MHhRlfSxLPaIUPwEJt3hRkhN
         TvXdYY6gnUS0MAv9WfA0xrkCYAlBvjpS1YUX4=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=/0JTs7lzKy1KmxInySYqtXc6bdE6WiZl69F+ZSZ6dsQ=;
        b=Dn+1QkHA7IfeCbycekJtIcPHRtuDNBbf7q3JptoLEtCrkJHxc0u/jh09IdbaK/GPHg
         4PyzDHGwNEcixD6VD36ig9fvAFMprbfug0f0OVeABQ/PfOMlR15emH1nZSzghoPhitG1
         I96UpYLzyHkTRWyyVVPgi3rnvrN8qg8BsyjAHf/rOcYkZH1KLG7ufLMFRrBAYW2OLtDY
         c0qEPZIMHIuHLs68QTlZczOVMIsA0qF3T3uS96ZbU7HnEBqGITuSRleEt5LiqaZh5FTc
         u3/JwrEM6E+uFbruELYE0aISYarxTCvecqccCh0RBMWmnIBD76LUC3Dv2fjDFCVo2n+W
         oBkw==
X-Gm-Message-State: ALoCoQlMbx+FeSgVUXdG6Q8Kko9zqxkoYwo2ITwetCNCxbCWoFrwOFugwYclpVWw+iYCeAapeoZm
MIME-Version: 1.0
X-Received: by 10.55.26.104 with SMTP id a101mr40613717qka.81.1425978377041;
 Tue, 10 Mar 2015 02:06:17 -0700 (PDT)
Received: by 10.229.233.136 with HTTP; Tue, 10 Mar 2015 02:06:16 -0700 (PDT)
Date: Tue, 10 Mar 2015 17:06:16 +0800
Message-ID: <CANrtgzUX4pM3akkPE3a2AcHxWDXdaw3OazpDt2CARcOEffeQsg@mail.gmail.com>
Subject: SparkSQL 1.3.0 (RC3) failed to read parquet file generated by 1.1.1
From: Pei-Lun Lee <pllee@appier.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a114592a69f114f0510eb7621
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114592a69f114f0510eb7621
Content-Type: text/plain; charset=UTF-8

Hi,

I found that if I try to read parquet file generated by spark 1.1.1 using
1.3.0-rc3 by default settings, I got this error:

com.fasterxml.jackson.core.JsonParseException: Unrecognized token
'StructType': was expecting ('true', 'false' or 'null')
 at [Source: StructType(List(StructField(a,IntegerType,false))); line: 1,
column: 11]
        at
com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1419)
        at
com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508)
        at
com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2300)
        at
com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1459)
        at
com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:683)
        at
com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3105)
        at
com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3051)
        at
com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2161)
        at org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:19)
        at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:44)
        at org.apache.spark.sql.types.DataType$.fromJson(dataTypes.scala:41)
        at
org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
        at
org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)



this is how I save parquet file with 1.1.1:

sql("select 1 as a").saveAsParquetFile("/tmp/foo")



and this is the meta data of the 1.1.1 parquet file:

creator:     parquet-mr version 1.4.3
extra:       org.apache.spark.sql.parquet.row.metadata =
StructType(List(StructField(a,IntegerType,false)))



by comparison, this is 1.3.0 meta:

creator:     parquet-mr version 1.6.0rc3
extra:       org.apache.spark.sql.parquet.row.metadata =
{"type":"struct","fields":[{"name":"a","type":"integer","nullable":t
[more]...



It looks like now ParquetRelation2 is used to load parquet file by default
and it only recognizes JSON format schema but 1.1.1 schema was case class
string format.

Setting spark.sql.parquet.useDataSourceApi to false will fix it, but I
don't know the differences.
Is this considered a bug? We have a lot of parquet files from 1.1.1, should
we disable data source api in order to read them if we want to upgrade to
1.3?

Thanks,
--
Pei-Lun

--001a114592a69f114f0510eb7621--

From dev-return-11942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 10:38:29 2015
Return-Path: <dev-return-11942-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 417DE174BE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 10:38:29 +0000 (UTC)
Received: (qmail 89900 invoked by uid 500); 10 Mar 2015 10:38:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89751 invoked by uid 500); 10 Mar 2015 10:38:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89010 invoked by uid 99); 10 Mar 2015 10:38:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 10:38:26 +0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [203.81.22.165] (HELO mail1.qilinsoft.com) (203.81.22.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 10:38:00 +0000
X-MimeOLE: Produced By Microsoft Exchange V6.5
Content-class: urn:content-classes:message
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----_=_NextPart_001_01D05B1E.5672B950"
Subject: [SparkSQL] Reuse HiveContext to different Hive warehouse?
Date: Tue, 10 Mar 2015 18:37:34 +0800
Message-ID: <2EB23AF5EEEA2140946B8F292EB2EB9F1AD8F1@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
Thread-Topic: [SparkSQL] Reuse HiveContext to different Hive warehouse?
Thread-Index: AdBbHjiP/Y9s/nWcT+SQW6TSLqlZdg==
From: "Haopu Wang" <HWang@qilinsoft.com>
To: "user" <user@spark.apache.org>,
	<dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

------_=_NextPart_001_01D05B1E.5672B950
Content-Type: text/plain;
	charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

I'm using Spark 1.3.0 RC3 build with Hive support.

=20

In Spark Shell, I want to reuse the HiveContext instance to different
warehouse locations. Below are the steps for my test (Assume I have
loaded a file into table "src").

=20

=3D=3D=3D=3D=3D=3D

15/03/10 18:22:59 INFO SparkILoop: Created sql context (with Hive
support)..

SQL context available as sqlContext.

scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w")

scala> sqlContext.sql("SELECT * from src").saveAsTable("table1")

scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w2")

scala> sqlContext.sql("SELECT * from src").saveAsTable("table2")

=3D=3D=3D=3D=3D=3D

After these steps, the tables are stored in "/test/w" only. I expect
"table2" to be stored in "/test/w2" folder.

=20

Another question is: if I set "hive.metastore.warehouse.dir" to a HDFS
folder, I cannot use saveAsTable()? Is this by design? Exception stack
trace is below:

=3D=3D=3D=3D=3D=3D

15/03/10 18:35:28 INFO BlockManagerMaster: Updated info of block
broadcast_0_piece0

15/03/10 18:35:28 INFO SparkContext: Created broadcast 0 from broadcast
at TableReader.scala:74

java.lang.IllegalArgumentException: Wrong FS:
hdfs://server:8020/space/warehouse/table2, expected: file:///

        at
org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)

        at
org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:463)

        at
org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.jav
a:118)

        at
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.a
pply(newParquet.scala:252)

        at
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.a
pply(newParquet.scala:251)

        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sc
ala:244)

        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sc
ala:244)

        at scala.collection.immutable.List.foreach(List.scala:318)

        at
scala.collection.TraversableLike$class.map(TraversableLike.scala:244)

        at
scala.collection.AbstractTraversable.map(Traversable.scala:105)

        at
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newP
arquet.scala:251)

        at
org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.scala:37
0)

        at
org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.sca
la:96)

        at
org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.sca
la:125)

        at
org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:308)

        at
org.apache.spark.sql.hive.execution.CreateMetastoreDataSourceAsSelect.ru
n(commands.scala:217)

        at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompu
te(commands.scala:55)

        at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands
.scala:55)

        at
org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65
)

        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLConte
xt.scala:1088)

        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:10
88)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:1048)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:998)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:964)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:942)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:27)

        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29)

        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:31)

        at $iwC$$iwC$$iwC.<init>(<console>:33)

        at $iwC$$iwC.<init>(<console>:35)

        at $iwC.<init>(<console>:37)

        at <init>(<console>:39)

=20

Thank you very much!

=20


------_=_NextPart_001_01D05B1E.5672B950--

From dev-return-11943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 16:26:29 2015
Return-Path: <dev-return-11943-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 534BD176B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 16:26:29 +0000 (UTC)
Received: (qmail 81852 invoked by uid 500); 10 Mar 2015 16:26:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81769 invoked by uid 500); 10 Mar 2015 16:26:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81742 invoked by uid 99); 10 Mar 2015 16:26:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 16:26:27 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.240.92.66] (HELO g9t5008.houston.hp.com) (15.240.92.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 16:25:59 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5008.houston.hp.com (Postfix) with ESMTPS id 52DD917E;
	Tue, 10 Mar 2015 16:25:56 +0000 (UTC)
Received: from G9W3617.americas.hpqcorp.net (16.216.186.52) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Tue, 10 Mar 2015 16:25:09 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G9W3617.americas.hpqcorp.net ([16.216.186.52]) with mapi id 14.03.0169.001;
 Tue, 10 Mar 2015 16:25:09 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Sam Halliday <sam.halliday@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, Xiangrui Meng
	<mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, "Evan R. Sparks"
	<evan.sparks@gmail.com>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAB//iiA=
Date: Tue, 10 Mar 2015 16:25:07 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE18268@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
In-Reply-To: <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE18268G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE18268G4W3292americas_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE18268G4W3292americas_--

From dev-return-11944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 18:13:58 2015
Return-Path: <dev-return-11944-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B3F117D8B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 18:13:58 +0000 (UTC)
Received: (qmail 7498 invoked by uid 500); 10 Mar 2015 18:13:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7418 invoked by uid 500); 10 Mar 2015 18:13:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7406 invoked by uid 99); 10 Mar 2015 18:13:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 18:13:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 209.85.217.169 as permitted sender)
Received: from [209.85.217.169] (HELO mail-lb0-f169.google.com) (209.85.217.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 18:13:23 +0000
Received: by lbiz12 with SMTP id z12so3649172lbi.5
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 11:12:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=xZ3AnMKul/bMus16ehY7/xTUlIK+vFPiMip3A+rl86w=;
        b=Xt5inx9kEduRA6ly6Hl0CsY4ZmQoXwt6cJkIi6Ea+FkjJG5A26t5HdkKX0DjCMaCij
         KrvQYQvEueyk16PWB20PYIrUwP3hLQOUcF+PJ88WYieZlYNhHPTbqhukxSuV4D4yxZS8
         Iq7Yu+ZPtlX6Mc5Ua9BX/KxgNKNk7IcFIvBKcbj8MYCJ2fdmhRH/WIpRD1L58vz8AeTx
         biJ0+GkGVzPMXJGWIp8+sV9Ou0+ZUxXlK/dsxP+FlYcf+rmJF5IElCV+HioNSYXlhpaA
         HqfJdto75EJUMFaA2oInTVb4BLgLq1QMGaIiBn02jdzO0X00BJUX/o7MHdc6Vhq2SDea
         V6aw==
X-Gm-Message-State: ALoCoQm1Wyr0HsmJA6N3LDItcYM9LGtc2Lr6uEpr/ann6i0d/i8BX5NEaA2gEn2Ufa0USgWN7bEC
MIME-Version: 1.0
X-Received: by 10.112.223.7 with SMTP id qq7mr31537040lbc.81.1426011155899;
 Tue, 10 Mar 2015 11:12:35 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.25.160.202 with HTTP; Tue, 10 Mar 2015 11:12:35 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE18268@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE18268@G4W3292.americas.hpqcorp.net>
Date: Tue, 10 Mar 2015 11:12:35 -0700
Message-ID: <CAKx7Bf_kyhjjzvuNwdZOBs=iwF2BCXd-CAESG4H2+98t5tU1aQ@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Sam Halliday <sam.halliday@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, 
	"Evan R. Sparks" <evan.sparks@gmail.com>
Content-Type: multipart/alternative; boundary=001a113464ac64b6dc0510f318f9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113464ac64b6dc0510f318f9
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I have run some BLAS comparison benchmarks on different EC2 instance sizes
and also on NERSC super computers. I can put together a github-backed
website where we can host latest benchmark results and update them over
time.

Sam -- Does that sound like what you had in mind ?

Thanks
Shivaram

On Tue, Mar 10, 2015 at 9:25 AM, Ulanov, Alexander <alexander.ulanov@hp.com=
>
wrote:

> I can run benchmark on another machine with GPU nVidia Titan and Intel
> Xeon E5-2650 v2, although it runs Windows and I have to run Linux tests i=
n
> VirtualBox.
>
> It would be also interesting to add results on netlib+nvblas, however I a=
m
> not sure I understand in details how to build this and will appreciate an=
y
> help from you =E2=98=BA
>
> From: Sam Halliday [mailto:sam.halliday@gmail.com]
> Sent: Monday, March 09, 2015 6:01 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
> Thanks so much for following up on this!
>
> Hmm, I wonder if we should have a concerted effort to chart performance o=
n
> various pieces of hardware...
> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto:
> alexander.ulanov@hp.com>> wrote:
> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
> support of Double in the current source code), did the test with BIDMat a=
nd
> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
> sam.halliday@gmail.com>]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
> dev@spark.apache.org>
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
> <mailto:joseph@databricks.com>> wrote:
> >> Better documentation for linking would be very helpful!  Here's a JIRA=
:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for larger
> matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org=
>
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com>]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley; dev@spark.apache.org<mailto:dev@spark.apache.org=
>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
> to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked up b=
y
> netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I understand
> how to force use a specific blas (not specific wrapper for blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already have
> it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
> joseph@databricks.com><mailto:
> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons with
> Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential to
> machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and =
4
> GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x faster a=
t
> matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster within
> Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better performance
> only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was not a
> test for ONLY multiplication since there are other operations involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
> dev-unsubscribe@spark.apache.org><mailto:
> >>>> dev-unsubscribe@spark.apache.org<mailto:
> dev-unsubscribe@spark.apache.org>><mailto:dev-unsubscribe@spark.apac
> <mailto:dev-unsubscribe@spark.apac>
> >>>> he.org<http://he.org> <mailto:dev-unsubscribe@spark.apache.org
> <mailto:dev-unsubscribe@spark.apache.org>>>
> >>>> For additional commands, e-mail: dev-help@spark.apache.org<mailto:
> dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --
> Best regards,
> Sam
>

--001a113464ac64b6dc0510f318f9--

From dev-return-11945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 20:53:47 2015
Return-Path: <dev-return-11945-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E251177D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 20:53:47 +0000 (UTC)
Received: (qmail 6711 invoked by uid 500); 10 Mar 2015 20:53:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6624 invoked by uid 500); 10 Mar 2015 20:53:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6613 invoked by uid 99); 10 Mar 2015 20:53:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 20:53:45 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_IMAGE_ONLY_24,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nitay@actioniq.co designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 20:53:21 +0000
Received: by obcva2 with SMTP id va2so4588053obc.3
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 13:51:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=wMpk3Mlc9cbhSxM37FRyx5b+Rqnc12jImJNp3pQxQJg=;
        b=IgT54PW+kNOZdCVZTKlUGRVmNQLj7DMgI/5K8/htWEPgkxc+Iytqtm8q42eKY8R6zb
         4wS8Uy18xNKhjAQRP9ipZe75fR9nvhPcJBXZ/dEzfWiUu38ht9AQEKnaLBlVQaM+AWwH
         UQ5aBwgi6PSWHFF4KhtZU07AyAgY9kNGXCyHxmeE1bRGfwAF7bF31vstiv6kdUbIHe1c
         KJYjZvjJ2U5IJWpoCrAbcWcto1NYQDvpDxVzFOY4GCmJtInReIO+YIXQVFGrjwktXbvg
         su+LW46aswrFC9NtQO0/KEEfQGUunXcBcckLX+spN29Mi5hR0t3roG+/2obiltoLadVO
         g1+g==
X-Gm-Message-State: ALoCoQllqvcjcSBSsT0P9G077JEN1irZi2thbTI5YwuHvx4rHyHiUTpYaCmN0subbAKCPKPuoy/j
MIME-Version: 1.0
X-Received: by 10.202.215.212 with SMTP id o203mr26170567oig.85.1426020663875;
 Tue, 10 Mar 2015 13:51:03 -0700 (PDT)
Received: by 10.182.29.35 with HTTP; Tue, 10 Mar 2015 13:51:03 -0700 (PDT)
Date: Tue, 10 Mar 2015 16:51:03 -0400
Message-ID: <CAO8MLFaaduGye7D4VXtJKPJEqKFzV33Dj5BBeCjOvqnWFbJJZg@mail.gmail.com>
Subject: Spark 1.3 SQL Type Parser Changes?
From: Nitay Joffe <nitay@actioniq.co>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113d465c1d1e410510f54f53
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d465c1d1e410510f54f53
Content-Type: text/plain; charset=UTF-8

In Spark 1.2 I used to be able to do this:

scala>
org.apache.spark.sql.hive.HiveMetastoreTypes.toDataType("struct<int:bigint>")
res30: org.apache.spark.sql.catalyst.types.DataType =
StructType(List(StructField(int,LongType,true)))

That is, the name of a column can be a keyword like "int". This is no
longer the case in 1.3:

data-pipeline-shell> HiveTypeHelper.toDataType("struct<int:bigint>")
org.apache.spark.sql.sources.DDLException: Unsupported dataType: [1.8]
failure: ``>'' expected but `int' found

struct<int:bigint>
       ^
        at org.apache.spark.sql.sources.DDLParser.parseType(ddl.scala:52)
        at
org.apache.spark.sql.hive.HiveMetastoreTypes$.toDataType(HiveMetastoreCatalog.scala:785)
        at
org.apache.spark.sql.hive.HiveTypeHelper$.toDataType(HiveTypeHelper.scala:9)

Note HiveTypeHelper is simply an object I load in to expose
HiveMetastoreTypes since it was made private. See
https://gist.github.com/nitay/460b41ed5fd7608507f5
<https://app.relateiq.com/r?c=chrome_gmail&url=https%3A%2F%2Fgist.github.com%2Fnitay%2F460b41ed5fd7608507f5&t=AFwhZf262cJFT8YSR54ZotvY2aTmpm_zHTSKNSd4jeT-a6b8q-yMXQ-BqEX9-Ym54J1bkDFiFOXyRKsNxXoDGIh7bhqbBVKsGGq6YTJIfLZxs375XXPdS13KHsE_3Lffk4UIFkRFZ_7c>

This is actually a pretty big problem for us as we have a bunch of legacy
tables with column names like "timestamp". They work fine in 1.2, but now
everything throws in 1.3.

Any thoughts?

Thanks,
- Nitay
Founder & CTO

--001a113d465c1d1e410510f54f53--

From dev-return-11946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 10 21:46:13 2015
Return-Path: <dev-return-11946-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DEBE917A65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 10 Mar 2015 21:46:12 +0000 (UTC)
Received: (qmail 47826 invoked by uid 500); 10 Mar 2015 21:46:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47692 invoked by uid 500); 10 Mar 2015 21:46:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46871 invoked by uid 99); 10 Mar 2015 21:46:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 21:46:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_IMAGE_ONLY_32,HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.51] (HELO mail-la0-f51.google.com) (209.85.215.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 10 Mar 2015 21:45:43 +0000
Received: by labhs14 with SMTP id hs14so4322082lab.5
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 14:43:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=QqUxQc3UcA8u+/GEjbpgu9KTY/SJCnyti1Qf9GPpuPE=;
        b=BQK2aTpxcmd1NhqFGngRKfD0eMp/ozfIzfLuqe3+gYzr6XlV16hmt81XPPV3Hil2q7
         D436BbrcohmwH284QmD50PeknQCrrGKq5tTFq8VIEXBBsJJZId4+mH9BSssWG+Wi4YKx
         YPUIBGjeobwffi3btdIYDokkXivoVHkduzoGdR7hZrm1zwbbCT24uYkQIGnPX0JMLfeJ
         sjh1zcegAMulFZtiUCvUMFt4aWnPED879vXE+WPIKpvutYTzedG3pbJTAUmIa5C8yDoW
         Nwa2Jt6Vj1ZPEJQFhQOwizr6NqNaIvYnZkgqHWGY9dhjJ8zsFxvJSXueKQ7HvbU9DAwE
         0dPg==
X-Gm-Message-State: ALoCoQmvpKV6CsTDOwkIudnD7dcmeMKK9yC61ewxZzPu26t84yDBu/3p6IkunTfMAY/JJd/zKKLw
X-Received: by 10.152.29.68 with SMTP id i4mr31939683lah.5.1426023831648; Tue,
 10 Mar 2015 14:43:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.213.18 with HTTP; Tue, 10 Mar 2015 14:43:31 -0700 (PDT)
In-Reply-To: <CAO8MLFaaduGye7D4VXtJKPJEqKFzV33Dj5BBeCjOvqnWFbJJZg@mail.gmail.com>
References: <CAO8MLFaaduGye7D4VXtJKPJEqKFzV33Dj5BBeCjOvqnWFbJJZg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 10 Mar 2015 14:43:31 -0700
Message-ID: <CAAswR-4dtgPfHq5sPadvt1FqBjNAkLetMjmj5RZN8R3RAZ9myw@mail.gmail.com>
Subject: Re: Spark 1.3 SQL Type Parser Changes?
To: Nitay Joffe <nitay@actioniq.co>
Cc: user <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158c0a4ee29500510f60be7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c0a4ee29500510f60be7
Content-Type: text/plain; charset=UTF-8

Thanks for reporting.  This was a result of a change to our DDL parser that
resulted in types becoming reserved words.  I've filled a JIRA and will
investigate if this is something we can fix.
https://issues.apache.org/jira/browse/SPARK-6250

On Tue, Mar 10, 2015 at 1:51 PM, Nitay Joffe <nitay@actioniq.co> wrote:

> In Spark 1.2 I used to be able to do this:
>
> scala>
> org.apache.spark.sql.hive.HiveMetastoreTypes.toDataType("struct<int:bigint>")
> res30: org.apache.spark.sql.catalyst.types.DataType =
> StructType(List(StructField(int,LongType,true)))
>
> That is, the name of a column can be a keyword like "int". This is no
> longer the case in 1.3:
>
> data-pipeline-shell> HiveTypeHelper.toDataType("struct<int:bigint>")
> org.apache.spark.sql.sources.DDLException: Unsupported dataType: [1.8]
> failure: ``>'' expected but `int' found
>
> struct<int:bigint>
>        ^
>         at org.apache.spark.sql.sources.DDLParser.parseType(ddl.scala:52)
>         at
> org.apache.spark.sql.hive.HiveMetastoreTypes$.toDataType(HiveMetastoreCatalog.scala:785)
>         at
> org.apache.spark.sql.hive.HiveTypeHelper$.toDataType(HiveTypeHelper.scala:9)
>
> Note HiveTypeHelper is simply an object I load in to expose
> HiveMetastoreTypes since it was made private. See
> https://gist.github.com/nitay/460b41ed5fd7608507f5
> <https://app.relateiq.com/r?c=chrome_gmail&url=https%3A%2F%2Fgist.github.com%2Fnitay%2F460b41ed5fd7608507f5&t=AFwhZf262cJFT8YSR54ZotvY2aTmpm_zHTSKNSd4jeT-a6b8q-yMXQ-BqEX9-Ym54J1bkDFiFOXyRKsNxXoDGIh7bhqbBVKsGGq6YTJIfLZxs375XXPdS13KHsE_3Lffk4UIFkRFZ_7c>
>
> This is actually a pretty big problem for us as we have a bunch of legacy
> tables with column names like "timestamp". They work fine in 1.2, but now
> everything throws in 1.3.
>
> Any thoughts?
>
> Thanks,
> - Nitay
> Founder & CTO
>
>

--089e0158c0a4ee29500510f60be7--

From dev-return-11947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 00:07:09 2015
Return-Path: <dev-return-11947-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED88D1728C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 00:07:08 +0000 (UTC)
Received: (qmail 70715 invoked by uid 500); 11 Mar 2015 00:07:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70627 invoked by uid 500); 11 Mar 2015 00:07:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70616 invoked by uid 99); 11 Mar 2015 00:07:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:07:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:07:03 +0000
Received: by lbvn10 with SMTP id n10so5442838lbv.1
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 17:06:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=fx33jx7hySYYdTDMRpc6tfxfnF0o06r5ao1E/wzOlco=;
        b=bX6U0b9gVgwhgiU6WGgxkW5ekByxGOpzLyaRrT6edDCJJHdoGuioHRpWKYtXjRPYgy
         RY/0OHOjPDn7g0Q/C90GVGpI1GyRVyKnoG4hfEt1OD7/0KUVBtwiHeEIvIcMa40ioENS
         Vd1kV7ywRNyt24eEERMgzrPbgBIlQlTYMpLbAT72rfOVJ2UVMV3wHboGH4Kyadv+TaJd
         mhRxI5s0YQgx2vyoi8tV3yk8h3+nUuW0aTqxUdYon0tdPb2IZx9A1RkSse3fjItvYEEq
         g2r+eORL6RpSQchO11B7c6roP5YZjgTDydJBHj2WJEFbEn6ZaCwkZaVflplsUuxr7ieQ
         YdxA==
X-Gm-Message-State: ALoCoQmldPseKVYgvBcPKiv/u2jJKVrJxVvtwS4OHuHOGkJKsTNCtCtAkVUiyzca5EVC7j0wYWy/
MIME-Version: 1.0
X-Received: by 10.152.36.138 with SMTP id q10mr32063835laj.113.1426032381837;
 Tue, 10 Mar 2015 17:06:21 -0700 (PDT)
Received: by 10.25.86.136 with HTTP; Tue, 10 Mar 2015 17:06:21 -0700 (PDT)
In-Reply-To: <CAAswR-4dtgPfHq5sPadvt1FqBjNAkLetMjmj5RZN8R3RAZ9myw@mail.gmail.com>
References: <CAO8MLFaaduGye7D4VXtJKPJEqKFzV33Dj5BBeCjOvqnWFbJJZg@mail.gmail.com>
	<CAAswR-4dtgPfHq5sPadvt1FqBjNAkLetMjmj5RZN8R3RAZ9myw@mail.gmail.com>
Date: Tue, 10 Mar 2015 17:06:21 -0700
Message-ID: <CAHP0waJ6rE98YQbWYnZXv1OwuPgw1nxcbW=3j8mqsiR5fpXzBA@mail.gmail.com>
Subject: Re: Spark 1.3 SQL Type Parser Changes?
From: Yin Huai <yhuai@databricks.com>
To: Michael Armbrust <michael@databricks.com>
Cc: Nitay Joffe <nitay@actioniq.co>, user <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160b9228ed7e20510f8091d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b9228ed7e20510f8091d
Content-Type: text/plain; charset=UTF-8

Hi Nitay,

Can you try using backticks to quote the column name? Like
org.apache.spark.sql.hive.HiveMetastoreTypes.toDataType(
"struct<`int`:bigint>")?

Thanks,

Yin

On Tue, Mar 10, 2015 at 2:43 PM, Michael Armbrust <michael@databricks.com>
wrote:

> Thanks for reporting.  This was a result of a change to our DDL parser
> that resulted in types becoming reserved words.  I've filled a JIRA and
> will investigate if this is something we can fix.
> https://issues.apache.org/jira/browse/SPARK-6250
>
> On Tue, Mar 10, 2015 at 1:51 PM, Nitay Joffe <nitay@actioniq.co> wrote:
>
>> In Spark 1.2 I used to be able to do this:
>>
>> scala>
>> org.apache.spark.sql.hive.HiveMetastoreTypes.toDataType("struct<int:bigint>")
>> res30: org.apache.spark.sql.catalyst.types.DataType =
>> StructType(List(StructField(int,LongType,true)))
>>
>> That is, the name of a column can be a keyword like "int". This is no
>> longer the case in 1.3:
>>
>> data-pipeline-shell> HiveTypeHelper.toDataType("struct<int:bigint>")
>> org.apache.spark.sql.sources.DDLException: Unsupported dataType: [1.8]
>> failure: ``>'' expected but `int' found
>>
>> struct<int:bigint>
>>        ^
>>         at org.apache.spark.sql.sources.DDLParser.parseType(ddl.scala:52)
>>         at
>> org.apache.spark.sql.hive.HiveMetastoreTypes$.toDataType(HiveMetastoreCatalog.scala:785)
>>         at
>> org.apache.spark.sql.hive.HiveTypeHelper$.toDataType(HiveTypeHelper.scala:9)
>>
>> Note HiveTypeHelper is simply an object I load in to expose
>> HiveMetastoreTypes since it was made private. See
>> https://gist.github.com/nitay/460b41ed5fd7608507f5
>> <https://app.relateiq.com/r?c=chrome_gmail&url=https%3A%2F%2Fgist.github.com%2Fnitay%2F460b41ed5fd7608507f5&t=AFwhZf262cJFT8YSR54ZotvY2aTmpm_zHTSKNSd4jeT-a6b8q-yMXQ-BqEX9-Ym54J1bkDFiFOXyRKsNxXoDGIh7bhqbBVKsGGq6YTJIfLZxs375XXPdS13KHsE_3Lffk4UIFkRFZ_7c>
>>
>> This is actually a pretty big problem for us as we have a bunch of legacy
>> tables with column names like "timestamp". They work fine in 1.2, but now
>> everything throws in 1.3.
>>
>> Any thoughts?
>>
>> Thanks,
>> - Nitay
>> Founder & CTO
>>
>>
>

--089e0160b9228ed7e20510f8091d--

From dev-return-11948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 00:10:33 2015
Return-Path: <dev-return-11948-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 120B4172AE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 00:10:33 +0000 (UTC)
Received: (qmail 78290 invoked by uid 500); 11 Mar 2015 00:09:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78199 invoked by uid 500); 11 Mar 2015 00:09:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78187 invoked by uid 99); 11 Mar 2015 00:09:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:09:56 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Ilya.Ganelin@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail03.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:09:31 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=6305; q=dns/txt; s=SM2048Apr2013K;
  t=1426032590; x=1426118990;
  h=from:to:date:subject:message-id:mime-version;
  bh=Y7z9DQ6OSIPwUY/EITtORmsiDnXi6y2JRMV8M1akDb4=;
  b=v7gztAiIjdKKPIuzhb4G1e2COjPu64GXSesC+I1jdrYgLHfLRaR6E/wZ
   f1ykdPH5WH0iBuO/hh/qqz4pbBYJyoNGtUdu/zYXVlbkb/hbFqHfZMJsU
   efEkxMxIStPrQWSNp1Qg0Cty/83NTFoujcG9rfelHrZM+J0yRp0dv4zug
   H4w6MyLsd3GW7pUGESLY4E6SvwXO1RZQKWWRsoXrPHZRmLE+lx+ExXYN2
   kiCFnLxIUmi33gWV5IGXNBx4dotLPWPXaVr6ZWpFMyNrJg6VnWKUgKaRP
   eZMcc7Dzk4gJnZXED3M6LO9DarcCCi/oW9AWK5OxCdbMYq9kFjstckQkR
   A==;
X-IronPort-AV: E=McAfee;i="5600,1067,7736"; a="208600191"
X-IronPort-AV: E=Sophos;i="5.11,378,1422939600"; 
   d="scan'208,217";a="208600191"
X-HTML-Disclaimer: True
Received: from kdcpexcasht05.cof.ds.capitalone.com ([10.37.194.49])
  by komail03.kdc.capitalone.com with ESMTP; 10 Mar 2015 20:09:28 -0400
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.56]) by
 KDCPEXCASHT05.cof.ds.capitalone.com ([10.37.194.49]) with mapi; Tue, 10 Mar
 2015 20:09:28 -0400
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: dev <dev@spark.apache.org>
Date: Tue, 10 Mar 2015 20:09:47 -0400
Subject: Spark tests hang on local machine due to "testGuavaOptional" in
 JavaAPISuite
Thread-Topic: Spark tests hang on local machine due to "testGuavaOptional"
 in JavaAPISuite
Thread-Index: AdBbj6OJCz2C3uJBTOiIL3euk8gaZw==
Message-ID: <D124D5DB.1BB7D%ilya.ganelin@capitalone.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.7.141117
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_D124D5DB1BB7Dilyaganelincapitalonecom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D124D5DB1BB7Dilyaganelincapitalonecom_
MIME-Version: 1.0
Content-Type: text/plain; charset="windows-1252"
Content-Transfer-Encoding: quoted-printable

Hi all =96 building Spark on my local machine with build/mvn clean package =
test runs until it hits the JavaAPISuite where it hangs indefinitely. Throu=
gh some experimentation, I=92ve narrowed it down to the following test:


/**
 * Test for SPARK-3647. This test needs to use the maven-built assembly to =
trigger the issue,
 * since that's the only artifact where Guava classes have been relocated.
 */
@Test
public void testGuavaOptional() {
  // Stop the context created in setUp() and start a local-cluster one, to =
force usage of the
  // assembly.
  sc.stop();
  JavaSparkContext localCluster =3D new JavaSparkContext("local-cluster[1,1=
,512]", "JavaAPISuite");
  try {
    JavaRDD<Integer> rdd1 =3D localCluster.parallelize(Arrays.asList(1, 2, =
null), 3);
    JavaRDD<Optional<Integer>> rdd2 =3D rdd1.map(
      new Function<Integer, Optional<Integer>>() {
        @Override
        public Optional<Integer> call(Integer i) {
          return Optional.fromNullable(i);
        }
      });
    rdd2.collect();
  } finally {
    localCluster.stop();
  }
}


If I remove this test, things work smoothly. Has anyone else seen this? Tha=
nks.
________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.

--_000_D124D5DB1BB7Dilyaganelincapitalonecom_--


From dev-return-11949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 00:15:53 2015
Return-Path: <dev-return-11949-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 679A3172D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 00:15:53 +0000 (UTC)
Received: (qmail 88309 invoked by uid 500); 11 Mar 2015 00:15:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88235 invoked by uid 500); 11 Mar 2015 00:15:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88224 invoked by uid 99); 11 Mar 2015 00:15:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:15:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:15:05 +0000
Received: by wibbs8 with SMTP id bs8so7364008wib.4
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 17:13:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=tdBTk612kms846jNR48ueqKrxfkBhEW3t2Lh8u1F1Cw=;
        b=RqgkqS+r86dLpDbMnuVk9sWJo6kU/IHRgBFs8WGY6XEi391THsSH7Y1+/MeR787MEP
         pPCAWX6dc2LOy5mR2O8Dlyk71w8aVPqw0FvkKuEjYGDyYBa/yPgWYBU3hvjwIJnYkw3w
         s28bckq3jWolwk4MWIBULChLXTZhktQkoXFrcCKJgGTBBTR1AKXdkiRrd8c4GNI3yQfp
         wxdAsnFyJRu1S9AH2QlX2n9Icc7NXesJh2c7IFW7bX6QTuI6/kxxKDAKmYgGD7eer62I
         AwYwIXrh1Vbpt0Ff8Ok0pob9Bq2XMtR1aA+XXczJ/1nZXxr/rNbRhUNPqKbujlsvC5Rm
         ZsAA==
X-Gm-Message-State: ALoCoQmTlGZdIajJVjuDbqdRIbB0gjSB9v9bWHx8HqURvyy2Wg3utg8OcezphaGxeQ/CFMuvhv8t
X-Received: by 10.180.171.35 with SMTP id ar3mr75789093wic.24.1426032813962;
 Tue, 10 Mar 2015 17:13:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Tue, 10 Mar 2015 17:13:12 -0700 (PDT)
In-Reply-To: <D124D5DB.1BB7D%ilya.ganelin@capitalone.com>
References: <D124D5DB.1BB7D%ilya.ganelin@capitalone.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 11 Mar 2015 00:13:12 +0000
Message-ID: <CAMAsSdJDqbM0S-O8V0VsU+iBYkigh5y6oep0c-2fyLtxoxLhvg@mail.gmail.com>
Subject: Re: Spark tests hang on local machine due to "testGuavaOptional" in JavaAPISuite
To: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Yes and I remember it was caused by ... well something related to the
Guava shading and the fact that you're running a mini cluster and then
talking to it. I can't remember what exactly resolved it but try a
clean build. Somehow I think it had to do with multiple assembly files
or something like that.

On Wed, Mar 11, 2015 at 12:09 AM, Ganelin, Ilya
<Ilya.Ganelin@capitalone.com> wrote:
> Hi all =E2=80=93 building Spark on my local machine with build/mvn clean =
package test runs until it hits the JavaAPISuite where it hangs indefinitel=
y. Through some experimentation, I=E2=80=99ve narrowed it down to the follo=
wing test:
>
>
> /**
>  * Test for SPARK-3647. This test needs to use the maven-built assembly t=
o trigger the issue,
>  * since that's the only artifact where Guava classes have been relocated=
.
>  */
> @Test
> public void testGuavaOptional() {
>   // Stop the context created in setUp() and start a local-cluster one, t=
o force usage of the
>   // assembly.
>   sc.stop();
>   JavaSparkContext localCluster =3D new JavaSparkContext("local-cluster[1=
,1,512]", "JavaAPISuite");
>   try {
>     JavaRDD<Integer> rdd1 =3D localCluster.parallelize(Arrays.asList(1, 2=
, null), 3);
>     JavaRDD<Optional<Integer>> rdd2 =3D rdd1.map(
>       new Function<Integer, Optional<Integer>>() {
>         @Override
>         public Optional<Integer> call(Integer i) {
>           return Optional.fromNullable(i);
>         }
>       });
>     rdd2.collect();
>   } finally {
>     localCluster.stop();
>   }
> }
>
>
> If I remove this test, things work smoothly. Has anyone else seen this? T=
hanks.
> ________________________________________________________
>
> The information contained in this e-mail is confidential and/or proprieta=
ry to Capital One and/or its affiliates. The information transmitted herewi=
th is intended only for use by the individual or entity to which it is addr=
essed.  If the reader of this message is not the intended recipient, you ar=
e hereby notified that any review, retransmission, dissemination, distribut=
ion, copying or other use of, or taking of any action in reliance upon this=
 information is strictly prohibited. If you have received this communicatio=
n in error, please contact the sender and delete the material from your com=
puter.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 00:27:30 2015
Return-Path: <dev-return-11950-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 944E11734A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 00:27:30 +0000 (UTC)
Received: (qmail 10794 invoked by uid 500); 11 Mar 2015 00:27:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10658 invoked by uid 500); 11 Mar 2015 00:27:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9863 invoked by uid 99); 11 Mar 2015 00:27:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:27:25 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hao.cheng@intel.com designates 192.55.52.93 as permitted sender)
Received: from [192.55.52.93] (HELO mga11.intel.com) (192.55.52.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 00:27:19 +0000
Received: from orsmga003.jf.intel.com ([10.7.209.27])
  by fmsmga102.fm.intel.com with ESMTP; 10 Mar 2015 17:24:59 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.11,378,1422950400"; 
   d="scan'208,217";a="538984774"
Received: from kmsmsx151.gar.corp.intel.com ([172.21.73.86])
  by orsmga003.jf.intel.com with ESMTP; 10 Mar 2015 17:24:22 -0700
Received: from shsmsx103.ccr.corp.intel.com (10.239.4.69) by
 KMSMSX151.gar.corp.intel.com (172.21.73.86) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Wed, 11 Mar 2015 08:24:56 +0800
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.26]) by
 SHSMSX103.ccr.corp.intel.com ([169.254.4.45]) with mapi id 14.03.0224.002;
 Wed, 11 Mar 2015 08:24:55 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: Haopu Wang <HWang@qilinsoft.com>, user <user@spark.apache.org>,
	"dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: [SparkSQL] Reuse HiveContext to different Hive warehouse?
Thread-Topic: [SparkSQL] Reuse HiveContext to different Hive warehouse?
Thread-Index: AdBbHjiP/Y9s/nWcT+SQW6TSLqlZdgAHLlrA
Date: Wed, 11 Mar 2015 00:24:54 +0000
Message-ID: <80833ADD533E324CA05C160E41B636610284D5B9@shsmsx102.ccr.corp.intel.com>
References: <2EB23AF5EEEA2140946B8F292EB2EB9F1AD8F1@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
In-Reply-To: <2EB23AF5EEEA2140946B8F292EB2EB9F1AD8F1@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_80833ADD533E324CA05C160E41B636610284D5B9shsmsx102ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_80833ADD533E324CA05C160E41B636610284D5B9shsmsx102ccrcor_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

I am not so sure if Hive supports change the metastore after initialized, I=
 guess not. Spark SQL totally rely on Hive Metastore in HiveContext, probab=
ly that's why it doesn't work as expected for Q1.

BTW, in most of cases, people configure the metastore settings in hive-site=
.xml, and will not change that since then, is there any reason that you wan=
t to change that in runtime?

For Q2, probably something wrong in configuration, seems the HDFS run into =
the pseudo/single node mode, can you double check that? Or can you run the =
DDL (like create a table) from the spark shell with HiveContext?

From: Haopu Wang [mailto:HWang@qilinsoft.com]
Sent: Tuesday, March 10, 2015 6:38 PM
To: user; dev@spark.apache.org
Subject: [SparkSQL] Reuse HiveContext to different Hive warehouse?


I'm using Spark 1.3.0 RC3 build with Hive support.



In Spark Shell, I want to reuse the HiveContext instance to different wareh=
ouse locations. Below are the steps for my test (Assume I have loaded a fil=
e into table "src").



=3D=3D=3D=3D=3D=3D

15/03/10 18:22:59 INFO SparkILoop: Created sql context (with Hive support).=
.

SQL context available as sqlContext.

scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w")

scala> sqlContext.sql("SELECT * from src").saveAsTable("table1")

scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w2")

scala> sqlContext.sql("SELECT * from src").saveAsTable("table2")

=3D=3D=3D=3D=3D=3D

After these steps, the tables are stored in "/test/w" only. I expect "table=
2" to be stored in "/test/w2" folder.



Another question is: if I set "hive.metastore.warehouse.dir" to a HDFS fold=
er, I cannot use saveAsTable()? Is this by design? Exception stack trace is=
 below:

=3D=3D=3D=3D=3D=3D

15/03/10 18:35:28 INFO BlockManagerMaster: Updated info of block broadcast_=
0_piece0

15/03/10 18:35:28 INFO SparkContext: Created broadcast 0 from broadcast at =
TableReader.scala:74

java.lang.IllegalArgumentException: Wrong FS: hdfs://server:8020/space/ware=
house/table2, expected: file:///<file:///\\>

        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)

        at org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:46=
3)

        at org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSy=
stem.java:118)

        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$ano=
nfun$6.apply(newParquet.scala:252)

        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$ano=
nfun$6.apply(newParquet.scala:251)

        at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversabl=
eLike.scala:244)

        at scala.collection.TraversableLike$$anonfun$map$1.apply(Traversabl=
eLike.scala:244)

        at scala.collection.immutable.List.foreach(List.scala:318)

        at scala.collection.TraversableLike$class.map(TraversableLike.scala=
:244)

        at scala.collection.AbstractTraversable.map(Traversable.scala:105)

        at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refr=
esh(newParquet.scala:251)

        at org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.=
scala:370)

        at org.apache.spark.sql.parquet.DefaultSource.createRelation(newPar=
quet.scala:96)

        at org.apache.spark.sql.parquet.DefaultSource.createRelation(newPar=
quet.scala:125)

        at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala=
:308)

        at org.apache.spark.sql.hive.execution.CreateMetastoreDataSourceAsS=
elect.run(commands.scala:217)

        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$=
lzycompute(commands.scala:55)

        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(=
commands.scala:55)

        at org.apache.spark.sql.execution.ExecutedCommand.execute(commands.=
scala:65)

        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(=
SQLContext.scala:1088)

        at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.=
scala:1088)

        at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:1048)

        at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:998)

        at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:964)

        at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:942)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:27)

        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29)

        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:31)

        at $iwC$$iwC$$iwC.<init>(<console>:33)

        at $iwC$$iwC.<init>(<console>:35)

        at $iwC.<init>(<console>:37)

        at <init>(<console>:39)



Thank you very much!



--_000_80833ADD533E324CA05C160E41B636610284D5B9shsmsx102ccrcor_--

From dev-return-11951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 01:57:37 2015
Return-Path: <dev-return-11951-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DC4F1760F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 01:57:37 +0000 (UTC)
Received: (qmail 64689 invoked by uid 500); 11 Mar 2015 01:57:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64605 invoked by uid 500); 11 Mar 2015 01:57:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64594 invoked by uid 99); 11 Mar 2015 01:57:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 01:57:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.178] (HELO mail-lb0-f178.google.com) (209.85.217.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 01:57:29 +0000
Received: by lbjb6 with SMTP id b6so5718957lbj.9
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 18:54:33 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=qR8GIDewX7CTCMSDGf8x3KvZeHLuNdkoxtFQpG8sKM8=;
        b=P74pZiCRjHdlF/K8x8WkNfI4sddueorM6JhVgDBzZarNBbUHPoIyOXrNoovs7ux9M4
         eqNv2jr3JccnJ32zzLyw+iqReZpvlUStbHGn9o0t5OvMZ4EWSwk/+ScuRK2TBjcjM+px
         C/xek34AYapegt8RR/5VqtlwaP6OVp1ISyU6ysFhD6dBYQIl1MtNVQNjiYmlqYW17gtw
         B+f0dejcFQ3tKBs045dNEaodUCw+wyyMmSxqcfEzWAe38DsbJ4Y2CGsXPE1iZtAJIH5C
         g/1wxwLPADuSQ6VjI+tl1ZEq0paQUP4rxmdPfsO82ZHMuC78e9rHjpOt5dCcAnETpRoW
         6LAQ==
X-Gm-Message-State: ALoCoQmP4zl5eBNgNtadT8ZGybX+RYHOUSwFMS82GoOlYQvVRcvKNz1pw5p+AfMv8YvKfmVjFNMN
X-Received: by 10.112.92.66 with SMTP id ck2mr32517933lbb.105.1426038873616;
 Tue, 10 Mar 2015 18:54:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.213.18 with HTTP; Tue, 10 Mar 2015 18:54:13 -0700 (PDT)
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 10 Mar 2015 18:54:13 -0700
Message-ID: <CAAswR-7-Si_joxcv_beu7z9FGRbTx=i9itRB_8TsS5=quVvG_A@mail.gmail.com>
Subject: GitHub Syncing Down
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113372b47f88dc0510f98cf5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113372b47f88dc0510f98cf5
Content-Type: text/plain; charset=UTF-8

FYI: https://issues.apache.org/jira/browse/INFRA-9259

--001a113372b47f88dc0510f98cf5--

From dev-return-11952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 04:19:54 2015
Return-Path: <dev-return-11952-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9D4E51797C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 04:19:54 +0000 (UTC)
Received: (qmail 43158 invoked by uid 500); 11 Mar 2015 04:19:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43083 invoked by uid 500); 11 Mar 2015 04:19:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43071 invoked by uid 99); 11 Mar 2015 04:19:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 04:19:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 04:19:27 +0000
Received: by qcvs11 with SMTP id s11so7477868qcv.6
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 21:16:49 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=dTUS5fknpADW03q5mt0mnP7SCvC/p+22Xgcr0KlA5Lg=;
        b=F1KyPSDAyLYsH7ZG/WEDfhgZotLMMZvaDnJw7i/B0mswOs/YJ+458vAxFFepu39q24
         7aN9sGnLS6/7jB2Hs5Rv1cKh/YFd8V2o5KbmHmffolX5+QSbbmZyD9KB6pPblHmROCXK
         Egn/6DDQf95CtbPOZUFDKX/nSSfaBnZDmFnz9UJlIZUmB0PVZiGH3S3EfwMdxspneOLu
         Sp2Ge0JRvCIpb7Z53TbynJoOMeeThVCPgP6eONIXrUWzZC5ZSxVCFtsmUzQSY9mm6Q9c
         uevIUXrXnFeBmVdkOQZA21oH2FzHdCj2acdlSVoK2qdza3E2zbwTUuQdQ6QQ6cAf6CNY
         bCYQ==
X-Gm-Message-State: ALoCoQnJx6ZsvFWozngcnpVHTlWudeh1/x8UZ8eUKR82wG//c21Kh+J4auEcj7QYq4z93ro+9jRa
X-Received: by 10.55.26.104 with SMTP id a101mr49316735qka.81.1426047408954;
 Tue, 10 Mar 2015 21:16:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.109.9 with HTTP; Tue, 10 Mar 2015 21:16:28 -0700 (PDT)
In-Reply-To: <54F9763A.6030503@ugent.be>
References: <54B67039.9070408@ugent.be> <54F9763A.6030503@ugent.be>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 10 Mar 2015 21:16:28 -0700
Message-ID: <CAPh_B=YFZUb+SLYJQ7Z1MYro4yKo9yujDfyh2PBmOUXS_1tMmg@mail.gmail.com>
Subject: Re: SparkSpark-perf terasort WIP branch
To: Ewan Higgs <ewan.higgs@ugent.be>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a114592a63e84e00510fb8918
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114592a63e84e00510fb8918
Content-Type: text/plain; charset=UTF-8

Hi Ewan,

Sorry it took a while for us to reply. I don't know spark-perf that well,
but I think this would be problematic if it works with only a specific
version of Hadoop. Maybe we can take a different approach -- just have a
bunch of tasks using the HDFS client API to read data, and not relying on
input formats?


On Fri, Mar 6, 2015 at 1:41 AM, Ewan Higgs <ewan.higgs@ugent.be> wrote:

> Hi all,
> I never heard from anyone on this and have received emails in private that
> people would like to add terasort to their spark-perf installs so it
> becomes part of their cluster validation checks.
>
> Yours,
> Ewan
>
>
> -------- Forwarded Message --------
> Subject:        SparkSpark-perf terasort WIP branch
> Date:   Wed, 14 Jan 2015 14:33:45 +0100
> From:   Ewan Higgs <ewan.higgs@ugent.be>
> To:     dev@spark.apache.org <dev@spark.apache.org>
>
>
>
> Hi all,
> I'm trying to build the Spark-perf WIP code but there are some errors to
> do with Hadoop APIs. I presume this is because there is some Hadoop
> version set and it's referring to that. But I can't seem to find it.
>
> The errors are as follows:
>
> [info] Compiling 15 Scala sources and 2 Java sources to
> /home/ehiggs/src/spark-perf/spark-tests/target/scala-2.10/classes...
> [error]
> /home/ehiggs/src/spark-perf/spark-tests/src/main/scala/
> spark/perf/terasort/TeraInputFormat.scala:40:
> object task is not a member of package org.apache.hadoop.mapreduce
> [error] import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
> [error]                                    ^
> [error]
> /home/ehiggs/src/spark-perf/spark-tests/src/main/scala/
> spark/perf/terasort/TeraInputFormat.scala:132:
> not found: type TaskAttemptContextImpl
> [error]             val context = new TaskAttemptContextImpl(
> [error]                               ^
> [error]
> /home/ehiggs/src/spark-perf/spark-tests/src/main/scala/
> spark/perf/terasort/TeraScheduler.scala:37:
> object TTConfig is not a member of package
> org.apache.hadoop.mapreduce.server.tasktracker
> [error] import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig
> [error]        ^
> [error]
> /home/ehiggs/src/spark-perf/spark-tests/src/main/scala/
> spark/perf/terasort/TeraScheduler.scala:91:
> not found: value TTConfig
> [error]   var slotsPerHost : Int = conf.getInt(TTConfig.TT_MAP_SLOTS, 4)
> [error]                                        ^
> [error]
> /home/ehiggs/src/spark-perf/spark-tests/src/main/scala/
> spark/perf/terasort/TeraSortAll.scala:7:
> value run is not a member of org.apache.spark.examples.terasort.TeraGen
> [error]     tg.run(Array[String]("10M", "/tmp/terasort_in"))
> [error]        ^
> [error]
> /home/ehiggs/src/spark-perf/spark-tests/src/main/scala/
> spark/perf/terasort/TeraSortAll.scala:9:
> value run is not a member of org.apache.spark.examples.terasort.TeraSort
> [error]     ts.run(Array[String]("/tmp/terasort_in", "/tmp/terasort_out"))
> [error]        ^
> [error] 6 errors found
> [error] (compile:compile) Compilation failed
> [error] Total time: 13 s, completed 05-Jan-2015 12:21:47
>
> I can build the same code if it's in the Spark tree using the following
> command:
> mvn -Dhadoop.version=2.5.0 -DskipTests=true install
>
> Is there a way I can convince spark-perf to build this code with the
> appropriate Hadoop library version? I tried to apply the following to
> spark-tests/project/SparkTestsBuild.scala but it didn't seem to work as
> I expected:
>
> $ git diff project/SparkTestsBuild.scala
> diff --git a/spark-tests/project/SparkTestsBuild.scala
> b/spark-tests/project/SparkTestsBuild.scala
> index 4116326..4ed5f0c 100644
> --- a/spark-tests/project/SparkTestsBuild.scala
> +++ b/spark-tests/project/SparkTestsBuild.scala
> @@ -16,7 +16,9 @@ object SparkTestsBuild extends Build {
>           "org.scalatest" %% "scalatest" % "2.2.1" % "test",
>           "com.google.guava" % "guava" % "14.0.1",
>           "org.apache.spark" %% "spark-core" % "1.0.0" % "provided",
> -        "org.json4s" %% "json4s-native" % "3.2.9"
> +        "org.json4s" %% "json4s-native" % "3.2.9",
> +        "org.apache.hadoop" % "hadoop-common" % "2.5.0",
> +        "org.apache.hadoop" % "hadoop-mapreduce" % "2.5.0"
>         ),
>         test in assembly := {},
>         outputPath in assembly :=
> file("target/spark-perf-tests-assembly.jar"),
> @@ -36,4 +38,4 @@ object SparkTestsBuild extends Build {
>           case _ => MergeStrategy.first
>         }
>       ))
> -}
> \ No newline at end of file
> +}
>
>
> Yours,
> Ewan
>
>
>
>

--001a114592a63e84e00510fb8918--

From dev-return-11953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 05:25:47 2015
Return-Path: <dev-return-11953-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D00A417AD0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 05:25:47 +0000 (UTC)
Received: (qmail 15319 invoked by uid 500); 11 Mar 2015 05:25:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15232 invoked by uid 500); 11 Mar 2015 05:25:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15220 invoked by uid 99); 11 Mar 2015 05:25:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 05:25:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 05:25:21 +0000
Received: by obcva2 with SMTP id va2so6633174obc.3
        for <dev@spark.apache.org>; Tue, 10 Mar 2015 22:24:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type
         :content-transfer-encoding;
        bh=OveCVF0Wpm/B0po0k3uZToKoPq72I0/lbycKQZJOiFY=;
        b=001CCKKXGf4j06dXtVQOCx09eMYbyYvu1TQ5azeYSL0xohpFXyfYCr7S4XeenCX5qA
         S9GbfYc2D4KOgoRj3SNxhPXX7CImPimM/zcwinbpu/BuOnzmUSXZY6Ia3i3zJLSamblo
         8nNCbZYCcbApCBaOS4GeoVnkN96DiW9vTNiGoLxS3jw9wNCirw7DHyBi/J2ct08JCz3V
         O/xle4+Dc/99NCWLvNwYd3XHN/Xu/ms6B+5SZ2exo8/HKBi3C1EPU+4yZQ2NoV45FRoC
         T8vvhCQ76UsYrz4pqTJJeKtDHKyeNFFZVbDi3hBrRF2D2ueb8ng7fVhhEHcpYGF0EgMI
         Q0Aw==
MIME-Version: 1.0
X-Received: by 10.60.155.225 with SMTP id vz1mr839710oeb.52.1426051474888;
 Tue, 10 Mar 2015 22:24:34 -0700 (PDT)
Received: by 10.202.226.137 with HTTP; Tue, 10 Mar 2015 22:24:34 -0700 (PDT)
Date: Tue, 10 Mar 2015 22:24:34 -0700
Message-ID: <CABPQxsvGdhMTH+Mu7vD+FBLyr_kmuX_0bvikB=RQFs-6J8OQ2A@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.3.0 (RC3)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=GB2312
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

This vote passes with 13 +1 votes (6 binding) and no 0 or -1 votes:

+1 (13):
Patrick Wendell*
Marcelo Vanzin
Krishna Sankar
Sean Owen*
Matei Zaharia*
Sandy Ryza
Tom Graves*
Sean McNamara*
Denny Lee
Kostas Sakellis
Joseph Bradley*
Corey Nolet
GuoQiang Li

0:
-1:

I will finalize the release notes and packaging and will post the
release in the next two days.

- Patrick

On Mon, Mar 9, 2015 at 11:51 PM, GuoQiang Li <witgo@qq.com> wrote:
> I'm sorry, this is my mistake. :)
>
>
> ------------------ =D4=AD=CA=BC=D3=CA=BC=FE ------------------
> =B7=A2=BC=FE=C8=CB: "Patrick Wendell"<pwendell@gmail.com>;
> =B7=A2=CB=CD=CA=B1=BC=E4: 2015=C4=EA3=D4=C210=C8=D5(=D0=C7=C6=DA=B6=FE) =
=CF=C2=CE=E72:20
> =CA=D5=BC=FE=C8=CB: "GuoQiang Li"<witgo@qq.com>;
> =D6=F7=CC=E2: Re: [VOTE] Release Apache Spark 1.3.0 (RC3)
>
> Thanks! But please e-mail the dev list and not just me personally :)
>
> On Mon, Mar 9, 2015 at 11:08 PM, GuoQiang Li <witgo@qq.com> wrote:
>> +1 (non-binding)
>>
>> Test on Mac OS X 10.10.2 and CentOS 6.5
>>
>>
>> ------------------ Original ------------------
>> From:  "Patrick Wendell";<pwendell@gmail.com>;
>> Date:  Fri, Mar 6, 2015 10:52 AM
>> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>> Subject:  [VOTE] Release Apache Spark 1.3.0 (RC3)
>>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.3.0!
>>
>> The tag to be voted on is v1.3.0-rc2 (commit 4aaf48d4):
>>
>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D4aa=
f48d46d13129f0f9bdafd771dd80fe568a7dc
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc3/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> Staging repositories for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1078
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.3.0-rc3-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.3.0!
>>
>> The vote is open until Monday, March 09, at 02:52 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.3.0
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> =3D=3D How does this compare to RC2 =3D=3D
>> This release includes the following bug fixes:
>>
>> https://issues.apache.org/jira/browse/SPARK-6144
>> https://issues.apache.org/jira/browse/SPARK-6171
>> https://issues.apache.org/jira/browse/SPARK-5143
>> https://issues.apache.org/jira/browse/SPARK-6182
>> https://issues.apache.org/jira/browse/SPARK-6175
>>
>> =3D=3D How can I help test this release? =3D=3D
>> If you are a Spark user, you can help us test this release by
>> taking a Spark 1.2 workload and running on this release candidate,
>> then reporting any regressions.
>>
>> If you are happy with this release based on your own testing, give a +1
>> vote.
>>
>> =3D=3D What justifies a -1 vote for this release? =3D=3D
>> This vote is happening towards the end of the 1.3 QA period,
>> so -1 votes should only occur for significant regressions from 1.2.1.
>> Bugs already present in 1.2.X, minor regressions, or bugs related
>> to new features will not block this release.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 06:36:59 2015
Return-Path: <dev-return-11954-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D571C17CEB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 06:36:59 +0000 (UTC)
Received: (qmail 31666 invoked by uid 500); 11 Mar 2015 06:36:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31534 invoked by uid 500); 11 Mar 2015 06:36:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30716 invoked by uid 99); 11 Mar 2015 06:36:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 06:36:54 +0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [203.81.22.165] (HELO mail1.qilinsoft.com) (203.81.22.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 06:36:49 +0000
X-MimeOLE: Produced By Microsoft Exchange V6.5
Content-class: urn:content-classes:message
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----_=_NextPart_001_01D05BC5.C4DFE7BC"
Subject: RE: [SparkSQL] Reuse HiveContext to different Hive warehouse?
Date: Wed, 11 Mar 2015 14:36:05 +0800
Message-ID: <2EB23AF5EEEA2140946B8F292EB2EB9F1AD8FD@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
In-Reply-To: <80833ADD533E324CA05C160E41B636610284D5B9@shsmsx102.ccr.corp.intel.com>
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
Thread-Topic: [SparkSQL] Reuse HiveContext to different Hive warehouse?
Thread-Index: AdBbHjiP/Y9s/nWcT+SQW6TSLqlZdgAHLlrAACGe0tA=
From: "Haopu Wang" <HWang@qilinsoft.com>
To: "Cheng, Hao" <hao.cheng@intel.com>,
	"user" <user@spark.apache.org>,
	<dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

------_=_NextPart_001_01D05BC5.C4DFE7BC
Content-Type: text/plain;
	charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hao, thanks for the response.

=20

For Q1, in my case, I have a tool on SparkShell which serves multiple
users where they can use different Hive installation. I take a look at
the code of HiveContext. It looks like I cannot do that today because
"catalog" field cannot be changed after initialize.

=20

  /* A catalyst metadata catalog that points to the Hive Metastore. */

  @transient

  override protected[sql] lazy val catalog =3D new
HiveMetastoreCatalog(this) with OverrideCatalog

=20

For Q2, I check HDFS and it is running as a cluster. I can run the DDL
from spark shell with HiveContext as well. To reproduce the exception, I
just run below script. It happens in the last step.

=20

15/03/11 14:24:48 INFO SparkILoop: Created sql context (with Hive
support)..

SQL context available as sqlContext.

scala> sqlContext.sql("SET
hive.metastore.warehouse.dir=3Dhdfs://server:8020/space/warehouse")

scala> sqlContext.sql("CREATE TABLE IF NOT EXISTS src(key INT, value
STRING)")

scala> sqlContext.sql("LOAD DATA LOCAL INPATH
'examples/src/main/resources/kv1.txt' INTO TABLE src")

scala> var output =3D sqlContext.sql("SELECT key,value FROM src")

scala> output.saveAsTable("outputtable")

=20

________________________________

From: Cheng, Hao [mailto:hao.cheng@intel.com]=20
Sent: Wednesday, March 11, 2015 8:25 AM
To: Haopu Wang; user; dev@spark.apache.org
Subject: RE: [SparkSQL] Reuse HiveContext to different Hive warehouse?

=20

I am not so sure if Hive supports change the metastore after
initialized, I guess not. Spark SQL totally rely on Hive Metastore in
HiveContext, probably that's why it doesn't work as expected for Q1.

=20

BTW, in most of cases, people configure the metastore settings in
hive-site.xml, and will not change that since then, is there any reason
that you want to change that in runtime?

=20

For Q2, probably something wrong in configuration, seems the HDFS run
into the pseudo/single node mode, can you double check that? Or can you
run the DDL (like create a table) from the spark shell with HiveContext?


=20

From: Haopu Wang [mailto:HWang@qilinsoft.com]=20
Sent: Tuesday, March 10, 2015 6:38 PM
To: user; dev@spark.apache.org
Subject: [SparkSQL] Reuse HiveContext to different Hive warehouse?

=20

I'm using Spark 1.3.0 RC3 build with Hive support.

=20

In Spark Shell, I want to reuse the HiveContext instance to different
warehouse locations. Below are the steps for my test (Assume I have
loaded a file into table "src").

=20

=3D=3D=3D=3D=3D=3D

15/03/10 18:22:59 INFO SparkILoop: Created sql context (with Hive
support)..

SQL context available as sqlContext.

scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w")

scala> sqlContext.sql("SELECT * from src").saveAsTable("table1")

scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w2")

scala> sqlContext.sql("SELECT * from src").saveAsTable("table2")

=3D=3D=3D=3D=3D=3D

After these steps, the tables are stored in "/test/w" only. I expect
"table2" to be stored in "/test/w2" folder.

=20

Another question is: if I set "hive.metastore.warehouse.dir" to a HDFS
folder, I cannot use saveAsTable()? Is this by design? Exception stack
trace is below:

=3D=3D=3D=3D=3D=3D

15/03/10 18:35:28 INFO BlockManagerMaster: Updated info of block
broadcast_0_piece0

15/03/10 18:35:28 INFO SparkContext: Created broadcast 0 from broadcast
at TableReader.scala:74

java.lang.IllegalArgumentException: Wrong FS:
hdfs://server:8020/space/warehouse/table2, expected: file:///
<file:///\\>=20

        at
org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)

        at
org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:463)

        at
org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.jav
a:118)

        at
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.a
pply(newParquet.scala:252)

        at
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.a
pply(newParquet.scala:251)

        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sc
ala:244)

        at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sc
ala:244)

        at scala.collection.immutable.List.foreach(List.scala:318)

        at
scala.collection.TraversableLike$class.map(TraversableLike.scala:244)

        at
scala.collection.AbstractTraversable.map(Traversable.scala:105)

        at
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newP
arquet.scala:251)

        at
org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.scala:37
0)

        at
org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.sca
la:96)

        at
org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.sca
la:125)

        at
org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:308)

        at
org.apache.spark.sql.hive.execution.CreateMetastoreDataSourceAsSelect.ru
n(commands.scala:217)

        at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompu
te(commands.scala:55)

        at
org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands
.scala:55)

        at
org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65
)

        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLConte
xt.scala:1088)

        at
org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:10
88)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:1048)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:998)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:964)

        at
org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:942)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)

        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:27)

        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29)

        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:31)

        at $iwC$$iwC$$iwC.<init>(<console>:33)

        at $iwC$$iwC.<init>(<console>:35)

        at $iwC.<init>(<console>:37)

        at <init>(<console>:39)

=20

Thank you very much!

=20


------_=_NextPart_001_01D05BC5.C4DFE7BC--

From dev-return-11955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 13:59:45 2015
Return-Path: <dev-return-11955-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D8A3017379
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 13:59:45 +0000 (UTC)
Received: (qmail 70035 invoked by uid 500); 11 Mar 2015 13:59:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69965 invoked by uid 500); 11 Mar 2015 13:59:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69757 invoked by uid 99); 11 Mar 2015 13:59:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 13:59:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 13:59:15 +0000
Received: by obbnt9 with SMTP id nt9so8927892obb.9
        for <dev@spark.apache.org>; Wed, 11 Mar 2015 06:58:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=yLP16JBeyur8ySncP5hwMxvmExfEaTJhsKKnUHDD6ck=;
        b=Xdy7sypjUAQfVOxYnCqmnnAaOr23Nh2VrhienUfxdzQpfxn8ScNx5BIx/KTDw7CDne
         n2rCfyq6exCX4j2O9XyhvJgP2udVkzIVkxFTGQdNOObg9a39NPmc4IywYT/3ZC9dW4Ct
         jdvGYCuY3FBAHARlHlaFfnV5lOf3LpBi3tehksnqdyJ52X+LAZ83Tpt73J65LxtEm/j/
         qSfQW5qW92K/KTX2LgRMY9U0WLKiCvIPdW3WwLGQAuILCs7YNzHA3NZxGTAI7Qhf39mB
         ryjpoqpeULTISqypOEJXdwyvjsvjm/Xq8xyJux2iYBbb76R5V5/70w/srOwb1KqL/Sen
         mgEQ==
MIME-Version: 1.0
X-Received: by 10.202.183.138 with SMTP id h132mr29231279oif.132.1426082308291;
 Wed, 11 Mar 2015 06:58:28 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Wed, 11 Mar 2015 06:58:27 -0700 (PDT)
Date: Wed, 11 Mar 2015 14:58:27 +0100
Message-ID: <CAO=evYdA-MMvK4eXS_jhUCHq8=EnkvDnTOFm3S=VnvJmTshepg@mail.gmail.com>
Subject: Spark Streaming - received block allocation to batch
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113ce1ee68046d051103a900
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ce1ee68046d051103a900
Content-Type: text/plain; charset=UTF-8

I'm trying to understand the block allocation mechanism Spark uses to
generate batch jobs and a JobSet.

The JobGenerator.generateJobs tries to allocate received blocks to batch,
effectively in ReceivedBlockTracker.allocateBlocksToBatch creates
a streamIdToBlocks, where steam ID's (Int) mapped to Seq[ReceivedBlockInfo]
using getReceivedBlockQueue. This is where it gets tricky for me.

getReceivedBlockQueue of class ReceivedBlockTracker reads
streamIdToUnallocatedBlockQueues
that should be populated with ReceivedBlockQueues? Who inserts these
ReceivedBlockQueues into streamIdToUnallocatedBlockQueues and where does it
get written? I've found only usages of 'effectively' value read.

At a point streamIdToBlocks get packed into a case class
of AllocatedBlocks. Why is it necessary?

Also, at JobGenerator.generateJobs the line where receivedBlockInfos created,
shouldn't it be empty, because streamIdToUnallocatedBlockQueues never got
written to? Where do I miss the point? How does the JobGenerator.generateJobs
able to retrieve the received block infos?

Thanks,

ZZ

--001a113ce1ee68046d051103a900--

From dev-return-11956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 14:39:46 2015
Return-Path: <dev-return-11956-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF2FF174DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 14:39:45 +0000 (UTC)
Received: (qmail 64648 invoked by uid 500); 11 Mar 2015 14:39:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64578 invoked by uid 500); 11 Mar 2015 14:39:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64566 invoked by uid 99); 11 Mar 2015 14:39:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 14:39:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 14:39:19 +0000
Received: by igqa13 with SMTP id a13so40422462igq.0
        for <dev@spark.apache.org>; Wed, 11 Mar 2015 07:38:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=CebATqTyaPffb9ge+wHWpXnzahkY2x/GLk/In9fv6oE=;
        b=Z95dCkTaMzXryx8cCub5Q3DG2wcXpk6PAevXaQsXgZu5Dgo3DKS7DW00xMz5hDZzhr
         +3lCdpHu7xxzzGrgZDWDEI55BF0muTWWfhqVKMC4tU9uMRz0PqnKLXlQdlyJGQKS0Lw5
         gvJzjIhlP2pAaRgPfcWvRsr+ityrz8DBhcFLdK3IwFu1gNUEqnP2+8fOcqcOaVD51G6d
         n/PZS2a4fLWi25Jb0CttsVQOYDpIm5Z7fFYJhzp+RrGgY0lN0Gr7fy2k2gxV/0DzQZqu
         jMXcfo/Yk5NVHdUCSMqw4E4VZTVrRUc00FE5ML3xUOrLaqvwHHlYdzgLvzEvdKS/g9OF
         sJHg==
MIME-Version: 1.0
X-Received: by 10.107.170.33 with SMTP id t33mr50900123ioe.7.1426084712704;
 Wed, 11 Mar 2015 07:38:32 -0700 (PDT)
Received: by 10.36.53.148 with HTTP; Wed, 11 Mar 2015 07:38:32 -0700 (PDT)
In-Reply-To: <CAAswR-7-Si_joxcv_beu7z9FGRbTx=i9itRB_8TsS5=quVvG_A@mail.gmail.com>
References: <CAAswR-7-Si_joxcv_beu7z9FGRbTx=i9itRB_8TsS5=quVvG_A@mail.gmail.com>
Date: Wed, 11 Mar 2015 07:38:32 -0700
Message-ID: <CALte62wu_7V39nT5Y+wtkTvMvtZKCYyCZRjmoJP+235AfxsBcg@mail.gmail.com>
Subject: Re: GitHub Syncing Down
From: Ted Yu <yuzhihong@gmail.com>
To: Michael Armbrust <michael@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1142d85eb86fc0051104380f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1142d85eb86fc0051104380f
Content-Type: text/plain; charset=UTF-8

Looks like github is functioning again (I no longer encounter this problem
when pushing to hbase repo).

Do you want to give it a try ?

Cheers

On Tue, Mar 10, 2015 at 6:54 PM, Michael Armbrust <michael@databricks.com>
wrote:

> FYI: https://issues.apache.org/jira/browse/INFRA-9259
>

--001a1142d85eb86fc0051104380f--

From dev-return-11957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 14:45:07 2015
Return-Path: <dev-return-11957-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2BA8D17507
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 14:45:07 +0000 (UTC)
Received: (qmail 81495 invoked by uid 500); 11 Mar 2015 14:45:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81414 invoked by uid 500); 11 Mar 2015 14:45:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81403 invoked by uid 99); 11 Mar 2015 14:45:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 14:45:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 14:45:01 +0000
Received: by widem10 with SMTP id em10so24928912wid.2
        for <dev@spark.apache.org>; Wed, 11 Mar 2015 07:43:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=akIWoVw37Frqq2DGk2Gm9t3MA+YAnNuYyxUyfaDUfwc=;
        b=ISSdUEbOeb9bPhvIas76K5njU6plZNCq4+ePpttL/AdcaCzucJQ+WWR3Y+6zMT+yMt
         I7gEkFtu5pLwzjhadckV5eRAnqXM7ToX+E3QrwCpoWC7KKB2j6Cp2F4I7qCCdDYa/dib
         Op8hqL7veO+y/6zzVCVX3ALz5PdGJenJYKEtEphCp1P4oQyddSdIhUzNoURVShBEJZJa
         XQzjbv0vA2d1Hscng0cPxEEqn/4FLXLw+VPXC9g8YTefaQZt1dqqoN59bi4JnSSKz/wE
         z1x9FZisHvNynrWVR7biok1Ih28XLQV5909jMzZWsCDWy2dx1pJUMjBrf2doZEgk0fkk
         8OPg==
X-Gm-Message-State: ALoCoQlHUDRg6g+CU4Eh/GS1BMBPZefZ8BsDTGyKCL/atRpLX4wkH7fSAOfxWWWm+vMrih9YGdY+
X-Received: by 10.180.210.231 with SMTP id mx7mr47611093wic.31.1426085035556;
 Wed, 11 Mar 2015 07:43:55 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Wed, 11 Mar 2015 07:43:35 -0700 (PDT)
In-Reply-To: <CALte62wu_7V39nT5Y+wtkTvMvtZKCYyCZRjmoJP+235AfxsBcg@mail.gmail.com>
References: <CAAswR-7-Si_joxcv_beu7z9FGRbTx=i9itRB_8TsS5=quVvG_A@mail.gmail.com>
 <CALte62wu_7V39nT5Y+wtkTvMvtZKCYyCZRjmoJP+235AfxsBcg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 11 Mar 2015 14:43:35 +0000
Message-ID: <CAMAsSdK=HDGwvos_ShtOyqk5S=4VPm6GG=6yLPyZBBNfAM_iAQ@mail.gmail.com>
Subject: Re: GitHub Syncing Down
To: Ted Yu <yuzhihong@gmail.com>
Cc: Michael Armbrust <michael@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

(I have been able to push over the last few hours and see the commits in github)

On Wed, Mar 11, 2015 at 2:38 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> Looks like github is functioning again (I no longer encounter this problem
> when pushing to hbase repo).
>
> Do you want to give it a try ?
>
> Cheers
>
> On Tue, Mar 10, 2015 at 6:54 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
>> FYI: https://issues.apache.org/jira/browse/INFRA-9259
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 15:40:38 2015
Return-Path: <dev-return-11958-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20FC21782C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 15:40:38 +0000 (UTC)
Received: (qmail 70775 invoked by uid 500); 11 Mar 2015 15:40:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70676 invoked by uid 500); 11 Mar 2015 15:40:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70357 invoked by uid 99); 11 Mar 2015 15:40:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 15:40:31 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: unknown (nike.apache.org: error in processing during lookup of lior.c@taboola.com)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 15:40:05 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 8E7C116ECF31
	for <dev@spark.apache.org>; Wed, 11 Mar 2015 08:40:10 -0700 (PDT)
Date: Wed, 11 Mar 2015 08:40:03 -0700 (MST)
From: "lior.c" <lior.c@taboola.com>
To: dev@spark.apache.org
Message-ID: <1426088403633-11009.post@n3.nabble.com>
Subject: Using Log4j2 in spark executors
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I'd like to allow using log4j2 in executor code.
As spark contains dependencies to log4j 1.2, I would like to support spark
build with log4j2 instead of log4j 1.2. 
To accomplish that, I suggest creating a new profile for log4j2 in
spark-parent.
The default profile (log4j12), would include dependencies for log4j and
slf4j-log4j12 with default scope (I would remove the dependencies from sub
modules of spark-parent).
The log4j2 profile would instead include same dependencies with scope
provided (to avoid the shading plugin add those jars as a result of
transitive dependencies from other jars that depend on log4j 12), and in
addition would include the dependencies required to log4j2.

Already tested it and it seems to work properly, and I would like to offer
it as a pull request.
What do you think of this solution?

Lior



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Using-Log4j2-in-spark-executors-tp11009.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 18:26:47 2015
Return-Path: <dev-return-11959-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6980B172C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 18:26:47 +0000 (UTC)
Received: (qmail 6744 invoked by uid 500); 11 Mar 2015 18:26:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6666 invoked by uid 500); 11 Mar 2015 18:26:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6655 invoked by uid 99); 11 Mar 2015 18:26:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 18:26:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 18:26:40 +0000
Received: by labgq15 with SMTP id gq15so10759059lab.1
        for <dev@spark.apache.org>; Wed, 11 Mar 2015 11:24:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=3nDzuyruaVQsYt6MWJflX6pYMAOOloA7aSqpypkvyi8=;
        b=aVaClcQV9Na5hGl3gEF2IxfC5MhksxjPrbq6AtPRxN+rdEtHFRriGDxNmLqBnRQ89l
         eTKOrgxAyuPcRVOjW/QBNebIgTD9CisTg94bNNEhcUvdrC0oM4H8UnVLoeiAfqU+eQm2
         xOwu8/jgxMblheVWeCjaA2Tv3AA2Hhuhr9+DoqQc10MMNA6SQAu2YyYfbyqvlojWSnMf
         08Bf7TuzOwfZnlY/sbPRsYJWEsohI++nu7reb+vn8Qev4hlU0btN9GYMvJmKJw82KbkF
         Go+kWc238nyglg2FjJF6eMRtPi3sr2qLT/jv/utFA69DCqoBnJLXF6R9X4I+yBIxy7TG
         Ljuw==
X-Gm-Message-State: ALoCoQkk770GkZVIegdTfk3TpiBqQG7BIefcf5Yjf8FUVCwHZBcR8nGVxgnPwrPOkXbSLD4WvgPH
X-Received: by 10.152.29.102 with SMTP id j6mr36457340lah.12.1426098268148;
 Wed, 11 Mar 2015 11:24:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.213.18 with HTTP; Wed, 11 Mar 2015 11:24:07 -0700 (PDT)
In-Reply-To: <2EB23AF5EEEA2140946B8F292EB2EB9F1AD8FD@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
References: <80833ADD533E324CA05C160E41B636610284D5B9@shsmsx102.ccr.corp.intel.com>
 <2EB23AF5EEEA2140946B8F292EB2EB9F1AD8FD@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
From: Michael Armbrust <michael@databricks.com>
Date: Wed, 11 Mar 2015 11:24:07 -0700
Message-ID: <CAAswR-4WVDXsLSqYxFOdGELJF_vz3jvK5FFrpA60Nr1JMevjNg@mail.gmail.com>
Subject: Re: [SparkSQL] Reuse HiveContext to different Hive warehouse?
To: Haopu Wang <HWang@qilinsoft.com>
Cc: "Cheng, Hao" <hao.cheng@intel.com>, user <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158c098b02ac20511076037
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c098b02ac20511076037
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That val is not really your problem.  In general, there is a lot of global
state throughout the hive codebase that make it unsafe to try and connect
to more than one hive installation from the same JVM.

On Tue, Mar 10, 2015 at 11:36 PM, Haopu Wang <HWang@qilinsoft.com> wrote:

>  Hao, thanks for the response.
>
>
>
> For Q1, in my case, I have a tool on SparkShell which serves multiple
> users where they can use different Hive installation. I take a look at th=
e
> code of HiveContext. It looks like I cannot do that today because "catalo=
g"
> field cannot be changed after initialize.
>
>
>
>   /* A catalyst metadata catalog that points to the Hive Metastore. */
>
>   @transient
>
>   *override* *protected*[sql] *lazy* *val* catalog =3D *new*
> HiveMetastoreCatalog(*this*) *with* OverrideCatalog
>
>
>
> For Q2, I check HDFS and it is running as a cluster. I can run the DDL
> from spark shell with HiveContext as well. To reproduce the exception, I
> just run below script. It happens in the last step.
>
>
>
> 15/03/11 14:24:48 INFO SparkILoop: Created sql context (with Hive
> support)..
>
> SQL context available as sqlContext.
>
> scala> sqlContext.sql("SET
> hive.metastore.warehouse.dir=3Dhdfs://server:8020/space/warehouse")
>
> scala> sqlContext.sql("CREATE TABLE IF NOT EXISTS src(key INT, value
> STRING)")
>
> scala> sqlContext.sql("LOAD DATA LOCAL INPATH
> 'examples/src/main/resources/kv1.txt' INTO TABLE src")
>
> scala> var output =3D sqlContext.sql("SELECT key,value FROM src")
>
> scala> output.saveAsTable("outputtable")
>
>
>  ------------------------------
>
> *From:* Cheng, Hao [mailto:hao.cheng@intel.com]
> *Sent:* Wednesday, March 11, 2015 8:25 AM
> *To:* Haopu Wang; user; dev@spark.apache.org
> *Subject:* RE: [SparkSQL] Reuse HiveContext to different Hive warehouse?
>
>
>
> I am not so sure if Hive supports change the metastore after initialized,
> I guess not. Spark SQL totally rely on Hive Metastore in HiveContext,
> probably that=E2=80=99s why it doesn=E2=80=99t work as expected for Q1.
>
>
>
> BTW, in most of cases, people configure the metastore settings in
> hive-site.xml, and will not change that since then, is there any reason
> that you want to change that in runtime?
>
>
>
> For Q2, probably something wrong in configuration, seems the HDFS run int=
o
> the pseudo/single node mode, can you double check that? Or can you run th=
e
> DDL (like create a table) from the spark shell with HiveContext?
>
>
>
> *From:* Haopu Wang [mailto:HWang@qilinsoft.com]
> *Sent:* Tuesday, March 10, 2015 6:38 PM
> *To:* user; dev@spark.apache.org
> *Subject:* [SparkSQL] Reuse HiveContext to different Hive warehouse?
>
>
>
> I'm using Spark 1.3.0 RC3 build with Hive support.
>
>
>
> In Spark Shell, I want to reuse the HiveContext instance to different
> warehouse locations. Below are the steps for my test (Assume I have loade=
d
> a file into table "src").
>
>
>
> =3D=3D=3D=3D=3D=3D
>
> 15/03/10 18:22:59 INFO SparkILoop: Created sql context (with Hive
> support)..
>
> SQL context available as sqlContext.
>
> scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w")
>
> scala> sqlContext.sql("SELECT * from src").saveAsTable("table1")
>
> scala> sqlContext.sql("SET hive.metastore.warehouse.dir=3D/test/w2")
>
> scala> sqlContext.sql("SELECT * from src").saveAsTable("table2")
>
> =3D=3D=3D=3D=3D=3D
>
> After these steps, the tables are stored in "/test/w" only. I expect
> "table2" to be stored in "/test/w2" folder.
>
>
>
> Another question is: if I set "hive.metastore.warehouse.dir" to a HDFS
> folder, I cannot use saveAsTable()? Is this by design? Exception stack
> trace is below:
>
> =3D=3D=3D=3D=3D=3D
>
> 15/03/10 18:35:28 INFO BlockManagerMaster: Updated info of block
> broadcast_0_piece0
>
> 15/03/10 18:35:28 INFO SparkContext: Created broadcast 0 from broadcast a=
t
> TableReader.scala:74
>
> java.lang.IllegalArgumentException: Wrong FS:
> hdfs://server:8020/space/warehouse/table2, expected: file:///
>
>         at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:643)
>
>         at
> org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:463)
>
>         at
> org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java=
:118)
>
>         at
> org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.ap=
ply(newParquet.scala:252)
>
>         at
> org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.ap=
ply(newParquet.scala:251)
>
>         at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
>         at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.sca=
la:244)
>
>         at scala.collection.immutable.List.foreach(List.scala:318)
>
>         at
> scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>
>         at scala.collection.AbstractTraversable.map(Traversable.scala:105=
)
>
>         at
> org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newPa=
rquet.scala:251)
>
>         at
> org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.scala:370=
)
>
>         at
> org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.scal=
a:96)
>
>         at
> org.apache.spark.sql.parquet.DefaultSource.createRelation(newParquet.scal=
a:125)
>
>         at
> org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:308)
>
>         at
> org.apache.spark.sql.hive.execution.CreateMetastoreDataSourceAsSelect.run=
(commands.scala:217)
>
>         at
> org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycomput=
e(commands.scala:55)
>
>         at
> org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.=
scala:55)
>
>         at
> org.apache.spark.sql.execution.ExecutedCommand.execute(commands.scala:65)
>
>         at
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContex=
t.scala:1088)
>
>         at
> org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:108=
8)
>
>         at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:104=
8)
>
>         at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:998=
)
>
>         at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:964=
)
>
>         at org.apache.spark.sql.DataFrame.saveAsTable(DataFrame.scala:942=
)
>
>         at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:20)
>
>         at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:25)
>
>         at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:27)
>
>         at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:29)
>
>         at $iwC$$iwC$$iwC$$iwC.<init>(<console>:31)
>
>         at $iwC$$iwC$$iwC.<init>(<console>:33)
>
>         at $iwC$$iwC.<init>(<console>:35)
>
>         at $iwC.<init>(<console>:37)
>
>         at <init>(<console>:39)
>
>
>
> Thank you very much!
>
>
>

--089e0158c098b02ac20511076037--

From dev-return-11960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 19:57:07 2015
Return-Path: <dev-return-11960-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2437D17715
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 19:57:07 +0000 (UTC)
Received: (qmail 48874 invoked by uid 500); 11 Mar 2015 19:57:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48797 invoked by uid 500); 11 Mar 2015 19:57:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48784 invoked by uid 99); 11 Mar 2015 19:57:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 19:57:05 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 19:57:01 +0000
Received: by wiwl15 with SMTP id l15so42027061wiw.4
        for <dev@spark.apache.org>; Wed, 11 Mar 2015 12:56:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=l5ICTGZYxVGFcGqQMu+btlGl2u8Cl6gCqp7gUCkDmtg=;
        b=AeecfoRf8y2NIwGaxT4/lCjgn10UPD7+pLyqS8ZQ1FNYphpLiWv/6NriqMnlWCqlAV
         q2odsODPQjlrYUgQSEFiD2sK/+db0e6tsux5SHM/aceH7vQueSz/J+3hIHG/DWU6x1U/
         F+mxvjqw7M/UC4Gt2hXFyE50FMWl4WKUhrTbwHtLMZY1BLpco5pucDP0aTtnTQPu75YZ
         oUVTbQQ3CNhTo13WX5cfuRv8J5gjAmaPXSQl25VF08wI7cbqTchWb5sbDdXNomeo5aS/
         KJbdogf/Gab1pnj8RHjw3MqO/LpOaZlVaMcexchsK+b0t9kLZggvmuo/9yN5fAU1GBc8
         YrIQ==
MIME-Version: 1.0
X-Received: by 10.194.61.51 with SMTP id m19mr81592342wjr.39.1426103800456;
 Wed, 11 Mar 2015 12:56:40 -0700 (PDT)
Received: by 10.194.43.162 with HTTP; Wed, 11 Mar 2015 12:56:40 -0700 (PDT)
In-Reply-To: <CABPQxsthhRTkypYyOtdrZTT1SwX-hpD+ZAtV7vtOLdRbtAhdQA@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
	<CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
	<CA+3qhFQGGQnLfnFy8iXmEK_uwgA0HJJ=Nh=1fYrOTtnYJTDQ+A@mail.gmail.com>
	<CANGvG8pXqPCFs4dhEJVKkwtfmPFVYkgaaqb0SaBMN-33otVUYQ@mail.gmail.com>
	<CABPQxsthhRTkypYyOtdrZTT1SwX-hpD+ZAtV7vtOLdRbtAhdQA@mail.gmail.com>
Date: Wed, 11 Mar 2015 14:56:40 -0500
Message-ID: <CADtDQQ+oQ-_WR8dteYPKQ-o0Vp_vH0O9ZD=on1RUh20zri59_g@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: RJ Nowling <rnowling@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Aaron Davidson <ilikerps@gmail.com>, Imran Rashid <irashid@cloudera.com>, 
	Mridul Muralidharan <mridul@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b66fa5f706e10051108aa87
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b66fa5f706e10051108aa87
Content-Type: text/plain; charset=UTF-8

How do these proposals affect PySpark?  I think compatibility with PySpark
through Py4J should be considered.

On Mon, Mar 9, 2015 at 8:39 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Does this matter for our own internal types in Spark? I don't think
> any of these types are designed to be used in RDD records, for
> instance.
>
> On Mon, Mar 9, 2015 at 6:25 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> > Perhaps the problem with Java enums that was brought up was actually that
> > their hashCode is not stable across JVMs, as it depends on the memory
> > location of the enum itself.
> >
> > On Mon, Mar 9, 2015 at 6:15 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
> >
> >> Can you expand on the serde issues w/ java enum's at all?  I haven't
> heard
> >> of any problems specific to enums.  The java object serialization rules
> >> seem very clear and it doesn't seem like different jvms should have a
> >> choice on what they do:
> >>
> >>
> >>
> http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serial-arch.html#6469
> >>
> >> (in a nutshell, serialization must use enum.name())
> >>
> >> of course there are plenty of ways the user could screw this up(eg.
> rename
> >> the enums, or change their meaning, or remove them).  But then again,
> all
> >> of java serialization has issues w/ serialization the user has to be
> aware
> >> of.  Eg., if we go with case objects, than java serialization blows up
> if
> >> you add another helper method, even if that helper method is completely
> >> compatible.
> >>
> >> Some prior debate in the scala community:
> >>
> >>
> https://groups.google.com/d/msg/scala-internals/8RWkccSRBxQ/AN5F_ZbdKIsJ
> >>
> >> SO post on which version to use in scala:
> >>
> >>
> >>
> http://stackoverflow.com/questions/1321745/how-to-model-type-safe-enum-types
> >>
> >> SO post about the macro-craziness people try to add to scala to make
> them
> >> almost as good as a simple java enum:
> >> (NB: the accepted answer doesn't actually work in all cases ...)
> >>
> >>
> >>
> http://stackoverflow.com/questions/20089920/custom-scala-enum-most-elegant-version-searched
> >>
> >> Another proposal to add better enums built into scala ... but seems to
> be
> >> dormant:
> >>
> >> https://groups.google.com/forum/#!topic/scala-sips/Bf82LxK02Kk
> >>
> >>
> >>
> >> On Thu, Mar 5, 2015 at 10:49 PM, Mridul Muralidharan <mridul@gmail.com>
> >> wrote:
> >>
> >> >   I have a strong dislike for java enum's due to the fact that they
> >> > are not stable across JVM's - if it undergoes serde, you end up with
> >> > unpredictable results at times [1].
> >> > One of the reasons why we prevent enum's from being key : though it is
> >> > highly possible users might depend on it internally and shoot
> >> > themselves in the foot.
> >> >
> >> > Would be better to keep away from them in general and use something
> more
> >> > stable.
> >> >
> >> > Regards,
> >> > Mridul
> >> >
> >> > [1] Having had to debug this issue for 2 weeks - I really really hate
> it.
> >> >
> >> >
> >> > On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com>
> >> wrote:
> >> > > I have a very strong dislike for #1 (scala enumerations).   I'm ok
> with
> >> > #4
> >> > > (with Xiangrui's final suggestion, especially making it sealed &
> >> > available
> >> > > in Java), but I really think #2, java enums, are the best option.
> >> > >
> >> > > Java enums actually have some very real advantages over the other
> >> > > approaches -- you get values(), valueOf(), EnumSet, and EnumMap.
> There
> >> > has
> >> > > been endless debate in the Scala community about the problems with
> the
> >> > > approaches in Scala.  Very smart, level-headed Scala gurus have
> >> > complained
> >> > > about their short-comings (Rex Kerr's name is coming to mind, though
> >> I'm
> >> > > not positive about that); there have been numerous well-thought out
> >> > > proposals to give Scala a better enum.  But the powers-that-be in
> Scala
> >> > > always reject them.  IIRC the explanation for rejecting is basically
> >> that
> >> > > (a) enums aren't important enough for introducing some new special
> >> > feature,
> >> > > scala's got bigger things to work on and (b) if you really need a
> good
> >> > > enum, just use java's enum.
> >> > >
> >> > > I doubt it really matters that much for Spark internals, which is
> why I
> >> > > think #4 is fine.  But I figured I'd give my spiel, because every
> >> > developer
> >> > > loves language wars :)
> >> > >
> >> > > Imran
> >> > >
> >> > >
> >> > >
> >> > > On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> > >
> >> > >> `case object` inside an `object` doesn't show up in Java. This is
> the
> >> > >> minimal code I found to make everything show up correctly in both
> >> > >> Scala and Java:
> >> > >>
> >> > >> sealed abstract class StorageLevel // cannot be a trait
> >> > >>
> >> > >> object StorageLevel {
> >> > >>   private[this] case object _MemoryOnly extends StorageLevel
> >> > >>   final val MemoryOnly: StorageLevel = _MemoryOnly
> >> > >>
> >> > >>   private[this] case object _DiskOnly extends StorageLevel
> >> > >>   final val DiskOnly: StorageLevel = _DiskOnly
> >> > >> }
> >> > >>
> >> > >> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <
> pwendell@gmail.com>
> >> > >> wrote:
> >> > >> > I like #4 as well and agree with Aaron's suggestion.
> >> > >> >
> >> > >> > - Patrick
> >> > >> >
> >> > >> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <
> ilikerps@gmail.com>
> >> > >> wrote:
> >> > >> >> I'm cool with #4 as well, but make sure we dictate that the
> values
> >> > >> should
> >> > >> >> be defined within an object with the same name as the
> enumeration
> >> > (like
> >> > >> we
> >> > >> >> do for StorageLevel). Otherwise we may pollute a higher
> namespace.
> >> > >> >>
> >> > >> >> e.g. we SHOULD do:
> >> > >> >>
> >> > >> >> trait StorageLevel
> >> > >> >> object StorageLevel {
> >> > >> >>   case object MemoryOnly extends StorageLevel
> >> > >> >>   case object DiskOnly extends StorageLevel
> >> > >> >> }
> >> > >> >>
> >> > >> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
> >> > >> michael@databricks.com>
> >> > >> >> wrote:
> >> > >> >>
> >> > >> >>> #4 with a preference for CamelCaseEnums
> >> > >> >>>
> >> > >> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
> >> > joseph@databricks.com>
> >> > >> >>> wrote:
> >> > >> >>>
> >> > >> >>> > another vote for #4
> >> > >> >>> > People are already used to adding "()" in Java.
> >> > >> >>> >
> >> > >> >>> >
> >> > >> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <
> >> javadba@gmail.com
> >> > >
> >> > >> >>> wrote:
> >> > >> >>> >
> >> > >> >>> > > #4 but with MemoryOnly (more scala-like)
> >> > >> >>> > >
> >> > >> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
> >> > >> >>> > >
> >> > >> >>> > > Constants, Values, Variable and Methods
> >> > >> >>> > >
> >> > >> >>> > > Constant names should be in upper camel case. That is, if
> the
> >> > >> member is
> >> > >> >>> > > final, immutable and it belongs to a package object or an
> >> > object,
> >> > >> it
> >> > >> >>> may
> >> > >> >>> > be
> >> > >> >>> > > considered a constant (similar to Java'sstatic final
> members):
> >> > >> >>> > >
> >> > >> >>> > >
> >> > >> >>> > >    1. object Container {
> >> > >> >>> > >    2.     val MyConstant = ...
> >> > >> >>> > >    3. }
> >> > >> >>> > >
> >> > >> >>> > >
> >> > >> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com
> >:
> >> > >> >>> > >
> >> > >> >>> > > > Hi all,
> >> > >> >>> > > >
> >> > >> >>> > > > There are many places where we use enum-like types in
> Spark,
> >> > but
> >> > >> in
> >> > >> >>> > > > different ways. Every approach has both pros and cons. I
> >> > wonder
> >> > >> >>> > > > whether there should be an "official" approach for
> enum-like
> >> > >> types in
> >> > >> >>> > > > Spark.
> >> > >> >>> > > >
> >> > >> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode,
> WorkerState,
> >> > etc)
> >> > >> >>> > > >
> >> > >> >>> > > > * All types show up as Enumeration.Value in Java.
> >> > >> >>> > > >
> >> > >> >>> > > >
> >> > >> >>> > >
> >> > >> >>> >
> >> > >> >>>
> >> > >>
> >> >
> >>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
> >> > >> >>> > > >
> >> > >> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
> >> > >> >>> > > >
> >> > >> >>> > > > * Implementation must be in a Java file.
> >> > >> >>> > > > * Values doesn't show up in the ScalaDoc:
> >> > >> >>> > > >
> >> > >> >>> > > >
> >> > >> >>> > >
> >> > >> >>> >
> >> > >> >>>
> >> > >>
> >> >
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
> >> > >> >>> > > >
> >> > >> >>> > > > 3. Static fields in Java (e.g., TripletFields)
> >> > >> >>> > > >
> >> > >> >>> > > > * Implementation must be in a Java file.
> >> > >> >>> > > > * Doesn't need "()" in Java code.
> >> > >> >>> > > > * Values don't show up in the ScalaDoc:
> >> > >> >>> > > >
> >> > >> >>> > > >
> >> > >> >>> > >
> >> > >> >>> >
> >> > >> >>>
> >> > >>
> >> >
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
> >> > >> >>> > > >
> >> > >> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
> >> > >> >>> > > >
> >> > >> >>> > > > * Needs "()" in Java code.
> >> > >> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
> >> > >> >>> > > >
> >> > >> >>> > > >
> >> > >> >>> > >
> >> > >> >>> >
> >> > >> >>>
> >> > >>
> >> >
> >>
> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
> >> > >> >>> > > >
> >> > >> >>> > > >
> >> > >> >>> > >
> >> > >> >>> >
> >> > >> >>>
> >> > >>
> >> >
> >>
> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
> >> > >> >>> > > >
> >> > >> >>> > > > It would be great if we have an "official" approach for
> this
> >> > as
> >> > >> well
> >> > >> >>> > > > as the naming convention for enum-like values
> ("MEMORY_ONLY"
> >> > or
> >> > >> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY".
> Any
> >> > >> >>> thoughts?
> >> > >> >>> > > >
> >> > >> >>> > > > Best,
> >> > >> >>> > > > Xiangrui
> >> > >> >>> > > >
> >> > >> >>> > > >
> >> > >>
> ---------------------------------------------------------------------
> >> > >> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > >> >>> > > > For additional commands, e-mail:
> dev-help@spark.apache.org
> >> > >> >>> > > >
> >> > >> >>> > > >
> >> > >> >>> > >
> >> > >> >>> >
> >> > >> >>>
> >> > >>
> >> > >>
> ---------------------------------------------------------------------
> >> > >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> > >> For additional commands, e-mail: dev-help@spark.apache.org
> >> > >>
> >> > >>
> >> >
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b66fa5f706e10051108aa87--

From dev-return-11961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 11 20:23:31 2015
Return-Path: <dev-return-11961-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5605817888
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 11 Mar 2015 20:23:31 +0000 (UTC)
Received: (qmail 43697 invoked by uid 500); 11 Mar 2015 20:23:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43622 invoked by uid 500); 11 Mar 2015 20:23:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43611 invoked by uid 99); 11 Mar 2015 20:23:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 20:23:29 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tamertas@outlook.com designates 157.55.1.163 as permitted sender)
Received: from [157.55.1.163] (HELO DUB004-OMC2S24.hotmail.com) (157.55.1.163)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 11 Mar 2015 20:23:23 +0000
Received: from DUB122-W32 ([157.55.1.137]) by DUB004-OMC2S24.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Wed, 11 Mar 2015 13:22:39 -0700
X-TMN: [d1yNdqsdZcUsvYe6UUF9ceCv4rfZKiC8]
X-Originating-Email: [tamertas@outlook.com]
Message-ID: <DUB122-W32C124CE4867562FF60508D5190@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_513b7b59-2315-4437-ac54-2141a1971b2a_"
From: Tamer TAS <tamertas@outlook.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Apache Spark GSOC 2015
Date: Wed, 11 Mar 2015 22:22:39 +0200
Importance: Normal
MIME-Version: 1.0
X-OriginalArrivalTime: 11 Mar 2015 20:22:39.0994 (UTC) FILETIME=[1F30FDA0:01D05C39]
X-Virus-Checked: Checked by ClamAV on apache.org

--_513b7b59-2315-4437-ac54-2141a1971b2a_
Content-Type: text/plain; charset="windows-1254"
Content-Transfer-Encoding: base64
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--_513b7b59-2315-4437-ac54-2141a1971b2a_--

From dev-return-11962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 12 01:59:08 2015
Return-Path: <dev-return-11962-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 446BB179D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Mar 2015 01:59:08 +0000 (UTC)
Received: (qmail 92447 invoked by uid 500); 12 Mar 2015 01:59:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92367 invoked by uid 500); 12 Mar 2015 01:59:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92340 invoked by uid 99); 12 Mar 2015 01:59:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 01:59:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 01:59:02 +0000
Received: by wiwl15 with SMTP id l15so43801547wiw.4
        for <dev@spark.apache.org>; Wed, 11 Mar 2015 18:58:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=AhRtW0T8qABhEId0+UY3/0yiawC27AfRvWfy0KTXsYc=;
        b=C7ikJfAr5jIltbfb7hjKXI6EXitsZiDJGGxkNr7EcePWB90pCtyGpjOeq/B+Wv8C8/
         z720siZ56mTnKVkwFH+SrWo4+p9Ay4CDDj+JZbmt17vmQRjB3+gBJNGt66u/D1l3epal
         1QzPBy5llbYzNWVHW+kj99n7lY2p06y7umvpZQwMoHUHcdd2m8DfYyvJeNPakCFbH5SS
         Kq/e6TWI5oNdZIDsGNuSgSEigySjkyFt+Cmx449ITiBdUnSSrKy9H+E1xKlQ1HSZuHRz
         JxxKRBWLjS8eErZEfGHYbaZceObelxlF73tL+OF993ZAYDOpJbInxBHGk5Yytc4/wfN+
         6tww==
X-Gm-Message-State: ALoCoQk0QxCmcLWbtI2fS6wOO3+8oRb1TAmi4oohrY/ecg5JRF+c2ZdZ//2gHMaZ8nQgw0qI+DDt
X-Received: by 10.181.9.107 with SMTP id dr11mr38624006wid.40.1426125500756;
 Wed, 11 Mar 2015 18:58:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.139.207 with HTTP; Wed, 11 Mar 2015 18:57:49 -0700 (PDT)
In-Reply-To: <CAO=evYdA-MMvK4eXS_jhUCHq8=EnkvDnTOFm3S=VnvJmTshepg@mail.gmail.com>
References: <CAO=evYdA-MMvK4eXS_jhUCHq8=EnkvDnTOFm3S=VnvJmTshepg@mail.gmail.com>
From: Tathagata Das <tdas@databricks.com>
Date: Wed, 11 Mar 2015 18:57:49 -0700
Message-ID: <CA+AHuK=-kekb_OK0vORouzSa1s_pL0gpTaB0h8t=F++jZKH1oA@mail.gmail.com>
Subject: Re: Spark Streaming - received block allocation to batch
To: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113602ace0c8d305110db7a2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113602ace0c8d305110db7a2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

See responses inline.

On Wed, Mar 11, 2015 at 6:58 AM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.com>
wrote:

> I'm trying to understand the block allocation mechanism Spark uses to
> generate batch jobs and a JobSet.
>
> The JobGenerator.generateJobs tries to allocate received blocks to batch,
> effectively in ReceivedBlockTracker.allocateBlocksToBatch creates
> a streamIdToBlocks, where steam ID's (Int) mapped to Seq[ReceivedBlockInf=
o]
> using getReceivedBlockQueue. This is where it gets tricky for me.
>
> getReceivedBlockQueue of class ReceivedBlockTracker reads
> streamIdToUnallocatedBlockQueues
> that should be populated with ReceivedBlockQueues? Who inserts these
> ReceivedBlockQueues into streamIdToUnallocatedBlockQueues and where does =
it
> get written? I've found only usages of 'effectively' value read.
>
> Inserted here.
https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/ap=
ache/spark/streaming/scheduler/ReceivedBlockTracker.scala#L84


> At a point streamIdToBlocks get packed into a case class
> of AllocatedBlocks. Why is it necessary?
>

Just a container that captures all the blocks allocated to a batch. Used
for both tracking in memory as well as writing it out to the write ahead
log.


>
> Also, at JobGenerator.generateJobs the line where receivedBlockInfos
> created,
> shouldn't it be empty, because streamIdToUnallocatedBlockQueues never got
> written to? Where do I miss the point? How does the
> JobGenerator.generateJobs
> able to retrieve the received block infos?
>
> I think the above line number answers that.


> Thanks,
>
> ZZ
>

--001a113602ace0c8d305110db7a2--

From dev-return-11963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 12 06:49:03 2015
Return-Path: <dev-return-11963-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1BFD17434
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Mar 2015 06:49:02 +0000 (UTC)
Received: (qmail 71228 invoked by uid 500); 12 Mar 2015 06:49:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71153 invoked by uid 500); 12 Mar 2015 06:49:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71142 invoked by uid 99); 12 Mar 2015 06:49:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 06:49:01 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liyun.zhang@intel.com designates 134.134.136.65 as permitted sender)
Received: from [134.134.136.65] (HELO mga03.intel.com) (134.134.136.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 06:48:55 +0000
Received: from orsmga001.jf.intel.com ([10.7.209.18])
  by orsmga103.jf.intel.com with ESMTP; 11 Mar 2015 23:44:40 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.11,387,1422950400"; 
   d="scan'208,217";a="664135941"
Received: from pgsmsx107.gar.corp.intel.com ([10.221.44.105])
  by orsmga001.jf.intel.com with ESMTP; 11 Mar 2015 23:47:34 -0700
Received: from shsmsx101.ccr.corp.intel.com (10.239.4.153) by
 PGSMSX107.gar.corp.intel.com (10.221.44.105) with Microsoft SMTP Server (TLS)
 id 14.3.195.1; Thu, 12 Mar 2015 14:47:33 +0800
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.26]) by
 SHSMSX101.ccr.corp.intel.com ([169.254.1.150]) with mapi id 14.03.0224.002;
 Thu, 12 Mar 2015 14:47:31 +0800
From: "Zhang, Liyun" <liyun.zhang@intel.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: is there any api in spark like getInstance(className:String):AnyRef 
Thread-Topic: is there any api in spark like
 getInstance(className:String):AnyRef 
Thread-Index: AdBckFZhh9Tas9mQSyWEU4Rrcd7LVg==
Date: Thu, 12 Mar 2015 06:47:30 +0000
Message-ID: <AF9B21681D3EE94AA935901747BE4B8101E211BF@shsmsx102.ccr.corp.intel.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: multipart/alternative;
	boundary="_000_AF9B21681D3EE94AA935901747BE4B8101E211BFshsmsx102ccrcor_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_AF9B21681D3EE94AA935901747BE4B8101E211BFshsmsx102ccrcor_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi all:
  I'm a newbie to spark and scala and now I am working on SPARK-5682<https:=
//issues.apache.org/jira/browse/SPARK-5682>(Add encrypted shuffle in spark)=
. I met a problem:is there any api in spark like getInstance(className:Stri=
ng):AnyRef ? I saw org.apache.spark.sql.hive
.thriftserver.ReflectionUtils.scala, but not provide getInstance function i=
n it.

Now i only can implement this function by following code:
object ReflectionUtils1 {
  import scala.reflect.runtime.universe
  abstract case class CryptoCodec() {

  }

  class JceAesCtrCryptoCodec extends CryptoCodec {

  }

  class OpensslAesCtrCryptoCodec extends CryptoCodec {

  }

  def main(args: Array[String]) =3D {
    val className:String =3D  "JceAesCtrCryptoCodec"
    val obj =3D getInstance(className)
    val codec:CryptoCodec =3D obj.asInstanceOf[CryptoCodec]
    println(codec)
  }

  def getInstance(className:String):AnyRef=3D{
    val m =3D universe.runtimeMirror(getClass.getClassLoader)
    var c: CryptoCodec =3D null
    if (className.equals("JceAesCtrCryptoCodec")) {
      val classCryptoCodec =3D universe.typeOf[JceAesCtrCryptoCodec]
        .typeSymbol.asClass
      val cm =3D m.reflectClass(classCryptoCodec)
      val ctor =3D universe.typeOf[JceAesCtrCryptoCodec].declaration(
        universe.nme.CONSTRUCTOR).asMethod
      val ctorm =3D cm.reflectConstructor(ctor)
      val p =3D ctorm()
      c =3D p.asInstanceOf[CryptoCodec]
    } else {
      val classCryptoCodec =3D universe.typeOf[OpensslAesCtrCryptoCodec]
        .typeSymbol.asClass
      val cm =3D m.reflectClass(classCryptoCodec)
      val ctor =3D universe.typeOf[OpensslAesCtrCryptoCodec].declaration(
        universe.nme.CONSTRUCTOR).asMethod
      val ctorm =3D cm.reflectConstructor(ctor)
      val p =3D ctorm()
      c =3D p.asInstanceOf[CryptoCodec]
    }
   c
  }
}


in my getInstance(className:String), i judge classname with "JceAesCtrCrypt=
oCodec" and
"OpensslAesCtrCryptoCodec" and if the name equals "JceAesCtrCryptoCodec", i=
t creates the instance by scala.reflect.runtime.universe api. The code can =
be better like following way but I do not know how to write it:
   def getInstance1(className:String):AnyRef=3D{
       val m =3D universe.runtimeMirror(getClass.getClassLoader)
       var classLoader: ClassLoader =3D Thread.currentThread.getContextClas=
sLoader
       val aClass:Class[_] =3D   Class.forName(className, true, classLoader=
)
       val aType: scala.reflect.api.TypeTags.TypeTag =3D  // how to write t=
his line?
       val classCryptoCodec =3D universe.typeOf[aType]
         .typeSymbol.asClass
       val cm =3D m.reflectClass(classCryptoCodec)
       val ctor =3D universe.typeOf[aType].declaration(
         universe.nme.CONSTRUCTOR).asMethod
       val ctorm =3D cm.reflectConstructor(ctor)
       val p =3D ctorm()
       p
     }

Guidance/advice appreciated!



Best regards
Kelly Zhang/Zhang,Liyun


--_000_AF9B21681D3EE94AA935901747BE4B8101E211BFshsmsx102ccrcor_--

From dev-return-11964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 12 07:14:24 2015
Return-Path: <dev-return-11964-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 69962174C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Mar 2015 07:14:24 +0000 (UTC)
Received: (qmail 4773 invoked by uid 500); 12 Mar 2015 07:14:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4692 invoked by uid 500); 12 Mar 2015 07:14:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4681 invoked by uid 99); 12 Mar 2015 07:14:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 07:14:22 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of yaochunnan@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 07:13:57 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 842161700F16
	for <dev@spark.apache.org>; Thu, 12 Mar 2015 00:14:02 -0700 (PDT)
Date: Thu, 12 Mar 2015 00:13:55 -0700 (MST)
From: Chunnan Yao <yaochunnan@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426144435074-11015.post@n3.nabble.com>
Subject: Is this a bug in MLlib.stat.test ? About the mapPartitions API used
 in Chi-Squared test
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi everyone!
I am digging into MLlib of Spark 1.2.1 currently. When reading codes of
MLlib.stat.test, in the file ChiSqTest.scala under
/spark/mllib/src/main/scala/org/apache/spark/mllib/stat/test, I am confused
by the usage of mapPartitions API in the function  
def chiSquaredFeatures(data: RDD[LabeledPoint],
      methodName: String = PEARSON.name): Array[ChiSqTestResult]

According to my statistical testing knowledge, Chi-Square test requires
large numbers (>5 for 80% entries) in its contingency matrix in order to
satisfy good approximation
(http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test). Thus the number
of feature & label categories cannot be too large because if otherwise,
there would be too few items in each categories, which fails to meet  the
constraint in usage of Chi-square test. 

I do see in the function above, Spark will throw exceptions when
distinctLabels.size and distinctFeatures.size exceed maxCategories defined
as 10000, but the  two HashSets distinctLabels and distinctFeatures are
initialized inside mapPartition, which means Spark will only be sensitive to
the number of feature & label categories in one partition. This will make
the reduced result---contingency matrix still have exceeded number of
categories and thus small matrix entries which makes Chi-Square inaccurate.
I've made a unit test on this function, which proves the case. 

Maybe I am just being trapped by a misunderstanding. Could any one please
give me a hint on this issue?



-----
Feel the sparking Spark!
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Is-this-a-bug-in-MLlib-stat-test-About-the-mapPartitions-API-used-in-Chi-Squared-test-tp11015.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 12 22:11:56 2015
Return-Path: <dev-return-11965-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7B07E17763
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Mar 2015 22:11:56 +0000 (UTC)
Received: (qmail 94507 invoked by uid 500); 12 Mar 2015 22:11:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94430 invoked by uid 500); 12 Mar 2015 22:11:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94419 invoked by uid 99); 12 Mar 2015 22:11:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 22:11:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.42 as permitted sender)
Received: from [209.85.215.42] (HELO mail-la0-f42.google.com) (209.85.215.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 22:11:27 +0000
Received: by lams18 with SMTP id s18so18997990lam.2
        for <dev@spark.apache.org>; Thu, 12 Mar 2015 15:10:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=S6878gX+3YIHwxio94GTsqVbCppsbDG232J+fFcWWmo=;
        b=E1QXkb/0LyIAKNjhotP71voO4YjuXI5fFEbcRlrGd7qCYGQKOvWtkQYPW5ek4eS4jH
         SSHWazsdCbi8FFjWOyOKFhp4MzWCaYV66iIQGi4aeJVWqT5bn/01V1okVy4jzjZXeCO9
         D33so5zjLWsZLsl42Q/hzDYPE1uGvLh/k4tfWIR0jwRFcuktvKJJTI4PfM1XQUw4anCZ
         +nBAWyoSrIgu7RpSjJ50lBCZU0iohtt/8BUuUxGcz+FCGSil+E8PY1Bg7o04TqATr9wM
         vkvMg3kiMJ0q6iIbC8s/Ikj3Cm2K2aakPYzilBqUCVp+nhVytB9NpVVgcAZ7JQ8eVUXg
         +dzw==
X-Gm-Message-State: ALoCoQlGl8ur67KxmRbt6wpxc5r8dNGvYIWn/bsBIFsPLhpQs5ayocbBGgDDIViVL3ZQB+QKckuO
X-Received: by 10.152.219.2 with SMTP id pk2mr40214042lac.107.1426198240253;
 Thu, 12 Mar 2015 15:10:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Thu, 12 Mar 2015 15:10:19 -0700 (PDT)
In-Reply-To: <CACdU-dTXnSw0ksNQuytngXZnDtZS13HKC=UkE3ghJizVBjEKUg@mail.gmail.com>
References: <CACdU-dTXnSw0ksNQuytngXZnDtZS13HKC=UkE3ghJizVBjEKUg@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Thu, 12 Mar 2015 15:10:19 -0700
Message-ID: <CACdU-dTzTfS6UCUUHBCy30e64EeUHKrLZVD==4Q0hLZV4tt7LA@mail.gmail.com>
Subject: Re: adding some temporary jenkins worker nodes...
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134188c7d6f1105111ea74d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134188c7d6f1105111ea74d
Content-Type: text/plain; charset=UTF-8

the big 1.3 push is over, so i'll be reclaiming these three extra workers.
 :)

On Mon, Feb 9, 2015 at 5:18 PM, shane knapp <sknapp@berkeley.edu> wrote:

> ...to help w/the build backlog.  let's all welcome
> amp-jenkins-slave-{01..03} back to the fray!
>

--001a1134188c7d6f1105111ea74d--

From dev-return-11966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 12 23:10:26 2015
Return-Path: <dev-return-11966-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A50A7179BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 12 Mar 2015 23:10:26 +0000 (UTC)
Received: (qmail 77727 invoked by uid 500); 12 Mar 2015 23:10:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77648 invoked by uid 500); 12 Mar 2015 23:10:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77637 invoked by uid 99); 12 Mar 2015 23:10:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 23:10:09 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of thubregtsen@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 12 Mar 2015 23:09:43 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 518C41718451
	for <dev@spark.apache.org>; Thu, 12 Mar 2015 16:09:19 -0700 (PDT)
Date: Thu, 12 Mar 2015 16:09:11 -0700 (MST)
From: Tom Hubregtsen <thubregtsen@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426201751505-11017.post@n3.nabble.com>
Subject: Spilling when not expected
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I'm running the teraSort benchmark with a relative small input set: 5GB.
During profiling, I can see I am using a total of 68GB. I've got a terabyte
of memory in my system, and set
spark.executor.memory 900g
spark.driver.memory 900g
I use the default for 
spark.shuffle.memoryFraction 
spark.storage.memoryFraction
I believe that I now have 0.2*900=180GB for shuffle and 0.6*900=540GB for
storage.

I noticed a lot of variation in runtime (under the same load), and tracked
this down to this function in 
core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala
  private def spillToPartitionFiles(collection:
SizeTrackingPairCollection[(Int, K), C]): Unit = {
    spillToPartitionFiles(collection.iterator)
  }
In a slow run, it would loop through this function 12000 times, in a fast
run only 700 times, even though the settings in both runs are the same and
there are no other users on the system. When I look at the function calling
this (insertAll, also in ExternalSorter), I see that spillToPartitionFiles
is only called 700 times in both fast and slow runs, meaning that the
function recursively calls itself very often. Because of the function name,
I assume the system is spilling to disk. As I have sufficient memory, I
assume that I forgot to set a certain memory setting. Anybody any idea which
other setting I have to set, in order to not spill data in this scenario?

Thanks,

Tom



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Spilling-when-not-expected-tp11017.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 00:37:44 2015
Return-Path: <dev-return-11967-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 459E517CA7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 00:37:44 +0000 (UTC)
Received: (qmail 53992 invoked by uid 500); 13 Mar 2015 00:37:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53926 invoked by uid 500); 13 Mar 2015 00:37:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53915 invoked by uid 99); 13 Mar 2015 00:37:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 00:37:42 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.240.92.66] (HELO g9t5008.houston.hp.com) (15.240.92.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 00:37:14 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5008.houston.hp.com (Postfix) with ESMTPS id F0D7C8E
	for <dev@spark.apache.org>; Fri, 13 Mar 2015 00:36:41 +0000 (UTC)
Received: from G9W3613.americas.hpqcorp.net (16.216.186.48) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Fri, 13 Mar 2015 00:34:41 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.48]) by
 G9W3613.americas.hpqcorp.net ([16.216.186.48]) with mapi id 14.03.0169.001;
 Fri, 13 Mar 2015 00:34:41 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Profiling Spark: MemoryStore
Thread-Topic: Profiling Spark: MemoryStore
Thread-Index: AdBdHXfnfL6nfie+TtGqY5pYClyCWA==
Date: Fri, 13 Mar 2015 00:34:40 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1961B@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.17]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE1961BG4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE1961BG4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

I am working on artificial neural networks for Spark. It is solved with Gra=
dient Descent, so each step the data is read, sum of gradients is calculate=
d for each data partition (on each worker), aggregated (on the driver) and =
broadcasted back. I noticed that the gradient computation time is few times=
 less than the total time needed for each step. To narrow down my observati=
on, I run the gradient on a single machine with single partition of data of=
 site 100MB that I persist (data.persist). This should minimize the overhea=
d for aggregation at least, but the gradient computation still takes much l=
ess time than the whole step. Just in case, data is loaded by MLUtil. loadL=
ibSVMFile in RDD[LabeledPoint], this is my code:

    val conf =3D new SparkConf().setAppName("myApp").setMaster("local[2]")
    val train =3D MLUtils.loadLibSVMFile(new SparkContext(conf), "/data/mni=
st/mnist.scale").repartition(1).persist()
    val model =3D ANN2Classifier.train(train, 1000, Array[Int](32), 10, 1e-=
4) //training data, batch size, hidden layer size, iterations, LBFGS tolera=
nce

Profiler shows that there are two threads, one is doing Gradient and the ot=
her I don't know what. The Gradient takes 10% of this thread. Almost all ot=
her time is spent by MemoryStore. Below is the screenshot (first thread):
https://drive.google.com/file/d/0BzYMzvDiCep5bGp2S2F6eE9TRlk/view?usp=3Dsha=
ring
Second thread:
https://drive.google.com/file/d/0BzYMzvDiCep5OHA0WUtQbXd3WmM/view?usp=3Dsha=
ring

Could Spark developers please elaborate what's going on in MemoryStore? It =
seems that it does some string operations (parsing libsvm file? Why every s=
tep?) and a lot of InputStream reading. It seems that the overall time depe=
nds on the size of the data batch (or size of vector) I am processing. Howe=
ver it does not seems linear to me.

Also, I would like to know how to speedup these operations.

Best regards, Alexander


--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE1961BG4W3292americas_--

From dev-return-11968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 01:24:13 2015
Return-Path: <dev-return-11968-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8771117DF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 01:24:13 +0000 (UTC)
Received: (qmail 26026 invoked by uid 500); 13 Mar 2015 01:24:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25941 invoked by uid 500); 13 Mar 2015 01:24:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25930 invoked by uid 99); 13 Mar 2015 01:24:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 01:24:12 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 01:23:47 +0000
Received: by iecvj10 with SMTP id vj10so69992503iec.0
        for <dev@spark.apache.org>; Thu, 12 Mar 2015 18:21:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=FOAfc2SAGIk0EWhrRQc4TKYMY088311O7Z41yEpm3nY=;
        b=E491nswpLJalLMiOAeEnALONbH0muXw5UG6ml/DUn66zyC3dT3A7pM9/S6ABlnYjNu
         TEfNQvu1fdfA0tMkp+jYdYs3uyjaD35qbdd1UKChAH40z58XbvcMiCI9hvPN9WOipfSk
         smN1j5/b0qBTayCna44K4cBVJTEG4D9twki+pPUzH6Hd718dOCGOjXYugXAc2W+ghALg
         jmKQkPvQdxgEqyILqeSikoxgTwGmUG5Tg4jubFNMzE57dBdtEpYebj7sywOZVBA15+Hs
         Y1JrrEpTUCa30MB7SGc5xVSz9elDpscDEA4dfstNaYRDzrJMTDM+rGh7HQhrepm9ggGt
         dBnA==
X-Gm-Message-State: ALoCoQmsYXUEgNvHR/p181E0VMRilj5U7KzgXeNFmQB4t3j6LiEGnK9emJRuUncLGivEVgl3IEDy
MIME-Version: 1.0
X-Received: by 10.107.133.16 with SMTP id h16mr39936298iod.31.1426209669654;
 Thu, 12 Mar 2015 18:21:09 -0700 (PDT)
Received: by 10.36.118.7 with HTTP; Thu, 12 Mar 2015 18:21:09 -0700 (PDT)
In-Reply-To: <1426144435074-11015.post@n3.nabble.com>
References: <1426144435074-11015.post@n3.nabble.com>
Date: Thu, 12 Mar 2015 18:21:09 -0700
Message-ID: <CAF7ADNr8-iSXSvQuToQoBjC_OsfS6tKJf-8Xk6Noxf0z3W6w5w@mail.gmail.com>
Subject: Re: Is this a bug in MLlib.stat.test ? About the mapPartitions API
 used in Chi-Squared test
From: Joseph Bradley <joseph@databricks.com>
To: Chunnan Yao <yaochunnan@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113f9a3abc433905112150dc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f9a3abc433905112150dc
Content-Type: text/plain; charset=UTF-8

The checks against maxCategories are not for statistical purposes; they are
to make sure communication does not blow up.  There currently are not
checks to make sure that there are enough entries for statistically
significant results.  That is up to the user.

I do like the idea of adding a warning.  A reasonable fix for now might be
to print a logWarning message and add a note to the documentation.  On the
JIRA, we could also discuss whether the result should be set to some value
to indicate a meaningless test (e.g., a very bad fixed pValue).

I made a JIRA to track this issue: SPARK-6312

Joseph

On Thu, Mar 12, 2015 at 12:13 AM, Chunnan Yao <yaochunnan@gmail.com> wrote:

> Hi everyone!
> I am digging into MLlib of Spark 1.2.1 currently. When reading codes of
> MLlib.stat.test, in the file ChiSqTest.scala under
> /spark/mllib/src/main/scala/org/apache/spark/mllib/stat/test, I am confused
> by the usage of mapPartitions API in the function
> def chiSquaredFeatures(data: RDD[LabeledPoint],
>       methodName: String = PEARSON.name): Array[ChiSqTestResult]
>
> According to my statistical testing knowledge, Chi-Square test requires
> large numbers (>5 for 80% entries) in its contingency matrix in order to
> satisfy good approximation
> (http://en.wikipedia.org/wiki/Pearson%27s_chi-squared_test). Thus the
> number
> of feature & label categories cannot be too large because if otherwise,
> there would be too few items in each categories, which fails to meet  the
> constraint in usage of Chi-square test.
>
> I do see in the function above, Spark will throw exceptions when
> distinctLabels.size and distinctFeatures.size exceed maxCategories defined
> as 10000, but the  two HashSets distinctLabels and distinctFeatures are
> initialized inside mapPartition, which means Spark will only be sensitive
> to
> the number of feature & label categories in one partition. This will make
> the reduced result---contingency matrix still have exceeded number of
> categories and thus small matrix entries which makes Chi-Square inaccurate.
> I've made a unit test on this function, which proves the case.
>
> Maybe I am just being trapped by a misunderstanding. Could any one please
> give me a hint on this issue?
>
>
>
> -----
> Feel the sparking Spark!
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Is-this-a-bug-in-MLlib-stat-test-About-the-mapPartitions-API-used-in-Chi-Squared-test-tp11015.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113f9a3abc433905112150dc--

From dev-return-11969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 03:27:43 2015
Return-Path: <dev-return-11969-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C85D71746A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 03:27:43 +0000 (UTC)
Received: (qmail 42726 invoked by uid 500); 13 Mar 2015 03:27:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42659 invoked by uid 500); 13 Mar 2015 03:27:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42639 invoked by uid 99); 13 Mar 2015 03:27:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 03:27:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of thegiive@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 03:27:16 +0000
Received: by wivr20 with SMTP id r20so2741661wiv.5
        for <dev@spark.apache.org>; Thu, 12 Mar 2015 20:25:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=19oDa71zh2Aa8jzb/vYe5xm++FpTWEB5rQn5/GU4z8k=;
        b=kmaIJPJgfRqlvPnuCM6ozbWwNsJeOWy4oAPRYQc9CDtU13fhupPAdIaHKxJ3u2GmEh
         RY2xQtEzA2c0Br+PDiTfk7Pl52YsZ33Rq/9SeK9cfse/EoDSB/LemovO3MtkSTQSeUwr
         1KCf28Dv0+yDwIY93bLrtOHFwy0p8Y6DnV1Lvfhq15nXjyumjFm0NPlRP52MirBe3rrz
         OewTJD5JS0dTmtDvyvjXb4wTAkSsRQInAFhjsVmut928j4tzVcIbanr9ypct721dSW5H
         zB+W7R2+5epNwHqGO9LN6vRfcdHGs52iTyDwYmPVT8hYi3vX7a22E8U5/E7B+e0CEuut
         otoA==
MIME-Version: 1.0
X-Received: by 10.194.187.236 with SMTP id fv12mr94061594wjc.131.1426217144792;
 Thu, 12 Mar 2015 20:25:44 -0700 (PDT)
Received: by 10.194.109.196 with HTTP; Thu, 12 Mar 2015 20:25:44 -0700 (PDT)
In-Reply-To: <CANrtgzUX4pM3akkPE3a2AcHxWDXdaw3OazpDt2CARcOEffeQsg@mail.gmail.com>
References: <CANrtgzUX4pM3akkPE3a2AcHxWDXdaw3OazpDt2CARcOEffeQsg@mail.gmail.com>
Date: Fri, 13 Mar 2015 11:25:44 +0800
Message-ID: <CAH9GxT+bdfG6ZeXkgjAeqxx3_9_QHFmVefC0feVuKN1Nm+8kVg@mail.gmail.com>
Subject: Re: SparkSQL 1.3.0 (RC3) failed to read parquet file generated by 1.1.1
From: giive chen <thegiive@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bea40cc49bec50511230ef0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bea40cc49bec50511230ef0
Content-Type: text/plain; charset=ISO-8859-1

Hi all

My team has the same issue. It looks like Spark 1.3's sparkSQL cannot read
parquet file generated by Spark 1.1. It will cost a lot of migration work
when we wanna to upgrade Spark 1.3.

Is there  anyone can help me?


Thanks

Wisely Chen


On Tue, Mar 10, 2015 at 5:06 PM, Pei-Lun Lee <pllee@appier.com> wrote:

> Hi,
>
> I found that if I try to read parquet file generated by spark 1.1.1 using
> 1.3.0-rc3 by default settings, I got this error:
>
> com.fasterxml.jackson.core.JsonParseException: Unrecognized token
> 'StructType': was expecting ('true', 'false' or 'null')
>  at [Source: StructType(List(StructField(a,IntegerType,false))); line: 1,
> column: 11]
>         at
> com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1419)
>         at
>
> com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508)
>         at
>
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2300)
>         at
>
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1459)
>         at
>
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:683)
>         at
>
> com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3105)
>         at
>
> com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3051)
>         at
>
> com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2161)
>         at org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:19)
>         at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:44)
>         at
> org.apache.spark.sql.types.DataType$.fromJson(dataTypes.scala:41)
>         at
>
> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
>         at
>
> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
>
>
>
> this is how I save parquet file with 1.1.1:
>
> sql("select 1 as a").saveAsParquetFile("/tmp/foo")
>
>
>
> and this is the meta data of the 1.1.1 parquet file:
>
> creator:     parquet-mr version 1.4.3
> extra:       org.apache.spark.sql.parquet.row.metadata =
> StructType(List(StructField(a,IntegerType,false)))
>
>
>
> by comparison, this is 1.3.0 meta:
>
> creator:     parquet-mr version 1.6.0rc3
> extra:       org.apache.spark.sql.parquet.row.metadata =
> {"type":"struct","fields":[{"name":"a","type":"integer","nullable":t
> [more]...
>
>
>
> It looks like now ParquetRelation2 is used to load parquet file by default
> and it only recognizes JSON format schema but 1.1.1 schema was case class
> string format.
>
> Setting spark.sql.parquet.useDataSourceApi to false will fix it, but I
> don't know the differences.
> Is this considered a bug? We have a lot of parquet files from 1.1.1, should
> we disable data source api in order to read them if we want to upgrade to
> 1.3?
>
> Thanks,
> --
> Pei-Lun
>

--047d7bea40cc49bec50511230ef0--

From dev-return-11970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 03:52:24 2015
Return-Path: <dev-return-11970-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4DF22174BD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 03:52:24 +0000 (UTC)
Received: (qmail 69762 invoked by uid 500); 13 Mar 2015 03:52:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69676 invoked by uid 500); 13 Mar 2015 03:52:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69432 invoked by uid 99); 13 Mar 2015 03:52:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 03:52:22 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of canny@berkeley.edu does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 03:51:57 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 80743171C71A
	for <dev@spark.apache.org>; Thu, 12 Mar 2015 20:50:32 -0700 (PDT)
Date: Thu, 12 Mar 2015 20:50:24 -0700 (MST)
From: jfcanny <canny@berkeley.edu>
To: dev@spark.apache.org
Message-ID: <1426218624554-11021.post@n3.nabble.com>
In-Reply-To: <CAKx7Bf_kyhjjzvuNwdZOBs=iwF2BCXd-CAESG4H2+98t5tU1aQ@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net> <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com> <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com> <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com> <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net> <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE18268@G4W3292.americas.hpqcorp.net> <CAKx7Bf_kyhjjzvuNwdZOBs=iwF2BCXd-CAESG4H2+98t5tU1aQ@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

If you're contemplating GPU acceleration in Spark, its important to look
beyond BLAS. Dense BLAS probably account for only 10% of the cycles in the
datasets we've tested in BIDMach, and we've tried to make them
representative of industry machine learning workloads. Unless you're
crunching images or audio, the majority of data will be very sparse and
power law distributed. You need a good sparse BLAS, and in practice it seems
like you need a sparse BLAS tailored for power-law data. We had to write our
own since the NVIDIA libraries didnt perform well on typical power-law data.
Intel MKL sparse BLAS also have issues and we only use some of them. 

You also need 2D reductions, scan operations, slicing, element-wise
transcendental functions and operators, many kinds of sort, random number
generators etc, and some kind of memory management strategy. Some of this
was layered on top of Thrust in BIDMat, but most had to be written from
scratch. Its all been rooflined, typically to memory throughput of current
GPUs (around 200 GB/s). 

When you have all this you can write Learning Algorithms in the same
high-level primitives available in Breeze or Numpy/Scipy. Its literally the
same in BIDMat, since the generic matrix operations are implemented on both
CPU and GPU, so the same code runs on either platform. 

A lesser known fact is that GPUs are around 10x faster for *all* those
operations, not just dense BLAS. Its mostly due to faster streaming memory
speeds, but some kernels (random number generation and transcendentals) are
more than an order of magnitude thanks to some specialized hardware for
power series on the GPU chip. 

When you have all this there is no need to move data back and forth across
the PCI bus. The CPU only has to pull chunks of data off disk, unpack them,
and feed them to the available GPUs. Most models fit comfortably in GPU
memory these days (4-12 GB). With minibatch algorithms you can push TBs of
data through the GPU this way. 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-within-Spark-boosting-linear-algebra-tp10481p11021.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 05:55:44 2015
Return-Path: <dev-return-11971-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47F68176C5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 05:55:44 +0000 (UTC)
Received: (qmail 2083 invoked by uid 500); 13 Mar 2015 05:55:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2005 invoked by uid 500); 13 Mar 2015 05:55:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1994 invoked by uid 99); 13 Mar 2015 05:55:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 05:55:42 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 05:55:38 +0000
Received: by qgfh3 with SMTP id h3so23599730qgf.13
        for <dev@spark.apache.org>; Thu, 12 Mar 2015 22:54:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=zbOeDYDb4jzHgB8cETt13vnD1Qt3hRWrW8V+NueEOeI=;
        b=UowlQUD78JPyzdB+t9OSAsaeuCvLfEyqx2p+OE2x9a3NIJL2kWVYPZtMcs1GcFs9Ef
         NFcBTMWu45LwGfxRQarecjc8vEKsEGJOtA2iNyXlR07/dY9EhmU6XOPdFMg4qZdKGaYK
         BpWckYo8/Qcx7toPY6esoQMF9n2yLoiYEq8kylWns7S6TOhoaDExKHlMhP7D4GE+K1/k
         Vzipd8CMZDT2Xl6DmrwcacBJxyHLirI/O1szqfthTtGNJK0r6cX+QCJsJV6VCFi3GRCy
         lgmNFDb9XhEGLY/Hkaxi4xRd/vvean+pulS8sgY9bIXDFCpCb0A0ECJW+XkstHf0yABR
         wgVA==
X-Gm-Message-State: ALoCoQmkupvED8BmN3AcMQnT+zmBtsnnxXnU5QFTu9+tVQqKw7jLQo3ka9OZSPqipCRLDthQLRAV
X-Received: by 10.140.150.142 with SMTP id 136mr60033053qhw.33.1426226052747;
 Thu, 12 Mar 2015 22:54:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.109.9 with HTTP; Thu, 12 Mar 2015 22:53:52 -0700 (PDT)
In-Reply-To: <1426218624554-11021.post@n3.nabble.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
 <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
 <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE18268@G4W3292.americas.hpqcorp.net>
 <CAKx7Bf_kyhjjzvuNwdZOBs=iwF2BCXd-CAESG4H2+98t5tU1aQ@mail.gmail.com> <1426218624554-11021.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 12 Mar 2015 22:53:52 -0700
Message-ID: <CAPh_B=aZsWY6DVHgPX4JG-FP0eQjPREN=DHPK22gU+uwKrhE-g@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
To: jfcanny <canny@berkeley.edu>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11357d4a3e70830511252178
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11357d4a3e70830511252178
Content-Type: text/plain; charset=UTF-8

Thanks for chiming in, John. I missed your meetup last night - do you have
any writeups or slides about roofline design? In particular, I'm curious
about what optimizations are available for power-law dense * sparse? (I
don't have any background in optimizations)



On Thu, Mar 12, 2015 at 8:50 PM, jfcanny <canny@berkeley.edu> wrote:

> If you're contemplating GPU acceleration in Spark, its important to look
> beyond BLAS. Dense BLAS probably account for only 10% of the cycles in the
> datasets we've tested in BIDMach, and we've tried to make them
> representative of industry machine learning workloads. Unless you're
> crunching images or audio, the majority of data will be very sparse and
> power law distributed. You need a good sparse BLAS, and in practice it
> seems
> like you need a sparse BLAS tailored for power-law data. We had to write
> our
> own since the NVIDIA libraries didnt perform well on typical power-law
> data.
> Intel MKL sparse BLAS also have issues and we only use some of them.
>
> You also need 2D reductions, scan operations, slicing, element-wise
> transcendental functions and operators, many kinds of sort, random number
> generators etc, and some kind of memory management strategy. Some of this
> was layered on top of Thrust in BIDMat, but most had to be written from
> scratch. Its all been rooflined, typically to memory throughput of current
> GPUs (around 200 GB/s).
>
> When you have all this you can write Learning Algorithms in the same
> high-level primitives available in Breeze or Numpy/Scipy. Its literally the
> same in BIDMat, since the generic matrix operations are implemented on both
> CPU and GPU, so the same code runs on either platform.
>
> A lesser known fact is that GPUs are around 10x faster for *all* those
> operations, not just dense BLAS. Its mostly due to faster streaming memory
> speeds, but some kernels (random number generation and transcendentals) are
> more than an order of magnitude thanks to some specialized hardware for
> power series on the GPU chip.
>
> When you have all this there is no need to move data back and forth across
> the PCI bus. The CPU only has to pull chunks of data off disk, unpack them,
> and feed them to the available GPUs. Most models fit comfortably in GPU
> memory these days (4-12 GB). With minibatch algorithms you can push TBs of
> data through the GPU this way.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-within-Spark-boosting-linear-algebra-tp10481p11021.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11357d4a3e70830511252178--

From dev-return-11972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 06:01:59 2015
Return-Path: <dev-return-11972-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58EBA176DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 06:01:59 +0000 (UTC)
Received: (qmail 8358 invoked by uid 500); 13 Mar 2015 06:01:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8276 invoked by uid 500); 13 Mar 2015 06:01:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8265 invoked by uid 99); 13 Mar 2015 06:01:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 06:01:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.47] (HELO mail-la0-f47.google.com) (209.85.215.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 06:01:53 +0000
Received: by labgd6 with SMTP id gd6so20411463lab.6
        for <dev@spark.apache.org>; Thu, 12 Mar 2015 23:01:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=oQ+gXSj6lo+s1/BjaW42rrZdGxtKaiob27Ee3CtnfvY=;
        b=W2ftBOkZUzXRfWs/n9wEFajpFmOdckuV8KD18m5zM8AuxA50mjUdht3G4Pz2Tmwjkq
         FblNKgi5tl138KEYdY4aQXTCSX1bKQK1gmmiCGuu6rQOpUHfqR5zPGPMHr/XbQNujWQU
         jnt6p3pniTjv9W8lwgIHwB+S6SvRSDxhbEbFfFTk81SgnhyiQxWRBKTzrnk3g2Wdk5UI
         NnWgqaZE1spz+pKJzvjyRfCudWHNJgfx/e6D9CM4NvL0HNDD1XjxG2UTvgdzZZItZ/u9
         UiU9YcnvS5caL+VGgagdu24cmrn7Frl4qkLyY61UDZW/VMl9iQA9ukEW6o6IeVdfOhGD
         AdtQ==
X-Gm-Message-State: ALoCoQn8E1WtRRQjUyiFGTDwQvLhmxFkMDRcFlfqfw+WWJW7NVqExcSsK5W7ofyB0pzl3XX7IXEX
X-Received: by 10.112.150.73 with SMTP id ug9mr42479588lbb.31.1426226471959;
 Thu, 12 Mar 2015 23:01:11 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.213.18 with HTTP; Thu, 12 Mar 2015 23:00:51 -0700 (PDT)
In-Reply-To: <CAH9GxT+bdfG6ZeXkgjAeqxx3_9_QHFmVefC0feVuKN1Nm+8kVg@mail.gmail.com>
References: <CANrtgzUX4pM3akkPE3a2AcHxWDXdaw3OazpDt2CARcOEffeQsg@mail.gmail.com>
 <CAH9GxT+bdfG6ZeXkgjAeqxx3_9_QHFmVefC0feVuKN1Nm+8kVg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 12 Mar 2015 23:00:51 -0700
Message-ID: <CAAswR-4Briw3txmJB5L+aLykzzYYZJBeNQD6zwE=GdamgLRxSA@mail.gmail.com>
Subject: Re: SparkSQL 1.3.0 (RC3) failed to read parquet file generated by 1.1.1
To: giive chen <thegiive@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Cheng Lian <lian@databricks.com>
Content-Type: multipart/alternative; boundary=047d7b3432903b16f40511253ad1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3432903b16f40511253ad1
Content-Type: text/plain; charset=UTF-8

We are looking at the issue and will likely fix it for Spark 1.3.1.

On Thu, Mar 12, 2015 at 8:25 PM, giive chen <thegiive@gmail.com> wrote:

> Hi all
>
> My team has the same issue. It looks like Spark 1.3's sparkSQL cannot read
> parquet file generated by Spark 1.1. It will cost a lot of migration work
> when we wanna to upgrade Spark 1.3.
>
> Is there  anyone can help me?
>
>
> Thanks
>
> Wisely Chen
>
>
> On Tue, Mar 10, 2015 at 5:06 PM, Pei-Lun Lee <pllee@appier.com> wrote:
>
> > Hi,
> >
> > I found that if I try to read parquet file generated by spark 1.1.1 using
> > 1.3.0-rc3 by default settings, I got this error:
> >
> > com.fasterxml.jackson.core.JsonParseException: Unrecognized token
> > 'StructType': was expecting ('true', 'false' or 'null')
> >  at [Source: StructType(List(StructField(a,IntegerType,false))); line: 1,
> > column: 11]
> >         at
> >
> com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1419)
> >         at
> >
> >
> com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508)
> >         at
> >
> >
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2300)
> >         at
> >
> >
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1459)
> >         at
> >
> >
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:683)
> >         at
> >
> >
> com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3105)
> >         at
> >
> >
> com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3051)
> >         at
> >
> >
> com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2161)
> >         at
> org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:19)
> >         at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:44)
> >         at
> > org.apache.spark.sql.types.DataType$.fromJson(dataTypes.scala:41)
> >         at
> >
> >
> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
> >         at
> >
> >
> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
> >
> >
> >
> > this is how I save parquet file with 1.1.1:
> >
> > sql("select 1 as a").saveAsParquetFile("/tmp/foo")
> >
> >
> >
> > and this is the meta data of the 1.1.1 parquet file:
> >
> > creator:     parquet-mr version 1.4.3
> > extra:       org.apache.spark.sql.parquet.row.metadata =
> > StructType(List(StructField(a,IntegerType,false)))
> >
> >
> >
> > by comparison, this is 1.3.0 meta:
> >
> > creator:     parquet-mr version 1.6.0rc3
> > extra:       org.apache.spark.sql.parquet.row.metadata =
> > {"type":"struct","fields":[{"name":"a","type":"integer","nullable":t
> > [more]...
> >
> >
> >
> > It looks like now ParquetRelation2 is used to load parquet file by
> default
> > and it only recognizes JSON format schema but 1.1.1 schema was case class
> > string format.
> >
> > Setting spark.sql.parquet.useDataSourceApi to false will fix it, but I
> > don't know the differences.
> > Is this considered a bug? We have a lot of parquet files from 1.1.1,
> should
> > we disable data source api in order to read them if we want to upgrade to
> > 1.3?
> >
> > Thanks,
> > --
> > Pei-Lun
> >
>

--047d7b3432903b16f40511253ad1--

From dev-return-11973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 07:05:46 2015
Return-Path: <dev-return-11973-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1758917815
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 07:05:46 +0000 (UTC)
Received: (qmail 36601 invoked by uid 500); 13 Mar 2015 07:05:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36526 invoked by uid 500); 13 Mar 2015 07:05:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36514 invoked by uid 99); 13 Mar 2015 07:05:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 07:05:44 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mkim@palantir.com designates 66.70.54.21 as permitted sender)
Received: from [66.70.54.21] (HELO mxw1.palantir.com) (66.70.54.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 07:05:19 +0000
Received: from EX02-WEST.YOJOE.local ([169.254.1.145]) by
 EX03-WEST.YOJOE.local ([169.254.2.196]) with mapi id 14.03.0195.001; Fri, 13
 Mar 2015 00:04:33 -0700
From: Mingyu Kim <mkim@palantir.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
CC: Eric Lin <elin@palantir.com>, Andrew Shi <ashi@palantir.com>
Subject: toLocalIterator creates as many jobs as # of partitions, and it
 ends up spamming Spark UI
Thread-Topic: toLocalIterator creates as many jobs as # of partitions, and
 it ends up spamming Spark UI
Thread-Index: AQHQXVv069Ykl4AHP0ynn6Pa5/bFcQ==
Date: Fri, 13 Mar 2015 07:04:32 +0000
Message-ID: <D127DA0C.2042B%mkim@palantir.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.1.140326
x-originating-ip: [10.160.122.85]
Content-Type: multipart/alternative;
	boundary="_000_D127DA0C2042Bmkimpalantircom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D127DA0C2042Bmkimpalantircom_
Content-Type: text/plain; charset="Windows-1252"
Content-Transfer-Encoding: quoted-printable

Hi all,

RDD.toLocalIterator() creates as many jobs as # of partitions and it spams =
Spark UI especially when the method is used on an RDD with hundreds or thou=
sands of partitions.

Does anyone have a way to work around this issue? What do people think abou=
t introducing a SparkContext local property (analogous to =93spark.schedule=
r.pool=94 set as a thread-local property) that determines if the job info s=
hould be shown on the Spark UI?

Thanks,
Mingyu

--_000_D127DA0C2042Bmkimpalantircom_--

From dev-return-11974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 07:07:00 2015
Return-Path: <dev-return-11974-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CFD5D17817
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 07:07:00 +0000 (UTC)
Received: (qmail 38770 invoked by uid 500); 13 Mar 2015 07:06:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38692 invoked by uid 500); 13 Mar 2015 07:06:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38681 invoked by uid 99); 13 Mar 2015 07:06:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 07:06:59 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 07:06:34 +0000
Received: by qcvs11 with SMTP id s11so24530794qcv.6
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 00:05:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=6aCWq1ZG2bQ7iKnN0z5OpXIDiu5Zj3Mht9XbgVZcUiY=;
        b=LpuVP6E8Z8ZoYGeRvTHsvo0rgRWbFUB0k76z3EyRYg2SYZ0TMgZUd2MB9HA+JDuoCD
         b2TsFUM3r2Oq4HHn/NYNAttl7yLRRysI9dcwnDSx15qBu2uc2oqIYO9vMnWG8cLZ0HnI
         U17SE3ArVYMI1tpqY625Uoas3q3WJ1VhQH+7U83uLUruqUYFqOL7X1/Nv3fN51vmJd3Y
         F+7PoDkZJV0tgOTRvzIpiKSBWN2qhsY84+EQaDNTYX5IO+JdLgd3O6LiXmYc/hYVXw0g
         oy1SqXvCTMXSRgGgIj6+ivArT0p67Ilk3j3rTMom7feS55tPV7LBNucehK5TRX27fmO9
         vfaA==
X-Gm-Message-State: ALoCoQmYqlN6fAlj4S0fDT5an3WRLPlukU3gYQh2mdg9yuRtoMPetAwVxPiwWb2K7BWoknDlEtNx
X-Received: by 10.140.93.199 with SMTP id d65mr56220824qge.104.1426230327153;
 Fri, 13 Mar 2015 00:05:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.109.9 with HTTP; Fri, 13 Mar 2015 00:05:06 -0700 (PDT)
In-Reply-To: <1426201751505-11017.post@n3.nabble.com>
References: <1426201751505-11017.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 13 Mar 2015 00:05:06 -0700
Message-ID: <CAPh_B=Z_P8efOyGvEDmP9_ALQy=Dz334Sby_6fVp2Xn1VXZAPA@mail.gmail.com>
Subject: Re: Spilling when not expected
To: Tom Hubregtsen <thubregtsen@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11395a3004b4f305112620a2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11395a3004b4f305112620a2
Content-Type: text/plain; charset=UTF-8

How did you run the Spark command? Maybe the memory setting didn't actually
apply? How much memory does the web ui say is available?

BTW - I don't think any JVM can actually handle 700G heap ... (maybe Zing).

On Thu, Mar 12, 2015 at 4:09 PM, Tom Hubregtsen <thubregtsen@gmail.com>
wrote:

> Hi all,
>
> I'm running the teraSort benchmark with a relative small input set: 5GB.
> During profiling, I can see I am using a total of 68GB. I've got a terabyte
> of memory in my system, and set
> spark.executor.memory 900g
> spark.driver.memory 900g
> I use the default for
> spark.shuffle.memoryFraction
> spark.storage.memoryFraction
> I believe that I now have 0.2*900=180GB for shuffle and 0.6*900=540GB for
> storage.
>
> I noticed a lot of variation in runtime (under the same load), and tracked
> this down to this function in
> core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala
>   private def spillToPartitionFiles(collection:
> SizeTrackingPairCollection[(Int, K), C]): Unit = {
>     spillToPartitionFiles(collection.iterator)
>   }
> In a slow run, it would loop through this function 12000 times, in a fast
> run only 700 times, even though the settings in both runs are the same and
> there are no other users on the system. When I look at the function calling
> this (insertAll, also in ExternalSorter), I see that spillToPartitionFiles
> is only called 700 times in both fast and slow runs, meaning that the
> function recursively calls itself very often. Because of the function name,
> I assume the system is spilling to disk. As I have sufficient memory, I
> assume that I forgot to set a certain memory setting. Anybody any idea
> which
> other setting I have to set, in order to not spill data in this scenario?
>
> Thanks,
>
> Tom
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Spilling-when-not-expected-tp11017.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11395a3004b4f305112620a2--

From dev-return-11975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 10:07:44 2015
Return-Path: <dev-return-11975-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C64DA17D11
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 10:07:44 +0000 (UTC)
Received: (qmail 52929 invoked by uid 500); 13 Mar 2015 10:07:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52843 invoked by uid 500); 13 Mar 2015 10:07:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52832 invoked by uid 99); 13 Mar 2015 10:07:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 10:07:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 10:07:20 +0000
Received: by wesw55 with SMTP id w55so22173965wes.3
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 03:06:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=VCGNIG7VTc2v1Taxwg9QzUeM1MrxVxe7dlgAVlVXSCU=;
        b=akuINPxdGLqUikCgJJKA+7VXunCziHPRmjnowuiOqvgBzpu0SBResCYY49qOr2TSFH
         PNe3EbjeaVdEUUXQAnplFNZNC4P8BadnrIY4M9poY55f9BD2cMVMLZo9hCAZTpcMsWxH
         N5iRmGEjbTiBAOZXmqCJwauSADzq9UC6WrY2ZDjRQiA/fZp8ejGtyib1Sr51T6DAMD+o
         AiDfNnLwlvsH/pZWQD1P3Hj/HY/oxh96EdZNZDKA7FQuYHY5MRMmlHkgDoshwdiw3RIS
         DvzuWJMvpBXrEEH65W3beG8IeWwdPMGJ4KDX+rOtVFgCPqrHdNeaXc9G1h1gcjWnk4Kr
         OZMA==
X-Gm-Message-State: ALoCoQmFdT6zedqtzOaIAPI88oIOpL5z+FNuiqejUuA3rmm7UlnAz7FX68BvYfbm0rBmC3/xw0gb
X-Received: by 10.180.79.1 with SMTP id f1mr26688469wix.24.1426241219604; Fri,
 13 Mar 2015 03:06:59 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 13 Mar 2015 03:06:39 -0700 (PDT)
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 13 Mar 2015 10:06:39 +0000
Message-ID: <CAMAsSdKHQ69Pebzv3mmo_ZO3L+-u=k9BMuTSnGhTAv3VWSvdxg@mail.gmail.com>
Subject: May we merge into branch-1.3 at this point?
To: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Is the release certain enough that we can resume merging into
branch-1.3 at this point? I have a number of back-ports queued up and
didn't want to merge in case another last RC was needed. I see a few
commits to the branch though.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 10:08:47 2015
Return-Path: <dev-return-11976-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A707C17D25
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 10:08:47 +0000 (UTC)
Received: (qmail 57631 invoked by uid 500); 13 Mar 2015 10:08:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57551 invoked by uid 500); 13 Mar 2015 10:08:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57540 invoked by uid 99); 13 Mar 2015 10:08:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 10:08:46 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dale__r@hotmail.com designates 65.54.190.91 as permitted sender)
Received: from [65.54.190.91] (HELO BAY004-OMC2S16.hotmail.com) (65.54.190.91)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 10:08:20 +0000
Received: from BAY180-W63 ([65.54.190.124]) by BAY004-OMC2S16.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Fri, 13 Mar 2015 03:07:57 -0700
X-TMN: [zsJU7ZU1O9TmdHTX85s+cinDKPvOap6PBxAkrN2drus=]
X-Originating-Email: [dale__r@hotmail.com]
Message-ID: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_a13bdb28-7899-4007-926c-b7639a215ce7_"
From: Dale Richardson <dale__r@hotmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Spark config option 'expression language' feedback request
Date: Fri, 13 Mar 2015 10:07:57 +0000
Importance: Normal
MIME-Version: 1.0
X-OriginalArrivalTime: 13 Mar 2015 10:07:57.0740 (UTC) FILETIME=[947B26C0:01D05D75]
X-Virus-Checked: Checked by ClamAV on apache.org

--_a13bdb28-7899-4007-926c-b7639a215ce7_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature to allow =
for Spark configuration options (whether on command line=2C environment var=
iable or a configuration file) to be specified via a simple expression lang=
uage.=0A=

=0A=
Such a feature has the following end-user benefits:=0A=
- Allows for the flexibility in specifying time intervals or byte quantitie=
s in appropriate and easy to follow units e.g. 1 week rather rather then 60=
4800 seconds
=0A=
- Allows for the scaling of a configuration option in relation to a system =
attributes. e.g.
=0A=
SPARK_WORKER_CORES =3D numCores - 1
=0A=
SPARK_WORKER_MEMORY =3D physicalMemoryBytes - 1.5 GB
=0A=
- Gives the ability to scale multiple configuration options together eg:
=0A=
spark.driver.memory =3D 0.75 * physicalMemoryBytes
=0A=
spark.driver.maxResultSize =3D spark.driver.memory * 0.8=0A=

=0A=
The following functions are currently supported by this PR:=0A=
NumCores:             Number of cores assigned to the JVM (usually =3D=3D P=
hysical machine cores)
PhysicalMemoryBytes:  Memory size of hosting machine
=0A=
JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
=0A=
JVMMaxMemoryBytes:    Maximum number of bytes of memory available to the JV=
M
=0A=
JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes=0A=

=0A=
I was wondering if anybody on the mailing list has any further ideas on oth=
er functions that could be useful to have when specifying spark configurati=
on options?
Regards=2CDale.=0A=
 		 	   		  =

--_a13bdb28-7899-4007-926c-b7639a215ce7_--

From dev-return-11977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 11:31:21 2015
Return-Path: <dev-return-11977-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C6DF17FF1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 11:31:21 +0000 (UTC)
Received: (qmail 22442 invoked by uid 500); 13 Mar 2015 11:31:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22358 invoked by uid 500); 13 Mar 2015 11:31:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22347 invoked by uid 99); 13 Mar 2015 11:31:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 11:31:20 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.220.42 as permitted sender)
Received: from [209.85.220.42] (HELO mail-pa0-f42.google.com) (209.85.220.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 11:31:15 +0000
Received: by padfa1 with SMTP id fa1so28827234pad.9
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 04:30:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:references:in-reply-to:mime-version
         :content-transfer-encoding:content-type:message-id:cc:from:subject
         :date:to;
        bh=Y+7xupax/OK81/ctrXOILQBMjDEGVq4BQlgcMM4Go6k=;
        b=JpNH1bnF39aVW/ysGsOCGZxTXb5kUOtVFl5oCwJo1E+woLCSgN71rr1jaBOdeWdLQB
         wDqZv6lEhVJbMcrxTm3DIBHPci8EPPTSMvi0A40ypPeUNQk05SaiBMJryVf2Z2V+ck0u
         ZRZHLHjaoDFSyJp+tqNPgV7P+LJjdfLUe7X8u9MrAJzZqkUz0yWPTW5PsImjLr5+IN82
         uAGYxMCgwmFvBTKlulzcBDYXCS28SmLbZ45cbLXjNhGUgcHxsyY+duBF8SUiFBo6jzPz
         TjZqnGeXW9rzdkNGWobxz/GFDPdzEJn2mXPuYSVpcy6Ri2jHzXVEeNIcovGFAWxvpV3g
         uu7w==
X-Gm-Message-State: ALoCoQkZKMonWB5AbeO1jNJTu1JZtDb547FcGlgoADXIjOKD2RthDGYV7pTOSM0cUo7NQNdjhg5C
X-Received: by 10.66.55.42 with SMTP id o10mr51303938pap.148.1426246210409;
        Fri, 13 Mar 2015 04:30:10 -0700 (PDT)
Received: from [192.168.2.11] (c-24-5-225-89.hsd1.ca.comcast.net. [24.5.225.89])
        by mx.google.com with ESMTPSA id z4sm3100978pdn.46.2015.03.13.04.30.09
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 13 Mar 2015 04:30:09 -0700 (PDT)
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net> <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com> <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com> <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com> <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net> <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE18268@G4W3292.americas.hpqcorp.net> <CAKx7Bf_kyhjjzvuNwdZOBs=iwF2BCXd-CAESG4H2+98t5tU1aQ@mail.gmail.com> <1426218624554-11021.post@n3.nabble.com> <CAPh_B=aZsWY6DVHgPX4JG-FP0eQjPREN=DHPK22gU+uwKrhE-g@mail.gmail.com>
In-Reply-To: <CAPh_B=aZsWY6DVHgPX4JG-FP0eQjPREN=DHPK22gU+uwKrhE-g@mail.gmail.com>
Mime-Version: 1.0 (iPad Mail 8F191)
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii
Message-Id: <8BC2F386-1660-4205-9C92-BE913BA41EF8@alpinenow.com>
Cc: jfcanny <canny@berkeley.edu>,
 "dev@spark.apache.org" <dev@spark.apache.org>
X-Mailer: iPad Mail (8F191)
From: Chester At Work <chester@alpinenow.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
Date: Fri, 13 Mar 2015 04:31:24 -0700
To: Reynold Xin <rxin@databricks.com>
X-Virus-Checked: Checked by ClamAV on apache.org

Reyonld,=20

    Prof Canny gives me the slides yesterday I will posted the link to the s=
lides to both SF BIg Analytics and SF Machine Learning meetups.

Chester

Sent from my iPad

On Mar 12, 2015, at 22:53, Reynold Xin <rxin@databricks.com> wrote:

> Thanks for chiming in, John. I missed your meetup last night - do you have=

> any writeups or slides about roofline design? In particular, I'm curious
> about what optimizations are available for power-law dense * sparse? (I
> don't have any background in optimizations)
>=20
>=20
>=20
> On Thu, Mar 12, 2015 at 8:50 PM, jfcanny <canny@berkeley.edu> wrote:
>=20
>> If you're contemplating GPU acceleration in Spark, its important to look
>> beyond BLAS. Dense BLAS probably account for only 10% of the cycles in th=
e
>> datasets we've tested in BIDMach, and we've tried to make them
>> representative of industry machine learning workloads. Unless you're
>> crunching images or audio, the majority of data will be very sparse and
>> power law distributed. You need a good sparse BLAS, and in practice it
>> seems
>> like you need a sparse BLAS tailored for power-law data. We had to write
>> our
>> own since the NVIDIA libraries didnt perform well on typical power-law
>> data.
>> Intel MKL sparse BLAS also have issues and we only use some of them.
>>=20
>> You also need 2D reductions, scan operations, slicing, element-wise
>> transcendental functions and operators, many kinds of sort, random number=

>> generators etc, and some kind of memory management strategy. Some of this=

>> was layered on top of Thrust in BIDMat, but most had to be written from
>> scratch. Its all been rooflined, typically to memory throughput of curren=
t
>> GPUs (around 200 GB/s).
>>=20
>> When you have all this you can write Learning Algorithms in the same
>> high-level primitives available in Breeze or Numpy/Scipy. Its literally t=
he
>> same in BIDMat, since the generic matrix operations are implemented on bo=
th
>> CPU and GPU, so the same code runs on either platform.
>>=20
>> A lesser known fact is that GPUs are around 10x faster for *all* those
>> operations, not just dense BLAS. Its mostly due to faster streaming memor=
y
>> speeds, but some kernels (random number generation and transcendentals) a=
re
>> more than an order of magnitude thanks to some specialized hardware for
>> power series on the GPU chip.
>>=20
>> When you have all this there is no need to move data back and forth acros=
s
>> the PCI bus. The CPU only has to pull chunks of data off disk, unpack the=
m,
>> and feed them to the available GPUs. Most models fit comfortably in GPU
>> memory these days (4-12 GB). With minibatch algorithms you can push TBs o=
f
>> data through the GPU this way.
>>=20
>>=20
>>=20
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-with=
in-Spark-boosting-linear-algebra-tp10481p11021.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>=20
>>=20

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 15:21:01 2015
Return-Path: <dev-return-11978-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A678517CDC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 15:21:01 +0000 (UTC)
Received: (qmail 67331 invoked by uid 500); 13 Mar 2015 15:21:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67253 invoked by uid 500); 13 Mar 2015 15:21:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67241 invoked by uid 99); 13 Mar 2015 15:20:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 15:20:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 15:20:54 +0000
Received: by qgfh3 with SMTP id h3so26556302qgf.13
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 08:20:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mOuQuKu5BO6WwrwlEY0dahLQOapYRMlGIKKr1VfkVPM=;
        b=JeZGRn3JC0s8NuARNsPzrZ3V6zYAg/fieKRdjGx8XHmS3mH7PtkQ2kHUsJrDTSCTK0
         eDwtGC9jZRkX0YECcgF6HwNvHLFJTI4/ek7tEhH0QVZ0jr2DcjtTRv3gND3t+vcCQcrZ
         3u1bW1j28K1Dzj0CM8CabWRy1NG5TsumryhyoGDcEswcrXCdTSJ6ScuEiP+3OzF7Nq5O
         AMHu9AoghBATlOICmNxx/XhEodiKezlBJyA7mPU6Ux6kQdTIWg8Zik7l1AhgJdetaeLh
         j51jFvTv6+Mz9YNm/Lscrg89knhwXM8RPksSv0kYV/nk182Xykpk0+7xa6hPF+Iio0b9
         f2kg==
MIME-Version: 1.0
X-Received: by 10.140.129.65 with SMTP id 62mr62497042qhb.11.1426260033786;
 Fri, 13 Mar 2015 08:20:33 -0700 (PDT)
Received: by 10.140.33.131 with HTTP; Fri, 13 Mar 2015 08:20:33 -0700 (PDT)
In-Reply-To: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>
References: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>
Date: Fri, 13 Mar 2015 08:20:33 -0700
Message-ID: <CAJiQeYJ7k4Obu+TBzfTeU3wgz=ou5+vix0GLBTq_nN045mM0RQ@mail.gmail.com>
Subject: Re: Spark config option 'expression language' feedback request
From: Mridul Muralidharan <mridul@gmail.com>
To: Dale Richardson <dale__r@hotmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113525f2abeea105112d0abd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113525f2abeea105112d0abd
Content-Type: text/plain; charset=UTF-8

I am curious how you are going to support these over mesos and yarn.
Any configure change like this should be applicable to all of them, not
just local and standalone modes.

Regards
Mridul

On Friday, March 13, 2015, Dale Richardson <dale__r@hotmail.com> wrote:

>
>
>
>
>
>
>
>
>
>
>
> PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature to
> allow for Spark configuration options (whether on command line, environment
> variable or a configuration file) to be specified via a simple expression
> language.
>
>
> Such a feature has the following end-user benefits:
> - Allows for the flexibility in specifying time intervals or byte
> quantities in appropriate and easy to follow units e.g. 1 week rather
> rather then 604800 seconds
>
> - Allows for the scaling of a configuration option in relation to a system
> attributes. e.g.
>
> SPARK_WORKER_CORES = numCores - 1
>
> SPARK_WORKER_MEMORY = physicalMemoryBytes - 1.5 GB
>
> - Gives the ability to scale multiple configuration options together eg:
>
> spark.driver.memory = 0.75 * physicalMemoryBytes
>
> spark.driver.maxResultSize = spark.driver.memory * 0.8
>
>
> The following functions are currently supported by this PR:
> NumCores:             Number of cores assigned to the JVM (usually ==
> Physical machine cores)
> PhysicalMemoryBytes:  Memory size of hosting machine
>
> JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
>
> JVMMaxMemoryBytes:    Maximum number of bytes of memory available to the
> JVM
>
> JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
>
>
> I was wondering if anybody on the mailing list has any further ideas on
> other functions that could be useful to have when specifying spark
> configuration options?
> Regards,Dale.
>

--001a113525f2abeea105112d0abd--

From dev-return-11979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 16:21:45 2015
Return-Path: <dev-return-11979-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81E2917289
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 16:21:45 +0000 (UTC)
Received: (qmail 59571 invoked by uid 500); 13 Mar 2015 16:21:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59498 invoked by uid 500); 13 Mar 2015 16:21:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59481 invoked by uid 99); 13 Mar 2015 16:21:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:21:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.160.175 as permitted sender)
Received: from [209.85.160.175] (HELO mail-yk0-f175.google.com) (209.85.160.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:21:39 +0000
Received: by ykp9 with SMTP id 9so10958747ykp.3
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 09:20:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :content-type;
        bh=zFCR4D+Z51WF9bbL4ItEs2okX17KGlWnOm6PjOxYIW8=;
        b=WBXZcM8o306eyMOVqm/7iS5NLgxce98ON40+EYjaKMVjPuyL6ZaDBnxZfhKL/bNquv
         u910aCrZLlZqIIRX4pAHMBHQd7PYF7X2oDVZ1JyhojPi6WP20uX/7oxqIItF26XhNHpF
         vOLk7ZHweckxEc2opTPJgxloFPkjSr/LaBiUZC15RdmAD9AdqcR+0qf6Oz0P+CBYHpUd
         uouW4oGX/bEZu6p2pY9qgnlSewqcY00pIfIwEhQIzufIa8DZKaDNGY5EFsXzASYukRl7
         xi638eYzK6jbkXTX/+mgvVoJsAMvagRiWnmZkAs/31TspemMqZXVHU8tGCswQFEv0Cww
         YyaQ==
X-Received: by 10.170.187.5 with SMTP id d5mr52366262yke.20.1426263633565;
 Fri, 13 Mar 2015 09:20:33 -0700 (PDT)
MIME-Version: 1.0
References: <CAMAsSdKHQ69Pebzv3mmo_ZO3L+-u=k9BMuTSnGhTAv3VWSvdxg@mail.gmail.com>
In-Reply-To: <CAMAsSdKHQ69Pebzv3mmo_ZO3L+-u=k9BMuTSnGhTAv3VWSvdxg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 13 Mar 2015 16:20:32 +0000
Message-ID: <CAOhmDzfdFa2E9DFiC65Uj2c13MjoXfKT1DXqyfmFgRZvWp90kw@mail.gmail.com>
Subject: Re: May we merge into branch-1.3 at this point?
To: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a67863c1e8e05112de14a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a67863c1e8e05112de14a
Content-Type: text/plain; charset=UTF-8

Looks like the release is out:
http://spark.apache.org/releases/spark-release-1-3-0.html

Though, interestingly, I think we are missing the appropriate v1.3.0 tag:
https://github.com/apache/spark/releases

Nick

On Fri, Mar 13, 2015 at 6:07 AM Sean Owen <sowen@cloudera.com> wrote:

> Is the release certain enough that we can resume merging into
> branch-1.3 at this point? I have a number of back-ports queued up and
> didn't want to merge in case another last RC was needed. I see a few
> commits to the branch though.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113a67863c1e8e05112de14a--

From dev-return-11980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 16:23:03 2015
Return-Path: <dev-return-11980-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EEC4317296
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 16:23:02 +0000 (UTC)
Received: (qmail 62329 invoked by uid 500); 13 Mar 2015 16:23:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62247 invoked by uid 500); 13 Mar 2015 16:23:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62236 invoked by uid 99); 13 Mar 2015 16:23:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:23:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:22:56 +0000
Received: by wghk14 with SMTP id k14so24329495wgh.7
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 09:22:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=O1a5zuiT6KFXOkXkWq6xaQvSt4ZvT3xqxr2VOf8B9nw=;
        b=LORPMA/eMz5tdPWLIz/OAo+t8oKI5mpXNNiat/Dp1ihAUm5artUh4qq5zt4VYVsrJD
         3kGi4JLleMpubYQ1yrrum4YQKVHq+fNMmbRqxmYoShLGYC9bE4TheXVDJrAcZEdepNSd
         1VAwhx8NQ3Z7jU3l80PXqYQEDFDJW8PkxLR4Ple6Eq36uIQh8XfY8zXipYq5E/fgqCMx
         Brn5i7t+/rsyBFnRc1aZViyzbKyx0DomuEuOLlGlXAsIgWLCKcQbV66Cclv1ib5PhHzb
         gfYMbllgTkLpKNT+NNTXhJOIECSyRT7fLPm6vcZVupwFi8mZ8IlAMWF/FWLtFSHDukns
         WnXw==
X-Gm-Message-State: ALoCoQlgGRWO7PDFikM4doPJ5ckSq3yxNU2uaFe9p3D///YmR4wg38qcV3yzBM2LACChM0/Whnzs
X-Received: by 10.194.185.9 with SMTP id ey9mr99871794wjc.135.1426263755385;
 Fri, 13 Mar 2015 09:22:35 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 13 Mar 2015 09:22:15 -0700 (PDT)
In-Reply-To: <CAOhmDzfdFa2E9DFiC65Uj2c13MjoXfKT1DXqyfmFgRZvWp90kw@mail.gmail.com>
References: <CAMAsSdKHQ69Pebzv3mmo_ZO3L+-u=k9BMuTSnGhTAv3VWSvdxg@mail.gmail.com>
 <CAOhmDzfdFa2E9DFiC65Uj2c13MjoXfKT1DXqyfmFgRZvWp90kw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 13 Mar 2015 16:22:15 +0000
Message-ID: <CAMAsSdLPYxApYZJu=qEbm57u7+_afFYt2Esyr0eUHFDnKj7ZOg@mail.gmail.com>
Subject: Re: May we merge into branch-1.3 at this point?
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, I'm guessing that is all happening quite literally as we speak.
The Apache git tag is the one of reference:
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc

Open season on 1.3 branch then...

On Fri, Mar 13, 2015 at 4:20 PM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> Looks like the release is out:
> http://spark.apache.org/releases/spark-release-1-3-0.html
>
> Though, interestingly, I think we are missing the appropriate v1.3.0 tag:
> https://github.com/apache/spark/releases
>
> Nick
>
> On Fri, Mar 13, 2015 at 6:07 AM Sean Owen <sowen@cloudera.com> wrote:
>>
>> Is the release certain enough that we can resume merging into
>> branch-1.3 at this point? I have a number of back-ports queued up and
>> didn't want to merge in case another last RC was needed. I see a few
>> commits to the branch though.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 16:32:26 2015
Return-Path: <dev-return-11981-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D43EE17308
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 16:32:26 +0000 (UTC)
Received: (qmail 88806 invoked by uid 500); 13 Mar 2015 16:32:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88731 invoked by uid 500); 13 Mar 2015 16:32:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88719 invoked by uid 99); 13 Mar 2015 16:32:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:32:25 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:32:00 +0000
Received: by qgfh3 with SMTP id h3so27100430qgf.13
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 09:31:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=bjNkudAGke1pQ12zev6yKm0sbzN6hpzEIp4FKIZC8yM=;
        b=Ud6O8NQeJucavIMkEG7cTDULU+ocAXBRnulHGzIZKYlMAUOuHMezBBZZgc+KsMSpQO
         a5/m5Bk8W7ogtb/CLwTWUvJu+w51i3dkRBSO5Uwf5pCKKub0KCTxLy30NeC2CY7AxYMC
         /xEGfDZLme8d/1B5iHiaKhd742XuJ5wXsMoB7qBNlVMXF7+K4pea/kuoww+w9/+R5isP
         Uu2F8wQb+VpJvjOmbMDX5nB7KtVebQBPHRMKvhN3d63uo3k0eeCQroiKeaDbTdXwSOsL
         x4jbFH7VjkuPyKFgutZNSNMXx61bTYV8LDpe4dfxZwlsDwzWsOEXCa2iMP4C78GEyW5O
         eJfA==
MIME-Version: 1.0
X-Received: by 10.229.214.199 with SMTP id hb7mr42379826qcb.12.1426264318990;
 Fri, 13 Mar 2015 09:31:58 -0700 (PDT)
Received: by 10.140.33.131 with HTTP; Fri, 13 Mar 2015 09:31:58 -0700 (PDT)
In-Reply-To: <CAMAsSdLPYxApYZJu=qEbm57u7+_afFYt2Esyr0eUHFDnKj7ZOg@mail.gmail.com>
References: <CAMAsSdKHQ69Pebzv3mmo_ZO3L+-u=k9BMuTSnGhTAv3VWSvdxg@mail.gmail.com>
	<CAOhmDzfdFa2E9DFiC65Uj2c13MjoXfKT1DXqyfmFgRZvWp90kw@mail.gmail.com>
	<CAMAsSdLPYxApYZJu=qEbm57u7+_afFYt2Esyr0eUHFDnKj7ZOg@mail.gmail.com>
Date: Fri, 13 Mar 2015 09:31:58 -0700
Message-ID: <CAJiQeYL+cZ0NJn=--+vARo-t1+0=_11bJvmHhSQcVrQCKZnx7Q@mail.gmail.com>
Subject: Re: May we merge into branch-1.3 at this point?
From: Mridul Muralidharan <mridul@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Nicholas Chammas <nicholas.chammas@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132f7ea16e48405112e0af3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132f7ea16e48405112e0af3
Content-Type: text/plain; charset=UTF-8

Who is managing 1.3 release ? You might want to coordinate with them before
porting changes to branch.

Regards
Mridul

On Friday, March 13, 2015, Sean Owen <sowen@cloudera.com> wrote:

> Yeah, I'm guessing that is all happening quite literally as we speak.
> The Apache git tag is the one of reference:
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
>
> Open season on 1.3 branch then...
>
> On Fri, Mar 13, 2015 at 4:20 PM, Nicholas Chammas
> <nicholas.chammas@gmail.com <javascript:;>> wrote:
> > Looks like the release is out:
> > http://spark.apache.org/releases/spark-release-1-3-0.html
> >
> > Though, interestingly, I think we are missing the appropriate v1.3.0 tag:
> > https://github.com/apache/spark/releases
> >
> > Nick
> >
> > On Fri, Mar 13, 2015 at 6:07 AM Sean Owen <sowen@cloudera.com
> <javascript:;>> wrote:
> >>
> >> Is the release certain enough that we can resume merging into
> >> branch-1.3 at this point? I have a number of back-ports queued up and
> >> didn't want to merge in case another last RC was needed. I see a few
> >> commits to the branch though.
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
> >> For additional commands, e-mail: dev-help@spark.apache.org
> <javascript:;>
> >>
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
> For additional commands, e-mail: dev-help@spark.apache.org <javascript:;>
>
>

--001a1132f7ea16e48405112e0af3--

From dev-return-11982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 16:34:28 2015
Return-Path: <dev-return-11982-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D0B417321
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 16:34:28 +0000 (UTC)
Received: (qmail 94420 invoked by uid 500); 13 Mar 2015 16:34:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94340 invoked by uid 500); 13 Mar 2015 16:34:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94328 invoked by uid 99); 13 Mar 2015 16:34:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:34:24 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of thubregtsen@gmail.com designates 209.85.215.50 as permitted sender)
Received: from [209.85.215.50] (HELO mail-la0-f50.google.com) (209.85.215.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:34:19 +0000
Received: by lams18 with SMTP id s18so23847133lam.2
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 09:33:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=t40lqqcjP5MJGK9VExgfMnT5cZ+FXpofci0xK/aMtQo=;
        b=e2Qr4bKfHsDXzfuTiCyFULVXGjgEidoyQq4YNduOA7C2Gd3Y5PhiGymqIv0wakllNp
         s4EwFDA6g3LkmB1ri3AkNAaKLsUjt3DXt5DB8H5BjxRZFLMX64eHDgABXPo0fYtUPL9S
         t6cRouk6wqrECvR1GCHd0PtNFG08pWnxFLtdZY1XFpBLmg7+Dx4vazsXu+yf0zYULgA/
         PlpLTW8otvl+9D0rVw+NkEFOl8cgAKswAH4Jn2ovK+ALig/vv4QQIRK5QE7w53PEKilO
         lMT/ul9eYQm2PhFE8cXOnWqPzMMW9/A6P6fIEa7r5W7Ki9GjLzxuTEl/SxlRGErCXmCW
         jEsg==
X-Received: by 10.152.43.51 with SMTP id t19mr43453823lal.73.1426264438967;
 Fri, 13 Mar 2015 09:33:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.130.203 with HTTP; Fri, 13 Mar 2015 09:33:38 -0700 (PDT)
In-Reply-To: <CAPh_B=Z_P8efOyGvEDmP9_ALQy=Dz334Sby_6fVp2Xn1VXZAPA@mail.gmail.com>
References: <1426201751505-11017.post@n3.nabble.com> <CAPh_B=Z_P8efOyGvEDmP9_ALQy=Dz334Sby_6fVp2Xn1VXZAPA@mail.gmail.com>
From: Tom Hubregtsen <thubregtsen@gmail.com>
Date: Fri, 13 Mar 2015 11:33:38 -0500
Message-ID: <CA+bq_A+uQbYLJrqyzd0SyBVaYGDy=kHFF173_oN+dnsBjqa=Gw@mail.gmail.com>
Subject: Re: Spilling when not expected
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c235783d946805112e11ce
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c235783d946805112e11ce
Content-Type: text/plain; charset=UTF-8

I use the spark-submit script and the config files in a conf directory. I
see the memory settings reflected in the stdout, as well as in the webUI.
(it prints all variables from spark-default.conf, and metions I have 540GB
free memory available when trying to store a broadcast variable or RDD). I
also run "ps -aux | grep java | grep th", which show me that I called java
with "-Xms1000g -Xmx1000g"

I also tested if these numbers are realistic for the J9 JVM. Outside of
Spark, when setting just the initial heapsize (Xms), it gives an error, but
if I also define the maximum option with it (Xmx), it seems to us that it
is accepting it. Also, in IBM's J9 health center, I see it reserve the
900g, and use up to 68g.

Thanks,

Tom

On 13 March 2015 at 02:05, Reynold Xin <rxin@databricks.com> wrote:

> How did you run the Spark command? Maybe the memory setting didn't
> actually apply? How much memory does the web ui say is available?
>
> BTW - I don't think any JVM can actually handle 700G heap ... (maybe Zing).
>
> On Thu, Mar 12, 2015 at 4:09 PM, Tom Hubregtsen <thubregtsen@gmail.com>
> wrote:
>
>> Hi all,
>>
>> I'm running the teraSort benchmark with a relative small input set: 5GB.
>> During profiling, I can see I am using a total of 68GB. I've got a
>> terabyte
>> of memory in my system, and set
>> spark.executor.memory 900g
>> spark.driver.memory 900g
>> I use the default for
>> spark.shuffle.memoryFraction
>> spark.storage.memoryFraction
>> I believe that I now have 0.2*900=180GB for shuffle and 0.6*900=540GB for
>> storage.
>>
>> I noticed a lot of variation in runtime (under the same load), and tracked
>> this down to this function in
>> core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala
>>   private def spillToPartitionFiles(collection:
>> SizeTrackingPairCollection[(Int, K), C]): Unit = {
>>     spillToPartitionFiles(collection.iterator)
>>   }
>> In a slow run, it would loop through this function 12000 times, in a fast
>> run only 700 times, even though the settings in both runs are the same and
>> there are no other users on the system. When I look at the function
>> calling
>> this (insertAll, also in ExternalSorter), I see that spillToPartitionFiles
>> is only called 700 times in both fast and slow runs, meaning that the
>> function recursively calls itself very often. Because of the function
>> name,
>> I assume the system is spilling to disk. As I have sufficient memory, I
>> assume that I forgot to set a certain memory setting. Anybody any idea
>> which
>> other setting I have to set, in order to not spill data in this scenario?
>>
>> Thanks,
>>
>> Tom
>>
>>
>>
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/Spilling-when-not-expected-tp11017.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--001a11c235783d946805112e11ce--

From dev-return-11983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 16:44:59 2015
Return-Path: <dev-return-11983-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 71FA2173AB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 16:44:59 +0000 (UTC)
Received: (qmail 28954 invoked by uid 500); 13 Mar 2015 16:44:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28878 invoked by uid 500); 13 Mar 2015 16:44:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28867 invoked by uid 99); 13 Mar 2015 16:44:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:44:57 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of canny@berkeley.edu does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 16:44:31 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 5E9E2172D36F
	for <dev@spark.apache.org>; Fri, 13 Mar 2015 09:43:36 -0700 (PDT)
Date: Fri, 13 Mar 2015 09:43:28 -0700 (MST)
From: jfcanny <canny@berkeley.edu>
To: dev@spark.apache.org
Message-ID: <550313A1.8030506@berkeley.edu>
In-Reply-To: <CAPh_B=aZsWY6DVHgPX4JG-FP0eQjPREN=DHPK22gU+uwKrhE-g@mail.gmail.com>
References: <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com> <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com> <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com> <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net> <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE18268@G4W3292.americas.hpqcorp.net> <CAKx7Bf_kyhjjzvuNwdZOBs=iwF2BCXd-CAESG4H2+98t5tU1aQ@mail.gmail.com> <1426218624554-11021.post@n3.nabble.com> <CAPh_B=aZsWY6DVHgPX4JG-FP0eQjPREN=DHPK22gU+uwKrhE-g@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_156004_69068619.1426265008128"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_156004_69068619.1426265008128
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Reynold,
I left Chester with a copy of the slides, so I assume they'll be posted 
on the SF ML or Big Data sites. We have a draft paper under review. I 
can ask the co-authors about arxiv'ing it.

We have a few heuristics for power-law data. One of them is to keep the 
feature set sorted by frequency. Power-law data has roughly the same 
mass in each power-of-two range of feature frequency. By keeping the 
most frequent features together, you get a lot more value out of the 
caches on the device (even GPUs have them, albeit smaller ones). e.g. 
with 100 million features, 1/2 of the feature instances will be in the 
range 1...,10,000. If they're consecutive they will all hit a fast 
cache. Another 1/4 will be in 1,...,1,000,000 hitting the next cache etc.

Another is to subdivide sparse matrices using the vector of elements 
rather than rows or columns. Splitting power-law matrices by either rows 
or columns gives very uneven splits. That means we store sparse matrices 
in coordinate form rather than compressed row or column format.

Other than that, rooflining gives you a goal that you should be able to 
reach. If you arent at the limit, just knowing that gives you a target 
to aim at. You can try profiling the kernel to figure out why its slower 
than it should be. There are a few common reasons (low occupancy, 
imbalanced thread blocks, thread divergence) that you can discover with 
the profiler. Then hopefully you can solve them.

-John


On 3/12/2015 10:56 PM, rxin [via Apache Spark Developers List] wrote:
> Thanks for chiming in, John. I missed your meetup last night - do you 
> have
> any writeups or slides about roofline design? In particular, I'm curious
> about what optimizations are available for power-law dense * sparse? (I
> don't have any background in optimizations)
>
>
>
> On Thu, Mar 12, 2015 at 8:50 PM, jfcanny <[hidden email] 
> </user/SendEmail.jtp?type=node&node=11022&i=0>> wrote:
>
> > If you're contemplating GPU acceleration in Spark, its important to 
> look
> > beyond BLAS. Dense BLAS probably account for only 10% of the cycles 
> in the
> > datasets we've tested in BIDMach, and we've tried to make them
> > representative of industry machine learning workloads. Unless you're
> > crunching images or audio, the majority of data will be very sparse and
> > power law distributed. You need a good sparse BLAS, and in practice it
> > seems
> > like you need a sparse BLAS tailored for power-law data. We had to 
> write
> > our
> > own since the NVIDIA libraries didnt perform well on typical power-law
> > data.
> > Intel MKL sparse BLAS also have issues and we only use some of them.
> >
> > You also need 2D reductions, scan operations, slicing, element-wise
> > transcendental functions and operators, many kinds of sort, random 
> number
> > generators etc, and some kind of memory management strategy. Some of 
> this
> > was layered on top of Thrust in BIDMat, but most had to be written from
> > scratch. Its all been rooflined, typically to memory throughput of 
> current
> > GPUs (around 200 GB/s).
> >
> > When you have all this you can write Learning Algorithms in the same
> > high-level primitives available in Breeze or Numpy/Scipy. Its 
> literally the
> > same in BIDMat, since the generic matrix operations are implemented 
> on both
> > CPU and GPU, so the same code runs on either platform.
> >
> > A lesser known fact is that GPUs are around 10x faster for *all* those
> > operations, not just dense BLAS. Its mostly due to faster streaming 
> memory
> > speeds, but some kernels (random number generation and 
> transcendentals) are
> > more than an order of magnitude thanks to some specialized hardware for
> > power series on the GPU chip.
> >
> > When you have all this there is no need to move data back and forth 
> across
> > the PCI bus. The CPU only has to pull chunks of data off disk, 
> unpack them,
> > and feed them to the available GPUs. Most models fit comfortably in GPU
> > memory these days (4-12 GB). With minibatch algorithms you can push 
> TBs of
> > data through the GPU this way.
> >
> >
> >
> > --
> > View this message in context:
> > 
> http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-within-Spark-boosting-linear-algebra-tp10481p11021.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: [hidden email] 
> </user/SendEmail.jtp?type=node&node=11022&i=1>
> > For additional commands, e-mail: [hidden email] 
> </user/SendEmail.jtp?type=node&node=11022&i=2>
> >
> >
>
>
> ------------------------------------------------------------------------
> If you reply to this email, your message will be added to the 
> discussion below:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-within-Spark-boosting-linear-algebra-tp10481p11022.html 
>
> To unsubscribe from Using CUDA within Spark / boosting linear algebra, 
> click here 
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=10481&code=Y2FubnlAYmVya2VsZXkuZWR1fDEwNDgxfC00MzIwNjcxNzY=>.
> NAML 
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml> 
>





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-within-Spark-boosting-linear-algebra-tp10481p11036.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_156004_69068619.1426265008128--

From dev-return-11984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 17:01:09 2015
Return-Path: <dev-return-11984-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD82A174A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 17:01:09 +0000 (UTC)
Received: (qmail 88411 invoked by uid 500); 13 Mar 2015 17:00:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88251 invoked by uid 500); 13 Mar 2015 17:00:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87575 invoked by uid 99); 13 Mar 2015 17:00:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:00:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:00:16 +0000
Received: by obcvb8 with SMTP id vb8so20929120obc.10;
        Fri, 13 Mar 2015 10:00:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=6skhSt31D3yXVvT2CnoBf0FeY10KCtqmiFuJhfEVVic=;
        b=ryDmaFvgQ2gYLH6NVkvx4zCx1e+bFRNpZm+WvEePWn2gPk+A6c1SOEZbo4WhQTuHbl
         pXtESwUTqmsTph95HI7jYyza7/PcVNjKtYh0wp+4aYjSy6Qu7vi+SV7AKWl9RP9WdpMo
         fiHn2DnA5bh1YNbkjBwEGqSJXKMrBiAd7ZOVp42WSIYV1CYUpT6sm56ynd8MtoYQ+zF5
         VVXN1Gx7NhQ50AWGTI4bkb8eGkFpC/Zp7w6PCkMjO9crBwcvBF4gYQ+4oat9YFVVPW45
         dDFYmxOYzIIv9iM7qX9TP2JlKxut4b9jpyf3de6GAxMXImxrgh0InVT6MXG03AR2cBtP
         QUEQ==
MIME-Version: 1.0
X-Received: by 10.202.97.130 with SMTP id v124mr37241133oib.34.1426266014712;
 Fri, 13 Mar 2015 10:00:14 -0700 (PDT)
Received: by 10.202.79.81 with HTTP; Fri, 13 Mar 2015 10:00:14 -0700 (PDT)
Date: Fri, 13 Mar 2015 10:00:14 -0700
Message-ID: <CABPQxsvzVsqcAKq=beY8ycYNBjqV7FxvmLTNJMTqzebTD5sonQ@mail.gmail.com>
Subject: [ANNOUNCE] Announcing Spark 1.3!
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi All,

I'm happy to announce the availability of Spark 1.3.0! Spark 1.3.0 is
the fourth release on the API-compatible 1.X line. It is Spark's
largest release ever, with contributions from 172 developers and more
than 1,000 commits!

Visit the release notes [1] to read about the new features, or
download [2] the release today.

For errata in the contributions or release notes, please e-mail me
*directly* (not on-list).

Thanks to everyone who helped work on this release!

[1] http://spark.apache.org/releases/spark-release-1-3-0.html
[2] http://spark.apache.org/downloads.html

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 17:13:35 2015
Return-Path: <dev-return-11985-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A36BA1758D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 17:13:35 +0000 (UTC)
Received: (qmail 37123 invoked by uid 500); 13 Mar 2015 17:13:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36968 invoked by uid 500); 13 Mar 2015 17:13:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36258 invoked by uid 99); 13 Mar 2015 17:13:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:13:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kushal.datta@gmail.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:13:25 +0000
Received: by qgdz107 with SMTP id z107so27417603qgd.4;
        Fri, 13 Mar 2015 10:13:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=bdCQy0E3Hel7FQmEYLvPxYz0NVkdQjDwokj3/FdAft0=;
        b=sXXQ4J/EC4gKLyPSM8PQSbwoRUFR+MlYuhVk7PLXpTMOFZ/izOa1huL3+beks9ArHL
         mnj4pFX3reGVw2EJE7TQ2a/YLvEhTPpBthXWk+2GzUc8Ov13v46BASv02b6YZNpVpySn
         aoxepqfu2nG6PMhRu4piDAF+3tqaN1JyZ2QJSfkODOEE40j4zW6Z3GG/c7mF7Kl0O59/
         8ylYIuDWj/yzTJCzt4Ax2PzR7oK+t0vnK23IGBoj2z0X7I5XrqQqN2LWy9jWKQfVihdn
         mFp7shMQPDU7W/EMuCGgnky/wK4IiCqWBazicBNjDV6V+faakRAkKXLUPstmq6NtCf7e
         oY+w==
X-Received: by 10.140.150.19 with SMTP id 19mr14604928qhw.69.1426266784940;
 Fri, 13 Mar 2015 10:13:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.85.52 with HTTP; Fri, 13 Mar 2015 10:12:44 -0700 (PDT)
In-Reply-To: <CABPQxsvzVsqcAKq=beY8ycYNBjqV7FxvmLTNJMTqzebTD5sonQ@mail.gmail.com>
References: <CABPQxsvzVsqcAKq=beY8ycYNBjqV7FxvmLTNJMTqzebTD5sonQ@mail.gmail.com>
From: Kushal Datta <kushal.datta@gmail.com>
Date: Fri, 13 Mar 2015 10:12:44 -0700
Message-ID: <CANjHi9o8VZo+9w4LBLjTOAuoORRRtkUf7KddhSnLWBBY-3J3Xw@mail.gmail.com>
Subject: Re: [ANNOUNCE] Announcing Spark 1.3!
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135a274124a9705112e9d38
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135a274124a9705112e9d38
Content-Type: text/plain; charset=UTF-8

Kudos to the whole team for such a significant achievement!

On Fri, Mar 13, 2015 at 10:00 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Hi All,
>
> I'm happy to announce the availability of Spark 1.3.0! Spark 1.3.0 is
> the fourth release on the API-compatible 1.X line. It is Spark's
> largest release ever, with contributions from 172 developers and more
> than 1,000 commits!
>
> Visit the release notes [1] to read about the new features, or
> download [2] the release today.
>
> For errata in the contributions or release notes, please e-mail me
> *directly* (not on-list).
>
> Thanks to everyone who helped work on this release!
>
> [1] http://spark.apache.org/releases/spark-release-1-3-0.html
> [2] http://spark.apache.org/downloads.html
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1135a274124a9705112e9d38--

From dev-return-11986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 17:14:05 2015
Return-Path: <dev-return-11986-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D0F117594
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 17:14:05 +0000 (UTC)
Received: (qmail 45635 invoked by uid 500); 13 Mar 2015 17:14:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45552 invoked by uid 500); 13 Mar 2015 17:14:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45541 invoked by uid 99); 13 Mar 2015 17:14:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:14:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.180 as permitted sender)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:13:38 +0000
Received: by lbdu10 with SMTP id u10so24158068lbd.4
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 10:12:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=foDssTaoHumdpLHYYxQtBG1cHXa1KenrlnuEbFUuk4k=;
        b=lJi0U/mOKpGnvfvDIRNIybNYcKKZjf90UvUg3ibSremgZ295/UXdKmI5v3IMNVR++B
         gEXC8h62cYBpTj6isJKtYOleiBl+b0GkKpHRNYJ1taMoKUPfwHAYgPiru76YPg5gZUzo
         XZ0GhpsXpUxedvstkUX2lRXbH69EiRT2SFvDdG2sHF5UwWhWblnNx/siREpUCy0Xt7FM
         SmMt9VFZHZp4fal1Fj1w/hv5P9le0y/r+qy8/xkYbSrxZM2+ePlSL1/CzbYhBQJT+Qm4
         /RmO82d2zjWec9sPW4srpbriSSX51nRHWamvGRdo03939FposfKCJwPeDWzdYtMl/v2q
         WgwQ==
X-Gm-Message-State: ALoCoQleci78aQ3ubJEmeK51Asl6BirXo3rkSuy/3U4zIq0l3awEMc4dxbm3gyUuqY5S6qOky8AB
X-Received: by 10.152.180.202 with SMTP id dq10mr42666694lac.74.1426266770582;
 Fri, 13 Mar 2015 10:12:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Fri, 13 Mar 2015 10:12:30 -0700 (PDT)
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 13 Mar 2015 10:12:30 -0700
Message-ID: <CACdU-dQiG+2CpLt+zDRCfLfaPOza5XdBNNGF1uwWTxcV2FD+fw@mail.gmail.com>
Subject: extended jenkins downtime monday, march 16th, plus some hints at the future
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a1134788a373bb805112e9c96
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134788a373bb805112e9c96
Content-Type: text/plain; charset=UTF-8

i'll be taking jenkins down for some much-needed plugin updates, as well as
potentially upgrading jenkins itself.

this will start at 730am PDT, and i'm hoping to have everything up by noon.

the move to the anaconda python will take place in the next couple of weeks
as i'm in the process of rebuilding my staging environment (much needed) to
better reflect production, and allow me to better test the change.

and finally, some teasers for what's coming up in the next month or so:

* move to a fully puppetized environment (yay no more shell script
deployments!)
* virtualized workers (including multiple OSes -- OS X, ubuntu, ...,
profit?)

more details as they come.

happy friday!

shane

--001a1134788a373bb805112e9c96--

From dev-return-11987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 17:39:12 2015
Return-Path: <dev-return-11987-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74FE11770F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 17:39:12 +0000 (UTC)
Received: (qmail 24225 invoked by uid 500); 13 Mar 2015 17:39:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24133 invoked by uid 500); 13 Mar 2015 17:39:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23859 invoked by uid 99); 13 Mar 2015 17:39:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:39:09 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 17:39:05 +0000
Received: by obcva8 with SMTP id va8so21281208obc.8
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 10:36:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=IxOhf17He6dKNTIZxC9x8HqcHP6e1S55yR/LUsD7DvI=;
        b=AxvX+ZSyJkAM0VE2ygRu67VFTJlBVh/4LOe0otvhRmCmQEtrb1RwafzLzDlSCZv2EE
         wLXUh/bLzTRZi5cI2HlVz7Kx34MW3avdP5v2wnxh6xP05v/bpjDGr2cFWObOwH/K1+zF
         0b31yQ2yf4I5fFTSxILWNqNRFJcQ7SwG79JQJjnR6OkUut11hGs6mN4hNeegcH22cDG7
         N3IWgU9TBpFK5nuEAD9GnSZtuH/C6xDRVgWU4c1wMtbJC6vrbWYMAQrQ9hcbOfPpNMMl
         zuUpRR1cbOWwIMvhPDNqZE2Zc51viBDBDUV3jTzoYVk83VXn3xUdbr5lSWXd7l48Lw7p
         0hVQ==
MIME-Version: 1.0
X-Received: by 10.202.177.195 with SMTP id a186mr37628989oif.76.1426268189643;
 Fri, 13 Mar 2015 10:36:29 -0700 (PDT)
Received: by 10.202.79.81 with HTTP; Fri, 13 Mar 2015 10:36:29 -0700 (PDT)
In-Reply-To: <CAJiQeYL+cZ0NJn=--+vARo-t1+0=_11bJvmHhSQcVrQCKZnx7Q@mail.gmail.com>
References: <CAMAsSdKHQ69Pebzv3mmo_ZO3L+-u=k9BMuTSnGhTAv3VWSvdxg@mail.gmail.com>
	<CAOhmDzfdFa2E9DFiC65Uj2c13MjoXfKT1DXqyfmFgRZvWp90kw@mail.gmail.com>
	<CAMAsSdLPYxApYZJu=qEbm57u7+_afFYt2Esyr0eUHFDnKj7ZOg@mail.gmail.com>
	<CAJiQeYL+cZ0NJn=--+vARo-t1+0=_11bJvmHhSQcVrQCKZnx7Q@mail.gmail.com>
Date: Fri, 13 Mar 2015 10:36:29 -0700
Message-ID: <CABPQxstkvH+WCORO3fUpdB8JE3B9NfHiZ5ptaBKxLWeP0qNHcA@mail.gmail.com>
Subject: Re: May we merge into branch-1.3 at this point?
From: Patrick Wendell <pwendell@gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Nicholas Chammas <nicholas.chammas@gmail.com>, 
	dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Sean,

Yes, go crazy. Once we close the release vote, it's open season to
merge backports into that release.

- Patrick

On Fri, Mar 13, 2015 at 9:31 AM, Mridul Muralidharan <mridul@gmail.com> wrote:
> Who is managing 1.3 release ? You might want to coordinate with them before
> porting changes to branch.
>
> Regards
> Mridul
>
> On Friday, March 13, 2015, Sean Owen <sowen@cloudera.com> wrote:
>
>> Yeah, I'm guessing that is all happening quite literally as we speak.
>> The Apache git tag is the one of reference:
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4aaf48d46d13129f0f9bdafd771dd80fe568a7dc
>>
>> Open season on 1.3 branch then...
>>
>> On Fri, Mar 13, 2015 at 4:20 PM, Nicholas Chammas
>> <nicholas.chammas@gmail.com <javascript:;>> wrote:
>> > Looks like the release is out:
>> > http://spark.apache.org/releases/spark-release-1-3-0.html
>> >
>> > Though, interestingly, I think we are missing the appropriate v1.3.0 tag:
>> > https://github.com/apache/spark/releases
>> >
>> > Nick
>> >
>> > On Fri, Mar 13, 2015 at 6:07 AM Sean Owen <sowen@cloudera.com
>> <javascript:;>> wrote:
>> >>
>> >> Is the release certain enough that we can resume merging into
>> >> branch-1.3 at this point? I have a number of back-ports queued up and
>> >> didn't want to merge in case another last RC was needed. I see a few
>> >> commits to the branch though.
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
>> >> For additional commands, e-mail: dev-help@spark.apache.org
>> <javascript:;>
>> >>
>> >
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org <javascript:;>
>> For additional commands, e-mail: dev-help@spark.apache.org <javascript:;>
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-11988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 18:29:27 2015
Return-Path: <dev-return-11988-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6E3617B16
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 18:29:27 +0000 (UTC)
Received: (qmail 30046 invoked by uid 500); 13 Mar 2015 18:29:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29969 invoked by uid 500); 13 Mar 2015 18:29:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29958 invoked by uid 99); 13 Mar 2015 18:29:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 18:29:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 18:29:15 +0000
Received: by qgdq107 with SMTP id q107so27977391qgd.6
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 11:27:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=57w1Q4g0+5Qj69HLfZI00KH7J9MKpFHelazcXOZvMOU=;
        b=PsXRAlDC22GOdXAWpvNyjQxZKUInOn98z42DSsvgu1Axm9+7bYnAIZLNXLuJIziXFZ
         bvFIz3bWaXDPbspYXVqNdfgPMlt5vAWO9RWIOzogQYQI6+p61+Ay81blQ97XVUAp8irR
         7ytcu06dLUX+OfbV9wAjf3h1wKCdFe68/gtoec+GoBxa1SgvGYUSM2d0aCKrSy7olqRv
         68ICnPq+o+ocdvAM7gkE8YL8MzS3tieFiLKVRfSctqlA8yCdqNGkcSTtG9ZDk+KqSU3N
         /kQB7yY49oMjMA3aWDcuYR7aP5hp3GsAvvWGM2upHzVpI7GS/tPZ3EInYAS6x6BG2gxU
         yqEw==
X-Gm-Message-State: ALoCoQkOphCw6m/hiEwos7mX7m9gG6d67mHUV/6EtoRB6clXFCnLOH8QqZpeqA2SOQqtbFI9aqtt
X-Received: by 10.140.44.97 with SMTP id f88mr59550313qga.88.1426271224392;
 Fri, 13 Mar 2015 11:27:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.109.9 with HTTP; Fri, 13 Mar 2015 11:26:44 -0700 (PDT)
In-Reply-To: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>
References: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 13 Mar 2015 11:26:44 -0700
Message-ID: <CAPh_B=a9hFHQpW_NxR5uinWZjJFJsb_Q5_p63M0bhdNLzDT95Q@mail.gmail.com>
Subject: Re: Spark config option 'expression language' feedback request
To: Dale Richardson <dale__r@hotmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11397d96af0a2b05112fa51a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11397d96af0a2b05112fa51a
Content-Type: text/plain; charset=UTF-8

This is an interesting idea.

Are there well known libraries for doing this? Config is the one place
where it would be great to have something ridiculously simple, so it is
more or less bug free. I'm concerned about the complexity in this patch and
subtle bugs that it might introduce to config options that users will have
no workarounds. Also I believe it is fairly hard for nice error messages to
propagate when using Scala's parser combinator.


On Fri, Mar 13, 2015 at 3:07 AM, Dale Richardson <dale__r@hotmail.com>
wrote:

>
> PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature to
> allow for Spark configuration options (whether on command line, environment
> variable or a configuration file) to be specified via a simple expression
> language.
>
>
> Such a feature has the following end-user benefits:
> - Allows for the flexibility in specifying time intervals or byte
> quantities in appropriate and easy to follow units e.g. 1 week rather
> rather then 604800 seconds
>
> - Allows for the scaling of a configuration option in relation to a system
> attributes. e.g.
>
> SPARK_WORKER_CORES = numCores - 1
>
> SPARK_WORKER_MEMORY = physicalMemoryBytes - 1.5 GB
>
> - Gives the ability to scale multiple configuration options together eg:
>
> spark.driver.memory = 0.75 * physicalMemoryBytes
>
> spark.driver.maxResultSize = spark.driver.memory * 0.8
>
>
> The following functions are currently supported by this PR:
> NumCores:             Number of cores assigned to the JVM (usually ==
> Physical machine cores)
> PhysicalMemoryBytes:  Memory size of hosting machine
>
> JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
>
> JVMMaxMemoryBytes:    Maximum number of bytes of memory available to the
> JVM
>
> JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
>
>
> I was wondering if anybody on the mailing list has any further ideas on
> other functions that could be useful to have when specifying spark
> configuration options?
> Regards,Dale.
>

--001a11397d96af0a2b05112fa51a--

From dev-return-11989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 18:54:55 2015
Return-Path: <dev-return-11989-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 54DE817D03
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 18:54:55 +0000 (UTC)
Received: (qmail 13930 invoked by uid 500); 13 Mar 2015 18:54:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13851 invoked by uid 500); 13 Mar 2015 18:54:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13840 invoked by uid 99); 13 Mar 2015 18:54:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 18:54:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hshreedharan@cloudera.com designates 209.85.223.172 as permitted sender)
Received: from [209.85.223.172] (HELO mail-ie0-f172.google.com) (209.85.223.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 18:54:27 +0000
Received: by ieclw3 with SMTP id lw3so120757809iec.2
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 11:53:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to:cc
         :content-type;
        bh=APhZgFDK9iJVuF+kLO2SLRaEL808Hfs7H8yEmVRGlCg=;
        b=Cl3J15QOkEZKtgDPk5mI7YkdDSMiOTf3EaqCQphQviK7vYHF1Hd2kS4vCMQikSQVDI
         Q8k0BbXA/+iPTKlK3kOCBlhb/SZ8YqEr62Bb94YuTKPzZ583Hvc5IUXEKMAcKJH2oa2/
         +irRsPRpqk4R+kvvasU3JnrZryzpiJa9i8UzC5qW0WKxNm+/tDZ2Lfb2GkcmRM3wFE2O
         jVYDW0E610gW1QOC2APXxEAq8eSBto61Q17gf/SXLRrGfFimGnCzYitTMO0XZ9It8525
         3eq06tg+zyDrf3EJ0raV+TKmHUF8LE2ykBWEDSkXOW0rmsfi7FuIYtVh95dhJ1joZ4Gx
         rvng==
X-Gm-Message-State: ALoCoQmaelW8pRtiXfcJMq3NkINMy0oRcydTsTB2ZdSHoZqhi0KT46M5QV9v05Hu8Pp1NukoRaGe
X-Received: by 10.50.25.225 with SMTP id f1mr86083235igg.29.1426272816885;
 Fri, 13 Mar 2015 11:53:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.138.28 with HTTP; Fri, 13 Mar 2015 11:53:16 -0700 (PDT)
From: Hari Shreedharan <hshreedharan@cloudera.com>
Date: Fri, 13 Mar 2015 11:53:16 -0700
Message-ID: <CAHbPYVaQ0ghkbTTXJbnDKheU3yvLc8eo0LP7hTpZfV7SgwjWeQ@mail.gmail.com>
Subject: PR Builder timing out due to ivy cache lock
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: sknapp@berkeley.edu
Content-Type: multipart/alternative; boundary=047d7bd76f7c9a8a6e05113004a9
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd76f7c9a8a6e05113004a9
Content-Type: text/plain; charset=UTF-8

Looks like something is causing the PR Builder to timeout since this
morning with the ivy cache being locked.

Any idea what is happening?

--047d7bd76f7c9a8a6e05113004a9--

From dev-return-11990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 18:59:41 2015
Return-Path: <dev-return-11990-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8D53917D31
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 18:59:41 +0000 (UTC)
Received: (qmail 33790 invoked by uid 500); 13 Mar 2015 18:59:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33714 invoked by uid 500); 13 Mar 2015 18:59:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33697 invoked by uid 99); 13 Mar 2015 18:59:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 18:59:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 18:59:35 +0000
Received: by lbiw7 with SMTP id w7so24807418lbi.6
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 11:58:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=U74fXsU1W1Q+puzhoWMKt03Zo3bLA39XEb9pUIzaqNM=;
        b=jy7kwtWCe5VvugVfxB6F/n+hYU3wz+dRNTJ/FlPQeBKKiGR4/5uyatU6l6UZK/gt18
         YJL3obTCB9t/dnTS7MSA2JaHH63JsPQi513bEKycH2PJ3CcvU5Q1MMbwAwcksUZzAeeS
         sIf5j4/xheZvHJs3DND+AafSdDdv3XGN44dzoQJo5S1nR4M4siYK5u85x8xQyIO26dwT
         LL13sQkg7SHHjNfF2wuH8OF1ISCI73FhUL0aRQjv2dxBa50SzmIr35Cv2oZ1cV7biOON
         2Wn2/tlzCseTlodpqrpWDgv9e2DTskaTaavepELYOVm08yPRKndIZx8TQ6/MLGBThds2
         11SA==
X-Gm-Message-State: ALoCoQmNJMNjeiMf9pyscMw3Fwqma4A3zdHvGU6XYtHfSXmxIOFHyIwtEEal+9y1Az4fVBGg9NtI
X-Received: by 10.112.93.167 with SMTP id cv7mr12272480lbb.77.1426273109800;
 Fri, 13 Mar 2015 11:58:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Fri, 13 Mar 2015 11:58:09 -0700 (PDT)
In-Reply-To: <CAHbPYVaQ0ghkbTTXJbnDKheU3yvLc8eo0LP7hTpZfV7SgwjWeQ@mail.gmail.com>
References: <CAHbPYVaQ0ghkbTTXJbnDKheU3yvLc8eo0LP7hTpZfV7SgwjWeQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 13 Mar 2015 11:58:09 -0700
Message-ID: <CACdU-dT3dugUR8xh7BJW95P9ymr9DPmR95X2y3FN1gskwUjRzA@mail.gmail.com>
Subject: Re: PR Builder timing out due to ivy cache lock
To: Hari Shreedharan <hshreedharan@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134976610085205113016fc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134976610085205113016fc
Content-Type: text/plain; charset=UTF-8

link to a build, please?

On Fri, Mar 13, 2015 at 11:53 AM, Hari Shreedharan <
hshreedharan@cloudera.com> wrote:

> Looks like something is causing the PR Builder to timeout since this
> morning with the ivy cache being locked.
>
> Any idea what is happening?
>

--001a1134976610085205113016fc--

From dev-return-11991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 19:05:34 2015
Return-Path: <dev-return-11991-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7E8C417D70
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 19:05:34 +0000 (UTC)
Received: (qmail 55397 invoked by uid 500); 13 Mar 2015 19:05:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55328 invoked by uid 500); 13 Mar 2015 19:05:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55317 invoked by uid 99); 13 Mar 2015 19:05:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:05:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hshreedharan@cloudera.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:05:28 +0000
Received: by iecsl2 with SMTP id sl2so120351070iec.1
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 12:04:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=HPF9Dt4FZTzJi5tZqF1YkHduPUqm8IRk+qYDxdsgno4=;
        b=RQG4oX7MYQyc9HQhEj0aw3e3ofLfTMRvxUHFRkb3lYDl4rMDRk28u9FCMD08RnuKAU
         CNkGGE+4y+HlS1bRDj4Tuz5oerfgqp/9O9rNDoyozf88Q+4CD15IK74UXaylvIUYi926
         zjwcgXrn55RMoZA4m55wLM7Syze1Ajd71JfGsNmlQuajWTbsa6fcYKvYAXfM8Omp8mXv
         tSgbkl3skM0ePzMylnD69drPzdjKcM48vZlkcveTdWVy/RrcN+b61uRiVhlDkPhi4C8F
         TZiFQ7uCMTcg73IMI5R7GO3Oxdhu7S4+Trom6CZJGhrDH4J3EbzgCk+71wp/asb3xeRW
         EatA==
X-Gm-Message-State: ALoCoQl8PCIgjwdEeLhydG6JoglThYlbe/56vnb10uBVhrQf+HLH6SllKUgW4DenjGoLWF3ZUHqf
X-Received: by 10.107.31.14 with SMTP id f14mr49032992iof.28.1426273446794;
 Fri, 13 Mar 2015 12:04:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.138.28 with HTTP; Fri, 13 Mar 2015 12:03:46 -0700 (PDT)
In-Reply-To: <CACdU-dT3dugUR8xh7BJW95P9ymr9DPmR95X2y3FN1gskwUjRzA@mail.gmail.com>
References: <CAHbPYVaQ0ghkbTTXJbnDKheU3yvLc8eo0LP7hTpZfV7SgwjWeQ@mail.gmail.com>
 <CACdU-dT3dugUR8xh7BJW95P9ymr9DPmR95X2y3FN1gskwUjRzA@mail.gmail.com>
From: Hari Shreedharan <hshreedharan@cloudera.com>
Date: Fri, 13 Mar 2015 12:03:46 -0700
Message-ID: <CAHbPYVaJMj9T_ymF+nokSSkMc15TZkVuogsqO3_tfLPMZqhR+Q@mail.gmail.com>
Subject: Re: PR Builder timing out due to ivy cache lock
To: shane knapp <sknapp@berkeley.edu>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1141c3c426330c0511302a5c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1141c3c426330c0511302a5c
Content-Type: text/plain; charset=UTF-8

Here you are:
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/28571/consoleFull

On Fri, Mar 13, 2015 at 11:58 AM, shane knapp <sknapp@berkeley.edu> wrote:

> link to a build, please?
>
> On Fri, Mar 13, 2015 at 11:53 AM, Hari Shreedharan <
> hshreedharan@cloudera.com> wrote:
>
>> Looks like something is causing the PR Builder to timeout since this
>> morning with the ivy cache being locked.
>>
>> Any idea what is happening?
>>
>
>

--001a1141c3c426330c0511302a5c--

From dev-return-11992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 19:06:36 2015
Return-Path: <dev-return-11992-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5057917D73
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 19:06:36 +0000 (UTC)
Received: (qmail 57896 invoked by uid 500); 13 Mar 2015 19:06:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57821 invoked by uid 500); 13 Mar 2015 19:06:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57810 invoked by uid 99); 13 Mar 2015 19:06:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:06:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.181 as permitted sender)
Received: from [209.85.217.181] (HELO mail-lb0-f181.google.com) (209.85.217.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:06:29 +0000
Received: by lbiz12 with SMTP id z12so24808069lbi.12
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 12:04:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=g2VfgT6dgzJ/qW9nriK3uXW8vAw+h1JIYEVLhVCiQSg=;
        b=SIZiu/aWRUwq0HGrJWFDnIy/o/BesMH5plycMHSoExrwRfzEQuxovdi6lYHkHL+opK
         4zKhXjWGh1SQI1pYz+0rn/gkAeYucAIG77nDqFt08b8vfH+7oSlUlFP9mPJlM/N/d0Rx
         6FFhsFSR0zrgy0uZX7VJucVz7h0797IKSL/hUninGMvQFGhp7rMtxtYoKyM8HvE/BFUx
         fNE9Eso5YO9MnAnWXnp9VsX1Zc1yweVHwM9CVDdwj3brjKFmSE3x4tXDM2hZeI0noBUj
         5+7b2UrigVEfFwHaLX5CUwKLfs1N8dP26uyJqtzquM9fKvCyuNXN7g1ky4Rjyv74Attu
         SI+A==
X-Gm-Message-State: ALoCoQlLDZ83nzJCocImY6vnP4zazETmKsuO7ck4ZMyN+wr5jSfy4BQUPbAvwBtiOOHItBoWU+SB
X-Received: by 10.152.116.11 with SMTP id js11mr22710029lab.106.1426273477898;
 Fri, 13 Mar 2015 12:04:37 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Fri, 13 Mar 2015 12:04:17 -0700 (PDT)
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 13 Mar 2015 12:04:17 -0700
Message-ID: <CACdU-dShYM-8So5KHgcmeuwVZ9cLdz2jA3Xw88rkCZK_E-eOig@mail.gmail.com>
Subject: jenkins httpd being flaky
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11c1a70a00c51f0511302c78
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1a70a00c51f0511302c78
Content-Type: text/plain; charset=UTF-8

we just started having issues when visiting jenkins and getting 503 service
unavailable errors.

i'm on it and will report back with an all-clear.

--001a11c1a70a00c51f0511302c78--

From dev-return-11993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 19:33:28 2015
Return-Path: <dev-return-11993-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D189617EB4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 19:33:28 +0000 (UTC)
Received: (qmail 27951 invoked by uid 500); 13 Mar 2015 19:33:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27873 invoked by uid 500); 13 Mar 2015 19:33:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27862 invoked by uid 99); 13 Mar 2015 19:33:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:33:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:32:57 +0000
Received: by lbiz12 with SMTP id z12so24948274lbi.12
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 12:31:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=8/rVku6zIxapfLh13CZKuyQDKXc8FY/3blQCm1AAzaE=;
        b=UWIpRMJyEZyWGyC2oNQ4XlrFlcO8YL2GunUR2wjCG0xV9weheiWQYU+2z8aH8rgCC/
         NFTliJxJdz2dl7zu4YqQlwfsiFv3HqCHmpW+JwjOg+sJMZWxV1SPjbioqM4lxi3aEQFh
         bXZgDsLIYEP/IncDjRG0Pb5fE5hRjvRDlIzXPk2LRMeeP1DCjObpuKB0lRoymhByqHc/
         9FToBxs9yXNTptWLwmKwI4ZysHiQfQ84hfqFLsGWa60PvPjL5/tzxZAbKHJdKWJlCvNS
         06hJletOSMqnBU3nYpDeTgSpnqVqj7Mq25E0X/3UPr3MJDAePmJJekgUqeKWx+p+8FRh
         o9DQ==
X-Gm-Message-State: ALoCoQkZBMYhmgVpZnv4LzXarO/nYfUjvYhRfuCykeCgR7jQAaXlG0jzzN+9mAKRkgV9oxk8emy5
X-Received: by 10.113.11.12 with SMTP id ee12mr44340539lbd.5.1426275110858;
 Fri, 13 Mar 2015 12:31:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.213.18 with HTTP; Fri, 13 Mar 2015 12:31:30 -0700 (PDT)
In-Reply-To: <CAAswR-4Briw3txmJB5L+aLykzzYYZJBeNQD6zwE=GdamgLRxSA@mail.gmail.com>
References: <CANrtgzUX4pM3akkPE3a2AcHxWDXdaw3OazpDt2CARcOEffeQsg@mail.gmail.com>
 <CAH9GxT+bdfG6ZeXkgjAeqxx3_9_QHFmVefC0feVuKN1Nm+8kVg@mail.gmail.com> <CAAswR-4Briw3txmJB5L+aLykzzYYZJBeNQD6zwE=GdamgLRxSA@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 13 Mar 2015 12:31:30 -0700
Message-ID: <CAAswR-4egeoXbH3DxJht7iO54+1tiq4y5fYeHtezAjiEPdiBVw@mail.gmail.com>
Subject: Re: SparkSQL 1.3.0 (RC3) failed to read parquet file generated by 1.1.1
To: giive chen <thegiive@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Cheng Lian <lian@databricks.com>
Content-Type: multipart/alternative; boundary=001a1133a5b455c7b20511308d1d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a5b455c7b20511308d1d
Content-Type: text/plain; charset=UTF-8

Here is the JIRA: https://issues.apache.org/jira/browse/SPARK-6315

On Thu, Mar 12, 2015 at 11:00 PM, Michael Armbrust <michael@databricks.com>
wrote:

> We are looking at the issue and will likely fix it for Spark 1.3.1.
>
> On Thu, Mar 12, 2015 at 8:25 PM, giive chen <thegiive@gmail.com> wrote:
>
>> Hi all
>>
>> My team has the same issue. It looks like Spark 1.3's sparkSQL cannot read
>> parquet file generated by Spark 1.1. It will cost a lot of migration work
>> when we wanna to upgrade Spark 1.3.
>>
>> Is there  anyone can help me?
>>
>>
>> Thanks
>>
>> Wisely Chen
>>
>>
>> On Tue, Mar 10, 2015 at 5:06 PM, Pei-Lun Lee <pllee@appier.com> wrote:
>>
>> > Hi,
>> >
>> > I found that if I try to read parquet file generated by spark 1.1.1
>> using
>> > 1.3.0-rc3 by default settings, I got this error:
>> >
>> > com.fasterxml.jackson.core.JsonParseException: Unrecognized token
>> > 'StructType': was expecting ('true', 'false' or 'null')
>> >  at [Source: StructType(List(StructField(a,IntegerType,false))); line:
>> 1,
>> > column: 11]
>> >         at
>> >
>> com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1419)
>> >         at
>> >
>> >
>> com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508)
>> >         at
>> >
>> >
>> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2300)
>> >         at
>> >
>> >
>> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1459)
>> >         at
>> >
>> >
>> com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:683)
>> >         at
>> >
>> >
>> com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3105)
>> >         at
>> >
>> >
>> com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3051)
>> >         at
>> >
>> >
>> com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2161)
>> >         at
>> org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:19)
>> >         at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:44)
>> >         at
>> > org.apache.spark.sql.types.DataType$.fromJson(dataTypes.scala:41)
>> >         at
>> >
>> >
>> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
>> >         at
>> >
>> >
>> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
>> >
>> >
>> >
>> > this is how I save parquet file with 1.1.1:
>> >
>> > sql("select 1 as a").saveAsParquetFile("/tmp/foo")
>> >
>> >
>> >
>> > and this is the meta data of the 1.1.1 parquet file:
>> >
>> > creator:     parquet-mr version 1.4.3
>> > extra:       org.apache.spark.sql.parquet.row.metadata =
>> > StructType(List(StructField(a,IntegerType,false)))
>> >
>> >
>> >
>> > by comparison, this is 1.3.0 meta:
>> >
>> > creator:     parquet-mr version 1.6.0rc3
>> > extra:       org.apache.spark.sql.parquet.row.metadata =
>> > {"type":"struct","fields":[{"name":"a","type":"integer","nullable":t
>> > [more]...
>> >
>> >
>> >
>> > It looks like now ParquetRelation2 is used to load parquet file by
>> default
>> > and it only recognizes JSON format schema but 1.1.1 schema was case
>> class
>> > string format.
>> >
>> > Setting spark.sql.parquet.useDataSourceApi to false will fix it, but I
>> > don't know the differences.
>> > Is this considered a bug? We have a lot of parquet files from 1.1.1,
>> should
>> > we disable data source api in order to read them if we want to upgrade
>> to
>> > 1.3?
>> >
>> > Thanks,
>> > --
>> > Pei-Lun
>> >
>>
>
>

--001a1133a5b455c7b20511308d1d--

From dev-return-11994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 19:41:39 2015
Return-Path: <dev-return-11994-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 336C017F01
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 19:41:39 +0000 (UTC)
Received: (qmail 51114 invoked by uid 500); 13 Mar 2015 19:41:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51035 invoked by uid 500); 13 Mar 2015 19:41:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51024 invoked by uid 99); 13 Mar 2015 19:41:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:41:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.43 as permitted sender)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:41:33 +0000
Received: by labge10 with SMTP id ge10so24751290lab.7
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 12:40:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=VA9ibnWylA671i0VK8XWeXaXqpNib2VTd5tyiz8ehao=;
        b=APJzGaiIjBCgMvf7MP+iEDHlUQrJSWK2I+n8on+NdJf/KGJS6SFmPaGiow9kzdXWWu
         RHYu1OKnNiM0xT0ZhYaDT/BpdkK2nNen62KEeG3dqatGdqe7BtjXmY2W9W+IIV/7Mre5
         dPvbNLMXApbm7SxvBw3EWRtgKWik4qOKzx3XgrhzCOhZ7jIdHac2RG1QBxZEi0b9+Slh
         2VZwK4rnfXI/jzQE/2eyqGPYmdh/nw5hpSJooLN3JkPLfROBB5VlPQGk7oppV0OpmdG5
         KGUmt+jhNLAipfzCzziukFlJdMeat+0LwBJ8qyKOmsQUvragDGNUnbkKDgMRNkH2oTtg
         430g==
X-Gm-Message-State: ALoCoQlZgMTxY2tLpoOMvOEr6wGmdHJ708dFpvxU7JKlAcvxsaz5l8Kq9t8XNp+XT3S+mhA30e3k
X-Received: by 10.152.179.139 with SMTP id dg11mr39009202lac.28.1426275626963;
 Fri, 13 Mar 2015 12:40:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Fri, 13 Mar 2015 12:40:06 -0700 (PDT)
In-Reply-To: <CACdU-dShYM-8So5KHgcmeuwVZ9cLdz2jA3Xw88rkCZK_E-eOig@mail.gmail.com>
References: <CACdU-dShYM-8So5KHgcmeuwVZ9cLdz2jA3Xw88rkCZK_E-eOig@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 13 Mar 2015 12:40:06 -0700
Message-ID: <CACdU-dTtnpQmMR+cvnRsMPJ_8ep6ZNhZkqaS8pGKjp8nYQ=_OQ@mail.gmail.com>
Subject: Re: jenkins httpd being flaky
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a1134919218fce6051130ac2d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134919218fce6051130ac2d
Content-Type: text/plain; charset=UTF-8

ok we have a few different things happening:

1) httpd on the jenkins master is randomly (though not currently) flaking
out and causing visits to the site to return a 503.  nothing in the logs
shows any problems.

2) there are some github timeouts, which i tracked down and think it's a
problem with github themselves (see:  https://status.github.com/ and scroll
down to 'mean hook delivery time')

3) we have one spark job w/a strange ivy lock issue, that i just
retriggered (https://github.com/apache/spark/pull/4964)

4) there's an errant, unkillable pull request builder job (
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/28574/console
)

more updates forthcoming.

On Fri, Mar 13, 2015 at 12:04 PM, shane knapp <sknapp@berkeley.edu> wrote:

> we just started having issues when visiting jenkins and getting 503
> service unavailable errors.
>
> i'm on it and will report back with an all-clear.
>

--001a1134919218fce6051130ac2d--

From dev-return-11995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 19:44:14 2015
Return-Path: <dev-return-11995-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3058417F26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 19:44:14 +0000 (UTC)
Received: (qmail 65551 invoked by uid 500); 13 Mar 2015 19:44:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65472 invoked by uid 500); 13 Mar 2015 19:44:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65461 invoked by uid 99); 13 Mar 2015 19:44:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:44:12 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of alee526@hotmail.com designates 65.55.111.101 as permitted sender)
Received: from [65.55.111.101] (HELO BLU004-OMC2S26.hotmail.com) (65.55.111.101)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 19:43:46 +0000
Received: from BLU184-W52 ([65.55.111.71]) by BLU004-OMC2S26.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Fri, 13 Mar 2015 12:43:44 -0700
X-TMN: [LjHb0iekmNq6F7Xh7wIJmUCgX87Nwsap]
X-Originating-Email: [alee526@hotmail.com]
Message-ID: <BLU184-W52E9EB8B9323EE5DAF2EE6F3070@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_74240a75-a5c9-49c6-a0ad-aebdb47cc995_"
From: Andrew Lee <alee526@hotmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Spark ThriftServer encounter java.lang.IllegalArgumentException:
 Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
Date: Fri, 13 Mar 2015 12:43:44 -0700
Importance: Normal
MIME-Version: 1.0
X-OriginalArrivalTime: 13 Mar 2015 19:43:44.0463 (UTC) FILETIME=[03EEBDF0:01D05DC6]
X-Virus-Checked: Checked by ClamAV on apache.org

--_74240a75-a5c9-49c6-a0ad-aebdb47cc995_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

When Kerberos is enabled=2C I get the following exceptions. (Spark 1.2.1 gi=
t commit=20
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
b6eaf77d4332bfb0a698849b1f5f917d20d70e97=2C Hive 0.13.1=2C Apache Hadoop 2.=
4.1) when starting Spark ThriftServer.
Command to start thriftserver
./start-thriftserver.sh --hiveconf hive.server2.thrift.port=3D20000 --hivec=
onf hive.server2.thrift.bind.host=3D$(hostname) --master yarn-client
Error message in spark.log

2015-03-13 18:26:05=2C363 ERROR org.apache.hive.service.cli.thrift.ThriftCL=
IService (ThriftBinaryCLIService.java:run(93)) - Error: =0A=
java.lang.IllegalArgumentException: Unknown auth type: null Allowed values =
are: [auth-int=2C auth-conf=2C auth]=0A=
        at org.apache.hive.service.auth.SaslQOP.fromString(SaslQOP.java:56)=
=0A=
        at org.apache.hive.service.auth.HiveAuthFactory.getSaslProperties(H=
iveAuthFactory.java:118)=0A=
        at org.apache.hive.service.auth.HiveAuthFactory.getAuthTransFactory=
(HiveAuthFactory.java:133)=0A=
        at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(Th=
riftBinaryCLIService.java:43)=0A=
        at java.lang.Thread.run(Thread.java:744)

I'm wondering if this is due to the same problem described in HIVE-8154 HIV=
E-7620 due to an older code based for the Spark ThriftServer?
Any insights are appreciated. Currently=2C I can't get Spark ThriftServer t=
o run against a Kerberos cluster (Apache 2.4.1).

My hive-site.xml looks like the following for spark/conf.
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
=0A=
<property>=0A=
  <name>hive.semantic.analyzer.factory.impl</name>=0A=
  <value>org.apache.hcatalog.cli.HCatSemanticAnalyzerFactory</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.metastore.execute.setugi</name>=0A=
  <value>true</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.stats.autogather</name>=0A=
  <value>false</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.session.history.enabled</name>=0A=
  <value>true</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.querylog.location</name>=0A=
  <value>/home/hive/log/${user.name}</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.exec.local.scratchdir</name>=0A=
  <value>/tmp/hive/scratch/${user.name}</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.metastore.uris</name>=0A=
  <value>thrift://somehostname:9083</value>=0A=
</property>=0A=
<!-- HIVE SERVER 2 -->=0A=
<property>=0A=
  <name>hive.server2.authentication</name>=0A=
  <value>KERBEROS</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.server2.authentication.kerberos.principal</name>=0A=
  <value>***</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.server2.authentication.kerberos.keytab</name>=0A=
  <value>***</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.server2.thrift.sasl.qop</name>=0A=
  <value>auth</value>=0A=
  <description>Sasl QOP value=3B one of 'auth'=2C 'auth-int' and 'auth-conf=
'</description>=0A=
</property>=0A=
<property>=0A=
  <name>hive.server2.enable.impersonation</name>=0A=
  <description>Enable user impersonation for HiveServer2</description>=0A=
  <value>true</value>=0A=
</property>=0A=
<!-- HIVE METASTORE -->=0A=
<property>=0A=
  <name>hive.metastore.sasl.enabled</name>=0A=
  <value>true</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.metastore.kerberos.keytab.file</name>=0A=
  <value>***</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.metastore.kerberos.principal</name>=0A=
  <value>***</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.metastore.cache.pinobjtypes</name>=0A=
  <value>Table=2CDatabase=2CType=2CFieldSchema=2COrder</value>=0A=
</property>=0A=
<property>=0A=
  <name>hdfs_sentinel_file</name>=0A=
  <value>***</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.metastore.warehouse.dir</name>=0A=
  <value>/hive</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.metastore.client.socket.timeout</name>=0A=
  <value>600</value>=0A=
</property>=0A=
<property>=0A=
  <name>hive.warehouse.subdir.inherit.perms</name>=0A=
  <value>true</value>=0A=
</property> 		 	   		  =

--_74240a75-a5c9-49c6-a0ad-aebdb47cc995_--

From dev-return-11996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 20:09:36 2015
Return-Path: <dev-return-11996-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 852AD17222
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 20:09:36 +0000 (UTC)
Received: (qmail 40949 invoked by uid 500); 13 Mar 2015 20:09:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40868 invoked by uid 500); 13 Mar 2015 20:09:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40857 invoked by uid 99); 13 Mar 2015 20:09:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 20:09:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.176 as permitted sender)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 20:09:30 +0000
Received: by lbiz11 with SMTP id z11so25109112lbi.13
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 13:09:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=FdfSrS+A3+By5Ml+jCwxYxgbx94omVONaVEw8binvkM=;
        b=kL7yNwb9aKUPCgJ3b6vzakv6eGGeC4HZfWNQlJVY8D2+K2fNR0UD88TD8mcZMah0wi
         Jst7mqn9TRUq6qkAMwoibDHfb6I1a9HN2VVIECvbiQi6Y78+UDS8iH250ZG6bHCbWFSi
         z+ZKddaCv1/Oej8YLHBJp3LhVBPtzMXHckUKxZ9r8mdYAXdxJ688/5wNdsfI75NiO+dc
         4V63Lg9vB3X53EeSX1p0s4g+gchK1BLScKTkIhyHsyEAu1R8/Ah2QeUUyZXbhuh8G6+k
         wkZLKl6gil6vHX0PrAijZxotW8Cm/v1WQBoAeRDFg+2yYIYyzYpALujYdD2vklsytreB
         X9hg==
X-Gm-Message-State: ALoCoQkZTX66qlwt90eAjUgdlzSz2sAIyZoTMOiaLJyuJMn45zTfph+jMRQpIYLUXfTLdbnrwNpk
X-Received: by 10.152.180.202 with SMTP id dq10mr43246105lac.74.1426277349623;
 Fri, 13 Mar 2015 13:09:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Fri, 13 Mar 2015 13:08:49 -0700 (PDT)
In-Reply-To: <CACdU-dTtnpQmMR+cvnRsMPJ_8ep6ZNhZkqaS8pGKjp8nYQ=_OQ@mail.gmail.com>
References: <CACdU-dShYM-8So5KHgcmeuwVZ9cLdz2jA3Xw88rkCZK_E-eOig@mail.gmail.com>
 <CACdU-dTtnpQmMR+cvnRsMPJ_8ep6ZNhZkqaS8pGKjp8nYQ=_OQ@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 13 Mar 2015 13:08:49 -0700
Message-ID: <CACdU-dSw9usNDEFokHiyF-WMGZVP7XcX-Mw7fcEFLA8qc67FrA@mail.gmail.com>
Subject: Re: jenkins httpd being flaky
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a1134788ac6982f0511311213
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134788ac6982f0511311213
Content-Type: text/plain; charset=UTF-8

i tried a couple of things, but will also be doing a jenkins reboot as soon
as the current batch of builds finish.



On Fri, Mar 13, 2015 at 12:40 PM, shane knapp <sknapp@berkeley.edu> wrote:

> ok we have a few different things happening:
>
> 1) httpd on the jenkins master is randomly (though not currently) flaking
> out and causing visits to the site to return a 503.  nothing in the logs
> shows any problems.
>
> 2) there are some github timeouts, which i tracked down and think it's a
> problem with github themselves (see:  https://status.github.com/ and
> scroll down to 'mean hook delivery time')
>
> 3) we have one spark job w/a strange ivy lock issue, that i just
> retriggered (https://github.com/apache/spark/pull/4964)
>
> 4) there's an errant, unkillable pull request builder job (
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/28574/console
> )
>
> more updates forthcoming.
>
> On Fri, Mar 13, 2015 at 12:04 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> we just started having issues when visiting jenkins and getting 503
>> service unavailable errors.
>>
>> i'm on it and will report back with an all-clear.
>>
>
>

--001a1134788ac6982f0511311213--

From dev-return-11997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 20:56:18 2015
Return-Path: <dev-return-11997-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C5E3174B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 20:56:18 +0000 (UTC)
Received: (qmail 67916 invoked by uid 500); 13 Mar 2015 20:56:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67845 invoked by uid 500); 13 Mar 2015 20:56:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67834 invoked by uid 99); 13 Mar 2015 20:56:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 20:56:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.170 as permitted sender)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 20:56:12 +0000
Received: by lbjf15 with SMTP id f15so25315847lbj.2
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 13:55:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=70RFGr0RdTFs5u8F4FWdbrxqei5FXVit0S6P1Yly2HA=;
        b=P8NsFvPImUTumnRU2IYZNo1OQbWNfYSYbjFKp3IKN4eYnUTBpmRVm/WI1YjK8gZRk/
         M1KUw5H71xaLCV3+iHfItG0/zVMksCFH8T5dg3iveHPOA88hpKxM9YiLA4U+Pj6nDWZ2
         qX37+nFHAzBz0nPJk1VYeEZpudMeVdipWlfraNFmJL0cbx7hmurdYyOihUtuHDAHRBCD
         wtNBl/1B8TlaGvWbb9M7AZ0WmC1ASx0kqIS+vxEOCNMegbkU13WK8KyxHipdRr0BwHUt
         5os1mNUwgORmOehoBWe7o8h8jgCAekWPNgfbGHg07oPgWnqqK9Yn76T0lFLGcKAEwOR1
         +S6w==
X-Gm-Message-State: ALoCoQkP8eJ+1VNo+rOpgdIbiyotJIKrSSUTlwhydJKTdV2Yk1AaSSxROEDuR2cfHWRuVtzJy2XD
X-Received: by 10.112.114.164 with SMTP id jh4mr45127100lbb.21.1426280151678;
 Fri, 13 Mar 2015 13:55:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Fri, 13 Mar 2015 13:55:31 -0700 (PDT)
In-Reply-To: <CACdU-dSw9usNDEFokHiyF-WMGZVP7XcX-Mw7fcEFLA8qc67FrA@mail.gmail.com>
References: <CACdU-dShYM-8So5KHgcmeuwVZ9cLdz2jA3Xw88rkCZK_E-eOig@mail.gmail.com>
 <CACdU-dTtnpQmMR+cvnRsMPJ_8ep6ZNhZkqaS8pGKjp8nYQ=_OQ@mail.gmail.com> <CACdU-dSw9usNDEFokHiyF-WMGZVP7XcX-Mw7fcEFLA8qc67FrA@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 13 Mar 2015 13:55:31 -0700
Message-ID: <CACdU-dTGDvWHtM9kLZ2-x6jU5A-DELq-DkQhEkxbUqSZVCbhwg@mail.gmail.com>
Subject: Re: jenkins httpd being flaky
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a1135ff76ca99bc051131b949
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135ff76ca99bc051131b949
Content-Type: text/plain; charset=UTF-8

ok, things seem to have stabilized...  httpd hasn't flaked since ~noon, the
hanging PRB job on amp-jenkins-worker-06 was removed w/the restart and
things are now building.

i cancelled and retriggered a bunch of PRB builds, btw:
4848 (https://github.com/apache/spark/pull/3699)
5922 (https://github.com/apache/spark/pull/4733)
5987 (https://github.com/apache/spark/pull/4986)
6222 (https://github.com/apache/spark/pull/4964)
6325 (https://github.com/apache/spark/pull/5018)

as well as:
spark-master-maven-with-yarn

sorry for the inconvenience...  i'm still a little stumped as to what
happened, but i think it was a confluence of events (httpd flaking,
problems at github, mercury in retrograde, friday thinking it's monday).

shane

On Fri, Mar 13, 2015 at 1:08 PM, shane knapp <sknapp@berkeley.edu> wrote:

> i tried a couple of things, but will also be doing a jenkins reboot as
> soon as the current batch of builds finish.
>
>
>
> On Fri, Mar 13, 2015 at 12:40 PM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> ok we have a few different things happening:
>>
>> 1) httpd on the jenkins master is randomly (though not currently) flaking
>> out and causing visits to the site to return a 503.  nothing in the logs
>> shows any problems.
>>
>> 2) there are some github timeouts, which i tracked down and think it's a
>> problem with github themselves (see:  https://status.github.com/ and
>> scroll down to 'mean hook delivery time')
>>
>> 3) we have one spark job w/a strange ivy lock issue, that i just
>> retriggered (https://github.com/apache/spark/pull/4964)
>>
>> 4) there's an errant, unkillable pull request builder job (
>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/28574/console
>> )
>>
>> more updates forthcoming.
>>
>> On Fri, Mar 13, 2015 at 12:04 PM, shane knapp <sknapp@berkeley.edu>
>> wrote:
>>
>>> we just started having issues when visiting jenkins and getting 503
>>> service unavailable errors.
>>>
>>> i'm on it and will report back with an all-clear.
>>>
>>
>>
>

--001a1135ff76ca99bc051131b949--

From dev-return-11998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 21:23:12 2015
Return-Path: <dev-return-11998-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC4451764D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 21:23:11 +0000 (UTC)
Received: (qmail 41673 invoked by uid 500); 13 Mar 2015 21:23:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41598 invoked by uid 500); 13 Mar 2015 21:23:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41587 invoked by uid 99); 13 Mar 2015 21:23:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 21:23:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.52 as permitted sender)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 21:22:45 +0000
Received: by labgq15 with SMTP id gq15so25284305lab.1
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 14:22:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=RYLYU2kHYeHygnx8v91QwRD1M3xBlEwN+JFe3lzck8o=;
        b=Gst+pLJfopLVSe5tntoRipVb3OyqvnCYfHSUnG67XnLz++IIhCECTU7S9dn1Xl6e+P
         gfhxyQP9ZxRqnh9HySpEQ+ge0K8ilWExPj4MQvsT5SkkQnwPQzz11dAr/oEM6jRoILiy
         8W8U1CKUp0/KR1rrMWpyHftNXGQ0lNsIyxjD38Z0TYBGqeR8Jknwbhy+rf3CsxKqi8k+
         ykGerRbiEsv6Efr8LPDPQO4MVgiTt8CltYfiV1WZaB2LTLG++OrkrfeGH3H5fPztJTgt
         3TkMoyuCFmgcR7HXO/riH6RX7hQ8g6zQ0xP8ykdfLcFxZLuz0Q//V8ZnyiMqNz1HuCm3
         HYjw==
X-Gm-Message-State: ALoCoQnbRkNDrqtHIejo0r7dfv5IdR6grKs8pC40LUzgBhPh5yTHDyNnIR9Yfrq1ouJ9c5ODHyaj
X-Received: by 10.112.173.41 with SMTP id bh9mr44347979lbc.107.1426281763441;
 Fri, 13 Mar 2015 14:22:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Fri, 13 Mar 2015 14:22:23 -0700 (PDT)
In-Reply-To: <CAHbPYVaJMj9T_ymF+nokSSkMc15TZkVuogsqO3_tfLPMZqhR+Q@mail.gmail.com>
References: <CAHbPYVaQ0ghkbTTXJbnDKheU3yvLc8eo0LP7hTpZfV7SgwjWeQ@mail.gmail.com>
 <CACdU-dT3dugUR8xh7BJW95P9ymr9DPmR95X2y3FN1gskwUjRzA@mail.gmail.com> <CAHbPYVaJMj9T_ymF+nokSSkMc15TZkVuogsqO3_tfLPMZqhR+Q@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Fri, 13 Mar 2015 14:22:23 -0700
Message-ID: <CACdU-dQ_8RO6C6VVZXwPqf43aeXOazcKG0zQ-WUYerCmjZjbuQ@mail.gmail.com>
Subject: Re: PR Builder timing out due to ivy cache lock
To: Hari Shreedharan <hshreedharan@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2aee6dc1dde05113219d7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2aee6dc1dde05113219d7
Content-Type: text/plain; charset=UTF-8

i'm thinking that this was something transient, and hopefully won't happen
again.  a ton of weird stuff happened around the time of this failure (see
my flaky httpd email), and this was the only build exhibiting this behavior.

i'll keep an eye out for this failure over the weekend...



On Fri, Mar 13, 2015 at 12:03 PM, Hari Shreedharan <
hshreedharan@cloudera.com> wrote:

> Here you are:
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/28571/consoleFull
>
> On Fri, Mar 13, 2015 at 11:58 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> link to a build, please?
>>
>> On Fri, Mar 13, 2015 at 11:53 AM, Hari Shreedharan <
>> hshreedharan@cloudera.com> wrote:
>>
>>> Looks like something is causing the PR Builder to timeout since this
>>> morning with the ivy cache being locked.
>>>
>>> Any idea what is happening?
>>>
>>
>>
>

--001a11c2aee6dc1dde05113219d7--

From dev-return-11999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 22:53:39 2015
Return-Path: <dev-return-11999-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1140317A3D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 22:53:39 +0000 (UTC)
Received: (qmail 98767 invoked by uid 500); 13 Mar 2015 22:53:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98694 invoked by uid 500); 13 Mar 2015 22:53:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98683 invoked by uid 99); 13 Mar 2015 22:53:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 22:53:32 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dale__r@hotmail.com designates 65.54.190.76 as permitted sender)
Received: from [65.54.190.76] (HELO BAY004-OMC2S1.hotmail.com) (65.54.190.76)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 22:53:28 +0000
Received: from BAY180-W23 ([65.54.190.125]) by BAY004-OMC2S1.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Fri, 13 Mar 2015 15:52:46 -0700
X-TMN: [RiNQIEHWiTT/bQeDJwc5YCH96GOmj4dpJ6Bd/Ngq/bQ=]
X-Originating-Email: [dale__r@hotmail.com]
Message-ID: <BAY180-W23E814384AE76FA624AA92B1070@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_ee3e7428-64b8-4ff1-be02-c9ef36441503_"
From: Dale Richardson <dale__r@hotmail.com>
To: Reynold Xin <rxin@databricks.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Spark config option 'expression language' feedback request
Date: Fri, 13 Mar 2015 22:52:45 +0000
Importance: Normal
In-Reply-To:
 <CAPh_B=a9hFHQpW_NxR5uinWZjJFJsb_Q5_p63M0bhdNLzDT95Q@mail.gmail.com>
References:
 <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>,<CAPh_B=a9hFHQpW_NxR5uinWZjJFJsb_Q5_p63M0bhdNLzDT95Q@mail.gmail.com>
MIME-Version: 1.0
X-OriginalArrivalTime: 13 Mar 2015 22:52:46.0607 (UTC) FILETIME=[6C6091F0:01D05DE0]
X-Virus-Checked: Checked by ClamAV on apache.org

--_ee3e7428-64b8-4ff1-be02-c9ef36441503_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

=0A=
=0A=
=0A=
Hi Reynold=2CThey are some very good questions.
Re: Known libraries
There are a number of well known libraries that we could use to implement t=
his features=2C including MVEL=2C OGNL and JBOSS EL=2C or even Spring's EL.=
I looked at using them to prototype this feature in the beginning=2C but th=
ey all ended up bringing in a lot of code to service a pretty small functio=
nal requirement.The prime requirement I was trying to meet was:
1. Be able to specify quantities in kb=2Cmb=2Cgb etc transparently.2. Be ab=
le to specify some options as fractions of system attributes eg cpuCores * =
0.8
By just implementing this functionality and nothing else I figured I was co=
nstraining things enough that end-users got useful functionality but not en=
ough functionality to shoot themselves in the foot in new and interesting w=
ays. I couldn't see a nice way of limiting the expressiveness of 3rd party =
libraries to this extent.
I'd be happy to re-look at the feasibility of pulling in one of the 3rd par=
ty libraries if you think this approach has more merit=2C but I do caution =
that we may be opening a Pandora's box of potential functionality.  Those 3=
rd party libraries have a lot of (potentially excess) functionality in them=
.
Re: Code ComplexityI wrote the bare minimum code I could come up with to se=
rvice the above mentioned functionality=2C and then refactored it to use a =
stacked traits pattern which increased the code size by about a further 30%=
.  The expression code as it stands is pretty minimal=2C and has more then =
120 unit tests proving its functionality. More then half the code that is t=
here is taken up by utility classes to allow easy reference to byte quantit=
ies and time units. The design was deliberately limited to meeting the abov=
e requirements and not much more to reduce the chance for other subtleties =
to raise their heads.=20
Re: Work arounds.It would be pretty simple to implement fall back functiona=
lity to disable expression parsing by:1. Globally having a configuration op=
tion to disable all expression parsing and fall back to simple java propert=
y parsing.2. Locally having a known prefix that disables expression parsing=
 for that option.This should give enough workarounds to keep things running=
 in the unlikely event that something crops up no matter what happens.
Re: Error messagesIn regards to your comment about nice error messages I wo=
uld have to agree with you=2C it would have been nice.  In the end I just r=
eturn an option[Double] to the calling code for the parsed expression if th=
e entire string is parsed correctly. Given the additional complexity adding=
 error messages involved I retrospectively justify this by saying how much =
info do you need debug an expression like 'cpuCores * 0.8'? :)
Thanks for the feedback.
Regards=2CDale.
> From: rxin@databricks.com
> Date: Fri=2C 13 Mar 2015 11:26:44 -0700
> Subject: Re: Spark config option 'expression language' feedback request
> To: dale__r@hotmail.com
> CC: dev@spark.apache.org
>=20
> This is an interesting idea.
>=20
> Are there well known libraries for doing this? Config is the one place
> where it would be great to have something ridiculously simple=2C so it is
> more or less bug free. I'm concerned about the complexity in this patch a=
nd
> subtle bugs that it might introduce to config options that users will hav=
e
> no workarounds. Also I believe it is fairly hard for nice error messages =
to
> propagate when using Scala's parser combinator.
>=20
>=20
> On Fri=2C Mar 13=2C 2015 at 3:07 AM=2C Dale Richardson <dale__r@hotmail.c=
om>
> wrote:
>=20
> >
> > PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature to
> > allow for Spark configuration options (whether on command line=2C envir=
onment
> > variable or a configuration file) to be specified via a simple expressi=
on
> > language.
> >
> >
> > Such a feature has the following end-user benefits:
> > - Allows for the flexibility in specifying time intervals or byte
> > quantities in appropriate and easy to follow units e.g. 1 week rather
> > rather then 604800 seconds
> >
> > - Allows for the scaling of a configuration option in relation to a sys=
tem
> > attributes. e.g.
> >
> > SPARK_WORKER_CORES =3D numCores - 1
> >
> > SPARK_WORKER_MEMORY =3D physicalMemoryBytes - 1.5 GB
> >
> > - Gives the ability to scale multiple configuration options together eg=
:
> >
> > spark.driver.memory =3D 0.75 * physicalMemoryBytes
> >
> > spark.driver.maxResultSize =3D spark.driver.memory * 0.8
> >
> >
> > The following functions are currently supported by this PR:
> > NumCores:             Number of cores assigned to the JVM (usually =3D=
=3D
> > Physical machine cores)
> > PhysicalMemoryBytes:  Memory size of hosting machine
> >
> > JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
> >
> > JVMMaxMemoryBytes:    Maximum number of bytes of memory available to th=
e
> > JVM
> >
> > JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
> >
> >
> > I was wondering if anybody on the mailing list has any further ideas on
> > other functions that could be useful to have when specifying spark
> > configuration options?
> > Regards=2CDale.
> >
=0A=
 		 	   		  =

--_ee3e7428-64b8-4ff1-be02-c9ef36441503_--

From dev-return-12000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 13 23:07:25 2015
Return-Path: <dev-return-12000-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9113217AC7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 13 Mar 2015 23:07:25 +0000 (UTC)
Received: (qmail 48747 invoked by uid 500); 13 Mar 2015 23:07:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48678 invoked by uid 500); 13 Mar 2015 23:07:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48667 invoked by uid 99); 13 Mar 2015 23:07:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 23:07:08 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dale__r@hotmail.com designates 65.54.190.88 as permitted sender)
Received: from [65.54.190.88] (HELO BAY004-OMC2S13.hotmail.com) (65.54.190.88)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 13 Mar 2015 23:07:03 +0000
Received: from BAY180-W19 ([65.54.190.124]) by BAY004-OMC2S13.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Fri, 13 Mar 2015 16:06:42 -0700
X-TMN: [YjG8GvG0AFPubNIQ/o9aTPi9JIwBbIRldGpP/ap/XAI=]
X-Originating-Email: [dale__r@hotmail.com]
Message-ID: <BAY180-W19B8A71D6FB18B714BCAF1B1070@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_e65781bb-e2be-495f-bb4b-e56253e5408d_"
From: Dale Richardson <dale__r@hotmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Spark config option 'expression language' feedback request
Date: Fri, 13 Mar 2015 23:06:41 +0000
Importance: Normal
In-Reply-To:
 <CAJiQeYJ7k4Obu+TBzfTeU3wgz=ou5+vix0GLBTq_nN045mM0RQ@mail.gmail.com>
References:
 <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>,<CAJiQeYJ7k4Obu+TBzfTeU3wgz=ou5+vix0GLBTq_nN045mM0RQ@mail.gmail.com>
MIME-Version: 1.0
X-OriginalArrivalTime: 13 Mar 2015 23:06:42.0571 (UTC) FILETIME=[5EA68DB0:01D05DE2]
X-Virus-Checked: Checked by ClamAV on apache.org

--_e65781bb-e2be-495f-bb4b-e56253e5408d_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

=0A=
=0A=
=0A=
Thanks for your questions Mridul.
I assume you are referring to how the functionality to query system state w=
orks in Yarn and Mesos?
The API's used are the standard JVM API's so the functionality will work wi=
thout change. There is no real use case for using 'physicalMemoryBytes' in =
these cases though=2C as the JVM size has already been limited by the resou=
rce manager.
Regards=2CDale.
> Date: Fri=2C 13 Mar 2015 08:20:33 -0700
> Subject: Re: Spark config option 'expression language' feedback request
> From: mridul@gmail.com
> To: dale__r@hotmail.com
> CC: dev@spark.apache.org
>=20
> I am curious how you are going to support these over mesos and yarn.
> Any configure change like this should be applicable to all of them=2C not
> just local and standalone modes.
>=20
> Regards
> Mridul
>=20
> On Friday=2C March 13=2C 2015=2C Dale Richardson <dale__r@hotmail.com> wr=
ote:
>=20
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> >
> > PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature to
> > allow for Spark configuration options (whether on command line=2C envir=
onment
> > variable or a configuration file) to be specified via a simple expressi=
on
> > language.
> >
> >
> > Such a feature has the following end-user benefits:
> > - Allows for the flexibility in specifying time intervals or byte
> > quantities in appropriate and easy to follow units e.g. 1 week rather
> > rather then 604800 seconds
> >
> > - Allows for the scaling of a configuration option in relation to a sys=
tem
> > attributes. e.g.
> >
> > SPARK_WORKER_CORES =3D numCores - 1
> >
> > SPARK_WORKER_MEMORY =3D physicalMemoryBytes - 1.5 GB
> >
> > - Gives the ability to scale multiple configuration options together eg=
:
> >
> > spark.driver.memory =3D 0.75 * physicalMemoryBytes
> >
> > spark.driver.maxResultSize =3D spark.driver.memory * 0.8
> >
> >
> > The following functions are currently supported by this PR:
> > NumCores:             Number of cores assigned to the JVM (usually =3D=
=3D
> > Physical machine cores)
> > PhysicalMemoryBytes:  Memory size of hosting machine
> >
> > JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
> >
> > JVMMaxMemoryBytes:    Maximum number of bytes of memory available to th=
e
> > JVM
> >
> > JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
> >
> >
> > I was wondering if anybody on the mailing list has any further ideas on
> > other functions that could be useful to have when specifying spark
> > configuration options?
> > Regards=2CDale.
> >
=0A=
 		 	   		  =

--_e65781bb-e2be-495f-bb4b-e56253e5408d_--

From dev-return-12001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 14 00:32:04 2015
Return-Path: <dev-return-12001-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C956017E74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 14 Mar 2015 00:32:04 +0000 (UTC)
Received: (qmail 54880 invoked by uid 500); 14 Mar 2015 00:32:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54806 invoked by uid 500); 14 Mar 2015 00:32:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54794 invoked by uid 99); 14 Mar 2015 00:32:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Mar 2015 00:32:03 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Mar 2015 00:31:58 +0000
Received: by qgez64 with SMTP id z64so446128qge.2
        for <dev@spark.apache.org>; Fri, 13 Mar 2015 17:30:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=mRRmYccZaqTDgA+RjYaCw8awEa6qnqxcbEB15/ZUjFI=;
        b=Sq/aeTtrhUTi7UjBnSr8VHXAF5zv0smfOudE0LgoIlBB7FUWncCThtMOjKW5euOi+z
         GYn2NH8aki1w5ZqwlnYL6Quvz8uRfUY03i1pHWOJIN/ttqpVtoef/HrOY2LD6Y79adY6
         nYCtJB6NuLwyTkNVtoMtQl5z0l5Z6kRxd9cXe5bL0NU9tTN9JWZNvPvn6EFHYCkiF+I1
         cxtOJBc0YvLFpvPKoy8LkvpBWuhXGsozzAA/WOlGyMH8ko//pL2VtTwaza/ZWW1BU8dp
         2wq+CMnvKdR6MyHW4N9kF9sXJRLriO9mVgXVlHCkYSbvYDGaeYeiAUB2+d/EIQOSNPxt
         60fg==
MIME-Version: 1.0
X-Received: by 10.140.85.9 with SMTP id m9mr60906454qgd.7.1426293052020; Fri,
 13 Mar 2015 17:30:52 -0700 (PDT)
Received: by 10.140.33.131 with HTTP; Fri, 13 Mar 2015 17:30:51 -0700 (PDT)
In-Reply-To: <BAY180-W19B8A71D6FB18B714BCAF1B1070@phx.gbl>
References: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>
	<CAJiQeYJ7k4Obu+TBzfTeU3wgz=ou5+vix0GLBTq_nN045mM0RQ@mail.gmail.com>
	<BAY180-W19B8A71D6FB18B714BCAF1B1070@phx.gbl>
Date: Fri, 13 Mar 2015 17:30:51 -0700
Message-ID: <CAJiQeYJv-x27FtvPtopLh=B3t2dzJRfaQYqVVH_xbZJqKr0sUg@mail.gmail.com>
Subject: Re: Spark config option 'expression language' feedback request
From: Mridul Muralidharan <mridul@gmail.com>
To: Dale Richardson <dale__r@hotmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Let me try to rephrase my query.
How can a user specify, for example, what the executor memory should
be or number of cores should be.

I dont want a situation where some variables can be specified using
one set of idioms (from this PR for example) and another set cannot
be.


Regards,
Mridul




On Fri, Mar 13, 2015 at 4:06 PM, Dale Richardson <dale__r@hotmail.com> wrote:
>
>
>
> Thanks for your questions Mridul.
> I assume you are referring to how the functionality to query system state works in Yarn and Mesos?
> The API's used are the standard JVM API's so the functionality will work without change. There is no real use case for using 'physicalMemoryBytes' in these cases though, as the JVM size has already been limited by the resource manager.
> Regards,Dale.
>> Date: Fri, 13 Mar 2015 08:20:33 -0700
>> Subject: Re: Spark config option 'expression language' feedback request
>> From: mridul@gmail.com
>> To: dale__r@hotmail.com
>> CC: dev@spark.apache.org
>>
>> I am curious how you are going to support these over mesos and yarn.
>> Any configure change like this should be applicable to all of them, not
>> just local and standalone modes.
>>
>> Regards
>> Mridul
>>
>> On Friday, March 13, 2015, Dale Richardson <dale__r@hotmail.com> wrote:
>>
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> >
>> > PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature to
>> > allow for Spark configuration options (whether on command line, environment
>> > variable or a configuration file) to be specified via a simple expression
>> > language.
>> >
>> >
>> > Such a feature has the following end-user benefits:
>> > - Allows for the flexibility in specifying time intervals or byte
>> > quantities in appropriate and easy to follow units e.g. 1 week rather
>> > rather then 604800 seconds
>> >
>> > - Allows for the scaling of a configuration option in relation to a system
>> > attributes. e.g.
>> >
>> > SPARK_WORKER_CORES = numCores - 1
>> >
>> > SPARK_WORKER_MEMORY = physicalMemoryBytes - 1.5 GB
>> >
>> > - Gives the ability to scale multiple configuration options together eg:
>> >
>> > spark.driver.memory = 0.75 * physicalMemoryBytes
>> >
>> > spark.driver.maxResultSize = spark.driver.memory * 0.8
>> >
>> >
>> > The following functions are currently supported by this PR:
>> > NumCores:             Number of cores assigned to the JVM (usually ==
>> > Physical machine cores)
>> > PhysicalMemoryBytes:  Memory size of hosting machine
>> >
>> > JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
>> >
>> > JVMMaxMemoryBytes:    Maximum number of bytes of memory available to the
>> > JVM
>> >
>> > JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
>> >
>> >
>> > I was wondering if anybody on the mailing list has any further ideas on
>> > other functions that could be useful to have when specifying spark
>> > configuration options?
>> > Regards,Dale.
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 14 02:58:03 2015
Return-Path: <dev-return-12002-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8DD5C174B3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 14 Mar 2015 02:58:03 +0000 (UTC)
Received: (qmail 67765 invoked by uid 500); 14 Mar 2015 02:58:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67668 invoked by uid 500); 14 Mar 2015 02:58:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67656 invoked by uid 99); 14 Mar 2015 02:58:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Mar 2015 02:58:02 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dale__r@hotmail.com designates 65.54.190.94 as permitted sender)
Received: from [65.54.190.94] (HELO BAY004-OMC2S19.hotmail.com) (65.54.190.94)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 14 Mar 2015 02:57:36 +0000
Received: from BAY180-W51 ([65.54.190.123]) by BAY004-OMC2S19.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Fri, 13 Mar 2015 19:57:33 -0700
X-TMN: [JWXeMJL41hge0X2UgTe0SytUWYt4CDOFusT4wOUbO5w=]
X-Originating-Email: [dale__r@hotmail.com]
Message-ID: <BAY180-W51C42605342232B7571A72B1040@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_d8dadfc3-2a71-4de6-b515-4312e7096920_"
From: Dale Richardson <dale__r@hotmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Spark config option 'expression language' feedback request
Date: Sat, 14 Mar 2015 02:57:33 +0000
Importance: Normal
In-Reply-To:
 <CAJiQeYJv-x27FtvPtopLh=B3t2dzJRfaQYqVVH_xbZJqKr0sUg@mail.gmail.com>
References:
 <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>,<CAJiQeYJ7k4Obu+TBzfTeU3wgz=ou5+vix0GLBTq_nN045mM0RQ@mail.gmail.com>,<BAY180-W19B8A71D6FB18B714BCAF1B1070@phx.gbl>,<CAJiQeYJv-x27FtvPtopLh=B3t2dzJRfaQYqVVH_xbZJqKr0sUg@mail.gmail.com>
MIME-Version: 1.0
X-OriginalArrivalTime: 14 Mar 2015 02:57:33.0908 (UTC) FILETIME=[9EB0D940:01D05E02]
X-Virus-Checked: Checked by ClamAV on apache.org

--_d8dadfc3-2a71-4de6-b515-4312e7096920_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Mridul=2CI may have added some confusion by giving examples in completely d=
ifferent areas. For example the number of cores available for tasking on ea=
ch worker machine is a resource-controller level configuration variable. In=
 standalone mode (ie using Spark's home-grown resource manager) the configu=
ration variable SPARK_WORKER_CORES is an item that spark admins can set (an=
d we can use expressions for). The equivalent variable for YARN (Yarn.nodem=
anager.resource.cpu-vcores) is only used by Yarn's node manager setup and i=
s set by Yarn administrators and outside of control of spark (and most user=
s).  If you are not a cluster administrator then both variables are irrelev=
ant to you. The same goes for SPARK_WORKER_MEMORY.

As for spark.executor.memory=2C  As there is no way to know the attributes =
of a machine before a task is allocated to it=2C we cannot use any of the J=
VMInfo functions. For options like that the expression parser can easily be=
 limited to supporting different byte units of scale (kb/mb/gb etc) and oth=
er configuration variables only. =20
Regards=2CDale.




> Date: Fri=2C 13 Mar 2015 17:30:51 -0700
> Subject: Re: Spark config option 'expression language' feedback request
> From: mridul@gmail.com
> To: dale__r@hotmail.com
> CC: dev@spark.apache.org
>=20
> Let me try to rephrase my query.
> How can a user specify=2C for example=2C what the executor memory should
> be or number of cores should be.
>=20
> I dont want a situation where some variables can be specified using
> one set of idioms (from this PR for example) and another set cannot
> be.
>=20
>=20
> Regards=2C
> Mridul
>=20
>=20
>=20
>=20
> On Fri=2C Mar 13=2C 2015 at 4:06 PM=2C Dale Richardson <dale__r@hotmail.c=
om> wrote:
> >
> >
> >
> > Thanks for your questions Mridul.
> > I assume you are referring to how the functionality to query system sta=
te works in Yarn and Mesos?
> > The API's used are the standard JVM API's so the functionality will wor=
k without change. There is no real use case for using 'physicalMemoryBytes'=
 in these cases though=2C as the JVM size has already been limited by the r=
esource manager.
> > Regards=2CDale.
> >> Date: Fri=2C 13 Mar 2015 08:20:33 -0700
> >> Subject: Re: Spark config option 'expression language' feedback reques=
t
> >> From: mridul@gmail.com
> >> To: dale__r@hotmail.com
> >> CC: dev@spark.apache.org
> >>
> >> I am curious how you are going to support these over mesos and yarn.
> >> Any configure change like this should be applicable to all of them=2C =
not
> >> just local and standalone modes.
> >>
> >> Regards
> >> Mridul
> >>
> >> On Friday=2C March 13=2C 2015=2C Dale Richardson <dale__r@hotmail.com>=
 wrote:
> >>
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> >
> >> > PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature to
> >> > allow for Spark configuration options (whether on command line=2C en=
vironment
> >> > variable or a configuration file) to be specified via a simple expre=
ssion
> >> > language.
> >> >
> >> >
> >> > Such a feature has the following end-user benefits:
> >> > - Allows for the flexibility in specifying time intervals or byte
> >> > quantities in appropriate and easy to follow units e.g. 1 week rathe=
r
> >> > rather then 604800 seconds
> >> >
> >> > - Allows for the scaling of a configuration option in relation to a =
system
> >> > attributes. e.g.
> >> >
> >> > SPARK_WORKER_CORES =3D numCores - 1
> >> >
> >> > SPARK_WORKER_MEMORY =3D physicalMemoryBytes - 1.5 GB
> >> >
> >> > - Gives the ability to scale multiple configuration options together=
 eg:
> >> >
> >> > spark.driver.memory =3D 0.75 * physicalMemoryBytes
> >> >
> >> > spark.driver.maxResultSize =3D spark.driver.memory * 0.8
> >> >
> >> >
> >> > The following functions are currently supported by this PR:
> >> > NumCores:             Number of cores assigned to the JVM (usually =
=3D=3D
> >> > Physical machine cores)
> >> > PhysicalMemoryBytes:  Memory size of hosting machine
> >> >
> >> > JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
> >> >
> >> > JVMMaxMemoryBytes:    Maximum number of bytes of memory available to=
 the
> >> > JVM
> >> >
> >> > JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
> >> >
> >> >
> >> > I was wondering if anybody on the mailing list has any further ideas=
 on
> >> > other functions that could be useful to have when specifying spark
> >> > configuration options?
> >> > Regards=2CDale.
> >> >
> >
> >
>=20
> ---------------------------------------------------------------------
> To unsubscribe=2C e-mail: dev-unsubscribe@spark.apache.org
> For additional commands=2C e-mail: dev-help@spark.apache.org
>=20
 		 	   		  =

--_d8dadfc3-2a71-4de6-b515-4312e7096920_--

From dev-return-12003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 15 07:46:03 2015
Return-Path: <dev-return-12003-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3570C101CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Mar 2015 07:46:03 +0000 (UTC)
Received: (qmail 55438 invoked by uid 500); 15 Mar 2015 07:45:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55355 invoked by uid 500); 15 Mar 2015 07:45:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55344 invoked by uid 99); 15 Mar 2015 07:45:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 07:45:50 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 07:45:25 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 831B5174FA1E
	for <dev@spark.apache.org>; Sun, 15 Mar 2015 00:45:31 -0700 (PDT)
Date: Sun, 15 Mar 2015 00:45:22 -0700 (MST)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426405522047-11056.post@n3.nabble.com>
Subject: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

Is there any bugs to divide a Breeze sparse vector at Spark v1.3.0-rc3? When
I tried to divide a sparse vector at Spark v1.3.0-rc3, I got a wrong result
if the target vector has any zero values.

Spark v1.3.0-rc3 depends on Breeze v0.11.1. And Breeze v0.11.1 seems to have
any bugs to divide a sparse vector by a scalar value. When dividing a breeze
sparse vector which has any zero values, the result seems to be a zero
vector. However, we can run the same code on Spark v1.2.x.

However, there is no problem to multiply a breeze sparse vector. I asked the
breeze community this problem on the below issue.
https://github.com/scalanlp/breeze/issues/382

For example,
```
test("dividing a breeze spark vector") {
    val vec = Vectors.sparse(6, Array(0, 4), Array(0.0, 10.0)).toBreeze
    val n = 60.0
    val answer1 = vec :/ n
    val answer2 = vec.toDenseVector :/ n
    println(vec)
    println(answer1)
    println(answer2)
    assert(answer1.toDenseVector === answer2)
}

SparseVector((0,0.0), (4,10.0))
SparseVector()
DenseVector(0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0)

DenseVector(0.0, 0.0, 0.0, 0.0, 0.0, 0.0) did not equal DenseVector(0.0,
0.0, 0.0, 0.0, 0.16666666666666666, 0.0)
org.scalatest.exceptions.TestFailedException: DenseVector(0.0, 0.0, 0.0,
0.0, 0.0, 0.0) did not equal DenseVector(0.0, 0.0, 0.0, 0.0,
0.16666666666666666, 0.0)
```

Thanks,
Yu Ishikawa



-----
-- Yu Ishikawa
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 15 07:59:46 2015
Return-Path: <dev-return-12004-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C83171020A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Mar 2015 07:59:46 +0000 (UTC)
Received: (qmail 70083 invoked by uid 500); 15 Mar 2015 07:59:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70000 invoked by uid 500); 15 Mar 2015 07:59:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69989 invoked by uid 99); 15 Mar 2015 07:59:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 07:59:45 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 07:59:20 +0000
Received: by qgfa8 with SMTP id a8so18155626qgf.0
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 00:57:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=Jcex4c1i6sKbAIguncclgL6+dUyOv073y/Uh9gZdnR8=;
        b=ItPbrX6v2+8uHxvn8KHiYtk3qKVK+AIkb/TYlWo3ydqmkhZDKEGmJ11Wl2UBHq4WlM
         9+Md/B+UhSNT6Ca2h/21RqHrvuAFAqZ6KcFkg6QFKSjrVbqNBrvTnGjdB6/k2aJjhI5r
         1AZc847lyBLdkY9uQ8V9QRT+1+X9Yci4FLZKKvVI7AJ+LydwjNj1aA9u2KBCeCPK/Klr
         EbDpsOOexlwyRzp3TkHHl1I+KR/sxgBVbYuaNCW7r7dLzNaVc4rIRZtyPrwxdKFRxy3i
         /EsqcaqQlDRbKyIs/2yPd0u4vJK3XGonepFAZ545lEz+xbCdjPR14jbi9coMoOqPLdrb
         92Ng==
X-Gm-Message-State: ALoCoQn4rxjSvVogHnScQba5DrraaH5tMfnQ5xHM95/VblBCjjHQWgksplfRlwDxXZogN4EiexxQ
MIME-Version: 1.0
X-Received: by 10.140.109.99 with SMTP id k90mr67541798qgf.35.1426406247813;
 Sun, 15 Mar 2015 00:57:27 -0700 (PDT)
Received: by 10.229.9.130 with HTTP; Sun, 15 Mar 2015 00:57:27 -0700 (PDT)
In-Reply-To: <1426405522047-11056.post@n3.nabble.com>
References: <1426405522047-11056.post@n3.nabble.com>
Date: Sun, 15 Mar 2015 00:57:27 -0700
Message-ID: <CAEYYnxaZhJgtWEW4jDncOrg4vjrgJcwi-2LOw5YWhFqGFrMidA@mail.gmail.com>
Subject: Re: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
From: DB Tsai <dbtsai@dbtsai.com>
To: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
Cc: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

It's a bug in breeze's side. Once David fixes it and publishes it to
maven, we can upgrade to breeze 0.11.2. Please file a jira ticket for
this issue. thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com


On Sun, Mar 15, 2015 at 12:45 AM, Yu Ishikawa
<yuu.ishikawa+spark@gmail.com> wrote:
> Hi all,
>
> Is there any bugs to divide a Breeze sparse vector at Spark v1.3.0-rc3? When
> I tried to divide a sparse vector at Spark v1.3.0-rc3, I got a wrong result
> if the target vector has any zero values.
>
> Spark v1.3.0-rc3 depends on Breeze v0.11.1. And Breeze v0.11.1 seems to have
> any bugs to divide a sparse vector by a scalar value. When dividing a breeze
> sparse vector which has any zero values, the result seems to be a zero
> vector. However, we can run the same code on Spark v1.2.x.
>
> However, there is no problem to multiply a breeze sparse vector. I asked the
> breeze community this problem on the below issue.
> https://github.com/scalanlp/breeze/issues/382
>
> For example,
> ```
> test("dividing a breeze spark vector") {
>     val vec = Vectors.sparse(6, Array(0, 4), Array(0.0, 10.0)).toBreeze
>     val n = 60.0
>     val answer1 = vec :/ n
>     val answer2 = vec.toDenseVector :/ n
>     println(vec)
>     println(answer1)
>     println(answer2)
>     assert(answer1.toDenseVector === answer2)
> }
>
> SparseVector((0,0.0), (4,10.0))
> SparseVector()
> DenseVector(0.0, 0.0, 0.0, 0.0, 0.16666666666666666, 0.0)
>
> DenseVector(0.0, 0.0, 0.0, 0.0, 0.0, 0.0) did not equal DenseVector(0.0,
> 0.0, 0.0, 0.0, 0.16666666666666666, 0.0)
> org.scalatest.exceptions.TestFailedException: DenseVector(0.0, 0.0, 0.0,
> 0.0, 0.0, 0.0) did not equal DenseVector(0.0, 0.0, 0.0, 0.0,
> 0.16666666666666666, 0.0)
> ```
>
> Thanks,
> Yu Ishikawa
>
>
>
> -----
> -- Yu Ishikawa
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 15 08:16:46 2015
Return-Path: <dev-return-12005-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA27310237
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Mar 2015 08:16:46 +0000 (UTC)
Received: (qmail 77625 invoked by uid 500); 15 Mar 2015 08:16:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77538 invoked by uid 500); 15 Mar 2015 08:16:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77527 invoked by uid 99); 15 Mar 2015 08:16:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 08:16:44 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 08:16:19 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id EDCF01750395
	for <dev@spark.apache.org>; Sun, 15 Mar 2015 01:14:56 -0700 (PDT)
Date: Sun, 15 Mar 2015 01:14:47 -0700 (MST)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426407287506-11058.post@n3.nabble.com>
In-Reply-To: <1426405522047-11056.post@n3.nabble.com>
References: <1426405522047-11056.post@n3.nabble.com>
Subject: Re: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

David Hall who is a breeze creator told me that it's a bug. So, I made a jira
ticket about this issue. We need to upgrade breeze from 0.11.1 to 0.11.2 or
later in order to fix the bug, when the new version of breeze will be
released.

[SPARK-6341] Upgrade breeze from 0.11.1 to 0.11.2 or later - ASF JIRA
https://issues.apache.org/jira/browse/SPARK-6341

Thanks,
Yu Ishikawa



-----
-- Yu Ishikawa
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056p11058.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 15 13:06:20 2015
Return-Path: <dev-return-12006-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD75B1068F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Mar 2015 13:06:20 +0000 (UTC)
Received: (qmail 56231 invoked by uid 500); 15 Mar 2015 13:06:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56156 invoked by uid 500); 15 Mar 2015 13:06:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56144 invoked by uid 99); 15 Mar 2015 13:06:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 13:06:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 13:05:53 +0000
Received: by padcy3 with SMTP id cy3so36621061pad.3
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 06:03:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=sl3vqL/DkkISK59rbURNkHOTljpPSFBJDAI0bs3ffZs=;
        b=ZtlhCsldLicF4PUK8xyibIEPUbxy5UonZkXrnVP1g5v9RKYXcF4y6k8ZBo3k3Yge5k
         vhE0e9/ZYi4CRcZnDZuUxIg9phrKe5y0P2WC+IFUMi5tXx+oqMVZEvrPVsH4+wx/hPhX
         7gt6UqvMYAP0gCG5yePq/bSPA4jQf9aTbymKwf4Mi/t4Z/9Y1MOKhSCEiDOEgESwm7Cq
         A4NjdoFw+K8WCv4hdWmjGPkRIO+awWn89S/KIL3I1IMRedKzmiXoGek+C53iK6JoZKXQ
         21o/ze6gAYf5v/4ygg208ceYOFAp5mUf6a37luIXJKsilfYnefIx0EG2cxUp7/lAD3vW
         9gtA==
X-Received: by 10.70.123.131 with SMTP id ma3mr97583974pdb.16.1426424616166;
        Sun, 15 Mar 2015 06:03:36 -0700 (PDT)
Received: from [10.10.0.21] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id c8sm12400556pds.5.2015.03.15.06.03.31
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 15 Mar 2015 06:03:33 -0700 (PDT)
Message-ID: <55058326.3000203@gmail.com>
Date: Sun, 15 Mar 2015 21:03:34 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Andrew Lee <alee526@hotmail.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Spark ThriftServer encounter java.lang.IllegalArgumentException:
 Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
References: <BLU184-W52E9EB8B9323EE5DAF2EE6F3070@phx.gbl>
In-Reply-To: <BLU184-W52E9EB8B9323EE5DAF2EE6F3070@phx.gbl>
Content-Type: text/plain; charset=windows-1252; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Andrew,

Would you please create a JIRA ticket for this? To preserve 
compatibility with existing Hive JDBC/ODBC drivers, Spark SQL's 
HiveThriftServer intercepts some HiveServer2 components and injects 
Spark stuff into it. This makes the implementation details are somewhat 
hacky (e.g. a bunch of reflection tricks were used). We haven't include 
KRB tests in Spark unit/integration test suites, and it's possible that 
HiveThriftServer2 somehow breaks Hive's KRB feature.

Cheng

On 3/14/15 3:43 AM, Andrew Lee wrote:
> When Kerberos is enabled, I get the following exceptions. (Spark 1.2.1 git commit
>
>
>
>
>
>
>
>
> b6eaf77d4332bfb0a698849b1f5f917d20d70e97, Hive 0.13.1, Apache Hadoop 2.4.1) when starting Spark ThriftServer.
> Command to start thriftserver
> ./start-thriftserver.sh --hiveconf hive.server2.thrift.port=20000 --hiveconf hive.server2.thrift.bind.host=$(hostname) --master yarn-client
> Error message in spark.log
>
> 2015-03-13 18:26:05,363 ERROR org.apache.hive.service.cli.thrift.ThriftCLIService (ThriftBinaryCLIService.java:run(93)) - Error:
> java.lang.IllegalArgumentException: Unknown auth type: null Allowed values are: [auth-int, auth-conf, auth]
>          at org.apache.hive.service.auth.SaslQOP.fromString(SaslQOP.java:56)
>          at org.apache.hive.service.auth.HiveAuthFactory.getSaslProperties(HiveAuthFactory.java:118)
>          at org.apache.hive.service.auth.HiveAuthFactory.getAuthTransFactory(HiveAuthFactory.java:133)
>          at org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:43)
>          at java.lang.Thread.run(Thread.java:744)
>
> I'm wondering if this is due to the same problem described in HIVE-8154 HIVE-7620 due to an older code based for the Spark ThriftServer?
> Any insights are appreciated. Currently, I can't get Spark ThriftServer to run against a Kerberos cluster (Apache 2.4.1).
>
> My hive-site.xml looks like the following for spark/conf.
>
>
>
>
>
>
>
>
> <property>
>    <name>hive.semantic.analyzer.factory.impl</name>
>    <value>org.apache.hcatalog.cli.HCatSemanticAnalyzerFactory</value>
> </property>
> <property>
>    <name>hive.metastore.execute.setugi</name>
>    <value>true</value>
> </property>
> <property>
>    <name>hive.stats.autogather</name>
>    <value>false</value>
> </property>
> <property>
>    <name>hive.session.history.enabled</name>
>    <value>true</value>
> </property>
> <property>
>    <name>hive.querylog.location</name>
>    <value>/home/hive/log/${user.name}</value>
> </property>
> <property>
>    <name>hive.exec.local.scratchdir</name>
>    <value>/tmp/hive/scratch/${user.name}</value>
> </property>
> <property>
>    <name>hive.metastore.uris</name>
>    <value>thrift://somehostname:9083</value>
> </property>
> <!-- HIVE SERVER 2 -->
> <property>
>    <name>hive.server2.authentication</name>
>    <value>KERBEROS</value>
> </property>
> <property>
>    <name>hive.server2.authentication.kerberos.principal</name>
>    <value>***</value>
> </property>
> <property>
>    <name>hive.server2.authentication.kerberos.keytab</name>
>    <value>***</value>
> </property>
> <property>
>    <name>hive.server2.thrift.sasl.qop</name>
>    <value>auth</value>
>    <description>Sasl QOP value; one of 'auth', 'auth-int' and 'auth-conf'</description>
> </property>
> <property>
>    <name>hive.server2.enable.impersonation</name>
>    <description>Enable user impersonation for HiveServer2</description>
>    <value>true</value>
> </property>
> <!-- HIVE METASTORE -->
> <property>
>    <name>hive.metastore.sasl.enabled</name>
>    <value>true</value>
> </property>
> <property>
>    <name>hive.metastore.kerberos.keytab.file</name>
>    <value>***</value>
> </property>
> <property>
>    <name>hive.metastore.kerberos.principal</name>
>    <value>***</value>
> </property>
> <property>
>    <name>hive.metastore.cache.pinobjtypes</name>
>    <value>Table,Database,Type,FieldSchema,Order</value>
> </property>
> <property>
>    <name>hdfs_sentinel_file</name>
>    <value>***</value>
> </property>
> <property>
>    <name>hive.metastore.warehouse.dir</name>
>    <value>/hive</value>
> </property>
> <property>
>    <name>hive.metastore.client.socket.timeout</name>
>    <value>600</value>
> </property>
> <property>
>    <name>hive.warehouse.subdir.inherit.perms</name>
>    <value>true</value>
> </property> 		 	   		


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 15 17:42:51 2015
Return-Path: <dev-return-12007-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3EB0F10AD6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Mar 2015 17:42:51 +0000 (UTC)
Received: (qmail 13316 invoked by uid 500); 15 Mar 2015 17:42:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13238 invoked by uid 500); 15 Mar 2015 17:42:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13226 invoked by uid 99); 15 Mar 2015 17:42:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 17:42:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 17:42:22 +0000
Received: by pacwe9 with SMTP id we9so40813759pac.1
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 10:40:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject
         :content-type:content-transfer-encoding;
        bh=9UuwoozT7ygxVXU4QIeDrEac3bGK9147FHaYwH+LMPQ=;
        b=hRa69TKio9g2/DG+4jAKDHIFrMgW5ZWMgtHSRr4gmjEdI0E3vDOTmdyNTAEhYi/zw8
         oOkULqpiwxqkdqXlHRo/5zCPykO2MsI7kL5PCH5wyvycTrJ86KqabYYfE8GrsgcueEqH
         BeneWoXrnYg+mokrdc20BTX7hx1Lg3w23gymkUi9aPEIJ9bcn9R6NR0huAjMTz56Bwk9
         i06FK8xNE1UTFoPtCJZPooX/AJ9eiULq75p2UBBrHJHe7ni8RNliJRXkiOOlUQ57yjSH
         rZKeePXX4Ilcfr9APDd8liishMg66qA7/RTWm4ZcaLgJ91B2Y4cMsEGAy/++wzB6qp26
         CYtw==
X-Received: by 10.70.34.79 with SMTP id x15mr89230909pdi.6.1426441205579;
        Sun, 15 Mar 2015 10:40:05 -0700 (PDT)
Received: from [10.10.0.21] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id z4sm13236740pdi.90.2015.03.15.10.40.03
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sun, 15 Mar 2015 10:40:05 -0700 (PDT)
Message-ID: <5505C3F7.5090502@gmail.com>
Date: Mon, 16 Mar 2015 01:40:07 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Wrong version on the Spark documentation page
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

It's still marked as 1.2.1 here http://spark.apache.org/docs/latest/

But this page is updated (1.3.0) 
http://spark.apache.org/docs/latest/index.html

Cheng

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 15 18:13:24 2015
Return-Path: <dev-return-12008-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E800110B6B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Mar 2015 18:13:23 +0000 (UTC)
Received: (qmail 66565 invoked by uid 500); 15 Mar 2015 18:13:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66487 invoked by uid 500); 15 Mar 2015 18:13:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66471 invoked by uid 99); 15 Mar 2015 18:13:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 18:13:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.179 as permitted sender)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 18:13:11 +0000
Received: by obbgg8 with SMTP id gg8so21194400obb.1
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 11:12:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=r2uz8sRx4V3nQUq0Ioc3RxT3E9zVGgXgivFFMRWzzjw=;
        b=g9Tawl9v/6M4fwrp3tqKidoO4ewzus+KPkANFnMh64vI04isjaqMqkRlFenbWZVUzO
         9YJ9jDomTskljtTXbNvw9J5DLdBdkPXbmGC+m6K2laEykxkE9y+TLC6RgHvm79wU8c57
         MG2BSyh773MG8S1ng65PyMQ6fcApDtP5OoOupZzAOBVohaUvElPqog3D0tliYKrxUaNm
         QDDPzYFA3YqIHEqsiMvCU2WtpJQIcAEAT1r4gC03HZXPLb2uQ6zTsWvL5oTSLMYI47Zi
         iqqDieSlshwofKawK1Mt4ecALCqiT+l4fhvwU78aX3sNhS3g8BYsAFzU6tpfmyq9Dyv7
         TpFg==
MIME-Version: 1.0
X-Received: by 10.202.220.9 with SMTP id t9mr42796588oig.102.1426443170943;
 Sun, 15 Mar 2015 11:12:50 -0700 (PDT)
Received: by 10.202.199.135 with HTTP; Sun, 15 Mar 2015 11:12:50 -0700 (PDT)
In-Reply-To: <5505C3F7.5090502@gmail.com>
References: <5505C3F7.5090502@gmail.com>
Date: Sun, 15 Mar 2015 11:12:50 -0700
Message-ID: <CABPQxsu_X6PAoFunrO0au6kqMV6_sUi2wSjR3k6XyigihsFzfw@mail.gmail.com>
Subject: Re: Wrong version on the Spark documentation page
From: Patrick Wendell <pwendell@gmail.com>
To: Cheng Lian <lian.cs.zju@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Cheng - what if you hold shift+refresh? For me the /latest link
correctly points to 1.3.0

On Sun, Mar 15, 2015 at 10:40 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:
> It's still marked as 1.2.1 here http://spark.apache.org/docs/latest/
>
> But this page is updated (1.3.0)
> http://spark.apache.org/docs/latest/index.html
>
> Cheng
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 15 18:33:43 2015
Return-Path: <dev-return-12009-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E6EC10BC2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 15 Mar 2015 18:33:43 +0000 (UTC)
Received: (qmail 86542 invoked by uid 500); 15 Mar 2015 18:33:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86457 invoked by uid 500); 15 Mar 2015 18:33:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86445 invoked by uid 99); 15 Mar 2015 18:33:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 18:33:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 15 Mar 2015 18:33:36 +0000
Received: by igbue6 with SMTP id ue6so19871038igb.1
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 11:31:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=l10S9+tMlTtsgoUi9sygApBHmPM1pxKgnweVEblXSOE=;
        b=ffyq26ZWC0hXrCv3VQ2q16f94RyjBFxOejot6F7+4SLd2d7os7PhGEyIcrk9wodJyV
         wl8yeh2ZbxZTV0wCAbwMiYO43IC/3Z54yvSMECRdDBoMLn735wLumQXHYhs7h04bFpbi
         X0Ku2R1R6bXFu+kIztgujIItooogyHQ5aJ1XgLhZKa6QkG/9P0tixPNXSM1WTMLUTKmp
         dj4HCA6iUgU7XqLSUUHAg6TndaQSfZEhr8EyfLVsQz5mQl1XrC7Vns3a2RLBUagfQYpa
         sfTRY1C/BsnTve0c+AiHDPwNMzjhkyjvHaPhYgZ5AVY+4R3hopNDGBY/EN9IsCtQGXyW
         gTgQ==
MIME-Version: 1.0
X-Received: by 10.50.82.68 with SMTP id g4mr128111802igy.26.1426444305739;
 Sun, 15 Mar 2015 11:31:45 -0700 (PDT)
Received: by 10.36.53.148 with HTTP; Sun, 15 Mar 2015 11:31:45 -0700 (PDT)
In-Reply-To: <CABPQxsu_X6PAoFunrO0au6kqMV6_sUi2wSjR3k6XyigihsFzfw@mail.gmail.com>
References: <5505C3F7.5090502@gmail.com>
	<CABPQxsu_X6PAoFunrO0au6kqMV6_sUi2wSjR3k6XyigihsFzfw@mail.gmail.com>
Date: Sun, 15 Mar 2015 11:31:45 -0700
Message-ID: <CALte62x-s7tqkq73crryHLTru--WqKcDJ3zkYYgNTorgDs9Sww@mail.gmail.com>
Subject: Re: Wrong version on the Spark documentation page
From: Ted Yu <yuzhihong@gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Cheng Lian <lian.cs.zju@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf18db022bb06051157f2bf
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf18db022bb06051157f2bf
Content-Type: text/plain; charset=UTF-8

When I enter  http://spark.apache.org/docs/latest/ into Chrome address bar,
I saw 1.3.0

Cheers

On Sun, Mar 15, 2015 at 11:12 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Cheng - what if you hold shift+refresh? For me the /latest link
> correctly points to 1.3.0
>
> On Sun, Mar 15, 2015 at 10:40 AM, Cheng Lian <lian.cs.zju@gmail.com>
> wrote:
> > It's still marked as 1.2.1 here http://spark.apache.org/docs/latest/
> >
> > But this page is updated (1.3.0)
> > http://spark.apache.org/docs/latest/index.html
> >
> > Cheng
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7bf18db022bb06051157f2bf--

From dev-return-12010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 03:03:13 2015
Return-Path: <dev-return-12010-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 814B117740
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 03:03:13 +0000 (UTC)
Received: (qmail 68604 invoked by uid 500); 16 Mar 2015 03:03:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68524 invoked by uid 500); 16 Mar 2015 03:03:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68510 invoked by uid 99); 16 Mar 2015 03:03:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:03:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pllee@appier.com designates 209.85.216.173 as permitted sender)
Received: from [209.85.216.173] (HELO mail-qc0-f173.google.com) (209.85.216.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:02:41 +0000
Received: by qcto4 with SMTP id o4so33267180qct.3
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 20:01:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Far5xz3pGI5SBg3S3Mu8zJh4dg+0KdGHTWw6VNBAh0Y=;
        b=iS5H87DFxjqtTRGDvtjv1wXQbCGybaqV7yhpojyuja915UiXiC+YAFaqeYoWlGxkfp
         geKUy4cc5yUxx2/cZ3zkdXz4iW1ysbzQW8SlY5Zqqf5vJlpJaBWGUt5oWMLD4MmohQ2s
         vpoVDmbZF5myEzf0PtpyB5kbzIQXxHXz8MIwA=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=Far5xz3pGI5SBg3S3Mu8zJh4dg+0KdGHTWw6VNBAh0Y=;
        b=EWiysIX/74ce0SkLGYdRQeIj9Nqd+hD4PAYa81XZKnAUgEt7NF1PkhCMvrfRbwWf2p
         WlDqPTWf8uFrqkUTKG2uJa09SeINkbMpmj3amThiHq5I33+SP0Kdx8KQzn6PEMY47GUi
         PBbvTyb70upCrhURHfA/4SVTK9O4q8iiFYqA9MELW14T2VURBwSr0UqrKbkbFMW7EbY+
         wAr0mEI4YxiPPY4kU+WIc4O64J6QzVZrdz6YJSNs5q8x+RRDwZZf2+2V6CnSHAIyFdBF
         tg4zWDiu5EcW6w4KFYuwpyBQFr8JFwKfhMZOfwS0oQMe2T0kUPl8hoETSVVxYhpGLD24
         ok4w==
X-Gm-Message-State: ALoCoQnndK/mzzg+WsLbLHDuoEwGKCNZbPNh+UFU/kneSrwO8E8UdDHxQD4HopEUFHb3DPIf651/
MIME-Version: 1.0
X-Received: by 10.140.218.196 with SMTP id o187mr47851321qhb.30.1426474914626;
 Sun, 15 Mar 2015 20:01:54 -0700 (PDT)
Received: by 10.229.233.136 with HTTP; Sun, 15 Mar 2015 20:01:54 -0700 (PDT)
In-Reply-To: <CAAswR-4egeoXbH3DxJht7iO54+1tiq4y5fYeHtezAjiEPdiBVw@mail.gmail.com>
References: <CANrtgzUX4pM3akkPE3a2AcHxWDXdaw3OazpDt2CARcOEffeQsg@mail.gmail.com>
	<CAH9GxT+bdfG6ZeXkgjAeqxx3_9_QHFmVefC0feVuKN1Nm+8kVg@mail.gmail.com>
	<CAAswR-4Briw3txmJB5L+aLykzzYYZJBeNQD6zwE=GdamgLRxSA@mail.gmail.com>
	<CAAswR-4egeoXbH3DxJht7iO54+1tiq4y5fYeHtezAjiEPdiBVw@mail.gmail.com>
Date: Mon, 16 Mar 2015 11:01:54 +0800
Message-ID: <CANrtgzXrMp_306e98F2qQuZg6HSH4bp1om68HL20b6FEj9kZTw@mail.gmail.com>
Subject: Re: SparkSQL 1.3.0 (RC3) failed to read parquet file generated by 1.1.1
From: Pei-Lun Lee <pllee@appier.com>
To: Michael Armbrust <michael@databricks.com>
Cc: giive chen <thegiive@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Cheng Lian <lian@databricks.com>
Content-Type: multipart/alternative; boundary=001a1139c9ae91591b05115f1242
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139c9ae91591b05115f1242
Content-Type: text/plain; charset=UTF-8

Thanks!

On Sat, Mar 14, 2015 at 3:31 AM, Michael Armbrust <michael@databricks.com>
wrote:

> Here is the JIRA: https://issues.apache.org/jira/browse/SPARK-6315
>
> On Thu, Mar 12, 2015 at 11:00 PM, Michael Armbrust <michael@databricks.com
> >
> wrote:
>
> > We are looking at the issue and will likely fix it for Spark 1.3.1.
> >
> > On Thu, Mar 12, 2015 at 8:25 PM, giive chen <thegiive@gmail.com> wrote:
> >
> >> Hi all
> >>
> >> My team has the same issue. It looks like Spark 1.3's sparkSQL cannot
> read
> >> parquet file generated by Spark 1.1. It will cost a lot of migration
> work
> >> when we wanna to upgrade Spark 1.3.
> >>
> >> Is there  anyone can help me?
> >>
> >>
> >> Thanks
> >>
> >> Wisely Chen
> >>
> >>
> >> On Tue, Mar 10, 2015 at 5:06 PM, Pei-Lun Lee <pllee@appier.com> wrote:
> >>
> >> > Hi,
> >> >
> >> > I found that if I try to read parquet file generated by spark 1.1.1
> >> using
> >> > 1.3.0-rc3 by default settings, I got this error:
> >> >
> >> > com.fasterxml.jackson.core.JsonParseException: Unrecognized token
> >> > 'StructType': was expecting ('true', 'false' or 'null')
> >> >  at [Source: StructType(List(StructField(a,IntegerType,false))); line:
> >> 1,
> >> > column: 11]
> >> >         at
> >> >
> >>
> com.fasterxml.jackson.core.JsonParser._constructError(JsonParser.java:1419)
> >> >         at
> >> >
> >> >
> >>
> com.fasterxml.jackson.core.base.ParserMinimalBase._reportError(ParserMinimalBase.java:508)
> >> >         at
> >> >
> >> >
> >>
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._reportInvalidToken(ReaderBasedJsonParser.java:2300)
> >> >         at
> >> >
> >> >
> >>
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser._handleOddValue(ReaderBasedJsonParser.java:1459)
> >> >         at
> >> >
> >> >
> >>
> com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:683)
> >> >         at
> >> >
> >> >
> >>
> com.fasterxml.jackson.databind.ObjectMapper._initForReading(ObjectMapper.java:3105)
> >> >         at
> >> >
> >> >
> >>
> com.fasterxml.jackson.databind.ObjectMapper._readMapAndClose(ObjectMapper.java:3051)
> >> >         at
> >> >
> >> >
> >>
> com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2161)
> >> >         at
> >> org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:19)
> >> >         at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:44)
> >> >         at
> >> > org.apache.spark.sql.types.DataType$.fromJson(dataTypes.scala:41)
> >> >         at
> >> >
> >> >
> >>
> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
> >> >         at
> >> >
> >> >
> >>
> org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$readSchema$1$$anonfun$25.apply(newParquet.scala:675)
> >> >
> >> >
> >> >
> >> > this is how I save parquet file with 1.1.1:
> >> >
> >> > sql("select 1 as a").saveAsParquetFile("/tmp/foo")
> >> >
> >> >
> >> >
> >> > and this is the meta data of the 1.1.1 parquet file:
> >> >
> >> > creator:     parquet-mr version 1.4.3
> >> > extra:       org.apache.spark.sql.parquet.row.metadata =
> >> > StructType(List(StructField(a,IntegerType,false)))
> >> >
> >> >
> >> >
> >> > by comparison, this is 1.3.0 meta:
> >> >
> >> > creator:     parquet-mr version 1.6.0rc3
> >> > extra:       org.apache.spark.sql.parquet.row.metadata =
> >> > {"type":"struct","fields":[{"name":"a","type":"integer","nullable":t
> >> > [more]...
> >> >
> >> >
> >> >
> >> > It looks like now ParquetRelation2 is used to load parquet file by
> >> default
> >> > and it only recognizes JSON format schema but 1.1.1 schema was case
> >> class
> >> > string format.
> >> >
> >> > Setting spark.sql.parquet.useDataSourceApi to false will fix it, but I
> >> > don't know the differences.
> >> > Is this considered a bug? We have a lot of parquet files from 1.1.1,
> >> should
> >> > we disable data source api in order to read them if we want to upgrade
> >> to
> >> > 1.3?
> >> >
> >> > Thanks,
> >> > --
> >> > Pei-Lun
> >> >
> >>
> >
> >
>

--001a1139c9ae91591b05115f1242--

From dev-return-12011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 03:08:48 2015
Return-Path: <dev-return-12011-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 94C8617751
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 03:08:48 +0000 (UTC)
Received: (qmail 77149 invoked by uid 500); 16 Mar 2015 03:08:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77069 invoked by uid 500); 16 Mar 2015 03:08:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77056 invoked by uid 99); 16 Mar 2015 03:08:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:08:40 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lonely8658@gmail.com designates 209.85.214.173 as permitted sender)
Received: from [209.85.214.173] (HELO mail-ob0-f173.google.com) (209.85.214.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:08:35 +0000
Received: by obbgg8 with SMTP id gg8so26434834obb.1
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 20:08:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=sd+fhNN0HC1bu2tf4zknHPeHJG+bcFHJFWmEmaF2oqc=;
        b=vZS2T23CEg+mvQpJAIGuh4UcFsOyhpEE14eKus1Q2Rc5jiak8WwI1k/kJXPJKJttg0
         2LcV8igeXeb4g7YcRevt3AgQkmB2U+I4dPFodfSLYnxU7ogKzXt+kSAAIKXLsq9agFob
         t/sAqm8CYDHvezpCSn9/G7t5cUIn/GJB62voboJXh0lnyqNI4sXuc6akP9iLXEsisWXz
         DTMmg5lmbQR8i5auGGz2AgaJTvkHK0RWxtzKf7RT4VwfymEu7I/QvOUpNhOLGCCY2Zyh
         xbIaunNWIc6Z1CAuhbyGrkaQejaAuY5ZNQSTRWjHv9xrvMaRaK+7bd2rx6mHTW9r76jm
         wNvw==
MIME-Version: 1.0
X-Received: by 10.60.131.206 with SMTP id oo14mr36392799oeb.30.1426475294848;
 Sun, 15 Mar 2015 20:08:14 -0700 (PDT)
Received: by 10.76.123.143 with HTTP; Sun, 15 Mar 2015 20:08:14 -0700 (PDT)
Date: Mon, 16 Mar 2015 11:08:14 +0800
Message-ID: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
Subject: broadcast hang out
From: lonely Feb <lonely8658@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b4724ac3afefa05115f29b4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4724ac3afefa05115f29b4
Content-Type: text/plain; charset=UTF-8

Hi all, i meet up with a problem that torrent broadcast hang out in my
spark cluster (1.2, standalone) , particularly serious when driver and
executors are cross-region. when i read the code of broadcast i found that
a sync block read here:

  def fetchBlockSync(host: String, port: Int, execId: String, blockId:
String): ManagedBuffer = {
    // A monitor for the thread to wait on.
    val result = Promise[ManagedBuffer]()
    fetchBlocks(host, port, execId, Array(blockId),
      new BlockFetchingListener {
        override def onBlockFetchFailure(blockId: String, exception:
Throwable): Unit = {
          result.failure(exception)
        }
        override def onBlockFetchSuccess(blockId: String, data:
ManagedBuffer): Unit = {
          val ret = ByteBuffer.allocate(data.size.toInt)
          ret.put(data.nioByteBuffer())
          ret.flip()
          result.success(new NioManagedBuffer(ret))
        }
      })

    Await.result(result.future, Duration.Inf)
  }

it seems that fetchBlockSync method does not have a timeout limit but wait
forever ? Anybody can show me how to control the timeout here?

--047d7b4724ac3afefa05115f29b4--

From dev-return-12012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 03:27:50 2015
Return-Path: <dev-return-12012-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F2E2C1778F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 03:27:50 +0000 (UTC)
Received: (qmail 99393 invoked by uid 500); 16 Mar 2015 03:27:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99328 invoked by uid 500); 16 Mar 2015 03:27:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99317 invoked by uid 99); 16 Mar 2015 03:27:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:27:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:27:38 +0000
Received: by qgg60 with SMTP id 60so30349215qgg.3
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 20:27:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=hlUsycwt1iQKNyAI8MKJKE/628r7IW2ASwpIn6TcAfA=;
        b=HFIIto18VWhQfUkSJm8plCzuDKIpo0FKtUeUjDaPregIkX5Ekxf6+PtADKhi5049so
         6HZsppUNRSWWoGuTj57c4LellPs/q2mKTMpyVhZ17VZuk+HWvBDq6bUhuYE6hjrOepyY
         ZYDozJLNw9aGIeJhw4VXjcNzpf1JVUtwATsQoJjRcOEk5WzFHZeM8I/Pdu/OcBubsfq+
         YEuY5Nsf54YHNh5dQK7FwRSp0uNL0erS+wi82xpDNbKNWPiE06NCC90yW1T6arXd7pZb
         uv0lQutxja1Ln8JVIRJS2q7Usw4T4jDcOYDIWwXc+Dv1L8Q5pQSCsKYo8nuWVmchU3fX
         zNgQ==
X-Gm-Message-State: ALoCoQnEB6i9xF5DH0tfNIVK5tVPkL1Q+RxTffkr9CVgV5I5/C26RgQk2N93DUydqjLCdCV5u9SC
MIME-Version: 1.0
X-Received: by 10.140.101.22 with SMTP id t22mr72955827qge.9.1426476437596;
 Sun, 15 Mar 2015 20:27:17 -0700 (PDT)
Received: by 10.96.154.229 with HTTP; Sun, 15 Mar 2015 20:27:17 -0700 (PDT)
In-Reply-To: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
References: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
Date: Sun, 15 Mar 2015 20:27:17 -0700
Message-ID: <CAPYnQ0Xo=jZwhhMjgsjuLineL92+N0GhHnVjiiOY8WAJ+Q7gnA@mail.gmail.com>
Subject: Re: broadcast hang out
From: Chester Chen <chester@alpinenow.com>
To: lonely Feb <lonely8658@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c16ffa581a2505115f6de0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c16ffa581a2505115f6de0
Content-Type: text/plain; charset=UTF-8

can you just replace "Duration.Inf" with a shorter duration  ? how about

      import scala.concurrent.duration._
      val timeout = new Timeout(10 seconds)
      Await.result(result.future, timeout.duration)

      or

      val timeout = new FiniteDuration(10, TimeUnit.SECONDS)
      Await.result(result.future, timeout)

      or simply
      import scala.concurrent.duration._
      Await.result(result.future, 10 seconds)



On Sun, Mar 15, 2015 at 8:08 PM, lonely Feb <lonely8658@gmail.com> wrote:

> Hi all, i meet up with a problem that torrent broadcast hang out in my
> spark cluster (1.2, standalone) , particularly serious when driver and
> executors are cross-region. when i read the code of broadcast i found that
> a sync block read here:
>
>   def fetchBlockSync(host: String, port: Int, execId: String, blockId:
> String): ManagedBuffer = {
>     // A monitor for the thread to wait on.
>     val result = Promise[ManagedBuffer]()
>     fetchBlocks(host, port, execId, Array(blockId),
>       new BlockFetchingListener {
>         override def onBlockFetchFailure(blockId: String, exception:
> Throwable): Unit = {
>           result.failure(exception)
>         }
>         override def onBlockFetchSuccess(blockId: String, data:
> ManagedBuffer): Unit = {
>           val ret = ByteBuffer.allocate(data.size.toInt)
>           ret.put(data.nioByteBuffer())
>           ret.flip()
>           result.success(new NioManagedBuffer(ret))
>         }
>       })
>
>     Await.result(result.future, Duration.Inf)
>   }
>
> it seems that fetchBlockSync method does not have a timeout limit but wait
> forever ? Anybody can show me how to control the timeout here?
>

--001a11c16ffa581a2505115f6de0--

From dev-return-12013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 03:32:50 2015
Return-Path: <dev-return-12013-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6F3BF177A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 03:32:50 +0000 (UTC)
Received: (qmail 6455 invoked by uid 500); 16 Mar 2015 03:32:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6372 invoked by uid 500); 16 Mar 2015 03:32:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6360 invoked by uid 99); 16 Mar 2015 03:32:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:32:48 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lonely8658@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:32:44 +0000
Received: by obdfc2 with SMTP id fc2so26671498obd.3
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 20:31:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=aqcg9cbJX5opwiEaeWRL60+oEbm1PYTP3AVtEHSlyDE=;
        b=Yil+Clm9LOoBxjHCszDWKiWtHf8HdWGBvPbq4ql2tPySRCx0R0fzx0ewcV6rqfA5jx
         B1DC295Qa4HZ/6CS1ARdXbjSuOKTUbrMfUINFamd4p+5JtIBUFkO6rdRFkw7QxHOYFNa
         agiSzHgW1EWDHsKHIWvUWqDMHsnxshMc9LdhDm4UOa9+k8Lx/vHlehQ9QuddgkBcBK7z
         u8ev4ugl6mjRp1cqcloeef33/oGwBpHxFzfGUmA1o4rtqsM/m3gh9/cfVgTAjDJoGs5j
         GCD2V803LLi/PaGAJRyBuXikgG53jxRstwM8AlrKcTfsGHooI18og+MgV7zB22AcXO3r
         d01Q==
MIME-Version: 1.0
X-Received: by 10.182.19.132 with SMTP id f4mr36521845obe.8.1426476698870;
 Sun, 15 Mar 2015 20:31:38 -0700 (PDT)
Received: by 10.76.123.143 with HTTP; Sun, 15 Mar 2015 20:31:38 -0700 (PDT)
In-Reply-To: <CAPYnQ0Xo=jZwhhMjgsjuLineL92+N0GhHnVjiiOY8WAJ+Q7gnA@mail.gmail.com>
References: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
	<CAPYnQ0Xo=jZwhhMjgsjuLineL92+N0GhHnVjiiOY8WAJ+Q7gnA@mail.gmail.com>
Date: Mon, 16 Mar 2015 11:31:38 +0800
Message-ID: <CAPszQwi7ur5hZsMSB2rMvgpQJ-XXPJDS=FZ8Qus6CBGO10BoZQ@mail.gmail.com>
Subject: Re: broadcast hang out
From: lonely Feb <lonely8658@gmail.com>
To: Chester Chen <chester@alpinenow.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2aa6aead54805115f7c05
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2aa6aead54805115f7c05
Content-Type: text/plain; charset=UTF-8

Thx. But this method is in BlockTransferService.scala of spark which i can
not replace unless i rewrite the core code. I wonder if it is handled
somewhere already.

2015-03-16 11:27 GMT+08:00 Chester Chen <chester@alpinenow.com>:

> can you just replace "Duration.Inf" with a shorter duration  ? how about
>
>       import scala.concurrent.duration._
>       val timeout = new Timeout(10 seconds)
>       Await.result(result.future, timeout.duration)
>
>       or
>
>       val timeout = new FiniteDuration(10, TimeUnit.SECONDS)
>       Await.result(result.future, timeout)
>
>       or simply
>       import scala.concurrent.duration._
>       Await.result(result.future, 10 seconds)
>
>
>
> On Sun, Mar 15, 2015 at 8:08 PM, lonely Feb <lonely8658@gmail.com> wrote:
>
>> Hi all, i meet up with a problem that torrent broadcast hang out in my
>> spark cluster (1.2, standalone) , particularly serious when driver and
>> executors are cross-region. when i read the code of broadcast i found that
>> a sync block read here:
>>
>>   def fetchBlockSync(host: String, port: Int, execId: String, blockId:
>> String): ManagedBuffer = {
>>     // A monitor for the thread to wait on.
>>     val result = Promise[ManagedBuffer]()
>>     fetchBlocks(host, port, execId, Array(blockId),
>>       new BlockFetchingListener {
>>         override def onBlockFetchFailure(blockId: String, exception:
>> Throwable): Unit = {
>>           result.failure(exception)
>>         }
>>         override def onBlockFetchSuccess(blockId: String, data:
>> ManagedBuffer): Unit = {
>>           val ret = ByteBuffer.allocate(data.size.toInt)
>>           ret.put(data.nioByteBuffer())
>>           ret.flip()
>>           result.success(new NioManagedBuffer(ret))
>>         }
>>       })
>>
>>     Await.result(result.future, Duration.Inf)
>>   }
>>
>> it seems that fetchBlockSync method does not have a timeout limit but wait
>> forever ? Anybody can show me how to control the timeout here?
>>
>
>

--001a11c2aa6aead54805115f7c05--

From dev-return-12014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 03:45:05 2015
Return-Path: <dev-return-12014-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7CC7E178CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 03:45:05 +0000 (UTC)
Received: (qmail 78054 invoked by uid 500); 16 Mar 2015 03:45:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77977 invoked by uid 500); 16 Mar 2015 03:45:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77965 invoked by uid 99); 16 Mar 2015 03:45:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:45:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:44:36 +0000
Received: by qcto4 with SMTP id o4so33753237qct.3
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 20:43:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=7uynMoaI9tVRDV2RuQsBBeMmgCTgYvO9vFubrTS31qI=;
        b=kqo57ROPL26HTtO747fsKAcux3yhPFkEBTMS8WMsdzCR2a1ZrQPITHrogMRU8127Yh
         4RdYMt5hMLQSxzyaeFkWhatd5kRRPAi4oKRSwN3LFc7FQzzcHSYuqqafGGkEcnxRiTmJ
         YiizhBwdEOax79PUb/hmx1wOCgHcHoM2kjn0K9HfzPx6qUYB4LewF86qMrDO31aKH1NS
         0QIArqdO+6upxwm3YQCeChxlIs0mFCe5OC6+1zbN6W4P76fTKwoRvUmo6b/PD610Ebcl
         QcVI0QmuNZf4z7Fgrc8Ics0b7drraMo43J0JcWzmVWeojYBu+UPGZDxt+yBIXygmNwMK
         5V0w==
MIME-Version: 1.0
X-Received: by 10.140.108.201 with SMTP id j67mr70532517qgf.86.1426477384801;
 Sun, 15 Mar 2015 20:43:04 -0700 (PDT)
Received: by 10.140.33.131 with HTTP; Sun, 15 Mar 2015 20:43:04 -0700 (PDT)
In-Reply-To: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
References: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
Date: Sun, 15 Mar 2015 20:43:04 -0700
Message-ID: <CAJiQeYJkBG5p43DoTCeF4O+Fx7baEoxcc7jc-qL-1=doP9eHUg@mail.gmail.com>
Subject: Re: broadcast hang out
From: Mridul Muralidharan <mridul@gmail.com>
To: lonely Feb <lonely8658@gmail.com>
Cc: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Cross region as in different data centers ?

- Mridul

On Sun, Mar 15, 2015 at 8:08 PM, lonely Feb <lonely8658@gmail.com> wrote:
> Hi all, i meet up with a problem that torrent broadcast hang out in my
> spark cluster (1.2, standalone) , particularly serious when driver and
> executors are cross-region. when i read the code of broadcast i found that
> a sync block read here:
>
>   def fetchBlockSync(host: String, port: Int, execId: String, blockId:
> String): ManagedBuffer = {
>     // A monitor for the thread to wait on.
>     val result = Promise[ManagedBuffer]()
>     fetchBlocks(host, port, execId, Array(blockId),
>       new BlockFetchingListener {
>         override def onBlockFetchFailure(blockId: String, exception:
> Throwable): Unit = {
>           result.failure(exception)
>         }
>         override def onBlockFetchSuccess(blockId: String, data:
> ManagedBuffer): Unit = {
>           val ret = ByteBuffer.allocate(data.size.toInt)
>           ret.put(data.nioByteBuffer())
>           ret.flip()
>           result.success(new NioManagedBuffer(ret))
>         }
>       })
>
>     Await.result(result.future, Duration.Inf)
>   }
>
> it seems that fetchBlockSync method does not have a timeout limit but wait
> forever ? Anybody can show me how to control the timeout here?

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 03:47:05 2015
Return-Path: <dev-return-12015-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F8F5178D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 03:47:05 +0000 (UTC)
Received: (qmail 82078 invoked by uid 500); 16 Mar 2015 03:47:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81995 invoked by uid 500); 16 Mar 2015 03:47:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81982 invoked by uid 99); 16 Mar 2015 03:47:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:47:04 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lonely8658@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 03:46:58 +0000
Received: by oiaz123 with SMTP id z123so27681558oia.3
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 20:45:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=pBxqnaWQJdLse8xi5MYU2BUL6h2gmoYj6uYZGWK1N10=;
        b=nYN/Ck2uSu1k2SR0IdiFlTbJp4Ak8+pPa4hdW8KJCVJKmnlFVQ3AAG9uJiaSGA8oWV
         lOM1diWBnmiSUFM5JJr9cIbzsfB9wGi5GmkIBreIwS3E+8zcY3BK0PMJbgTwRUmgcKeR
         5CvY4MEoBI9PD4XnqGeU03Cp5VBTVMXBd79oVxuiLNdcf5vXWHg7vw3W97KYZThwyY5L
         FIKTiqeyBOYP57LPJxNoLq4Nb5ENlazdE+EitZ1agG2xyOMa2JIX3wMuMjzYvgAQ5a3H
         fLVvk/JaXeJ0V7tu4NVa2dECysIJ9yIZaemNFmvyuTz6P/1xqhl/aGHrQFUfNEYuiUHP
         PLYg==
MIME-Version: 1.0
X-Received: by 10.60.131.206 with SMTP id oo14mr36460734oeb.30.1426477507682;
 Sun, 15 Mar 2015 20:45:07 -0700 (PDT)
Received: by 10.76.123.143 with HTTP; Sun, 15 Mar 2015 20:45:07 -0700 (PDT)
In-Reply-To: <CAJiQeYJkBG5p43DoTCeF4O+Fx7baEoxcc7jc-qL-1=doP9eHUg@mail.gmail.com>
References: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
	<CAJiQeYJkBG5p43DoTCeF4O+Fx7baEoxcc7jc-qL-1=doP9eHUg@mail.gmail.com>
Date: Mon, 16 Mar 2015 11:45:07 +0800
Message-ID: <CAPszQwg=Lsj4MOkBE=_tTHH3mK5Md21rDEEbYk+qJv0+zxqUQg@mail.gmail.com>
Subject: Re: broadcast hang out
From: lonely Feb <lonely8658@gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b4724ac204bda05115fad86
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4724ac204bda05115fad86
Content-Type: text/plain; charset=UTF-8

yes

2015-03-16 11:43 GMT+08:00 Mridul Muralidharan <mridul@gmail.com>:

> Cross region as in different data centers ?
>
> - Mridul
>
> On Sun, Mar 15, 2015 at 8:08 PM, lonely Feb <lonely8658@gmail.com> wrote:
> > Hi all, i meet up with a problem that torrent broadcast hang out in my
> > spark cluster (1.2, standalone) , particularly serious when driver and
> > executors are cross-region. when i read the code of broadcast i found
> that
> > a sync block read here:
> >
> >   def fetchBlockSync(host: String, port: Int, execId: String, blockId:
> > String): ManagedBuffer = {
> >     // A monitor for the thread to wait on.
> >     val result = Promise[ManagedBuffer]()
> >     fetchBlocks(host, port, execId, Array(blockId),
> >       new BlockFetchingListener {
> >         override def onBlockFetchFailure(blockId: String, exception:
> > Throwable): Unit = {
> >           result.failure(exception)
> >         }
> >         override def onBlockFetchSuccess(blockId: String, data:
> > ManagedBuffer): Unit = {
> >           val ret = ByteBuffer.allocate(data.size.toInt)
> >           ret.put(data.nioByteBuffer())
> >           ret.flip()
> >           result.success(new NioManagedBuffer(ret))
> >         }
> >       })
> >
> >     Await.result(result.future, Duration.Inf)
> >   }
> >
> > it seems that fetchBlockSync method does not have a timeout limit but
> wait
> > forever ? Anybody can show me how to control the timeout here?
>

--047d7b4724ac204bda05115fad86--

From dev-return-12016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 04:39:44 2015
Return-Path: <dev-return-12016-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 381171799C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 04:39:44 +0000 (UTC)
Received: (qmail 46423 invoked by uid 500); 16 Mar 2015 04:39:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46346 invoked by uid 500); 16 Mar 2015 04:39:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46313 invoked by uid 99); 16 Mar 2015 04:39:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 04:39:42 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 04:39:37 +0000
Received: by igcqo1 with SMTP id qo1so31043468igc.0
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 21:38:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=v3xcE+07D3zUXUYY41HtMBVQR2NCzM/XxRskOvoQs2M=;
        b=Joa/hkkx+d/d0CLtFJbX94EzHsiKHbp9BZJz5mJlcZpVMRAVK/H9JyMWVz0e8az8E0
         dYlJPnszrRZ4N8zuIqx/l39OSobMCtnf0bwAj4cdnL+dqrk92v0/Q/Z4XBHcfG0KxstW
         tjtrpXmi0LFN53QHPy70dq5SecxREeMX0XXws0LBLSNvZAS2SpK3xsqnfetfQsKqmB/9
         pZhwxcRLOtXvHQnCQL5KaCjCe3E01pS5v1ymlnuKgWFTaDdcq4/2f/uMgUe/leJXhObB
         xguYx9eYuGbXhxbVlmRr3JsiabVzZZK2n9PhsFy0I2nvI4zN2gEL+LyuIrqVAKlgCp0+
         dxFQ==
MIME-Version: 1.0
X-Received: by 10.42.147.9 with SMTP id l9mr74288609icv.41.1426480712464; Sun,
 15 Mar 2015 21:38:32 -0700 (PDT)
Received: by 10.107.150.134 with HTTP; Sun, 15 Mar 2015 21:38:32 -0700 (PDT)
In-Reply-To: <1426407287506-11058.post@n3.nabble.com>
References: <1426405522047-11056.post@n3.nabble.com>
	<1426407287506-11058.post@n3.nabble.com>
Date: Sun, 15 Mar 2015 21:38:32 -0700
Message-ID: <CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
Subject: Re: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
From: David Hall <david.lw.hall@gmail.com>
To: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
Cc: "<dev@spark.apache.org>" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba2121db2542800511606cd5
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba2121db2542800511606cd5
Content-Type: text/plain; charset=UTF-8

snapshot is pushed. If you verify I'll publish the new artifacts.

On Sun, Mar 15, 2015 at 1:14 AM, Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
wrote:

> David Hall who is a breeze creator told me that it's a bug. So, I made a
> jira
> ticket about this issue. We need to upgrade breeze from 0.11.1 to 0.11.2 or
> later in order to fix the bug, when the new version of breeze will be
> released.
>
> [SPARK-6341] Upgrade breeze from 0.11.1 to 0.11.2 or later - ASF JIRA
> https://issues.apache.org/jira/browse/SPARK-6341
>
> Thanks,
> Yu Ishikawa
>
>
>
> -----
> -- Yu Ishikawa
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056p11058.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--90e6ba2121db2542800511606cd5--

From dev-return-12017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 05:43:03 2015
Return-Path: <dev-return-12017-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1F95717AC4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 05:43:03 +0000 (UTC)
Received: (qmail 39146 invoked by uid 500); 16 Mar 2015 05:43:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39062 invoked by uid 500); 16 Mar 2015 05:43:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39050 invoked by uid 99); 16 Mar 2015 05:43:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 05:43:01 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lonely8658@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 05:42:56 +0000
Received: by obdfc2 with SMTP id fc2so28091712obd.3
        for <dev@spark.apache.org>; Sun, 15 Mar 2015 22:41:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=msr0gsYhDF94Imdf5Asn7EAHgLRIAut8F9G1IsYIy9U=;
        b=yuVfhCiU383RYPHSTPFlyvO80OLZfavXIFEkn8ncDyEBNtiQ818NQ8k2YRK8XcPg2d
         BqB1zWlrmUmKJ0jHY+GM1DOnpPaeyosbYJxoz+UWYYgF4amZsTNmnHQsPGsUhP4hN83o
         sLrUk46GokZSK0PkXNjvyyz8aPN9v81mkqNjIYXglvt0uw3762nuaLIIc/ZySoh4p8g5
         t8rtqvts49Eg4vIWUUaAYWc5Q05+KevkZanpq4AZWvxV5UvaZ00pOu/EdkrLS/nMdxVW
         eiPFUtn7AbazqOSD1rGPvLWgdjgwacyuaZ6qd0/Ked7jPKQi3VT5tvpE2poXNpG2jpKT
         1o4w==
MIME-Version: 1.0
X-Received: by 10.202.171.213 with SMTP id u204mr15234581oie.123.1426484465554;
 Sun, 15 Mar 2015 22:41:05 -0700 (PDT)
Received: by 10.76.123.143 with HTTP; Sun, 15 Mar 2015 22:41:05 -0700 (PDT)
In-Reply-To: <CAPszQwg=Lsj4MOkBE=_tTHH3mK5Md21rDEEbYk+qJv0+zxqUQg@mail.gmail.com>
References: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
	<CAJiQeYJkBG5p43DoTCeF4O+Fx7baEoxcc7jc-qL-1=doP9eHUg@mail.gmail.com>
	<CAPszQwg=Lsj4MOkBE=_tTHH3mK5Md21rDEEbYk+qJv0+zxqUQg@mail.gmail.com>
Date: Mon, 16 Mar 2015 13:41:05 +0800
Message-ID: <CAPszQwg+tu_NnL9mvAC0gpkO3_HXJ7-XAo2pnQGK2kco4ukPuA@mail.gmail.com>
Subject: Re: broadcast hang out
From: lonely Feb <lonely8658@gmail.com>
To: Mridul Muralidharan <mridul@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113c385ad8dcd90511614b44
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113c385ad8dcd90511614b44
Content-Type: text/plain; charset=UTF-8

Anyone can help? Thanks a lot !

2015-03-16 11:45 GMT+08:00 lonely Feb <lonely8658@gmail.com>:

> yes
>
> 2015-03-16 11:43 GMT+08:00 Mridul Muralidharan <mridul@gmail.com>:
>
>> Cross region as in different data centers ?
>>
>> - Mridul
>>
>> On Sun, Mar 15, 2015 at 8:08 PM, lonely Feb <lonely8658@gmail.com> wrote:
>> > Hi all, i meet up with a problem that torrent broadcast hang out in my
>> > spark cluster (1.2, standalone) , particularly serious when driver and
>> > executors are cross-region. when i read the code of broadcast i found
>> that
>> > a sync block read here:
>> >
>> >   def fetchBlockSync(host: String, port: Int, execId: String, blockId:
>> > String): ManagedBuffer = {
>> >     // A monitor for the thread to wait on.
>> >     val result = Promise[ManagedBuffer]()
>> >     fetchBlocks(host, port, execId, Array(blockId),
>> >       new BlockFetchingListener {
>> >         override def onBlockFetchFailure(blockId: String, exception:
>> > Throwable): Unit = {
>> >           result.failure(exception)
>> >         }
>> >         override def onBlockFetchSuccess(blockId: String, data:
>> > ManagedBuffer): Unit = {
>> >           val ret = ByteBuffer.allocate(data.size.toInt)
>> >           ret.put(data.nioByteBuffer())
>> >           ret.flip()
>> >           result.success(new NioManagedBuffer(ret))
>> >         }
>> >       })
>> >
>> >     Await.result(result.future, Duration.Inf)
>> >   }
>> >
>> > it seems that fetchBlockSync method does not have a timeout limit but
>> wait
>> > forever ? Anybody can show me how to control the timeout here?
>>
>
>

--001a113c385ad8dcd90511614b44--

From dev-return-12018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 08:04:54 2015
Return-Path: <dev-return-12018-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E63E417E44
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 08:04:53 +0000 (UTC)
Received: (qmail 18468 invoked by uid 500); 16 Mar 2015 08:04:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18268 invoked by uid 500); 16 Mar 2015 08:04:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18199 invoked by uid 99); 16 Mar 2015 08:04:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 08:04:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pllee@appier.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 08:04:48 +0000
Received: by qcaz10 with SMTP id z10so36908776qca.1
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 01:03:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=l0jTB7BcAIBzUXoOenFMXw4t3sPd5nBJ6GwO5TJtslk=;
        b=g5VFe87ySyntAHNJ5RrBuhoIgVFtF4Nz0tqUmzYxqcKqmEEyPRfgdVvi2xuAMhRpb1
         nJreHQ4guufo7timIUm5CK1A3xooY/4hh80TQNUNc5V+wjHY8d9PfnvGjFMgQ2/2MGiJ
         CNZr+AjFAJqHfC82SevCzZkkxMSoehzEXxJlc=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=l0jTB7BcAIBzUXoOenFMXw4t3sPd5nBJ6GwO5TJtslk=;
        b=R+YepcmmY3Z60NiCIgWdg7gFh2p1kTmyxsYn9+RF4z3QtinqNpHLvwzvg0L1lZj2/f
         K8yk0bBrZ+P1n3UXH9FVrCsgsSY2D4DxARvdn9AJBduHMRDdoSZ9RiXqoT89or9yhG9k
         ImhcSmyVeLt6Xmgs/j5b22/lLo44gakeb4XhennA2wINa98CEcUzAizW+VdYG1wnZTPX
         TyRfWxgwLtqw+yOqu42pbFbm+VspH31Vx7JPFD5VPyED8TpxolB313KNdSiBXBcH1ynU
         lmzmGnsX/0pyvutkThe7MK8bFT0RzhnxdClVQCKfrGMnE21q54krPFIFqwxT2AYPMXB+
         8UPA==
X-Gm-Message-State: ALoCoQlB+cYWKsioLxrq6U13hV3G7UTUGT03j3a96ktQbHz4ugkdoTsz5AJ8bgqI4fN2Afjy56f3
MIME-Version: 1.0
X-Received: by 10.140.145.3 with SMTP id 3mr77377749qhr.42.1426493021921; Mon,
 16 Mar 2015 01:03:41 -0700 (PDT)
Received: by 10.229.233.136 with HTTP; Mon, 16 Mar 2015 01:03:41 -0700 (PDT)
Date: Mon, 16 Mar 2015 16:03:41 +0800
Message-ID: <CANrtgzU-e7kWkNafRyOSDO290rgJMwh-ZQz2APYhb5dPOfFJFg@mail.gmail.com>
Subject: SparkSQL 1.3.0 cannot read parquet files from different file system
From: Pei-Lun Lee <pllee@appier.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1137471cd8baf90511634926
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137471cd8baf90511634926
Content-Type: text/plain; charset=UTF-8

Hi,

I am using Spark 1.3.0, where I cannot load parquet files from more than
one file system, say one s3n://... and another hdfs://..., which worked in
older version, or if I set spark.sql.parquet.useDataSourceApi=false in 1.3.

One way to fix this is instead of get a single FileSystem from default
configuration in ParquetRelation2, call Path.getFileSystem for each path.

Here's the JIRA link and pull request:
https://issues.apache.org/jira/browse/SPARK-6351
https://github.com/apache/spark/pull/5039

Thanks,
--
Pei-Lun

--001a1137471cd8baf90511634926--

From dev-return-12019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 09:24:30 2015
Return-Path: <dev-return-12019-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F27A17335
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 09:24:30 +0000 (UTC)
Received: (qmail 85526 invoked by uid 500); 16 Mar 2015 09:24:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85402 invoked by uid 500); 16 Mar 2015 09:24:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84557 invoked by uid 99); 16 Mar 2015 09:24:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 09:24:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pllee@appier.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 09:24:23 +0000
Received: by qcto4 with SMTP id o4so38006253qct.3
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 02:23:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0E+azqjEvuPKWDVNWEMg8GUtQ5ud0M+hq6cQ6KasKmY=;
        b=iIye2ZQPmA9eFw3Kfy02Y5QEjRvFddwbm3S2CAWwGIqJQmcek7gdT0vjh0bDjPK9rw
         92ecEDn3j+jlHaicLwviPYciZyFrqcRiwG05c3++Hnynrx55MVLnSolyjtu9EGwNe3Sh
         g/pLi8Ub9rWKPoaBXt6yHgmF3csrUrGqFEI1U=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=0E+azqjEvuPKWDVNWEMg8GUtQ5ud0M+hq6cQ6KasKmY=;
        b=eJO31bvcm7IbOs9n8mGqnYY+YNRgLRNtoYEob5lV7xAfDGr5D+VN7KgotLOnxY0UxA
         NL+LIjWx/2g8SlfrRS/MCqPzP3a+QvCTEExToQXnb3ElwJkdPFQ+e/M8KY49hDxLvEgr
         h40C4kUJMN5QYmXm181d2HwPaNd0IVhWu6oKLj3CfcBFxz2aWPO0bq9wy1cV08LyZ0mx
         odPOrdQEmNJO8WIRJU5+c+Al91peDHHYS5suMfAtiM2e/z7B4WFMMdMz8Juc1ZFPDABf
         VF2Qr0c5tYK0r1xxa3a+OrvRBr5Z6PBhSt+Q4sOoSiTop3CNToY8FLUjTx8YzZTeVZdc
         q5ZQ==
X-Gm-Message-State: ALoCoQl4mQ4LH2AuNWmFQaoDfIyQiigX8cD9KdoTsznX8By+o1NDN/N3QSfCZwytMWzDpJ0PKGQb
MIME-Version: 1.0
X-Received: by 10.140.46.7 with SMTP id j7mr72197574qga.12.1426497797217; Mon,
 16 Mar 2015 02:23:17 -0700 (PDT)
Received: by 10.229.233.136 with HTTP; Mon, 16 Mar 2015 02:23:17 -0700 (PDT)
In-Reply-To: <CANGvG8pQD8A84a6VOXXjo8VcGnXYBadYZr=oYAgdaQnJLY7Xnw@mail.gmail.com>
References: <D110C7BA.1DBB7%mkim@palantir.com>
	<483876581.4513560.1424722604100.JavaMail.yahoo@mail.yahoo.com>
	<1508291672.8549572.1424726193790.JavaMail.yahoo@mail.yahoo.com>
	<CAH70K74N+hu2r5XEZOYFg0sDjurCZuuLvYeUvF8E9UyUw81GQg@mail.gmail.com>
	<CANrtgzUBgyF4WtZunieeKhPiL_rxwUEhHcQ8GDQ0EPvE4WGv1g@mail.gmail.com>
	<CANGvG8pQD8A84a6VOXXjo8VcGnXYBadYZr=oYAgdaQnJLY7Xnw@mail.gmail.com>
Date: Mon, 16 Mar 2015 17:23:17 +0800
Message-ID: <CANrtgzXP-5DbN7TG_kWop4OqYJbvr7=ZY9ufo_7MevRpWtnQCA@mail.gmail.com>
Subject: Re: Which OutputCommitter to use for S3?
From: Pei-Lun Lee <pllee@appier.com>
To: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113959bc79fa7305116466ae
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113959bc79fa7305116466ae
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

I created a JIRA and PR for supporting a s3 friendly output committer for
saveAsParquetFile:
https://issues.apache.org/jira/browse/SPARK-6352
https://github.com/apache/spark/pull/5042

My approach is add a DirectParquetOutputCommitter class in spark-sql
package and use a boolean config variable
spark.sql.parquet.useDirectParquetOutputCommitter to choose between default
output committer.
This may not be the smartest solution but it works for me.
Tested on spark 1.1, 1.3 with hadoop 1.0.4.


On Thu, Mar 5, 2015 at 4:32 PM, Aaron Davidson <ilikerps@gmail.com> wrote:

> Yes, unfortunately that direct dependency makes this injection much more
> difficult for saveAsParquetFile.
>
> On Thu, Mar 5, 2015 at 12:28 AM, Pei-Lun Lee <pllee@appier.com> wrote:
>
>> Thanks for the DirectOutputCommitter example.
>> However I found it only works for saveAsHadoopFile. What about
>> saveAsParquetFile?
>> It looks like SparkSQL is using ParquetOutputCommitter, which is subclas=
s
>> of FileOutputCommitter.
>>
>> On Fri, Feb 27, 2015 at 1:52 AM, Thomas Demoor <
>> thomas.demoor@amplidata.com>
>> wrote:
>>
>> > FYI. We're currently addressing this at the Hadoop level in
>> > https://issues.apache.org/jira/browse/HADOOP-9565
>> >
>> >
>> > Thomas Demoor
>> >
>> > On Mon, Feb 23, 2015 at 10:16 PM, Darin McBeath <
>> > ddmcbeath@yahoo.com.invalid> wrote:
>> >
>> >> Just to close the loop in case anyone runs into the same problem I ha=
d.
>> >>
>> >> By setting --hadoop-major-version=3D2 when using the ec2 scripts,
>> >> everything worked fine.
>> >>
>> >> Darin.
>> >>
>> >>
>> >> ----- Original Message -----
>> >> From: Darin McBeath <ddmcbeath@yahoo.com.INVALID>
>> >> To: Mingyu Kim <mkim@palantir.com>; Aaron Davidson <ilikerps@gmail.co=
m
>> >
>> >> Cc: "user@spark.apache.org" <user@spark.apache.org>
>> >> Sent: Monday, February 23, 2015 3:16 PM
>> >> Subject: Re: Which OutputCommitter to use for S3?
>> >>
>> >> Thanks.  I think my problem might actually be the other way around.
>> >>
>> >> I'm compiling with hadoop 2,  but when I startup Spark, using the ec2
>> >> scripts, I don't specify a
>> >> -hadoop-major-version and the default is 1.   I'm guessing that if I
>> make
>> >> that a 2 that it might work correctly.  I'll try it and post a
>> response.
>> >>
>> >>
>> >> ----- Original Message -----
>> >> From: Mingyu Kim <mkim@palantir.com>
>> >> To: Darin McBeath <ddmcbeath@yahoo.com>; Aaron Davidson <
>> >> ilikerps@gmail.com>
>> >> Cc: "user@spark.apache.org" <user@spark.apache.org>
>> >> Sent: Monday, February 23, 2015 3:06 PM
>> >> Subject: Re: Which OutputCommitter to use for S3?
>> >>
>> >> Cool, we will start from there. Thanks Aaron and Josh!
>> >>
>> >> Darin, it=C2=B9s likely because the DirectOutputCommitter is compiled=
 with
>> >> Hadoop 1 classes and you=C2=B9re running it with Hadoop 2.
>> >> org.apache.hadoop.mapred.JobContext used to be a class in Hadoop 1,
>> and it
>> >> became an interface in Hadoop 2.
>> >>
>> >> Mingyu
>> >>
>> >>
>> >>
>> >>
>> >>
>> >> On 2/23/15, 11:52 AM, "Darin McBeath" <ddmcbeath@yahoo.com.INVALID>
>> >> wrote:
>> >>
>> >> >Aaron.  Thanks for the class. Since I'm currently writing Java based
>> >> >Spark applications, I tried converting your class to Java (it seemed
>> >> >pretty straightforward).
>> >> >
>> >> >I set up the use of the class as follows:
>> >> >
>> >> >SparkConf conf =3D new SparkConf()
>> >> >.set("spark.hadoop.mapred.output.committer.class",
>> >> >"com.elsevier.common.DirectOutputCommitter");
>> >> >
>> >> >And I then try and save a file to S3 (which I believe should use the
>> old
>> >> >hadoop apis).
>> >> >
>> >> >JavaPairRDD<Text, Text> newBaselineRDDWritable =3D
>> >> >reducedhsfPairRDD.mapToPair(new ConvertToWritableTypes());
>> >> >newBaselineRDDWritable.saveAsHadoopFile(baselineOutputBucketFile,
>> >> >Text.class, Text.class, SequenceFileOutputFormat.class,
>> >> >org.apache.hadoop.io.compress.GzipCodec.class);
>> >> >
>> >> >But, I get the following error message.
>> >> >
>> >> >Exception in thread "main" java.lang.IncompatibleClassChangeError:
>> Found
>> >> >class org.apache.hadoop.mapred.JobContext, but interface was expecte=
d
>> >> >at
>> >>
>> >>
>> >com.elsevier.common.DirectOutputCommitter.commitJob(DirectOutputCommitt=
er.
>> >> >java:68)
>> >> >at
>> >>
>> >org.apache.spark.SparkHadoopWriter.commitJob(SparkHadoopWriter.scala:12=
7)
>> >> >at
>> >>
>> >>
>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFuncti=
ons
>> >> >.scala:1075)
>> >> >at
>> >>
>> >>
>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions=
.sc
>> >> >ala:940)
>> >> >at
>> >>
>> >>
>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions=
.sc
>> >> >ala:902)
>> >> >at
>> >>
>> >>
>> >org.apache.spark.api.java.JavaPairRDD.saveAsHadoopFile(JavaPairRDD.scal=
a:7
>> >> >71)
>> >> >at com.elsevier.spark.SparkSyncDedup.main(SparkSyncDedup.java:156)
>> >> >
>> >> >In my class, JobContext is an interface of  type
>> >> >org.apache.hadoop.mapred.JobContext.
>> >> >
>> >> >Is there something obvious that I might be doing wrong (or messed up
>> in
>> >> >the translation from Scala to Java) or something I should look into?
>> I'm
>> >> >using Spark 1.2 with hadoop 2.4.
>> >> >
>> >> >
>> >> >Thanks.
>> >> >
>> >> >Darin.
>> >> >
>> >> >
>> >> >________________________________
>> >> >
>> >> >
>> >> >From: Aaron Davidson <ilikerps@gmail.com>
>> >> >To: Andrew Ash <andrew@andrewash.com>
>> >> >Cc: Josh Rosen <rosenville@gmail.com>; Mingyu Kim <mkim@palantir.com
>> >;
>> >> >"user@spark.apache.org" <user@spark.apache.org>; Aaron Davidson
>> >> ><aaron@databricks.com>
>> >> >Sent: Saturday, February 21, 2015 7:01 PM
>> >> >Subject: Re: Which OutputCommitter to use for S3?
>> >> >
>> >> >
>> >> >
>> >> >Here is the class:
>> >> >
>> >>
>> https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__gist.github.com_a=
aron
>> >>
>> >>
>> >dav_c513916e72101bbe14ec&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBr=
Z4tFb6o
>> >>
>> >>
>> >Onmz8&r=3DennQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmuK=
ZRf6sFs
>> >>
>> >zOvl_-ZnxmkBPHo1K24TfGE&s=3DcwSCPKlJO-BJcz4UcGck3xOE2N-4V3eoNvgtFCdMLP8=
&e=3D
>> >> >
>> >> >You can use it by setting "mapred.output.committer.class" in the
>> Hadoop
>> >> >configuration (or "spark.hadoop.mapred.output.committer.class" in th=
e
>> >> >Spark configuration). Note that this only works for the old Hadoop
>> APIs,
>> >> >I believe the new Hadoop APIs strongly tie committer to input format
>> (so
>> >> >FileInputFormat always uses FileOutputCommitter), which makes this f=
ix
>> >> >more difficult to apply.
>> >> >
>> >> >
>> >> >
>> >> >
>> >> >On Sat, Feb 21, 2015 at 12:12 PM, Andrew Ash <andrew@andrewash.com>
>> >> wrote:
>> >> >
>> >> >Josh is that class something you guys would consider open sourcing, =
or
>> >> >would you rather the community step up and create an OutputCommitter
>> >> >implementation optimized for S3?
>> >> >>
>> >> >>
>> >> >>On Fri, Feb 20, 2015 at 4:02 PM, Josh Rosen <rosenville@gmail.com>
>> >> wrote:
>> >> >>
>> >> >>We (Databricks) use our own DirectOutputCommitter implementation,
>> which
>> >> >>is a couple tens of lines of Scala code.  The class would almost
>> >> >>entirely be a no-op except we took some care to properly handle the
>> >> >>_SUCCESS file.
>> >> >>>
>> >> >>>
>> >> >>>On Fri, Feb 20, 2015 at 3:52 PM, Mingyu Kim <mkim@palantir.com>
>> wrote:
>> >> >>>
>> >> >>>I didn=C2=B9t get any response. It=C2=B9d be really appreciated if=
 anyone
>> using a
>> >> >>>special OutputCommitter for S3 can comment on this!
>> >> >>>>
>> >> >>>>
>> >> >>>>Thanks,
>> >> >>>>Mingyu
>> >> >>>>
>> >> >>>>
>> >> >>>>From: Mingyu Kim <mkim@palantir.com>
>> >> >>>>Date: Monday, February 16, 2015 at 1:15 AM
>> >> >>>>To: "user@spark.apache.org" <user@spark.apache.org>
>> >> >>>>Subject: Which OutputCommitter to use for S3?
>> >> >>>>
>> >> >>>>
>> >> >>>>
>> >> >>>>HI all,
>> >> >>>>
>> >> >>>>
>> >> >>>>The default OutputCommitter used by RDD, which is
>> FileOutputCommitter,
>> >> >>>>seems to require moving files at the commit step, which is not a
>> >> >>>>constant operation in S3, as discussed in
>> >> >>>>
>> >>
>> https://urldefense.proofpoint.com/v2/url?u=3Dhttp-3A__mail-2Darchives.ap=
a
>> >>
>> >>
>> >>>>che.org_mod-5Fmbox_spark-2Duser_201410.mbox_-253C543E33FA.2000802-40=
ent
>> >>
>> >>
>> >>>>ropy.be-253E&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oOn=
mz8&r=3De
>> >>
>> >>
>> >>>>nnQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmuKZRf6sFsz=
Ovl_-
>> >> >>>>ZnxmkBPHo1K24TfGE&s=3DEQOZaHRANJupdjXCfHSXL2t5BZ9YgMt2pRc3pht4o7o=
&e=3D
>> .
>> >>
>> >> >>>>People seem to develop their own NullOutputCommitter
>> implementation or
>> >> >>>>use DirectFileOutputCommitter (as mentioned in SPARK-3595), but I
>> >> >>>>wanted to check if there is a de facto standard, publicly availab=
le
>> >> >>>>OutputCommitter to use for S3 in conjunction with Spark.
>> >> >>>>
>> >> >>>>
>> >> >>>>Thanks,
>> >> >>>>Mingyu
>> >> >>>
>> >> >>
>> >> >
>> >> >--------------------------------------------------------------------=
-
>> >> >To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> >> >For additional commands, e-mail: user-help@spark.apache.org
>> >>
>> >> >
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: user-help@spark.apache.org
>> >>
>> >> ---------------------------------------------------------------------
>> >> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> >> For additional commands, e-mail: user-help@spark.apache.org
>> >>
>> >>
>> >
>>
>
>

--001a113959bc79fa7305116466ae--

From dev-return-12020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 10:40:52 2015
Return-Path: <dev-return-12020-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D802617561
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 10:40:52 +0000 (UTC)
Received: (qmail 60053 invoked by uid 500); 16 Mar 2015 10:40:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59999 invoked by uid 500); 16 Mar 2015 10:40:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59987 invoked by uid 99); 16 Mar 2015 10:40:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 10:40:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 10:40:45 +0000
Received: by pdbni2 with SMTP id ni2so54946571pdb.1
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 03:39:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type;
        bh=bD0IesY6n0hdkbte8h6njmwX/n3eaSNy77NQFdOWHdk=;
        b=Hlx9t8Ig8tDdhHg7GSDAw6QHJTiZsc+bXK2ppcL9w0/asnm0WCk92LvzbOSnOnUlOM
         HmetEoARXezuQ3htj1UQiinIta5I57qh3t9rerire3ZHQCQ7dMGw5hXhoSqKGj6SHS5V
         tebawoLImQWGzZnY3KZ/sVSQ2WjsrIELbgDkqkRvdwvlo9UbM8rvLXpvO4HVPap1u3zQ
         4XP4tNYNi61INN+DRx5NPGPhffjvZGv2AbovXpIcFHZc0Da8LKhpjKdf9xlKhcIuq/C0
         l7ncx42gkgNApu1ppFJfLmuetx+auy7lZjPOgDd50XElA1YefE1kNI50qUhPkPaslPd2
         GBvg==
X-Received: by 10.66.62.201 with SMTP id a9mr136188648pas.101.1426502380443;
        Mon, 16 Mar 2015 03:39:40 -0700 (PDT)
Received: from [10.10.0.18] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id pm9sm16618629pdb.59.2015.03.16.03.39.32
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 16 Mar 2015 03:39:39 -0700 (PDT)
Message-ID: <5506B2E3.60608@gmail.com>
Date: Mon, 16 Mar 2015 18:39:31 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Pei-Lun Lee <pllee@appier.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: SparkSQL 1.3.0 cannot read parquet files from different file
 system
References: <CANrtgzU-e7kWkNafRyOSDO290rgJMwh-ZQz2APYhb5dPOfFJFg@mail.gmail.com>
In-Reply-To: <CANrtgzU-e7kWkNafRyOSDO290rgJMwh-ZQz2APYhb5dPOfFJFg@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------030205010402060301020403"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------030205010402060301020403
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Hi Pei-Lun,

We intentionally disallowed passing multiple comma separated paths in 
1.3.0. One of the reason is that users report that this fail when a file 
path contain an actual comma in it. In your case, you may do something 
like this:

|val  s3nDF  =  parquetFile("s3n://...")
val  hdfsDF  =  parquetFile("hdfs://...")
val  finalDF  =  s3nDF.union(finalDF)
|

Cheng

On 3/16/15 4:03 PM, Pei-Lun Lee wrote:

> Hi,
>
> I am using Spark 1.3.0, where I cannot load parquet files from more than
> one file system, say one s3n://... and another hdfs://..., which worked in
> older version, or if I set spark.sql.parquet.useDataSourceApi=false in 1.3.
>
> One way to fix this is instead of get a single FileSystem from default
> configuration in ParquetRelation2, call Path.getFileSystem for each path.
>
> Here's the JIRA link and pull request:
> https://issues.apache.org/jira/browse/SPARK-6351
> https://github.com/apache/spark/pull/5039
>
> Thanks,
> --
> Pei-Lun
>
​

--------------030205010402060301020403--

From dev-return-12021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 10:43:56 2015
Return-Path: <dev-return-12021-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1FEF917573
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 10:43:56 +0000 (UTC)
Received: (qmail 65910 invoked by uid 500); 16 Mar 2015 10:43:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65838 invoked by uid 500); 16 Mar 2015 10:43:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65826 invoked by uid 99); 16 Mar 2015 10:43:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 10:43:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.174 as permitted sender)
Received: from [209.85.192.174] (HELO mail-pd0-f174.google.com) (209.85.192.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 10:43:48 +0000
Received: by pdbcz9 with SMTP id cz9so54987346pdb.3
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 03:43:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type;
        bh=XbKS7CK2zz+CcGMeeU3iT+jxYqaAfWOf01QNWUqgmBM=;
        b=wGxTWIpJYNqmM1RbMDz/vDpljXuexKZWkh7Dh69MQSAKYl+hXh8WR2EanPsV+/KPm1
         CpinOG7WmGcXjURCdcbVwen412gtvXMIpkjyBXhP0lLuPUZw2vC3pL62TlOGpSWNSCVc
         TSTyiV9/XdapxtvHLw+aVsKcBSpXxE2MBRhaAf1W33U8Hx+9YQTKFp+V7BTMMvhpRKtn
         EmfGVEB4n+Kn7rlO9b3Pga0ziy/KSVuEUomvheL/WJheqvSfMJ706uTKxtDzEuQSxMvD
         9tM+Vm+U/Sg2mvjA+edmxZI/Zz3b+8QH2qzIRBJFnSH8ozEe2DrWGLGjc61Xe2WbKtCo
         jKdA==
X-Received: by 10.70.55.36 with SMTP id o4mr3054365pdp.168.1426502608221;
        Mon, 16 Mar 2015 03:43:28 -0700 (PDT)
Received: from [10.10.0.18] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id qj3sm16696325pac.31.2015.03.16.03.43.25
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 16 Mar 2015 03:43:27 -0700 (PDT)
Message-ID: <5506B3CB.9040205@gmail.com>
Date: Mon, 16 Mar 2015 18:43:23 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Pei-Lun Lee <pllee@appier.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: SparkSQL 1.3.0 cannot read parquet files from different file
 system
References: <CANrtgzU-e7kWkNafRyOSDO290rgJMwh-ZQz2APYhb5dPOfFJFg@mail.gmail.com> <5506B2E3.60608@gmail.com>
In-Reply-To: <5506B2E3.60608@gmail.com>
Content-Type: multipart/alternative;
 boundary="------------030009000900010002000304"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------030009000900010002000304
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Oh sorry, I misread your question. I thought you were trying something 
like |parquetFile(“s3n://file1,hdfs://file2”)|. Yeah, it’s a valid bug. 
Thanks for opening the JIRA ticket and the PR!


Cheng

On 3/16/15 6:39 PM, Cheng Lian wrote:

> Hi Pei-Lun,
>
> We intentionally disallowed passing multiple comma separated paths in 
> 1.3.0. One of the reason is that users report that this fail when a 
> file path contain an actual comma in it. In your case, you may do 
> something like this:
>
> |val  s3nDF  =  parquetFile("s3n://...
> ")
> val  hdfsDF  =  parquetFile("hdfs://...")
> val  finalDF  =  s3nDF.union(finalDF)
> |
>
> Cheng
>
> On 3/16/15 4:03 PM, Pei-Lun Lee wrote:
>
>> Hi,
>>
>> I am using Spark 1.3.0, where I cannot load parquet files from more than
>> one file system, say one s3n://... and another hdfs://..., which worked in
>> older version, or if I set spark.sql.parquet.useDataSourceApi=false in 1.3.
>>
>> One way to fix this is instead of get a single FileSystem from default
>> configuration in ParquetRelation2, call Path.getFileSystem for each path.
>>
>> Here's the JIRA link and pull request:
>> https://issues.apache.org/jira/browse/SPARK-6351
>> https://github.com/apache/spark/pull/5039
>>
>> Thanks,
>> --
>> Pei-Lun
>>
> ​

​
​

​

--------------030009000900010002000304--

From dev-return-12022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 12:07:02 2015
Return-Path: <dev-return-12022-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 04B6117857
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 12:07:02 +0000 (UTC)
Received: (qmail 75833 invoked by uid 500); 16 Mar 2015 12:07:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75760 invoked by uid 500); 16 Mar 2015 12:07:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75741 invoked by uid 99); 16 Mar 2015 12:07:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 12:07:00 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.171 as permitted sender)
Received: from [209.85.192.171] (HELO mail-pd0-f171.google.com) (209.85.192.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 12:06:33 +0000
Received: by pdbni2 with SMTP id ni2so56750457pdb.1
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 05:06:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:cc:subject
         :references:in-reply-to:content-type;
        bh=JNX17xrS155K91HaWvFuOVCPIKvkrkginbRgwEe68AQ=;
        b=eopFet52LEyd6pXWp91wbmzgpIANa3LeC46yullccx4+HfXvtExLDiihmAgJt9E/8D
         5qhy5wGLIpySzM5jOhYNRGzFoza9pZBtg/yR0AqSRErdKymSDplFOLt2HXMVIf31pv7N
         P8NqQX4lOpmseI7hHiK8x0Ertr0HdxFBzRk8hSsSXJskiBlIicrZKPI984VarckBjqKY
         28PwUSzWpD0V/PZBjxuxM1bQSxupz6vdDTt8qM9od9O79H0WiSTYINalgUyS+0rjs6pZ
         mZX9VB0+J6Mn8munaO4Tzs6IUwtMWNs2XlgwLA7AX7vfpbSNeIQH9C5uLzuGphXanxbI
         3Cqg==
X-Received: by 10.66.55.104 with SMTP id r8mr133796119pap.75.1426507591011;
        Mon, 16 Mar 2015 05:06:31 -0700 (PDT)
Received: from [10.10.0.7] ([116.251.221.227])
        by mx.google.com with ESMTPSA id qo4sm17108057pab.33.2015.03.16.05.06.11
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 16 Mar 2015 05:06:30 -0700 (PDT)
Message-ID: <5506C72F.4090909@gmail.com>
Date: Mon, 16 Mar 2015 20:06:07 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Ted Yu <yuzhihong@gmail.com>, Patrick Wendell <pwendell@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Wrong version on the Spark documentation page
References: <5505C3F7.5090502@gmail.com>	<CABPQxsu_X6PAoFunrO0au6kqMV6_sUi2wSjR3k6XyigihsFzfw@mail.gmail.com> <CALte62x-s7tqkq73crryHLTru--WqKcDJ3zkYYgNTorgDs9Sww@mail.gmail.com>
In-Reply-To: <CALte62x-s7tqkq73crryHLTru--WqKcDJ3zkYYgNTorgDs9Sww@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------040902030009010201000605"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------040902030009010201000605
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit

Patrick, Ted - My bad, yeah, it's because of browser cache.

On 3/16/15 2:31 AM, Ted Yu wrote:
> When I enter http://spark.apache.org/docs/latest/ into Chrome address 
> bar, I saw 1.3.0
>
> Cheers
>
> On Sun, Mar 15, 2015 at 11:12 AM, Patrick Wendell <pwendell@gmail.com 
> <mailto:pwendell@gmail.com>> wrote:
>
>     Cheng - what if you hold shift+refresh? For me the /latest link
>     correctly points to 1.3.0
>
>     On Sun, Mar 15, 2015 at 10:40 AM, Cheng Lian
>     <lian.cs.zju@gmail.com <mailto:lian.cs.zju@gmail.com>> wrote:
>     > It's still marked as 1.2.1 here http://spark.apache.org/docs/latest/
>     >
>     > But this page is updated (1.3.0)
>     > http://spark.apache.org/docs/latest/index.html
>     >
>     > Cheng
>     >
>     >
>     ---------------------------------------------------------------------
>     > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>
>     > For additional commands, e-mail: dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org>
>     >
>
>     ---------------------------------------------------------------------
>     To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>
>     For additional commands, e-mail: dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org>
>
>


--------------040902030009010201000605--

From dev-return-12023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 12:11:25 2015
Return-Path: <dev-return-12023-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 67B1C17888
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 12:11:25 +0000 (UTC)
Received: (qmail 85365 invoked by uid 500); 16 Mar 2015 12:11:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85284 invoked by uid 500); 16 Mar 2015 12:11:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85271 invoked by uid 99); 16 Mar 2015 12:11:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 12:11:19 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gilv@il.ibm.com designates 195.75.94.113 as permitted sender)
Received: from [195.75.94.113] (HELO e06smtp17.uk.ibm.com) (195.75.94.113)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 12:11:10 +0000
Received: from /spool/local
	by e06smtp17.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <gilv@il.ibm.com>;
	Mon, 16 Mar 2015 12:09:48 -0000
Received: from d06dlp01.portsmouth.uk.ibm.com (9.149.20.13)
	by e06smtp17.uk.ibm.com (192.168.101.147) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Mon, 16 Mar 2015 12:09:46 -0000
Received: from b06cxnps4074.portsmouth.uk.ibm.com (d06relay11.portsmouth.uk.ibm.com [9.149.109.196])
	by d06dlp01.portsmouth.uk.ibm.com (Postfix) with ESMTP id E4AE117D8042
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 12:10:09 +0000 (GMT)
Received: from d06av08.portsmouth.uk.ibm.com (d06av08.portsmouth.uk.ibm.com [9.149.37.249])
	by b06cxnps4074.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id t2GC9jNs4456954
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 12:09:45 GMT
Received: from d06av08.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id t2GC9jDi030328
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 06:09:45 -0600
Received: from d06ml319.portsmouth.uk.ibm.com (d06ml319.portsmouth.uk.ibm.com [9.149.76.146])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id t2GC9jHR030325
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 06:09:45 -0600
To: dev <dev@spark.apache.org>
MIME-Version: 1.0
Subject: problems with Parquet in Spark 1.3.0
X-KeepSent: 8D3C8CCC:11A29CEA-C2257E0A:00422A55;
 type=4; name=$KeepSent
X-Mailer: IBM Notes Release 9.0.1SHF211 December 19, 2013
From: Gil Vernik <GILV@il.ibm.com>
Message-ID: <OF8D3C8CCC.11A29CEA-ONC2257E0A.00422A55-C2257E0A.0042CF73@il.ibm.com>
Date: Mon, 16 Mar 2015 14:09:44 +0200
X-MIMETrack: Serialize by Router on D06ML319/06/M/IBM(Release 9.0.1FP3|January  12, 2015) at
 16/03/2015 14:09:44,
	Serialize complete at 16/03/2015 14:09:44
Content-Type: multipart/alternative; boundary="=_alternative 0042CDA0C2257E0A_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15031612-0029-0000-0000-000003CA2166
X-Virus-Checked: Checked by ClamAV on apache.org

--=_alternative 0042CDA0C2257E0A_=
Content-Type: text/plain; charset="US-ASCII"

Hi,

I am storing Parquet files in the OpenStack Swift and access those files 
from Spark.

This works perfectly in Spark prior 1.3.0, but in 1.3.0 I  am getting this 
error:
Is there some configuration i missed? I am not sure where this error get 
from, does Spark 1.3.0 requires Parquet files to be accessed via "file://" 
?
I will be glad to dig into this in case it's a bug, but would like to know 
if this is something intentionally in Spark 1.3.0
( I do can access swift:// names pace from SparkContext, only sqlContext 
has this issue )

Thanks,
Gil Vernik.

scala> val parquetFile = 
sqlContext.parquetFile("swift://ptest.localSwift12/SF311new3.parquet")

java.lang.IllegalArgumentException: Wrong FS: 
swift://ptest.localSwift12/SF311new3.parquet, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
        at 
org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465)
        at 
org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:119)
        at 
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:252)
        at 
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:251)
        at 
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at 
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at 
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at 
scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at 
scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at 
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newParquet.scala:251)
        at 
org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.scala:370)
        at 
org.apache.spark.sql.SQLContext.parquetFile(SQLContext.scala:522)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:19)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:24)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:28)
        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
        at $iwC$$iwC$$iwC.<init>(<console>:32)
        at $iwC$$iwC.<init>(<console>:34)
        at $iwC.<init>(<console>:36)
        at <init>(<console>:38)
        at .<init>(<console>:42)
        at .<clinit>(<console>)
        at .<init>(<console>:7)
        at .<clinit>(<console>)
        at $print(<console>)
--=_alternative 0042CDA0C2257E0A_=--


From dev-return-12024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 12:33:42 2015
Return-Path: <dev-return-12024-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DDA4D17906
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 12:33:42 +0000 (UTC)
Received: (qmail 16069 invoked by uid 500); 16 Mar 2015 12:33:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15989 invoked by uid 500); 16 Mar 2015 12:33:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15978 invoked by uid 99); 16 Mar 2015 12:33:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 12:33:41 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gilv@il.ibm.com designates 195.75.94.112 as permitted sender)
Received: from [195.75.94.112] (HELO e06smtp16.uk.ibm.com) (195.75.94.112)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 12:33:34 +0000
Received: from /spool/local
	by e06smtp16.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <gilv@il.ibm.com>;
	Mon, 16 Mar 2015 12:33:13 -0000
Received: from d06dlp03.portsmouth.uk.ibm.com (9.149.20.15)
	by e06smtp16.uk.ibm.com (192.168.101.146) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Mon, 16 Mar 2015 12:33:11 -0000
Received: from b06cxnps4075.portsmouth.uk.ibm.com (d06relay12.portsmouth.uk.ibm.com [9.149.109.197])
	by d06dlp03.portsmouth.uk.ibm.com (Postfix) with ESMTP id 8CC911B08061
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 12:33:33 +0000 (GMT)
Received: from d06av08.portsmouth.uk.ibm.com (d06av08.portsmouth.uk.ibm.com [9.149.37.249])
	by b06cxnps4075.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id t2GCXAYG32047238
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 12:33:10 GMT
Received: from d06av08.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id t2GCXApK028473
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 06:33:10 -0600
Received: from d06ml319.portsmouth.uk.ibm.com (d06ml319.portsmouth.uk.ibm.com [9.149.76.146])
	by d06av08.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id t2GCXAGS028470
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 06:33:10 -0600
In-Reply-To: <OF8D3C8CCC.11A29CEA-ONC2257E0A.00422A55-C2257E0A.0042CF73@il.ibm.com>
References: <OF8D3C8CCC.11A29CEA-ONC2257E0A.00422A55-C2257E0A.0042CF73@il.ibm.com>
To: Gil Vernik <GILV@il.ibm.com>
Cc: dev <dev@spark.apache.org>
MIME-Version: 1.0
Subject: Re: problems with Parquet in Spark 1.3.0
X-KeepSent: 86814BA9:A4E13AD8-C2257E0A:0044DA2F;
 type=4; name=$KeepSent
X-Mailer: IBM Notes Release 9.0.1SHF211 December 19, 2013
From: Gil Vernik <GILV@il.ibm.com>
Message-ID: <OF86814BA9.A4E13AD8-ONC2257E0A.0044DA2F-C2257E0A.0044F472@il.ibm.com>
Date: Mon, 16 Mar 2015 14:33:09 +0200
X-MIMETrack: Serialize by Router on D06ML319/06/M/IBM(Release 9.0.1FP3|January  12, 2015) at
 16/03/2015 14:33:09,
	Serialize complete at 16/03/2015 14:33:09
Content-Type: multipart/alternative; boundary="=_alternative 0044F3E9C2257E0A_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15031612-0025-0000-0000-0000044FBF10
X-Virus-Checked: Checked by ClamAV on apache.org

--=_alternative 0044F3E9C2257E0A_=
Content-Type: text/plain; charset="US-ASCII"

I just noticed about this one

https://issues.apache.org/jira/browse/SPARK-6351
https://github.com/apache/spark/pull/5039


I verified it and this resolves my issues with Parquet and swift:// name 
space.





From:   Gil Vernik/Haifa/IBM@IBMIL
To:     dev <dev@spark.apache.org>
Date:   16/03/2015 02:11 PM
Subject:        problems with Parquet in Spark 1.3.0



Hi,

I am storing Parquet files in the OpenStack Swift and access those files 
from Spark.

This works perfectly in Spark prior 1.3.0, but in 1.3.0 I  am getting this 

error:
Is there some configuration i missed? I am not sure where this error get 
from, does Spark 1.3.0 requires Parquet files to be accessed via "file://" 

?
I will be glad to dig into this in case it's a bug, but would like to know 

if this is something intentionally in Spark 1.3.0
( I do can access swift:// names pace from SparkContext, only sqlContext 
has this issue )

Thanks,
Gil Vernik.

scala> val parquetFile = 
sqlContext.parquetFile("swift://ptest.localSwift12/SF311new3.parquet")

java.lang.IllegalArgumentException: Wrong FS: 
swift://ptest.localSwift12/SF311new3.parquet, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
        at 
org.apache.hadoop.fs.FileSystem.makeQualified(FileSystem.java:465)
        at 
org.apache.hadoop.fs.FilterFileSystem.makeQualified(FilterFileSystem.java:119)
        at 
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:252)
        at 
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache$$anonfun$6.apply(newParquet.scala:251)
        at 
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at 
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at 
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at 
scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
        at 
scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at 
org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newParquet.scala:251)
        at 
org.apache.spark.sql.parquet.ParquetRelation2.<init>(newParquet.scala:370)
        at 
org.apache.spark.sql.SQLContext.parquetFile(SQLContext.scala:522)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:19)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:24)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
        at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:28)
        at $iwC$$iwC$$iwC$$iwC.<init>(<console>:30)
        at $iwC$$iwC$$iwC.<init>(<console>:32)
        at $iwC$$iwC.<init>(<console>:34)
        at $iwC.<init>(<console>:36)
        at <init>(<console>:38)
        at .<init>(<console>:42)
        at .<clinit>(<console>)
        at .<init>(<console>:7)
        at .<clinit>(<console>)
        at $print(<console>)

--=_alternative 0044F3E9C2257E0A_=--


From dev-return-12025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 13:19:44 2015
Return-Path: <dev-return-12025-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B52FC17AAF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 13:19:44 +0000 (UTC)
Received: (qmail 46827 invoked by uid 500); 16 Mar 2015 13:19:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46745 invoked by uid 500); 16 Mar 2015 13:19:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46731 invoked by uid 99); 16 Mar 2015 13:19:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 13:19:42 +0000
X-ASF-Spam-Status: No, hits=5.5 required=10.0
	tests=DCC_CHECK,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of joe.halliwell@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 13:19:36 +0000
Received: by qgg60 with SMTP id 60so39155357qgg.3
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 06:18:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:from:to:subject:content-type;
        bh=FaLKXA2HquZm4Dm3+SfKPuDtZMpodLraEGdMZb/3nHQ=;
        b=ZdPsMdMAvs1JS4qcZ2xLIYl8mmckaOctaJ4a8LLs17tcWkZWW2E9e13qojcjasIF5R
         q8sJ/w/Xs/InL2fEDCtcAL3ReWFZV93/Cihbs/swL8E6tlY0d1cciSnezBKHJhpnbTgy
         w6smUNzsdINx1nF0v2fiRPPWSpedCS0MJhk7NxJOeor9xa2Oy0IkJ90rsybVtY0C15RM
         fkUH2ykIq+smrqYgGdz1s26mHLMl+3Mi7YDblwAP7LPmwkLhpIx1dIXqtYNe7htBNN1K
         kwa/9pVkK1V3l7jiInp1caVzqXcVp+3T7D+mDom5e7K7kpsrz0kZgkGaGSEerBavEJ+/
         UcGQ==
X-Received: by 10.140.96.87 with SMTP id j81mr73209935qge.92.1426511910639;
        Mon, 16 Mar 2015 06:18:30 -0700 (PDT)
Received: from hedwig-73.prd.orcali.com (ec2-54-85-253-161.compute-1.amazonaws.com. [54.85.253.161])
        by mx.google.com with ESMTPSA id e92sm7442639qgd.39.2015.03.16.06.18.28
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 16 Mar 2015 06:18:29 -0700 (PDT)
Date: Mon, 16 Mar 2015 06:18:29 -0700 (PDT)
X-Google-Original-Date: Mon, 16 Mar 2015 13:18:28 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1426511908560.972dd821@Nodemailer>
X-Orchestra-Oid: 0E1EBB48-01FD-488E-8FFD-641A7FB76093
X-Orchestra-Sig: 37c917a76655b343f7626748bc20b382e0e6357c
X-Orchestra-Thrid: 02635EF9-B6F3-46C2-A6EF-FD264D949486
X-Orchestra-Thrid-Sig: b2a40de9c38a09c828f8e9bc804f61a5f5e8f25d
X-Orchestra-Account: d97b77c0c018a60767359a96aaebbac2d94f5060
From: "Joe Halliwell" <joe.halliwell@gmail.com>
To: dev@spark.apache.org
Subject: Typo in 1.3.0 release notes: s/extended renamed/renamed/
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1426511909252"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1426511909252
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Cheers,
Joe

Best regards, Joe
------Nodemailer-0.5.0-?=_1-1426511909252--

From dev-return-12026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 13:23:23 2015
Return-Path: <dev-return-12026-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 10D8A17ABF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 13:23:23 +0000 (UTC)
Received: (qmail 52697 invoked by uid 500); 16 Mar 2015 13:23:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52611 invoked by uid 500); 16 Mar 2015 13:23:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52600 invoked by uid 99); 16 Mar 2015 13:23:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 13:23:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 13:23:16 +0000
Received: by wixw10 with SMTP id w10so43287488wix.0
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 06:22:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=Zuoc7didKl268fyJOQhxrAtamRa6aAiJtd5s9sJjNx8=;
        b=R1iq1gb8dQkuh9xVRWVcBfHAjTYejqXlK0IEiDMkqZMCCAt2OY8IItO3A9boCh97HP
         V3oYeFIcZjnekXjUhPy8ex8cN6IHX8tE1i/s4AXM326iat7jq8QP9/v6yGXHumB27k8m
         yEcW2ZxQ9/Rkwtv3459c4zqvkkEYKcaraYb+z7B3uiwn6meLVWxm3ZCtvdOg2Leeqyqk
         X9UuYBQzftcJmwkODe+2Xkxn38kybEAxnbX92NV4+VFvIO00pdWk41wX74NbiiqDqF7Z
         3QMzcRXNk3lV4FLqj+sv0TeW2KUEqTTgxeCPK/cidq9G1vbH1n+W28fvlUxP14zJVaCG
         7HyQ==
X-Gm-Message-State: ALoCoQl8qwl8NKaiq1OvENBz+EOTVI6Lk/Jq6tBvbELG/T/kWl8vFqzLCncJ/BRo4ZX6xaQKozoz
X-Received: by 10.194.200.166 with SMTP id jt6mr77671230wjc.66.1426512129967;
 Mon, 16 Mar 2015 06:22:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Mon, 16 Mar 2015 06:21:49 -0700 (PDT)
In-Reply-To: <1426511908560.972dd821@Nodemailer>
References: <1426511908560.972dd821@Nodemailer>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 16 Mar 2015 13:21:49 +0000
Message-ID: <CAMAsSdLYH8LE1__sJUDT+dHV8Kr118=y8fMvQfpDifijZ7cO4g@mail.gmail.com>
Subject: Re: Typo in 1.3.0 release notes: s/extended renamed/renamed/
To: Joe Halliwell <joe.halliwell@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Here's the sentence:

As part of stabilizing the Spark SQL API, the SchemaRDD class has been
extended renamed to DataFrame.

Yes, I can remove the word 'extended'


On Mon, Mar 16, 2015 at 1:18 PM, Joe Halliwell <joe.halliwell@gmail.com> wrote:
> Cheers,
> Joe
>
> Best regards, Joe

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 14:52:38 2015
Return-Path: <dev-return-12027-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7223B17EC0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 14:52:38 +0000 (UTC)
Received: (qmail 78661 invoked by uid 500); 16 Mar 2015 14:52:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78565 invoked by uid 500); 16 Mar 2015 14:52:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78553 invoked by uid 99); 16 Mar 2015 14:52:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 14:52:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.173 as permitted sender)
Received: from [209.85.217.173] (HELO mail-lb0-f173.google.com) (209.85.217.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 14:52:27 +0000
Received: by lbbsy1 with SMTP id sy1so32626565lbb.1
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 07:52:06 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=UK1553VpFsRX9NtH9Ds8mhSXGJW6Y6O+St5GXjekzUw=;
        b=emLQTnNSyiNade87wm+poVv+r60D/afj3OBq77nK5MlO4Q9VJ11bCW58IFyP8GvyQ8
         sg21zNOQORYcefGwv4HuBSQ3fmjIwP0J5bNLxhITCakon41c+HGS3rs/IsfxrETsHbv7
         GSem/sWjEwKv/GmSu3dgKSP2OoLGBi+Py5kT9+obww7oenvcb/lpYELNX2DjStxSrRmd
         v12lP5eDzQ7XlntZ1YF4jWFsA9KmxtvGDJon4AItOEa2Q5V5nrRLl/B8tTPgd/kRLqm1
         fW/Pcli69EYadshdsHs2xdJm6I7RHS16iaenflElYZueACBVNe04bdrs4yjEFSTWFoWW
         UqvQ==
X-Gm-Message-State: ALoCoQlJBneQapu0jAvz59y0wJHyr7piioNvL28Xrc2YmLeZ8x0R1o/KvcvvUwpvO+5CHEb/6Fkx
X-Received: by 10.112.55.5 with SMTP id n5mr34881160lbp.75.1426517526318; Mon,
 16 Mar 2015 07:52:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Mon, 16 Mar 2015 07:51:45 -0700 (PDT)
In-Reply-To: <CACdU-dQiG+2CpLt+zDRCfLfaPOza5XdBNNGF1uwWTxcV2FD+fw@mail.gmail.com>
References: <CACdU-dQiG+2CpLt+zDRCfLfaPOza5XdBNNGF1uwWTxcV2FD+fw@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Mon, 16 Mar 2015 07:51:45 -0700
Message-ID: <CACdU-dRR4Rayn1qAA3SDcYqjhNbUEgYVmh9_FXcgbPuouXDv9w@mail.gmail.com>
Subject: Re: extended jenkins downtime monday, march 16th, plus some hints at
 the future
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11c3b2446c2be2051168fe73
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3b2446c2be2051168fe73
Content-Type: text/plain; charset=UTF-8

this is starting now.

On Fri, Mar 13, 2015 at 10:12 AM, shane knapp <sknapp@berkeley.edu> wrote:

> i'll be taking jenkins down for some much-needed plugin updates, as well
> as potentially upgrading jenkins itself.
>
> this will start at 730am PDT, and i'm hoping to have everything up by noon.
>
> the move to the anaconda python will take place in the next couple of
> weeks as i'm in the process of rebuilding my staging environment (much
> needed) to better reflect production, and allow me to better test the
> change.
>
> and finally, some teasers for what's coming up in the next month or so:
>
> * move to a fully puppetized environment (yay no more shell script
> deployments!)
> * virtualized workers (including multiple OSes -- OS X, ubuntu, ...,
> profit?)
>
> more details as they come.
>
> happy friday!
>
> shane
>

--001a11c3b2446c2be2051168fe73--

From dev-return-12028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 16:07:10 2015
Return-Path: <dev-return-12028-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 601DA10461
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 16:07:10 +0000 (UTC)
Received: (qmail 47981 invoked by uid 500); 16 Mar 2015 16:07:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47909 invoked by uid 500); 16 Mar 2015 16:07:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47898 invoked by uid 99); 16 Mar 2015 16:07:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 16:07:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sknapp@berkeley.edu designates 209.85.215.44 as permitted sender)
Received: from [209.85.215.44] (HELO mail-la0-f44.google.com) (209.85.215.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 16:07:04 +0000
Received: by lamx15 with SMTP id x15so44163544lam.3
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 09:06:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=pLdGSrR2dTCB2xkBVfT3WUb7285MPJX9zAEU8CY4V9s=;
        b=mmh0YFQMQqbcbg53iXylFTLYd4SgTq/iwiC4iY+g5jGAQnw5bOmKG/rYoYjighqJGJ
         lAyy9QjIw0JFgLIkUYmYcy+wS6vEV6geqiTETrRTaplw2GP9O/wdACdtmkVjL8HSR/AI
         mmLqJQYsMpE1Bya1oXzQM+2wTAPqO+Zu8oUXseoD6hDFyTL99/I3hTrpvov3II8J1NdP
         r93u5HjsM8EnxQH1HiCZ+lLcLfe+Our6Mo+4qfkV3Xr0HoeFMZ/HB84HOH9hTcd/HoQ9
         nCzEmmMZKHmzjzk7Xew/MG1mXc5YEE1MG117jAP0qtwcm0Am3wW2cYx/odst4zG1bVeF
         kYqw==
X-Gm-Message-State: ALoCoQljJw+l6kdcyOHqRIb76jHdSPX6wdR/Q5TZlSiRpLufX6f+BdY2SInTLG7WIBq0CZtP0g2j
X-Received: by 10.152.116.11 with SMTP id js11mr34091944lab.106.1426522003372;
 Mon, 16 Mar 2015 09:06:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Mon, 16 Mar 2015 09:06:12 -0700 (PDT)
In-Reply-To: <CACdU-dRR4Rayn1qAA3SDcYqjhNbUEgYVmh9_FXcgbPuouXDv9w@mail.gmail.com>
References: <CACdU-dQiG+2CpLt+zDRCfLfaPOza5XdBNNGF1uwWTxcV2FD+fw@mail.gmail.com>
 <CACdU-dRR4Rayn1qAA3SDcYqjhNbUEgYVmh9_FXcgbPuouXDv9w@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Mon, 16 Mar 2015 09:06:12 -0700
Message-ID: <CACdU-dTCgmbRztC=oKDGP5xNgDLJHt28YSB_KZO4FsUt8p0wBA@mail.gmail.com>
Subject: Re: extended jenkins downtime monday, march 16th, plus some hints at
 the future
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11c1a70a468bd605116a0974
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1a70a468bd605116a0974
Content-Type: text/plain; charset=UTF-8

looks like we're having some issues w/the pull request builder and cron
stacktraces in the logs.  i'll be investigating further and will update
when i figure out what's going on.

On Mon, Mar 16, 2015 at 7:51 AM, shane knapp <sknapp@berkeley.edu> wrote:

> this is starting now.
>
> On Fri, Mar 13, 2015 at 10:12 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> i'll be taking jenkins down for some much-needed plugin updates, as well
>> as potentially upgrading jenkins itself.
>>
>> this will start at 730am PDT, and i'm hoping to have everything up by
>> noon.
>>
>> the move to the anaconda python will take place in the next couple of
>> weeks as i'm in the process of rebuilding my staging environment (much
>> needed) to better reflect production, and allow me to better test the
>> change.
>>
>> and finally, some teasers for what's coming up in the next month or so:
>>
>> * move to a fully puppetized environment (yay no more shell script
>> deployments!)
>> * virtualized workers (including multiple OSes -- OS X, ubuntu, ...,
>> profit?)
>>
>> more details as they come.
>>
>> happy friday!
>>
>> shane
>>
>
>

--001a11c1a70a468bd605116a0974--

From dev-return-12029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 16:43:21 2015
Return-Path: <dev-return-12029-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A22E1064B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 16:43:21 +0000 (UTC)
Received: (qmail 80578 invoked by uid 500); 16 Mar 2015 16:43:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80503 invoked by uid 500); 16 Mar 2015 16:43:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80492 invoked by uid 99); 16 Mar 2015 16:43:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 16:43:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sknapp@berkeley.edu designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 16:42:54 +0000
Received: by lbbsy1 with SMTP id sy1so35083121lbb.1
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 09:42:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=D8qfOs+4tCFdftUhYUF3J/q3izB4cMr6kx8RXVHmdiM=;
        b=ft7hSU/tCc+n5h05+WkYBZxZyfUCqphdp4nTQ0vu681Q5fbXXeMNg4zk6xAeOAmto4
         mbizH12K8wJt9zHo6mJh3LaSCEEw+NbkYnl0dlLcmF5WYtIGo271YacQXw1zisVIw39c
         0ne6Q7WHr4LMUDbqRXs115oCaJdp/hyqkFUKC5uQ0MEnJ+M/D0LRPbQ7jsPNbvJO/9ZX
         AW19qWkwwza5FaP7Q188apWUR+IVy75/q8hG/nE4N+EK6lm2AgDwyLj2/Yfxy9gNggrX
         gJHW5j/0oao08KvVOHuUwTcrRMZ/BP01OTz5e36cxvwpx/n1JtKwf+Q6L+ljs+XRnEZ6
         rr5A==
X-Gm-Message-State: ALoCoQmvcHPbBXyBpasF8Ozd17FI8MsKMrE3I+oQka55Dk/raFI0yO/anpElsZCA9L0fF0lXbzC1
X-Received: by 10.112.62.167 with SMTP id z7mr26782421lbr.115.1426524173222;
 Mon, 16 Mar 2015 09:42:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Mon, 16 Mar 2015 09:42:33 -0700 (PDT)
In-Reply-To: <CACdU-dTCgmbRztC=oKDGP5xNgDLJHt28YSB_KZO4FsUt8p0wBA@mail.gmail.com>
References: <CACdU-dQiG+2CpLt+zDRCfLfaPOza5XdBNNGF1uwWTxcV2FD+fw@mail.gmail.com>
 <CACdU-dRR4Rayn1qAA3SDcYqjhNbUEgYVmh9_FXcgbPuouXDv9w@mail.gmail.com> <CACdU-dTCgmbRztC=oKDGP5xNgDLJHt28YSB_KZO4FsUt8p0wBA@mail.gmail.com>
From: shane knapp <sknapp@berkeley.edu>
Date: Mon, 16 Mar 2015 09:42:33 -0700
Message-ID: <CACdU-dROVfBcjk3bMOPtSi4Ev_3sp2pPN_OQ0YDpj07daqTS3g@mail.gmail.com>
Subject: Re: extended jenkins downtime monday, march 16th, plus some hints at
 the future
To: dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>
Content-Type: multipart/alternative; boundary=001a11c3ee5e9bdc8505116a8ac8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3ee5e9bdc8505116a8ac8
Content-Type: text/plain; charset=UTF-8

ok, we're back up and building.  upgrading the github plugin (and possibly
EnvInject) caused the stacktraces, so i've kept those at the old versions
that were working before.  jenkins and the rest of the plugins are updated
and we're g2g.

i'll be, of course, keeping an eye on things today and will squash anything
else that pops up.

On Mon, Mar 16, 2015 at 9:06 AM, shane knapp <sknapp@berkeley.edu> wrote:

> looks like we're having some issues w/the pull request builder and cron
> stacktraces in the logs.  i'll be investigating further and will update
> when i figure out what's going on.
>
> On Mon, Mar 16, 2015 at 7:51 AM, shane knapp <sknapp@berkeley.edu> wrote:
>
>> this is starting now.
>>
>> On Fri, Mar 13, 2015 at 10:12 AM, shane knapp <sknapp@berkeley.edu>
>> wrote:
>>
>>> i'll be taking jenkins down for some much-needed plugin updates, as well
>>> as potentially upgrading jenkins itself.
>>>
>>> this will start at 730am PDT, and i'm hoping to have everything up by
>>> noon.
>>>
>>> the move to the anaconda python will take place in the next couple of
>>> weeks as i'm in the process of rebuilding my staging environment (much
>>> needed) to better reflect production, and allow me to better test the
>>> change.
>>>
>>> and finally, some teasers for what's coming up in the next month or so:
>>>
>>> * move to a fully puppetized environment (yay no more shell script
>>> deployments!)
>>> * virtualized workers (including multiple OSes -- OS X, ubuntu, ...,
>>> profit?)
>>>
>>> more details as they come.
>>>
>>> happy friday!
>>>
>>> shane
>>>
>>
>>
>

--001a11c3ee5e9bdc8505116a8ac8--

From dev-return-12030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 18:38:29 2015
Return-Path: <dev-return-12030-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB6F410C4F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 18:38:29 +0000 (UTC)
Received: (qmail 35446 invoked by uid 500); 16 Mar 2015 18:38:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35365 invoked by uid 500); 16 Mar 2015 18:38:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35351 invoked by uid 99); 16 Mar 2015 18:38:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 18:38:28 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 18:38:24 +0000
Received: by igbue6 with SMTP id ue6so50180087igb.1
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 11:38:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=PY6by+1LieAf02jT3z0/kKFFXaRpOZVVocrZDaMNFak=;
        b=LvoToRF4rj16tlEvcYNLSWNKtH/xOdokMKobM1OVpEj8pany635BNNcLqddlYvVrJ6
         2N1N9GhTloxWB0AU+j17Xu3pIjcLenGbWR6PLx/H58iamYraCFHZJGtTvuk5kyCq+wvx
         cYN4PLpscy14DfLsY2oeimb+OA7Pz3uCW7qZJD0PsC7YIFOU9iHQZ8u7Yq2ApghlsWX0
         XqnLQbAajurz3MV2LV5zVDZ2HrH5cwKOE0sEbugTkgftatsXJM4AJRoufiZtYBGkvEap
         LZMj8X5n3AAqJEGznYG7laJL8L+fWaDSxx2w6RenPoEHIuEJoDJTP9P1J+vYGgYKl3G1
         kn6Q==
MIME-Version: 1.0
X-Received: by 10.42.167.8 with SMTP id q8mr80624843icy.94.1426531083790; Mon,
 16 Mar 2015 11:38:03 -0700 (PDT)
Received: by 10.36.69.8 with HTTP; Mon, 16 Mar 2015 11:38:03 -0700 (PDT)
In-Reply-To: <CADtDQQ+oQ-_WR8dteYPKQ-o0Vp_vH0O9ZD=on1RUh20zri59_g@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
	<CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
	<CA+3qhFQGGQnLfnFy8iXmEK_uwgA0HJJ=Nh=1fYrOTtnYJTDQ+A@mail.gmail.com>
	<CANGvG8pXqPCFs4dhEJVKkwtfmPFVYkgaaqb0SaBMN-33otVUYQ@mail.gmail.com>
	<CABPQxsthhRTkypYyOtdrZTT1SwX-hpD+ZAtV7vtOLdRbtAhdQA@mail.gmail.com>
	<CADtDQQ+oQ-_WR8dteYPKQ-o0Vp_vH0O9ZD=on1RUh20zri59_g@mail.gmail.com>
Date: Mon, 16 Mar 2015 11:38:03 -0700
Message-ID: <CAJgQjQ8H_GiK2r-qRUFwK1Uvi93Rniwdvk3qjMTsHPSyqMEEMQ@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: RJ Nowling <rnowling@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, Aaron Davidson <ilikerps@gmail.com>, 
	Imran Rashid <irashid@cloudera.com>, Mridul Muralidharan <mridul@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

In MLlib, we use strings for emu-like types in Python APIs, which is
quite common in Python and easy for py4j. On the JVM side, we
implement `fromString` to convert them back to enums. -Xiangrui

On Wed, Mar 11, 2015 at 12:56 PM, RJ Nowling <rnowling@gmail.com> wrote:
> How do these proposals affect PySpark?  I think compatibility with PySpark
> through Py4J should be considered.
>
> On Mon, Mar 9, 2015 at 8:39 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Does this matter for our own internal types in Spark? I don't think
>> any of these types are designed to be used in RDD records, for
>> instance.
>>
>> On Mon, Mar 9, 2015 at 6:25 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
>> > Perhaps the problem with Java enums that was brought up was actually that
>> > their hashCode is not stable across JVMs, as it depends on the memory
>> > location of the enum itself.
>> >
>> > On Mon, Mar 9, 2015 at 6:15 PM, Imran Rashid <irashid@cloudera.com>
>> wrote:
>> >
>> >> Can you expand on the serde issues w/ java enum's at all?  I haven't
>> heard
>> >> of any problems specific to enums.  The java object serialization rules
>> >> seem very clear and it doesn't seem like different jvms should have a
>> >> choice on what they do:
>> >>
>> >>
>> >>
>> http://docs.oracle.com/javase/6/docs/platform/serialization/spec/serial-arch.html#6469
>> >>
>> >> (in a nutshell, serialization must use enum.name())
>> >>
>> >> of course there are plenty of ways the user could screw this up(eg.
>> rename
>> >> the enums, or change their meaning, or remove them).  But then again,
>> all
>> >> of java serialization has issues w/ serialization the user has to be
>> aware
>> >> of.  Eg., if we go with case objects, than java serialization blows up
>> if
>> >> you add another helper method, even if that helper method is completely
>> >> compatible.
>> >>
>> >> Some prior debate in the scala community:
>> >>
>> >>
>> https://groups.google.com/d/msg/scala-internals/8RWkccSRBxQ/AN5F_ZbdKIsJ
>> >>
>> >> SO post on which version to use in scala:
>> >>
>> >>
>> >>
>> http://stackoverflow.com/questions/1321745/how-to-model-type-safe-enum-types
>> >>
>> >> SO post about the macro-craziness people try to add to scala to make
>> them
>> >> almost as good as a simple java enum:
>> >> (NB: the accepted answer doesn't actually work in all cases ...)
>> >>
>> >>
>> >>
>> http://stackoverflow.com/questions/20089920/custom-scala-enum-most-elegant-version-searched
>> >>
>> >> Another proposal to add better enums built into scala ... but seems to
>> be
>> >> dormant:
>> >>
>> >> https://groups.google.com/forum/#!topic/scala-sips/Bf82LxK02Kk
>> >>
>> >>
>> >>
>> >> On Thu, Mar 5, 2015 at 10:49 PM, Mridul Muralidharan <mridul@gmail.com>
>> >> wrote:
>> >>
>> >> >   I have a strong dislike for java enum's due to the fact that they
>> >> > are not stable across JVM's - if it undergoes serde, you end up with
>> >> > unpredictable results at times [1].
>> >> > One of the reasons why we prevent enum's from being key : though it is
>> >> > highly possible users might depend on it internally and shoot
>> >> > themselves in the foot.
>> >> >
>> >> > Would be better to keep away from them in general and use something
>> more
>> >> > stable.
>> >> >
>> >> > Regards,
>> >> > Mridul
>> >> >
>> >> > [1] Having had to debug this issue for 2 weeks - I really really hate
>> it.
>> >> >
>> >> >
>> >> > On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com>
>> >> wrote:
>> >> > > I have a very strong dislike for #1 (scala enumerations).   I'm ok
>> with
>> >> > #4
>> >> > > (with Xiangrui's final suggestion, especially making it sealed &
>> >> > available
>> >> > > in Java), but I really think #2, java enums, are the best option.
>> >> > >
>> >> > > Java enums actually have some very real advantages over the other
>> >> > > approaches -- you get values(), valueOf(), EnumSet, and EnumMap.
>> There
>> >> > has
>> >> > > been endless debate in the Scala community about the problems with
>> the
>> >> > > approaches in Scala.  Very smart, level-headed Scala gurus have
>> >> > complained
>> >> > > about their short-comings (Rex Kerr's name is coming to mind, though
>> >> I'm
>> >> > > not positive about that); there have been numerous well-thought out
>> >> > > proposals to give Scala a better enum.  But the powers-that-be in
>> Scala
>> >> > > always reject them.  IIRC the explanation for rejecting is basically
>> >> that
>> >> > > (a) enums aren't important enough for introducing some new special
>> >> > feature,
>> >> > > scala's got bigger things to work on and (b) if you really need a
>> good
>> >> > > enum, just use java's enum.
>> >> > >
>> >> > > I doubt it really matters that much for Spark internals, which is
>> why I
>> >> > > think #4 is fine.  But I figured I'd give my spiel, because every
>> >> > developer
>> >> > > loves language wars :)
>> >> > >
>> >> > > Imran
>> >> > >
>> >> > >
>> >> > >
>> >> > > On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com>
>> >> wrote:
>> >> > >
>> >> > >> `case object` inside an `object` doesn't show up in Java. This is
>> the
>> >> > >> minimal code I found to make everything show up correctly in both
>> >> > >> Scala and Java:
>> >> > >>
>> >> > >> sealed abstract class StorageLevel // cannot be a trait
>> >> > >>
>> >> > >> object StorageLevel {
>> >> > >>   private[this] case object _MemoryOnly extends StorageLevel
>> >> > >>   final val MemoryOnly: StorageLevel = _MemoryOnly
>> >> > >>
>> >> > >>   private[this] case object _DiskOnly extends StorageLevel
>> >> > >>   final val DiskOnly: StorageLevel = _DiskOnly
>> >> > >> }
>> >> > >>
>> >> > >> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <
>> pwendell@gmail.com>
>> >> > >> wrote:
>> >> > >> > I like #4 as well and agree with Aaron's suggestion.
>> >> > >> >
>> >> > >> > - Patrick
>> >> > >> >
>> >> > >> > On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <
>> ilikerps@gmail.com>
>> >> > >> wrote:
>> >> > >> >> I'm cool with #4 as well, but make sure we dictate that the
>> values
>> >> > >> should
>> >> > >> >> be defined within an object with the same name as the
>> enumeration
>> >> > (like
>> >> > >> we
>> >> > >> >> do for StorageLevel). Otherwise we may pollute a higher
>> namespace.
>> >> > >> >>
>> >> > >> >> e.g. we SHOULD do:
>> >> > >> >>
>> >> > >> >> trait StorageLevel
>> >> > >> >> object StorageLevel {
>> >> > >> >>   case object MemoryOnly extends StorageLevel
>> >> > >> >>   case object DiskOnly extends StorageLevel
>> >> > >> >> }
>> >> > >> >>
>> >> > >> >> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>> >> > >> michael@databricks.com>
>> >> > >> >> wrote:
>> >> > >> >>
>> >> > >> >>> #4 with a preference for CamelCaseEnums
>> >> > >> >>>
>> >> > >> >>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
>> >> > joseph@databricks.com>
>> >> > >> >>> wrote:
>> >> > >> >>>
>> >> > >> >>> > another vote for #4
>> >> > >> >>> > People are already used to adding "()" in Java.
>> >> > >> >>> >
>> >> > >> >>> >
>> >> > >> >>> > On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <
>> >> javadba@gmail.com
>> >> > >
>> >> > >> >>> wrote:
>> >> > >> >>> >
>> >> > >> >>> > > #4 but with MemoryOnly (more scala-like)
>> >> > >> >>> > >
>> >> > >> >>> > > http://docs.scala-lang.org/style/naming-conventions.html
>> >> > >> >>> > >
>> >> > >> >>> > > Constants, Values, Variable and Methods
>> >> > >> >>> > >
>> >> > >> >>> > > Constant names should be in upper camel case. That is, if
>> the
>> >> > >> member is
>> >> > >> >>> > > final, immutable and it belongs to a package object or an
>> >> > object,
>> >> > >> it
>> >> > >> >>> may
>> >> > >> >>> > be
>> >> > >> >>> > > considered a constant (similar to Java'sstatic final
>> members):
>> >> > >> >>> > >
>> >> > >> >>> > >
>> >> > >> >>> > >    1. object Container {
>> >> > >> >>> > >    2.     val MyConstant = ...
>> >> > >> >>> > >    3. }
>> >> > >> >>> > >
>> >> > >> >>> > >
>> >> > >> >>> > > 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com
>> >:
>> >> > >> >>> > >
>> >> > >> >>> > > > Hi all,
>> >> > >> >>> > > >
>> >> > >> >>> > > > There are many places where we use enum-like types in
>> Spark,
>> >> > but
>> >> > >> in
>> >> > >> >>> > > > different ways. Every approach has both pros and cons. I
>> >> > wonder
>> >> > >> >>> > > > whether there should be an "official" approach for
>> enum-like
>> >> > >> types in
>> >> > >> >>> > > > Spark.
>> >> > >> >>> > > >
>> >> > >> >>> > > > 1. Scala's Enumeration (e.g., SchedulingMode,
>> WorkerState,
>> >> > etc)
>> >> > >> >>> > > >
>> >> > >> >>> > > > * All types show up as Enumeration.Value in Java.
>> >> > >> >>> > > >
>> >> > >> >>> > > >
>> >> > >> >>> > >
>> >> > >> >>> >
>> >> > >> >>>
>> >> > >>
>> >> >
>> >>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>> >> > >> >>> > > >
>> >> > >> >>> > > > 2. Java's Enum (e.g., SaveMode, IOMode)
>> >> > >> >>> > > >
>> >> > >> >>> > > > * Implementation must be in a Java file.
>> >> > >> >>> > > > * Values doesn't show up in the ScalaDoc:
>> >> > >> >>> > > >
>> >> > >> >>> > > >
>> >> > >> >>> > >
>> >> > >> >>> >
>> >> > >> >>>
>> >> > >>
>> >> >
>> >>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>> >> > >> >>> > > >
>> >> > >> >>> > > > 3. Static fields in Java (e.g., TripletFields)
>> >> > >> >>> > > >
>> >> > >> >>> > > > * Implementation must be in a Java file.
>> >> > >> >>> > > > * Doesn't need "()" in Java code.
>> >> > >> >>> > > > * Values don't show up in the ScalaDoc:
>> >> > >> >>> > > >
>> >> > >> >>> > > >
>> >> > >> >>> > >
>> >> > >> >>> >
>> >> > >> >>>
>> >> > >>
>> >> >
>> >>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>> >> > >> >>> > > >
>> >> > >> >>> > > > 4. Objects in Scala. (e.g., StorageLevel)
>> >> > >> >>> > > >
>> >> > >> >>> > > > * Needs "()" in Java code.
>> >> > >> >>> > > > * Values show up in both ScalaDoc and JavaDoc:
>> >> > >> >>> > > >
>> >> > >> >>> > > >
>> >> > >> >>> > >
>> >> > >> >>> >
>> >> > >> >>>
>> >> > >>
>> >> >
>> >>
>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>> >> > >> >>> > > >
>> >> > >> >>> > > >
>> >> > >> >>> > >
>> >> > >> >>> >
>> >> > >> >>>
>> >> > >>
>> >> >
>> >>
>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>> >> > >> >>> > > >
>> >> > >> >>> > > > It would be great if we have an "official" approach for
>> this
>> >> > as
>> >> > >> well
>> >> > >> >>> > > > as the naming convention for enum-like values
>> ("MEMORY_ONLY"
>> >> > or
>> >> > >> >>> > > > "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY".
>> Any
>> >> > >> >>> thoughts?
>> >> > >> >>> > > >
>> >> > >> >>> > > > Best,
>> >> > >> >>> > > > Xiangrui
>> >> > >> >>> > > >
>> >> > >> >>> > > >
>> >> > >>
>> ---------------------------------------------------------------------
>> >> > >> >>> > > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> > >> >>> > > > For additional commands, e-mail:
>> dev-help@spark.apache.org
>> >> > >> >>> > > >
>> >> > >> >>> > > >
>> >> > >> >>> > >
>> >> > >> >>> >
>> >> > >> >>>
>> >> > >>
>> >> > >>
>> ---------------------------------------------------------------------
>> >> > >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >> > >> For additional commands, e-mail: dev-help@spark.apache.org
>> >> > >>
>> >> > >>
>> >> >
>> >>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 19:16:02 2015
Return-Path: <dev-return-12031-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8244510F1B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 19:16:02 +0000 (UTC)
Received: (qmail 82516 invoked by uid 500); 16 Mar 2015 19:16:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82434 invoked by uid 500); 16 Mar 2015 19:16:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82423 invoked by uid 99); 16 Mar 2015 19:16:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 19:16:00 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kevin.markey@oracle.com designates 141.146.126.69 as permitted sender)
Received: from [141.146.126.69] (HELO aserp1040.oracle.com) (141.146.126.69)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 19:15:55 +0000
Received: from acsinet21.oracle.com (acsinet21.oracle.com [141.146.126.237])
	by aserp1040.oracle.com (Sentrion-MTA-4.3.2/Sentrion-MTA-4.3.2) with ESMTP id t2GJCYY2004124
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=OK)
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 19:12:35 GMT
Received: from userz7021.oracle.com (userz7021.oracle.com [156.151.31.85])
	by acsinet21.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id t2GJCXZg021126
	(version=TLSv1/SSLv3 cipher=DHE-RSA-AES256-SHA bits=256 verify=FAIL)
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 19:12:34 GMT
Received: from abhmp0010.oracle.com (abhmp0010.oracle.com [141.146.116.16])
	by userz7021.oracle.com (8.14.4+Sun/8.14.4) with ESMTP id t2GJCXlG028073
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 19:12:33 GMT
Received: from [10.135.123.92] (/10.135.123.92)
	by default (Oracle Beehive Gateway v4.0)
	with ESMTP ; Mon, 16 Mar 2015 12:12:32 -0700
Message-ID: <55072B1A.7000907@oracle.com>
Date: Mon, 16 Mar 2015 13:12:26 -0600
From: Kevin Markey <kevin.markey@oracle.com>
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.4.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: enum-like types in Spark
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com> <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
In-Reply-To: <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Source-IP: acsinet21.oracle.com [141.146.126.237]
X-Virus-Checked: Checked by ClamAV on apache.org

In some applications, I have rather heavy use of Java enums which are 
needed for related Java APIs that the application uses.  And 
unfortunately, they are also used as keys.  As such, using the native 
hashcodes makes any function over keys unstable and unpredictable, so we 
now use Enum.name() as the key instead.  Oh well.  But it works and 
seems to work well.

Kevin

On 03/05/2015 09:49 PM, Mridul Muralidharan wrote:
>    I have a strong dislike for java enum's due to the fact that they
> are not stable across JVM's - if it undergoes serde, you end up with
> unpredictable results at times [1].
> One of the reasons why we prevent enum's from being key : though it is
> highly possible users might depend on it internally and shoot
> themselves in the foot.
>
> Would be better to keep away from them in general and use something more stable.
>
> Regards,
> Mridul
>
> [1] Having had to debug this issue for 2 weeks - I really really hate it.
>
>
> On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com> wrote:
>> I have a very strong dislike for #1 (scala enumerations).   I'm ok with #4
>> (with Xiangrui's final suggestion, especially making it sealed & available
>> in Java), but I really think #2, java enums, are the best option.
>>
>> Java enums actually have some very real advantages over the other
>> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There has
>> been endless debate in the Scala community about the problems with the
>> approaches in Scala.  Very smart, level-headed Scala gurus have complained
>> about their short-comings (Rex Kerr's name is coming to mind, though I'm
>> not positive about that); there have been numerous well-thought out
>> proposals to give Scala a better enum.  But the powers-that-be in Scala
>> always reject them.  IIRC the explanation for rejecting is basically that
>> (a) enums aren't important enough for introducing some new special feature,
>> scala's got bigger things to work on and (b) if you really need a good
>> enum, just use java's enum.
>>
>> I doubt it really matters that much for Spark internals, which is why I
>> think #4 is fine.  But I figured I'd give my spiel, because every developer
>> loves language wars :)
>>
>> Imran
>>
>>
>>
>> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>
>>> `case object` inside an `object` doesn't show up in Java. This is the
>>> minimal code I found to make everything show up correctly in both
>>> Scala and Java:
>>>
>>> sealed abstract class StorageLevel // cannot be a trait
>>>
>>> object StorageLevel {
>>>    private[this] case object _MemoryOnly extends StorageLevel
>>>    final val MemoryOnly: StorageLevel = _MemoryOnly
>>>
>>>    private[this] case object _DiskOnly extends StorageLevel
>>>    final val DiskOnly: StorageLevel = _DiskOnly
>>> }
>>>
>>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>> I like #4 as well and agree with Aaron's suggestion.
>>>>
>>>> - Patrick
>>>>
>>>> On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>>> wrote:
>>>>> I'm cool with #4 as well, but make sure we dictate that the values
>>> should
>>>>> be defined within an object with the same name as the enumeration (like
>>> we
>>>>> do for StorageLevel). Otherwise we may pollute a higher namespace.
>>>>>
>>>>> e.g. we SHOULD do:
>>>>>
>>>>> trait StorageLevel
>>>>> object StorageLevel {
>>>>>    case object MemoryOnly extends StorageLevel
>>>>>    case object DiskOnly extends StorageLevel
>>>>> }
>>>>>
>>>>> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>>> michael@databricks.com>
>>>>> wrote:
>>>>>
>>>>>> #4 with a preference for CamelCaseEnums
>>>>>>
>>>>>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <joseph@databricks.com>
>>>>>> wrote:
>>>>>>
>>>>>>> another vote for #4
>>>>>>> People are already used to adding "()" in Java.
>>>>>>>
>>>>>>>
>>>>>>> On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>>>>>> wrote:
>>>>>>>> #4 but with MemoryOnly (more scala-like)
>>>>>>>>
>>>>>>>> http://docs.scala-lang.org/style/naming-conventions.html
>>>>>>>>
>>>>>>>> Constants, Values, Variable and Methods
>>>>>>>>
>>>>>>>> Constant names should be in upper camel case. That is, if the
>>> member is
>>>>>>>> final, immutable and it belongs to a package object or an object,
>>> it
>>>>>> may
>>>>>>> be
>>>>>>>> considered a constant (similar to Java'sstatic final members):
>>>>>>>>
>>>>>>>>
>>>>>>>>     1. object Container {
>>>>>>>>     2.     val MyConstant = ...
>>>>>>>>     3. }
>>>>>>>>
>>>>>>>>
>>>>>>>> 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>>>>>>>>
>>>>>>>>> Hi all,
>>>>>>>>>
>>>>>>>>> There are many places where we use enum-like types in Spark, but
>>> in
>>>>>>>>> different ways. Every approach has both pros and cons. I wonder
>>>>>>>>> whether there should be an "official" approach for enum-like
>>> types in
>>>>>>>>> Spark.
>>>>>>>>>
>>>>>>>>> 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>>>>>>>>>
>>>>>>>>> * All types show up as Enumeration.Value in Java.
>>>>>>>>>
>>>>>>>>>
>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>>>>>>>>> 2. Java's Enum (e.g., SaveMode, IOMode)
>>>>>>>>>
>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>> * Values doesn't show up in the ScalaDoc:
>>>>>>>>>
>>>>>>>>>
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>>>>>>>>> 3. Static fields in Java (e.g., TripletFields)
>>>>>>>>>
>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>> * Doesn't need "()" in Java code.
>>>>>>>>> * Values don't show up in the ScalaDoc:
>>>>>>>>>
>>>>>>>>>
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>>>>>>>>> 4. Objects in Scala. (e.g., StorageLevel)
>>>>>>>>>
>>>>>>>>> * Needs "()" in Java code.
>>>>>>>>> * Values show up in both ScalaDoc and JavaDoc:
>>>>>>>>>
>>>>>>>>>
>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>>>>>>>>>
>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>>>>>>>>> It would be great if we have an "official" approach for this as
>>> well
>>>>>>>>> as the naming convention for enum-like values ("MEMORY_ONLY" or
>>>>>>>>> "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>>>>>> thoughts?
>>>>>>>>> Best,
>>>>>>>>> Xiangrui
>>>>>>>>>
>>>>>>>>>
>>> ---------------------------------------------------------------------
>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>
>>>>>>>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 20:56:35 2015
Return-Path: <dev-return-12032-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A04B6177D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 20:56:35 +0000 (UTC)
Received: (qmail 57339 invoked by uid 500); 16 Mar 2015 20:56:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57258 invoked by uid 500); 16 Mar 2015 20:56:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57246 invoked by uid 99); 16 Mar 2015 20:56:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 20:56:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 20:56:30 +0000
Received: by oibu204 with SMTP id u204so48822879oib.0
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 13:54:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=KR7UT9BDdtFm6YtCv1gwjj6BD1NWzlvUtAivOw3gO6M=;
        b=GwdVP0pHR2PnwLTma0kzsk+xEXgCQ//hm9xAsXoUVrgBXFIK0STfmSVD/cCmwzGuud
         dGJz08jiSUQ5KzgHsMAih1c7BItrOtNKE366RWeb0hNShA7OY95b3NSK4zp/SrhsSXc5
         xlgjIQgmJKXYh/pBHJatPw0+zKnSvYWde3zyY1hh7FH+VGN+R0hDyJzJHWiCliHIB4ID
         LBR8D1YBq7KJHK+GIjtvgQatJtDKRFT9SRCzsh5gA1VJewAfnbwCkQJmwRWAI6go9ZYX
         sIdRZn6MSTwgxXZ0GObM5J0F9pWbYm2x+9Yb9bV74T5ILaOhoow6S1FZ2uzA3WR4J7vP
         JIkQ==
MIME-Version: 1.0
X-Received: by 10.60.16.168 with SMTP id h8mr30912845oed.4.1426539279831; Mon,
 16 Mar 2015 13:54:39 -0700 (PDT)
Received: by 10.202.199.135 with HTTP; Mon, 16 Mar 2015 13:54:39 -0700 (PDT)
In-Reply-To: <55072B1A.7000907@oracle.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
	<CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
	<55072B1A.7000907@oracle.com>
Date: Mon, 16 Mar 2015 13:54:39 -0700
Message-ID: <CABPQxstMFMafJYeFbs-+pgMXr2NeV7RPMfLShuRY3nCP+rpt6w@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Kevin Markey <kevin.markey@oracle.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Xiangrui,

Do you want to write up a straw man proposal based on this line of discussion?

- Patrick

On Mon, Mar 16, 2015 at 12:12 PM, Kevin Markey <kevin.markey@oracle.com> wrote:
> In some applications, I have rather heavy use of Java enums which are needed
> for related Java APIs that the application uses.  And unfortunately, they
> are also used as keys.  As such, using the native hashcodes makes any
> function over keys unstable and unpredictable, so we now use Enum.name() as
> the key instead.  Oh well.  But it works and seems to work well.
>
> Kevin
>
>
> On 03/05/2015 09:49 PM, Mridul Muralidharan wrote:
>>
>>    I have a strong dislike for java enum's due to the fact that they
>> are not stable across JVM's - if it undergoes serde, you end up with
>> unpredictable results at times [1].
>> One of the reasons why we prevent enum's from being key : though it is
>> highly possible users might depend on it internally and shoot
>> themselves in the foot.
>>
>> Would be better to keep away from them in general and use something more
>> stable.
>>
>> Regards,
>> Mridul
>>
>> [1] Having had to debug this issue for 2 weeks - I really really hate it.
>>
>>
>> On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com> wrote:
>>>
>>> I have a very strong dislike for #1 (scala enumerations).   I'm ok with
>>> #4
>>> (with Xiangrui's final suggestion, especially making it sealed &
>>> available
>>> in Java), but I really think #2, java enums, are the best option.
>>>
>>> Java enums actually have some very real advantages over the other
>>> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There
>>> has
>>> been endless debate in the Scala community about the problems with the
>>> approaches in Scala.  Very smart, level-headed Scala gurus have
>>> complained
>>> about their short-comings (Rex Kerr's name is coming to mind, though I'm
>>> not positive about that); there have been numerous well-thought out
>>> proposals to give Scala a better enum.  But the powers-that-be in Scala
>>> always reject them.  IIRC the explanation for rejecting is basically that
>>> (a) enums aren't important enough for introducing some new special
>>> feature,
>>> scala's got bigger things to work on and (b) if you really need a good
>>> enum, just use java's enum.
>>>
>>> I doubt it really matters that much for Spark internals, which is why I
>>> think #4 is fine.  But I figured I'd give my spiel, because every
>>> developer
>>> loves language wars :)
>>>
>>> Imran
>>>
>>>
>>>
>>> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>
>>>> `case object` inside an `object` doesn't show up in Java. This is the
>>>> minimal code I found to make everything show up correctly in both
>>>> Scala and Java:
>>>>
>>>> sealed abstract class StorageLevel // cannot be a trait
>>>>
>>>> object StorageLevel {
>>>>    private[this] case object _MemoryOnly extends StorageLevel
>>>>    final val MemoryOnly: StorageLevel = _MemoryOnly
>>>>
>>>>    private[this] case object _DiskOnly extends StorageLevel
>>>>    final val DiskOnly: StorageLevel = _DiskOnly
>>>> }
>>>>
>>>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>>>> wrote:
>>>>>
>>>>> I like #4 as well and agree with Aaron's suggestion.
>>>>>
>>>>> - Patrick
>>>>>
>>>>> On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>>>>
>>>> wrote:
>>>>>>
>>>>>> I'm cool with #4 as well, but make sure we dictate that the values
>>>>
>>>> should
>>>>>>
>>>>>> be defined within an object with the same name as the enumeration
>>>>>> (like
>>>>
>>>> we
>>>>>>
>>>>>> do for StorageLevel). Otherwise we may pollute a higher namespace.
>>>>>>
>>>>>> e.g. we SHOULD do:
>>>>>>
>>>>>> trait StorageLevel
>>>>>> object StorageLevel {
>>>>>>    case object MemoryOnly extends StorageLevel
>>>>>>    case object DiskOnly extends StorageLevel
>>>>>> }
>>>>>>
>>>>>> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>>>>
>>>> michael@databricks.com>
>>>>>>
>>>>>> wrote:
>>>>>>
>>>>>>> #4 with a preference for CamelCaseEnums
>>>>>>>
>>>>>>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley
>>>>>>> <joseph@databricks.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> another vote for #4
>>>>>>>> People are already used to adding "()" in Java.
>>>>>>>>
>>>>>>>>
>>>>>>>> On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>>>>>>>
>>>>>>> wrote:
>>>>>>>>>
>>>>>>>>> #4 but with MemoryOnly (more scala-like)
>>>>>>>>>
>>>>>>>>> http://docs.scala-lang.org/style/naming-conventions.html
>>>>>>>>>
>>>>>>>>> Constants, Values, Variable and Methods
>>>>>>>>>
>>>>>>>>> Constant names should be in upper camel case. That is, if the
>>>>
>>>> member is
>>>>>>>>>
>>>>>>>>> final, immutable and it belongs to a package object or an object,
>>>>
>>>> it
>>>>>>>
>>>>>>> may
>>>>>>>>
>>>>>>>> be
>>>>>>>>>
>>>>>>>>> considered a constant (similar to Java'sstatic final members):
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>     1. object Container {
>>>>>>>>>     2.     val MyConstant = ...
>>>>>>>>>     3. }
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>>>>>>>>>
>>>>>>>>>> Hi all,
>>>>>>>>>>
>>>>>>>>>> There are many places where we use enum-like types in Spark, but
>>>>
>>>> in
>>>>>>>>>>
>>>>>>>>>> different ways. Every approach has both pros and cons. I wonder
>>>>>>>>>> whether there should be an "official" approach for enum-like
>>>>
>>>> types in
>>>>>>>>>>
>>>>>>>>>> Spark.
>>>>>>>>>>
>>>>>>>>>> 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>>>>>>>>>>
>>>>>>>>>> * All types show up as Enumeration.Value in Java.
>>>>>>>>>>
>>>>>>>>>>
>>>>
>>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/scheduler/SchedulingMode.html
>>>>>>>>>>
>>>>>>>>>> 2. Java's Enum (e.g., SaveMode, IOMode)
>>>>>>>>>>
>>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>>> * Values doesn't show up in the ScalaDoc:
>>>>>>>>>>
>>>>>>>>>>
>>>>
>>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.network.util.IOMode
>>>>>>>>>>
>>>>>>>>>> 3. Static fields in Java (e.g., TripletFields)
>>>>>>>>>>
>>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>>> * Doesn't need "()" in Java code.
>>>>>>>>>> * Values don't show up in the ScalaDoc:
>>>>>>>>>>
>>>>>>>>>>
>>>>
>>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.graphx.TripletFields
>>>>>>>>>>
>>>>>>>>>> 4. Objects in Scala. (e.g., StorageLevel)
>>>>>>>>>>
>>>>>>>>>> * Needs "()" in Java code.
>>>>>>>>>> * Values show up in both ScalaDoc and JavaDoc:
>>>>>>>>>>
>>>>>>>>>>
>>>>
>>>> http://spark.apache.org/docs/latest/api/scala/#org.apache.spark.storage.StorageLevel$
>>>>>>>>>>
>>>>>>>>>>
>>>>
>>>> http://spark.apache.org/docs/latest/api/java/org/apache/spark/storage/StorageLevel.html
>>>>>>>>>>
>>>>>>>>>> It would be great if we have an "official" approach for this as
>>>>
>>>> well
>>>>>>>>>>
>>>>>>>>>> as the naming convention for enum-like values ("MEMORY_ONLY" or
>>>>>>>>>> "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>>>>>>>
>>>>>>> thoughts?
>>>>>>>>>>
>>>>>>>>>> Best,
>>>>>>>>>> Xiangrui
>>>>>>>>>>
>>>>>>>>>>
>>>> ---------------------------------------------------------------------
>>>>>>>>>>
>>>>>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>>
>>>>>>>>>>
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 16 22:07:42 2015
Return-Path: <dev-return-12033-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C98BE17B84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 16 Mar 2015 22:07:42 +0000 (UTC)
Received: (qmail 84995 invoked by uid 500); 16 Mar 2015 22:07:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84922 invoked by uid 500); 16 Mar 2015 22:07:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84910 invoked by uid 99); 16 Mar 2015 22:07:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 22:07:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 74.125.82.173 as permitted sender)
Received: from [74.125.82.173] (HELO mail-we0-f173.google.com) (74.125.82.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 16 Mar 2015 22:07:37 +0000
Received: by weop45 with SMTP id p45so22722866weo.0
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 15:05:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=sDS6Gba8nES+RnufNmYNt5qiLA+1nuWrOVFMHb2U6nU=;
        b=QY7IhbqLPrmaDtUdMwevoAU4Ep/7pdroGP81LbsEcFGF3+NmQRNakDY9v+P7hGtLYv
         j7iKnIkZtZrrgo/8wVtcJXPKW4np7jOn+USkufRKlqYSULP0whZNgeXjQ/TFgEGyr+XL
         wc6St1GDorU0WWzCBTaUyxc5oksPj0WsnzO275V/w4x0wC5C9mL+t3NeCY5nZrwuxwzj
         jG/DT/FZk5m0RydHBUbxems1aK3ViWqUvuuz2OtRGhqHY4RL2USlZZwkAE2ppXGVidz4
         KZ4JMb1asIyRvzZRpRl+1L5JI+DoehM4FNSIyIJ+UQ6cG8LEpZsCEbrWih++Qnretz37
         9WqA==
X-Received: by 10.181.11.202 with SMTP id ek10mr87667999wid.37.1426543501178;
 Mon, 16 Mar 2015 15:05:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Mon, 16 Mar 2015 15:04:40 -0700 (PDT)
In-Reply-To: <55072B1A.7000907@oracle.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
 <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com> <55072B1A.7000907@oracle.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 16 Mar 2015 15:04:40 -0700
Message-ID: <CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Kevin Markey <kevin.markey@oracle.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043c0904a4fbb905116f0a66
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c0904a4fbb905116f0a66
Content-Type: text/plain; charset=UTF-8

It's unrelated to the proposal, but Enum#ordinal() should be much faster,
assuming it's not serialized to JVMs with different versions of the enum :)

On Mon, Mar 16, 2015 at 12:12 PM, Kevin Markey <kevin.markey@oracle.com>
wrote:

> In some applications, I have rather heavy use of Java enums which are
> needed for related Java APIs that the application uses.  And unfortunately,
> they are also used as keys.  As such, using the native hashcodes makes any
> function over keys unstable and unpredictable, so we now use Enum.name() as
> the key instead.  Oh well.  But it works and seems to work well.
>
> Kevin
>
>
> On 03/05/2015 09:49 PM, Mridul Muralidharan wrote:
>
>>    I have a strong dislike for java enum's due to the fact that they
>> are not stable across JVM's - if it undergoes serde, you end up with
>> unpredictable results at times [1].
>> One of the reasons why we prevent enum's from being key : though it is
>> highly possible users might depend on it internally and shoot
>> themselves in the foot.
>>
>> Would be better to keep away from them in general and use something more
>> stable.
>>
>> Regards,
>> Mridul
>>
>> [1] Having had to debug this issue for 2 weeks - I really really hate it.
>>
>>
>> On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com>
>> wrote:
>>
>>> I have a very strong dislike for #1 (scala enumerations).   I'm ok with
>>> #4
>>> (with Xiangrui's final suggestion, especially making it sealed &
>>> available
>>> in Java), but I really think #2, java enums, are the best option.
>>>
>>> Java enums actually have some very real advantages over the other
>>> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There
>>> has
>>> been endless debate in the Scala community about the problems with the
>>> approaches in Scala.  Very smart, level-headed Scala gurus have
>>> complained
>>> about their short-comings (Rex Kerr's name is coming to mind, though I'm
>>> not positive about that); there have been numerous well-thought out
>>> proposals to give Scala a better enum.  But the powers-that-be in Scala
>>> always reject them.  IIRC the explanation for rejecting is basically that
>>> (a) enums aren't important enough for introducing some new special
>>> feature,
>>> scala's got bigger things to work on and (b) if you really need a good
>>> enum, just use java's enum.
>>>
>>> I doubt it really matters that much for Spark internals, which is why I
>>> think #4 is fine.  But I figured I'd give my spiel, because every
>>> developer
>>> loves language wars :)
>>>
>>> Imran
>>>
>>>
>>>
>>> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>
>>>  `case object` inside an `object` doesn't show up in Java. This is the
>>>> minimal code I found to make everything show up correctly in both
>>>> Scala and Java:
>>>>
>>>> sealed abstract class StorageLevel // cannot be a trait
>>>>
>>>> object StorageLevel {
>>>>    private[this] case object _MemoryOnly extends StorageLevel
>>>>    final val MemoryOnly: StorageLevel = _MemoryOnly
>>>>
>>>>    private[this] case object _DiskOnly extends StorageLevel
>>>>    final val DiskOnly: StorageLevel = _DiskOnly
>>>> }
>>>>
>>>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>>>> wrote:
>>>>
>>>>> I like #4 as well and agree with Aaron's suggestion.
>>>>>
>>>>> - Patrick
>>>>>
>>>>> On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>>>>>
>>>> wrote:
>>>>
>>>>> I'm cool with #4 as well, but make sure we dictate that the values
>>>>>>
>>>>> should
>>>>
>>>>> be defined within an object with the same name as the enumeration (like
>>>>>>
>>>>> we
>>>>
>>>>> do for StorageLevel). Otherwise we may pollute a higher namespace.
>>>>>>
>>>>>> e.g. we SHOULD do:
>>>>>>
>>>>>> trait StorageLevel
>>>>>> object StorageLevel {
>>>>>>    case object MemoryOnly extends StorageLevel
>>>>>>    case object DiskOnly extends StorageLevel
>>>>>> }
>>>>>>
>>>>>> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>>>>>>
>>>>> michael@databricks.com>
>>>>
>>>>> wrote:
>>>>>>
>>>>>>  #4 with a preference for CamelCaseEnums
>>>>>>>
>>>>>>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
>>>>>>> joseph@databricks.com>
>>>>>>> wrote:
>>>>>>>
>>>>>>>  another vote for #4
>>>>>>>> People are already used to adding "()" in Java.
>>>>>>>>
>>>>>>>>
>>>>>>>> On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>>>>>>>>
>>>>>>> wrote:
>>>>>>>
>>>>>>>> #4 but with MemoryOnly (more scala-like)
>>>>>>>>>
>>>>>>>>> http://docs.scala-lang.org/style/naming-conventions.html
>>>>>>>>>
>>>>>>>>> Constants, Values, Variable and Methods
>>>>>>>>>
>>>>>>>>> Constant names should be in upper camel case. That is, if the
>>>>>>>>>
>>>>>>>> member is
>>>>
>>>>> final, immutable and it belongs to a package object or an object,
>>>>>>>>>
>>>>>>>> it
>>>>
>>>>> may
>>>>>>>
>>>>>>>> be
>>>>>>>>
>>>>>>>>> considered a constant (similar to Java'sstatic final members):
>>>>>>>>>
>>>>>>>>>
>>>>>>>>>     1. object Container {
>>>>>>>>>     2.     val MyConstant = ...
>>>>>>>>>     3. }
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>>>>>>>>>
>>>>>>>>>  Hi all,
>>>>>>>>>>
>>>>>>>>>> There are many places where we use enum-like types in Spark, but
>>>>>>>>>>
>>>>>>>>> in
>>>>
>>>>> different ways. Every approach has both pros and cons. I wonder
>>>>>>>>>> whether there should be an "official" approach for enum-like
>>>>>>>>>>
>>>>>>>>> types in
>>>>
>>>>> Spark.
>>>>>>>>>>
>>>>>>>>>> 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>>>>>>>>>>
>>>>>>>>>> * All types show up as Enumeration.Value in Java.
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  http://spark.apache.org/docs/latest/api/java/org/apache/
>>>> spark/scheduler/SchedulingMode.html
>>>>
>>>>> 2. Java's Enum (e.g., SaveMode, IOMode)
>>>>>>>>>>
>>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>>> * Values doesn't show up in the ScalaDoc:
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
>>>> spark.network.util.IOMode
>>>>
>>>>> 3. Static fields in Java (e.g., TripletFields)
>>>>>>>>>>
>>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>>> * Doesn't need "()" in Java code.
>>>>>>>>>> * Values don't show up in the ScalaDoc:
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
>>>> spark.graphx.TripletFields
>>>>
>>>>> 4. Objects in Scala. (e.g., StorageLevel)
>>>>>>>>>>
>>>>>>>>>> * Needs "()" in Java code.
>>>>>>>>>> * Values show up in both ScalaDoc and JavaDoc:
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
>>>> spark.storage.StorageLevel$
>>>>
>>>>>
>>>>>>>>>>  http://spark.apache.org/docs/latest/api/java/org/apache/
>>>> spark/storage/StorageLevel.html
>>>>
>>>>> It would be great if we have an "official" approach for this as
>>>>>>>>>>
>>>>>>>>> well
>>>>
>>>>> as the naming convention for enum-like values ("MEMORY_ONLY" or
>>>>>>>>>> "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>>>>>>>>>>
>>>>>>>>> thoughts?
>>>>>>>
>>>>>>>> Best,
>>>>>>>>>> Xiangrui
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  ------------------------------------------------------------
>>>> ---------
>>>>
>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>  ------------------------------------------------------------
>>>> ---------
>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>
>>>>
>>>>  ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--f46d043c0904a4fbb905116f0a66--

From dev-return-12034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 02:49:20 2015
Return-Path: <dev-return-12034-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4DD11781C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 02:49:20 +0000 (UTC)
Received: (qmail 56128 invoked by uid 500); 17 Mar 2015 02:49:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56034 invoked by uid 500); 17 Mar 2015 02:49:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56022 invoked by uid 99); 17 Mar 2015 02:49:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 02:49:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 02:49:08 +0000
Received: by qgh62 with SMTP id 62so58168795qgh.1
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 19:46:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ZmVhk9Zk3dezRQ3DRid+Rw3vmTw2or5tSS836advrNE=;
        b=hzD2pzLytyG2A6dWXjlf47d9lvBBSng7Peu9JWx/qvbqOMYI818QYASJilXZD8z6IF
         4GQtl/Gr3xUqkGw2lrFMPf+Rcjyi1t8RyE6SoaVSaztdb126yKrw5vVBYeIZVxkM5t7y
         Locy85qJMkld5bLxMf/EIukKGtE103qsB5QoLZP8k40JS/FY0HVEDmxvkn+Wg7FHxHmC
         rNVg9E8ttyZE8x2ypHCUiKyJHMmX8Ale6DqnOAGUKOiA41xTz1jD6Ip5+sIxGHeKLgzB
         chH3K8sX4E9jXurdeSLUu7hCZeU835WBk3hFkdGanDZrkArT63PJsYZdIDG5+QoXi6kP
         J4cw==
X-Gm-Message-State: ALoCoQlZQ//ZYi9hYu4utpVWrfPNoWsMtV0zDhYHbjp64AjY/4NknHJUWYwwZ6zYR+AyMLHvBFCB
X-Received: by 10.229.71.72 with SMTP id g8mr39781722qcj.25.1426560372507;
 Mon, 16 Mar 2015 19:46:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.109.9 with HTTP; Mon, 16 Mar 2015 19:45:52 -0700 (PDT)
In-Reply-To: <CAPszQwg+tu_NnL9mvAC0gpkO3_HXJ7-XAo2pnQGK2kco4ukPuA@mail.gmail.com>
References: <CAPszQwipUk+d=daGSW1+SGquOV0YMOtpYMVUYVyzcFS16788YQ@mail.gmail.com>
 <CAJiQeYJkBG5p43DoTCeF4O+Fx7baEoxcc7jc-qL-1=doP9eHUg@mail.gmail.com>
 <CAPszQwg=Lsj4MOkBE=_tTHH3mK5Md21rDEEbYk+qJv0+zxqUQg@mail.gmail.com> <CAPszQwg+tu_NnL9mvAC0gpkO3_HXJ7-XAo2pnQGK2kco4ukPuA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 16 Mar 2015 19:45:52 -0700
Message-ID: <CAPh_B=Yb81B9Q=TSDX7yWdBrmiheCpcgVg35uLnLA-EWsBg6aA@mail.gmail.com>
Subject: Re: broadcast hang out
To: lonely Feb <lonely8658@gmail.com>
Cc: Mridul Muralidharan <mridul@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133b51e411fcf051172f806
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133b51e411fcf051172f806
Content-Type: text/plain; charset=UTF-8

It would be great to add a timeout. Do you mind submitting a pull request?


On Sun, Mar 15, 2015 at 10:41 PM, lonely Feb <lonely8658@gmail.com> wrote:

> Anyone can help? Thanks a lot !
>
> 2015-03-16 11:45 GMT+08:00 lonely Feb <lonely8658@gmail.com>:
>
> > yes
> >
> > 2015-03-16 11:43 GMT+08:00 Mridul Muralidharan <mridul@gmail.com>:
> >
> >> Cross region as in different data centers ?
> >>
> >> - Mridul
> >>
> >> On Sun, Mar 15, 2015 at 8:08 PM, lonely Feb <lonely8658@gmail.com>
> wrote:
> >> > Hi all, i meet up with a problem that torrent broadcast hang out in my
> >> > spark cluster (1.2, standalone) , particularly serious when driver and
> >> > executors are cross-region. when i read the code of broadcast i found
> >> that
> >> > a sync block read here:
> >> >
> >> >   def fetchBlockSync(host: String, port: Int, execId: String, blockId:
> >> > String): ManagedBuffer = {
> >> >     // A monitor for the thread to wait on.
> >> >     val result = Promise[ManagedBuffer]()
> >> >     fetchBlocks(host, port, execId, Array(blockId),
> >> >       new BlockFetchingListener {
> >> >         override def onBlockFetchFailure(blockId: String, exception:
> >> > Throwable): Unit = {
> >> >           result.failure(exception)
> >> >         }
> >> >         override def onBlockFetchSuccess(blockId: String, data:
> >> > ManagedBuffer): Unit = {
> >> >           val ret = ByteBuffer.allocate(data.size.toInt)
> >> >           ret.put(data.nioByteBuffer())
> >> >           ret.flip()
> >> >           result.success(new NioManagedBuffer(ret))
> >> >         }
> >> >       })
> >> >
> >> >     Await.result(result.future, Duration.Inf)
> >> >   }
> >> >
> >> > it seems that fetchBlockSync method does not have a timeout limit but
> >> wait
> >> > forever ? Anybody can show me how to control the timeout here?
> >>
> >
> >
>

--001a1133b51e411fcf051172f806--

From dev-return-12035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 02:51:19 2015
Return-Path: <dev-return-12035-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 925541783C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 02:51:19 +0000 (UTC)
Received: (qmail 60506 invoked by uid 500); 17 Mar 2015 02:51:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60418 invoked by uid 500); 17 Mar 2015 02:51:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60404 invoked by uid 99); 17 Mar 2015 02:51:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 02:51:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pllee@appier.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 02:51:14 +0000
Received: by qcto4 with SMTP id o4so62493831qct.3
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 19:50:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=9qg41fb95BQujf0SqoMSF8ZEfRQ3nlyk+XF9Pf6aUko=;
        b=WvNWGwHqlfWmlbekMuKYSNJBU2jzdj+5jvDeOrzBOyqFkdxeQ92aoIALa3tC/2FNjg
         ptIptAg5281Y2ps1svdkYoXOefDQANnh8ZSN0AQ8d9vQDYM2bN0Zgk8TQJBiNdxlaBd6
         LQEV5HDvc3FBxyJaUJqYbvkwt0Z3hSo9FSKy0=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=9qg41fb95BQujf0SqoMSF8ZEfRQ3nlyk+XF9Pf6aUko=;
        b=VaGfWdkEc3zuC2EDJ2DuFhNhmRyvmfZjk1iXLUzqn2IebzT4X+Ii7t3ql8ZyglG/7+
         A7HXX5N80zZxE8n3DwXbsB9T6WTWmq7dbqqBleCGvkzHn54As2thrDBSXNN5Zc9MWmu+
         s2z10JEofXe/iHKWkldvrtcW1fYWDoO1C+eWNyq2LuFjNGSno1dojf9LlwNYwWfi4hcA
         7sTZBHHllB34GZEV1kuJUevfEWtyn5/UwbsS51CvSMWULhEPQT4kfHYnYLLAvgIIcogU
         0Qn3TJI8T4kNpLhFN3K95HtEUt59KcYJPMEAf8uGWcgWaW+JfqKO//alE5JLwpI+fUjq
         JlUA==
X-Gm-Message-State: ALoCoQl2XJVXuqR7pKb5SkvlTFgsL8WodjPcVAYdvSxxGX+j8UUYkmB+QpbROjgEwW+YVTThNT97
MIME-Version: 1.0
X-Received: by 10.140.145.3 with SMTP id 3mr83008939qhr.42.1426560607860; Mon,
 16 Mar 2015 19:50:07 -0700 (PDT)
Received: by 10.229.233.136 with HTTP; Mon, 16 Mar 2015 19:50:07 -0700 (PDT)
In-Reply-To: <5506B3CB.9040205@gmail.com>
References: <CANrtgzU-e7kWkNafRyOSDO290rgJMwh-ZQz2APYhb5dPOfFJFg@mail.gmail.com>
	<5506B2E3.60608@gmail.com>
	<5506B3CB.9040205@gmail.com>
Date: Tue, 17 Mar 2015 10:50:07 +0800
Message-ID: <CANrtgzXepXLrhWp4PRCN-CH5GTRFdyWgKaY0SS-XkJE9zi7EYA@mail.gmail.com>
Subject: Re: SparkSQL 1.3.0 cannot read parquet files from different file system
From: Pei-Lun Lee <pllee@appier.com>
To: Cheng Lian <lian.cs.zju@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1137471c48551d0511730634
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1137471c48551d0511730634
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Looks like this is already solved in
https://issues.apache.org/jira/browse/SPARK-6330

On Mon, Mar 16, 2015 at 6:43 PM, Cheng Lian <lian.cs.zju@gmail.com> wrote:

>  Oh sorry, I misread your question. I thought you were trying something
> like parquetFile(=E2=80=9Cs3n://file1,hdfs://file2=E2=80=9D). Yeah, it=E2=
=80=99s a valid bug.
> Thanks for opening the JIRA ticket and the PR!
>
>
> Cheng
>
> On 3/16/15 6:39 PM, Cheng Lian wrote:
>
>   Hi Pei-Lun,
>
> We intentionally disallowed passing multiple comma separated paths in
> 1.3.0. One of the reason is that users report that this fail when a file
> path contain an actual comma in it. In your case, you may do something li=
ke
> this:
>
> val s3nDF =3D parquetFile("s3n
> ://...
> ")val hdfsDF =3D parquetFile("hdfs://...")val finalDF =3D s3nDF.union(fin=
alDF)
>
> Cheng
>
> On 3/16/15 4:03 PM, Pei-Lun Lee wrote:
>
> Hi,
>
> I am using Spark 1.3.0, where I cannot load parquet files from more than
> one file system, say one s3n://... and another hdfs://..., which worked i=
n
> older version, or if I set spark.sql.parquet.useDataSourceApi=3Dfalse in =
1.3.
>
> One way to fix this is instead of get a single FileSystem from default
> configuration in ParquetRelation2, call Path.getFileSystem for each path.
>
> Here's the JIRA link and pull request:https://issues.apache.org/jira/brow=
se/SPARK-6351https://github.com/apache/spark/pull/5039
>
> Thanks,
> --
> Pei-Lun
>
>
>  =E2=80=8B
>
>   =E2=80=8B
> =E2=80=8B
> =E2=80=8B
>

--001a1137471c48551d0511730634--

From dev-return-12036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 02:57:16 2015
Return-Path: <dev-return-12036-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D1A9217876
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 02:57:16 +0000 (UTC)
Received: (qmail 82417 invoked by uid 500); 17 Mar 2015 02:57:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82342 invoked by uid 500); 17 Mar 2015 02:57:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82330 invoked by uid 99); 17 Mar 2015 02:57:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 02:57:15 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of turp1twin@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 02:57:10 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id B78A9178140B
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 19:57:00 -0700 (PDT)
Date: Mon, 16 Mar 2015 19:56:50 -0700 (MST)
From: turp1twin <turp1twin@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426561010013-11089.post@n3.nabble.com>
In-Reply-To: <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com>
References: <1425687609663-10934.post@n3.nabble.com> <CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com> <CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com> <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Patrick,

Sorry for the delay, I was at Elastic{ON} last week and well, my day job has
been keeping me busy... I went ahead and opened a Jira feature request,
https://issues.apache.org/jira/browse/SPARK-6373. In it I reference a commit
I made in my fork which is a "rough" implementation, definitely still a WIP.
Would like to iterate the design if possible, as there are some performance
trade offs for using SSL for sure.. Zero copy will not be possible with SSL,
so there will definitely be a hit there.. That being said, for my use case,
which is health care related and involves processing personal health
information, I have no choice, as all data must be encrypted in transit and
at rest... Cheers!

Jeff

https://github.com/turp1twin/spark/commit/024b559f27945eb63068d1badf7f82e4e7c3621c




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934p11089.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 03:28:36 2015
Return-Path: <dev-return-12037-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F25917947
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 03:28:36 +0000 (UTC)
Received: (qmail 15694 invoked by uid 500); 17 Mar 2015 03:28:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15610 invoked by uid 500); 17 Mar 2015 03:28:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15595 invoked by uid 99); 17 Mar 2015 03:28:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 03:28:34 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 03:28:09 +0000
Received: by wixw10 with SMTP id w10so40066798wix.0
        for <dev@spark.apache.org>; Mon, 16 Mar 2015 20:27:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=1VmKynvMCN1A+E07TJ9TsRz8bL9SsEuudEFTc9MHhls=;
        b=Q6L9ktxgJqzV0Tegk/4vrYvpfYAa9oRrGtIHUaunuBoNwqRJcFl8/I4COeotjkRd83
         CzBcHEJ/WkhfG4Au47IB8TTxDVHMndGlzbyrM6KeJm41PsA52WpDx0bbGkqFAQKx/0sV
         bmCBFZbc23Kj4S9a9Sb4yeAs2tZNBJtxcAyzFcvX6/A8QLr2OXCurgRHLZncSrnl+4Zf
         Z4Fovq4R3xJalPGULN6/7ATOS3miWRMnez4sIZ1UriC/+LCS/ydPSislPfp9/GF2kwI6
         85tPyVv3mjXBTsik6fuwNqLzMpkLt3Dx4FLv27S/JuDUQ6w2cnaPimyi7fkfNegjOpQB
         X1jQ==
X-Received: by 10.180.126.69 with SMTP id mw5mr173760678wib.12.1426562843407;
 Mon, 16 Mar 2015 20:27:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Mon, 16 Mar 2015 20:27:03 -0700 (PDT)
In-Reply-To: <1426561010013-11089.post@n3.nabble.com>
References: <1425687609663-10934.post@n3.nabble.com> <CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com>
 <CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com>
 <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com> <1426561010013-11089.post@n3.nabble.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 16 Mar 2015 20:27:03 -0700
Message-ID: <CANGvG8qDAiOfskWykg_d1VKBioPrhep6dfU83irWuqgUwDUMTQ@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
To: turp1twin <turp1twin@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=e89a8f839f1b87fdef0511738b0e
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f839f1b87fdef0511738b0e
Content-Type: text/plain; charset=UTF-8

Out of curiosity, why could we not use Netty's SslHandler injected into the
TransportContext pipeline?

On Mon, Mar 16, 2015 at 7:56 PM, turp1twin <turp1twin@gmail.com> wrote:

> Hey Patrick,
>
> Sorry for the delay, I was at Elastic{ON} last week and well, my day job
> has
> been keeping me busy... I went ahead and opened a Jira feature request,
> https://issues.apache.org/jira/browse/SPARK-6373. In it I reference a
> commit
> I made in my fork which is a "rough" implementation, definitely still a
> WIP.
> Would like to iterate the design if possible, as there are some performance
> trade offs for using SSL for sure.. Zero copy will not be possible with
> SSL,
> so there will definitely be a hit there.. That being said, for my use case,
> which is health care related and involves processing personal health
> information, I have no choice, as all data must be encrypted in transit and
> at rest... Cheers!
>
> Jeff
>
>
> https://github.com/turp1twin/spark/commit/024b559f27945eb63068d1badf7f82e4e7c3621c
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934p11089.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--e89a8f839f1b87fdef0511738b0e--

From dev-return-12038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 03:38:28 2015
Return-Path: <dev-return-12038-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1C3717990
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 03:38:28 +0000 (UTC)
Received: (qmail 30765 invoked by uid 500); 17 Mar 2015 03:38:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30677 invoked by uid 500); 17 Mar 2015 03:38:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30665 invoked by uid 99); 17 Mar 2015 03:38:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 03:38:27 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of turp1twin@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 03:38:01 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 8E43D1782B92
	for <dev@spark.apache.org>; Mon, 16 Mar 2015 20:37:40 -0700 (PDT)
Date: Mon, 16 Mar 2015 20:37:29 -0700 (MST)
From: turp1twin <turp1twin@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426563449859-11091.post@n3.nabble.com>
In-Reply-To: <CANGvG8qDAiOfskWykg_d1VKBioPrhep6dfU83irWuqgUwDUMTQ@mail.gmail.com>
References: <1425687609663-10934.post@n3.nabble.com> <CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com> <CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com> <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com> <1426561010013-11089.post@n3.nabble.com> <CANGvG8qDAiOfskWykg_d1VKBioPrhep6dfU83irWuqgUwDUMTQ@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Aaron,

That is what I do, except I add the Netty SslHandler in the TransportServer
and the TransportClientFactory.... I do this because the Server pipeline is
a bit different as I have to add a Netty ChunkedWriteHandler... Again, this
is a "rough" prototype, just to get something working... Cheers!

Jeff




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934p11091.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 17:56:27 2015
Return-Path: <dev-return-12039-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 568C9176B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 17:56:27 +0000 (UTC)
Received: (qmail 9663 invoked by uid 500); 17 Mar 2015 17:56:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9591 invoked by uid 500); 17 Mar 2015 17:56:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9572 invoked by uid 99); 17 Mar 2015 17:56:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 17:56:25 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.146 as permitted sender)
Received: from [169.229.218.146] (HELO cm05fe.IST.Berkeley.EDU) (169.229.218.146)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 17:55:58 +0000
Received: from mail-yk0-f171.google.com ([209.85.160.171])
	by cm05fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1YXvfY-0001SM-IM
	for dev@spark.apache.org; Tue, 17 Mar 2015 10:52:46 -0700
Received: by ykcn8 with SMTP id n8so6840748ykc.3
        for <dev@spark.apache.org>; Tue, 17 Mar 2015 10:52:43 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.170.100.87 with SMTP id r84mr77051003yka.73.1426614763831;
 Tue, 17 Mar 2015 10:52:43 -0700 (PDT)
Received: by 10.170.198.142 with HTTP; Tue, 17 Mar 2015 10:52:43 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1961B@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE1961B@G4W3292.americas.hpqcorp.net>
Date: Tue, 17 Mar 2015 10:52:43 -0700
Message-ID: <CAKJXNjGE5WfKEBH-kZRAUhXO4SNd994dYbwY-Fwdtte0=ii0fw@mail.gmail.com>
Subject: Re: Profiling Spark: MemoryStore
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113b29123ac88605117fa2fd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b29123ac88605117fa2fd
Content-Type: text/plain; charset=UTF-8

Hi Alexander,

The stack trace is a little misleading here: all of the time is spent in
MemoryStore, but that's because MemoryStore is unrolling an iterator (note
the iterator.next()) call so that it can be stored in-memory.  Essentially
all of the computation for the tasks happens as part of that
iterator.next() call, which is why you're seeing a combination of
deserializing input data with Snappy (the InputStream reading) and some
MLLib processing.

-Kay

On Thu, Mar 12, 2015 at 5:34 PM, Ulanov, Alexander <alexander.ulanov@hp.com>
wrote:

> Hi,
>
> I am working on artificial neural networks for Spark. It is solved with
> Gradient Descent, so each step the data is read, sum of gradients is
> calculated for each data partition (on each worker), aggregated (on the
> driver) and broadcasted back. I noticed that the gradient computation time
> is few times less than the total time needed for each step. To narrow down
> my observation, I run the gradient on a single machine with single
> partition of data of site 100MB that I persist (data.persist). This should
> minimize the overhead for aggregation at least, but the gradient
> computation still takes much less time than the whole step. Just in case,
> data is loaded by MLUtil. loadLibSVMFile in RDD[LabeledPoint], this is my
> code:
>
>     val conf = new SparkConf().setAppName("myApp").setMaster("local[2]")
>     val train = MLUtils.loadLibSVMFile(new SparkContext(conf),
> "/data/mnist/mnist.scale").repartition(1).persist()
>     val model = ANN2Classifier.train(train, 1000, Array[Int](32), 10,
> 1e-4) //training data, batch size, hidden layer size, iterations, LBFGS
> tolerance
>
> Profiler shows that there are two threads, one is doing Gradient and the
> other I don't know what. The Gradient takes 10% of this thread. Almost all
> other time is spent by MemoryStore. Below is the screenshot (first thread):
>
> https://drive.google.com/file/d/0BzYMzvDiCep5bGp2S2F6eE9TRlk/view?usp=sharing
> Second thread:
>
> https://drive.google.com/file/d/0BzYMzvDiCep5OHA0WUtQbXd3WmM/view?usp=sharing
>
> Could Spark developers please elaborate what's going on in MemoryStore? It
> seems that it does some string operations (parsing libsvm file? Why every
> step?) and a lot of InputStream reading. It seems that the overall time
> depends on the size of the data batch (or size of vector) I am processing.
> However it does not seems linear to me.
>
> Also, I would like to know how to speedup these operations.
>
> Best regards, Alexander
>
>

--001a113b29123ac88605117fa2fd--

From dev-return-12040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 18:57:19 2015
Return-Path: <dev-return-12040-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F2756179CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 18:57:18 +0000 (UTC)
Received: (qmail 9505 invoked by uid 500); 17 Mar 2015 18:57:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9425 invoked by uid 500); 17 Mar 2015 18:57:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9413 invoked by uid 99); 17 Mar 2015 18:57:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 18:57:17 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 18:57:12 +0000
Received: by igbue6 with SMTP id ue6so80237909igb.1
        for <dev@spark.apache.org>; Tue, 17 Mar 2015 11:56:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=MG1CvQnSWcJWpPAt6WsVF5GhftU2W2yYyb9abLcw37k=;
        b=K0TQ6ADgLr0E4G+hxrWxodZSF5AiRrRw6ae1pTCGJes8I0nS1uXAf+SQxaNq/r2v0M
         DjBWSaFlKJ7wZ2ecf7v7fKnu9FJ9qrYRCknytVIEuhiL2PJFy96aSSaOjKFOARZ5GpIZ
         07NjObSbfXuTZqjCIq+2/sVOZGckrnurNsarHP+bWb0ujlqevWqlJnfEeytG+y2Rma9h
         uwSkkC09tbvwCgaXfM6O85x1FgsEkrQvHmV+R57aL0U3kGSEjtrzZEf60O+Tg3pqX1iF
         O0F5pS8kM7Q67wwIeWDYooYgssBmWyI3cxcdSVEiJB5wdm01jsKlEWGHp5TUsllxBQ1h
         X1QQ==
MIME-Version: 1.0
X-Received: by 10.107.29.21 with SMTP id d21mr56789188iod.11.1426618612225;
 Tue, 17 Mar 2015 11:56:52 -0700 (PDT)
Received: by 10.107.152.130 with HTTP; Tue, 17 Mar 2015 11:56:52 -0700 (PDT)
In-Reply-To: <CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
References: <1426405522047-11056.post@n3.nabble.com>
	<1426407287506-11058.post@n3.nabble.com>
	<CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
Date: Tue, 17 Mar 2015 11:56:52 -0700
Message-ID: <CALW2ey07WDmG36xpenAzJbrWQc0DsuBMTNsR6zYx1xdE+cApbg@mail.gmail.com>
Subject: Re: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
From: David Hall <david.lw.hall@gmail.com>
To: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
Cc: "<dev@spark.apache.org>" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1140a4fa9c9df105118087bb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1140a4fa9c9df105118087bb
Content-Type: text/plain; charset=UTF-8

ping?

On Sun, Mar 15, 2015 at 9:38 PM, David Hall <david.lw.hall@gmail.com> wrote:

> snapshot is pushed. If you verify I'll publish the new artifacts.
>
> On Sun, Mar 15, 2015 at 1:14 AM, Yu Ishikawa <yuu.ishikawa+spark@gmail.com
> > wrote:
>
>> David Hall who is a breeze creator told me that it's a bug. So, I made a
>> jira
>> ticket about this issue. We need to upgrade breeze from 0.11.1 to 0.11.2
>> or
>> later in order to fix the bug, when the new version of breeze will be
>> released.
>>
>> [SPARK-6341] Upgrade breeze from 0.11.1 to 0.11.2 or later - ASF JIRA
>> https://issues.apache.org/jira/browse/SPARK-6341
>>
>> Thanks,
>> Yu Ishikawa
>>
>>
>>
>> -----
>> -- Yu Ishikawa
>> --
>> View this message in context:
>> http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056p11058.html
>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--001a1140a4fa9c9df105118087bb--

From dev-return-12041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 17 19:09:15 2015
Return-Path: <dev-return-12041-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C06D317A77
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 17 Mar 2015 19:09:15 +0000 (UTC)
Received: (qmail 46192 invoked by uid 500); 17 Mar 2015 19:09:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46113 invoked by uid 500); 17 Mar 2015 19:09:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46089 invoked by uid 99); 17 Mar 2015 19:09:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 19:09:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.173 as permitted sender)
Received: from [209.85.213.173] (HELO mail-ig0-f173.google.com) (209.85.213.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 17 Mar 2015 19:08:44 +0000
Received: by igbue6 with SMTP id ue6so22142960igb.1
        for <dev@spark.apache.org>; Tue, 17 Mar 2015 12:07:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=lDTTH7lzIAyvYrVDLxcU6YGeHMOVAfeG1uG067sPy/4=;
        b=ida5Yo5yf0lXsu6g6xn+a8NM4yPk/BNcbIgBkJIuxrgxyvBZdVew/xQ9ePk1HUdQWk
         QrEJjyHn3yyqMcdh2mVv1edX56gnifAbMao30RIxpNT33Ep8cmy9z5b7rT6PQQKSxf1a
         //Bcs6Ky3cCNrc/YQ+OchDLNmX2MswpiXPGuiSb5eJX8g6jUOA2xggiIT18ARmVlpkMT
         BhbsqFDcd6X0qU6UOkBh+MYmIlGrM8GaeNby5fjvgfnoc5Y+75IPI8sYtJnfiAGzJxaI
         VUvlLAY5ZqOYcjiZVSNnFbperRNpat21jH+bBkb897NKXSyky88OYa01+sp31fQgqxqc
         7NMQ==
MIME-Version: 1.0
X-Received: by 10.42.167.8 with SMTP id q8mr89118209icy.94.1426619232858; Tue,
 17 Mar 2015 12:07:12 -0700 (PDT)
Received: by 10.36.69.8 with HTTP; Tue, 17 Mar 2015 12:07:12 -0700 (PDT)
In-Reply-To: <CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
	<CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
	<55072B1A.7000907@oracle.com>
	<CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
Date: Tue, 17 Mar 2015 12:07:12 -0700
Message-ID: <CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: Aaron Davidson <ilikerps@gmail.com>
Cc: Kevin Markey <kevin.markey@oracle.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Let me put a quick summary. #4 got majority vote with CamelCase but
not UPPERCASE. The following is a minimal implementation that works
for both Scala and Java. In Python, we use string for enums. This
proposal is only for new public APIs. We are not going to change
existing ones. -Xiangrui

~~~
sealed abstract class StorageLevel

object StorageLevel {

  def fromString(name: String): StorageLevel = ???

  val MemoryOnly: StorageLevel = {
    case object MemoryOnly extends StorageLevel
    MemoryOnly
  }

  val DiskOnly: StorageLevel = {
    case object DiskOnly extends StorageLevel
    DiskOnly
 }
}
~~~

On Mon, Mar 16, 2015 at 3:04 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> It's unrelated to the proposal, but Enum#ordinal() should be much faster,
> assuming it's not serialized to JVMs with different versions of the enum :)
>
> On Mon, Mar 16, 2015 at 12:12 PM, Kevin Markey <kevin.markey@oracle.com>
> wrote:
>
>> In some applications, I have rather heavy use of Java enums which are
>> needed for related Java APIs that the application uses.  And unfortunately,
>> they are also used as keys.  As such, using the native hashcodes makes any
>> function over keys unstable and unpredictable, so we now use Enum.name() as
>> the key instead.  Oh well.  But it works and seems to work well.
>>
>> Kevin
>>
>>
>> On 03/05/2015 09:49 PM, Mridul Muralidharan wrote:
>>
>>>    I have a strong dislike for java enum's due to the fact that they
>>> are not stable across JVM's - if it undergoes serde, you end up with
>>> unpredictable results at times [1].
>>> One of the reasons why we prevent enum's from being key : though it is
>>> highly possible users might depend on it internally and shoot
>>> themselves in the foot.
>>>
>>> Would be better to keep away from them in general and use something more
>>> stable.
>>>
>>> Regards,
>>> Mridul
>>>
>>> [1] Having had to debug this issue for 2 weeks - I really really hate it.
>>>
>>>
>>> On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com>
>>> wrote:
>>>
>>>> I have a very strong dislike for #1 (scala enumerations).   I'm ok with
>>>> #4
>>>> (with Xiangrui's final suggestion, especially making it sealed &
>>>> available
>>>> in Java), but I really think #2, java enums, are the best option.
>>>>
>>>> Java enums actually have some very real advantages over the other
>>>> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.  There
>>>> has
>>>> been endless debate in the Scala community about the problems with the
>>>> approaches in Scala.  Very smart, level-headed Scala gurus have
>>>> complained
>>>> about their short-comings (Rex Kerr's name is coming to mind, though I'm
>>>> not positive about that); there have been numerous well-thought out
>>>> proposals to give Scala a better enum.  But the powers-that-be in Scala
>>>> always reject them.  IIRC the explanation for rejecting is basically that
>>>> (a) enums aren't important enough for introducing some new special
>>>> feature,
>>>> scala's got bigger things to work on and (b) if you really need a good
>>>> enum, just use java's enum.
>>>>
>>>> I doubt it really matters that much for Spark internals, which is why I
>>>> think #4 is fine.  But I figured I'd give my spiel, because every
>>>> developer
>>>> loves language wars :)
>>>>
>>>> Imran
>>>>
>>>>
>>>>
>>>> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>>
>>>>  `case object` inside an `object` doesn't show up in Java. This is the
>>>>> minimal code I found to make everything show up correctly in both
>>>>> Scala and Java:
>>>>>
>>>>> sealed abstract class StorageLevel // cannot be a trait
>>>>>
>>>>> object StorageLevel {
>>>>>    private[this] case object _MemoryOnly extends StorageLevel
>>>>>    final val MemoryOnly: StorageLevel = _MemoryOnly
>>>>>
>>>>>    private[this] case object _DiskOnly extends StorageLevel
>>>>>    final val DiskOnly: StorageLevel = _DiskOnly
>>>>> }
>>>>>
>>>>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
>>>>> wrote:
>>>>>
>>>>>> I like #4 as well and agree with Aaron's suggestion.
>>>>>>
>>>>>> - Patrick
>>>>>>
>>>>>> On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
>>>>>>
>>>>> wrote:
>>>>>
>>>>>> I'm cool with #4 as well, but make sure we dictate that the values
>>>>>>>
>>>>>> should
>>>>>
>>>>>> be defined within an object with the same name as the enumeration (like
>>>>>>>
>>>>>> we
>>>>>
>>>>>> do for StorageLevel). Otherwise we may pollute a higher namespace.
>>>>>>>
>>>>>>> e.g. we SHOULD do:
>>>>>>>
>>>>>>> trait StorageLevel
>>>>>>> object StorageLevel {
>>>>>>>    case object MemoryOnly extends StorageLevel
>>>>>>>    case object DiskOnly extends StorageLevel
>>>>>>> }
>>>>>>>
>>>>>>> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
>>>>>>>
>>>>>> michael@databricks.com>
>>>>>
>>>>>> wrote:
>>>>>>>
>>>>>>>  #4 with a preference for CamelCaseEnums
>>>>>>>>
>>>>>>>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
>>>>>>>> joseph@databricks.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>  another vote for #4
>>>>>>>>> People are already used to adding "()" in Java.
>>>>>>>>>
>>>>>>>>>
>>>>>>>>> On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <javadba@gmail.com>
>>>>>>>>>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> #4 but with MemoryOnly (more scala-like)
>>>>>>>>>>
>>>>>>>>>> http://docs.scala-lang.org/style/naming-conventions.html
>>>>>>>>>>
>>>>>>>>>> Constants, Values, Variable and Methods
>>>>>>>>>>
>>>>>>>>>> Constant names should be in upper camel case. That is, if the
>>>>>>>>>>
>>>>>>>>> member is
>>>>>
>>>>>> final, immutable and it belongs to a package object or an object,
>>>>>>>>>>
>>>>>>>>> it
>>>>>
>>>>>> may
>>>>>>>>
>>>>>>>>> be
>>>>>>>>>
>>>>>>>>>> considered a constant (similar to Java'sstatic final members):
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>>     1. object Container {
>>>>>>>>>>     2.     val MyConstant = ...
>>>>>>>>>>     3. }
>>>>>>>>>>
>>>>>>>>>>
>>>>>>>>>> 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
>>>>>>>>>>
>>>>>>>>>>  Hi all,
>>>>>>>>>>>
>>>>>>>>>>> There are many places where we use enum-like types in Spark, but
>>>>>>>>>>>
>>>>>>>>>> in
>>>>>
>>>>>> different ways. Every approach has both pros and cons. I wonder
>>>>>>>>>>> whether there should be an "official" approach for enum-like
>>>>>>>>>>>
>>>>>>>>>> types in
>>>>>
>>>>>> Spark.
>>>>>>>>>>>
>>>>>>>>>>> 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
>>>>>>>>>>>
>>>>>>>>>>> * All types show up as Enumeration.Value in Java.
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>  http://spark.apache.org/docs/latest/api/java/org/apache/
>>>>> spark/scheduler/SchedulingMode.html
>>>>>
>>>>>> 2. Java's Enum (e.g., SaveMode, IOMode)
>>>>>>>>>>>
>>>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>>>> * Values doesn't show up in the ScalaDoc:
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
>>>>> spark.network.util.IOMode
>>>>>
>>>>>> 3. Static fields in Java (e.g., TripletFields)
>>>>>>>>>>>
>>>>>>>>>>> * Implementation must be in a Java file.
>>>>>>>>>>> * Doesn't need "()" in Java code.
>>>>>>>>>>> * Values don't show up in the ScalaDoc:
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
>>>>> spark.graphx.TripletFields
>>>>>
>>>>>> 4. Objects in Scala. (e.g., StorageLevel)
>>>>>>>>>>>
>>>>>>>>>>> * Needs "()" in Java code.
>>>>>>>>>>> * Values show up in both ScalaDoc and JavaDoc:
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
>>>>> spark.storage.StorageLevel$
>>>>>
>>>>>>
>>>>>>>>>>>  http://spark.apache.org/docs/latest/api/java/org/apache/
>>>>> spark/storage/StorageLevel.html
>>>>>
>>>>>> It would be great if we have an "official" approach for this as
>>>>>>>>>>>
>>>>>>>>>> well
>>>>>
>>>>>> as the naming convention for enum-like values ("MEMORY_ONLY" or
>>>>>>>>>>> "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
>>>>>>>>>>>
>>>>>>>>>> thoughts?
>>>>>>>>
>>>>>>>>> Best,
>>>>>>>>>>> Xiangrui
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>  ------------------------------------------------------------
>>>>> ---------
>>>>>
>>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>>>>>>>
>>>>>>>>>>>
>>>>>>>>>>>  ------------------------------------------------------------
>>>>> ---------
>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>>>
>>>>>
>>>>>  ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 00:18:50 2015
Return-Path: <dev-return-12042-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 857DC10C5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 00:18:50 +0000 (UTC)
Received: (qmail 36627 invoked by uid 500); 18 Mar 2015 00:18:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36502 invoked by uid 500); 18 Mar 2015 00:18:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35684 invoked by uid 99); 18 Mar 2015 00:18:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 00:18:46 +0000
X-ASF-Spam-Status: No, hits=-10.3 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS,USER_IN_DEF_SPF_WL
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of prvs=512bd2b8a=jonathak@amazon.com designates 207.171.189.228 as permitted sender)
Received: from [207.171.189.228] (HELO smtp-fw-33001.amazon.com) (207.171.189.228)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 00:18:21 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
  d=amazon.com; i=@amazon.com; q=dns/txt; s=amazon201209;
  t=1426637919; x=1458173919;
  h=from:to:subject:date:message-id:mime-version;
  bh=0mRxyv8sk338ez1OVlICH+CRgBI+G1w1D4ILGfufezc=;
  b=JK76Dit32IYSyU0u9jLtCYRfqjqCaw7K0tg6nNYdKOkYXgvvvWrPUqLH
   7Cxuh/MavQI7ol76dwfqft517QxFoNdfcdp4EpkXgR1afv776qxhUc4IZ
   eV6+KtqpRLGMrUEOGK02pRoT+KsVGm5wOk1Yrvwejrhvw1i+uI1c1//tx
   k=;
X-IronPort-AV: E=Sophos;i="5.11,418,1422921600"; 
   d="scan'208,217";a="208201279"
Received: from email-inbound-relay-62002.pdx2.amazon.com ([10.241.21.79])
  by smtp-border-fw-out-33001.sea14.amazon.com with ESMTP/TLS/DHE-RSA-AES256-SHA; 18 Mar 2015 00:15:18 +0000
Received: from ex10-hub-31002.ant.amazon.com (pdx2-ws-svc-lb17-vlan3.amazon.com [10.247.140.70])
	by email-inbound-relay-62002.pdx2.amazon.com (8.14.7/8.14.7) with ESMTP id t2I0FIlE018789
	(version=TLSv1/SSLv3 cipher=AES256-SHA bits=256 verify=FAIL);
	Wed, 18 Mar 2015 00:15:18 GMT
Received: from EX10-MBX-31002.ant.amazon.com ([fe80::b9:949c:242a:99ad]) by
 ex10-hub-31002.ant.amazon.com ([::1]) with mapi id 14.03.0181.006; Tue, 17
 Mar 2015 17:15:16 -0700
From: "Kelly, Jonathan" <jonathak@amazon.com>
To: "user@spark.apache.org" <user@spark.apache.org>,
        "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: Using Spark with a SOCKS proxy
Thread-Topic: Using Spark with a SOCKS proxy
Thread-Index: AQHQYRCcMod7O80DW0KYxh2yY4TVvg==
Date: Wed, 18 Mar 2015 00:15:16 +0000
Message-ID: <D12E11A3.AE7A3%jonathak@amazon.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.8.150116
x-originating-ip: [10.184.49.66]
Content-Type: multipart/alternative;
	boundary="_000_D12E11A3AE7A3jonathakamazoncom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D12E11A3AE7A3jonathakamazoncom_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

I'm trying to figure out how I might be able to use Spark with a SOCKS prox=
y.  That is, my dream is to be able to write code in my IDE then run it wit=
hout much trouble on a remote cluster, accessible only via a SOCKS proxy be=
tween the local development machine and the master node of the cluster (ign=
oring, for now, any dependencies that would need to be transferred--assume =
it's a very simple app with no dependencies that aren't part of the Spark c=
lasspath on the cluster).  This is possible with Hadoop by setting hadoop.r=
pc.socket.factory.class.default to org.apache.hadoop.net.SocksSocketFactory=
 and hadoop.socks.server to localhost:<port on which a SOCKS proxy has been=
 opened via "ssh -D" to the master node>.  However, I can't seem to find an=
ything like this for Spark, and I only see very few mentions of it on the u=
ser list and on stackoverflow, with no real answers.  (See links below.)

I thought I might be able to use the JVM's -DsocksProxyHost and -DsocksProx=
yPort system properties, but it still does not seem to work.  That is, if I=
 start a SOCKS proxy to my master node using something like "ssh -D 2600 <m=
aster node public name>" then run a simple Spark app that calls SparkConf.s=
etMaster("spark://<master node private IP>:7077"), passing in JVM args of "=
-DsocksProxyHost=3Dlocahost -DsocksProxyPort=3D2600", the driver hangs for =
a while before finally giving up ("Application has been killed. Reason: All=
 masters are unresponsive! Giving up.").  It seems like it is not even atte=
mpting to use the SOCKS proxy.  Do -DsocksProxyHost/-DsocksProxyPort not ev=
en work for Spark?

http://stackoverflow.com/questions/28047000/connect-to-spark-through-a-sock=
s-proxy (unanswered similar question from somebody else about a month ago)
https://issues.apache.org/jira/browse/SPARK-5004 (unresolved, somewhat rela=
ted JIRA from a few months ago)

Thanks,
Jonathan

--_000_D12E11A3AE7A3jonathakamazoncom_--

From dev-return-12043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 02:08:20 2015
Return-Path: <dev-return-12043-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BFA6E17266
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 02:08:20 +0000 (UTC)
Received: (qmail 57632 invoked by uid 500); 18 Mar 2015 02:08:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57579 invoked by uid 500); 18 Mar 2015 02:08:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57567 invoked by uid 99); 18 Mar 2015 02:08:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 02:08:18 +0000
X-ASF-Spam-Status: No, hits=2.6 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FROM_EXCESS_BASE64,HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of 261810726@qq.com designates 184.105.206.84 as permitted sender)
Received: from [184.105.206.84] (HELO smtpproxy19.qq.com) (184.105.206.84)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 02:07:53 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1426644426; bh=AAfRMOe0iavKlDHLGEttmLnUVrf0kV0Ho/iv0o7tmF0=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-SENDSIZE;
	b=Vg2C81Jw28TFxDxAIaRe/clfYEa5ZOfRrhQv4omyDlhBv6Pz3qfQQLbsvo3b8pBNY
	 uk/KWbHsNhPeuWL9r+//wpDZdur/f3Uhdy09Qog7mWBxd9Asdw3DVXcPHKqZ3ls1hF
	 qJqQMGi9owBEOkOG689FBshl3p3dE9oz1C7OXBDk=
X-QQ-FEAT: 6WrCsiFB1WFCqAMVBNf9jmK9Xa7p94gKMe2GcLDTOBBBujNzA+mCH7hSgewn6
	zsJ3JzINzkimaNrzJwbEBhVELw+gItlxMoHAN9VG8hy0lgrir/DbgX31SJcU/klyW3hl42Y
	JsfKXxm/XKSpWs7NQ//thBw+Qg9LC/Er7uEpYSm7VjhTHkrxD1UGwwPK0KjGK7Ymx7Z0cdj
	cN+lt8jr0w1tWCgfGkBRk
X-QQ-SSF: 00000000000000F000000000000000Z
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 111.207.123.132
X-QQ-STYLE: 
X-QQ-mid: webmail249t1426644425t179953
From: "=?utf-8?B?U2Vh?=" <261810726@qq.com>
To: "=?utf-8?B?ZGV2?=" <dev@spark.apache.org>, "=?utf-8?B?dXNlcg==?=" <user@spark.apache.org>
Subject: InvalidAuxServiceException in dynamicAllocation
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_5508DDC9_0A0F9D40_5CC162D7"
Content-Transfer-Encoding: 8Bit
Date: Wed, 18 Mar 2015 10:07:05 +0800
X-Priority: 3
Message-ID: <tencent_3069740E2CD5D9E6117DA928@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_5508DDC9_0A0F9D40_5CC162D7
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64

SGksIGFsbDoNCg0KDQpTcGFyazEuMy4wIGhhZG9vcDIuMi4wDQoNCg0KSSBwdXQgdGhlIGZv
bGxvd2luZyBwYXJhbXMgaW4gdGhlIHNwYXJrLWRlZmF1bHRzLmNvbmYgDQoNCg0Kc3Bhcmsu
ZHluYW1pY0FsbG9jYXRpb24uZW5hYmxlZCB0cnVlDQpzcGFyay5keW5hbWljQWxsb2NhdGlv
bi5taW5FeGVjdXRvcnMgMjANCnNwYXJrLmR5bmFtaWNBbGxvY2F0aW9uLm1heEV4ZWN1dG9y
cyAzMDANCnNwYXJrLmR5bmFtaWNBbGxvY2F0aW9uLmV4ZWN1dG9ySWRsZVRpbWVvdXQgMzAw
DQpzcGFyay5zaHVmZmxlLnNlcnZpY2UuZW5hYmxlZCB0cnVl4oCNDQoNCg0KDQpJIHN0YXJ0
ZWQgdGhlIHRocmlmdHNlcnZlciBhbmQgZG8gYSBxdWVyeS4gRXhjZXB0aW9uIGhhcHBlbmVk
IQ0KSSBmaW5kIGl0IGluIEpJUkEgaHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jy
b3dzZS9TUEFSSy01NzU54oCNIA0KSXQgc2F5cyBmaXhlZCB2ZXJzaW9uIDEuMy4wDQoNCg0K
Q2F1c2VkIGJ5OiBvcmcuYXBhY2hlLmhhZG9vcC55YXJuLmV4Y2VwdGlvbnMuSW52YWxpZEF1
eFNlcnZpY2VFeGNlcHRpb246IFRoZSBhdXhTZXJ2aWNlOnNwYXJrX3NodWZmbGUgZG9lcyBu
b3QgZXhpc3QgCWF0IHN1bi5yZWZsZWN0LkdlbmVyYXRlZENvbnN0cnVjdG9yQWNjZXNzb3Iy
OC5uZXdJbnN0YW5jZShVbmtub3duIFNvdXJjZSkgCWF0IHN1bi5yZWZsZWN0LkRlbGVnYXRp
bmdDb25zdHJ1Y3RvckFjY2Vzc29ySW1wbC5uZXdJbnN0YW5jZShEZWxlZ2F0aW5nQ29uc3Ry
dWN0b3JBY2Nlc3NvckltcGwuamF2YToyNykgCWF0IGphdmEubGFuZy5yZWZsZWN0LkNvbnN0
cnVjdG9yLm5ld0luc3RhbmNlKENvbnN0cnVjdG9yLmphdmE6NTEzKSAJYXQgb3JnLmFwYWNo
ZS5oYWRvb3AueWFybi5hcGkucmVjb3Jkcy5pbXBsLnBiLlNlcmlhbGl6ZWRFeGNlcHRpb25Q
QkltcGwuaW5zdGFudGlhdGVFeGNlcHRpb24oU2VyaWFsaXplZEV4Y2VwdGlvblBCSW1wbC5q
YXZhOjE1MikgCWF0IG9yZy5hcGFjaGUuaGFkb29wLnlhcm4uYXBpLnJlY29yZHMuaW1wbC5w
Yi5TZXJpYWxpemVkRXhjZXB0aW9uUEJJbXBsLmRlU2VyaWFsaXplKFNlcmlhbGl6ZWRFeGNl
cHRpb25QQkltcGwuamF2YToxMDYpIAlhdCBvcmcuYXBhY2hlLmhhZG9vcC55YXJuLmNsaWVu
dC5hcGkuaW1wbC5OTUNsaWVudEltcGwuc3RhcnRDb250YWluZXIoTk1DbGllbnRJbXBsLmph
dmE6MjAzKSAJYXQgb3JnLmFwYWNoZS5zcGFyay5kZXBsb3kueWFybi5FeGVjdXRvclJ1bm5h
YmxlLnN0YXJ0Q29udGFpbmVyKEV4ZWN1dG9yUnVubmFibGUuc2NhbGE6MTEzKSAJLi4uIDQg
bW9yZeKAjQ==

------=_NextPart_5508DDC9_0A0F9D40_5CC162D7--




From dev-return-12044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 02:16:55 2015
Return-Path: <dev-return-12044-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A975172A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 02:16:55 +0000 (UTC)
Received: (qmail 69336 invoked by uid 500); 18 Mar 2015 02:16:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69261 invoked by uid 500); 18 Mar 2015 02:16:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69249 invoked by uid 99); 18 Mar 2015 02:16:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 02:16:53 +0000
X-ASF-Spam-Status: No, hits=2.6 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FROM_EXCESS_BASE64,HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_NONE,SPF_PASS,T_TVD_MIME_EPI
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of 261810726@qq.com designates 54.254.200.128 as permitted sender)
Received: from [54.254.200.128] (HELO smtpbgsg2.qq.com) (54.254.200.128)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 02:16:48 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1426644919; bh=tQk5nDX35sCAzFsbBp5qxSgsN+NcbgbNgO3xKfgA9H8=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=O+cMgwxQMu7lRhIHuyDEnqH9I+MDrf+TLMrjFGGWMA7ll5C4ZwRkBuHzJin25s25j
	 VXMRlkNieknMi9Ju5gmzdjPyNix3m0SlE9me3aMLM2yqCGaPKgdyrb8KPQFSnNlqdw
	 11mdkkNx4WvbFdLeaBU5ITBDrYVyNcV9HuYoI8ZM=
X-QQ-FEAT: j5Y3lWpKjFZUzq38jMsurZYTFh4EBhH1/PEYQILdNhvuon2IyUIx43aDgQZsy
	+iBp1ax/ID3EVJJcOeMSPiZcSGJZJuLOU1K2gP8+C/coyZwDM6onihch1LZMnYnK90hLxtW
	4qESjgifZxY/zKOKDIwoRBU05WB3DHCfQmLtXCuvkmJ4vZiFsvDmyRZx4S2r9FmZ9ecEJTY
	0LcJcGdzQUtbINYh5AdY2
X-QQ-SSF: 00000000000000F000000000000000Z
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 111.207.123.132
In-Reply-To: <tencent_3069740E2CD5D9E6117DA928@qq.com>
References: <tencent_3069740E2CD5D9E6117DA928@qq.com>
X-QQ-STYLE: 
X-QQ-mid: webmail249t1426644918t72122
From: "=?utf-8?B?U2Vh?=" <261810726@qq.com>
To: "=?utf-8?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?=" <dev@spark.apache.org>, "=?utf-8?B?dXNlckBzcGFyay5hcGFjaGUub3Jn?=" <user@spark.apache.org>
Subject: InvalidAuxServiceException in dynamicAllocation
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_5508DFB6_09B6DD50_6ED1EE0A"
Content-Transfer-Encoding: 8Bit
Date: Wed, 18 Mar 2015 10:15:18 +0800
X-Priority: 3
Message-ID: <tencent_3F8F750212936FDC3034ED2C@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 4284101159
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_5508DFB6_09B6DD50_6ED1EE0A
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64

SGksIGFsbDoNCg0KDQpTcGFyazEuMy4wIGhhZG9vcDIuMi4wDQoNCg0KSSBwdXQgdGhlIGZv
bGxvd2luZyBwYXJhbXMgaW4gdGhlIHNwYXJrLWRlZmF1bHRzLmNvbmYgDQoNCg0Kc3Bhcmsu
ZHluYW1pY0FsbG9jYXRpb24uZW5hYmxlZCB0cnVlDQpzcGFyay5keW5hbWljQWxsb2NhdGlv
bi5taW5FeGVjdXRvcnMgMjANCnNwYXJrLmR5bmFtaWNBbGxvY2F0aW9uLm1heEV4ZWN1dG9y
cyAzMDANCnNwYXJrLmR5bmFtaWNBbGxvY2F0aW9uLmV4ZWN1dG9ySWRsZVRpbWVvdXQgMzAw
DQpzcGFyay5zaHVmZmxlLnNlcnZpY2UuZW5hYmxlZCB0cnVl4oCNDQoNCg0KDQpJIHN0YXJ0
ZWQgdGhlIHRocmlmdHNlcnZlciBhbmQgZG8gYSBxdWVyeS4gRXhjZXB0aW9uIGhhcHBlbmVk
IQ0KSSBmaW5kIGl0IGluIEpJUkEgaHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jy
b3dzZS9TUEFSSy01NzU54oCNIA0KSXQgc2F5cyBmaXhlZCB2ZXJzaW9uIDEuMy4wDQoNCg0K
Q2F1c2VkIGJ5OiBvcmcuYXBhY2hlLmhhZG9vcC55YXJuLmV4Y2VwdGlvbnMuSW52YWxpZEF1
eFNlcnZpY2VFeGNlcHRpb246IFRoZSBhdXhTZXJ2aWNlOnNwYXJrX3NodWZmbGUgZG9lcyBu
b3QgZXhpc3QgCWF0IHN1bi5yZWZsZWN0LkdlbmVyYXRlZENvbnN0cnVjdG9yQWNjZXNzb3Iy
OC5uZXdJbnN0YW5jZShVbmtub3duIFNvdXJjZSkgCWF0IHN1bi5yZWZsZWN0LkRlbGVnYXRp
bmdDb25zdHJ1Y3RvckFjY2Vzc29ySW1wbC5uZXdJbnN0YW5jZShEZWxlZ2F0aW5nQ29uc3Ry
dWN0b3JBY2Nlc3NvckltcGwuamF2YToyNykgCWF0IGphdmEubGFuZy5yZWZsZWN0LkNvbnN0
cnVjdG9yLm5ld0luc3RhbmNlKENvbnN0cnVjdG9yLmphdmE6NTEzKSAJYXQgb3JnLmFwYWNo
ZS5oYWRvb3AueWFybi5hcGkucmVjb3Jkcy5pbXBsLnBiLlNlcmlhbGl6ZWRFeGNlcHRpb25Q
QkltcGwuaW5zdGFudGlhdGVFeGNlcHRpb24oU2VyaWFsaXplZEV4Y2VwdGlvblBCSW1wbC5q
YXZhOjE1MikgCWF0IG9yZy5hcGFjaGUuaGFkb29wLnlhcm4uYXBpLnJlY29yZHMuaW1wbC5w
Yi5TZXJpYWxpemVkRXhjZXB0aW9uUEJJbXBsLmRlU2VyaWFsaXplKFNlcmlhbGl6ZWRFeGNl
cHRpb25QQkltcGwuamF2YToxMDYpIAlhdCBvcmcuYXBhY2hlLmhhZG9vcC55YXJuLmNsaWVu
dC5hcGkuaW1wbC5OTUNsaWVudEltcGwuc3RhcnRDb250YWluZXIoTk1DbGllbnRJbXBsLmph
dmE6MjAzKSAJYXQgb3JnLmFwYWNoZS5zcGFyay5kZXBsb3kueWFybi5FeGVjdXRvclJ1bm5h
YmxlLnN0YXJ0Q29udGFpbmVyKEV4ZWN1dG9yUnVubmFibGUuc2NhbGE6MTEzKSAJLi4uIDQg
bW9yZeKAjQ==

------=_NextPart_5508DFB6_09B6DD50_6ED1EE0A--


	

From dev-return-12045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 03:35:33 2015
Return-Path: <dev-return-12045-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1D72A175A9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 03:35:33 +0000 (UTC)
Received: (qmail 14181 invoked by uid 500); 18 Mar 2015 03:35:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14100 invoked by uid 500); 18 Mar 2015 03:35:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14087 invoked by uid 99); 18 Mar 2015 03:35:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 03:35:28 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 03:35:24 +0000
Received: by qgf3 with SMTP id 3so27037986qgf.3
        for <dev@spark.apache.org>; Tue, 17 Mar 2015 20:34:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=FOA3Q5VWU669LngsuBkZRa0SQ6DaYonXhERT6xo8+no=;
        b=ERFq+2VWzXUAyYNkAFRAvbOWkkFp0vQVEx5jGnRUuePyufZmzKOQp5uAtQxSCQ5LCL
         YhNZS/6V2Cpwov/ROHEOEyIM2eLqXdwBy+BYEqN0YRNUNTmQJg/5fiP86IOIpBIZHaJa
         ld8NINdTfPipsZaeVNHe+1nAoCc4yW9npVPVKpmjKYYvcL1leB32rgpBt1D5WNt2ijlg
         W73arDNUrJnyhtteGer6uWd0sTTurKNpDjlvf1aaItMqhueECH1kXi8jAUMP0tP9ZYNt
         GLLOAGOGZ7yCzhUwIJfn73Hr5zLysQ6cBQ+Vq+RtCZpbXe9ioNC0LmEgxi6FfN3ZkD16
         1Y0w==
X-Gm-Message-State: ALoCoQmekVjhVm6rUlVWm9pp6DciaaN6t9lFoXkkFUGMBUkBSSuPjAI6d2QDtfTdwi19jaO+W1RJ
X-Received: by 10.55.21.39 with SMTP id f39mr58471197qkh.44.1426649683269;
 Tue, 17 Mar 2015 20:34:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.109.9 with HTTP; Tue, 17 Mar 2015 20:34:23 -0700 (PDT)
In-Reply-To: <CA+bq_A+uQbYLJrqyzd0SyBVaYGDy=kHFF173_oN+dnsBjqa=Gw@mail.gmail.com>
References: <1426201751505-11017.post@n3.nabble.com> <CAPh_B=Z_P8efOyGvEDmP9_ALQy=Dz334Sby_6fVp2Xn1VXZAPA@mail.gmail.com>
 <CA+bq_A+uQbYLJrqyzd0SyBVaYGDy=kHFF173_oN+dnsBjqa=Gw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 17 Mar 2015 23:34:23 -0400
Message-ID: <CAPh_B=ZHK4wqVQmKSRndzGmodi3e80uCUV+NY+jBxtqhc+KeRg@mail.gmail.com>
Subject: Re: Spilling when not expected
To: Tom Hubregtsen <thubregtsen@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1147eb94972e06051187c308
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1147eb94972e06051187c308
Content-Type: text/plain; charset=UTF-8

Tom - sorry for the delay. If you try OpenJDK (on a smaller heap), do you
see the same problem? Would be great to isolate whether the problem is
related to J9 or not. In either case we should fix it though.

On Fri, Mar 13, 2015 at 9:33 AM, Tom Hubregtsen <thubregtsen@gmail.com>
wrote:

> I use the spark-submit script and the config files in a conf directory. I
> see the memory settings reflected in the stdout, as well as in the webUI.
> (it prints all variables from spark-default.conf, and metions I have 540GB
> free memory available when trying to store a broadcast variable or RDD). I
> also run "ps -aux | grep java | grep th", which show me that I called java
> with "-Xms1000g -Xmx1000g"
>
> I also tested if these numbers are realistic for the J9 JVM. Outside of
> Spark, when setting just the initial heapsize (Xms), it gives an error, but
> if I also define the maximum option with it (Xmx), it seems to us that it
> is accepting it. Also, in IBM's J9 health center, I see it reserve the
> 900g, and use up to 68g.
>
> Thanks,
>
> Tom
>
> On 13 March 2015 at 02:05, Reynold Xin <rxin@databricks.com> wrote:
>
>> How did you run the Spark command? Maybe the memory setting didn't
>> actually apply? How much memory does the web ui say is available?
>>
>> BTW - I don't think any JVM can actually handle 700G heap ... (maybe
>> Zing).
>>
>> On Thu, Mar 12, 2015 at 4:09 PM, Tom Hubregtsen <thubregtsen@gmail.com>
>> wrote:
>>
>>> Hi all,
>>>
>>> I'm running the teraSort benchmark with a relative small input set: 5GB.
>>> During profiling, I can see I am using a total of 68GB. I've got a
>>> terabyte
>>> of memory in my system, and set
>>> spark.executor.memory 900g
>>> spark.driver.memory 900g
>>> I use the default for
>>> spark.shuffle.memoryFraction
>>> spark.storage.memoryFraction
>>> I believe that I now have 0.2*900=180GB for shuffle and 0.6*900=540GB for
>>> storage.
>>>
>>> I noticed a lot of variation in runtime (under the same load), and
>>> tracked
>>> this down to this function in
>>> core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala
>>>   private def spillToPartitionFiles(collection:
>>> SizeTrackingPairCollection[(Int, K), C]): Unit = {
>>>     spillToPartitionFiles(collection.iterator)
>>>   }
>>> In a slow run, it would loop through this function 12000 times, in a fast
>>> run only 700 times, even though the settings in both runs are the same
>>> and
>>> there are no other users on the system. When I look at the function
>>> calling
>>> this (insertAll, also in ExternalSorter), I see that
>>> spillToPartitionFiles
>>> is only called 700 times in both fast and slow runs, meaning that the
>>> function recursively calls itself very often. Because of the function
>>> name,
>>> I assume the system is spilling to disk. As I have sufficient memory, I
>>> assume that I forgot to set a certain memory setting. Anybody any idea
>>> which
>>> other setting I have to set, in order to not spill data in this scenario?
>>>
>>> Thanks,
>>>
>>> Tom
>>>
>>>
>>>
>>> --
>>> View this message in context:
>>> http://apache-spark-developers-list.1001551.n3.nabble.com/Spilling-when-not-expected-tp11017.html
>>> Sent from the Apache Spark Developers List mailing list archive at
>>> Nabble.com.
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>>
>>
>

--001a1147eb94972e06051187c308--

From dev-return-12046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 05:43:03 2015
Return-Path: <dev-return-12046-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DD1EA178FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 05:43:03 +0000 (UTC)
Received: (qmail 95109 invoked by uid 500); 18 Mar 2015 05:43:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95027 invoked by uid 500); 18 Mar 2015 05:43:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95014 invoked by uid 99); 18 Mar 2015 05:43:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 05:43:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of niranda.perera@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 05:42:57 +0000
Received: by oiag65 with SMTP id g65so27761120oia.2
        for <dev@spark.apache.org>; Tue, 17 Mar 2015 22:40:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=7flR+WAwzxxc+vDgyGJUq34UaiL+26sQzOkpVvl7MIA=;
        b=P0DzyLBICj+IOUuR7y44qkHihJsU30k0VvTfFSS/Mej6yBYTlQFjPI9j0quN17SKGb
         AdgG8NdrI66FQl/X+gXLm+5GaTmVxTUF4JeSVJUQ1f4IRC6UJ3+wX0GDXoNlrMKguOsI
         YyjB9yeb3qT5K5FCKuqY5PS9HcTNl9svi00Vg4r0KvJ9Q6/axmF/TZrF7WTg/1WohPw0
         DtiwOrUUxmpAQkDpyRrpoIKrvFSQLxsoovPsqPWqm9MSiTh/TSrkPH8/TRahd0N6VJax
         tn5f1ssAuLlngsIlPFjq8XZwEtkbDTbzcVC8QfjkMkvUbgE/IAmmQYnyrzUiSc8teTUJ
         SvrQ==
MIME-Version: 1.0
X-Received: by 10.60.150.129 with SMTP id ui1mr56267485oeb.10.1426657221625;
 Tue, 17 Mar 2015 22:40:21 -0700 (PDT)
Received: by 10.182.58.69 with HTTP; Tue, 17 Mar 2015 22:40:21 -0700 (PDT)
Date: Wed, 18 Mar 2015 11:10:21 +0530
Message-ID: <CANCoaU7Hj3OC2edR0bNm_rVn-44LO63n2hSq56bjwyCN+81jGQ@mail.gmail.com>
Subject: Fixed worker ports in the spark worker
From: Niranda Perera <niranda.perera@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b4513d8e94ec505118984bf
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4513d8e94ec505118984bf
Content-Type: text/plain; charset=UTF-8

Hi all,

I see that spark server opens up random ports, especially in the workers.

is there any way to fix these ports or give an set of ports for the worker
to choose from?

cheers

-- 
Niranda

--047d7b4513d8e94ec505118984bf--

From dev-return-12047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 07:20:47 2015
Return-Path: <dev-return-12047-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B689817B0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 07:20:47 +0000 (UTC)
Received: (qmail 47744 invoked by uid 500); 18 Mar 2015 07:20:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47670 invoked by uid 500); 18 Mar 2015 07:20:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47658 invoked by uid 99); 18 Mar 2015 07:20:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 07:20:46 +0000
X-ASF-Spam-Status: No, hits=4.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.215.44 as permitted sender)
Received: from [209.85.215.44] (HELO mail-la0-f44.google.com) (209.85.215.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 07:20:41 +0000
Received: by lamx15 with SMTP id x15so28508869lam.3
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 00:19:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=e44gYaIooi9zN7kvz9KQNoIBgtdKXFH+EPwbXNML4pE=;
        b=G612cI1uVn2905rEcP1xBG/jgvVwp6uDSH2dyO2e1xJ/S+Kk879feuoAwb6N4KvypF
         H0Cc7KvQHk7kQrap/sulumJMcbFLvMzt9EtYzRu4Q2k8UGgS0ju/kXNVJaKHYjMkJulo
         dALTkfXVde++kNvg6HdNjH6dn9RMQB71c8F5hIIOQJJqeq59/y3tet0IraLFOqYrZ+c2
         dd4C39K6JgEWIliPCoLWp1S9p7blbWrrOsehXm1DX+8tdjIDvYwLjn1chgrIhJydw8ql
         fNg5DYxx4nUqLECbCvwwqjbYNQwGd7EQEQmHxih3UaxvSQ/kswNm/ROInL1ywjzqf3VC
         4/UA==
MIME-Version: 1.0
X-Received: by 10.112.46.201 with SMTP id x9mr63563373lbm.65.1426663175487;
 Wed, 18 Mar 2015 00:19:35 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Wed, 18 Mar 2015 00:19:35 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Wed, 18 Mar 2015 00:19:35 -0700 (PDT)
In-Reply-To: <CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
References: <1426405522047-11056.post@n3.nabble.com>
	<1426407287506-11058.post@n3.nabble.com>
	<CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
Date: Wed, 18 Mar 2015 00:19:35 -0700
Message-ID: <CA+B-+fz9DxY4s5Kf22_itJGSUjwaVP-fCcUEw6AP1xkF=8RQqg@mail.gmail.com>
Subject: Re: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
From: Debasish Das <debasish.das83@gmail.com>
To: David Hall <david.lw.hall@gmail.com>
Cc: dev <dev@spark.apache.org>, Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
Content-Type: multipart/alternative; boundary=001a1134586aca068405118ae7c0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134586aca068405118ae7c0
Content-Type: text/plain; charset=UTF-8

Hi David,

We are stress testing breeze.optimize.proximal and nnls...if you are
cutting a release now, we will need another release soon once we get the
runtime optimizations in place and merged to breeze.

Thanks.
Deb
 On Mar 15, 2015 9:39 PM, "David Hall" <david.lw.hall@gmail.com> wrote:

> snapshot is pushed. If you verify I'll publish the new artifacts.
>
> On Sun, Mar 15, 2015 at 1:14 AM, Yu Ishikawa <yuu.ishikawa+spark@gmail.com
> >
> wrote:
>
> > David Hall who is a breeze creator told me that it's a bug. So, I made a
> > jira
> > ticket about this issue. We need to upgrade breeze from 0.11.1 to 0.11.2
> or
> > later in order to fix the bug, when the new version of breeze will be
> > released.
> >
> > [SPARK-6341] Upgrade breeze from 0.11.1 to 0.11.2 or later - ASF JIRA
> > https://issues.apache.org/jira/browse/SPARK-6341
> >
> > Thanks,
> > Yu Ishikawa
> >
> >
> >
> > -----
> > -- Yu Ishikawa
> > --
> > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056p11058.html
> > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a1134586aca068405118ae7c0--

From dev-return-12048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 08:01:26 2015
Return-Path: <dev-return-12048-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8653B17BDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 08:01:26 +0000 (UTC)
Received: (qmail 22734 invoked by uid 500); 18 Mar 2015 08:01:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22654 invoked by uid 500); 18 Mar 2015 08:01:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22643 invoked by uid 99); 18 Mar 2015 08:01:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:01:24 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_IMAGE_ONLY_24,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:01:20 +0000
Received: by iecvj10 with SMTP id vj10so32354658iec.0
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 01:00:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=w1eYEUd1++vlaOTS7xigjbJhK3boy6mioGtZ93vNfFk=;
        b=TXqJtkc7cC75UQE1f/YsSld3x8SqDVNBUFB/efu2LgXTbVLxoRi6O2FNfsAFBbUf8T
         8ZkRn0cKm4vIPiFAfnSvm5ExUL27xbDbOE1AiyPVThDE65ei9si7nt+MVXz8m+4f0u1o
         HiLb8STwKVPwso+fXiXLFSvSBYVjKiNhc/WK1TQ9fzmVz6wJlHxJ+RkQVqoehSnGGq1t
         j8jcz60PIa96yotDPENb5lGJHWlJMsZF+5ocDd0JZPONMjhLq57jICFTi66nooGJgQZs
         5AWV0M7wNwC8zwQ0mcVxXHMbv/T//MUeVsTbItR+X3d2ZE10Q61ovwKdgcYQqD+ZgL1f
         W/sw==
X-Gm-Message-State: ALoCoQmk06znHtFO0lG+Pgf8OXZa9dEMGexU6lkUva4Sf51/3ApgTSO59DaIYR2YsXZ/GXUhLnYa
MIME-Version: 1.0
X-Received: by 10.107.137.25 with SMTP id l25mr94315870iod.23.1426665640494;
 Wed, 18 Mar 2015 01:00:40 -0700 (PDT)
Received: by 10.107.46.32 with HTTP; Wed, 18 Mar 2015 01:00:40 -0700 (PDT)
In-Reply-To: <CANCoaU7Hj3OC2edR0bNm_rVn-44LO63n2hSq56bjwyCN+81jGQ@mail.gmail.com>
References: <CANCoaU7Hj3OC2edR0bNm_rVn-44LO63n2hSq56bjwyCN+81jGQ@mail.gmail.com>
Date: Wed, 18 Mar 2015 13:30:40 +0530
Message-ID: <CABD4CGLbiiv2g-NTqOdNMROvjftSvs-tCGniJWsYxMMDJpYO9A@mail.gmail.com>
Subject: Re: Fixed worker ports in the spark worker
From: Arush Kharbanda <arush@sigmoidanalytics.com>
To: Niranda Perera <niranda.perera@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113fb158b7415e05118b7abf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113fb158b7415e05118b7abf
Content-Type: text/plain; charset=UTF-8

You can fix the ports in the configuration -

http://spark.apache.org/docs/1.2.0/configuration.html#networking

On Wed, Mar 18, 2015 at 11:10 AM, Niranda Perera <niranda.perera@gmail.com>
wrote:

> Hi all,
>
> I see that spark server opens up random ports, especially in the workers.
>
> is there any way to fix these ports or give an set of ports for the worker
> to choose from?
>
> cheers
>
> --
> Niranda
>



-- 

[image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>

*Arush Kharbanda* || Technical Teamlead

arush@sigmoidanalytics.com || www.sigmoidanalytics.com

--001a113fb158b7415e05118b7abf--

From dev-return-12049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 08:05:07 2015
Return-Path: <dev-return-12049-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5158017BF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 08:05:07 +0000 (UTC)
Received: (qmail 28280 invoked by uid 500); 18 Mar 2015 08:05:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28208 invoked by uid 500); 18 Mar 2015 08:05:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28196 invoked by uid 99); 18 Mar 2015 08:05:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:05:05 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of david.lw.hall@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:05:01 +0000
Received: by igbue6 with SMTP id ue6so36614602igb.1
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 01:04:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=uIdg3O10mYNGA6IzVMm5Q/iF5j5Znfc2xWfmSJyg1nI=;
        b=sfaVzboCbxrbc1BAwpn4IGL88znjfEpFDx5k3c0zd8BrM5mXT4AXsvPI0wQiu+f0I5
         3I8o96XF3kvE5zzLVAjcXZJ1ZrVdS0SC8NvA/26Mq8DV0FvMyOmyW7IOGYVpY/bv6+Ph
         85P8qbfzT7wsB1I5AfyJDpnGXUBA0UfyayJWaJ0SoCirkZXn+C4q6gB1u1dF2j13ZPkF
         FhKaSgjPnI5lDmUIUIGEeZ9gt1WBoOMAz44TasVV5dxm8j35p18/2rCEG/B3liSL62Tn
         89xjMP8rhkuZcMYHEwifiESUuP1jex8Kl9ZcPOvcvMe9M35D5ZrWldMWgWLMip8rup84
         nh+Q==
MIME-Version: 1.0
X-Received: by 10.50.138.68 with SMTP id qo4mr4328652igb.33.1426665880982;
 Wed, 18 Mar 2015 01:04:40 -0700 (PDT)
Received: by 10.107.152.130 with HTTP; Wed, 18 Mar 2015 01:04:40 -0700 (PDT)
In-Reply-To: <CA+B-+fz9DxY4s5Kf22_itJGSUjwaVP-fCcUEw6AP1xkF=8RQqg@mail.gmail.com>
References: <1426405522047-11056.post@n3.nabble.com>
	<1426407287506-11058.post@n3.nabble.com>
	<CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
	<CA+B-+fz9DxY4s5Kf22_itJGSUjwaVP-fCcUEw6AP1xkF=8RQqg@mail.gmail.com>
Date: Wed, 18 Mar 2015 01:04:40 -0700
Message-ID: <CALW2ey07nKnn1963FFxX1QxBDHZLC3qxyEmYr0rGGf0BckJjLw@mail.gmail.com>
Subject: Re: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
From: David Hall <david.lw.hall@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>, Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
Content-Type: multipart/alternative; boundary=001a1134bd800c9ce105118b8987
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134bd800c9ce105118b8987
Content-Type: text/plain; charset=UTF-8

sure.

On Wed, Mar 18, 2015 at 12:19 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi David,
>
> We are stress testing breeze.optimize.proximal and nnls...if you are
> cutting a release now, we will need another release soon once we get the
> runtime optimizations in place and merged to breeze.
>
> Thanks.
> Deb
>  On Mar 15, 2015 9:39 PM, "David Hall" <david.lw.hall@gmail.com> wrote:
>
>> snapshot is pushed. If you verify I'll publish the new artifacts.
>>
>> On Sun, Mar 15, 2015 at 1:14 AM, Yu Ishikawa <
>> yuu.ishikawa+spark@gmail.com>
>> wrote:
>>
>> > David Hall who is a breeze creator told me that it's a bug. So, I made a
>> > jira
>> > ticket about this issue. We need to upgrade breeze from 0.11.1 to
>> 0.11.2 or
>> > later in order to fix the bug, when the new version of breeze will be
>> > released.
>> >
>> > [SPARK-6341] Upgrade breeze from 0.11.1 to 0.11.2 or later - ASF JIRA
>> > https://issues.apache.org/jira/browse/SPARK-6341
>> >
>> > Thanks,
>> > Yu Ishikawa
>> >
>> >
>> >
>> > -----
>> > -- Yu Ishikawa
>> > --
>> > View this message in context:
>> >
>> http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056p11058.html
>> > Sent from the Apache Spark Developers List mailing list archive at
>> > Nabble.com.
>> >
>> > ---------------------------------------------------------------------
>> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> > For additional commands, e-mail: dev-help@spark.apache.org
>> >
>> >
>>
>

--001a1134bd800c9ce105118b8987--

From dev-return-12050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 08:06:21 2015
Return-Path: <dev-return-12050-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D767817BFB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 08:06:20 +0000 (UTC)
Received: (qmail 32078 invoked by uid 500); 18 Mar 2015 08:06:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31994 invoked by uid 500); 18 Mar 2015 08:06:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31981 invoked by uid 99); 18 Mar 2015 08:06:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:06:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of niranda.perera@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:05:54 +0000
Received: by obcxo2 with SMTP id xo2so26315638obc.0
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 01:05:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=74PUko1+WRgKWvcE6N9fdI5DUGobp4jrbGLMx/Sc1Ao=;
        b=nLiXzYdl53M2B87bBMQNpedCYmEXjW/k5V1Btssryd3LYCQs6jOhatO8DkSm+AGeDF
         I15Dli5kgjHLfycZOOvV/ON56yKna2hEKL9L2A7NeZB4wYYTYFIK46gCiEUYsr/5S34g
         hopcoYZ4/L4SSo0hkYkvxYiMTn27DbFaXgscgIF3qxDpkUATxAK0r58czfR+dOyv74qc
         71a/NPMzyg7r6xaQAtcjYzMIqbhyy4bznVeWlzgr74FzxJdps3DUrBK/AYt53x3dV/Qo
         felyE8ec36ovtrMMEe0fG8PsZMoGfFIRi3mHJ3grXbLpmTOgFxEkdEWs79RrBanKT5RI
         tq5Q==
MIME-Version: 1.0
X-Received: by 10.182.88.136 with SMTP id bg8mr41294990obb.86.1426665907754;
 Wed, 18 Mar 2015 01:05:07 -0700 (PDT)
Received: by 10.182.58.69 with HTTP; Wed, 18 Mar 2015 01:05:07 -0700 (PDT)
In-Reply-To: <CABD4CGLbiiv2g-NTqOdNMROvjftSvs-tCGniJWsYxMMDJpYO9A@mail.gmail.com>
References: <CANCoaU7Hj3OC2edR0bNm_rVn-44LO63n2hSq56bjwyCN+81jGQ@mail.gmail.com>
	<CABD4CGLbiiv2g-NTqOdNMROvjftSvs-tCGniJWsYxMMDJpYO9A@mail.gmail.com>
Date: Wed, 18 Mar 2015 13:35:07 +0530
Message-ID: <CANCoaU4gTJAifieCssayMftTDWAUsM2ks_pvCAZuPKntBTHMdA@mail.gmail.com>
Subject: Re: Fixed worker ports in the spark worker
From: Niranda Perera <niranda.perera@gmail.com>
To: Arush Kharbanda <arush@sigmoidanalytics.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0111bd76a51fcd05118b8a1e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0111bd76a51fcd05118b8a1e
Content-Type: text/plain; charset=UTF-8

Thanks Arush.

this is governed by the conf/spark-defaults.conf config, is it?

On Wed, Mar 18, 2015 at 1:30 PM, Arush Kharbanda <arush@sigmoidanalytics.com
> wrote:

> You can fix the ports in the configuration -
>
> http://spark.apache.org/docs/1.2.0/configuration.html#networking
>
> On Wed, Mar 18, 2015 at 11:10 AM, Niranda Perera <niranda.perera@gmail.com
> > wrote:
>
>> Hi all,
>>
>> I see that spark server opens up random ports, especially in the workers.
>>
>> is there any way to fix these ports or give an set of ports for the worker
>> to choose from?
>>
>> cheers
>>
>> --
>> Niranda
>>
>
>
>
> --
>
> [image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>
>
> *Arush Kharbanda* || Technical Teamlead
>
> arush@sigmoidanalytics.com || www.sigmoidanalytics.com
>



-- 
Niranda

--089e0111bd76a51fcd05118b8a1e--

From dev-return-12051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 08:24:04 2015
Return-Path: <dev-return-12051-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7E9AF17C78
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 08:24:04 +0000 (UTC)
Received: (qmail 59584 invoked by uid 500); 18 Mar 2015 08:24:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59506 invoked by uid 500); 18 Mar 2015 08:24:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59495 invoked by uid 99); 18 Mar 2015 08:23:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:23:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 08:23:35 +0000
Received: by iecsl2 with SMTP id sl2so31964376iec.1
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 01:23:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=bDw4qRuVsn7cKC1nKHmGtNrOWfnzkwuNzAXriVffeX4=;
        b=A4Q0i8L2OeccrZ6gw+8cJdQsQspNl/+0gazSsvfdRWhtqxL22O5athjbCIlwoJwThS
         HNVodSaaalvOiu7hb7hRRXROarMG5sO3rsH9XSq9B2GTgz3GqzJUB7/cAwRF9s+XPC/y
         LkbkSENSjAY/sXEIA7ap3MCqYBFaZPFy1qd4xcQTI79DNzagkq3Nb/QWIRzYbf44Rtdw
         fv3vqhM4sWdt/qYCYpiaFJkD0FZ/E+zJIGzJVAqbKaQ3m2lSacQVWOpDlBjR8cUxiMgR
         cRv/Dy7rJKrtCwlA7HyaApOO3fhbTFiL9nB+Okdi0huc7UGSeHBpYhRqWfJiEPZc8qh/
         0SUg==
X-Gm-Message-State: ALoCoQnVZubdfCJcGR+91oT37DEmRy8KA0GPSdAtoIXFica1MXoI+j4L6tDhdyekAJiiwVLjWVb1
MIME-Version: 1.0
X-Received: by 10.107.130.197 with SMTP id m66mr109174619ioi.19.1426666993258;
 Wed, 18 Mar 2015 01:23:13 -0700 (PDT)
Received: by 10.107.46.32 with HTTP; Wed, 18 Mar 2015 01:23:13 -0700 (PDT)
In-Reply-To: <CANCoaU4gTJAifieCssayMftTDWAUsM2ks_pvCAZuPKntBTHMdA@mail.gmail.com>
References: <CANCoaU7Hj3OC2edR0bNm_rVn-44LO63n2hSq56bjwyCN+81jGQ@mail.gmail.com>
	<CABD4CGLbiiv2g-NTqOdNMROvjftSvs-tCGniJWsYxMMDJpYO9A@mail.gmail.com>
	<CANCoaU4gTJAifieCssayMftTDWAUsM2ks_pvCAZuPKntBTHMdA@mail.gmail.com>
Date: Wed, 18 Mar 2015 13:53:13 +0530
Message-ID: <CABD4CGLNSY5EWYccBDjxh3cxKn9PctdnjO=QkVuWccEJ43vggw@mail.gmail.com>
Subject: Re: Fixed worker ports in the spark worker
From: Arush Kharbanda <arush@sigmoidanalytics.com>
To: Niranda Perera <niranda.perera@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ed0e258a35805118bcb4e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ed0e258a35805118bcb4e
Content-Type: text/plain; charset=UTF-8

Yes

On Wed, Mar 18, 2015 at 1:35 PM, Niranda Perera <niranda.perera@gmail.com>
wrote:

> Thanks Arush.
>
> this is governed by the conf/spark-defaults.conf config, is it?
>
> On Wed, Mar 18, 2015 at 1:30 PM, Arush Kharbanda <
> arush@sigmoidanalytics.com> wrote:
>
>> You can fix the ports in the configuration -
>>
>> http://spark.apache.org/docs/1.2.0/configuration.html#networking
>>
>> On Wed, Mar 18, 2015 at 11:10 AM, Niranda Perera <
>> niranda.perera@gmail.com> wrote:
>>
>>> Hi all,
>>>
>>> I see that spark server opens up random ports, especially in the workers.
>>>
>>> is there any way to fix these ports or give an set of ports for the
>>> worker
>>> to choose from?
>>>
>>> cheers
>>>
>>> --
>>> Niranda
>>>
>>
>>
>>
>> --
>>
>> [image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>
>>
>> *Arush Kharbanda* || Technical Teamlead
>>
>> arush@sigmoidanalytics.com || www.sigmoidanalytics.com
>>
>
>
>
> --
> Niranda
>



-- 

[image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>

*Arush Kharbanda* || Technical Teamlead

arush@sigmoidanalytics.com || www.sigmoidanalytics.com

--001a113ed0e258a35805118bcb4e--

From dev-return-12052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 10:49:33 2015
Return-Path: <dev-return-12052-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00AB1172FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 10:49:33 +0000 (UTC)
Received: (qmail 9284 invoked by uid 500); 18 Mar 2015 10:49:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9202 invoked by uid 500); 18 Mar 2015 10:49:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9191 invoked by uid 99); 18 Mar 2015 10:49:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 10:49:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.182] (HELO mail-lb0-f182.google.com) (209.85.217.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 10:49:06 +0000
Received: by lbbsy1 with SMTP id sy1so26494412lbb.1
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 03:48:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=QF8sL5+QGm50Jg13EWabn+H55B4PGV45v4SP01b07tw=;
        b=hzIlSz88JBUWboQ8y8gP0191zyAF2ffgcf7XYNsjssQg1ok15VYw//T9GKcNBFviIj
         PaFap/f9UrGX8GYFJ9paCzbuWJe9ClFb3wQLYeBcfQDtDEs520Vm/qhOUT61xc1l7e39
         1/nxbUQdRsx60ypHhcv8LXt9kHCY5TZcQJsqZPdhLqva2fqeJaqS+q4XFlaX1zFfJkcy
         izg4vzl7o6KyTG4RDPPWUAKgMQ1ld4imZE8MqDOy2hqsuisaAiX3giiTaF2GnVHu/qud
         CbHhZOHZ0eGnQIEGPY199sxbShqniq0KezYg4lM2QwwRn9KwY8Xjh/zJrJ3UsgcEnfPq
         ISUA==
X-Gm-Message-State: ALoCoQmLpwEDtupC8OyRvhfFhiispFz9SMsJPISvLZqy5U9WxgPZtOLb7Hrvm2fWASv03whrj9yx
MIME-Version: 1.0
X-Received: by 10.112.167.231 with SMTP id zr7mr64298259lbb.123.1426675680023;
 Wed, 18 Mar 2015 03:48:00 -0700 (PDT)
Received: by 10.152.43.234 with HTTP; Wed, 18 Mar 2015 03:47:59 -0700 (PDT)
In-Reply-To: <D12E11A3.AE7A3%jonathak@amazon.com>
References: <D12E11A3.AE7A3%jonathak@amazon.com>
Date: Wed, 18 Mar 2015 16:17:59 +0530
Message-ID: <CAHUQ+_ajudCtVCttamBfeZmQoudX0=54KcgfBPHvkibSLDB6iA@mail.gmail.com>
Subject: Re: Using Spark with a SOCKS proxy
From: Akhil Das <akhil@sigmoidanalytics.com>
To: "Kelly, Jonathan" <jonathak@amazon.com>
Cc: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c235e21e2d7c05118dd1fb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c235e21e2d7c05118dd1fb
Content-Type: text/plain; charset=UTF-8

Did you try ssh tunneling instead of SOCKS?

Thanks
Best Regards

On Wed, Mar 18, 2015 at 5:45 AM, Kelly, Jonathan <jonathak@amazon.com>
wrote:

>  I'm trying to figure out how I might be able to use Spark with a SOCKS
> proxy.  That is, my dream is to be able to write code in my IDE then run it
> without much trouble on a remote cluster, accessible only via a SOCKS proxy
> between the local development machine and the master node of the
> cluster (ignoring, for now, any dependencies that would need to be
> transferred--assume it's a very simple app with no dependencies that aren't
> part of the Spark classpath on the cluster).  This is possible with Hadoop
> by setting hadoop.rpc.socket.factory.class.default to
> org.apache.hadoop.net.SocksSocketFactory and hadoop.socks.server to
> localhost:<port on which a SOCKS proxy has been opened via "ssh -D" to the
> master node>.  However, I can't seem to find anything like this for Spark,
> and I only see very few mentions of it on the user list and on
> stackoverflow, with no real answers.  (See links below.)
>
>  I thought I might be able to use the JVM's -DsocksProxyHost and
> -DsocksProxyPort system properties, but it still does not seem to work.
> That is, if I start a SOCKS proxy to my master node using something like
> "ssh -D 2600 <master node public name>" then run a simple Spark app that
> calls SparkConf.setMaster("spark://<master node private IP>:7077"), passing
> in JVM args of "-DsocksProxyHost=locahost -DsocksProxyPort=2600", the
> driver hangs for a while before finally giving up ("Application has been
> killed. Reason: All masters are unresponsive! Giving up.").  It seems like
> it is not even attempting to use the SOCKS proxy.  Do
> -DsocksProxyHost/-DsocksProxyPort not even work for Spark?
>
>
> http://stackoverflow.com/questions/28047000/connect-to-spark-through-a-socks-proxy (unanswered
> similar question from somebody else about a month ago)
> https://issues.apache.org/jira/browse/SPARK-5004 (unresolved, somewhat
> related JIRA from a few months ago)
>
>  Thanks,
>  Jonathan
>

--001a11c235e21e2d7c05118dd1fb--

From dev-return-12053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 12:15:44 2015
Return-Path: <dev-return-12053-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C67A017584
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 12:15:44 +0000 (UTC)
Received: (qmail 93309 invoked by uid 500); 18 Mar 2015 12:15:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93239 invoked by uid 500); 18 Mar 2015 12:15:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93227 invoked by uid 99); 18 Mar 2015 12:15:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 12:15:43 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 12:15:18 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 8BB2F17B8C53
	for <dev@spark.apache.org>; Wed, 18 Mar 2015 05:15:27 -0700 (PDT)
Date: Wed, 18 Mar 2015 05:15:15 -0700 (MST)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426680915805-11107.post@n3.nabble.com>
In-Reply-To: <CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
References: <1426405522047-11056.post@n3.nabble.com> <1426407287506-11058.post@n3.nabble.com> <CALW2ey1mXWP-fAOGzy6d9rP5=G8UY6t7Ddr=EoqJ=rodYDicGw@mail.gmail.com>
Subject: Re: [mllib] Is there any bugs to divide a Breeze sparse vectors at
 Spark v1.3.0-rc3?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry for the delay in replying. I moved from Tokyo to New York in order to
attend Spark Summit East.
I verified the snapshot and the difference.
https://github.com/scalanlp/breeze/commit/f61d2f61137807651fc860404a244640e213f6d3

Thank you for your great work!
Yu Ishikawa



-----
-- Yu Ishikawa
--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/mllib-Is-there-any-bugs-to-divide-a-Breeze-sparse-vectors-at-Spark-v1-3-0-rc3-tp11056p11107.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 13:46:57 2015
Return-Path: <dev-return-12054-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B09B178EF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 13:46:57 +0000 (UTC)
Received: (qmail 51284 invoked by uid 500); 18 Mar 2015 13:46:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51210 invoked by uid 500); 18 Mar 2015 13:46:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51195 invoked by uid 99); 18 Mar 2015 13:46:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 13:46:55 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gilv@il.ibm.com designates 195.75.94.112 as permitted sender)
Received: from [195.75.94.112] (HELO e06smtp16.uk.ibm.com) (195.75.94.112)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 13:46:46 +0000
Received: from /spool/local
	by e06smtp16.uk.ibm.com with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted
	for <dev@spark.apache.org> from <gilv@il.ibm.com>;
	Wed, 18 Mar 2015 13:46:25 -0000
Received: from d06dlp03.portsmouth.uk.ibm.com (9.149.20.15)
	by e06smtp16.uk.ibm.com (192.168.101.146) with IBM ESMTP SMTP Gateway: Authorized Use Only! Violators will be prosecuted;
	Wed, 18 Mar 2015 13:46:23 -0000
Received: from b06cxnps4076.portsmouth.uk.ibm.com (d06relay13.portsmouth.uk.ibm.com [9.149.109.198])
	by d06dlp03.portsmouth.uk.ibm.com (Postfix) with ESMTP id 3E1EB1B0805F
	for <dev@spark.apache.org>; Wed, 18 Mar 2015 13:46:46 +0000 (GMT)
Received: from d06av09.portsmouth.uk.ibm.com (d06av09.portsmouth.uk.ibm.com [9.149.37.250])
	by b06cxnps4076.portsmouth.uk.ibm.com (8.14.9/8.14.9/NCO v10.0) with ESMTP id t2IDkM8v9765232
	for <dev@spark.apache.org>; Wed, 18 Mar 2015 13:46:22 GMT
Received: from d06av09.portsmouth.uk.ibm.com (localhost [127.0.0.1])
	by d06av09.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVout) with ESMTP id t2IDkMcV017196
	for <dev@spark.apache.org>; Wed, 18 Mar 2015 07:46:22 -0600
Received: from d06ml319.portsmouth.uk.ibm.com (d06ml319.portsmouth.uk.ibm.com [9.149.76.146])
	by d06av09.portsmouth.uk.ibm.com (8.14.4/8.14.4/NCO v10.0 AVin) with ESMTP id t2IDkMSh017173
	for <dev@spark.apache.org>; Wed, 18 Mar 2015 07:46:22 -0600
To: dev <dev@spark.apache.org>
MIME-Version: 1.0
Subject: parquet support  - some questions about code
X-KeepSent: 574CB584:B160B14A-C2257E0C:0049955F;
 type=4; name=$KeepSent
X-Mailer: IBM Notes Release 9.0.1SHF211 December 19, 2013
From: Gil Vernik <GILV@il.ibm.com>
Message-ID: <OF574CB584.B160B14A-ONC2257E0C.0049955F-C2257E0C.004BA805@il.ibm.com>
Date: Wed, 18 Mar 2015 15:46:21 +0200
X-MIMETrack: Serialize by Router on D06ML319/06/M/IBM(Release 9.0.1FP3|January  12, 2015) at
 18/03/2015 15:46:22,
	Serialize complete at 18/03/2015 15:46:22
Content-Type: multipart/alternative; boundary="=_alternative 004BA792C2257E0C_="
X-TM-AS-MML: disable
X-Content-Scanned: Fidelis XPS MAILER
x-cbid: 15031813-0025-0000-0000-0000045BEA3D
X-Virus-Checked: Checked by ClamAV on apache.org

--=_alternative 004BA792C2257E0C_=
Content-Type: text/plain; charset="US-ASCII"

Hi,

I am trying to better understand the code for  Parquet support.
In particular i got lost trying to understand ParquetRelation and 
ParquetRelation2. Does ParquetRelation2 is the new code that should 
completely remove ParquetRelation? ( I think there is some remark in the 
code notifying this )

Assuming i am using 
spark.sql.parquet.filterPushdown = true
spark.sql.parquet.useDataSourceApi = true

I saw that method buildScan from newParquet.scala has filtering push down 
into Parquet, but i also saw that there is filtering and projection push 
down from ParquetOperations inside SparkStrategies.scala
However every time i debug it, the 
 object ParquetOperations extends Strategy {
    def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
..........
Never evaluated to  case PhysicalOperation(projectList, filters: 
Seq[Expression], relation: ParquetRelation) =>

In which cases it will match this case?

Also, where is the code for Parquet projection and filter push down, is it 
inside ParquetOperations in SparkStrategies.scala or inside buildScan of 
newParquet.scala? Or both? If so i am not sure how it works...

Thanks,
Gil.

--=_alternative 004BA792C2257E0C_=--


From dev-return-12055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 14:07:00 2015
Return-Path: <dev-return-12055-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38D4217A1A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 14:07:00 +0000 (UTC)
Received: (qmail 39380 invoked by uid 500); 18 Mar 2015 14:06:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39310 invoked by uid 500); 18 Mar 2015 14:06:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39285 invoked by uid 99); 18 Mar 2015 14:06:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 14:06:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.44 as permitted sender)
Received: from [209.85.220.44] (HELO mail-pa0-f44.google.com) (209.85.220.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 14:06:31 +0000
Received: by pabyw6 with SMTP id yw6so43678409pab.2
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 07:05:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=byFKMdLYj+iLyZr11RdOKFAaV8qOgZgO2KLmvhVfScc=;
        b=bcr0ZjJcBejakrjEkzYqz11AcgriE9dR6tCV21SEfl3PfZRCs3nYAfo3z1OTGMi6+n
         kOwBXjyEj4byRZ+fJprqXOqbn12JFtHslwnCQjHhVts9pEJGNGWLoku9gkEXzxrFQhSe
         DIbVXhCR+kDEzXU5kcgTrSnH60R6LuDiQIzIXvlmZQG0+IQtCk/7X5VhYWmzkEyxKJ8Q
         +n0rGYGr/AzlvuEWlUa3zglhrmRhhNG8Q9vPZEFVzfjMQoIdUYRsFF+J8iJeEE1m09aQ
         UQldu5VCLQVA0xncD+BSWsDvgA0Q67b20BC+DvNvzzG1X8qS//D1If5NvqzIyV9/vXr8
         PhKQ==
X-Received: by 10.66.229.34 with SMTP id sn2mr94124123pac.92.1426687544370;
        Wed, 18 Mar 2015 07:05:44 -0700 (PDT)
Received: from [192.168.10.2] (c-50-131-222-227.hsd1.ca.comcast.net. [50.131.222.227])
        by mx.google.com with ESMTPSA id dq4sm27781701pdb.96.2015.03.18.07.05.41
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 18 Mar 2015 07:05:43 -0700 (PDT)
Message-ID: <55098637.1010202@gmail.com>
Date: Wed, 18 Mar 2015 22:05:43 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Gil Vernik <GILV@il.ibm.com>, dev <dev@spark.apache.org>
Subject: Re: parquet support  - some questions about code
References: <OF574CB584.B160B14A-ONC2257E0C.0049955F-C2257E0C.004BA805@il.ibm.com>
In-Reply-To: <OF574CB584.B160B14A-ONC2257E0C.0049955F-C2257E0C.004BA805@il.ibm.com>
Content-Type: text/plain; charset=windows-1252; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Gil,

ParquetRelation2 is based on the external data sources API, which is a 
more modular and non-intrusive way to add external data sources to Spark 
SQL. We are planning to replace ParquetRelation with ParquetRelation2 
entirely after the latter is more mature and stable. That's why you see 
two separate sets of Parquet code in the code base, and currently they 
also share part of the code.

In Spark 1.3, the new Parquet data source (ParquetRelation2) is enabled 
by default. So you can find entries of projection and filter push-down 
code in newParquet.scala.

Cheng

On 3/18/15 9:46 PM, Gil Vernik wrote:
> Hi,
>
> I am trying to better understand the code for  Parquet support.
> In particular i got lost trying to understand ParquetRelation and
> ParquetRelation2. Does ParquetRelation2 is the new code that should
> completely remove ParquetRelation? ( I think there is some remark in the
> code notifying this )
>
> Assuming i am using
> spark.sql.parquet.filterPushdown = true
> spark.sql.parquet.useDataSourceApi = true
>
> I saw that method buildScan from newParquet.scala has filtering push down
> into Parquet, but i also saw that there is filtering and projection push
> down from ParquetOperations inside SparkStrategies.scala
> However every time i debug it, the
>   object ParquetOperations extends Strategy {
>      def apply(plan: LogicalPlan): Seq[SparkPlan] = plan match {
> ..........
> Never evaluated to  case PhysicalOperation(projectList, filters:
> Seq[Expression], relation: ParquetRelation) =>
>
> In which cases it will match this case?
>
> Also, where is the code for Parquet projection and filter push down, is it
> inside ParquetOperations in SparkStrategies.scala or inside buildScan of
> newParquet.scala? Or both? If so i am not sure how it works...
>
> Thanks,
> Gil.
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 18 19:16:17 2015
Return-Path: <dev-return-12056-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7BE0517B26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 18 Mar 2015 19:16:17 +0000 (UTC)
Received: (qmail 67295 invoked by uid 500); 18 Mar 2015 19:16:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67209 invoked by uid 500); 18 Mar 2015 19:16:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67198 invoked by uid 99); 18 Mar 2015 19:16:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 19:16:16 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of turp1twin@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 18 Mar 2015 19:15:50 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 5523317C65CA
	for <dev@spark.apache.org>; Wed, 18 Mar 2015 12:15:30 -0700 (PDT)
Date: Wed, 18 Mar 2015 12:15:18 -0700 (MST)
From: turp1twin <turp1twin@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426706118372-11110.post@n3.nabble.com>
In-Reply-To: <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com>
References: <1425687609663-10934.post@n3.nabble.com> <CA+-p3AEf-PajzL8xQ-Fn5bNc=0GFXGciOYGypF_7R-JE6Qc0Eg@mail.gmail.com> <CACCBg5cJU+ogf3eW5Md1P4UWZCd3cKKBL5GqmL75d0qPczJRRg@mail.gmail.com> <CABPQxssmTvNt3+TFV2JYg0iZmZ3HrE-m5Z05avfk5ctP1QKNKw@mail.gmail.com>
Subject: Re: Block Transfer Service encryption support
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Still looking for feedback... I opened the ticket as a minor, thinking of
changing it to a major? Anyone object. I did see that a related ticket
(https://issues.apache.org/jira/browse/SPARK-6229) is marked as a major...
Cheers!

Jeff




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Block-Transfer-Service-encryption-support-tp10934p11110.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 03:13:11 2015
Return-Path: <dev-return-12057-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D2C910223
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 03:13:11 +0000 (UTC)
Received: (qmail 11274 invoked by uid 500); 19 Mar 2015 03:13:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11185 invoked by uid 500); 19 Mar 2015 03:13:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11133 invoked by uid 99); 19 Mar 2015 03:13:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 03:13:09 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 03:13:02 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id CE28B8E
	for <dev@spark.apache.org>; Thu, 19 Mar 2015 03:11:39 +0000 (UTC)
Received: from G9W3613.americas.hpqcorp.net (16.216.186.48) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 19 Mar 2015 03:09:18 +0000
Received: from G9W0737.americas.hpqcorp.net ([169.254.9.248]) by
 G9W3613.americas.hpqcorp.net ([16.216.186.48]) with mapi id 14.03.0169.001;
 Thu, 19 Mar 2015 03:09:18 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Which linear algebra interface to use within Spark MLlib?
Thread-Topic: Which linear algebra interface to use within Spark MLlib?
Thread-Index: AdBh8OyFI7ZEM0r2SyyYjAycG95Jqg==
Date: Thu, 19 Mar 2015 03:09:17 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.29]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1ADG9W0737americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1ADG9W0737americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

Currently I am using Breeze within Spark MLlib for linear algebra. I would =
like to reuse previously allocated matrices for storing the result of matri=
ces multiplication, i.e. I need to use "gemm" function C:=3Dq*A*B+p*C, whic=
h is missing in Breeze (Breeze automatically allocates a new matrix to stor=
e the result of multiplication). Also, I would like to minimize gemm calls =
that Breeze does. Should I use mllib.linalg.BLAS functions instead? While i=
t has gemm and axpy, it has rather limited number of operations. For exampl=
e, I need sum of the matrix by row or by columns, or applying a function to=
 all elements in a matrix. Also, MLlib Vector and Matrix interfaces that li=
nalg.BLAS operates seems to be rather undeveloped. Should I use plain netli=
b-java instead (will it remain in MLlib in future releases)?

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1ADG9W0737americas_--

From dev-return-12058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 04:21:27 2015
Return-Path: <dev-return-12058-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2D87B10467
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 04:21:27 +0000 (UTC)
Received: (qmail 49209 invoked by uid 500); 19 Mar 2015 04:21:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49120 invoked by uid 500); 19 Mar 2015 04:21:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49108 invoked by uid 99); 19 Mar 2015 04:21:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 04:21:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pllee@appier.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 04:21:20 +0000
Received: by qgh62 with SMTP id 62so56199102qgh.1
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 21:20:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=iMWn6GLrrqHd3gJNRnaZuvkNFKeSpgGVsyudWYXtSzc=;
        b=cM5/3GCFm3IfM9yca2U9GuMOJIkCFfNyfMz4OKaYPbTzXxOFfOx7PeK+w3Lu9JTOYt
         HXyuFacBSObdNDPgbdfkmM8RxGxbtbTGU4gNtLVS75f4xm2Tg+4IU9Etr4CaOkPJpgL6
         V7iD3rD6Kajtqg2kdraLchRbPh+NLuCDu+/RY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=iMWn6GLrrqHd3gJNRnaZuvkNFKeSpgGVsyudWYXtSzc=;
        b=QG218pNDf6L/E7L2TZ6t1DgWOiUp1A+/bs5rt+t0AMazvv80rseCaaZEVOes9wrTZV
         lOFlKvAFPoQ/ATYwiDul3L44l8h4NPmPcb63V6OWGmmzs1AEbCq2NPVIuZ/HERjUuesy
         tNsAHfAwQNqJM1kSpWSKFzb4aBtIUQSN5BFDdZ2A8Sd/QSmmTC2nG0Fxl8cLplZyuIJ1
         oKheIDjJwMgfTq53DhssnSuJnk+zZqymEqeNdgaqoNfZF3lRfo4SMBBJOG2dlu0zgMfI
         qefcJAHvzT2CSdZiGcAP9xjOWA95QiLC861soQvDP8hF61lYSRlDktt91HG+pmP07H8z
         Md2A==
X-Gm-Message-State: ALoCoQk9Df0TcQ73TpVqtQLdchXO66sTXhF7IZei+4aoX9wq1dKu93OZE6EmyWH+Z3vuvS33cARJ
MIME-Version: 1.0
X-Received: by 10.229.80.3 with SMTP id r3mr92955937qck.23.1426738859081; Wed,
 18 Mar 2015 21:20:59 -0700 (PDT)
Received: by 10.229.233.136 with HTTP; Wed, 18 Mar 2015 21:20:59 -0700 (PDT)
Date: Thu, 19 Mar 2015 12:20:59 +0800
Message-ID: <CANrtgzX2PKRt92UOzoSCzZ-wqm85LEY2B4Odk=EahBfA=QwZ6Q@mail.gmail.com>
Subject: SparkSQL 1.3.0 JDBC data source issues
From: Pei-Lun Lee <pllee@appier.com>
To: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133cb78e21e9105119c868e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133cb78e21e9105119c868e
Content-Type: text/plain; charset=UTF-8

Hi,

I am trying jdbc data source in spark sql 1.3.0 and found some issues.

First, the syntax "where str_col='value'" will give error for both
postgresql and mysql:

psql> create table foo(id int primary key,name text,age int);
bash> SPARK_CLASSPATH=postgresql-9.4-1201-jdbc41.jar spark/bin/spark-shell
scala>
sqlContext.load("jdbc",Map("url"->"jdbc:postgresql://XXX","dbtable"->"foo")).registerTempTable("foo")
scala> sql("select * from foo where name='bar'").collect
org.postgresql.util.PSQLException: ERROR: operator does not exist: text =
bar
  Hint: No operator matches the given name and argument type(s). You might
need to add explicit type casts.
  Position: 40
scala> sql("select * from foo where name like '%foo'").collect

bash> SPARK_CLASSPATH=mysql-connector-java-5.1.34.jar spark/bin/spark-shell
scala>
sqlContext.load("jdbc",Map("url"->"jdbc:mysql://XXX","dbtable"->"foo")).registerTempTable("foo")
scala> sql("select * from foo where name='bar'").collect
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column
'bar' in 'where clause'



Second, postgresql table with json data type does not work:

psql> create table foo(id int primary key, data json);
scala>
sqlContext.load("jdbc",Map("url"->"jdbc:mysql://XXX","dbtable"->"foo")).registerTempTable("foo")
java.sql.SQLException: Unsupported type 1111



Not sure these are bug in spark sql or jdbc. I can file JIRA ticket if
needed.

Thanks,
--
Pei-Lun

--001a1133cb78e21e9105119c868e--

From dev-return-12059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 06:22:21 2015
Return-Path: <dev-return-12059-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7846D10843
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 06:22:21 +0000 (UTC)
Received: (qmail 69246 invoked by uid 500); 19 Mar 2015 06:22:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69162 invoked by uid 500); 19 Mar 2015 06:22:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69148 invoked by uid 99); 19 Mar 2015 06:22:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 06:22:19 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 06:22:14 +0000
Received: by wixw10 with SMTP id w10so59656858wix.0
        for <dev@spark.apache.org>; Wed, 18 Mar 2015 23:21:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=olTHsJudzWPbMWl2onFWcGWRhjVRFqGnOLGEV7pPFLA=;
        b=bMfs2L4aOnq4iuBA//hXLQSI356SzuZiqk1+4yVcIpe90ZrNGs80P77qb+WoobBBF4
         8KCburP/UGV3PVDNXn0epuVwjP2wRHPJg6bCLjJPMQ95cwG625lphrmPUU9SOGKhrYav
         qMz1AUEmt03r8AI2i+Jlb6yRGSB1qzekvd6whD1rktj38JB95T77zzAxH3k3vRpzJ/5S
         TTawKP+BZk6pQr4m8O8iktf/GT1rJaREb9R4FuEwifassHmz6NXufcxzVbB2OJLdG4TV
         le3lFnYvZ6mb5aiUu57JZb4B77030PnRtgq/bx95Xgq+Ko7K52gjWYuIA4/j3gRgfhlG
         lFVg==
MIME-Version: 1.0
X-Received: by 10.195.12.35 with SMTP id en3mr143717533wjd.129.1426746113239;
 Wed, 18 Mar 2015 23:21:53 -0700 (PDT)
Received: by 10.27.77.205 with HTTP; Wed, 18 Mar 2015 23:21:53 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
Date: Wed, 18 Mar 2015 23:21:53 -0700
Message-ID: <CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
Subject: Re: Which linear algebra interface to use within Spark MLlib?
From: Debasish Das <debasish.das83@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfcf78843b09305119e37e1
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcf78843b09305119e37e1
Content-Type: text/plain; charset=UTF-8

dgemm dgemv and dot come to Breeze and Spark through netlib-java....

Right now both in dot and dgemv Breeze does a extra memory allocate but we
already found the issue and we are working on adding a common trait that
will provide a sink operation (basically memory will be allocated by
user)...adding more BLAS operators in breeze will also help in general as
lot more operations are defined over there...


On Wed, Mar 18, 2015 at 8:09 PM, Ulanov, Alexander <alexander.ulanov@hp.com>
wrote:

> Hi,
>
> Currently I am using Breeze within Spark MLlib for linear algebra. I would
> like to reuse previously allocated matrices for storing the result of
> matrices multiplication, i.e. I need to use "gemm" function C:=q*A*B+p*C,
> which is missing in Breeze (Breeze automatically allocates a new matrix to
> store the result of multiplication). Also, I would like to minimize gemm
> calls that Breeze does. Should I use mllib.linalg.BLAS functions instead?
> While it has gemm and axpy, it has rather limited number of operations. For
> example, I need sum of the matrix by row or by columns, or applying a
> function to all elements in a matrix. Also, MLlib Vector and Matrix
> interfaces that linalg.BLAS operates seems to be rather undeveloped. Should
> I use plain netlib-java instead (will it remain in MLlib in future
> releases)?
>
> Best regards, Alexander
>

--047d7bfcf78843b09305119e37e1--

From dev-return-12060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 08:31:45 2015
Return-Path: <dev-return-12060-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D48EC10C21
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 08:31:45 +0000 (UTC)
Received: (qmail 18352 invoked by uid 500); 19 Mar 2015 08:31:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18275 invoked by uid 500); 19 Mar 2015 08:31:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18263 invoked by uid 99); 19 Mar 2015 08:31:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 08:31:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pllee@appier.com designates 209.85.218.42 as permitted sender)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 08:31:39 +0000
Received: by oifl3 with SMTP id l3so28886314oif.0
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 01:29:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=7V3po/nsqT1CBdyCDM6KgA0ZA7FgU6gDiFGinBUz6SA=;
        b=eitnTgtKM8q7vFcuK8i8q5mnR0iovtSKduaoru1k0D286QbDWCl62uCtT/sEI//Ihu
         XRXLSlG5uhdgv6mcRImElcwp6A9pqJN76h3IkeEWJTJq+yYSEiVnDsHM84JixVrx2ldT
         M+5KL7HMI7fx+UUDnCPg9FhyXK9qatlR083iU=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=7V3po/nsqT1CBdyCDM6KgA0ZA7FgU6gDiFGinBUz6SA=;
        b=R2C/mfDbH+SklRETh/l7WhDqUrabymdZJQb/mYsG1LCtEPcrdTplJgT2ffdWyhcBE5
         bq7BFqWD/rC4BvjyZS/OCk/Fy9xkrtRcshEupowIJozFD5z3QIrznrsn6ixzs1GywVOi
         +JOJv6s6dEq6ur85f6uCZtB5lPtoq1mSGucAMcRYoICWWKrmuOiVdibxfy2HTQLlfviZ
         zmae4kvWdn9GonTxxl1ZsliYfjCW+DdWWREEjJ4XI6Bk5uUx9u9A+Jmb+t4SN7+ISMOV
         c7JunZRnyhW83uLBoxUYTll8kzU0XxKEFuSmmmbBK5XrF9xGDd72Byr3s5CwjtkVA09s
         YKkg==
X-Gm-Message-State: ALoCoQmTtg0I61zNk+zNVvCfVmmbZ62Qe7gSPHLZBbk33nIT5hcp28St3NJJ6gQOhoGtDsPIKU8Y
MIME-Version: 1.0
X-Received: by 10.202.72.141 with SMTP id v135mr5719525oia.24.1426753787722;
 Thu, 19 Mar 2015 01:29:47 -0700 (PDT)
Received: by 10.182.116.195 with HTTP; Thu, 19 Mar 2015 01:29:47 -0700 (PDT)
In-Reply-To: <CANrtgzX2PKRt92UOzoSCzZ-wqm85LEY2B4Odk=EahBfA=QwZ6Q@mail.gmail.com>
References: <CANrtgzX2PKRt92UOzoSCzZ-wqm85LEY2B4Odk=EahBfA=QwZ6Q@mail.gmail.com>
Date: Thu, 19 Mar 2015 16:29:47 +0800
Message-ID: <CANrtgzV7J4PgKTuhn8fQMO35MuKpXLP_M5JXkCAhETGrKbGicQ@mail.gmail.com>
Subject: Re: SparkSQL 1.3.0 JDBC data source issues
From: Pei-Lun Lee <pllee@appier.com>
To: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113e579eb30d190511a0000f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113e579eb30d190511a0000f
Content-Type: text/plain; charset=UTF-8

JIRA and PR for first issue:
https://issues.apache.org/jira/browse/SPARK-6408
https://github.com/apache/spark/pull/5087

On Thu, Mar 19, 2015 at 12:20 PM, Pei-Lun Lee <pllee@appier.com> wrote:

> Hi,
>
> I am trying jdbc data source in spark sql 1.3.0 and found some issues.
>
> First, the syntax "where str_col='value'" will give error for both
> postgresql and mysql:
>
> psql> create table foo(id int primary key,name text,age int);
> bash> SPARK_CLASSPATH=postgresql-9.4-1201-jdbc41.jar spark/bin/spark-shell
> scala>
> sqlContext.load("jdbc",Map("url"->"jdbc:postgresql://XXX","dbtable"->"foo")).registerTempTable("foo")
> scala> sql("select * from foo where name='bar'").collect
> org.postgresql.util.PSQLException: ERROR: operator does not exist: text =
> bar
>   Hint: No operator matches the given name and argument type(s). You might
> need to add explicit type casts.
>   Position: 40
> scala> sql("select * from foo where name like '%foo'").collect
>
> bash> SPARK_CLASSPATH=mysql-connector-java-5.1.34.jar spark/bin/spark-shell
> scala>
> sqlContext.load("jdbc",Map("url"->"jdbc:mysql://XXX","dbtable"->"foo")).registerTempTable("foo")
> scala> sql("select * from foo where name='bar'").collect
> com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: Unknown column
> 'bar' in 'where clause'
>
>
>
> Second, postgresql table with json data type does not work:
>
> psql> create table foo(id int primary key, data json);
> scala>
> sqlContext.load("jdbc",Map("url"->"jdbc:mysql://XXX","dbtable"->"foo")).registerTempTable("foo")
> java.sql.SQLException: Unsupported type 1111
>
>
>
> Not sure these are bug in spark sql or jdbc. I can file JIRA ticket if
> needed.
>
> Thanks,
> --
> Pei-Lun
>
>

--001a113e579eb30d190511a0000f--

From dev-return-12061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 10:28:57 2015
Return-Path: <dev-return-12061-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 598291732F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 10:28:57 +0000 (UTC)
Received: (qmail 42665 invoked by uid 500); 19 Mar 2015 10:21:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42589 invoked by uid 500); 19 Mar 2015 10:21:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42575 invoked by uid 99); 19 Mar 2015 10:21:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 10:21:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.218.51 as permitted sender)
Received: from [209.85.218.51] (HELO mail-oi0-f51.google.com) (209.85.218.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 10:21:30 +0000
Received: by oier21 with SMTP id r21so60872905oie.1
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 03:20:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=UvhZ82aOJC0ZzwitdwctRZuxYCL1x9mhOenkBOzjYEQ=;
        b=aJymfLJF6UrVIJyrHD0d8dM7q2+AP1qDynKx9dlGj4Ai8nfBbH5mVMnyvOOLiWsgWK
         Qvh9XkIr4kIYYeS/sRvtrK5UZfoliqJiSOgBWPdVaQmxWIavPcDD07wummqiCgZ/2Cr3
         rjVcwnvg3MhMzbD01X/iMqwXqSkPAd1tevd5eKyl8DMTBvBRaubwb7WCx4RjzODCb6gZ
         llJCrjvCGR6U8GP3u7sRC4AXRVx1QxnO7OmO4QrG53wbAkJM0PRzlCrFmp5AbREU8U5g
         jbwucnRUljvNL5sK3Cesfbh/ZflAaLaH65YY7EOXSgeVHlaBzqCtn0vPW11TjgL6mo3K
         SoXw==
MIME-Version: 1.0
X-Received: by 10.60.33.106 with SMTP id q10mr28129247oei.67.1426760425579;
 Thu, 19 Mar 2015 03:20:25 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Thu, 19 Mar 2015 03:20:25 -0700 (PDT)
Date: Thu, 19 Mar 2015 11:20:25 +0100
Message-ID: <CAO=evYeAwt4jarz16Ju21BNrSOa0EqMkkn+hF_cre53gw33_gQ@mail.gmail.com>
Subject: Spark scheduling, data locality
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01176d6558a9960511a18c06
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01176d6558a9960511a18c06
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm trying to understand the task scheduling mechanism of Spark, and I'm
curious about where does locality preferences get evaluated? I'm trying to
determine if locality preferences are fetchable before the task get
serialized. A HintSet would be most appreciated!

Have nice day!

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

--089e01176d6558a9960511a18c06--

From dev-return-12062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 18:00:39 2015
Return-Path: <dev-return-12062-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 23477179FA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 18:00:39 +0000 (UTC)
Received: (qmail 21388 invoked by uid 500); 19 Mar 2015 17:53:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21306 invoked by uid 500); 19 Mar 2015 17:53:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21292 invoked by uid 99); 19 Mar 2015 17:53:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 17:53:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of michael@videoamp.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 17:52:49 +0000
Received: by pdnc3 with SMTP id c3so82787786pdn.0
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 10:51:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:from:content-type:content-transfer-encoding
         :subject:message-id:date:to:mime-version;
        bh=HYu8SVMyu10WLFXEGlwnVB0qY8hunRBiVzDSNXhwdi4=;
        b=l6Ghqurn0GY+p1wu+pnEDZ1ID648c88tvPwzA4EGKIz0XN0bo2sJr3ww56xf0PavCb
         Fo5Eog7lfJ7w9G4xCfY444Lyof1jErcKj3QGfE4MTjI2dDBR8SD0BGZB7H+BTzagLJSN
         DFdlZ4xl9kRaFVAeHIQ4/htN2ZrJ2zPQ/jzQ7bdLkMQvsDxRGTeHl9WZPBzsUQhkQoy/
         E+b7SZUTvTQ9tuJHnz000xK7bw5uJ7FYmtIXYD7w+6pZZfz6GbsF9/PMonvYnKHDWAEJ
         qawnzpnnT+8eO/UiNrRnGWwHe4MuXRG9RhXDy3MrC+dYv2hOcW3a4WO5WMe/HZNg2uUZ
         f9dg==
X-Gm-Message-State: ALoCoQlqJ9r1DNUDzPmJ+RfPym82LNbY6ETQKu3vtxMjzVDpWR1XzF7kZX4X0cfWptgnjvVlYlYU
X-Received: by 10.66.154.162 with SMTP id vp2mr175703698pab.73.1426787476747;
        Thu, 19 Mar 2015 10:51:16 -0700 (PDT)
Received: from [192.168.1.248] (cpe-23-242-94-210.socal.res.rr.com. [23.242.94.210])
        by mx.google.com with ESMTPSA id ow2sm4118742pdb.14.2015.03.19.10.51.15
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 19 Mar 2015 10:51:16 -0700 (PDT)
From: Michael Allman <michael@videoamp.com>
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: quoted-printable
Subject: Spark SQL ExternalSorter not stopped
Message-Id: <9B241471-BBD8-4A63-B4EB-E5DA625A4F38@videoamp.com>
Date: Thu, 19 Mar 2015 10:51:14 -0700
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
X-Mailer: Apple Mail (2.2070.6)
X-Virus-Checked: Checked by ClamAV on apache.org

I've examined the experimental support for ExternalSorter in Spark SQL, =
and it does not appear that the external sorted is ever stopped =
(ExternalSorter.stop). According to the API documentation, this suggests =
a resource leak. Before I file a bug report in Jira, can someone =
familiar with the codebase confirm this is indeed a bug?

Thanks,

Michael=

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 19:11:35 2015
Return-Path: <dev-return-12063-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6264217D62
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 19:11:35 +0000 (UTC)
Received: (qmail 54951 invoked by uid 500); 19 Mar 2015 19:11:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54887 invoked by uid 500); 19 Mar 2015 19:11:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54557 invoked by uid 99); 19 Mar 2015 19:10:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 19:10:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ardlema@gmail.com designates 209.85.220.179 as permitted sender)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 19:10:39 +0000
Received: by mail-vc0-f179.google.com with SMTP id hq11so665939vcb.10
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 12:10:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=NTZfYhJT6U2aGnhOu1Mw01p8fAtO+1TeTCmVdeMfe44=;
        b=TFtf7fOAMncg/9vdUJjHo3VsTSSWNAlyBB+I0N7eaX98MaDXy5T0Vk6ElGqdjc6mcA
         u6sj3akAYd6lZ8/BrSU16RT8Ajfc4OgFQ2NXep1rT66tB5UZPM0vIB87tCF2d/3Flmk0
         utsZNlaNIV2+t7gEL4811x3838jeJZbfXLPNyn/8DxPevV/go2Mgx6IGKjiDPqNnhwi2
         sdJa/uZiVW77A1AZ3xZzVS+hRC9TLcXoHxj3fWVhIdQVuzm0i2lEA6rJif+KUZorlqcm
         ZJqYRChnXmYyY1qBz/Rtffs2Hn2wlSxCE3DR3256gAErfg3war6uTqmUAP+hZX+D3WQ2
         WlTQ==
MIME-Version: 1.0
X-Received: by 10.52.27.211 with SMTP id v19mr49353161vdg.35.1426792218628;
 Thu, 19 Mar 2015 12:10:18 -0700 (PDT)
Received: by 10.52.120.77 with HTTP; Thu, 19 Mar 2015 12:10:18 -0700 (PDT)
Date: Thu, 19 Mar 2015 20:10:18 +0100
Message-ID: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
Subject: Exception using the new createDirectStream util method
From: Alberto Rodriguez <ardlema@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf307d06d25c17170511a8f319
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307d06d25c17170511a8f319
Content-Type: text/plain; charset=UTF-8

Hi all,

I am trying to make the new kafka and spark streaming integration work (direct
approach "no receivers"
<http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html>). I
have created an unit test where I configure and start both zookeeper and
kafka.

When I try to create the InputDStream using the createDirectStream method
of the KafkaUtils class I am getting the following error:

org.apache.spark.SparkException:* Couldn't find leader offsets for Set()*
org.apache.spark.SparkException: org.apache.spark.SparkException: Couldn't
find leader offsets for Set()
at
org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)

Following is the code that tries to create the DStream:

val messages: InputDStream[(String, String)] =
KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](
        ssc, kafkaParams, topics)

Does anyone faced this problem?

Thank you in advance.

Kind regards,

Alberto

--20cf307d06d25c17170511a8f319--

From dev-return-12064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 20:12:48 2015
Return-Path: <dev-return-12064-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EDFFF172D9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 20:12:47 +0000 (UTC)
Received: (qmail 50173 invoked by uid 500); 19 Mar 2015 20:12:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50100 invoked by uid 500); 19 Mar 2015 20:12:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50079 invoked by uid 99); 19 Mar 2015 20:12:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 20:12:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 20:12:10 +0000
Received: by igbqf9 with SMTP id qf9so251154igb.1
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 13:10:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=53BmnGYoXu+qyNfSJMqqg913hfKxy24ZRdSlomtT1GI=;
        b=Lx8Ct/pU6I9km+VBakTObcXj0l0H9S9RrB//nMnjON/hIqZ24g9kANOr70nbVhLFXp
         5RG9+YiFcFlS6zuytqJJDmnKdJLgVVL/7Xz/XaAIajeHB+uClqd8UAE75AH7bKhWFKhl
         mHcUG7qJ1HMOJiOr77ti/gmFwLGuEGMQSTQiQt2WiWcZnFLiThunw0DsXK4PIFJe1RKK
         MY2vUV89q8kSSDlOQ+dqkdjdn1UFR/ZvY1W1OiuRK1KsVN0JNiMgryKx4eJ8lEkVD/lQ
         +xX7E1hFPcMilqHAFg7raGndJz/0slzeHtfGGWGDHabMZCodqlyCD9PND7gZx43u9mdr
         DMUw==
MIME-Version: 1.0
X-Received: by 10.107.170.33 with SMTP id t33mr126049664ioe.7.1426795838515;
 Thu, 19 Mar 2015 13:10:38 -0700 (PDT)
Received: by 10.36.53.148 with HTTP; Thu, 19 Mar 2015 13:10:38 -0700 (PDT)
In-Reply-To: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
References: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
Date: Thu, 19 Mar 2015 13:10:38 -0700
Message-ID: <CALte62ycYF4PPZkp-62DBdUy13U2aQnvOecae6LQdqArjAwxuQ@mail.gmail.com>
Subject: Re: Exception using the new createDirectStream util method
From: Ted Yu <yuzhihong@gmail.com>
To: Alberto Rodriguez <ardlema@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1142d85e1f31f50511a9cbb4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1142d85e1f31f50511a9cbb4
Content-Type: text/plain; charset=UTF-8

Looking at KafkaCluster#getLeaderOffsets():

          respMap.get(tp).foreach { por: PartitionOffsetsResponse =>
            if (por.error == ErrorMapping.NoError) {
...
            } else {
              errs.append(ErrorMapping.exceptionFor(por.error))
            }
There should be some error other than "Couldn't find leader offsets for
Set()"

Can you check again ?

Thanks

On Thu, Mar 19, 2015 at 12:10 PM, Alberto Rodriguez <ardlema@gmail.com>
wrote:

> Hi all,
>
> I am trying to make the new kafka and spark streaming integration work
> (direct
> approach "no receivers"
> <http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html>). I
> have created an unit test where I configure and start both zookeeper and
> kafka.
>
> When I try to create the InputDStream using the createDirectStream method
> of the KafkaUtils class I am getting the following error:
>
> org.apache.spark.SparkException:* Couldn't find leader offsets for Set()*
> org.apache.spark.SparkException: org.apache.spark.SparkException: Couldn't
> find leader offsets for Set()
> at
>
> org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)
>
> Following is the code that tries to create the DStream:
>
> val messages: InputDStream[(String, String)] =
> KafkaUtils.createDirectStream[String, String, StringDecoder,
> StringDecoder](
>         ssc, kafkaParams, topics)
>
> Does anyone faced this problem?
>
> Thank you in advance.
>
> Kind regards,
>
> Alberto
>

--001a1142d85e1f31f50511a9cbb4--

From dev-return-12065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 20:52:34 2015
Return-Path: <dev-return-12065-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8CBA11747B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 20:52:34 +0000 (UTC)
Received: (qmail 83182 invoked by uid 500); 19 Mar 2015 20:52:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83104 invoked by uid 500); 19 Mar 2015 20:52:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83093 invoked by uid 99); 19 Mar 2015 20:52:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 20:52:32 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.240.92.67] (HELO g9t5009.houston.hp.com) (15.240.92.67)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 20:52:24 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5009.houston.hp.com (Postfix) with ESMTPS id C31E318A
	for <dev@spark.apache.org>; Thu, 19 Mar 2015 20:51:03 +0000 (UTC)
Received: from G4W6300.americas.hpqcorp.net (16.210.26.225) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 19 Mar 2015 20:49:25 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G4W6300.americas.hpqcorp.net ([16.210.26.225]) with mapi id 14.03.0169.001;
 Thu, 19 Mar 2015 20:49:24 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Debasish Das <debasish.das83@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Which linear algebra interface to use within Spark MLlib?
Thread-Topic: Which linear algebra interface to use within Spark MLlib?
Thread-Index: AdBh8OyFI7ZEM0r2SyyYjAycG95JqgAHBE6AAB5MNhc=
Date: Thu, 19 Mar 2015 20:49:23 +0000
Message-ID: <09B9C296-EBE4-48C6-8B59-4BD26EDB6F3D@hp.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>,<CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
In-Reply-To: <CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
Accept-Language: en-US
Content-Language: ru-RU
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
Content-Type: text/plain; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Thank you! When do you expect to have gemm in Breeze and that version of Br=
eeze to ship with MLlib?

Also, could someone please elaborate on the linalg.BLAS and Matrix? Are the=
y going to be developed further, should in long term all developers use the=
m?

Best regards, Alexander

18.03.2015, =D7 23:21, "Debasish Das" <debasish.das83@gmail.com<mailto:deba=
sish.das83@gmail.com>> =CE=C1=D0=C9=D3=C1=CC(=C1):

dgemm dgemv and dot come to Breeze and Spark through netlib-java....

Right now both in dot and dgemv Breeze does a extra memory allocate but we =
already found the issue and we are working on adding a common trait that wi=
ll provide a sink operation (basically memory will be allocated by user)...=
adding more BLAS operators in breeze will also help in general as lot more =
operations are defined over there...


On Wed, Mar 18, 2015 at 8:09 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
<mailto:alexander.ulanov@hp.com>> wrote:
Hi,

Currently I am using Breeze within Spark MLlib for linear algebra. I would =
like to reuse previously allocated matrices for storing the result of matri=
ces multiplication, i.e. I need to use "gemm" function C:=3Dq*A*B+p*C, whic=
h is missing in Breeze (Breeze automatically allocates a new matrix to stor=
e the result of multiplication). Also, I would like to minimize gemm calls =
that Breeze does. Should I use mllib.linalg.BLAS functions instead? While i=
t has gemm and axpy, it has rather limited number of operations. For exampl=
e, I need sum of the matrix by row or by columns, or applying a function to=
 all elements in a matrix. Also, MLlib Vector and Matrix interfaces that li=
nalg.BLAS operates seems to be rather undeveloped. Should I use plain netli=
b-java instead (will it remain in MLlib in future releases)?

Best regards, Alexander


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 21:15:19 2015
Return-Path: <dev-return-12066-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 495E817573
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 21:15:19 +0000 (UTC)
Received: (qmail 46220 invoked by uid 500); 19 Mar 2015 21:15:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46145 invoked by uid 500); 19 Mar 2015 21:15:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46134 invoked by uid 99); 19 Mar 2015 21:15:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:15:17 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:15:11 +0000
Received: by oiag65 with SMTP id g65so75967030oia.2
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 14:13:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:cc:content-type;
        bh=SW4IqIBzMF3Zr92pnTXAr3KU+lqTWvkgT2O9HwY886g=;
        b=eqnwHQAkVCOFVhLOrhCXhdmZP7It8YqIpavJPyVAntGpinKwr0Wv9nKwuxwcMC0NMe
         nyMFDVgkhZ8Ov5UJoIrJmVZtbg2Pz77rCyjK8wkk7mMx3/6/menzhTkPu0M694akgilE
         i118d3bjnYdhNH1XOkC++nlU0PS6qRhA06lhQYgfoDB9dln17epVS6fs5pSqiO589aT4
         C6s5hvr0y9XXOYs/EiD4MqVFHHq+XDJz/1IWpunBkkd1qbXWNv4Z1JGVlk56fEOPZnuV
         Iizd4IU00VQW3YDsLY3nBXC7i4CHZCOO/BNn5xcKTglDpgy3Xu2CUOBSje8Kw162/+qp
         QWOQ==
X-Gm-Message-State: ALoCoQkU+OhD0QbrZ8XfwhA2PgUKQq6zIjEXUUVo6owVs9+m5wapANGX26e7QxrXAeI+lNw39iro
MIME-Version: 1.0
X-Received: by 10.182.248.161 with SMTP id yn1mt44664169obc.25.1426799580623;
 Thu, 19 Mar 2015 14:13:00 -0700 (PDT)
Received: by 10.76.87.36 with HTTP; Thu, 19 Mar 2015 14:13:00 -0700 (PDT)
In-Reply-To: <CALte62ycYF4PPZkp-62DBdUy13U2aQnvOecae6LQdqArjAwxuQ@mail.gmail.com>
References: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
	<CALte62ycYF4PPZkp-62DBdUy13U2aQnvOecae6LQdqArjAwxuQ@mail.gmail.com>
Date: Thu, 19 Mar 2015 16:13:00 -0500
Message-ID: <CAKWX9VWSbZhoh5KS4+grsQ-tMO5cgrnQ7LFSRisL2D2KqMNaPQ@mail.gmail.com>
Subject: Re: Exception using the new createDirectStream util method
From: Cody Koeninger <cody@koeninger.org>
Cc: Alberto Rodriguez <ardlema@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2a8ac2c180a0511aaaadb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a8ac2c180a0511aaaadb
Content-Type: text/plain; charset=UTF-8

What is the value of your topics variable, and does it correspond to topics
that already exist on the cluster and have messages in them?

On Thu, Mar 19, 2015 at 3:10 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> Looking at KafkaCluster#getLeaderOffsets():
>
>           respMap.get(tp).foreach { por: PartitionOffsetsResponse =>
>             if (por.error == ErrorMapping.NoError) {
> ...
>             } else {
>               errs.append(ErrorMapping.exceptionFor(por.error))
>             }
> There should be some error other than "Couldn't find leader offsets for
> Set()"
>
> Can you check again ?
>
> Thanks
>
> On Thu, Mar 19, 2015 at 12:10 PM, Alberto Rodriguez <ardlema@gmail.com>
> wrote:
>
> > Hi all,
> >
> > I am trying to make the new kafka and spark streaming integration work
> > (direct
> > approach "no receivers"
> > <http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html>).
> I
> > have created an unit test where I configure and start both zookeeper and
> > kafka.
> >
> > When I try to create the InputDStream using the createDirectStream method
> > of the KafkaUtils class I am getting the following error:
> >
> > org.apache.spark.SparkException:* Couldn't find leader offsets for Set()*
> > org.apache.spark.SparkException: org.apache.spark.SparkException:
> Couldn't
> > find leader offsets for Set()
> > at
> >
> >
> org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)
> >
> > Following is the code that tries to create the DStream:
> >
> > val messages: InputDStream[(String, String)] =
> > KafkaUtils.createDirectStream[String, String, StringDecoder,
> > StringDecoder](
> >         ssc, kafkaParams, topics)
> >
> > Does anyone faced this problem?
> >
> > Thank you in advance.
> >
> > Kind regards,
> >
> > Alberto
> >
>

--001a11c2a8ac2c180a0511aaaadb--

From dev-return-12067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 21:17:16 2015
Return-Path: <dev-return-12067-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A7211758C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 21:17:16 +0000 (UTC)
Received: (qmail 52713 invoked by uid 500); 19 Mar 2015 21:17:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52636 invoked by uid 500); 19 Mar 2015 21:17:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52624 invoked by uid 99); 19 Mar 2015 21:17:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:17:14 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.215.44 as permitted sender)
Received: from [209.85.215.44] (HELO mail-la0-f44.google.com) (209.85.215.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:16:48 +0000
Received: by lagg8 with SMTP id g8so72710525lag.1
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 14:16:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=8kf69x/DUHhdcktVqYMn9eDLQzFZAwk9BxRQMlNKrgg=;
        b=Fg0uHpW0pLIsUb5LwaSs7B7BWnhDTGY8g4QFh+MoBlk7s1S4DU7AulMh2q3GGFZ9Gr
         AU8OyaMxGlzSLLdHByKPLepF9/k7TdLZW43yC4wGVg5dEVX5tt8d/9olIYlij4B6BxDJ
         iTykzqK7tj2ad/xlg3u0Eg1dw/Ti/0VfcR3f/vEgerPsn4GCedNomdAxzpuCvQ6gjj/X
         HPz6NZZArRZrksLoGiTQUWAf54d0s4K9tv8V3qCqqt7zg/53elbacMy7iyTGi90/TF7h
         MbX6eKX+PTVgWUIv600QVvsOV+LajixgbMOLDFrZtd0IPl7yCuWOfvIu/5HTIONWdhkH
         A/cw==
MIME-Version: 1.0
X-Received: by 10.152.19.228 with SMTP id i4mr734317lae.77.1426799761895; Thu,
 19 Mar 2015 14:16:01 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Thu, 19 Mar 2015 14:16:01 -0700 (PDT)
In-Reply-To: <09B9C296-EBE4-48C6-8B59-4BD26EDB6F3D@hp.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
	<CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
	<09B9C296-EBE4-48C6-8B59-4BD26EDB6F3D@hp.com>
Date: Thu, 19 Mar 2015 14:16:01 -0700
Message-ID: <CA+B-+fycWb5JCO1yA2Kh970g+7cAORm_UpQYjpC5OadKfFkx3w@mail.gmail.com>
Subject: Re: Which linear algebra interface to use within Spark MLlib?
From: Debasish Das <debasish.das83@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01493c22f938d10511aab4e0
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01493c22f938d10511aab4e0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I think for Breeze we are focused on dot and dgemv right now (along with
several other matrix vector style operations)...

For dgemm it is tricky since you need to do add dgemm for both DenseMatrix
and CSCMatrix...and for CSCMatrix you need to get something like
SuiteSparse which is under lgpl...so we have to think more on it..

For now can't you use dgemm directly from mllib.linalg.BLAS ? It's in
master...


On Thu, Mar 19, 2015 at 1:49 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
>
wrote:

>  Thank you! When do you expect to have gemm in Breeze and that version of
> Breeze to ship with MLlib?
>
>  Also, could someone please elaborate on the linalg.BLAS and Matrix? Are
> they going to be developed further, should in long term all developers us=
e
> them?
>
> Best regards, Alexander
>
> 18.03.2015, =D0=B2 23:21, "Debasish Das" <debasish.das83@gmail.com> =D0=
=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
>
>   dgemm dgemv and dot come to Breeze and Spark through netlib-java....
>
>  Right now both in dot and dgemv Breeze does a extra memory allocate but
> we already found the issue and we are working on adding a common trait th=
at
> will provide a sink operation (basically memory will be allocated by
> user)...adding more BLAS operators in breeze will also help in general as
> lot more operations are defined over there...
>
>
> On Wed, Mar 18, 2015 at 8:09 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
>
>> Hi,
>>
>> Currently I am using Breeze within Spark MLlib for linear algebra. I
>> would like to reuse previously allocated matrices for storing the result=
 of
>> matrices multiplication, i.e. I need to use "gemm" function C:=3Dq*A*B+p=
*C,
>> which is missing in Breeze (Breeze automatically allocates a new matrix =
to
>> store the result of multiplication). Also, I would like to minimize gemm
>> calls that Breeze does. Should I use mllib.linalg.BLAS functions instead=
?
>> While it has gemm and axpy, it has rather limited number of operations. =
For
>> example, I need sum of the matrix by row or by columns, or applying a
>> function to all elements in a matrix. Also, MLlib Vector and Matrix
>> interfaces that linalg.BLAS operates seems to be rather undeveloped. Sho=
uld
>> I use plain netlib-java instead (will it remain in MLlib in future
>> releases)?
>>
>> Best regards, Alexander
>>
>
>

--089e01493c22f938d10511aab4e0--

From dev-return-12068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 21:28:11 2015
Return-Path: <dev-return-12068-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7FF417660
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 21:28:11 +0000 (UTC)
Received: (qmail 95681 invoked by uid 500); 19 Mar 2015 21:28:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95601 invoked by uid 500); 19 Mar 2015 21:28:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95589 invoked by uid 99); 19 Mar 2015 21:28:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:28:10 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:28:02 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id 0809E74
	for <dev@spark.apache.org>; Thu, 19 Mar 2015 21:27:42 +0000 (UTC)
Received: from G9W3617.americas.hpqcorp.net (16.216.186.52) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 19 Mar 2015 21:25:29 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G9W3617.americas.hpqcorp.net ([16.216.186.52]) with mapi id 14.03.0169.001;
 Thu, 19 Mar 2015 21:25:29 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Debasish Das <debasish.das83@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Which linear algebra interface to use within Spark MLlib?
Thread-Topic: Which linear algebra interface to use within Spark MLlib?
Thread-Index: AdBh8OyFI7ZEM0r2SyyYjAycG95JqgAHBE6AAB5MNhcAAO33gAAAVKqI
Date: Thu, 19 Mar 2015 21:25:29 +0000
Message-ID: <FD914DA8-C1E7-48CA-B1CE-F6597265901B@hp.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
	<CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
	<09B9C296-EBE4-48C6-8B59-4BD26EDB6F3D@hp.com>,<CA+B-+fycWb5JCO1yA2Kh970g+7cAORm_UpQYjpC5OadKfFkx3w@mail.gmail.com>
In-Reply-To: <CA+B-+fycWb5JCO1yA2Kh970g+7cAORm_UpQYjpC5OadKfFkx3w@mail.gmail.com>
Accept-Language: en-US
Content-Language: ru-RU
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
Content-Type: text/plain; charset="koi8-r"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for quick response.

I can use linealg.BLAS.gemm, and this means that I have to use MLlib Matrix=
. The latter does not support some useful functionality needed for optimiza=
tion. For example, creation of Matrix given matrix size, array and offset i=
n this array. This means that I will need to create matrix in Breeze and co=
nvert it to MLlib. Also, linalg.BLAS misses some useful BLAS functions I ne=
ed, that can be found in Breeze (and netlib-java). The same concerns are ap=
plicable to MLlib Vector.

Best regards, Alexander

19.03.2015, =D7 14:16, "Debasish Das" <debasish.das83@gmail.com<mailto:deba=
sish.das83@gmail.com>> =CE=C1=D0=C9=D3=C1=CC(=C1):

I think for Breeze we are focused on dot and dgemv right now (along with se=
veral other matrix vector style operations)...

For dgemm it is tricky since you need to do add dgemm for both DenseMatrix =
and CSCMatrix...and for CSCMatrix you need to get something like SuiteSpars=
e which is under lgpl...so we have to think more on it..

For now can't you use dgemm directly from mllib.linalg.BLAS ? It's in maste=
r...


On Thu, Mar 19, 2015 at 1:49 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
<mailto:alexander.ulanov@hp.com>> wrote:
Thank you! When do you expect to have gemm in Breeze and that version of Br=
eeze to ship with MLlib?

Also, could someone please elaborate on the linalg.BLAS and Matrix? Are the=
y going to be developed further, should in long term all developers use the=
m?

Best regards, Alexander

18.03.2015, =D7 23:21, "Debasish Das" <debasish.das83@gmail.com<mailto:deba=
sish.das83@gmail.com>> =CE=C1=D0=C9=D3=C1=CC(=C1):

dgemm dgemv and dot come to Breeze and Spark through netlib-java....

Right now both in dot and dgemv Breeze does a extra memory allocate but we =
already found the issue and we are working on adding a common trait that wi=
ll provide a sink operation (basically memory will be allocated by user)...=
adding more BLAS operators in breeze will also help in general as lot more =
operations are defined over there...


On Wed, Mar 18, 2015 at 8:09 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
<mailto:alexander.ulanov@hp.com>> wrote:
Hi,

Currently I am using Breeze within Spark MLlib for linear algebra. I would =
like to reuse previously allocated matrices for storing the result of matri=
ces multiplication, i.e. I need to use "gemm" function C:=3Dq*A*B+p*C, whic=
h is missing in Breeze (Breeze automatically allocates a new matrix to stor=
e the result of multiplication). Also, I would like to minimize gemm calls =
that Breeze does. Should I use mllib.linalg.BLAS functions instead? While i=
t has gemm and axpy, it has rather limited number of operations. For exampl=
e, I need sum of the matrix by row or by columns, or applying a function to=
 all elements in a matrix. Also, MLlib Vector and Matrix interfaces that li=
nalg.BLAS operates seems to be rather undeveloped. Should I use plain netli=
b-java instead (will it remain in MLlib in future releases)?

Best regards, Alexander



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 21:38:37 2015
Return-Path: <dev-return-12069-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9881D176B8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 21:38:37 +0000 (UTC)
Received: (qmail 16780 invoked by uid 500); 19 Mar 2015 21:38:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16706 invoked by uid 500); 19 Mar 2015 21:38:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16694 invoked by uid 99); 19 Mar 2015 21:38:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:38:36 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ardlema@gmail.com designates 209.85.160.172 as permitted sender)
Received: from [209.85.160.172] (HELO mail-yk0-f172.google.com) (209.85.160.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 21:38:11 +0000
Received: by ykcn8 with SMTP id n8so33526149ykc.3
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 14:38:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:cc
         :content-type;
        bh=VS6k98D7kPL38HYwZa8mtm8c0IMoaFtBMjyMgBBEUMU=;
        b=da+6105jFmCyW1nxG/4tUIjoDPZoR6InsYS/j5sbD3wph5xQgql26ZGBHpbl+U3d+L
         MHd9WmnQ1Xxlq3aQ5F/ltZOOGwtQ7VjGBtI7tZ2xpCZDlvD9ehdD9XWDWGsEvMxtwld6
         vsUcdRzo+x2Lq0+S2xj+ybnkuSuKrPAHueMgQ3h4EaJSz/KKjkLgJbuD7bEGbi/5UMxw
         t+4uiNu/sW7NtUIW7UoQchcnyqKw0T9SLvY8ehrK/ifU+y6FDF9/WdiSk80zqzDPzRAN
         cd9AC2QAINl7tL2lyyvDbims3WwiatyLxobkeyL4Ufjcc+vzppLxHHVZGC/omfsxCWdB
         gUMw==
MIME-Version: 1.0
X-Received: by 10.52.27.211 with SMTP id v19mr49891852vdg.35.1426801089226;
 Thu, 19 Mar 2015 14:38:09 -0700 (PDT)
Received: by 10.52.120.77 with HTTP; Thu, 19 Mar 2015 14:38:08 -0700 (PDT)
In-Reply-To: <CAKWX9VWSbZhoh5KS4+grsQ-tMO5cgrnQ7LFSRisL2D2KqMNaPQ@mail.gmail.com>
References: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
	<CALte62ycYF4PPZkp-62DBdUy13U2aQnvOecae6LQdqArjAwxuQ@mail.gmail.com>
	<CAKWX9VWSbZhoh5KS4+grsQ-tMO5cgrnQ7LFSRisL2D2KqMNaPQ@mail.gmail.com>
Date: Thu, 19 Mar 2015 22:38:08 +0100
Message-ID: <CAMjNSowQ4FhxUzpqQ9etUDC2xgJTuRO=HBPyhh-m3Q_XyqGBRg@mail.gmail.com>
Subject: Re: Exception using the new createDirectStream util method
From: Alberto Rodriguez <ardlema@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf307d06d216ad6f0511ab04f8
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307d06d216ad6f0511ab04f8
Content-Type: text/plain; charset=UTF-8

Thank you for replying,

Ted, I have been debuging and the getLeaderOffsets method is not appending
errors because the method findLeaders that is called at the first line of
getLeaderOffsets is not returning leaders.

Cody, the topics do not have any messages yet. Could this be an issue??

If you guys want to have a look at the code I've just uploaded it to my
github account: big-brother <https://github.com/ardlema/big-brother> (see
DirectKafkaWordCountTest.scala).

Thank you again!!

2015-03-19 22:13 GMT+01:00 Cody Koeninger <cody@koeninger.org>:

> What is the value of your topics variable, and does it correspond to
> topics that already exist on the cluster and have messages in them?
>
> On Thu, Mar 19, 2015 at 3:10 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>
>> Looking at KafkaCluster#getLeaderOffsets():
>>
>>           respMap.get(tp).foreach { por: PartitionOffsetsResponse =>
>>             if (por.error == ErrorMapping.NoError) {
>> ...
>>             } else {
>>               errs.append(ErrorMapping.exceptionFor(por.error))
>>             }
>> There should be some error other than "Couldn't find leader offsets for
>> Set()"
>>
>> Can you check again ?
>>
>> Thanks
>>
>> On Thu, Mar 19, 2015 at 12:10 PM, Alberto Rodriguez <ardlema@gmail.com>
>> wrote:
>>
>> > Hi all,
>> >
>> > I am trying to make the new kafka and spark streaming integration work
>> > (direct
>> > approach "no receivers"
>> > <http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html>).
>> I
>> > have created an unit test where I configure and start both zookeeper and
>> > kafka.
>> >
>> > When I try to create the InputDStream using the createDirectStream
>> method
>> > of the KafkaUtils class I am getting the following error:
>> >
>> > org.apache.spark.SparkException:* Couldn't find leader offsets for
>> Set()*
>> > org.apache.spark.SparkException: org.apache.spark.SparkException:
>> Couldn't
>> > find leader offsets for Set()
>> > at
>> >
>> >
>> org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)
>> >
>> > Following is the code that tries to create the DStream:
>> >
>> > val messages: InputDStream[(String, String)] =
>> > KafkaUtils.createDirectStream[String, String, StringDecoder,
>> > StringDecoder](
>> >         ssc, kafkaParams, topics)
>> >
>> > Does anyone faced this problem?
>> >
>> > Thank you in advance.
>> >
>> > Kind regards,
>> >
>> > Alberto
>> >
>>
>
>

--20cf307d06d216ad6f0511ab04f8--

From dev-return-12070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 22:08:16 2015
Return-Path: <dev-return-12070-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5732B177B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 22:08:16 +0000 (UTC)
Received: (qmail 83350 invoked by uid 500); 19 Mar 2015 22:08:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83260 invoked by uid 500); 19 Mar 2015 22:08:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83248 invoked by uid 99); 19 Mar 2015 22:08:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 22:08:11 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.215.53 as permitted sender)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 22:08:06 +0000
Received: by ladw1 with SMTP id w1so73533036lad.0
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 15:06:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=uKOymZkx//J/Wi0Lbk1zWFfVeMBhpvrAiWv+lr9R7p0=;
        b=JC18AjaArMcg7aqst/GHKSvydZUr5ZQgQ9Mhk3plL+5uzqlvuL3mw2RxpE82LaZsQA
         musCrXQII5RO032377GTcogfA3kx0aJlcrqgwa1ovODGtKVm1nP2DQ8eozidFX33J7jQ
         rLfEo5hjpC6ddcVfiAYWmYjqEzpDzmcNzvw5uioCl1F0GfGP0TCakSkx6ajUPQwRdUCF
         0BmDyWbWPciRqTM/QJRyQMkDJaeUl6x4XGL4fvz4R+n1zSNtm+vJres3makRxiBf13PV
         /0GiCHvWjcfyzWdFsgEVQ6C08dKZTqSlqa+U+m7A183WRPhq9Mv3RFZ3LgRIj1P8Y7mo
         aDHA==
MIME-Version: 1.0
X-Received: by 10.112.212.106 with SMTP id nj10mr51682325lbc.36.1426802819785;
 Thu, 19 Mar 2015 15:06:59 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Thu, 19 Mar 2015 15:06:59 -0700 (PDT)
In-Reply-To: <FD914DA8-C1E7-48CA-B1CE-F6597265901B@hp.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
	<CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
	<09B9C296-EBE4-48C6-8B59-4BD26EDB6F3D@hp.com>
	<CA+B-+fycWb5JCO1yA2Kh970g+7cAORm_UpQYjpC5OadKfFkx3w@mail.gmail.com>
	<FD914DA8-C1E7-48CA-B1CE-F6597265901B@hp.com>
Date: Thu, 19 Mar 2015 15:06:59 -0700
Message-ID: <CA+B-+fxSC6TA6KU81KXuFpvf1NnDpsfyMcUzAs7_8J_nJzrZ2w@mail.gmail.com>
Subject: Re: Which linear algebra interface to use within Spark MLlib?
From: Debasish Das <debasish.das83@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11346dda3ce8280511ab6b5b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11346dda3ce8280511ab6b5b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yeah it will be better if we consolidate the development on one of
them...either Breeze or mllib.BLAS...

On Thu, Mar 19, 2015 at 2:25 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
>
wrote:

>  Thanks for quick response.
>
>  I can use linealg.BLAS.gemm, and this means that I have to use MLlib
> Matrix. The latter does not support some useful functionality needed for
> optimization. For example, creation of Matrix given matrix size, array an=
d
> offset in this array. This means that I will need to create matrix in
> Breeze and convert it to MLlib. Also, linalg.BLAS misses some useful BLAS
> functions I need, that can be found in Breeze (and netlib-java). The same
> concerns are applicable to MLlib Vector.
>
> Best regards, Alexander
>
> 19.03.2015, =D0=B2 14:16, "Debasish Das" <debasish.das83@gmail.com> =D0=
=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
>
>   I think for Breeze we are focused on dot and dgemv right now (along
> with several other matrix vector style operations)...
>
>  For dgemm it is tricky since you need to do add dgemm for both
> DenseMatrix and CSCMatrix...and for CSCMatrix you need to get something
> like SuiteSparse which is under lgpl...so we have to think more on it..
>
>  For now can't you use dgemm directly from mllib.linalg.BLAS ? It's in
> master...
>
>
> On Thu, Mar 19, 2015 at 1:49 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
>
>>  Thank you! When do you expect to have gemm in Breeze and that version
>> of Breeze to ship with MLlib?
>>
>>  Also, could someone please elaborate on the linalg.BLAS and Matrix? Are
>> they going to be developed further, should in long term all developers u=
se
>> them?
>>
>> Best regards, Alexander
>>
>> 18.03.2015, =D0=B2 23:21, "Debasish Das" <debasish.das83@gmail.com>
>> =D0=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
>>
>>    dgemm dgemv and dot come to Breeze and Spark through netlib-java....
>>
>>  Right now both in dot and dgemv Breeze does a extra memory allocate but
>> we already found the issue and we are working on adding a common trait t=
hat
>> will provide a sink operation (basically memory will be allocated by
>> user)...adding more BLAS operators in breeze will also help in general a=
s
>> lot more operations are defined over there...
>>
>>
>> On Wed, Mar 18, 2015 at 8:09 PM, Ulanov, Alexander <
>> alexander.ulanov@hp.com> wrote:
>>
>>> Hi,
>>>
>>> Currently I am using Breeze within Spark MLlib for linear algebra. I
>>> would like to reuse previously allocated matrices for storing the resul=
t of
>>> matrices multiplication, i.e. I need to use "gemm" function C:=3Dq*A*B+=
p*C,
>>> which is missing in Breeze (Breeze automatically allocates a new matrix=
 to
>>> store the result of multiplication). Also, I would like to minimize gem=
m
>>> calls that Breeze does. Should I use mllib.linalg.BLAS functions instea=
d?
>>> While it has gemm and axpy, it has rather limited number of operations.=
 For
>>> example, I need sum of the matrix by row or by columns, or applying a
>>> function to all elements in a matrix. Also, MLlib Vector and Matrix
>>> interfaces that linalg.BLAS operates seems to be rather undeveloped. Sh=
ould
>>> I use plain netlib-java instead (will it remain in MLlib in future
>>> releases)?
>>>
>>> Best regards, Alexander
>>>
>>
>>
>

--001a11346dda3ce8280511ab6b5b--

From dev-return-12071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 19 22:09:34 2015
Return-Path: <dev-return-12071-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 188DE177CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 19 Mar 2015 22:09:34 +0000 (UTC)
Received: (qmail 88659 invoked by uid 500); 19 Mar 2015 22:09:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88583 invoked by uid 500); 19 Mar 2015 22:09:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88572 invoked by uid 99); 19 Mar 2015 22:09:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 22:09:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 19 Mar 2015 22:09:28 +0000
Received: by obcxo2 with SMTP id xo2so65401188obc.0
        for <dev@spark.apache.org>; Thu, 19 Mar 2015 15:08:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=8vKbLmALQePkY1EZQHkPe/QbwZdlFKQB14DJsCpYnow=;
        b=YwxiNhaRrnTNlYkfX4c6ay7hxXSvCEAiD8Tj/7eSUkDgzcIll2wNRNPAEVksYpxcN6
         CUHNGjxsFQ9u56/YxaimO51G4yRrk4xOj4bpMqLVEWAwQGMa7luASwX1W64vvxCP+al8
         uxdW8bD72oNcIIXXIHq2HgD5pGnk4nlwGlVquiu7Ja8aN7lBxgz9CMeQjt/C1bfaX+wS
         7hkeknjrIIvrNbe70E1e5n71MHsDcDcyLiG36DbGBfbyLccevFy54eQigS3mpH61Wl2k
         1ou6ttQJwVwNM7NUmjlYNwzFaQs+Wp0pD0pY1KNcRzbGxzAbQe32qhl6lK5+xXsPHZo+
         wQcA==
X-Gm-Message-State: ALoCoQncFl2CwebfgB3O5vKHz2Rf0V7GKiQwFHzvbR9xzSqZAx5tC2OagLgtclayaJ0T6xehzuaK
MIME-Version: 1.0
X-Received: by 10.182.22.167 with SMTP id e7mr62034103obf.31.1426802927832;
 Thu, 19 Mar 2015 15:08:47 -0700 (PDT)
Received: by 10.76.87.36 with HTTP; Thu, 19 Mar 2015 15:08:47 -0700 (PDT)
In-Reply-To: <CAMjNSowQ4FhxUzpqQ9etUDC2xgJTuRO=HBPyhh-m3Q_XyqGBRg@mail.gmail.com>
References: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
	<CALte62ycYF4PPZkp-62DBdUy13U2aQnvOecae6LQdqArjAwxuQ@mail.gmail.com>
	<CAKWX9VWSbZhoh5KS4+grsQ-tMO5cgrnQ7LFSRisL2D2KqMNaPQ@mail.gmail.com>
	<CAMjNSowQ4FhxUzpqQ9etUDC2xgJTuRO=HBPyhh-m3Q_XyqGBRg@mail.gmail.com>
Date: Thu, 19 Mar 2015 17:08:47 -0500
Message-ID: <CAKWX9VXpLxaOD5VjsQtzPeq4x8VGwYvPOi7QHYegCmFkBpmVjA@mail.gmail.com>
Subject: Re: Exception using the new createDirectStream util method
From: Cody Koeninger <cody@koeninger.org>
To: Alberto Rodriguez <ardlema@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e0baada7590511ab714f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e0baada7590511ab714f
Content-Type: text/plain; charset=UTF-8

Yeah, I wouldn't be shocked if Kafka's metadata apis didn't return results
for topics that don't have any messages.  (sorry about the triple negative,
but I think you get my meaning).

Try putting a message in the topic and seeing what happens.

On Thu, Mar 19, 2015 at 4:38 PM, Alberto Rodriguez <ardlema@gmail.com>
wrote:

> Thank you for replying,
>
> Ted, I have been debuging and the getLeaderOffsets method is not appending
> errors because the method findLeaders that is called at the first line of
> getLeaderOffsets is not returning leaders.
>
> Cody, the topics do not have any messages yet. Could this be an issue??
>
> If you guys want to have a look at the code I've just uploaded it to my
> github account: big-brother <https://github.com/ardlema/big-brother> (see
> DirectKafkaWordCountTest.scala).
>
> Thank you again!!
>
> 2015-03-19 22:13 GMT+01:00 Cody Koeninger <cody@koeninger.org>:
>
> > What is the value of your topics variable, and does it correspond to
> > topics that already exist on the cluster and have messages in them?
> >
> > On Thu, Mar 19, 2015 at 3:10 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >
> >> Looking at KafkaCluster#getLeaderOffsets():
> >>
> >>           respMap.get(tp).foreach { por: PartitionOffsetsResponse =>
> >>             if (por.error == ErrorMapping.NoError) {
> >> ...
> >>             } else {
> >>               errs.append(ErrorMapping.exceptionFor(por.error))
> >>             }
> >> There should be some error other than "Couldn't find leader offsets for
> >> Set()"
> >>
> >> Can you check again ?
> >>
> >> Thanks
> >>
> >> On Thu, Mar 19, 2015 at 12:10 PM, Alberto Rodriguez <ardlema@gmail.com>
> >> wrote:
> >>
> >> > Hi all,
> >> >
> >> > I am trying to make the new kafka and spark streaming integration work
> >> > (direct
> >> > approach "no receivers"
> >> > <http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html
> >).
> >> I
> >> > have created an unit test where I configure and start both zookeeper
> and
> >> > kafka.
> >> >
> >> > When I try to create the InputDStream using the createDirectStream
> >> method
> >> > of the KafkaUtils class I am getting the following error:
> >> >
> >> > org.apache.spark.SparkException:* Couldn't find leader offsets for
> >> Set()*
> >> > org.apache.spark.SparkException: org.apache.spark.SparkException:
> >> Couldn't
> >> > find leader offsets for Set()
> >> > at
> >> >
> >> >
> >>
> org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)
> >> >
> >> > Following is the code that tries to create the DStream:
> >> >
> >> > val messages: InputDStream[(String, String)] =
> >> > KafkaUtils.createDirectStream[String, String, StringDecoder,
> >> > StringDecoder](
> >> >         ssc, kafkaParams, topics)
> >> >
> >> > Does anyone faced this problem?
> >> >
> >> > Thank you in advance.
> >> >
> >> > Kind regards,
> >> >
> >> > Alberto
> >> >
> >>
> >
> >
>

--001a11c2e0baada7590511ab714f--

From dev-return-12072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 01:58:05 2015
Return-Path: <dev-return-12072-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 875D917317
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 01:58:05 +0000 (UTC)
Received: (qmail 77019 invoked by uid 500); 20 Mar 2015 01:57:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76943 invoked by uid 500); 20 Mar 2015 01:57:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76924 invoked by uid 99); 20 Mar 2015 01:57:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 01:57:30 +0000
X-ASF-Spam-Status: No, hits=5.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_PSBL,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kaka_1992@163.com designates 220.181.13.20 as permitted sender)
Received: from [220.181.13.20] (HELO m13-20.163.com) (220.181.13.20)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 01:57:23 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=uaWyf
	fJDPM+hhnvubFsYVLuYB7qFWgrlcKeKJ/nU654=; b=lpuwQ4mcODTvMW8AyhT+f
	Uk3eL64ySdKlcDVyr2ERuaXskjSpNuE1ceeZBQxtht79ejMBh9Mto6FAE1GkZyau
	sT8arV5TXeUTd1T3FIppRHEQ0w+hGvYp5hGz5/5t6M8bVfYBj4SxVKaGPc/c7BAz
	D5V/X3rVtFoXnZfvrbXhCM=
Received: from kaka_1992$163.com ( [182.92.253.26] ) by ajax-webmail-wmsvr20
 (Coremail) ; Fri, 20 Mar 2015 09:56:10 +0800 (CST)
X-Originating-IP: [182.92.253.26]
Date: Fri, 20 Mar 2015 09:56:10 +0800 (CST)
From: "A.M.Chan" <kaka_1992@163.com>
To: spark-dev <dev@spark.apache.org>
Subject: Add Char support in SQL dataTypes
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20150119(59087.7062) Copyright (c) 2002-2015 www.mailtech.cn 163com
X-CM-CTRLDATA: WhRJD2Zvb3Rlcl9odG09MzI5Nzo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_325627_1974646981.1426816570761"
MIME-Version: 1.0
Message-ID: <1f45e78d.14177.14c34e51589.Coremail.kaka_1992@163.com>
X-CM-TRANSID:FMGowABnOlA7fgtVcWh6AA--.25054W
X-CM-SenderInfo: 5ndntsarzzjqqrwthudrp/xtbBERvCbFD+bZgAdAACsJ
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_325627_1974646981.1426816570761
Content-Type: text/plain; charset=GBK
Content-Transfer-Encoding: base64

Y2FzZSBjbGFzcyBQcmltaXRpdmVEYXRhKAogICAgY2hhckZpZWxkOiBDaGFyLCAvLyBDYW4ndCBn
ZXQgdGhlIGNoYXIgc2NoZW1hIGluZm8KICAgIGludEZpZWxkOiBJbnQsCiAgICBsb25nRmllbGQ6
IExvbmcsCiAgICBkb3VibGVGaWVsZDogRG91YmxlLAogICAgZmxvYXRGaWVsZDogRmxvYXQsCiAg
ICBzaG9ydEZpZWxkOiBTaG9ydCwKICAgIGJ5dGVGaWVsZDogQnl0ZSwKCiAgICBib29sZWFuRmll
bGQ6IEJvb2xlYW4pCkkgY2FuJ3QgZ2V0IHRoZSBzY2hlbWEgZnJvbSBjYXNlIGNsYXNzIFByaW1p
dGl2ZURhdGEuCkFuIGVycm9yIG9jY3VycmVkIHdoaWxlIEkgdXNlIHNjaGVtYUZvcltQcmltaXRp
dmVEYXRhXQpDaGFyIChvZiBjbGFzcyBzY2FsYS5yZWZsZWN0LmludGVybmFsLlR5cGVzJFR5cGVS
ZWYkJGFub24kNikKc2NhbGEuTWF0Y2hFcnJvcjogQ2hhciAob2YgY2xhc3Mgc2NhbGEucmVmbGVj
dC5pbnRlcm5hbC5UeXBlcyRUeXBlUmVmJCRhbm9uJDYpCmF0IG9yZy5hcGFjaGUuc3Bhcmsuc3Fs
LmNhdGFseXN0LlNjYWxhUmVmbGVjdGlvbiRjbGFzcy5zY2hlbWFGb3IoU2NhbGFSZWZsZWN0aW9u
LnNjYWxhOjExMikKCgoKCgotLQoKa2FrYTE5OTI=
------=_Part_325627_1974646981.1426816570761--


From dev-return-12073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 04:22:48 2015
Return-Path: <dev-return-12073-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20B5B17671
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 04:22:48 +0000 (UTC)
Received: (qmail 53367 invoked by uid 500); 20 Mar 2015 04:22:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53278 invoked by uid 500); 20 Mar 2015 04:22:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53267 invoked by uid 99); 20 Mar 2015 04:22:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 04:22:46 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hao.cheng@intel.com designates 192.55.52.88 as permitted sender)
Received: from [192.55.52.88] (HELO mga01.intel.com) (192.55.52.88)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 04:22:21 +0000
Received: from orsmga002.jf.intel.com ([10.7.209.21])
  by fmsmga101.fm.intel.com with ESMTP; 19 Mar 2015 21:21:18 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.11,434,1422950400"; 
   d="scan'208";a="701469066"
Received: from kmsmsx151.gar.corp.intel.com ([172.21.73.86])
  by orsmga002.jf.intel.com with ESMTP; 19 Mar 2015 21:21:17 -0700
Received: from shsmsx103.ccr.corp.intel.com (10.239.4.69) by
 KMSMSX151.gar.corp.intel.com (172.21.73.86) with Microsoft SMTP Server (TLS)
 id 14.3.224.2; Fri, 20 Mar 2015 12:20:50 +0800
Received: from shsmsx102.ccr.corp.intel.com ([169.254.2.198]) by
 SHSMSX103.ccr.corp.intel.com ([169.254.4.108]) with mapi id 14.03.0224.002;
 Fri, 20 Mar 2015 12:20:49 +0800
From: "Cheng, Hao" <hao.cheng@intel.com>
To: A.M.Chan <kaka_1992@163.com>, spark-dev <dev@spark.apache.org>
Subject: RE: Add Char support in SQL dataTypes
Thread-Topic: Add Char support in SQL dataTypes
Thread-Index: AQHQYrFJbFYTr3XUtUCf/vw+nKRMfZ0kxE7A
Date: Fri, 20 Mar 2015 04:20:49 +0000
Message-ID: <80833ADD533E324CA05C160E41B6366102866D5D@shsmsx102.ccr.corp.intel.com>
References: <1f45e78d.14177.14c34e51589.Coremail.kaka_1992@163.com>
In-Reply-To: <1f45e78d.14177.14c34e51589.Coremail.kaka_1992@163.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Can you use the Varchar or String instead? Currently, Spark SQL will conver=
t the varchar into string type internally(without max length limitation). H=
owever, "char" type is not supported yet.

-----Original Message-----
From: A.M.Chan [mailto:kaka_1992@163.com]=20
Sent: Friday, March 20, 2015 9:56 AM
To: spark-dev
Subject: Add Char support in SQL dataTypes

case class PrimitiveData(
    charField: Char, // Can't get the char schema info
    intField: Int,
    longField: Long,
    doubleField: Double,
    floatField: Float,
    shortField: Short,
    byteField: Byte,

    booleanField: Boolean)
I can't get the schema from case class PrimitiveData.
An error occurred while I use schemaFor[PrimitiveData] Char (of class scala=
.reflect.internal.Types$TypeRef$$anon$6)
scala.MatchError: Char (of class scala.reflect.internal.Types$TypeRef$$anon=
$6)
at org.apache.spark.sql.catalyst.ScalaReflection$class.schemaFor(ScalaRefle=
ction.scala:112)





--

kaka1992

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 07:45:21 2015
Return-Path: <dev-return-12074-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C37E17B0C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 07:45:21 +0000 (UTC)
Received: (qmail 60325 invoked by uid 500); 20 Mar 2015 07:45:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60248 invoked by uid 500); 20 Mar 2015 07:45:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60236 invoked by uid 99); 20 Mar 2015 07:45:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 07:45:20 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ardlema@gmail.com designates 209.85.213.44 as permitted sender)
Received: from [209.85.213.44] (HELO mail-yh0-f44.google.com) (209.85.213.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 07:44:55 +0000
Received: by yhpt93 with SMTP id t93so36328535yhp.0
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 00:44:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:cc
         :content-type;
        bh=hK4RCCiEYUQAbz3hSi++Qp96nKamxMICYlBkYzIh2bw=;
        b=hbzvQSCs++bdhn9cotEQLZepoh5iLCqoIqz9W34FzxWOlWZkNzgL+Kv4j6xjWhpSFQ
         7v63c+BpXdsMxXrBThvEmd8wWjGUAKb8DUYmTNug30E00YkMxbr3aDfDQKe9AiMSg3wJ
         wV5mhrlEONhATG91WOZoiCi1xIGHj44He82oPd8yEebhMCQbpO1UXEfcaFz8eMufcfJ5
         GKa4g1EeVqSkW/AGGA1K6JrDIw2oB3SvtitCs8/dXQSVOAmioNt+pfeF09zDHZ9MkcS2
         vfsakpjwQqerlPjHmsF1T/eHRr/ZvSGUnrMHzUHEgnnw033EniBQukt2H8kP7cZn1UeJ
         3F3g==
MIME-Version: 1.0
X-Received: by 10.52.237.232 with SMTP id vf8mr34271099vdc.86.1426837493540;
 Fri, 20 Mar 2015 00:44:53 -0700 (PDT)
Received: by 10.52.120.77 with HTTP; Fri, 20 Mar 2015 00:44:53 -0700 (PDT)
In-Reply-To: <CAKWX9VXpLxaOD5VjsQtzPeq4x8VGwYvPOi7QHYegCmFkBpmVjA@mail.gmail.com>
References: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
	<CALte62ycYF4PPZkp-62DBdUy13U2aQnvOecae6LQdqArjAwxuQ@mail.gmail.com>
	<CAKWX9VWSbZhoh5KS4+grsQ-tMO5cgrnQ7LFSRisL2D2KqMNaPQ@mail.gmail.com>
	<CAMjNSowQ4FhxUzpqQ9etUDC2xgJTuRO=HBPyhh-m3Q_XyqGBRg@mail.gmail.com>
	<CAKWX9VXpLxaOD5VjsQtzPeq4x8VGwYvPOi7QHYegCmFkBpmVjA@mail.gmail.com>
Date: Fri, 20 Mar 2015 08:44:53 +0100
Message-ID: <CAMjNSowqQMX11MjEVg5f_0=m8uhnDLh-U7jBtiCZ-E=1ZGgisg@mail.gmail.com>
Subject: Re: Exception using the new createDirectStream util method
From: Alberto Rodriguez <ardlema@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013c6928f46e840511b37d86
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013c6928f46e840511b37d86
Content-Type: text/plain; charset=UTF-8

You were absolutely right Cody!! I have just put a message in the kafka
topic before creating the DirectStream and now is working fine!

Do you think that I should open an issue to warn that the kafka topic must
contain at least one message before the DirectStream creation?

Thank you very much! You've just made my day ;)

2015-03-19 23:08 GMT+01:00 Cody Koeninger <cody@koeninger.org>:

> Yeah, I wouldn't be shocked if Kafka's metadata apis didn't return results
> for topics that don't have any messages.  (sorry about the triple negative,
> but I think you get my meaning).
>
> Try putting a message in the topic and seeing what happens.
>
> On Thu, Mar 19, 2015 at 4:38 PM, Alberto Rodriguez <ardlema@gmail.com>
> wrote:
>
>> Thank you for replying,
>>
>> Ted, I have been debuging and the getLeaderOffsets method is not appending
>> errors because the method findLeaders that is called at the first line of
>> getLeaderOffsets is not returning leaders.
>>
>> Cody, the topics do not have any messages yet. Could this be an issue??
>>
>> If you guys want to have a look at the code I've just uploaded it to my
>> github account: big-brother <https://github.com/ardlema/big-brother> (see
>>
>> DirectKafkaWordCountTest.scala).
>>
>> Thank you again!!
>>
>> 2015-03-19 22:13 GMT+01:00 Cody Koeninger <cody@koeninger.org>:
>>
>> > What is the value of your topics variable, and does it correspond to
>> > topics that already exist on the cluster and have messages in them?
>> >
>> > On Thu, Mar 19, 2015 at 3:10 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> >
>> >> Looking at KafkaCluster#getLeaderOffsets():
>> >>
>> >>           respMap.get(tp).foreach { por: PartitionOffsetsResponse =>
>> >>             if (por.error == ErrorMapping.NoError) {
>> >> ...
>> >>             } else {
>> >>               errs.append(ErrorMapping.exceptionFor(por.error))
>> >>             }
>> >> There should be some error other than "Couldn't find leader offsets for
>> >> Set()"
>> >>
>> >> Can you check again ?
>> >>
>> >> Thanks
>> >>
>> >> On Thu, Mar 19, 2015 at 12:10 PM, Alberto Rodriguez <ardlema@gmail.com
>> >
>> >> wrote:
>> >>
>> >> > Hi all,
>> >> >
>> >> > I am trying to make the new kafka and spark streaming integration
>> work
>> >> > (direct
>> >> > approach "no receivers"
>> >> > <http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html
>> >).
>> >> I
>> >> > have created an unit test where I configure and start both zookeeper
>> and
>> >> > kafka.
>> >> >
>> >> > When I try to create the InputDStream using the createDirectStream
>> >> method
>> >> > of the KafkaUtils class I am getting the following error:
>> >> >
>> >> > org.apache.spark.SparkException:* Couldn't find leader offsets for
>> >> Set()*
>> >> > org.apache.spark.SparkException: org.apache.spark.SparkException:
>> >> Couldn't
>> >> > find leader offsets for Set()
>> >> > at
>> >> >
>> >> >
>> >>
>> org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)
>> >> >
>> >> > Following is the code that tries to create the DStream:
>> >> >
>> >> > val messages: InputDStream[(String, String)] =
>> >> > KafkaUtils.createDirectStream[String, String, StringDecoder,
>> >> > StringDecoder](
>> >> >         ssc, kafkaParams, topics)
>> >> >
>> >> > Does anyone faced this problem?
>> >> >
>> >> > Thank you in advance.
>> >> >
>> >> > Kind regards,
>> >> >
>> >> > Alberto
>> >> >
>> >>
>> >
>> >
>>
>
>

--089e013c6928f46e840511b37d86--

From dev-return-12075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 07:51:30 2015
Return-Path: <dev-return-12075-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D46CB17B1A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 07:51:30 +0000 (UTC)
Received: (qmail 65130 invoked by uid 500); 20 Mar 2015 07:51:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65036 invoked by uid 500); 20 Mar 2015 07:51:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65023 invoked by uid 99); 20 Mar 2015 07:51:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 07:51:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vikas.v.iitr@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 07:51:23 +0000
Received: by qcbjx9 with SMTP id jx9so48254560qcb.0
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 00:50:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=wXQaSpBRaft6rAEwzgoHnQ4MAfgnEjg06edUHztJyUU=;
        b=cOoh5P0tuWfecdldOSk2FKgEGkcuaHY5VyDg1xjVtKFmTWLmcdSoCBJoAwbQH1sQ4q
         QSPD3SRtJviHE/QTDFCS5nJ6ePuVCN62zK0Y1i1WHeilsdkSgWaIr/4XQ91V53jalnPQ
         GKSxTblK5IgaHLttskCGS9M/xIZhRx9+p5J+qIRaiyor+YACmm5q9ph9Sjx/gQUI2KRT
         bwlYsy/JhG0/CAvrMdhlqcGyAB5VwcxEJH0AbfodwzYfrJJwQXtLeEuCodGxHfN+L5MY
         6MQliKJvZQnK4C5O2vAZl+E6C7+yJlelUjr8KCb5NzA8MZapTF7jQlVdrxm475XK9OfH
         blNQ==
MIME-Version: 1.0
X-Received: by 10.55.55.4 with SMTP id e4mr149936181qka.97.1426837818464; Fri,
 20 Mar 2015 00:50:18 -0700 (PDT)
Received: by 10.229.211.2 with HTTP; Fri, 20 Mar 2015 00:50:18 -0700 (PDT)
Received: by 10.229.211.2 with HTTP; Fri, 20 Mar 2015 00:50:18 -0700 (PDT)
Date: Fri, 20 Mar 2015 13:20:18 +0530
Message-ID: <CABFxkcDoGJco4kdEgRV7N=GWPHAUic5QpYB2uC+NmFRaCskG7A@mail.gmail.com>
Subject: Contributing to Spark
From: "vikas.v.iitr@gmail.com" <vikas.v.iitr@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1149047e525e3f0511b39133
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1149047e525e3f0511b39133
Content-Type: text/plain; charset=UTF-8

Hi ,

I have read and gone through most Spark tutorials and materials out there.
I have also downloaded and build the spark code base .

Can someone point me to some existing Jira where I can start contributing ?
Eventually I want to do some good contribution to mlLib .

Thanks,
Vikas

--001a1149047e525e3f0511b39133--

From dev-return-12076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 08:46:17 2015
Return-Path: <dev-return-12076-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A053217C90
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 08:46:17 +0000 (UTC)
Received: (qmail 54939 invoked by uid 500); 20 Mar 2015 08:45:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54848 invoked by uid 500); 20 Mar 2015 08:45:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54837 invoked by uid 99); 20 Mar 2015 08:45:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 08:45:54 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tanyinyan@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 08:45:50 +0000
Received: from 172.24.2.119 (EHLO SZXEMA412-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id BDK21221;
	Fri, 20 Mar 2015 16:45:27 +0800 (CST)
Received: from SZXEMA502-MBS.china.huawei.com ([169.254.4.144]) by
 SZXEMA412-HUB.china.huawei.com ([10.82.72.71]) with mapi id 14.03.0158.001;
 Fri, 20 Mar 2015 16:42:23 +0800
From: Tanyinyan <tanyinyan@huawei.com>
To: "vikas.v.iitr@gmail.com" <vikas.v.iitr@gmail.com>,
        "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: =?utf-8?B?562U5aSNOiBDb250cmlidXRpbmcgdG8gU3Bhcms=?=
Thread-Topic: Contributing to Spark
Thread-Index: AQHQYuKxGqIX1TOa4kWOflAOPjIQ350lDE7g
Date: Fri, 20 Mar 2015 08:42:22 +0000
Message-ID: <7E03A627485F1B4BBEE6AB8979E8459C9B0A62DF@SZXEMA502-MBS.china.huawei.com>
References: <CABFxkcDoGJco4kdEgRV7N=GWPHAUic5QpYB2uC+NmFRaCskG7A@mail.gmail.com>
In-Reply-To: <CABFxkcDoGJco4kdEgRV7N=GWPHAUic5QpYB2uC+NmFRaCskG7A@mail.gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: zh-CN
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.63.187.28]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A020203.550BDE28.004D,ss=1,re=0.001,recu=0.000,reip=0.000,cl=1,cld=1,fgs=0,
	ip=169.254.4.144,
	so=2013-05-26 15:14:31,
	dmn=2013-03-21 17:37:32
X-Mirapoint-Loop-Id: 6d3741a621aeef5d335bbd36d9f37d68
X-Virus-Checked: Checked by ClamAV on apache.org

SGVsbG8gVmlrYXMsDQoNClRoZXNlIHR3byBsaW5rcyBtYXliZSB3aGF0IHlvdSB3YW50Lg0KDQpq
aXJhOiAgaHR0cHM6Ly9pc3N1ZXMuYXBhY2hlLm9yZy9qaXJhL2Jyb3dzZS9TUEFSSy8/c2VsZWN0
ZWRUYWI9Y29tLmF0bGFzc2lhbi5qaXJhLmppcmEtcHJvamVjdHMtcGx1Z2luOnN1bW1hcnktcGFu
ZWwNCg0KcHVsbCByZXF1ZXN0OiAgaHR0cHM6Ly9naXRodWIuY29tL2FwYWNoZS9zcGFyay9wdWxs
cw0KDQpSZWdhcmRzLA0KDQpKZXNzaWNhDQoNCg0KLS0tLS3pgq7ku7bljp/ku7YtLS0tLQ0K5Y+R
5Lu25Lq6OiB2aWthcy52LmlpdHJAZ21haWwuY29tIFttYWlsdG86dmlrYXMudi5paXRyQGdtYWls
LmNvbV0gDQrlj5HpgIHml7bpl7Q6IDIwMTXlubQz5pyIMjDml6UgMTU6NTANCuaUtuS7tuS6ujog
ZGV2QHNwYXJrLmFwYWNoZS5vcmcNCuS4u+mimDogQ29udHJpYnV0aW5nIHRvIFNwYXJrDQoNCkhp
ICwNCg0KSSBoYXZlIHJlYWQgYW5kIGdvbmUgdGhyb3VnaCBtb3N0IFNwYXJrIHR1dG9yaWFscyBh
bmQgbWF0ZXJpYWxzIG91dCB0aGVyZS4NCkkgaGF2ZSBhbHNvIGRvd25sb2FkZWQgYW5kIGJ1aWxk
IHRoZSBzcGFyayBjb2RlIGJhc2UgLg0KDQpDYW4gc29tZW9uZSBwb2ludCBtZSB0byBzb21lIGV4
aXN0aW5nIEppcmEgd2hlcmUgSSBjYW4gc3RhcnQgY29udHJpYnV0aW5nID8NCkV2ZW50dWFsbHkg
SSB3YW50IHRvIGRvIHNvbWUgZ29vZCBjb250cmlidXRpb24gdG8gbWxMaWIgLg0KDQpUaGFua3Ms
DQpWaWthcw0K
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-12077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 09:14:51 2015
Return-Path: <dev-return-12077-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A6E1F17DA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 09:14:51 +0000 (UTC)
Received: (qmail 30743 invoked by uid 500); 20 Mar 2015 09:14:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30655 invoked by uid 500); 20 Mar 2015 09:14:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30643 invoked by uid 99); 20 Mar 2015 09:14:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 09:14:47 +0000
X-ASF-Spam-Status: No, hits=3.7 required=10.0
	tests=HTML_IMAGE_ONLY_12,HTML_IMAGE_RATIO_04,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of niranda.perera@gmail.com designates 209.85.214.176 as permitted sender)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 09:14:17 +0000
Received: by obdfc2 with SMTP id fc2so73802386obd.3
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 02:13:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=yoGXO0SwCuoPUmkRg+YM2qDIyOTanmeiFLGgOK8U5Uo=;
        b=gdmApod/ocQ61cHoWaaZR71/Jj9bcMjki8kOr9DwlvTZB1rCH15ZYKiInL1gylUAWc
         r9vORie6MNqM2NhHt1siYPlnRKYTXNFTI8/AkC4rWM75i+DeJXYEYXHFOklm7H6/t+Ii
         mGMFN6rzW3kytBXoXF4dECcRkWF+/Y7U0C/uPp7FlLS0djEc2ELPtLm3P9xgh/RxiHwt
         bAV2OeEgpL3LwtgbDIyN/RlhN95MWj41pjZPikCGQy4IT7ST1L8nkDLEOuz2lF/L6pQr
         IcMAp+1lL5aOnDmXXXqp+QODuFT2OtjxvcARuCIcO69TSqKGUV0qOZu37IDlRgz1nX9P
         APKA==
MIME-Version: 1.0
X-Received: by 10.202.57.195 with SMTP id g186mr62017942oia.86.1426842809973;
 Fri, 20 Mar 2015 02:13:29 -0700 (PDT)
Received: by 10.182.58.69 with HTTP; Fri, 20 Mar 2015 02:13:29 -0700 (PDT)
Date: Fri, 20 Mar 2015 14:43:29 +0530
Message-ID: <CANCoaU6DkfsLfpdXti73rKU-6-Bgqyyv7JjWsYxD=4CBYgXJ7g@mail.gmail.com>
Subject: Connecting a worker to the master after a spark context is made
From: Niranda Perera <niranda.perera@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/related; boundary=001a113cfe5cd8302d0511b4ba26
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113cfe5cd8302d0511b4ba26
Content-Type: multipart/alternative; boundary=001a113cfe5cd8302a0511b4ba25

--001a113cfe5cd8302a0511b4ba25
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

Please consider the following scenario.

I've started the spark master by invoking
the org.apache.spark.deploy.master.Master.startSystemAndActor method in a
java code and connected a worker to it using
the org.apache.spark.deploy.worker.Worker.startSystemAndActor method. and
then I have successfully created a java spark & SQL contexts and performed
SQL queries.

My question is, can I change this order?
Can I start the master first, then create a spark context... and later on
connect a worker to the master?

While trying out this scenario, I have successfully started the master.
Please see the screenshot here.



But when I create an spark context, it terminates automatically. is it
because the master not being connected to a worker?

cheers


--=20
Niranda
=E2=80=8B

--001a113cfe5cd8302a0511b4ba25
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Hi,=C2=A0<div><br></div><div>Please consider the following=
 scenario.=C2=A0</div><div><br></div><div>I&#39;ve started the spark master=
 by invoking the=C2=A0org.apache.spark.deploy.master.Master.startSystemAndA=
ctor method in a java code and connected a worker to it using the=C2=A0org.=
apache.spark.deploy.worker.Worker.startSystemAndActor method. and then I ha=
ve successfully created a java spark &amp; SQL contexts and performed SQL q=
ueries.=C2=A0</div><div><br></div><div>My question is, can I change this or=
der?=C2=A0</div><div>Can I start the master first, then create a spark cont=
ext... and later on connect a worker to the master?=C2=A0</div><div><br></d=
iv><div>While trying out this scenario, I have successfully started the mas=
ter. Please see the screenshot here.=C2=A0</div><div><img src=3D"cid:ii_i7h=
d8km10_14c36721c3a0291e" width=3D"544" height=3D"306"><br></div><div><br></=
div><div><br></div><div>But when I create an spark context, it terminates a=
utomatically. is it because the master not being connected to a worker?</di=
v><div><br></div><div>cheers</div><div><br></div><div><br>-- <br><div><div =
dir=3D"ltr"><div><div dir=3D"ltr"><div>Niranda <br>=E2=80=8B<br></div></div=
></div></div></div>
</div></div>

--001a113cfe5cd8302a0511b4ba25--
--001a113cfe5cd8302d0511b4ba26--

From dev-return-12078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 09:19:46 2015
Return-Path: <dev-return-12078-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9816017DC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 09:19:46 +0000 (UTC)
Received: (qmail 38445 invoked by uid 500); 20 Mar 2015 09:19:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38317 invoked by uid 500); 20 Mar 2015 09:19:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38294 invoked by uid 99); 20 Mar 2015 09:19:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 09:19:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vikas.v.iitr@gmail.com designates 209.85.216.177 as permitted sender)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 09:19:16 +0000
Received: by qcto4 with SMTP id o4so88151671qct.3
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 02:18:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=qCVgjROtXYHSaZu02Du9a2H5cQEw9cksxhuZ/w4kGXw=;
        b=kbKsYE9/Qib3KXHyEavtxl9GEmrGlRq/blTuUXUS3Er7E15vyThUYI7TMqd1vbxZEu
         FqfEu+y/VvnyCHbbMAruycQKtLtLHQGLE98ZDKDjPK713EKHR47I6Tz9qvbLx112RSji
         rbZwmVl4xcDdEOolNRGITGSwIPYUqxnQMoc8Syl4y6l9BUF0D17fkUPtiY6mDQknWXdU
         etK1hUZdutL/WQ6wQqNCpS76Tm+UraMOB6GfM7F44D6ZpWLVQFOSvBoDdTwUwLFo+FZJ
         b7K1UcaBLvCnkKBdWzNqt7WMcGZl4k1Y3eEOhtiSw9VDqZreLRJUF+x91Vo7whyw1Vu0
         sD4A==
MIME-Version: 1.0
X-Received: by 10.140.83.202 with SMTP id j68mr97051756qgd.18.1426843109755;
 Fri, 20 Mar 2015 02:18:29 -0700 (PDT)
Received: by 10.229.211.2 with HTTP; Fri, 20 Mar 2015 02:18:29 -0700 (PDT)
Received: by 10.229.211.2 with HTTP; Fri, 20 Mar 2015 02:18:29 -0700 (PDT)
In-Reply-To: <7E03A627485F1B4BBEE6AB8979E8459C9B0A62DF@SZXEMA502-MBS.china.huawei.com>
References: <CABFxkcDoGJco4kdEgRV7N=GWPHAUic5QpYB2uC+NmFRaCskG7A@mail.gmail.com>
	<7E03A627485F1B4BBEE6AB8979E8459C9B0A62DF@SZXEMA502-MBS.china.huawei.com>
Date: Fri, 20 Mar 2015 14:48:29 +0530
Message-ID: <CABFxkcBuR4fGMZqd=H26w2Ntz8qz4kcPAP=J-4WJbBV4CYEDDw@mail.gmail.com>
Subject: =?UTF-8?B?UmU6IOetlOWkjTogQ29udHJpYnV0aW5nIHRvIFNwYXJr?=
From: "vikas.v.iitr@gmail.com" <vikas.v.iitr@gmail.com>
To: Tanyinyan <tanyinyan@huawei.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1379cb513390511b4ccbe
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1379cb513390511b4ccbe
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Jessica, thanks for links. I am aware of these but am looking for some ml
related jira issues which I can contribute as starting point.

Thanks,
Vikas
On Mar 20, 2015 2:12 PM, "Tanyinyan" <tanyinyan@huawei.com> wrote:

> Hello Vikas,
>
> These two links maybe what you want.
>
> jira:
> https://issues.apache.org/jira/browse/SPARK/?selectedTab=3Dcom.atlassian.=
jira.jira-projects-plugin:summary-panel
>
> pull request:  https://github.com/apache/spark/pulls
>
> Regards,
>
> Jessica
>
>
> -----=E9=82=AE=E4=BB=B6=E5=8E=9F=E4=BB=B6-----
> =E5=8F=91=E4=BB=B6=E4=BA=BA: vikas.v.iitr@gmail.com [mailto:vikas.v.iitr@=
gmail.com]
> =E5=8F=91=E9=80=81=E6=97=B6=E9=97=B4: 2015=E5=B9=B43=E6=9C=8820=E6=97=A5 =
15:50
> =E6=94=B6=E4=BB=B6=E4=BA=BA: dev@spark.apache.org
> =E4=B8=BB=E9=A2=98: Contributing to Spark
>
> Hi ,
>
> I have read and gone through most Spark tutorials and materials out there=
.
> I have also downloaded and build the spark code base .
>
> Can someone point me to some existing Jira where I can start contributing=
 ?
> Eventually I want to do some good contribution to mlLib .
>
> Thanks,
> Vikas
>

--001a11c1379cb513390511b4ccbe--

From dev-return-12079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 14:05:13 2015
Return-Path: <dev-return-12079-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6C1C178C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 14:05:13 +0000 (UTC)
Received: (qmail 28639 invoked by uid 500); 20 Mar 2015 13:57:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28564 invoked by uid 500); 20 Mar 2015 13:57:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28543 invoked by uid 99); 20 Mar 2015 13:57:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 13:57:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.218.42] (HELO mail-oi0-f42.google.com) (209.85.218.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 13:57:36 +0000
Received: by oier21 with SMTP id r21so91693854oie.1
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 06:56:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=XLins6CXt/MuuR9fstjtsj0jRK0XvXtmf82IbbJfOmM=;
        b=EyDqKU5Gderc5U+FPv8weiYDvlzPeBxCZpY2dipeqxQm5p8bSJe/JEtTM/HQPb8Rk2
         FDfDlX7tqcsqH/jlOnDZpLdyZjXsIqr1aosjD2OLaKk4b58kb+q09zzEQW+J0qrp5sUw
         L/p1Vl08eNQS6tDkLh9VcskINiMpoczdKx3olEGtrQcOqJcYrf57GxjSct/tMQXvd/3u
         efAWZzeztX8VAuHkNSuJKiQzVOHcO0znPREyNn8Dyc54XROg0jXYIYJ/7BD0SSnw4McK
         CWQevmXGQ28ltmN3rrEhh1fVV1PGSxT2As4O9be4TIZPvcpDwMoPqa7Or9z68jB2I8Qb
         j6/g==
X-Gm-Message-State: ALoCoQm9zm0/3s45Khq+iHip+LjrPepzHiAfmOaXZTDUpSRlxWUVFktgbIFiUZdFDKAPKMoVTD3i
MIME-Version: 1.0
X-Received: by 10.202.172.212 with SMTP id v203mr13632784oie.48.1426859815040;
 Fri, 20 Mar 2015 06:56:55 -0700 (PDT)
Received: by 10.76.87.36 with HTTP; Fri, 20 Mar 2015 06:56:54 -0700 (PDT)
In-Reply-To: <CAMjNSowqQMX11MjEVg5f_0=m8uhnDLh-U7jBtiCZ-E=1ZGgisg@mail.gmail.com>
References: <CAMjNSozMdNChYZYzgi2O=MRd8aQYBTEtys=TtX-64VLi+KAOVw@mail.gmail.com>
	<CALte62ycYF4PPZkp-62DBdUy13U2aQnvOecae6LQdqArjAwxuQ@mail.gmail.com>
	<CAKWX9VWSbZhoh5KS4+grsQ-tMO5cgrnQ7LFSRisL2D2KqMNaPQ@mail.gmail.com>
	<CAMjNSowQ4FhxUzpqQ9etUDC2xgJTuRO=HBPyhh-m3Q_XyqGBRg@mail.gmail.com>
	<CAKWX9VXpLxaOD5VjsQtzPeq4x8VGwYvPOi7QHYegCmFkBpmVjA@mail.gmail.com>
	<CAMjNSowqQMX11MjEVg5f_0=m8uhnDLh-U7jBtiCZ-E=1ZGgisg@mail.gmail.com>
Date: Fri, 20 Mar 2015 08:56:54 -0500
Message-ID: <CAKWX9VXiX2mxRKqjSywOt7eHqee7kEJzshjjQmpkDYatKCwFZg@mail.gmail.com>
Subject: Re: Exception using the new createDirectStream util method
From: Cody Koeninger <cody@koeninger.org>
To: Alberto Rodriguez <ardlema@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ce4ca6b8c100511b8b04a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ce4ca6b8c100511b8b04a
Content-Type: text/plain; charset=UTF-8

I went ahead and created

https://issues.apache.org/jira/browse/SPARK-6434

to track this

On Fri, Mar 20, 2015 at 2:44 AM, Alberto Rodriguez <ardlema@gmail.com>
wrote:

> You were absolutely right Cody!! I have just put a message in the kafka
> topic before creating the DirectStream and now is working fine!
>
> Do you think that I should open an issue to warn that the kafka topic must
> contain at least one message before the DirectStream creation?
>
> Thank you very much! You've just made my day ;)
>
> 2015-03-19 23:08 GMT+01:00 Cody Koeninger <cody@koeninger.org>:
>
> > Yeah, I wouldn't be shocked if Kafka's metadata apis didn't return
> results
> > for topics that don't have any messages.  (sorry about the triple
> negative,
> > but I think you get my meaning).
> >
> > Try putting a message in the topic and seeing what happens.
> >
> > On Thu, Mar 19, 2015 at 4:38 PM, Alberto Rodriguez <ardlema@gmail.com>
> > wrote:
> >
> >> Thank you for replying,
> >>
> >> Ted, I have been debuging and the getLeaderOffsets method is not
> appending
> >> errors because the method findLeaders that is called at the first line
> of
> >> getLeaderOffsets is not returning leaders.
> >>
> >> Cody, the topics do not have any messages yet. Could this be an issue??
> >>
> >> If you guys want to have a look at the code I've just uploaded it to my
> >> github account: big-brother <https://github.com/ardlema/big-brother>
> (see
> >>
> >> DirectKafkaWordCountTest.scala).
> >>
> >> Thank you again!!
> >>
> >> 2015-03-19 22:13 GMT+01:00 Cody Koeninger <cody@koeninger.org>:
> >>
> >> > What is the value of your topics variable, and does it correspond to
> >> > topics that already exist on the cluster and have messages in them?
> >> >
> >> > On Thu, Mar 19, 2015 at 3:10 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> >
> >> >> Looking at KafkaCluster#getLeaderOffsets():
> >> >>
> >> >>           respMap.get(tp).foreach { por: PartitionOffsetsResponse =>
> >> >>             if (por.error == ErrorMapping.NoError) {
> >> >> ...
> >> >>             } else {
> >> >>               errs.append(ErrorMapping.exceptionFor(por.error))
> >> >>             }
> >> >> There should be some error other than "Couldn't find leader offsets
> for
> >> >> Set()"
> >> >>
> >> >> Can you check again ?
> >> >>
> >> >> Thanks
> >> >>
> >> >> On Thu, Mar 19, 2015 at 12:10 PM, Alberto Rodriguez <
> ardlema@gmail.com
> >> >
> >> >> wrote:
> >> >>
> >> >> > Hi all,
> >> >> >
> >> >> > I am trying to make the new kafka and spark streaming integration
> >> work
> >> >> > (direct
> >> >> > approach "no receivers"
> >> >> > <
> http://spark.apache.org/docs/1.3.0/streaming-kafka-integration.html
> >> >).
> >> >> I
> >> >> > have created an unit test where I configure and start both
> zookeeper
> >> and
> >> >> > kafka.
> >> >> >
> >> >> > When I try to create the InputDStream using the createDirectStream
> >> >> method
> >> >> > of the KafkaUtils class I am getting the following error:
> >> >> >
> >> >> > org.apache.spark.SparkException:* Couldn't find leader offsets for
> >> >> Set()*
> >> >> > org.apache.spark.SparkException: org.apache.spark.SparkException:
> >> >> Couldn't
> >> >> > find leader offsets for Set()
> >> >> > at
> >> >> >
> >> >> >
> >> >>
> >>
> org.apache.spark.streaming.kafka.KafkaUtils$$anonfun$createDirectStream$2.apply(KafkaUtils.scala:413)
> >> >> >
> >> >> > Following is the code that tries to create the DStream:
> >> >> >
> >> >> > val messages: InputDStream[(String, String)] =
> >> >> > KafkaUtils.createDirectStream[String, String, StringDecoder,
> >> >> > StringDecoder](
> >> >> >         ssc, kafkaParams, topics)
> >> >> >
> >> >> > Does anyone faced this problem?
> >> >> >
> >> >> > Thank you in advance.
> >> >> >
> >> >> > Kind regards,
> >> >> >
> >> >> > Alberto
> >> >> >
> >> >>
> >> >
> >> >
> >>
> >
> >
>

--001a113ce4ca6b8c100511b8b04a--

From dev-return-12080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 19:10:40 2015
Return-Path: <dev-return-12080-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0DF9D17A49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 19:10:40 +0000 (UTC)
Received: (qmail 20551 invoked by uid 500); 20 Mar 2015 19:10:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20455 invoked by uid 500); 20 Mar 2015 19:10:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20397 invoked by uid 99); 20 Mar 2015 19:10:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 19:10:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [95.143.172.231] (HELO bootes.uberspace.de) (95.143.172.231)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 19:10:31 +0000
Received: (qmail 19107 invoked from network); 20 Mar 2015 19:09:08 -0000
Received: from localhost (HELO webmail.bootes.uberspace.de) (127.0.0.1)
  by ::1 with SMTP; 20 Mar 2015 19:09:08 -0000
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII;
 format=flowed
Content-Transfer-Encoding: 7bit
Date: Fri, 20 Mar 2015 20:09:07 +0100
From: Karlson <ksonspark@siberie.de>
To: dev@spark.apache.org
Subject: Storage of RDDs created via sc.parallelize
Message-ID: <95b62b2b250440458965ccb6448da1ee@siberie.de>
X-Sender: ksonspark@siberie.de
User-Agent: Roundcube Webmail/1.0.5
X-Virus-Checked: Checked by ClamAV on apache.org


Hi all,

where is the data stored that is passed to sc.parallelize? Or put 
differently, where is the data for the base RDD fetched from when the 
DAG is executed, if the base RDD is constructed via sc.parallelize?

I am reading a csv file via the Python csv module and am feeding the 
parsed data chunkwise to sc.parallelize, because the whole file would 
not fit into memory on the driver. Reading the file with sc.textfile 
first is not an option, as there might be linebreaks inside the csv 
fields, preventing me from parsing the file line by line.

The problem I am facing right now is that even though I am feeding only 
one chunk at a time to Spark, I will eventually run out of memory on the 
driver.

Thanks in advance!

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 21:02:00 2015
Return-Path: <dev-return-12081-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B1AC17FFC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 21:02:00 +0000 (UTC)
Received: (qmail 53489 invoked by uid 500); 20 Mar 2015 21:01:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53406 invoked by uid 500); 20 Mar 2015 21:01:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53395 invoked by uid 99); 20 Mar 2015 21:01:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 21:01:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.173] (HELO mail-lb0-f173.google.com) (209.85.217.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 21:01:34 +0000
Received: by lbbrr9 with SMTP id rr9so19460481lbb.0
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 14:00:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wBNGM3v6k/lIPexcZY/2+i1dwLAAhUNm2d+iC+4So5w=;
        b=YVa61y4zu4rbIfdgzzuiKqMJKSs1zNMZTmn+rSfGKPFscUcJKyxaueOwUOF2cwHwFx
         1028YihDWz6rrwyrC24QZk+HdZGYjjhgIsrrKye4qrJiBYnwgTGKy1aieoGMe+I/5Pal
         MWI1YjziRglzSNZD43NRhFX/T0yguheOG3sImlrZoH8+YmpICKUR/tJSY6RyZz7GJG8W
         KAgrO34Hlufgfm8ks6/ntRUJGjTVDxDvN0JG7komfv88RNFL3oFX1ZdGIXj41jE1l3Pu
         NumHHUcGHeh/dMyX7LoaQprb4kJ1Y+B4qXgt74t/sdFp9lu/Rx+ne9K3nXB+zkYXP4DG
         OOlQ==
X-Gm-Message-State: ALoCoQnKVYUkkBNXih4encsMOdPX+NCHirSI1YxLQWMG7/4iNT9neUwwtztS+Whh2vu8py+4hMWD
MIME-Version: 1.0
X-Received: by 10.152.180.202 with SMTP id dq10mr71824375lac.74.1426885227313;
 Fri, 20 Mar 2015 14:00:27 -0700 (PDT)
Received: by 10.25.86.136 with HTTP; Fri, 20 Mar 2015 14:00:27 -0700 (PDT)
In-Reply-To: <9B241471-BBD8-4A63-B4EB-E5DA625A4F38@videoamp.com>
References: <9B241471-BBD8-4A63-B4EB-E5DA625A4F38@videoamp.com>
Date: Fri, 20 Mar 2015 14:00:27 -0700
Message-ID: <CAHP0waJu9EDg-qB=sVvaaKm66=ZtiCnheiHxjp0y2ELhbb+BKw@mail.gmail.com>
Subject: Re: Spark SQL ExternalSorter not stopped
From: Yin Huai <yhuai@databricks.com>
To: Michael Allman <michael@videoamp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134788a1c10730511be9bb7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134788a1c10730511be9bb7
Content-Type: text/plain; charset=UTF-8

Hi Michael,

Thanks for reporting it. Yes, it is a bug. I have created
https://issues.apache.org/jira/browse/SPARK-6437 to track it.

Thanks,

Yin

On Thu, Mar 19, 2015 at 10:51 AM, Michael Allman <michael@videoamp.com>
wrote:

> I've examined the experimental support for ExternalSorter in Spark SQL,
> and it does not appear that the external sorted is ever stopped
> (ExternalSorter.stop). According to the API documentation, this suggests a
> resource leak. Before I file a bug report in Jira, can someone familiar
> with the codebase confirm this is indeed a bug?
>
> Thanks,
>
> Michael
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1134788a1c10730511be9bb7--

From dev-return-12082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 21:54:07 2015
Return-Path: <dev-return-12082-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2972A174A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 21:54:07 +0000 (UTC)
Received: (qmail 97196 invoked by uid 500); 20 Mar 2015 21:54:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97129 invoked by uid 500); 20 Mar 2015 21:54:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97117 invoked by uid 99); 20 Mar 2015 21:54:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 21:54:05 +0000
X-ASF-Spam-Status: No, hits=-10.8 required=10.0
	tests=ENV_AND_HDR_SPF_MATCH,HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS,USER_IN_DEF_SPF_WL
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vmuttineni@ebay.com designates 216.113.172.65 as permitted sender)
Received: from [216.113.172.65] (HELO phx-mipot-002.corp.ebay.com) (216.113.172.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 21:54:00 +0000
DomainKey-Signature: s=ebaycorp; d=ebay.com; c=nofws; q=dns;
  h=X-EBay-Corp:X-IronPort-AV:Received:Received:From:To:
   Subject:Thread-Topic:Thread-Index:Date:Message-ID:
   Accept-Language:Content-Language:X-MS-Has-Attach:
   X-MS-TNEF-Correlator:x-originating-ip:Content-Type:
   MIME-Version:X-CFilter;
  b=N8KnOTXp/6z3Eiu2PF/w/w+L9JSW+0VJQSDJsduwIEKKaj6z2N42Fa5P
   3mmMSpiPv0ovUwExklS0PSTAmYhtmsXdWuJINY1KKsszI18atEkKJkalj
   UbvUeeADrI2OV6YIPQSX3x5ghmUz0FdVTWid20NlugxNCIDR+4wCt42IB
   g=;
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=ebay.com; i=@ebay.com; q=dns/txt; s=ebaycorp;
  t=1426888440; x=1458424440;
  h=from:to:subject:date:message-id:mime-version;
  bh=EtDcvu3HZoR23vnyEankNadYEnk8KN+XWr0/KBbmQOU=;
  b=YublOFxA3HLHj5UpyIYqNynOBkGB4JX1XOr7B+DzI4Uv4MomBwBDATge
   8M9brbNYbd6mGZK5TGkV5UJQBEo3ahW1shQZcFvnskW5LEVEXuCUFE0s9
   YA2JLfqlbAf07F1DR5CzPlxgfGOg23LCtvrCrzFp/yAp/h2NsIUt462EP
   c=;
X-EBay-Corp: Yes
X-IronPort-AV: E=Sophos;i="5.11,439,1422950400"; 
   d="scan'208,217";a="250230445"
Received: from phx-vteml-004.corp.ebay.com (HELO PHX-EXMHT-003.corp.ebay.com) ([10.58.40.103])
  by phx-mipot-002.corp.ebay.com with ESMTP; 20 Mar 2015 14:53:40 -0700
Received: from PHX-EXRDA-S71.corp.ebay.com ([169.254.7.196]) by
 PHX-EXMHT-003.corp.ebay.com ([10.58.12.74]) with mapi id 14.03.0224.002; Fri,
 20 Mar 2015 14:53:38 -0700
From: "Muttineni, Vinay" <vmuttineni@ebay.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Minor Edit in the programming guide
Thread-Topic: Minor Edit in the programming guide
Thread-Index: AdBjWCGpI/JsmBhOQWicZ9u3Dp50Zg==
Date: Fri, 20 Mar 2015 21:53:38 +0000
Message-ID: <66B0336C091EE542A3F5EC764B63BE1C15C19971@PHX-EXRDA-S71.corp.ebay.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.58.61.242]
Content-Type: multipart/alternative;
	boundary="_000_66B0336C091EE542A3F5EC764B63BE1C15C19971PHXEXRDAS71corp_"
MIME-Version: 1.0
X-CFilter: Scanned
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_66B0336C091EE542A3F5EC764B63BE1C15C19971PHXEXRDAS71corp_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hey guys,
In the Spark 1.3.0 documentation provided here, http://spark.apache.org/doc=
s/latest/sql-programming-guide.html ,
Under the "Programmatically Specifying the Schema" section , it's mentioned=
 that SQL data types are in the following package org.apache.spark.sql, but=
 I guess it has changed to org.apache.spark.sql.types
Great work with the Data Frames btw! :)
Thanks,
Vinay



--_000_66B0336C091EE542A3F5EC764B63BE1C15C19971PHXEXRDAS71corp_--

From dev-return-12083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 22:24:15 2015
Return-Path: <dev-return-12083-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8889E17593
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 22:24:15 +0000 (UTC)
Received: (qmail 48892 invoked by uid 500); 20 Mar 2015 22:24:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48809 invoked by uid 500); 20 Mar 2015 22:24:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48797 invoked by uid 99); 20 Mar 2015 22:24:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 22:24:13 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of guillaume.pitel@exensa.com designates 91.121.232.90 as permitted sender)
Received: from [91.121.232.90] (HELO mail.exensa.com) (91.121.232.90)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 22:23:46 +0000
Received: from localhost (localhost [127.0.0.1])
	by mail.exensa.com (Postfix) with ESMTP id 6C102641C04
	for <dev@spark.apache.org>; Fri, 20 Mar 2015 22:23:15 +0000 (UTC)
Received: from mail.exensa.com ([127.0.0.1])
	by localhost (mail.exensa.com [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id b2GdyPdKvO9Y for <dev@spark.apache.org>;
	Fri, 20 Mar 2015 22:23:15 +0000 (UTC)
Received: from [192.168.1.101] (89-157-16-87.rev.auchanbox.fr [89.157.16.87])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128 bits))
	(No client certificate requested)
	by mail.exensa.com (Postfix) with ESMTPSA id D49EE6410CB
	for <dev@spark.apache.org>; Fri, 20 Mar 2015 22:23:14 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=exensa.com;
	s=default; t=1426890195;
	bh=kvtFFgpacjUf8rA0Agdr+8/iW/Ar8/fkDf+ibRgdoGQ=;
	h=Date:From:To:Subject;
	b=q0/kv/FyrdeHKT2fePycvW0Q/v/fNVol3gcsAXQJqghgDO1yGknJJLxwwvpiCFlXD
	 eVSOwL6C8J1goV2Xf4vxNgs4IlIednn3GIoZyEjDpRcRIJ92ZKIbuZAZISgf0jS
Message-ID: <550C9E28.8010404@exensa.com>
Date: Fri, 20 Mar 2015 23:24:40 +0100
From: Guillaume Pitel <guillaume.pitel@exensa.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Directly broadcasting (sort of) RDDs
Content-Type: multipart/alternative;
 boundary="------------030600050001010706090608"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------030600050001010706090608
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Hi,

I have an idea that I would like to discuss with the Spark devs. The 
idea comes from a very real problem that I have struggled with since 
almost a year. My problem is very simple, it's a dense matrix * sparse 
matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is 
divided in X large blocks (one block per partition), and a sparse matrix 
RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The 
most efficient way to perform the operation is to collectAsMap() the 
dense matrix and broadcast it, then perform the block-local 
mutliplications, and combine the results by column.

This is quite fine, unless the matrix is too big to fit in memory 
(especially since the multiplication is performed several times 
iteratively, and the broadcasts are not always cleaned from memory as I 
would naively expect).

When the dense matrix is too big, a second solution is to split the big 
sparse matrix in several RDD, and do several broadcasts. Doing this 
creates quite a big overhead, but it mostly works, even though I often 
face some problems with unaccessible broadcast files, for instance.

Then there is the terrible but apparently very effective good old join. 
Since X blocks of the sparse matrix use the same block from the dense 
matrix, I suspect that the dense matrix is somehow replicated X times 
(either on disk or in the network), which is the reason why the join 
takes so much time.

After this bit of a context, here is my idea : would it be possible to 
somehow "broadcast" (or maybe more accurately, share or serve) a 
persisted RDD which is distributed on all workers, in a way that would, 
a bit like the IndexedRDD, allow a task to access a partition or an 
element of a partition in the closure, with a worker-local memory cache 
. i.e. the information about where each block resides would be 
distributed on the workers, to allow them to access parts of the RDD 
directly. I think that's already a bit how RDD are shuffled ?

The RDD could stay distributed (no need to collect then broadcast), and 
only necessary transfers would be required.

Is this a bad idea, is it already implemented somewhere (I would love it 
!) ?or is it something that could add efficiency not only for my use 
case, but maybe for others ? Could someone give me some hint about how I 
could add this possibility to Spark ? I would probably try to extend a 
RDD into a specific SharedIndexedRDD with a special lookup that would be 
allowed from tasks as a special case, and that would try to contact the 
blockManager and reach the corresponding data from the right worker.

Thanks in advance for your advices

Guillaume
-- 
eXenSa

	
*Guillaume PITEL, Président*
+33(0)626 222 431

eXenSa S.A.S. <http://www.exensa.com/>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705


--------------030600050001010706090608
Content-Type: multipart/related;
 boundary="------------020707020006050404040104"


--------------020707020006050404040104
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: 8bit

<html>
  <head>

    <meta http-equiv="content-type" content="text/html; charset=utf-8">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    Hi,<br>
    <br>
    I have an idea that I would like to discuss with the Spark devs. The
    idea comes from a very real problem that I have struggled with since
    almost a year. My problem is very simple, it's a dense matrix *
    sparse matrix  operation. I have a dense matrix
    RDD[(Int,FloatMatrix)] which is divided in X large blocks (one block
    per partition), and a sparse matrix
    RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks.
    The most efficient way to perform the operation is to collectAsMap()
    the dense matrix and broadcast it, then perform the block-local
    mutliplications, and combine the results by column.<br>
    <br>
    This is quite fine, unless the matrix is too big to fit in memory
    (especially since the multiplication is performed several times
    iteratively, and the broadcasts are not always cleaned from memory
    as I would naively expect). <br>
    <br>
    When the dense matrix is too big, a second solution is to split the
    big sparse matrix in several RDD, and do several broadcasts. Doing
    this creates quite a big overhead, but it mostly works, even though
    I often face some problems with unaccessible broadcast files, for
    instance.<br>
    <br>
    Then there is the terrible but apparently very effective good old
    join. Since X blocks of the sparse matrix use the same block from
    the dense matrix, I suspect that the dense matrix is somehow
    replicated X times (either on disk or in the network), which is the
    reason why the join takes so much time.<br>
    <br>
    After this bit of a context, here is my idea : would it be possible
    to somehow "broadcast" (or maybe more accurately, share or serve) a
    persisted RDD which is distributed on all workers, in a way that
    would, a bit like the IndexedRDD, allow a task to access a partition
    or an element of a partition in the closure, with a worker-local
    memory cache . i.e. the information about where each block resides
    would be distributed on the workers, to allow them to access parts
    of the RDD directly. I think that's already a bit how RDD are
    shuffled ?<br>
    <br>
    The RDD could stay distributed (no need to collect then broadcast),
    and only necessary transfers would be required.<br>
    <br>
    Is this a bad idea, is it already implemented somewhere (I would
    love it !) ?or is it something that could add efficiency not only
    for my use case, but maybe for others ? Could someone give me some
    hint about how I could add this possibility to Spark ? I would
    probably try to extend a RDD into a specific SharedIndexedRDD with a
    special lookup that would be allowed from tasks as a special case,
    and that would try to contact the blockManager and reach the
    corresponding data from the right worker.<br>
    <br>
    Thanks in advance for your advices<br>
    <br>
    Guillaume<br>
    <div class="moz-signature">-- <br>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <span style="font-size: 8.5pt; color: rgb(51, 51, 51);
        font-family: Helvetica;">
        <div style="width: 529px;" align="left">
          <table style="width: 524px;" border="0" cellpadding="0"
            cellspacing="0">
            <tbody>
              <tr>
                <td style="border-top: medium none rgb(236, 233, 216);
                  border-right: medium none rgb(236, 233, 216);
                  border-bottom: medium none rgb(236, 233, 216);
                  padding-right: 0cm; width: 219px; height: 33.75pt;
                  background-color: transparent; vertical-align:
                  middle;">
                  <center style="width: 210px;"><img style="border: 0px
                      solid ; width: 160px; height: 64px;" alt="eXenSa"
                      src="cid:part1.09080406.06010509@exensa.com"></center>
                </td>
                <td style="padding-left:10px; border-top: medium none
                  rgb(236, 233, 216); border-right: medium none rgb(236,
                  233, 216); border-bottom: medium none rgb(236, 233,
                  216); padding-right: 0cm; width: 358px; height:
                  33.75pt; background-color: transparent;
                  vertical-align: top;">
                  <div style="text-align: left; width: 299px;"
                    align="left"> <span style="font-size: 7.5pt; color:
                      rgb(75, 80, 85); font-family: Helvetica;"> <b>Guillaume
                        PITEL, Président</b> <br>
                      +33(0)626 222 431<br>
                    </span> <br>
                    <span style="font-size: 7.5pt; color: #505050;
                      font-family: Helvetica;"><a
                        href="http://www.exensa.com/" target="_blank">eXenSa
                        S.A.S.</a> </span><br>
                    <span style="font-size: 7.5pt; color: rgb(75, 80,
                      85);"> <font face="Helvetica">41, rue Périer -
                        92120 Montrouge - FRANCE <br>
                        Tel +33(0)184 163 677 / Fax +33(0)972 283 705 </font>
                    </span> </div>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </span>
    </div>
  </body>
</html>

--------------020707020006050404040104
Content-Type: image/png;
 name="exensa_logo_mail.png"
Content-Transfer-Encoding: base64
Content-ID: <part1.09080406.06010509@exensa.com>
Content-Disposition: inline;
 filename="exensa_logo_mail.png"
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==
--------------020707020006050404040104--

--------------030600050001010706090608--

From dev-return-12084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 20 23:00:48 2015
Return-Path: <dev-return-12084-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7CA8B176EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 20 Mar 2015 23:00:48 +0000 (UTC)
Received: (qmail 24234 invoked by uid 500); 20 Mar 2015 23:00:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24130 invoked by uid 500); 20 Mar 2015 23:00:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24119 invoked by uid 99); 20 Mar 2015 23:00:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 23:00:37 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of ehomecity@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 20 Mar 2015 23:00:12 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 204DB180EF7D
	for <dev@spark.apache.org>; Fri, 20 Mar 2015 15:59:23 -0700 (PDT)
Date: Fri, 20 Mar 2015 15:59:09 -0700 (MST)
From: xing <ehomecity@gmail.com>
To: dev@spark.apache.org
Message-ID: <1426892349523-11140.post@n3.nabble.com>
In-Reply-To: <1426881788855-11136.post@n3.nabble.com>
References: <1426881788855-11136.post@n3.nabble.com>
Subject: Re: Error:  'SparkContext' object has no attribute
 'getActiveStageIds'
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

getStageInfo in self._jtracker.getStageInfo below seems not
implemented/included in the current python library.

   def getStageInfo(self, stageId):
        """
        Returns a :class:`SparkStageInfo` object, or None if the stage
        info could not be found or was garbage collected.
        """
        stage = self._jtracker.getStageInfo(stageId)
        if stage is not None:
            # TODO: fetch them in batch for better performance
            attrs = [getattr(stage, f)() for f in
SparkStageInfo._fields[1:]]
            return SparkStageInfo(stageId, *attrs)



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Error-SparkContext-object-has-no-attribute-getActiveStageIds-tp11136p11140.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 01:03:30 2015
Return-Path: <dev-return-12085-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E18017B11
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 01:03:30 +0000 (UTC)
Received: (qmail 28472 invoked by uid 500); 21 Mar 2015 01:03:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28393 invoked by uid 500); 21 Mar 2015 01:03:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28382 invoked by uid 99); 21 Mar 2015 01:03:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 01:03:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.223.170] (HELO mail-ie0-f170.google.com) (209.85.223.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 01:03:03 +0000
Received: by iedfl3 with SMTP id fl3so2502147ied.1
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 18:02:06 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=GoCemCEtxHJnQg1X19lXonjn7qBtaE1qcDGZ40+L+Lc=;
        b=ZN1OmcH/Euh4xBTTdNoKe/p45QyPrPz6/QjnhY8TN1i1Y/LNYm7YfD5WPXT3JRfNIk
         tArj4ovjd/keKVgpDOusSTPN0RbA3itxTv9jUlKjcc9uoLCqRzX4gIo1ZvyEa2LCCT+A
         7gsLLN+UDDSq5RVymrVnXTuSVtjwNv+8a2gFlD7IRofVt0YfxblCOORgMziu30sxGbgE
         vxiVLI4mBBeGMwfRPE9ZHO41nOuHr01vaZZIz70iXK6T7mZoIpxnA0N5vmI8aXaiZ6w8
         NbU21baITWar5XKhNdy3m++BrnRiVIT4DhMjpWKvlqCN8V6p53rbEt2xFe/B6LxNfZns
         KCCg==
X-Gm-Message-State: ALoCoQnKnvqT4wEziUkj4hbuVrgDvZoVrcL0HFmcl/RkhuKuqGtoHbuVJkUP72u90shHEk/2C4UT
MIME-Version: 1.0
X-Received: by 10.107.41.10 with SMTP id p10mr28396163iop.58.1426899726438;
 Fri, 20 Mar 2015 18:02:06 -0700 (PDT)
Received: by 10.36.58.2 with HTTP; Fri, 20 Mar 2015 18:02:06 -0700 (PDT)
In-Reply-To: <66B0336C091EE542A3F5EC764B63BE1C15C19971@PHX-EXRDA-S71.corp.ebay.com>
References: <66B0336C091EE542A3F5EC764B63BE1C15C19971@PHX-EXRDA-S71.corp.ebay.com>
Date: Fri, 20 Mar 2015 21:02:06 -0400
Message-ID: <CAF7ADNoUf5HF--u28qOET8KLYucZ4C1u=ZXgUfQ5wcHK2zkpog@mail.gmail.com>
Subject: Re: Minor Edit in the programming guide
From: Joseph Bradley <joseph@databricks.com>
To: "Muttineni, Vinay" <vmuttineni@ebay.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1141f2ee532b680511c1fbd2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1141f2ee532b680511c1fbd2
Content-Type: text/plain; charset=UTF-8

Thanks!  I added it to a few other items:
https://issues.apache.org/jira/browse/SPARK-6337

On Fri, Mar 20, 2015 at 5:53 PM, Muttineni, Vinay <vmuttineni@ebay.com>
wrote:

> Hey guys,
> In the Spark 1.3.0 documentation provided here,
> http://spark.apache.org/docs/latest/sql-programming-guide.html ,
> Under the "Programmatically Specifying the Schema" section , it's
> mentioned that SQL data types are in the following package
> org.apache.spark.sql, but I guess it has changed to
> org.apache.spark.sql.types
> Great work with the Data Frames btw! :)
> Thanks,
> Vinay
>
>
>

--001a1141f2ee532b680511c1fbd2--

From dev-return-12086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 01:03:45 2015
Return-Path: <dev-return-12086-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C749D17B15
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 01:03:45 +0000 (UTC)
Received: (qmail 30238 invoked by uid 500); 21 Mar 2015 01:03:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30154 invoked by uid 500); 21 Mar 2015 01:03:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30142 invoked by uid 99); 21 Mar 2015 01:03:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 01:03:44 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.223.170 as permitted sender)
Received: from [209.85.223.170] (HELO mail-ie0-f170.google.com) (209.85.223.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 01:03:39 +0000
Received: by ieclw3 with SMTP id lw3so2131330iec.2
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 18:03:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=1qboQStuJhIg83nWkDeuD25W4bk6tom3X32fmJmmEDU=;
        b=KQzjewyikJrjuBCjdIvm/WuRVEP0X8Tj5gx/x1HwAlU4uTeiaFvvgVCuQ3Ofj4H9JG
         PEyW0MnlOF1QnG+l0p1VxM/vhpwGsiWK12wTb9sdCWc8Nwhn1TvDPwgKswD7opbKb8pA
         fMo5iRk3fbL33C6dnb/Zisn5noSbdzYOzEOKdvDz0cCbV4S8gooXVSyFq7M3dJUNSKe0
         ZAXOlq0eMJXOUwXU3R336VOvypIqEL2b9kz5IHhoC6owegxbNmQ2pUF9K42x4zCE3hf+
         4PNo9wo7JcO4uiGbkcY/dMMugFOVox7rCZwoO5q8hjEvZLdShwRvFjIjTypc6Gc2QpVd
         xhYw==
MIME-Version: 1.0
X-Received: by 10.50.78.232 with SMTP id e8mr587043igx.5.1426899799136; Fri,
 20 Mar 2015 18:03:19 -0700 (PDT)
Received: by 10.36.53.148 with HTTP; Fri, 20 Mar 2015 18:03:19 -0700 (PDT)
In-Reply-To: <1426892349523-11140.post@n3.nabble.com>
References: <1426881788855-11136.post@n3.nabble.com>
	<1426892349523-11140.post@n3.nabble.com>
Date: Fri, 20 Mar 2015 18:03:19 -0700
Message-ID: <CALte62yiLPETiQBCZJcDFy_715aEdR5EEdBa1DdgaR=-TQboNw@mail.gmail.com>
Subject: Re: Error: 'SparkContext' object has no attribute 'getActiveStageIds'
From: Ted Yu <yuzhihong@gmail.com>
To: xing <ehomecity@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013c6a20a85df70511c1ffc4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013c6a20a85df70511c1ffc4
Content-Type: text/plain; charset=UTF-8

Please take a look
at core/src/main/scala/org/apache/spark/SparkStatusTracker.scala, around
line 58:
  def getActiveStageIds(): Array[Int] = {

Cheers

On Fri, Mar 20, 2015 at 3:59 PM, xing <ehomecity@gmail.com> wrote:

> getStageInfo in self._jtracker.getStageInfo below seems not
> implemented/included in the current python library.
>
>    def getStageInfo(self, stageId):
>         """
>         Returns a :class:`SparkStageInfo` object, or None if the stage
>         info could not be found or was garbage collected.
>         """
>         stage = self._jtracker.getStageInfo(stageId)
>         if stage is not None:
>             # TODO: fetch them in batch for better performance
>             attrs = [getattr(stage, f)() for f in
> SparkStageInfo._fields[1:]]
>             return SparkStageInfo(stageId, *attrs)
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Error-SparkContext-object-has-no-attribute-getActiveStageIds-tp11136p11140.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e013c6a20a85df70511c1ffc4--

From dev-return-12087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 01:55:42 2015
Return-Path: <dev-return-12087-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7395117BE8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 01:55:42 +0000 (UTC)
Received: (qmail 86129 invoked by uid 500); 21 Mar 2015 01:55:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86050 invoked by uid 500); 21 Mar 2015 01:55:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86036 invoked by uid 99); 21 Mar 2015 01:55:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 01:55:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of brkyvz@gmail.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 01:55:36 +0000
Received: by wgdm6 with SMTP id m6so102878398wgd.2
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 18:54:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=sk2lJjd9TQ3+rpT8y1qBidhsfn/88sAU3zDr9j2clSg=;
        b=uLN5veWSOwRSb/Kc6JuYMkqvRlOSbKQQfb+1GtiDvG0PQ4RkTRYovtdcgoDfWM6Y/L
         9g/An7EbIbmgoos1vH9li5DEDY0R2Lge9pgw2DdeUkmwH2p/2LsamyGYf9Z1wvQSu72f
         Y0HXmGGO3ua1VCCI5Y2/1ILrUC2CJht0DOOEzwuCsnB3vI7gyaMkXpSLnx2FpIny367o
         b/mNmNlyNlDNE/HVKas3cSdEvXgzyh771WOGqcFF1bU/fSrNbkjYLP3U7YpE9juNQgNW
         GoDPr6q8EBsenr31Wa0Rrlt2X2iKt0gFmErOyhEaE1sbFdPF0UZNbvES+hluBOBlhzfG
         JB1A==
MIME-Version: 1.0
X-Received: by 10.194.63.206 with SMTP id i14mr168656568wjs.107.1426902870517;
 Fri, 20 Mar 2015 18:54:30 -0700 (PDT)
Received: by 10.27.101.65 with HTTP; Fri, 20 Mar 2015 18:54:30 -0700 (PDT)
In-Reply-To: <CA+B-+fxSC6TA6KU81KXuFpvf1NnDpsfyMcUzAs7_8J_nJzrZ2w@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
	<CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
	<09B9C296-EBE4-48C6-8B59-4BD26EDB6F3D@hp.com>
	<CA+B-+fycWb5JCO1yA2Kh970g+7cAORm_UpQYjpC5OadKfFkx3w@mail.gmail.com>
	<FD914DA8-C1E7-48CA-B1CE-F6597265901B@hp.com>
	<CA+B-+fxSC6TA6KU81KXuFpvf1NnDpsfyMcUzAs7_8J_nJzrZ2w@mail.gmail.com>
Date: Fri, 20 Mar 2015 18:54:30 -0700
Message-ID: <CABvqBfmR5waEKrW30y4GQFOA6p6yutL7tN73wNa88fN4vb4ZzA@mail.gmail.com>
Subject: Re: Which linear algebra interface to use within Spark MLlib?
From: Burak Yavuz <brkyvz@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: "Ulanov, Alexander" <alexander.ulanov@hp.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b86d6cab9eb080511c2b601
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86d6cab9eb080511c2b601
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi,

We plan to add a more comprehensive local linear algebra package for MLlib
1.4. This local linear algebra package can then easily be extended to
BlockMatrix to support the same operations in a distributed fashion.

You may find the JIRA to track this here: SPARK-6442
<https://issues.apache.org/jira/browse/SPARK-6442>

The design doc is here: http://goo.gl/sf5LCE

We would very much appreciate your feedback and input.

Best,
Burak

On Thu, Mar 19, 2015 at 3:06 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Yeah it will be better if we consolidate the development on one of
> them...either Breeze or mllib.BLAS...
>
> On Thu, Mar 19, 2015 at 2:25 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com>
> wrote:
>
> >  Thanks for quick response.
> >
> >  I can use linealg.BLAS.gemm, and this means that I have to use MLlib
> > Matrix. The latter does not support some useful functionality needed fo=
r
> > optimization. For example, creation of Matrix given matrix size, array
> and
> > offset in this array. This means that I will need to create matrix in
> > Breeze and convert it to MLlib. Also, linalg.BLAS misses some useful BL=
AS
> > functions I need, that can be found in Breeze (and netlib-java). The sa=
me
> > concerns are applicable to MLlib Vector.
> >
> > Best regards, Alexander
> >
> > 19.03.2015, =D0=B2 14:16, "Debasish Das" <debasish.das83@gmail.com>
> =D0=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
> >
> >   I think for Breeze we are focused on dot and dgemv right now (along
> > with several other matrix vector style operations)...
> >
> >  For dgemm it is tricky since you need to do add dgemm for both
> > DenseMatrix and CSCMatrix...and for CSCMatrix you need to get something
> > like SuiteSparse which is under lgpl...so we have to think more on it..
> >
> >  For now can't you use dgemm directly from mllib.linalg.BLAS ? It's in
> > master...
> >
> >
> > On Thu, Mar 19, 2015 at 1:49 PM, Ulanov, Alexander <
> > alexander.ulanov@hp.com> wrote:
> >
> >>  Thank you! When do you expect to have gemm in Breeze and that version
> >> of Breeze to ship with MLlib?
> >>
> >>  Also, could someone please elaborate on the linalg.BLAS and Matrix? A=
re
> >> they going to be developed further, should in long term all developers
> use
> >> them?
> >>
> >> Best regards, Alexander
> >>
> >> 18.03.2015, =D0=B2 23:21, "Debasish Das" <debasish.das83@gmail.com>
> >> =D0=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
> >>
> >>    dgemm dgemv and dot come to Breeze and Spark through netlib-java...=
.
> >>
> >>  Right now both in dot and dgemv Breeze does a extra memory allocate b=
ut
> >> we already found the issue and we are working on adding a common trait
> that
> >> will provide a sink operation (basically memory will be allocated by
> >> user)...adding more BLAS operators in breeze will also help in general
> as
> >> lot more operations are defined over there...
> >>
> >>
> >> On Wed, Mar 18, 2015 at 8:09 PM, Ulanov, Alexander <
> >> alexander.ulanov@hp.com> wrote:
> >>
> >>> Hi,
> >>>
> >>> Currently I am using Breeze within Spark MLlib for linear algebra. I
> >>> would like to reuse previously allocated matrices for storing the
> result of
> >>> matrices multiplication, i.e. I need to use "gemm" function
> C:=3Dq*A*B+p*C,
> >>> which is missing in Breeze (Breeze automatically allocates a new
> matrix to
> >>> store the result of multiplication). Also, I would like to minimize
> gemm
> >>> calls that Breeze does. Should I use mllib.linalg.BLAS functions
> instead?
> >>> While it has gemm and axpy, it has rather limited number of
> operations. For
> >>> example, I need sum of the matrix by row or by columns, or applying a
> >>> function to all elements in a matrix. Also, MLlib Vector and Matrix
> >>> interfaces that linalg.BLAS operates seems to be rather undeveloped.
> Should
> >>> I use plain netlib-java instead (will it remain in MLlib in future
> >>> releases)?
> >>>
> >>> Best regards, Alexander
> >>>
> >>
> >>
> >
>

--047d7b86d6cab9eb080511c2b601--

From dev-return-12088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 05:35:51 2015
Return-Path: <dev-return-12088-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BE7DF17F84
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 05:35:51 +0000 (UTC)
Received: (qmail 52524 invoked by uid 500); 21 Mar 2015 05:35:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52442 invoked by uid 500); 21 Mar 2015 05:35:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52430 invoked by uid 99); 21 Mar 2015 05:35:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 05:35:49 +0000
X-ASF-Spam-Status: No, hits=2.6 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of 261810726@qq.com designates 54.206.16.166 as permitted sender)
Received: from [54.206.16.166] (HELO smtpbgau1.qq.com) (54.206.16.166)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 05:35:24 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1426916103; bh=ezU+K3bd5TvfZwXay2zayfZqRrpQab/V7TwvXtwSEhQ=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-SENDSIZE;
	b=cx6Qv23FGWuMR4w7W4Dn5kpNx6413u+pAj/RUeb3+Ev1fOVfZSXDZV1mhHdmsieLO
	 2oiXbEK0cDbF/BDornLqZlCPGWEEEJM2PckwJiWVDNsq5M4rudaVWYa03772ISNyzP
	 sQogC+5r3HGh0R/436+CHJ8GMjdYmUrEGxhz0qxk=
X-QQ-FEAT: BItkyeoPD2L7rQ354a8TIbPSOULbs4ky49/4tOE+XeHEhzvjkfT/Frz89RDVN
	Sso3kJNIg27fUCuYf+79UDVTpPiuKr+JblvzB+QJNs7e/F96l+pCnwDXSMv99hDO4ayGFjv
	b5Uz+JV3Q+09WGy5luSqKsd4fxaBZ0hSiPVd6aQ8tCivpr+nt7tePpInrDdUd4tQdNVYCFM
	=
X-QQ-SSF: 00000000000000F000000000000000Z
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 111.207.232.157
X-QQ-STYLE: 
X-QQ-mid: webmail249t1426916101t3303679
From: "=?utf-8?B?U2Vh?=" <261810726@qq.com>
To: "=?utf-8?B?dXNlcg==?=" <user@spark.apache.org>, "=?utf-8?B?ZGV2?=" <dev@spark.apache.org>
Subject: Filesystem closed Exception
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_550D0305_091D3820_7D96B78D"
Content-Transfer-Encoding: 8Bit
Date: Sat, 21 Mar 2015 13:35:01 +0800
X-Priority: 3
Message-ID: <tencent_2D66D9B901B151693BBBC6B0@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_550D0305_091D3820_7D96B78D
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64

SGksIGFsbDoNCg0KDQoNCg0KV2hlbiBJIGV4aXQgdGhlIGNvbnNvbGUgb2Ygc3Bhcmstc3Fs
77yMIHRoZSBmb2xsb3dpbmcgZXhjZXB0aW9uIHRocm93ZWQuLi4uLi4NCg0KDQpFeGNlcHRp
b24gaW4gdGhyZWFkICJUaHJlYWQtMyIgamF2YS5pby5JT0V4Y2VwdGlvbjogRmlsZXN5c3Rl
bSBjbG9zZWQNCiAgICAgICAgYXQgb3JnLmFwYWNoZS5oYWRvb3AuaGRmcy5ERlNDbGllbnQu
Y2hlY2tPcGVuKERGU0NsaWVudC5qYXZhOjYyOSkNCiAgICAgICAgYXQgb3JnLmFwYWNoZS5o
YWRvb3AuaGRmcy5ERlNDbGllbnQuZ2V0RmlsZUluZm8oREZTQ2xpZW50LmphdmE6MTY3NykN
CiAgICAgICAgYXQgb3JnLmFwYWNoZS5oYWRvb3AuaGRmcy5EaXN0cmlidXRlZEZpbGVTeXN0
ZW0kMTcuZG9DYWxsKERpc3RyaWJ1dGVkRmlsZVN5c3RlbS5qYXZhOjExMDYpDQogICAgICAg
IGF0IG9yZy5hcGFjaGUuaGFkb29wLmhkZnMuRGlzdHJpYnV0ZWRGaWxlU3lzdGVtJDE3LmRv
Q2FsbChEaXN0cmlidXRlZEZpbGVTeXN0ZW0uamF2YToxMTAyKQ0KICAgICAgICBhdCBvcmcu
YXBhY2hlLmhhZG9vcC5mcy5GaWxlU3lzdGVtTGlua1Jlc29sdmVyLnJlc29sdmUoRmlsZVN5
c3RlbUxpbmtSZXNvbHZlci5qYXZhOjgxKQ0KICAgICAgICBhdCBvcmcuYXBhY2hlLmhhZG9v
cC5oZGZzLkRpc3RyaWJ1dGVkRmlsZVN5c3RlbS5nZXRGaWxlU3RhdHVzKERpc3RyaWJ1dGVk
RmlsZVN5c3RlbS5qYXZhOjExMDIpDQogICAgICAgIGF0IG9yZy5hcGFjaGUuaGFkb29wLmZz
LkZpbGVTeXN0ZW0uZXhpc3RzKEZpbGVTeXN0ZW0uamF2YToxMzk3KQ0KICAgICAgICBhdCBv
cmcuYXBhY2hlLnNwYXJrLnNjaGVkdWxlci5FdmVudExvZ2dpbmdMaXN0ZW5lci5zdG9wKEV2
ZW50TG9nZ2luZ0xpc3RlbmVyLnNjYWxhOjE5NikNCiAgICAgICAgYXQgb3JnLmFwYWNoZS5z
cGFyay5TcGFya0NvbnRleHQkJGFub25mdW4kc3RvcCQ0LmFwcGx5KFNwYXJrQ29udGV4dC5z
Y2FsYToxMzg4KQ0KICAgICAgICBhdCBvcmcuYXBhY2hlLnNwYXJrLlNwYXJrQ29udGV4dCQk
YW5vbmZ1biRzdG9wJDQuYXBwbHkoU3BhcmtDb250ZXh0LnNjYWxhOjEzODgpDQogICAgICAg
IGF0IHNjYWxhLk9wdGlvbi5mb3JlYWNoKE9wdGlvbi5zY2FsYToyMzYpDQogICAgICAgIGF0
IG9yZy5hcGFjaGUuc3BhcmsuU3BhcmtDb250ZXh0LnN0b3AoU3BhcmtDb250ZXh0LnNjYWxh
OjEzODgpDQogICAgICAgIGF0IG9yZy5hcGFjaGUuc3Bhcmsuc3FsLmhpdmUudGhyaWZ0c2Vy
dmVyLlNwYXJrU1FMRW52JC5zdG9wKFNwYXJrU1FMRW52LnNjYWxhOjY2KQ0KICAgICAgICBh
dCBvcmcuYXBhY2hlLnNwYXJrLnNxbC5oaXZlLnRocmlmdHNlcnZlci5TcGFya1NRTENMSURy
aXZlciQkYW5vbiQxLnJ1bihTcGFya1NRTENMSURyaXZlci5zY2FsYToxMDcp4oCN

------=_NextPart_550D0305_091D3820_7D96B78D--




From dev-return-12089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 05:36:50 2015
Return-Path: <dev-return-12089-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ADB3E17F89
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 05:36:50 +0000 (UTC)
Received: (qmail 60171 invoked by uid 500); 21 Mar 2015 05:36:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60077 invoked by uid 500); 21 Mar 2015 05:36:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60065 invoked by uid 99); 21 Mar 2015 05:36:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 05:36:43 +0000
X-ASF-Spam-Status: No, hits=2.6 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of 261810726@qq.com designates 54.254.200.128 as permitted sender)
Received: from [54.254.200.128] (HELO smtpbgsg2.qq.com) (54.254.200.128)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 05:36:18 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1426916141; bh=7AaUF+8Cq6A/UdX0M0Zzn3yrhlccXvwkaN4gKvpNPpw=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-SENDSIZE;
	b=BMqNu3ElvL9jY6q5/60CUPHruXgECDOORp8WOSQpspjFUEPQ1xIZix1kKDSIijuDF
	 +R5TBbUE7M+KLo4AgtJ+FLiisldxQ0MSqfe6MyAojVrLJztHh4ONvnwkg7hjlfglRm
	 UT6phKN+l1P9YqFkqFNuxncY9IZ3FtRbhLDUekGU=
X-QQ-FEAT: CYXv01b5grFGd3dqUKaMuY2X1PPxCwwWSHo7NjtRLY0FpxeJDL6umje+86sGM
	vprTn/UxQVXqCbba1wU4HlM8qgdWE9o59FUzkVh66JuirGsA4MdnSK6E3VMs+5C2462HQHd
	gIKrCLFQ9GQKlVbGSORycH8JmTUaQlC6EG0NEvVlwytv0PZfZBMHXSV9ooyeZUT00+IpaYw
	=
X-QQ-SSF: 00000000000000F000000000000000Z
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 111.207.232.157
X-QQ-STYLE: 
X-QQ-mid: webmail249t1426916140t9924616
From: "=?utf-8?B?U2Vh?=" <261810726@qq.com>
To: "=?utf-8?B?dXNlckBzcGFyay5hcGFjaGUub3Jn?=" <user@spark.apache.org>, "=?utf-8?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?=" <dev@spark.apache.org>
Subject: Filesystem closed Exception
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_550D032C_091DB118_4BCDB682"
Content-Transfer-Encoding: 8Bit
Date: Sat, 21 Mar 2015 13:35:40 +0800
X-Priority: 3
Message-ID: <tencent_2620B5127DD3422E188DB431@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_550D032C_091DB118_4BCDB682
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64

SGksIGFsbDoNCg0KDQoNCg0KV2hlbiBJIGV4aXQgdGhlIGNvbnNvbGUgb2Ygc3Bhcmstc3Fs
77yMIHRoZSBmb2xsb3dpbmcgZXhjZXB0aW9uIHRocm93ZWQuLi4uLi4NCg0KDQpNeSBzcGFy
ayB2ZXJzaW9uIGlzIDEuMy4wLCBoYWRvb3AgdmVyc2lvbiBpcyAyLjIuMA0KDQoNCkV4Y2Vw
dGlvbiBpbiB0aHJlYWQgIlRocmVhZC0zIiBqYXZhLmlvLklPRXhjZXB0aW9uOiBGaWxlc3lz
dGVtIGNsb3NlZA0KICAgICAgICBhdCBvcmcuYXBhY2hlLmhhZG9vcC5oZGZzLkRGU0NsaWVu
dC5jaGVja09wZW4oREZTQ2xpZW50LmphdmE6NjI5KQ0KICAgICAgICBhdCBvcmcuYXBhY2hl
LmhhZG9vcC5oZGZzLkRGU0NsaWVudC5nZXRGaWxlSW5mbyhERlNDbGllbnQuamF2YToxNjc3
KQ0KICAgICAgICBhdCBvcmcuYXBhY2hlLmhhZG9vcC5oZGZzLkRpc3RyaWJ1dGVkRmlsZVN5
c3RlbSQxNy5kb0NhbGwoRGlzdHJpYnV0ZWRGaWxlU3lzdGVtLmphdmE6MTEwNikNCiAgICAg
ICAgYXQgb3JnLmFwYWNoZS5oYWRvb3AuaGRmcy5EaXN0cmlidXRlZEZpbGVTeXN0ZW0kMTcu
ZG9DYWxsKERpc3RyaWJ1dGVkRmlsZVN5c3RlbS5qYXZhOjExMDIpDQogICAgICAgIGF0IG9y
Zy5hcGFjaGUuaGFkb29wLmZzLkZpbGVTeXN0ZW1MaW5rUmVzb2x2ZXIucmVzb2x2ZShGaWxl
U3lzdGVtTGlua1Jlc29sdmVyLmphdmE6ODEpDQogICAgICAgIGF0IG9yZy5hcGFjaGUuaGFk
b29wLmhkZnMuRGlzdHJpYnV0ZWRGaWxlU3lzdGVtLmdldEZpbGVTdGF0dXMoRGlzdHJpYnV0
ZWRGaWxlU3lzdGVtLmphdmE6MTEwMikNCiAgICAgICAgYXQgb3JnLmFwYWNoZS5oYWRvb3Au
ZnMuRmlsZVN5c3RlbS5leGlzdHMoRmlsZVN5c3RlbS5qYXZhOjEzOTcpDQogICAgICAgIGF0
IG9yZy5hcGFjaGUuc3Bhcmsuc2NoZWR1bGVyLkV2ZW50TG9nZ2luZ0xpc3RlbmVyLnN0b3Ao
RXZlbnRMb2dnaW5nTGlzdGVuZXIuc2NhbGE6MTk2KQ0KICAgICAgICBhdCBvcmcuYXBhY2hl
LnNwYXJrLlNwYXJrQ29udGV4dCQkYW5vbmZ1biRzdG9wJDQuYXBwbHkoU3BhcmtDb250ZXh0
LnNjYWxhOjEzODgpDQogICAgICAgIGF0IG9yZy5hcGFjaGUuc3BhcmsuU3BhcmtDb250ZXh0
JCRhbm9uZnVuJHN0b3AkNC5hcHBseShTcGFya0NvbnRleHQuc2NhbGE6MTM4OCkNCiAgICAg
ICAgYXQgc2NhbGEuT3B0aW9uLmZvcmVhY2goT3B0aW9uLnNjYWxhOjIzNikNCiAgICAgICAg
YXQgb3JnLmFwYWNoZS5zcGFyay5TcGFya0NvbnRleHQuc3RvcChTcGFya0NvbnRleHQuc2Nh
bGE6MTM4OCkNCiAgICAgICAgYXQgb3JnLmFwYWNoZS5zcGFyay5zcWwuaGl2ZS50aHJpZnRz
ZXJ2ZXIuU3BhcmtTUUxFbnYkLnN0b3AoU3BhcmtTUUxFbnYuc2NhbGE6NjYpDQogICAgICAg
IGF0IG9yZy5hcGFjaGUuc3Bhcmsuc3FsLmhpdmUudGhyaWZ0c2VydmVyLlNwYXJrU1FMQ0xJ
RHJpdmVyJCRhbm9uJDEucnVuKFNwYXJrU1FMQ0xJRHJpdmVyLnNjYWxhOjEwNynigI0=

------=_NextPart_550D032C_091DB118_4BCDB682--




From dev-return-12090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 06:30:18 2015
Return-Path: <dev-return-12090-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47D0F10058
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 06:30:18 +0000 (UTC)
Received: (qmail 95214 invoked by uid 500); 21 Mar 2015 06:30:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95130 invoked by uid 500); 21 Mar 2015 06:30:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95118 invoked by uid 99); 21 Mar 2015 06:30:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 06:30:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 06:29:41 +0000
Received: by qgez102 with SMTP id z102so21306912qge.3
        for <dev@spark.apache.org>; Fri, 20 Mar 2015 23:29:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=2x5lCO5bJfC6iE3Lm8Y3VNigbQUlvKghW4qSJnvBDFY=;
        b=mlIBUVlrzreMz/hQl8BGzvKR5TKZEKZDWJyZHgjRBrq3WorKdLT+BVnMMIaoQQKznE
         Bm/dTLujffHzB6iVSGHXNBrzA0PxKDmv3t2LPmURNb4488A7wOLY8sIzhTlHC5nPYnhM
         AraBUvuwwCwn0MpEMsYHXuLjOpi6HSs9w6HD9Ob3ZUbSk+wDmEy0v2pyLZnTD2WTDRu1
         JttPVA+eJU8BDQ4/LpSR2R0T8mEvQaSlWtSCKSgA4YUkloKWblKZFNtikfhQOCfxppIF
         59pUQBRdeqaUMUpkA8Ghfk3j81plWONzswsfRYLCFlFYSEEE8HYBZYdhVsnOLjPzqCaP
         ZdwQ==
X-Received: by 10.55.42.17 with SMTP id q17mr156612699qkh.61.1426919379685;
        Fri, 20 Mar 2015 23:29:39 -0700 (PDT)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id 21sm4632115qgi.36.2015.03.20.23.29.38
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 20 Mar 2015 23:29:38 -0700 (PDT)
Date: Fri, 20 Mar 2015 23:29:38 -0700 (PDT)
X-Google-Original-Date: Sat, 21 Mar 2015 06:29:38 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1426919378095.915ff721@Nodemailer>
In-Reply-To: <550C9E28.8010404@exensa.com>
References: <550C9E28.8010404@exensa.com>
X-Orchestra-Oid: 846700C5-361C-4F38-9838-8DE4F8957B88
X-Orchestra-Sig: bea71cdbf86246fae08c2852c9a707d8e6516722
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1496202878392621837
X-Orchestra-Thrid-Sig: fb862695bc37850718275adb20905228088ec802
X-Orchestra-Account: f7d347e75551f9f28820864ea8c8f93844095837
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: "Guillaume Pitel" <guillaume.pitel@exensa.com>
Cc: dev@spark.apache.org
Subject: Re: Directly broadcasting (sort of) RDDs
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1426919378813"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1426919378813
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

There is block matrix in Spark 1.3 -=C2=A0http://spark.apache.=
org/docs/latest/mllib-data-types.html#blockmatrix





However I believe it only supports dense matrix blocks.




Still, might be possible to use it or exetend=C2=A0




JIRAs:


https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434





Was based on=C2=A0


https://github.com/amplab/ml-matrix





Another lib:


https://github.com/PasaLab/marlin/blob/master/README.md







=E2=80=94
Sent from Mailbox

On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
<guillaume.pitel@exensa.com> wrote:

> Hi,
> I have an idea that I would like to discuss with the Spark devs. The=20
> idea comes from a very real problem that I have struggled with since=20
> almost a year. My problem is very simple, it's a dense matrix * =
sparse=20
> matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which =
is=20
> divided in X large blocks (one block per partition), and a sparse =
matrix=20
> RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. =
The=20
> most efficient way to perform the operation is to collectAsMap() the=20
> dense matrix and broadcast it, then perform the block-local=20
> mutliplications, and combine the results by column.
> This is quite fine, unless the matrix is too big to fit in memory=20
> (especially since the multiplication is performed several times=20
> iteratively, and the broadcasts are not always cleaned from memory as =
I=20
> would naively expect).
> When the dense matrix is too big, a second solution is to split the =
big=20
> sparse matrix in several RDD, and do several broadcasts. Doing this=20
> creates quite a big overhead, but it mostly works, even though I =
often=20
> face some problems with unaccessible broadcast files, for instance.
> Then there is the terrible but apparently very effective good old join.=
=20
> Since X blocks of the sparse matrix use the same block from the dense=20
> matrix, I suspect that the dense matrix is somehow replicated X times=20
> (either on disk or in the network), which is the reason why the join=20
> takes so much time.
> After this bit of a context, here is my idea : would it be possible =
to=20
> somehow =22broadcast=22 (or maybe more accurately, share or serve) a=20
> persisted RDD which is distributed on all workers, in a way that would,=
=20
> a bit like the IndexedRDD, allow a task to access a partition or an=20
> element of a partition in the closure, with a worker-local memory =
cache=20
> . i.e. the information about where each block resides would be=20
> distributed on the workers, to allow them to access parts of the RDD=20
> directly. I think that's already a bit how RDD are shuffled =3F
> The RDD could stay distributed (no need to collect then broadcast), =
and=20
> only necessary transfers would be required.
> Is this a bad idea, is it already implemented somewhere (I would love =
it=20
> !) =3For is it something that could add efficiency not only for my =
use=20
> case, but maybe for others =3F Could someone give me some hint about how =
I=20
> could add this possibility to Spark =3F I would probably try to extend =
a=20
> RDD into a specific SharedIndexedRDD with a special lookup that would =
be=20
> allowed from tasks as a special case, and that would try to contact =
the=20
> blockManager and reach the corresponding data from the right worker.
> Thanks in advance for your advices
> Guillaume
> --=20
> eXenSa
>=20=09
> *Guillaume PITEL, Pr=C3=A9sident*
> +33(0)626 222 431
> eXenSa S.A.S. <http://www.exensa.com/>
> 41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
------Nodemailer-0.5.0-?=_1-1426919378813--

From dev-return-12091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 07:10:07 2015
Return-Path: <dev-return-12091-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1DB4110107
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 07:10:07 +0000 (UTC)
Received: (qmail 19315 invoked by uid 500); 21 Mar 2015 07:10:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19234 invoked by uid 500); 21 Mar 2015 07:10:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19222 invoked by uid 99); 21 Mar 2015 07:10:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 07:10:05 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of guillaume.pitel@exensa.com designates 91.121.232.90 as permitted sender)
Received: from [91.121.232.90] (HELO mail.exensa.com) (91.121.232.90)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 07:09:39 +0000
Received: from localhost (localhost [127.0.0.1])
	by mail.exensa.com (Postfix) with ESMTP id 946D3641C04
	for <dev@spark.apache.org>; Sat, 21 Mar 2015 07:09:37 +0000 (UTC)
Received: from mail.exensa.com ([127.0.0.1])
	by localhost (mail.exensa.com [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id BLhrRM+veX3r for <dev@spark.apache.org>;
	Sat, 21 Mar 2015 07:09:37 +0000 (UTC)
Received: from [192.168.1.101] (unknown [89.157.16.87])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128 bits))
	(No client certificate requested)
	by mail.exensa.com (Postfix) with ESMTPSA id F1C246410CB
	for <dev@spark.apache.org>; Sat, 21 Mar 2015 07:09:36 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=exensa.com;
	s=default; t=1426921777;
	bh=465QK1MkwSwIbmdMNb2B2PBvNfFiWX0SOevMmklhVZ0=;
	h=Date:From:To:Subject:References:In-Reply-To;
	b=sSKaYy/IVN89PVR0X7PmYfngHxnqVqlXvDE3aKTtm5QpCkgJdnvCXpsDGr+CfeG9f
	 7F2uwtOdOyjcj/pvQ7GLjPxAsD0DXLUbTZ3bwn9C9Tr6ByMQXFIlBhl/Vw5IR0X
Message-ID: <550D1985.8070307@exensa.com>
Date: Sat, 21 Mar 2015 08:11:01 +0100
From: Guillaume Pitel <guillaume.pitel@exensa.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Directly broadcasting (sort of) RDDs
References: <550C9E28.8010404@exensa.com> <1426919378095.915ff721@Nodemailer>
In-Reply-To: <1426919378095.915ff721@Nodemailer>
Content-Type: multipart/alternative;
 boundary="------------060906050006030104070408"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------060906050006030104070408
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Hi,

Thanks for your answer. This is precisely the use case I'm interested 
in, but I know it already, I should have mentionned it. Unfortunately 
this implementation of BlockMatrix has (in my opinion) some 
disadvantages (the fact that it split the matrix by range instead of 
using a modulo is bad for block skewness). Besides, and more 
importantly, as I was writing, it uses the join solution (actually a 
cogroup : 
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala, 
line 361). The reduplication of the elements of the dense matrix is thus 
dependent on the block size.

Actually I'm wondering if what I want to achieve could be made with a 
simple modification to the join, allowing a partition to be weakly 
cached wafter being retrieved.

Guillaume


> There is block matrix in Spark 1.3 - http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix
>
>
>
>
>
> However I believe it only supports dense matrix blocks.
>
>
>
>
> Still, might be possible to use it or exetend
>
>
>
>
> JIRAs:
>
>
> https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434
>
>
>
>
>
> Was based on
>
>
> https://github.com/amplab/ml-matrix
>
>
>
>
>
> Another lib:
>
>
> https://github.com/PasaLab/marlin/blob/master/README.md
>
>
>
>
>
>
>
> —
> Sent from Mailbox
>
> On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
> <guillaume.pitel@exensa.com> wrote:
>
>> Hi,
>> I have an idea that I would like to discuss with the Spark devs. The
>> idea comes from a very real problem that I have struggled with since
>> almost a year. My problem is very simple, it's a dense matrix * sparse
>> matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is
>> divided in X large blocks (one block per partition), and a sparse matrix
>> RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The
>> most efficient way to perform the operation is to collectAsMap() the
>> dense matrix and broadcast it, then perform the block-local
>> mutliplications, and combine the results by column.
>> This is quite fine, unless the matrix is too big to fit in memory
>> (especially since the multiplication is performed several times
>> iteratively, and the broadcasts are not always cleaned from memory as I
>> would naively expect).
>> When the dense matrix is too big, a second solution is to split the big
>> sparse matrix in several RDD, and do several broadcasts. Doing this
>> creates quite a big overhead, but it mostly works, even though I often
>> face some problems with unaccessible broadcast files, for instance.
>> Then there is the terrible but apparently very effective good old join.
>> Since X blocks of the sparse matrix use the same block from the dense
>> matrix, I suspect that the dense matrix is somehow replicated X times
>> (either on disk or in the network), which is the reason why the join
>> takes so much time.
>> After this bit of a context, here is my idea : would it be possible to
>> somehow "broadcast" (or maybe more accurately, share or serve) a
>> persisted RDD which is distributed on all workers, in a way that would,
>> a bit like the IndexedRDD, allow a task to access a partition or an
>> element of a partition in the closure, with a worker-local memory cache
>> . i.e. the information about where each block resides would be
>> distributed on the workers, to allow them to access parts of the RDD
>> directly. I think that's already a bit how RDD are shuffled ?
>> The RDD could stay distributed (no need to collect then broadcast), and
>> only necessary transfers would be required.
>> Is this a bad idea, is it already implemented somewhere (I would love it
>> !) ?or is it something that could add efficiency not only for my use
>> case, but maybe for others ? Could someone give me some hint about how I
>> could add this possibility to Spark ? I would probably try to extend a
>> RDD into a specific SharedIndexedRDD with a special lookup that would be
>> allowed from tasks as a special case, and that would try to contact the
>> blockManager and reach the corresponding data from the right worker.
>> Thanks in advance for your advices
>> Guillaume
>> -- 
>> eXenSa
>> 	
>> *Guillaume PITEL, Président*
>> +33(0)626 222 431
>> eXenSa S.A.S. <http://www.exensa.com/>
>> 41, rue Périer - 92120 Montrouge - FRANCE
>> Tel +33(0)184 163 677 / Fax +33(0)972 283 705


-- 
eXenSa

	
*Guillaume PITEL, Président*
+33(0)626 222 431

eXenSa S.A.S. <http://www.exensa.com/>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705


--------------060906050006030104070408
Content-Type: multipart/related;
 boundary="------------040106060608030102050003"


--------------040106060608030102050003
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: 8bit

<html>
  <head>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <div class="moz-cite-prefix">Hi, <br>
      <br>
      Thanks for your answer. This is precisely the use case I'm
      interested in, but I know it already, I should have mentionned it.
      Unfortunately this implementation of BlockMatrix has (in my
      opinion) some disadvantages (the fact that it split the matrix by
      range instead of using a modulo is bad for block skewness).
      Besides, and more importantly, as I was writing, it uses the join
      solution (actually a cogroup :
      <a class="moz-txt-link-freetext" href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala">https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala</a>,
      line 361). The reduplication of the elements of the dense matrix
      is thus dependent on the block size.<br>
      <br>
      Actually I'm wondering if what I want to achieve could be made
      with a simple modification to the join, allowing a partition to be
      weakly cached wafter being retrieved. <br>
      <br>
      Guillaume<br>
      <br>
      <br>
    </div>
    <blockquote cite="mid:1426919378095.915ff721@Nodemailer" type="cite">
      <pre wrap="">There is block matrix in Spark 1.3 - <a class="moz-txt-link-freetext" href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix</a>





However I believe it only supports dense matrix blocks.




Still, might be possible to use it or exetend 




JIRAs:


<a class="moz-txt-link-freetext" href="https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434">https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434</a>





Was based on 


<a class="moz-txt-link-freetext" href="https://github.com/amplab/ml-matrix">https://github.com/amplab/ml-matrix</a>





Another lib:


<a class="moz-txt-link-freetext" href="https://github.com/PasaLab/marlin/blob/master/README.md">https://github.com/PasaLab/marlin/blob/master/README.md</a>







—
Sent from Mailbox

On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
<a class="moz-txt-link-rfc2396E" href="mailto:guillaume.pitel@exensa.com">&lt;guillaume.pitel@exensa.com&gt;</a> wrote:

</pre>
      <blockquote type="cite">
        <pre wrap="">Hi,
I have an idea that I would like to discuss with the Spark devs. The 
idea comes from a very real problem that I have struggled with since 
almost a year. My problem is very simple, it's a dense matrix * sparse 
matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is 
divided in X large blocks (one block per partition), and a sparse matrix 
RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The 
most efficient way to perform the operation is to collectAsMap() the 
dense matrix and broadcast it, then perform the block-local 
mutliplications, and combine the results by column.
This is quite fine, unless the matrix is too big to fit in memory 
(especially since the multiplication is performed several times 
iteratively, and the broadcasts are not always cleaned from memory as I 
would naively expect).
When the dense matrix is too big, a second solution is to split the big 
sparse matrix in several RDD, and do several broadcasts. Doing this 
creates quite a big overhead, but it mostly works, even though I often 
face some problems with unaccessible broadcast files, for instance.
Then there is the terrible but apparently very effective good old join. 
Since X blocks of the sparse matrix use the same block from the dense 
matrix, I suspect that the dense matrix is somehow replicated X times 
(either on disk or in the network), which is the reason why the join 
takes so much time.
After this bit of a context, here is my idea : would it be possible to 
somehow "broadcast" (or maybe more accurately, share or serve) a 
persisted RDD which is distributed on all workers, in a way that would, 
a bit like the IndexedRDD, allow a task to access a partition or an 
element of a partition in the closure, with a worker-local memory cache 
. i.e. the information about where each block resides would be 
distributed on the workers, to allow them to access parts of the RDD 
directly. I think that's already a bit how RDD are shuffled ?
The RDD could stay distributed (no need to collect then broadcast), and 
only necessary transfers would be required.
Is this a bad idea, is it already implemented somewhere (I would love it 
!) ?or is it something that could add efficiency not only for my use 
case, but maybe for others ? Could someone give me some hint about how I 
could add this possibility to Spark ? I would probably try to extend a 
RDD into a specific SharedIndexedRDD with a special lookup that would be 
allowed from tasks as a special case, and that would try to contact the 
blockManager and reach the corresponding data from the right worker.
Thanks in advance for your advices
Guillaume
-- 
eXenSa
	
*Guillaume PITEL, Président*
+33(0)626 222 431
eXenSa S.A.S. <a class="moz-txt-link-rfc2396E" href="http://www.exensa.com/">&lt;http://www.exensa.com/&gt;</a>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705
</pre>
      </blockquote>
    </blockquote>
    <br>
    <br>
    <div class="moz-signature">-- <br>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <span style="font-size: 8.5pt; color: rgb(51, 51, 51);
        font-family: Helvetica;">
        <div style="width: 529px;" align="left">
          <table style="width: 524px;" border="0" cellpadding="0"
            cellspacing="0">
            <tbody>
              <tr>
                <td style="border-top: medium none rgb(236, 233, 216);
                  border-right: medium none rgb(236, 233, 216);
                  border-bottom: medium none rgb(236, 233, 216);
                  padding-right: 0cm; width: 219px; height: 33.75pt;
                  background-color: transparent; vertical-align:
                  middle;">
                  <center style="width: 210px;"><img style="border: 0px
                      solid ; width: 160px; height: 64px;" alt="eXenSa"
                      src="cid:part1.09080406.07080002@exensa.com"></center>
                </td>
                <td style="padding-left:10px; border-top: medium none
                  rgb(236, 233, 216); border-right: medium none rgb(236,
                  233, 216); border-bottom: medium none rgb(236, 233,
                  216); padding-right: 0cm; width: 358px; height:
                  33.75pt; background-color: transparent;
                  vertical-align: top;">
                  <div style="text-align: left; width: 299px;"
                    align="left"> <span style="font-size: 7.5pt; color:
                      rgb(75, 80, 85); font-family: Helvetica;"> <b>Guillaume
                        PITEL, Président</b> <br>
                      +33(0)626 222 431<br>
                    </span> <br>
                    <span style="font-size: 7.5pt; color: #505050;
                      font-family: Helvetica;"><a
                        href="http://www.exensa.com/" target="_blank">eXenSa
                        S.A.S.</a> </span><br>
                    <span style="font-size: 7.5pt; color: rgb(75, 80,
                      85);"> <font face="Helvetica">41, rue Périer -
                        92120 Montrouge - FRANCE <br>
                        Tel +33(0)184 163 677 / Fax +33(0)972 283 705 </font>
                    </span> </div>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </span>
    </div>
  </body>
</html>

--------------040106060608030102050003
Content-Type: image/png;
 name="exensa_logo_mail.png"
Content-Transfer-Encoding: base64
Content-ID: <part1.09080406.07080002@exensa.com>
Content-Disposition: inline;
 filename="exensa_logo_mail.png"
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==
--------------040106060608030102050003--

--------------060906050006030104070408--

From dev-return-12092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 15:29:23 2015
Return-Path: <dev-return-12092-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B23A610B49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 15:29:23 +0000 (UTC)
Received: (qmail 70421 invoked by uid 500); 21 Mar 2015 15:29:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70339 invoked by uid 500); 21 Mar 2015 15:29:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 73752 invoked by uid 99); 21 Mar 2015 08:54:23 -0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of vinod.kc.in@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Sat, 21 Mar 2015 01:52:24 -0700 (MST)
From: vinodkc <vinod.kc.in@gmail.com>
To: dev@spark.apache.org
Message-ID: <CAHK2WQjv=cRFZ4f1kmd1DJ0SNwJwH-i28+Mp9ARLUqOtLovx1A@mail.gmail.com>
In-Reply-To: <tencent_2620B5127DD3422E188DB431@qq.com>
References: <tencent_2620B5127DD3422E188DB431@qq.com>
Subject: Re: Filesystem closed Exception
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_76678_796228222.1426927944238"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_76678_796228222.1426927944238
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Sea,

I've raised a  JIRA Issue on this :
https://issues.apache.org/jira/browse/SPARK-6445 . Making a PR  now

On Sat, Mar 21, 2015 at 11:06 AM, Sea [via Apache Spark Developers List] <
ml-node+s1001551n11145h68@n3.nabble.com> wrote:

> Hi, all:
>
>
>
>
> When I exit the console of spark-sql=EF=BC=8C the following exception
> throwed......
>
>
> My spark version is 1.3.0, hadoop version is 2.2.0
>
>
> Exception in thread "Thread-3" java.io.IOException: Filesystem closed
>         at org.apache.hadoop.hdfs.DFSClient.checkOpen(DFSClient.java:629)
>         at
> org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1677)
>         at
> org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSys=
tem.java:1106)
>
>         at
> org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSys=
tem.java:1102)
>
>         at
> org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolve=
r.java:81)
>
>         at
> org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFil=
eSystem.java:1102)
>
>         at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1397)
>         at
> org.apache.spark.scheduler.EventLoggingListener.stop(EventLoggingListener=
.scala:196)
>
>         at
> org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:13=
88)
>
>         at
> org.apache.spark.SparkContext$$anonfun$stop$4.apply(SparkContext.scala:13=
88)
>
>         at scala.Option.foreach(Option.scala:236)
>         at org.apache.spark.SparkContext.stop(SparkContext.scala:1388)
>         at
> org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.stop(SparkSQLEnv.scal=
a:66)
>
>         at
> org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$$anon$1.run(Spar=
kSQLCLIDriver.scala:107)=E2=80=8D
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Filesystem-clos=
ed-Exception-tp11145.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h4@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dunsubscribe_by_code&node=3D1&code=3Ddmlub2Qua2MuaW5AZ21h=
aWwuY29tfDF8MTk2Mjg4MTAzOA=3D=3D>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&bas=
e=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNa=
mespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabb=
leNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_sub=
scribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_in=
stant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/Filesystem-closed-Exception-tp11145p11148.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.
------=_Part_76678_796228222.1426927944238--

From dev-return-12093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 15:40:43 2015
Return-Path: <dev-return-12093-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6791C10B76
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 15:40:43 +0000 (UTC)
Received: (qmail 80689 invoked by uid 500); 21 Mar 2015 15:40:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80621 invoked by uid 500); 21 Mar 2015 15:40:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80609 invoked by uid 99); 21 Mar 2015 15:40:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 15:40:41 +0000
X-ASF-Spam-Status: No, hits=3.9 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of 261810726@qq.com designates 14.17.32.37 as permitted sender)
Received: from [14.17.32.37] (HELO smtpbg326.qq.com) (14.17.32.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 15:40:37 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1426952315; bh=vaAQO/B5kE0KAFUBk5c9l5tvvdzslBkdn05WREzQ2NE=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=d/z+AtnUqQQwYF2Yd5LwVmRov/JO/FdG52eo+ayAtAkZovPFVC3I7oAB7R4FF8m48
	 bRwqEw0vJ0C0xsvWG7nDob/LG6TF30aIJ2X7cescTFey2ByTNN32TqPip5mQJdgNY2
	 i9tzE8J65F9Jm8Ajw58HCppeNBRxdRsrFcc0vCMg=
X-QQ-FEAT: mmMowWBN6dYam/Y/9QUqiNaErJwTheWZseeoN39Owq1xlyL+UXJUS/8P1T0iK
	06DHM1/qZABLz0OxoBHTRyuTUqeMysRFGLGpA9zNkzJxseJTL1D/cgV08Ky3OVPJVfwh3F2
	+DUJvy6EcDdFUdrkWLlIwN2vL0pHQye7P1r0BSL4UuTLe5DFEgId5TxGLLgAjiD8zcEe/r9
	NdPBzDEI9XYoY0tAG0etdGkr1VBkLyA0=
X-QQ-SSF: 00000000000000F000000000000000Z
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 221.221.61.92
In-Reply-To: <CAHK2WQjv=cRFZ4f1kmd1DJ0SNwJwH-i28+Mp9ARLUqOtLovx1A@mail.gmail.com>
References: <tencent_2620B5127DD3422E188DB431@qq.com>
	<CAHK2WQjv=cRFZ4f1kmd1DJ0SNwJwH-i28+Mp9ARLUqOtLovx1A@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail249t1426952314t6042972
From: "=?utf-8?B?U2Vh?=" <261810726@qq.com>
To: "=?utf-8?B?dmlub2RrYw==?=" <vinod.kc.in@gmail.com>, "=?utf-8?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?=" <dev@spark.apache.org>
Subject: =?utf-8?B?5Zue5aSN77yaIEZpbGVzeXN0ZW0gY2xvc2VkIEV4?=
 =?utf-8?B?Y2VwdGlvbg==?=
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_550D907A_09B92770_58E79D8E"
Content-Transfer-Encoding: 8Bit
Date: Sat, 21 Mar 2015 23:38:34 +0800
X-Priority: 3
Message-ID: <tencent_3BD0C5BA532FA82B1590E3D5@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 1935999009
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_550D907A_09B92770_58E79D8E
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64
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------=_NextPart_550D907A_09B92770_58E79D8E--




From dev-return-12094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 21 16:51:24 2015
Return-Path: <dev-return-12094-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C279D10D1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 21 Mar 2015 16:51:24 +0000 (UTC)
Received: (qmail 63634 invoked by uid 500); 21 Mar 2015 16:51:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63554 invoked by uid 500); 21 Mar 2015 16:51:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63542 invoked by uid 99); 21 Mar 2015 16:51:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 16:51:23 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.178 as permitted sender)
Received: from [209.85.217.178] (HELO mail-lb0-f178.google.com) (209.85.217.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 21 Mar 2015 16:50:57 +0000
Received: by lbbsy1 with SMTP id sy1so94194453lbb.1
        for <dev@spark.apache.org>; Sat, 21 Mar 2015 09:50:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=bdNXaacJMV17qohA0FPUFGHz+GxlyAQGDEPRmXQjgIM=;
        b=SjtKxTR0UZqgk0ib0C9rQQK+vz+arrRfAjSJ+M8LstDw60ZN0fQgyu1rj0TFpj4KxD
         XRm1byyX2MG0wa56bejases57gPFxAA4lW/ZT43Jd6F5e3A/m/Z2QiQrSAT+OgU6MSnA
         mt+/NLJaRVuAJ6kAES8E278edgo7edjbKq42UiWDOt6IRkAvCkYJvAEpHhSO7XhY+qoq
         K5EywpdK5g3shwtvnMiP/GWH2vAlEqPtY/rfMIUpiOPfPH4aQGA3xpYBVKTQ/ltt4i32
         QqWnMv0GdQPY8mpxB5AJgDE2Pg+34/fb9hcATm7337ExM9hrTymdqDPuwEsahtevRLSV
         Vn0A==
MIME-Version: 1.0
X-Received: by 10.112.212.106 with SMTP id nj10mr58204075lbc.36.1426956611413;
 Sat, 21 Mar 2015 09:50:11 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Sat, 21 Mar 2015 09:50:11 -0700 (PDT)
In-Reply-To: <CABvqBfmR5waEKrW30y4GQFOA6p6yutL7tN73wNa88fN4vb4ZzA@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE2A1AD@G9W0737.americas.hpqcorp.net>
	<CA+B-+fxi=9RkR_YMEtcb6wKb+r2BNUN_G-SqhswAJ3bEYXUCRA@mail.gmail.com>
	<09B9C296-EBE4-48C6-8B59-4BD26EDB6F3D@hp.com>
	<CA+B-+fycWb5JCO1yA2Kh970g+7cAORm_UpQYjpC5OadKfFkx3w@mail.gmail.com>
	<FD914DA8-C1E7-48CA-B1CE-F6597265901B@hp.com>
	<CA+B-+fxSC6TA6KU81KXuFpvf1NnDpsfyMcUzAs7_8J_nJzrZ2w@mail.gmail.com>
	<CABvqBfmR5waEKrW30y4GQFOA6p6yutL7tN73wNa88fN4vb4ZzA@mail.gmail.com>
Date: Sat, 21 Mar 2015 09:50:11 -0700
Message-ID: <CA+B-+fzCuO=xB1h4s9WWe6q1jtJseXj45ZOzhHxHGvEwHYDgJw@mail.gmail.com>
Subject: Re: Which linear algebra interface to use within Spark MLlib?
From: Debasish Das <debasish.das83@gmail.com>
To: Burak Yavuz <brkyvz@gmail.com>
Cc: "Ulanov, Alexander" <alexander.ulanov@hp.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11346ddaeeee2a0511cf3980
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11346ddaeeee2a0511cf3980
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Burak,

For local linear algebra package why are we not extending breeze ?

Breeze is a mllib dependency...Also that way the local linear algebra
package will be used by other scala based frontend APIs as well that do not
necessarily pull in Spark dependencies...

Thanks.
Deb


On Fri, Mar 20, 2015 at 6:54 PM, Burak Yavuz <brkyvz@gmail.com> wrote:

> Hi,
>
> We plan to add a more comprehensive local linear algebra package for MLli=
b
> 1.4. This local linear algebra package can then easily be extended to
> BlockMatrix to support the same operations in a distributed fashion.
>
> You may find the JIRA to track this here: SPARK-6442
> <https://issues.apache.org/jira/browse/SPARK-6442>
>
> The design doc is here: http://goo.gl/sf5LCE
>
> We would very much appreciate your feedback and input.
>
> Best,
> Burak
>
> On Thu, Mar 19, 2015 at 3:06 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Yeah it will be better if we consolidate the development on one of
>> them...either Breeze or mllib.BLAS...
>>
>> On Thu, Mar 19, 2015 at 2:25 PM, Ulanov, Alexander <
>> alexander.ulanov@hp.com>
>> wrote:
>>
>> >  Thanks for quick response.
>> >
>> >  I can use linealg.BLAS.gemm, and this means that I have to use MLlib
>> > Matrix. The latter does not support some useful functionality needed f=
or
>> > optimization. For example, creation of Matrix given matrix size, array
>> and
>> > offset in this array. This means that I will need to create matrix in
>> > Breeze and convert it to MLlib. Also, linalg.BLAS misses some useful
>> BLAS
>> > functions I need, that can be found in Breeze (and netlib-java). The
>> same
>> > concerns are applicable to MLlib Vector.
>> >
>> > Best regards, Alexander
>> >
>> > 19.03.2015, =D0=B2 14:16, "Debasish Das" <debasish.das83@gmail.com>
>> =D0=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
>> >
>> >   I think for Breeze we are focused on dot and dgemv right now (along
>> > with several other matrix vector style operations)...
>> >
>> >  For dgemm it is tricky since you need to do add dgemm for both
>> > DenseMatrix and CSCMatrix...and for CSCMatrix you need to get somethin=
g
>> > like SuiteSparse which is under lgpl...so we have to think more on it.=
.
>> >
>> >  For now can't you use dgemm directly from mllib.linalg.BLAS ? It's in
>> > master...
>> >
>> >
>> > On Thu, Mar 19, 2015 at 1:49 PM, Ulanov, Alexander <
>> > alexander.ulanov@hp.com> wrote:
>> >
>> >>  Thank you! When do you expect to have gemm in Breeze and that versio=
n
>> >> of Breeze to ship with MLlib?
>> >>
>> >>  Also, could someone please elaborate on the linalg.BLAS and Matrix?
>> Are
>> >> they going to be developed further, should in long term all developer=
s
>> use
>> >> them?
>> >>
>> >> Best regards, Alexander
>> >>
>> >> 18.03.2015, =D0=B2 23:21, "Debasish Das" <debasish.das83@gmail.com>
>> >> =D0=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
>> >>
>> >>    dgemm dgemv and dot come to Breeze and Spark through netlib-java..=
..
>> >>
>> >>  Right now both in dot and dgemv Breeze does a extra memory allocate
>> but
>> >> we already found the issue and we are working on adding a common trai=
t
>> that
>> >> will provide a sink operation (basically memory will be allocated by
>> >> user)...adding more BLAS operators in breeze will also help in genera=
l
>> as
>> >> lot more operations are defined over there...
>> >>
>> >>
>> >> On Wed, Mar 18, 2015 at 8:09 PM, Ulanov, Alexander <
>> >> alexander.ulanov@hp.com> wrote:
>> >>
>> >>> Hi,
>> >>>
>> >>> Currently I am using Breeze within Spark MLlib for linear algebra. I
>> >>> would like to reuse previously allocated matrices for storing the
>> result of
>> >>> matrices multiplication, i.e. I need to use "gemm" function
>> C:=3Dq*A*B+p*C,
>> >>> which is missing in Breeze (Breeze automatically allocates a new
>> matrix to
>> >>> store the result of multiplication). Also, I would like to minimize
>> gemm
>> >>> calls that Breeze does. Should I use mllib.linalg.BLAS functions
>> instead?
>> >>> While it has gemm and axpy, it has rather limited number of
>> operations. For
>> >>> example, I need sum of the matrix by row or by columns, or applying =
a
>> >>> function to all elements in a matrix. Also, MLlib Vector and Matrix
>> >>> interfaces that linalg.BLAS operates seems to be rather undeveloped.
>> Should
>> >>> I use plain netlib-java instead (will it remain in MLlib in future
>> >>> releases)?
>> >>>
>> >>> Best regards, Alexander
>> >>>
>> >>
>> >>
>> >
>>
>
>

--001a11346ddaeeee2a0511cf3980--

From dev-return-12095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 22 15:41:01 2015
Return-Path: <dev-return-12095-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19410104F1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 22 Mar 2015 15:41:01 +0000 (UTC)
Received: (qmail 12075 invoked by uid 500); 22 Mar 2015 15:40:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11998 invoked by uid 500); 22 Mar 2015 15:40:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11985 invoked by uid 99); 22 Mar 2015 15:40:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Mar 2015 15:40:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of marek.wiewiorka@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Mar 2015 15:40:33 +0000
Received: by iedfl3 with SMTP id fl3so23769343ied.1
        for <dev@spark.apache.org>; Sun, 22 Mar 2015 08:40:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=6R4OrbBFh1bBinQo/ZZchmrtdjDNGFj1Jpfcon5Uvq0=;
        b=OAzyR53505ALxBzfsMqeZ80lnqXKtmd5Nmksc6rnwZBhT9fBDuaWfdRYZM+yy0gkWU
         +5nRtpXzjFDQ6EgKj+lBhHa/a+9tVtRbvZ6UGaj4ie3tfzDql5UjzJVEQrcKFLeUoRH1
         mbqXOn3Eg11yG7c/VUhoohklDPuww6borz/xy9vY0qRUbnNTnAPYUl9Z2ZdE0x9eepRR
         zEku1iRHKTkXC6zBelahIfwmCisHqPE2QbSCMx0+U7WBPThV3cmkz1tKFWUlWTc+2+AE
         rmpopYTKhtUjuKQ/qcRVPRKObKuTg5sixF0xy+GWypCBlznjJu9XWxDU67PMZfFwH+U4
         cXcA==
X-Received: by 10.50.73.99 with SMTP id k3mr9043548igv.21.1427038831312; Sun,
 22 Mar 2015 08:40:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.36.56.136 with HTTP; Sun, 22 Mar 2015 08:39:51 -0700 (PDT)
From: Marek Wiewiorka <marek.wiewiorka@gmail.com>
Date: Sun, 22 Mar 2015 16:39:51 +0100
Message-ID: <CAJbo4nf6ffhvfG8qxFFPKDthrFxhAkPOLOyindWeUFK-M=5PWQ@mail.gmail.com>
Subject: lower&upperBound not working/spark 1/3
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01184e229f05830511e25e36
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184e229f05830511e25e36
Content-Type: text/plain; charset=UTF-8

Hi All - I try to use the new SQLContext API for populating DataFrame from
jdbc data source.
like this:

val jdbcDF = sqlContext.jdbc(url =
"jdbc:postgresql://localhost:5430/dbname?user=user&password=111", table =
"se_staging.exp_table3" ,columnName="cs_id",lowerBound=1 ,upperBound =
10000, numPartitions=12 )

No matter how I set lower and upper bounds I always get all the rows from
my table.
The API is marked as experimental so I assume there might by some bugs in
it but
did anybody come across a similar issue?

Thanks!
Marek

--089e01184e229f05830511e25e36--

From dev-return-12096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 22 21:20:54 2015
Return-Path: <dev-return-12096-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D7EE610D9F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 22 Mar 2015 21:20:54 +0000 (UTC)
Received: (qmail 32438 invoked by uid 500); 22 Mar 2015 21:20:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32344 invoked by uid 500); 22 Mar 2015 21:20:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32333 invoked by uid 99); 22 Mar 2015 21:20:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Mar 2015 21:20:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Mar 2015 21:20:47 +0000
Received: by labe2 with SMTP id e2so45421575lab.3
        for <dev@spark.apache.org>; Sun, 22 Mar 2015 14:20:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=OToxfkwNWuKQIry5HyK7Zl0pXKazQuOvjkK2ZY/L9Tc=;
        b=AhAjrj3Xy3zpn2Sv08yajAESNh7xUn/JKMgMAsN5Oah8ti4eMUNMsNIS1mMDVETZBT
         cDyBw2WOCfi0ioLxohM8dKn+wrbffmO/Kjage/qghyGc6sxpXh5JNlq9Ikt61tNi3zX0
         otqPBe1SW2ryUWR9S4PQECZJh/mm6Rx5wtBVRrVjEJ44kmdkey8uFF5Pc6oCL5ClsTnt
         q/O0k38fsmL0SalVCG0DoxDQiFk/tVQa8KWBpB433bAHRjxf/4XB62jxD9NOdq/T40RO
         lH4V/nkwl+2sGDWzIGmqwMCcnpw2DnhL9ayJ2vHIVAWelcfE/qJC6hbraKVcXV5wCSXv
         I8jw==
X-Gm-Message-State: ALoCoQkgFydPbyXsiayIiSbC2HUH99Jvq/PcNjKi6LVt+oJcoznjT/PIwy7SppOUPLLqDkGT3N7Q
X-Received: by 10.112.140.74 with SMTP id re10mr82252712lbb.80.1427059205276;
 Sun, 22 Mar 2015 14:20:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.213.18 with HTTP; Sun, 22 Mar 2015 14:19:45 -0700 (PDT)
In-Reply-To: <CAJbo4nf6ffhvfG8qxFFPKDthrFxhAkPOLOyindWeUFK-M=5PWQ@mail.gmail.com>
References: <CAJbo4nf6ffhvfG8qxFFPKDthrFxhAkPOLOyindWeUFK-M=5PWQ@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Sun, 22 Mar 2015 14:19:45 -0700
Message-ID: <CAAswR-5fup6Tz=1ZPZn29vNdBOV7j54K2BQKjvVr7c+gzvSv2A@mail.gmail.com>
Subject: Re: lower&upperBound not working/spark 1/3
To: Marek Wiewiorka <marek.wiewiorka@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c341e0011b630511e71de5
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c341e0011b630511e71de5
Content-Type: text/plain; charset=UTF-8

I have not heard this reported yet, but your invocation looks correct to
me.  Can you open a JIRA?

On Sun, Mar 22, 2015 at 8:39 AM, Marek Wiewiorka <marek.wiewiorka@gmail.com>
wrote:

> Hi All - I try to use the new SQLContext API for populating DataFrame from
> jdbc data source.
> like this:
>
> val jdbcDF = sqlContext.jdbc(url =
> "jdbc:postgresql://localhost:5430/dbname?user=user&password=111", table =
> "se_staging.exp_table3" ,columnName="cs_id",lowerBound=1 ,upperBound =
> 10000, numPartitions=12 )
>
> No matter how I set lower and upper bounds I always get all the rows from
> my table.
> The API is marked as experimental so I assume there might by some bugs in
> it but
> did anybody come across a similar issue?
>
> Thanks!
> Marek
>

--001a11c341e0011b630511e71de5--

From dev-return-12097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 22 22:58:07 2015
Return-Path: <dev-return-12097-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9BDF10FD0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 22 Mar 2015 22:58:07 +0000 (UTC)
Received: (qmail 25060 invoked by uid 500); 22 Mar 2015 22:57:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24980 invoked by uid 500); 22 Mar 2015 22:57:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24964 invoked by uid 99); 22 Mar 2015 22:57:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Mar 2015 22:57:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 22 Mar 2015 22:57:55 +0000
Received: by igbud6 with SMTP id ud6so28209615igb.1
        for <dev@spark.apache.org>; Sun, 22 Mar 2015 15:56:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=0uAQ0leLULcgtgJ+vkC6wzBq02cRxcoDBnwy3Q8aiPA=;
        b=eqs8jYyIMhJJceCje+p+4M8H20ipOXt9J2SEOUOoaa0ePB1AsH3MM812pQFZHXYu4e
         6UN/Jri8k7vjz87BH/9sjr/BazQaEs+U2gctQfv0kWry8D2ketxevwaSOTAHiXpwWxZ7
         I/pwr1z+cIAJld3cHvix2kOAvnGpI2PN90mWPQTXJYy7ymAA75J9nQFDM3LPiMsxIRef
         j26o5FyisiI1fm2CzXiB4ko1cZ2psGIJ96U9UZdtvwf52RstcJpZOTBzN4bbZ3qkQAK7
         DlnalDi4KTFMN4qhmtQQVsd+lDP2sHUVjWudYjjdbodFC1D6IdxlVerh0jkYY8l+yAEy
         w8Uw==
X-Gm-Message-State: ALoCoQmpz/fl38ZcV9xIjiyXS9V3HhmdUCDnVtb5xkISP84rpIUBN0PBDPvSXWihHx2eySPsQjPl
MIME-Version: 1.0
X-Received: by 10.42.89.72 with SMTP id f8mr16512922icm.24.1427064964655; Sun,
 22 Mar 2015 15:56:04 -0700 (PDT)
Received: by 10.36.90.208 with HTTP; Sun, 22 Mar 2015 15:56:04 -0700 (PDT)
In-Reply-To: <550D1985.8070307@exensa.com>
References: <550C9E28.8010404@exensa.com>
	<1426919378095.915ff721@Nodemailer>
	<550D1985.8070307@exensa.com>
Date: Sun, 22 Mar 2015 18:56:04 -0400
Message-ID: <CACBYxKJ01tgLKOFh+4A48rZT-5yhTo1oc0hkS+5zdYEK0tCYGA@mail.gmail.com>
Subject: Re: Directly broadcasting (sort of) RDDs
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Guillaume Pitel <guillaume.pitel@exensa.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/related; boundary=90e6ba6147e24b87840511e87482
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6147e24b87840511e87482
Content-Type: multipart/alternative; boundary=90e6ba6147e24b87820511e87481

--90e6ba6147e24b87820511e87481
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Guillaume,

I've long thought something like this would be useful - i.e. the ability to
broadcast RDDs directly without first pulling data through the driver.  If
I understand correctly, your requirement to "block" a matrix up and only
fetch the needed parts could be implemented on top of this by splitting an
RDD into a set of smaller RDDs and then broadcasting each one on its own.

Unfortunately nobody is working on this currently (and I couldn't promise
to have bandwidth to review it at the moment either), but I suspect we'll
eventually need to add something like this for map joins in Hive on Spark
and Spark SQL.

-Sandy



On Sat, Mar 21, 2015 at 3:11 AM, Guillaume Pitel <guillaume.pitel@exensa.co=
m
> wrote:

>  Hi,
>
> Thanks for your answer. This is precisely the use case I'm interested in,
> but I know it already, I should have mentionned it. Unfortunately this
> implementation of BlockMatrix has (in my opinion) some disadvantages (the
> fact that it split the matrix by range instead of using a modulo is bad f=
or
> block skewness). Besides, and more importantly, as I was writing, it uses
> the join solution (actually a cogroup :
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/linalg/distributed/BlockMatrix.scala,
> line 361). The reduplication of the elements of the dense matrix is thus
> dependent on the block size.
>
> Actually I'm wondering if what I want to achieve could be made with a
> simple modification to the join, allowing a partition to be weakly cached
> wafter being retrieved.
>
> Guillaume
>
>
>  There is block matrix in Spark 1.3 - http://spark.apache.org/docs/latest=
/mllib-data-types.html#blockmatrix
>
>
>
>
>
> However I believe it only supports dense matrix blocks.
>
>
>
>
> Still, might be possible to use it or exetend
>
>
>
>
> JIRAs:
>
> https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434
>
>
>
>
>
> Was based on
>
> https://github.com/amplab/ml-matrix
>
>
>
>
>
> Another lib:
>
> https://github.com/PasaLab/marlin/blob/master/README.md
>
>
>
>
>
>
>
> =E2=80=94
> Sent from Mailbox
>
> On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel<guillaume.pitel@exensa.=
com> <guillaume.pitel@exensa.com> wrote:
>
>
>  Hi,
> I have an idea that I would like to discuss with the Spark devs. The
> idea comes from a very real problem that I have struggled with since
> almost a year. My problem is very simple, it's a dense matrix * sparse
> matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is
> divided in X large blocks (one block per partition), and a sparse matrix
> RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The
> most efficient way to perform the operation is to collectAsMap() the
> dense matrix and broadcast it, then perform the block-local
> mutliplications, and combine the results by column.
> This is quite fine, unless the matrix is too big to fit in memory
> (especially since the multiplication is performed several times
> iteratively, and the broadcasts are not always cleaned from memory as I
> would naively expect).
> When the dense matrix is too big, a second solution is to split the big
> sparse matrix in several RDD, and do several broadcasts. Doing this
> creates quite a big overhead, but it mostly works, even though I often
> face some problems with unaccessible broadcast files, for instance.
> Then there is the terrible but apparently very effective good old join.
> Since X blocks of the sparse matrix use the same block from the dense
> matrix, I suspect that the dense matrix is somehow replicated X times
> (either on disk or in the network), which is the reason why the join
> takes so much time.
> After this bit of a context, here is my idea : would it be possible to
> somehow "broadcast" (or maybe more accurately, share or serve) a
> persisted RDD which is distributed on all workers, in a way that would,
> a bit like the IndexedRDD, allow a task to access a partition or an
> element of a partition in the closure, with a worker-local memory cache
> . i.e. the information about where each block resides would be
> distributed on the workers, to allow them to access parts of the RDD
> directly. I think that's already a bit how RDD are shuffled ?
> The RDD could stay distributed (no need to collect then broadcast), and
> only necessary transfers would be required.
> Is this a bad idea, is it already implemented somewhere (I would love it
> !) ?or is it something that could add efficiency not only for my use
> case, but maybe for others ? Could someone give me some hint about how I
> could add this possibility to Spark ? I would probably try to extend a
> RDD into a specific SharedIndexedRDD with a special lookup that would be
> allowed from tasks as a special case, and that would try to contact the
> blockManager and reach the corresponding data from the right worker.
> Thanks in advance for your advices
> Guillaume
> --
> eXenSa
> =09
> *Guillaume PITEL, Pr=C3=A9sident*
> +33(0)626 222 431
> eXenSa S.A.S. <http://www.exensa.com/> <http://www.exensa.com/>
> 41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>
>
>
> --
>    [image: eXenSa]
>  *Guillaume PITEL, Pr=C3=A9sident*
> +33(0)626 222 431
>
> eXenSa S.A.S. <http://www.exensa.com/>
>  41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>

--90e6ba6147e24b87820511e87481
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div><div><div>Hi Guillaume,<br><br></div>I&#39;ve long th=
ought something like this would be useful - i.e. the ability to broadcast R=
DDs directly without first pulling data through the driver.=C2=A0 If I unde=
rstand correctly, your requirement to &quot;block&quot; a matrix up and onl=
y fetch the needed parts could be implemented on top of this by splitting a=
n RDD into a set of smaller RDDs and then broadcasting each one on its own.=
<br><br></div>Unfortunately nobody is working on this currently (and I coul=
dn&#39;t promise to have bandwidth to review it at the moment either), but =
I suspect we&#39;ll eventually need to add something like this for map join=
s in Hive on Spark and Spark SQL.<br><br></div>-Sandy<br><div><div><br><br>=
</div></div></div><div class=3D"gmail_extra"><br><div class=3D"gmail_quote"=
>On Sat, Mar 21, 2015 at 3:11 AM, Guillaume Pitel <span dir=3D"ltr">&lt;<a =
href=3D"mailto:guillaume.pitel@exensa.com" target=3D"_blank">guillaume.pite=
l@exensa.com</a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" sty=
le=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">
 =20
   =20
 =20
  <div bgcolor=3D"#FFFFFF" text=3D"#000000">
    <div>Hi, <br>
      <br>
      Thanks for your answer. This is precisely the use case I&#39;m
      interested in, but I know it already, I should have mentionned it.
      Unfortunately this implementation of BlockMatrix has (in my
      opinion) some disadvantages (the fact that it split the matrix by
      range instead of using a modulo is bad for block skewness).
      Besides, and more importantly, as I was writing, it uses the join
      solution (actually a cogroup :
      <a href=3D"https://github.com/apache/spark/blob/master/mllib/src/main=
/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala" target=
=3D"_blank">https://github.com/apache/spark/blob/master/mllib/src/main/scal=
a/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala</a>,
      line 361). The reduplication of the elements of the dense matrix
      is thus dependent on the block size.<br>
      <br>
      Actually I&#39;m wondering if what I want to achieve could be made
      with a simple modification to the join, allowing a partition to be
      weakly cached wafter being retrieved. <br>
      <br>
      Guillaume<br>
      <br>
      <br>
    </div><div><div class=3D"h5">
    <blockquote type=3D"cite">
      <pre>There is block matrix in Spark 1.3 -=C2=A0<a href=3D"http://spar=
k.apache.org/docs/latest/mllib-data-types.html#blockmatrix" target=3D"_blan=
k">http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix</a=
>





However I believe it only supports dense matrix blocks.




Still, might be possible to use it or exetend=C2=A0




JIRAs:


<a href=3D"https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPAR=
K-3434" target=3D"_blank">https://issues.apache.org/jira/plugins/servlet/mo=
bile#issue/SPARK-3434</a>





Was based on=C2=A0


<a href=3D"https://github.com/amplab/ml-matrix" target=3D"_blank">https://g=
ithub.com/amplab/ml-matrix</a>





Another lib:


<a href=3D"https://github.com/PasaLab/marlin/blob/master/README.md" target=
=3D"_blank">https://github.com/PasaLab/marlin/blob/master/README.md</a>







=E2=80=94
Sent from Mailbox

On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
<a href=3D"mailto:guillaume.pitel@exensa.com" target=3D"_blank">&lt;guillau=
me.pitel@exensa.com&gt;</a> wrote:

</pre>
      <blockquote type=3D"cite">
        <pre>Hi,
I have an idea that I would like to discuss with the Spark devs. The=20
idea comes from a very real problem that I have struggled with since=20
almost a year. My problem is very simple, it&#39;s a dense matrix * sparse=
=20
matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is=20
divided in X large blocks (one block per partition), and a sparse matrix=20
RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The=20
most efficient way to perform the operation is to collectAsMap() the=20
dense matrix and broadcast it, then perform the block-local=20
mutliplications, and combine the results by column.
This is quite fine, unless the matrix is too big to fit in memory=20
(especially since the multiplication is performed several times=20
iteratively, and the broadcasts are not always cleaned from memory as I=20
would naively expect).
When the dense matrix is too big, a second solution is to split the big=20
sparse matrix in several RDD, and do several broadcasts. Doing this=20
creates quite a big overhead, but it mostly works, even though I often=20
face some problems with unaccessible broadcast files, for instance.
Then there is the terrible but apparently very effective good old join.=20
Since X blocks of the sparse matrix use the same block from the dense=20
matrix, I suspect that the dense matrix is somehow replicated X times=20
(either on disk or in the network), which is the reason why the join=20
takes so much time.
After this bit of a context, here is my idea : would it be possible to=20
somehow &quot;broadcast&quot; (or maybe more accurately, share or serve) a=
=20
persisted RDD which is distributed on all workers, in a way that would,=20
a bit like the IndexedRDD, allow a task to access a partition or an=20
element of a partition in the closure, with a worker-local memory cache=20
. i.e. the information about where each block resides would be=20
distributed on the workers, to allow them to access parts of the RDD=20
directly. I think that&#39;s already a bit how RDD are shuffled ?
The RDD could stay distributed (no need to collect then broadcast), and=20
only necessary transfers would be required.
Is this a bad idea, is it already implemented somewhere (I would love it=20
!) ?or is it something that could add efficiency not only for my use=20
case, but maybe for others ? Could someone give me some hint about how I=20
could add this possibility to Spark ? I would probably try to extend a=20
RDD into a specific SharedIndexedRDD with a special lookup that would be=20
allowed from tasks as a special case, and that would try to contact the=20
blockManager and reach the corresponding data from the right worker.
Thanks in advance for your advices
Guillaume
--=20
eXenSa
=09
*Guillaume PITEL, Pr=C3=A9sident*
+33(0)626 222 431
eXenSa S.A.S. <a href=3D"http://www.exensa.com/" target=3D"_blank">&lt;http=
://www.exensa.com/&gt;</a>
41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705
</pre>
      </blockquote>
    </blockquote>
    <br>
    <br>
    </div></div><div><span class=3D"HOEnZb"><font color=3D"#888888">-- <br>
     =20
      </font></span><span style=3D"font-size:8.5pt;color:rgb(51,51,51);font=
-family:Helvetica"><span class=3D"HOEnZb"><font color=3D"#888888">
        </font></span><div style=3D"width:529px" align=3D"left"><span class=
=3D"HOEnZb"><font color=3D"#888888">
          </font></span><span class=3D"HOEnZb"><font color=3D"#888888">
            </font></span><span class=3D"HOEnZb"><font color=3D"#888888">
              </font></span><span class=3D"HOEnZb"><font color=3D"#888888">
                </font></span><table style=3D"width:524px" border=3D"0" cel=
lpadding=3D"0" cellspacing=3D"0"><tbody><tr><td style=3D"border-top:medium =
none rgb(236,233,216);border-right:medium none rgb(236,233,216);border-bott=
om:medium none rgb(236,233,216);padding-right:0cm;width:219px;height:33.75p=
t;background-color:transparent;vertical-align:middle">
                  <center style=3D"width:210px"><img style=3D"border:0px so=
lid;width:160px;min-height:64px" alt=3D"eXenSa" src=3D"cid:part1.09080406.0=
7080002@exensa.com"></center>
                </td>
                <td style=3D"padding-left:10px;border-top:medium none rgb(2=
36,233,216);border-right:medium none rgb(236,233,216);border-bottom:medium =
none rgb(236,233,216);padding-right:0cm;width:358px;height:33.75pt;backgrou=
nd-color:transparent;vertical-align:top"><span class=3D"HOEnZb"><font color=
=3D"#888888">
                  </font></span><div style=3D"text-align:left;width:299px" =
align=3D"left"><span class=3D"HOEnZb"><font color=3D"#888888"> <span style=
=3D"font-size:7.5pt;color:rgb(75,80,85);font-family:Helvetica"> <b>Guillaum=
e
                        PITEL, Pr=C3=A9sident</b> <br>
                      +33(0)626 222 431<br>
                    </span> <br>
                    <span style=3D"font-size:7.5pt;color:#505050;font-famil=
y:Helvetica"><a href=3D"http://www.exensa.com/" target=3D"_blank">eXenSa
                        S.A.S.</a> </span><br></font></span><span class=3D"=
">
                    <span style=3D"font-size:7.5pt;color:rgb(75,80,85)"> <f=
ont face=3D"Helvetica">41, rue P=C3=A9rier -
                        92120 Montrouge - FRANCE <br>
                        Tel +33(0)184 163 677 / Fax +33(0)972 283 705 </fon=
t>
                    </span> </span></div>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </span>
    </div>
  </div>

</blockquote></div><br></div>

--90e6ba6147e24b87820511e87481--
--90e6ba6147e24b87840511e87482--

From dev-return-12098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 00:42:57 2015
Return-Path: <dev-return-12098-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 205621734C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 00:42:57 +0000 (UTC)
Received: (qmail 92890 invoked by uid 500); 23 Mar 2015 00:42:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92817 invoked by uid 500); 23 Mar 2015 00:42:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92806 invoked by uid 99); 23 Mar 2015 00:42:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 00:42:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 00:42:51 +0000
Received: by wgdm6 with SMTP id m6so133651703wgd.2
        for <dev@spark.apache.org>; Sun, 22 Mar 2015 17:42:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=auRdoQ76GSi6IYJcSzj3Uy7EORYU/hFZbGWTeRn8RZI=;
        b=CqpkA7PKJy/Pb4ywUVU5zfT9H+f0Mol8DTk/plUmQ0EGGfTI3Dd4h/wBGcbEhbXjmS
         9ESFEPz+Lkgn5YbT3oKuFlMMv1yrFfUJ2KXONfk5lYKO7Qnrq6DXfyRReuRp+iS2KhZz
         8YfnBfGQRUd6/L1rfu7ViPASAUp6eMuwX9oZJDTf1A4EZ0JpTeGLZnJjHS9WBD5KHOzx
         keN2Ebh+9Ef4WQbbyvRV8okVeny+yoDF49ewwYODQ23iGbmNM9Cfq1YShPOtYHHKEqwi
         kZKFMY87MiRNj41vrb28d+yg+fZr+gvZB5lB3IJ9n7q/ANhsUfe4y9xSffqPU/nP7DZP
         PrxQ==
X-Gm-Message-State: ALoCoQmpt4kVwVA4Vu+lhhdizFrJDSECNY7QL3Lo/ZYK0jrrCECodWoSC4Tdp/GoyKhc/eUjmBRC
X-Received: by 10.194.185.68 with SMTP id fa4mr176506742wjc.111.1427071350331;
 Sun, 22 Mar 2015 17:42:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Sun, 22 Mar 2015 17:42:10 -0700 (PDT)
In-Reply-To: <CACBYxKJ01tgLKOFh+4A48rZT-5yhTo1oc0hkS+5zdYEK0tCYGA@mail.gmail.com>
References: <550C9E28.8010404@exensa.com> <1426919378095.915ff721@Nodemailer>
 <550D1985.8070307@exensa.com> <CACBYxKJ01tgLKOFh+4A48rZT-5yhTo1oc0hkS+5zdYEK0tCYGA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 23 Mar 2015 00:42:10 +0000
Message-ID: <CAMAsSdJ3SF55gHzY5Am4FXMCZm8svZus8kUeyayFmFXb=EtoGA@mail.gmail.com>
Subject: Re: Directly broadcasting (sort of) RDDs
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Guillaume Pitel <guillaume.pitel@exensa.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

In a sentence, is this the idea of collecting an RDD to memory on each
executor directly?

On Sun, Mar 22, 2015 at 10:56 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrot=
e:
> Hi Guillaume,
>
> I've long thought something like this would be useful - i.e. the ability =
to
> broadcast RDDs directly without first pulling data through the driver.  I=
f I
> understand correctly, your requirement to "block" a matrix up and only fe=
tch
> the needed parts could be implemented on top of this by splitting an RDD
> into a set of smaller RDDs and then broadcasting each one on its own.
>
> Unfortunately nobody is working on this currently (and I couldn't promise=
 to
> have bandwidth to review it at the moment either), but I suspect we'll
> eventually need to add something like this for map joins in Hive on Spark
> and Spark SQL.
>
> -Sandy
>
>
>
> On Sat, Mar 21, 2015 at 3:11 AM, Guillaume Pitel
> <guillaume.pitel@exensa.com> wrote:
>>
>> Hi,
>>
>> Thanks for your answer. This is precisely the use case I'm interested in=
,
>> but I know it already, I should have mentionned it. Unfortunately this
>> implementation of BlockMatrix has (in my opinion) some disadvantages (th=
e
>> fact that it split the matrix by range instead of using a modulo is bad =
for
>> block skewness). Besides, and more importantly, as I was writing, it use=
s
>> the join solution (actually a cogroup :
>> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apa=
che/spark/mllib/linalg/distributed/BlockMatrix.scala,
>> line 361). The reduplication of the elements of the dense matrix is thus
>> dependent on the block size.
>>
>> Actually I'm wondering if what I want to achieve could be made with a
>> simple modification to the join, allowing a partition to be weakly cache=
d
>> wafter being retrieved.
>>
>> Guillaume
>>
>>
>> There is block matrix in Spark 1.3 -
>> http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix
>>
>>
>>
>>
>>
>> However I believe it only supports dense matrix blocks.
>>
>>
>>
>>
>> Still, might be possible to use it or exetend
>>
>>
>>
>>
>> JIRAs:
>>
>>
>> https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434
>>
>>
>>
>>
>>
>> Was based on
>>
>>
>> https://github.com/amplab/ml-matrix
>>
>>
>>
>>
>>
>> Another lib:
>>
>>
>> https://github.com/PasaLab/marlin/blob/master/README.md
>>
>>
>>
>>
>>
>>
>>
>> =E2=80=94
>> Sent from Mailbox
>>
>> On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
>> <guillaume.pitel@exensa.com> wrote:
>>
>> Hi,
>> I have an idea that I would like to discuss with the Spark devs. The
>> idea comes from a very real problem that I have struggled with since
>> almost a year. My problem is very simple, it's a dense matrix * sparse
>> matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is
>> divided in X large blocks (one block per partition), and a sparse matrix
>> RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The
>> most efficient way to perform the operation is to collectAsMap() the
>> dense matrix and broadcast it, then perform the block-local
>> mutliplications, and combine the results by column.
>> This is quite fine, unless the matrix is too big to fit in memory
>> (especially since the multiplication is performed several times
>> iteratively, and the broadcasts are not always cleaned from memory as I
>> would naively expect).
>> When the dense matrix is too big, a second solution is to split the big
>> sparse matrix in several RDD, and do several broadcasts. Doing this
>> creates quite a big overhead, but it mostly works, even though I often
>> face some problems with unaccessible broadcast files, for instance.
>> Then there is the terrible but apparently very effective good old join.
>> Since X blocks of the sparse matrix use the same block from the dense
>> matrix, I suspect that the dense matrix is somehow replicated X times
>> (either on disk or in the network), which is the reason why the join
>> takes so much time.
>> After this bit of a context, here is my idea : would it be possible to
>> somehow "broadcast" (or maybe more accurately, share or serve) a
>> persisted RDD which is distributed on all workers, in a way that would,
>> a bit like the IndexedRDD, allow a task to access a partition or an
>> element of a partition in the closure, with a worker-local memory cache
>> . i.e. the information about where each block resides would be
>> distributed on the workers, to allow them to access parts of the RDD
>> directly. I think that's already a bit how RDD are shuffled ?
>> The RDD could stay distributed (no need to collect then broadcast), and
>> only necessary transfers would be required.
>> Is this a bad idea, is it already implemented somewhere (I would love it
>> !) ?or is it something that could add efficiency not only for my use
>> case, but maybe for others ? Could someone give me some hint about how I
>> could add this possibility to Spark ? I would probably try to extend a
>> RDD into a specific SharedIndexedRDD with a special lookup that would be
>> allowed from tasks as a special case, and that would try to contact the
>> blockManager and reach the corresponding data from the right worker.
>> Thanks in advance for your advices
>> Guillaume
>> --
>> eXenSa
>> =09
>> *Guillaume PITEL, Pr=C3=A9sident*
>> +33(0)626 222 431
>> eXenSa S.A.S. <http://www.exensa.com/>
>> 41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
>> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>>
>>
>>
>> --
>> Guillaume PITEL, Pr=C3=A9sident
>> +33(0)626 222 431
>>
>> eXenSa S.A.S.
>> 41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
>> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 06:54:57 2015
Return-Path: <dev-return-12099-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4EA29179F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 06:54:57 +0000 (UTC)
Received: (qmail 31510 invoked by uid 500); 23 Mar 2015 06:54:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31434 invoked by uid 500); 23 Mar 2015 06:54:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31423 invoked by uid 99); 23 Mar 2015 06:54:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 06:54:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 06:54:30 +0000
Received: by lbcgn8 with SMTP id gn8so111620858lbc.2
        for <dev@spark.apache.org>; Sun, 22 Mar 2015 23:52:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=4a99teYdmdQ/FelTHMEr7sHWFl7odFi0CqQfQPIkFjY=;
        b=eCxaqmO7NPDw3buVgPI9lpKFoKSPGIsGWIG/NDMOXFJzYBHbD5LUUC88a7WAHA9zYi
         YeOg2oTcCWE87KFSz//QUIOkOHJuJjpAHfD3kpmWlcKhypubA3owUzZ/zHDuhyKeis8j
         SSOC3AV5xIzrjoA91DhRPN+/Ptp9URxvbS4mePQFb5crG+mTQqqPwEPRC3vK3inqBxzQ
         0Ff6+w8CwavK8wVY01cRwSGsWvMsKeSyMWXa5mKGlPQkWcSQ6CUyotgus322Zn1b3lfr
         D/4i+xu/uZIQ2gNKPuUsLJgcSmMfUJPfp915ONHrSQuM+GsV/wbt3yqxfEjh72cjcm4c
         Z6pA==
X-Gm-Message-State: ALoCoQk9lS1OGMEa+fEau/YiMli7MxP2meMY+84LXTBhbmuqoAzgHJuPALpsxzXCXSWJSKZiVnLu
MIME-Version: 1.0
X-Received: by 10.152.26.136 with SMTP id l8mr83142829lag.109.1427093558860;
 Sun, 22 Mar 2015 23:52:38 -0700 (PDT)
Received: by 10.152.43.234 with HTTP; Sun, 22 Mar 2015 23:52:38 -0700 (PDT)
In-Reply-To: <95b62b2b250440458965ccb6448da1ee@siberie.de>
References: <95b62b2b250440458965ccb6448da1ee@siberie.de>
Date: Mon, 23 Mar 2015 12:22:38 +0530
Message-ID: <CAHUQ+_b-8_MjbsFogV1McAEN2rZnZGdyAgADCnZzD1s0ky5wcA@mail.gmail.com>
Subject: Re: Storage of RDDs created via sc.parallelize
From: Akhil Das <akhil@sigmoidanalytics.com>
To: Karlson <ksonspark@siberie.de>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160c2b8a33cea0511ef1c31
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160c2b8a33cea0511ef1c31
Content-Type: text/plain; charset=UTF-8

You can use sc.newAPIHadoopFile
<http://spark.apache.org/docs/1.2.0/api/scala/index.html#org.apache.spark.SparkContext>
with CSVInputFormat <https://github.com/mvallebr/CSVInputFormat> so that it
will read the csv file properly.

Thanks
Best Regards

On Sat, Mar 21, 2015 at 12:39 AM, Karlson <ksonspark@siberie.de> wrote:

>
> Hi all,
>
> where is the data stored that is passed to sc.parallelize? Or put
> differently, where is the data for the base RDD fetched from when the DAG
> is executed, if the base RDD is constructed via sc.parallelize?
>
> I am reading a csv file via the Python csv module and am feeding the
> parsed data chunkwise to sc.parallelize, because the whole file would not
> fit into memory on the driver. Reading the file with sc.textfile first is
> not an option, as there might be linebreaks inside the csv fields,
> preventing me from parsing the file line by line.
>
> The problem I am facing right now is that even though I am feeding only
> one chunk at a time to Spark, I will eventually run out of memory on the
> driver.
>
> Thanks in advance!
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--089e0160c2b8a33cea0511ef1c31--

From dev-return-12100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 07:46:05 2015
Return-Path: <dev-return-12100-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6EDF117B3A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 07:46:05 +0000 (UTC)
Received: (qmail 23561 invoked by uid 500); 23 Mar 2015 07:45:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23432 invoked by uid 500); 23 Mar 2015 07:45:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22476 invoked by uid 99); 23 Mar 2015 07:45:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 07:45:56 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lonely8658@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 07:45:31 +0000
Received: by obbgg8 with SMTP id gg8so117086397obb.1;
        Mon, 23 Mar 2015 00:43:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=RlffNjORJ6Q2IN/FRlhtVq3Xio6j1UEF0oZpnk2FLtM=;
        b=DN6ZnJUmAUavwGfWLosqq/m946aEWM/DQFdX9rhP+lxC1xmZHjtg91Jyir94uApaO6
         MA9KnvNfwq2+KJqvBpF1rFXjMOWF4+iFwlFaNqUW4SQaKfyoM4hqYrzb2xjPfAB0JisN
         /lm8vy5oDI+ofvA42VAv7oy8eZ8BX3AbgJ91orZDAU6jCOwP7IAxsMJUvs9YXI1r507W
         dFg8Mjllg1qefPG64a6pLVpCJhF4u/Gfyxu/xiR6RT+c4ObhOtaWbhxOfp2GuRrQ0gGc
         8/1hgUvebhfagUuj1KW2AbXk3lpSG5lP1YHHnzlfRm7fzya08jgqSaZWlKHCqge25Hjt
         m0WQ==
MIME-Version: 1.0
X-Received: by 10.182.128.199 with SMTP id nq7mr51734451obb.47.1427096639143;
 Mon, 23 Mar 2015 00:43:59 -0700 (PDT)
Received: by 10.76.123.143 with HTTP; Mon, 23 Mar 2015 00:43:59 -0700 (PDT)
Date: Mon, 23 Mar 2015 15:43:59 +0800
Message-ID: <CAPszQwjEQ232+os1rPiCXWtA1SdJTZPOr=AAr6SqdJ0F+8YqUA@mail.gmail.com>
Subject: Spark Sql with python udf fail
From: lonely Feb <lonely8658@gmail.com>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01537e763cabeb0511efd4ca
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01537e763cabeb0511efd4ca
Content-Type: text/plain; charset=UTF-8

Hi all, I tried to transfer some hive jobs into spark-sql. When i ran a sql
job with python udf i got a exception:

java.lang.ArrayIndexOutOfBoundsException: 9
        at
org.apache.spark.sql.catalyst.expressions.GenericRow.apply(Row.scala:142)
        at
org.apache.spark.sql.catalyst.expressions.BoundReference.eval(BoundAttribute.scala:37)
        at
org.apache.spark.sql.catalyst.expressions.EqualTo.eval(predicates.scala:166)
        at
org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$apply$1.apply(predicates.scala:30)
        at
org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$apply$1.apply(predicates.scala:30)
        at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:156)
        at
org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:151)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:68)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:197)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)

I suspected there was an odd line in the input file. But the input file is
so large and i could not found any abnormal lines with several jobs to
check. How can i get the abnormal line here ?

--089e01537e763cabeb0511efd4ca--

From dev-return-12101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 09:03:46 2015
Return-Path: <dev-return-12101-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6377417DA8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 09:03:46 +0000 (UTC)
Received: (qmail 4117 invoked by uid 500); 23 Mar 2015 09:03:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4042 invoked by uid 500); 23 Mar 2015 09:03:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4029 invoked by uid 99); 23 Mar 2015 09:03:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 09:03:35 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hedonplay@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 09:03:10 +0000
Received: by oiag65 with SMTP id g65so135212133oia.2
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 02:03:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=DiD+rmk76q8bIgleTV0DGa7EKn7lETgNColfMVnA7X8=;
        b=0ViZPqFDyeRgXOuXGft2WgnUO4NG+pBPQFqs83vS0n2t+BeWc3XeogdBybfYYzjX7w
         0MiOhQglJbhaTN6c01D5eniIjZnWkKLWjpoNzxGXp62bHZ1X2y9rUtFAGvxtU2vMO6tf
         oVS2xTwcL1ALW6fMiqAsiveS2gxc/QW/9N0AoQYbJ/kJPtiRMo8SqEz18fuYC4bsAMHH
         q5UlX8ViIjjg4A/k7YIqZdKFCJ8P/yj2RvDgDdJm4T3+DvRy1FRtLVIDOs0qinU9K4rZ
         2U32Kfcgv0zuFULY5dYmt8S4k+kl/fdF+otB3xG7nU0kpH6cfjOwTlts8AIMEGfKuv79
         +C3A==
X-Received: by 10.202.206.85 with SMTP id e82mr8560941oig.112.1427101388774;
 Mon, 23 Mar 2015 02:03:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.202.177.212 with HTTP; Mon, 23 Mar 2015 02:02:48 -0700 (PDT)
In-Reply-To: <CAO=evYeAwt4jarz16Ju21BNrSOa0EqMkkn+hF_cre53gw33_gQ@mail.gmail.com>
References: <CAO=evYeAwt4jarz16Ju21BNrSOa0EqMkkn+hF_cre53gw33_gQ@mail.gmail.com>
From: Hui WANG <hedonplay@gmail.com>
Date: Mon, 23 Mar 2015 10:02:48 +0100
Message-ID: <CADUODdmqXC1P1utjPYpOaV3Vvg3L96=Mt9V4Ro_B6_0jSg803w@mail.gmail.com>
Subject: Re: Spark scheduling, data locality
To: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113ad11a5625530511f0ef69
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ad11a5625530511f0ef69
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hello Zoltan,

I'm a spark beginner but i think that the locality preferences should be
prepared before the sending of
tasks.
One important element of a RDD is the metadata on the scheme and location
of its partitions. Tasks created in the driver program should be based on
this information.

I'm also interested by a detailed answer of this question. Could someone
please provide a few more hints on it ?

Regards,
Hui

On Thu, Mar 19, 2015 at 11:20 AM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.com=
>
wrote:

> I'm trying to understand the task scheduling mechanism of Spark, and I'm
> curious about where does locality preferences get evaluated? I'm trying t=
o
> determine if locality preferences are fetchable before the task get
> serialized. A HintSet would be most appreciated!
>
> Have nice day!
>
> Zvara Zolt=C3=A1n
>
>
>
> mail, hangout, skype: zoltan.zvara@gmail.com
>
> mobile, viber: +36203129543
>
> bank: 10918001-00000021-50480008
>
> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>
> elte: HSKSJZ (ZVZOAAI.ELTE)
>



--=20
Hui WANG
Tel : +33 (0) 6 71 33 45 39
Blog : http://www.hui-wang.info

--001a113ad11a5625530511f0ef69--

From dev-return-12102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 09:34:15 2015
Return-Path: <dev-return-12102-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CCDF017E86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 09:34:15 +0000 (UTC)
Received: (qmail 76770 invoked by uid 500); 23 Mar 2015 09:33:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76692 invoked by uid 500); 23 Mar 2015 09:33:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76680 invoked by uid 99); 23 Mar 2015 09:33:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 09:33:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of marek.wiewiorka@gmail.com designates 209.85.223.182 as permitted sender)
Received: from [209.85.223.182] (HELO mail-ie0-f182.google.com) (209.85.223.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 09:33:36 +0000
Received: by iedfl3 with SMTP id fl3so36188961ied.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 02:33:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=qUSqdup9II5NyAEHaqQWb8pTK61aCRjBI9IYvo1o+AI=;
        b=KZ/py/LPPJ5vhxkmKfy2rTg+7eqCY4YRxZMGJQ+j8hiRLUo/Sud2mqmsADssTv8vXW
         yGB7rWGzcYlzaCJxQ5HjYjatNsLbpwHQN0LBq/oHiyF8G531GSW3YuyHH0UDNyg/c2SO
         ScGCjc+ztv3vwkP45PATiqbDtxpeMUPzb2A0Rr4iRMa+TbdAnP46l7VeUTZ8cihk+cgu
         8eZkZKfjBL+m9wrYonjUGqdRwIPwYGOkAI6axHgcOdX8JEFgPpWoqcZxmFowy3vGRNZ2
         TbsDBxS45b5Wj+OsyT+8B1AELlOZjJrDKKkbDsWb5iXMV8WP+LxIGEa4Lx+jS+nUeblP
         rQJg==
X-Received: by 10.107.29.21 with SMTP id d21mr35483969iod.11.1427103195954;
 Mon, 23 Mar 2015 02:33:15 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.36.56.136 with HTTP; Mon, 23 Mar 2015 02:32:35 -0700 (PDT)
In-Reply-To: <CAAswR-5fup6Tz=1ZPZn29vNdBOV7j54K2BQKjvVr7c+gzvSv2A@mail.gmail.com>
References: <CAJbo4nf6ffhvfG8qxFFPKDthrFxhAkPOLOyindWeUFK-M=5PWQ@mail.gmail.com>
 <CAAswR-5fup6Tz=1ZPZn29vNdBOV7j54K2BQKjvVr7c+gzvSv2A@mail.gmail.com>
From: Marek Wiewiorka <marek.wiewiorka@gmail.com>
Date: Mon, 23 Mar 2015 10:32:35 +0100
Message-ID: <CAJbo4nfEBAyLFeb7rUzESOO508r76+6r6e-7qac0bybJz2qA9w@mail.gmail.com>
Subject: Re: lower&upperBound not working/spark 1/3
To: Michael Armbrust <michael@databricks.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1140a4fa0d99d60511f15b8b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1140a4fa0d99d60511f15b8b
Content-Type: text/plain; charset=UTF-8

Ok- thanks Michael I will do another series of tests to confirm this and
then report an issue.

Regards,
Marek

2015-03-22 22:19 GMT+01:00 Michael Armbrust <michael@databricks.com>:

> I have not heard this reported yet, but your invocation looks correct to
> me.  Can you open a JIRA?
>
> On Sun, Mar 22, 2015 at 8:39 AM, Marek Wiewiorka <
> marek.wiewiorka@gmail.com> wrote:
>
>> Hi All - I try to use the new SQLContext API for populating DataFrame from
>> jdbc data source.
>> like this:
>>
>> val jdbcDF = sqlContext.jdbc(url =
>> "jdbc:postgresql://localhost:5430/dbname?user=user&password=111", table =
>> "se_staging.exp_table3" ,columnName="cs_id",lowerBound=1 ,upperBound =
>> 10000, numPartitions=12 )
>>
>> No matter how I set lower and upper bounds I always get all the rows from
>> my table.
>> The API is marked as experimental so I assume there might by some bugs in
>> it but
>> did anybody come across a similar issue?
>>
>> Thanks!
>> Marek
>>
>
>

--001a1140a4fa0d99d60511f15b8b--

From dev-return-12103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 12:01:47 2015
Return-Path: <dev-return-12103-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3427A175E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 12:01:47 +0000 (UTC)
Received: (qmail 74254 invoked by uid 500); 23 Mar 2015 12:01:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74180 invoked by uid 500); 23 Mar 2015 12:01:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74167 invoked by uid 99); 23 Mar 2015 12:01:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 12:01:35 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of guillaume.pitel@exensa.com designates 91.121.232.90 as permitted sender)
Received: from [91.121.232.90] (HELO mail.exensa.com) (91.121.232.90)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 12:01:09 +0000
Received: from localhost (localhost [127.0.0.1])
	by mail.exensa.com (Postfix) with ESMTP id DADA9641C04
	for <dev@spark.apache.org>; Mon, 23 Mar 2015 12:00:37 +0000 (UTC)
Received: from mail.exensa.com ([127.0.0.1])
	by localhost (mail.exensa.com [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id llGK5Gsx8rNa for <dev@spark.apache.org>;
	Mon, 23 Mar 2015 12:00:37 +0000 (UTC)
Received: from [10.1.42.6] (LPuteaux-656-1-229-158.w80-12.abo.wanadoo.fr [80.12.90.158])
	(using TLSv1.2 with cipher ECDHE-RSA-AES128-GCM-SHA256 (128/128 bits))
	(No client certificate requested)
	by mail.exensa.com (Postfix) with ESMTPSA id 410FE641C03
	for <dev@spark.apache.org>; Mon, 23 Mar 2015 12:00:37 +0000 (UTC)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/simple; d=exensa.com;
	s=default; t=1427112037;
	bh=6dtz1LD3tVjeBUb0wADcB+pLzeJ0eXFoY1C6V/QE8hA=;
	h=Date:From:To:Subject:References:In-Reply-To;
	b=TRf0FWyOgPGGBJNSKvAIND7wwe84hOXJ95qnAvjEtclzXszzlRG5Mb5LdbgrHvuO4
	 CxHB6SAjaLVLzvBuP3GmNvwwNcQqHcXloiu1kjhmoCWrTfET8Ay9krWKXOy/dVE
Message-ID: <55100064.7050606@exensa.com>
Date: Mon, 23 Mar 2015 13:00:36 +0100
From: Guillaume Pitel <guillaume.pitel@exensa.com>
Organization: eXenSa
User-Agent: Mozilla/5.0 (X11; Linux x86_64; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Directly broadcasting (sort of) RDDs
References: <550C9E28.8010404@exensa.com> <1426919378095.915ff721@Nodemailer> <550D1985.8070307@exensa.com> <CACBYxKJ01tgLKOFh+4A48rZT-5yhTo1oc0hkS+5zdYEK0tCYGA@mail.gmail.com> <CAMAsSdJ3SF55gHzY5Am4FXMCZm8svZus8kUeyayFmFXb=EtoGA@mail.gmail.com>
In-Reply-To: <CAMAsSdJ3SF55gHzY5Am4FXMCZm8svZus8kUeyayFmFXb=EtoGA@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------040801010608040605020300"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------040801010608040605020300
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Not far, but not exactly. The RDD could be too big to fit in memory,

The idea is more like a worker-side rdd.lookup() with local cache.

Guillaume
> In a sentence, is this the idea of collecting an RDD to memory on each
> executor directly?
>
> On Sun, Mar 22, 2015 at 10:56 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
>> Hi Guillaume,
>>
>> I've long thought something like this would be useful - i.e. the ability to
>> broadcast RDDs directly without first pulling data through the driver.  If I
>> understand correctly, your requirement to "block" a matrix up and only fetch
>> the needed parts could be implemented on top of this by splitting an RDD
>> into a set of smaller RDDs and then broadcasting each one on its own.
>>
>> Unfortunately nobody is working on this currently (and I couldn't promise to
>> have bandwidth to review it at the moment either), but I suspect we'll
>> eventually need to add something like this for map joins in Hive on Spark
>> and Spark SQL.
>>
>> -Sandy
>>
>>
>>
>> On Sat, Mar 21, 2015 at 3:11 AM, Guillaume Pitel
>> <guillaume.pitel@exensa.com> wrote:
>>> Hi,
>>>
>>> Thanks for your answer. This is precisely the use case I'm interested in,
>>> but I know it already, I should have mentionned it. Unfortunately this
>>> implementation of BlockMatrix has (in my opinion) some disadvantages (the
>>> fact that it split the matrix by range instead of using a modulo is bad for
>>> block skewness). Besides, and more importantly, as I was writing, it uses
>>> the join solution (actually a cogroup :
>>> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala,
>>> line 361). The reduplication of the elements of the dense matrix is thus
>>> dependent on the block size.
>>>
>>> Actually I'm wondering if what I want to achieve could be made with a
>>> simple modification to the join, allowing a partition to be weakly cached
>>> wafter being retrieved.
>>>
>>> Guillaume
>>>
>>>
>>> There is block matrix in Spark 1.3 -
>>> http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix
>>>
>>>
>>>
>>>
>>>
>>> However I believe it only supports dense matrix blocks.
>>>
>>>
>>>
>>>
>>> Still, might be possible to use it or exetend
>>>
>>>
>>>
>>>
>>> JIRAs:
>>>
>>>
>>> https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434
>>>
>>>
>>>
>>>
>>>
>>> Was based on
>>>
>>>
>>> https://github.com/amplab/ml-matrix
>>>
>>>
>>>
>>>
>>>
>>> Another lib:
>>>
>>>
>>> https://github.com/PasaLab/marlin/blob/master/README.md
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> —
>>> Sent from Mailbox
>>>
>>> On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
>>> <guillaume.pitel@exensa.com> wrote:
>>>
>>> Hi,
>>> I have an idea that I would like to discuss with the Spark devs. The
>>> idea comes from a very real problem that I have struggled with since
>>> almost a year. My problem is very simple, it's a dense matrix * sparse
>>> matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is
>>> divided in X large blocks (one block per partition), and a sparse matrix
>>> RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The
>>> most efficient way to perform the operation is to collectAsMap() the
>>> dense matrix and broadcast it, then perform the block-local
>>> mutliplications, and combine the results by column.
>>> This is quite fine, unless the matrix is too big to fit in memory
>>> (especially since the multiplication is performed several times
>>> iteratively, and the broadcasts are not always cleaned from memory as I
>>> would naively expect).
>>> When the dense matrix is too big, a second solution is to split the big
>>> sparse matrix in several RDD, and do several broadcasts. Doing this
>>> creates quite a big overhead, but it mostly works, even though I often
>>> face some problems with unaccessible broadcast files, for instance.
>>> Then there is the terrible but apparently very effective good old join.
>>> Since X blocks of the sparse matrix use the same block from the dense
>>> matrix, I suspect that the dense matrix is somehow replicated X times
>>> (either on disk or in the network), which is the reason why the join
>>> takes so much time.
>>> After this bit of a context, here is my idea : would it be possible to
>>> somehow "broadcast" (or maybe more accurately, share or serve) a
>>> persisted RDD which is distributed on all workers, in a way that would,
>>> a bit like the IndexedRDD, allow a task to access a partition or an
>>> element of a partition in the closure, with a worker-local memory cache
>>> . i.e. the information about where each block resides would be
>>> distributed on the workers, to allow them to access parts of the RDD
>>> directly. I think that's already a bit how RDD are shuffled ?
>>> The RDD could stay distributed (no need to collect then broadcast), and
>>> only necessary transfers would be required.
>>> Is this a bad idea, is it already implemented somewhere (I would love it
>>> !) ?or is it something that could add efficiency not only for my use
>>> case, but maybe for others ? Could someone give me some hint about how I
>>> could add this possibility to Spark ? I would probably try to extend a
>>> RDD into a specific SharedIndexedRDD with a special lookup that would be
>>> allowed from tasks as a special case, and that would try to contact the
>>> blockManager and reach the corresponding data from the right worker.
>>> Thanks in advance for your advices
>>> Guillaume
>>> --
>>> eXenSa
>>> 	
>>> *Guillaume PITEL, Président*
>>> +33(0)626 222 431
>>> eXenSa S.A.S. <http://www.exensa.com/>
>>> 41, rue Périer - 92120 Montrouge - FRANCE
>>> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>>>
>>>
>>>
>>> --
>>> Guillaume PITEL, Président
>>> +33(0)626 222 431
>>>
>>> eXenSa S.A.S.
>>> 41, rue Périer - 92120 Montrouge - FRANCE
>>> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>


-- 
eXenSa

	
*Guillaume PITEL, Président*
+33(0)626 222 431

eXenSa S.A.S. <http://www.exensa.com/>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705


--------------040801010608040605020300
Content-Type: multipart/related;
 boundary="------------010009030704070407020303"


--------------010009030704070407020303
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: 8bit

<html>
  <head>
    <meta content="text/html; charset=utf-8" http-equiv="Content-Type">
  </head>
  <body bgcolor="#FFFFFF" text="#000000">
    <div class="moz-cite-prefix">Not far, but not exactly. The RDD could
      be too big to fit in memory,<br>
      <br>
      The idea is more like a worker-side rdd.lookup() with local cache.<br>
      <br>
      Guillaume<br>
    </div>
    <blockquote
cite="mid:CAMAsSdJ3SF55gHzY5Am4FXMCZm8svZus8kUeyayFmFXb=EtoGA@mail.gmail.com"
      type="cite">
      <pre wrap="">In a sentence, is this the idea of collecting an RDD to memory on each
executor directly?

On Sun, Mar 22, 2015 at 10:56 PM, Sandy Ryza <a class="moz-txt-link-rfc2396E" href="mailto:sandy.ryza@cloudera.com">&lt;sandy.ryza@cloudera.com&gt;</a> wrote:
</pre>
      <blockquote type="cite">
        <pre wrap="">Hi Guillaume,

I've long thought something like this would be useful - i.e. the ability to
broadcast RDDs directly without first pulling data through the driver.  If I
understand correctly, your requirement to "block" a matrix up and only fetch
the needed parts could be implemented on top of this by splitting an RDD
into a set of smaller RDDs and then broadcasting each one on its own.

Unfortunately nobody is working on this currently (and I couldn't promise to
have bandwidth to review it at the moment either), but I suspect we'll
eventually need to add something like this for map joins in Hive on Spark
and Spark SQL.

-Sandy



On Sat, Mar 21, 2015 at 3:11 AM, Guillaume Pitel
<a class="moz-txt-link-rfc2396E" href="mailto:guillaume.pitel@exensa.com">&lt;guillaume.pitel@exensa.com&gt;</a> wrote:
</pre>
        <blockquote type="cite">
          <pre wrap="">
Hi,

Thanks for your answer. This is precisely the use case I'm interested in,
but I know it already, I should have mentionned it. Unfortunately this
implementation of BlockMatrix has (in my opinion) some disadvantages (the
fact that it split the matrix by range instead of using a modulo is bad for
block skewness). Besides, and more importantly, as I was writing, it uses
the join solution (actually a cogroup :
<a class="moz-txt-link-freetext" href="https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala">https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/distributed/BlockMatrix.scala</a>,
line 361). The reduplication of the elements of the dense matrix is thus
dependent on the block size.

Actually I'm wondering if what I want to achieve could be made with a
simple modification to the join, allowing a partition to be weakly cached
wafter being retrieved.

Guillaume


There is block matrix in Spark 1.3 -
<a class="moz-txt-link-freetext" href="http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix">http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix</a>





However I believe it only supports dense matrix blocks.




Still, might be possible to use it or exetend




JIRAs:


<a class="moz-txt-link-freetext" href="https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434">https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434</a>





Was based on


<a class="moz-txt-link-freetext" href="https://github.com/amplab/ml-matrix">https://github.com/amplab/ml-matrix</a>





Another lib:


<a class="moz-txt-link-freetext" href="https://github.com/PasaLab/marlin/blob/master/README.md">https://github.com/PasaLab/marlin/blob/master/README.md</a>







—
Sent from Mailbox

On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
<a class="moz-txt-link-rfc2396E" href="mailto:guillaume.pitel@exensa.com">&lt;guillaume.pitel@exensa.com&gt;</a> wrote:

Hi,
I have an idea that I would like to discuss with the Spark devs. The
idea comes from a very real problem that I have struggled with since
almost a year. My problem is very simple, it's a dense matrix * sparse
matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is
divided in X large blocks (one block per partition), and a sparse matrix
RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The
most efficient way to perform the operation is to collectAsMap() the
dense matrix and broadcast it, then perform the block-local
mutliplications, and combine the results by column.
This is quite fine, unless the matrix is too big to fit in memory
(especially since the multiplication is performed several times
iteratively, and the broadcasts are not always cleaned from memory as I
would naively expect).
When the dense matrix is too big, a second solution is to split the big
sparse matrix in several RDD, and do several broadcasts. Doing this
creates quite a big overhead, but it mostly works, even though I often
face some problems with unaccessible broadcast files, for instance.
Then there is the terrible but apparently very effective good old join.
Since X blocks of the sparse matrix use the same block from the dense
matrix, I suspect that the dense matrix is somehow replicated X times
(either on disk or in the network), which is the reason why the join
takes so much time.
After this bit of a context, here is my idea : would it be possible to
somehow "broadcast" (or maybe more accurately, share or serve) a
persisted RDD which is distributed on all workers, in a way that would,
a bit like the IndexedRDD, allow a task to access a partition or an
element of a partition in the closure, with a worker-local memory cache
. i.e. the information about where each block resides would be
distributed on the workers, to allow them to access parts of the RDD
directly. I think that's already a bit how RDD are shuffled ?
The RDD could stay distributed (no need to collect then broadcast), and
only necessary transfers would be required.
Is this a bad idea, is it already implemented somewhere (I would love it
!) ?or is it something that could add efficiency not only for my use
case, but maybe for others ? Could someone give me some hint about how I
could add this possibility to Spark ? I would probably try to extend a
RDD into a specific SharedIndexedRDD with a special lookup that would be
allowed from tasks as a special case, and that would try to contact the
blockManager and reach the corresponding data from the right worker.
Thanks in advance for your advices
Guillaume
--
eXenSa
	
*Guillaume PITEL, Président*
+33(0)626 222 431
eXenSa S.A.S. <a class="moz-txt-link-rfc2396E" href="http://www.exensa.com/">&lt;http://www.exensa.com/&gt;</a>
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705



--
Guillaume PITEL, Président
+33(0)626 222 431

eXenSa S.A.S.
41, rue Périer - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705
</pre>
        </blockquote>
        <pre wrap="">

</pre>
      </blockquote>
      <pre wrap="">
---------------------------------------------------------------------
To unsubscribe, e-mail: <a class="moz-txt-link-abbreviated" href="mailto:dev-unsubscribe@spark.apache.org">dev-unsubscribe@spark.apache.org</a>
For additional commands, e-mail: <a class="moz-txt-link-abbreviated" href="mailto:dev-help@spark.apache.org">dev-help@spark.apache.org</a>

</pre>
    </blockquote>
    <br>
    <br>
    <div class="moz-signature">-- <br>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
      <span style="font-size: 8.5pt; color: rgb(51, 51, 51);
        font-family: Helvetica;">
        <div style="width: 529px;" align="left">
          <table style="width: 524px;" border="0" cellpadding="0"
            cellspacing="0">
            <tbody>
              <tr>
                <td style="border-top: medium none rgb(236, 233, 216);
                  border-right: medium none rgb(236, 233, 216);
                  border-bottom: medium none rgb(236, 233, 216);
                  padding-right: 0cm; width: 219px; height: 33.75pt;
                  background-color: transparent; vertical-align:
                  middle;">
                  <center style="width: 210px;"><img style="border: 0px
                      solid ; width: 160px; height: 64px;" alt="eXenSa"
                      src="cid:part1.02070304.01020906@exensa.com"></center>
                </td>
                <td style="padding-left:10px; border-top: medium none
                  rgb(236, 233, 216); border-right: medium none rgb(236,
                  233, 216); border-bottom: medium none rgb(236, 233,
                  216); padding-right: 0cm; width: 358px; height:
                  33.75pt; background-color: transparent;
                  vertical-align: top;">
                  <div style="text-align: left; width: 299px;"
                    align="left"> <span style="font-size: 7.5pt; color:
                      rgb(75, 80, 85); font-family: Helvetica;"> <b>Guillaume
                        PITEL, Président</b> <br>
                      +33(0)626 222 431<br>
                    </span> <br>
                    <span style="font-size: 7.5pt; color: #505050;
                      font-family: Helvetica;"><a
                        href="http://www.exensa.com/" target="_blank">eXenSa
                        S.A.S.</a> </span><br>
                    <span style="font-size: 7.5pt; color: rgb(75, 80,
                      85);"> <font face="Helvetica">41, rue Périer -
                        92120 Montrouge - FRANCE <br>
                        Tel +33(0)184 163 677 / Fax +33(0)972 283 705 </font>
                    </span> </div>
                </td>
              </tr>
            </tbody>
          </table>
        </div>
      </span>
    </div>
  </body>
</html>

--------------010009030704070407020303
Content-Type: image/png;
 name="exensa_logo_mail.png"
Content-Transfer-Encoding: base64
Content-ID: <part1.02070304.01020906@exensa.com>
Content-Disposition: inline;
 filename="exensa_logo_mail.png"
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==
--------------010009030704070407020303--

--------------040801010608040605020300--

From dev-return-12104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 12:14:45 2015
Return-Path: <dev-return-12104-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 61AC017669
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 12:14:45 +0000 (UTC)
Received: (qmail 9740 invoked by uid 500); 23 Mar 2015 12:14:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9663 invoked by uid 500); 23 Mar 2015 12:14:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9652 invoked by uid 99); 23 Mar 2015 12:14:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 12:14:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 12:14:19 +0000
Received: by wibgn9 with SMTP id gn9so60400984wib.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 05:12:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type:content-transfer-encoding;
        bh=dAsACqlLrJhj5DFQe0x474SyzAXTUJyVFYaP0NeE1BU=;
        b=QwY+Ik4uKFhjWkC0mbK3KryFJq3rxL2IPzFp9ve+y6OhCc62x/EZIrbMHIllkEd/SF
         gQ398+ch6h0+A6ZYNlHjtNNQO6jXj+mjAc2EiK6ohFO+xAeEo8r8pnbE9E1xDAtDck+c
         J+XItuW7y3ypWZYFGit6IB5/CZroYUfYxteB0cPqwBM98Kktzu4NXQerK4Qfikbs3SOv
         hWQ0s8xGVkoKJsVHQOwHJxVDh7fjvFvOwUqCgf6USOkCdPDONXaLc3QTDk9RAzu5zI2v
         6meHoMOnb1Hm1ik11qCug72ZhNN/ZcFhQUzKXS8kzmBruKmBGVNzf7JfpOeBjmxyg8/i
         Ggow==
X-Gm-Message-State: ALoCoQlfazrdSRZFOvRS6G/lIB0SqS1mJqjv7e5CkLnjuoS7EQ+U0ZUKOVwvE4Bm6WyzeeFZN9jR
X-Received: by 10.180.84.3 with SMTP id u3mr18951805wiy.38.1427112358594; Mon,
 23 Mar 2015 05:05:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Mon, 23 Mar 2015 05:05:38 -0700 (PDT)
In-Reply-To: <55100064.7050606@exensa.com>
References: <550C9E28.8010404@exensa.com> <1426919378095.915ff721@Nodemailer>
 <550D1985.8070307@exensa.com> <CACBYxKJ01tgLKOFh+4A48rZT-5yhTo1oc0hkS+5zdYEK0tCYGA@mail.gmail.com>
 <CAMAsSdJ3SF55gHzY5Am4FXMCZm8svZus8kUeyayFmFXb=EtoGA@mail.gmail.com> <55100064.7050606@exensa.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 23 Mar 2015 12:05:38 +0000
Message-ID: <CAMAsSd+yryH8uDGjeOh_6AbYSq4i=XFpmODgkmUhtYzoWyQRCA@mail.gmail.com>
Subject: Re: Directly broadcasting (sort of) RDDs
To: Guillaume Pitel <guillaume.pitel@exensa.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Since RDDs aren't designed as random-access maps, and are basically
bits of bookkeeping that make sense only on the driver, I think the
realization of something like this in Spark would realistically be
"collect RDD to local data structure" if anything.

It sounds like you're looking for a distributed cache, and there are
frameworks for that that can be used with Spark without Spark
rebuilding that too.

On Mon, Mar 23, 2015 at 12:00 PM, Guillaume Pitel
<guillaume.pitel@exensa.com> wrote:
> Not far, but not exactly. The RDD could be too big to fit in memory,
>
> The idea is more like a worker-side rdd.lookup() with local cache.
>
> Guillaume
>
> In a sentence, is this the idea of collecting an RDD to memory on each
> executor directly?
>
> On Sun, Mar 22, 2015 at 10:56 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
>
> Hi Guillaume,
>
> I've long thought something like this would be useful - i.e. the ability =
to
> broadcast RDDs directly without first pulling data through the driver.  I=
f I
> understand correctly, your requirement to "block" a matrix up and only fe=
tch
> the needed parts could be implemented on top of this by splitting an RDD
> into a set of smaller RDDs and then broadcasting each one on its own.
>
> Unfortunately nobody is working on this currently (and I couldn't promise=
 to
> have bandwidth to review it at the moment either), but I suspect we'll
> eventually need to add something like this for map joins in Hive on Spark
> and Spark SQL.
>
> -Sandy
>
>
>
> On Sat, Mar 21, 2015 at 3:11 AM, Guillaume Pitel
> <guillaume.pitel@exensa.com> wrote:
>
> Hi,
>
> Thanks for your answer. This is precisely the use case I'm interested in,
> but I know it already, I should have mentionned it. Unfortunately this
> implementation of BlockMatrix has (in my opinion) some disadvantages (the
> fact that it split the matrix by range instead of using a modulo is bad f=
or
> block skewness). Besides, and more importantly, as I was writing, it uses
> the join solution (actually a cogroup :
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/linalg/distributed/BlockMatrix.scala,
> line 361). The reduplication of the elements of the dense matrix is thus
> dependent on the block size.
>
> Actually I'm wondering if what I want to achieve could be made with a
> simple modification to the join, allowing a partition to be weakly cached
> wafter being retrieved.
>
> Guillaume
>
>
> There is block matrix in Spark 1.3 -
> http://spark.apache.org/docs/latest/mllib-data-types.html#blockmatrix
>
>
>
>
>
> However I believe it only supports dense matrix blocks.
>
>
>
>
> Still, might be possible to use it or exetend
>
>
>
>
> JIRAs:
>
>
> https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-3434
>
>
>
>
>
> Was based on
>
>
> https://github.com/amplab/ml-matrix
>
>
>
>
>
> Another lib:
>
>
> https://github.com/PasaLab/marlin/blob/master/README.md
>
>
>
>
>
>
>
> =E2=80=94
> Sent from Mailbox
>
> On Sat, Mar 21, 2015 at 12:24 AM, Guillaume Pitel
> <guillaume.pitel@exensa.com> wrote:
>
> Hi,
> I have an idea that I would like to discuss with the Spark devs. The
> idea comes from a very real problem that I have struggled with since
> almost a year. My problem is very simple, it's a dense matrix * sparse
> matrix  operation. I have a dense matrix RDD[(Int,FloatMatrix)] which is
> divided in X large blocks (one block per partition), and a sparse matrix
> RDD[((Int,Int),Array[Array[(Int,Float)]]] , divided in X * Y blocks. The
> most efficient way to perform the operation is to collectAsMap() the
> dense matrix and broadcast it, then perform the block-local
> mutliplications, and combine the results by column.
> This is quite fine, unless the matrix is too big to fit in memory
> (especially since the multiplication is performed several times
> iteratively, and the broadcasts are not always cleaned from memory as I
> would naively expect).
> When the dense matrix is too big, a second solution is to split the big
> sparse matrix in several RDD, and do several broadcasts. Doing this
> creates quite a big overhead, but it mostly works, even though I often
> face some problems with unaccessible broadcast files, for instance.
> Then there is the terrible but apparently very effective good old join.
> Since X blocks of the sparse matrix use the same block from the dense
> matrix, I suspect that the dense matrix is somehow replicated X times
> (either on disk or in the network), which is the reason why the join
> takes so much time.
> After this bit of a context, here is my idea : would it be possible to
> somehow "broadcast" (or maybe more accurately, share or serve) a
> persisted RDD which is distributed on all workers, in a way that would,
> a bit like the IndexedRDD, allow a task to access a partition or an
> element of a partition in the closure, with a worker-local memory cache
> . i.e. the information about where each block resides would be
> distributed on the workers, to allow them to access parts of the RDD
> directly. I think that's already a bit how RDD are shuffled ?
> The RDD could stay distributed (no need to collect then broadcast), and
> only necessary transfers would be required.
> Is this a bad idea, is it already implemented somewhere (I would love it
> !) ?or is it something that could add efficiency not only for my use
> case, but maybe for others ? Could someone give me some hint about how I
> could add this possibility to Spark ? I would probably try to extend a
> RDD into a specific SharedIndexedRDD with a special lookup that would be
> allowed from tasks as a special case, and that would try to contact the
> blockManager and reach the corresponding data from the right worker.
> Thanks in advance for your advices
> Guillaume
> --
> eXenSa
> =09
> *Guillaume PITEL, Pr=C3=A9sident*
> +33(0)626 222 431
> eXenSa S.A.S. <http://www.exensa.com/>
> 41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>
>
>
> --
> Guillaume PITEL, Pr=C3=A9sident
> +33(0)626 222 431
>
> eXenSa S.A.S.
> 41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
> Tel +33(0)184 163 677 / Fax +33(0)972 283 705
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>
>
> --
> Guillaume PITEL, Pr=C3=A9sident
> +33(0)626 222 431
>
> eXenSa S.A.S.
> 41, rue P=C3=A9rier - 92120 Montrouge - FRANCE
> Tel +33(0)184 163 677 / Fax +33(0)972 283 705

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 15:39:17 2015
Return-Path: <dev-return-12105-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D5E0172D1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 15:39:17 +0000 (UTC)
Received: (qmail 44380 invoked by uid 500); 23 Mar 2015 15:39:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44288 invoked by uid 500); 23 Mar 2015 15:39:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44276 invoked by uid 99); 23 Mar 2015 15:39:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 15:39:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of neilkdev@gmail.com designates 209.85.217.170 as permitted sender)
Received: from [209.85.217.170] (HELO mail-lb0-f170.google.com) (209.85.217.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 15:38:51 +0000
Received: by lbbsy1 with SMTP id sy1so121255169lbb.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 08:37:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=dNuaBQzCkyWXsR2jtvczN83U5FgIh4a7+4O/1wh9wzM=;
        b=tgXicC950TGwP344XmGGRTWblnzIG+mBfFY8u+Fv8lvedrfDRoJFm1eDmmREa2KzOe
         CKyc6r9OIufExQtyx04/i3oFyJAsxERqzq6PXEg8eSrm2hzOUM8b1YeM50st/Um5Q2Q5
         tdRjQiM99dr2r1Gz4uAoYFtLEtXCPya6m7ix0g6yBRy9q7qjez7KZchsJpMwOIRm8QgJ
         GTzznmgmvmRBT242bGzRtCj0PrxZiQ9exM+IHJJsg3yEoyKCtypsUdajaPut8k0CTkMY
         IUaQ3fIIHG4qzCjneWxFkZH9CFiAxoW5gegv/UF9PYJTqAgE4ISIF+azV5ziiJApCmSl
         aN8Q==
MIME-Version: 1.0
X-Received: by 10.152.6.71 with SMTP id y7mr71944746lay.116.1427125039397;
 Mon, 23 Mar 2015 08:37:19 -0700 (PDT)
Received: by 10.152.136.233 with HTTP; Mon, 23 Mar 2015 08:37:19 -0700 (PDT)
Date: Mon, 23 Mar 2015 11:37:19 -0400
Message-ID: <CADfwMNJ2+eicw2YJkw3cRo9NmBU=2771bNSsCj73rePMuMmN9g@mail.gmail.com>
Subject: Starting sparkthrift server
From: Neil Dev <neilkdev@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149412e05fe840511f67106
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149412e05fe840511f67106
Content-Type: text/plain; charset=UTF-8

Hi,

I am having issues starting spark-thriftserver. I'm running spark 1.3.o
with Hadoop 2.4.0. I would like to be able to change its port too so, I can
hive hive-thriftserver as well as spark-thriftserver running at the same
time.

Starting sparkthrift server:-
sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
--executor-memory 2G

Error:-
I created the folder manually but still getting the following error----
Exception in thread "main" java.lang.IllegalArgumentException: Log
directory /tmp/spark-events does not exist.


I am getting the following error
15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
org.apache.thrift.transport.TTransportException: Could not create
ServerSocket on address0.0.0.0/0.0.0.0:10000.
        at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
        at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
        at
org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactory.java:236)
        at
org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:69)
        at java.lang.Thread.run(Thread.java:745)

Thanks
Neil

--089e0149412e05fe840511f67106--

From dev-return-12106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 19:03:47 2015
Return-Path: <dev-return-12106-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 781C417EE7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 19:03:47 +0000 (UTC)
Received: (qmail 45244 invoked by uid 500); 23 Mar 2015 19:02:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45186 invoked by uid 500); 23 Mar 2015 19:02:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44969 invoked by uid 99); 23 Mar 2015 19:02:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 19:02:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of neilkdev@gmail.com designates 209.85.217.181 as permitted sender)
Received: from [209.85.217.181] (HELO mail-lb0-f181.google.com) (209.85.217.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 19:02:35 +0000
Received: by lbcmq2 with SMTP id mq2so15248450lbc.0;
        Mon, 23 Mar 2015 12:01:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=OhVTvaOl+vNgvJYS5MqjiytD0MP2NjNRCyz8SMcL5dI=;
        b=r1MyHCpjnoGryINQPKDrmSXM7p3UuoYora5NSdsd4xmc4pM1MW79ZzfJ05CI/nWR9i
         6qCbXdwjpHgg1sQVaWCjsAarbO1xi3bkH+4CGdT7SMFSL1WLPK2YdqiOsEicHyI5SuDU
         6ADce4O9Qiak2XYgP0Y33wJw1Y2WIuVfr1KX2a3vOPlYyixhslGDTd5B1FdTAa9yKPUw
         /naUy7Ykg0qfxwUmLsCD6TdIRkil6iQfhoH2oMGs/h/VwezZ+H6XKZLAhHzGqs23r4oB
         LjGmV8TJVNk2svxMQQSe3iUMwb5wFJSMJp/vlJcp/qHc2bQWaXWIg8H9cvpMRv0ZsMn8
         ICKg==
MIME-Version: 1.0
X-Received: by 10.112.140.38 with SMTP id rd6mr495379lbb.116.1427137289296;
 Mon, 23 Mar 2015 12:01:29 -0700 (PDT)
Received: by 10.152.136.233 with HTTP; Mon, 23 Mar 2015 12:01:29 -0700 (PDT)
Date: Mon, 23 Mar 2015 15:01:29 -0400
Message-ID: <CADfwMNLmtg6mKEyeRMCc9uCqg5+ECd7aA5mNux4ph2uq-14heA@mail.gmail.com>
Subject: Spark-thriftserver Issue
From: Neil Dev <neilkdev@gmail.com>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c349002c9f250511f94bc1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c349002c9f250511f94bc1
Content-Type: text/plain; charset=UTF-8

Hi,

I am having issue starting spark-thriftserver. I'm running spark 1.3.with
Hadoop 2.4.0. I would like to be able to change its port too so, I can hive
hive-thriftserver as well as spark-thriftserver running at the same time.

Starting sparkthrift server:-
sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
--executor-memory 2G

Error:-
I created the folder manually but still getting the following error----
Exception in thread "main" java.lang.IllegalArgumentException: Log
directory /tmp/spark-events does not exist.


I am getting the following error
15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
org.apache.thrift.transport.TTransportException: Could not create
ServerSocket on address0.0.0.0/0.0.0.0:10000.
        at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
        at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
        at
org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactory.java:236)
        at
org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:69)
        at java.lang.Thread.run(Thread.java:745)

Thanks
Neil

--001a11c349002c9f250511f94bc1--

From dev-return-12107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 19:38:38 2015
Return-Path: <dev-return-12107-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D103172D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 19:38:38 +0000 (UTC)
Received: (qmail 55283 invoked by uid 500); 23 Mar 2015 19:38:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55205 invoked by uid 500); 23 Mar 2015 19:38:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55185 invoked by uid 99); 23 Mar 2015 19:38:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 19:38:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of denny.g.lee@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 19:38:27 +0000
Received: by igbqf9 with SMTP id qf9so48238921igb.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 12:38:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :content-type;
        bh=YXWGEg7/G2ILYKH5K0/GMZTF4oNUJaBAYL7HvdMcogU=;
        b=kM4PnOw1BQOMUHBijeEbCYqjSP07sggkRuZg/Li3ygdf6X8Pom7OGxURiHOPL0Fdst
         TujvCgmkuOB1xIEYVlTPjoMRbFRQTeWDGuUQ+mgZY2T5BhcVv42t9G4w/x8uwwfUza26
         wXT6At9fONGKk0zWM4PDi0uKYj/Z/nc4bxjDdqjBYsHLx6xwlJudXAObzxxQkY83OuHc
         XObAw+dovOz5NWBNsHp1wlabsWKvr3gwCwQ9oYbwYTS2NjKx+iyg2fP+Rb71SOg8CoTO
         OQAA/CxvMN++bfnEyzxjJB64VOZwMbz+PBEZgEwZIUkfg1V+PKuha3d3W+KsmkvFbjyi
         poOg==
X-Received: by 10.43.6.74 with SMTP id oj10mr21691913icb.92.1427139486790;
 Mon, 23 Mar 2015 12:38:06 -0700 (PDT)
MIME-Version: 1.0
References: <CADfwMNJ2+eicw2YJkw3cRo9NmBU=2771bNSsCj73rePMuMmN9g@mail.gmail.com>
In-Reply-To: <CADfwMNJ2+eicw2YJkw3cRo9NmBU=2771bNSsCj73rePMuMmN9g@mail.gmail.com>
From: Denny Lee <denny.g.lee@gmail.com>
Date: Mon, 23 Mar 2015 19:38:05 +0000
Message-ID: <CABjYQ3-H8v+FuLissgtUsfXZ_20-ygYaYvdQJ_Gkfj+o4-WSbw@mail.gmail.com>
Subject: Re: Starting sparkthrift server
To: Neil Dev <neilkdev@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec5101d6d27c0dd0511f9ceb5
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec5101d6d27c0dd0511f9ceb5
Content-Type: text/plain; charset=UTF-8

It appears that you are running the thrift-server using the spark-events
account but the /tmp/spark-events folder doesn't exist or the user running
thrift-server does not have access to it.  Have you been able to run Hive
using the spark-events user so that way the /tmp/spark-events folder has
been created.  If you need to reassign the scratch dir / log dir to another
folder (instead of /tmp/spark-events), you could use  --hiveconf to assign
those to another folder.


On Mon, Mar 23, 2015 at 8:39 AM Neil Dev <neilkdev@gmail.com> wrote:

> Hi,
>
> I am having issues starting spark-thriftserver. I'm running spark 1.3.o
> with Hadoop 2.4.0. I would like to be able to change its port too so, I can
> hive hive-thriftserver as well as spark-thriftserver running at the same
> time.
>
> Starting sparkthrift server:-
> sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
> --executor-memory 2G
>
> Error:-
> I created the folder manually but still getting the following error----
> Exception in thread "main" java.lang.IllegalArgumentException: Log
> directory /tmp/spark-events does not exist.
>
>
> I am getting the following error
> 15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
> org.apache.thrift.transport.TTransportException: Could not create
> ServerSocket on address0.0.0.0/0.0.0.0:10000.
>         at
> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
>         at
> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
>         at
> org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(
> HiveAuthFactory.java:236)
>         at
> org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.
> run(ThriftBinaryCLIService.java:69)
>         at java.lang.Thread.run(Thread.java:745)
>
> Thanks
> Neil
>

--bcaec5101d6d27c0dd0511f9ceb5--

From dev-return-12108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 20:03:25 2015
Return-Path: <dev-return-12108-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 963BA173FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 20:03:25 +0000 (UTC)
Received: (qmail 18783 invoked by uid 500); 23 Mar 2015 20:03:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18710 invoked by uid 500); 23 Mar 2015 20:03:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18698 invoked by uid 99); 23 Mar 2015 20:03:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:03:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of neilkdev@gmail.com designates 209.85.215.45 as permitted sender)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:03:13 +0000
Received: by lagg8 with SMTP id g8so143285410lag.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 13:02:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=pa5LfB0eJPxg73249yNc0KO2Vx2FrfGlc/RkbJz2O6Q=;
        b=SQIX7XiLT+30ZMUhm9JXD0DujBY+d/hyDFR3HrcJtZE0P+v4dSD6rOUHPIdEOB+hWN
         CAp8YSEKpMSlrLwfFTDY1eoIf6uhBKyNcYtzeBANpl28Gd6LWIBUsu3kkDatCJAOM9em
         Mwex6bYqGpwZKc4MNBexQBdUiGNpCjwgek8xkrnKccXvf5xoHrwnEX6MILGSrODQgPl7
         tpBIaSMbNIJmbzYT0dKsxq2lyA86gV9Gv9dMIZcqjacIeJOrCMWMHbbhvh6ZrPecw508
         piUdvhj4OGnFiebMVMGUVNaTpfKE4Bf/fCh3AefiIrva8ETOSliDUi3w9xYPaaRtBh5V
         NAgQ==
MIME-Version: 1.0
X-Received: by 10.112.39.69 with SMTP id n5mr681349lbk.1.1427140927784; Mon,
 23 Mar 2015 13:02:07 -0700 (PDT)
Received: by 10.152.136.233 with HTTP; Mon, 23 Mar 2015 13:02:07 -0700 (PDT)
In-Reply-To: <CABjYQ3-H8v+FuLissgtUsfXZ_20-ygYaYvdQJ_Gkfj+o4-WSbw@mail.gmail.com>
References: <CADfwMNJ2+eicw2YJkw3cRo9NmBU=2771bNSsCj73rePMuMmN9g@mail.gmail.com>
	<CABjYQ3-H8v+FuLissgtUsfXZ_20-ygYaYvdQJ_Gkfj+o4-WSbw@mail.gmail.com>
Date: Mon, 23 Mar 2015 16:02:07 -0400
Message-ID: <CADfwMNJ7+jQ8Gyrxp4q8cUU4ZGwb_24iBpsKos5Jsx7YOenACQ@mail.gmail.com>
Subject: Re: Starting sparkthrift server
From: Neil Dev <neilkdev@gmail.com>
To: Denny Lee <denny.g.lee@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11349d200b8c6e0511fa24fd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11349d200b8c6e0511fa24fd
Content-Type: text/plain; charset=UTF-8

we are running this right now as root user and the folder /tmp/spark-events
was manually created and the Job has access to this folder

On Mon, Mar 23, 2015 at 3:38 PM, Denny Lee <denny.g.lee@gmail.com> wrote:

> It appears that you are running the thrift-server using the spark-events
> account but the /tmp/spark-events folder doesn't exist or the user running
> thrift-server does not have access to it.  Have you been able to run Hive
> using the spark-events user so that way the /tmp/spark-events folder has
> been created.  If you need to reassign the scratch dir / log dir to another
> folder (instead of /tmp/spark-events), you could use  --hiveconf to assign
> those to another folder.
>
>
> On Mon, Mar 23, 2015 at 8:39 AM Neil Dev <neilkdev@gmail.com> wrote:
>
>> Hi,
>>
>> I am having issues starting spark-thriftserver. I'm running spark 1.3.o
>> with Hadoop 2.4.0. I would like to be able to change its port too so, I
>> can
>> hive hive-thriftserver as well as spark-thriftserver running at the same
>> time.
>>
>> Starting sparkthrift server:-
>> sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
>> --executor-memory 2G
>>
>> Error:-
>> I created the folder manually but still getting the following error----
>> Exception in thread "main" java.lang.IllegalArgumentException: Log
>> directory /tmp/spark-events does not exist.
>>
>>
>> I am getting the following error
>> 15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
>> org.apache.thrift.transport.TTransportException: Could not create
>> ServerSocket on address0.0.0.0/0.0.0.0:10000.
>>         at
>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
>>         at
>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
>>         at
>> org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(
>> HiveAuthFactory.java:236)
>>         at
>> org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.
>> run(ThriftBinaryCLIService.java:69)
>>         at java.lang.Thread.run(Thread.java:745)
>>
>> Thanks
>> Neil
>>
>

--001a11349d200b8c6e0511fa24fd--

From dev-return-12109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 20:34:59 2015
Return-Path: <dev-return-12109-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9599017670
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 20:34:59 +0000 (UTC)
Received: (qmail 61952 invoked by uid 500); 23 Mar 2015 20:34:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61875 invoked by uid 500); 23 Mar 2015 20:34:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61864 invoked by uid 99); 23 Mar 2015 20:34:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:34:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of irashid@cloudera.com designates 74.125.82.48 as permitted sender)
Received: from [74.125.82.48] (HELO mail-wg0-f48.google.com) (74.125.82.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:34:54 +0000
Received: by wgbcc7 with SMTP id cc7so155328054wgb.0
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 13:33:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=KggXCiNGBaYHZguVLVsca/Ch52pdyEE3BvpCTBsYbUs=;
        b=YBtzc9W6dBishg1YiTASuJnwcs8xcshcYwORGICnjnsMZxti2QhD+9wbLxuPrIjMoI
         dlhwby5jCrRSNHIqNY6swwmamFMh2reYVMw8SQJGAuoftWFspRd42BHNdllcCSM/Dfw6
         vmjV7+PWuH+E+kyKmTnrTaJOFLiIJ/oLe0PKpPnbQ14JgohLOf5OJpvw3bXt+EznyfJQ
         h0un+YEHn1/VqrYvjoQ2VOz+y4V5y7AMhdOuzV5YbIpkYtY5J4qqHevexa2jOFK4L9Yp
         ss4CVEHYGlgagowhRYPOlpI77TaA77HQgs5M97NWyRASjfVe56Sbq9DExB/DjKHPA7uX
         m7CQ==
X-Gm-Message-State: ALoCoQmHZU0jUJVn7G2Qvhe1OIDxDUKJejE+m5XOVyCCf+bXeou/YXCdrUAUBIR3LbrT1yyQpj5v
X-Received: by 10.194.191.228 with SMTP id hb4mr1735216wjc.116.1427142828271;
 Mon, 23 Mar 2015 13:33:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.62.197 with HTTP; Mon, 23 Mar 2015 13:33:27 -0700 (PDT)
In-Reply-To: <CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
 <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
 <55072B1A.7000907@oracle.com> <CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
 <CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Mon, 23 Mar 2015 15:33:27 -0500
Message-ID: <CA+3qhFTx0_YTrNChkj0nJSH+SOs7N=ZOkBjiLzbhg1ceUUMpBQ@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Xiangrui Meng <mengxr@gmail.com>
Cc: Aaron Davidson <ilikerps@gmail.com>, Kevin Markey <kevin.markey@oracle.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b8750be52b89e0511fa9516
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b8750be52b89e0511fa9516
Content-Type: text/plain; charset=UTF-8

I've just switched some of my code over to the new format, and I just want
to make sure everyone realizes what we are getting into.  I went from 10
lines as java enums

https://github.com/squito/spark/blob/fef66058612ebf225e58dd5f5fea6bae1afd5b31/core/src/main/java/org/apache/spark/status/api/StageStatus.java#L20

to 30 lines with the new format:

https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L250

its not just that its verbose.  each name has to be repeated 4 times, with
potential typos in some locations that won't be caught by the compiler.
Also, you have to manually maintain the "values" as you update the set of
enums, the compiler won't do it for you.

The only downside I've heard for java enums is enum.hashcode().  OTOH, the
downsides for this version are: maintainability / verbosity, no values(),
more cumbersome to use from java, no enum map / enumset.

I did put together a little util to at least get back the equivalent of
enum.valueOf() with this format

https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/util/SparkEnum.scala

I'm not trying to prevent us from moving forward on this, its fine if this
is still what everyone wants, but I feel pretty strongly java enums make
more sense.

thanks,
Imran


On Tue, Mar 17, 2015 at 2:07 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Let me put a quick summary. #4 got majority vote with CamelCase but
> not UPPERCASE. The following is a minimal implementation that works
> for both Scala and Java. In Python, we use string for enums. This
> proposal is only for new public APIs. We are not going to change
> existing ones. -Xiangrui
>
> ~~~
> sealed abstract class StorageLevel
>
> object StorageLevel {
>
>   def fromString(name: String): StorageLevel = ???
>
>   val MemoryOnly: StorageLevel = {
>     case object MemoryOnly extends StorageLevel
>     MemoryOnly
>   }
>
>   val DiskOnly: StorageLevel = {
>     case object DiskOnly extends StorageLevel
>     DiskOnly
>  }
> }
> ~~~
>
> On Mon, Mar 16, 2015 at 3:04 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> > It's unrelated to the proposal, but Enum#ordinal() should be much faster,
> > assuming it's not serialized to JVMs with different versions of the enum
> :)
> >
> > On Mon, Mar 16, 2015 at 12:12 PM, Kevin Markey <kevin.markey@oracle.com>
> > wrote:
> >
> >> In some applications, I have rather heavy use of Java enums which are
> >> needed for related Java APIs that the application uses.  And
> unfortunately,
> >> they are also used as keys.  As such, using the native hashcodes makes
> any
> >> function over keys unstable and unpredictable, so we now use
> Enum.name() as
> >> the key instead.  Oh well.  But it works and seems to work well.
> >>
> >> Kevin
> >>
> >>
> >> On 03/05/2015 09:49 PM, Mridul Muralidharan wrote:
> >>
> >>>    I have a strong dislike for java enum's due to the fact that they
> >>> are not stable across JVM's - if it undergoes serde, you end up with
> >>> unpredictable results at times [1].
> >>> One of the reasons why we prevent enum's from being key : though it is
> >>> highly possible users might depend on it internally and shoot
> >>> themselves in the foot.
> >>>
> >>> Would be better to keep away from them in general and use something
> more
> >>> stable.
> >>>
> >>> Regards,
> >>> Mridul
> >>>
> >>> [1] Having had to debug this issue for 2 weeks - I really really hate
> it.
> >>>
> >>>
> >>> On Thu, Mar 5, 2015 at 1:08 PM, Imran Rashid <irashid@cloudera.com>
> >>> wrote:
> >>>
> >>>> I have a very strong dislike for #1 (scala enumerations).   I'm ok
> with
> >>>> #4
> >>>> (with Xiangrui's final suggestion, especially making it sealed &
> >>>> available
> >>>> in Java), but I really think #2, java enums, are the best option.
> >>>>
> >>>> Java enums actually have some very real advantages over the other
> >>>> approaches -- you get values(), valueOf(), EnumSet, and EnumMap.
> There
> >>>> has
> >>>> been endless debate in the Scala community about the problems with the
> >>>> approaches in Scala.  Very smart, level-headed Scala gurus have
> >>>> complained
> >>>> about their short-comings (Rex Kerr's name is coming to mind, though
> I'm
> >>>> not positive about that); there have been numerous well-thought out
> >>>> proposals to give Scala a better enum.  But the powers-that-be in
> Scala
> >>>> always reject them.  IIRC the explanation for rejecting is basically
> that
> >>>> (a) enums aren't important enough for introducing some new special
> >>>> feature,
> >>>> scala's got bigger things to work on and (b) if you really need a good
> >>>> enum, just use java's enum.
> >>>>
> >>>> I doubt it really matters that much for Spark internals, which is why
> I
> >>>> think #4 is fine.  But I figured I'd give my spiel, because every
> >>>> developer
> >>>> loves language wars :)
> >>>>
> >>>> Imran
> >>>>
> >>>>
> >>>>
> >>>> On Thu, Mar 5, 2015 at 1:35 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>>>
> >>>>  `case object` inside an `object` doesn't show up in Java. This is the
> >>>>> minimal code I found to make everything show up correctly in both
> >>>>> Scala and Java:
> >>>>>
> >>>>> sealed abstract class StorageLevel // cannot be a trait
> >>>>>
> >>>>> object StorageLevel {
> >>>>>    private[this] case object _MemoryOnly extends StorageLevel
> >>>>>    final val MemoryOnly: StorageLevel = _MemoryOnly
> >>>>>
> >>>>>    private[this] case object _DiskOnly extends StorageLevel
> >>>>>    final val DiskOnly: StorageLevel = _DiskOnly
> >>>>> }
> >>>>>
> >>>>> On Wed, Mar 4, 2015 at 8:10 PM, Patrick Wendell <pwendell@gmail.com>
> >>>>> wrote:
> >>>>>
> >>>>>> I like #4 as well and agree with Aaron's suggestion.
> >>>>>>
> >>>>>> - Patrick
> >>>>>>
> >>>>>> On Wed, Mar 4, 2015 at 6:07 PM, Aaron Davidson <ilikerps@gmail.com>
> >>>>>>
> >>>>> wrote:
> >>>>>
> >>>>>> I'm cool with #4 as well, but make sure we dictate that the values
> >>>>>>>
> >>>>>> should
> >>>>>
> >>>>>> be defined within an object with the same name as the enumeration
> (like
> >>>>>>>
> >>>>>> we
> >>>>>
> >>>>>> do for StorageLevel). Otherwise we may pollute a higher namespace.
> >>>>>>>
> >>>>>>> e.g. we SHOULD do:
> >>>>>>>
> >>>>>>> trait StorageLevel
> >>>>>>> object StorageLevel {
> >>>>>>>    case object MemoryOnly extends StorageLevel
> >>>>>>>    case object DiskOnly extends StorageLevel
> >>>>>>> }
> >>>>>>>
> >>>>>>> On Wed, Mar 4, 2015 at 5:37 PM, Michael Armbrust <
> >>>>>>>
> >>>>>> michael@databricks.com>
> >>>>>
> >>>>>> wrote:
> >>>>>>>
> >>>>>>>  #4 with a preference for CamelCaseEnums
> >>>>>>>>
> >>>>>>>> On Wed, Mar 4, 2015 at 5:29 PM, Joseph Bradley <
> >>>>>>>> joseph@databricks.com>
> >>>>>>>> wrote:
> >>>>>>>>
> >>>>>>>>  another vote for #4
> >>>>>>>>> People are already used to adding "()" in Java.
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Wed, Mar 4, 2015 at 5:14 PM, Stephen Boesch <
> javadba@gmail.com>
> >>>>>>>>>
> >>>>>>>> wrote:
> >>>>>>>>
> >>>>>>>>> #4 but with MemoryOnly (more scala-like)
> >>>>>>>>>>
> >>>>>>>>>> http://docs.scala-lang.org/style/naming-conventions.html
> >>>>>>>>>>
> >>>>>>>>>> Constants, Values, Variable and Methods
> >>>>>>>>>>
> >>>>>>>>>> Constant names should be in upper camel case. That is, if the
> >>>>>>>>>>
> >>>>>>>>> member is
> >>>>>
> >>>>>> final, immutable and it belongs to a package object or an object,
> >>>>>>>>>>
> >>>>>>>>> it
> >>>>>
> >>>>>> may
> >>>>>>>>
> >>>>>>>>> be
> >>>>>>>>>
> >>>>>>>>>> considered a constant (similar to Java'sstatic final members):
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>>     1. object Container {
> >>>>>>>>>>     2.     val MyConstant = ...
> >>>>>>>>>>     3. }
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>> 2015-03-04 17:11 GMT-08:00 Xiangrui Meng <mengxr@gmail.com>:
> >>>>>>>>>>
> >>>>>>>>>>  Hi all,
> >>>>>>>>>>>
> >>>>>>>>>>> There are many places where we use enum-like types in Spark,
> but
> >>>>>>>>>>>
> >>>>>>>>>> in
> >>>>>
> >>>>>> different ways. Every approach has both pros and cons. I wonder
> >>>>>>>>>>> whether there should be an "official" approach for enum-like
> >>>>>>>>>>>
> >>>>>>>>>> types in
> >>>>>
> >>>>>> Spark.
> >>>>>>>>>>>
> >>>>>>>>>>> 1. Scala's Enumeration (e.g., SchedulingMode, WorkerState, etc)
> >>>>>>>>>>>
> >>>>>>>>>>> * All types show up as Enumeration.Value in Java.
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>  http://spark.apache.org/docs/latest/api/java/org/apache/
> >>>>> spark/scheduler/SchedulingMode.html
> >>>>>
> >>>>>> 2. Java's Enum (e.g., SaveMode, IOMode)
> >>>>>>>>>>>
> >>>>>>>>>>> * Implementation must be in a Java file.
> >>>>>>>>>>> * Values doesn't show up in the ScalaDoc:
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
> >>>>> spark.network.util.IOMode
> >>>>>
> >>>>>> 3. Static fields in Java (e.g., TripletFields)
> >>>>>>>>>>>
> >>>>>>>>>>> * Implementation must be in a Java file.
> >>>>>>>>>>> * Doesn't need "()" in Java code.
> >>>>>>>>>>> * Values don't show up in the ScalaDoc:
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
> >>>>> spark.graphx.TripletFields
> >>>>>
> >>>>>> 4. Objects in Scala. (e.g., StorageLevel)
> >>>>>>>>>>>
> >>>>>>>>>>> * Needs "()" in Java code.
> >>>>>>>>>>> * Values show up in both ScalaDoc and JavaDoc:
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>  http://spark.apache.org/docs/latest/api/scala/#org.apache.
> >>>>> spark.storage.StorageLevel$
> >>>>>
> >>>>>>
> >>>>>>>>>>>  http://spark.apache.org/docs/latest/api/java/org/apache/
> >>>>> spark/storage/StorageLevel.html
> >>>>>
> >>>>>> It would be great if we have an "official" approach for this as
> >>>>>>>>>>>
> >>>>>>>>>> well
> >>>>>
> >>>>>> as the naming convention for enum-like values ("MEMORY_ONLY" or
> >>>>>>>>>>> "MemoryOnly"). Personally, I like 4) with "MEMORY_ONLY". Any
> >>>>>>>>>>>
> >>>>>>>>>> thoughts?
> >>>>>>>>
> >>>>>>>>> Best,
> >>>>>>>>>>> Xiangrui
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>  ------------------------------------------------------------
> >>>>> ---------
> >>>>>
> >>>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>>>>>>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>>>>>>>>
> >>>>>>>>>>>
> >>>>>>>>>>>  ------------------------------------------------------------
> >>>>> ---------
> >>>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>>>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>>>
> >>>>>
> >>>>>
> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>>
> >>
> >> ---------------------------------------------------------------------
> >> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> For additional commands, e-mail: dev-help@spark.apache.org
> >>
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b8750be52b89e0511fa9516--

From dev-return-12110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 20:48:46 2015
Return-Path: <dev-return-12110-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4928317706
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 20:48:46 +0000 (UTC)
Received: (qmail 96156 invoked by uid 500); 23 Mar 2015 20:48:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96075 invoked by uid 500); 23 Mar 2015 20:48:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96054 invoked by uid 99); 23 Mar 2015 20:48:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:48:32 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:48:27 +0000
Received: by wixw10 with SMTP id w10so74919065wix.0
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 13:46:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=zPLKlpQucxetN9GatAPptdZokiavnasI/EVp00EpI88=;
        b=GxmSZVvEpO5pfdVhbn6e8VDisSBpzeMM4wER1GAMlg6DnmztafWc3iBiTau7HyU+8A
         D3+ziJgFPpYDdXRyC/VX/ZcVcVOptw98YQ4Tv+brAFWYmwI6fr0BRl5LmzPfZfQ5N6Yo
         qikBYLm+F0b/OFt9S4Hf2Cql6g7lh7LSVFrL0qo2YdCLLXB24JEIGotz2x8DDmgxUooP
         JKIGm7dxH28VXikaqV85iJn8yxFJA2HxJw6xyK5YLmRpqqPAFpFWbf3/bIayPE0XLiPP
         5N7sVqbzrDZHXiHjoYguQ23n4yy3fkjoUI4fEudD79Lrppp2TmhzLloZLEX+CWabk1f+
         j3pg==
X-Gm-Message-State: ALoCoQmdbJR3J1+9s9WIepQwngiWyrBPN221VBSyu39Q1GkexDnUCb10vurAM4r77rVB2GvtgzeQ
X-Received: by 10.180.20.43 with SMTP id k11mr22288509wie.93.1427143597218;
 Mon, 23 Mar 2015 13:46:37 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Mon, 23 Mar 2015 13:46:16 -0700 (PDT)
In-Reply-To: <CA+3qhFTx0_YTrNChkj0nJSH+SOs7N=ZOkBjiLzbhg1ceUUMpBQ@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
 <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
 <55072B1A.7000907@oracle.com> <CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
 <CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com> <CA+3qhFTx0_YTrNChkj0nJSH+SOs7N=ZOkBjiLzbhg1ceUUMpBQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 23 Mar 2015 20:46:16 +0000
Message-ID: <CAMAsSd+K4igb059ZX9uC0mmOYpaHWwTCY9fbUV8A2Zv5sysq3w@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Imran Rashid <irashid@cloudera.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, Aaron Davidson <ilikerps@gmail.com>, 
	Kevin Markey <kevin.markey@oracle.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah the fully realized #4, which gets back the ability to use it in
switch statements (? in Scala but not Java?) does end up being kind of
huge.

I confess I'm swayed a bit back to Java enums, seeing what it
involves. The hashCode() issue can be 'solved' with the hash of the
String representation.

On Mon, Mar 23, 2015 at 8:33 PM, Imran Rashid <irashid@cloudera.com> wrote:
> I've just switched some of my code over to the new format, and I just want
> to make sure everyone realizes what we are getting into.  I went from 10
> lines as java enums
>
> https://github.com/squito/spark/blob/fef66058612ebf225e58dd5f5fea6bae1afd5b31/core/src/main/java/org/apache/spark/status/api/StageStatus.java#L20
>
> to 30 lines with the new format:
>
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L250
>
> its not just that its verbose.  each name has to be repeated 4 times, with
> potential typos in some locations that won't be caught by the compiler.
> Also, you have to manually maintain the "values" as you update the set of
> enums, the compiler won't do it for you.
>
> The only downside I've heard for java enums is enum.hashcode().  OTOH, the
> downsides for this version are: maintainability / verbosity, no values(),
> more cumbersome to use from java, no enum map / enumset.
>
> I did put together a little util to at least get back the equivalent of
> enum.valueOf() with this format
>
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/util/SparkEnum.scala
>
> I'm not trying to prevent us from moving forward on this, its fine if this
> is still what everyone wants, but I feel pretty strongly java enums make
> more sense.
>
> thanks,
> Imran

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 20:52:00 2015
Return-Path: <dev-return-12111-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E54A017725
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 20:52:00 +0000 (UTC)
Received: (qmail 5383 invoked by uid 500); 23 Mar 2015 20:51:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5301 invoked by uid 500); 23 Mar 2015 20:51:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5281 invoked by uid 99); 23 Mar 2015 20:51:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:51:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of denny.g.lee@gmail.com designates 209.85.223.176 as permitted sender)
Received: from [209.85.223.176] (HELO mail-ie0-f176.google.com) (209.85.223.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:51:54 +0000
Received: by iedfl3 with SMTP id fl3so52462103ied.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 13:50:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc:content-type;
        bh=fYeOPLo5UZiKaamoaez1Yphs4MiIB/I8ODCdeffWcWQ=;
        b=OnFzP7VZKwi0VkL5oCBuSMFdEROvBCAOxNDsvOv7t3RsAvFW0WTv9C5fFuzuDoUMYg
         wyUdSS0zT9nfapi9HPmzeW6Aqs7/yPKjbvvramaU0Ihjjo5FicNbgsDB/ETChU0D/+JE
         Mp2okYfNyhuwtSeV9LXFwUH+GdW0vTEBYg9PcAg6TMh8w+JPLWbfFYsr/tJoc88GprhM
         4ZABAacADwd5YMvAxXgGygjfF2fjKlNWLk2KOjGbxaTFwlAeXVErzhWS8jJAAGePrIbJ
         tCW2FvZPjRhV8BwQwyS0owi95y2tukw6gtNYflgSsuCr3frArNC1AhTQrhRMP/gtWivd
         qOQg==
X-Received: by 10.43.163.71 with SMTP id mn7mr21459134icc.72.1427143824472;
 Mon, 23 Mar 2015 13:50:24 -0700 (PDT)
MIME-Version: 1.0
References: <CADfwMNJ2+eicw2YJkw3cRo9NmBU=2771bNSsCj73rePMuMmN9g@mail.gmail.com>
 <CABjYQ3-H8v+FuLissgtUsfXZ_20-ygYaYvdQJ_Gkfj+o4-WSbw@mail.gmail.com> <CADfwMNJ7+jQ8Gyrxp4q8cUU4ZGwb_24iBpsKos5Jsx7YOenACQ@mail.gmail.com>
In-Reply-To: <CADfwMNJ7+jQ8Gyrxp4q8cUU4ZGwb_24iBpsKos5Jsx7YOenACQ@mail.gmail.com>
From: Denny Lee <denny.g.lee@gmail.com>
Date: Mon, 23 Mar 2015 20:50:23 +0000
Message-ID: <CABjYQ3-wn1vrUAYW1A6BznGnnsfLtfHTZ-abcCBYB7yq5AE_uA@mail.gmail.com>
Subject: Re: Starting sparkthrift server
To: Neil Dev <neilkdev@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2e246b382040511fad04e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e246b382040511fad04e
Content-Type: text/plain; charset=UTF-8

When you say the job has access, do you mean that when you run spark-submit
or spark-shell (for example), it is able to write to the /tmp/spark-events
folder?


On Mon, Mar 23, 2015 at 1:02 PM Neil Dev <neilkdev@gmail.com> wrote:

> we are running this right now as root user and the folder /tmp/spark-events
> was manually created and the Job has access to this folder
>
> On Mon, Mar 23, 2015 at 3:38 PM, Denny Lee <denny.g.lee@gmail.com> wrote:
>
>> It appears that you are running the thrift-server using the spark-events
>> account but the /tmp/spark-events folder doesn't exist or the user running
>> thrift-server does not have access to it.  Have you been able to run Hive
>> using the spark-events user so that way the /tmp/spark-events folder has
>> been created.  If you need to reassign the scratch dir / log dir to another
>> folder (instead of /tmp/spark-events), you could use  --hiveconf to assign
>> those to another folder.
>>
>>
>> On Mon, Mar 23, 2015 at 8:39 AM Neil Dev <neilkdev@gmail.com> wrote:
>>
>>> Hi,
>>>
>>> I am having issues starting spark-thriftserver. I'm running spark 1.3.o
>>> with Hadoop 2.4.0. I would like to be able to change its port too so, I
>>> can
>>> hive hive-thriftserver as well as spark-thriftserver running at the same
>>> time.
>>>
>>> Starting sparkthrift server:-
>>> sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
>>> --executor-memory 2G
>>>
>>> Error:-
>>> I created the folder manually but still getting the following error----
>>> Exception in thread "main" java.lang.IllegalArgumentException: Log
>>> directory /tmp/spark-events does not exist.
>>>
>>>
>>> I am getting the following error
>>> 15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
>>> org.apache.thrift.transport.TTransportException: Could not create
>>> ServerSocket on address0.0.0.0/0.0.0.0:10000.
>>>         at
>>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
>>>         at
>>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
>>>         at
>>> org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(
>>> HiveAuthFactory.java:236)
>>>         at
>>> org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.
>>> run(ThriftBinaryCLIService.java:69)
>>>         at java.lang.Thread.run(Thread.java:745)
>>>
>>> Thanks
>>> Neil
>>>
>>
>

--001a11c2e246b382040511fad04e--

From dev-return-12112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 20:53:44 2015
Return-Path: <dev-return-12112-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 42E411772D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 20:53:44 +0000 (UTC)
Received: (qmail 19510 invoked by uid 500); 23 Mar 2015 20:53:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19432 invoked by uid 500); 23 Mar 2015 20:53:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19419 invoked by uid 99); 23 Mar 2015 20:53:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:53:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.176] (HELO mail-lb0-f176.google.com) (209.85.217.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 20:53:37 +0000
Received: by lbbug6 with SMTP id ug6so32986255lbb.3
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 13:52:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=3mWT1J375hCVR/ZXjrWuBIZYSxQ8XYaKmao8JcVy6tA=;
        b=H01ibHWmUuTbA+UEF1TyQ9FwawHf8a0SKJB/l9l3aSUP0Fh59u5LSaPlrC2XbqfETM
         c9wZ1cuztBAO7eb5DYtnhC52xhgvMaN6Serj9Kd7w0h9Xu9wSXQjh6edfVAWBXimj1xa
         1wK0z3o3CA5y+JZy2c9BPvYxECKp//JXPHADPcoCuTCFBlyFZoyrdgS9PCGOLaNuP8Dp
         g//T/HhPOjg/Yi8ftzlMvb69W/X1WI2vIoNpt92vWOCWkZ7T4lIEuFNqppz3v6uG/0y4
         azer87To6AEt5khFQuLIoX6mrNw1Zgp1okIxpuP2pJsyKJvD+huAjG+122JD2oNofKWE
         Yz9w==
X-Gm-Message-State: ALoCoQm0aBR7BEHH9ebTiY1rd4V79GA9VD7ZRBbpPG5o68mBBD7d3p/UvuHhGpRRjE7ZgJGJEDwL
MIME-Version: 1.0
X-Received: by 10.112.155.196 with SMTP id vy4mr924154lbb.56.1427143930770;
 Mon, 23 Mar 2015 13:52:10 -0700 (PDT)
Received: by 10.114.69.198 with HTTP; Mon, 23 Mar 2015 13:52:10 -0700 (PDT)
X-Originating-IP: [204.148.13.62]
In-Reply-To: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
Date: Mon, 23 Mar 2015 16:52:10 -0400
Message-ID: <CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
Subject: Fwd: hadoop input/output format advanced control
From: Koert Kuipers <koert@tresata.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0112cac2098c370511fad7dc
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0112cac2098c370511fad7dc
Content-Type: text/plain; charset=UTF-8

see email below. reynold suggested i send it to dev instead of user

---------- Forwarded message ----------
From: Koert Kuipers <koert@tresata.com>
Date: Mon, Mar 23, 2015 at 4:36 PM
Subject: hadoop input/output format advanced control
To: "user@spark.apache.org" <user@spark.apache.org>


currently its pretty hard to control the Hadoop Input/Output formats used
in Spark. The conventions seems to be to add extra parameters to all
methods and then somewhere deep inside the code (for example in
PairRDDFunctions.saveAsHadoopFile) all these parameters get translated into
settings on the Hadoop Configuration object.

for example for compression i see "codec: Option[Class[_ <:
CompressionCodec]] = None" added to a bunch of methods.

how scalable is this solution really?

for example i need to read from a hadoop dataset and i dont want the input
(part) files to get split up. the way to do this is to set
"mapred.min.split.size". now i dont want to set this at the level of the
SparkContext (which can be done), since i dont want it to apply to input
formats in general. i want it to apply to just this one specific input
dataset i need to read. which leaves me with no options currently. i could
go add yet another input parameter to all the methods
(SparkContext.textFile, SparkContext.hadoopFile, SparkContext.objectFile,
etc.). but that seems ineffective.

why can we not expose a Map[String, String] or some other generic way to
manipulate settings for hadoop input/output formats? it would require
adding one more parameter to all methods to deal with hadoop input/output
formats, but after that its done. one parameter to rule them all....

then i could do:
val x = sc.textFile("/some/path", formatSettings =
Map("mapred.min.split.size" -> "12345"))

or
rdd.saveAsTextFile("/some/path, formatSettings =
Map(mapred.output.compress" -> "true", "mapred.output.compression.codec" ->
"somecodec"))

--089e0112cac2098c370511fad7dc--

From dev-return-12113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:00:46 2015
Return-Path: <dev-return-12113-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DA4A17792
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:00:46 +0000 (UTC)
Received: (qmail 37447 invoked by uid 500); 23 Mar 2015 21:00:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37363 invoked by uid 500); 23 Mar 2015 21:00:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37349 invoked by uid 99); 23 Mar 2015 21:00:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:00:44 +0000
X-ASF-Spam-Status: No, hits=1.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of anubhav33@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:00:40 +0000
Received: by igbud6 with SMTP id ud6so54428753igb.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:00:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=3I0FJEywkB9yjuPxFnPkLO5pJkKZ9aLxWJ7KGntlya8=;
        b=ihVnJ6ble7E2gcWcqMLZe/CHeailx82UZWTiD8gNySVcBaMOfuKyFCQReSVQZ4seXL
         5nssgqjorChzzWRWVPxee8/gZeSFGrytpSKZ3gakLEyX8owiWbSMe9vj7OyQChwgLuWI
         jMWb46lH1nszLs23qMcAGdtcrYuaxJqc+5c8Lzfje3474g1q4tqcMvENIGH+b3Mu+3OZ
         4w6Mq+v7S/R0VfyC+uL02pjJiojbWV+5Ljw1JbPGmqWRq77kay0DA7bLxyHWXCZLZoYh
         oKEgy7Ygu7cKHJigrV/kJKQAUS/In/UzOyyZ02PEsevrQGPpanCSztZL2gZDEOXQjjQH
         t8tA==
MIME-Version: 1.0
X-Received: by 10.107.130.197 with SMTP id m66mr1612646ioi.19.1427144419721;
 Mon, 23 Mar 2015 14:00:19 -0700 (PDT)
Received: by 10.107.8.99 with HTTP; Mon, 23 Mar 2015 14:00:19 -0700 (PDT)
In-Reply-To: <CABjYQ3-wn1vrUAYW1A6BznGnnsfLtfHTZ-abcCBYB7yq5AE_uA@mail.gmail.com>
References: <CADfwMNJ2+eicw2YJkw3cRo9NmBU=2771bNSsCj73rePMuMmN9g@mail.gmail.com>
	<CABjYQ3-H8v+FuLissgtUsfXZ_20-ygYaYvdQJ_Gkfj+o4-WSbw@mail.gmail.com>
	<CADfwMNJ7+jQ8Gyrxp4q8cUU4ZGwb_24iBpsKos5Jsx7YOenACQ@mail.gmail.com>
	<CABjYQ3-wn1vrUAYW1A6BznGnnsfLtfHTZ-abcCBYB7yq5AE_uA@mail.gmail.com>
Date: Mon, 23 Mar 2015 17:00:19 -0400
Message-ID: <CAFL=5a8s5odAhVdzGkzjXJ-q2dBFSv2_ECpmCcK8sbWuFkmmnA@mail.gmail.com>
Subject: Re: Starting sparkthrift server
From: Anubhav Agarwal <anubhav33@gmail.com>
To: Denny Lee <denny.g.lee@gmail.com>
Cc: Neil Dev <neilkdev@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113ed0e22e480a0511faf4e4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ed0e22e480a0511faf4e4
Content-Type: text/plain; charset=UTF-8

When I start spark-shell (for example) it does not write to the
/tmp/spark-events folder. It remains empty. I have even tried it after
giving that folder rwx permission for user, group and others.

Neil's colleague,
Anu

On Mon, Mar 23, 2015 at 4:50 PM, Denny Lee <denny.g.lee@gmail.com> wrote:

> When you say the job has access, do you mean that when you run spark-submit
> or spark-shell (for example), it is able to write to the /tmp/spark-events
> folder?
>
>
> On Mon, Mar 23, 2015 at 1:02 PM Neil Dev <neilkdev@gmail.com> wrote:
>
> > we are running this right now as root user and the folder
> /tmp/spark-events
> > was manually created and the Job has access to this folder
> >
> > On Mon, Mar 23, 2015 at 3:38 PM, Denny Lee <denny.g.lee@gmail.com>
> wrote:
> >
> >> It appears that you are running the thrift-server using the spark-events
> >> account but the /tmp/spark-events folder doesn't exist or the user
> running
> >> thrift-server does not have access to it.  Have you been able to run
> Hive
> >> using the spark-events user so that way the /tmp/spark-events folder has
> >> been created.  If you need to reassign the scratch dir / log dir to
> another
> >> folder (instead of /tmp/spark-events), you could use  --hiveconf to
> assign
> >> those to another folder.
> >>
> >>
> >> On Mon, Mar 23, 2015 at 8:39 AM Neil Dev <neilkdev@gmail.com> wrote:
> >>
> >>> Hi,
> >>>
> >>> I am having issues starting spark-thriftserver. I'm running spark 1.3.o
> >>> with Hadoop 2.4.0. I would like to be able to change its port too so, I
> >>> can
> >>> hive hive-thriftserver as well as spark-thriftserver running at the
> same
> >>> time.
> >>>
> >>> Starting sparkthrift server:-
> >>> sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
> >>> --executor-memory 2G
> >>>
> >>> Error:-
> >>> I created the folder manually but still getting the following error----
> >>> Exception in thread "main" java.lang.IllegalArgumentException: Log
> >>> directory /tmp/spark-events does not exist.
> >>>
> >>>
> >>> I am getting the following error
> >>> 15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
> >>> org.apache.thrift.transport.TTransportException: Could not create
> >>> ServerSocket on address0.0.0.0/0.0.0.0:10000.
> >>>         at
> >>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
> >>>         at
> >>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
> >>>         at
> >>> org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(
> >>> HiveAuthFactory.java:236)
> >>>         at
> >>> org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.
> >>> run(ThriftBinaryCLIService.java:69)
> >>>         at java.lang.Thread.run(Thread.java:745)
> >>>
> >>> Thanks
> >>> Neil
> >>>
> >>
> >
>

--001a113ed0e22e480a0511faf4e4--

From dev-return-12114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:10:37 2015
Return-Path: <dev-return-12114-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 46B05177FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:10:37 +0000 (UTC)
Received: (qmail 59826 invoked by uid 500); 23 Mar 2015 21:10:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59749 invoked by uid 500); 23 Mar 2015 21:10:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59731 invoked by uid 99); 23 Mar 2015 21:10:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:10:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:10:23 +0000
Received: by obbgg8 with SMTP id gg8so133040146obb.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:08:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=jS7bt2U2nezTFhoXvwVkdCzTohDVFSRXowoJUSfRBGQ=;
        b=H2JTWnyBKEYhW70L7vgbqdHaA6MxAaAeWEbFJ+5Dn77KYOmKjThruYgDvr9xSmWrLs
         xPuBH4G8cg6xQq2jJuShiXeYw8/j1foulccTHDk80gw4g+vsrySia7ghVC2JlErndEYA
         wwBGzPZyIWKin9JTRodJDy+QUDhhKqPnd83df+43mvRX1QuYIBUgc/Hg4b1y6SP3pjw8
         ONo26K5L4GuNk3aZSloA1Ob06nFSgxd9F6TYcYS4UYHLNWsHoNGcdGnaw++hhP4+UI+9
         ODWgi8pmE0pPSCxDVX9tMm3AYoPiDJAABJPaTBGVklQ02av3ZGfe5tPocHO1whtYIAZ4
         cbGw==
MIME-Version: 1.0
X-Received: by 10.60.174.19 with SMTP id bo19mr858626oec.5.1427144913148; Mon,
 23 Mar 2015 14:08:33 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Mon, 23 Mar 2015 14:08:33 -0700 (PDT)
Date: Mon, 23 Mar 2015 22:08:33 +0100
Message-ID: <CAO=evYeFGTrDFfyjhpYP_Nf3am_oFLtSTa4PhXZg=uyX28+4Zw@mail.gmail.com>
Subject: Spark Executor resources
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01182362975ef30511fb118f
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01182362975ef30511fb118f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Let's say I'm an Executor instance in a Spark system. Who started me and
where, when I run on a worker node supervised by (a) Mesos, (b) YARN? I
suppose I'm the only one Executor on a worker node for a given framework
scheduler (driver). If I'm an Executor instance, who is the closest object
to me who can tell me how many resources do I have on (a) Mesos, (b) YARN?

Thank you for your kind input!

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

--089e01182362975ef30511fb118f--

From dev-return-12115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:11:04 2015
Return-Path: <dev-return-12115-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DA20917802
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:11:04 +0000 (UTC)
Received: (qmail 62458 invoked by uid 500); 23 Mar 2015 21:11:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62379 invoked by uid 500); 23 Mar 2015 21:11:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62366 invoked by uid 99); 23 Mar 2015 21:11:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:11:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:10:38 +0000
Received: by wixw10 with SMTP id w10so75461121wix.0
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:09:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=wUiCD9J7WNdCJ4KLSLmbag4y58FITQ7eOaePP1W2mTw=;
        b=YKrwY1z8amQTgx3Eq6ZSOEPzXAwUpqfsF7FWOOcfSBz/urrMyIF5x7GS4HIM60U6C6
         d4KkXdfpT8gX+a4iZgaSsJ/GyONNDNNAMj93T5/1ApwApUuO9v8JEPsSkRfJgH/Keeto
         qeR/FiHdyQaQG8iKFESOWUhnobY9dGEBfZ135OLxRCjFO4v5GDbaOpGBQkyQQIwSRaa4
         JblpSdwyPLf6yVQSU92XUdlsej9rNNYHM9jFmkOAOsUoFeGpqCTSMVTnS/AnwXH2V6am
         FXN9ABUtj/y5m6jFmG4bia0czZLu8sSr6FPY9LYEAeUTdqDxmtYDKtIjl1iMwXCYzTL9
         QEng==
X-Received: by 10.180.80.9 with SMTP id n9mr22571330wix.34.1427144992032; Mon,
 23 Mar 2015 14:09:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Mon, 23 Mar 2015 14:09:31 -0700 (PDT)
In-Reply-To: <CAMAsSd+K4igb059ZX9uC0mmOYpaHWwTCY9fbUV8A2Zv5sysq3w@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
 <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
 <55072B1A.7000907@oracle.com> <CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
 <CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com>
 <CA+3qhFTx0_YTrNChkj0nJSH+SOs7N=ZOkBjiLzbhg1ceUUMpBQ@mail.gmail.com> <CAMAsSd+K4igb059ZX9uC0mmOYpaHWwTCY9fbUV8A2Zv5sysq3w@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 23 Mar 2015 14:09:31 -0700
Message-ID: <CANGvG8rbB0bP8PRAonoH4-3rhoM97Lc6QwQp711+cdDoPis8Ow@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Sean Owen <sowen@cloudera.com>
Cc: Imran Rashid <irashid@cloudera.com>, Xiangrui Meng <mengxr@gmail.com>, 
	Kevin Markey <kevin.markey@oracle.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d041825444b0ab40511fb1631
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d041825444b0ab40511fb1631
Content-Type: text/plain; charset=UTF-8

The only issue I knew of with Java enums was that it does not appear in the
Scala documentation.

On Mon, Mar 23, 2015 at 1:46 PM, Sean Owen <sowen@cloudera.com> wrote:

> Yeah the fully realized #4, which gets back the ability to use it in
> switch statements (? in Scala but not Java?) does end up being kind of
> huge.
>
> I confess I'm swayed a bit back to Java enums, seeing what it
> involves. The hashCode() issue can be 'solved' with the hash of the
> String representation.
>
> On Mon, Mar 23, 2015 at 8:33 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
> > I've just switched some of my code over to the new format, and I just
> want
> > to make sure everyone realizes what we are getting into.  I went from 10
> > lines as java enums
> >
> >
> https://github.com/squito/spark/blob/fef66058612ebf225e58dd5f5fea6bae1afd5b31/core/src/main/java/org/apache/spark/status/api/StageStatus.java#L20
> >
> > to 30 lines with the new format:
> >
> >
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L250
> >
> > its not just that its verbose.  each name has to be repeated 4 times,
> with
> > potential typos in some locations that won't be caught by the compiler.
> > Also, you have to manually maintain the "values" as you update the set of
> > enums, the compiler won't do it for you.
> >
> > The only downside I've heard for java enums is enum.hashcode().  OTOH,
> the
> > downsides for this version are: maintainability / verbosity, no values(),
> > more cumbersome to use from java, no enum map / enumset.
> >
> > I did put together a little util to at least get back the equivalent of
> > enum.valueOf() with this format
> >
> >
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/util/SparkEnum.scala
> >
> > I'm not trying to prevent us from moving forward on this, its fine if
> this
> > is still what everyone wants, but I feel pretty strongly java enums make
> > more sense.
> >
> > thanks,
> > Imran
>

--f46d041825444b0ab40511fb1631--

From dev-return-12116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:12:01 2015
Return-Path: <dev-return-12116-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8213817807
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:12:01 +0000 (UTC)
Received: (qmail 64874 invoked by uid 500); 23 Mar 2015 21:12:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64790 invoked by uid 500); 23 Mar 2015 21:12:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64778 invoked by uid 99); 23 Mar 2015 21:12:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:12:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:11:35 +0000
Received: by oiag65 with SMTP id g65so151871946oia.2
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:11:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=pKl2ZDchDwM8dI5i7lVC2zAXl2M8pYx6VFpagrtQJuo=;
        b=vj/JJGldmSwG4c43DAeIvPxlhifttgzyQZ87oUZDa0YKaDljcVIzLQW5qshBn76quf
         zZfc9U2IJ9cgIzhBZaMRKcBuOe1TY+nzxKhhbdeff6/rVSLVI49ysVkaOjPR0FleTev6
         2wFSdNWRW2XxiWe07NN9VSFqflNMZqRCE+kvMQ5ZEPho6TUcGHyQwIEzkBn7FEjmubYt
         cwywJh7fHvZwUeL2TBByGrSCJqrP4cV8z+faMiaVKnV/kmagi9Z0f+oMROGYfY2Vq13B
         Len3Z1SXzGE3+PKbMu2tU3sVxB5PwrSOGGnWDlhII5YqkMkApW6zUil25M3WUPgb1vKu
         ldIg==
MIME-Version: 1.0
X-Received: by 10.202.231.85 with SMTP id e82mr807421oih.104.1427145093973;
 Mon, 23 Mar 2015 14:11:33 -0700 (PDT)
Received: by 10.202.1.23 with HTTP; Mon, 23 Mar 2015 14:11:33 -0700 (PDT)
In-Reply-To: <CAMAsSd+K4igb059ZX9uC0mmOYpaHWwTCY9fbUV8A2Zv5sysq3w@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
	<CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
	<CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
	<CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
	<CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
	<CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
	<CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
	<CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
	<CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
	<55072B1A.7000907@oracle.com>
	<CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
	<CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com>
	<CA+3qhFTx0_YTrNChkj0nJSH+SOs7N=ZOkBjiLzbhg1ceUUMpBQ@mail.gmail.com>
	<CAMAsSd+K4igb059ZX9uC0mmOYpaHWwTCY9fbUV8A2Zv5sysq3w@mail.gmail.com>
Date: Mon, 23 Mar 2015 14:11:33 -0700
Message-ID: <CABPQxsuUGNeBO=dHt6b2YoeFqZ5AJDWkUPEziUKStWwjWCCrww@mail.gmail.com>
Subject: Re: enum-like types in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: Imran Rashid <irashid@cloudera.com>, Xiangrui Meng <mengxr@gmail.com>, 
	Aaron Davidson <ilikerps@gmail.com>, Kevin Markey <kevin.markey@oracle.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

If the official solution from the Scala community is to use Java
enums, then it seems strange they aren't generated in scaldoc? Maybe
we can just fix that w/ Typesafe's help and then we can use them.

On Mon, Mar 23, 2015 at 1:46 PM, Sean Owen <sowen@cloudera.com> wrote:
> Yeah the fully realized #4, which gets back the ability to use it in
> switch statements (? in Scala but not Java?) does end up being kind of
> huge.
>
> I confess I'm swayed a bit back to Java enums, seeing what it
> involves. The hashCode() issue can be 'solved' with the hash of the
> String representation.
>
> On Mon, Mar 23, 2015 at 8:33 PM, Imran Rashid <irashid@cloudera.com> wrote:
>> I've just switched some of my code over to the new format, and I just want
>> to make sure everyone realizes what we are getting into.  I went from 10
>> lines as java enums
>>
>> https://github.com/squito/spark/blob/fef66058612ebf225e58dd5f5fea6bae1afd5b31/core/src/main/java/org/apache/spark/status/api/StageStatus.java#L20
>>
>> to 30 lines with the new format:
>>
>> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L250
>>
>> its not just that its verbose.  each name has to be repeated 4 times, with
>> potential typos in some locations that won't be caught by the compiler.
>> Also, you have to manually maintain the "values" as you update the set of
>> enums, the compiler won't do it for you.
>>
>> The only downside I've heard for java enums is enum.hashcode().  OTOH, the
>> downsides for this version are: maintainability / verbosity, no values(),
>> more cumbersome to use from java, no enum map / enumset.
>>
>> I did put together a little util to at least get back the equivalent of
>> enum.valueOf() with this format
>>
>> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/util/SparkEnum.scala
>>
>> I'm not trying to prevent us from moving forward on this, its fine if this
>> is still what everyone wants, but I feel pretty strongly java enums make
>> more sense.
>>
>> thanks,
>> Imran
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:14:48 2015
Return-Path: <dev-return-12117-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DDBF01781F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:14:48 +0000 (UTC)
Received: (qmail 73771 invoked by uid 500); 23 Mar 2015 21:14:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73690 invoked by uid 500); 23 Mar 2015 21:14:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73679 invoked by uid 99); 23 Mar 2015 21:14:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:14:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:14:43 +0000
Received: by qgf74 with SMTP id 74so39957398qgf.2
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:14:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ufy4afrSxHakN2DhDh/MJ7mMlSXa5l8lWTN89isfS5o=;
        b=JE47L0GhcZTdAChqdYM5SFuoAXkhg4B8shzslDTe66wskqXaXzq67vnEcGEHSuIG53
         2iJOtOV0h8aD5J58CIJhq94HWfpEMyWyArAlADk7lHHQ/WJPsFml6UXfmCyLsigGtLwV
         rZc043Mm6Gf7KB9jRu7yktoqrLoCHfhJ544Kw8IQzPmROEjuQSs7ck8EiEiz5mB+BVd2
         lVBmFt+scSn5oJZKd0xJ7/6haX5KYSB6hdXiqPDErZnmufC3NzfMqT+hLnNlH9bFrznj
         Rb66YR+4lvvcw5CexRyl454Egj7aZDUBgMUT/plz+zVIJOcmP0Xwd4TZ7Mm4ZPjlwg9L
         iL8w==
X-Gm-Message-State: ALoCoQntXAWDjaMGglBTyIb1l33M7FhpkmUiLQQHlqH2rQVCCx+rkZWYrJfS0CnxrnEszdwCeLaX
X-Received: by 10.55.26.104 with SMTP id a101mr2299909qka.81.1427145241876;
 Mon, 23 Mar 2015 14:14:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Mon, 23 Mar 2015 14:13:41 -0700 (PDT)
In-Reply-To: <CABPQxsuUGNeBO=dHt6b2YoeFqZ5AJDWkUPEziUKStWwjWCCrww@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
 <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
 <55072B1A.7000907@oracle.com> <CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
 <CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com>
 <CA+3qhFTx0_YTrNChkj0nJSH+SOs7N=ZOkBjiLzbhg1ceUUMpBQ@mail.gmail.com>
 <CAMAsSd+K4igb059ZX9uC0mmOYpaHWwTCY9fbUV8A2Zv5sysq3w@mail.gmail.com> <CABPQxsuUGNeBO=dHt6b2YoeFqZ5AJDWkUPEziUKStWwjWCCrww@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 23 Mar 2015 14:13:41 -0700
Message-ID: <CAPh_B=YnG-ROWTRcOFOq+0+vrOBrbJYE+oyvhY82AkOEtAF1Ww@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Patrick Wendell <pwendell@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Imran Rashid <irashid@cloudera.com>, 
	Xiangrui Meng <mengxr@gmail.com>, Aaron Davidson <ilikerps@gmail.com>, 
	Kevin Markey <kevin.markey@oracle.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a114592a62f73430511fb25f9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114592a62f73430511fb25f9
Content-Type: text/plain; charset=UTF-8

If scaladoc can show the Java enum types, I do think the best way is then
just Java enum types.


On Mon, Mar 23, 2015 at 2:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> If the official solution from the Scala community is to use Java
> enums, then it seems strange they aren't generated in scaldoc? Maybe
> we can just fix that w/ Typesafe's help and then we can use them.
>
> On Mon, Mar 23, 2015 at 1:46 PM, Sean Owen <sowen@cloudera.com> wrote:
> > Yeah the fully realized #4, which gets back the ability to use it in
> > switch statements (? in Scala but not Java?) does end up being kind of
> > huge.
> >
> > I confess I'm swayed a bit back to Java enums, seeing what it
> > involves. The hashCode() issue can be 'solved' with the hash of the
> > String representation.
> >
> > On Mon, Mar 23, 2015 at 8:33 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
> >> I've just switched some of my code over to the new format, and I just
> want
> >> to make sure everyone realizes what we are getting into.  I went from 10
> >> lines as java enums
> >>
> >>
> https://github.com/squito/spark/blob/fef66058612ebf225e58dd5f5fea6bae1afd5b31/core/src/main/java/org/apache/spark/status/api/StageStatus.java#L20
> >>
> >> to 30 lines with the new format:
> >>
> >>
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L250
> >>
> >> its not just that its verbose.  each name has to be repeated 4 times,
> with
> >> potential typos in some locations that won't be caught by the compiler.
> >> Also, you have to manually maintain the "values" as you update the set
> of
> >> enums, the compiler won't do it for you.
> >>
> >> The only downside I've heard for java enums is enum.hashcode().  OTOH,
> the
> >> downsides for this version are: maintainability / verbosity, no
> values(),
> >> more cumbersome to use from java, no enum map / enumset.
> >>
> >> I did put together a little util to at least get back the equivalent of
> >> enum.valueOf() with this format
> >>
> >>
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/util/SparkEnum.scala
> >>
> >> I'm not trying to prevent us from moving forward on this, its fine if
> this
> >> is still what everyone wants, but I feel pretty strongly java enums make
> >> more sense.
> >>
> >> thanks,
> >> Imran
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a114592a62f73430511fb25f9--

From dev-return-12118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:25:58 2015
Return-Path: <dev-return-12118-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DBBAC178A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:25:58 +0000 (UTC)
Received: (qmail 17694 invoked by uid 500); 23 Mar 2015 21:25:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17618 invoked by uid 500); 23 Mar 2015 21:25:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17607 invoked by uid 99); 23 Mar 2015 21:25:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:25:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:25:52 +0000
Received: by oigv203 with SMTP id v203so152135997oig.3
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:25:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=L5fKg0uRj3r0H0gu8xBX+p0kQs1BImKa9FLlQlJeZWU=;
        b=WbqOkJhITt9kcNVcT0rs+aMqTxTAxYY1BLFdHK3r3GH9n+0TBupmTQFbD1UsgWdP0z
         0pm9nYOV4mrqAEgALZPM6e5qqmDnLiTW92+y6/riI+fGzjvuuK3MLT+016uONfZPx7lE
         d18Rz7RjW8iZLfFKY12Iiafujp5s2dfniw20ZIhPK0GYz28JaxREDv+G3ulCM1XZi5jK
         Lxnnz2sQS0zgzpz9ht7wiEvlRJBUP9l6QjVxRZz0AeBTF4HcJOGqP9+y+Y5kexTGy2ih
         tKi6q69QKbABCqgb29/y2WFpa1QkPqfbC1Np6OLSV6T0VfltVyaDk8KzJ8LydBqb5ROm
         gBrQ==
X-Gm-Message-State: ALoCoQkcsxqIc7RFrHzQjILpeGq0eo0tSczK2J+kUAcqFI1l4JV2IT0qVSQ5zWrM+iviyE2XRNcv
MIME-Version: 1.0
X-Received: by 10.60.132.33 with SMTP id or1mr922747oeb.82.1427145911902; Mon,
 23 Mar 2015 14:25:11 -0700 (PDT)
Received: by 10.76.34.101 with HTTP; Mon, 23 Mar 2015 14:25:11 -0700 (PDT)
Date: Mon, 23 Mar 2015 14:25:11 -0700
Message-ID: <CANe15CHkyru0YDjLYgFfBpF8APo01-LFEhrf=WR6E6UO-pLDSA@mail.gmail.com>
Subject: Shuffle Spill Memory and Shuffle Spill Disk
From: Bijay Pathak <bijay.pathak@cloudwick.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b41ce5a1f391c0511fb4d13
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b41ce5a1f391c0511fb4d13
Content-Type: text/plain; charset=UTF-8

Hello,

I am running  TeraSort <https://github.com/ehiggs/spark-terasort> on 100GB
of data. The final metrics I am getting on Shuffle Spill are:

Shuffle Spill(Memory): 122.5 GB
Shuffle Spill(Disk): 3.4 GB

What's the difference and relation between these two metrics? Does these
mean 122.5 GB was spill from memory during the shuffle?

thank you,
bijay

--047d7b41ce5a1f391c0511fb4d13--

From dev-return-12119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:50:54 2015
Return-Path: <dev-return-12119-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B515E1796F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:50:54 +0000 (UTC)
Received: (qmail 75578 invoked by uid 500); 23 Mar 2015 21:50:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75467 invoked by uid 500); 23 Mar 2015 21:50:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75444 invoked by uid 99); 23 Mar 2015 21:50:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:50:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of irashid@cloudera.com designates 74.125.82.54 as permitted sender)
Received: from [74.125.82.54] (HELO mail-wg0-f54.google.com) (74.125.82.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:50:28 +0000
Received: by wgs2 with SMTP id 2so49812805wgs.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:50:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=JM2bGmktdQU2uo3kBbHt4DHQSmDQyNyNyv09yUVFVKA=;
        b=VyP2Il7yNrXzDycFfDjmrajNnxKsgtKou0goaLSfB94NeHq2yjXBYvBL4QDkBp9T5R
         aybTeKLEW9qYqSA0/Zewo30a0AIUsnNW53OmdhIeAd5URz54LPViXexPLYQQuTbRTY3b
         EWNmoNl4KeOnC6tbKBpwAym6Se4oBfCTLC/l7mi7u+iwFI1HUXBuIB1jQ/6ZEQhqpPUa
         yq9Y5TYKf/IyFb24Ay04s8Kd+fgDG8kMdcN9y8oOYVJDs39soX5nizEdB2D4QkuIQiz4
         x5+tzeTqTut4FWS+1YqQfphBCRdk2gXF7dju/2ObDSHPRs3shF+FWVfSI1EzzJus61YE
         AWvw==
X-Gm-Message-State: ALoCoQnJ79hg5V01By7Tk815a+tHd44aMtgwdGc3gKnIEzcDKN0O5GZxPEk8lj1D2PUU9Q/a3l2c
X-Received: by 10.194.208.229 with SMTP id mh5mr2159331wjc.108.1427147426731;
 Mon, 23 Mar 2015 14:50:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.62.197 with HTTP; Mon, 23 Mar 2015 14:50:06 -0700 (PDT)
In-Reply-To: <CABPQxsuUGNeBO=dHt6b2YoeFqZ5AJDWkUPEziUKStWwjWCCrww@mail.gmail.com>
References: <CAJgQjQ_Bd43aN9SiGPU8B9Rjw4CReOAjmu-Vyn0Fs8mV+pr79A@mail.gmail.com>
 <CACkSZy0uMLFrRjQmNK0NjDb6y5x+iTYhFOHLD0Ve8tWciSWgLg@mail.gmail.com>
 <CAF7ADNp6zCVYsq6TZu-9Fset-2FG03cjmbJ1+8GSe-O3XjqrAA@mail.gmail.com>
 <CAAswR-6bPY0V_=enSZpD5BiSAWEGScgaQZgwR07_PGoQgKRRNA@mail.gmail.com>
 <CANGvG8oakV37k4-TT++Ps6tAbzB1JfQ=o7eB5-EVt8ksUXtLjw@mail.gmail.com>
 <CABPQxsv5MOBzyFX_CC4Fdv+vErg=43LkEQB844LUR-TprVX=sA@mail.gmail.com>
 <CAJgQjQ-9-qaYgLKcX+5tq1zDXYkvtEc7xSwhh97Ag++_+iRbEw@mail.gmail.com>
 <CA+3qhFTVFcN3MfgAfF67ootRO7XMfQWgy6UiMKPnLtyWgKS+Cg@mail.gmail.com>
 <CAJiQeYLb+_f8ew865bQVSrFx071dOydo_Xv+f0wxsYzynTfThg@mail.gmail.com>
 <55072B1A.7000907@oracle.com> <CANGvG8rx7BE2kVYhjU9Rc+R88S8SAGRdE3AibzxBJ2U1m7MS6g@mail.gmail.com>
 <CAJgQjQ8nLN4Mi3NckQ4dX7+ELbPEDMGc0AFVEyhFDJEpDvtO_g@mail.gmail.com>
 <CA+3qhFTx0_YTrNChkj0nJSH+SOs7N=ZOkBjiLzbhg1ceUUMpBQ@mail.gmail.com>
 <CAMAsSd+K4igb059ZX9uC0mmOYpaHWwTCY9fbUV8A2Zv5sysq3w@mail.gmail.com> <CABPQxsuUGNeBO=dHt6b2YoeFqZ5AJDWkUPEziUKStWwjWCCrww@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Mon, 23 Mar 2015 16:50:06 -0500
Message-ID: <CA+3qhFSj8NRPtw+mEsxdGyK=Po-U2Ju0Z8HLpN5z5VJQrc15eg@mail.gmail.com>
Subject: Re: enum-like types in Spark
To: Patrick Wendell <pwendell@gmail.com>
Cc: Sean Owen <sowen@cloudera.com>, Xiangrui Meng <mengxr@gmail.com>, 
	Aaron Davidson <ilikerps@gmail.com>, Kevin Markey <kevin.markey@oracle.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11338f6c69c3f10511fba7dc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11338f6c69c3f10511fba7dc
Content-Type: text/plain; charset=UTF-8

well, perhaps I overstated things a little, I wouldn't call it the
"official" solution, just a recommendation in the never-ending debate (and
the recommendation from folks with their hands on scala itself).

Even if we do get this fixed in scaladoc eventually -- as its not in the
current versions, where does that leave this proposal?  personally I'd
*still* prefer java enums, even if it doesn't get into scaladoc.  btw, even
with sealed traits, the scaladoc still isn't great -- you don't see the
values from the class, you only see them listed from the companion object.
 (though, that is somewhat standard for scaladoc, so maybe I'm reaching a
little)



On Mon, Mar 23, 2015 at 4:11 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> If the official solution from the Scala community is to use Java
> enums, then it seems strange they aren't generated in scaldoc? Maybe
> we can just fix that w/ Typesafe's help and then we can use them.
>
> On Mon, Mar 23, 2015 at 1:46 PM, Sean Owen <sowen@cloudera.com> wrote:
> > Yeah the fully realized #4, which gets back the ability to use it in
> > switch statements (? in Scala but not Java?) does end up being kind of
> > huge.
> >
> > I confess I'm swayed a bit back to Java enums, seeing what it
> > involves. The hashCode() issue can be 'solved' with the hash of the
> > String representation.
> >
> > On Mon, Mar 23, 2015 at 8:33 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
> >> I've just switched some of my code over to the new format, and I just
> want
> >> to make sure everyone realizes what we are getting into.  I went from 10
> >> lines as java enums
> >>
> >>
> https://github.com/squito/spark/blob/fef66058612ebf225e58dd5f5fea6bae1afd5b31/core/src/main/java/org/apache/spark/status/api/StageStatus.java#L20
> >>
> >> to 30 lines with the new format:
> >>
> >>
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/status/api/v1/api.scala#L250
> >>
> >> its not just that its verbose.  each name has to be repeated 4 times,
> with
> >> potential typos in some locations that won't be caught by the compiler.
> >> Also, you have to manually maintain the "values" as you update the set
> of
> >> enums, the compiler won't do it for you.
> >>
> >> The only downside I've heard for java enums is enum.hashcode().  OTOH,
> the
> >> downsides for this version are: maintainability / verbosity, no
> values(),
> >> more cumbersome to use from java, no enum map / enumset.
> >>
> >> I did put together a little util to at least get back the equivalent of
> >> enum.valueOf() with this format
> >>
> >>
> https://github.com/squito/spark/blob/SPARK-3454_w_jersey/core/src/main/scala/org/apache/spark/util/SparkEnum.scala
> >>
> >> I'm not trying to prevent us from moving forward on this, its fine if
> this
> >> is still what everyone wants, but I feel pretty strongly java enums make
> >> more sense.
> >>
> >> thanks,
> >> Imran
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>

--001a11338f6c69c3f10511fba7dc--

From dev-return-12120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 21:53:18 2015
Return-Path: <dev-return-12120-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD1D717988
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 21:53:18 +0000 (UTC)
Received: (qmail 84862 invoked by uid 500); 23 Mar 2015 21:53:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84787 invoked by uid 500); 23 Mar 2015 21:53:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84776 invoked by uid 99); 23 Mar 2015 21:53:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:53:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 21:52:53 +0000
Received: by obbgg8 with SMTP id gg8so133784854obb.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 14:52:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=oNd5oqh13QwaZnvrnsN1UpEeUNmfvFHlALIQjOSQk7A=;
        b=PcG3jPgk197B3AeyOvwC1E0BP8TgYKpmHoWC1TUVuVuT1nJut6PVWC93NZGG00K0Gn
         aBp7X8oJF9QPJf8kCEsqz7Ib8woZwzX0iOusky/+/kfEs86IPNHUFM0mIWQJ7Y/HSZXK
         3mczyMn86QHrl81yoO1MjvnI2R1Y65GgljD7wl/AhhHqWd0X4rp7iHcEdAcgPKf3huJG
         SPcm4JJy1230Pnn23oQ2XehZCVPYSPbYWG5/hzr0DoRYG2Pgk1ninBHcuIj2tgU+Q/qw
         8YgPi12pJfFviv29etYs+1085ZidbyDVxrnyBxH7MJhhQM1OPyKUU8hI+nVz5bMEGamZ
         pZeQ==
X-Gm-Message-State: ALoCoQmJott4S1aGpwZcdXhPk7IY54TMC+W5ZLTtb+j9GAO/cIbqSQsM0klyREUnstRNpSXcPokf
MIME-Version: 1.0
X-Received: by 10.202.190.135 with SMTP id o129mr889659oif.106.1427147551027;
 Mon, 23 Mar 2015 14:52:31 -0700 (PDT)
Received: by 10.76.34.101 with HTTP; Mon, 23 Mar 2015 14:52:30 -0700 (PDT)
In-Reply-To: <CANe15CHkyru0YDjLYgFfBpF8APo01-LFEhrf=WR6E6UO-pLDSA@mail.gmail.com>
References: <CANe15CHkyru0YDjLYgFfBpF8APo01-LFEhrf=WR6E6UO-pLDSA@mail.gmail.com>
Date: Mon, 23 Mar 2015 14:52:30 -0700
Message-ID: <CANe15CH2B=_OqvSwJ68MLMO-o_-VRP8MEovGU1cKWggKupgBGA@mail.gmail.com>
Subject: Re: Shuffle Spill Memory and Shuffle Spill Disk
From: Bijay Pathak <bijay.pathak@cloudwick.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113dc952d24fe70511fbaec7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113dc952d24fe70511fbaec7
Content-Type: text/plain; charset=UTF-8

It looks this is not the right place for this question, I have send the
question to user group.

thank you,
bijay

On Mon, Mar 23, 2015 at 2:25 PM, Bijay Pathak <bijay.pathak@cloudwick.com>
wrote:

> Hello,
>
> I am running  TeraSort <https://github.com/ehiggs/spark-terasort> on
> 100GB of data. The final metrics I am getting on Shuffle Spill are:
>
> Shuffle Spill(Memory): 122.5 GB
> Shuffle Spill(Disk): 3.4 GB
>
> What's the difference and relation between these two metrics? Does these
> mean 122.5 GB was spill from memory during the shuffle?
>
> thank you,
> bijay
>

--001a113dc952d24fe70511fbaec7--

From dev-return-12121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 22:48:21 2015
Return-Path: <dev-return-12121-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D029117CE4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 22:48:21 +0000 (UTC)
Received: (qmail 13604 invoked by uid 500); 23 Mar 2015 22:48:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13528 invoked by uid 500); 23 Mar 2015 22:48:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13517 invoked by uid 99); 23 Mar 2015 22:48:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 22:48:20 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zzhang@hortonworks.com designates 64.78.56.46 as permitted sender)
Received: from [64.78.56.46] (HELO relayvx11a.securemail.intermedia.net) (64.78.56.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 22:48:13 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id 8A15053E40
	for <dev@spark.apache.org>; Mon, 23 Mar 2015 15:47:53 -0700 (PDT)
Subject: Review request for SPARK-6112:Provide OffHeap support through HDFS
 RAM_DISK
MIME-Version: 1.0
x-echoworx-emg-received: Mon, 23 Mar 2015 15:47:53.549 -0700
x-echoworx-msg-id: f6604da5-3597-4ee4-97f3-cfaa5401cb27
x-echoworx-action: delivered
Received: from emg-ca-1-1.securemail.intermedia.net ([10.254.155.11])
          by emg-ca-1-1 (JAMES SMTP Server 2.3.2) with SMTP ID 533
          for <dev@spark.apache.org>;
          Mon, 23 Mar 2015 15:47:53 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (unknown [10.224.117.101])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id 5DA2A53E40
	for <dev@spark.apache.org>; Mon, 23 Mar 2015 15:47:53 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Mon, 23 Mar 2015 15:47:51 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Mon, 23 Mar 2015 15:47:51 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: dev <dev@spark.apache.org>
Thread-Topic: Review request for SPARK-6112:Provide OffHeap support through
 HDFS RAM_DISK
Thread-Index: AQHQZbtkqIpzTXcUb0qqljSB+5bTvA==
Date: Mon, 23 Mar 2015 22:47:51 +0000
Message-ID: <26219145-4637-4E9F-B2D3-10D874B9D046@hortonworks.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [192.175.27.17]
x-source-routing-agent: Processed
Content-Type: multipart/alternative;
	boundary="_000_2621914546374E9FB2D310D874B9D046hortonworkscom_"
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_2621914546374E9FB2D310D874B9D046hortonworkscom_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi Folks,

I am planning to implement hdfs off heap support for spark, and have upload=
ed the design doc for the off heap support through hdfs ramdisk in jira SPA=
RK-6112. Please review it and provide your feedback if anybody are interest=
ed.

https://issues.apache.org/jira/browse/SPARK-6112

Thanks.

Zhan Zhang

--_000_2621914546374E9FB2D310D874B9D046hortonworkscom_--

From dev-return-12122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 22:52:21 2015
Return-Path: <dev-return-12122-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 97AC817D06
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 22:52:21 +0000 (UTC)
Received: (qmail 19696 invoked by uid 500); 23 Mar 2015 22:52:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19558 invoked by uid 500); 23 Mar 2015 22:52:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18842 invoked by uid 99); 23 Mar 2015 22:52:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 22:52:18 +0000
X-ASF-Spam-Status: No, hits=3.2 required=5.0
	tests=FSL_HELO_BARE_IP_2,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zzhang@hortonworks.com designates 64.78.52.187 as permitted sender)
Received: from [64.78.52.187] (HELO relayvx12c.securemail.intermedia.net) (64.78.52.187)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 22:51:51 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id 8F5A953DFF;
	Mon, 23 Mar 2015 15:51:07 -0700 (PDT)
Subject: Re: Spark-thriftserver Issue
MIME-Version: 1.0
x-echoworx-emg-received: Mon, 23 Mar 2015 15:51:07.566 -0700
x-echoworx-msg-id: e70f1314-fe57-48b0-af83-6518a277d6da
x-echoworx-action: delivered
Received: from 10.254.155.17 ([10.254.155.17])
          by emg-ca-1-2 (JAMES SMTP Server 2.3.2) with SMTP ID 549;
          Mon, 23 Mar 2015 15:51:07 -0700 (PDT)
Received: from MBX080-W4-CO-2.exch080.serverpod.net (unknown [10.224.117.102])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id 5A82C53E48;
	Mon, 23 Mar 2015 15:51:07 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-2.exch080.serverpod.net (10.224.117.102) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Mon, 23 Mar 2015 15:51:06 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Mon, 23 Mar 2015 15:51:05 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: Neil Dev <neilkdev@gmail.com>
CC: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Thread-Topic: Spark-thriftserver Issue
Thread-Index: AQHQZbvYAA+wv4K+Q0ml1HcAYZi7xw==
Date: Mon, 23 Mar 2015 22:51:05 +0000
Message-ID: <6F61F95B-9B00-4BE4-99E8-902E420B838C@hortonworks.com>
References: <CADfwMNLmtg6mKEyeRMCc9uCqg5+ECd7aA5mNux4ph2uq-14heA@mail.gmail.com>
In-Reply-To: <CADfwMNLmtg6mKEyeRMCc9uCqg5+ECd7aA5mNux4ph2uq-14heA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [192.175.27.17]
x-source-routing-agent: Processed
Content-Type: multipart/alternative;
	boundary="_000_6F61F95B9B004BE499E8902E420B838Chortonworkscom_"
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_6F61F95B9B004BE499E8902E420B838Chortonworkscom_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Probably the port is already used by others, e.g., hive. You can change the=
 port similar to below


 ./sbin/start-thriftserver.sh --master yarn --executor-memory 512m --hiveco=
nf hive.server2.thrift.port=3D10001

Thanks.

Zhan Zhang

On Mar 23, 2015, at 12:01 PM, Neil Dev <neilkdev@gmail.com<mailto:neilkdev@=
gmail.com>> wrote:

Hi,

I am having issue starting spark-thriftserver. I'm running spark 1.3.with
Hadoop 2.4.0. I would like to be able to change its port too so, I can hive
hive-thriftserver as well as spark-thriftserver running at the same time.

Starting sparkthrift server:-
sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
--executor-memory 2G

Error:-
I created the folder manually but still getting the following error----
Exception in thread "main" java.lang.IllegalArgumentException: Log
directory /tmp/spark-events does not exist.


I am getting the following error
15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
org.apache.thrift.transport.TTransportException: Could not create
ServerSocket on address0.0.0.0/0.0.0.0:10000.
       at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
       at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
       at
org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactor=
y.java:236)
       at
org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryC=
LIService.java:69)
       at java.lang.Thread.run(Thread.java:745)

Thanks
Neil


--_000_6F61F95B9B004BE499E8902E420B838Chortonworkscom_--

From dev-return-12123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 23:04:30 2015
Return-Path: <dev-return-12123-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0DC2417D52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 23:04:30 +0000 (UTC)
Received: (qmail 43517 invoked by uid 500); 23 Mar 2015 23:04:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43432 invoked by uid 500); 23 Mar 2015 23:04:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43419 invoked by uid 99); 23 Mar 2015 23:04:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 23:04:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 23:03:51 +0000
Received: by qgfa8 with SMTP id a8so170300458qgf.0
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 16:03:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=9sTKjaqDArgugYzsTnoqFOoNYwtK80vJZPTzdNefXW4=;
        b=aQSPF0SxCfJdDLnW2/hSqg7Pos/qTh9Q/j6dYM2M7c/htRWsthawoio4dGCPgxxVG3
         dNQtueQFScJS4f17flHlB8pT9r6XObkur7RCReH/97ehGOAedldefDm34dVI1KtWa/9L
         OppDvEacofc/rQAh+nBVDvpMwidEGnSVS6Ii+yZ2Fp320g8eZhUm0F5iZeVqI6YDSUjb
         eFj3VyZBfVTMAXC6vq2E4UwJjQUfEC6bqJIc8w3QAu8tIiwjskL8g2uUqceaLSdv5Dn3
         507mDY7e2RzTqNlAG61HvZLfiveW5b4v+raX1eyxXfOSvmNEbhJxDkyyOr9fpsCGissu
         XHIw==
X-Gm-Message-State: ALoCoQmB2bOCoSBYMzJ1hI+t46v9iSg1Ow4J1lmq6boSWZrCmBoyWsG5nqs1swqphpvmyFfiqatf
X-Received: by 10.55.15.104 with SMTP id z101mr3325642qkg.19.1427151808842;
 Mon, 23 Mar 2015 16:03:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Mon, 23 Mar 2015 16:03:08 -0700 (PDT)
In-Reply-To: <26219145-4637-4E9F-B2D3-10D874B9D046@hortonworks.com>
References: <26219145-4637-4E9F-B2D3-10D874B9D046@hortonworks.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 23 Mar 2015 16:03:08 -0700
Message-ID: <CAPh_B=afz7NXyhzUUXbuS56o_o9yzLmW3zgAkNOXtN9bBXzPfA@mail.gmail.com>
Subject: Re: Review request for SPARK-6112:Provide OffHeap support through
 HDFS RAM_DISK
To: Zhan Zhang <zzhang@hortonworks.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a114750129b60ea0511fcac22
X-Virus-Checked: Checked by ClamAV on apache.org

--001a114750129b60ea0511fcac22
Content-Type: text/plain; charset=UTF-8

I created a ticket to separate the API refactoring from the implementation.
Would be great to have these as two separate patches to make it easier to
review (similar to the way we are doing RPC refactoring -- first
introducing an internal RPC api, port akka to it, and then add an
alternative implementation).

https://issues.apache.org/jira/browse/SPARK-6479

Can you upload your design doc there so we can discuss the block store api?
Thanks.


On Mon, Mar 23, 2015 at 3:47 PM, Zhan Zhang <zzhang@hortonworks.com> wrote:

> Hi Folks,
>
> I am planning to implement hdfs off heap support for spark, and have
> uploaded the design doc for the off heap support through hdfs ramdisk in
> jira SPARK-6112. Please review it and provide your feedback if anybody are
> interested.
>
> https://issues.apache.org/jira/browse/SPARK-6112
>
> Thanks.
>
> Zhan Zhang
>

--001a114750129b60ea0511fcac22--

From dev-return-12124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 23 23:14:58 2015
Return-Path: <dev-return-12124-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4B6917D95
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 23 Mar 2015 23:14:58 +0000 (UTC)
Received: (qmail 66626 invoked by uid 500); 23 Mar 2015 23:14:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66540 invoked by uid 500); 23 Mar 2015 23:14:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66529 invoked by uid 99); 23 Mar 2015 23:14:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 23:14:47 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zzhang@hortonworks.com designates 64.78.56.46 as permitted sender)
Received: from [64.78.56.46] (HELO relayvx11a.securemail.intermedia.net) (64.78.56.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 23 Mar 2015 23:14:43 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id B841B53E15;
	Mon, 23 Mar 2015 16:14:22 -0700 (PDT)
Subject: Re: Review request for SPARK-6112:Provide OffHeap support through
 HDFS RAM_DISK
MIME-Version: 1.0
x-echoworx-emg-received: Mon, 23 Mar 2015 16:14:22.732 -0700
x-echoworx-msg-id: 798fe1e2-c1d7-4244-9f26-53f1a1e73c79
x-echoworx-action: delivered
Received: from emg-ca-1-1.securemail.intermedia.net ([10.254.155.11])
          by emg-ca-1-1 (JAMES SMTP Server 2.3.2) with SMTP ID 479;
          Mon, 23 Mar 2015 16:14:22 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (unknown [10.224.117.101])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id 7FDAC53E15;
	Mon, 23 Mar 2015 16:14:22 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Mon, 23 Mar 2015 16:14:21 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Mon, 23 Mar 2015 16:14:21 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: Reynold Xin <rxin@databricks.com>
CC: dev <dev@spark.apache.org>
Thread-Topic: Review request for SPARK-6112:Provide OffHeap support through
 HDFS RAM_DISK
Thread-Index: AQHQZbtkqIpzTXcUb0qqljSB+5bTvA==
Date: Mon, 23 Mar 2015 23:14:20 +0000
Message-ID: <000D8C1F-F5B7-4C25-97B7-E2FC1072F21D@hortonworks.com>
References: <26219145-4637-4E9F-B2D3-10D874B9D046@hortonworks.com>
 <CAPh_B=afz7NXyhzUUXbuS56o_o9yzLmW3zgAkNOXtN9bBXzPfA@mail.gmail.com>
In-Reply-To: <CAPh_B=afz7NXyhzUUXbuS56o_o9yzLmW3zgAkNOXtN9bBXzPfA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [192.175.27.17]
x-source-routing-agent: Processed
Content-Type: multipart/alternative;
	boundary="_000_000D8C1FF5B74C2597B7E2FC1072F21Dhortonworkscom_"
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_000D8C1FF5B74C2597B7E2FC1072F21Dhortonworkscom_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Thanks Reynold,

Agree with you to open another JIRA to unify the block storage API.  I have=
 upload the design doc to SPARK-6479 as well.

Thanks.

Zhan Zhang

On Mar 23, 2015, at 4:03 PM, Reynold Xin <rxin@databricks.com<mailto:rxin@d=
atabricks.com>> wrote:

I created a ticket to separate the API refactoring from the implementation.=
 Would be great to have these as two separate patches to make it easier to =
review (similar to the way we are doing RPC refactoring -- first introducin=
g an internal RPC api, port akka to it, and then add an alternative impleme=
ntation).

https://issues.apache.org/jira/browse/SPARK-6479

Can you upload your design doc there so we can discuss the block store api?=
 Thanks.


On Mon, Mar 23, 2015 at 3:47 PM, Zhan Zhang <zzhang@hortonworks.com<mailto:=
zzhang@hortonworks.com>> wrote:
Hi Folks,

I am planning to implement hdfs off heap support for spark, and have upload=
ed the design doc for the off heap support through hdfs ramdisk in jira SPA=
RK-6112. Please review it and provide your feedback if anybody are interest=
ed.

https://issues.apache.org/jira/browse/SPARK-6112

Thanks.

Zhan Zhang



--_000_000D8C1FF5B74C2597B7E2FC1072F21Dhortonworkscom_--

From dev-return-12125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 00:43:15 2015
Return-Path: <dev-return-12125-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 97F84101B2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 00:43:15 +0000 (UTC)
Received: (qmail 4327 invoked by uid 500); 24 Mar 2015 00:43:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4244 invoked by uid 500); 24 Mar 2015 00:43:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4231 invoked by uid 99); 24 Mar 2015 00:43:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 00:43:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of denny.g.lee@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 00:43:09 +0000
Received: by igcau2 with SMTP id au2so42806997igc.1
        for <dev@spark.apache.org>; Mon, 23 Mar 2015 17:41:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc:content-type;
        bh=6d+SfYXaMQfYWGnSKUo1f1MS0K3UKs4vv2PypSwec0c=;
        b=zr5GjlXjEn/XizO8+tyJeg81fUThmzvAeGajv4ywD3wcLmxnkWsM1aYbJAfdFu+0s0
         rRwH1MtRSHG73Vw0gxIeIO6Oyc8wOFoDBUDjMggA4TWdW1Ld/BiBp+CVTCpXD+uyZXK9
         /PFVVNkE1ha9mbY61r2XYBNdLnf2p48asBaenklQDxAVf/zGT2GNXnd0LNwMRATKfMLQ
         /nHFZUWb8JO0mUFpjAaiLVqqbRrjkSPG1ZHIeLkKps48nWi1gDLlR+QubKgTIrO9nRhr
         tb6k7hIZ4pNgB6u2IZyfIg8AFmb9iNF4+pZw/dE/l8dPERCzELSrgpiYpWImKYh0thv3
         +/rg==
X-Received: by 10.42.101.83 with SMTP id d19mr11899938ico.50.1427157678967;
 Mon, 23 Mar 2015 17:41:18 -0700 (PDT)
MIME-Version: 1.0
References: <CADfwMNJ2+eicw2YJkw3cRo9NmBU=2771bNSsCj73rePMuMmN9g@mail.gmail.com>
 <CABjYQ3-H8v+FuLissgtUsfXZ_20-ygYaYvdQJ_Gkfj+o4-WSbw@mail.gmail.com>
 <CADfwMNJ7+jQ8Gyrxp4q8cUU4ZGwb_24iBpsKos5Jsx7YOenACQ@mail.gmail.com>
 <CABjYQ3-wn1vrUAYW1A6BznGnnsfLtfHTZ-abcCBYB7yq5AE_uA@mail.gmail.com> <CAFL=5a8s5odAhVdzGkzjXJ-q2dBFSv2_ECpmCcK8sbWuFkmmnA@mail.gmail.com>
In-Reply-To: <CAFL=5a8s5odAhVdzGkzjXJ-q2dBFSv2_ECpmCcK8sbWuFkmmnA@mail.gmail.com>
From: Denny Lee <denny.g.lee@gmail.com>
Date: Tue, 24 Mar 2015 00:41:18 +0000
Message-ID: <CABjYQ39i2o=tsDWAqv6Mo=gWyJsfhMvahKNy0Dha=-WaVWB3vw@mail.gmail.com>
Subject: Re: Starting sparkthrift server
To: Anubhav Agarwal <anubhav33@gmail.com>
Cc: Neil Dev <neilkdev@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf301b66897e509e0511fe0a24
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf301b66897e509e0511fe0a24
Content-Type: text/plain; charset=UTF-8

In that case, can you use the configurations to specify the folders?  I'm
wondering if this actually Hive in play here and somehow the
/tmp/spark-events is being specified for the logs for Hive?

On Mon, Mar 23, 2015 at 2:00 PM Anubhav Agarwal <anubhav33@gmail.com> wrote:

> When I start spark-shell (for example) it does not write to the
> /tmp/spark-events folder. It remains empty. I have even tried it after
> giving that folder rwx permission for user, group and others.
>
> Neil's colleague,
> Anu
>
> On Mon, Mar 23, 2015 at 4:50 PM, Denny Lee <denny.g.lee@gmail.com> wrote:
>
>> When you say the job has access, do you mean that when you run
>> spark-submit
>> or spark-shell (for example), it is able to write to the /tmp/spark-events
>> folder?
>>
>>
>> On Mon, Mar 23, 2015 at 1:02 PM Neil Dev <neilkdev@gmail.com> wrote:
>>
>> > we are running this right now as root user and the folder
>> /tmp/spark-events
>> > was manually created and the Job has access to this folder
>> >
>> > On Mon, Mar 23, 2015 at 3:38 PM, Denny Lee <denny.g.lee@gmail.com>
>> wrote:
>> >
>> >> It appears that you are running the thrift-server using the
>> spark-events
>> >> account but the /tmp/spark-events folder doesn't exist or the user
>> running
>> >> thrift-server does not have access to it.  Have you been able to run
>> Hive
>> >> using the spark-events user so that way the /tmp/spark-events folder
>> has
>> >> been created.  If you need to reassign the scratch dir / log dir to
>> another
>> >> folder (instead of /tmp/spark-events), you could use  --hiveconf to
>> assign
>> >> those to another folder.
>> >>
>> >>
>> >> On Mon, Mar 23, 2015 at 8:39 AM Neil Dev <neilkdev@gmail.com> wrote:
>> >>
>> >>> Hi,
>> >>>
>> >>> I am having issues starting spark-thriftserver. I'm running spark
>> 1.3.o
>> >>> with Hadoop 2.4.0. I would like to be able to change its port too so,
>> I
>> >>> can
>> >>> hive hive-thriftserver as well as spark-thriftserver running at the
>> same
>> >>> time.
>> >>>
>> >>> Starting sparkthrift server:-
>> >>> sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
>> >>> --executor-memory 2G
>> >>>
>> >>> Error:-
>> >>> I created the folder manually but still getting the following
>> error----
>> >>> Exception in thread "main" java.lang.IllegalArgumentException: Log
>> >>> directory /tmp/spark-events does not exist.
>> >>>
>> >>>
>> >>> I am getting the following error
>> >>> 15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
>> >>> org.apache.thrift.transport.TTransportException: Could not create
>> >>> ServerSocket on address0.0.0.0/0.0.0.0:10000.
>> >>>         at
>> >>>
>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
>> >>>         at
>> >>>
>> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
>> >>>         at
>> >>> org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(
>> >>> HiveAuthFactory.java:236)
>> >>>         at
>> >>> org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.
>> >>> run(ThriftBinaryCLIService.java:69)
>> >>>         at java.lang.Thread.run(Thread.java:745)
>> >>>
>> >>> Thanks
>> >>> Neil
>> >>>
>> >>
>> >
>>
>
>

--20cf301b66897e509e0511fe0a24--

From dev-return-12126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 08:10:48 2015
Return-Path: <dev-return-12126-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B511310F54
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 08:10:48 +0000 (UTC)
Received: (qmail 87045 invoked by uid 500); 24 Mar 2015 08:10:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86971 invoked by uid 500); 24 Mar 2015 08:10:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86957 invoked by uid 99); 24 Mar 2015 08:10:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 08:10:25 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wbin00@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 08:09:58 +0000
Received: by obcjt1 with SMTP id jt1so121410834obc.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 01:08:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=U5+FJ+LHWsNENkSjq7++iwJOr38H0S2PJyLY5Rvpwv8=;
        b=T3Yx5vFC8c+RkANitSZMUMSX+8IZUK2kBBt1rkn8m7zKlhh1wHRLNPK+34wU7pLslC
         5FyVTDSLu4SmpYghjR/se8e3gOAgWyKdeeYUbAomnK1xt68Z2jzRswzZ0IIt15joP/Nb
         CBywlJr7RvPw+imReV6OfSfeRjMmJg99cenel2vb1h6dwlWIgj1gIp/dsiV+ZMwcxCY0
         FRCIAUlcUwI71o/dxvY8Tmsw9BoIXSpwJV+eJVuWEIz4P6Wzvq1Mz5jjmeqRA8TJ87jQ
         Q1cHFb6I5zCrkHt7E8NdzD7xCNfY7RgjAbLfgzOu0Icrontsp+YFeQlKDHX9UXMtl1Wk
         ZNLg==
X-Received: by 10.60.35.102 with SMTP id g6mr2398613oej.7.1427184506932; Tue,
 24 Mar 2015 01:08:26 -0700 (PDT)
MIME-Version: 1.0
From: Bin Wang <wbin00@gmail.com>
Date: Tue, 24 Mar 2015 08:08:26 +0000
Message-ID: <CAD_32VXw3p1GqKOkAhDH=ghNRitnsKzZNG2w4P3f5nFvi5waHA@mail.gmail.com>
Subject: Optimize the first map reduce of DStream
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013c6f9290955605120449e9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013c6f9290955605120449e9
Content-Type: text/plain; charset=UTF-8

Hi,

I'm learning Spark and I find there could be some optimize for the current
streaming implementation. Correct me if I'm wrong.

The current streaming implementation put the data of one batch into memory
(as RDD). But it seems not necessary.

For example, if I want to count the lines which contains word "Spark", I
just need to map every line to see if it contains word, then reduce it with
a sum function. After that, this line is no longer useful to keep it in
memory.

That is said, if the DStream only have one map and/or reduce operation on
it. It is not necessary to keep all the batch data in the memory. Something
like a pipeline should be OK.

Is it difficult to implement on top of the current implementation?

Thanks.

---
Bin Wang

--089e013c6f9290955605120449e9--

From dev-return-12127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 10:56:00 2015
Return-Path: <dev-return-12127-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 933BD177F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 10:56:00 +0000 (UTC)
Received: (qmail 57238 invoked by uid 500); 24 Mar 2015 10:55:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57170 invoked by uid 500); 24 Mar 2015 10:55:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57159 invoked by uid 99); 24 Mar 2015 10:55:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 10:55:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_IMAGE_ONLY_28,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.213.176] (HELO mail-ig0-f176.google.com) (209.85.213.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 10:55:55 +0000
Received: by igbud6 with SMTP id ud6so68057688igb.1
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 03:55:14 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=1viokk/VYvNs8WzWvh2IkH5Egc62LJP24p9MiM6vCJs=;
        b=XB3zHC7S/kslGVQnjaYl7S+ORX/13mlbFTinI5m1yr6H+I1iV9an2TJESqGowI7snJ
         fJOkubQmbOUnrO2coc9oTAlneX4K2LT1Ej6/iuFjc9t6e7x5zxMqf4KlDtOhCqTD/i2T
         4jgw8NuAvLiRXCFExKJsrYhzypDPktXfuCTL7Y0fba/5hLxFZgLI1Z+iRPWMSzxONuFn
         HRfOtV2QMIBQuEE8XEni0JbdDmk/YMdmAuddbYzGhTb3p4eyoXDfRPlZ9HnJDQKEVHo3
         NZ7oZVLcBbYHhTCxtIFVW6QEHYGKomfDzsS+MWcR+gxknYa5zmZOkrUUV9mRf0fe/ZIn
         jBLQ==
X-Gm-Message-State: ALoCoQlz/ueamFmq8k22f8BNjVjfi6txI0iOnKq8osYv3/8XHKbj4nby8z6ckZ8KkhjlqxVuu1Xs
MIME-Version: 1.0
X-Received: by 10.50.56.114 with SMTP id z18mr13321058igp.8.1427194514244;
 Tue, 24 Mar 2015 03:55:14 -0700 (PDT)
Received: by 10.107.46.32 with HTTP; Tue, 24 Mar 2015 03:55:14 -0700 (PDT)
In-Reply-To: <CAD_32VXw3p1GqKOkAhDH=ghNRitnsKzZNG2w4P3f5nFvi5waHA@mail.gmail.com>
References: <CAD_32VXw3p1GqKOkAhDH=ghNRitnsKzZNG2w4P3f5nFvi5waHA@mail.gmail.com>
Date: Tue, 24 Mar 2015 16:25:14 +0530
Message-ID: <CABD4CG+E7nYQHzuTJAnHENgFCNrsMTL1TU3am85zyO=vG_Zygw@mail.gmail.com>
Subject: Re: Optimize the first map reduce of DStream
From: Arush Kharbanda <arush@sigmoidanalytics.com>
To: Bin Wang <wbin00@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0158b3060c39d20512069ece
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b3060c39d20512069ece
Content-Type: text/plain; charset=UTF-8

The block size is configurable and that way I think you can reduce the
block interval, to keep the block in memory only for the limiter interval?
Is that what you are looking for?

On Tue, Mar 24, 2015 at 1:38 PM, Bin Wang <wbin00@gmail.com> wrote:

> Hi,
>
> I'm learning Spark and I find there could be some optimize for the current
> streaming implementation. Correct me if I'm wrong.
>
> The current streaming implementation put the data of one batch into memory
> (as RDD). But it seems not necessary.
>
> For example, if I want to count the lines which contains word "Spark", I
> just need to map every line to see if it contains word, then reduce it with
> a sum function. After that, this line is no longer useful to keep it in
> memory.
>
> That is said, if the DStream only have one map and/or reduce operation on
> it. It is not necessary to keep all the batch data in the memory. Something
> like a pipeline should be OK.
>
> Is it difficult to implement on top of the current implementation?
>
> Thanks.
>
> ---
> Bin Wang
>



-- 

[image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>

*Arush Kharbanda* || Technical Teamlead

arush@sigmoidanalytics.com || www.sigmoidanalytics.com

--089e0158b3060c39d20512069ece--

From dev-return-12128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 11:06:25 2015
Return-Path: <dev-return-12128-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C510917856
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 11:06:25 +0000 (UTC)
Received: (qmail 83906 invoked by uid 500); 24 Mar 2015 11:06:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83811 invoked by uid 500); 24 Mar 2015 11:06:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83798 invoked by uid 99); 24 Mar 2015 11:06:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 11:06:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 11:05:54 +0000
Received: by obcxo2 with SMTP id xo2so144379527obc.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 04:04:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gapSuLIxlmAAv6tn2moKrib3tWWIGZUwMH1qqM0OXB8=;
        b=jSvbSz1M6gg9h/058IZBEl5UP5Co7cr5OaENVSQ8OEmhbGGEF/DsnXxDPz2xqLMmwI
         X27cQkOQtrnP1Mp+Sz4sNmAOyUpGT2aAmyCHy8WP/bIHqpEQ02QHOVlYI7lOvGLlIcaK
         QETCaCmousUK6HdcauYZci7er85AXX60fKqlRVSyyggOFVTcDMC8jDbvDpAfhx+F3zjO
         yvTzJBr/Bo6mSdYTUjLJedJy/uNkq/7IAk4mw5qRq1/LaiJa11RD9sNs8BPZbDqhDI+d
         KJiVw7e1NcCgD2434938h4E+z4YLuORh2IjpkT7qZ8ymqPXWP6uSsHmlU/ckhHPBUtv5
         uAqg==
MIME-Version: 1.0
X-Received: by 10.60.98.2 with SMTP id ee2mr2825009oeb.39.1427195062348; Tue,
 24 Mar 2015 04:04:22 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Tue, 24 Mar 2015 04:04:22 -0700 (PDT)
In-Reply-To: <CABD4CG+E7nYQHzuTJAnHENgFCNrsMTL1TU3am85zyO=vG_Zygw@mail.gmail.com>
References: <CAD_32VXw3p1GqKOkAhDH=ghNRitnsKzZNG2w4P3f5nFvi5waHA@mail.gmail.com>
	<CABD4CG+E7nYQHzuTJAnHENgFCNrsMTL1TU3am85zyO=vG_Zygw@mail.gmail.com>
Date: Tue, 24 Mar 2015 12:04:22 +0100
Message-ID: <CAO=evYc8aZBrmR3X89nkgvgE_pb253s8KKyO3UeZQtzHVCpWtA@mail.gmail.com>
Subject: Re: Optimize the first map reduce of DStream
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: Bin Wang <wbin00@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e011619c4b76f4b051206be53
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011619c4b76f4b051206be53
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

There is a BlockGenerator on each worker node next to the
ReceiverSupervisorImpl, which generates Blocks out of an ArrayBuffer in
each interval (block_interval). These Blocks are passed to
ReceiverSupervisorImpl, which throws these blocks to into the BlockManager
for storage. BlockInfos are passed to the driver. Mini-batches are created
by the JobGenerator component on the driver each batch_interval. I guess
what you are looking for is provided by a continuous model like Flink's. We
are creating mini-batches to provide fault tolerance.

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-03-24 11:55 GMT+01:00 Arush Kharbanda <arush@sigmoidanalytics.com>:

> The block size is configurable and that way I think you can reduce the
> block interval, to keep the block in memory only for the limiter interval=
?
> Is that what you are looking for?
>
> On Tue, Mar 24, 2015 at 1:38 PM, Bin Wang <wbin00@gmail.com> wrote:
>
> > Hi,
> >
> > I'm learning Spark and I find there could be some optimize for the
> current
> > streaming implementation. Correct me if I'm wrong.
> >
> > The current streaming implementation put the data of one batch into
> memory
> > (as RDD). But it seems not necessary.
> >
> > For example, if I want to count the lines which contains word "Spark", =
I
> > just need to map every line to see if it contains word, then reduce it
> with
> > a sum function. After that, this line is no longer useful to keep it in
> > memory.
> >
> > That is said, if the DStream only have one map and/or reduce operation =
on
> > it. It is not necessary to keep all the batch data in the memory.
> Something
> > like a pipeline should be OK.
> >
> > Is it difficult to implement on top of the current implementation?
> >
> > Thanks.
> >
> > ---
> > Bin Wang
> >
>
>
>
> --
>
> [image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>
>
> *Arush Kharbanda* || Technical Teamlead
>
> arush@sigmoidanalytics.com || www.sigmoidanalytics.com
>

--089e011619c4b76f4b051206be53--

From dev-return-12129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 13:14:46 2015
Return-Path: <dev-return-12129-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 288D9173D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 13:14:46 +0000 (UTC)
Received: (qmail 37453 invoked by uid 500); 24 Mar 2015 13:14:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37355 invoked by uid 500); 24 Mar 2015 13:14:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37344 invoked by uid 99); 24 Mar 2015 13:14:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 13:14:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 13:14:37 +0000
Received: by wibdy8 with SMTP id dy8so74560185wib.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 06:13:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=Av2fXkxtyb1wcexd7GReo3DWuPpukp3fP9XjqTv37Gw=;
        b=J6Qlj1UTeEnJb4GqTpH2B1az2+yskvkF5qt8c7x86Pfa8c6IQgYEbQdP+ey9xCKLe9
         flWdIsREW88eX8JV+azw2XT901G4z1+TN6EjoXvQA2g5HBcvz3Rk6joiaH+hhbqp33ar
         EYUQ7S9pFI1XDhaRxionAIWvT4d8D8kDQNZQBs3x5hR1KP2yoG38Ywrp/Kp0bVl3jng8
         qqifHhULJm8A8mGNqdQ/IfDFMTSXLcgxgLoX7rcaGsQOIIhIzxEpe44QKMfGgT+6dlNs
         TRKqU9drA7MYZ+uBicqwXuuJWdirnsH0fZ3xLJ43II8PKCsNVTgmJWBOS8T9a07dpr8V
         aJJw==
X-Gm-Message-State: ALoCoQlQaLTy1xqp5/fbbap+3a7FlT2/c8FNoPW4oH+jRWYBqdvQSKF7Mk9NDaOA10VBggPpyUSp
X-Received: by 10.194.59.112 with SMTP id y16mr8335618wjq.36.1427202811432;
 Tue, 24 Mar 2015 06:13:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Tue, 24 Mar 2015 06:13:11 -0700 (PDT)
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 24 Mar 2015 13:13:11 +0000
Message-ID: <CAMAsSdLGx8=JE+7O=03GmXaVt_5eL=sE8ZKjKtdtUDwUzRcg-A@mail.gmail.com>
Subject: Any guidance on when to back port and how far?
To: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

So far, my rule of thumb has been:

- Don't back-port new features or improvements in general, only bug fixes
- Don't back-port minor bug fixes
- Back-port bug fixes that seem important enough to not wait for the
next minor release
- Back-port site doc changes to the release most likely to go out
next, to make it a part of the next site publish

But, how far should back-ports go, in general? If the last minor
release was 1.N, then to branch 1.N surely. Farther back is a question
of expectation for support of past minor releases. Given the pace of
change and time available, I assume there's not much support for
continuing to use release 1.(N-1) and very little for 1.(N-2).

Concretely: does anyone expect a 1.1.2 release ever? a 1.2.2 release?
It'd be good to hear the received wisdom explicitly.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 14:04:22 2015
Return-Path: <dev-return-12130-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E3E1C175CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 14:04:21 +0000 (UTC)
Received: (qmail 81037 invoked by uid 500); 24 Mar 2015 14:04:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80959 invoked by uid 500); 24 Mar 2015 14:04:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80947 invoked by uid 99); 24 Mar 2015 14:04:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 14:04:19 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wbin00@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 14:03:55 +0000
Received: by oigv203 with SMTP id v203so168371682oig.3
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 07:03:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:in-reply-to:from:date:message-id:subject:to
         :cc:content-type;
        bh=qzFRqwsMB+1Ob21Ea8wqXXXQkTaFYRrNrUNY9ISCqL0=;
        b=ixoV1Q9GMYRH24F0EsX9da4wCgPGbPoBDDtiD3UYFuyLA8XbE3XujDi7AoYIxbCivP
         45QuokCpV46dOv9N9IwpSIvEC98SZ1swtvI5wJkqzuncWApKA5hBn4n0PJH0/0sPko/G
         1bUm5zCp5o9uAubRC2yPtacHu4SXylfRioG3I72uqxFQmHcmdfhkX4l3Ut9A1tbtuR9e
         tq6lSzwfxkYw/7hDJOiX5KCbYTh5ldWGZ583gF6SB9VxKufxneLy9URfNUbOpj7gdtSj
         PiEn01Y8EpH1YbvVJRrTEzfDp3zy7qT7jHf5NOA/Srm45yvY+w1mIDxNHoGGrCi+qN5N
         t8mQ==
X-Received: by 10.202.168.15 with SMTP id r15mr3325982oie.92.1427205833181;
 Tue, 24 Mar 2015 07:03:53 -0700 (PDT)
MIME-Version: 1.0
References: <CAD_32VXw3p1GqKOkAhDH=ghNRitnsKzZNG2w4P3f5nFvi5waHA@mail.gmail.com>
 <CABD4CG+E7nYQHzuTJAnHENgFCNrsMTL1TU3am85zyO=vG_Zygw@mail.gmail.com> <CAO=evYc8aZBrmR3X89nkgvgE_pb253s8KKyO3UeZQtzHVCpWtA@mail.gmail.com>
In-Reply-To: <CAO=evYc8aZBrmR3X89nkgvgE_pb253s8KKyO3UeZQtzHVCpWtA@mail.gmail.com>
From: Bin Wang <wbin00@gmail.com>
Date: Tue, 24 Mar 2015 14:03:52 +0000
Message-ID: <CAD_32VUshKDDNXQ_E29YgV=qJz04eWfQ7xrYKN7w14kWZnzjxw@mail.gmail.com>
Subject: Re: Optimize the first map reduce of DStream
To: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113c328cb54eba05120940f7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113c328cb54eba05120940f7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm not looking for limit the block size.

Here is another example. Say we want to count the lines from the stream in
one hour. In a normal program, we may write it like this:

int sum =3D 0
while (line =3D getFromStream()) {
    store(line) // store the line into storage instead of memory.
    sum++
}

This could be seen as a reduce. The only memory used here is just the
variable named "line", need not store all the lines into memory (if lines
would not use in other places). If we want to provide fault tolerance, we
may just store lines into storage instead of in the memory. Could Spark
streaming work like this way? Dose Flink work like this?





On Tue, Mar 24, 2015 at 7:04 PM Zolt=C3=A1n Zvara <zoltan.zvara@gmail.com> =
wrote:

> There is a BlockGenerator on each worker node next to the
> ReceiverSupervisorImpl, which generates Blocks out of an ArrayBuffer in
> each interval (block_interval). These Blocks are passed to
> ReceiverSupervisorImpl, which throws these blocks to into the BlockManage=
r
> for storage. BlockInfos are passed to the driver. Mini-batches are create=
d
> by the JobGenerator component on the driver each batch_interval. I guess
> what you are looking for is provided by a continuous model like Flink's. =
We
> are creating mini-batches to provide fault tolerance.
>
> Zvara Zolt=C3=A1n
>
>
>
> mail, hangout, skype: zoltan.zvara@gmail.com
>
> mobile, viber: +36203129543
>
> bank: 10918001-00000021-50480008
>
> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>
> elte: HSKSJZ (ZVZOAAI.ELTE)
>
> 2015-03-24 11:55 GMT+01:00 Arush Kharbanda <arush@sigmoidanalytics.com>:
>
>> The block size is configurable and that way I think you can reduce the
>> block interval, to keep the block in memory only for the limiter interva=
l?
>> Is that what you are looking for?
>>
>> On Tue, Mar 24, 2015 at 1:38 PM, Bin Wang <wbin00@gmail.com> wrote:
>>
>> > Hi,
>> >
>> > I'm learning Spark and I find there could be some optimize for the
>> current
>> > streaming implementation. Correct me if I'm wrong.
>> >
>> > The current streaming implementation put the data of one batch into
>> memory
>> > (as RDD). But it seems not necessary.
>> >
>> > For example, if I want to count the lines which contains word "Spark",=
 I
>> > just need to map every line to see if it contains word, then reduce it
>> with
>> > a sum function. After that, this line is no longer useful to keep it i=
n
>> > memory.
>> >
>> > That is said, if the DStream only have one map and/or reduce operation
>> on
>> > it. It is not necessary to keep all the batch data in the memory.
>> Something
>> > like a pipeline should be OK.
>> >
>> > Is it difficult to implement on top of the current implementation?
>> >
>> > Thanks.
>> >
>> > ---
>> > Bin Wang
>> >
>>
>>
>>
>> --
>>
>> [image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com>
>>
>> *Arush Kharbanda* || Technical Teamlead
>>
>> arush@sigmoidanalytics.com || www.sigmoidanalytics.com
>>
>

--001a113c328cb54eba05120940f7--

From dev-return-12131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 14:22:41 2015
Return-Path: <dev-return-12131-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 644C717687
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 14:22:41 +0000 (UTC)
Received: (qmail 25808 invoked by uid 500); 24 Mar 2015 14:22:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25739 invoked by uid 500); 24 Mar 2015 14:22:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25727 invoked by uid 99); 24 Mar 2015 14:22:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 14:22:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 14:22:34 +0000
Received: by obdfc2 with SMTP id fc2so148325267obd.3
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 07:21:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=GtKVDOYCPHKb9yq72wN4z+7pUVDlaqysO4447Djsqhg=;
        b=onab6FyULZeeADjCOoc7twAErbJk9QJiEahZ3PNQmn/JuqVFfcYHwb8K4Pk+rIejcx
         NIg0p0nE5bICNFJVqARPHYljntsKUB0XSCSLsw7qtLN41KfjEUiLbOIYuVbSBF7qN5yG
         81g10Nxtt6XiAW4EXwl2HHLQQgOpsZA2PEooHQ3QTnI3IEkZxO3m6VWNhW53Q2OagcZ2
         +IDdaVGbcmCzq8fEksEwFD20+vgKHOFrTchmL9tXnOpDRP+Wi/mqp/Pywn82pgeza4ka
         AM4A+9xR3yjmxAo+E3y+oN9kj0bUzwwVXpKWoI8A1C6ObrWL5qLYcEFMXI0nFU09vZrn
         5XCg==
MIME-Version: 1.0
X-Received: by 10.202.175.76 with SMTP id y73mr3186757oie.81.1427206889354;
 Tue, 24 Mar 2015 07:21:29 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Tue, 24 Mar 2015 07:21:29 -0700 (PDT)
In-Reply-To: <CAD_32VUshKDDNXQ_E29YgV=qJz04eWfQ7xrYKN7w14kWZnzjxw@mail.gmail.com>
References: <CAD_32VXw3p1GqKOkAhDH=ghNRitnsKzZNG2w4P3f5nFvi5waHA@mail.gmail.com>
	<CABD4CG+E7nYQHzuTJAnHENgFCNrsMTL1TU3am85zyO=vG_Zygw@mail.gmail.com>
	<CAO=evYc8aZBrmR3X89nkgvgE_pb253s8KKyO3UeZQtzHVCpWtA@mail.gmail.com>
	<CAD_32VUshKDDNXQ_E29YgV=qJz04eWfQ7xrYKN7w14kWZnzjxw@mail.gmail.com>
Date: Tue, 24 Mar 2015 15:21:29 +0100
Message-ID: <CAO=evYdsrtyVUuKNbzpgJQCkwwuE5wHa=izR3-bYr_3SuBnosA@mail.gmail.com>
Subject: Re: Optimize the first map reduce of DStream
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: Bin Wang <wbin00@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ce60ca93b2e0512097f72
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ce60ca93b2e0512097f72
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

=E2=80=8BAFAIK Spark Streaming can not work in a way like this. Transformat=
ions are
made on DStreams, where DStreams are basically hold (time,
allocatedBlocksForBatch) pairs.=E2=80=8B Allocated blocks are allocated by =
the
JobGenerator, unallocated blocks (infos) are collected by
ReceivedBlockTracker. In Spark Streaming you define transformations and
actions on DStreams. The operators define RDD chains, tasks are created by
spark-core. You manipulate DStreams, not single unit of data. Flink for
example uses a continuous model. It optimizes for memory usage and latency.
Read the Spark Streaming paper and Spark paper for more reference.

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-03-24 15:03 GMT+01:00 Bin Wang <wbin00@gmail.com>:

> I'm not looking for limit the block size.
>
> Here is another example. Say we want to count the lines from the stream i=
n
> one hour. In a normal program, we may write it like this:
>
> int sum =3D 0
> while (line =3D getFromStream()) {
>     store(line) // store the line into storage instead of memory.
>     sum++
> }
>
> This could be seen as a reduce. The only memory used here is just the
> variable named "line", need not store all the lines into memory (if lines
> would not use in other places). If we want to provide fault tolerance, we
> may just store lines into storage instead of in the memory. Could Spark
> streaming work like this way? Dose Flink work like this?
>
>
>
>
>
> On Tue, Mar 24, 2015 at 7:04 PM Zolt=C3=A1n Zvara <zoltan.zvara@gmail.com=
>
> wrote:
>
>> There is a BlockGenerator on each worker node next to the
>> ReceiverSupervisorImpl, which generates Blocks out of an ArrayBuffer in
>> each interval (block_interval). These Blocks are passed to
>> ReceiverSupervisorImpl, which throws these blocks to into the BlockManag=
er
>> for storage. BlockInfos are passed to the driver. Mini-batches are creat=
ed
>> by the JobGenerator component on the driver each batch_interval. I guess
>> what you are looking for is provided by a continuous model like Flink's.=
 We
>> are creating mini-batches to provide fault tolerance.
>>
>> Zvara Zolt=C3=A1n
>>
>>
>>
>> mail, hangout, skype: zoltan.zvara@gmail.com
>>
>> mobile, viber: +36203129543
>>
>> bank: 10918001-00000021-50480008
>>
>> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>>
>> elte: HSKSJZ (ZVZOAAI.ELTE)
>>
>> 2015-03-24 11:55 GMT+01:00 Arush Kharbanda <arush@sigmoidanalytics.com>:
>>
>>> The block size is configurable and that way I think you can reduce the
>>> block interval, to keep the block in memory only for the limiter
>>> interval?
>>> Is that what you are looking for?
>>>
>>> On Tue, Mar 24, 2015 at 1:38 PM, Bin Wang <wbin00@gmail.com> wrote:
>>>
>>> > Hi,
>>> >
>>> > I'm learning Spark and I find there could be some optimize for the
>>> current
>>> > streaming implementation. Correct me if I'm wrong.
>>> >
>>> > The current streaming implementation put the data of one batch into
>>> memory
>>> > (as RDD). But it seems not necessary.
>>> >
>>> > For example, if I want to count the lines which contains word "Spark"=
,
>>> I
>>> > just need to map every line to see if it contains word, then reduce i=
t
>>> with
>>> > a sum function. After that, this line is no longer useful to keep it =
in
>>> > memory.
>>> >
>>> > That is said, if the DStream only have one map and/or reduce operatio=
n
>>> on
>>> > it. It is not necessary to keep all the batch data in the memory.
>>> Something
>>> > like a pipeline should be OK.
>>> >
>>> > Is it difficult to implement on top of the current implementation?
>>> >
>>> > Thanks.
>>> >
>>> > ---
>>> > Bin Wang
>>> >
>>>
>>>
>>>
>>> --
>>>
>>> [image: Sigmoid Analytics] <http://htmlsig.com/www.sigmoidanalytics.com=
>
>>>
>>> *Arush Kharbanda* || Technical Teamlead
>>>
>>> arush@sigmoidanalytics.com || www.sigmoidanalytics.com
>>>
>>

--001a113ce60ca93b2e0512097f72--

From dev-return-12132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 15:29:33 2015
Return-Path: <dev-return-12132-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EAE81179DC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 15:29:32 +0000 (UTC)
Received: (qmail 85844 invoked by uid 500); 24 Mar 2015 15:29:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85767 invoked by uid 500); 24 Mar 2015 15:29:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85756 invoked by uid 99); 24 Mar 2015 15:29:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:29:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of irashid@cloudera.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:29:26 +0000
Received: by wgra20 with SMTP id a20so174059835wgr.3
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 08:29:06 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=WmmMmUhcmx4vLrbgu6oJ94ntfvK9nrM3zdaN3oknfEQ=;
        b=X06r5Ll5KKKGyoy37bwwqbE+PmNRCEfa4j6pvZPfDO5W+UFlJYl8e0N4NzBI1DBnxD
         IN82YHdw059ZvoiZpFfbBzMG18TMWDvphkZobmfKe4L+OBTfsmuwOl6wv8OyiggfWcaz
         BLNTYqcny7yGxLBi/yyH9iYFYhXocgLedj3tmBlBHAY666lmtpXfeQcQKGzF8NZlbPVt
         aPGMeiFLgX+dq9lH96S4gRMqs05pzE8NUtuyFdaweFY74uOAlGjoF+ywZ1esHjbUZiYx
         ASsUMN/rP1FZm0s0imckcO8fe9EoaGIwN2hgNehRKHrmEOKtoZuAFQn768LRlVqo7ak+
         4nYQ==
X-Gm-Message-State: ALoCoQlCBRwGmcfx/Pe57IULXxNPiF6dfamXYWoF3zyf6wsal8JqQKdC2ZmCUBTExKExYHaDjbo/
X-Received: by 10.180.14.66 with SMTP id n2mr26144904wic.64.1427210946142;
 Tue, 24 Mar 2015 08:29:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.62.197 with HTTP; Tue, 24 Mar 2015 08:28:45 -0700 (PDT)
In-Reply-To: <CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
 <CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Tue, 24 Mar 2015 10:28:45 -0500
Message-ID: <CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
To: Koert Kuipers <koert@tresata.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04138ccd77033505120a716e
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04138ccd77033505120a716e
Content-Type: text/plain; charset=UTF-8

I think this would be a great addition, I totally agree that you need to be
able to set these at a finer context than just the SparkContext.

Just to play devil's advocate, though -- the alternative is for you just
subclass HadoopRDD yourself, or make a totally new RDD, and then you could
expose whatever you need.  Why is this solution better?  IMO the criteria
are:
(a) common operations
(b) error-prone / difficult to implement
(c) non-obvious, but important for performance

I think this case fits (a) & (c), so I think its still worthwhile.  But its
also worth asking whether or not its too difficult for a user to extend
HadoopRDD right now.  There have been several cases in the past week where
we've suggested that a user should read from hdfs themselves (eg., to read
multiple files together in one partition) -- with*out* reusing the code in
HadoopRDD, though they would lose things like the metric tracking &
preferred locations you get from HadoopRDD.  Does HadoopRDD need to some
refactoring to make that easier to do?  Or do we just need a good example?

Imran

(sorry for hijacking your thread, Koert)



On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com> wrote:

> see email below. reynold suggested i send it to dev instead of user
>
> ---------- Forwarded message ----------
> From: Koert Kuipers <koert@tresata.com>
> Date: Mon, Mar 23, 2015 at 4:36 PM
> Subject: hadoop input/output format advanced control
> To: "user@spark.apache.org" <user@spark.apache.org>
>
>
> currently its pretty hard to control the Hadoop Input/Output formats used
> in Spark. The conventions seems to be to add extra parameters to all
> methods and then somewhere deep inside the code (for example in
> PairRDDFunctions.saveAsHadoopFile) all these parameters get translated into
> settings on the Hadoop Configuration object.
>
> for example for compression i see "codec: Option[Class[_ <:
> CompressionCodec]] = None" added to a bunch of methods.
>
> how scalable is this solution really?
>
> for example i need to read from a hadoop dataset and i dont want the input
> (part) files to get split up. the way to do this is to set
> "mapred.min.split.size". now i dont want to set this at the level of the
> SparkContext (which can be done), since i dont want it to apply to input
> formats in general. i want it to apply to just this one specific input
> dataset i need to read. which leaves me with no options currently. i could
> go add yet another input parameter to all the methods
> (SparkContext.textFile, SparkContext.hadoopFile, SparkContext.objectFile,
> etc.). but that seems ineffective.
>
> why can we not expose a Map[String, String] or some other generic way to
> manipulate settings for hadoop input/output formats? it would require
> adding one more parameter to all methods to deal with hadoop input/output
> formats, but after that its done. one parameter to rule them all....
>
> then i could do:
> val x = sc.textFile("/some/path", formatSettings =
> Map("mapred.min.split.size" -> "12345"))
>
> or
> rdd.saveAsTextFile("/some/path, formatSettings =
> Map(mapred.output.compress" -> "true", "mapred.output.compression.codec" ->
> "somecodec"))
>

--f46d04138ccd77033505120a716e--

From dev-return-12133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 15:30:41 2015
Return-Path: <dev-return-12133-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8DF6E179EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 15:30:41 +0000 (UTC)
Received: (qmail 95539 invoked by uid 500); 24 Mar 2015 15:30:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95454 invoked by uid 500); 24 Mar 2015 15:30:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95188 invoked by uid 99); 24 Mar 2015 15:30:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:30:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:30:07 +0000
Received: by iecvj10 with SMTP id vj10so414757iec.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 08:30:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=7jYd878+LLgZAJ/JNoLuEHdEWDAEMABdfzq8PqaV9P0=;
        b=DmevFnGOFd2v3ESMwl+CaMUZI7P5v2TtE/fi4tvPxofyJgs7rY8qZPImxrQRZ6uBXb
         Ilk2fDo474KHqasVzYx2nzib0sl/Q+JDGSdHftlMafNSWYHClj1/y4x0PPz8ws9aDHJh
         4hbPZ5nOuK0ZiJIMMpJqzwKnbWHCy3N0EMMMAa2YL9JvH7kYy13Zq6bOhjeqznXEIN15
         bbDhWT+XZ4G08TOZv76LFNGqKsqndSoL6WQRTgq9gGmD24/Jbch2onfZskwLpV74slRo
         M+FF2tzW0cfI63nWoYwSNspd1vqJjSZ8zews4PyidfIkW5rP+wwceadUcPBkk6vTEdhF
         yX6A==
X-Gm-Message-State: ALoCoQm82IYmwRV7+XsoGXXtFw6aKV6FCOHrs7dRolJed2Sd3RZtIsOsphsikbLumqW4mcxr4Wzq
MIME-Version: 1.0
X-Received: by 10.43.14.10 with SMTP id po10mr25984997icb.64.1427211005409;
 Tue, 24 Mar 2015 08:30:05 -0700 (PDT)
Received: by 10.36.90.208 with HTTP; Tue, 24 Mar 2015 08:30:05 -0700 (PDT)
In-Reply-To: <CAO=evYeFGTrDFfyjhpYP_Nf3am_oFLtSTa4PhXZg=uyX28+4Zw@mail.gmail.com>
References: <CAO=evYeFGTrDFfyjhpYP_Nf3am_oFLtSTa4PhXZg=uyX28+4Zw@mail.gmail.com>
Date: Tue, 24 Mar 2015 11:30:05 -0400
Message-ID: <CACBYxKLJc-FXjP2riTj=L3Ffy1BTa6_NOj7S=sFgTF6euKDY9A@mail.gmail.com>
Subject: Re: Spark Executor resources
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec50fdfbbff4a6205120a7434
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec50fdfbbff4a6205120a7434
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Zoltan,

If running on YARN, the YARN NodeManager starts executors.  I don't think
there's a 100% precise way for the Spark executor way to know how many
resources are allotted to it.  It can come close by looking at the Spark
configuration options used to request it (spark.executor.memory and
spark.yarn.executor.memoryOverhead), but it can't necessarily for the
amount that YARN has rounded up if those configuration properties
(yarn.scheduler.minimum-allocation-mb and
yarn.scheduler.increment-allocation-mb) are not present on the node.

-Sandy

-Sandy

On Mon, Mar 23, 2015 at 5:08 PM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.com>
wrote:

> Let's say I'm an Executor instance in a Spark system. Who started me and
> where, when I run on a worker node supervised by (a) Mesos, (b) YARN? I
> suppose I'm the only one Executor on a worker node for a given framework
> scheduler (driver). If I'm an Executor instance, who is the closest objec=
t
> to me who can tell me how many resources do I have on (a) Mesos, (b) YARN=
?
>
> Thank you for your kind input!
>
> Zvara Zolt=C3=A1n
>
>
>
> mail, hangout, skype: zoltan.zvara@gmail.com
>
> mobile, viber: +36203129543
>
> bank: 10918001-00000021-50480008
>
> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>
> elte: HSKSJZ (ZVZOAAI.ELTE)
>

--bcaec50fdfbbff4a6205120a7434--

From dev-return-12134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 15:42:12 2015
Return-Path: <dev-return-12134-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7C87317A56
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 15:42:12 +0000 (UTC)
Received: (qmail 35116 invoked by uid 500); 24 Mar 2015 15:42:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35043 invoked by uid 500); 24 Mar 2015 15:42:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35031 invoked by uid 99); 24 Mar 2015 15:42:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:42:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:41:46 +0000
Received: by oifl3 with SMTP id l3so141072191oif.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 08:41:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/6xd/b/M9p2hl1TeqeMn9D7LryvtpqKf5YnRPz4DHt0=;
        b=WbhWCiwjjW7gppTZTq7FhGdazmh5QH+lrSwMj8E6ZtR+xyPF+7/H316uwCBHyTtFPg
         hRI4sU2r/P+dFbBMYw6+TvW8w1ezTkm6IJ/zNMcG3JHelw8uxXeo50Hsv/6YJsTX+75R
         SxIvEnRjuUg3/yXCOajJWaxxGf2QVbKqNYMDiafaeUnPfrUIpSYpovSGygr+YFKzHsmc
         r1o5nwv+W+Ixw1qSQ36ixpw4iy4f8b7lggwowPzYXfP9YG4Oy4DqNSc4pb9A3OnWQ1DF
         LQLPrsJaqajUdpP5izQXHvDGFPz8Rz/FUDBspEKm/aNP5TyX1GDa1dZnflUetv/eMdwt
         6IxA==
MIME-Version: 1.0
X-Received: by 10.60.33.106 with SMTP id q10mr3682213oei.67.1427211704088;
 Tue, 24 Mar 2015 08:41:44 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Tue, 24 Mar 2015 08:41:43 -0700 (PDT)
In-Reply-To: <CACBYxKLJc-FXjP2riTj=L3Ffy1BTa6_NOj7S=sFgTF6euKDY9A@mail.gmail.com>
References: <CAO=evYeFGTrDFfyjhpYP_Nf3am_oFLtSTa4PhXZg=uyX28+4Zw@mail.gmail.com>
	<CACBYxKLJc-FXjP2riTj=L3Ffy1BTa6_NOj7S=sFgTF6euKDY9A@mail.gmail.com>
Date: Tue, 24 Mar 2015 16:41:43 +0100
Message-ID: <CAO=evYdPseXrnopuaUYhL1BX24W3dN-kVYOPsLswQu5_PELqqQ@mail.gmail.com>
Subject: Re: Spark Executor resources
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01176d65a43e7005120a9ef2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01176d65a43e7005120a9ef2
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thank you for your response!

I guess the (Spark)AM, who gives the container leash to the NM (along with
the executor JAR and command to run) must know how many CPU or RAM that
container capped, isolated at. There must be a resource vector along the
encrypted container leash if I'm right that describes this. Or maybe is
there a way for the ExecutorBackend to fetch this information directly from
the environment? Then, the ExecutorBackend would be able to hand over this
information to the actual Executor who creates the TaskRunner.

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-03-24 16:30 GMT+01:00 Sandy Ryza <sandy.ryza@cloudera.com>:

> Hi Zoltan,
>
> If running on YARN, the YARN NodeManager starts executors.  I don't think
> there's a 100% precise way for the Spark executor way to know how many
> resources are allotted to it.  It can come close by looking at the Spark
> configuration options used to request it (spark.executor.memory and
> spark.yarn.executor.memoryOverhead), but it can't necessarily for the
> amount that YARN has rounded up if those configuration properties
> (yarn.scheduler.minimum-allocation-mb and
> yarn.scheduler.increment-allocation-mb) are not present on the node.
>
> -Sandy
>
> -Sandy
>
> On Mon, Mar 23, 2015 at 5:08 PM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.co=
m>
> wrote:
>
>> Let's say I'm an Executor instance in a Spark system. Who started me and
>> where, when I run on a worker node supervised by (a) Mesos, (b) YARN? I
>> suppose I'm the only one Executor on a worker node for a given framework
>> scheduler (driver). If I'm an Executor instance, who is the closest obje=
ct
>> to me who can tell me how many resources do I have on (a) Mesos, (b) YAR=
N?
>>
>> Thank you for your kind input!
>>
>> Zvara Zolt=C3=A1n
>>
>>
>>
>> mail, hangout, skype: zoltan.zvara@gmail.com
>>
>> mobile, viber: +36203129543
>>
>> bank: 10918001-00000021-50480008
>>
>> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>>
>> elte: HSKSJZ (ZVZOAAI.ELTE)
>>
>
>

--089e01176d65a43e7005120a9ef2--

From dev-return-12135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 15:44:30 2015
Return-Path: <dev-return-12135-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 762BF17A77
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 15:44:30 +0000 (UTC)
Received: (qmail 46626 invoked by uid 500); 24 Mar 2015 15:44:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46548 invoked by uid 500); 24 Mar 2015 15:44:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46535 invoked by uid 99); 24 Mar 2015 15:44:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:44:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:44:24 +0000
Received: by iedm5 with SMTP id m5so532774ied.3
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 08:42:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=fdNfHnRJrlxdYSHkzXQdRwA0oK+4bb8pxoDDH5xOFGQ=;
        b=Cx2h/ETuM8TSmZxDk044rn0JGKwi4klz1spXAOIUiP3Ou27zjoSbHuIw/50qP9iJbo
         HQ48u46Wj2UDu2fn+xRgQIqDcS4+1yzvCDyJb9Ej/wxkzaI2XRwL4mQlNAoz5BazOkI7
         1pwZ9ENhu3aHumGEsI306PxRfR83g3TWV38f+BeqSd+tcoqB4iYcMEHIb7zfu29WoWv0
         KGMJqrX8yv7odupWlJSR+Y8UGhjI+J3XdVelYQqGpG/tlCQLs0cIcNEcO6sexInSPe7a
         AJPKLAIPF3aoROKU65COCwcWQY3Ffxwa0lZlr+ydZjaE0GrPhwGWcdYLT+dRx5Ud7FAs
         PVUw==
X-Gm-Message-State: ALoCoQmiTW9cVTFn78BavLKhNkyG1UsGXZTpC/Z8aK8DfMml8V7OEo5RLSarbryHd+ebvIY2EB3/
MIME-Version: 1.0
X-Received: by 10.50.6.4 with SMTP id w4mr23436277igw.36.1427211754361; Tue,
 24 Mar 2015 08:42:34 -0700 (PDT)
Received: by 10.36.90.208 with HTTP; Tue, 24 Mar 2015 08:42:34 -0700 (PDT)
In-Reply-To: <CAO=evYdPseXrnopuaUYhL1BX24W3dN-kVYOPsLswQu5_PELqqQ@mail.gmail.com>
References: <CAO=evYeFGTrDFfyjhpYP_Nf3am_oFLtSTa4PhXZg=uyX28+4Zw@mail.gmail.com>
	<CACBYxKLJc-FXjP2riTj=L3Ffy1BTa6_NOj7S=sFgTF6euKDY9A@mail.gmail.com>
	<CAO=evYdPseXrnopuaUYhL1BX24W3dN-kVYOPsLswQu5_PELqqQ@mail.gmail.com>
Date: Tue, 24 Mar 2015 11:42:34 -0400
Message-ID: <CACBYxKLUrLM-xztv0JsnbWtDwrkTj1BrpajHV4ObH6tiQ-khFQ@mail.gmail.com>
Subject: Re: Spark Executor resources
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc1686a36ddb05120aa17a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc1686a36ddb05120aa17a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That's correct.  What's the reason this information is needed?

-Sandy

On Tue, Mar 24, 2015 at 11:41 AM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.com=
>
wrote:

> Thank you for your response!
>
> I guess the (Spark)AM, who gives the container leash to the NM (along wit=
h
> the executor JAR and command to run) must know how many CPU or RAM that
> container capped, isolated at. There must be a resource vector along the
> encrypted container leash if I'm right that describes this. Or maybe is
> there a way for the ExecutorBackend to fetch this information directly fr=
om
> the environment? Then, the ExecutorBackend would be able to hand over thi=
s
> information to the actual Executor who creates the TaskRunner.
>
> Zvara Zolt=C3=A1n
>
>
>
> mail, hangout, skype: zoltan.zvara@gmail.com
>
> mobile, viber: +36203129543
>
> bank: 10918001-00000021-50480008
>
> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>
> elte: HSKSJZ (ZVZOAAI.ELTE)
>
> 2015-03-24 16:30 GMT+01:00 Sandy Ryza <sandy.ryza@cloudera.com>:
>
>> Hi Zoltan,
>>
>> If running on YARN, the YARN NodeManager starts executors.  I don't thin=
k
>> there's a 100% precise way for the Spark executor way to know how many
>> resources are allotted to it.  It can come close by looking at the Spark
>> configuration options used to request it (spark.executor.memory and
>> spark.yarn.executor.memoryOverhead), but it can't necessarily for the
>> amount that YARN has rounded up if those configuration properties
>> (yarn.scheduler.minimum-allocation-mb and
>> yarn.scheduler.increment-allocation-mb) are not present on the node.
>>
>> -Sandy
>>
>> -Sandy
>>
>> On Mon, Mar 23, 2015 at 5:08 PM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.c=
om>
>> wrote:
>>
>>> Let's say I'm an Executor instance in a Spark system. Who started me an=
d
>>> where, when I run on a worker node supervised by (a) Mesos, (b) YARN? I
>>> suppose I'm the only one Executor on a worker node for a given framewor=
k
>>> scheduler (driver). If I'm an Executor instance, who is the closest
>>> object
>>> to me who can tell me how many resources do I have on (a) Mesos, (b)
>>> YARN?
>>>
>>> Thank you for your kind input!
>>>
>>> Zvara Zolt=C3=A1n
>>>
>>>
>>>
>>> mail, hangout, skype: zoltan.zvara@gmail.com
>>>
>>> mobile, viber: +36203129543
>>>
>>> bank: 10918001-00000021-50480008
>>>
>>> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>>>
>>> elte: HSKSJZ (ZVZOAAI.ELTE)
>>>
>>
>>
>

--047d7bdc1686a36ddb05120aa17a--

From dev-return-12136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 15:49:29 2015
Return-Path: <dev-return-12136-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D408E17A96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 15:49:29 +0000 (UTC)
Received: (qmail 59283 invoked by uid 500); 24 Mar 2015 15:49:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59208 invoked by uid 500); 24 Mar 2015 15:49:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59196 invoked by uid 99); 24 Mar 2015 15:49:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:49:06 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 15:49:01 +0000
Received: by obcjt1 with SMTP id jt1so130153571obc.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 08:48:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gaKG6dbVZgMdRQtoycFuQjab05JH+daeGAKzN2+Zs28=;
        b=be4TNLnQC0B21jyyPJj7E/hCSoinhaHkqs9/ZShgrmw894kcCijNG66nz7WGHd1uA2
         GfmLIyLvoOxePYAX4xfFFCVhqdpdoY37TnHkZEvNniWALPSelLjpzvtvyUQZb12Pds8m
         9gxHEezQB93eiOmIbbrm4Co3KCB5WRgPZd8AZsMUXSgNDIFrlbAzcVragoOc6du7GKGN
         WSqhprnSP/MIyLyLL3O4FNtPhDM/hq6tZWN6+1r7qaUL39JzJvlDrl0XRjfEmkz8R3CO
         lkNnYcL9OKCS1wCBRYGYzfcUcPc/UcZUPWskVJDkNbJwGSTJZM7kiiAd12Z2ZhV+5Nhl
         EYHg==
MIME-Version: 1.0
X-Received: by 10.182.46.129 with SMTP id v1mr2476498obm.22.1427212121481;
 Tue, 24 Mar 2015 08:48:41 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Tue, 24 Mar 2015 08:48:41 -0700 (PDT)
In-Reply-To: <CACBYxKLUrLM-xztv0JsnbWtDwrkTj1BrpajHV4ObH6tiQ-khFQ@mail.gmail.com>
References: <CAO=evYeFGTrDFfyjhpYP_Nf3am_oFLtSTa4PhXZg=uyX28+4Zw@mail.gmail.com>
	<CACBYxKLJc-FXjP2riTj=L3Ffy1BTa6_NOj7S=sFgTF6euKDY9A@mail.gmail.com>
	<CAO=evYdPseXrnopuaUYhL1BX24W3dN-kVYOPsLswQu5_PELqqQ@mail.gmail.com>
	<CACBYxKLUrLM-xztv0JsnbWtDwrkTj1BrpajHV4ObH6tiQ-khFQ@mail.gmail.com>
Date: Tue, 24 Mar 2015 16:48:41 +0100
Message-ID: <CAO=evYfY9TkH=bTQAC-J7tnJdfFpng28ouBJvU-kuFhPve26mA@mail.gmail.com>
Subject: Re: Spark Executor resources
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1d9d28525a805120ab77d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1d9d28525a805120ab77d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm trying to log Tasks to understand physical plan and to visualize which
RDD's which partition is currently computed from which creation site along
with other information. I want to charge the TaskRunner to do this before
actually invoking runTask() on Task and again just before giving the Task
to the GC when metrics are collected. Along with the information I wish to
log, I want to report, log the resources the Executor allocates to run its
Tasks.

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-03-24 16:42 GMT+01:00 Sandy Ryza <sandy.ryza@cloudera.com>:

> That's correct.  What's the reason this information is needed?
>
> -Sandy
>
> On Tue, Mar 24, 2015 at 11:41 AM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.c=
om>
> wrote:
>
>> Thank you for your response!
>>
>> I guess the (Spark)AM, who gives the container leash to the NM (along
>> with the executor JAR and command to run) must know how many CPU or RAM
>> that container capped, isolated at. There must be a resource vector alon=
g
>> the encrypted container leash if I'm right that describes this. Or maybe=
 is
>> there a way for the ExecutorBackend to fetch this information directly f=
rom
>> the environment? Then, the ExecutorBackend would be able to hand over th=
is
>> information to the actual Executor who creates the TaskRunner.
>>
>> Zvara Zolt=C3=A1n
>>
>>
>>
>> mail, hangout, skype: zoltan.zvara@gmail.com
>>
>> mobile, viber: +36203129543
>>
>> bank: 10918001-00000021-50480008
>>
>> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>>
>> elte: HSKSJZ (ZVZOAAI.ELTE)
>>
>> 2015-03-24 16:30 GMT+01:00 Sandy Ryza <sandy.ryza@cloudera.com>:
>>
>>> Hi Zoltan,
>>>
>>> If running on YARN, the YARN NodeManager starts executors.  I don't
>>> think there's a 100% precise way for the Spark executor way to know how
>>> many resources are allotted to it.  It can come close by looking at the
>>> Spark configuration options used to request it (spark.executor.memory a=
nd
>>> spark.yarn.executor.memoryOverhead), but it can't necessarily for the
>>> amount that YARN has rounded up if those configuration properties
>>> (yarn.scheduler.minimum-allocation-mb and
>>> yarn.scheduler.increment-allocation-mb) are not present on the node.
>>>
>>> -Sandy
>>>
>>> -Sandy
>>>
>>> On Mon, Mar 23, 2015 at 5:08 PM, Zolt=C3=A1n Zvara <zoltan.zvara@gmail.=
com>
>>> wrote:
>>>
>>>> Let's say I'm an Executor instance in a Spark system. Who started me a=
nd
>>>> where, when I run on a worker node supervised by (a) Mesos, (b) YARN? =
I
>>>> suppose I'm the only one Executor on a worker node for a given framewo=
rk
>>>> scheduler (driver). If I'm an Executor instance, who is the closest
>>>> object
>>>> to me who can tell me how many resources do I have on (a) Mesos, (b)
>>>> YARN?
>>>>
>>>> Thank you for your kind input!
>>>>
>>>> Zvara Zolt=C3=A1n
>>>>
>>>>
>>>>
>>>> mail, hangout, skype: zoltan.zvara@gmail.com
>>>>
>>>> mobile, viber: +36203129543
>>>>
>>>> bank: 10918001-00000021-50480008
>>>>
>>>> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>>>>
>>>> elte: HSKSJZ (ZVZOAAI.ELTE)
>>>>
>>>
>>>
>>
>

--001a11c1d9d28525a805120ab77d--

From dev-return-12137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 17:57:05 2015
Return-Path: <dev-return-12137-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C8819173AE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 17:57:05 +0000 (UTC)
Received: (qmail 37724 invoked by uid 500); 24 Mar 2015 17:56:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37649 invoked by uid 500); 24 Mar 2015 17:56:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37629 invoked by uid 99); 24 Mar 2015 17:56:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 17:56:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of krajah@maprtech.com designates 209.85.160.173 as permitted sender)
Received: from [209.85.160.173] (HELO mail-yk0-f173.google.com) (209.85.160.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 17:56:47 +0000
Received: by ykek76 with SMTP id k76so388762yke.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 10:56:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=QGc5Y4lQiQSNzW1dfpZtftaXF/Uh7mB4+bIGb63RZIY=;
        b=Al+rB8vyLK/4/DN3Nqxi0hukAjFWYZ/Ja42fAqdLfLplCePutMrfjgv7LjqQB6/qaU
         lBvoiEx3RJcjF/XFAp8k73LylKvobsgmUVPTF3ChLE2xWSySzB668ZbgWrmY9Gs5azMp
         zm7ubOwLkMttXhTB/4U/buJyBmkdm40fLsdsdyLz2Zu3G3KkhOJtwOFyMNnqYBv0YZRu
         kNYkAfghKe/DovcZx9/b0LeKu18ahkR8lKMZ5Ps61q1W6S1o4OTR6YIJEaL29CU/KuOE
         upulTaz/pFE/en8vLOtPoVZSMIPD2+9Mh91NAZH9uMvnRh1TIaqJr9ZWFonP/HwyGG2/
         J46Q==
X-Gm-Message-State: ALoCoQl1ISueGJYU6VDJYPBHehcplf4ZJ+LPV+DfetTrV05JSoNmmu3RCME5OT1tAVx9YeS43plH
MIME-Version: 1.0
X-Received: by 10.55.23.34 with SMTP id i34mr11942840qkh.6.1427219786626; Tue,
 24 Mar 2015 10:56:26 -0700 (PDT)
Received: by 10.140.104.132 with HTTP; Tue, 24 Mar 2015 10:56:26 -0700 (PDT)
Date: Tue, 24 Mar 2015 10:56:26 -0700
Message-ID: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
Subject: Understanding shuffle file name conflicts
From: Kannan Rajah <krajah@maprtech.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1147a09e660e2705120c8083
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1147a09e660e2705120c8083
Content-Type: text/plain; charset=UTF-8

I am working on SPARK-1529. I ran into an issue with my change, where the
same shuffle file was being reused across 2 jobs. Please note this only
happens when I use a hard coded location to use for shuffle files, say
"/tmp". It does not happen with normal code path that uses DiskBlockManager
to pick different directories for each run. So I want to understand how
DiskBlockManager guarantees that such a conflict will never happen.

Let's say the shuffle block id has a value of shuffle_0_0_0. So the data
file name is shuffle_0_0_0.data and index file name is shuffle_0_0_0.index.
If I run a spark job twice, one after another, these files get created
under different directories because of the hashing logic in
DiskBlockManager. But the hash is based off the file name, so how are we
sure that there won't be a conflict ever?

--
Kannan

--001a1147a09e660e2705120c8083--

From dev-return-12138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 18:25:22 2015
Return-Path: <dev-return-12138-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 471C4174FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 18:25:22 +0000 (UTC)
Received: (qmail 22581 invoked by uid 500); 24 Mar 2015 18:25:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22513 invoked by uid 500); 24 Mar 2015 18:25:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22502 invoked by uid 99); 24 Mar 2015 18:25:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 18:25:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.54] (HELO mail-la0-f54.google.com) (209.85.215.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 18:24:55 +0000
Received: by lagg8 with SMTP id g8so1024277lag.1
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 11:23:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=5V6zLTXO/ALVIo8tZvF3RSTEMS57fqK/Y1H/P0QiER0=;
        b=ND35ug8I/NQPRUtpMRY05QVP7aidiYJZxB6X949uFttWXExl03akCx7z6ODTEmvOu0
         OyaRh7J+uYLs8VU4/bIR45Vsg5MSLVNMHevyJpzHdSUpmcJL0i0HVWTpZdJtn0v4MIL9
         RWuMTQB/P4H0tJPOGsT9DiohoLr2bf/jf9SDIqjY8pokZpA2tKq2BHlysZYnqS0Iuc8+
         WmliDKxjrLdrthziAYgcKHMCzJqX3Ef3LCDbpqxmBUF8W0DTjcGdCb2NpJgAcmF+hToz
         e9g+0+D/eC/N0hZ5XYvIKXgnCd1RMuIRlP+nsfARnaCQkAhs6V2sAWpbOxfGF0klLJqm
         rB1Q==
X-Gm-Message-State: ALoCoQnbrkWv07oUESLzmrUbopEOwWs45CNROx7t7j+HnBiZh8vl5jwYh/WsMq0dybmQQiUif61A
X-Received: by 10.152.206.70 with SMTP id lm6mr4960334lac.35.1427221428244;
 Tue, 24 Mar 2015 11:23:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.25.213.18 with HTTP; Tue, 24 Mar 2015 11:23:27 -0700 (PDT)
In-Reply-To: <CAMAsSdLGx8=JE+7O=03GmXaVt_5eL=sE8ZKjKtdtUDwUzRcg-A@mail.gmail.com>
References: <CAMAsSdLGx8=JE+7O=03GmXaVt_5eL=sE8ZKjKtdtUDwUzRcg-A@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Tue, 24 Mar 2015 11:23:27 -0700
Message-ID: <CAAswR-4EVehzvFAEBM_kvvP7ht_ZnpR5uCBeuiOHL0BK6SYx7Q@mail.gmail.com>
Subject: Re: Any guidance on when to back port and how far?
To: Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134a00a3f277a05120ce2fa
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134a00a3f277a05120ce2fa
Content-Type: text/plain; charset=UTF-8

Two other criteria that I use when deciding what to backport:
 - Is it a regression from a previous minor release?  I'm much more likely
to backport fixes in this case, as I'd love for most people to stay up to
date.
 - How scary is the change?  I think the primary goal is stability of the
maintenance branches.  When I am confident that something is isolated and
unlikely to break things (i.e. I'm fixing a confusing error message), then
i'm much more likely to backport it.

Regarding the length of time to continue backporting, I mostly don't
backport to N-1, but this is partially because SQL is changing too fast for
that to generally be useful.  These old branches usually only get attention
from me when there is an explicit request.

I'd love to hear more feedback from others.

Michael

On Tue, Mar 24, 2015 at 6:13 AM, Sean Owen <sowen@cloudera.com> wrote:

> So far, my rule of thumb has been:
>
> - Don't back-port new features or improvements in general, only bug fixes
> - Don't back-port minor bug fixes
> - Back-port bug fixes that seem important enough to not wait for the
> next minor release
> - Back-port site doc changes to the release most likely to go out
> next, to make it a part of the next site publish
>
> But, how far should back-ports go, in general? If the last minor
> release was 1.N, then to branch 1.N surely. Farther back is a question
> of expectation for support of past minor releases. Given the pace of
> change and time available, I assume there's not much support for
> continuing to use release 1.(N-1) and very little for 1.(N-2).
>
> Concretely: does anyone expect a 1.1.2 release ever? a 1.2.2 release?
> It'd be good to hear the received wisdom explicitly.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1134a00a3f277a05120ce2fa--

From dev-return-12139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 18:51:29 2015
Return-Path: <dev-return-12139-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90F2B17650
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 18:51:29 +0000 (UTC)
Received: (qmail 35809 invoked by uid 500); 24 Mar 2015 18:51:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35731 invoked by uid 500); 24 Mar 2015 18:51:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35718 invoked by uid 99); 24 Mar 2015 18:51:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 18:51:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 18:50:58 +0000
Received: by weoy45 with SMTP id y45so1534626weo.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 11:50:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JLK1cPuHP3GL23UJP45prnAAMXsZkDCEZabSISEV9RU=;
        b=yK9+HRwunEQftyDsuMQKMux2g4f7sK1HSpDfLMkQXKE/t+vQIqcJdcKPrOduoDrG16
         DMPG53NBH+PGGZIK3DLWS+KAVjIVVCizO0q/b9U+tFJdQXGaSaWUV1uDEaSYMgan09m3
         udkVPXeVP2BilGZ0s4w3oL0AvI4hOCGVfu0BHb+4Fz6p4fF4cbASWccNRcIDH6CukvFl
         pC75ppLqb8gIYsPq+EyLVtDomyTqRI8tYGFMnk8iUu0N5cVju+Bk9qLhecs5VwjKpynQ
         laUqBD6ark2j5UUrBgz9u4aUT4LDfFlVR3/1Pz+eNZAUtfiJfKjnGAtytzBP7MwYvnhN
         E21Q==
MIME-Version: 1.0
X-Received: by 10.180.211.144 with SMTP id nc16mr31466719wic.82.1427223011965;
 Tue, 24 Mar 2015 11:50:11 -0700 (PDT)
Received: by 10.27.205.136 with HTTP; Tue, 24 Mar 2015 11:50:11 -0700 (PDT)
In-Reply-To: <CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
Date: Tue, 24 Mar 2015 20:50:11 +0200
Message-ID: <CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Nick Pentreath <nick.pentreath@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c26890a4c10005120d40c3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c26890a4c10005120d40c3
Content-Type: text/plain; charset=UTF-8

Imran, on your point to read multiple files together in a partition, is it
not simpler to use the approach of copy Hadoop conf and set per-RDD
settings for min split to control the input size per partition, together
with something like CombineFileInputFormat?

On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com> wrote:

> I think this would be a great addition, I totally agree that you need to be
> able to set these at a finer context than just the SparkContext.
>
> Just to play devil's advocate, though -- the alternative is for you just
> subclass HadoopRDD yourself, or make a totally new RDD, and then you could
> expose whatever you need.  Why is this solution better?  IMO the criteria
> are:
> (a) common operations
> (b) error-prone / difficult to implement
> (c) non-obvious, but important for performance
>
> I think this case fits (a) & (c), so I think its still worthwhile.  But its
> also worth asking whether or not its too difficult for a user to extend
> HadoopRDD right now.  There have been several cases in the past week where
> we've suggested that a user should read from hdfs themselves (eg., to read
> multiple files together in one partition) -- with*out* reusing the code in
> HadoopRDD, though they would lose things like the metric tracking &
> preferred locations you get from HadoopRDD.  Does HadoopRDD need to some
> refactoring to make that easier to do?  Or do we just need a good example?
>
> Imran
>
> (sorry for hijacking your thread, Koert)
>
>
>
> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com> wrote:
>
> > see email below. reynold suggested i send it to dev instead of user
> >
> > ---------- Forwarded message ----------
> > From: Koert Kuipers <koert@tresata.com>
> > Date: Mon, Mar 23, 2015 at 4:36 PM
> > Subject: hadoop input/output format advanced control
> > To: "user@spark.apache.org" <user@spark.apache.org>
> >
> >
> > currently its pretty hard to control the Hadoop Input/Output formats used
> > in Spark. The conventions seems to be to add extra parameters to all
> > methods and then somewhere deep inside the code (for example in
> > PairRDDFunctions.saveAsHadoopFile) all these parameters get translated
> into
> > settings on the Hadoop Configuration object.
> >
> > for example for compression i see "codec: Option[Class[_ <:
> > CompressionCodec]] = None" added to a bunch of methods.
> >
> > how scalable is this solution really?
> >
> > for example i need to read from a hadoop dataset and i dont want the
> input
> > (part) files to get split up. the way to do this is to set
> > "mapred.min.split.size". now i dont want to set this at the level of the
> > SparkContext (which can be done), since i dont want it to apply to input
> > formats in general. i want it to apply to just this one specific input
> > dataset i need to read. which leaves me with no options currently. i
> could
> > go add yet another input parameter to all the methods
> > (SparkContext.textFile, SparkContext.hadoopFile, SparkContext.objectFile,
> > etc.). but that seems ineffective.
> >
> > why can we not expose a Map[String, String] or some other generic way to
> > manipulate settings for hadoop input/output formats? it would require
> > adding one more parameter to all methods to deal with hadoop input/output
> > formats, but after that its done. one parameter to rule them all....
> >
> > then i could do:
> > val x = sc.textFile("/some/path", formatSettings =
> > Map("mapred.min.split.size" -> "12345"))
> >
> > or
> > rdd.saveAsTextFile("/some/path, formatSettings =
> > Map(mapred.output.compress" -> "true", "mapred.output.compression.codec"
> ->
> > "somecodec"))
> >
>

--001a11c26890a4c10005120d40c3--

From dev-return-12140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 18:56:42 2015
Return-Path: <dev-return-12140-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B80617689
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 18:56:42 +0000 (UTC)
Received: (qmail 54997 invoked by uid 500); 24 Mar 2015 18:56:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54917 invoked by uid 500); 24 Mar 2015 18:56:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54905 invoked by uid 99); 24 Mar 2015 18:56:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 18:56:37 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 18:56:33 +0000
Received: by oifl3 with SMTP id l3so2086824oif.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 11:55:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=oI0pG47ShvNUsI+Z9VwNr1FbJktWLw60ZguV2UoTGhY=;
        b=QldBQOKJbt0J8IOT0JdOrnI70y1IdiBxZEDr4slzibeh9t9LzeNojg8lKVYaTyX8QG
         Sa2F0p1/LJKw1ZRp9G2WWvNVz03zm24o4KumvDAEmQJqw8yebnyRTTic6Z8neD8xOw5f
         HEPa4R60h5c8W23rSpNPdy9YUrC/U5MBo3hexvIshu+a1Drs2PFCVLY70OSo9iPE06NV
         HsESMOVaqMa9tg+5xa3f86olfsBNKl17W65l1aIelQ821S9Ok/KlD/Clg/bavYFx/oat
         tN2w+BKkkwx1t9LG0R+eIf3fkchKa23YC+drIe18L4LrYpTN/FHHTbhmrhO7JxcUUPuY
         /w/Q==
MIME-Version: 1.0
X-Received: by 10.60.139.1 with SMTP id qu1mr4390421oeb.83.1427223327311; Tue,
 24 Mar 2015 11:55:27 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Tue, 24 Mar 2015 11:55:27 -0700 (PDT)
In-Reply-To: <CAAswR-4EVehzvFAEBM_kvvP7ht_ZnpR5uCBeuiOHL0BK6SYx7Q@mail.gmail.com>
References: <CAMAsSdLGx8=JE+7O=03GmXaVt_5eL=sE8ZKjKtdtUDwUzRcg-A@mail.gmail.com>
	<CAAswR-4EVehzvFAEBM_kvvP7ht_ZnpR5uCBeuiOHL0BK6SYx7Q@mail.gmail.com>
Date: Tue, 24 Mar 2015 11:55:27 -0700
Message-ID: <CABPQxsvs=9PajQtCJK+MLAM_g93Rg5Pe4GurPgpQdUHMJY2BGw@mail.gmail.com>
Subject: Re: Any guidance on when to back port and how far?
From: Patrick Wendell <pwendell@gmail.com>
To: Michael Armbrust <michael@databricks.com>
Cc: Sean Owen <sowen@cloudera.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

My philosophy has been basically what you suggested, Sean. One thing
you didn't mention though is if a bug fix seems complicated, I will
think very hard before back-porting it. This is because "fixes" can
introduce their own new bugs, in some cases worse than the original
issue. It's really bad to have some upgrade to a patch release and see
a regression - with our current approach this almost never happens.

I will usually try to backport up to N-2, if it can be back-ported
reasonably easily (for instance, with minor or no code changes). The
reason I do this is that vendors do end up supporting older versions,
and it's nice for them if some committer has backported a fix that
they can then pull in, even if we never ship it.

In terms of doing older maintenance releases, this one I think we
should do according to severity of issues (for instance, if there is a
security issue) or based on general command from the community. I
haven't initiated many 1.X.2 releases recently because I didn't see
huge demand. However, personally I don't mind doing these if there is
a lot of demand, at least for releases where ".0" has gone out in the
last six months.

On Tue, Mar 24, 2015 at 11:23 AM, Michael Armbrust
<michael@databricks.com> wrote:
> Two other criteria that I use when deciding what to backport:
>  - Is it a regression from a previous minor release?  I'm much more likely
> to backport fixes in this case, as I'd love for most people to stay up to
> date.
>  - How scary is the change?  I think the primary goal is stability of the
> maintenance branches.  When I am confident that something is isolated and
> unlikely to break things (i.e. I'm fixing a confusing error message), then
> i'm much more likely to backport it.
>
> Regarding the length of time to continue backporting, I mostly don't
> backport to N-1, but this is partially because SQL is changing too fast for
> that to generally be useful.  These old branches usually only get attention
> from me when there is an explicit request.
>
> I'd love to hear more feedback from others.
>
> Michael
>
> On Tue, Mar 24, 2015 at 6:13 AM, Sean Owen <sowen@cloudera.com> wrote:
>
>> So far, my rule of thumb has been:
>>
>> - Don't back-port new features or improvements in general, only bug fixes
>> - Don't back-port minor bug fixes
>> - Back-port bug fixes that seem important enough to not wait for the
>> next minor release
>> - Back-port site doc changes to the release most likely to go out
>> next, to make it a part of the next site publish
>>
>> But, how far should back-ports go, in general? If the last minor
>> release was 1.N, then to branch 1.N surely. Farther back is a question
>> of expectation for support of past minor releases. Given the pace of
>> change and time available, I assume there's not much support for
>> continuing to use release 1.(N-1) and very little for 1.(N-2).
>>
>> Concretely: does anyone expect a 1.1.2 release ever? a 1.2.2 release?
>> It'd be good to hear the received wisdom explicitly.
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 19:00:39 2015
Return-Path: <dev-return-12141-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 30701176D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 19:00:39 +0000 (UTC)
Received: (qmail 66001 invoked by uid 500); 24 Mar 2015 19:00:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65917 invoked by uid 500); 24 Mar 2015 19:00:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65905 invoked by uid 99); 24 Mar 2015 19:00:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 19:00:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.175 as permitted sender)
Received: from [209.85.214.175] (HELO mail-ob0-f175.google.com) (209.85.214.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 18:59:57 +0000
Received: by obdfc2 with SMTP id fc2so1690572obd.3
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 11:59:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=HC6Ph8O6noQ48ZPQlcxXb99s87lUkUctzV4YzPFQXyM=;
        b=bCSGeNynzXq8H/X+rrQb/z0+LST7+dk7F+mMTW5xhb4wjfOM05y6EfVWzZP9JbU2LT
         xIk4bRLUFQQX3eFywVHLS0U0zZ7t4UNI9zhTQ5c74ogbe1r497AILPOf6urqa4Jh9afa
         N4r5+b1ilNrLVCssQvNYG1gfMjqeN47sFv92UPCmTX0mRg57+oAWYo0JuxmKkxaKyQ+M
         6NTwgaVEQmcUciEGRjexara6tpaz2fL7NxYDOdGGrbvBhPc+UXczqLDk6QgLQemcFiwn
         SgfE+0PEnY19DPF4xjVRN5SNt0JfvumkF5JHM0gEwNiTypKkU3NybLRnFNIDUSDSfoz3
         PnfQ==
MIME-Version: 1.0
X-Received: by 10.202.231.85 with SMTP id e82mr4213482oih.104.1427223550645;
 Tue, 24 Mar 2015 11:59:10 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Tue, 24 Mar 2015 11:59:10 -0700 (PDT)
In-Reply-To: <CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
Date: Tue, 24 Mar 2015 11:59:10 -0700
Message-ID: <CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Patrick Wendell <pwendell@gmail.com>
To: Nick Pentreath <nick.pentreath@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah - to Nick's point, I think the way to do this is to pass in a
custom conf when you create a Hadoop RDD (that's AFAIK why the conf
field is there). Is there anything you can't do with that feature?

On Tue, Mar 24, 2015 at 11:50 AM, Nick Pentreath
<nick.pentreath@gmail.com> wrote:
> Imran, on your point to read multiple files together in a partition, is it
> not simpler to use the approach of copy Hadoop conf and set per-RDD
> settings for min split to control the input size per partition, together
> with something like CombineFileInputFormat?
>
> On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com> wrote:
>
>> I think this would be a great addition, I totally agree that you need to be
>> able to set these at a finer context than just the SparkContext.
>>
>> Just to play devil's advocate, though -- the alternative is for you just
>> subclass HadoopRDD yourself, or make a totally new RDD, and then you could
>> expose whatever you need.  Why is this solution better?  IMO the criteria
>> are:
>> (a) common operations
>> (b) error-prone / difficult to implement
>> (c) non-obvious, but important for performance
>>
>> I think this case fits (a) & (c), so I think its still worthwhile.  But its
>> also worth asking whether or not its too difficult for a user to extend
>> HadoopRDD right now.  There have been several cases in the past week where
>> we've suggested that a user should read from hdfs themselves (eg., to read
>> multiple files together in one partition) -- with*out* reusing the code in
>> HadoopRDD, though they would lose things like the metric tracking &
>> preferred locations you get from HadoopRDD.  Does HadoopRDD need to some
>> refactoring to make that easier to do?  Or do we just need a good example?
>>
>> Imran
>>
>> (sorry for hijacking your thread, Koert)
>>
>>
>>
>> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com> wrote:
>>
>> > see email below. reynold suggested i send it to dev instead of user
>> >
>> > ---------- Forwarded message ----------
>> > From: Koert Kuipers <koert@tresata.com>
>> > Date: Mon, Mar 23, 2015 at 4:36 PM
>> > Subject: hadoop input/output format advanced control
>> > To: "user@spark.apache.org" <user@spark.apache.org>
>> >
>> >
>> > currently its pretty hard to control the Hadoop Input/Output formats used
>> > in Spark. The conventions seems to be to add extra parameters to all
>> > methods and then somewhere deep inside the code (for example in
>> > PairRDDFunctions.saveAsHadoopFile) all these parameters get translated
>> into
>> > settings on the Hadoop Configuration object.
>> >
>> > for example for compression i see "codec: Option[Class[_ <:
>> > CompressionCodec]] = None" added to a bunch of methods.
>> >
>> > how scalable is this solution really?
>> >
>> > for example i need to read from a hadoop dataset and i dont want the
>> input
>> > (part) files to get split up. the way to do this is to set
>> > "mapred.min.split.size". now i dont want to set this at the level of the
>> > SparkContext (which can be done), since i dont want it to apply to input
>> > formats in general. i want it to apply to just this one specific input
>> > dataset i need to read. which leaves me with no options currently. i
>> could
>> > go add yet another input parameter to all the methods
>> > (SparkContext.textFile, SparkContext.hadoopFile, SparkContext.objectFile,
>> > etc.). but that seems ineffective.
>> >
>> > why can we not expose a Map[String, String] or some other generic way to
>> > manipulate settings for hadoop input/output formats? it would require
>> > adding one more parameter to all methods to deal with hadoop input/output
>> > formats, but after that its done. one parameter to rule them all....
>> >
>> > then i could do:
>> > val x = sc.textFile("/some/path", formatSettings =
>> > Map("mapred.min.split.size" -> "12345"))
>> >
>> > or
>> > rdd.saveAsTextFile("/some/path, formatSettings =
>> > Map(mapred.output.compress" -> "true", "mapred.output.compression.codec"
>> ->
>> > "somecodec"))
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 19:07:04 2015
Return-Path: <dev-return-12142-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E26591772B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 19:07:03 +0000 (UTC)
Received: (qmail 84905 invoked by uid 500); 24 Mar 2015 19:07:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84828 invoked by uid 500); 24 Mar 2015 19:07:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84816 invoked by uid 99); 24 Mar 2015 19:07:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 19:07:01 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.46 as permitted sender)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 19:06:54 +0000
Received: by oier21 with SMTP id r21so2231914oie.1
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 12:05:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=RCN7YQ8+xZt2M/xFR5S0+6GwkNOCfXBzyLT5vN+WH7Q=;
        b=JBkHatEI34VDXWcIjhkORHgdxNVvcoAKsv7DH2oyG6+asMNbuAFcDa+Gnfvav7qOf+
         NfXAQ159k51XUFZ04paNe3O5ISLtmD/CE9GwRYSS4BJFGdeBgjTvqCxTkoek9LJOo+Zu
         hItWe7vFOKC0BYTDoq7DCcQfQ3tSKu/ReG3fYJeVOuu1txrnkCuACfbJcqpsqBENSXn5
         g52sVQ+rmOpoj9+VJHB9KF/s+D/yNWP6DUYAr1rRhLByXajjqP7+Ai0ulubtN3yutTf5
         kUMl/58CG98LKFFQWKzfeH++Hda9BemxK/CSmpMixgEjWQm2Yq+dvYZK1G0Sd8lKB0Dd
         N4+A==
MIME-Version: 1.0
X-Received: by 10.202.229.141 with SMTP id c135mr4324689oih.44.1427223903862;
 Tue, 24 Mar 2015 12:05:03 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Tue, 24 Mar 2015 12:05:03 -0700 (PDT)
Date: Tue, 24 Mar 2015 12:05:03 -0700
Message-ID: <CABPQxstq5=2gv=Ov30gWzKW6jTxfHwXfCcrFXrcRd7y3bBGCKA@mail.gmail.com>
Subject: Experience using binary packages on various Hadoop distros
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

For a while we've published binary packages with different Hadoop
client's pre-bundled. We currently have three interfaces to a Hadoop
cluster (a) the HDFS client (b) the YARN client (c) the Hive client.

Because (a) and (b) are supposed to be backwards compatible
interfaces. My working assumption was that for the most part (modulo
Hive) our packages work with *newer* Hadoop versions. For instance,
our Hadoop 2.4 package should work with HDFS 2.6 and YARN 2.6.
However, I have heard murmurings that these are not compatible in
practice.

So I have three questions I'd like to put out to the community:

1. Have people had difficulty using 2.4 packages with newer Hadoop
versions? If so, what specific incompatibilities have you hit?
2. Have people had issues using our binary Hadoop packages in general
with commercial or Apache Hadoop distro's, such that you have to build
from source?
3. How would people feel about publishing a "bring your own Hadoop"
binary, where you are required to point us to a local Hadoop
distribution by setting HADOOP_HOME? This might be better for ensuring
full compatibility:
https://issues.apache.org/jira/browse/SPARK-6511

- Patrick

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 19:10:50 2015
Return-Path: <dev-return-12143-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72CD017762
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 19:10:50 +0000 (UTC)
Received: (qmail 95433 invoked by uid 500); 24 Mar 2015 19:10:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95291 invoked by uid 500); 24 Mar 2015 19:10:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94578 invoked by uid 99); 24 Mar 2015 19:10:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 19:10:44 +0000
X-ASF-Spam-Status: No, hits=1.8 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anubhav33@gmail.com designates 209.85.213.173 as permitted sender)
Received: from [209.85.213.173] (HELO mail-ig0-f173.google.com) (209.85.213.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 19:10:20 +0000
Received: by igcau2 with SMTP id au2so81161313igc.0;
        Tue, 24 Mar 2015 12:10:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=pis+u4mR5MJ4TjyVGDu5bHe32KhMy6YkIf5pzVcXR7k=;
        b=AS85OzVzYPgmZLsZ/Ce8oqC3sGx01Smc7m+ADWBk50Ob/9jjuXz1zpm9LwTiiPm0tY
         +tEixuVKVV6xThyYP/PlBrbGBrxCIGLOAYJy51JGFkGHgHylfaGx1qFCXjknBHE/+ace
         PLItzTTe/TbVcG9NqIZw0jLzvRxpkBP4sJ0o+f8Km/nYY/qyCAkIb0jC8zrcSu934tdC
         qZbH/WOLM6HyzzwshFCGzqOxzALp7amtx0p43ArNsr3rR5bTh76SuJvqvJi/UxrVn3/u
         CXv5hBv2QF/Yuxrw5h7/MErgH5T/+5pPWj0eTUpd25Jytouan2VKD3Rfj0prOpX0LNsM
         68Zg==
MIME-Version: 1.0
X-Received: by 10.42.166.1 with SMTP id m1mr28715858icy.10.1427224218329; Tue,
 24 Mar 2015 12:10:18 -0700 (PDT)
Received: by 10.107.8.99 with HTTP; Tue, 24 Mar 2015 12:10:18 -0700 (PDT)
In-Reply-To: <6F61F95B-9B00-4BE4-99E8-902E420B838C@hortonworks.com>
References: <CADfwMNLmtg6mKEyeRMCc9uCqg5+ECd7aA5mNux4ph2uq-14heA@mail.gmail.com>
	<6F61F95B-9B00-4BE4-99E8-902E420B838C@hortonworks.com>
Date: Tue, 24 Mar 2015 15:10:18 -0400
Message-ID: <CAFL=5a8DXvrLYfSsKXKprg62RnapXxQWgWadzVpXsG_VwXfuwQ@mail.gmail.com>
Subject: Re: Spark-thriftserver Issue
From: Anubhav Agarwal <anubhav33@gmail.com>
To: Zhan Zhang <zzhang@hortonworks.com>
Cc: Neil Dev <neilkdev@gmail.com>, "user@spark.apache.org" <user@spark.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba6e86148c71ed05120d8803
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6e86148c71ed05120d8803
Content-Type: text/plain; charset=UTF-8

Zhan specifying port fixed the port issue.

Is it possible to specify the log directory while starting the spark
thriftserver?
Still getting this error even through the folder exists and everyone has
permission to use that directory.
drwxr-xr-x  2 root         root          4096 Mar 24 19:04 spark-events


Exception in thread "main" java.lang.IllegalArgumentException: Log
directory /tmp/spark-events does not exist.
        at
org.apache.spark.scheduler.EventLoggingListener.start(EventLoggingListener.scala:99)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:399)
        at
org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQLEnv.scala:49)
        at
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(HiveThriftServer2.scala:58)
        at
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(HiveThriftServer2.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
        at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
        at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



On Mon, Mar 23, 2015 at 6:51 PM, Zhan Zhang <zzhang@hortonworks.com> wrote:

>  Probably the port is already used by others, e.g., hive. You can change
> the port similar to below
>
>   ./sbin/start-thriftserver.sh --master yarn --executor-memory 512m --hiveconf hive.server2.thrift.port=10001
>
>
>  Thanks.
>
>  Zhan Zhang
>
>   On Mar 23, 2015, at 12:01 PM, Neil Dev <neilkdev@gmail.com> wrote:
>
> Hi,
>
> I am having issue starting spark-thriftserver. I'm running spark 1.3.with
> Hadoop 2.4.0. I would like to be able to change its port too so, I can hive
> hive-thriftserver as well as spark-thriftserver running at the same time.
>
> Starting sparkthrift server:-
> sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
> --executor-memory 2G
>
> Error:-
> I created the folder manually but still getting the following error----
> Exception in thread "main" java.lang.IllegalArgumentException: Log
> directory /tmp/spark-events does not exist.
>
>
> I am getting the following error
> 15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
> org.apache.thrift.transport.TTransportException: Could not create
> ServerSocket on address0.0.0.0/0.0.0.0:10000.
>        at
> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
>        at
> org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
>        at
>
> org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactory.java:236)
>        at
>
> org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryCLIService.java:69)
>        at java.lang.Thread.run(Thread.java:745)
>
> Thanks
> Neil
>
>
>

--90e6ba6e86148c71ed05120d8803--

From dev-return-12144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 19:51:19 2015
Return-Path: <dev-return-12144-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20D5A17449
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 19:51:19 +0000 (UTC)
Received: (qmail 3337 invoked by uid 500); 24 Mar 2015 19:51:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3205 invoked by uid 500); 24 Mar 2015 19:51:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2533 invoked by uid 99); 24 Mar 2015 19:51:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 19:51:14 +0000
X-ASF-Spam-Status: No, hits=3.2 required=5.0
	tests=FSL_HELO_BARE_IP_2,HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zzhang@hortonworks.com designates 64.78.52.187 as permitted sender)
Received: from [64.78.52.187] (HELO relayvx12c.securemail.intermedia.net) (64.78.52.187)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 19:51:10 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id C22AA53E79;
	Tue, 24 Mar 2015 12:50:07 -0700 (PDT)
Subject: Re: Spark-thriftserver Issue
MIME-Version: 1.0
x-echoworx-emg-received: Tue, 24 Mar 2015 12:50:07.770 -0700
x-echoworx-msg-id: 245b51a0-c7cd-4882-b0b0-982e76ad9819
x-echoworx-action: delivered
Received: from 10.254.155.17 ([10.254.155.17])
          by emg-ca-1-2 (JAMES SMTP Server 2.3.2) with SMTP ID 785;
          Tue, 24 Mar 2015 12:50:07 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (unknown [10.224.117.101])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id 812FB53E77;
	Tue, 24 Mar 2015 12:50:07 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Tue, 24 Mar 2015 12:50:06 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Tue, 24 Mar 2015 12:50:06 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: Anubhav Agarwal <anubhav33@gmail.com>
CC: Neil Dev <neilkdev@gmail.com>, "user@spark.apache.org"
	<user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Thread-Topic: Spark-thriftserver Issue
Thread-Index: AQHQZbvYAA+wv4K+Q0ml1HcAYZi7xw==
Date: Tue, 24 Mar 2015 19:50:06 +0000
Message-ID: <972059EC-AB08-4549-9710-E7EDD3AEB8C7@hortonworks.com>
References: <CADfwMNLmtg6mKEyeRMCc9uCqg5+ECd7aA5mNux4ph2uq-14heA@mail.gmail.com>
 <6F61F95B-9B00-4BE4-99E8-902E420B838C@hortonworks.com>
 <CAFL=5a8DXvrLYfSsKXKprg62RnapXxQWgWadzVpXsG_VwXfuwQ@mail.gmail.com>
In-Reply-To: <CAFL=5a8DXvrLYfSsKXKprg62RnapXxQWgWadzVpXsG_VwXfuwQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [192.175.27.23]
x-source-routing-agent: Processed
Content-Type: multipart/alternative;
	boundary="_000_972059ECAB0845499710E7EDD3AEB8C7hortonworkscom_"
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_972059ECAB0845499710E7EDD3AEB8C7hortonworkscom_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

You can try to set it in spark-env.sh.

# - SPARK_LOG_DIR       Where log files are stored.  (Default: ${SPARK_HOME=
}/logs)
# - SPARK_PID_DIR       Where the pid file is stored. (Default: /tmp)

Thanks.

Zhan Zhang

On Mar 24, 2015, at 12:10 PM, Anubhav Agarwal <anubhav33@gmail.com<mailto:a=
nubhav33@gmail.com>> wrote:

Zhan specifying port fixed the port issue.

Is it possible to specify the log directory while starting the spark thrift=
server?
Still getting this error even through the folder exists and everyone has pe=
rmission to use that directory.
drwxr-xr-x  2 root         root          4096 Mar 24 19:04 spark-events


Exception in thread "main" java.lang.IllegalArgumentException: Log director=
y /tmp/spark-events does not exist.
        at org.apache.spark.scheduler.EventLoggingListener.start(EventLoggi=
ngListener.scala:99)
        at org.apache.spark.SparkContext.<init>(SparkContext.scala:399)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLEnv$.init(SparkSQ=
LEnv.scala:49)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2$.main(H=
iveThriftServer2.scala:58)
        at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2.main(Hi=
veThriftServer2.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessor=
Impl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethod=
AccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$Spa=
rkSubmit$$runMain(SparkSubmit.scala:569)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.sca=
la:166)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:18=
9)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)



On Mon, Mar 23, 2015 at 6:51 PM, Zhan Zhang <zzhang@hortonworks.com<mailto:=
zzhang@hortonworks.com>> wrote:
Probably the port is already used by others, e.g., hive. You can change the=
 port similar to below


 ./sbin/start-thriftserver.sh --master yarn --executor-memory 512m --hiveco=
nf hive.server2.thrift.port=3D10001

Thanks.

Zhan Zhang

On Mar 23, 2015, at 12:01 PM, Neil Dev <neilkdev@gmail.com<mailto:neilkdev@=
gmail.com>> wrote:

Hi,

I am having issue starting spark-thriftserver. I'm running spark 1.3.with
Hadoop 2.4.0. I would like to be able to change its port too so, I can hive
hive-thriftserver as well as spark-thriftserver running at the same time.

Starting sparkthrift server:-
sudo ./start-thriftserver.sh --master spark://ip-172-31-10-124:7077
--executor-memory 2G

Error:-
I created the folder manually but still getting the following error----
Exception in thread "main" java.lang.IllegalArgumentException: Log
directory /tmp/spark-events does not exist.


I am getting the following error
15/03/23 15:07:02 ERROR thrift.ThriftCLIService: Error:
org.apache.thrift.transport.TTransportException: Could not create
ServerSocket on address0.0.0.0/0.0.0.0:10000<http://0.0.0.0:10000/>.
       at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:93)
       at
org.apache.thrift.transport.TServerSocket.<init>(TServerSocket.java:79)
       at
org.apache.hive.service.auth.HiveAuthFactory.getServerSocket(HiveAuthFactor=
y.java:236)
       at
org.apache.hive.service.cli.thrift.ThriftBinaryCLIService.run(ThriftBinaryC=
LIService.java:69)
       at java.lang.Thread.run(Thread.java:745)

Thanks
Neil




--_000_972059ECAB0845499710E7EDD3AEB8C7hortonworkscom_--

From dev-return-12145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 21:29:08 2015
Return-Path: <dev-return-12145-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 58708174C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 21:29:08 +0000 (UTC)
Received: (qmail 47622 invoked by uid 500); 24 Mar 2015 21:28:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47543 invoked by uid 500); 24 Mar 2015 21:28:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47508 invoked by uid 99); 24 Mar 2015 21:28:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 21:28:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 21:28:47 +0000
Received: by qgh3 with SMTP id 3so1378299qgh.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 14:28:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=datMkBzr1I+KMT7avmRgPcKWJwEmA9LTX6N1/WBPpck=;
        b=u35LNJGuq6rgZkCb7A8Y1asbl6f20tMhQ22JV6v9Hzt9QJtbfOlNSHepaMwNSulzGG
         I4aCa8CbjlBRRTA+ySfoqYTgZMex6y33/ITjO5Lgf5q70j8h+4VWfi0mAWvlWR/z9z6S
         cQa63vkLuiC01uAn17KKv+kj2p5CBZNHCYj+QahNm+e/k8+/6gDOud1vuDaiLZSjFdjR
         fEMwyMwbOpYc526w9tG8uZOBxDhmvtD5Kdhdyu11ZgcGBBzEC7sldnYiT1fSWzCx3npm
         hFdh5eC9HigS0Rx2IALNOtUlVJsEFdS7qvHza3lMOk1Vh84I7kT2cPkLIF3eqekpd13J
         +ydg==
X-Received: by 10.55.53.85 with SMTP id c82mr13434849qka.45.1427232506878;
        Tue, 24 Mar 2015 14:28:26 -0700 (PDT)
Received: from 30-9-135.wireless.csail.mit.edu (30-9-135.wireless.csail.mit.edu. [128.30.9.135])
        by mx.google.com with ESMTPSA id n77sm359683qha.19.2015.03.24.14.28.25
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 24 Mar 2015 14:28:26 -0700 (PDT)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
Subject: Re: Experience using binary packages on various Hadoop distros
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxstq5=2gv=Ov30gWzKW6jTxfHwXfCcrFXrcRd7y3bBGCKA@mail.gmail.com>
Date: Tue, 24 Mar 2015 17:28:24 -0400
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <6E82B1CC-FE2A-417B-9DF9-42482E324E4A@gmail.com>
References: <CABPQxstq5=2gv=Ov30gWzKW6jTxfHwXfCcrFXrcRd7y3bBGCKA@mail.gmail.com>
To: Patrick Wendell <pwendell@gmail.com>
X-Mailer: Apple Mail (2.2070.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Just a note, one challenge with the BYOH version might be that users who =
download that can't run in local mode without also having Hadoop. But if =
we describe it correctly then hopefully it's okay.

Matei

> On Mar 24, 2015, at 3:05 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>=20
> Hey All,
>=20
> For a while we've published binary packages with different Hadoop
> client's pre-bundled. We currently have three interfaces to a Hadoop
> cluster (a) the HDFS client (b) the YARN client (c) the Hive client.
>=20
> Because (a) and (b) are supposed to be backwards compatible
> interfaces. My working assumption was that for the most part (modulo
> Hive) our packages work with *newer* Hadoop versions. For instance,
> our Hadoop 2.4 package should work with HDFS 2.6 and YARN 2.6.
> However, I have heard murmurings that these are not compatible in
> practice.
>=20
> So I have three questions I'd like to put out to the community:
>=20
> 1. Have people had difficulty using 2.4 packages with newer Hadoop
> versions? If so, what specific incompatibilities have you hit?
> 2. Have people had issues using our binary Hadoop packages in general
> with commercial or Apache Hadoop distro's, such that you have to build
> from source?
> 3. How would people feel about publishing a "bring your own Hadoop"
> binary, where you are required to point us to a local Hadoop
> distribution by setting HADOOP_HOME? This might be better for ensuring
> full compatibility:
> https://issues.apache.org/jira/browse/SPARK-6511
>=20
> - Patrick
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 22:42:16 2015
Return-Path: <dev-return-12146-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A91B11794A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 22:42:16 +0000 (UTC)
Received: (qmail 78110 invoked by uid 500); 24 Mar 2015 22:42:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78037 invoked by uid 500); 24 Mar 2015 22:42:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78023 invoked by uid 99); 24 Mar 2015 22:42:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 22:42:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 22:41:49 +0000
Received: by wixw10 with SMTP id w10so12818650wix.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 15:40:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=BIuWXOzWVIC8VKUoqXeVjQV5h+xq006ulpaGI/iKriY=;
        b=DApseHZafcqpiBNgMK308Vd2qMh5NTkq4xU4cOUQ3qBqDI6EZmoQBIZIpNhb75E/4t
         6Hxx5GVyPvacNQY7QEo+GilI8g8hbcPP1Sy83mgVu//ZrVjj2wlneDtEdEdgEBsbRzqF
         Yvvv/CHiP7ZLHr1d08ZjQibWOJICyEXvkx/F5lIdzMWSlJTLQviSN4VwTIB0acIN2KFo
         Gw9DVIvftiXJK8I9t4TCNHdYPKRJ5wl4yJ9tnP2MNV8FRYN1aJgpC+IJ0a4mH/zcpmTa
         U4Y26H5grQ7MsvA+UunlY5Vp2b6+4GKyagxo1ToL4oujmC5/0xjRi+TCgFNet8Kc970Q
         +v5w==
X-Gm-Message-State: ALoCoQnV3ATfrqV3D3g2Kzp3i9vB23aCQ2DUq3tM5m31k6u6bJBn7XzdTLWEwgha0W9cuC5pYiky
MIME-Version: 1.0
X-Received: by 10.180.218.200 with SMTP id pi8mr32699233wic.71.1427236843304;
 Tue, 24 Mar 2015 15:40:43 -0700 (PDT)
Received: by 10.28.47.193 with HTTP; Tue, 24 Mar 2015 15:40:43 -0700 (PDT)
X-Originating-IP: [204.148.13.62]
In-Reply-To: <CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
Date: Tue, 24 Mar 2015 18:40:43 -0400
Message-ID: <CANx3uAjb78p0NUSgnUw7dKTek+oLH1=NSz_CLps2T2SdWFmV0g@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Koert Kuipers <koert@tresata.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135e4620e504505121079ab
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135e4620e504505121079ab
Content-Type: text/plain; charset=UTF-8

i would like to use objectFile with some tweaks to the hadoop conf.
currently there is no way to do that, except recreating objectFile myself.
and some of the code objectFile uses i have no access to, since its private
to spark.


On Tue, Mar 24, 2015 at 2:59 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Yeah - to Nick's point, I think the way to do this is to pass in a
> custom conf when you create a Hadoop RDD (that's AFAIK why the conf
> field is there). Is there anything you can't do with that feature?
>
> On Tue, Mar 24, 2015 at 11:50 AM, Nick Pentreath
> <nick.pentreath@gmail.com> wrote:
> > Imran, on your point to read multiple files together in a partition, is
> it
> > not simpler to use the approach of copy Hadoop conf and set per-RDD
> > settings for min split to control the input size per partition, together
> > with something like CombineFileInputFormat?
> >
> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
> >
> >> I think this would be a great addition, I totally agree that you need
> to be
> >> able to set these at a finer context than just the SparkContext.
> >>
> >> Just to play devil's advocate, though -- the alternative is for you just
> >> subclass HadoopRDD yourself, or make a totally new RDD, and then you
> could
> >> expose whatever you need.  Why is this solution better?  IMO the
> criteria
> >> are:
> >> (a) common operations
> >> (b) error-prone / difficult to implement
> >> (c) non-obvious, but important for performance
> >>
> >> I think this case fits (a) & (c), so I think its still worthwhile.  But
> its
> >> also worth asking whether or not its too difficult for a user to extend
> >> HadoopRDD right now.  There have been several cases in the past week
> where
> >> we've suggested that a user should read from hdfs themselves (eg., to
> read
> >> multiple files together in one partition) -- with*out* reusing the code
> in
> >> HadoopRDD, though they would lose things like the metric tracking &
> >> preferred locations you get from HadoopRDD.  Does HadoopRDD need to some
> >> refactoring to make that easier to do?  Or do we just need a good
> example?
> >>
> >> Imran
> >>
> >> (sorry for hijacking your thread, Koert)
> >>
> >>
> >>
> >> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
> wrote:
> >>
> >> > see email below. reynold suggested i send it to dev instead of user
> >> >
> >> > ---------- Forwarded message ----------
> >> > From: Koert Kuipers <koert@tresata.com>
> >> > Date: Mon, Mar 23, 2015 at 4:36 PM
> >> > Subject: hadoop input/output format advanced control
> >> > To: "user@spark.apache.org" <user@spark.apache.org>
> >> >
> >> >
> >> > currently its pretty hard to control the Hadoop Input/Output formats
> used
> >> > in Spark. The conventions seems to be to add extra parameters to all
> >> > methods and then somewhere deep inside the code (for example in
> >> > PairRDDFunctions.saveAsHadoopFile) all these parameters get translated
> >> into
> >> > settings on the Hadoop Configuration object.
> >> >
> >> > for example for compression i see "codec: Option[Class[_ <:
> >> > CompressionCodec]] = None" added to a bunch of methods.
> >> >
> >> > how scalable is this solution really?
> >> >
> >> > for example i need to read from a hadoop dataset and i dont want the
> >> input
> >> > (part) files to get split up. the way to do this is to set
> >> > "mapred.min.split.size". now i dont want to set this at the level of
> the
> >> > SparkContext (which can be done), since i dont want it to apply to
> input
> >> > formats in general. i want it to apply to just this one specific input
> >> > dataset i need to read. which leaves me with no options currently. i
> >> could
> >> > go add yet another input parameter to all the methods
> >> > (SparkContext.textFile, SparkContext.hadoopFile,
> SparkContext.objectFile,
> >> > etc.). but that seems ineffective.
> >> >
> >> > why can we not expose a Map[String, String] or some other generic way
> to
> >> > manipulate settings for hadoop input/output formats? it would require
> >> > adding one more parameter to all methods to deal with hadoop
> input/output
> >> > formats, but after that its done. one parameter to rule them all....
> >> >
> >> > then i could do:
> >> > val x = sc.textFile("/some/path", formatSettings =
> >> > Map("mapred.min.split.size" -> "12345"))
> >> >
> >> > or
> >> > rdd.saveAsTextFile("/some/path, formatSettings =
> >> > Map(mapred.output.compress" -> "true",
> "mapred.output.compression.codec"
> >> ->
> >> > "somecodec"))
> >> >
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a1135e4620e504505121079ab--

From dev-return-12147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 22:47:56 2015
Return-Path: <dev-return-12147-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06EDA179F1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 22:47:56 +0000 (UTC)
Received: (qmail 4374 invoked by uid 500); 24 Mar 2015 22:47:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4296 invoked by uid 500); 24 Mar 2015 22:47:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4280 invoked by uid 99); 24 Mar 2015 22:47:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 22:47:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 22:47:29 +0000
Received: by wgdm6 with SMTP id m6so6790751wgd.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 15:46:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=fdPu19lGduGKaHWdHMrY/PAFl5j2qTKLHfGZ96xvkrg=;
        b=eKVJUrs4jUMrqhNVFuoTJYu+ReUEel04Qeq6narQxIhT5261kta4rxBM6rMZWC2emA
         +tKleKkRme8lpg4Ads1WwP6uTc+3G8LPUzs2KHGFieWIU8cfiOdN9rI4lZrV4MS8GmM3
         tmMHj1jqaGDCfzxRxToVaHnOT7l9cL20P+nSPUkBPFqEGO3mFcYvrXTZDPFsPsaElnak
         vNS7eRIB4qc3Ym5F0taGeGZeW5PXhECAMGtbJynY0rzsUQsZ5I55SQfy+mEj8k/w8UoT
         gII5jvTFiojLwMH6Q5oHseU4D1VpaSVQfa4F+HoQF+7NqfEHJipA9xBfQLN63FaulxbU
         HVaA==
X-Gm-Message-State: ALoCoQlL1rvdygc9yE55l1hn3WPmy68GZ/9sDP83kwfGGJcK0w7oGSR6OPbN89schBZpGQgbP0ER
MIME-Version: 1.0
X-Received: by 10.194.20.67 with SMTP id l3mr12062007wje.94.1427237179617;
 Tue, 24 Mar 2015 15:46:19 -0700 (PDT)
Received: by 10.28.47.193 with HTTP; Tue, 24 Mar 2015 15:46:19 -0700 (PDT)
X-Originating-IP: [204.148.13.62]
In-Reply-To: <CANx3uAjb78p0NUSgnUw7dKTek+oLH1=NSz_CLps2T2SdWFmV0g@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
	<CANx3uAjb78p0NUSgnUw7dKTek+oLH1=NSz_CLps2T2SdWFmV0g@mail.gmail.com>
Date: Tue, 24 Mar 2015 18:46:19 -0400
Message-ID: <CANx3uAiWX8k=55hXnjmUoL5V7JQYyNCcPhwOZgNk9s7pQp2WNg@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Koert Kuipers <koert@tresata.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5d649a1a168d0512108d93
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d649a1a168d0512108d93
Content-Type: text/plain; charset=UTF-8

the (compression) codec parameter that is now part of many saveAs...
methods came from a similar need. see SPARK-763
<https://issues.apache.org/jira/browse/SPARK-763>
hadoop has many options like this. you either going to have to allow many
more of these optional arguments to all the methods that read from hadoop
inputformats and write to hadoop outputformats, or you force people to
re-create these methods using HadoopRDD, i think (if thats even possible).

On Tue, Mar 24, 2015 at 6:40 PM, Koert Kuipers <koert@tresata.com> wrote:

> i would like to use objectFile with some tweaks to the hadoop conf.
> currently there is no way to do that, except recreating objectFile myself.
> and some of the code objectFile uses i have no access to, since its private
> to spark.
>
>
> On Tue, Mar 24, 2015 at 2:59 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Yeah - to Nick's point, I think the way to do this is to pass in a
>> custom conf when you create a Hadoop RDD (that's AFAIK why the conf
>> field is there). Is there anything you can't do with that feature?
>>
>> On Tue, Mar 24, 2015 at 11:50 AM, Nick Pentreath
>> <nick.pentreath@gmail.com> wrote:
>> > Imran, on your point to read multiple files together in a partition, is
>> it
>> > not simpler to use the approach of copy Hadoop conf and set per-RDD
>> > settings for min split to control the input size per partition, together
>> > with something like CombineFileInputFormat?
>> >
>> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
>> wrote:
>> >
>> >> I think this would be a great addition, I totally agree that you need
>> to be
>> >> able to set these at a finer context than just the SparkContext.
>> >>
>> >> Just to play devil's advocate, though -- the alternative is for you
>> just
>> >> subclass HadoopRDD yourself, or make a totally new RDD, and then you
>> could
>> >> expose whatever you need.  Why is this solution better?  IMO the
>> criteria
>> >> are:
>> >> (a) common operations
>> >> (b) error-prone / difficult to implement
>> >> (c) non-obvious, but important for performance
>> >>
>> >> I think this case fits (a) & (c), so I think its still worthwhile.
>> But its
>> >> also worth asking whether or not its too difficult for a user to extend
>> >> HadoopRDD right now.  There have been several cases in the past week
>> where
>> >> we've suggested that a user should read from hdfs themselves (eg., to
>> read
>> >> multiple files together in one partition) -- with*out* reusing the
>> code in
>> >> HadoopRDD, though they would lose things like the metric tracking &
>> >> preferred locations you get from HadoopRDD.  Does HadoopRDD need to
>> some
>> >> refactoring to make that easier to do?  Or do we just need a good
>> example?
>> >>
>> >> Imran
>> >>
>> >> (sorry for hijacking your thread, Koert)
>> >>
>> >>
>> >>
>> >> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
>> wrote:
>> >>
>> >> > see email below. reynold suggested i send it to dev instead of user
>> >> >
>> >> > ---------- Forwarded message ----------
>> >> > From: Koert Kuipers <koert@tresata.com>
>> >> > Date: Mon, Mar 23, 2015 at 4:36 PM
>> >> > Subject: hadoop input/output format advanced control
>> >> > To: "user@spark.apache.org" <user@spark.apache.org>
>> >> >
>> >> >
>> >> > currently its pretty hard to control the Hadoop Input/Output formats
>> used
>> >> > in Spark. The conventions seems to be to add extra parameters to all
>> >> > methods and then somewhere deep inside the code (for example in
>> >> > PairRDDFunctions.saveAsHadoopFile) all these parameters get
>> translated
>> >> into
>> >> > settings on the Hadoop Configuration object.
>> >> >
>> >> > for example for compression i see "codec: Option[Class[_ <:
>> >> > CompressionCodec]] = None" added to a bunch of methods.
>> >> >
>> >> > how scalable is this solution really?
>> >> >
>> >> > for example i need to read from a hadoop dataset and i dont want the
>> >> input
>> >> > (part) files to get split up. the way to do this is to set
>> >> > "mapred.min.split.size". now i dont want to set this at the level of
>> the
>> >> > SparkContext (which can be done), since i dont want it to apply to
>> input
>> >> > formats in general. i want it to apply to just this one specific
>> input
>> >> > dataset i need to read. which leaves me with no options currently. i
>> >> could
>> >> > go add yet another input parameter to all the methods
>> >> > (SparkContext.textFile, SparkContext.hadoopFile,
>> SparkContext.objectFile,
>> >> > etc.). but that seems ineffective.
>> >> >
>> >> > why can we not expose a Map[String, String] or some other generic
>> way to
>> >> > manipulate settings for hadoop input/output formats? it would require
>> >> > adding one more parameter to all methods to deal with hadoop
>> input/output
>> >> > formats, but after that its done. one parameter to rule them all....
>> >> >
>> >> > then i could do:
>> >> > val x = sc.textFile("/some/path", formatSettings =
>> >> > Map("mapred.min.split.size" -> "12345"))
>> >> >
>> >> > or
>> >> > rdd.saveAsTextFile("/some/path, formatSettings =
>> >> > Map(mapred.output.compress" -> "true",
>> "mapred.output.compression.codec"
>> >> ->
>> >> > "somecodec"))
>> >> >
>> >>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>>
>

--047d7b5d649a1a168d0512108d93--

From dev-return-12148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 23:17:44 2015
Return-Path: <dev-return-12148-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C31D617C74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 23:17:44 +0000 (UTC)
Received: (qmail 78929 invoked by uid 500); 24 Mar 2015 23:17:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78843 invoked by uid 500); 24 Mar 2015 23:17:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78829 invoked by uid 99); 24 Mar 2015 23:17:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 23:17:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jey@berkeley.edu designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 23:17:37 +0000
Received: by obcxo2 with SMTP id xo2so6695239obc.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 16:16:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:cc:content-type;
        bh=LHN7tDm5TWzk0WIcRgJfaXOLmk8BKVPtN/KDVzis7OU=;
        b=UG/LUg2UZ6yD9gboU8UKzlUJM4HRi3IdsOLFntuF+9ZeXyFNnDjOuviCJ96ZyTn+sd
         +C8oBbrRqIkcjMDeCRcDjfZmtzVSbqrtZO8q2TKjp9WhqvlkQmfOSAfbCvhEwpuieXuQ
         hyQ6wE0GguLsTJ2aNuFgbstzgFcZBB6cEJvPYwKAsQeazv6JAD6KNcdETXYtlMv7PIJj
         KZwIyhKx1RcG7FeqfFM/kA4GYYxgtgOvHiyAD22c4/KxuerZMUdoHhVTwIeV3h/vhgbY
         GWC2LJpIVLvYo1x4Q5lokV0MOL9J8oj7JNYezducBdrSfu8+SoUztQ4e99jOp4ZnHHyz
         KAzA==
X-Gm-Message-State: ALoCoQki1oFNnton0H32SRr+s1yjfNNjuSAqokOAujkmrZRaNgTsrmqKDwBrDojJ3NFF+8+wy3rh
MIME-Version: 1.0
X-Received: by 10.202.223.6 with SMTP id w6mr4754742oig.89.1427238992139; Tue,
 24 Mar 2015 16:16:32 -0700 (PDT)
Reply-To: jey@cs.berkeley.edu
Received: by 10.202.175.74 with HTTP; Tue, 24 Mar 2015 16:16:32 -0700 (PDT)
In-Reply-To: <6E82B1CC-FE2A-417B-9DF9-42482E324E4A@gmail.com>
References: <CABPQxstq5=2gv=Ov30gWzKW6jTxfHwXfCcrFXrcRd7y3bBGCKA@mail.gmail.com>
	<6E82B1CC-FE2A-417B-9DF9-42482E324E4A@gmail.com>
Date: Tue, 24 Mar 2015 16:16:32 -0700
Message-ID: <CAOYNe+PtqMpwzYR03ZjXhg+_n+CfGiro9CP4P8Hf3VfiNJ97OQ@mail.gmail.com>
Subject: Re: Experience using binary packages on various Hadoop distros
From: Jey Kottalam <jey@cs.berkeley.edu>
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: Patrick Wendell <pwendell@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Could we gracefully fallback to an in-tree Hadoop binary (e.g. 1.0.4)
in that case? I think many new Spark users are confused about why
Spark has anything to do with Hadoop, e.g. I could see myself being
confused when the download page asks me to select a "package type". I
know that what I want is not "source code", but I'd have no idea how
to choose amongst the apparently multiple types of binaries.

On Tue, Mar 24, 2015 at 2:28 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Just a note, one challenge with the BYOH version might be that users who download that can't run in local mode without also having Hadoop. But if we describe it correctly then hopefully it's okay.
>
> Matei
>
>> On Mar 24, 2015, at 3:05 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>> Hey All,
>>
>> For a while we've published binary packages with different Hadoop
>> client's pre-bundled. We currently have three interfaces to a Hadoop
>> cluster (a) the HDFS client (b) the YARN client (c) the Hive client.
>>
>> Because (a) and (b) are supposed to be backwards compatible
>> interfaces. My working assumption was that for the most part (modulo
>> Hive) our packages work with *newer* Hadoop versions. For instance,
>> our Hadoop 2.4 package should work with HDFS 2.6 and YARN 2.6.
>> However, I have heard murmurings that these are not compatible in
>> practice.
>>
>> So I have three questions I'd like to put out to the community:
>>
>> 1. Have people had difficulty using 2.4 packages with newer Hadoop
>> versions? If so, what specific incompatibilities have you hit?
>> 2. Have people had issues using our binary Hadoop packages in general
>> with commercial or Apache Hadoop distro's, such that you have to build
>> from source?
>> 3. How would people feel about publishing a "bring your own Hadoop"
>> binary, where you are required to point us to a local Hadoop
>> distribution by setting HADOOP_HOME? This might be better for ensuring
>> full compatibility:
>> https://issues.apache.org/jira/browse/SPARK-6511
>>
>> - Patrick
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 24 23:31:59 2015
Return-Path: <dev-return-12149-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6AC8C17DAA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 24 Mar 2015 23:31:59 +0000 (UTC)
Received: (qmail 13318 invoked by uid 500); 24 Mar 2015 23:31:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13243 invoked by uid 500); 24 Mar 2015 23:31:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13231 invoked by uid 99); 24 Mar 2015 23:31:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 23:31:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 24 Mar 2015 23:31:33 +0000
Received: by oifl3 with SMTP id l3so7833094oif.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 16:30:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=afjdamsEWEXHSUzw6uAr3XeUWn33m5gH4H4CiRUyr2E=;
        b=lhJ1+pYs4BEKpZBf6cMhDRtPLCkOr7TwvaoV34wzwOZS8VDb3REDsdQtKYWTlkY01q
         k9tbaytGgNGm0DReDyjvUH226QsRK+/tm1RhGVCwRvJSpIETmrOdxd/65NsjpVqQUzhL
         KVkciAYyn6SIEL+8xMNP2HTs9i1T+pAN1w17Fcc3pEAJwPr9mhgZjty50xHJV/vEsCnT
         Vh4oCzylmMrNCnJLpF3ud6IIaVOKAHF+/A6FiCkCwtxefcReXOvXyaksH8pGV3S34jGl
         jxjC7A/9kGONb5TpZ1EVkaJOUhD8KJ/RymoJR9zwKECGWzRScPaQBmYY6cRznBM7BQyx
         IbBQ==
MIME-Version: 1.0
X-Received: by 10.60.45.165 with SMTP id o5mr5308109oem.44.1427239846629; Tue,
 24 Mar 2015 16:30:46 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Tue, 24 Mar 2015 16:30:46 -0700 (PDT)
In-Reply-To: <CAOYNe+PtqMpwzYR03ZjXhg+_n+CfGiro9CP4P8Hf3VfiNJ97OQ@mail.gmail.com>
References: <CABPQxstq5=2gv=Ov30gWzKW6jTxfHwXfCcrFXrcRd7y3bBGCKA@mail.gmail.com>
	<6E82B1CC-FE2A-417B-9DF9-42482E324E4A@gmail.com>
	<CAOYNe+PtqMpwzYR03ZjXhg+_n+CfGiro9CP4P8Hf3VfiNJ97OQ@mail.gmail.com>
Date: Tue, 24 Mar 2015 16:30:46 -0700
Message-ID: <CABPQxss4ARwa+7oZvao8rx-wORsgr4m-bre8xd91giHrp+g-Zg@mail.gmail.com>
Subject: Re: Experience using binary packages on various Hadoop distros
From: Patrick Wendell <pwendell@gmail.com>
To: Jey Kottalam <jey@cs.berkeley.edu>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

We can probably better explain that if you are not using HDFS or YARN,
you can download any binary.

However, my question was about if the existing binaries do not work
well with newer Hadoop versions, which I heard some people suggest but
I'm looking for more specific issues.

On Tue, Mar 24, 2015 at 4:16 PM, Jey Kottalam <jey@cs.berkeley.edu> wrote:
> Could we gracefully fallback to an in-tree Hadoop binary (e.g. 1.0.4)
> in that case? I think many new Spark users are confused about why
> Spark has anything to do with Hadoop, e.g. I could see myself being
> confused when the download page asks me to select a "package type". I
> know that what I want is not "source code", but I'd have no idea how
> to choose amongst the apparently multiple types of binaries.
>
> On Tue, Mar 24, 2015 at 2:28 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
>> Just a note, one challenge with the BYOH version might be that users who download that can't run in local mode without also having Hadoop. But if we describe it correctly then hopefully it's okay.
>>
>> Matei
>>
>>> On Mar 24, 2015, at 3:05 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>
>>> Hey All,
>>>
>>> For a while we've published binary packages with different Hadoop
>>> client's pre-bundled. We currently have three interfaces to a Hadoop
>>> cluster (a) the HDFS client (b) the YARN client (c) the Hive client.
>>>
>>> Because (a) and (b) are supposed to be backwards compatible
>>> interfaces. My working assumption was that for the most part (modulo
>>> Hive) our packages work with *newer* Hadoop versions. For instance,
>>> our Hadoop 2.4 package should work with HDFS 2.6 and YARN 2.6.
>>> However, I have heard murmurings that these are not compatible in
>>> practice.
>>>
>>> So I have three questions I'd like to put out to the community:
>>>
>>> 1. Have people had difficulty using 2.4 packages with newer Hadoop
>>> versions? If so, what specific incompatibilities have you hit?
>>> 2. Have people had issues using our binary Hadoop packages in general
>>> with commercial or Apache Hadoop distro's, such that you have to build
>>> from source?
>>> 3. How would people feel about publishing a "bring your own Hadoop"
>>> binary, where you are required to point us to a local Hadoop
>>> distribution by setting HADOOP_HOME? This might be better for ensuring
>>> full compatibility:
>>> https://issues.apache.org/jira/browse/SPARK-6511
>>>
>>> - Patrick
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 01:59:26 2015
Return-Path: <dev-return-12150-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 003A9173AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 01:59:26 +0000 (UTC)
Received: (qmail 13128 invoked by uid 500); 25 Mar 2015 01:59:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13054 invoked by uid 500); 25 Mar 2015 01:59:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13043 invoked by uid 99); 25 Mar 2015 01:59:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 01:59:23 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 01:58:54 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id 191ED7E;
	Wed, 25 Mar 2015 01:58:20 +0000 (UTC)
Received: from G4W6301.americas.hpqcorp.net (16.210.26.226) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Wed, 25 Mar 2015 01:56:53 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G4W6301.americas.hpqcorp.net ([16.210.26.226]) with mapi id 14.03.0169.001;
 Wed, 25 Mar 2015 01:56:53 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Sam Halliday <sam.halliday@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, Xiangrui Meng
	<mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, "Evan R. Sparks"
	<evan.sparks@gmail.com>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAvQQdDA=
Date: Wed, 25 Mar 2015 01:56:52 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
In-Reply-To: <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.192.234]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7G4W3292americas_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7G4W3292americas_--

From dev-return-12151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 02:36:25 2015
Return-Path: <dev-return-12151-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8896D177EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 02:36:25 +0000 (UTC)
Received: (qmail 70428 invoked by uid 500); 25 Mar 2015 02:36:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70340 invoked by uid 500); 25 Mar 2015 02:36:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70328 invoked by uid 99); 25 Mar 2015 02:36:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 02:36:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sai.sai.shao@gmail.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 02:36:19 +0000
Received: by iedfl3 with SMTP id fl3so14131525ied.1
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 19:35:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=xRhBMimaLS3yAMcTsIMCiK2CeQbAEeB/Rd+2ANAscKE=;
        b=p4HMTa2EDXr91/wn1H7NPsMXvYPscD78Jia3O7BTy5mcyVE3uzGPLlYTrltcnNf8HI
         ryskIvKD856ncZTyw1bnh5v7tIR/RPIkKl4GgLseuG34TiQ7v6aqXAS8nQWu3Ljt8WfT
         jqwRsovjtBAs+oFwfZwScmIPWjTJCUYkw87EyqkmhLmAtv+LU3GNmupigMC7rVSAabFq
         Lpcgyf2ziyNqb/5ct/PNejw/77v5L4HygrawJdYEoyiJc6dtKB5uHVClelMMYvPRwBVU
         ZdDP8CugnQvkWaG8oWsD/d9sGcLgs5nTcfFB6C/6YAHc9tzRQxWUhp6Hle1VUKQwDznn
         nHuQ==
MIME-Version: 1.0
X-Received: by 10.107.10.82 with SMTP id u79mr11225236ioi.65.1427250923999;
 Tue, 24 Mar 2015 19:35:23 -0700 (PDT)
Received: by 10.36.117.86 with HTTP; Tue, 24 Mar 2015 19:35:23 -0700 (PDT)
In-Reply-To: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
Date: Wed, 25 Mar 2015 10:35:23 +0800
Message-ID: <CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
Subject: Re: Understanding shuffle file name conflicts
From: Saisai Shao <sai.sai.shao@gmail.com>
To: Kannan Rajah <krajah@maprtech.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113f9b4a5498f6051213c0ae
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f9b4a5498f6051213c0ae
Content-Type: text/plain; charset=UTF-8

Hi Kannan,

As I know the shuffle Id in ShuffleDependency will be increased, so even if
you run the same job twice, the shuffle dependency as well as shuffle id is
different, so the shuffle file name which is combined by
(shuffleId+mapId+reduceId) will be changed, so there's no name conflict
even in the same directory as I know.

Thanks
Jerry


2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:

> I am working on SPARK-1529. I ran into an issue with my change, where the
> same shuffle file was being reused across 2 jobs. Please note this only
> happens when I use a hard coded location to use for shuffle files, say
> "/tmp". It does not happen with normal code path that uses DiskBlockManager
> to pick different directories for each run. So I want to understand how
> DiskBlockManager guarantees that such a conflict will never happen.
>
> Let's say the shuffle block id has a value of shuffle_0_0_0. So the data
> file name is shuffle_0_0_0.data and index file name is shuffle_0_0_0.index.
> If I run a spark job twice, one after another, these files get created
> under different directories because of the hashing logic in
> DiskBlockManager. But the hash is based off the file name, so how are we
> sure that there won't be a conflict ever?
>
> --
> Kannan
>

--001a113f9b4a5498f6051213c0ae--

From dev-return-12152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 03:48:24 2015
Return-Path: <dev-return-12152-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1024717C35
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 03:48:24 +0000 (UTC)
Received: (qmail 77083 invoked by uid 500); 25 Mar 2015 03:48:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76998 invoked by uid 500); 25 Mar 2015 03:48:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76986 invoked by uid 99); 25 Mar 2015 03:48:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 03:48:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of z.w.chan.jason@gmail.com designates 209.85.223.173 as permitted sender)
Received: from [209.85.223.173] (HELO mail-ie0-f173.google.com) (209.85.223.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 03:47:56 +0000
Received: by ieclw3 with SMTP id lw3so13340749iec.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 20:46:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=ZohLKqcn/DI2hKl7Wj5vqhFVgrcToxsYGKf5f6N6Bfw=;
        b=lF8SsxE5qOuN8M9JNAWnhP7al53wCLt5O+jT61vPVrvNF2/A3ATbhn9+sjVffXnq44
         I0m+Ed/lir9rkUVjT6Uzcr/fAsSgsj1g5yf/1MuWsiiW868Iuri7WTY0BJL4ARzac0IO
         2X1y4DIBqP0mJERbfd4MU/XdvweRrAppf0MYBy4YDul2QuO2k9q3yfYNymGgzoqQlQ9a
         CArna/82ZmT/hg/2ndFkG74ouM10XxmAVOMwIA3jX3e+mayrp9MMq7aVZv8BhiXDUlYx
         NQ/wylJxOgXWlA1J8SOeLbTrhYMECGlj9qi0413wukZmYSEQYgwA8JfH1e2DtVQJ+1yP
         GeLQ==
MIME-Version: 1.0
X-Received: by 10.107.16.31 with SMTP id y31mr11133453ioi.53.1427255184128;
 Tue, 24 Mar 2015 20:46:24 -0700 (PDT)
Received: by 10.107.48.73 with HTTP; Tue, 24 Mar 2015 20:46:24 -0700 (PDT)
Date: Wed, 25 Mar 2015 11:46:24 +0800
Message-ID: <CAMv5TiCNDiZ8PAx97orKVv=KVD58DLYfbkCTbModRj075heE6w@mail.gmail.com>
Subject: Spark SQL(1.3.0) "import sqlContext.implicits._" seems not work for
 converting a case class RDD to DataFrame
From: Zhiwei Chan <z.w.chan.jason@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113f1f164102ee051214be77
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f1f164102ee051214be77
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi all,

  I just upgraded spark from 1.2.1 to 1.3.0, and changed the "import
sqlContext.createSchemaRDD" to "import sqlContext.implicits._" in my code.
(I scan the programming guide and it seems this is the only change I need
to do). But it come to an error when run compile as following:
>>>
[ERROR] ...\magic.scala:527: error: value registerTempTable is not a member
of org.apache.spark.rdd.RDD[com.yhd.ycache.magic.Table]
[INFO]     tableRdd.registerTempTable(tableName)
<<<

Then I try the exactly example in the programming guide of 1.3  in
spark-shell, it come to the same error.
>>>
scala> sys.env.get("CLASSPATH")
res7: Option[String] =3D
Some(:/root/scala/spark-1.3.0-bin-hadoop2.4/conf:/root/scala/spark-1.3.0-bi=
n-hadoop2.4/lib/spark-assembly-1.3.0-hadoop2.4.0.jar:/root/scala/spark-1.3.=
0-bin-hadoop2.4/lib/datanucleus-core-3.2.10.jar:/root/scala/spark-1.3.0-bin=
-hadoop2.4/lib/datanucleus-rdbms-3.2.9.jar:/root/scala/spark-1.3.0-bin-hado=
op2.4/lib/datanucleus-api-jdo-3.2.6.jar)

scala>  val sqlContext =3D new org.apache.spark.sql.SQLContext(sc)
sqlContext: org.apache.spark.sql.SQLContext =3D
org.apache.spark.sql.SQLContext@4b05b3ff

scala>  import sqlContext.implicits._
import sqlContext.implicits._

scala>  case class Person(name: String, age: Int)
defined class Person

scala>   val t1 =3D
sc.textFile("hdfs://heju:8020/user/root/magic/poolInfo.txt")
15/03/25 11:13:35 INFO MemoryStore: ensureFreeSpace(81443) called with
curMem=3D186397, maxMem=3D278302556
15/03/25 11:13:35 INFO MemoryStore: Block broadcast_3 stored as values in
memory (estimated size 79.5 KB, free 265.2 MB)
15/03/25 11:13:35 INFO MemoryStore: ensureFreeSpace(31262) called with
curMem=3D267840, maxMem=3D278302556
15/03/25 11:13:35 INFO MemoryStore: Block broadcast_3_piece0 stored as
bytes in memory (estimated size 30.5 KB, free 265.1 MB)
15/03/25 11:13:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory
on heju:48885 (size: 30.5 KB, free: 265.4 MB)
15/03/25 11:13:35 INFO BlockManagerMaster: Updated info of block
broadcast_3_piece0
15/03/25 11:13:35 INFO SparkContext: Created broadcast 3 from textFile at
<console>:34
t1: org.apache.spark.rdd.RDD[String] =3D
hdfs://heju:8020/user/root/magic/poolInfo.txt MapPartitionsRDD[9] at
textFile at <console>:34

scala>  val t2 =3D t1.flatMap(_.split("\n")).map(_.split(" ")).map(p =3D>
Person(p(0),1))
t2: org.apache.spark.rdd.RDD[Person] =3D MapPartitionsRDD[12] at map at
<console>:38

scala>  t2.registerTempTable("people")
<console>:41: error: value registerTempTable is not a member of
org.apache.spark.rdd.RDD[Person]
               t2.registerTempTable("people")
                  ^
<<<

I found the following explanation in programming guide about implicit
convert case class to DataFrams, but I don't understand what I should do.
Could any one tell me how should I do if I want to convert a case class RDD
to DataFrame?

>>>
Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)

Many of the code examples prior to Spark 1.3 started with import
sqlContext._, which brought all of the functions from sqlContext into
scope. In Spark 1.3 we have isolated the implicit conversions for
converting RDDs into DataFrames into an object inside of the SQLContext.
Users should now write import sqlContext.implicits._.

Additionally, the implicit conversions now only augment RDDs that are
composed of Products (i.e., case classes or tuples) with a method toDF,
instead of applying automatically.

<<<
Thanks
Jason

--001a113f1f164102ee051214be77--

From dev-return-12153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 04:08:23 2015
Return-Path: <dev-return-12153-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B080717DB4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 04:08:23 +0000 (UTC)
Received: (qmail 20330 invoked by uid 500); 25 Mar 2015 04:08:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20239 invoked by uid 500); 25 Mar 2015 04:08:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20217 invoked by uid 99); 25 Mar 2015 04:08:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 04:08:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.223.171 as permitted sender)
Received: from [209.85.223.171] (HELO mail-ie0-f171.google.com) (209.85.223.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 04:08:16 +0000
Received: by ieclw3 with SMTP id lw3so13573888iec.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 21:07:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=f67WCSoAvoSWIE5tnwetcaMpXv2CKkQYrEp4nmYnRgA=;
        b=0lLAxzYV47ejnBHEth4YRXH9qqd5r/quGpckoCUocBRp4yLp4LY6DyAn4n9e2gGnWI
         TJGtV7iMQWHwSneCaynQT9Zk719d4TsAMq4+s4V/uQLGCB0IR+HGCeIvax/w5nqLxmC6
         RQsWTdQgBDDDsAhSUPo38Ft8Utop23hM/5lLCWIbUSHSF8+UePricZHqci79K53497RJ
         WAZyZkOupchfcdVKlOkz/6ZoBdqnGD7P+BkaMWb4/w2AFlo7AaksN8AYuC/sof1KyvQR
         UUYdDFMzSQft5xK3RNVjVZNvSrOFVkDlyGE8XkiP4m9MspxVnPizamcQ8+uZVPedueRU
         dBYw==
MIME-Version: 1.0
X-Received: by 10.107.16.31 with SMTP id y31mr11204891ioi.53.1427256430963;
 Tue, 24 Mar 2015 21:07:10 -0700 (PDT)
Received: by 10.36.53.148 with HTTP; Tue, 24 Mar 2015 21:07:10 -0700 (PDT)
In-Reply-To: <CAMv5TiCNDiZ8PAx97orKVv=KVD58DLYfbkCTbModRj075heE6w@mail.gmail.com>
References: <CAMv5TiCNDiZ8PAx97orKVv=KVD58DLYfbkCTbModRj075heE6w@mail.gmail.com>
Date: Tue, 24 Mar 2015 21:07:10 -0700
Message-ID: <CALte62wS8GGP7dV+WM_xH1Lht6OQPs62THTAfWRFCy3eFhGiiQ@mail.gmail.com>
Subject: Re: Spark SQL(1.3.0) "import sqlContext.implicits._" seems not work
 for converting a case class RDD to DataFrame
From: Ted Yu <yuzhihong@gmail.com>
To: Zhiwei Chan <z.w.chan.jason@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113f1f169233c9051215083a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f1f169233c9051215083a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Please take a look at:
./sql/core/src/main/scala/org/apache/spark/sql/DataFrameHolder.scala
./sql/core/src/main/scala/org/apache/spark/sql/GroupedData.scala

Cheers

On Tue, Mar 24, 2015 at 8:46 PM, Zhiwei Chan <z.w.chan.jason@gmail.com>
wrote:

> Hi all,
>
>   I just upgraded spark from 1.2.1 to 1.3.0, and changed the "import
> sqlContext.createSchemaRDD" to "import sqlContext.implicits._" in my code=
.
> (I scan the programming guide and it seems this is the only change I need
> to do). But it come to an error when run compile as following:
> >>>
> [ERROR] ...\magic.scala:527: error: value registerTempTable is not a memb=
er
> of org.apache.spark.rdd.RDD[com.yhd.ycache.magic.Table]
> [INFO]     tableRdd.registerTempTable(tableName)
> <<<
>
> Then I try the exactly example in the programming guide of 1.3  in
> spark-shell, it come to the same error.
> >>>
> scala> sys.env.get("CLASSPATH")
> res7: Option[String] =3D
>
> Some(:/root/scala/spark-1.3.0-bin-hadoop2.4/conf:/root/scala/spark-1.3.0-=
bin-hadoop2.4/lib/spark-assembly-1.3.0-hadoop2.4.0.jar:/root/scala/spark-1.=
3.0-bin-hadoop2.4/lib/datanucleus-core-3.2.10.jar:/root/scala/spark-1.3.0-b=
in-hadoop2.4/lib/datanucleus-rdbms-3.2.9.jar:/root/scala/spark-1.3.0-bin-ha=
doop2.4/lib/datanucleus-api-jdo-3.2.6.jar)
>
> scala>  val sqlContext =3D new org.apache.spark.sql.SQLContext(sc)
> sqlContext: org.apache.spark.sql.SQLContext =3D
> org.apache.spark.sql.SQLContext@4b05b3ff
>
> scala>  import sqlContext.implicits._
> import sqlContext.implicits._
>
> scala>  case class Person(name: String, age: Int)
> defined class Person
>
> scala>   val t1 =3D
> sc.textFile("hdfs://heju:8020/user/root/magic/poolInfo.txt")
> 15/03/25 11:13:35 INFO MemoryStore: ensureFreeSpace(81443) called with
> curMem=3D186397, maxMem=3D278302556
> 15/03/25 11:13:35 INFO MemoryStore: Block broadcast_3 stored as values in
> memory (estimated size 79.5 KB, free 265.2 MB)
> 15/03/25 11:13:35 INFO MemoryStore: ensureFreeSpace(31262) called with
> curMem=3D267840, maxMem=3D278302556
> 15/03/25 11:13:35 INFO MemoryStore: Block broadcast_3_piece0 stored as
> bytes in memory (estimated size 30.5 KB, free 265.1 MB)
> 15/03/25 11:13:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in memo=
ry
> on heju:48885 (size: 30.5 KB, free: 265.4 MB)
> 15/03/25 11:13:35 INFO BlockManagerMaster: Updated info of block
> broadcast_3_piece0
> 15/03/25 11:13:35 INFO SparkContext: Created broadcast 3 from textFile at
> <console>:34
> t1: org.apache.spark.rdd.RDD[String] =3D
> hdfs://heju:8020/user/root/magic/poolInfo.txt MapPartitionsRDD[9] at
> textFile at <console>:34
>
> scala>  val t2 =3D t1.flatMap(_.split("\n")).map(_.split(" ")).map(p =3D>
> Person(p(0),1))
> t2: org.apache.spark.rdd.RDD[Person] =3D MapPartitionsRDD[12] at map at
> <console>:38
>
> scala>  t2.registerTempTable("people")
> <console>:41: error: value registerTempTable is not a member of
> org.apache.spark.rdd.RDD[Person]
>                t2.registerTempTable("people")
>                   ^
> <<<
>
> I found the following explanation in programming guide about implicit
> convert case class to DataFrams, but I don't understand what I should do.
> Could any one tell me how should I do if I want to convert a case class R=
DD
> to DataFrame?
>
> >>>
> Isolation of Implicit Conversions and Removal of dsl Package (Scala-only)
>
> Many of the code examples prior to Spark 1.3 started with import
> sqlContext._, which brought all of the functions from sqlContext into
> scope. In Spark 1.3 we have isolated the implicit conversions for
> converting RDDs into DataFrames into an object inside of the SQLContext.
> Users should now write import sqlContext.implicits._.
>
> Additionally, the implicit conversions now only augment RDDs that are
> composed of Products (i.e., case classes or tuples) with a method toDF,
> instead of applying automatically.
>
> <<<
> Thanks
> Jason
>

--001a113f1f169233c9051215083a--

From dev-return-12154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 04:22:37 2015
Return-Path: <dev-return-12154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6641117F9B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 04:22:37 +0000 (UTC)
Received: (qmail 42246 invoked by uid 500); 25 Mar 2015 04:21:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42176 invoked by uid 500); 25 Mar 2015 04:21:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42165 invoked by uid 99); 25 Mar 2015 04:21:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 04:21:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 04:21:11 +0000
Received: by qgh3 with SMTP id 3so13971705qgh.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 21:20:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=81rfiXGYznN/ENKNoCbwRQEFYQAs7k1naE9jSxAo1o0=;
        b=Kqxe6GZdPkYr04ABZpGEtWVgiV/kSAwNDdMLhhVwRjaz4YjQEanUfMEAYlQCO3QISd
         kNX8zEwBIHivFuSqa3kS1IILCUkE6COuQ2nSsWsyAMGwdjZMlh14rdhfN3E3IP7vxupV
         kdMTYFpTpm1Cc7TJQwagy/ejNWnBrZZ3+wuUvhOblnKg8mH0xi47n5JHJMw0VfeoL/8m
         V9N3nQAxcn0X8yQvx/b6vROQ20SZ4WvW7mWBkO56lGUjF/Zj5fvgouc5enqS6OKYlT5W
         cOEhzfVl37jmuZIayZab6ib45yWHNDWaaNM6WKN0pItDEDkXFRenBw2kp+1eD6S62mxE
         2oyg==
X-Gm-Message-State: ALoCoQkKxU7yo4HKuB1V9z6TMIpjaHDVPHL7JVDuWbOqokySt6/SqieT1qN+YiADCkXEStfHSIj7
X-Received: by 10.229.68.136 with SMTP id v8mr10202826qci.16.1427257248064;
 Tue, 24 Mar 2015 21:20:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Tue, 24 Mar 2015 21:20:27 -0700 (PDT)
In-Reply-To: <CALte62wS8GGP7dV+WM_xH1Lht6OQPs62THTAfWRFCy3eFhGiiQ@mail.gmail.com>
References: <CAMv5TiCNDiZ8PAx97orKVv=KVD58DLYfbkCTbModRj075heE6w@mail.gmail.com>
 <CALte62wS8GGP7dV+WM_xH1Lht6OQPs62THTAfWRFCy3eFhGiiQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 24 Mar 2015 21:20:27 -0700
Message-ID: <CAPh_B=bnYghAOTnoHfst-ri+nB6_zpO+x3qP5=M3hVa+MTQu1A@mail.gmail.com>
Subject: Re: Spark SQL(1.3.0) "import sqlContext.implicits._" seems not work
 for converting a case class RDD to DataFrame
To: Ted Yu <yuzhihong@gmail.com>
Cc: Zhiwei Chan <z.w.chan.jason@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133984446407305121539ce
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133984446407305121539ce
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

In particular:

http://spark.apache.org/docs/latest/sql-programming-guide.html


"Additionally, the implicit conversions now only augment RDDs that are
composed of Products (i.e., case classes or tuples) with a method toDF,
instead of applying automatically."



On Tue, Mar 24, 2015 at 9:07 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> Please take a look at:
> ./sql/core/src/main/scala/org/apache/spark/sql/DataFrameHolder.scala
> ./sql/core/src/main/scala/org/apache/spark/sql/GroupedData.scala
>
> Cheers
>
> On Tue, Mar 24, 2015 at 8:46 PM, Zhiwei Chan <z.w.chan.jason@gmail.com>
> wrote:
>
> > Hi all,
> >
> >   I just upgraded spark from 1.2.1 to 1.3.0, and changed the "import
> > sqlContext.createSchemaRDD" to "import sqlContext.implicits._" in my
> code.
> > (I scan the programming guide and it seems this is the only change I ne=
ed
> > to do). But it come to an error when run compile as following:
> > >>>
> > [ERROR] ...\magic.scala:527: error: value registerTempTable is not a
> member
> > of org.apache.spark.rdd.RDD[com.yhd.ycache.magic.Table]
> > [INFO]     tableRdd.registerTempTable(tableName)
> > <<<
> >
> > Then I try the exactly example in the programming guide of 1.3  in
> > spark-shell, it come to the same error.
> > >>>
> > scala> sys.env.get("CLASSPATH")
> > res7: Option[String] =3D
> >
> >
> Some(:/root/scala/spark-1.3.0-bin-hadoop2.4/conf:/root/scala/spark-1.3.0-=
bin-hadoop2.4/lib/spark-assembly-1.3.0-hadoop2.4.0.jar:/root/scala/spark-1.=
3.0-bin-hadoop2.4/lib/datanucleus-core-3.2.10.jar:/root/scala/spark-1.3.0-b=
in-hadoop2.4/lib/datanucleus-rdbms-3.2.9.jar:/root/scala/spark-1.3.0-bin-ha=
doop2.4/lib/datanucleus-api-jdo-3.2.6.jar)
> >
> > scala>  val sqlContext =3D new org.apache.spark.sql.SQLContext(sc)
> > sqlContext: org.apache.spark.sql.SQLContext =3D
> > org.apache.spark.sql.SQLContext@4b05b3ff
> >
> > scala>  import sqlContext.implicits._
> > import sqlContext.implicits._
> >
> > scala>  case class Person(name: String, age: Int)
> > defined class Person
> >
> > scala>   val t1 =3D
> > sc.textFile("hdfs://heju:8020/user/root/magic/poolInfo.txt")
> > 15/03/25 11:13:35 INFO MemoryStore: ensureFreeSpace(81443) called with
> > curMem=3D186397, maxMem=3D278302556
> > 15/03/25 11:13:35 INFO MemoryStore: Block broadcast_3 stored as values =
in
> > memory (estimated size 79.5 KB, free 265.2 MB)
> > 15/03/25 11:13:35 INFO MemoryStore: ensureFreeSpace(31262) called with
> > curMem=3D267840, maxMem=3D278302556
> > 15/03/25 11:13:35 INFO MemoryStore: Block broadcast_3_piece0 stored as
> > bytes in memory (estimated size 30.5 KB, free 265.1 MB)
> > 15/03/25 11:13:35 INFO BlockManagerInfo: Added broadcast_3_piece0 in
> memory
> > on heju:48885 (size: 30.5 KB, free: 265.4 MB)
> > 15/03/25 11:13:35 INFO BlockManagerMaster: Updated info of block
> > broadcast_3_piece0
> > 15/03/25 11:13:35 INFO SparkContext: Created broadcast 3 from textFile =
at
> > <console>:34
> > t1: org.apache.spark.rdd.RDD[String] =3D
> > hdfs://heju:8020/user/root/magic/poolInfo.txt MapPartitionsRDD[9] at
> > textFile at <console>:34
> >
> > scala>  val t2 =3D t1.flatMap(_.split("\n")).map(_.split(" ")).map(p =
=3D>
> > Person(p(0),1))
> > t2: org.apache.spark.rdd.RDD[Person] =3D MapPartitionsRDD[12] at map at
> > <console>:38
> >
> > scala>  t2.registerTempTable("people")
> > <console>:41: error: value registerTempTable is not a member of
> > org.apache.spark.rdd.RDD[Person]
> >                t2.registerTempTable("people")
> >                   ^
> > <<<
> >
> > I found the following explanation in programming guide about implicit
> > convert case class to DataFrams, but I don't understand what I should d=
o.
> > Could any one tell me how should I do if I want to convert a case class
> RDD
> > to DataFrame?
> >
> > >>>
> > Isolation of Implicit Conversions and Removal of dsl Package (Scala-onl=
y)
> >
> > Many of the code examples prior to Spark 1.3 started with import
> > sqlContext._, which brought all of the functions from sqlContext into
> > scope. In Spark 1.3 we have isolated the implicit conversions for
> > converting RDDs into DataFrames into an object inside of the SQLContext=
.
> > Users should now write import sqlContext.implicits._.
> >
> > Additionally, the implicit conversions now only augment RDDs that are
> > composed of Products (i.e., case classes or tuples) with a method toDF,
> > instead of applying automatically.
> >
> > <<<
> > Thanks
> > Jason
> >
>

--001a1133984446407305121539ce--

From dev-return-12155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 05:25:15 2015
Return-Path: <dev-return-12155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9A1E1734E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 05:25:15 +0000 (UTC)
Received: (qmail 25915 invoked by uid 500); 25 Mar 2015 05:25:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25831 invoked by uid 500); 25 Mar 2015 05:25:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25819 invoked by uid 99); 25 Mar 2015 05:25:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 05:25:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 05:25:00 +0000
Received: by obcjt1 with SMTP id jt1so11310989obc.2
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 22:23:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=x5/xMHGq1n34SWuO+ySFywFUcN33VlI+IYOfUU5RdK8=;
        b=YZ9tSYdmhfIItRzN0UbWjBrKwqYTKu2c0HPqAkvkQG/xA7dGaCJiKy1RhIuA6/Ld6b
         gGFI+MaRkKyMC6bsmoho73HGYbbfnvoS2BYOFY9g04pN3J1xeT+BTt/KaThQDDLtRA9Q
         VCVD/Vjv9v1KqDZP9LAdbDvNlizJk2HLVYl15fnEBLOPZSZZPYJKcTEdTvb6VdxEHg0n
         y5zNdMMIHbQChgh3iKAtEpYaWjp7FgeQ5zNa8lidFbnzuhOvJMAAQFiDTfrGQW8pUJZo
         GpORvjz2ckzsDZ/l7aXSUeFqxCT5cani/GwRwItrAjUXq6Kq5ihLK/wxgvMmkjOwWChk
         omlw==
MIME-Version: 1.0
X-Received: by 10.202.96.69 with SMTP id u66mr5727810oib.3.1427261035061; Tue,
 24 Mar 2015 22:23:55 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Tue, 24 Mar 2015 22:23:54 -0700 (PDT)
In-Reply-To: <CANx3uAiWX8k=55hXnjmUoL5V7JQYyNCcPhwOZgNk9s7pQp2WNg@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
	<CANx3uAjb78p0NUSgnUw7dKTek+oLH1=NSz_CLps2T2SdWFmV0g@mail.gmail.com>
	<CANx3uAiWX8k=55hXnjmUoL5V7JQYyNCcPhwOZgNk9s7pQp2WNg@mail.gmail.com>
Date: Tue, 24 Mar 2015 22:23:54 -0700
Message-ID: <CABPQxsu4i7ae_FaR99ASXfUgFmGp27MB4s8t9v46gB0TZUMCfg@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Patrick Wendell <pwendell@gmail.com>
To: Koert Kuipers <koert@tresata.com>
Cc: Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I see - if you look, in the saving functions we have the option for
the user to pass an arbitrary Configuration.

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L894

It seems fine to have the same option for the loading functions, if
it's easy to just pass this config into the input format.



On Tue, Mar 24, 2015 at 3:46 PM, Koert Kuipers <koert@tresata.com> wrote:
> the (compression) codec parameter that is now part of many saveAs... methods
> came from a similar need. see SPARK-763
> hadoop has many options like this. you either going to have to allow many
> more of these optional arguments to all the methods that read from hadoop
> inputformats and write to hadoop outputformats, or you force people to
> re-create these methods using HadoopRDD, i think (if thats even possible).
>
> On Tue, Mar 24, 2015 at 6:40 PM, Koert Kuipers <koert@tresata.com> wrote:
>>
>> i would like to use objectFile with some tweaks to the hadoop conf.
>> currently there is no way to do that, except recreating objectFile myself.
>> and some of the code objectFile uses i have no access to, since its private
>> to spark.
>>
>>
>> On Tue, Mar 24, 2015 at 2:59 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>>
>>> Yeah - to Nick's point, I think the way to do this is to pass in a
>>> custom conf when you create a Hadoop RDD (that's AFAIK why the conf
>>> field is there). Is there anything you can't do with that feature?
>>>
>>> On Tue, Mar 24, 2015 at 11:50 AM, Nick Pentreath
>>> <nick.pentreath@gmail.com> wrote:
>>> > Imran, on your point to read multiple files together in a partition, is
>>> > it
>>> > not simpler to use the approach of copy Hadoop conf and set per-RDD
>>> > settings for min split to control the input size per partition,
>>> > together
>>> > with something like CombineFileInputFormat?
>>> >
>>> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
>>> > wrote:
>>> >
>>> >> I think this would be a great addition, I totally agree that you need
>>> >> to be
>>> >> able to set these at a finer context than just the SparkContext.
>>> >>
>>> >> Just to play devil's advocate, though -- the alternative is for you
>>> >> just
>>> >> subclass HadoopRDD yourself, or make a totally new RDD, and then you
>>> >> could
>>> >> expose whatever you need.  Why is this solution better?  IMO the
>>> >> criteria
>>> >> are:
>>> >> (a) common operations
>>> >> (b) error-prone / difficult to implement
>>> >> (c) non-obvious, but important for performance
>>> >>
>>> >> I think this case fits (a) & (c), so I think its still worthwhile.
>>> >> But its
>>> >> also worth asking whether or not its too difficult for a user to
>>> >> extend
>>> >> HadoopRDD right now.  There have been several cases in the past week
>>> >> where
>>> >> we've suggested that a user should read from hdfs themselves (eg., to
>>> >> read
>>> >> multiple files together in one partition) -- with*out* reusing the
>>> >> code in
>>> >> HadoopRDD, though they would lose things like the metric tracking &
>>> >> preferred locations you get from HadoopRDD.  Does HadoopRDD need to
>>> >> some
>>> >> refactoring to make that easier to do?  Or do we just need a good
>>> >> example?
>>> >>
>>> >> Imran
>>> >>
>>> >> (sorry for hijacking your thread, Koert)
>>> >>
>>> >>
>>> >>
>>> >> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
>>> >> wrote:
>>> >>
>>> >> > see email below. reynold suggested i send it to dev instead of user
>>> >> >
>>> >> > ---------- Forwarded message ----------
>>> >> > From: Koert Kuipers <koert@tresata.com>
>>> >> > Date: Mon, Mar 23, 2015 at 4:36 PM
>>> >> > Subject: hadoop input/output format advanced control
>>> >> > To: "user@spark.apache.org" <user@spark.apache.org>
>>> >> >
>>> >> >
>>> >> > currently its pretty hard to control the Hadoop Input/Output formats
>>> >> > used
>>> >> > in Spark. The conventions seems to be to add extra parameters to all
>>> >> > methods and then somewhere deep inside the code (for example in
>>> >> > PairRDDFunctions.saveAsHadoopFile) all these parameters get
>>> >> > translated
>>> >> into
>>> >> > settings on the Hadoop Configuration object.
>>> >> >
>>> >> > for example for compression i see "codec: Option[Class[_ <:
>>> >> > CompressionCodec]] = None" added to a bunch of methods.
>>> >> >
>>> >> > how scalable is this solution really?
>>> >> >
>>> >> > for example i need to read from a hadoop dataset and i dont want the
>>> >> input
>>> >> > (part) files to get split up. the way to do this is to set
>>> >> > "mapred.min.split.size". now i dont want to set this at the level of
>>> >> > the
>>> >> > SparkContext (which can be done), since i dont want it to apply to
>>> >> > input
>>> >> > formats in general. i want it to apply to just this one specific
>>> >> > input
>>> >> > dataset i need to read. which leaves me with no options currently. i
>>> >> could
>>> >> > go add yet another input parameter to all the methods
>>> >> > (SparkContext.textFile, SparkContext.hadoopFile,
>>> >> > SparkContext.objectFile,
>>> >> > etc.). but that seems ineffective.
>>> >> >
>>> >> > why can we not expose a Map[String, String] or some other generic
>>> >> > way to
>>> >> > manipulate settings for hadoop input/output formats? it would
>>> >> > require
>>> >> > adding one more parameter to all methods to deal with hadoop
>>> >> > input/output
>>> >> > formats, but after that its done. one parameter to rule them all....
>>> >> >
>>> >> > then i could do:
>>> >> > val x = sc.textFile("/some/path", formatSettings =
>>> >> > Map("mapred.min.split.size" -> "12345"))
>>> >> >
>>> >> > or
>>> >> > rdd.saveAsTextFile("/some/path, formatSettings =
>>> >> > Map(mapred.output.compress" -> "true",
>>> >> > "mapred.output.compression.codec"
>>> >> ->
>>> >> > "somecodec"))
>>> >> >
>>> >>
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 06:06:31 2015
Return-Path: <dev-return-12156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28E981747C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 06:06:31 +0000 (UTC)
Received: (qmail 77370 invoked by uid 500); 25 Mar 2015 06:06:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77288 invoked by uid 500); 25 Mar 2015 06:06:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77277 invoked by uid 99); 25 Mar 2015 06:06:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 06:06:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of krajah@maprtech.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 06:06:04 +0000
Received: by qgfa8 with SMTP id a8so25570695qgf.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 23:04:33 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=IXiQIUysMJLp1l6JB1/cp4Uxi72FXhC74Zx9FTseMJw=;
        b=Z0YWU64Z2PCcdsADlt4+VOWYJ8QnsqNw3tggTygCU82XRUaWIbUqnVNesCBmw4qIIW
         nvzSSWWFqjO1KpDR3ZfLEc9bZyealcZZz298MW61AAv5ER5mgqpp1a9yjE7DGVgf4H8/
         bK6brNBKkaoAnZMtAj/0ILKxgthoSQVM8gsjn8LOyvVb7ReX060+x4iGCIy6II0Q6Cap
         KE0YL/6ai2LaRd4Uo/loBBp/d5v+YW6+PMpDoq+yFNR7BmdRj6G3+aBTcZyGPe+RYURv
         pO9YkLxqSjeV+nBULxflOAja3tbz3dQONNvJI2D3Skjb1ZZlfGmiELWEueemVLaoOHK4
         8Ipw==
X-Gm-Message-State: ALoCoQn25Q8DC22yiWsCxpMgqco6V3GUVgC2maZTLPQwyTzAvD8TfsfMWDLOwMXIsy0yUXi9xEKB
MIME-Version: 1.0
X-Received: by 10.55.42.27 with SMTP id q27mr16417560qkh.64.1427263472882;
 Tue, 24 Mar 2015 23:04:32 -0700 (PDT)
Received: by 10.140.104.132 with HTTP; Tue, 24 Mar 2015 23:04:32 -0700 (PDT)
In-Reply-To: <CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
	<CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
Date: Tue, 24 Mar 2015 23:04:32 -0700
Message-ID: <CALH4WSOkiNwxGJZ7BDf8Ws-rz_ScGjMeT++5nZuSHL4pfwkfMQ@mail.gmail.com>
Subject: Re: Understanding shuffle file name conflicts
From: Kannan Rajah <krajah@maprtech.com>
To: Saisai Shao <sai.sai.shao@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11494a364d6deb051216acec
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11494a364d6deb051216acec
Content-Type: text/plain; charset=UTF-8

Saisai,
This is the not the case when I use spark-submit to run 2 jobs, one after
another. The shuffle id remains the same.


--
Kannan

On Tue, Mar 24, 2015 at 7:35 PM, Saisai Shao <sai.sai.shao@gmail.com> wrote:

> Hi Kannan,
>
> As I know the shuffle Id in ShuffleDependency will be increased, so even
> if you run the same job twice, the shuffle dependency as well as shuffle id
> is different, so the shuffle file name which is combined by
> (shuffleId+mapId+reduceId) will be changed, so there's no name conflict
> even in the same directory as I know.
>
> Thanks
> Jerry
>
>
> 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
>
>> I am working on SPARK-1529. I ran into an issue with my change, where the
>> same shuffle file was being reused across 2 jobs. Please note this only
>> happens when I use a hard coded location to use for shuffle files, say
>> "/tmp". It does not happen with normal code path that uses
>> DiskBlockManager
>> to pick different directories for each run. So I want to understand how
>> DiskBlockManager guarantees that such a conflict will never happen.
>>
>> Let's say the shuffle block id has a value of shuffle_0_0_0. So the data
>> file name is shuffle_0_0_0.data and index file name is
>> shuffle_0_0_0.index.
>> If I run a spark job twice, one after another, these files get created
>> under different directories because of the hashing logic in
>> DiskBlockManager. But the hash is based off the file name, so how are we
>> sure that there won't be a conflict ever?
>>
>> --
>> Kannan
>>
>
>

--001a11494a364d6deb051216acec--

From dev-return-12157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 06:24:31 2015
Return-Path: <dev-return-12157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C0C41751A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 06:24:31 +0000 (UTC)
Received: (qmail 3220 invoked by uid 500); 25 Mar 2015 06:24:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3135 invoked by uid 500); 25 Mar 2015 06:24:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3123 invoked by uid 99); 25 Mar 2015 06:24:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 06:24:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.220.47 as permitted sender)
Received: from [209.85.220.47] (HELO mail-pa0-f47.google.com) (209.85.220.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 06:24:04 +0000
Received: by padcy3 with SMTP id cy3so18293076pad.3
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 23:22:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Re5Lp39/ARaZ7hGH01S3xwmx4U5Jpbq+uzDM8PGsvew=;
        b=asV44Ynfl872SOvAI41RsLyL43SYx5RUimpf3K1Ld/4H2m7k5tnsgnELAZEUIOiAa8
         e5wys6wM/GXsJ0/9ajbA0tL8CVqbrbW3kLvs5Z+lAq9l8RDgJDM2lSyqwNFAuh2RVKZx
         x/RrQNH3U3Og6FrlCaok3BzN0kEjce7d0NwIHbbifrocjHEN7pZYIP0DK/14jC7UmJm/
         T5EyjQgwHDeEBYiWutu03vIAZ5VRg0N+/o6en2lHxjV/49s8gD9lhLlAYVE/Uw8u6TCW
         RUOOHsrBRPGn9fsd+M6snDSMiA/dkm8p4sYpfdWrpCPRKP0HYxzYbZ6vDi0k+Nf1LjJD
         jD3g==
MIME-Version: 1.0
X-Received: by 10.68.113.161 with SMTP id iz1mr14285690pbb.30.1427264552871;
 Tue, 24 Mar 2015 23:22:32 -0700 (PDT)
Received: by 10.70.135.200 with HTTP; Tue, 24 Mar 2015 23:22:32 -0700 (PDT)
In-Reply-To: <CALH4WSOkiNwxGJZ7BDf8Ws-rz_ScGjMeT++5nZuSHL4pfwkfMQ@mail.gmail.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
	<CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
	<CALH4WSOkiNwxGJZ7BDf8Ws-rz_ScGjMeT++5nZuSHL4pfwkfMQ@mail.gmail.com>
Date: Tue, 24 Mar 2015 23:22:32 -0700
Message-ID: <CAOEPXP7THEy5gtrrqqiY4wNhdgFSgfn5LNdYv_DYMTaGn0CXHg@mail.gmail.com>
Subject: Re: Understanding shuffle file name conflicts
From: Josh Rosen <rosenville@gmail.com>
To: Kannan Rajah <krajah@maprtech.com>
Cc: Saisai Shao <sai.sai.shao@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6dc87eacadb0051216ec0a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dc87eacadb0051216ec0a
Content-Type: text/plain; charset=UTF-8

Which version of Spark are you using?  What do you mean when you say that
you used a hardcoded location for shuffle files?

If you look at the current DiskBlockManager code, it looks like it will
create a per-application subdirectory in each of the local root directories.

Here's the call to create a subdirectory in each root dir:
https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala#L126

This call to Utils.createDirectory() should result in a fresh subdirectory
being created for just this application (note the use of random UUIDs, plus
the check to ensure that the directory doesn't already exist):
https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/util/Utils.scala#L273

So, although the filenames for shuffle files are not globally unique, their
full paths should be unique due to these unique per-application
subdirectories.  Have you observed an instance where this isn't the case?

- Josh

On Tue, Mar 24, 2015 at 11:04 PM, Kannan Rajah <krajah@maprtech.com> wrote:

> Saisai,
> This is the not the case when I use spark-submit to run 2 jobs, one after
> another. The shuffle id remains the same.
>
>
> --
> Kannan
>
> On Tue, Mar 24, 2015 at 7:35 PM, Saisai Shao <sai.sai.shao@gmail.com>
> wrote:
>
> > Hi Kannan,
> >
> > As I know the shuffle Id in ShuffleDependency will be increased, so even
> > if you run the same job twice, the shuffle dependency as well as shuffle
> id
> > is different, so the shuffle file name which is combined by
> > (shuffleId+mapId+reduceId) will be changed, so there's no name conflict
> > even in the same directory as I know.
> >
> > Thanks
> > Jerry
> >
> >
> > 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
> >
> >> I am working on SPARK-1529. I ran into an issue with my change, where
> the
> >> same shuffle file was being reused across 2 jobs. Please note this only
> >> happens when I use a hard coded location to use for shuffle files, say
> >> "/tmp". It does not happen with normal code path that uses
> >> DiskBlockManager
> >> to pick different directories for each run. So I want to understand how
> >> DiskBlockManager guarantees that such a conflict will never happen.
> >>
> >> Let's say the shuffle block id has a value of shuffle_0_0_0. So the data
> >> file name is shuffle_0_0_0.data and index file name is
> >> shuffle_0_0_0.index.
> >> If I run a spark job twice, one after another, these files get created
> >> under different directories because of the hashing logic in
> >> DiskBlockManager. But the hash is based off the file name, so how are we
> >> sure that there won't be a conflict ever?
> >>
> >> --
> >> Kannan
> >>
> >
> >
>

--047d7b6dc87eacadb0051216ec0a--

From dev-return-12158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 06:59:13 2015
Return-Path: <dev-return-12158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A19FC17633
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 06:59:13 +0000 (UTC)
Received: (qmail 79837 invoked by uid 500); 25 Mar 2015 06:58:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79760 invoked by uid 500); 25 Mar 2015 06:58:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79746 invoked by uid 99); 25 Mar 2015 06:58:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 06:58:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sai.sai.shao@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 06:58:12 +0000
Received: by ignm3 with SMTP id m3so65745717ign.0
        for <dev@spark.apache.org>; Tue, 24 Mar 2015 23:56:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=awO4UR1zSklt5CEK9wgKrKYTZvFq08wUKF/fLvzLmcI=;
        b=IAQRj0YVQ72EW58adOod8IC5i21XTu71nGHdIGyu1wOZJlZSWAG8Uvtr1CKnJINO3a
         nlGQyzW2UdFy8khDwogrZi/wNUIA/dA53GYOgImVnIJNgkarcgmH23s/AHPYdnhvcvla
         w7DREQf7e2oDHYbMSfF62+z54/zV3GHSg8slKsFibcJo8s4mE/zbpomr6gVEFSyPmBba
         T60jNbR3B/9FkyGQuxBADI4quMRuKztOv3zPbAMMFXvh8qkgYSIY0SmFHuXF+f0GNuz3
         15QF22lU4MWfoi7vx2C9IBz/8tfnrGgkpQB0bjKw362SnUEsWj3dq+5VG6+2RtpJVWZh
         Yjjw==
MIME-Version: 1.0
X-Received: by 10.107.10.82 with SMTP id u79mr12205293ioi.65.1427266600921;
 Tue, 24 Mar 2015 23:56:40 -0700 (PDT)
Received: by 10.36.117.86 with HTTP; Tue, 24 Mar 2015 23:56:40 -0700 (PDT)
In-Reply-To: <CAOEPXP7THEy5gtrrqqiY4wNhdgFSgfn5LNdYv_DYMTaGn0CXHg@mail.gmail.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
	<CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
	<CALH4WSOkiNwxGJZ7BDf8Ws-rz_ScGjMeT++5nZuSHL4pfwkfMQ@mail.gmail.com>
	<CAOEPXP7THEy5gtrrqqiY4wNhdgFSgfn5LNdYv_DYMTaGn0CXHg@mail.gmail.com>
Date: Wed, 25 Mar 2015 14:56:40 +0800
Message-ID: <CANvfmP_=dXZYx9gEg+RFnT44E_jEh04iytfK-XpsXB2SC=y+Ww@mail.gmail.com>
Subject: Re: Understanding shuffle file name conflicts
From: Saisai Shao <sai.sai.shao@gmail.com>
To: Kannan Rajah <krajah@maprtech.com>
Cc: dev <dev@spark.apache.org>, Josh Rosen <rosenville@gmail.com>
Content-Type: multipart/alternative; boundary=001a113f9b4abf731605121766cd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f9b4abf731605121766cd
Content-Type: text/plain; charset=UTF-8

Yes as Josh said, when application is started, Spark will create a unique
application-wide folder for related temporary files. And jobs in this
application will have a unique shuffle id with unique file names, so
shuffle stages within app will not meet name conflicts.

Also shuffle files between applications are separated by application
folder, so the name conflicts cannot be happened.

Maybe you changed some parts of the code while do the patch.

Thanks
Jerry


2015-03-25 14:22 GMT+08:00 Josh Rosen <rosenville@gmail.com>:

> Which version of Spark are you using?  What do you mean when you say that
> you used a hardcoded location for shuffle files?
>
> If you look at the current DiskBlockManager code, it looks like it will
> create a per-application subdirectory in each of the local root directories.
>
> Here's the call to create a subdirectory in each root dir:
> https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala#L126
>
> This call to Utils.createDirectory() should result in a fresh subdirectory
> being created for just this application (note the use of random UUIDs, plus
> the check to ensure that the directory doesn't already exist):
>
> https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/util/Utils.scala#L273
>
> So, although the filenames for shuffle files are not globally unique,
> their full paths should be unique due to these unique per-application
> subdirectories.  Have you observed an instance where this isn't the case?
>
> - Josh
>
> On Tue, Mar 24, 2015 at 11:04 PM, Kannan Rajah <krajah@maprtech.com>
> wrote:
>
>> Saisai,
>> This is the not the case when I use spark-submit to run 2 jobs, one after
>> another. The shuffle id remains the same.
>>
>>
>> --
>> Kannan
>>
>> On Tue, Mar 24, 2015 at 7:35 PM, Saisai Shao <sai.sai.shao@gmail.com>
>> wrote:
>>
>> > Hi Kannan,
>> >
>> > As I know the shuffle Id in ShuffleDependency will be increased, so even
>> > if you run the same job twice, the shuffle dependency as well as
>> shuffle id
>> > is different, so the shuffle file name which is combined by
>> > (shuffleId+mapId+reduceId) will be changed, so there's no name conflict
>> > even in the same directory as I know.
>> >
>> > Thanks
>> > Jerry
>> >
>> >
>> > 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
>> >
>> >> I am working on SPARK-1529. I ran into an issue with my change, where
>> the
>> >> same shuffle file was being reused across 2 jobs. Please note this only
>> >> happens when I use a hard coded location to use for shuffle files, say
>> >> "/tmp". It does not happen with normal code path that uses
>> >> DiskBlockManager
>> >> to pick different directories for each run. So I want to understand how
>> >> DiskBlockManager guarantees that such a conflict will never happen.
>> >>
>> >> Let's say the shuffle block id has a value of shuffle_0_0_0. So the
>> data
>> >> file name is shuffle_0_0_0.data and index file name is
>> >> shuffle_0_0_0.index.
>> >> If I run a spark job twice, one after another, these files get created
>> >> under different directories because of the hashing logic in
>> >> DiskBlockManager. But the hash is based off the file name, so how are
>> we
>> >> sure that there won't be a conflict ever?
>> >>
>> >> --
>> >> Kannan
>> >>
>> >
>> >
>>
>
>

--001a113f9b4abf731605121766cd--

From dev-return-12159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 07:04:04 2015
Return-Path: <dev-return-12159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 697FA1766A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 07:04:04 +0000 (UTC)
Received: (qmail 94222 invoked by uid 500); 25 Mar 2015 07:04:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94147 invoked by uid 500); 25 Mar 2015 07:04:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94135 invoked by uid 99); 25 Mar 2015 07:04:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 07:04:02 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of krajah@maprtech.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 07:03:58 +0000
Received: by qgfa8 with SMTP id a8so26894362qgf.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 00:03:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=j3oNZrFJTG+P4HyDkTDQjUX9cGRlYMtKtHOB3wdGuJA=;
        b=BXQymMYcQAdAwHjproPvBWypLaGsA5S/cNafkNzPkyu3ngvYsb+Q+zE3u1+gTJJM3P
         jHlhzraUSQ4OiKeVsJrFu+bVVimHyejoYq/zP/5IxLHatijbGm9rZv/NJh77394Y3wvO
         sDnBIfng0Wldvv9ievMnhXmtJShvx9OvsnOuu/8ifIFiCP4PofCu6h9DuKGtCj7gp4sR
         wIPfz6r8jlsxjZN35pjGWqCydrbW3WJ3vjwfNUmCWPhWp97MDtdFs0kFa7+0g6f2YLd4
         Mfn032anJoJBiVrDjJ0vlykQDmrRcOEpGcn7hhxbo82V3Nc+z5OEPGo7FLBXbbJeq3om
         PL8g==
X-Gm-Message-State: ALoCoQkCtV6afncAzIhioHlHoHh8TeQcgHTFyTark2bIzgqe31mW6citOfnDzSp/Ee8aW1CsL0u4
MIME-Version: 1.0
X-Received: by 10.140.148.20 with SMTP id 20mr10116754qhu.67.1427267018121;
 Wed, 25 Mar 2015 00:03:38 -0700 (PDT)
Received: by 10.140.104.132 with HTTP; Wed, 25 Mar 2015 00:03:38 -0700 (PDT)
In-Reply-To: <CANvfmP_=dXZYx9gEg+RFnT44E_jEh04iytfK-XpsXB2SC=y+Ww@mail.gmail.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
	<CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
	<CALH4WSOkiNwxGJZ7BDf8Ws-rz_ScGjMeT++5nZuSHL4pfwkfMQ@mail.gmail.com>
	<CAOEPXP7THEy5gtrrqqiY4wNhdgFSgfn5LNdYv_DYMTaGn0CXHg@mail.gmail.com>
	<CANvfmP_=dXZYx9gEg+RFnT44E_jEh04iytfK-XpsXB2SC=y+Ww@mail.gmail.com>
Date: Wed, 25 Mar 2015 00:03:38 -0700
Message-ID: <CALH4WSNR02CKRJHYG4PyPJRYpXQFqm496ouyJyoM8vCBUcpyrw@mail.gmail.com>
Subject: Re: Understanding shuffle file name conflicts
From: Kannan Rajah <krajah@maprtech.com>
To: Saisai Shao <sai.sai.shao@gmail.com>
Cc: dev <dev@spark.apache.org>, Josh Rosen <rosenville@gmail.com>
Content-Type: multipart/alternative; boundary=001a113bb4cc9d83140512177f72
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113bb4cc9d83140512177f72
Content-Type: text/plain; charset=UTF-8

Josh & Saisai,
When I say I am using a hardcoded location for shuffle files, I mean that I
am not using DiskBlockManager.getFile API because that uses the directories
created locally on the node. But for my use case, I need to look at
creating those shuffle files on HDFS.

I will take a closer look at this. But I have a couple of questions. From
what I understand, DiskBlockManager code does not know about any
application ID. It seems to pick up the top root temp dir location from
SparkConf and then creates a bunch of sub dir under it. When a shuffle file
needs to be created using getFile API, it hashes it to one of the existing
dir. At this point, I don't see any app specific directory. Can you point
out what I am missing here? The getFile API does not involve the random
UUIDs. The random UUID generation happens inside createTempShuffleBlock and
that is invoke only from ExternalSorter. On the other hand,
DiskBlockManager.getFile is used to create the shuffle index and data file.


--
Kannan

On Tue, Mar 24, 2015 at 11:56 PM, Saisai Shao <sai.sai.shao@gmail.com>
wrote:

> Yes as Josh said, when application is started, Spark will create a unique
> application-wide folder for related temporary files. And jobs in this
> application will have a unique shuffle id with unique file names, so
> shuffle stages within app will not meet name conflicts.
>
> Also shuffle files between applications are separated by application
> folder, so the name conflicts cannot be happened.
>
> Maybe you changed some parts of the code while do the patch.
>
> Thanks
> Jerry
>
>
> 2015-03-25 14:22 GMT+08:00 Josh Rosen <rosenville@gmail.com>:
>
>> Which version of Spark are you using?  What do you mean when you say that
>> you used a hardcoded location for shuffle files?
>>
>> If you look at the current DiskBlockManager code, it looks like it will
>> create a per-application subdirectory in each of the local root directories.
>>
>> Here's the call to create a subdirectory in each root dir:
>> https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala#L126
>>
>> This call to Utils.createDirectory() should result in a fresh
>> subdirectory being created for just this application (note the use of
>> random UUIDs, plus the check to ensure that the directory doesn't already
>> exist):
>>
>> https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/util/Utils.scala#L273
>>
>> So, although the filenames for shuffle files are not globally unique,
>> their full paths should be unique due to these unique per-application
>> subdirectories.  Have you observed an instance where this isn't the case?
>>
>> - Josh
>>
>> On Tue, Mar 24, 2015 at 11:04 PM, Kannan Rajah <krajah@maprtech.com>
>> wrote:
>>
>>> Saisai,
>>> This is the not the case when I use spark-submit to run 2 jobs, one after
>>> another. The shuffle id remains the same.
>>>
>>>
>>> --
>>> Kannan
>>>
>>> On Tue, Mar 24, 2015 at 7:35 PM, Saisai Shao <sai.sai.shao@gmail.com>
>>> wrote:
>>>
>>> > Hi Kannan,
>>> >
>>> > As I know the shuffle Id in ShuffleDependency will be increased, so
>>> even
>>> > if you run the same job twice, the shuffle dependency as well as
>>> shuffle id
>>> > is different, so the shuffle file name which is combined by
>>> > (shuffleId+mapId+reduceId) will be changed, so there's no name conflict
>>> > even in the same directory as I know.
>>> >
>>> > Thanks
>>> > Jerry
>>> >
>>> >
>>> > 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
>>> >
>>> >> I am working on SPARK-1529. I ran into an issue with my change, where
>>> the
>>> >> same shuffle file was being reused across 2 jobs. Please note this
>>> only
>>> >> happens when I use a hard coded location to use for shuffle files, say
>>> >> "/tmp". It does not happen with normal code path that uses
>>> >> DiskBlockManager
>>> >> to pick different directories for each run. So I want to understand
>>> how
>>> >> DiskBlockManager guarantees that such a conflict will never happen.
>>> >>
>>> >> Let's say the shuffle block id has a value of shuffle_0_0_0. So the
>>> data
>>> >> file name is shuffle_0_0_0.data and index file name is
>>> >> shuffle_0_0_0.index.
>>> >> If I run a spark job twice, one after another, these files get created
>>> >> under different directories because of the hashing logic in
>>> >> DiskBlockManager. But the hash is based off the file name, so how are
>>> we
>>> >> sure that there won't be a conflict ever?
>>> >>
>>> >> --
>>> >> Kannan
>>> >>
>>> >
>>> >
>>>
>>
>>
>

--001a113bb4cc9d83140512177f72--

From dev-return-12160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 07:13:03 2015
Return-Path: <dev-return-12160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9A63E176F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 07:13:03 +0000 (UTC)
Received: (qmail 20882 invoked by uid 500); 25 Mar 2015 07:12:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20795 invoked by uid 500); 25 Mar 2015 07:12:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20783 invoked by uid 99); 25 Mar 2015 07:12:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 07:12:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sai.sai.shao@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 07:12:55 +0000
Received: by ignm3 with SMTP id m3so65919116ign.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 00:12:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=HdXUNyAHX8etyQqD6qmk1rfoUAyYE7snREbOHXSPNtc=;
        b=pjpQOhIq94Leabw1oz8VcW7zwlfoo9mTyojM/dY7fH0RQ/KgKkgYZtU3gfd3t8C9ud
         FlbyePf9KFpkAfQ/uTZZdEH0E5z0WFYJQ+zxf1Yr/7guXI8YSJ82wI8k8DJCRvYMN9nI
         It5x390hfReZOApVqbmbpMDC90wgqSEAWrL/FsRIO+QIew/+GxQgZ4fGh1smBohWs0eD
         GxvydQoHD6ZeD6RxTU/RDlOzPvxVTcWOfYj2zLMQU7qTueEEGvBpc53lALUqn/uyq7wF
         mBvCKbQZ9RgucRAgVONblRpRApVUmeEYZje9SD76JricgSJ+wFYXYF/mgj3c8kUuXji+
         k51g==
MIME-Version: 1.0
X-Received: by 10.107.10.82 with SMTP id u79mr12274629ioi.65.1427267554658;
 Wed, 25 Mar 2015 00:12:34 -0700 (PDT)
Received: by 10.36.117.86 with HTTP; Wed, 25 Mar 2015 00:12:34 -0700 (PDT)
In-Reply-To: <CALH4WSNR02CKRJHYG4PyPJRYpXQFqm496ouyJyoM8vCBUcpyrw@mail.gmail.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
	<CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
	<CALH4WSOkiNwxGJZ7BDf8Ws-rz_ScGjMeT++5nZuSHL4pfwkfMQ@mail.gmail.com>
	<CAOEPXP7THEy5gtrrqqiY4wNhdgFSgfn5LNdYv_DYMTaGn0CXHg@mail.gmail.com>
	<CANvfmP_=dXZYx9gEg+RFnT44E_jEh04iytfK-XpsXB2SC=y+Ww@mail.gmail.com>
	<CALH4WSNR02CKRJHYG4PyPJRYpXQFqm496ouyJyoM8vCBUcpyrw@mail.gmail.com>
Date: Wed, 25 Mar 2015 15:12:34 +0800
Message-ID: <CANvfmP9Sq=z-QD=t5ZFNqjK9GPw31ykks4fhVpoDKu5B9+ZubA@mail.gmail.com>
Subject: Re: Understanding shuffle file name conflicts
From: Saisai Shao <sai.sai.shao@gmail.com>
To: Kannan Rajah <krajah@maprtech.com>
Cc: dev <dev@spark.apache.org>, Josh Rosen <rosenville@gmail.com>
Content-Type: multipart/alternative; boundary=001a113f9b4a98513d0512179fa3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113f9b4a98513d0512179fa3
Content-Type: text/plain; charset=UTF-8

DIskBlockManager doesn't need to know the app id, all it need to do is to
create a folder with a unique name (UUID based) and then put all the
shuffle files into it.

you can see the code in DiskBlockManager as below, it will create a bunch
unique folders when initialized, these folders are app specific

private[spark] val localDirs: Array[File] = createLocalDirs(conf)

UUID is for creating an app specific folder. and shuffle file hashed by
shuffle block id, which is deterministic by using getFile as you mentioned.



2015-03-25 15:03 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:

> Josh & Saisai,
> When I say I am using a hardcoded location for shuffle files, I mean that
> I am not using DiskBlockManager.getFile API because that uses the
> directories created locally on the node. But for my use case, I need to
> look at creating those shuffle files on HDFS.
>
> I will take a closer look at this. But I have a couple of questions. From
> what I understand, DiskBlockManager code does not know about any
> application ID. It seems to pick up the top root temp dir location from
> SparkConf and then creates a bunch of sub dir under it. When a shuffle file
> needs to be created using getFile API, it hashes it to one of the existing
> dir. At this point, I don't see any app specific directory. Can you point
> out what I am missing here? The getFile API does not involve the random
> UUIDs. The random UUID generation happens inside createTempShuffleBlock and
> that is invoke only from ExternalSorter. On the other hand,
> DiskBlockManager.getFile is used to create the shuffle index and data file.
>
>
> --
> Kannan
>
> On Tue, Mar 24, 2015 at 11:56 PM, Saisai Shao <sai.sai.shao@gmail.com>
> wrote:
>
>> Yes as Josh said, when application is started, Spark will create a unique
>> application-wide folder for related temporary files. And jobs in this
>> application will have a unique shuffle id with unique file names, so
>> shuffle stages within app will not meet name conflicts.
>>
>> Also shuffle files between applications are separated by application
>> folder, so the name conflicts cannot be happened.
>>
>> Maybe you changed some parts of the code while do the patch.
>>
>> Thanks
>> Jerry
>>
>>
>> 2015-03-25 14:22 GMT+08:00 Josh Rosen <rosenville@gmail.com>:
>>
>>> Which version of Spark are you using?  What do you mean when you say
>>> that you used a hardcoded location for shuffle files?
>>>
>>> If you look at the current DiskBlockManager code, it looks like it will
>>> create a per-application subdirectory in each of the local root directories.
>>>
>>> Here's the call to create a subdirectory in each root dir:
>>> https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala#L126
>>>
>>> This call to Utils.createDirectory() should result in a fresh
>>> subdirectory being created for just this application (note the use of
>>> random UUIDs, plus the check to ensure that the directory doesn't already
>>> exist):
>>>
>>> https://github.com/apache/spark/blob/c5cc41468e8709d09c09289bb55bc8edc99404b1/core/src/main/scala/org/apache/spark/util/Utils.scala#L273
>>>
>>> So, although the filenames for shuffle files are not globally unique,
>>> their full paths should be unique due to these unique per-application
>>> subdirectories.  Have you observed an instance where this isn't the case?
>>>
>>> - Josh
>>>
>>> On Tue, Mar 24, 2015 at 11:04 PM, Kannan Rajah <krajah@maprtech.com>
>>> wrote:
>>>
>>>> Saisai,
>>>> This is the not the case when I use spark-submit to run 2 jobs, one
>>>> after
>>>> another. The shuffle id remains the same.
>>>>
>>>>
>>>> --
>>>> Kannan
>>>>
>>>> On Tue, Mar 24, 2015 at 7:35 PM, Saisai Shao <sai.sai.shao@gmail.com>
>>>> wrote:
>>>>
>>>> > Hi Kannan,
>>>> >
>>>> > As I know the shuffle Id in ShuffleDependency will be increased, so
>>>> even
>>>> > if you run the same job twice, the shuffle dependency as well as
>>>> shuffle id
>>>> > is different, so the shuffle file name which is combined by
>>>> > (shuffleId+mapId+reduceId) will be changed, so there's no name
>>>> conflict
>>>> > even in the same directory as I know.
>>>> >
>>>> > Thanks
>>>> > Jerry
>>>> >
>>>> >
>>>> > 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
>>>> >
>>>> >> I am working on SPARK-1529. I ran into an issue with my change,
>>>> where the
>>>> >> same shuffle file was being reused across 2 jobs. Please note this
>>>> only
>>>> >> happens when I use a hard coded location to use for shuffle files,
>>>> say
>>>> >> "/tmp". It does not happen with normal code path that uses
>>>> >> DiskBlockManager
>>>> >> to pick different directories for each run. So I want to understand
>>>> how
>>>> >> DiskBlockManager guarantees that such a conflict will never happen.
>>>> >>
>>>> >> Let's say the shuffle block id has a value of shuffle_0_0_0. So the
>>>> data
>>>> >> file name is shuffle_0_0_0.data and index file name is
>>>> >> shuffle_0_0_0.index.
>>>> >> If I run a spark job twice, one after another, these files get
>>>> created
>>>> >> under different directories because of the hashing logic in
>>>> >> DiskBlockManager. But the hash is based off the file name, so how
>>>> are we
>>>> >> sure that there won't be a conflict ever?
>>>> >>
>>>> >> --
>>>> >> Kannan
>>>> >>
>>>> >
>>>> >
>>>>
>>>
>>>
>>
>

--001a113f9b4a98513d0512179fa3--

From dev-return-12161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 08:47:28 2015
Return-Path: <dev-return-12161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B07091753C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 08:47:28 +0000 (UTC)
Received: (qmail 18633 invoked by uid 500); 25 Mar 2015 08:47:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18564 invoked by uid 500); 25 Mar 2015 08:47:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18550 invoked by uid 99); 25 Mar 2015 08:47:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 08:47:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 08:46:57 +0000
Received: by oier21 with SMTP id r21so15650926oie.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 01:45:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=q3HywSEj74hjlv/eGM9n82DnP3ZYbqnbGdd0gUqV0Z4=;
        b=gV4NY4y4OW8vtek/aT1OTmEQ98z1xq1RCCr0UIbvT31Wh6goC/GEVU/R8sFkJCX4vK
         nij+VmxoLoY2z6LpdQtoobk0mqRjEDSCYzondyL9lqlKKGmvjYnS0sKlDdAC7MGT+Uhf
         rX/8EUIX7m2fzgezkndN+a66QTqcjqw5l7eBahFvPDc8tfubpJ3pFSep6AqgmPbzRz5O
         DlalLjAQVTQuUZv7qVOkn5NJmB3Pmns4mLyfyhGcEcylTLMQRcf0r9Xprg0P7DR/00S5
         HElP9tNHoX4A0KOxiE/U0HQrl/dph0jP+8vro3eieBbNvr2U5muHTWasu9cpozdLENze
         KtYw==
MIME-Version: 1.0
X-Received: by 10.202.13.203 with SMTP id 194mr6207353oin.130.1427273126014;
 Wed, 25 Mar 2015 01:45:26 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Wed, 25 Mar 2015 01:45:25 -0700 (PDT)
Date: Wed, 25 Mar 2015 09:45:25 +0100
Message-ID: <CAO=evYckrpk8jPC49GcL4p6HjETesOW6Kc4mL7AXozkwfe9xAQ@mail.gmail.com>
Subject: Can't assembly YARN project with SBT
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113d149cac7508051218ebbe
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113d149cac7508051218ebbe
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi!

I'm using the latest IntelliJ and I can't compile the yarn project into the
Spark assembly fat JAR. That is why I'm getting a SparkException with
message "Unable to load YARN support". The yarn project is also missing
from SBT tasks and I can't add it. How can I force sbt to include?

Thanks!

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

--001a113d149cac7508051218ebbe--

From dev-return-12162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 11:42:43 2015
Return-Path: <dev-return-12162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A036717232
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 11:42:43 +0000 (UTC)
Received: (qmail 68664 invoked by uid 500); 25 Mar 2015 11:42:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68580 invoked by uid 500); 25 Mar 2015 11:42:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68557 invoked by uid 99); 25 Mar 2015 11:42:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 11:42:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.174 as permitted sender)
Received: from [209.85.192.174] (HELO mail-pd0-f174.google.com) (209.85.192.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 11:42:36 +0000
Received: by pdbni2 with SMTP id ni2so25999050pdb.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 04:41:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:cc:subject
         :references:in-reply-to:content-type;
        bh=g/CSVr3l4wKUmw7/LQABE2NNiyPwcqULtE1NxY3vTjA=;
        b=WVRk00ryHOd2OPSttbgEcIwEkp69MPq3CqKbqsTQOe+rej4BNr3Xr0Znxy9CJGePeY
         5h2NjWlHq2OHQyQg7d8fW6UzUMPo9B8Fyc4KOLIET+I7ouHtcAXVMzYTeso5q62Ea/M9
         1i8fK3lUjJFLzOoXnFQsvHfXDpe4u03nOVW7utBEh3trJ/OpcTXwyVi7xEH8JiuR8ST4
         vYz0/BQb5gwL4PauDrVp0YbGsRen8COQNAh9ibTLoNGuQV7leDd/1nE2D/d0t4logLm/
         0S6kf4qiKC7ODasKHss75kpKn9yKIF7XpL2/2g3Df9lOS4Eg8cSOnqmmPua6i4XwtUz8
         cunw==
X-Received: by 10.66.141.165 with SMTP id rp5mr16125057pab.93.1427283691496;
        Wed, 25 Mar 2015 04:41:31 -0700 (PDT)
Received: from [10.10.0.13] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id zi5sm2211261pbc.39.2015.03.25.04.40.46
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 25 Mar 2015 04:41:30 -0700 (PDT)
Message-ID: <55129EA3.20000@gmail.com>
Date: Wed, 25 Mar 2015 19:40:19 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Saisai Shao <sai.sai.shao@gmail.com>, 
 Kannan Rajah <krajah@maprtech.com>
CC: dev@spark.apache.org
Subject: Re: Understanding shuffle file name conflicts
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com> <CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
In-Reply-To: <CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------010007010703020702050400"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------010007010703020702050400
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Hi Jerry & Josh

It has been a while since the last time I looked into Spark core shuffle 
code, maybe I’m wrong here. But the shuffle ID is created along with 
ShuffleDependency, which is part of the RDD DAG. So if we submit 
multiple jobs over the same RDD DAG, I think the shuffle IDs in these 
jobs should duplicate. For example:

|val  dag  =  sc.parallelize(Array(1,2,3)).map(i => i -> i).reduceByKey(_ + _)
dag.collect()
dag.collect()
|

 From the debug log output, I did see duplicated shuffle IDs in both 
jobs. Something like this:

|# Job 1
15/03/25 19:26:34 DEBUG BlockStoreShuffleFetcher: Fetching outputs for shuffle 0, reduce 2

# Job 2
15/03/25 19:26:36 DEBUG BlockStoreShuffleFetcher: Fetching outputs for shuffle 0, reduce 5
|

So it’s also possible that some shuffle output files get reused in 
different jobs. But Kannan, did you submit separate jobs over the same 
RDD DAG as I did above? If not, I’d agree with Jerry and Josh.

(Did I miss something here?)

Cheng

On 3/25/15 10:35 AM, Saisai Shao wrote:

> Hi Kannan,
>
> As I know the shuffle Id in ShuffleDependency will be increased, so even if
> you run the same job twice, the shuffle dependency as well as shuffle id is
> different, so the shuffle file name which is combined by
> (shuffleId+mapId+reduceId) will be changed, so there's no name conflict
> even in the same directory as I know.
>
> Thanks
> Jerry
>
>
> 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
>
>> I am working on SPARK-1529. I ran into an issue with my change, where the
>> same shuffle file was being reused across 2 jobs. Please note this only
>> happens when I use a hard coded location to use for shuffle files, say
>> "/tmp". It does not happen with normal code path that uses DiskBlockManager
>> to pick different directories for each run. So I want to understand how
>> DiskBlockManager guarantees that such a conflict will never happen.
>>
>> Let's say the shuffle block id has a value of shuffle_0_0_0. So the data
>> file name is shuffle_0_0_0.data and index file name is shuffle_0_0_0.index.
>> If I run a spark job twice, one after another, these files get created
>> under different directories because of the hashing logic in
>> DiskBlockManager. But the hash is based off the file name, so how are we
>> sure that there won't be a conflict ever?
>>
>> --
>> Kannan
>>
​

--------------010007010703020702050400--

From dev-return-12163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 11:44:17 2015
Return-Path: <dev-return-12163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D362A1726B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 11:44:17 +0000 (UTC)
Received: (qmail 73517 invoked by uid 500); 25 Mar 2015 11:44:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73439 invoked by uid 500); 25 Mar 2015 11:44:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73427 invoked by uid 99); 25 Mar 2015 11:44:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 11:44:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zoltan.zvara@gmail.com designates 209.85.218.54 as permitted sender)
Received: from [209.85.218.54] (HELO mail-oi0-f54.google.com) (209.85.218.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 11:44:07 +0000
Received: by oier21 with SMTP id r21so18778859oie.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 04:43:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=h5xF0ArOxVRKOGawtP7CBFmMa+cHm2dSqVIIG9ODOIM=;
        b=tEMdO+KJcGXzu6I3EPfnkKI76t43i06+7eCu2ftogA/cbLiGwCv3kUBSCDJjm1bNLm
         P8CUmi5+TLqgYo3HU0A6+NfJmM7Gj35l3RMU26axhmJUt9335nOAOf5NQZgFaJd/yIbT
         bNq3GQ9sPfa+p4yZbmBmhYRiChUHBzjx5kNuohhMGWfSwMk6m5NXN+7yFcBbRE3Ddh9v
         oxVhjvcAP0+3zKHmqgZfqWIRXw1YyyNRIEVAPr9TnutYTe24lr/kn1YYStRdVpY+hBwT
         5mFyfEOxp2W1H8yAdix/fy2vhZII8qnlkBdy+VHkllJgbekAUO6EMTeYZZbPaRW3Tjmb
         +2jg==
MIME-Version: 1.0
X-Received: by 10.60.174.19 with SMTP id bo19mr7126847oec.5.1427283827416;
 Wed, 25 Mar 2015 04:43:47 -0700 (PDT)
Received: by 10.202.66.136 with HTTP; Wed, 25 Mar 2015 04:43:47 -0700 (PDT)
In-Reply-To: <CAO=evYckrpk8jPC49GcL4p6HjETesOW6Kc4mL7AXozkwfe9xAQ@mail.gmail.com>
References: <CAO=evYckrpk8jPC49GcL4p6HjETesOW6Kc4mL7AXozkwfe9xAQ@mail.gmail.com>
Date: Wed, 25 Mar 2015 12:43:47 +0100
Message-ID: <CAO=evYda_0_C5CjJWW5wTsZntwT6ww--NxekapqFPa7nJfhumA@mail.gmail.com>
Subject: Re: Can't assembly YARN project with SBT
From: =?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0118236286e4b405121b6905
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0118236286e4b405121b6905
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi!

It seems that the problem of "unable to load YARN support" present only
when I run my job from code and by not using the spark-submit script. IMO
this is related to SPARK-5144
<https://issues.apache.org/jira/browse/SPARK-5144>. I'm running QueueStream
example with a single change: sparkConf.setMaster("yarn-client").

I'm building Spark with: sbt/sbt -Pyarn -Dhadoop.version=3D2.6.0 assembly

Zvara Zolt=C3=A1n



mail, hangout, skype: zoltan.zvara@gmail.com

mobile, viber: +36203129543

bank: 10918001-00000021-50480008

address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a

elte: HSKSJZ (ZVZOAAI.ELTE)

2015-03-25 9:45 GMT+01:00 Zolt=C3=A1n Zvara <zoltan.zvara@gmail.com>:

> Hi!
>
> I'm using the latest IntelliJ and I can't compile the yarn project into
> the Spark assembly fat JAR. That is why I'm getting a SparkException with
> message "Unable to load YARN support". The yarn project is also missing
> from SBT tasks and I can't add it. How can I force sbt to include?
>
> Thanks!
>
> Zvara Zolt=C3=A1n
>
>
>
> mail, hangout, skype: zoltan.zvara@gmail.com
>
> mobile, viber: +36203129543
>
> bank: 10918001-00000021-50480008
>
> address: Hungary, 2475 K=C3=A1poln=C3=A1sny=C3=A9k, Kossuth 6/a
>
> elte: HSKSJZ (ZVZOAAI.ELTE)
>

--089e0118236286e4b405121b6905--

From dev-return-12164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 12:43:01 2015
Return-Path: <dev-return-12164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD5C617872
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 12:43:01 +0000 (UTC)
Received: (qmail 20671 invoked by uid 500); 25 Mar 2015 12:43:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20593 invoked by uid 500); 25 Mar 2015 12:43:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20580 invoked by uid 99); 25 Mar 2015 12:43:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 12:43:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [95.143.172.231] (HELO bootes.uberspace.de) (95.143.172.231)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 12:42:35 +0000
Received: (qmail 15737 invoked from network); 25 Mar 2015 12:42:33 -0000
Received: from localhost (HELO webmail.bootes.uberspace.de) (127.0.0.1)
  by ::1 with SMTP; 25 Mar 2015 12:42:33 -0000
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII;
 format=flowed
Content-Transfer-Encoding: 7bit
Date: Wed, 25 Mar 2015 13:42:32 +0100
From: Karlson <ksonspark@siberie.de>
To: dev@spark.apache.org
Subject: functools.partial as UserDefinedFunction
Message-ID: <3560e95f48681fccd9266e86bbff0e51@siberie.de>
X-Sender: ksonspark@siberie.de
User-Agent: Roundcube Webmail/1.0.5
X-Virus-Checked: Checked by ClamAV on apache.org


Hi all,

passing a functools.partial-function as a UserDefinedFunction to 
DataFrame.select raises an AttributeException, because functools.partial 
does not have the attribute __name__. Is there any alternative to 
relying on __name__ in pyspark/sql/functions.py:126 ?


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 13:32:04 2015
Return-Path: <dev-return-12165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EE41217CF4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 13:32:04 +0000 (UTC)
Received: (qmail 48600 invoked by uid 500); 25 Mar 2015 13:32:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48519 invoked by uid 500); 25 Mar 2015 13:32:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48508 invoked by uid 99); 25 Mar 2015 13:32:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 13:32:03 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of saisai.shao@intel.com designates 192.55.52.93 as permitted sender)
Received: from [192.55.52.93] (HELO mga11.intel.com) (192.55.52.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 13:31:38 +0000
Received: from orsmga002.jf.intel.com ([10.7.209.21])
  by fmsmga102.fm.intel.com with ESMTP; 25 Mar 2015 06:31:35 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.11,465,1422950400"; 
   d="scan'208";a="703964750"
Received: from pgsmsx102.gar.corp.intel.com ([10.221.44.80])
  by orsmga002.jf.intel.com with ESMTP; 25 Mar 2015 06:31:29 -0700
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 PGSMSX102.gar.corp.intel.com (10.221.44.80) with Microsoft SMTP Server (TLS)
 id 14.3.224.2; Wed, 25 Mar 2015 21:31:27 +0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.149]) by
 shsmsx102.ccr.corp.intel.com ([169.254.2.198]) with mapi id 14.03.0224.002;
 Wed, 25 Mar 2015 21:31:26 +0800
From: "Shao, Saisai" <saisai.shao@intel.com>
To: Cheng Lian <lian.cs.zju@gmail.com>, Saisai Shao <sai.sai.shao@gmail.com>,
	Kannan Rajah <krajah@maprtech.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Understanding shuffle file name conflicts
Thread-Topic: Understanding shuffle file name conflicts
Thread-Index: AQHQZlvxEHuehBb1FkqN0Y0TtW8cJJ0r9duAgACYQYCAAKGaMA==
Date: Wed, 25 Mar 2015 13:31:25 +0000
Message-ID: <64474308D680D540A4D8151B0F7C03F7027C42F2@SHSMSX104.ccr.corp.intel.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
 <CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
 <55129EA3.20000@gmail.com>
In-Reply-To: <55129EA3.20000@gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-12166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 13:36:45 2015
Return-Path: <dev-return-12166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 80BE317D3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 13:36:45 +0000 (UTC)
Received: (qmail 57861 invoked by uid 500); 25 Mar 2015 13:36:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57789 invoked by uid 500); 25 Mar 2015 13:36:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57778 invoked by uid 99); 25 Mar 2015 13:36:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 13:36:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 13:36:35 +0000
Received: by wgdm6 with SMTP id m6so27702466wgd.2
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 06:34:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=s16hOTMZFB2RpDO+vwVEbJ7LuWjxsaF1/Q7zadxm/tI=;
        b=b3p19QZXQjMNJQDQI8jgXhESxgTA6YDfy1gHG6QGCB7Xlyh5Xya6/UGB7hOCpua6og
         P5pF/BS870+HbKfp1gJl40tsWhF5MfAFcbjpMGdv52P2ZkUV98mgzaUHL6meJApRgLCu
         2yeKj9I/PPQiOYCgaIT18FN6vVbL0wzzhOs4ZMOa6S7+U9zWWQmeHfgzON4wDSVn9r59
         9SBNPKURQFEv72tDb8g26D8Yt2eGACxlVnAkUsaVoRO/vNEzQcuanmeAAcIbkYFNJYR4
         fAi0LWn2yoV6sQR7aFMYnuRPxZ5XgiL1bZPFQnWkifuHnx5d8uK/kd4ikwcVHF5UHOzP
         QAWg==
X-Gm-Message-State: ALoCoQk5Ymq9GQIjhgzZ0ersEbahVYOH6wvrBtlGILMTYiFY06Nb5WCg/05ZSnEE5AJHDTfb6Nm7
MIME-Version: 1.0
X-Received: by 10.180.35.97 with SMTP id g1mr37475000wij.17.1427290464707;
 Wed, 25 Mar 2015 06:34:24 -0700 (PDT)
Received: by 10.28.47.193 with HTTP; Wed, 25 Mar 2015 06:34:24 -0700 (PDT)
X-Originating-IP: [209.150.41.132]
In-Reply-To: <CABPQxsu4i7ae_FaR99ASXfUgFmGp27MB4s8t9v46gB0TZUMCfg@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
	<CANx3uAjb78p0NUSgnUw7dKTek+oLH1=NSz_CLps2T2SdWFmV0g@mail.gmail.com>
	<CANx3uAiWX8k=55hXnjmUoL5V7JQYyNCcPhwOZgNk9s7pQp2WNg@mail.gmail.com>
	<CABPQxsu4i7ae_FaR99ASXfUgFmGp27MB4s8t9v46gB0TZUMCfg@mail.gmail.com>
Date: Wed, 25 Mar 2015 09:34:24 -0400
Message-ID: <CANx3uAjtSWMivcyBvzqMB_giK66LejWDj4uLw2-4JFtAND-+Pg@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Koert Kuipers <koert@tresata.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149462c24006a05121cf5df
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149462c24006a05121cf5df
Content-Type: text/plain; charset=UTF-8

my personal preference would be something like a Map[String, String] that
only reflects the changes you want to make the Configuration for the given
input/output format (so system wide defaults continue to come from
sc.hadoopConfiguration), similarly to what cascading/scalding did, but am
arbitrary Configuration will work too.

i will make a jira and pullreq when i have some time.



On Wed, Mar 25, 2015 at 1:23 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> I see - if you look, in the saving functions we have the option for
> the user to pass an arbitrary Configuration.
>
>
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L894
>
> It seems fine to have the same option for the loading functions, if
> it's easy to just pass this config into the input format.
>
>
>
> On Tue, Mar 24, 2015 at 3:46 PM, Koert Kuipers <koert@tresata.com> wrote:
> > the (compression) codec parameter that is now part of many saveAs...
> methods
> > came from a similar need. see SPARK-763
> > hadoop has many options like this. you either going to have to allow many
> > more of these optional arguments to all the methods that read from hadoop
> > inputformats and write to hadoop outputformats, or you force people to
> > re-create these methods using HadoopRDD, i think (if thats even
> possible).
> >
> > On Tue, Mar 24, 2015 at 6:40 PM, Koert Kuipers <koert@tresata.com>
> wrote:
> >>
> >> i would like to use objectFile with some tweaks to the hadoop conf.
> >> currently there is no way to do that, except recreating objectFile
> myself.
> >> and some of the code objectFile uses i have no access to, since its
> private
> >> to spark.
> >>
> >>
> >> On Tue, Mar 24, 2015 at 2:59 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>>
> >>> Yeah - to Nick's point, I think the way to do this is to pass in a
> >>> custom conf when you create a Hadoop RDD (that's AFAIK why the conf
> >>> field is there). Is there anything you can't do with that feature?
> >>>
> >>> On Tue, Mar 24, 2015 at 11:50 AM, Nick Pentreath
> >>> <nick.pentreath@gmail.com> wrote:
> >>> > Imran, on your point to read multiple files together in a partition,
> is
> >>> > it
> >>> > not simpler to use the approach of copy Hadoop conf and set per-RDD
> >>> > settings for min split to control the input size per partition,
> >>> > together
> >>> > with something like CombineFileInputFormat?
> >>> >
> >>> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
> >>> > wrote:
> >>> >
> >>> >> I think this would be a great addition, I totally agree that you
> need
> >>> >> to be
> >>> >> able to set these at a finer context than just the SparkContext.
> >>> >>
> >>> >> Just to play devil's advocate, though -- the alternative is for you
> >>> >> just
> >>> >> subclass HadoopRDD yourself, or make a totally new RDD, and then you
> >>> >> could
> >>> >> expose whatever you need.  Why is this solution better?  IMO the
> >>> >> criteria
> >>> >> are:
> >>> >> (a) common operations
> >>> >> (b) error-prone / difficult to implement
> >>> >> (c) non-obvious, but important for performance
> >>> >>
> >>> >> I think this case fits (a) & (c), so I think its still worthwhile.
> >>> >> But its
> >>> >> also worth asking whether or not its too difficult for a user to
> >>> >> extend
> >>> >> HadoopRDD right now.  There have been several cases in the past week
> >>> >> where
> >>> >> we've suggested that a user should read from hdfs themselves (eg.,
> to
> >>> >> read
> >>> >> multiple files together in one partition) -- with*out* reusing the
> >>> >> code in
> >>> >> HadoopRDD, though they would lose things like the metric tracking &
> >>> >> preferred locations you get from HadoopRDD.  Does HadoopRDD need to
> >>> >> some
> >>> >> refactoring to make that easier to do?  Or do we just need a good
> >>> >> example?
> >>> >>
> >>> >> Imran
> >>> >>
> >>> >> (sorry for hijacking your thread, Koert)
> >>> >>
> >>> >>
> >>> >>
> >>> >> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
> >>> >> wrote:
> >>> >>
> >>> >> > see email below. reynold suggested i send it to dev instead of
> user
> >>> >> >
> >>> >> > ---------- Forwarded message ----------
> >>> >> > From: Koert Kuipers <koert@tresata.com>
> >>> >> > Date: Mon, Mar 23, 2015 at 4:36 PM
> >>> >> > Subject: hadoop input/output format advanced control
> >>> >> > To: "user@spark.apache.org" <user@spark.apache.org>
> >>> >> >
> >>> >> >
> >>> >> > currently its pretty hard to control the Hadoop Input/Output
> formats
> >>> >> > used
> >>> >> > in Spark. The conventions seems to be to add extra parameters to
> all
> >>> >> > methods and then somewhere deep inside the code (for example in
> >>> >> > PairRDDFunctions.saveAsHadoopFile) all these parameters get
> >>> >> > translated
> >>> >> into
> >>> >> > settings on the Hadoop Configuration object.
> >>> >> >
> >>> >> > for example for compression i see "codec: Option[Class[_ <:
> >>> >> > CompressionCodec]] = None" added to a bunch of methods.
> >>> >> >
> >>> >> > how scalable is this solution really?
> >>> >> >
> >>> >> > for example i need to read from a hadoop dataset and i dont want
> the
> >>> >> input
> >>> >> > (part) files to get split up. the way to do this is to set
> >>> >> > "mapred.min.split.size". now i dont want to set this at the level
> of
> >>> >> > the
> >>> >> > SparkContext (which can be done), since i dont want it to apply to
> >>> >> > input
> >>> >> > formats in general. i want it to apply to just this one specific
> >>> >> > input
> >>> >> > dataset i need to read. which leaves me with no options
> currently. i
> >>> >> could
> >>> >> > go add yet another input parameter to all the methods
> >>> >> > (SparkContext.textFile, SparkContext.hadoopFile,
> >>> >> > SparkContext.objectFile,
> >>> >> > etc.). but that seems ineffective.
> >>> >> >
> >>> >> > why can we not expose a Map[String, String] or some other generic
> >>> >> > way to
> >>> >> > manipulate settings for hadoop input/output formats? it would
> >>> >> > require
> >>> >> > adding one more parameter to all methods to deal with hadoop
> >>> >> > input/output
> >>> >> > formats, but after that its done. one parameter to rule them
> all....
> >>> >> >
> >>> >> > then i could do:
> >>> >> > val x = sc.textFile("/some/path", formatSettings =
> >>> >> > Map("mapred.min.split.size" -> "12345"))
> >>> >> >
> >>> >> > or
> >>> >> > rdd.saveAsTextFile("/some/path, formatSettings =
> >>> >> > Map(mapred.output.compress" -> "true",
> >>> >> > "mapred.output.compression.codec"
> >>> >> ->
> >>> >> > "somecodec"))
> >>> >> >
> >>> >>
> >>>
> >>> ---------------------------------------------------------------------
> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >>>
> >>
> >
>

--089e0149462c24006a05121cf5df--

From dev-return-12167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 14:35:13 2015
Return-Path: <dev-return-12167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C58A17503
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 14:35:13 +0000 (UTC)
Received: (qmail 11583 invoked by uid 500); 25 Mar 2015 14:35:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11512 invoked by uid 500); 25 Mar 2015 14:35:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11500 invoked by uid 99); 25 Mar 2015 14:35:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 14:35:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.42 as permitted sender)
Received: from [209.85.220.42] (HELO mail-pa0-f42.google.com) (209.85.220.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 14:34:44 +0000
Received: by pagj7 with SMTP id j7so30865800pag.2
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 07:33:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:cc:subject
         :references:in-reply-to:content-type:content-transfer-encoding;
        bh=HzhWxsxrYIdqew1vfXUk4VgxFDGdpi+k0LiwlkCgCEI=;
        b=lT5mlH8y5d512JHA3Tvc0Wv543LNTiUPH/Nyo1YgHzgYaZtxds3Z98Dik0GPyKRn/t
         PSkn0zyHCWLdIVaGS0usaeiCyOonXCKsqZw3ucXVJbioFDiRGe0KUkKkN8Hi0llpUOr0
         6s4hjpAlKJGbh38zFbXBg/GnWiP6WKnuYcN9ymuGxMvHsF52ncQcfDTkK037xkLTZz/r
         bwz3AYcMPNKOesgNIkWqDioVQRHF6J8QOQUhGZeKMqCGqZSNhak3RP8+D0TqTB1+G2Vu
         KrXpb4pnOF1du4zMlbiDOI3Htgfs7Gn3rP2k0sv63FK/zAIFROzi5+pqyoxmvwmjxylb
         CY/g==
X-Received: by 10.66.55.74 with SMTP id q10mr17810010pap.94.1427294037570;
        Wed, 25 Mar 2015 07:33:57 -0700 (PDT)
Received: from [10.10.0.6] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id hz8sm2705035pac.5.2015.03.25.07.33.52
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Wed, 25 Mar 2015 07:33:56 -0700 (PDT)
Message-ID: <5512C755.2080509@gmail.com>
Date: Wed, 25 Mar 2015 22:33:57 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: "Shao, Saisai" <saisai.shao@intel.com>, 
 Saisai Shao <sai.sai.shao@gmail.com>,
 Kannan Rajah <krajah@maprtech.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: Understanding shuffle file name conflicts
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com> <CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com> <55129EA3.20000@gmail.com> <64474308D680D540A4D8151B0F7C03F7027C42F2@SHSMSX104.ccr.corp.intel.com>
In-Reply-To: <64474308D680D540A4D8151B0F7C03F7027C42F2@SHSMSX104.ccr.corp.intel.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
X-Virus-Checked: Checked by ClamAV on apache.org

Ah, I see where I'm wrong here. What are reused here are the shuffle map 
output files themselves, rather than the file paths. No new shuffle map 
output files are generated for the 2nd job. Thanks! Really need to walk 
through Spark core code again :)

Cheng

On 3/25/15 9:31 PM, Shao, Saisai wrote:
> Hi Cheng,
>
> I think your scenario is acceptable for Spark's shuffle mechanism and will not occur shuffle file name conflicts.
>
>  From my understanding I think the code snippet you mentioned is the same RDD graph, just running twice, these two jobs will generate 3 stages, map stage and collect stage for the first job, only collect stage for the second job (map stage is the same as previous job). So these two jobs will only generate one copy of shuffle files in the first job, and fetch the shuffle data twice for each job. So name conflicts will not be occurred, since these two jobs rely on the same ShuffledRDD.
>
> I think only shuffle write which generates shuffle files will have chance to meet name conflicts, multiple times of shuffle read is acceptable as the code snippet shows.
>
> Thanks
> Jerry
>
>
>
> -----Original Message-----
> From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
> Sent: Wednesday, March 25, 2015 7:40 PM
> To: Saisai Shao; Kannan Rajah
> Cc: dev@spark.apache.org
> Subject: Re: Understanding shuffle file name conflicts
>
> Hi Jerry & Josh
>
> It has been a while since the last time I looked into Spark core shuffle code, maybe I’m wrong here. But the shuffle ID is created along with ShuffleDependency, which is part of the RDD DAG. So if we submit multiple jobs over the same RDD DAG, I think the shuffle IDs in these jobs should duplicate. For example:
>
> |val  dag  =  sc.parallelize(Array(1,2,3)).map(i => i ->
> |i).reduceByKey(_ + _)
> dag.collect()
> dag.collect()
> |
>
>   From the debug log output, I did see duplicated shuffle IDs in both jobs. Something like this:
>
> |# Job 1
> 15/03/25 19:26:34 DEBUG BlockStoreShuffleFetcher: Fetching outputs for shuffle 0, reduce 2
>
> # Job 2
> 15/03/25 19:26:36 DEBUG BlockStoreShuffleFetcher: Fetching outputs for shuffle 0, reduce 5
> |
>
> So it’s also possible that some shuffle output files get reused in different jobs. But Kannan, did you submit separate jobs over the same RDD DAG as I did above? If not, I’d agree with Jerry and Josh.
>
> (Did I miss something here?)
>
> Cheng
>
> On 3/25/15 10:35 AM, Saisai Shao wrote:
>
>> Hi Kannan,
>>
>> As I know the shuffle Id in ShuffleDependency will be increased, so
>> even if you run the same job twice, the shuffle dependency as well as
>> shuffle id is different, so the shuffle file name which is combined by
>> (shuffleId+mapId+reduceId) will be changed, so there's no name
>> conflict even in the same directory as I know.
>>
>> Thanks
>> Jerry
>>
>>
>> 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
>>
>>> I am working on SPARK-1529. I ran into an issue with my change, where
>>> the same shuffle file was being reused across 2 jobs. Please note
>>> this only happens when I use a hard coded location to use for shuffle
>>> files, say "/tmp". It does not happen with normal code path that uses
>>> DiskBlockManager to pick different directories for each run. So I
>>> want to understand how DiskBlockManager guarantees that such a conflict will never happen.
>>>
>>> Let's say the shuffle block id has a value of shuffle_0_0_0. So the
>>> data file name is shuffle_0_0_0.data and index file name is shuffle_0_0_0.index.
>>> If I run a spark job twice, one after another, these files get
>>> created under different directories because of the hashing logic in
>>> DiskBlockManager. But the hash is based off the file name, so how are
>>> we sure that there won't be a conflict ever?
>>>
>>> --
>>> Kannan
>>>
> ​


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 15:00:27 2015
Return-Path: <dev-return-12168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A513F178BD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 15:00:27 +0000 (UTC)
Received: (qmail 28216 invoked by uid 500); 25 Mar 2015 15:00:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28140 invoked by uid 500); 25 Mar 2015 15:00:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28125 invoked by uid 99); 25 Mar 2015 15:00:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 15:00:25 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.215.45 as permitted sender)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 15:00:00 +0000
Received: by labto5 with SMTP id to5so22240900lab.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 07:59:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=YlzsEih/Gu6Hgso09AYb13odhRzkUElvh/X/wsfdMQs=;
        b=oFXq+Mi5RGK9MNI/eZZ/nnDY6RRP9YWN9dqW12OnjsHt+2NSiHKSMA51aEvbCnJPcc
         ny0vupoXyk+imIhtLacNNqoAo/CGB6b7Fvg5TBY/5EtXxtoG0PbZ27E9aQh7t/fX6YLz
         snh8iCD8q3bm3a70SQiOUnrWG9mZvm9Fd52bXiJuKM4KJiOoGUv1Do9gefq2tMcz2aVd
         Tssrnz2+IhpzM7L9kJwrCkeVYXqgXhN8Li3EJgoQBDEodmJcKupiR+VlL24jRPzW7EPz
         pJrtVP2WEvmWeUdNKjPm+tW/vKZc8l7jmYfMtdRLkd9uY1unOydyJ7YVc1wUcH2XR4Ym
         FuAg==
MIME-Version: 1.0
X-Received: by 10.112.241.72 with SMTP id wg8mr8898387lbc.65.1427295599240;
 Wed, 25 Mar 2015 07:59:59 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Wed, 25 Mar 2015 07:59:59 -0700 (PDT)
In-Reply-To: <CA+B-+fx_1tZNoJQ5b1BreQCKGUR9VvBiQU3Q1_iE4iuB9obzmw@mail.gmail.com>
References: <CA+B-+fwL-t9sGDZ7yYfxwXDJUM6bcLiy1UYdP=NRKpfebCaBhA@mail.gmail.com>
	<CAJgQjQ-oCe+OHqX-4BBy_Le6g61A3nuZJwxLmj21_Ux2f0cH+Q@mail.gmail.com>
	<CA+B-+fx_1tZNoJQ5b1BreQCKGUR9VvBiQU3Q1_iE4iuB9obzmw@mail.gmail.com>
Date: Wed, 25 Mar 2015 07:59:59 -0700
Message-ID: <CA+B-+fwq1-SE4ntnRkJYagovn+8x66pGsBpEyLm80sgjUQ=6Mw@mail.gmail.com>
Subject: Re: mllib.recommendation Design
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133a6f42ead5d05121e279e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a6f42ead5d05121e279e
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

I am facing some minor issues in implementing Alternating Nonlinear
Minimization as documented in this JIRA due to the ALS code being in ml
package: https://issues.apache.org/jira/browse/SPARK-6323

I need to use Vectors.fromBreeze / Vectors.toBreeze but they are package
private on mllib. For now I removed private but not sure that's the correct
way...

I also need to re-use lot of building blocks from ml.ALS and so I am
writing ALM in ml package...

I thought the plan was to still write core algorithms in mllib and pipeline
integration in ml...It will be great if you can move the ALS object from ml
to mllib and that way I can also move ALM to mllib (which I feel is the
right place)...Of course the Pipeline based flow will stay in ml package...

We can decide later if ALM needs to be in recommendation or a better place
is package called factorization but the idea is that ALM will support MAP
(and may be KL divergence loss) with sparsity constraints (probability
simplex and bounds are fine for what I am focused at right now)...

Thanks.
Deb

On Tue, Feb 17, 2015 at 4:40 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> There is a usability difference...I am not sure if recommendation.ALS
> would like to add both userConstraint and productConstraint ? GraphLab CF
> for example has it and we are ready to support all the features for modest
> ranks where gram matrices can be made...
>
> For large ranks I am still working on the code
>
> On Tue, Feb 17, 2015 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> The current ALS implementation allow pluggable solvers for
>> NormalEquation, where we put CholeskeySolver and NNLS solver. Please
>> check the current implementation and let us know how your constraint
>> solver would fit. For a general matrix factorization package, let's
>> make a JIRA and move our discussion there. -Xiangrui
>>
>> On Fri, Feb 13, 2015 at 7:46 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > Hi,
>> >
>> > I am bit confused on the mllib design in the master. I thought that core
>> > algorithms will stay in mllib and ml will define the pipelines over the
>> > core algorithm but looks like in master ALS is moved from mllib to ml...
>> >
>> > I am refactoring my PR to a factorization package and I want to build
>> it on
>> > top of ml.recommendation.ALS (possibly extend from ml.recommendation.ALS
>> > since first version will use very similar RDD handling as ALS and a
>> > proximal solver that's being added to breeze)
>> >
>> > https://issues.apache.org/jira/browse/SPARK-2426
>> > https://github.com/scalanlp/breeze/pull/321
>> >
>> > Basically I am not sure if we should merge it with recommendation.ALS
>> since
>> > this is more generic than recommendation. I am considering calling it
>> > ConstrainedALS where user can specify different constraint for user and
>> > product factors (Similar to GraphLab CF structure).
>> >
>> > I am also working on ConstrainedALM where the underlying algorithm is no
>> > longer ALS but nonlinear alternating minimization with constraints.
>> > https://github.com/scalanlp/breeze/pull/364
>> > This will let us do large rank matrix completion where there is no need
>> to
>> > construct gram matrices. I will open up the JIRA soon after getting
>> initial
>> > results
>> >
>> > I am bit confused that where should I add the factorization package. It
>> > will use the current ALS test-cases and I have to construct more
>> test-cases
>> > for sparse coding and PLSA formulations.
>> >
>> > Thanks.
>> > Deb
>>
>
>

--001a1133a6f42ead5d05121e279e--

From dev-return-12169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 15:15:40 2015
Return-Path: <dev-return-12169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 290AC17AB1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 15:15:40 +0000 (UTC)
Received: (qmail 92157 invoked by uid 500); 25 Mar 2015 15:15:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92079 invoked by uid 500); 25 Mar 2015 15:15:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92067 invoked by uid 99); 25 Mar 2015 15:15:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 15:15:35 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.215.51 as permitted sender)
Received: from [209.85.215.51] (HELO mail-la0-f51.google.com) (209.85.215.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 15:15:30 +0000
Received: by lagg8 with SMTP id g8so22509686lag.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 08:15:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=eHbh6xmjQEuh+87jUZXTTtzuaOQn5TDDaGaxjPjZnXk=;
        b=dqYo/dI9A7/NATVmrfYgjqddmu0TgmrgWTmNZTMjd2rjzQ1gsb7TAPZjLqTwwqiYXu
         ChYpXqA7UwcVIfs6E3XeWYBXEx6fvBGdCU/J06AoHh5R6Su30ElQxDMepXu1rT0wf7Xh
         3JKj17E0bsuC7X5ccQStrBOYZfZHEL3NF4e0YGTQrPer7TeMOEASjYxutt6OhKIJJp6x
         hLrwc74MMnkJxdBbEPmt/lESM9b22JSU3KBOr/7VTzSzWEIAxFq+oOIaBPE58heA+dXn
         Ox/ARHgUx6i496AwudWFrTCNNK7sawAG5YD9IPcdo0oqlUJfEdNe/Zbgm07tgH1kqygk
         Ft1w==
MIME-Version: 1.0
X-Received: by 10.112.241.72 with SMTP id wg8mr8961523lbc.65.1427296509155;
 Wed, 25 Mar 2015 08:15:09 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Wed, 25 Mar 2015 08:15:09 -0700 (PDT)
Date: Wed, 25 Mar 2015 08:15:09 -0700
Message-ID: <CA+B-+fwZ50HOuAKJjkcy87Rym++kdh4A_Gj32N6cZZy-7WwZZA@mail.gmail.com>
Subject: LogisticGradient Design
From: Debasish Das <debasish.das83@gmail.com>
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1133a6f46ae11405121e5da9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133a6f46ae11405121e5da9
Content-Type: text/plain; charset=UTF-8

Hi,

Right now LogisticGradient implements both binary and multi-class in the
same class using an if-else statement which is a bit convoluted.

For Generalized matrix factorization, if the data has distinct ratings I
want to use LeastSquareGradient (regression has given best results to date)
but if the data has binary labels 0/1 based on domain knowledge (implicit
for example, visits no-visits) I want to use a LogisticGradient without any
overhead for multi-class if-else...

I can compare the performance of LeastSquareGradient and multi-class
LogisticGradient on the recommendation metrics but it will be great if we
can separate binary and multi-class in Separate
classes....MultiClassLogistic can extend BinaryLogistic but mixing them in
the same class is an overhead for users (like me) who wants to use
BinaryLogistic for his application..

Thanks.
Deb

--001a1133a6f46ae11405121e5da9--

From dev-return-12170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 17:05:35 2015
Return-Path: <dev-return-12170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB37D17C85
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 17:05:35 +0000 (UTC)
Received: (qmail 43574 invoked by uid 500); 25 Mar 2015 17:05:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43497 invoked by uid 500); 25 Mar 2015 17:05:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43484 invoked by uid 99); 25 Mar 2015 17:05:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 17:05:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 17:05:09 +0000
Received: by lbbsy1 with SMTP id sy1so22416540lbb.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 10:03:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=tFwt/4/4tRjF4y9RXpwr3jHM+Tac0CYvfmJcIGmpiKs=;
        b=Eoz45+kP3a8Vb5RvGPQZQXKTMItxplAYDWWgAyBtcUYaJn82k/KZGwdzEbL7UsY6UC
         rAEZaZDMJHZLkawZuBbjz106rOawMDTSQsmlJki616f8ijL+iicdCC6c0r1NsiHONUPs
         eX4/1sXcOZYmtqGLAqIzqbA5nrBh6QEathXywOxVSRXkBcTrQVAjIcNhara6STISV90p
         GNrGMg+vxiUXhhMG/gOcsiagB4Vc7WUohzN11RvPg7thRN9PXjtmd3WSCG6VLRS8foLb
         CEI6Jum0pGC1pli0uq8J9r7o/UbqPBkZJJtGMHxGD9HES50M7x4NbnlAM4IskgkD+JHk
         00ug==
X-Gm-Message-State: ALoCoQkLd0NH+FrKP7s4vbgxHSTPi/Twrepik+huno5c8dBNZTDsbXpN5vLZNqaXW0EMDiNDR5lX
X-Received: by 10.152.22.229 with SMTP id h5mr9415536laf.21.1427302997666;
 Wed, 25 Mar 2015 10:03:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.114.2.74 with HTTP; Wed, 25 Mar 2015 10:02:57 -0700 (PDT)
From: shane knapp <sknapp@berkeley.edu>
Date: Wed, 25 Mar 2015 10:02:57 -0700
Message-ID: <CACdU-dTi-3CyqqG1PJsCzU-_vPWm-6mP4WwMqe94NfP10jjM-A@mail.gmail.com>
Subject: jenkins upgraded to 1.606....
To: amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160b6de29bc9805121fe02b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b6de29bc9805121fe02b
Content-Type: text/plain; charset=UTF-8

...due to some big security fixes:

https://wiki.jenkins-ci.org/display/SECURITY/Jenkins+Security+Advisory+2015-03-23

:)

shane

--089e0160b6de29bc9805121fe02b--

From dev-return-12171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 18:44:30 2015
Return-Path: <dev-return-12171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DAA0C1747E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 18:44:29 +0000 (UTC)
Received: (qmail 4091 invoked by uid 500); 25 Mar 2015 18:44:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4016 invoked by uid 500); 25 Mar 2015 18:44:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4001 invoked by uid 99); 25 Mar 2015 18:44:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:44:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:44:03 +0000
Received: by oifl3 with SMTP id l3so29608510oif.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 11:41:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=DafWGwbNPZk3Ub3278YLhKoNfY9Jp/7qksoe8rLQEg8=;
        b=MUmDWe956aXTpPa8tEPCFQ/wr+abGI5C8P3xrzn6s8vaa4nQoWHgZspngER4b8mCSj
         4z/H02xnsrvLcF9vbDk76cV8BNQrkElyq28ovKswWgRB0AjuVW/bNuTGCE/bW5p/onrG
         6hUgpquGOWHz3JAnQPfTTAlTU61RevEqYLYL2aPtCxqNcA2DXaAmwccnjfmYLRLtifrY
         zuAKVBylu9EjHK5aAmLPDcKNCYIf+BOsTvARqzYQVfxec5HYOJq0jL1g0lhIMZ13/98P
         YnV2XwXQfrcD2ui7SYv8u93lDhwj1CWr4Zca8ca1JPDEAxOqIk1T6IdQb6HqYb8ajAXT
         QSTQ==
MIME-Version: 1.0
X-Received: by 10.182.24.34 with SMTP id r2mr8730031obf.43.1427308906219; Wed,
 25 Mar 2015 11:41:46 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Wed, 25 Mar 2015 11:41:46 -0700 (PDT)
In-Reply-To: <CANx3uAjtSWMivcyBvzqMB_giK66LejWDj4uLw2-4JFtAND-+Pg@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
	<CANx3uAjb78p0NUSgnUw7dKTek+oLH1=NSz_CLps2T2SdWFmV0g@mail.gmail.com>
	<CANx3uAiWX8k=55hXnjmUoL5V7JQYyNCcPhwOZgNk9s7pQp2WNg@mail.gmail.com>
	<CABPQxsu4i7ae_FaR99ASXfUgFmGp27MB4s8t9v46gB0TZUMCfg@mail.gmail.com>
	<CANx3uAjtSWMivcyBvzqMB_giK66LejWDj4uLw2-4JFtAND-+Pg@mail.gmail.com>
Date: Wed, 25 Mar 2015 11:41:46 -0700
Message-ID: <CABPQxssGMtRVUiDFgUvq7p8JPr8ehwRzYCinmviru_NKr4gvaA@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Patrick Wendell <pwendell@gmail.com>
To: Koert Kuipers <koert@tresata.com>
Cc: Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah I agree that might have been nicer, but I think for consistency
with the input API's maybe we should do the same thing. We can also
give an example of how to clone sc.hadoopConfiguration and then set
some new values:

val conf = sc.hadoopConfiguration.clone()
  .set("k1", "v1")
  .set("k2", "v2")

val rdd = sc.objectFile(..., conf)

I have no idea if that's the correct syntax, but something like that
seems almost as easy as passing a hashmap with deltas.

- Patrick

On Wed, Mar 25, 2015 at 6:34 AM, Koert Kuipers <koert@tresata.com> wrote:
> my personal preference would be something like a Map[String, String] that
> only reflects the changes you want to make the Configuration for the given
> input/output format (so system wide defaults continue to come from
> sc.hadoopConfiguration), similarly to what cascading/scalding did, but am
> arbitrary Configuration will work too.
>
> i will make a jira and pullreq when i have some time.
>
>
>
> On Wed, Mar 25, 2015 at 1:23 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>> I see - if you look, in the saving functions we have the option for
>> the user to pass an arbitrary Configuration.
>>
>>
>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L894
>>
>> It seems fine to have the same option for the loading functions, if
>> it's easy to just pass this config into the input format.
>>
>>
>>
>> On Tue, Mar 24, 2015 at 3:46 PM, Koert Kuipers <koert@tresata.com> wrote:
>> > the (compression) codec parameter that is now part of many saveAs...
>> > methods
>> > came from a similar need. see SPARK-763
>> > hadoop has many options like this. you either going to have to allow
>> > many
>> > more of these optional arguments to all the methods that read from
>> > hadoop
>> > inputformats and write to hadoop outputformats, or you force people to
>> > re-create these methods using HadoopRDD, i think (if thats even
>> > possible).
>> >
>> > On Tue, Mar 24, 2015 at 6:40 PM, Koert Kuipers <koert@tresata.com>
>> > wrote:
>> >>
>> >> i would like to use objectFile with some tweaks to the hadoop conf.
>> >> currently there is no way to do that, except recreating objectFile
>> >> myself.
>> >> and some of the code objectFile uses i have no access to, since its
>> >> private
>> >> to spark.
>> >>
>> >>
>> >> On Tue, Mar 24, 2015 at 2:59 PM, Patrick Wendell <pwendell@gmail.com>
>> >> wrote:
>> >>>
>> >>> Yeah - to Nick's point, I think the way to do this is to pass in a
>> >>> custom conf when you create a Hadoop RDD (that's AFAIK why the conf
>> >>> field is there). Is there anything you can't do with that feature?
>> >>>
>> >>> On Tue, Mar 24, 2015 at 11:50 AM, Nick Pentreath
>> >>> <nick.pentreath@gmail.com> wrote:
>> >>> > Imran, on your point to read multiple files together in a partition,
>> >>> > is
>> >>> > it
>> >>> > not simpler to use the approach of copy Hadoop conf and set per-RDD
>> >>> > settings for min split to control the input size per partition,
>> >>> > together
>> >>> > with something like CombineFileInputFormat?
>> >>> >
>> >>> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
>> >>> > wrote:
>> >>> >
>> >>> >> I think this would be a great addition, I totally agree that you
>> >>> >> need
>> >>> >> to be
>> >>> >> able to set these at a finer context than just the SparkContext.
>> >>> >>
>> >>> >> Just to play devil's advocate, though -- the alternative is for you
>> >>> >> just
>> >>> >> subclass HadoopRDD yourself, or make a totally new RDD, and then
>> >>> >> you
>> >>> >> could
>> >>> >> expose whatever you need.  Why is this solution better?  IMO the
>> >>> >> criteria
>> >>> >> are:
>> >>> >> (a) common operations
>> >>> >> (b) error-prone / difficult to implement
>> >>> >> (c) non-obvious, but important for performance
>> >>> >>
>> >>> >> I think this case fits (a) & (c), so I think its still worthwhile.
>> >>> >> But its
>> >>> >> also worth asking whether or not its too difficult for a user to
>> >>> >> extend
>> >>> >> HadoopRDD right now.  There have been several cases in the past
>> >>> >> week
>> >>> >> where
>> >>> >> we've suggested that a user should read from hdfs themselves (eg.,
>> >>> >> to
>> >>> >> read
>> >>> >> multiple files together in one partition) -- with*out* reusing the
>> >>> >> code in
>> >>> >> HadoopRDD, though they would lose things like the metric tracking &
>> >>> >> preferred locations you get from HadoopRDD.  Does HadoopRDD need to
>> >>> >> some
>> >>> >> refactoring to make that easier to do?  Or do we just need a good
>> >>> >> example?
>> >>> >>
>> >>> >> Imran
>> >>> >>
>> >>> >> (sorry for hijacking your thread, Koert)
>> >>> >>
>> >>> >>
>> >>> >>
>> >>> >> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
>> >>> >> wrote:
>> >>> >>
>> >>> >> > see email below. reynold suggested i send it to dev instead of
>> >>> >> > user
>> >>> >> >
>> >>> >> > ---------- Forwarded message ----------
>> >>> >> > From: Koert Kuipers <koert@tresata.com>
>> >>> >> > Date: Mon, Mar 23, 2015 at 4:36 PM
>> >>> >> > Subject: hadoop input/output format advanced control
>> >>> >> > To: "user@spark.apache.org" <user@spark.apache.org>
>> >>> >> >
>> >>> >> >
>> >>> >> > currently its pretty hard to control the Hadoop Input/Output
>> >>> >> > formats
>> >>> >> > used
>> >>> >> > in Spark. The conventions seems to be to add extra parameters to
>> >>> >> > all
>> >>> >> > methods and then somewhere deep inside the code (for example in
>> >>> >> > PairRDDFunctions.saveAsHadoopFile) all these parameters get
>> >>> >> > translated
>> >>> >> into
>> >>> >> > settings on the Hadoop Configuration object.
>> >>> >> >
>> >>> >> > for example for compression i see "codec: Option[Class[_ <:
>> >>> >> > CompressionCodec]] = None" added to a bunch of methods.
>> >>> >> >
>> >>> >> > how scalable is this solution really?
>> >>> >> >
>> >>> >> > for example i need to read from a hadoop dataset and i dont want
>> >>> >> > the
>> >>> >> input
>> >>> >> > (part) files to get split up. the way to do this is to set
>> >>> >> > "mapred.min.split.size". now i dont want to set this at the level
>> >>> >> > of
>> >>> >> > the
>> >>> >> > SparkContext (which can be done), since i dont want it to apply
>> >>> >> > to
>> >>> >> > input
>> >>> >> > formats in general. i want it to apply to just this one specific
>> >>> >> > input
>> >>> >> > dataset i need to read. which leaves me with no options
>> >>> >> > currently. i
>> >>> >> could
>> >>> >> > go add yet another input parameter to all the methods
>> >>> >> > (SparkContext.textFile, SparkContext.hadoopFile,
>> >>> >> > SparkContext.objectFile,
>> >>> >> > etc.). but that seems ineffective.
>> >>> >> >
>> >>> >> > why can we not expose a Map[String, String] or some other generic
>> >>> >> > way to
>> >>> >> > manipulate settings for hadoop input/output formats? it would
>> >>> >> > require
>> >>> >> > adding one more parameter to all methods to deal with hadoop
>> >>> >> > input/output
>> >>> >> > formats, but after that its done. one parameter to rule them
>> >>> >> > all....
>> >>> >> >
>> >>> >> > then i could do:
>> >>> >> > val x = sc.textFile("/some/path", formatSettings =
>> >>> >> > Map("mapred.min.split.size" -> "12345"))
>> >>> >> >
>> >>> >> > or
>> >>> >> > rdd.saveAsTextFile("/some/path, formatSettings =
>> >>> >> > Map(mapred.output.compress" -> "true",
>> >>> >> > "mapred.output.compression.codec"
>> >>> >> ->
>> >>> >> > "somecodec"))
>> >>> >> >
>> >>> >>
>> >>>
>> >>> ---------------------------------------------------------------------
>> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> >>> For additional commands, e-mail: dev-help@spark.apache.org
>> >>>
>> >>
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 18:50:44 2015
Return-Path: <dev-return-12172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 55805175BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 18:50:44 +0000 (UTC)
Received: (qmail 42758 invoked by uid 500); 25 Mar 2015 18:50:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42673 invoked by uid 500); 25 Mar 2015 18:50:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42656 invoked by uid 99); 25 Mar 2015 18:50:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:50:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.175] (HELO mail-pd0-f175.google.com) (209.85.192.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:50:37 +0000
Received: by pdnc3 with SMTP id c3so37089025pdn.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 11:49:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:date:from:to:cc:message-id:in-reply-to
         :references:subject:mime-version:content-type;
        bh=YQwTEl+2hULZrbBZSKAvCyIUKqBm5DrgOnl6m8MPiwA=;
        b=Ap56IG0yyd3bZDxgm1/gtF6Xitnm2IM85kLjKgd5wijStn2Oop5ImRDQkbedG2o1hM
         DkSwPr4RTFzwT+LYLggeDsdsVjyLz7Fs+gpCaHh4d/GRs6At+Pog0bBDEhxMK2R7JO+P
         5JnFJr+zicwBk+IhQAAraiud7EVDB5B7l6kMUZyEmAgbYEejevkQAU3aBXZwPfPUZLii
         HLVhf8bP4jsIaFfkQmyZbrIH2x1pMe9WwGoxlXfXpBmgm1/qTNIv3ZYfUPhuVQxEwNYc
         qHehnKdX95Tr7M/Y0BM4DtppQvqElA8qddbN/v6gvIIWPGLEwwNunZRhCuLq6PBcbEKz
         QwlQ==
X-Gm-Message-State: ALoCoQnr5+OZre3SbFqMstwm0rHFEqk1VUOdffFgAWuMg2ecqz1/8f+qrnT5NpII9fgCuwjbDmRR
X-Received: by 10.68.69.77 with SMTP id c13mr20132773pbu.38.1427309351411;
        Wed, 25 Mar 2015 11:49:11 -0700 (PDT)
Received: from [192.168.1.11] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id jd5sm3167556pbd.35.2015.03.25.11.49.09
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Wed, 25 Mar 2015 11:49:10 -0700 (PDT)
Date: Wed, 25 Mar 2015 11:49:08 -0700
From: Davies Liu <davies@databricks.com>
To: Karlson <ksonspark@siberie.de>
Cc: dev@spark.apache.org
Message-ID: <7461D7919B3047C5821FC90B78F82254@databricks.com>
In-Reply-To: <3560e95f48681fccd9266e86bbff0e51@siberie.de>
References: <3560e95f48681fccd9266e86bbff0e51@siberie.de>
Subject: Re: functools.partial as UserDefinedFunction
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="55130324_435d38d_11a"
X-Virus-Checked: Checked by ClamAV on apache.org

--55130324_435d38d_11a
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

It=E2=80=99s good to support functools.partial, could you file a JIRA for=
 it=3F


On Wednesday, March 25, 2015 at 5:42 AM, Karlson wrote:

> =20
> Hi all,
> =20
> passing a functools.partial-function as a UserDefined=46unction to =20
> Data=46rame.select raises an AttributeException, because functools.part=
ial =20
> does not have the attribute =5F=5Fname=5F=5F. Is there any alternative =
to =20
> relying on =5F=5Fname=5F=5F in pyspark/sql/functions.py:126 =3F
> =20
> =20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe=40spark.apache.org (mailto:dev-=
unsubscribe=40spark.apache.org)
> =46or additional commands, e-mail: dev-help=40spark.apache.org (mailto:=
dev-help=40spark.apache.org)
> =20
> =20



--55130324_435d38d_11a--


From dev-return-12173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 18:52:39 2015
Return-Path: <dev-return-12173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4AA4A1762C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 18:52:39 +0000 (UTC)
Received: (qmail 53334 invoked by uid 500); 25 Mar 2015 18:52:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53261 invoked by uid 500); 25 Mar 2015 18:52:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53250 invoked by uid 99); 25 Mar 2015 18:52:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:52:37 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.223.170 as permitted sender)
Received: from [209.85.223.170] (HELO mail-ie0-f170.google.com) (209.85.223.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:52:32 +0000
Received: by iedfl3 with SMTP id fl3so33055162ied.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 11:51:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=HBvGfN/X9nQnO6NmzwP9B0Y5EesXickrSUp2XqP6Vt0=;
        b=Z44J3Fqozqey517u1deAF7CHU81SgBKer2AduwhcMHTqWklMgN+P5fsdwsz1zbZXWA
         zQG5lvFxYXZDDK2griKlToPOtoUn7EIqg/SBe/NGeFKcVXXTlwr86mD9um8GR2AWSt/h
         uDQ/guKKAfK/AKxv8pVPaJUd4kfYfTzLO0Um9pR3oF+CS742nQFdvlsYXxdRVz7v0+aN
         ngCi9nV5uk8qtJF698bflEX70r4YRCAEGWf27aPR1JWfUnXVHKePz3AO9oqQaW6KhEav
         c2Pxh0msQmcuDz1MnNkhEccC/RqJLtJW6T+Q6gazyNXQgJeYiJaQXgUniqZBGRJBeb6j
         f4tQ==
X-Gm-Message-State: ALoCoQkCzpVpv0yfnTCsWccw6AsqqE7HOGWjg2EPl1ORsx3DjKCvtRz6+ggijygroz7OyOHUICt0
MIME-Version: 1.0
X-Received: by 10.50.253.12 with SMTP id zw12mr31046634igc.24.1427309497027;
 Wed, 25 Mar 2015 11:51:37 -0700 (PDT)
Received: by 10.36.192.2 with HTTP; Wed, 25 Mar 2015 11:51:36 -0700 (PDT)
In-Reply-To: <CABPQxstq5=2gv=Ov30gWzKW6jTxfHwXfCcrFXrcRd7y3bBGCKA@mail.gmail.com>
References: <CABPQxstq5=2gv=Ov30gWzKW6jTxfHwXfCcrFXrcRd7y3bBGCKA@mail.gmail.com>
Date: Wed, 25 Mar 2015 11:51:36 -0700
Message-ID: <CAAOnQ7uULcxGHj7mjW6fK0pED89birFSVAx8+Tpo55G+HPM3XA@mail.gmail.com>
Subject: Re: Experience using binary packages on various Hadoop distros
From: Marcelo Vanzin <vanzin@cloudera.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Patrick,

The only issue I've seen so far has been the YARN container ID issue.
That can be technically be described as a breakage in forwards
compatibility in YARN. The APIs didn't break, but the data transferred
through YARN's protocol has, and the old library cannot understand the
data sent by a new service (the new container ID).

The main issue with publishing BYOH is what Matei already mentioned.
It would be worth it to take a look at what projects that depend on
Hadoop do, though.

Speaking with the Cloudera hat on, Spark in CDH is already "BYOH",
except Hadoop is already there with the rest of CDH.


On Tue, Mar 24, 2015 at 12:05 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey All,
>
> For a while we've published binary packages with different Hadoop
> client's pre-bundled. We currently have three interfaces to a Hadoop
> cluster (a) the HDFS client (b) the YARN client (c) the Hive client.
>
> Because (a) and (b) are supposed to be backwards compatible
> interfaces. My working assumption was that for the most part (modulo
> Hive) our packages work with *newer* Hadoop versions. For instance,
> our Hadoop 2.4 package should work with HDFS 2.6 and YARN 2.6.
> However, I have heard murmurings that these are not compatible in
> practice.
>
> So I have three questions I'd like to put out to the community:
>
> 1. Have people had difficulty using 2.4 packages with newer Hadoop
> versions? If so, what specific incompatibilities have you hit?
> 2. Have people had issues using our binary Hadoop packages in general
> with commercial or Apache Hadoop distro's, such that you have to build
> from source?
> 3. How would people feel about publishing a "bring your own Hadoop"
> binary, where you are required to point us to a local Hadoop
> distribution by setting HADOOP_HOME? This might be better for ensuring
> full compatibility:
> https://issues.apache.org/jira/browse/SPARK-6511
>
> - Patrick
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>



-- 
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 18:58:05 2015
Return-Path: <dev-return-12174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0831C1772A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 18:58:05 +0000 (UTC)
Received: (qmail 70491 invoked by uid 500); 25 Mar 2015 18:58:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70412 invoked by uid 500); 25 Mar 2015 18:58:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70401 invoked by uid 99); 25 Mar 2015 18:58:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:58:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 18:57:37 +0000
Received: by igbqf9 with SMTP id qf9so33846528igb.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 11:57:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=AH7zn2yYuxQwSfP+UPKrZ7nAhP2HEXt1xH2IzwlzMVU=;
        b=KXbKq9dApR1bUd4dDzl0mvhip8RryoYRzeos8UoZ33ray4d4HuqkLTyIaox2Xic1VA
         H2dUUU5JnPQaYDWIZ1/z/CNWzCrCgz16XfRNfQygaQci4L5+1ZG0jdjoZM5M97sY5QoE
         b2v0m82rWG6uwzWCHvXjgdFEm6hj4SHmmYwdmpt15txXxDoNMHqqSV/VP0GdwToyrWTa
         p+L4HcEqA/oYg4sAdiv4R9J9vh9YkuKJf3qC0oCQ8ZQuPzZdon36b/SLhAHgxP+NlFtQ
         cH6jofxKG2Q8OItIDxsdHTSDMQGmUrM49aRZNsNEzUi2OFRh+7ud5E+4X60wq3j/rkAg
         071A==
X-Gm-Message-State: ALoCoQkmkyWAdcE4mymO/5nxZO7I3PaxQbDMjuYiQXZfMVTeBWhRT7ExZWmd/+Sme87P2ColKtPZ
MIME-Version: 1.0
X-Received: by 10.51.17.7 with SMTP id ga7mr31093788igd.42.1427309835134; Wed,
 25 Mar 2015 11:57:15 -0700 (PDT)
Received: by 10.36.58.2 with HTTP; Wed, 25 Mar 2015 11:57:15 -0700 (PDT)
In-Reply-To: <CA+B-+fwZ50HOuAKJjkcy87Rym++kdh4A_Gj32N6cZZy-7WwZZA@mail.gmail.com>
References: <CA+B-+fwZ50HOuAKJjkcy87Rym++kdh4A_Gj32N6cZZy-7WwZZA@mail.gmail.com>
Date: Wed, 25 Mar 2015 11:57:15 -0700
Message-ID: <CAF7ADNrcC-w+ZH9S_0R-y_iEi8GKPV7ybHsmP4SF2JD=HynnSg@mail.gmail.com>
Subject: Re: LogisticGradient Design
From: Joseph Bradley <joseph@databricks.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135fe3eb541ce0512217743
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135fe3eb541ce0512217743
Content-Type: text/plain; charset=UTF-8

It would be nice to see how big a performance hit we take from combining
binary & multiclass logistic loss/gradient.  If it's not a big hit, then it
might be simpler from an outside API perspective to keep them in 1 class
(even if it's more complicated within).
Joseph

On Wed, Mar 25, 2015 at 8:15 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi,
>
> Right now LogisticGradient implements both binary and multi-class in the
> same class using an if-else statement which is a bit convoluted.
>
> For Generalized matrix factorization, if the data has distinct ratings I
> want to use LeastSquareGradient (regression has given best results to date)
> but if the data has binary labels 0/1 based on domain knowledge (implicit
> for example, visits no-visits) I want to use a LogisticGradient without any
> overhead for multi-class if-else...
>
> I can compare the performance of LeastSquareGradient and multi-class
> LogisticGradient on the recommendation metrics but it will be great if we
> can separate binary and multi-class in Separate
> classes....MultiClassLogistic can extend BinaryLogistic but mixing them in
> the same class is an overhead for users (like me) who wants to use
> BinaryLogistic for his application..
>
> Thanks.
> Deb
>

--001a1135fe3eb541ce0512217743--

From dev-return-12175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 19:00:20 2015
Return-Path: <dev-return-12175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C5FF2177B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 19:00:20 +0000 (UTC)
Received: (qmail 77526 invoked by uid 500); 25 Mar 2015 19:00:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77443 invoked by uid 500); 25 Mar 2015 19:00:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77432 invoked by uid 99); 25 Mar 2015 19:00:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 19:00:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 19:00:14 +0000
Received: by wixw10 with SMTP id w10so52135799wix.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 11:58:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=C/2bz3w2aGxPcHc+u5K1BXUEOfxXJW3SBkNCWBxc+kY=;
        b=NhAqmjztB1pUJj8IPZZfsERfV7qdarE1Ig4hxjprKb4biN322G5uirQlfQylq7RWjL
         2k152Aihqksqi+o2ZRWS/+4cC248FLIubqPIT712n/GXFKn4obTm0SJNipP58Y4a7Ocf
         Rnkf2EmC2l0YhoR8WlGqQAXKsZ9eAu1iqds5ilPZuRFUknZ2LFrAGWjVHK07L6818gWX
         womV/IYfd7JX5MZ5P9xVJC314Y8bktymywVZWfk1JdYdv2Kq7dv78mqIOeDZYe94ek73
         0Il6bm11V5xS2wE88+uFE0MTcSOQbioE8U8uT4jSpOuxR/378GtT8Qj/dFYCdwnkdhJ8
         +1lw==
X-Gm-Message-State: ALoCoQkAXb6szNyS8/Vjc2/FPf/84Rj2IdMsafxvpJCD94nFokECN3FTT2XQaY5cvaUJrY2F4UE1
MIME-Version: 1.0
X-Received: by 10.194.9.98 with SMTP id y2mr21175816wja.85.1427309883329; Wed,
 25 Mar 2015 11:58:03 -0700 (PDT)
Received: by 10.28.47.193 with HTTP; Wed, 25 Mar 2015 11:58:03 -0700 (PDT)
X-Originating-IP: [204.148.13.62]
In-Reply-To: <CABPQxssGMtRVUiDFgUvq7p8JPr8ehwRzYCinmviru_NKr4gvaA@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CABPQxsu+x5kwvM2gQEm81NkiM2FL75ojCNhTpGDBNvCfEfbCGQ@mail.gmail.com>
	<CANx3uAjb78p0NUSgnUw7dKTek+oLH1=NSz_CLps2T2SdWFmV0g@mail.gmail.com>
	<CANx3uAiWX8k=55hXnjmUoL5V7JQYyNCcPhwOZgNk9s7pQp2WNg@mail.gmail.com>
	<CABPQxsu4i7ae_FaR99ASXfUgFmGp27MB4s8t9v46gB0TZUMCfg@mail.gmail.com>
	<CANx3uAjtSWMivcyBvzqMB_giK66LejWDj4uLw2-4JFtAND-+Pg@mail.gmail.com>
	<CABPQxssGMtRVUiDFgUvq7p8JPr8ehwRzYCinmviru_NKr4gvaA@mail.gmail.com>
Date: Wed, 25 Mar 2015 14:58:03 -0400
Message-ID: <CANx3uAjgUZ8jdaVctsgFK9kZBAw5pwUSTR-nDmEKKgpnYpAvkA@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Koert Kuipers <koert@tresata.com>
To: Patrick Wendell <pwendell@gmail.com>
Cc: Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5d8dab94a7610512217a25
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d8dab94a7610512217a25
Content-Type: text/plain; charset=UTF-8

yeah fair enough

On Wed, Mar 25, 2015 at 2:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Yeah I agree that might have been nicer, but I think for consistency
> with the input API's maybe we should do the same thing. We can also
> give an example of how to clone sc.hadoopConfiguration and then set
> some new values:
>
> val conf = sc.hadoopConfiguration.clone()
>   .set("k1", "v1")
>   .set("k2", "v2")
>
> val rdd = sc.objectFile(..., conf)
>
> I have no idea if that's the correct syntax, but something like that
> seems almost as easy as passing a hashmap with deltas.
>
> - Patrick
>
> On Wed, Mar 25, 2015 at 6:34 AM, Koert Kuipers <koert@tresata.com> wrote:
> > my personal preference would be something like a Map[String, String] that
> > only reflects the changes you want to make the Configuration for the
> given
> > input/output format (so system wide defaults continue to come from
> > sc.hadoopConfiguration), similarly to what cascading/scalding did, but am
> > arbitrary Configuration will work too.
> >
> > i will make a jira and pullreq when i have some time.
> >
> >
> >
> > On Wed, Mar 25, 2015 at 1:23 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >>
> >> I see - if you look, in the saving functions we have the option for
> >> the user to pass an arbitrary Configuration.
> >>
> >>
> >>
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala#L894
> >>
> >> It seems fine to have the same option for the loading functions, if
> >> it's easy to just pass this config into the input format.
> >>
> >>
> >>
> >> On Tue, Mar 24, 2015 at 3:46 PM, Koert Kuipers <koert@tresata.com>
> wrote:
> >> > the (compression) codec parameter that is now part of many saveAs...
> >> > methods
> >> > came from a similar need. see SPARK-763
> >> > hadoop has many options like this. you either going to have to allow
> >> > many
> >> > more of these optional arguments to all the methods that read from
> >> > hadoop
> >> > inputformats and write to hadoop outputformats, or you force people to
> >> > re-create these methods using HadoopRDD, i think (if thats even
> >> > possible).
> >> >
> >> > On Tue, Mar 24, 2015 at 6:40 PM, Koert Kuipers <koert@tresata.com>
> >> > wrote:
> >> >>
> >> >> i would like to use objectFile with some tweaks to the hadoop conf.
> >> >> currently there is no way to do that, except recreating objectFile
> >> >> myself.
> >> >> and some of the code objectFile uses i have no access to, since its
> >> >> private
> >> >> to spark.
> >> >>
> >> >>
> >> >> On Tue, Mar 24, 2015 at 2:59 PM, Patrick Wendell <pwendell@gmail.com
> >
> >> >> wrote:
> >> >>>
> >> >>> Yeah - to Nick's point, I think the way to do this is to pass in a
> >> >>> custom conf when you create a Hadoop RDD (that's AFAIK why the conf
> >> >>> field is there). Is there anything you can't do with that feature?
> >> >>>
> >> >>> On Tue, Mar 24, 2015 at 11:50 AM, Nick Pentreath
> >> >>> <nick.pentreath@gmail.com> wrote:
> >> >>> > Imran, on your point to read multiple files together in a
> partition,
> >> >>> > is
> >> >>> > it
> >> >>> > not simpler to use the approach of copy Hadoop conf and set
> per-RDD
> >> >>> > settings for min split to control the input size per partition,
> >> >>> > together
> >> >>> > with something like CombineFileInputFormat?
> >> >>> >
> >> >>> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <
> irashid@cloudera.com>
> >> >>> > wrote:
> >> >>> >
> >> >>> >> I think this would be a great addition, I totally agree that you
> >> >>> >> need
> >> >>> >> to be
> >> >>> >> able to set these at a finer context than just the SparkContext.
> >> >>> >>
> >> >>> >> Just to play devil's advocate, though -- the alternative is for
> you
> >> >>> >> just
> >> >>> >> subclass HadoopRDD yourself, or make a totally new RDD, and then
> >> >>> >> you
> >> >>> >> could
> >> >>> >> expose whatever you need.  Why is this solution better?  IMO the
> >> >>> >> criteria
> >> >>> >> are:
> >> >>> >> (a) common operations
> >> >>> >> (b) error-prone / difficult to implement
> >> >>> >> (c) non-obvious, but important for performance
> >> >>> >>
> >> >>> >> I think this case fits (a) & (c), so I think its still
> worthwhile.
> >> >>> >> But its
> >> >>> >> also worth asking whether or not its too difficult for a user to
> >> >>> >> extend
> >> >>> >> HadoopRDD right now.  There have been several cases in the past
> >> >>> >> week
> >> >>> >> where
> >> >>> >> we've suggested that a user should read from hdfs themselves
> (eg.,
> >> >>> >> to
> >> >>> >> read
> >> >>> >> multiple files together in one partition) -- with*out* reusing
> the
> >> >>> >> code in
> >> >>> >> HadoopRDD, though they would lose things like the metric
> tracking &
> >> >>> >> preferred locations you get from HadoopRDD.  Does HadoopRDD need
> to
> >> >>> >> some
> >> >>> >> refactoring to make that easier to do?  Or do we just need a good
> >> >>> >> example?
> >> >>> >>
> >> >>> >> Imran
> >> >>> >>
> >> >>> >> (sorry for hijacking your thread, Koert)
> >> >>> >>
> >> >>> >>
> >> >>> >>
> >> >>> >> On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <
> koert@tresata.com>
> >> >>> >> wrote:
> >> >>> >>
> >> >>> >> > see email below. reynold suggested i send it to dev instead of
> >> >>> >> > user
> >> >>> >> >
> >> >>> >> > ---------- Forwarded message ----------
> >> >>> >> > From: Koert Kuipers <koert@tresata.com>
> >> >>> >> > Date: Mon, Mar 23, 2015 at 4:36 PM
> >> >>> >> > Subject: hadoop input/output format advanced control
> >> >>> >> > To: "user@spark.apache.org" <user@spark.apache.org>
> >> >>> >> >
> >> >>> >> >
> >> >>> >> > currently its pretty hard to control the Hadoop Input/Output
> >> >>> >> > formats
> >> >>> >> > used
> >> >>> >> > in Spark. The conventions seems to be to add extra parameters
> to
> >> >>> >> > all
> >> >>> >> > methods and then somewhere deep inside the code (for example in
> >> >>> >> > PairRDDFunctions.saveAsHadoopFile) all these parameters get
> >> >>> >> > translated
> >> >>> >> into
> >> >>> >> > settings on the Hadoop Configuration object.
> >> >>> >> >
> >> >>> >> > for example for compression i see "codec: Option[Class[_ <:
> >> >>> >> > CompressionCodec]] = None" added to a bunch of methods.
> >> >>> >> >
> >> >>> >> > how scalable is this solution really?
> >> >>> >> >
> >> >>> >> > for example i need to read from a hadoop dataset and i dont
> want
> >> >>> >> > the
> >> >>> >> input
> >> >>> >> > (part) files to get split up. the way to do this is to set
> >> >>> >> > "mapred.min.split.size". now i dont want to set this at the
> level
> >> >>> >> > of
> >> >>> >> > the
> >> >>> >> > SparkContext (which can be done), since i dont want it to apply
> >> >>> >> > to
> >> >>> >> > input
> >> >>> >> > formats in general. i want it to apply to just this one
> specific
> >> >>> >> > input
> >> >>> >> > dataset i need to read. which leaves me with no options
> >> >>> >> > currently. i
> >> >>> >> could
> >> >>> >> > go add yet another input parameter to all the methods
> >> >>> >> > (SparkContext.textFile, SparkContext.hadoopFile,
> >> >>> >> > SparkContext.objectFile,
> >> >>> >> > etc.). but that seems ineffective.
> >> >>> >> >
> >> >>> >> > why can we not expose a Map[String, String] or some other
> generic
> >> >>> >> > way to
> >> >>> >> > manipulate settings for hadoop input/output formats? it would
> >> >>> >> > require
> >> >>> >> > adding one more parameter to all methods to deal with hadoop
> >> >>> >> > input/output
> >> >>> >> > formats, but after that its done. one parameter to rule them
> >> >>> >> > all....
> >> >>> >> >
> >> >>> >> > then i could do:
> >> >>> >> > val x = sc.textFile("/some/path", formatSettings =
> >> >>> >> > Map("mapred.min.split.size" -> "12345"))
> >> >>> >> >
> >> >>> >> > or
> >> >>> >> > rdd.saveAsTextFile("/some/path, formatSettings =
> >> >>> >> > Map(mapred.output.compress" -> "true",
> >> >>> >> > "mapred.output.compression.codec"
> >> >>> >> ->
> >> >>> >> > "somecodec"))
> >> >>> >> >
> >> >>> >>
> >> >>>
> >> >>>
> ---------------------------------------------------------------------
> >> >>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> >> >>> For additional commands, e-mail: dev-help@spark.apache.org
> >> >>>
> >> >>
> >> >
> >
> >
>

--047d7b5d8dab94a7610512217a25--

From dev-return-12176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 20:45:50 2015
Return-Path: <dev-return-12176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A85F917829
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 20:45:50 +0000 (UTC)
Received: (qmail 65306 invoked by uid 500); 25 Mar 2015 20:45:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65227 invoked by uid 500); 25 Mar 2015 20:45:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65211 invoked by uid 99); 25 Mar 2015 20:45:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 20:45:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of irashid@cloudera.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 20:45:08 +0000
Received: by wibgn9 with SMTP id gn9so56750716wib.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 13:42:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ue48a4Bd45EASmKHWJRXw8bPjatIc4yCPVSkH1ibZkc=;
        b=XJH2afAJ59LfWM6rxbKwto+bGI9Lh4ykt5azBpBD8K+iHZmE5bvFeUHYlIRiaxm4TZ
         HyPPsck1W+Lf9BkaSTJ/6TMWbhO3x0eApWkFN9YxKBxkIe0UBv2uliua4nsGngp1yLuM
         BtcyiVWqjln9DxImVTnVuEAqL41Th15o0TY5LTIk0J9OUsh0I9p7f5SZTTIEfWE6J16Q
         8V9N0IhIIhd1SnrufDT8TSUH1NMXimcNrCLHCts8TTNQ6JNG1DG+iKshNG8H3MWuyX7I
         7xK34vJBHbTZAIs5iEAfaTwXPK7CngzuEDMD/28SjGAJ+Stvbt1yTFs7AtSTiBw4TrPn
         AQ4A==
X-Gm-Message-State: ALoCoQnJrA7BTwatWG1fh3y+VHNnEc79Rr9J63TjlAz3mTSve74C1MYHW19NhQvcbJO7S35BZBfm
X-Received: by 10.194.8.99 with SMTP id q3mr22212563wja.88.1427316172295; Wed,
 25 Mar 2015 13:42:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.62.197 with HTTP; Wed, 25 Mar 2015 13:42:32 -0700 (PDT)
In-Reply-To: <CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
 <CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
 <CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com> <CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
From: Imran Rashid <irashid@cloudera.com>
Date: Wed, 25 Mar 2015 15:42:32 -0500
Message-ID: <CA+3qhFQUdMb+AWtSmNWTzMQJ_mdr3U8fCJ5mciCUpEPGahqirQ@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
To: Nick Pentreath <nick.pentreath@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5d65926ea564051222f145
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d65926ea564051222f145
Content-Type: text/plain; charset=UTF-8

Hi Nick,

I don't remember the exact details of these scenarios, but I think the user
wanted a lot more control over how the files got grouped into partitions,
to group the files together by some arbitrary function.  I didn't think
that was possible w/ CombineFileInputFormat, but maybe there is a way?

thanks

On Tue, Mar 24, 2015 at 1:50 PM, Nick Pentreath <nick.pentreath@gmail.com>
wrote:

> Imran, on your point to read multiple files together in a partition, is it
> not simpler to use the approach of copy Hadoop conf and set per-RDD
> settings for min split to control the input size per partition, together
> with something like CombineFileInputFormat?
>
> On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
>
> > I think this would be a great addition, I totally agree that you need to
> be
> > able to set these at a finer context than just the SparkContext.
> >
> > Just to play devil's advocate, though -- the alternative is for you just
> > subclass HadoopRDD yourself, or make a totally new RDD, and then you
> could
> > expose whatever you need.  Why is this solution better?  IMO the criteria
> > are:
> > (a) common operations
> > (b) error-prone / difficult to implement
> > (c) non-obvious, but important for performance
> >
> > I think this case fits (a) & (c), so I think its still worthwhile.  But
> its
> > also worth asking whether or not its too difficult for a user to extend
> > HadoopRDD right now.  There have been several cases in the past week
> where
> > we've suggested that a user should read from hdfs themselves (eg., to
> read
> > multiple files together in one partition) -- with*out* reusing the code
> in
> > HadoopRDD, though they would lose things like the metric tracking &
> > preferred locations you get from HadoopRDD.  Does HadoopRDD need to some
> > refactoring to make that easier to do?  Or do we just need a good
> example?
> >
> > Imran
> >
> > (sorry for hijacking your thread, Koert)
> >
> >
> >
> > On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
> wrote:
> >
> > > see email below. reynold suggested i send it to dev instead of user
> > >
> > > ---------- Forwarded message ----------
> > > From: Koert Kuipers <koert@tresata.com>
> > > Date: Mon, Mar 23, 2015 at 4:36 PM
> > > Subject: hadoop input/output format advanced control
> > > To: "user@spark.apache.org" <user@spark.apache.org>
> > >
> > >
> > > currently its pretty hard to control the Hadoop Input/Output formats
> used
> > > in Spark. The conventions seems to be to add extra parameters to all
> > > methods and then somewhere deep inside the code (for example in
> > > PairRDDFunctions.saveAsHadoopFile) all these parameters get translated
> > into
> > > settings on the Hadoop Configuration object.
> > >
> > > for example for compression i see "codec: Option[Class[_ <:
> > > CompressionCodec]] = None" added to a bunch of methods.
> > >
> > > how scalable is this solution really?
> > >
> > > for example i need to read from a hadoop dataset and i dont want the
> > input
> > > (part) files to get split up. the way to do this is to set
> > > "mapred.min.split.size". now i dont want to set this at the level of
> the
> > > SparkContext (which can be done), since i dont want it to apply to
> input
> > > formats in general. i want it to apply to just this one specific input
> > > dataset i need to read. which leaves me with no options currently. i
> > could
> > > go add yet another input parameter to all the methods
> > > (SparkContext.textFile, SparkContext.hadoopFile,
> SparkContext.objectFile,
> > > etc.). but that seems ineffective.
> > >
> > > why can we not expose a Map[String, String] or some other generic way
> to
> > > manipulate settings for hadoop input/output formats? it would require
> > > adding one more parameter to all methods to deal with hadoop
> input/output
> > > formats, but after that its done. one parameter to rule them all....
> > >
> > > then i could do:
> > > val x = sc.textFile("/some/path", formatSettings =
> > > Map("mapred.min.split.size" -> "12345"))
> > >
> > > or
> > > rdd.saveAsTextFile("/some/path, formatSettings =
> > > Map(mapred.output.compress" -> "true",
> "mapred.output.compression.codec"
> > ->
> > > "somecodec"))
> > >
> >
>

--047d7b5d65926ea564051222f145--

From dev-return-12177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 20:52:13 2015
Return-Path: <dev-return-12177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 57CCC178BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 20:52:13 +0000 (UTC)
Received: (qmail 93562 invoked by uid 500); 25 Mar 2015 20:51:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93489 invoked by uid 500); 25 Mar 2015 20:51:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93478 invoked by uid 99); 25 Mar 2015 20:51:55 -0000
Received: from mail-relay.apache.org (HELO mail-relay.apache.org) (140.211.11.15)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 20:51:55 +0000
Received: from mail-qg0-f48.google.com (mail-qg0-f48.google.com [209.85.192.48])
	by mail-relay.apache.org (ASF Mail Server at mail-relay.apache.org) with ESMTPSA id 2DD031A006C
	for <dev@spark.apache.org>; Wed, 25 Mar 2015 20:51:55 +0000 (UTC)
Received: by qgep97 with SMTP id p97so64334408qge.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 13:51:54 -0700 (PDT)
X-Received: by 10.140.28.36 with SMTP id 33mr13785716qgy.6.1427316713705; Wed,
 25 Mar 2015 13:51:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.229.162.9 with HTTP; Wed, 25 Mar 2015 13:51:31 -0700 (PDT)
From: Igor Costa <igorcosta@apache.org>
Date: Wed, 25 Mar 2015 17:51:31 -0300
Message-ID: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
Subject: Jira Issues
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a9a64b3d9ff0512231169

--001a113a9a64b3d9ff0512231169
Content-Type: text/plain; charset=UTF-8

Hi there Guys.

I want to be more collaborative to Spark, but I have two questions.


Issues are used in Github or jira Issues?

If so on Jira, Is there a way I can get in to see the issues?

I've tried to login but no success.


I'm PMC from another Apache project, flex.apache.org


Best Regards
Igor

--001a113a9a64b3d9ff0512231169--

From dev-return-12179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 20:54:04 2015
Return-Path: <dev-return-12179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6923F17909
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 20:54:04 +0000 (UTC)
Received: (qmail 3060 invoked by uid 500); 25 Mar 2015 20:54:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2969 invoked by uid 500); 25 Mar 2015 20:54:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2958 invoked by uid 99); 25 Mar 2015 20:54:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 20:54:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 20:53:57 +0000
Received: by qgh3 with SMTP id 3so51536157qgh.2
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 13:53:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=EXrtF7LDs7yki9NoRO9jFAwRCoAflv3OhBLWOtkjoZE=;
        b=E4vxXLMNkzIZWzqWPsUTplOWQf5t7ZfNMH1c832ftVuajzqDujrByGSbB7pt+lyZ3n
         AlYFbFGFWJxyO5r13PVJv/APrkS8lBC0SmLckW1KReZBLGfWyGYmKgnE/oM85/97zaO/
         2ODG604njAR2rQp+SQFQfPJUDCxfwgwQM4CIp/CPe4+rlcuVNCfm+L5Ehryqo9GhbrM4
         ELZLcK8NInxwt9mp76Amzmnfc9I5NJjGHnu+uIu//P4zVzDVAWRJWrRzqBpr+Rfzj1hL
         bb2zHizEzH60qmyJXqSdNwR+ch207CgJo6bx7DBB/v/KeV1LcQtVwTYaDRHQH7tLe6oE
         Mb2A==
X-Gm-Message-State: ALoCoQk7HhIDAJupzkCGzTlOABZtNgqbX1QgDvry476PRzrEiYLaGC6I3P6kN3f/uIFrbrj/S39O
X-Received: by 10.55.51.77 with SMTP id z74mr23106567qkz.84.1427316796342;
 Wed, 25 Mar 2015 13:53:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Wed, 25 Mar 2015 13:52:54 -0700 (PDT)
In-Reply-To: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
References: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 25 Mar 2015 13:52:54 -0700
Message-ID: <CAPh_B=aX49HAO62UqLj9jBJ1EhsuGs8c2Q+pJYPqq+4ScXce0A@mail.gmail.com>
Subject: Re: Jira Issues
To: Igor Costa <igorcosta@apache.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11490116a0e79305122316a8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11490116a0e79305122316a8
Content-Type: text/plain; charset=UTF-8

Igor,

Welcome -- everything is open here:
https://issues.apache.org/jira/browse/SPARK

You should be able to see them even if you are not an ASF member.


On Wed, Mar 25, 2015 at 1:51 PM, Igor Costa <igorcosta@apache.org> wrote:

> Hi there Guys.
>
> I want to be more collaborative to Spark, but I have two questions.
>
>
> Issues are used in Github or jira Issues?
>
> If so on Jira, Is there a way I can get in to see the issues?
>
> I've tried to login but no success.
>
>
> I'm PMC from another Apache project, flex.apache.org
>
>
> Best Regards
> Igor
>

--001a11490116a0e79305122316a8--

From dev-return-12178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 20:54:10 2015
Return-Path: <dev-return-12178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C7451790B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 20:54:10 +0000 (UTC)
Received: (qmail 99813 invoked by uid 500); 25 Mar 2015 20:53:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99720 invoked by uid 500); 25 Mar 2015 20:53:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99708 invoked by uid 99); 25 Mar 2015 20:53:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 20:53:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 20:53:47 +0000
Received: by iedm5 with SMTP id m5so32047597ied.3
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 13:53:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=BfSyqDSGR2CdiCjd2Yo3QmoePnMkYJ/g6XSiTvJ0H9A=;
        b=a8LK+Qlwnj3sVtSjYa8nFXuWCUejizuD2mxeT903R16nbL8vBnqj1E5Z2Buk1xJR1h
         +2awZ4naP5hY3XEy1gmdZGiHVnrO7fEA2yHWK5elnRnfKpmK8ayuwK9V3AIQ2KmXtoBP
         GmCeblSravuIcU2+1r7j6+wFpJuYlnhSR1Tr94sOMGsQt9p0M8fI82IXABAqPQ9xxgQS
         sQzB97nqg4RsdctTQ1i5pWD9/dSTzTQdHoNmTRFRYW3Dr2/S5dQFI4STFfRddTYosGVt
         e6Zm/FMXDGBaaigSjKY2ZNUXkuo1CW6gSYv/e8E+SYRI7mAtVBtRJT9wbqNJYbB93UlL
         CwPA==
MIME-Version: 1.0
X-Received: by 10.42.104.9 with SMTP id p9mr33385733ico.82.1427316807284; Wed,
 25 Mar 2015 13:53:27 -0700 (PDT)
Received: by 10.36.53.148 with HTTP; Wed, 25 Mar 2015 13:53:27 -0700 (PDT)
In-Reply-To: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
References: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
Date: Wed, 25 Mar 2015 13:53:27 -0700
Message-ID: <CALte62xubQOMHmGvX9wx2YgfqZOAONc0VFte3ibEAqn5Hg-XRQ@mail.gmail.com>
Subject: Re: Jira Issues
From: Ted Yu <yuzhihong@gmail.com>
To: Igor Costa <igorcosta@apache.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf306848d347d4980512231714
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf306848d347d4980512231714
Content-Type: text/plain; charset=UTF-8

Issues are tracked on Apache JIRA:
https://issues.apache.org/jira/browse/SPARK/?selectedTab=com.atlassian.jira.jira-projects-plugin:summary-panel

Cheers

On Wed, Mar 25, 2015 at 1:51 PM, Igor Costa <igorcosta@apache.org> wrote:

> Hi there Guys.
>
> I want to be more collaborative to Spark, but I have two questions.
>
>
> Issues are used in Github or jira Issues?
>
> If so on Jira, Is there a way I can get in to see the issues?
>
> I've tried to login but no success.
>
>
> I'm PMC from another Apache project, flex.apache.org
>
>
> Best Regards
> Igor
>

--20cf306848d347d4980512231714--

From dev-return-12180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 21:00:41 2015
Return-Path: <dev-return-12180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F2E1179C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 21:00:41 +0000 (UTC)
Received: (qmail 26477 invoked by uid 500); 25 Mar 2015 21:00:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26393 invoked by uid 500); 25 Mar 2015 21:00:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26382 invoked by uid 99); 25 Mar 2015 21:00:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:00:39 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:00:13 +0000
Received: by wibgn9 with SMTP id gn9so57284372wib.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 13:58:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=1i2NDBjXODoUjwbrOLF9hCSm9jtx4thIjqiDCfJd07o=;
        b=f37vmHX5sUBLdZ5MOmyRC1Clc5s+ayDY2TJXV6KOmrd4WlwgyiZjLxSLVOvv/zQg6c
         pHuvxhyEJcGAqFvgVzJu/W9F1zpCjtvFoPAMRJBUaxr10pGTbLh7873vZ9TnkKmp/iMS
         kFgAdGwkT5k/0o5gmaYGux03VJlIcUi7V66UKBQv/sJdqxeH4yYxLfXZSJcjtSkcjyGA
         IOwIg7KBR2G1grVTKNzQM3+KDPPW2X/jmYu6pAXit0jbowDHfBjRJU0OKnYA2QHvGDUp
         4SBnCgX5LBw252g36Qmn7jJb4ezwJKIGb8OzMwPQtDrXoAdhDRGHG0XbfrgPJYuPPe4k
         2Pqg==
X-Gm-Message-State: ALoCoQnXRr+FY//a+zTcGVpa3xRTR/Y3b1dShYm0qysdCeFDj56mIER8BTFehuPH9hvm6qn8QW/a
X-Received: by 10.194.21.193 with SMTP id x1mr21432951wje.144.1427317122234;
 Wed, 25 Mar 2015 13:58:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Wed, 25 Mar 2015 13:58:21 -0700 (PDT)
In-Reply-To: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
References: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 25 Mar 2015 20:58:21 +0000
Message-ID: <CAMAsSd+s_cb5rfOhd1Lx7iJoGEXXGSi+5UBUdRa5ac1_yEPojQ@mail.gmail.com>
Subject: Re: Jira Issues
To: Igor Costa <igorcosta@apache.org>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

It's just the standard Apache JIRA, nothing separate.

I'd say JIRA is used to track issues, bugs, features, but Github is
where the concrete changes to implement those things are discussed and
merged. So for a non-trivial issue, you'd want to describe the issue
in general in JIRA, and then open a PR with the JIRA name in the title
to propose the code change, rather than submit a patch or something.

On Wed, Mar 25, 2015 at 8:51 PM, Igor Costa <igorcosta@apache.org> wrote:
> Hi there Guys.
>
> I want to be more collaborative to Spark, but I have two questions.
>
>
> Issues are used in Github or jira Issues?
>
> If so on Jira, Is there a way I can get in to see the issues?
>
> I've tried to login but no success.
>
>
> I'm PMC from another Apache project, flex.apache.org
>
>
> Best Regards
> Igor

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 21:04:09 2015
Return-Path: <dev-return-12181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7834717AA2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 21:04:09 +0000 (UTC)
Received: (qmail 45295 invoked by uid 500); 25 Mar 2015 21:04:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45215 invoked by uid 500); 25 Mar 2015 21:04:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45172 invoked by uid 99); 25 Mar 2015 21:03:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:03:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of igorcosta@gmail.com designates 209.85.192.43 as permitted sender)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:03:34 +0000
Received: by qgfa8 with SMTP id a8so61240628qgf.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 14:03:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=iHkajp17HFSO01BG5CSTlIPtZDj8HWGnFFVNlvJuIVg=;
        b=lSf8VVJzIaX6dKHD5mS7BIg/4IsTFqzdKyCdMBy19cQjM4Xf7ryky97WsfUF6/EAkM
         mcX1jqPngL8fNeFye+8+6+b3a0ax4Eh7n6lXNY/RFrxQ9AR1P0lJw/nZYV2V+YMxGzxF
         wnXU8nLdZe0C0VTkrcN8khOTRoAJ82nn3QyufOJeDoaxkx1zdV4ZCwkgK4XtbrtfWjlT
         J3VEUVyUBQ2tE78u7IWg5JmPSmqTd/qFJgVMvBWqQuTKld7juqBwYdRkvt0qTG9dJ2xh
         NDxVy8V+kb1TFAg0LXKfJ5IssjueXuLQsEomoLRCPveSa3DJpany/KE3gJEH9YY+1UL4
         3MkA==
X-Received: by 10.140.237.194 with SMTP id i185mr14109509qhc.53.1427317405046;
 Wed, 25 Mar 2015 14:03:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.229.162.9 with HTTP; Wed, 25 Mar 2015 14:03:04 -0700 (PDT)
In-Reply-To: <CAMAsSd+s_cb5rfOhd1Lx7iJoGEXXGSi+5UBUdRa5ac1_yEPojQ@mail.gmail.com>
References: <CAPxxvdnpQMSKMWZGTwP+Ku+nvVaEi_wdu2MvknA=nEyU8=2dRA@mail.gmail.com>
 <CAMAsSd+s_cb5rfOhd1Lx7iJoGEXXGSi+5UBUdRa5ac1_yEPojQ@mail.gmail.com>
From: Igor Costa <igorcosta@gmail.com>
Date: Wed, 25 Mar 2015 18:03:04 -0300
Message-ID: <CAPxxvdkCsmbx4uwc+yRMRTDH519qAUne+mgcJL_=vjKZSoOWkg@mail.gmail.com>
Subject: Re: Jira Issues
To: Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11358002e8dc4d0512233a69
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11358002e8dc4d0512233a69
Content-Type: text/plain; charset=UTF-8

Thank you guys for the info.

Actually was a problem with my id on apache. Rather than need to be logged
in to view issues.

I'm browsing some issues now.


Best Regards

----------------------------
Igor Costa
www.igorcosta.com
www.igorcosta.org

On Wed, Mar 25, 2015 at 5:58 PM, Sean Owen <sowen@cloudera.com> wrote:

> It's just the standard Apache JIRA, nothing separate.
>
> I'd say JIRA is used to track issues, bugs, features, but Github is
> where the concrete changes to implement those things are discussed and
> merged. So for a non-trivial issue, you'd want to describe the issue
> in general in JIRA, and then open a PR with the JIRA name in the title
> to propose the code change, rather than submit a patch or something.
>
> On Wed, Mar 25, 2015 at 8:51 PM, Igor Costa <igorcosta@apache.org> wrote:
> > Hi there Guys.
> >
> > I want to be more collaborative to Spark, but I have two questions.
> >
> >
> > Issues are used in Github or jira Issues?
> >
> > If so on Jira, Is there a way I can get in to see the issues?
> >
> > I've tried to login but no success.
> >
> >
> > I'm PMC from another Apache project, flex.apache.org
> >
> >
> > Best Regards
> > Igor
>

--001a11358002e8dc4d0512233a69--

From dev-return-12182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 21:33:53 2015
Return-Path: <dev-return-12182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AC48B175E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 21:33:53 +0000 (UTC)
Received: (qmail 67263 invoked by uid 500); 25 Mar 2015 21:33:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67169 invoked by uid 500); 25 Mar 2015 21:33:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67158 invoked by uid 99); 25 Mar 2015 21:33:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:33:52 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.240.92.67] (HELO g9t5009.houston.hp.com) (15.240.92.67)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:33:48 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5009.houston.hp.com (Postfix) with ESMTPS id 6B6821CF;
	Wed, 25 Mar 2015 21:32:26 +0000 (UTC)
Received: from G9W3612.americas.hpqcorp.net (16.216.186.47) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Wed, 25 Mar 2015 21:31:19 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G9W3612.americas.hpqcorp.net ([16.216.186.47]) with mapi id 14.03.0169.001;
 Wed, 25 Mar 2015 21:31:19 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Sam Halliday <sam.halliday@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, Xiangrui Meng
	<mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, "Evan R. Sparks"
	<evan.sparks@gmail.com>, jfcanny <canny@berkeley.edu>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAvQQdDAAKEa4gA==
Date: Wed, 25 Mar 2015 21:31:18 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.192.232]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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==
DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-12183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 21:40:15 2015
Return-Path: <dev-return-12183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8199A17788
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 21:40:15 +0000 (UTC)
Received: (qmail 88592 invoked by uid 500); 25 Mar 2015 21:40:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88497 invoked by uid 500); 25 Mar 2015 21:40:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88261 invoked by uid 99); 25 Mar 2015 21:40:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:40:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:40:05 +0000
Received: by qgf60 with SMTP id 60so51292782qgf.3
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 14:37:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=uicwdPXDnLftTHy9tUQQP30bRkGvAj23FvHRn6/+a84=;
        b=Nc0W3gy2mpyd+5Qqs36ZuHrwPZtRpABvnKXSFtAOx0S1a7rV0IrWSZad4YJRYywCp8
         uD4AGR9lXPc5ubyTQRvz32lXLhWf4EQdAHoXKNQ42tOkcoQigtdS8jbfKjeZKBZy8y21
         brfZqdjxHvMtiGyl56eH21emZdIxcO5hd8m5hjyk4KpxRKRnarKMwUoYH6lkFcsK9fv6
         penA19G8JC3PBUvpicxfQjgskF6jryufJ1if7z25nSsUk5SHcdbqFFDIlkuRfwiqA2F9
         VDEjsPB7NP8jG5uCn7fcOLTLYWkXW9VX+KU44CUGzydfzCXLzlylAn1OJURcjNM05hrX
         4KZw==
X-Gm-Message-State: ALoCoQkShAt4/N1Jn/h1CGCSEgsPeLJQY9763R54Zy44f9HQsMpnLX5RkpD1G2QfVWY07k/ZAmIJ
MIME-Version: 1.0
X-Received: by 10.55.23.83 with SMTP id i80mr23532471qkh.104.1427319474883;
 Wed, 25 Mar 2015 14:37:54 -0700 (PDT)
Received: by 10.229.9.130 with HTTP; Wed, 25 Mar 2015 14:37:54 -0700 (PDT)
In-Reply-To: <CAF7ADNrcC-w+ZH9S_0R-y_iEi8GKPV7ybHsmP4SF2JD=HynnSg@mail.gmail.com>
References: <CA+B-+fwZ50HOuAKJjkcy87Rym++kdh4A_Gj32N6cZZy-7WwZZA@mail.gmail.com>
	<CAF7ADNrcC-w+ZH9S_0R-y_iEi8GKPV7ybHsmP4SF2JD=HynnSg@mail.gmail.com>
Date: Wed, 25 Mar 2015 14:37:54 -0700
Message-ID: <CAEYYnxZjN8vCQEuUzADYH8TTa3haBSP3P=JAJGy5Z39x0hdMaA@mail.gmail.com>
Subject: Re: LogisticGradient Design
From: DB Tsai <dbtsai@dbtsai.com>
To: Joseph Bradley <joseph@databricks.com>
Cc: Debasish Das <debasish.das83@gmail.com>, dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I did the benchmark when I used the if-else statement to switch the
binary & multinomial logistic loss and gradient, and there is no
performance hit at all. However, I'm refactoring the LogisticGradient
code so the addBias and scaling can be done in LogisticGradient
instead of the input dataset to avoid the second cache. In this case,
the code will be more complicated, so I will split the code into two
paths. Will be done in another PR.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com


On Wed, Mar 25, 2015 at 11:57 AM, Joseph Bradley <joseph@databricks.com> wrote:
> It would be nice to see how big a performance hit we take from combining
> binary & multiclass logistic loss/gradient.  If it's not a big hit, then it
> might be simpler from an outside API perspective to keep them in 1 class
> (even if it's more complicated within).
> Joseph
>
> On Wed, Mar 25, 2015 at 8:15 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
>> Hi,
>>
>> Right now LogisticGradient implements both binary and multi-class in the
>> same class using an if-else statement which is a bit convoluted.
>>
>> For Generalized matrix factorization, if the data has distinct ratings I
>> want to use LeastSquareGradient (regression has given best results to date)
>> but if the data has binary labels 0/1 based on domain knowledge (implicit
>> for example, visits no-visits) I want to use a LogisticGradient without any
>> overhead for multi-class if-else...
>>
>> I can compare the performance of LeastSquareGradient and multi-class
>> LogisticGradient on the recommendation metrics but it will be great if we
>> can separate binary and multi-class in Separate
>> classes....MultiClassLogistic can extend BinaryLogistic but mixing them in
>> the same class is an overhead for users (like me) who wants to use
>> BinaryLogistic for his application..
>>
>> Thanks.
>> Deb
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 21:49:34 2015
Return-Path: <dev-return-12184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F1D961790C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 21:49:34 +0000 (UTC)
Received: (qmail 9899 invoked by uid 500); 25 Mar 2015 21:49:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9820 invoked by uid 500); 25 Mar 2015 21:49:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9808 invoked by uid 99); 25 Mar 2015 21:49:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:49:30 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.182 as permitted sender)
Received: from [209.85.217.182] (HELO mail-lb0-f182.google.com) (209.85.217.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:49:26 +0000
Received: by lbcmq2 with SMTP id mq2so27865406lbc.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 14:46:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=AP/yV33uKZlb1Z84RlUZix9iiS8sTcIMdEICEnYdCVY=;
        b=J71pkH7bGtrSLeX/gib5WZfd6FJ4Av3VWQPgPh3updbrmvaG6HHgYhV6Ph6JY37mvU
         uVSzMawEX2u126donayt2GDSycmDjawpC5rWC/v19DMm2bYhsok04RnR/yPNDnf2Ocbi
         qYNHN5luX6f7a0L+I8uUqLn9Ah6e8F2ha03GgqSxMsQyf+EETZUkchvXENIRnukhz5GY
         FnbMrCi4BH0tgdCinc5dZxCGnyxIoE6CpBHaxYXHr/blgNz4yVPri2/+cZJ56CXrMPwY
         PFMAavvZmgeGAqkXm3ePd4zj+URfslgIXQ3B2LvhOl/U4SKqq1026FW7Xn6kvURxfTrS
         E4Cw==
MIME-Version: 1.0
X-Received: by 10.152.21.8 with SMTP id r8mr10224395lae.98.1427320010505; Wed,
 25 Mar 2015 14:46:50 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Wed, 25 Mar 2015 14:46:50 -0700 (PDT)
In-Reply-To: <CAEYYnxZjN8vCQEuUzADYH8TTa3haBSP3P=JAJGy5Z39x0hdMaA@mail.gmail.com>
References: <CA+B-+fwZ50HOuAKJjkcy87Rym++kdh4A_Gj32N6cZZy-7WwZZA@mail.gmail.com>
	<CAF7ADNrcC-w+ZH9S_0R-y_iEi8GKPV7ybHsmP4SF2JD=HynnSg@mail.gmail.com>
	<CAEYYnxZjN8vCQEuUzADYH8TTa3haBSP3P=JAJGy5Z39x0hdMaA@mail.gmail.com>
Date: Wed, 25 Mar 2015 14:46:50 -0700
Message-ID: <CA+B-+fywj3w_174h4nYGqVsYBvxfqCpWd1Avsa6Os4t9cy01JA@mail.gmail.com>
Subject: Re: LogisticGradient Design
From: Debasish Das <debasish.das83@gmail.com>
To: DB Tsai <dbtsai@dbtsai.com>
Cc: Joseph Bradley <joseph@databricks.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0141a4983502a6051223d611
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0141a4983502a6051223d611
Content-Type: text/plain; charset=UTF-8

Cool...Thanks...It will be great if they move in two code paths just for
the sake of code clean-up

On Wed, Mar 25, 2015 at 2:37 PM, DB Tsai <dbtsai@dbtsai.com> wrote:

> I did the benchmark when I used the if-else statement to switch the
> binary & multinomial logistic loss and gradient, and there is no
> performance hit at all. However, I'm refactoring the LogisticGradient
> code so the addBias and scaling can be done in LogisticGradient
> instead of the input dataset to avoid the second cache. In this case,
> the code will be more complicated, so I will split the code into two
> paths. Will be done in another PR.
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> Blog: https://www.dbtsai.com
>
>
> On Wed, Mar 25, 2015 at 11:57 AM, Joseph Bradley <joseph@databricks.com>
> wrote:
> > It would be nice to see how big a performance hit we take from combining
> > binary & multiclass logistic loss/gradient.  If it's not a big hit, then
> it
> > might be simpler from an outside API perspective to keep them in 1 class
> > (even if it's more complicated within).
> > Joseph
> >
> > On Wed, Mar 25, 2015 at 8:15 AM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >
> >> Hi,
> >>
> >> Right now LogisticGradient implements both binary and multi-class in the
> >> same class using an if-else statement which is a bit convoluted.
> >>
> >> For Generalized matrix factorization, if the data has distinct ratings I
> >> want to use LeastSquareGradient (regression has given best results to
> date)
> >> but if the data has binary labels 0/1 based on domain knowledge
> (implicit
> >> for example, visits no-visits) I want to use a LogisticGradient without
> any
> >> overhead for multi-class if-else...
> >>
> >> I can compare the performance of LeastSquareGradient and multi-class
> >> LogisticGradient on the recommendation metrics but it will be great if
> we
> >> can separate binary and multi-class in Separate
> >> classes....MultiClassLogistic can extend BinaryLogistic but mixing them
> in
> >> the same class is an overhead for users (like me) who wants to use
> >> BinaryLogistic for his application..
> >>
> >> Thanks.
> >> Deb
> >>
>

--089e0141a4983502a6051223d611--

From dev-return-12185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 21:55:36 2015
Return-Path: <dev-return-12185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D3B8179E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 21:55:36 +0000 (UTC)
Received: (qmail 31130 invoked by uid 500); 25 Mar 2015 21:55:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31051 invoked by uid 500); 25 Mar 2015 21:55:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31038 invoked by uid 99); 25 Mar 2015 21:55:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:55:34 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dlieu.7@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 21:55:05 +0000
Received: by oier21 with SMTP id r21so33937525oie.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 14:55:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=dqibrekwmrYJmPRChUZf+VG9ZSb2y/6Z9rIeYTrlEgU=;
        b=TuW97MjQwD2zE+zQMQpTrNBJ6du/B7RIQnSjGFthdanoXhlyED0TEbzvPl7F5E6V1f
         32cdH/v45/Nq+cov/bX5nCmhkbMTN6bBMV54nG82CuSJX03KqfogLJnCH2vh1tQdvMoV
         Ec2mkqO/3wIMfUlJktTtYEjwG1lbt4xVcroiY4fBt5Inwut/m58fmCzqYSKDKr4efJTN
         HOTJHi1GJnlPW4XpIagcfFtxD1V60sA6XbctGVWSYo8xWFjKX4KnN0hcdsY2ew4JD1Tk
         VqGVaTtuj0UdMDBPP+Dnv5d4F2eZNmqp1f7H6Otobi3971igbQEpteK8cGUJFVI+hPOs
         odnA==
MIME-Version: 1.0
X-Received: by 10.60.133.176 with SMTP id pd16mr9402849oeb.78.1427320503483;
 Wed, 25 Mar 2015 14:55:03 -0700 (PDT)
Received: by 10.76.188.163 with HTTP; Wed, 25 Mar 2015 14:55:03 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
Date: Wed, 25 Mar 2015 14:55:03 -0700
Message-ID: <CAPud8ToyQWm0LnFWd+dD37L=+fiDo0ZEpXugigUUAsEO9U6Dew@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Sam Halliday <sam.halliday@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, 
	"Evan R. Sparks" <evan.sparks@gmail.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=047d7b4728769741f8051223f3b7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4728769741f8051223f3b7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Alexander,

does using netlib imply that one cannot switch between CPU and GPU blas
alternatives at will at the same time? the choice is always determined by
linking aliternatives to libblas.so, right?

On Wed, Mar 25, 2015 at 2:31 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
>
wrote:

> Hi again,
>
> I finally managed to use nvblas within Spark+netlib-java. It has
> exceptional performance for big matrices with Double, faster than
> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
> to/from GPU, OpenBlas or MKL might be a better choice. This correlates wi=
th
> original nvblas presentation on GPU conf 2013 (slide 21):
> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-=
New-Features-CUDA%206%20-GPU-Acceleration.pdf
>
> My results:
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Just in case, these tests are not for generalization of performance of
> different libraries. I just want to pick a library that does at best dens=
e
> matrices multiplication for my task.
>
> P.S. My previous issue with nvblas was the following: it has Fortran blas
> functions, at the same time netlib-java uses C cblas functions. So, one
> needs cblas shared library to use nvblas through netlib-java. Fedora does
> not have cblas (but Debian and Ubuntu have), so I needed to compile it. I
> could not use cblas from Atlas or Openblas because they link to their
> implementation and not to Fortran blas.
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Tuesday, March 24, 2015 6:57 PM
> To: Sam Halliday
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
> Hi,
>
> I am trying to use nvblas with netlib-java from Spark. nvblas functions
> should replace current blas functions calls after executing LD_PRELOAD as
> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
> changes to netlib-java. It seems to work for simple Java example, but I
> cannot make it work with Spark. I run the following:
> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>
> +------------------------------------------------------------------------=
-----+
> | Processes:                                                       GPU
> Memory |
> |  GPU       PID  Type  Process name                               Usage
>     |
>
> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D|
> |    0      8873    C   bash
> 39MiB |
> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
> 39MiB |
>
> +------------------------------------------------------------------------=
-----+
>
> In Spark shell I do matrix multiplication and see the following:
> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
> So I am sure that netlib-native is loaded and cblas supposedly used.
> However, matrix multiplication does executes on CPU since I see 16% of CP=
U
> used and 0% of GPU used. I also checked different matrix sizes, from
> 100x100 to 12000x12000
>
> Could you suggest might the LD_PRELOAD not affect Spark shell?
>
> Best regards, Alexander
>
>
>
> From: Sam Halliday [mailto:sam.halliday@gmail.com]
> Sent: Monday, March 09, 2015 6:01 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
> Thanks so much for following up on this!
>
> Hmm, I wonder if we should have a concerted effort to chart performance o=
n
> various pieces of hardware...
> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto:
> alexander.ulanov@hp.com>> wrote:
> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
> support of Double in the current source code), did the test with BIDMat a=
nd
> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
> sam.halliday@gmail.com>]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
> dev@spark.apache.org>
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
> <mailto:joseph@databricks.com>> wrote:
> >> Better documentation for linking would be very helpful!  Here's a JIRA=
:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for larger
> matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks
> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
> to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked up b=
y
> netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I understand
> how to force use a specific blas (not specific wrapper for blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already have
> it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
> joseph@databricks.com><mailto:
> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons with
> Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential to
> machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and =
4
> GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x faster a=
t
> matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster within
> Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better performance
> only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was not a
> test for ONLY multiplication since there are other operations involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
> dev-unsubscribe@spark.apache.org><mailto:
> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
> >>>> ark.apac> he.org<http://he.org>
> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
> >>>> rk.apache.org>>> For additional commands, e-mail:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --
> Best regards,
> Sam
>

--047d7b4728769741f8051223f3b7--

From dev-return-12186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:00:44 2015
Return-Path: <dev-return-12186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 331B617A85
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:00:44 +0000 (UTC)
Received: (qmail 44288 invoked by uid 500); 25 Mar 2015 22:00:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44211 invoked by uid 500); 25 Mar 2015 22:00:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44199 invoked by uid 99); 25 Mar 2015 22:00:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:00:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.213.41 as permitted sender)
Received: from [209.85.213.41] (HELO mail-yh0-f41.google.com) (209.85.213.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:00:13 +0000
Received: by yhim52 with SMTP id m52so18494198yhi.2
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 15:00:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Qn1nXQppMyBgahdDNeCxJS7PW4+SNZ/5iBXwy89KNnI=;
        b=whzZKcAZgEs2+zLf0aExWTO+WgUNBrxH2Lq/pgWOcygh5LNWjcnaBhxpcVXq8bhasT
         aJx35s87Uypy1BlAS6X3bMzSmPL/G7t9yMr2k1dIoNqi29Gtbvmf7n/hJmH7hwY3rs44
         U3PUthdvPIB76OgJxGBF2OHQ1dWP05xujwjjn80ALl8avIGQwfuHHNPnhEe3/1oMjkxI
         d1/UENKAh8IwGzqw/EHMkPKp0lojRhrZBL7ncxQaXnULaLbJZyTwxW2OhgC830WBp7fh
         Y7RVhh3PcJwknuVmEX5s6NJYkVQ4Gn07hMtnd4uGjzNZ2TvsAcvNlOWNtriGaJXfTvOS
         auew==
X-Received: by 10.52.163.37 with SMTP id yf5mr12173489vdb.45.1427320811759;
 Wed, 25 Mar 2015 15:00:11 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Wed, 25 Mar 2015 14:59:51 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
 <CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
 <CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
 <CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
 <CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
 <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
 <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Wed, 25 Mar 2015 14:59:51 -0700
Message-ID: <CABjXkq5iEyfgNGS0CnAXqL93-0hs1KiMgVeqcKK3NxTpF1ZpBw@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Sam Halliday <sam.halliday@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=001a11c22c2af72c4c0512240564
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c22c2af72c4c0512240564
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Alex - great stuff, and the nvblas numbers are pretty remarkable (almost
too good... did you check the results for correctness? - also, is it
possible that the "unified memory model" of nvblas is somehow hiding pci
transfer time?)

this last bit (getting nvblas + netlib-java to play together) sounds like
it's non-trivial and took you a while to figure out! Would you mind posting
a gist or something of maybe the shell scripts/exports you used to make
this work - I can imagine it being highly useful for others in the future.

Thanks!
Evan

On Wed, Mar 25, 2015 at 2:31 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
>
wrote:

> Hi again,
>
> I finally managed to use nvblas within Spark+netlib-java. It has
> exceptional performance for big matrices with Double, faster than
> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
> to/from GPU, OpenBlas or MKL might be a better choice. This correlates wi=
th
> original nvblas presentation on GPU conf 2013 (slide 21):
> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-=
New-Features-CUDA%206%20-GPU-Acceleration.pdf
>
> My results:
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Just in case, these tests are not for generalization of performance of
> different libraries. I just want to pick a library that does at best dens=
e
> matrices multiplication for my task.
>
> P.S. My previous issue with nvblas was the following: it has Fortran blas
> functions, at the same time netlib-java uses C cblas functions. So, one
> needs cblas shared library to use nvblas through netlib-java. Fedora does
> not have cblas (but Debian and Ubuntu have), so I needed to compile it. I
> could not use cblas from Atlas or Openblas because they link to their
> implementation and not to Fortran blas.
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Tuesday, March 24, 2015 6:57 PM
> To: Sam Halliday
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
> Hi,
>
> I am trying to use nvblas with netlib-java from Spark. nvblas functions
> should replace current blas functions calls after executing LD_PRELOAD as
> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
> changes to netlib-java. It seems to work for simple Java example, but I
> cannot make it work with Spark. I run the following:
> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>
> +------------------------------------------------------------------------=
-----+
> | Processes:                                                       GPU
> Memory |
> |  GPU       PID  Type  Process name                               Usage
>     |
>
> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D|
> |    0      8873    C   bash
> 39MiB |
> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
> 39MiB |
>
> +------------------------------------------------------------------------=
-----+
>
> In Spark shell I do matrix multiplication and see the following:
> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
> So I am sure that netlib-native is loaded and cblas supposedly used.
> However, matrix multiplication does executes on CPU since I see 16% of CP=
U
> used and 0% of GPU used. I also checked different matrix sizes, from
> 100x100 to 12000x12000
>
> Could you suggest might the LD_PRELOAD not affect Spark shell?
>
> Best regards, Alexander
>
>
>
> From: Sam Halliday [mailto:sam.halliday@gmail.com]
> Sent: Monday, March 09, 2015 6:01 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
> Thanks so much for following up on this!
>
> Hmm, I wonder if we should have a concerted effort to chart performance o=
n
> various pieces of hardware...
> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto:
> alexander.ulanov@hp.com>> wrote:
> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
> support of Double in the current source code), did the test with BIDMat a=
nd
> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
> sam.halliday@gmail.com>]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
> dev@spark.apache.org>
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
> <mailto:joseph@databricks.com>> wrote:
> >> Better documentation for linking would be very helpful!  Here's a JIRA=
:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for larger
> matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks
> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
> to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked up b=
y
> netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I understand
> how to force use a specific blas (not specific wrapper for blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already have
> it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
> joseph@databricks.com><mailto:
> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons with
> Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential to
> machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and =
4
> GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x faster a=
t
> matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster within
> Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better performance
> only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was not a
> test for ONLY multiplication since there are other operations involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
> dev-unsubscribe@spark.apache.org><mailto:
> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
> >>>> ark.apac> he.org<http://he.org>
> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
> >>>> rk.apache.org>>> For additional commands, e-mail:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --
> Best regards,
> Sam
>

--001a11c22c2af72c4c0512240564--

From dev-return-12187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:04:37 2015
Return-Path: <dev-return-12187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DE56617B25
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:04:37 +0000 (UTC)
Received: (qmail 55463 invoked by uid 500); 25 Mar 2015 22:04:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55371 invoked by uid 500); 25 Mar 2015 22:04:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55359 invoked by uid 99); 25 Mar 2015 22:04:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:04:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sam.halliday@gmail.com designates 209.85.213.181 as permitted sender)
Received: from [209.85.213.181] (HELO mail-ig0-f181.google.com) (209.85.213.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:04:31 +0000
Received: by igcau2 with SMTP id au2so114155385igc.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 15:04:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=D6TSLmHHLkZfFjiwOt8fWi0RHRlucLpfjG2pDPnSLfE=;
        b=QDNlANW+GdtFiSy1HmjsJ9EuxP/vks55vxh0FjIiYgBMYyOjW+8R2Vv8GwElXDg46h
         lMoVf/76iVRj/cxHTnwVvey1/H2J93B4i1Et855j8RIKI/IA3/DlQzdj0AQiouMkWglj
         MKXrIa96yQ1iw3pUVJ45kxxNWIB7av1anlHt5a3AOLTpQKTulEpZJ7rpEgxx9dAcHrk9
         IGle/lSD7/X+JIXnYwhmZmo4uZ0HWfurJaRUWTwjPRPc1YMaHY1aUdvZDr85VZTwsQFd
         JLi6kC5t0HUfBz+6fdAHgY6NO7gzCnhC8TfkB1MGkUZ26TaO6MIInjqEFpDVH9yhVulb
         34UQ==
MIME-Version: 1.0
X-Received: by 10.42.100.211 with SMTP id b19mr20676844ico.5.1427321051084;
 Wed, 25 Mar 2015 15:04:11 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Wed, 25 Mar 2015 15:04:10 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Wed, 25 Mar 2015 15:04:10 -0700 (PDT)
In-Reply-To: <CABjXkq5iEyfgNGS0CnAXqL93-0hs1KiMgVeqcKK3NxTpF1ZpBw@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<CABjXkq5iEyfgNGS0CnAXqL93-0hs1KiMgVeqcKK3NxTpF1ZpBw@mail.gmail.com>
Date: Wed, 25 Mar 2015 22:04:10 +0000
Message-ID: <CALR_T9B+Y-gbYpTish-bDumD8+8nc93oFJpYybTxVQhcmdqqyw@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Sam Halliday <sam.halliday@gmail.com>
To: "Evan R. Sparks" <evan.sparks@gmail.com>, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=90e6ba613f8a3b16e40512241496
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba613f8a3b16e40512241496
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

If you write it up I'll add it to the netlib-java wiki :-)

BTW, does it automatically flip between cpu/GPU? I've a project called
MultiBLAS which was going to do this, it should be easy (but boring to
write)
On 25 Mar 2015 22:00, "Evan R. Sparks" <evan.sparks@gmail.com> wrote:

> Alex - great stuff, and the nvblas numbers are pretty remarkable (almost
> too good... did you check the results for correctness? - also, is it
> possible that the "unified memory model" of nvblas is somehow hiding pci
> transfer time?)
>
> this last bit (getting nvblas + netlib-java to play together) sounds like
> it's non-trivial and took you a while to figure out! Would you mind posti=
ng
> a gist or something of maybe the shell scripts/exports you used to make
> this work - I can imagine it being highly useful for others in the future=
.
>
> Thanks!
> Evan
>
> On Wed, Mar 25, 2015 at 2:31 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
>
>> Hi again,
>>
>> I finally managed to use nvblas within Spark+netlib-java. It has
>> exceptional performance for big matrices with Double, faster than
>> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
>> to/from GPU, OpenBlas or MKL might be a better choice. This correlates w=
ith
>> original nvblas presentation on GPU conf 2013 (slide 21):
>> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108=
-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>>
>> My results:
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Just in case, these tests are not for generalization of performance of
>> different libraries. I just want to pick a library that does at best den=
se
>> matrices multiplication for my task.
>>
>> P.S. My previous issue with nvblas was the following: it has Fortran bla=
s
>> functions, at the same time netlib-java uses C cblas functions. So, one
>> needs cblas shared library to use nvblas through netlib-java. Fedora doe=
s
>> not have cblas (but Debian and Ubuntu have), so I needed to compile it. =
I
>> could not use cblas from Atlas or Openblas because they link to their
>> implementation and not to Fortran blas.
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Ulanov, Alexander
>> Sent: Tuesday, March 24, 2015 6:57 PM
>> To: Sam Halliday
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>> Hi,
>>
>> I am trying to use nvblas with netlib-java from Spark. nvblas functions
>> should replace current blas functions calls after executing LD_PRELOAD a=
s
>> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
>> changes to netlib-java. It seems to work for simple Java example, but I
>> cannot make it work with Spark. I run the following:
>> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
>> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
>> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>>
>> +-----------------------------------------------------------------------=
------+
>> | Processes:                                                       GPU
>> Memory |
>> |  GPU       PID  Type  Process name                               Usage
>>     |
>>
>> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D|
>> |    0      8873    C   bash
>> 39MiB |
>> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
>> 39MiB |
>>
>> +-----------------------------------------------------------------------=
------+
>>
>> In Spark shell I do matrix multiplication and see the following:
>> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
>> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>> So I am sure that netlib-native is loaded and cblas supposedly used.
>> However, matrix multiplication does executes on CPU since I see 16% of C=
PU
>> used and 0% of GPU used. I also checked different matrix sizes, from
>> 100x100 to 12000x12000
>>
>> Could you suggest might the LD_PRELOAD not affect Spark shell?
>>
>> Best regards, Alexander
>>
>>
>>
>> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>> Sent: Monday, March 09, 2015 6:01 PM
>> To: Ulanov, Alexander
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>>
>> Thanks so much for following up on this!
>>
>> Hmm, I wonder if we should have a concerted effort to chart performance
>> on various pieces of hardware...
>> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto=
:
>> alexander.ulanov@hp.com>> wrote:
>> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
>> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
>> support of Double in the current source code), did the test with BIDMat =
and
>> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>>
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
>> sam.halliday@gmail.com>]
>> Sent: Tuesday, March 03, 2015 1:54 PM
>> To: Xiangrui Meng; Joseph Bradley
>> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
>> dev@spark.apache.org>
>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>
>> BTW, is anybody on this list going to the London Meetup in a few weeks?
>>
>>
>> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapre=
duce-world#community
>>
>> Would be nice to meet other people working on the guts of Spark! :-)
>>
>>
>> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>>
>> > Hey Alexander,
>> >
>> > I don't quite understand the part where netlib-cublas is about 20x
>> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
>> > with netlib-java?
>> >
>> > CC'ed Sam, the author of netlib-java.
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
>> <mailto:joseph@databricks.com>> wrote:
>> >> Better documentation for linking would be very helpful!  Here's a JIR=
A:
>> >> https://issues.apache.org/jira/browse/SPARK-6019
>> >>
>> >>
>> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>> >> wrote:
>> >>
>> >>> Thanks for compiling all the data and running these benchmarks,
>> >>> Alex. The big takeaways here can be seen with this chart:
>> >>>
>> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>> >>>
>> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>> >>> BIDMat+GPU) can provide substantial (but less than an order of
>> >>> BIDMat+magnitude)
>> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>> >>> netlib-java+openblas-compiled).
>> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>> >>> worse than a well-tuned CPU implementation, particularly for larger
>> matrices.
>> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>> >>> basically agrees with the authors own benchmarks (
>> >>> https://github.com/fommil/netlib-java)
>> >>>
>> >>> I think that most of our users are in a situation where using GPUs
>> >>> may not be practical - although we could consider having a good GPU
>> >>> backend available as an option. However, *ALL* users of MLlib could
>> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
>> >>> BLAS implementation. Perhaps we should consider updating the mllib
>> >>> guide with a more complete section for enabling high performance
>> >>> binaries on OSX and Linux? Or better, figure out a way for the
>> >>> system to fetch these automatically.
>> >>>
>> >>> - Evan
>> >>>
>> >>>
>> >>>
>> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>> >>>
>> >>>> Just to summarize this thread, I was finally able to make all
>> >>>> performance comparisons that we discussed. It turns out that:
>> >>>> BIDMat-cublas>>BIDMat
>> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-y=
um-repo=3D
>> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
>> >>>>
>> >>>> Below is the link to the spreadsheet with full results.
>> >>>>
>> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
>> >>>>
>> >>>> One thing still needs exploration: does BIDMat-cublas perform
>> >>>> copying to/from machine=E2=80=99s RAM?
>> >>>>
>> >>>> -----Original Message-----
>> >>>> From: Ulanov, Alexander
>> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>> >>>> To: Evan R. Sparks
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>> >>>> the original one discusses slightly different topic. I was able to
>> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>> >>>> statically linked inside a 60MB library.
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>> >>>> |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>> >>>> 1569,233228 |
>> >>>>
>> >>>> It turn out that pre-compiled MKL is faster than precompiled
>> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns=
 with
>> >>>> locally compiled openblas and cuda.
>> >>>>
>> >>>> Alexander
>> >>>>
>> >>>> From: Evan R. Sparks
>> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>> >>>> Sent: Monday, February 09, 2015 6:06 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Great - perhaps we can move this discussion off-list and onto a
>> >>>> JIRA ticket? (Here's one:
>> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>> >>>>
>> >>>> It seems like this is going to be somewhat exploratory for a while
>> >>>> (and there's probably only a handful of us who really care about
>> >>>> fast linear
>> >>>> algebra!)
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for explanation and useful link. I am going to build
>> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>> >>>>
>> >>>> Do I understand correctly that BIDMat binaries contain statically
>> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
>> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
>> >>>> it seems that in my case precompiled MKL BLAS performs better than
>> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
>> to be on par with JNI overheads.
>> >>>>
>> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
>> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>> >>>> Halliday
>> >>>> (Netlib-java) interested to compare their libraries.
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:58 PM
>> >>>>
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
>> >>>> from getting cache sizes, etc. set up correctly for your particular
>> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>> >>>> quickly and yields performance competitive with MKL.
>> >>>>
>> >>>> To make sure the right library is getting used, you have to make
>> >>>> sure it's first on the search path - export
>> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
>> >>>>
>> >>>> For some examples of getting netlib-java setup on an ec2 node and
>> >>>> some example benchmarking code we ran a while back, see:
>> >>>> https://github.com/shivaram/matrix-bench
>> >>>>
>> >>>> In particular - build-openblas-ec2.sh shows you how to build the
>> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>> >>>> shows you how to get the path setup and get that library picked up
>> by netlib-java.
>> >>>>
>> >>>> In this way - you could probably get cuBLAS set up to be used by
>> >>>> netlib-java as well.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
>> >>>> force loading the right blas? For netlib, I there are few JVM
>> >>>> flags, such as
>> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
>> >>>> so I can force it to use Java implementation. Not sure I understand
>> how to force use a specific blas (not specific wrapper for blas).
>> >>>>
>> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
>> >>>> that netlib is using it.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:19 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Getting breeze to pick up the right blas library is critical for
>> >>>> performance. I recommend using OpenBLAS (or MKL, if you already hav=
e
>> it).
>> >>>> It might make sense to force BIDMat to use the same underlying BLAS
>> >>>> library as well.
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan, Joseph
>> >>>>
>> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>> >>>> |native_system_linux_x86-64|
>> >>>> Breeze+Netlib-java f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>> >>>> ||
>> >>>>
>> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
>> >>>> 19 Linux, Scala 2.11.
>> >>>>
>> >>>> Later I will make tests with Cuda. I need to install new Cuda
>> >>>> version for this purpose.
>> >>>>
>> >>>> Do you have any ideas why breeze-netlib with native blas is so much
>> >>>> slower than BIDMat MKL?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>> joseph@databricks.com><mailto:
>> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>> >>>> Sent: Thursday, February 05, 2015 5:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Evan R. Sparks;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Hi Alexander,
>> >>>>
>> >>>> Using GPUs with Spark would be very exciting.  Small comment:
>> >>>> Concerning your question earlier about keeping data stored on the
>> >>>> GPU rather than having to move it between main memory and GPU
>> >>>> memory on each iteration, I would guess this would be critical to
>> >>>> getting good performance.  If you could do multiple local
>> >>>> iterations before aggregating results, then the cost of data
>> >>>> movement to the GPU could be amortized (and I believe that is done
>> >>>> in practice).  Having Spark be aware of the GPU and using it as
>> another part of memory sounds like a much bigger undertaking.
>> >>>>
>> >>>> Joseph
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach present=
ation by
>> >>>> John Canny and I am really inspired by his talk and comparisons wit=
h
>> Spark MLlib.
>> >>>>
>> >>>> I am very interested to find out what will be better within Spark:
>> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
>> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=
=80=9D test of
>> >>>> linear algebra, it involves some other things that are essential to
>> machine learning.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Thursday, February 05, 2015 1:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
>> >>>> netlib-java+data
>> >>>> layout and fewer levels of indirection - it's definitely a
>> >>>> worthwhile experiment to run. The main speedups I've seen from
>> >>>> using it come from highly optimized GPU code for linear algebra. I
>> >>>> know that in the past Canny has gone as far as to write custom GPU
>> >>>> kernels for performance-critical regions of code.[1]
>> >>>>
>> >>>> BIDMach is highly optimized for single node performance or
>> >>>> performance on small clusters.[2] Once data doesn't fit easily in
>> >>>> GPU memory (or can be batched in that way) the performance tends to
>> >>>> fall off. Canny argues for hardware/software codesign and as such
>> >>>> prefers machine configurations that are quite different than what
>> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and
>> 4 GPUs.
>> >>>>
>> >>>> In contrast, MLlib was designed for horizontal scalability on
>> >>>> commodity clusters and works best on very big datasets - order of
>> terabytes.
>> >>>>
>> >>>> For the most part, these projects developed concurrently to address
>> >>>> slightly different use cases. That said, there may be bits of
>> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>> >>>> careful about maintaining cross-language compatibility for our Java
>> >>>> and Python-users, though.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>> >>>> you know what makes them faster than netlib-java?
>> >>>>
>> >>>> The same group has BIDMach library that implements machine
>> >>>> learning. For some examples they use Caffe convolutional neural
>> >>>> network library owned by another group in Berkeley. Could you
>> >>>> elaborate on how these all might be connected with Spark Mllib? If
>> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMa=
ch for
>> optimization and learning?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>> >>>> Sent: Thursday, February 05, 2015 12:09 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>> >>>> blas in many cases.
>> >>>>
>> >>>> You might consider taking a look at the codepaths that BIDMat (
>> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>> >>>> optimizing to make this work really fast from Scala. I've run it on
>> >>>> my laptop and compared to MKL and in certain cases it's 10x faster
>> at matrix multiply.
>> >>>> There are a lot of layers of indirection here and you really want
>> >>>> to avoid data copying as much as possible.
>> >>>>
>> >>>> We could also consider swapping out BIDMat for Breeze, but that
>> >>>> would be a big project and if we can figure out how to get
>> >>>> breeze+cublas to comparable performance that would be a big win.
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Dear Spark developers,
>> >>>>
>> >>>> I am exploring how to make linear algebra operations faster within
>> Spark.
>> >>>> One way of doing this is to use Scala Breeze library that is
>> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
>> >>>> and LAPACK native binaries if they are available on the worker
>> >>>> node. It also has its own optimized Java implementation of BLAS. It
>> >>>> is worth mentioning, that native binaries provide better performanc=
e
>> only for BLAS level 3, i.e.
>> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>> >>>> This is confirmed by GEMM test on Netlib-java page
>> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>> >>>> experiments with training of artificial neural network
>> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>> >>>> However, I would like to boost performance more.
>> >>>>
>> >>>> GPU is supposed to work fast with linear algebra and there is
>> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
>> >>>> server with Nvidia GPU and I was able to do the following. I linked
>> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
>> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>> >>>> performance measurements with regards to artificial neural network
>> >>>> batch learning in Spark MLlib that involves matrix-matrix
>> >>>> multiplications. It turns out that for matrices of size less than
>> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
>> >>>> slower for bigger matrices. It worth mentioning that it is was not =
a
>> test for ONLY multiplication since there are other operations involved.
>> >>>> One of the reasons for slowdown might be the overhead of copying
>> >>>> the matrices from computer memory to graphic card memory and back.
>> >>>>
>> >>>> So, few questions:
>> >>>> 1) Do these results with CUDA make sense?
>> >>>> 2) If the problem is with copy overhead, are there any libraries
>> >>>> that allow to force intermediate results to stay in graphic card
>> >>>> memory thus removing the overhead?
>> >>>> 3) Any other options to speed-up linear algebra in Spark?
>> >>>>
>> >>>> Thank you, Alexander
>> >>>>
>> >>>> -------------------------------------------------------------------
>> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
>> dev-unsubscribe@spark.apache.org><mailto:
>> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
>> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
>> >>>> ark.apac> he.org<http://he.org>
>> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
>> >>>> rk.apache.org>>> For additional commands, e-mail:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto=
:
>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>
>>
>> --
>> Best regards,
>> Sam
>>
>
>

--90e6ba613f8a3b16e40512241496--

From dev-return-12188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:06:40 2015
Return-Path: <dev-return-12188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 13C1D17B75
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:06:40 +0000 (UTC)
Received: (qmail 65589 invoked by uid 500); 25 Mar 2015 22:06:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65511 invoked by uid 500); 25 Mar 2015 22:06:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65500 invoked by uid 99); 25 Mar 2015 22:06:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:06:35 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:06:05 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id 302127E;
	Wed, 25 Mar 2015 22:06:03 +0000 (UTC)
Received: from G4W6304.americas.hpqcorp.net (16.210.26.229) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Wed, 25 Mar 2015 22:04:31 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G4W6304.americas.hpqcorp.net ([16.210.26.229]) with mapi id 14.03.0169.001;
 Wed, 25 Mar 2015 22:04:31 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Dmitriy Lyubimov <dlieu.7@gmail.com>
CC: Sam Halliday <sam.halliday@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>, Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley
	<joseph@databricks.com>, "Evan R. Sparks" <evan.sparks@gmail.com>, jfcanny
	<canny@berkeley.edu>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAvQQdDAAKEa4gAAB1qSAAAAgxiA=
Date: Wed, 25 Mar 2015 22:04:30 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68C@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
 <CAPud8ToyQWm0LnFWd+dD37L=+fiDo0ZEpXugigUUAsEO9U6Dew@mail.gmail.com>
In-Reply-To: <CAPud8ToyQWm0LnFWd+dD37L=+fiDo0ZEpXugigUUAsEO9U6Dew@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.192.232]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68CG4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68CG4W3292americas_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64

TmV0bGliIGtub3dzIG5vdGhpbmcgYWJvdXQgR1BVIChvciBDUFUpLCBpdCBqdXN0IHVzZXMgY2Js
YXMgc3ltYm9scyBmcm9tIHRoZSBwcm92aWRlZCBsaWJibGFzLnNvLjMgbGlicmFyeSBhdCB0aGUg
cnVudGltZS4gU28sIHlvdSBjYW4gc3dpdGNoIGF0IHRoZSBydW50aW1lIGJ5IHByb3ZpZGluZyBh
bm90aGVyIGxpYnJhcnkuIFNhbSwgcGxlYXNlIHN1Z2dlc3QgaWYgdGhlcmUgaXMgYW5vdGhlciB3
YXkuDQoNCkZyb206IERtaXRyaXkgTHl1Ymltb3YgW21haWx0bzpkbGlldS43QGdtYWlsLmNvbV0N
ClNlbnQ6IFdlZG5lc2RheSwgTWFyY2ggMjUsIDIwMTUgMjo1NSBQTQ0KVG86IFVsYW5vdiwgQWxl
eGFuZGVyDQpDYzogU2FtIEhhbGxpZGF5OyBkZXZAc3BhcmsuYXBhY2hlLm9yZzsgWGlhbmdydWkg
TWVuZzsgSm9zZXBoIEJyYWRsZXk7IEV2YW4gUi4gU3BhcmtzOyBqZmNhbm55DQpTdWJqZWN0OiBS
ZTogVXNpbmcgQ1VEQSB3aXRoaW4gU3BhcmsgLyBib29zdGluZyBsaW5lYXIgYWxnZWJyYQ0KDQpB
bGV4YW5kZXIsDQoNCmRvZXMgdXNpbmcgbmV0bGliIGltcGx5IHRoYXQgb25lIGNhbm5vdCBzd2l0
Y2ggYmV0d2VlbiBDUFUgYW5kIEdQVSBibGFzIGFsdGVybmF0aXZlcyBhdCB3aWxsIGF0IHRoZSBz
YW1lIHRpbWU/IHRoZSBjaG9pY2UgaXMgYWx3YXlzIGRldGVybWluZWQgYnkgbGlua2luZyBhbGl0
ZXJuYXRpdmVzIHRvIGxpYmJsYXMuc28sIHJpZ2h0Pw0KDQpPbiBXZWQsIE1hciAyNSwgMjAxNSBh
dCAyOjMxIFBNLCBVbGFub3YsIEFsZXhhbmRlciA8YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFp
bHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPj4gd3JvdGU6DQpIaSBhZ2FpbiwNCg0KSSBmaW5h
bGx5IG1hbmFnZWQgdG8gdXNlIG52YmxhcyB3aXRoaW4gU3BhcmsrbmV0bGliLWphdmEuIEl0IGhh
cyBleGNlcHRpb25hbCBwZXJmb3JtYW5jZSBmb3IgYmlnIG1hdHJpY2VzIHdpdGggRG91YmxlLCBm
YXN0ZXIgdGhhbiBCSURNYXQtY3VkYSB3aXRoIEZsb2F0LiBCdXQgZm9yIHNtYWxsZXIgbWF0cmlj
ZXMsIGlmIHlvdSB3aWxsIGNvcHkgdGhlbSB0by9mcm9tIEdQVSwgT3BlbkJsYXMgb3IgTUtMIG1p
Z2h0IGJlIGEgYmV0dGVyIGNob2ljZS4gVGhpcyBjb3JyZWxhdGVzIHdpdGggb3JpZ2luYWwgbnZi
bGFzIHByZXNlbnRhdGlvbiBvbiBHUFUgY29uZiAyMDEzIChzbGlkZSAyMSk6IGh0dHA6Ly9vbi1k
ZW1hbmQuZ3B1dGVjaGNvbmYuY29tL3N1cGVyY29tcHV0aW5nLzIwMTMvcHJlc2VudGF0aW9uL1ND
MzEwOC1OZXctRmVhdHVyZXMtQ1VEQSUyMDYlMjAtR1BVLUFjY2VsZXJhdGlvbi5wZGYNCg0KTXkg
cmVzdWx0czoNCmh0dHBzOi8vZG9jcy5nb29nbGUuY29tL3NwcmVhZHNoZWV0cy9kLzFsV2RWU3VT
cmFnT29iYjBBX29lb3VRZ0hVTXgzNzhUOUo1cjdrd0tTUGtZL2VkaXQ/dXNwPXNoYXJpbmcNCg0K
SnVzdCBpbiBjYXNlLCB0aGVzZSB0ZXN0cyBhcmUgbm90IGZvciBnZW5lcmFsaXphdGlvbiBvZiBw
ZXJmb3JtYW5jZSBvZiBkaWZmZXJlbnQgbGlicmFyaWVzLiBJIGp1c3Qgd2FudCB0byBwaWNrIGEg
bGlicmFyeSB0aGF0IGRvZXMgYXQgYmVzdCBkZW5zZSBtYXRyaWNlcyBtdWx0aXBsaWNhdGlvbiBm
b3IgbXkgdGFzay4NCg0KUC5TLiBNeSBwcmV2aW91cyBpc3N1ZSB3aXRoIG52YmxhcyB3YXMgdGhl
IGZvbGxvd2luZzogaXQgaGFzIEZvcnRyYW4gYmxhcyBmdW5jdGlvbnMsIGF0IHRoZSBzYW1lIHRp
bWUgbmV0bGliLWphdmEgdXNlcyBDIGNibGFzIGZ1bmN0aW9ucy4gU28sIG9uZSBuZWVkcyBjYmxh
cyBzaGFyZWQgbGlicmFyeSB0byB1c2UgbnZibGFzIHRocm91Z2ggbmV0bGliLWphdmEuIEZlZG9y
YSBkb2VzIG5vdCBoYXZlIGNibGFzIChidXQgRGViaWFuIGFuZCBVYnVudHUgaGF2ZSksIHNvIEkg
bmVlZGVkIHRvIGNvbXBpbGUgaXQuIEkgY291bGQgbm90IHVzZSBjYmxhcyBmcm9tIEF0bGFzIG9y
IE9wZW5ibGFzIGJlY2F1c2UgdGhleSBsaW5rIHRvIHRoZWlyIGltcGxlbWVudGF0aW9uIGFuZCBu
b3QgdG8gRm9ydHJhbiBibGFzLg0KDQpCZXN0IHJlZ2FyZHMsIEFsZXhhbmRlcg0KDQotLS0tLU9y
aWdpbmFsIE1lc3NhZ2UtLS0tLQ0KRnJvbTogVWxhbm92LCBBbGV4YW5kZXINClNlbnQ6IFR1ZXNk
YXksIE1hcmNoIDI0LCAyMDE1IDY6NTcgUE0NClRvOiBTYW0gSGFsbGlkYXkNCkNjOiBkZXZAc3Bh
cmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+OyBYaWFuZ3J1aSBNZW5n
OyBKb3NlcGggQnJhZGxleTsgRXZhbiBSLiBTcGFya3MNClN1YmplY3Q6IFJFOiBVc2luZyBDVURB
IHdpdGhpbiBTcGFyayAvIGJvb3N0aW5nIGxpbmVhciBhbGdlYnJhDQoNCkhpLA0KDQpJIGFtIHRy
eWluZyB0byB1c2UgbnZibGFzIHdpdGggbmV0bGliLWphdmEgZnJvbSBTcGFyay4gbnZibGFzIGZ1
bmN0aW9ucyBzaG91bGQgcmVwbGFjZSBjdXJyZW50IGJsYXMgZnVuY3Rpb25zIGNhbGxzIGFmdGVy
IGV4ZWN1dGluZyBMRF9QUkVMT0FEIGFzIHN1Z2dlc3RlZCBpbiBodHRwOi8vZG9jcy5udmlkaWEu
Y29tL2N1ZGEvbnZibGFzLyNVc2FnZSB3aXRob3V0IGFueSBjaGFuZ2VzIHRvIG5ldGxpYi1qYXZh
LiBJdCBzZWVtcyB0byB3b3JrIGZvciBzaW1wbGUgSmF2YSBleGFtcGxlLCBidXQgSSBjYW5ub3Qg
bWFrZSBpdCB3b3JrIHdpdGggU3BhcmsuIEkgcnVuIHRoZSBmb2xsb3dpbmc6DQpleHBvcnQgTERf
TElCUkFSWV9QQVRIPS91c3IvbG9jYWwvY3VkYS02LjUvbGliNjQNCmVudiBMRF9QUkVMT0FEPS91
c3IvbG9jYWwvY3VkYS02LjUvbGliNjQvbGlibnZibGFzLnNvIC4vc3Bhcmstc2hlbGwgLS1kcml2
ZXItbWVtb3J5IDRHIEluIG52aWRpYS1zbWkgSSBvYnNlcnZlIHRoYXQgSmF2YSBpcyB0byB1c2Ug
R1BVOg0KKy0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tKw0KfCBQcm9jZXNzZXM6ICAgICAgICAgICAgICAg
ICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgIEdQVSBNZW1vcnkgfA0KfCAg
R1BVICAgICAgIFBJRCAgVHlwZSAgUHJvY2VzcyBuYW1lICAgICAgICAgICAgICAgICAgICAgICAg
ICAgICAgIFVzYWdlICAgICAgfA0KfD09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09
PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09PT09fA0KfCAgICAwICAgICAg
ODg3MyAgICBDICAgYmFzaCAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAg
ICAgMzlNaUIgfA0KfCAgICAwICAgICAgODkxMCAgICBDICAgL3Vzci9saWIvanZtL2phdmEtMS43
LjAvYmluL2phdmEgICAgICAgICAgICAgICAgMzlNaUIgfA0KKy0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
Kw0KDQpJbiBTcGFyayBzaGVsbCBJIGRvIG1hdHJpeCBtdWx0aXBsaWNhdGlvbiBhbmQgc2VlIHRo
ZSBmb2xsb3dpbmc6DQoxNS8wMy8yNSAwNjo0ODowMSBJTkZPIEpuaUxvYWRlcjogc3VjY2Vzc2Z1
bGx5IGxvYWRlZCAvdG1wL2puaWxvYWRlcjgxOTI5NjQzNzcwMDk5NjU0ODNuZXRsaWItbmF0aXZl
X3N5c3RlbS1saW51eC14ODZfNjQuc28NClNvIEkgYW0gc3VyZSB0aGF0IG5ldGxpYi1uYXRpdmUg
aXMgbG9hZGVkIGFuZCBjYmxhcyBzdXBwb3NlZGx5IHVzZWQuIEhvd2V2ZXIsIG1hdHJpeCBtdWx0
aXBsaWNhdGlvbiBkb2VzIGV4ZWN1dGVzIG9uIENQVSBzaW5jZSBJIHNlZSAxNiUgb2YgQ1BVIHVz
ZWQgYW5kIDAlIG9mIEdQVSB1c2VkLiBJIGFsc28gY2hlY2tlZCBkaWZmZXJlbnQgbWF0cml4IHNp
emVzLCBmcm9tIDEwMHgxMDAgdG8gMTIwMDB4MTIwMDANCg0KQ291bGQgeW91IHN1Z2dlc3QgbWln
aHQgdGhlIExEX1BSRUxPQUQgbm90IGFmZmVjdCBTcGFyayBzaGVsbD8NCg0KQmVzdCByZWdhcmRz
LCBBbGV4YW5kZXINCg0KDQoNCkZyb206IFNhbSBIYWxsaWRheSBbbWFpbHRvOnNhbS5oYWxsaWRh
eUBnbWFpbC5jb208bWFpbHRvOnNhbS5oYWxsaWRheUBnbWFpbC5jb20+XQ0KU2VudDogTW9uZGF5
LCBNYXJjaCAwOSwgMjAxNSA2OjAxIFBNDQpUbzogVWxhbm92LCBBbGV4YW5kZXINCkNjOiBkZXZA
c3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+OyBYaWFuZ3J1aSBN
ZW5nOyBKb3NlcGggQnJhZGxleTsgRXZhbiBSLiBTcGFya3MNClN1YmplY3Q6IFJFOiBVc2luZyBD
VURBIHdpdGhpbiBTcGFyayAvIGJvb3N0aW5nIGxpbmVhciBhbGdlYnJhDQoNCg0KVGhhbmtzIHNv
IG11Y2ggZm9yIGZvbGxvd2luZyB1cCBvbiB0aGlzIQ0KDQpIbW0sIEkgd29uZGVyIGlmIHdlIHNo
b3VsZCBoYXZlIGEgY29uY2VydGVkIGVmZm9ydCB0byBjaGFydCBwZXJmb3JtYW5jZSBvbiB2YXJp
b3VzIHBpZWNlcyBvZiBoYXJkd2FyZS4uLg0KT24gOSBNYXIgMjAxNSAyMTowOCwgIlVsYW5vdiwg
QWxleGFuZGVyIiA8YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51bGFu
b3ZAaHAuY29tPjxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRl
ci51bGFub3ZAaHAuY29tPj4+IHdyb3RlOg0KSGkgRXZlcnlvbmUsIEkndmUgdXBkYXRlZCB0aGUg
YmVuY2htYXJrIGFzIFhpYW5ncnVpIHN1Z2dlc3RlZC4gQWRkZWQgdGhlIGNvbW1lbnQgdGhhdCBC
SURNYXQgMC45LjcgdXNlcyBGbG9hdCBtYXRyaWNlcyBpbiBHUFUgKGFsdGhvdWdoIEkgc2VlIHRo
ZSBzdXBwb3J0IG9mIERvdWJsZSBpbiB0aGUgY3VycmVudCBzb3VyY2UgY29kZSksIGRpZCB0aGUg
dGVzdCB3aXRoIEJJRE1hdCBhbmQgQ1BVIERvdWJsZSBtYXRyaWNlcy4gQklETWF0IE1LTCBpcyBp
bmRlZWQgb24gcGFyIHdpdGggbmV0bGliIE1LTC4NCg0KaHR0cHM6Ly9kb2NzLmdvb2dsZS5jb20v
c3ByZWFkc2hlZXRzL2QvMWxXZFZTdVNyYWdPb2JiMEFfb2VvdVFnSFVNeDM3OFQ5SjVyN2t3S1NQ
a1kvZWRpdD91c3A9c2hhcmluZw0KDQpCZXN0IHJlZ2FyZHMsIEFsZXhhbmRlcg0KDQotLS0tLU9y
aWdpbmFsIE1lc3NhZ2UtLS0tLQ0KRnJvbTogU2FtIEhhbGxpZGF5IFttYWlsdG86c2FtLmhhbGxp
ZGF5QGdtYWlsLmNvbTxtYWlsdG86c2FtLmhhbGxpZGF5QGdtYWlsLmNvbT48bWFpbHRvOnNhbS5o
YWxsaWRheUBnbWFpbC5jb208bWFpbHRvOnNhbS5oYWxsaWRheUBnbWFpbC5jb20+Pl0NClNlbnQ6
IFR1ZXNkYXksIE1hcmNoIDAzLCAyMDE1IDE6NTQgUE0NClRvOiBYaWFuZ3J1aSBNZW5nOyBKb3Nl
cGggQnJhZGxleQ0KQ2M6IEV2YW4gUi4gU3BhcmtzOyBVbGFub3YsIEFsZXhhbmRlcjsgZGV2QHNw
YXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPjxtYWlsdG86ZGV2QHNw
YXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPj4NClN1YmplY3Q6IFJl
OiBVc2luZyBDVURBIHdpdGhpbiBTcGFyayAvIGJvb3N0aW5nIGxpbmVhciBhbGdlYnJhDQoNCkJU
VywgaXMgYW55Ym9keSBvbiB0aGlzIGxpc3QgZ29pbmcgdG8gdGhlIExvbmRvbiBNZWV0dXAgaW4g
YSBmZXcgd2Vla3M/DQoNCmh0dHBzOi8vc2tpbGxzbWF0dGVyLmNvbS9tZWV0dXBzLzY5ODctYXBh
Y2hlLXNwYXJrLWxpdmluZy10aGUtcG9zdC1tYXByZWR1Y2Utd29ybGQjY29tbXVuaXR5DQoNCldv
dWxkIGJlIG5pY2UgdG8gbWVldCBvdGhlciBwZW9wbGUgd29ya2luZyBvbiB0aGUgZ3V0cyBvZiBT
cGFyayEgOi0pDQoNCg0KWGlhbmdydWkgTWVuZyA8bWVuZ3hyQGdtYWlsLmNvbTxtYWlsdG86bWVu
Z3hyQGdtYWlsLmNvbT48bWFpbHRvOm1lbmd4ckBnbWFpbC5jb208bWFpbHRvOm1lbmd4ckBnbWFp
bC5jb20+Pj4gd3JpdGVzOg0KDQo+IEhleSBBbGV4YW5kZXIsDQo+DQo+IEkgZG9uJ3QgcXVpdGUg
dW5kZXJzdGFuZCB0aGUgcGFydCB3aGVyZSBuZXRsaWItY3VibGFzIGlzIGFib3V0IDIweA0KPiBz
bG93ZXIgdGhhbiBuZXRsaWItb3BlbmJsYXMuIFdoYXQgaXMgdGhlIG92ZXJoZWFkIG9mIHVzaW5n
IGEgR1BVIEJMQVMNCj4gd2l0aCBuZXRsaWItamF2YT8NCj4NCj4gQ0MnZWQgU2FtLCB0aGUgYXV0
aG9yIG9mIG5ldGxpYi1qYXZhLg0KPg0KPiBCZXN0LA0KPiBYaWFuZ3J1aQ0KPg0KPiBPbiBXZWQs
IEZlYiAyNSwgMjAxNSBhdCAzOjM2IFBNLCBKb3NlcGggQnJhZGxleSA8am9zZXBoQGRhdGFicmlj
a3MuY29tPG1haWx0bzpqb3NlcGhAZGF0YWJyaWNrcy5jb20+PG1haWx0bzpqb3NlcGhAZGF0YWJy
aWNrcy5jb208bWFpbHRvOmpvc2VwaEBkYXRhYnJpY2tzLmNvbT4+PiB3cm90ZToNCj4+IEJldHRl
ciBkb2N1bWVudGF0aW9uIGZvciBsaW5raW5nIHdvdWxkIGJlIHZlcnkgaGVscGZ1bCEgIEhlcmUn
cyBhIEpJUkE6DQo+PiBodHRwczovL2lzc3Vlcy5hcGFjaGUub3JnL2ppcmEvYnJvd3NlL1NQQVJL
LTYwMTkNCj4+DQo+Pg0KPj4gT24gV2VkLCBGZWIgMjUsIDIwMTUgYXQgMjo1MyBQTSwgRXZhbiBS
LiBTcGFya3MNCj4+IDxldmFuLnNwYXJrc0BnbWFpbC5jb208bWFpbHRvOmV2YW4uc3BhcmtzQGdt
YWlsLmNvbT48bWFpbHRvOmV2YW4uc3BhcmtzQGdtYWlsLmNvbTxtYWlsdG86ZXZhbi5zcGFya3NA
Z21haWwuY29tPj4+DQo+PiB3cm90ZToNCj4+DQo+Pj4gVGhhbmtzIGZvciBjb21waWxpbmcgYWxs
IHRoZSBkYXRhIGFuZCBydW5uaW5nIHRoZXNlIGJlbmNobWFya3MsDQo+Pj4gQWxleC4gVGhlIGJp
ZyB0YWtlYXdheXMgaGVyZSBjYW4gYmUgc2VlbiB3aXRoIHRoaXMgY2hhcnQ6DQo+Pj4NCj4+PiBo
dHRwczovL2RvY3MuZ29vZ2xlLmNvbS9zcHJlYWRzaGVldHMvZC8xYVJtMklBRFJmWFFWN0cydnJj
Vmg0U3RGNTB1Wg0KPj4+IEhsNmttQUplYVpaZ2dyMC9wdWJjaGFydD9vaWQ9MTg5OTc2NzExOSZm
b3JtYXQ9aW50ZXJhY3RpdmUNCj4+Pg0KPj4+IDEpIEEgcHJvcGVybHkgY29uZmlndXJlZCBHUFUg
bWF0cml4IG11bHRpcGx5IGltcGxlbWVudGF0aW9uIChlLmcuDQo+Pj4gQklETWF0K0dQVSkgY2Fu
IHByb3ZpZGUgc3Vic3RhbnRpYWwgKGJ1dCBsZXNzIHRoYW4gYW4gb3JkZXIgb2YNCj4+PiBCSURN
YXQrbWFnbml0dWRlKQ0KPj4+IGJlbmVmaXQgb3ZlciBhIHdlbGwtdHVuZWQgQ1BVIGltcGxlbWVu
dGF0aW9uIChlLmcuIEJJRE1hdCtNS0wgb3INCj4+PiBuZXRsaWItamF2YStvcGVuYmxhcy1jb21w
aWxlZCkuDQo+Pj4gMikgQSBwb29ybHkgdHVuZWQgQ1BVIGltcGxlbWVudGF0aW9uIGNhbiBiZSAx
LTIgb3JkZXJzIG9mIG1hZ25pdHVkZQ0KPj4+IHdvcnNlIHRoYW4gYSB3ZWxsLXR1bmVkIENQVSBp
bXBsZW1lbnRhdGlvbiwgcGFydGljdWxhcmx5IGZvciBsYXJnZXIgbWF0cmljZXMuDQo+Pj4gKG5l
dGxpYi1mMmpibGFzIG9yIG5ldGxpYi1yZWYpIFRoaXMgaXMgbm90IHRvIHBpY2sgb24gbmV0bGli
IC0gdGhpcw0KPj4+IGJhc2ljYWxseSBhZ3JlZXMgd2l0aCB0aGUgYXV0aG9ycyBvd24gYmVuY2ht
YXJrcyAoDQo+Pj4gaHR0cHM6Ly9naXRodWIuY29tL2ZvbW1pbC9uZXRsaWItamF2YSkNCj4+Pg0K
Pj4+IEkgdGhpbmsgdGhhdCBtb3N0IG9mIG91ciB1c2VycyBhcmUgaW4gYSBzaXR1YXRpb24gd2hl
cmUgdXNpbmcgR1BVcw0KPj4+IG1heSBub3QgYmUgcHJhY3RpY2FsIC0gYWx0aG91Z2ggd2UgY291
bGQgY29uc2lkZXIgaGF2aW5nIGEgZ29vZCBHUFUNCj4+PiBiYWNrZW5kIGF2YWlsYWJsZSBhcyBh
biBvcHRpb24uIEhvd2V2ZXIsICpBTEwqIHVzZXJzIG9mIE1MbGliIGNvdWxkDQo+Pj4gYmVuZWZp
dCAocG90ZW50aWFsbHkgdHJlbWVuZG91c2x5KSBmcm9tIHVzaW5nIGEgd2VsbC10dW5lZCBDUFUt
YmFzZWQNCj4+PiBCTEFTIGltcGxlbWVudGF0aW9uLiBQZXJoYXBzIHdlIHNob3VsZCBjb25zaWRl
ciB1cGRhdGluZyB0aGUgbWxsaWINCj4+PiBndWlkZSB3aXRoIGEgbW9yZSBjb21wbGV0ZSBzZWN0
aW9uIGZvciBlbmFibGluZyBoaWdoIHBlcmZvcm1hbmNlDQo+Pj4gYmluYXJpZXMgb24gT1NYIGFu
ZCBMaW51eD8gT3IgYmV0dGVyLCBmaWd1cmUgb3V0IGEgd2F5IGZvciB0aGUNCj4+PiBzeXN0ZW0g
dG8gZmV0Y2ggdGhlc2UgYXV0b21hdGljYWxseS4NCj4+Pg0KPj4+IC0gRXZhbg0KPj4+DQo+Pj4N
Cj4+Pg0KPj4+IE9uIFRodSwgRmViIDEyLCAyMDE1IGF0IDQ6MTggUE0sIFVsYW5vdiwgQWxleGFu
ZGVyIDwNCj4+PiBhbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVsYW5v
dkBocC5jb20+PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVy
LnVsYW5vdkBocC5jb20+Pj4gd3JvdGU6DQo+Pj4NCj4+Pj4gSnVzdCB0byBzdW1tYXJpemUgdGhp
cyB0aHJlYWQsIEkgd2FzIGZpbmFsbHkgYWJsZSB0byBtYWtlIGFsbA0KPj4+PiBwZXJmb3JtYW5j
ZSBjb21wYXJpc29ucyB0aGF0IHdlIGRpc2N1c3NlZC4gSXQgdHVybnMgb3V0IHRoYXQ6DQo+Pj4+
IEJJRE1hdC1jdWJsYXM+PkJJRE1hdA0KPj4+PiBNS0w9PW5ldGxpYi1ta2w9PW5ldGxpYi1vcGVu
Ymxhcy1jb21waWxlZD5uZXRsaWItb3BlbmJsYXMteXVtLXJlcG89DQo+Pj4+ID1uZXRsaWItY3Vi
bGFzPm5ldGxpYi1ibGFzPmYyamJsYXMNCj4+Pj4NCj4+Pj4gQmVsb3cgaXMgdGhlIGxpbmsgdG8g
dGhlIHNwcmVhZHNoZWV0IHdpdGggZnVsbCByZXN1bHRzLg0KPj4+Pg0KPj4+PiBodHRwczovL2Rv
Y3MuZ29vZ2xlLmNvbS9zcHJlYWRzaGVldHMvZC8xbFdkVlN1U3JhZ09vYmIwQV9vZW91UWdIVU14
DQo+Pj4+IDM3OFQ5SjVyN2t3S1NQa1kvZWRpdD91c3A9c2hhcmluZw0KPj4+Pg0KPj4+PiBPbmUg
dGhpbmcgc3RpbGwgbmVlZHMgZXhwbG9yYXRpb246IGRvZXMgQklETWF0LWN1YmxhcyBwZXJmb3Jt
DQo+Pj4+IGNvcHlpbmcgdG8vZnJvbSBtYWNoaW5l4oCZcyBSQU0/DQo+Pj4+DQo+Pj4+IC0tLS0t
T3JpZ2luYWwgTWVzc2FnZS0tLS0tDQo+Pj4+IEZyb206IFVsYW5vdiwgQWxleGFuZGVyDQo+Pj4+
IFNlbnQ6IFR1ZXNkYXksIEZlYnJ1YXJ5IDEwLCAyMDE1IDI6MTIgUE0NCj4+Pj4gVG86IEV2YW4g
Ui4gU3BhcmtzDQo+Pj4+IENjOiBKb3NlcGggQnJhZGxleTsNCj4+Pj4gZGV2QHNwYXJrLmFwYWNo
ZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPjxtYWlsdG86ZGV2QHNwYXJrLmFwYWNo
ZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPj4NCj4+Pj4gU3ViamVjdDogUkU6IFVz
aW5nIENVREEgd2l0aGluIFNwYXJrIC8gYm9vc3RpbmcgbGluZWFyIGFsZ2VicmENCj4+Pj4NCj4+
Pj4gVGhhbmtzLCBFdmFuISBJdCBzZWVtcyB0aGF0IHRpY2tldCB3YXMgbWFya2VkIGFzIGR1cGxp
Y2F0ZSB0aG91Z2gNCj4+Pj4gdGhlIG9yaWdpbmFsIG9uZSBkaXNjdXNzZXMgc2xpZ2h0bHkgZGlm
ZmVyZW50IHRvcGljLiBJIHdhcyBhYmxlIHRvDQo+Pj4+IGxpbmsgbmV0bGliIHdpdGggTUtMIGZy
b20gQklETWF0IGJpbmFyaWVzLiBJbmRlZWQsIE1LTCBpcw0KPj4+PiBzdGF0aWNhbGx5IGxpbmtl
ZCBpbnNpZGUgYSA2ME1CIGxpYnJhcnkuDQo+Pj4+DQo+Pj4+IHxBKkIgIHNpemUgfCBCSURNYXQg
TUtMIHwgQnJlZXplK05ldGxpYi1NS0wgIGZyb20gQklETWF0fA0KPj4+PiBCcmVlemUrTmV0bGli
LU9wZW5CbGFzKG5hdGl2ZSBzeXN0ZW0pfCBCcmVlemUrTmV0bGliLWYyamJsYXMgfA0KPj4+PiAr
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0rDQo+Pj4+IHwxMDB4MTAwKjEwMHgxMDAgfCAwLDAwMjA1NTk2IHwgMCww
MDAzODEgfCAwLDAzODEwMzI0IHwgMCwwMDI1NTYgfA0KPj4+PiB8MTAwMHgxMDAwKjEwMDB4MTAw
MCB8IDAsMDE4MzIwOTQ3IHwgMCwwMzgzMTY4NTcgfCAwLDUxODAzNTU3DQo+Pj4+IHwxLDYzODQ3
NTQ1OSB8DQo+Pj4+IHwxMDAwMHgxMDAwMCoxMDAwMHgxMDAwMCB8IDIzLDc4MDQ2NjMyIHwgMzIs
OTQ1NDY2OTcgfDQ0NSwwOTM1MjExIHwNCj4+Pj4gMTU2OSwyMzMyMjggfA0KPj4+Pg0KPj4+PiBJ
dCB0dXJuIG91dCB0aGF0IHByZS1jb21waWxlZCBNS0wgaXMgZmFzdGVyIHRoYW4gcHJlY29tcGls
ZWQNCj4+Pj4gT3BlbkJsYXMgb24gbXkgbWFjaGluZS4gUHJvYmFibHksIEnigJlsbCBhZGQgdHdv
IG1vcmUgY29sdW1ucyB3aXRoDQo+Pj4+IGxvY2FsbHkgY29tcGlsZWQgb3BlbmJsYXMgYW5kIGN1
ZGEuDQo+Pj4+DQo+Pj4+IEFsZXhhbmRlcg0KPj4+Pg0KPj4+PiBGcm9tOiBFdmFuIFIuIFNwYXJr
cw0KPj4+PiBbbWFpbHRvOmV2YW4uc3BhcmtzQGdtYWlsLmNvbTxtYWlsdG86ZXZhbi5zcGFya3NA
Z21haWwuY29tPjxtYWlsdG86ZXZhbi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpldmFuLnNwYXJr
c0BnbWFpbC5jb20+Pl0NCj4+Pj4gU2VudDogTW9uZGF5LCBGZWJydWFyeSAwOSwgMjAxNSA2OjA2
IFBNDQo+Pj4+IFRvOiBVbGFub3YsIEFsZXhhbmRlcg0KPj4+PiBDYzogSm9zZXBoIEJyYWRsZXk7
DQo+Pj4+IGRldkBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZz48
bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZz4+
DQo+Pj4+IFN1YmplY3Q6IFJlOiBVc2luZyBDVURBIHdpdGhpbiBTcGFyayAvIGJvb3N0aW5nIGxp
bmVhciBhbGdlYnJhDQo+Pj4+DQo+Pj4+IEdyZWF0IC0gcGVyaGFwcyB3ZSBjYW4gbW92ZSB0aGlz
IGRpc2N1c3Npb24gb2ZmLWxpc3QgYW5kIG9udG8gYQ0KPj4+PiBKSVJBIHRpY2tldD8gKEhlcmUn
cyBvbmU6DQo+Pj4+IGh0dHBzOi8vaXNzdWVzLmFwYWNoZS5vcmcvamlyYS9icm93c2UvU1BBUkst
NTcwNSkNCj4+Pj4NCj4+Pj4gSXQgc2VlbXMgbGlrZSB0aGlzIGlzIGdvaW5nIHRvIGJlIHNvbWV3
aGF0IGV4cGxvcmF0b3J5IGZvciBhIHdoaWxlDQo+Pj4+IChhbmQgdGhlcmUncyBwcm9iYWJseSBv
bmx5IGEgaGFuZGZ1bCBvZiB1cyB3aG8gcmVhbGx5IGNhcmUgYWJvdXQNCj4+Pj4gZmFzdCBsaW5l
YXINCj4+Pj4gYWxnZWJyYSEpDQo+Pj4+DQo+Pj4+IC0gRXZhbg0KPj4+Pg0KPj4+PiBPbiBNb24s
IEZlYiA5LCAyMDE1IGF0IDQ6NDggUE0sIFVsYW5vdiwgQWxleGFuZGVyIDwNCj4+Pj4gYWxleGFu
ZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPjxtYWlsdG86
YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPj48
bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhw
LmNvbT48bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxh
bm92QGhwLmNvbT4+Pj4gd3JvdGU6DQo+Pj4+IEhpIEV2YW4sDQo+Pj4+DQo+Pj4+IFRoYW5rIHlv
dSBmb3IgZXhwbGFuYXRpb24gYW5kIHVzZWZ1bCBsaW5rLiBJIGFtIGdvaW5nIHRvIGJ1aWxkDQo+
Pj4+IE9wZW5CTEFTLCBsaW5rIGl0IHdpdGggTmV0bGliLWphdmEgYW5kIHBlcmZvcm0gYmVuY2ht
YXJrIGFnYWluLg0KPj4+Pg0KPj4+PiBEbyBJIHVuZGVyc3RhbmQgY29ycmVjdGx5IHRoYXQgQklE
TWF0IGJpbmFyaWVzIGNvbnRhaW4gc3RhdGljYWxseQ0KPj4+PiBsaW5rZWQgSW50ZWwgTUtMIEJM
QVM/IEl0IG1pZ2h0IGJlIHRoZSByZWFzb24gd2h5IEkgYW0gYWJsZSB0byBydW4NCj4+Pj4gQklE
TWF0IG5vdCBoYXZpbmcgTUtMIEJMQVMgaW5zdGFsbGVkIG9uIG15IHNlcnZlci4gSWYgaXQgaXMg
dHJ1ZSwgSQ0KPj4+PiB3b25kZXIgaWYgaXQgaXMgT0sgYmVjYXVzZSBJbnRlbCBzZWxscyB0aGlz
IGxpYnJhcnkuIE5ldmVydGhlbGVzcywNCj4+Pj4gaXQgc2VlbXMgdGhhdCBpbiBteSBjYXNlIHBy
ZWNvbXBpbGVkIE1LTCBCTEFTIHBlcmZvcm1zIGJldHRlciB0aGFuDQo+Pj4+IHByZWNvbXBpbGVk
IE9wZW5CTEFTIGdpdmVuIHRoYXQgQklETWF0IGFuZCBOZXRsaWItamF2YSBhcmUgc3VwcG9zZWQg
dG8gYmUgb24gcGFyIHdpdGggSk5JIG92ZXJoZWFkcy4NCj4+Pj4NCj4+Pj4gVGhvdWdoLCBpdCBt
aWdodCBiZSBpbnRlcmVzdGluZyB0byBsaW5rIE5ldGxpYi1qYXZhIHdpdGggSW50ZWwgTUtMLA0K
Pj4+PiBhcyB5b3Ugc3VnZ2VzdGVkLiBJIHdvbmRlciwgYXJlIEpvaG4gQ2FubnkgKEJJRE1hdCkg
YW5kIFNhbQ0KPj4+PiBIYWxsaWRheQ0KPj4+PiAoTmV0bGliLWphdmEpIGludGVyZXN0ZWQgdG8g
Y29tcGFyZSB0aGVpciBsaWJyYXJpZXMuDQo+Pj4+DQo+Pj4+IEJlc3QgcmVnYXJkcywgQWxleGFu
ZGVyDQo+Pj4+DQo+Pj4+IEZyb206IEV2YW4gUi4gU3BhcmtzIFttYWlsdG86ZXZhbi5zcGFya3NA
Z21haWwuY29tPG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb20+PG1haWx0bzpldmFuLnNwYXJr
c0BnbWFpbC5jb208bWFpbHRvOmV2YW4uc3BhcmtzQGdtYWlsLmNvbT4+PG1haWx0bzoNCj4+Pj4g
ZXZhbi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb20+PG1haWx0
bzpldmFuLnNwYXJrc0BnbWFpbC5jb208bWFpbHRvOmV2YW4uc3BhcmtzQGdtYWlsLmNvbT4+Pl0N
Cj4+Pj4gU2VudDogRnJpZGF5LCBGZWJydWFyeSAwNiwgMjAxNSA1OjU4IFBNDQo+Pj4+DQo+Pj4+
IFRvOiBVbGFub3YsIEFsZXhhbmRlcg0KPj4+PiBDYzogSm9zZXBoIEJyYWRsZXk7DQo+Pj4+IGRl
dkBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZz48bWFpbHRvOmRl
dkBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZz4+PG1haWx0bzpk
ZXZAc3Bhcms8bWFpbHRvOmRldkBzcGFyaz4uDQo+Pj4+IGFwYWNoZS5vcmc8aHR0cDovL2FwYWNo
ZS5vcmc+PG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNo
ZS5vcmc+Pj4NCj4+Pj4gU3ViamVjdDogUmU6IFVzaW5nIENVREEgd2l0aGluIFNwYXJrIC8gYm9v
c3RpbmcgbGluZWFyIGFsZ2VicmENCj4+Pj4NCj4+Pj4gSSB3b3VsZCBidWlsZCBPcGVuQkxBUyB5
b3Vyc2VsZiwgc2luY2UgZ29vZCBCTEFTIHBlcmZvcm1hbmNlIGNvbWVzDQo+Pj4+IGZyb20gZ2V0
dGluZyBjYWNoZSBzaXplcywgZXRjLiBzZXQgdXAgY29ycmVjdGx5IGZvciB5b3VyIHBhcnRpY3Vs
YXINCj4+Pj4gaGFyZHdhcmUgLSB0aGlzIGlzIG9mdGVuIGEgdmVyeSB0cmlja3kgcHJvY2VzcyAo
c2VlLCBlLmcuIEFUTEFTKSwNCj4+Pj4gYnV0IHdlIGZvdW5kIHRoYXQgb24gcmVsYXRpdmVseSBt
b2Rlcm4gWGVvbiBjaGlwcywgT3BlbkJMQVMgYnVpbGRzDQo+Pj4+IHF1aWNrbHkgYW5kIHlpZWxk
cyBwZXJmb3JtYW5jZSBjb21wZXRpdGl2ZSB3aXRoIE1LTC4NCj4+Pj4NCj4+Pj4gVG8gbWFrZSBz
dXJlIHRoZSByaWdodCBsaWJyYXJ5IGlzIGdldHRpbmcgdXNlZCwgeW91IGhhdmUgdG8gbWFrZQ0K
Pj4+PiBzdXJlIGl0J3MgZmlyc3Qgb24gdGhlIHNlYXJjaCBwYXRoIC0gZXhwb3J0DQo+Pj4+IExE
X0xJQlJBUllfUEFUSD0vcGF0aC90by9ibGFzL2xpYnJhcnkuc28gd2lsbCBkbyB0aGUgdHJpY2sg
aGVyZS4NCj4+Pj4NCj4+Pj4gRm9yIHNvbWUgZXhhbXBsZXMgb2YgZ2V0dGluZyBuZXRsaWItamF2
YSBzZXR1cCBvbiBhbiBlYzIgbm9kZSBhbmQNCj4+Pj4gc29tZSBleGFtcGxlIGJlbmNobWFya2lu
ZyBjb2RlIHdlIHJhbiBhIHdoaWxlIGJhY2ssIHNlZToNCj4+Pj4gaHR0cHM6Ly9naXRodWIuY29t
L3NoaXZhcmFtL21hdHJpeC1iZW5jaA0KPj4+Pg0KPj4+PiBJbiBwYXJ0aWN1bGFyIC0gYnVpbGQt
b3BlbmJsYXMtZWMyLnNoIHNob3dzIHlvdSBob3cgdG8gYnVpbGQgdGhlDQo+Pj4+IGxpYnJhcnkg
YW5kIHNldCB1cCBzeW1saW5rcyBjb3JyZWN0bHksIGFuZCBzY2FsYS9ydW4tbmV0bGliLnNoDQo+
Pj4+IHNob3dzIHlvdSBob3cgdG8gZ2V0IHRoZSBwYXRoIHNldHVwIGFuZCBnZXQgdGhhdCBsaWJy
YXJ5IHBpY2tlZCB1cCBieSBuZXRsaWItamF2YS4NCj4+Pj4NCj4+Pj4gSW4gdGhpcyB3YXkgLSB5
b3UgY291bGQgcHJvYmFibHkgZ2V0IGN1QkxBUyBzZXQgdXAgdG8gYmUgdXNlZCBieQ0KPj4+PiBu
ZXRsaWItamF2YSBhcyB3ZWxsLg0KPj4+Pg0KPj4+PiAtIEV2YW4NCj4+Pj4NCj4+Pj4gT24gRnJp
LCBGZWIgNiwgMjAxNSBhdCA1OjQzIFBNLCBVbGFub3YsIEFsZXhhbmRlciA8DQo+Pj4+IGFsZXhh
bmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT48bWFpbHRv
OmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT4+
PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBo
cC5jb20+PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVs
YW5vdkBocC5jb20+Pj4+IHdyb3RlOg0KPj4+PiBFdmFuLCBjb3VsZCB5b3UgZWxhYm9yYXRlIG9u
IGhvdyB0byBmb3JjZSBCSURNYXQgYW5kIG5ldGxpYi1qYXZhIHRvDQo+Pj4+IGZvcmNlIGxvYWRp
bmcgdGhlIHJpZ2h0IGJsYXM/IEZvciBuZXRsaWIsIEkgdGhlcmUgYXJlIGZldyBKVk0NCj4+Pj4g
ZmxhZ3MsIHN1Y2ggYXMNCj4+Pj4gLURjb20uZ2l0aHViLmZvbW1pbC5uZXRsaWIuQkxBUz1jb20u
Z2l0aHViLmZvbW1pbC5uZXRsaWIuRjJqQkxBUywNCj4+Pj4gc28gSSBjYW4gZm9yY2UgaXQgdG8g
dXNlIEphdmEgaW1wbGVtZW50YXRpb24uIE5vdCBzdXJlIEkgdW5kZXJzdGFuZCBob3cgdG8gZm9y
Y2UgdXNlIGEgc3BlY2lmaWMgYmxhcyAobm90IHNwZWNpZmljIHdyYXBwZXIgZm9yIGJsYXMpLg0K
Pj4+Pg0KPj4+PiBCdHcuIEkgaGF2ZSBpbnN0YWxsZWQgb3BlbmJsYXMgKHl1bSBpbnN0YWxsIG9w
ZW5ibGFzKSwgc28gSSBzdXBwb3NlDQo+Pj4+IHRoYXQgbmV0bGliIGlzIHVzaW5nIGl0Lg0KPj4+
Pg0KPj4+PiBGcm9tOiBFdmFuIFIuIFNwYXJrcyBbbWFpbHRvOmV2YW4uc3BhcmtzQGdtYWlsLmNv
bTxtYWlsdG86ZXZhbi5zcGFya3NAZ21haWwuY29tPjxtYWlsdG86ZXZhbi5zcGFya3NAZ21haWwu
Y29tPG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb20+PjxtYWlsdG86DQo+Pj4+IGV2YW4uc3Bh
cmtzQGdtYWlsLmNvbTxtYWlsdG86ZXZhbi5zcGFya3NAZ21haWwuY29tPjxtYWlsdG86ZXZhbi5z
cGFya3NAZ21haWwuY29tPG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb20+Pj5dDQo+Pj4+IFNl
bnQ6IEZyaWRheSwgRmVicnVhcnkgMDYsIDIwMTUgNToxOSBQTQ0KPj4+PiBUbzogVWxhbm92LCBB
bGV4YW5kZXINCj4+Pj4gQ2M6IEpvc2VwaCBCcmFkbGV5Ow0KPj4+PiBkZXZAc3BhcmsuYXBhY2hl
Lm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+PG1haWx0bzpkZXZAc3BhcmsuYXBhY2hl
Lm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+PjxtYWlsdG86ZGV2QHNwYXJrPG1haWx0
bzpkZXZAc3Bhcms+Lg0KPj4+PiBhcGFjaGUub3JnPGh0dHA6Ly9hcGFjaGUub3JnPjxtYWlsdG86
ZGV2QHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPj4+DQo+Pj4+
DQo+Pj4+IFN1YmplY3Q6IFJlOiBVc2luZyBDVURBIHdpdGhpbiBTcGFyayAvIGJvb3N0aW5nIGxp
bmVhciBhbGdlYnJhDQo+Pj4+DQo+Pj4+IEdldHRpbmcgYnJlZXplIHRvIHBpY2sgdXAgdGhlIHJp
Z2h0IGJsYXMgbGlicmFyeSBpcyBjcml0aWNhbCBmb3INCj4+Pj4gcGVyZm9ybWFuY2UuIEkgcmVj
b21tZW5kIHVzaW5nIE9wZW5CTEFTIChvciBNS0wsIGlmIHlvdSBhbHJlYWR5IGhhdmUgaXQpLg0K
Pj4+PiBJdCBtaWdodCBtYWtlIHNlbnNlIHRvIGZvcmNlIEJJRE1hdCB0byB1c2UgdGhlIHNhbWUg
dW5kZXJseWluZyBCTEFTDQo+Pj4+IGxpYnJhcnkgYXMgd2VsbC4NCj4+Pj4NCj4+Pj4gT24gRnJp
LCBGZWIgNiwgMjAxNSBhdCA0OjQyIFBNLCBVbGFub3YsIEFsZXhhbmRlciA8DQo+Pj4+IGFsZXhh
bmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT48bWFpbHRv
OmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT4+
PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBo
cC5jb20+PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVs
YW5vdkBocC5jb20+Pj4+IHdyb3RlOg0KPj4+PiBIaSBFdmFuLCBKb3NlcGgNCj4+Pj4NCj4+Pj4g
SSBkaWQgZmV3IG1hdHJpeCBtdWx0aXBsaWNhdGlvbiB0ZXN0IGFuZCBCSURNYXQgc2VlbXMgdG8g
YmUgfjEweA0KPj4+PiBmYXN0ZXIgdGhhbiBuZXRsaWItamF2YSticmVlemUgKHNvcnJ5IGZvciB3
ZWlyZCB0YWJsZSBmb3JtYXR0aW5nKToNCj4+Pj4NCj4+Pj4gfEEqQiAgc2l6ZSB8IEJJRE1hdCBN
S0wgfCBCcmVlemUrTmV0bGliLWphdmENCj4+Pj4gfG5hdGl2ZV9zeXN0ZW1fbGludXhfeDg2LTY0
fA0KPj4+PiBCcmVlemUrTmV0bGliLWphdmEgZjJqYmxhcyB8DQo+Pj4+ICstLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LSsNCj4+Pj4gfDEwMHgxMDAqMTAweDEwMCB8IDAsMDAyMDU1OTYgfCAwLDAzODEwMzI0IHwgMCww
MDI1NTYgfA0KPj4+PiB8MTAwMHgxMDAwKjEwMDB4MTAwMCB8IDAsMDE4MzIwOTQ3IHwgMCw1MTgw
MzU1NyB8MSw2Mzg0NzU0NTkgfA0KPj4+PiB8MTAwMDB4MTAwMDAqMTAwMDB4MTAwMDAgfCAyMyw3
ODA0NjYzMiB8IDQ0NSwwOTM1MjExIHwgMTU2OSwyMzMyMjgNCj4+Pj4gfHwNCj4+Pj4NCj4+Pj4g
Q29uZmlndXJhdGlvbjogSW50ZWwoUikgWGVvbihSKSBDUFUgRTMxMjQwIDMuMyBHSHosIDZHQiBS
QU0sIEZlZG9yYQ0KPj4+PiAxOSBMaW51eCwgU2NhbGEgMi4xMS4NCj4+Pj4NCj4+Pj4gTGF0ZXIg
SSB3aWxsIG1ha2UgdGVzdHMgd2l0aCBDdWRhLiBJIG5lZWQgdG8gaW5zdGFsbCBuZXcgQ3VkYQ0K
Pj4+PiB2ZXJzaW9uIGZvciB0aGlzIHB1cnBvc2UuDQo+Pj4+DQo+Pj4+IERvIHlvdSBoYXZlIGFu
eSBpZGVhcyB3aHkgYnJlZXplLW5ldGxpYiB3aXRoIG5hdGl2ZSBibGFzIGlzIHNvIG11Y2gNCj4+
Pj4gc2xvd2VyIHRoYW4gQklETWF0IE1LTD8NCj4+Pj4NCj4+Pj4gQmVzdCByZWdhcmRzLCBBbGV4
YW5kZXINCj4+Pj4NCj4+Pj4gRnJvbTogSm9zZXBoIEJyYWRsZXkgW21haWx0bzpqb3NlcGhAZGF0
YWJyaWNrcy5jb208bWFpbHRvOmpvc2VwaEBkYXRhYnJpY2tzLmNvbT48bWFpbHRvOmpvc2VwaEBk
YXRhYnJpY2tzLmNvbTxtYWlsdG86am9zZXBoQGRhdGFicmlja3MuY29tPj48bWFpbHRvOg0KPj4+
PiBqb3NlcGhAZGF0YWJyaWNrcy5jb208bWFpbHRvOmpvc2VwaEBkYXRhYnJpY2tzLmNvbT48bWFp
bHRvOmpvc2VwaEBkYXRhYnJpY2tzLmNvbTxtYWlsdG86am9zZXBoQGRhdGFicmlja3MuY29tPj4+
XQ0KPj4+PiBTZW50OiBUaHVyc2RheSwgRmVicnVhcnkgMDUsIDIwMTUgNToyOSBQTQ0KPj4+PiBU
bzogVWxhbm92LCBBbGV4YW5kZXINCj4+Pj4gQ2M6IEV2YW4gUi4gU3BhcmtzOw0KPj4+PiBkZXZA
c3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+PG1haWx0bzpkZXZA
c3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+PjxtYWlsdG86ZGV2
QHNwYXJrPG1haWx0bzpkZXZAc3Bhcms+Lg0KPj4+PiBhcGFjaGUub3JnPGh0dHA6Ly9hcGFjaGUu
b3JnPjxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUu
b3JnPj4+DQo+Pj4+IFN1YmplY3Q6IFJlOiBVc2luZyBDVURBIHdpdGhpbiBTcGFyayAvIGJvb3N0
aW5nIGxpbmVhciBhbGdlYnJhDQo+Pj4+DQo+Pj4+IEhpIEFsZXhhbmRlciwNCj4+Pj4NCj4+Pj4g
VXNpbmcgR1BVcyB3aXRoIFNwYXJrIHdvdWxkIGJlIHZlcnkgZXhjaXRpbmcuICBTbWFsbCBjb21t
ZW50Og0KPj4+PiBDb25jZXJuaW5nIHlvdXIgcXVlc3Rpb24gZWFybGllciBhYm91dCBrZWVwaW5n
IGRhdGEgc3RvcmVkIG9uIHRoZQ0KPj4+PiBHUFUgcmF0aGVyIHRoYW4gaGF2aW5nIHRvIG1vdmUg
aXQgYmV0d2VlbiBtYWluIG1lbW9yeSBhbmQgR1BVDQo+Pj4+IG1lbW9yeSBvbiBlYWNoIGl0ZXJh
dGlvbiwgSSB3b3VsZCBndWVzcyB0aGlzIHdvdWxkIGJlIGNyaXRpY2FsIHRvDQo+Pj4+IGdldHRp
bmcgZ29vZCBwZXJmb3JtYW5jZS4gIElmIHlvdSBjb3VsZCBkbyBtdWx0aXBsZSBsb2NhbA0KPj4+
PiBpdGVyYXRpb25zIGJlZm9yZSBhZ2dyZWdhdGluZyByZXN1bHRzLCB0aGVuIHRoZSBjb3N0IG9m
IGRhdGENCj4+Pj4gbW92ZW1lbnQgdG8gdGhlIEdQVSBjb3VsZCBiZSBhbW9ydGl6ZWQgKGFuZCBJ
IGJlbGlldmUgdGhhdCBpcyBkb25lDQo+Pj4+IGluIHByYWN0aWNlKS4gIEhhdmluZyBTcGFyayBi
ZSBhd2FyZSBvZiB0aGUgR1BVIGFuZCB1c2luZyBpdCBhcyBhbm90aGVyIHBhcnQgb2YgbWVtb3J5
IHNvdW5kcyBsaWtlIGEgbXVjaCBiaWdnZXIgdW5kZXJ0YWtpbmcuDQo+Pj4+DQo+Pj4+IEpvc2Vw
aA0KPj4+Pg0KPj4+PiBPbiBUaHUsIEZlYiA1LCAyMDE1IGF0IDQ6NTkgUE0sIFVsYW5vdiwgQWxl
eGFuZGVyIDwNCj4+Pj4gYWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51
bGFub3ZAaHAuY29tPjxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhh
bmRlci51bGFub3ZAaHAuY29tPj48bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0
bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT48bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29t
PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT4+Pj4gd3JvdGU6DQo+Pj4+IFRoYW5rIHlv
dSBmb3IgZXhwbGFuYXRpb24hIEnigJl2ZSB3YXRjaGVkIHRoZSBCSURNYWNoIHByZXNlbnRhdGlv
biBieQ0KPj4+PiBKb2huIENhbm55IGFuZCBJIGFtIHJlYWxseSBpbnNwaXJlZCBieSBoaXMgdGFs
ayBhbmQgY29tcGFyaXNvbnMgd2l0aCBTcGFyayBNTGxpYi4NCj4+Pj4NCj4+Pj4gSSBhbSB2ZXJ5
IGludGVyZXN0ZWQgdG8gZmluZCBvdXQgd2hhdCB3aWxsIGJlIGJldHRlciB3aXRoaW4gU3Bhcms6
DQo+Pj4+IEJJRE1hdCBvciBuZXRsaWItamF2YSB3aXRoIENQVSBvciBHUFUgbmF0aXZlcy4gQ291
bGQgeW91IHN1Z2dlc3QgYQ0KPj4+PiBmYWlyIHdheSB0byBiZW5jaG1hcmsgdGhlbT8gQ3VycmVu
dGx5IEkgZG8gYmVuY2htYXJrcyBvbiBhcnRpZmljaWFsDQo+Pj4+IG5ldXJhbCBuZXR3b3JrcyBp
biBiYXRjaCBtb2RlLiBXaGlsZSBpdCBpcyBub3QgYSDigJxwdXJl4oCdIHRlc3Qgb2YNCj4+Pj4g
bGluZWFyIGFsZ2VicmEsIGl0IGludm9sdmVzIHNvbWUgb3RoZXIgdGhpbmdzIHRoYXQgYXJlIGVz
c2VudGlhbCB0byBtYWNoaW5lIGxlYXJuaW5nLg0KPj4+Pg0KPj4+PiBGcm9tOiBFdmFuIFIuIFNw
YXJrcyBbbWFpbHRvOmV2YW4uc3BhcmtzQGdtYWlsLmNvbTxtYWlsdG86ZXZhbi5zcGFya3NAZ21h
aWwuY29tPjxtYWlsdG86ZXZhbi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpldmFuLnNwYXJrc0Bn
bWFpbC5jb20+PjxtYWlsdG86DQo+Pj4+IGV2YW4uc3BhcmtzQGdtYWlsLmNvbTxtYWlsdG86ZXZh
bi5zcGFya3NAZ21haWwuY29tPjxtYWlsdG86ZXZhbi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpl
dmFuLnNwYXJrc0BnbWFpbC5jb20+Pj5dDQo+Pj4+IFNlbnQ6IFRodXJzZGF5LCBGZWJydWFyeSAw
NSwgMjAxNSAxOjI5IFBNDQo+Pj4+IFRvOiBVbGFub3YsIEFsZXhhbmRlcg0KPj4+PiBDYzoNCj4+
Pj4gZGV2QHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPjxtYWls
dG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPj48bWFp
bHRvOmRldkBzcGFyazxtYWlsdG86ZGV2QHNwYXJrPi4NCj4+Pj4gYXBhY2hlLm9yZzxodHRwOi8v
YXBhY2hlLm9yZz48bWFpbHRvOmRldkBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXZAc3Bhcmsu
YXBhY2hlLm9yZz4+Pg0KPj4+PiBTdWJqZWN0OiBSZTogVXNpbmcgQ1VEQSB3aXRoaW4gU3Bhcmsg
LyBib29zdGluZyBsaW5lYXIgYWxnZWJyYQ0KPj4+Pg0KPj4+PiBJJ2QgYmUgc3VycHJpc2VkIG9m
IEJJRE1hdCtPcGVuQkxBUyB3YXMgc2lnbmlmaWNhbnRseSBmYXN0ZXIgdGhhbg0KPj4+PiBuZXRs
aWItamF2YStPcGVuQkxBUywgYnV0IGlmIGl0IGlzIG11Y2ggZmFzdGVyIGl0J3MgcHJvYmFibHkg
ZHVlIHRvDQo+Pj4+IG5ldGxpYi1qYXZhK2RhdGENCj4+Pj4gbGF5b3V0IGFuZCBmZXdlciBsZXZl
bHMgb2YgaW5kaXJlY3Rpb24gLSBpdCdzIGRlZmluaXRlbHkgYQ0KPj4+PiB3b3J0aHdoaWxlIGV4
cGVyaW1lbnQgdG8gcnVuLiBUaGUgbWFpbiBzcGVlZHVwcyBJJ3ZlIHNlZW4gZnJvbQ0KPj4+PiB1
c2luZyBpdCBjb21lIGZyb20gaGlnaGx5IG9wdGltaXplZCBHUFUgY29kZSBmb3IgbGluZWFyIGFs
Z2VicmEuIEkNCj4+Pj4ga25vdyB0aGF0IGluIHRoZSBwYXN0IENhbm55IGhhcyBnb25lIGFzIGZh
ciBhcyB0byB3cml0ZSBjdXN0b20gR1BVDQo+Pj4+IGtlcm5lbHMgZm9yIHBlcmZvcm1hbmNlLWNy
aXRpY2FsIHJlZ2lvbnMgb2YgY29kZS5bMV0NCj4+Pj4NCj4+Pj4gQklETWFjaCBpcyBoaWdobHkg
b3B0aW1pemVkIGZvciBzaW5nbGUgbm9kZSBwZXJmb3JtYW5jZSBvcg0KPj4+PiBwZXJmb3JtYW5j
ZSBvbiBzbWFsbCBjbHVzdGVycy5bMl0gT25jZSBkYXRhIGRvZXNuJ3QgZml0IGVhc2lseSBpbg0K
Pj4+PiBHUFUgbWVtb3J5IChvciBjYW4gYmUgYmF0Y2hlZCBpbiB0aGF0IHdheSkgdGhlIHBlcmZv
cm1hbmNlIHRlbmRzIHRvDQo+Pj4+IGZhbGwgb2ZmLiBDYW5ueSBhcmd1ZXMgZm9yIGhhcmR3YXJl
L3NvZnR3YXJlIGNvZGVzaWduIGFuZCBhcyBzdWNoDQo+Pj4+IHByZWZlcnMgbWFjaGluZSBjb25m
aWd1cmF0aW9ucyB0aGF0IGFyZSBxdWl0ZSBkaWZmZXJlbnQgdGhhbiB3aGF0DQo+Pj4+IHdlIGZp
bmQgaW4gbW9zdCBjb21tb2RpdHkgY2x1c3RlciBub2RlcyAtIGUuZy4gMTAgZGlzayBjYWhubmVs
cyBhbmQgNCBHUFVzLg0KPj4+Pg0KPj4+PiBJbiBjb250cmFzdCwgTUxsaWIgd2FzIGRlc2lnbmVk
IGZvciBob3Jpem9udGFsIHNjYWxhYmlsaXR5IG9uDQo+Pj4+IGNvbW1vZGl0eSBjbHVzdGVycyBh
bmQgd29ya3MgYmVzdCBvbiB2ZXJ5IGJpZyBkYXRhc2V0cyAtIG9yZGVyIG9mIHRlcmFieXRlcy4N
Cj4+Pj4NCj4+Pj4gRm9yIHRoZSBtb3N0IHBhcnQsIHRoZXNlIHByb2plY3RzIGRldmVsb3BlZCBj
b25jdXJyZW50bHkgdG8gYWRkcmVzcw0KPj4+PiBzbGlnaHRseSBkaWZmZXJlbnQgdXNlIGNhc2Vz
LiBUaGF0IHNhaWQsIHRoZXJlIG1heSBiZSBiaXRzIG9mDQo+Pj4+IEJJRE1hY2ggd2UgY291bGQg
cmVwdXJwb3NlIGZvciBNTGxpYiAtIGtlZXAgaW4gbWluZCB3ZSBuZWVkIHRvIGJlDQo+Pj4+IGNh
cmVmdWwgYWJvdXQgbWFpbnRhaW5pbmcgY3Jvc3MtbGFuZ3VhZ2UgY29tcGF0aWJpbGl0eSBmb3Ig
b3VyIEphdmENCj4+Pj4gYW5kIFB5dGhvbi11c2VycywgdGhvdWdoLg0KPj4+Pg0KPj4+PiAtIEV2
YW4NCj4+Pj4NCj4+Pj4gWzFdIC0gaHR0cDovL2FyeGl2Lm9yZy9hYnMvMTQwOS41NDAyIFsyXSAt
DQo+Pj4+IGh0dHA6Ly9lZWNzLmJlcmtlbGV5LmVkdS9+aHpoYW8vcGFwZXJzL0JELnBkZg0KPj4+
Pg0KPj4+PiBPbiBUaHUsIEZlYiA1LCAyMDE1IGF0IDE6MDAgUE0sIFVsYW5vdiwgQWxleGFuZGVy
IDwNCj4+Pj4gYWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51bGFub3ZA
aHAuY29tPjxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51
bGFub3ZAaHAuY29tPj48bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4
YW5kZXIudWxhbm92QGhwLmNvbT48bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0
bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT4+PjxtYWlsdG86DQo+Pj4+IGFsZXhhbmRlci51bGFu
b3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT48bWFpbHRvOmFsZXhhbmRl
ci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT4+PG1haWx0bzph
bGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb20+PG1h
aWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5j
b20+Pj4+PiB3cm90ZToNCj4+Pj4gSGkgRXZhbiwNCj4+Pj4NCj4+Pj4gVGhhbmsgeW91IGZvciBz
dWdnZXN0aW9uISBCSURNYXQgc2VlbXMgdG8gaGF2ZSB0ZXJyaWZpYyBzcGVlZC4gRG8NCj4+Pj4g
eW91IGtub3cgd2hhdCBtYWtlcyB0aGVtIGZhc3RlciB0aGFuIG5ldGxpYi1qYXZhPw0KPj4+Pg0K
Pj4+PiBUaGUgc2FtZSBncm91cCBoYXMgQklETWFjaCBsaWJyYXJ5IHRoYXQgaW1wbGVtZW50cyBt
YWNoaW5lDQo+Pj4+IGxlYXJuaW5nLiBGb3Igc29tZSBleGFtcGxlcyB0aGV5IHVzZSBDYWZmZSBj
b252b2x1dGlvbmFsIG5ldXJhbA0KPj4+PiBuZXR3b3JrIGxpYnJhcnkgb3duZWQgYnkgYW5vdGhl
ciBncm91cCBpbiBCZXJrZWxleS4gQ291bGQgeW91DQo+Pj4+IGVsYWJvcmF0ZSBvbiBob3cgdGhl
c2UgYWxsIG1pZ2h0IGJlIGNvbm5lY3RlZCB3aXRoIFNwYXJrIE1sbGliPyBJZg0KPj4+PiB5b3Ug
dGFrZSBCSURNYXQgZm9yIGxpbmVhciBhbGdlYnJhIHdoeSBkb27igJl0IHlvdSB0YWtlIEJJRE1h
Y2ggZm9yIG9wdGltaXphdGlvbiBhbmQgbGVhcm5pbmc/DQo+Pj4+DQo+Pj4+IEJlc3QgcmVnYXJk
cywgQWxleGFuZGVyDQo+Pj4+DQo+Pj4+IEZyb206IEV2YW4gUi4gU3BhcmtzIFttYWlsdG86ZXZh
bi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb20+PG1haWx0bzpl
dmFuLnNwYXJrc0BnbWFpbC5jb208bWFpbHRvOmV2YW4uc3BhcmtzQGdtYWlsLmNvbT4+PG1haWx0
bzoNCj4+Pj4gZXZhbi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5j
b20+PG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb208bWFpbHRvOmV2YW4uc3BhcmtzQGdtYWls
LmNvbT4+PjxtYWlsdG86ZXZhbi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpldmFuLnNwYXJrc0Bn
bWFpbC5jb20+PG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb208bWFpbHRvOmV2YW4uc3Bhcmtz
QGdtYWlsLmNvbT4+PG1haWx0bzoNCj4+Pj4gZXZhbi5zcGFya3NAZ21haWwuY29tPG1haWx0bzpl
dmFuLnNwYXJrc0BnbWFpbC5jb20+PG1haWx0bzpldmFuLnNwYXJrc0BnbWFpbC5jb208bWFpbHRv
OmV2YW4uc3BhcmtzQGdtYWlsLmNvbT4+Pj5dDQo+Pj4+IFNlbnQ6IFRodXJzZGF5LCBGZWJydWFy
eSAwNSwgMjAxNSAxMjowOSBQTQ0KPj4+PiBUbzogVWxhbm92LCBBbGV4YW5kZXINCj4+Pj4gQ2M6
IGRldkBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZz48bWFpbHRv
OmRldkBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZz4+PG1haWx0
bzpkZXZAc3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+PG1haWx0
bzpkZXZAc3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc+Pj48bWFp
bHRvOg0KPj4+PiBkZXZAc3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5v
cmc+PG1haWx0bzpkZXZAc3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5v
cmc+PjxtYWlsdG86ZGV2QHNwYXJrPG1haWx0bzpkZXZAc3Bhcms+Lg0KPj4+PiBhcGFjaGUub3Jn
PGh0dHA6Ly9hcGFjaGUub3JnPjxtYWlsdG86ZGV2QHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRl
dkBzcGFyay5hcGFjaGUub3JnPj4+Pg0KPj4+PiBTdWJqZWN0OiBSZTogVXNpbmcgQ1VEQSB3aXRo
aW4gU3BhcmsgLyBib29zdGluZyBsaW5lYXIgYWxnZWJyYQ0KPj4+Pg0KPj4+PiBJJ2QgZXhwZWN0
IHRoYXQgd2UgY2FuIG1ha2UgR1BVLWFjY2VsZXJhdGVkIEJMQVMgZmFzdGVyIHRoYW4gQ1BVDQo+
Pj4+IGJsYXMgaW4gbWFueSBjYXNlcy4NCj4+Pj4NCj4+Pj4gWW91IG1pZ2h0IGNvbnNpZGVyIHRh
a2luZyBhIGxvb2sgYXQgdGhlIGNvZGVwYXRocyB0aGF0IEJJRE1hdCAoDQo+Pj4+IGh0dHBzOi8v
Z2l0aHViLmNvbS9CSUREYXRhL0JJRE1hdCkgdGFrZXMgYW5kIGNvbXBhcmluZyB0aGVtIHRvDQo+
Pj4+IG5ldGxpYi1qYXZhL2JyZWV6ZS4gSm9obiBDYW5ueSBldC4gYWwuIGhhdmUgZG9uZSBhIGJ1
bmNoIG9mIHdvcmsNCj4+Pj4gb3B0aW1pemluZyB0byBtYWtlIHRoaXMgd29yayByZWFsbHkgZmFz
dCBmcm9tIFNjYWxhLiBJJ3ZlIHJ1biBpdCBvbg0KPj4+PiBteSBsYXB0b3AgYW5kIGNvbXBhcmVk
IHRvIE1LTCBhbmQgaW4gY2VydGFpbiBjYXNlcyBpdCdzIDEweCBmYXN0ZXIgYXQgbWF0cml4IG11
bHRpcGx5Lg0KPj4+PiBUaGVyZSBhcmUgYSBsb3Qgb2YgbGF5ZXJzIG9mIGluZGlyZWN0aW9uIGhl
cmUgYW5kIHlvdSByZWFsbHkgd2FudA0KPj4+PiB0byBhdm9pZCBkYXRhIGNvcHlpbmcgYXMgbXVj
aCBhcyBwb3NzaWJsZS4NCj4+Pj4NCj4+Pj4gV2UgY291bGQgYWxzbyBjb25zaWRlciBzd2FwcGlu
ZyBvdXQgQklETWF0IGZvciBCcmVlemUsIGJ1dCB0aGF0DQo+Pj4+IHdvdWxkIGJlIGEgYmlnIHBy
b2plY3QgYW5kIGlmIHdlIGNhbiBmaWd1cmUgb3V0IGhvdyB0byBnZXQNCj4+Pj4gYnJlZXplK2N1
YmxhcyB0byBjb21wYXJhYmxlIHBlcmZvcm1hbmNlIHRoYXQgd291bGQgYmUgYSBiaWcgd2luLg0K
Pj4+Pg0KPj4+PiBPbiBUaHUsIEZlYiA1LCAyMDE1IGF0IDExOjU1IEFNLCBVbGFub3YsIEFsZXhh
bmRlciA8DQo+Pj4+IGFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxh
bm92QGhwLmNvbT48bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5k
ZXIudWxhbm92QGhwLmNvbT4+PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86
YWxleGFuZGVyLnVsYW5vdkBocC5jb20+PG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbTxt
YWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb20+Pj48bWFpbHRvOg0KPj4+PiBhbGV4YW5kZXIu
dWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb20+PG1haWx0bzphbGV4
YW5kZXIudWxhbm92QGhwLmNvbTxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb20+PjxtYWls
dG86YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51bGFub3ZAaHAuY29t
PjxtYWlsdG86YWxleGFuZGVyLnVsYW5vdkBocC5jb208bWFpbHRvOmFsZXhhbmRlci51bGFub3ZA
aHAuY29tPj4+Pj4gd3JvdGU6DQo+Pj4+IERlYXIgU3BhcmsgZGV2ZWxvcGVycywNCj4+Pj4NCj4+
Pj4gSSBhbSBleHBsb3JpbmcgaG93IHRvIG1ha2UgbGluZWFyIGFsZ2VicmEgb3BlcmF0aW9ucyBm
YXN0ZXIgd2l0aGluIFNwYXJrLg0KPj4+PiBPbmUgd2F5IG9mIGRvaW5nIHRoaXMgaXMgdG8gdXNl
IFNjYWxhIEJyZWV6ZSBsaWJyYXJ5IHRoYXQgaXMNCj4+Pj4gYnVuZGxlZCB3aXRoIFNwYXJrLiBG
b3IgbWF0cml4IG9wZXJhdGlvbnMsIGl0IGVtcGxveXMgTmV0bGliLWphdmENCj4+Pj4gdGhhdCBo
YXMgYSBKYXZhIHdyYXBwZXIgZm9yIEJMQVMgKGJhc2ljIGxpbmVhciBhbGdlYnJhIHN1YnByb2dy
YW1zKQ0KPj4+PiBhbmQgTEFQQUNLIG5hdGl2ZSBiaW5hcmllcyBpZiB0aGV5IGFyZSBhdmFpbGFi
bGUgb24gdGhlIHdvcmtlcg0KPj4+PiBub2RlLiBJdCBhbHNvIGhhcyBpdHMgb3duIG9wdGltaXpl
ZCBKYXZhIGltcGxlbWVudGF0aW9uIG9mIEJMQVMuIEl0DQo+Pj4+IGlzIHdvcnRoIG1lbnRpb25p
bmcsIHRoYXQgbmF0aXZlIGJpbmFyaWVzIHByb3ZpZGUgYmV0dGVyIHBlcmZvcm1hbmNlIG9ubHkg
Zm9yIEJMQVMgbGV2ZWwgMywgaS5lLg0KPj4+PiBtYXRyaXgtbWF0cml4IG9wZXJhdGlvbnMgb3Ig
Z2VuZXJhbCBtYXRyaXggbXVsdGlwbGljYXRpb24gKEdFTU0pLg0KPj4+PiBUaGlzIGlzIGNvbmZp
cm1lZCBieSBHRU1NIHRlc3Qgb24gTmV0bGliLWphdmEgcGFnZQ0KPj4+PiBodHRwczovL2dpdGh1
Yi5jb20vZm9tbWlsL25ldGxpYi1qYXZhLiBJIGFsc28gY29uZmlybWVkIGl0IHdpdGggbXkNCj4+
Pj4gZXhwZXJpbWVudHMgd2l0aCB0cmFpbmluZyBvZiBhcnRpZmljaWFsIG5ldXJhbCBuZXR3b3Jr
DQo+Pj4+IGh0dHBzOi8vZ2l0aHViLmNvbS9hcGFjaGUvc3BhcmsvcHVsbC8xMjkwI2lzc3VlY29t
bWVudC03MDMxMzk1Mi4NCj4+Pj4gSG93ZXZlciwgSSB3b3VsZCBsaWtlIHRvIGJvb3N0IHBlcmZv
cm1hbmNlIG1vcmUuDQo+Pj4+DQo+Pj4+IEdQVSBpcyBzdXBwb3NlZCB0byB3b3JrIGZhc3Qgd2l0
aCBsaW5lYXIgYWxnZWJyYSBhbmQgdGhlcmUgaXMNCj4+Pj4gTnZpZGlhIENVREEgaW1wbGVtZW50
YXRpb24gb2YgQkxBUywgY2FsbGVkIGN1Ymxhcy4gSSBoYXZlIG9uZSBMaW51eA0KPj4+PiBzZXJ2
ZXIgd2l0aCBOdmlkaWEgR1BVIGFuZCBJIHdhcyBhYmxlIHRvIGRvIHRoZSBmb2xsb3dpbmcuIEkg
bGlua2VkDQo+Pj4+IGN1YmxhcyAoaW5zdGVhZCBvZiBjcHUtYmFzZWQgYmxhcykgd2l0aCBOZXRs
aWItamF2YSB3cmFwcGVyIGFuZCBwdXQNCj4+Pj4gaXQgaW50byBTcGFyaywgc28gQnJlZXplL05l
dGxpYiBpcyB1c2luZyBpdC4gVGhlbiBJIGRpZCBzb21lDQo+Pj4+IHBlcmZvcm1hbmNlIG1lYXN1
cmVtZW50cyB3aXRoIHJlZ2FyZHMgdG8gYXJ0aWZpY2lhbCBuZXVyYWwgbmV0d29yaw0KPj4+PiBi
YXRjaCBsZWFybmluZyBpbiBTcGFyayBNTGxpYiB0aGF0IGludm9sdmVzIG1hdHJpeC1tYXRyaXgN
Cj4+Pj4gbXVsdGlwbGljYXRpb25zLiBJdCB0dXJucyBvdXQgdGhhdCBmb3IgbWF0cmljZXMgb2Yg
c2l6ZSBsZXNzIHRoYW4NCj4+Pj4gfjEwMDB4NzgwIEdQVSBjdWJsYXMgaGFzIHRoZSBzYW1lIHNw
ZWVkIGFzIENQVSBibGFzLiBDdWJsYXMgYmVjb21lcw0KPj4+PiBzbG93ZXIgZm9yIGJpZ2dlciBt
YXRyaWNlcy4gSXQgd29ydGggbWVudGlvbmluZyB0aGF0IGl0IGlzIHdhcyBub3QgYSB0ZXN0IGZv
ciBPTkxZIG11bHRpcGxpY2F0aW9uIHNpbmNlIHRoZXJlIGFyZSBvdGhlciBvcGVyYXRpb25zIGlu
dm9sdmVkLg0KPj4+PiBPbmUgb2YgdGhlIHJlYXNvbnMgZm9yIHNsb3dkb3duIG1pZ2h0IGJlIHRo
ZSBvdmVyaGVhZCBvZiBjb3B5aW5nDQo+Pj4+IHRoZSBtYXRyaWNlcyBmcm9tIGNvbXB1dGVyIG1l
bW9yeSB0byBncmFwaGljIGNhcmQgbWVtb3J5IGFuZCBiYWNrLg0KPj4+Pg0KPj4+PiBTbywgZmV3
IHF1ZXN0aW9uczoNCj4+Pj4gMSkgRG8gdGhlc2UgcmVzdWx0cyB3aXRoIENVREEgbWFrZSBzZW5z
ZT8NCj4+Pj4gMikgSWYgdGhlIHByb2JsZW0gaXMgd2l0aCBjb3B5IG92ZXJoZWFkLCBhcmUgdGhl
cmUgYW55IGxpYnJhcmllcw0KPj4+PiB0aGF0IGFsbG93IHRvIGZvcmNlIGludGVybWVkaWF0ZSBy
ZXN1bHRzIHRvIHN0YXkgaW4gZ3JhcGhpYyBjYXJkDQo+Pj4+IG1lbW9yeSB0aHVzIHJlbW92aW5n
IHRoZSBvdmVyaGVhZD8NCj4+Pj4gMykgQW55IG90aGVyIG9wdGlvbnMgdG8gc3BlZWQtdXAgbGlu
ZWFyIGFsZ2VicmEgaW4gU3Bhcms/DQo+Pj4+DQo+Pj4+IFRoYW5rIHlvdSwgQWxleGFuZGVyDQo+
Pj4+DQo+Pj4+IC0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0tLS0NCj4+Pj4gLS0gVG8gdW5zdWJzY3JpYmUsIGUtbWFpbDogZGV2
LXVuc3Vic2NyaWJlQHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldi11bnN1YnNjcmliZUBzcGFy
ay5hcGFjaGUub3JnPjxtYWlsdG86ZGV2LXVuc3Vic2NyaWJlQHNwYXJrLmFwYWNoZS5vcmc8bWFp
bHRvOmRldi11bnN1YnNjcmliZUBzcGFyay5hcGFjaGUub3JnPj48bWFpbHRvOg0KPj4+PiBkZXYt
dW5zdWJzY3JpYmVAc3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2LXVuc3Vic2NyaWJlQHNwYXJr
LmFwYWNoZS5vcmc+PG1haWx0bzpkZXYtdW5zdWJzY3JpYmVAc3BhcmsuYXBhY2g8bWFpbHRvOmRl
di11bnN1YnNjcmliZUBzcGFyay5hcGFjaD4NCj4+Pj4gZS5vcmc8aHR0cDovL2Uub3JnPj4+PG1h
aWx0bzpkZXYtdW5zdWJzY3JpYmVAc3BhcmsuYXBhYzxtYWlsdG86ZGV2LXVuc3Vic2NyaWJlQHNw
YXJrLmFwYWM+PG1haWx0bzpkZXYtdW5zdWJzY3JpYmVAc3A8bWFpbHRvOmRldi11bnN1YnNjcmli
ZUBzcD4NCj4+Pj4gYXJrLmFwYWM+IGhlLm9yZzxodHRwOi8vaGUub3JnPjxodHRwOi8vaGUub3Jn
Pg0KPj4+PiA8bWFpbHRvOmRldi11bnN1YnNjcmliZUBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpk
ZXYtdW5zdWJzY3JpYmVAc3BhcmsuYXBhY2hlLm9yZz48bWFpbHRvOmRldi11bnN1YnNjcmliZUBz
cGE8bWFpbHRvOmRldi11bnN1YnNjcmliZUBzcGE+DQo+Pj4+IHJrLmFwYWNoZS5vcmc8aHR0cDov
L3JrLmFwYWNoZS5vcmc+Pj4+IEZvciBhZGRpdGlvbmFsIGNvbW1hbmRzLCBlLW1haWw6DQo+Pj4+
IGRldi1oZWxwQHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldi1oZWxwQHNwYXJrLmFwYWNoZS5v
cmc+PG1haWx0bzpkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnPG1haWx0bzpkZXYtaGVscEBzcGFy
ay5hcGFjaGUub3JnPj48bWFpbHRvOg0KPj4+PiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnPG1h
aWx0bzpkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnPjxtYWlsdG86ZGV2LWhlbHBAc3BhcmsuYXBh
Y2hlLm9yZzxtYWlsdG86ZGV2LWhlbHBAc3BhcmsuYXBhY2hlLm9yZz4+PjxtYWlsdG86ZGV2LWhl
bHBAc3BhcmsuYXBhY2hlLm9yZzxtYWlsdG86ZGV2LWhlbHBAc3BhcmsuYXBhY2hlLm9yZz48bWFp
bHRvOmRldi1oZWxwQHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRldi1oZWxwQHNwYXJrLmFwYWNo
ZS5vcmc+PjxtYWlsdG86DQo+Pj4+IGRldi1oZWxwQHNwYXJrLmFwYWNoZS5vcmc8bWFpbHRvOmRl
di1oZWxwQHNwYXJrLmFwYWNoZS5vcmc+PG1haWx0bzpkZXYtaGVscEBzcGFyay5hcGFjaGUub3Jn
PG1haWx0bzpkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnPj4+Pg0KPj4+Pg0KPj4+Pg0KPj4+Pg0K
Pj4+Pg0KPj4+DQoNCi0tDQpCZXN0IHJlZ2FyZHMsDQpTYW0NCg0K

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68CG4W3292americas_--

From dev-return-12189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:08:20 2015
Return-Path: <dev-return-12189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC68617BB4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:08:20 +0000 (UTC)
Received: (qmail 69598 invoked by uid 500); 25 Mar 2015 22:08:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69506 invoked by uid 500); 25 Mar 2015 22:08:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69494 invoked by uid 99); 25 Mar 2015 22:08:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:08:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sam.halliday@gmail.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:08:15 +0000
Received: by ieclw3 with SMTP id lw3so33333114iec.2
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 15:07:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=kxoZjErACbyJA55D1kllhIv2ODKvB0g3teiM05uMKhM=;
        b=F/vfNnx7LITLi8Kcl9jHI7il/GqDjk28PmGiHwryHIPMmfvzNBOHoebbqhNeVPqIUP
         q7Cc+WUljffZbKClgJ4uiqf2k4Jcux06Lhj9y23T0Ctt21RcAfu4es0OZf69jJESh+EU
         fQbpD3PY5roL5L+BFhPXuCcJK9+tWm0WRhHWRmSFIMGBDHcQ0VTttEOop29bZb9FDrYD
         puK6A9Y4FX1TLfOAb1vbctccjFOHMdS5AB8QyfN0zX3FY1EQ3l/qeX6TDM7FaPfjPKba
         +T68zMK2PTu8ijcYphyskPc3IxohMncSmhZqY+AWPDhFRH4qGyHEApEEM3i+wHdnFHXs
         KDNQ==
MIME-Version: 1.0
X-Received: by 10.42.100.211 with SMTP id b19mr20695204ico.5.1427321274635;
 Wed, 25 Mar 2015 15:07:54 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Wed, 25 Mar 2015 15:07:54 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Wed, 25 Mar 2015 15:07:54 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68C@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<CAPud8ToyQWm0LnFWd+dD37L=+fiDo0ZEpXugigUUAsEO9U6Dew@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68C@G4W3292.americas.hpqcorp.net>
Date: Wed, 25 Mar 2015 22:07:54 +0000
Message-ID: <CALR_T9Aj52gtK-e81b8Opxg2bZj9ih9EqwKmvno=2PuwFY_HKg@mail.gmail.com>
Subject: RE: Using CUDA within Spark / boosting linear algebra
From: Sam Halliday <sam.halliday@gmail.com>
To: Alexander Ulanov <alexander.ulanov@hp.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org, 
	Joseph Bradley <joseph@databricks.com>, Dmitriy Lyubimov <dlieu.7@gmail.com>, 
	"Evan R. Sparks" <evan.sparks@gmail.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=90e6ba613f8a8e332b051224211e
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba613f8a8e332b051224211e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yeah, MultiBLAS... it is dynamic.

Except, I haven't written it yet :-P
On 25 Mar 2015 22:06, "Ulanov, Alexander" <alexander.ulanov@hp.com> wrote:

>  Netlib knows nothing about GPU (or CPU), it just uses cblas symbols from
> the provided libblas.so.3 library at the runtime. So, you can switch at t=
he
> runtime by providing another library. Sam, please suggest if there is
> another way.
>
>
>
> *From:* Dmitriy Lyubimov [mailto:dlieu.7@gmail.com]
> *Sent:* Wednesday, March 25, 2015 2:55 PM
> *To:* Ulanov, Alexander
> *Cc:* Sam Halliday; dev@spark.apache.org; Xiangrui Meng; Joseph Bradley;
> Evan R. Sparks; jfcanny
> *Subject:* Re: Using CUDA within Spark / boosting linear algebra
>
>
>
> Alexander,
>
>
>
> does using netlib imply that one cannot switch between CPU and GPU blas
> alternatives at will at the same time? the choice is always determined by
> linking aliternatives to libblas.so, right?
>
>
>
> On Wed, Mar 25, 2015 at 2:31 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
>
> Hi again,
>
> I finally managed to use nvblas within Spark+netlib-java. It has
> exceptional performance for big matrices with Double, faster than
> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
> to/from GPU, OpenBlas or MKL might be a better choice. This correlates wi=
th
> original nvblas presentation on GPU conf 2013 (slide 21):
> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-=
New-Features-CUDA%206%20-GPU-Acceleration.pdf
>
> My results:
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Just in case, these tests are not for generalization of performance of
> different libraries. I just want to pick a library that does at best dens=
e
> matrices multiplication for my task.
>
> P.S. My previous issue with nvblas was the following: it has Fortran blas
> functions, at the same time netlib-java uses C cblas functions. So, one
> needs cblas shared library to use nvblas through netlib-java. Fedora does
> not have cblas (but Debian and Ubuntu have), so I needed to compile it. I
> could not use cblas from Atlas or Openblas because they link to their
> implementation and not to Fortran blas.
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Ulanov, Alexander
>
> Sent: Tuesday, March 24, 2015 6:57 PM
> To: Sam Halliday
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
> Hi,
>
> I am trying to use nvblas with netlib-java from Spark. nvblas functions
> should replace current blas functions calls after executing LD_PRELOAD as
> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
> changes to netlib-java. It seems to work for simple Java example, but I
> cannot make it work with Spark. I run the following:
> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>
> +------------------------------------------------------------------------=
-----+
> | Processes:                                                       GPU
> Memory |
> |  GPU       PID  Type  Process name                               Usage
>     |
>
> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D|
> |    0      8873    C   bash
> 39MiB |
> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
> 39MiB |
>
> +------------------------------------------------------------------------=
-----+
>
> In Spark shell I do matrix multiplication and see the following:
> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
> So I am sure that netlib-native is loaded and cblas supposedly used.
> However, matrix multiplication does executes on CPU since I see 16% of CP=
U
> used and 0% of GPU used. I also checked different matrix sizes, from
> 100x100 to 12000x12000
>
> Could you suggest might the LD_PRELOAD not affect Spark shell?
>
> Best regards, Alexander
>
>
>
> From: Sam Halliday [mailto:sam.halliday@gmail.com]
> Sent: Monday, March 09, 2015 6:01 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
> Thanks so much for following up on this!
>
> Hmm, I wonder if we should have a concerted effort to chart performance o=
n
> various pieces of hardware...
> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto:
> alexander.ulanov@hp.com>> wrote:
> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
> support of Double in the current source code), did the test with BIDMat a=
nd
> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
> sam.halliday@gmail.com>]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
> dev@spark.apache.org>
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
> <mailto:joseph@databricks.com>> wrote:
> >> Better documentation for linking would be very helpful!  Here's a JIRA=
:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for larger
> matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks
> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
> to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked up b=
y
> netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I understand
> how to force use a specific blas (not specific wrapper for blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already have
> it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
> joseph@databricks.com><mailto:
> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons with
> Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential to
> machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and =
4
> GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x faster a=
t
> matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster within
> Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better performance
> only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was not a
> test for ONLY multiplication since there are other operations involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
> dev-unsubscribe@spark.apache.org><mailto:
> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
> >>>> ark.apac> he.org<http://he.org>
> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
> >>>> rk.apache.org>>> For additional commands, e-mail:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --
> Best regards,
> Sam
>
>
>

--90e6ba613f8a8e332b051224211e--

From dev-return-12190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:10:23 2015
Return-Path: <dev-return-12190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BE66C17BE8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:10:23 +0000 (UTC)
Received: (qmail 82919 invoked by uid 500); 25 Mar 2015 22:09:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82840 invoked by uid 500); 25 Mar 2015 22:09:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82829 invoked by uid 99); 25 Mar 2015 22:09:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:09:23 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of canny@berkeley.edu does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:09:18 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id B5D8A18AA39B
	for <dev@spark.apache.org>; Wed, 25 Mar 2015 15:09:13 -0700 (PDT)
Date: Wed, 25 Mar 2015 15:08:56 -0700 (MST)
From: jfcanny <canny@berkeley.edu>
To: dev@spark.apache.org
Message-ID: <551331DF.2040704@berkeley.edu>
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net> <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com> <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com> <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com> <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net> <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
Subject: Re: Using CUDA within Spark / boosting linear algebra
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_218278_346633934.1427321336448"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_218278_346633934.1427321336448
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Alex,
I think you should recheck your numbers. Both BIDMat and nvblas are=20
wrappers for cublas. The speeds are identical, except on machines that=20
have multiple GPUs which nvblas exploits and cublas doesnt.

It would be a good idea to add a column with Gflop throughput. Your=20
numbers for BIDMat 10kx10k multiply give about 300 single float gflops,=20
which seems about right for a Quadro 4000 (current generation devices=20
are > 10x faster than a 4000).

Your numbers for netlib-nvblas would indicate a double float throughput=20
of 8 tflops, which is physically impossible on that device.

It shouldnt matter which interface you use if you have a single GPU.

-John

On 3/25/2015 2:34 PM, Ulanov, Alexander [via Apache Spark Developers=20
List] wrote:
> Hi again,
>
> I finally managed to use nvblas within Spark+netlib-java. It has=20
> exceptional performance for big matrices with Double, faster than=20
> BIDMat-cuda with Float. But for smaller matrices, if you will copy=20
> them to/from GPU, OpenBlas or MKL might be a better choice. This=20
> correlates with original nvblas presentation on GPU conf 2013 (slide=20
> 21):=20
> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-=
New-Features-CUDA%206%20-GPU-Acceleration.pdf
>
> My results:
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing=20
>
>
> Just in case, these tests are not for generalization of performance of=20
> different libraries. I just want to pick a library that does at best=20
> dense matrices multiplication for my task.
>
> P.S. My previous issue with nvblas was the following: it has Fortran=20
> blas functions, at the same time netlib-java uses C cblas functions.=20
> So, one needs cblas shared library to use nvblas through netlib-java.=20
> Fedora does not have cblas (but Debian and Ubuntu have), so I needed=20
> to compile it. I could not use cblas from Atlas or Openblas because=20
> they link to their implementation and not to Fortran blas.
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Tuesday, March 24, 2015 6:57 PM
> To: Sam Halliday
> Cc: [hidden email]; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
> Hi,
>
> I am trying to use nvblas with netlib-java from Spark. nvblas=20
> functions should replace current blas functions calls after executing=20
> LD_PRELOAD as suggested in=20
> http://docs.nvidia.com/cuda/nvblas/#Usage without any changes to=20
> netlib-java. It seems to work for simple Java example, but I cannot=20
> make it work with Spark. I run the following:
> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell=20
> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
> +------------------------------------------------------------------------=
-----+=20
>
> | Processes: GPU Memory |
> |  GPU       PID  Type  Process name Usage      |
> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D|=20
>
> |    0      8873    C   bash      39MiB |
> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java      39MiB |
> +------------------------------------------------------------------------=
-----+=20
>
>
> In Spark shell I do matrix multiplication and see the following:
> 15/03/25 06:48:01 INFO JniLoader: successfully loaded=20
> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
> So I am sure that netlib-native is loaded and cblas supposedly used.=20
> However, matrix multiplication does executes on CPU since I see 16% of=20
> CPU used and 0% of GPU used. I also checked different matrix sizes,=20
> from 100x100 to 12000x12000
>
> Could you suggest might the LD_PRELOAD not affect Spark shell?
>
> Best regards, Alexander
>
>
>
> From: Sam Halliday [mailto:[hidden email]]
> Sent: Monday, March 09, 2015 6:01 PM
> To: Ulanov, Alexander
> Cc: [hidden email]; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
> Thanks so much for following up on this!
>
> Hmm, I wonder if we should have a concerted effort to chart=20
> performance on various pieces of hardware...
> On 9 Mar 2015 21:08, "Ulanov, Alexander" <[hidden=20
> email]<mailto:[hidden email]>> wrote:
> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added=20
> the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I=20
> see the support of Double in the current source code), did the test=20
> with BIDMat and CPU Double matrices. BIDMat MKL is indeed on par with=20
> netlib MKL.
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:[hidden email]<mailto:[hidden email]>]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; [hidden email]<mailto:[hidden=20
> email]>
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <[hidden email]<mailto:[hidden email]>> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <[hidden=20
> email]<mailto:[hidden email]>> wrote:
> >> Better documentation for linking would be very helpful!  Here's a=20
> JIRA:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <[hidden email]<mailto:[hidden email]>>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for=20
> larger matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> [hidden email]<mailto:[hidden email]>> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley;
> >>>> [hidden email]<mailto:[hidden email]>
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>=20
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks
> >>>> [mailto:[hidden email]<mailto:[hidden email]>]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> [hidden email]<mailto:[hidden email]>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are=20
> supposed to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:[hidden email]<mailto:[hidden=20
> email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> [hidden email]<mailto:[hidden email]><mailto:dev@spark.
> >>>> apache.org<mailto:[hidden email]>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked=20
> up by netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I=20
> understand how to force use a specific blas (not specific wrapper for=20
> blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:[hidden email]<mailto:[hidden=20
> email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> [hidden email]<mailto:[hidden email]><mailto:dev@spark.
> >>>> apache.org<mailto:[hidden email]>>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already=20
> have it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>=20
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:[hidden email]<mailto:[hidden=20
> email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> [hidden email]<mailto:[hidden email]><mailto:dev@spark.
> >>>> apache.org<mailto:[hidden email]>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as=20
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons=20
> with Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential=20
> to machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:[hidden email]<mailto:[hidden=20
> email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc:
> >>>> [hidden email]<mailto:[hidden email]><mailto:dev@spark.
> >>>> apache.org<mailto:[hidden email]>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels=20
> and 4 GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of=20
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf=20
> <http://eecs.berkeley.edu/%7Ehzhao/papers/BD.pdf>
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>><mailto:
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for=20
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:[hidden email]<mailto:[hidden=20
> email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>><mailto:[hidden=20
> email]<mailto:[hidden email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>><mailto:
> >>>> [hidden email]<mailto:[hidden email]><mailto:dev@spark.
> >>>> apache.org<mailto:[hidden email]>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x=20
> faster at matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>><mailto:
> >>>> [hidden email]<mailto:[hidden email]><mailto:[hidden=20
> email]<mailto:[hidden email]>>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster=20
> within Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better=20
> performance only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was=20
> not a test for ONLY multiplication since there are other operations=20
> involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: [hidden email]<mailto:[hidden=20
> email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]
> >>>> e.org>><mailto:[hidden email]<mailto:dev-unsubscribe@sp
> >>>> ark.apac> he.org<http://he.org>
> >>>> <mailto:[hidden email]<mailto:dev-unsubscribe@spa
> >>>> rk.apache.org>>> For additional commands, e-mail:
> >>>> [hidden email]<mailto:[hidden email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>><mailto:[hidden=20
> email]<mailto:[hidden email]><mailto:
> >>>> [hidden email]<mailto:[hidden email]>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --=20
> Best regards,
> Sam
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: [hidden email]
> For additional commands, e-mail: [hidden email]
>
>
> ------------------------------------------------------------------------
> If you reply to this email, your message will be added to the=20
> discussion below:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Using-CUDA-with=
in-Spark-boosting-linear-algebra-tp10481p11238.html=20
>
> To unsubscribe from Using CUDA within Spark / boosting linear algebra,=20
> click here=20
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dunsubscribe_by_code&node=3D10481&code=3DY2FubnlAYmVya2Vs=
ZXkuZWR1fDEwNDgxfC00MzIwNjcxNzY=3D>.
> NAML=20
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&bas=
e=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNa=
mespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.Nabb=
leNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_sub=
scribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_in=
stant_email%21nabble%3Aemail.naml>=20
>





--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/Using-CUDA-within-Spark-boosting-linear-algebra-tp10481p11246.=
html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.
------=_Part_218278_346633934.1427321336448--

From dev-return-12191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:17:30 2015
Return-Path: <dev-return-12191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92A5617C85
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:17:30 +0000 (UTC)
Received: (qmail 3362 invoked by uid 500); 25 Mar 2015 22:17:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3283 invoked by uid 500); 25 Mar 2015 22:17:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3270 invoked by uid 99); 25 Mar 2015 22:17:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:17:16 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dlieu.7@gmail.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:17:11 +0000
Received: by obcxo2 with SMTP id xo2so31606903obc.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 15:16:51 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=FIxmXihnr7N+liHr2pLiGZS/zkmDpaHCDHzoGTCcy34=;
        b=n8t13F+iFcpJe7FFHlN8NVvgeSAx8NibyZHvReKK/Cv1FYZ1du1+A1PBS2nZz/BgCJ
         JIDXlt39+tqJwUQp1lA8etITE5WbrVP/6raUnn0DUXJM9BkM8eDv/vyll4WE0bo4CBVJ
         xbNklrBreH7zJBHFb6qnhUJJALJZf+bERQw9s5Tosq4bEAwntyQrOhB15FU2kwmc6XGA
         ZOYVKC0YOBmFSNmY11q8wu2cP5TP7F8p6klj41/nXpVadD5btXvr1iTrYZi5VLP3lwko
         ePgRBhkJGGZ8ugdFqlr3xilaUF6IPwEx0RpjGN7khYhl9ACo4y0UiOhcFDKZmBbdIZNi
         Qc5A==
MIME-Version: 1.0
X-Received: by 10.60.133.176 with SMTP id pd16mr9469390oeb.78.1427321810974;
 Wed, 25 Mar 2015 15:16:50 -0700 (PDT)
Received: by 10.76.188.163 with HTTP; Wed, 25 Mar 2015 15:16:50 -0700 (PDT)
In-Reply-To: <CALR_T9Aj52gtK-e81b8Opxg2bZj9ih9EqwKmvno=2PuwFY_HKg@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<CAPud8ToyQWm0LnFWd+dD37L=+fiDo0ZEpXugigUUAsEO9U6Dew@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68C@G4W3292.americas.hpqcorp.net>
	<CALR_T9Aj52gtK-e81b8Opxg2bZj9ih9EqwKmvno=2PuwFY_HKg@mail.gmail.com>
Date: Wed, 25 Mar 2015 15:16:50 -0700
Message-ID: <CAPud8TpurJWXaVtmwVmYz7Ypp4=AreSe9SW5FQ2Gt5S8va-L4w@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: Sam Halliday <sam.halliday@gmail.com>
Cc: Alexander Ulanov <alexander.ulanov@hp.com>, Xiangrui Meng <mengxr@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, Joseph Bradley <joseph@databricks.com>, 
	"Evan R. Sparks" <evan.sparks@gmail.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=047d7b47287685ff1d0512244166
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b47287685ff1d0512244166
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Sam,

whould it be easier to hack netlib-java to allow multiple (configurable)
 library contexts? And so enable 3rd party configurations and optimizers to
make their own choices until then?

On Wed, Mar 25, 2015 at 3:07 PM, Sam Halliday <sam.halliday@gmail.com>
wrote:

> Yeah, MultiBLAS... it is dynamic.
>
> Except, I haven't written it yet :-P
> On 25 Mar 2015 22:06, "Ulanov, Alexander" <alexander.ulanov@hp.com> wrote=
:
>
>>  Netlib knows nothing about GPU (or CPU), it just uses cblas symbols
>> from the provided libblas.so.3 library at the runtime. So, you can switc=
h
>> at the runtime by providing another library. Sam, please suggest if ther=
e
>> is another way.
>>
>>
>>
>> *From:* Dmitriy Lyubimov [mailto:dlieu.7@gmail.com]
>> *Sent:* Wednesday, March 25, 2015 2:55 PM
>> *To:* Ulanov, Alexander
>> *Cc:* Sam Halliday; dev@spark.apache.org; Xiangrui Meng; Joseph Bradley;
>> Evan R. Sparks; jfcanny
>> *Subject:* Re: Using CUDA within Spark / boosting linear algebra
>>
>>
>>
>> Alexander,
>>
>>
>>
>> does using netlib imply that one cannot switch between CPU and GPU blas
>> alternatives at will at the same time? the choice is always determined b=
y
>> linking aliternatives to libblas.so, right?
>>
>>
>>
>> On Wed, Mar 25, 2015 at 2:31 PM, Ulanov, Alexander <
>> alexander.ulanov@hp.com> wrote:
>>
>> Hi again,
>>
>> I finally managed to use nvblas within Spark+netlib-java. It has
>> exceptional performance for big matrices with Double, faster than
>> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
>> to/from GPU, OpenBlas or MKL might be a better choice. This correlates w=
ith
>> original nvblas presentation on GPU conf 2013 (slide 21):
>> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108=
-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>>
>> My results:
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Just in case, these tests are not for generalization of performance of
>> different libraries. I just want to pick a library that does at best den=
se
>> matrices multiplication for my task.
>>
>> P.S. My previous issue with nvblas was the following: it has Fortran bla=
s
>> functions, at the same time netlib-java uses C cblas functions. So, one
>> needs cblas shared library to use nvblas through netlib-java. Fedora doe=
s
>> not have cblas (but Debian and Ubuntu have), so I needed to compile it. =
I
>> could not use cblas from Atlas or Openblas because they link to their
>> implementation and not to Fortran blas.
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Ulanov, Alexander
>>
>> Sent: Tuesday, March 24, 2015 6:57 PM
>> To: Sam Halliday
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>> Hi,
>>
>> I am trying to use nvblas with netlib-java from Spark. nvblas functions
>> should replace current blas functions calls after executing LD_PRELOAD a=
s
>> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
>> changes to netlib-java. It seems to work for simple Java example, but I
>> cannot make it work with Spark. I run the following:
>> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
>> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
>> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>>
>> +-----------------------------------------------------------------------=
------+
>> | Processes:                                                       GPU
>> Memory |
>> |  GPU       PID  Type  Process name                               Usage
>>     |
>>
>> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D|
>> |    0      8873    C   bash
>> 39MiB |
>> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
>> 39MiB |
>>
>> +-----------------------------------------------------------------------=
------+
>>
>> In Spark shell I do matrix multiplication and see the following:
>> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
>> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>> So I am sure that netlib-native is loaded and cblas supposedly used.
>> However, matrix multiplication does executes on CPU since I see 16% of C=
PU
>> used and 0% of GPU used. I also checked different matrix sizes, from
>> 100x100 to 12000x12000
>>
>> Could you suggest might the LD_PRELOAD not affect Spark shell?
>>
>> Best regards, Alexander
>>
>>
>>
>> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>> Sent: Monday, March 09, 2015 6:01 PM
>> To: Ulanov, Alexander
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>>
>> Thanks so much for following up on this!
>>
>> Hmm, I wonder if we should have a concerted effort to chart performance
>> on various pieces of hardware...
>> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto=
:
>> alexander.ulanov@hp.com>> wrote:
>> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
>> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
>> support of Double in the current source code), did the test with BIDMat =
and
>> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>>
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
>> sam.halliday@gmail.com>]
>> Sent: Tuesday, March 03, 2015 1:54 PM
>> To: Xiangrui Meng; Joseph Bradley
>> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
>> dev@spark.apache.org>
>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>
>> BTW, is anybody on this list going to the London Meetup in a few weeks?
>>
>>
>> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapre=
duce-world#community
>>
>> Would be nice to meet other people working on the guts of Spark! :-)
>>
>>
>> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>>
>> > Hey Alexander,
>> >
>> > I don't quite understand the part where netlib-cublas is about 20x
>> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
>> > with netlib-java?
>> >
>> > CC'ed Sam, the author of netlib-java.
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
>> <mailto:joseph@databricks.com>> wrote:
>> >> Better documentation for linking would be very helpful!  Here's a JIR=
A:
>> >> https://issues.apache.org/jira/browse/SPARK-6019
>> >>
>> >>
>> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>> >> wrote:
>> >>
>> >>> Thanks for compiling all the data and running these benchmarks,
>> >>> Alex. The big takeaways here can be seen with this chart:
>> >>>
>> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>> >>>
>> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>> >>> BIDMat+GPU) can provide substantial (but less than an order of
>> >>> BIDMat+magnitude)
>> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>> >>> netlib-java+openblas-compiled).
>> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>> >>> worse than a well-tuned CPU implementation, particularly for larger
>> matrices.
>> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>> >>> basically agrees with the authors own benchmarks (
>> >>> https://github.com/fommil/netlib-java)
>> >>>
>> >>> I think that most of our users are in a situation where using GPUs
>> >>> may not be practical - although we could consider having a good GPU
>> >>> backend available as an option. However, *ALL* users of MLlib could
>> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
>> >>> BLAS implementation. Perhaps we should consider updating the mllib
>> >>> guide with a more complete section for enabling high performance
>> >>> binaries on OSX and Linux? Or better, figure out a way for the
>> >>> system to fetch these automatically.
>> >>>
>> >>> - Evan
>> >>>
>> >>>
>> >>>
>> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>> >>>
>> >>>> Just to summarize this thread, I was finally able to make all
>> >>>> performance comparisons that we discussed. It turns out that:
>> >>>> BIDMat-cublas>>BIDMat
>> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-y=
um-repo=3D
>> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
>> >>>>
>> >>>> Below is the link to the spreadsheet with full results.
>> >>>>
>> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
>> >>>>
>> >>>> One thing still needs exploration: does BIDMat-cublas perform
>> >>>> copying to/from machine=E2=80=99s RAM?
>> >>>>
>> >>>> -----Original Message-----
>> >>>> From: Ulanov, Alexander
>> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>> >>>> To: Evan R. Sparks
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>> >>>> the original one discusses slightly different topic. I was able to
>> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>> >>>> statically linked inside a 60MB library.
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>> >>>> |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>> >>>> 1569,233228 |
>> >>>>
>> >>>> It turn out that pre-compiled MKL is faster than precompiled
>> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns=
 with
>> >>>> locally compiled openblas and cuda.
>> >>>>
>> >>>> Alexander
>> >>>>
>> >>>> From: Evan R. Sparks
>> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>> >>>> Sent: Monday, February 09, 2015 6:06 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Great - perhaps we can move this discussion off-list and onto a
>> >>>> JIRA ticket? (Here's one:
>> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>> >>>>
>> >>>> It seems like this is going to be somewhat exploratory for a while
>> >>>> (and there's probably only a handful of us who really care about
>> >>>> fast linear
>> >>>> algebra!)
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for explanation and useful link. I am going to build
>> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>> >>>>
>> >>>> Do I understand correctly that BIDMat binaries contain statically
>> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
>> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
>> >>>> it seems that in my case precompiled MKL BLAS performs better than
>> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
>> to be on par with JNI overheads.
>> >>>>
>> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
>> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>> >>>> Halliday
>> >>>> (Netlib-java) interested to compare their libraries.
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:58 PM
>> >>>>
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
>> >>>> from getting cache sizes, etc. set up correctly for your particular
>> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>> >>>> quickly and yields performance competitive with MKL.
>> >>>>
>> >>>> To make sure the right library is getting used, you have to make
>> >>>> sure it's first on the search path - export
>> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
>> >>>>
>> >>>> For some examples of getting netlib-java setup on an ec2 node and
>> >>>> some example benchmarking code we ran a while back, see:
>> >>>> https://github.com/shivaram/matrix-bench
>> >>>>
>> >>>> In particular - build-openblas-ec2.sh shows you how to build the
>> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>> >>>> shows you how to get the path setup and get that library picked up
>> by netlib-java.
>> >>>>
>> >>>> In this way - you could probably get cuBLAS set up to be used by
>> >>>> netlib-java as well.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
>> >>>> force loading the right blas? For netlib, I there are few JVM
>> >>>> flags, such as
>> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
>> >>>> so I can force it to use Java implementation. Not sure I understand
>> how to force use a specific blas (not specific wrapper for blas).
>> >>>>
>> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
>> >>>> that netlib is using it.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:19 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Getting breeze to pick up the right blas library is critical for
>> >>>> performance. I recommend using OpenBLAS (or MKL, if you already hav=
e
>> it).
>> >>>> It might make sense to force BIDMat to use the same underlying BLAS
>> >>>> library as well.
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan, Joseph
>> >>>>
>> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>> >>>> |native_system_linux_x86-64|
>> >>>> Breeze+Netlib-java f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>> >>>> ||
>> >>>>
>> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
>> >>>> 19 Linux, Scala 2.11.
>> >>>>
>> >>>> Later I will make tests with Cuda. I need to install new Cuda
>> >>>> version for this purpose.
>> >>>>
>> >>>> Do you have any ideas why breeze-netlib with native blas is so much
>> >>>> slower than BIDMat MKL?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>> joseph@databricks.com><mailto:
>> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>> >>>> Sent: Thursday, February 05, 2015 5:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Evan R. Sparks;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Hi Alexander,
>> >>>>
>> >>>> Using GPUs with Spark would be very exciting.  Small comment:
>> >>>> Concerning your question earlier about keeping data stored on the
>> >>>> GPU rather than having to move it between main memory and GPU
>> >>>> memory on each iteration, I would guess this would be critical to
>> >>>> getting good performance.  If you could do multiple local
>> >>>> iterations before aggregating results, then the cost of data
>> >>>> movement to the GPU could be amortized (and I believe that is done
>> >>>> in practice).  Having Spark be aware of the GPU and using it as
>> another part of memory sounds like a much bigger undertaking.
>> >>>>
>> >>>> Joseph
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach present=
ation by
>> >>>> John Canny and I am really inspired by his talk and comparisons wit=
h
>> Spark MLlib.
>> >>>>
>> >>>> I am very interested to find out what will be better within Spark:
>> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
>> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=
=80=9D test of
>> >>>> linear algebra, it involves some other things that are essential to
>> machine learning.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Thursday, February 05, 2015 1:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
>> >>>> netlib-java+data
>> >>>> layout and fewer levels of indirection - it's definitely a
>> >>>> worthwhile experiment to run. The main speedups I've seen from
>> >>>> using it come from highly optimized GPU code for linear algebra. I
>> >>>> know that in the past Canny has gone as far as to write custom GPU
>> >>>> kernels for performance-critical regions of code.[1]
>> >>>>
>> >>>> BIDMach is highly optimized for single node performance or
>> >>>> performance on small clusters.[2] Once data doesn't fit easily in
>> >>>> GPU memory (or can be batched in that way) the performance tends to
>> >>>> fall off. Canny argues for hardware/software codesign and as such
>> >>>> prefers machine configurations that are quite different than what
>> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and
>> 4 GPUs.
>> >>>>
>> >>>> In contrast, MLlib was designed for horizontal scalability on
>> >>>> commodity clusters and works best on very big datasets - order of
>> terabytes.
>> >>>>
>> >>>> For the most part, these projects developed concurrently to address
>> >>>> slightly different use cases. That said, there may be bits of
>> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>> >>>> careful about maintaining cross-language compatibility for our Java
>> >>>> and Python-users, though.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>> >>>> you know what makes them faster than netlib-java?
>> >>>>
>> >>>> The same group has BIDMach library that implements machine
>> >>>> learning. For some examples they use Caffe convolutional neural
>> >>>> network library owned by another group in Berkeley. Could you
>> >>>> elaborate on how these all might be connected with Spark Mllib? If
>> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMa=
ch for
>> optimization and learning?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>> >>>> Sent: Thursday, February 05, 2015 12:09 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>> >>>> blas in many cases.
>> >>>>
>> >>>> You might consider taking a look at the codepaths that BIDMat (
>> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>> >>>> optimizing to make this work really fast from Scala. I've run it on
>> >>>> my laptop and compared to MKL and in certain cases it's 10x faster
>> at matrix multiply.
>> >>>> There are a lot of layers of indirection here and you really want
>> >>>> to avoid data copying as much as possible.
>> >>>>
>> >>>> We could also consider swapping out BIDMat for Breeze, but that
>> >>>> would be a big project and if we can figure out how to get
>> >>>> breeze+cublas to comparable performance that would be a big win.
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Dear Spark developers,
>> >>>>
>> >>>> I am exploring how to make linear algebra operations faster within
>> Spark.
>> >>>> One way of doing this is to use Scala Breeze library that is
>> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
>> >>>> and LAPACK native binaries if they are available on the worker
>> >>>> node. It also has its own optimized Java implementation of BLAS. It
>> >>>> is worth mentioning, that native binaries provide better performanc=
e
>> only for BLAS level 3, i.e.
>> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>> >>>> This is confirmed by GEMM test on Netlib-java page
>> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>> >>>> experiments with training of artificial neural network
>> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>> >>>> However, I would like to boost performance more.
>> >>>>
>> >>>> GPU is supposed to work fast with linear algebra and there is
>> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
>> >>>> server with Nvidia GPU and I was able to do the following. I linked
>> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
>> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>> >>>> performance measurements with regards to artificial neural network
>> >>>> batch learning in Spark MLlib that involves matrix-matrix
>> >>>> multiplications. It turns out that for matrices of size less than
>> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
>> >>>> slower for bigger matrices. It worth mentioning that it is was not =
a
>> test for ONLY multiplication since there are other operations involved.
>> >>>> One of the reasons for slowdown might be the overhead of copying
>> >>>> the matrices from computer memory to graphic card memory and back.
>> >>>>
>> >>>> So, few questions:
>> >>>> 1) Do these results with CUDA make sense?
>> >>>> 2) If the problem is with copy overhead, are there any libraries
>> >>>> that allow to force intermediate results to stay in graphic card
>> >>>> memory thus removing the overhead?
>> >>>> 3) Any other options to speed-up linear algebra in Spark?
>> >>>>
>> >>>> Thank you, Alexander
>> >>>>
>> >>>> -------------------------------------------------------------------
>> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
>> dev-unsubscribe@spark.apache.org><mailto:
>> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
>> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
>> >>>> ark.apac> he.org<http://he.org>
>> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
>> >>>> rk.apache.org>>> For additional commands, e-mail:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto=
:
>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>
>>
>> --
>> Best regards,
>> Sam
>>
>>
>>
>

--047d7b47287685ff1d0512244166--

From dev-return-12192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:22:49 2015
Return-Path: <dev-return-12192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6DC0E17D03
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:22:49 +0000 (UTC)
Received: (qmail 16916 invoked by uid 500); 25 Mar 2015 22:22:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16846 invoked by uid 500); 25 Mar 2015 22:22:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16834 invoked by uid 99); 25 Mar 2015 22:22:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:22:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sam.halliday@gmail.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:22:18 +0000
Received: by iecvj10 with SMTP id vj10so33685601iec.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 15:20:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=6QfoOvqEvyi7JA6m1dUPGjVyEFe6k+7ovzDSdd85ygY=;
        b=0RFDLHMBHe/baj8Hb0nsc21Hf4Lpo20Wqjaei5GnIFhg32RCo6f6emet8UsCy5UZd0
         kcKpndeNRBXa3TpZOOl7mTYYjBrbzRv1yCtqiYy72oUyOFWL0kPyPtzlCcZQqTTDCFnz
         8z0AyO6p6wdF9Y8Lbt8maxzFHrmLYonhhSUUMopihJ6craVbHx/4ptvE9rsuv4UYUWXW
         JkdoKKpOpkAprpkHjam84OcNiCw5Y5LCxh2TLzEIgRHzPh2cJvPWFC1Z9Pgg6x7rTw+X
         +m2BoDYmPe+N+DDoMYOeZWZAFdg0S13oMUu47QSDgBm3wwMRQoniixgcH9rJXl8MjRvA
         wP+Q==
MIME-Version: 1.0
X-Received: by 10.50.253.11 with SMTP id zw11mr32629236igc.18.1427322046057;
 Wed, 25 Mar 2015 15:20:46 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Wed, 25 Mar 2015 15:20:45 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Wed, 25 Mar 2015 15:20:45 -0700 (PDT)
In-Reply-To: <CAPud8TpurJWXaVtmwVmYz7Ypp4=AreSe9SW5FQ2Gt5S8va-L4w@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<CAPud8ToyQWm0LnFWd+dD37L=+fiDo0ZEpXugigUUAsEO9U6Dew@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A68C@G4W3292.americas.hpqcorp.net>
	<CALR_T9Aj52gtK-e81b8Opxg2bZj9ih9EqwKmvno=2PuwFY_HKg@mail.gmail.com>
	<CAPud8TpurJWXaVtmwVmYz7Ypp4=AreSe9SW5FQ2Gt5S8va-L4w@mail.gmail.com>
Date: Wed, 25 Mar 2015 22:20:45 +0000
Message-ID: <CALR_T9ANoxE3Fd2WwSexS4rETg4W43PpL5iV9OuYtZ6jpwio6g@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Sam Halliday <sam.halliday@gmail.com>
To: Dmitriy Lyubimov <dlieu.7@gmail.com>
Cc: dev@spark.apache.org, Xiangrui Meng <mengxr@gmail.com>, 
	Joseph Bradley <joseph@databricks.com>, Alexander Ulanov <alexander.ulanov@hp.com>, 
	"Evan R. Sparks" <evan.sparks@gmail.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=001a11347a3a8936b60512244fca
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11347a3a8936b60512244fca
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That would be a difficult task that would only benefit users of
netlib-java. MultiBLAS is easily implemented (although a lot of
boilerplate) and benefits all BLAS users on the system.

If anyone knows of a funding route for it, I'd love to hear from them,
because it's too much work for me to take on at the moment as hobby.
On 25 Mar 2015 22:16, "Dmitriy Lyubimov" <dlieu.7@gmail.com> wrote:

> Sam,
>
> whould it be easier to hack netlib-java to allow multiple (configurable)
>  library contexts? And so enable 3rd party configurations and optimizers =
to
> make their own choices until then?
>
> On Wed, Mar 25, 2015 at 3:07 PM, Sam Halliday <sam.halliday@gmail.com>
> wrote:
>
>> Yeah, MultiBLAS... it is dynamic.
>>
>> Except, I haven't written it yet :-P
>> On 25 Mar 2015 22:06, "Ulanov, Alexander" <alexander.ulanov@hp.com>
>> wrote:
>>
>>>  Netlib knows nothing about GPU (or CPU), it just uses cblas symbols
>>> from the provided libblas.so.3 library at the runtime. So, you can swit=
ch
>>> at the runtime by providing another library. Sam, please suggest if the=
re
>>> is another way.
>>>
>>>
>>>
>>> *From:* Dmitriy Lyubimov [mailto:dlieu.7@gmail.com]
>>> *Sent:* Wednesday, March 25, 2015 2:55 PM
>>> *To:* Ulanov, Alexander
>>> *Cc:* Sam Halliday; dev@spark.apache.org; Xiangrui Meng; Joseph
>>> Bradley; Evan R. Sparks; jfcanny
>>> *Subject:* Re: Using CUDA within Spark / boosting linear algebra
>>>
>>>
>>>
>>> Alexander,
>>>
>>>
>>>
>>> does using netlib imply that one cannot switch between CPU and GPU blas
>>> alternatives at will at the same time? the choice is always determined =
by
>>> linking aliternatives to libblas.so, right?
>>>
>>>
>>>
>>> On Wed, Mar 25, 2015 at 2:31 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com> wrote:
>>>
>>> Hi again,
>>>
>>> I finally managed to use nvblas within Spark+netlib-java. It has
>>> exceptional performance for big matrices with Double, faster than
>>> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
>>> to/from GPU, OpenBlas or MKL might be a better choice. This correlates =
with
>>> original nvblas presentation on GPU conf 2013 (slide 21):
>>> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC310=
8-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>>>
>>> My results:
>>>
>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T=
9J5r7kwKSPkY/edit?usp=3Dsharing
>>>
>>> Just in case, these tests are not for generalization of performance of
>>> different libraries. I just want to pick a library that does at best de=
nse
>>> matrices multiplication for my task.
>>>
>>> P.S. My previous issue with nvblas was the following: it has Fortran
>>> blas functions, at the same time netlib-java uses C cblas functions. So=
,
>>> one needs cblas shared library to use nvblas through netlib-java. Fedor=
a
>>> does not have cblas (but Debian and Ubuntu have), so I needed to compil=
e
>>> it. I could not use cblas from Atlas or Openblas because they link to t=
heir
>>> implementation and not to Fortran blas.
>>>
>>> Best regards, Alexander
>>>
>>> -----Original Message-----
>>> From: Ulanov, Alexander
>>>
>>> Sent: Tuesday, March 24, 2015 6:57 PM
>>> To: Sam Halliday
>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>
>>> Hi,
>>>
>>> I am trying to use nvblas with netlib-java from Spark. nvblas functions
>>> should replace current blas functions calls after executing LD_PRELOAD =
as
>>> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
>>> changes to netlib-java. It seems to work for simple Java example, but I
>>> cannot make it work with Spark. I run the following:
>>> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
>>> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
>>> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>>>
>>> +----------------------------------------------------------------------=
-------+
>>> | Processes:                                                       GPU
>>> Memory |
>>> |  GPU       PID  Type  Process name
>>>  Usage      |
>>>
>>> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D|
>>> |    0      8873    C   bash
>>> 39MiB |
>>> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
>>> 39MiB |
>>>
>>> +----------------------------------------------------------------------=
-------+
>>>
>>> In Spark shell I do matrix multiplication and see the following:
>>> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
>>> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>>> So I am sure that netlib-native is loaded and cblas supposedly used.
>>> However, matrix multiplication does executes on CPU since I see 16% of =
CPU
>>> used and 0% of GPU used. I also checked different matrix sizes, from
>>> 100x100 to 12000x12000
>>>
>>> Could you suggest might the LD_PRELOAD not affect Spark shell?
>>>
>>> Best regards, Alexander
>>>
>>>
>>>
>>> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>>> Sent: Monday, March 09, 2015 6:01 PM
>>> To: Ulanov, Alexander
>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>
>>>
>>> Thanks so much for following up on this!
>>>
>>> Hmm, I wonder if we should have a concerted effort to chart performance
>>> on various pieces of hardware...
>>> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com
>>> <mailto:alexander.ulanov@hp.com>> wrote:
>>> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added th=
e
>>> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see th=
e
>>> support of Double in the current source code), did the test with BIDMat=
 and
>>> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>>>
>>>
>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T=
9J5r7kwKSPkY/edit?usp=3Dsharing
>>>
>>> Best regards, Alexander
>>>
>>> -----Original Message-----
>>> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
>>> sam.halliday@gmail.com>]
>>> Sent: Tuesday, March 03, 2015 1:54 PM
>>> To: Xiangrui Meng; Joseph Bradley
>>> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
>>> dev@spark.apache.org>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> BTW, is anybody on this list going to the London Meetup in a few weeks?
>>>
>>>
>>> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapr=
educe-world#community
>>>
>>> Would be nice to meet other people working on the guts of Spark! :-)
>>>
>>>
>>> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>>>
>>> > Hey Alexander,
>>> >
>>> > I don't quite understand the part where netlib-cublas is about 20x
>>> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
>>> > with netlib-java?
>>> >
>>> > CC'ed Sam, the author of netlib-java.
>>> >
>>> > Best,
>>> > Xiangrui
>>> >
>>> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.co=
m
>>> <mailto:joseph@databricks.com>> wrote:
>>> >> Better documentation for linking would be very helpful!  Here's a
>>> JIRA:
>>> >> https://issues.apache.org/jira/browse/SPARK-6019
>>> >>
>>> >>
>>> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>>> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>>> >> wrote:
>>> >>
>>> >>> Thanks for compiling all the data and running these benchmarks,
>>> >>> Alex. The big takeaways here can be seen with this chart:
>>> >>>
>>> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50u=
Z
>>> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>>> >>>
>>> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>>> >>> BIDMat+GPU) can provide substantial (but less than an order of
>>> >>> BIDMat+magnitude)
>>> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>>> >>> netlib-java+openblas-compiled).
>>> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>>> >>> worse than a well-tuned CPU implementation, particularly for larger
>>> matrices.
>>> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>>> >>> basically agrees with the authors own benchmarks (
>>> >>> https://github.com/fommil/netlib-java)
>>> >>>
>>> >>> I think that most of our users are in a situation where using GPUs
>>> >>> may not be practical - although we could consider having a good GPU
>>> >>> backend available as an option. However, *ALL* users of MLlib could
>>> >>> benefit (potentially tremendously) from using a well-tuned CPU-base=
d
>>> >>> BLAS implementation. Perhaps we should consider updating the mllib
>>> >>> guide with a more complete section for enabling high performance
>>> >>> binaries on OSX and Linux? Or better, figure out a way for the
>>> >>> system to fetch these automatically.
>>> >>>
>>> >>> - Evan
>>> >>>
>>> >>>
>>> >>>
>>> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>>> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>> >>>
>>> >>>> Just to summarize this thread, I was finally able to make all
>>> >>>> performance comparisons that we discussed. It turns out that:
>>> >>>> BIDMat-cublas>>BIDMat
>>> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-=
yum-repo=3D
>>> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
>>> >>>>
>>> >>>> Below is the link to the spreadsheet with full results.
>>> >>>>
>>> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUM=
x
>>> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
>>> >>>>
>>> >>>> One thing still needs exploration: does BIDMat-cublas perform
>>> >>>> copying to/from machine=E2=80=99s RAM?
>>> >>>>
>>> >>>> -----Original Message-----
>>> >>>> From: Ulanov, Alexander
>>> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>> >>>> To: Evan R. Sparks
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>>> >>>> the original one discusses slightly different topic. I was able to
>>> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>>> >>>> statically linked inside a 60MB library.
>>> >>>>
>>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>> >>>>
>>> +----------------------------------------------------------------------=
-+
>>> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>> >>>> |1,638475459 |
>>> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 =
|
>>> >>>> 1569,233228 |
>>> >>>>
>>> >>>> It turn out that pre-compiled MKL is faster than precompiled
>>> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more column=
s with
>>> >>>> locally compiled openblas and cuda.
>>> >>>>
>>> >>>> Alexander
>>> >>>>
>>> >>>> From: Evan R. Sparks
>>> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>>> >>>> Sent: Monday, February 09, 2015 6:06 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Great - perhaps we can move this discussion off-list and onto a
>>> >>>> JIRA ticket? (Here's one:
>>> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>>> >>>>
>>> >>>> It seems like this is going to be somewhat exploratory for a while
>>> >>>> (and there's probably only a handful of us who really care about
>>> >>>> fast linear
>>> >>>> algebra!)
>>> >>>>
>>> >>>> - Evan
>>> >>>>
>>> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Hi Evan,
>>> >>>>
>>> >>>> Thank you for explanation and useful link. I am going to build
>>> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>>> >>>>
>>> >>>> Do I understand correctly that BIDMat binaries contain statically
>>> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>>> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, =
I
>>> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
>>> >>>> it seems that in my case precompiled MKL BLAS performs better than
>>> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are suppose=
d
>>> to be on par with JNI overheads.
>>> >>>>
>>> >>>> Though, it might be interesting to link Netlib-java with Intel MKL=
,
>>> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>>> >>>> Halliday
>>> >>>> (Netlib-java) interested to compare their libraries.
>>> >>>>
>>> >>>> Best regards, Alexander
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> >>>> Sent: Friday, February 06, 2015 5:58 PM
>>> >>>>
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
>>> >>>> from getting cache sizes, etc. set up correctly for your particula=
r
>>> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>>> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>>> >>>> quickly and yields performance competitive with MKL.
>>> >>>>
>>> >>>> To make sure the right library is getting used, you have to make
>>> >>>> sure it's first on the search path - export
>>> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
>>> >>>>
>>> >>>> For some examples of getting netlib-java setup on an ec2 node and
>>> >>>> some example benchmarking code we ran a while back, see:
>>> >>>> https://github.com/shivaram/matrix-bench
>>> >>>>
>>> >>>> In particular - build-openblas-ec2.sh shows you how to build the
>>> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>>> >>>> shows you how to get the path setup and get that library picked up
>>> by netlib-java.
>>> >>>>
>>> >>>> In this way - you could probably get cuBLAS set up to be used by
>>> >>>> netlib-java as well.
>>> >>>>
>>> >>>> - Evan
>>> >>>>
>>> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java t=
o
>>> >>>> force loading the right blas? For netlib, I there are few JVM
>>> >>>> flags, such as
>>> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS=
,
>>> >>>> so I can force it to use Java implementation. Not sure I understan=
d
>>> how to force use a specific blas (not specific wrapper for blas).
>>> >>>>
>>> >>>> Btw. I have installed openblas (yum install openblas), so I suppos=
e
>>> >>>> that netlib is using it.
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> >>>> Sent: Friday, February 06, 2015 5:19 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Getting breeze to pick up the right blas library is critical for
>>> >>>> performance. I recommend using OpenBLAS (or MKL, if you already
>>> have it).
>>> >>>> It might make sense to force BIDMat to use the same underlying BLA=
S
>>> >>>> library as well.
>>> >>>>
>>> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Hi Evan, Joseph
>>> >>>>
>>> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>>> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
>>> >>>>
>>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>>> >>>> |native_system_linux_x86-64|
>>> >>>> Breeze+Netlib-java f2jblas |
>>> >>>>
>>> +----------------------------------------------------------------------=
-+
>>> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>>> >>>> ||
>>> >>>>
>>> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedor=
a
>>> >>>> 19 Linux, Scala 2.11.
>>> >>>>
>>> >>>> Later I will make tests with Cuda. I need to install new Cuda
>>> >>>> version for this purpose.
>>> >>>>
>>> >>>> Do you have any ideas why breeze-netlib with native blas is so muc=
h
>>> >>>> slower than BIDMat MKL?
>>> >>>>
>>> >>>> Best regards, Alexander
>>> >>>>
>>> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>>> joseph@databricks.com><mailto:
>>> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>>> >>>> Sent: Thursday, February 05, 2015 5:29 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Evan R. Sparks;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Hi Alexander,
>>> >>>>
>>> >>>> Using GPUs with Spark would be very exciting.  Small comment:
>>> >>>> Concerning your question earlier about keeping data stored on the
>>> >>>> GPU rather than having to move it between main memory and GPU
>>> >>>> memory on each iteration, I would guess this would be critical to
>>> >>>> getting good performance.  If you could do multiple local
>>> >>>> iterations before aggregating results, then the cost of data
>>> >>>> movement to the GPU could be amortized (and I believe that is done
>>> >>>> in practice).  Having Spark be aware of the GPU and using it as
>>> another part of memory sounds like a much bigger undertaking.
>>> >>>>
>>> >>>> Joseph
>>> >>>>
>>> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presen=
tation by
>>> >>>> John Canny and I am really inspired by his talk and comparisons
>>> with Spark MLlib.
>>> >>>>
>>> >>>> I am very interested to find out what will be better within Spark:
>>> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>>> >>>> fair way to benchmark them? Currently I do benchmarks on artificia=
l
>>> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=
=80=9D test of
>>> >>>> linear algebra, it involves some other things that are essential t=
o
>>> machine learning.
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> >>>> Sent: Thursday, February 05, 2015 1:29 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc:
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due t=
o
>>> >>>> netlib-java+data
>>> >>>> layout and fewer levels of indirection - it's definitely a
>>> >>>> worthwhile experiment to run. The main speedups I've seen from
>>> >>>> using it come from highly optimized GPU code for linear algebra. I
>>> >>>> know that in the past Canny has gone as far as to write custom GPU
>>> >>>> kernels for performance-critical regions of code.[1]
>>> >>>>
>>> >>>> BIDMach is highly optimized for single node performance or
>>> >>>> performance on small clusters.[2] Once data doesn't fit easily in
>>> >>>> GPU memory (or can be batched in that way) the performance tends t=
o
>>> >>>> fall off. Canny argues for hardware/software codesign and as such
>>> >>>> prefers machine configurations that are quite different than what
>>> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels an=
d
>>> 4 GPUs.
>>> >>>>
>>> >>>> In contrast, MLlib was designed for horizontal scalability on
>>> >>>> commodity clusters and works best on very big datasets - order of
>>> terabytes.
>>> >>>>
>>> >>>> For the most part, these projects developed concurrently to addres=
s
>>> >>>> slightly different use cases. That said, there may be bits of
>>> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>>> >>>> careful about maintaining cross-language compatibility for our Jav=
a
>>> >>>> and Python-users, though.
>>> >>>>
>>> >>>> - Evan
>>> >>>>
>>> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>>> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>> >>>>
>>> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> >>>> Hi Evan,
>>> >>>>
>>> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>>> >>>> you know what makes them faster than netlib-java?
>>> >>>>
>>> >>>> The same group has BIDMach library that implements machine
>>> >>>> learning. For some examples they use Caffe convolutional neural
>>> >>>> network library owned by another group in Berkeley. Could you
>>> >>>> elaborate on how these all might be connected with Spark Mllib? If
>>> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDM=
ach for
>>> optimization and learning?
>>> >>>>
>>> >>>> Best regards, Alexander
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>> >>>> Sent: Thursday, February 05, 2015 12:09 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>>> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>>> >>>> blas in many cases.
>>> >>>>
>>> >>>> You might consider taking a look at the codepaths that BIDMat (
>>> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>>> >>>> optimizing to make this work really fast from Scala. I've run it o=
n
>>> >>>> my laptop and compared to MKL and in certain cases it's 10x faster
>>> at matrix multiply.
>>> >>>> There are a lot of layers of indirection here and you really want
>>> >>>> to avoid data copying as much as possible.
>>> >>>>
>>> >>>> We could also consider swapping out BIDMat for Breeze, but that
>>> >>>> would be a big project and if we can figure out how to get
>>> >>>> breeze+cublas to comparable performance that would be a big win.
>>> >>>>
>>> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> >>>> Dear Spark developers,
>>> >>>>
>>> >>>> I am exploring how to make linear algebra operations faster within
>>> Spark.
>>> >>>> One way of doing this is to use Scala Breeze library that is
>>> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>>> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms=
)
>>> >>>> and LAPACK native binaries if they are available on the worker
>>> >>>> node. It also has its own optimized Java implementation of BLAS. I=
t
>>> >>>> is worth mentioning, that native binaries provide better
>>> performance only for BLAS level 3, i.e.
>>> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>>> >>>> This is confirmed by GEMM test on Netlib-java page
>>> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>>> >>>> experiments with training of artificial neural network
>>> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>> >>>> However, I would like to boost performance more.
>>> >>>>
>>> >>>> GPU is supposed to work fast with linear algebra and there is
>>> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linu=
x
>>> >>>> server with Nvidia GPU and I was able to do the following. I linke=
d
>>> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and pu=
t
>>> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>>> >>>> performance measurements with regards to artificial neural network
>>> >>>> batch learning in Spark MLlib that involves matrix-matrix
>>> >>>> multiplications. It turns out that for matrices of size less than
>>> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas become=
s
>>> >>>> slower for bigger matrices. It worth mentioning that it is was not
>>> a test for ONLY multiplication since there are other operations involve=
d.
>>> >>>> One of the reasons for slowdown might be the overhead of copying
>>> >>>> the matrices from computer memory to graphic card memory and back.
>>> >>>>
>>> >>>> So, few questions:
>>> >>>> 1) Do these results with CUDA make sense?
>>> >>>> 2) If the problem is with copy overhead, are there any libraries
>>> >>>> that allow to force intermediate results to stay in graphic card
>>> >>>> memory thus removing the overhead?
>>> >>>> 3) Any other options to speed-up linear algebra in Spark?
>>> >>>>
>>> >>>> Thank you, Alexander
>>> >>>>
>>> >>>> ------------------------------------------------------------------=
-
>>> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto=
:
>>> dev-unsubscribe@spark.apache.org><mailto:
>>> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apac=
h
>>> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@s=
p
>>> >>>> ark.apac> he.org<http://he.org>
>>> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@sp=
a
>>> >>>> rk.apache.org>>> For additional commands, e-mail:
>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto=
:
>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org
>>> >><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org
>>> ><mailto:
>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>
>>>
>>> --
>>> Best regards,
>>> Sam
>>>
>>>
>>>
>>
>

--001a11347a3a8936b60512244fca--

From dev-return-12193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:29:09 2015
Return-Path: <dev-return-12193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90BA017E83
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:29:09 +0000 (UTC)
Received: (qmail 43571 invoked by uid 500); 25 Mar 2015 22:29:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42495 invoked by uid 500); 25 Mar 2015 22:29:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42064 invoked by uid 99); 25 Mar 2015 22:29:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:29:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.223.176] (HELO mail-ie0-f176.google.com) (209.85.223.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:29:03 +0000
Received: by iecvj10 with SMTP id vj10so33799212iec.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 15:28:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=pbyE++YfkYd58/Y8a3q8QBrESCGeK3F1tZ7nnLNCknE=;
        b=Ppwr1OvKW+20AJIFCOBBojrQ+Tp0PstAOx43uzBz+2ecQAr4zdAmYL56Pl9ukwPkU1
         tR2vZmqHIt4VnU8HMznqywq409RbrZYhOoii+kNto3R6QHHZQ7eANKOLSyVAzPWS4gEU
         oWlTjb5EfTEyFCoRJKa0Z2r2LFxIQtB75Knq4R3xlYMz5w9pz23X5rWBW5JWCgisZmI+
         jrCjfhrJuknAqihe7vZhH63amU7eTEoBKMBx47Ae9q/dpdCrs1IfT7OoEWRxHOmaqCHu
         vxdU6IMqh8rqyGe3+EhwxdJzo6LFFU2eVPpBidHKUy5hn5x/Pozw17rwE6aQmyWxzvtu
         s3Xg==
X-Gm-Message-State: ALoCoQnKbLLZuLfD51HraG6TFXa8Xt+qCZKzmUGNadPi3bhrjdql+UxR7rrSIq+5Qc3TXSqyIuuK
MIME-Version: 1.0
X-Received: by 10.50.254.4 with SMTP id ae4mr7675275igd.10.1427322501936; Wed,
 25 Mar 2015 15:28:21 -0700 (PDT)
Received: by 10.107.27.2 with HTTP; Wed, 25 Mar 2015 15:28:21 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
Date: Wed, 25 Mar 2015 15:28:21 -0700
Message-ID: <CAHuE29bjJVX87SX6ZRovuHCZubKeyR3ZtiN8rdFoZ_unHp45TA@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Reza Zadeh <reza@databricks.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Sam Halliday <sam.halliday@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, 
	"Evan R. Sparks" <evan.sparks@gmail.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=001a11343b92b576db0512246aa6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11343b92b576db0512246aa6
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

These are awesome (and surprising) results, Alex. I've been following this
thread and really surprised by the improvement over BIDMat-cuda, almost 20x
faster.

Any chance you could send scripts or github gist for reproduction?

Thanks,
Reza

On Wed, Mar 25, 2015 at 2:31 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
>
wrote:

> Hi again,
>
> I finally managed to use nvblas within Spark+netlib-java. It has
> exceptional performance for big matrices with Double, faster than
> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
> to/from GPU, OpenBlas or MKL might be a better choice. This correlates wi=
th
> original nvblas presentation on GPU conf 2013 (slide 21):
> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-=
New-Features-CUDA%206%20-GPU-Acceleration.pdf
>
> My results:
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Just in case, these tests are not for generalization of performance of
> different libraries. I just want to pick a library that does at best dens=
e
> matrices multiplication for my task.
>
> P.S. My previous issue with nvblas was the following: it has Fortran blas
> functions, at the same time netlib-java uses C cblas functions. So, one
> needs cblas shared library to use nvblas through netlib-java. Fedora does
> not have cblas (but Debian and Ubuntu have), so I needed to compile it. I
> could not use cblas from Atlas or Openblas because they link to their
> implementation and not to Fortran blas.
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Tuesday, March 24, 2015 6:57 PM
> To: Sam Halliday
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
> Hi,
>
> I am trying to use nvblas with netlib-java from Spark. nvblas functions
> should replace current blas functions calls after executing LD_PRELOAD as
> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
> changes to netlib-java. It seems to work for simple Java example, but I
> cannot make it work with Spark. I run the following:
> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>
> +------------------------------------------------------------------------=
-----+
> | Processes:                                                       GPU
> Memory |
> |  GPU       PID  Type  Process name                               Usage
>     |
>
> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D|
> |    0      8873    C   bash
> 39MiB |
> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
> 39MiB |
>
> +------------------------------------------------------------------------=
-----+
>
> In Spark shell I do matrix multiplication and see the following:
> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
> So I am sure that netlib-native is loaded and cblas supposedly used.
> However, matrix multiplication does executes on CPU since I see 16% of CP=
U
> used and 0% of GPU used. I also checked different matrix sizes, from
> 100x100 to 12000x12000
>
> Could you suggest might the LD_PRELOAD not affect Spark shell?
>
> Best regards, Alexander
>
>
>
> From: Sam Halliday [mailto:sam.halliday@gmail.com]
> Sent: Monday, March 09, 2015 6:01 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
> Thanks so much for following up on this!
>
> Hmm, I wonder if we should have a concerted effort to chart performance o=
n
> various pieces of hardware...
> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto:
> alexander.ulanov@hp.com>> wrote:
> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
> support of Double in the current source code), did the test with BIDMat a=
nd
> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
> sam.halliday@gmail.com>]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
> dev@spark.apache.org>
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
> <mailto:joseph@databricks.com>> wrote:
> >> Better documentation for linking would be very helpful!  Here's a JIRA=
:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for larger
> matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks
> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
> to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked up b=
y
> netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I understand
> how to force use a specific blas (not specific wrapper for blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already have
> it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
> joseph@databricks.com><mailto:
> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons with
> Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential to
> machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and =
4
> GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x faster a=
t
> matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster within
> Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better performance
> only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was not a
> test for ONLY multiplication since there are other operations involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
> dev-unsubscribe@spark.apache.org><mailto:
> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
> >>>> ark.apac> he.org<http://he.org>
> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
> >>>> rk.apache.org>>> For additional commands, e-mail:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --
> Best regards,
> Sam
>

--001a11343b92b576db0512246aa6--

From dev-return-12194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:29:34 2015
Return-Path: <dev-return-12194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA68317EA2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:29:34 +0000 (UTC)
Received: (qmail 45740 invoked by uid 500); 25 Mar 2015 22:29:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45662 invoked by uid 500); 25 Mar 2015 22:29:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45632 invoked by uid 99); 25 Mar 2015 22:29:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:29:32 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.240.92.67] (HELO g9t5009.houston.hp.com) (15.240.92.67)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:29:07 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5009.houston.hp.com (Postfix) with ESMTPS id 7E534145
	for <dev@spark.apache.org>; Wed, 25 Mar 2015 22:29:04 +0000 (UTC)
Received: from G9W3616.americas.hpqcorp.net (16.216.186.51) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Wed, 25 Mar 2015 22:27:49 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G9W3616.americas.hpqcorp.net ([16.216.186.51]) with mapi id 14.03.0169.001;
 Wed, 25 Mar 2015 22:27:49 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Sam Halliday <sam.halliday@gmail.com>, "Evan R. Sparks"
	<evan.sparks@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAvQQdDAAKEa4gAACAY+AAAAmmAAAACQmYA==
Date: Wed, 25 Mar 2015 22:27:47 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A77B@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<CABjXkq5iEyfgNGS0CnAXqL93-0hs1KiMgVeqcKK3NxTpF1ZpBw@mail.gmail.com>
 <CALR_T9B+Y-gbYpTish-bDumD8+8nc93oFJpYybTxVQhcmdqqyw@mail.gmail.com>
In-Reply-To: <CALR_T9B+Y-gbYpTish-bDumD8+8nc93oFJpYybTxVQhcmdqqyw@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.192.232]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-12195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 22:31:38 2015
Return-Path: <dev-return-12195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D45DB17F22
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 22:31:38 +0000 (UTC)
Received: (qmail 53456 invoked by uid 500); 25 Mar 2015 22:31:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53365 invoked by uid 500); 25 Mar 2015 22:31:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53350 invoked by uid 99); 25 Mar 2015 22:31:37 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:31:37 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 22:31:32 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id 10C2371;
	Wed, 25 Mar 2015 22:30:40 +0000 (UTC)
Received: from G9W3615.americas.hpqcorp.net (16.216.186.50) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Wed, 25 Mar 2015 22:29:17 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G9W3615.americas.hpqcorp.net ([16.216.186.50]) with mapi id 14.03.0169.001;
 Wed, 25 Mar 2015 22:29:17 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: jfcanny <canny@berkeley.edu>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAvQQdDAAKEa4gAACUsUAAACMHQA=
Date: Wed, 25 Mar 2015 22:29:16 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A78E@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
 <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
 <87ioehu4qv.fsf@gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
 <551331DF.2040704@berkeley.edu>
In-Reply-To: <551331DF.2040704@berkeley.edu>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.192.232]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-12196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Mar 25 23:37:30 2015
Return-Path: <dev-return-12196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4F320179DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 25 Mar 2015 23:37:30 +0000 (UTC)
Received: (qmail 52820 invoked by uid 500); 25 Mar 2015 23:37:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52742 invoked by uid 500); 25 Mar 2015 23:37:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52731 invoked by uid 99); 25 Mar 2015 23:37:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 23:37:28 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of alessandro.andrioni@dafiti.com.br does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 25 Mar 2015 23:37:23 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 0ADF318ACA50
	for <dev@spark.apache.org>; Wed, 25 Mar 2015 16:35:51 -0700 (PDT)
Date: Wed, 25 Mar 2015 16:35:33 -0700 (MST)
From: "alessandro.andrioni" <alessandro.andrioni@dafiti.com.br>
To: dev@spark.apache.org
Message-ID: <1427326533707-11252.post@n3.nabble.com>
In-Reply-To: <CAJbo4nfEBAyLFeb7rUzESOO508r76+6r6e-7qac0bybJz2qA9w@mail.gmail.com>
References: <CAJbo4nf6ffhvfG8qxFFPKDthrFxhAkPOLOyindWeUFK-M=5PWQ@mail.gmail.com> <CAAswR-5fup6Tz=1ZPZn29vNdBOV7j54K2BQKjvVr7c+gzvSv2A@mail.gmail.com> <CAJbo4nfEBAyLFeb7rUzESOO508r76+6r6e-7qac0bybJz2qA9w@mail.gmail.com>
Subject: Re: lower&upperBound not working/spark 1/3
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

If I'm reading this comment[1] correctly, this is expected behavior: the
lower and upper bounds are used to make the partitioning more efficient, not
to limit the data returned.

>/**
> * Given a partitioning schematic (a column of integral type, a number of
> * partitions, and upper and lower bounds on the column's value), generate
> * WHERE clauses for each partition so that each row in the table appears
> * exactly once.  The parameters minValue and maxValue are advisory in that
> * incorrect values may cause the partitioning to be poor, but no data
> * will fail to be represented.
> */

I also got bit by this recently.

[1]:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/jdbc/JDBCRelation.scala#L49-L56


Marek Wiewiorka wrote
> Ok- thanks Michael I will do another series of tests to confirm this and
> then report an issue.
> 
> Regards,
> Marek
> 
> 2015-03-22 22:19 GMT+01:00 Michael Armbrust &lt;

> michael@

> &gt;:
> 
>> I have not heard this reported yet, but your invocation looks correct to
>> me.  Can you open a JIRA?
>>
>> On Sun, Mar 22, 2015 at 8:39 AM, Marek Wiewiorka <
>> 

> marek.wiewiorka@

>> wrote:
>>
>>> Hi All - I try to use the new SQLContext API for populating DataFrame
>>> from
>>> jdbc data source.
>>> like this:
>>>
>>> val jdbcDF = sqlContext.jdbc(url =
>>> "jdbc:postgresql://localhost:5430/dbname?user=user&password=111", table
>>> =
>>> "se_staging.exp_table3" ,columnName="cs_id",lowerBound=1 ,upperBound =
>>> 10000, numPartitions=12 )
>>>
>>> No matter how I set lower and upper bounds I always get all the rows
>>> from
>>> my table.
>>> The API is marked as experimental so I assume there might by some bugs
>>> in
>>> it but
>>> did anybody come across a similar issue?
>>>
>>> Thanks!
>>> Marek
>>>
>>
>>





--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/lower-upperBound-not-working-spark-1-3-tp11151p11252.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 01:05:13 2015
Return-Path: <dev-return-12197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E80717704
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 01:05:13 +0000 (UTC)
Received: (qmail 62713 invoked by uid 500); 26 Mar 2015 01:05:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62634 invoked by uid 500); 26 Mar 2015 01:05:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62623 invoked by uid 99); 26 Mar 2015 01:05:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 01:05:11 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.240.92.67] (HELO g9t5009.houston.hp.com) (15.240.92.67)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 01:05:07 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5009.houston.hp.com (Postfix) with ESMTPS id 13F21183;
	Thu, 26 Mar 2015 01:04:45 +0000 (UTC)
Received: from G4W6303.americas.hpqcorp.net (16.210.26.228) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 26 Mar 2015 01:03:05 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G4W6303.americas.hpqcorp.net ([16.210.26.228]) with mapi id 14.03.0169.001;
 Thu, 26 Mar 2015 01:03:05 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Sam Halliday <sam.halliday@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>, Xiangrui Meng
	<mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, "Evan R. Sparks"
	<evan.sparks@gmail.com>, jfcanny <canny@berkeley.edu>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAvQQdDAAKEa4gAAH6Pcw
Date: Thu, 26 Mar 2015 01:03:03 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.192.232]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-12198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 01:24:32 2015
Return-Path: <dev-return-12198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4D9E717A13
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 01:24:32 +0000 (UTC)
Received: (qmail 1785 invoked by uid 500); 26 Mar 2015 01:24:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1703 invoked by uid 500); 26 Mar 2015 01:24:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1678 invoked by uid 99); 26 Mar 2015 01:24:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 01:24:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.160.172 as permitted sender)
Received: from [209.85.160.172] (HELO mail-yk0-f172.google.com) (209.85.160.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 01:23:52 +0000
Received: by ykek76 with SMTP id k76so21906331yke.0
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 18:23:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=jCcFeSXyO0rZReONrikiJ9i975COtchMVBI/B/bMVms=;
        b=YxLf23RPFc4oQWMBf6VM6hHVZLD+QNclqoaTmSWv9yH6WdrAK5AtIEsIxGkaFYhcE3
         w+uMCW7BQC8zcgy93MDkj2JviVV9bJYUs7JXXcBcJw7JMkh8Y4AenBBfFUhOFyu/dK2U
         afSwWLp6JW3vZQk3YkCE/Zc7SNoQhzHVdjUvl9ptq4aqmkzkThHQ4khfsdgGb12Gso+W
         eKsKHNucCs3pHMm39nf7xDfjh2/GJXVd+mR/xmu4zwzZ2RiPUszNaitRIeUkdd86rPWu
         kEYRerzS9HfsD5f3NVTZG5XKr2oxQfiPqaBLiPocRccdqM0Jt2roQ9nrjSqgTVFoyGDY
         3k9g==
X-Received: by 10.52.116.197 with SMTP id jy5mr13298085vdb.13.1427333030227;
 Wed, 25 Mar 2015 18:23:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Wed, 25 Mar 2015 18:23:30 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
 <CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
 <CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
 <CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
 <CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
 <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
 <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Wed, 25 Mar 2015 18:23:30 -0700
Message-ID: <CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Sam Halliday <sam.halliday@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=bcaec5485ade3e350a051226def3
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec5485ade3e350a051226def3
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Yeah, much more reasonable - nice to know that we can get full GPU
performance from breeze/netlib-java - meaning there's no compelling
performance reason to switch out our current linear algebra library (at
least as far as this benchmark is concerned).

Instead, it looks like a user guide for configuring Spark/MLlib to use the
right BLAS library will get us most of the way there. Or, would it make
sense to finally ship openblas compiled for some common platforms (64-bit
linux, windows, mac) directly with Spark - hopefully eliminating the jblas
warnings once and for all for most users? (Licensing is BSD) Or am I
missing something?

On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <alexander.ulanov@hp.com=
>
wrote:

> As everyone suggested, the results were too good to be true, so I
> double-checked them. It turns that nvblas did not do multiplication due t=
o
> parameter NVBLAS_TILE_DIM from "nvblas.conf" and returned zero matrix. My
> previously posted results with nvblas are matrices copying only. The
> default NVBLAS_TILE_DIM=3D=3D2048 is too big for my graphic card/matrix s=
ize. I
> handpicked other values that worked. As a result, netlib+nvblas is on par
> with BIDMat-cuda. As promised, I am going to post a how-to for nvblas
> configuration.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
>
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Wednesday, March 25, 2015 2:31 PM
> To: Sam Halliday
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks;
> jfcanny
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
> Hi again,
>
> I finally managed to use nvblas within Spark+netlib-java. It has
> exceptional performance for big matrices with Double, faster than
> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
> to/from GPU, OpenBlas or MKL might be a better choice. This correlates wi=
th
> original nvblas presentation on GPU conf 2013 (slide 21):
> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-=
New-Features-CUDA%206%20-GPU-Acceleration.pdf
>
> My results:
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Just in case, these tests are not for generalization of performance of
> different libraries. I just want to pick a library that does at best dens=
e
> matrices multiplication for my task.
>
> P.S. My previous issue with nvblas was the following: it has Fortran blas
> functions, at the same time netlib-java uses C cblas functions. So, one
> needs cblas shared library to use nvblas through netlib-java. Fedora does
> not have cblas (but Debian and Ubuntu have), so I needed to compile it. I
> could not use cblas from Atlas or Openblas because they link to their
> implementation and not to Fortran blas.
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Ulanov, Alexander
> Sent: Tuesday, March 24, 2015 6:57 PM
> To: Sam Halliday
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
> Hi,
>
> I am trying to use nvblas with netlib-java from Spark. nvblas functions
> should replace current blas functions calls after executing LD_PRELOAD as
> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
> changes to netlib-java. It seems to work for simple Java example, but I
> cannot make it work with Spark. I run the following:
> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>
> +------------------------------------------------------------------------=
-----+
> | Processes:                                                       GPU
> Memory |
> |  GPU       PID  Type  Process name                               Usage
>     |
>
> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D|
> |    0      8873    C   bash
> 39MiB |
> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
> 39MiB |
>
> +------------------------------------------------------------------------=
-----+
>
> In Spark shell I do matrix multiplication and see the following:
> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
> So I am sure that netlib-native is loaded and cblas supposedly used.
> However, matrix multiplication does executes on CPU since I see 16% of CP=
U
> used and 0% of GPU used. I also checked different matrix sizes, from
> 100x100 to 12000x12000
>
> Could you suggest might the LD_PRELOAD not affect Spark shell?
>
> Best regards, Alexander
>
>
>
> From: Sam Halliday [mailto:sam.halliday@gmail.com]
> Sent: Monday, March 09, 2015 6:01 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
> Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
> Thanks so much for following up on this!
>
> Hmm, I wonder if we should have a concerted effort to chart performance o=
n
> various pieces of hardware...
> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto:
> alexander.ulanov@hp.com>> wrote:
> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
> support of Double in the current source code), did the test with BIDMat a=
nd
> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>
>
> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J=
5r7kwKSPkY/edit?usp=3Dsharing
>
> Best regards, Alexander
>
> -----Original Message-----
> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
> sam.halliday@gmail.com>]
> Sent: Tuesday, March 03, 2015 1:54 PM
> To: Xiangrui Meng; Joseph Bradley
> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
> dev@spark.apache.org>
> Subject: Re: Using CUDA within Spark / boosting linear algebra
>
> BTW, is anybody on this list going to the London Meetup in a few weeks?
>
>
> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapred=
uce-world#community
>
> Would be nice to meet other people working on the guts of Spark! :-)
>
>
> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>
> > Hey Alexander,
> >
> > I don't quite understand the part where netlib-cublas is about 20x
> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
> > with netlib-java?
> >
> > CC'ed Sam, the author of netlib-java.
> >
> > Best,
> > Xiangrui
> >
> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
> <mailto:joseph@databricks.com>> wrote:
> >> Better documentation for linking would be very helpful!  Here's a JIRA=
:
> >> https://issues.apache.org/jira/browse/SPARK-6019
> >>
> >>
> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
> >> wrote:
> >>
> >>> Thanks for compiling all the data and running these benchmarks,
> >>> Alex. The big takeaways here can be seen with this chart:
> >>>
> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
> >>>
> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
> >>> BIDMat+GPU) can provide substantial (but less than an order of
> >>> BIDMat+magnitude)
> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
> >>> netlib-java+openblas-compiled).
> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
> >>> worse than a well-tuned CPU implementation, particularly for larger
> matrices.
> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
> >>> basically agrees with the authors own benchmarks (
> >>> https://github.com/fommil/netlib-java)
> >>>
> >>> I think that most of our users are in a situation where using GPUs
> >>> may not be practical - although we could consider having a good GPU
> >>> backend available as an option. However, *ALL* users of MLlib could
> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
> >>> BLAS implementation. Perhaps we should consider updating the mllib
> >>> guide with a more complete section for enabling high performance
> >>> binaries on OSX and Linux? Or better, figure out a way for the
> >>> system to fetch these automatically.
> >>>
> >>> - Evan
> >>>
> >>>
> >>>
> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
> >>>
> >>>> Just to summarize this thread, I was finally able to make all
> >>>> performance comparisons that we discussed. It turns out that:
> >>>> BIDMat-cublas>>BIDMat
> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-yu=
m-repo=3D
> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
> >>>>
> >>>> Below is the link to the spreadsheet with full results.
> >>>>
> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
> >>>>
> >>>> One thing still needs exploration: does BIDMat-cublas perform
> >>>> copying to/from machine=E2=80=99s RAM?
> >>>>
> >>>> -----Original Message-----
> >>>> From: Ulanov, Alexander
> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
> >>>> To: Evan R. Sparks
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
> >>>> the original one discusses slightly different topic. I was able to
> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
> >>>> statically linked inside a 60MB library.
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
> >>>> |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
> >>>> 1569,233228 |
> >>>>
> >>>> It turn out that pre-compiled MKL is faster than precompiled
> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns =
with
> >>>> locally compiled openblas and cuda.
> >>>>
> >>>> Alexander
> >>>>
> >>>> From: Evan R. Sparks
> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
> >>>> Sent: Monday, February 09, 2015 6:06 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Great - perhaps we can move this discussion off-list and onto a
> >>>> JIRA ticket? (Here's one:
> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
> >>>>
> >>>> It seems like this is going to be somewhat exploratory for a while
> >>>> (and there's probably only a handful of us who really care about
> >>>> fast linear
> >>>> algebra!)
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for explanation and useful link. I am going to build
> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
> >>>>
> >>>> Do I understand correctly that BIDMat binaries contain statically
> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
> >>>> it seems that in my case precompiled MKL BLAS performs better than
> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
> to be on par with JNI overheads.
> >>>>
> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
> >>>> Halliday
> >>>> (Netlib-java) interested to compare their libraries.
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:58 PM
> >>>>
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
> >>>> from getting cache sizes, etc. set up correctly for your particular
> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
> >>>> quickly and yields performance competitive with MKL.
> >>>>
> >>>> To make sure the right library is getting used, you have to make
> >>>> sure it's first on the search path - export
> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
> >>>>
> >>>> For some examples of getting netlib-java setup on an ec2 node and
> >>>> some example benchmarking code we ran a while back, see:
> >>>> https://github.com/shivaram/matrix-bench
> >>>>
> >>>> In particular - build-openblas-ec2.sh shows you how to build the
> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
> >>>> shows you how to get the path setup and get that library picked up b=
y
> netlib-java.
> >>>>
> >>>> In this way - you could probably get cuBLAS set up to be used by
> >>>> netlib-java as well.
> >>>>
> >>>> - Evan
> >>>>
> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
> >>>> force loading the right blas? For netlib, I there are few JVM
> >>>> flags, such as
> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
> >>>> so I can force it to use Java implementation. Not sure I understand
> how to force use a specific blas (not specific wrapper for blas).
> >>>>
> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
> >>>> that netlib is using it.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Friday, February 06, 2015 5:19 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Joseph Bradley;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Getting breeze to pick up the right blas library is critical for
> >>>> performance. I recommend using OpenBLAS (or MKL, if you already have
> it).
> >>>> It might make sense to force BIDMat to use the same underlying BLAS
> >>>> library as well.
> >>>>
> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Hi Evan, Joseph
> >>>>
> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
> >>>>
> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
> >>>> |native_system_linux_x86-64|
> >>>> Breeze+Netlib-java f2jblas |
> >>>>
> +-----------------------------------------------------------------------+
> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
> >>>> ||
> >>>>
> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
> >>>> 19 Linux, Scala 2.11.
> >>>>
> >>>> Later I will make tests with Cuda. I need to install new Cuda
> >>>> version for this purpose.
> >>>>
> >>>> Do you have any ideas why breeze-netlib with native blas is so much
> >>>> slower than BIDMat MKL?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
> joseph@databricks.com><mailto:
> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
> >>>> Sent: Thursday, February 05, 2015 5:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: Evan R. Sparks;
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> Hi Alexander,
> >>>>
> >>>> Using GPUs with Spark would be very exciting.  Small comment:
> >>>> Concerning your question earlier about keeping data stored on the
> >>>> GPU rather than having to move it between main memory and GPU
> >>>> memory on each iteration, I would guess this would be critical to
> >>>> getting good performance.  If you could do multiple local
> >>>> iterations before aggregating results, then the cost of data
> >>>> movement to the GPU could be amortized (and I believe that is done
> >>>> in practice).  Having Spark be aware of the GPU and using it as
> another part of memory sounds like a much bigger undertaking.
> >>>>
> >>>> Joseph
> >>>>
> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presenta=
tion by
> >>>> John Canny and I am really inspired by his talk and comparisons with
> Spark MLlib.
> >>>>
> >>>> I am very interested to find out what will be better within Spark:
> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=80=
=9D test of
> >>>> linear algebra, it involves some other things that are essential to
> machine learning.
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
> >>>> Sent: Thursday, February 05, 2015 1:29 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
> >>>> netlib-java+data
> >>>> layout and fewer levels of indirection - it's definitely a
> >>>> worthwhile experiment to run. The main speedups I've seen from
> >>>> using it come from highly optimized GPU code for linear algebra. I
> >>>> know that in the past Canny has gone as far as to write custom GPU
> >>>> kernels for performance-critical regions of code.[1]
> >>>>
> >>>> BIDMach is highly optimized for single node performance or
> >>>> performance on small clusters.[2] Once data doesn't fit easily in
> >>>> GPU memory (or can be batched in that way) the performance tends to
> >>>> fall off. Canny argues for hardware/software codesign and as such
> >>>> prefers machine configurations that are quite different than what
> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and =
4
> GPUs.
> >>>>
> >>>> In contrast, MLlib was designed for horizontal scalability on
> >>>> commodity clusters and works best on very big datasets - order of
> terabytes.
> >>>>
> >>>> For the most part, these projects developed concurrently to address
> >>>> slightly different use cases. That said, there may be bits of
> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
> >>>> careful about maintaining cross-language compatibility for our Java
> >>>> and Python-users, though.
> >>>>
> >>>> - Evan
> >>>>
> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
> >>>>
> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Hi Evan,
> >>>>
> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
> >>>> you know what makes them faster than netlib-java?
> >>>>
> >>>> The same group has BIDMach library that implements machine
> >>>> learning. For some examples they use Caffe convolutional neural
> >>>> network library owned by another group in Berkeley. Could you
> >>>> elaborate on how these all might be connected with Spark Mllib? If
> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMac=
h for
> optimization and learning?
> >>>>
> >>>> Best regards, Alexander
> >>>>
> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
> evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
> >>>> Sent: Thursday, February 05, 2015 12:09 PM
> >>>> To: Ulanov, Alexander
> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
> >>>> apache.org<mailto:dev@spark.apache.org>>>
> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
> >>>>
> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
> >>>> blas in many cases.
> >>>>
> >>>> You might consider taking a look at the codepaths that BIDMat (
> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
> >>>> optimizing to make this work really fast from Scala. I've run it on
> >>>> my laptop and compared to MKL and in certain cases it's 10x faster a=
t
> matrix multiply.
> >>>> There are a lot of layers of indirection here and you really want
> >>>> to avoid data copying as much as possible.
> >>>>
> >>>> We could also consider swapping out BIDMat for Breeze, but that
> >>>> would be a big project and if we can figure out how to get
> >>>> breeze+cublas to comparable performance that would be a big win.
> >>>>
> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
> >>>> Dear Spark developers,
> >>>>
> >>>> I am exploring how to make linear algebra operations faster within
> Spark.
> >>>> One way of doing this is to use Scala Breeze library that is
> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
> >>>> and LAPACK native binaries if they are available on the worker
> >>>> node. It also has its own optimized Java implementation of BLAS. It
> >>>> is worth mentioning, that native binaries provide better performance
> only for BLAS level 3, i.e.
> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
> >>>> This is confirmed by GEMM test on Netlib-java page
> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
> >>>> experiments with training of artificial neural network
> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
> >>>> However, I would like to boost performance more.
> >>>>
> >>>> GPU is supposed to work fast with linear algebra and there is
> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
> >>>> server with Nvidia GPU and I was able to do the following. I linked
> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
> >>>> performance measurements with regards to artificial neural network
> >>>> batch learning in Spark MLlib that involves matrix-matrix
> >>>> multiplications. It turns out that for matrices of size less than
> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
> >>>> slower for bigger matrices. It worth mentioning that it is was not a
> test for ONLY multiplication since there are other operations involved.
> >>>> One of the reasons for slowdown might be the overhead of copying
> >>>> the matrices from computer memory to graphic card memory and back.
> >>>>
> >>>> So, few questions:
> >>>> 1) Do these results with CUDA make sense?
> >>>> 2) If the problem is with copy overhead, are there any libraries
> >>>> that allow to force intermediate results to stay in graphic card
> >>>> memory thus removing the overhead?
> >>>> 3) Any other options to speed-up linear algebra in Spark?
> >>>>
> >>>> Thank you, Alexander
> >>>>
> >>>> -------------------------------------------------------------------
> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
> dev-unsubscribe@spark.apache.org><mailto:
> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
> >>>> ark.apac> he.org<http://he.org>
> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
> >>>> rk.apache.org>>> For additional commands, e-mail:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto:
> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
> >>>>
> >>>>
> >>>>
> >>>>
> >>>
>
> --
> Best regards,
> Sam
>

--bcaec5485ade3e350a051226def3--

From dev-return-12199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 02:08:21 2015
Return-Path: <dev-return-12199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 10E9217FE3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 02:08:21 +0000 (UTC)
Received: (qmail 83544 invoked by uid 500); 26 Mar 2015 02:08:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83465 invoked by uid 500); 26 Mar 2015 02:08:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83454 invoked by uid 99); 26 Mar 2015 02:08:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 02:08:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.223.174 as permitted sender)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 02:08:15 +0000
Received: by iedfl3 with SMTP id fl3so41016990ied.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 19:06:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=29buxsJdVAbnfhSHmwkqMRwZUkXCDmYoXGvnpkDDibM=;
        b=JmL4wtx+9mg7H2YcHObVR+LslFYgDd3gnHhAN2p+ha4R5tND0zu2c7xrCONPExxspo
         Zyoep0ORLzpZWkqLVwLyWmdWugt2YglacQjW9WywBRzbwNwocB5V7hPcHp0JoClISMzO
         OsL48oFYDQXh6RXxUJH2pQTfEe0hTTgJUPhBRDUGudIqHd3+cQdqi1nXbcSCYlzh9ctb
         8lWrCljGME8/hOl4wKV4IcYGjfgXZ/0H6yNlIzfklHttkZBRBaXtZWOvvIoLk5YY3x52
         aXY02/zAhQ05bZ9FFjplJfdueWIII2xKikkt5wq/Iaw0xh20eN5RSkzAjbuWZfiZ9IDB
         YAQQ==
X-Gm-Message-State: ALoCoQm/jdvmIzpjhwauPp6r18fVR49gbwXIm89LfBtkUj+DtTo6eoyUzetv22LSrtzkohFexsd/
MIME-Version: 1.0
X-Received: by 10.50.6.4 with SMTP id w4mr34027426igw.36.1427335604973; Wed,
 25 Mar 2015 19:06:44 -0700 (PDT)
Received: by 10.36.90.208 with HTTP; Wed, 25 Mar 2015 19:06:44 -0700 (PDT)
In-Reply-To: <CA+3qhFQUdMb+AWtSmNWTzMQJ_mdr3U8fCJ5mciCUpEPGahqirQ@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CA+3qhFQUdMb+AWtSmNWTzMQJ_mdr3U8fCJ5mciCUpEPGahqirQ@mail.gmail.com>
Date: Wed, 25 Mar 2015 22:06:44 -0400
Message-ID: <CACBYxK+=zBLKfwmGpEoGan+mJ=R7qvh-4LX5M9eYyOpz3Bbkag@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Imran Rashid <irashid@cloudera.com>
Cc: Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc1686b5d6990512277796
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc1686b5d6990512277796
Content-Type: text/plain; charset=UTF-8

Regarding Patrick's question, you can just do "new Configuration(oldConf)"
to get a cloned Configuration object and add any new properties to it.

-Sandy

On Wed, Mar 25, 2015 at 4:42 PM, Imran Rashid <irashid@cloudera.com> wrote:

> Hi Nick,
>
> I don't remember the exact details of these scenarios, but I think the user
> wanted a lot more control over how the files got grouped into partitions,
> to group the files together by some arbitrary function.  I didn't think
> that was possible w/ CombineFileInputFormat, but maybe there is a way?
>
> thanks
>
> On Tue, Mar 24, 2015 at 1:50 PM, Nick Pentreath <nick.pentreath@gmail.com>
> wrote:
>
> > Imran, on your point to read multiple files together in a partition, is
> it
> > not simpler to use the approach of copy Hadoop conf and set per-RDD
> > settings for min split to control the input size per partition, together
> > with something like CombineFileInputFormat?
> >
> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
> > wrote:
> >
> > > I think this would be a great addition, I totally agree that you need
> to
> > be
> > > able to set these at a finer context than just the SparkContext.
> > >
> > > Just to play devil's advocate, though -- the alternative is for you
> just
> > > subclass HadoopRDD yourself, or make a totally new RDD, and then you
> > could
> > > expose whatever you need.  Why is this solution better?  IMO the
> criteria
> > > are:
> > > (a) common operations
> > > (b) error-prone / difficult to implement
> > > (c) non-obvious, but important for performance
> > >
> > > I think this case fits (a) & (c), so I think its still worthwhile.  But
> > its
> > > also worth asking whether or not its too difficult for a user to extend
> > > HadoopRDD right now.  There have been several cases in the past week
> > where
> > > we've suggested that a user should read from hdfs themselves (eg., to
> > read
> > > multiple files together in one partition) -- with*out* reusing the code
> > in
> > > HadoopRDD, though they would lose things like the metric tracking &
> > > preferred locations you get from HadoopRDD.  Does HadoopRDD need to
> some
> > > refactoring to make that easier to do?  Or do we just need a good
> > example?
> > >
> > > Imran
> > >
> > > (sorry for hijacking your thread, Koert)
> > >
> > >
> > >
> > > On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
> > wrote:
> > >
> > > > see email below. reynold suggested i send it to dev instead of user
> > > >
> > > > ---------- Forwarded message ----------
> > > > From: Koert Kuipers <koert@tresata.com>
> > > > Date: Mon, Mar 23, 2015 at 4:36 PM
> > > > Subject: hadoop input/output format advanced control
> > > > To: "user@spark.apache.org" <user@spark.apache.org>
> > > >
> > > >
> > > > currently its pretty hard to control the Hadoop Input/Output formats
> > used
> > > > in Spark. The conventions seems to be to add extra parameters to all
> > > > methods and then somewhere deep inside the code (for example in
> > > > PairRDDFunctions.saveAsHadoopFile) all these parameters get
> translated
> > > into
> > > > settings on the Hadoop Configuration object.
> > > >
> > > > for example for compression i see "codec: Option[Class[_ <:
> > > > CompressionCodec]] = None" added to a bunch of methods.
> > > >
> > > > how scalable is this solution really?
> > > >
> > > > for example i need to read from a hadoop dataset and i dont want the
> > > input
> > > > (part) files to get split up. the way to do this is to set
> > > > "mapred.min.split.size". now i dont want to set this at the level of
> > the
> > > > SparkContext (which can be done), since i dont want it to apply to
> > input
> > > > formats in general. i want it to apply to just this one specific
> input
> > > > dataset i need to read. which leaves me with no options currently. i
> > > could
> > > > go add yet another input parameter to all the methods
> > > > (SparkContext.textFile, SparkContext.hadoopFile,
> > SparkContext.objectFile,
> > > > etc.). but that seems ineffective.
> > > >
> > > > why can we not expose a Map[String, String] or some other generic way
> > to
> > > > manipulate settings for hadoop input/output formats? it would require
> > > > adding one more parameter to all methods to deal with hadoop
> > input/output
> > > > formats, but after that its done. one parameter to rule them all....
> > > >
> > > > then i could do:
> > > > val x = sc.textFile("/some/path", formatSettings =
> > > > Map("mapred.min.split.size" -> "12345"))
> > > >
> > > > or
> > > > rdd.saveAsTextFile("/some/path, formatSettings =
> > > > Map(mapred.output.compress" -> "true",
> > "mapred.output.compression.codec"
> > > ->
> > > > "somecodec"))
> > > >
> > >
> >
>

--047d7bdc1686b5d6990512277796--

From dev-return-12200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 02:17:20 2015
Return-Path: <dev-return-12200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 85445172C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 02:17:20 +0000 (UTC)
Received: (qmail 6700 invoked by uid 500); 26 Mar 2015 02:17:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6617 invoked by uid 500); 26 Mar 2015 02:17:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6605 invoked by uid 99); 26 Mar 2015 02:17:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 02:17:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 02:17:14 +0000
Received: by oicf142 with SMTP id f142so28879201oic.3
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 19:16:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=nJ0yOe3ALcyjRzbqZgbDHx0uA9uWCRYbGkkPGxbVZWw=;
        b=fFVei27VALualB0iF/o4nkijUQjrb8dXLBNXzctgeQDPCi1hei0LEj1pCWOcuOUd0+
         xZateeM3omL8nU7JllCqgEZoSkQiqeN4nTqHMqYj2BXDHxfMP0wO3iSB0AtyW5ZThSxC
         bs6pIqXy4XOoyWw0B27lVLhg1IrUCBVLQLvawUkrcLSdh7sbnVAfNKjFoPD/BH3SNfGB
         rQq8u63tUecGA5GLC2T6gn8WlliNO5xhGdgae5NJdnxxWjiPExwbecNwrvZmfC/48lNa
         /1cFDeVkTDy37/rjPV8yYXZXGJAByU1N+V1ksLviNy60mP/O0ip4rvw80fy5eKIMCo3T
         G8rA==
MIME-Version: 1.0
X-Received: by 10.182.80.103 with SMTP id q7mr10126614obx.18.1427336214626;
 Wed, 25 Mar 2015 19:16:54 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Wed, 25 Mar 2015 19:16:54 -0700 (PDT)
In-Reply-To: <CACBYxK+=zBLKfwmGpEoGan+mJ=R7qvh-4LX5M9eYyOpz3Bbkag@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
	<CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
	<CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
	<CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
	<CA+3qhFQUdMb+AWtSmNWTzMQJ_mdr3U8fCJ5mciCUpEPGahqirQ@mail.gmail.com>
	<CACBYxK+=zBLKfwmGpEoGan+mJ=R7qvh-4LX5M9eYyOpz3Bbkag@mail.gmail.com>
Date: Wed, 25 Mar 2015 19:16:54 -0700
Message-ID: <CABPQxst2a0t2SqT9a1X3qWzCfJfvjUDLTuovT3t0UujVNNzAVw@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
From: Patrick Wendell <pwendell@gmail.com>
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Imran Rashid <irashid@cloudera.com>, Nick Pentreath <nick.pentreath@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Great - that's even easier. Maybe we could have a simple example in the doc.

On Wed, Mar 25, 2015 at 7:06 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> Regarding Patrick's question, you can just do "new Configuration(oldConf)"
> to get a cloned Configuration object and add any new properties to it.
>
> -Sandy
>
> On Wed, Mar 25, 2015 at 4:42 PM, Imran Rashid <irashid@cloudera.com> wrote:
>
>> Hi Nick,
>>
>> I don't remember the exact details of these scenarios, but I think the user
>> wanted a lot more control over how the files got grouped into partitions,
>> to group the files together by some arbitrary function.  I didn't think
>> that was possible w/ CombineFileInputFormat, but maybe there is a way?
>>
>> thanks
>>
>> On Tue, Mar 24, 2015 at 1:50 PM, Nick Pentreath <nick.pentreath@gmail.com>
>> wrote:
>>
>> > Imran, on your point to read multiple files together in a partition, is
>> it
>> > not simpler to use the approach of copy Hadoop conf and set per-RDD
>> > settings for min split to control the input size per partition, together
>> > with something like CombineFileInputFormat?
>> >
>> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
>> > wrote:
>> >
>> > > I think this would be a great addition, I totally agree that you need
>> to
>> > be
>> > > able to set these at a finer context than just the SparkContext.
>> > >
>> > > Just to play devil's advocate, though -- the alternative is for you
>> just
>> > > subclass HadoopRDD yourself, or make a totally new RDD, and then you
>> > could
>> > > expose whatever you need.  Why is this solution better?  IMO the
>> criteria
>> > > are:
>> > > (a) common operations
>> > > (b) error-prone / difficult to implement
>> > > (c) non-obvious, but important for performance
>> > >
>> > > I think this case fits (a) & (c), so I think its still worthwhile.  But
>> > its
>> > > also worth asking whether or not its too difficult for a user to extend
>> > > HadoopRDD right now.  There have been several cases in the past week
>> > where
>> > > we've suggested that a user should read from hdfs themselves (eg., to
>> > read
>> > > multiple files together in one partition) -- with*out* reusing the code
>> > in
>> > > HadoopRDD, though they would lose things like the metric tracking &
>> > > preferred locations you get from HadoopRDD.  Does HadoopRDD need to
>> some
>> > > refactoring to make that easier to do?  Or do we just need a good
>> > example?
>> > >
>> > > Imran
>> > >
>> > > (sorry for hijacking your thread, Koert)
>> > >
>> > >
>> > >
>> > > On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
>> > wrote:
>> > >
>> > > > see email below. reynold suggested i send it to dev instead of user
>> > > >
>> > > > ---------- Forwarded message ----------
>> > > > From: Koert Kuipers <koert@tresata.com>
>> > > > Date: Mon, Mar 23, 2015 at 4:36 PM
>> > > > Subject: hadoop input/output format advanced control
>> > > > To: "user@spark.apache.org" <user@spark.apache.org>
>> > > >
>> > > >
>> > > > currently its pretty hard to control the Hadoop Input/Output formats
>> > used
>> > > > in Spark. The conventions seems to be to add extra parameters to all
>> > > > methods and then somewhere deep inside the code (for example in
>> > > > PairRDDFunctions.saveAsHadoopFile) all these parameters get
>> translated
>> > > into
>> > > > settings on the Hadoop Configuration object.
>> > > >
>> > > > for example for compression i see "codec: Option[Class[_ <:
>> > > > CompressionCodec]] = None" added to a bunch of methods.
>> > > >
>> > > > how scalable is this solution really?
>> > > >
>> > > > for example i need to read from a hadoop dataset and i dont want the
>> > > input
>> > > > (part) files to get split up. the way to do this is to set
>> > > > "mapred.min.split.size". now i dont want to set this at the level of
>> > the
>> > > > SparkContext (which can be done), since i dont want it to apply to
>> > input
>> > > > formats in general. i want it to apply to just this one specific
>> input
>> > > > dataset i need to read. which leaves me with no options currently. i
>> > > could
>> > > > go add yet another input parameter to all the methods
>> > > > (SparkContext.textFile, SparkContext.hadoopFile,
>> > SparkContext.objectFile,
>> > > > etc.). but that seems ineffective.
>> > > >
>> > > > why can we not expose a Map[String, String] or some other generic way
>> > to
>> > > > manipulate settings for hadoop input/output formats? it would require
>> > > > adding one more parameter to all methods to deal with hadoop
>> > input/output
>> > > > formats, but after that its done. one parameter to rule them all....
>> > > >
>> > > > then i could do:
>> > > > val x = sc.textFile("/some/path", formatSettings =
>> > > > Map("mapred.min.split.size" -> "12345"))
>> > > >
>> > > > or
>> > > > rdd.saveAsTextFile("/some/path, formatSettings =
>> > > > Map(mapred.output.compress" -> "true",
>> > "mapred.output.compression.codec"
>> > > ->
>> > > > "somecodec"))
>> > > >
>> > >
>> >
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 02:36:05 2015
Return-Path: <dev-return-12201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1F40C1740B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 02:36:05 +0000 (UTC)
Received: (qmail 36614 invoked by uid 500); 26 Mar 2015 02:36:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36538 invoked by uid 500); 26 Mar 2015 02:36:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36525 invoked by uid 99); 26 Mar 2015 02:36:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 02:36:03 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of wojciech.danilo@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 02:35:38 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 7999C18AF3F8
	for <dev@spark.apache.org>; Wed, 25 Mar 2015 19:35:53 -0700 (PDT)
Date: Wed, 25 Mar 2015 19:35:36 -0700 (MST)
From: danilo2 <wojciech.danilo@gmail.com>
To: dev@spark.apache.org
Message-ID: <1427337336047-11257.post@n3.nabble.com>
Subject: Haskell language Spark support
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi!
I'm a haskell developer and I have created many haskell libraries in my life
and some GHC extensions.
I would like to create Haskell binding for Spark. Where can I find any
documentation / sources describing the first steps in creation of a new
language binding?

I would be very thankful for any help! :)

Al the best,
Wojciech



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Haskell-language-Spark-support-tp11257.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 04:38:33 2015
Return-Path: <dev-return-12202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B329F179CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 04:38:33 +0000 (UTC)
Received: (qmail 15496 invoked by uid 500); 26 Mar 2015 04:38:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15426 invoked by uid 500); 26 Mar 2015 04:38:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15414 invoked by uid 99); 26 Mar 2015 04:38:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 04:38:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 04:38:03 +0000
Received: by wibgn9 with SMTP id gn9so68320996wib.1
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 21:37:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=avDZ1x+Y0VvfO3o15/GlVEGmlZ17/iMMmWxPvwLQWSs=;
        b=q4Ef962dQcIb/3Aa/b7tsqvdMzeEuarCdc+TPUYPybAVWMPdYDJPBSBbG3roJqOTDQ
         U5d4bB8BWMILb/UfI0d+NcD9I7CBD4AHBe7Gmrvc5Dsej5eKTFjKDZBsS09AVyQZ3Mcz
         L8VMEcukjkzcahaalLS4pNij51jxylywkxk1ggZuU6i/egVzvvs58Tmq493Xhyo1fVUc
         udN854Z8vC/kZvEwl6L4mQ6ufoFsQO29b6rDcC74L9pVUQ23E3wh6l/6gLYHH1WzCXoZ
         doCXts90TSaXzb99LCOyj7RvpYtSFd1xPK4CinS2EKPRtvbxkKBoLp4unzYC37KA8rPo
         hWsw==
X-Received: by 10.180.87.165 with SMTP id az5mr34368160wib.29.1427344637659;
 Wed, 25 Mar 2015 21:37:17 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.44.101 with HTTP; Wed, 25 Mar 2015 21:36:57 -0700 (PDT)
In-Reply-To: <CABPQxst2a0t2SqT9a1X3qWzCfJfvjUDLTuovT3t0UujVNNzAVw@mail.gmail.com>
References: <CANx3uAi6mHQm8ZSM+gQUTocXmKS0T-V5Thgx8PsfN9Jg-awCuA@mail.gmail.com>
 <CANx3uAiQs4onHehxd_DRVx7Ouh2zRdy2PS3hxnDKQ_LqCoaJnA@mail.gmail.com>
 <CA+3qhFR5MY12j-KL75=HK-taEv+XYC_F6Wm6ntn_FY=AcO3JXQ@mail.gmail.com>
 <CALD+6GOVmPDBS-+MSNrjAwEco=dbMVAUTEpCk9-D5UxQD32-XQ@mail.gmail.com>
 <CA+3qhFQUdMb+AWtSmNWTzMQJ_mdr3U8fCJ5mciCUpEPGahqirQ@mail.gmail.com>
 <CACBYxK+=zBLKfwmGpEoGan+mJ=R7qvh-4LX5M9eYyOpz3Bbkag@mail.gmail.com> <CABPQxst2a0t2SqT9a1X3qWzCfJfvjUDLTuovT3t0UujVNNzAVw@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Wed, 25 Mar 2015 21:36:57 -0700
Message-ID: <CANGvG8qRZY3ZeTWHmRPKmBp5ij=iDhxVMj+MxC9bE_Ts6YxUNg@mail.gmail.com>
Subject: Re: hadoop input/output format advanced control
To: Patrick Wendell <pwendell@gmail.com>
Cc: Sandy Ryza <sandy.ryza@cloudera.com>, Imran Rashid <irashid@cloudera.com>, 
	Nick Pentreath <nick.pentreath@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=14dae9cc9552198f8b05122992d0
X-Virus-Checked: Checked by ClamAV on apache.org

--14dae9cc9552198f8b05122992d0
Content-Type: text/plain; charset=UTF-8

Should we mention that you should synchronize
on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK to avoid a possible race
condition in cloning Hadoop Configuration objects prior to Hadoop 2.7.0? :)

On Wed, Mar 25, 2015 at 7:16 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Great - that's even easier. Maybe we could have a simple example in the
> doc.
>
> On Wed, Mar 25, 2015 at 7:06 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
> > Regarding Patrick's question, you can just do "new
> Configuration(oldConf)"
> > to get a cloned Configuration object and add any new properties to it.
> >
> > -Sandy
> >
> > On Wed, Mar 25, 2015 at 4:42 PM, Imran Rashid <irashid@cloudera.com>
> wrote:
> >
> >> Hi Nick,
> >>
> >> I don't remember the exact details of these scenarios, but I think the
> user
> >> wanted a lot more control over how the files got grouped into
> partitions,
> >> to group the files together by some arbitrary function.  I didn't think
> >> that was possible w/ CombineFileInputFormat, but maybe there is a way?
> >>
> >> thanks
> >>
> >> On Tue, Mar 24, 2015 at 1:50 PM, Nick Pentreath <
> nick.pentreath@gmail.com>
> >> wrote:
> >>
> >> > Imran, on your point to read multiple files together in a partition,
> is
> >> it
> >> > not simpler to use the approach of copy Hadoop conf and set per-RDD
> >> > settings for min split to control the input size per partition,
> together
> >> > with something like CombineFileInputFormat?
> >> >
> >> > On Tue, Mar 24, 2015 at 5:28 PM, Imran Rashid <irashid@cloudera.com>
> >> > wrote:
> >> >
> >> > > I think this would be a great addition, I totally agree that you
> need
> >> to
> >> > be
> >> > > able to set these at a finer context than just the SparkContext.
> >> > >
> >> > > Just to play devil's advocate, though -- the alternative is for you
> >> just
> >> > > subclass HadoopRDD yourself, or make a totally new RDD, and then you
> >> > could
> >> > > expose whatever you need.  Why is this solution better?  IMO the
> >> criteria
> >> > > are:
> >> > > (a) common operations
> >> > > (b) error-prone / difficult to implement
> >> > > (c) non-obvious, but important for performance
> >> > >
> >> > > I think this case fits (a) & (c), so I think its still worthwhile.
> But
> >> > its
> >> > > also worth asking whether or not its too difficult for a user to
> extend
> >> > > HadoopRDD right now.  There have been several cases in the past week
> >> > where
> >> > > we've suggested that a user should read from hdfs themselves (eg.,
> to
> >> > read
> >> > > multiple files together in one partition) -- with*out* reusing the
> code
> >> > in
> >> > > HadoopRDD, though they would lose things like the metric tracking &
> >> > > preferred locations you get from HadoopRDD.  Does HadoopRDD need to
> >> some
> >> > > refactoring to make that easier to do?  Or do we just need a good
> >> > example?
> >> > >
> >> > > Imran
> >> > >
> >> > > (sorry for hijacking your thread, Koert)
> >> > >
> >> > >
> >> > >
> >> > > On Mon, Mar 23, 2015 at 3:52 PM, Koert Kuipers <koert@tresata.com>
> >> > wrote:
> >> > >
> >> > > > see email below. reynold suggested i send it to dev instead of
> user
> >> > > >
> >> > > > ---------- Forwarded message ----------
> >> > > > From: Koert Kuipers <koert@tresata.com>
> >> > > > Date: Mon, Mar 23, 2015 at 4:36 PM
> >> > > > Subject: hadoop input/output format advanced control
> >> > > > To: "user@spark.apache.org" <user@spark.apache.org>
> >> > > >
> >> > > >
> >> > > > currently its pretty hard to control the Hadoop Input/Output
> formats
> >> > used
> >> > > > in Spark. The conventions seems to be to add extra parameters to
> all
> >> > > > methods and then somewhere deep inside the code (for example in
> >> > > > PairRDDFunctions.saveAsHadoopFile) all these parameters get
> >> translated
> >> > > into
> >> > > > settings on the Hadoop Configuration object.
> >> > > >
> >> > > > for example for compression i see "codec: Option[Class[_ <:
> >> > > > CompressionCodec]] = None" added to a bunch of methods.
> >> > > >
> >> > > > how scalable is this solution really?
> >> > > >
> >> > > > for example i need to read from a hadoop dataset and i dont want
> the
> >> > > input
> >> > > > (part) files to get split up. the way to do this is to set
> >> > > > "mapred.min.split.size". now i dont want to set this at the level
> of
> >> > the
> >> > > > SparkContext (which can be done), since i dont want it to apply to
> >> > input
> >> > > > formats in general. i want it to apply to just this one specific
> >> input
> >> > > > dataset i need to read. which leaves me with no options
> currently. i
> >> > > could
> >> > > > go add yet another input parameter to all the methods
> >> > > > (SparkContext.textFile, SparkContext.hadoopFile,
> >> > SparkContext.objectFile,
> >> > > > etc.). but that seems ineffective.
> >> > > >
> >> > > > why can we not expose a Map[String, String] or some other generic
> way
> >> > to
> >> > > > manipulate settings for hadoop input/output formats? it would
> require
> >> > > > adding one more parameter to all methods to deal with hadoop
> >> > input/output
> >> > > > formats, but after that its done. one parameter to rule them
> all....
> >> > > >
> >> > > > then i could do:
> >> > > > val x = sc.textFile("/some/path", formatSettings =
> >> > > > Map("mapred.min.split.size" -> "12345"))
> >> > > >
> >> > > > or
> >> > > > rdd.saveAsTextFile("/some/path, formatSettings =
> >> > > > Map(mapred.output.compress" -> "true",
> >> > "mapred.output.compression.codec"
> >> > > ->
> >> > > > "somecodec"))
> >> > > >
> >> > >
> >> >
> >>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--14dae9cc9552198f8b05122992d0--

From dev-return-12203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 05:56:27 2015
Return-Path: <dev-return-12203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6CB9917B59
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 05:56:27 +0000 (UTC)
Received: (qmail 18787 invoked by uid 500); 26 Mar 2015 05:56:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18699 invoked by uid 500); 26 Mar 2015 05:56:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18687 invoked by uid 99); 26 Mar 2015 05:56:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 05:56:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pllee@appier.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 05:56:00 +0000
Received: by qgh3 with SMTP id 3so67594493qgh.2
        for <dev@spark.apache.org>; Wed, 25 Mar 2015 22:54:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=appier.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Tcd8lsnNuDu78fPtJ+IYRcWFwJXC8lOo3u8IvRmcSfM=;
        b=RQ61Pi0vVKL4HjqVldaEXTfqK9Oug5srPtXV07d/h00nji0ArjmsEtK2mnQF7437Ee
         xQQFikHERzCuQE47d7KW1Eomb1cV2IlYWFDcHLZs29FVjmkAq7002Dk1ZRX+P4DHmTKE
         G5piTN7JNGz+I/YbY9yv4SGF100Bxm0f/3Uvg=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Tcd8lsnNuDu78fPtJ+IYRcWFwJXC8lOo3u8IvRmcSfM=;
        b=RB2a8DzNq/9kBSuljzsBLjguCUQWkipD35ce8R07YkAY/pvxA9WiAgf2L9HiUuRSxD
         qu8wY/EK2efokqUnNS6N8HyA/jJP+et0CpBFRGzt+8JqE/ej8mDI4azB0mZ92sC+5J6V
         ZHQ5XG5j3t2IFTWpSlrcTHMrbbBJG2NFRwKHTGp3+QqZ3vubisBLsxszu8F/gXI9onQL
         wRIlt6mKwUC16wxYc9n8cjU+oX3nVQ5pR3XOThsPVcO/cVsf8X9+Kg5f1HeTd7DQmwja
         96m/HXFz9UmRz6NYi0RvwtfXembvrHAarQIeCvxQVnQRdXqvOXbNepFU+sy+MTmxXeB+
         4WJQ==
X-Gm-Message-State: ALoCoQnigI13NfUxtOkYnaJi7JqDAuJKT5a8PIh0wbHoFyoutwvZZ5AU3YYMp3sfQ3Puc8D79uR6
MIME-Version: 1.0
X-Received: by 10.140.218.196 with SMTP id o187mr16852737qhb.30.1427349268752;
 Wed, 25 Mar 2015 22:54:28 -0700 (PDT)
Received: by 10.229.212.6 with HTTP; Wed, 25 Mar 2015 22:54:28 -0700 (PDT)
In-Reply-To: <CANrtgzXP-5DbN7TG_kWop4OqYJbvr7=ZY9ufo_7MevRpWtnQCA@mail.gmail.com>
References: <D110C7BA.1DBB7%mkim@palantir.com>
	<483876581.4513560.1424722604100.JavaMail.yahoo@mail.yahoo.com>
	<1508291672.8549572.1424726193790.JavaMail.yahoo@mail.yahoo.com>
	<CAH70K74N+hu2r5XEZOYFg0sDjurCZuuLvYeUvF8E9UyUw81GQg@mail.gmail.com>
	<CANrtgzUBgyF4WtZunieeKhPiL_rxwUEhHcQ8GDQ0EPvE4WGv1g@mail.gmail.com>
	<CANGvG8pQD8A84a6VOXXjo8VcGnXYBadYZr=oYAgdaQnJLY7Xnw@mail.gmail.com>
	<CANrtgzXP-5DbN7TG_kWop4OqYJbvr7=ZY9ufo_7MevRpWtnQCA@mail.gmail.com>
Date: Thu, 26 Mar 2015 13:54:28 +0800
Message-ID: <CANrtgzVyJAR95Wk8ezm9oS1V9_BcUizjLjS3Tx3Ta41ndVbSPQ@mail.gmail.com>
Subject: Re: Which OutputCommitter to use for S3?
From: Pei-Lun Lee <pllee@appier.com>
To: "user@spark.apache.org" <user@spark.apache.org>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1139c9ae22799905122aa699
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139c9ae22799905122aa699
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I updated the PR for SPARK-6352 to be more like SPARK-3595.
I added a new setting "spark.sql.parquet.output.committer.class" in hadoop
configuration to allow custom implementation of ParquetOutputCommitter.
Can someone take a look at the PR?

On Mon, Mar 16, 2015 at 5:23 PM, Pei-Lun Lee <pllee@appier.com> wrote:

> Hi,
>
> I created a JIRA and PR for supporting a s3 friendly output committer for
> saveAsParquetFile:
> https://issues.apache.org/jira/browse/SPARK-6352
> https://github.com/apache/spark/pull/5042
>
> My approach is add a DirectParquetOutputCommitter class in spark-sql
> package and use a boolean config variable
> spark.sql.parquet.useDirectParquetOutputCommitter to choose between defau=
lt
> output committer.
> This may not be the smartest solution but it works for me.
> Tested on spark 1.1, 1.3 with hadoop 1.0.4.
>
>
> On Thu, Mar 5, 2015 at 4:32 PM, Aaron Davidson <ilikerps@gmail.com> wrote=
:
>
>> Yes, unfortunately that direct dependency makes this injection much more
>> difficult for saveAsParquetFile.
>>
>> On Thu, Mar 5, 2015 at 12:28 AM, Pei-Lun Lee <pllee@appier.com> wrote:
>>
>>> Thanks for the DirectOutputCommitter example.
>>> However I found it only works for saveAsHadoopFile. What about
>>> saveAsParquetFile?
>>> It looks like SparkSQL is using ParquetOutputCommitter, which is subcla=
ss
>>> of FileOutputCommitter.
>>>
>>> On Fri, Feb 27, 2015 at 1:52 AM, Thomas Demoor <
>>> thomas.demoor@amplidata.com>
>>> wrote:
>>>
>>> > FYI. We're currently addressing this at the Hadoop level in
>>> > https://issues.apache.org/jira/browse/HADOOP-9565
>>> >
>>> >
>>> > Thomas Demoor
>>> >
>>> > On Mon, Feb 23, 2015 at 10:16 PM, Darin McBeath <
>>> > ddmcbeath@yahoo.com.invalid> wrote:
>>> >
>>> >> Just to close the loop in case anyone runs into the same problem I
>>> had.
>>> >>
>>> >> By setting --hadoop-major-version=3D2 when using the ec2 scripts,
>>> >> everything worked fine.
>>> >>
>>> >> Darin.
>>> >>
>>> >>
>>> >> ----- Original Message -----
>>> >> From: Darin McBeath <ddmcbeath@yahoo.com.INVALID>
>>> >> To: Mingyu Kim <mkim@palantir.com>; Aaron Davidson <
>>> ilikerps@gmail.com>
>>> >> Cc: "user@spark.apache.org" <user@spark.apache.org>
>>> >> Sent: Monday, February 23, 2015 3:16 PM
>>> >> Subject: Re: Which OutputCommitter to use for S3?
>>> >>
>>> >> Thanks.  I think my problem might actually be the other way around.
>>> >>
>>> >> I'm compiling with hadoop 2,  but when I startup Spark, using the ec=
2
>>> >> scripts, I don't specify a
>>> >> -hadoop-major-version and the default is 1.   I'm guessing that if I
>>> make
>>> >> that a 2 that it might work correctly.  I'll try it and post a
>>> response.
>>> >>
>>> >>
>>> >> ----- Original Message -----
>>> >> From: Mingyu Kim <mkim@palantir.com>
>>> >> To: Darin McBeath <ddmcbeath@yahoo.com>; Aaron Davidson <
>>> >> ilikerps@gmail.com>
>>> >> Cc: "user@spark.apache.org" <user@spark.apache.org>
>>> >> Sent: Monday, February 23, 2015 3:06 PM
>>> >> Subject: Re: Which OutputCommitter to use for S3?
>>> >>
>>> >> Cool, we will start from there. Thanks Aaron and Josh!
>>> >>
>>> >> Darin, it=C2=B9s likely because the DirectOutputCommitter is compile=
d with
>>> >> Hadoop 1 classes and you=C2=B9re running it with Hadoop 2.
>>> >> org.apache.hadoop.mapred.JobContext used to be a class in Hadoop 1,
>>> and it
>>> >> became an interface in Hadoop 2.
>>> >>
>>> >> Mingyu
>>> >>
>>> >>
>>> >>
>>> >>
>>> >>
>>> >> On 2/23/15, 11:52 AM, "Darin McBeath" <ddmcbeath@yahoo.com.INVALID>
>>> >> wrote:
>>> >>
>>> >> >Aaron.  Thanks for the class. Since I'm currently writing Java base=
d
>>> >> >Spark applications, I tried converting your class to Java (it seeme=
d
>>> >> >pretty straightforward).
>>> >> >
>>> >> >I set up the use of the class as follows:
>>> >> >
>>> >> >SparkConf conf =3D new SparkConf()
>>> >> >.set("spark.hadoop.mapred.output.committer.class",
>>> >> >"com.elsevier.common.DirectOutputCommitter");
>>> >> >
>>> >> >And I then try and save a file to S3 (which I believe should use th=
e
>>> old
>>> >> >hadoop apis).
>>> >> >
>>> >> >JavaPairRDD<Text, Text> newBaselineRDDWritable =3D
>>> >> >reducedhsfPairRDD.mapToPair(new ConvertToWritableTypes());
>>> >> >newBaselineRDDWritable.saveAsHadoopFile(baselineOutputBucketFile,
>>> >> >Text.class, Text.class, SequenceFileOutputFormat.class,
>>> >> >org.apache.hadoop.io.compress.GzipCodec.class);
>>> >> >
>>> >> >But, I get the following error message.
>>> >> >
>>> >> >Exception in thread "main" java.lang.IncompatibleClassChangeError:
>>> Found
>>> >> >class org.apache.hadoop.mapred.JobContext, but interface was expect=
ed
>>> >> >at
>>> >>
>>> >>
>>> >com.elsevier.common.DirectOutputCommitter.commitJob(DirectOutputCommit=
ter.
>>> >> >java:68)
>>> >> >at
>>> >>
>>> >org.apache.spark.SparkHadoopWriter.commitJob(SparkHadoopWriter.scala:1=
27)
>>> >> >at
>>> >>
>>> >>
>>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunct=
ions
>>> >> >.scala:1075)
>>> >> >at
>>> >>
>>> >>
>>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunction=
s.sc
>>> >> >ala:940)
>>> >> >at
>>> >>
>>> >>
>>> >org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunction=
s.sc
>>> >> >ala:902)
>>> >> >at
>>> >>
>>> >>
>>> >org.apache.spark.api.java.JavaPairRDD.saveAsHadoopFile(JavaPairRDD.sca=
la:7
>>> >> >71)
>>> >> >at com.elsevier.spark.SparkSyncDedup.main(SparkSyncDedup.java:156)
>>> >> >
>>> >> >In my class, JobContext is an interface of  type
>>> >> >org.apache.hadoop.mapred.JobContext.
>>> >> >
>>> >> >Is there something obvious that I might be doing wrong (or messed u=
p
>>> in
>>> >> >the translation from Scala to Java) or something I should look
>>> into?  I'm
>>> >> >using Spark 1.2 with hadoop 2.4.
>>> >> >
>>> >> >
>>> >> >Thanks.
>>> >> >
>>> >> >Darin.
>>> >> >
>>> >> >
>>> >> >________________________________
>>> >> >
>>> >> >
>>> >> >From: Aaron Davidson <ilikerps@gmail.com>
>>> >> >To: Andrew Ash <andrew@andrewash.com>
>>> >> >Cc: Josh Rosen <rosenville@gmail.com>; Mingyu Kim <mkim@palantir.co=
m
>>> >;
>>> >> >"user@spark.apache.org" <user@spark.apache.org>; Aaron Davidson
>>> >> ><aaron@databricks.com>
>>> >> >Sent: Saturday, February 21, 2015 7:01 PM
>>> >> >Subject: Re: Which OutputCommitter to use for S3?
>>> >> >
>>> >> >
>>> >> >
>>> >> >Here is the class:
>>> >> >
>>> >>
>>> https://urldefense.proofpoint.com/v2/url?u=3Dhttps-3A__gist.github.com_=
aaron
>>> >>
>>> >>
>>> >dav_c513916e72101bbe14ec&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXB=
rZ4tFb6o
>>> >>
>>> >>
>>> >Onmz8&r=3DennQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmu=
KZRf6sFs
>>> >>
>>> >zOvl_-ZnxmkBPHo1K24TfGE&s=3DcwSCPKlJO-BJcz4UcGck3xOE2N-4V3eoNvgtFCdMLP=
8&e=3D
>>> >> >
>>> >> >You can use it by setting "mapred.output.committer.class" in the
>>> Hadoop
>>> >> >configuration (or "spark.hadoop.mapred.output.committer.class" in t=
he
>>> >> >Spark configuration). Note that this only works for the old Hadoop
>>> APIs,
>>> >> >I believe the new Hadoop APIs strongly tie committer to input forma=
t
>>> (so
>>> >> >FileInputFormat always uses FileOutputCommitter), which makes this
>>> fix
>>> >> >more difficult to apply.
>>> >> >
>>> >> >
>>> >> >
>>> >> >
>>> >> >On Sat, Feb 21, 2015 at 12:12 PM, Andrew Ash <andrew@andrewash.com>
>>> >> wrote:
>>> >> >
>>> >> >Josh is that class something you guys would consider open sourcing,
>>> or
>>> >> >would you rather the community step up and create an OutputCommitte=
r
>>> >> >implementation optimized for S3?
>>> >> >>
>>> >> >>
>>> >> >>On Fri, Feb 20, 2015 at 4:02 PM, Josh Rosen <rosenville@gmail.com>
>>> >> wrote:
>>> >> >>
>>> >> >>We (Databricks) use our own DirectOutputCommitter implementation,
>>> which
>>> >> >>is a couple tens of lines of Scala code.  The class would almost
>>> >> >>entirely be a no-op except we took some care to properly handle th=
e
>>> >> >>_SUCCESS file.
>>> >> >>>
>>> >> >>>
>>> >> >>>On Fri, Feb 20, 2015 at 3:52 PM, Mingyu Kim <mkim@palantir.com>
>>> wrote:
>>> >> >>>
>>> >> >>>I didn=C2=B9t get any response. It=C2=B9d be really appreciated i=
f anyone
>>> using a
>>> >> >>>special OutputCommitter for S3 can comment on this!
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>Thanks,
>>> >> >>>>Mingyu
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>From: Mingyu Kim <mkim@palantir.com>
>>> >> >>>>Date: Monday, February 16, 2015 at 1:15 AM
>>> >> >>>>To: "user@spark.apache.org" <user@spark.apache.org>
>>> >> >>>>Subject: Which OutputCommitter to use for S3?
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>HI all,
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>The default OutputCommitter used by RDD, which is
>>> FileOutputCommitter,
>>> >> >>>>seems to require moving files at the commit step, which is not a
>>> >> >>>>constant operation in S3, as discussed in
>>> >> >>>>
>>> >>
>>> https://urldefense.proofpoint.com/v2/url?u=3Dhttp-3A__mail-2Darchives.a=
pa
>>> >>
>>> >>
>>> >>>>che.org_mod-5Fmbox_spark-2Duser_201410.mbox_-253C543E33FA.2000802-4=
0ent
>>> >>
>>> >>
>>> >>>>ropy.be-253E&d=3DAwIFaQ&c=3Dizlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oO=
nmz8&r=3De
>>> >>
>>> >>
>>> >>>>nnQJq47pNnObsDh-88a9YUrUulcYQoV8giPASqXB84&m=3D_2YAVrYZtQmuKZRf6sFs=
zOvl_-
>>> >>
>>> >>>>ZnxmkBPHo1K24TfGE&s=3DEQOZaHRANJupdjXCfHSXL2t5BZ9YgMt2pRc3pht4o7o&e=
=3D .
>>> >>
>>> >> >>>>People seem to develop their own NullOutputCommitter
>>> implementation or
>>> >> >>>>use DirectFileOutputCommitter (as mentioned in SPARK-3595), but =
I
>>> >> >>>>wanted to check if there is a de facto standard, publicly
>>> available
>>> >> >>>>OutputCommitter to use for S3 in conjunction with Spark.
>>> >> >>>>
>>> >> >>>>
>>> >> >>>>Thanks,
>>> >> >>>>Mingyu
>>> >> >>>
>>> >> >>
>>> >> >
>>> >> >-------------------------------------------------------------------=
--
>>> >> >To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>> >> >For additional commands, e-mail: user-help@spark.apache.org
>>> >>
>>> >> >
>>> >>
>>> >> --------------------------------------------------------------------=
-
>>> >> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>> >> For additional commands, e-mail: user-help@spark.apache.org
>>> >>
>>> >> --------------------------------------------------------------------=
-
>>> >> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>> >> For additional commands, e-mail: user-help@spark.apache.org
>>> >>
>>> >>
>>> >
>>>
>>
>>
>

--001a1139c9ae22799905122aa699--

From dev-return-12204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 07:38:32 2015
Return-Path: <dev-return-12204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1BB0F105B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 07:38:32 +0000 (UTC)
Received: (qmail 12618 invoked by uid 500); 26 Mar 2015 07:38:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12489 invoked by uid 500); 26 Mar 2015 07:38:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11698 invoked by uid 99); 26 Mar 2015 07:38:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 07:38:13 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [203.81.22.165] (HELO mail1.qilinsoft.com) (203.81.22.165)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 07:37:47 +0000
X-MimeOLE: Produced By Microsoft Exchange V6.5
Content-class: urn:content-classes:message
MIME-Version: 1.0
Content-Type: text/plain;
	charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
Subject: Can I call aggregate UDF in DataFrame?
Date: Thu, 26 Mar 2015 15:37:17 +0800
Message-ID: <2EB23AF5EEEA2140946B8F292EB2EB9F1ADB78@QS-PEK-DC1.qilinsoftcorp.qilinsoft.com>
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
Thread-Topic: Can I call aggregate UDF in DataFrame?
Thread-Index: AdBnl6+ktw04eLe+RhaaZkFpK88vlA==
From: "Haopu Wang" <HWang@qilinsoft.com>
To: "user" <user@spark.apache.org>,
	<dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

Specifically there are only 5 aggregate functions in class
org.apache.spark.sql.GroupedData: sum/max/min/mean/count.

Can I plugin a function to calculate stddev?

Thank you!


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 07:45:28 2015
Return-Path: <dev-return-12205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2AA291067C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 07:45:28 +0000 (UTC)
Received: (qmail 32025 invoked by uid 500); 26 Mar 2015 07:45:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31949 invoked by uid 500); 26 Mar 2015 07:45:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31934 invoked by uid 99); 26 Mar 2015 07:45:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 07:45:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sam.halliday@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 07:44:35 +0000
Received: by igcxg11 with SMTP id xg11so47004300igc.0
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 00:43:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gBz9mwxyQQWgZGVtdwdXsOBALHGWmRdYUNeyBkmQPXk=;
        b=xp56QmtB0ObgufRXHZ7ph+SO0VNwP4iBPgBQZ/vAdI3ID8d7rGptsyD7vf3gwtAZpN
         cCVPaTqckARjeub0zM413shlrbxB6hduuK8a6xH1NpJJlQZ05XXEPq0PgAedOiQR5NeC
         J3eiq9IR5fPCrTjYE84n6BzwdpDw91xKGD9q2RZw0TrDXcHe+eg9xgCuBr1XWk0PZCPB
         /nKEQvXFNiteUIB4HldlM1C0Plq5waHtsjvtsvxr+frkMWAJvtx3Yaz29F0SKVyqJc3r
         rWMhWYz3pvdPfAGClkQotF8sZVmgLirDbS1dCw7C6t6NsDGeyoHsrXXokbUAAb9M9nrd
         uHoQ==
MIME-Version: 1.0
X-Received: by 10.50.253.12 with SMTP id zw12mr34506415igc.24.1427355782824;
 Thu, 26 Mar 2015 00:43:02 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Thu, 26 Mar 2015 00:43:02 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Thu, 26 Mar 2015 00:43:02 -0700 (PDT)
In-Reply-To: <CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
	<CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
Date: Thu, 26 Mar 2015 07:43:02 +0000
Message-ID: <CALR_T9AtsmcXd8oO8yDohKmkUpZLREX3_MBhXqBiQUvrcZYe2w@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Sam Halliday <sam.halliday@gmail.com>
To: "Evan R. Sparks" <evan.sparks@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org, 
	Joseph Bradley <joseph@databricks.com>, "Ulanov, Alexander" <alexander.ulanov@hp.com>, 
	jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=001a11346e3c674e1b05122c2a86
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11346e3c674e1b05122c2a86
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm not at all surprised ;-) I fully expect the GPU performance to get
better automatically as the hardware improves.

Netlib natives still need to be shipped separately. I'd also oppose any
move to make Open BLAS the default - is not always better and I think
natives really need DevOps buy-in. It's not the right solution for
everybody.
On 26 Mar 2015 01:23, "Evan R. Sparks" <evan.sparks@gmail.com> wrote:

> Yeah, much more reasonable - nice to know that we can get full GPU
> performance from breeze/netlib-java - meaning there's no compelling
> performance reason to switch out our current linear algebra library (at
> least as far as this benchmark is concerned).
>
> Instead, it looks like a user guide for configuring Spark/MLlib to use th=
e
> right BLAS library will get us most of the way there. Or, would it make
> sense to finally ship openblas compiled for some common platforms (64-bit
> linux, windows, mac) directly with Spark - hopefully eliminating the jbla=
s
> warnings once and for all for most users? (Licensing is BSD) Or am I
> missing something?
>
> On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
>
>> As everyone suggested, the results were too good to be true, so I
>> double-checked them. It turns that nvblas did not do multiplication due =
to
>> parameter NVBLAS_TILE_DIM from "nvblas.conf" and returned zero matrix. M=
y
>> previously posted results with nvblas are matrices copying only. The
>> default NVBLAS_TILE_DIM=3D=3D2048 is too big for my graphic card/matrix =
size. I
>> handpicked other values that worked. As a result, netlib+nvblas is on pa=
r
>> with BIDMat-cuda. As promised, I am going to post a how-to for nvblas
>> configuration.
>>
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>>
>>
>> -----Original Message-----
>> From: Ulanov, Alexander
>> Sent: Wednesday, March 25, 2015 2:31 PM
>> To: Sam Halliday
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks;
>> jfcanny
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>> Hi again,
>>
>> I finally managed to use nvblas within Spark+netlib-java. It has
>> exceptional performance for big matrices with Double, faster than
>> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
>> to/from GPU, OpenBlas or MKL might be a better choice. This correlates w=
ith
>> original nvblas presentation on GPU conf 2013 (slide 21):
>> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108=
-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>>
>> My results:
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Just in case, these tests are not for generalization of performance of
>> different libraries. I just want to pick a library that does at best den=
se
>> matrices multiplication for my task.
>>
>> P.S. My previous issue with nvblas was the following: it has Fortran bla=
s
>> functions, at the same time netlib-java uses C cblas functions. So, one
>> needs cblas shared library to use nvblas through netlib-java. Fedora doe=
s
>> not have cblas (but Debian and Ubuntu have), so I needed to compile it. =
I
>> could not use cblas from Atlas or Openblas because they link to their
>> implementation and not to Fortran blas.
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Ulanov, Alexander
>> Sent: Tuesday, March 24, 2015 6:57 PM
>> To: Sam Halliday
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>> Hi,
>>
>> I am trying to use nvblas with netlib-java from Spark. nvblas functions
>> should replace current blas functions calls after executing LD_PRELOAD a=
s
>> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
>> changes to netlib-java. It seems to work for simple Java example, but I
>> cannot make it work with Spark. I run the following:
>> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
>> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
>> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>>
>> +-----------------------------------------------------------------------=
------+
>> | Processes:                                                       GPU
>> Memory |
>> |  GPU       PID  Type  Process name                               Usage
>>     |
>>
>> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D|
>> |    0      8873    C   bash
>> 39MiB |
>> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
>> 39MiB |
>>
>> +-----------------------------------------------------------------------=
------+
>>
>> In Spark shell I do matrix multiplication and see the following:
>> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
>> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>> So I am sure that netlib-native is loaded and cblas supposedly used.
>> However, matrix multiplication does executes on CPU since I see 16% of C=
PU
>> used and 0% of GPU used. I also checked different matrix sizes, from
>> 100x100 to 12000x12000
>>
>> Could you suggest might the LD_PRELOAD not affect Spark shell?
>>
>> Best regards, Alexander
>>
>>
>>
>> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>> Sent: Monday, March 09, 2015 6:01 PM
>> To: Ulanov, Alexander
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>>
>> Thanks so much for following up on this!
>>
>> Hmm, I wonder if we should have a concerted effort to chart performance
>> on various pieces of hardware...
>> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto=
:
>> alexander.ulanov@hp.com>> wrote:
>> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
>> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
>> support of Double in the current source code), did the test with BIDMat =
and
>> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>>
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
>> sam.halliday@gmail.com>]
>> Sent: Tuesday, March 03, 2015 1:54 PM
>> To: Xiangrui Meng; Joseph Bradley
>> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
>> dev@spark.apache.org>
>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>
>> BTW, is anybody on this list going to the London Meetup in a few weeks?
>>
>>
>> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapre=
duce-world#community
>>
>> Would be nice to meet other people working on the guts of Spark! :-)
>>
>>
>> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>>
>> > Hey Alexander,
>> >
>> > I don't quite understand the part where netlib-cublas is about 20x
>> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
>> > with netlib-java?
>> >
>> > CC'ed Sam, the author of netlib-java.
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
>> <mailto:joseph@databricks.com>> wrote:
>> >> Better documentation for linking would be very helpful!  Here's a JIR=
A:
>> >> https://issues.apache.org/jira/browse/SPARK-6019
>> >>
>> >>
>> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>> >> wrote:
>> >>
>> >>> Thanks for compiling all the data and running these benchmarks,
>> >>> Alex. The big takeaways here can be seen with this chart:
>> >>>
>> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>> >>>
>> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>> >>> BIDMat+GPU) can provide substantial (but less than an order of
>> >>> BIDMat+magnitude)
>> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>> >>> netlib-java+openblas-compiled).
>> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>> >>> worse than a well-tuned CPU implementation, particularly for larger
>> matrices.
>> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>> >>> basically agrees with the authors own benchmarks (
>> >>> https://github.com/fommil/netlib-java)
>> >>>
>> >>> I think that most of our users are in a situation where using GPUs
>> >>> may not be practical - although we could consider having a good GPU
>> >>> backend available as an option. However, *ALL* users of MLlib could
>> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
>> >>> BLAS implementation. Perhaps we should consider updating the mllib
>> >>> guide with a more complete section for enabling high performance
>> >>> binaries on OSX and Linux? Or better, figure out a way for the
>> >>> system to fetch these automatically.
>> >>>
>> >>> - Evan
>> >>>
>> >>>
>> >>>
>> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>> >>>
>> >>>> Just to summarize this thread, I was finally able to make all
>> >>>> performance comparisons that we discussed. It turns out that:
>> >>>> BIDMat-cublas>>BIDMat
>> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-y=
um-repo=3D
>> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
>> >>>>
>> >>>> Below is the link to the spreadsheet with full results.
>> >>>>
>> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
>> >>>>
>> >>>> One thing still needs exploration: does BIDMat-cublas perform
>> >>>> copying to/from machine=E2=80=99s RAM?
>> >>>>
>> >>>> -----Original Message-----
>> >>>> From: Ulanov, Alexander
>> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>> >>>> To: Evan R. Sparks
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>> >>>> the original one discusses slightly different topic. I was able to
>> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>> >>>> statically linked inside a 60MB library.
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>> >>>> |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>> >>>> 1569,233228 |
>> >>>>
>> >>>> It turn out that pre-compiled MKL is faster than precompiled
>> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns=
 with
>> >>>> locally compiled openblas and cuda.
>> >>>>
>> >>>> Alexander
>> >>>>
>> >>>> From: Evan R. Sparks
>> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>> >>>> Sent: Monday, February 09, 2015 6:06 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Great - perhaps we can move this discussion off-list and onto a
>> >>>> JIRA ticket? (Here's one:
>> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>> >>>>
>> >>>> It seems like this is going to be somewhat exploratory for a while
>> >>>> (and there's probably only a handful of us who really care about
>> >>>> fast linear
>> >>>> algebra!)
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for explanation and useful link. I am going to build
>> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>> >>>>
>> >>>> Do I understand correctly that BIDMat binaries contain statically
>> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
>> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
>> >>>> it seems that in my case precompiled MKL BLAS performs better than
>> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
>> to be on par with JNI overheads.
>> >>>>
>> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
>> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>> >>>> Halliday
>> >>>> (Netlib-java) interested to compare their libraries.
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:58 PM
>> >>>>
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
>> >>>> from getting cache sizes, etc. set up correctly for your particular
>> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>> >>>> quickly and yields performance competitive with MKL.
>> >>>>
>> >>>> To make sure the right library is getting used, you have to make
>> >>>> sure it's first on the search path - export
>> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
>> >>>>
>> >>>> For some examples of getting netlib-java setup on an ec2 node and
>> >>>> some example benchmarking code we ran a while back, see:
>> >>>> https://github.com/shivaram/matrix-bench
>> >>>>
>> >>>> In particular - build-openblas-ec2.sh shows you how to build the
>> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>> >>>> shows you how to get the path setup and get that library picked up
>> by netlib-java.
>> >>>>
>> >>>> In this way - you could probably get cuBLAS set up to be used by
>> >>>> netlib-java as well.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
>> >>>> force loading the right blas? For netlib, I there are few JVM
>> >>>> flags, such as
>> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
>> >>>> so I can force it to use Java implementation. Not sure I understand
>> how to force use a specific blas (not specific wrapper for blas).
>> >>>>
>> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
>> >>>> that netlib is using it.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:19 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Getting breeze to pick up the right blas library is critical for
>> >>>> performance. I recommend using OpenBLAS (or MKL, if you already hav=
e
>> it).
>> >>>> It might make sense to force BIDMat to use the same underlying BLAS
>> >>>> library as well.
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan, Joseph
>> >>>>
>> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>> >>>> |native_system_linux_x86-64|
>> >>>> Breeze+Netlib-java f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>> >>>> ||
>> >>>>
>> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
>> >>>> 19 Linux, Scala 2.11.
>> >>>>
>> >>>> Later I will make tests with Cuda. I need to install new Cuda
>> >>>> version for this purpose.
>> >>>>
>> >>>> Do you have any ideas why breeze-netlib with native blas is so much
>> >>>> slower than BIDMat MKL?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>> joseph@databricks.com><mailto:
>> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>> >>>> Sent: Thursday, February 05, 2015 5:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Evan R. Sparks;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Hi Alexander,
>> >>>>
>> >>>> Using GPUs with Spark would be very exciting.  Small comment:
>> >>>> Concerning your question earlier about keeping data stored on the
>> >>>> GPU rather than having to move it between main memory and GPU
>> >>>> memory on each iteration, I would guess this would be critical to
>> >>>> getting good performance.  If you could do multiple local
>> >>>> iterations before aggregating results, then the cost of data
>> >>>> movement to the GPU could be amortized (and I believe that is done
>> >>>> in practice).  Having Spark be aware of the GPU and using it as
>> another part of memory sounds like a much bigger undertaking.
>> >>>>
>> >>>> Joseph
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach present=
ation by
>> >>>> John Canny and I am really inspired by his talk and comparisons wit=
h
>> Spark MLlib.
>> >>>>
>> >>>> I am very interested to find out what will be better within Spark:
>> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
>> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=
=80=9D test of
>> >>>> linear algebra, it involves some other things that are essential to
>> machine learning.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Thursday, February 05, 2015 1:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
>> >>>> netlib-java+data
>> >>>> layout and fewer levels of indirection - it's definitely a
>> >>>> worthwhile experiment to run. The main speedups I've seen from
>> >>>> using it come from highly optimized GPU code for linear algebra. I
>> >>>> know that in the past Canny has gone as far as to write custom GPU
>> >>>> kernels for performance-critical regions of code.[1]
>> >>>>
>> >>>> BIDMach is highly optimized for single node performance or
>> >>>> performance on small clusters.[2] Once data doesn't fit easily in
>> >>>> GPU memory (or can be batched in that way) the performance tends to
>> >>>> fall off. Canny argues for hardware/software codesign and as such
>> >>>> prefers machine configurations that are quite different than what
>> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and
>> 4 GPUs.
>> >>>>
>> >>>> In contrast, MLlib was designed for horizontal scalability on
>> >>>> commodity clusters and works best on very big datasets - order of
>> terabytes.
>> >>>>
>> >>>> For the most part, these projects developed concurrently to address
>> >>>> slightly different use cases. That said, there may be bits of
>> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>> >>>> careful about maintaining cross-language compatibility for our Java
>> >>>> and Python-users, though.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>> >>>> you know what makes them faster than netlib-java?
>> >>>>
>> >>>> The same group has BIDMach library that implements machine
>> >>>> learning. For some examples they use Caffe convolutional neural
>> >>>> network library owned by another group in Berkeley. Could you
>> >>>> elaborate on how these all might be connected with Spark Mllib? If
>> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMa=
ch for
>> optimization and learning?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>> >>>> Sent: Thursday, February 05, 2015 12:09 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>> >>>> blas in many cases.
>> >>>>
>> >>>> You might consider taking a look at the codepaths that BIDMat (
>> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>> >>>> optimizing to make this work really fast from Scala. I've run it on
>> >>>> my laptop and compared to MKL and in certain cases it's 10x faster
>> at matrix multiply.
>> >>>> There are a lot of layers of indirection here and you really want
>> >>>> to avoid data copying as much as possible.
>> >>>>
>> >>>> We could also consider swapping out BIDMat for Breeze, but that
>> >>>> would be a big project and if we can figure out how to get
>> >>>> breeze+cublas to comparable performance that would be a big win.
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Dear Spark developers,
>> >>>>
>> >>>> I am exploring how to make linear algebra operations faster within
>> Spark.
>> >>>> One way of doing this is to use Scala Breeze library that is
>> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
>> >>>> and LAPACK native binaries if they are available on the worker
>> >>>> node. It also has its own optimized Java implementation of BLAS. It
>> >>>> is worth mentioning, that native binaries provide better performanc=
e
>> only for BLAS level 3, i.e.
>> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>> >>>> This is confirmed by GEMM test on Netlib-java page
>> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>> >>>> experiments with training of artificial neural network
>> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>> >>>> However, I would like to boost performance more.
>> >>>>
>> >>>> GPU is supposed to work fast with linear algebra and there is
>> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
>> >>>> server with Nvidia GPU and I was able to do the following. I linked
>> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
>> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>> >>>> performance measurements with regards to artificial neural network
>> >>>> batch learning in Spark MLlib that involves matrix-matrix
>> >>>> multiplications. It turns out that for matrices of size less than
>> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
>> >>>> slower for bigger matrices. It worth mentioning that it is was not =
a
>> test for ONLY multiplication since there are other operations involved.
>> >>>> One of the reasons for slowdown might be the overhead of copying
>> >>>> the matrices from computer memory to graphic card memory and back.
>> >>>>
>> >>>> So, few questions:
>> >>>> 1) Do these results with CUDA make sense?
>> >>>> 2) If the problem is with copy overhead, are there any libraries
>> >>>> that allow to force intermediate results to stay in graphic card
>> >>>> memory thus removing the overhead?
>> >>>> 3) Any other options to speed-up linear algebra in Spark?
>> >>>>
>> >>>> Thank you, Alexander
>> >>>>
>> >>>> -------------------------------------------------------------------
>> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
>> dev-unsubscribe@spark.apache.org><mailto:
>> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
>> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
>> >>>> ark.apac> he.org<http://he.org>
>> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
>> >>>> rk.apache.org>>> For additional commands, e-mail:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto=
:
>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>
>>
>> --
>> Best regards,
>> Sam
>>
>
>

--001a11346e3c674e1b05122c2a86--

From dev-return-12206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 07:55:14 2015
Return-Path: <dev-return-12206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 16C8F107E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 07:55:14 +0000 (UTC)
Received: (qmail 54568 invoked by uid 500); 26 Mar 2015 07:55:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54491 invoked by uid 500); 26 Mar 2015 07:55:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54477 invoked by uid 99); 26 Mar 2015 07:55:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 07:55:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sam.halliday@gmail.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 07:55:08 +0000
Received: by igbud6 with SMTP id ud6so123231521igb.1
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 00:54:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=1gBQt2RAgq+SE32FaHVOtT69e+t9YXRRM+u+F8a1cGw=;
        b=vVDqD94/vb2MgKbn9G4DUezqHDcfotMZniMgUxAGR7kcf+wVHSiTh8I8GYL+aymuUk
         94pYgAsgVRDA7i8sKmGrxmyp8sUt9MokpwEwUzuL4xYDmJYGZNFYaVsjkeObSGZ4TJ76
         MCPUkHfYPsznjxAJQWSdZlVuCilKxUOdE4W/VNNJTUc/YVzbFEM0wWVjCBV6dnId45Ss
         URq8zoC6wI5Wru41g2NaaQacvsloJSbdjLx6wcBMk+E5fGN39gihirzNQFxDh00OAQjn
         B5Mw3ikeeIfO1GvDP8eTkiprI91ILwhbGkZR+RHpBk2n8T6n07laMp2bJI8Y7sJnEDZk
         0rKQ==
MIME-Version: 1.0
X-Received: by 10.43.13.71 with SMTP id pl7mr37496672icb.31.1427356487623;
 Thu, 26 Mar 2015 00:54:47 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Thu, 26 Mar 2015 00:54:47 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Thu, 26 Mar 2015 00:54:47 -0700 (PDT)
In-Reply-To: <CALR_T9AtsmcXd8oO8yDohKmkUpZLREX3_MBhXqBiQUvrcZYe2w@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
	<CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
	<CALR_T9AtsmcXd8oO8yDohKmkUpZLREX3_MBhXqBiQUvrcZYe2w@mail.gmail.com>
Date: Thu, 26 Mar 2015 07:54:47 +0000
Message-ID: <CALR_T9C4mBOy0T6_K9+RQZtoeNGvhZOtmvz=ij0fhyO8SDbKUw@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Sam Halliday <sam.halliday@gmail.com>
To: "Evan R. Sparks" <evan.sparks@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, dev@spark.apache.org, 
	"Ulanov, Alexander" <alexander.ulanov@hp.com>, jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=bcaec511e11669ac9305122c5459
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec511e11669ac9305122c5459
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Btw, OpenBLAS requires GPL runtime binaries which are typically considered
"system libraries" (and these fall under something similar to the Java
classpath exception rule)... so it's basically impossible to distribute
OpenBLAS the way you're suggesting, sorry. Indeed, there is work ongoing in
Spark right now to clear up something of this nature.

On a more technical level, I'd recommend watching my talk at ScalaX which
explains in detail why high performance only comes from machine optimised
binaries, which requires DevOps buy-in (and, I'd recommend using MKL anyway
on the CPU, not OpenBLAS).

On an even deeper level, using natives has consequences to JIT and GC which
isn't suitable for everybody and we'd really like people to go into that
with their eyes wide open.
On 26 Mar 2015 07:43, "Sam Halliday" <sam.halliday@gmail.com> wrote:

> I'm not at all surprised ;-) I fully expect the GPU performance to get
> better automatically as the hardware improves.
>
> Netlib natives still need to be shipped separately. I'd also oppose any
> move to make Open BLAS the default - is not always better and I think
> natives really need DevOps buy-in. It's not the right solution for
> everybody.
> On 26 Mar 2015 01:23, "Evan R. Sparks" <evan.sparks@gmail.com> wrote:
>
>> Yeah, much more reasonable - nice to know that we can get full GPU
>> performance from breeze/netlib-java - meaning there's no compelling
>> performance reason to switch out our current linear algebra library (at
>> least as far as this benchmark is concerned).
>>
>> Instead, it looks like a user guide for configuring Spark/MLlib to use
>> the right BLAS library will get us most of the way there. Or, would it m=
ake
>> sense to finally ship openblas compiled for some common platforms (64-bi=
t
>> linux, windows, mac) directly with Spark - hopefully eliminating the jbl=
as
>> warnings once and for all for most users? (Licensing is BSD) Or am I
>> missing something?
>>
>> On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <
>> alexander.ulanov@hp.com> wrote:
>>
>>> As everyone suggested, the results were too good to be true, so I
>>> double-checked them. It turns that nvblas did not do multiplication due=
 to
>>> parameter NVBLAS_TILE_DIM from "nvblas.conf" and returned zero matrix. =
My
>>> previously posted results with nvblas are matrices copying only. The
>>> default NVBLAS_TILE_DIM=3D=3D2048 is too big for my graphic card/matrix=
 size. I
>>> handpicked other values that worked. As a result, netlib+nvblas is on p=
ar
>>> with BIDMat-cuda. As promised, I am going to post a how-to for nvblas
>>> configuration.
>>>
>>>
>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T=
9J5r7kwKSPkY/edit?usp=3Dsharing
>>>
>>>
>>>
>>> -----Original Message-----
>>> From: Ulanov, Alexander
>>> Sent: Wednesday, March 25, 2015 2:31 PM
>>> To: Sam Halliday
>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R.
>>> Sparks; jfcanny
>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>
>>> Hi again,
>>>
>>> I finally managed to use nvblas within Spark+netlib-java. It has
>>> exceptional performance for big matrices with Double, faster than
>>> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
>>> to/from GPU, OpenBlas or MKL might be a better choice. This correlates =
with
>>> original nvblas presentation on GPU conf 2013 (slide 21):
>>> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC310=
8-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>>>
>>> My results:
>>>
>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T=
9J5r7kwKSPkY/edit?usp=3Dsharing
>>>
>>> Just in case, these tests are not for generalization of performance of
>>> different libraries. I just want to pick a library that does at best de=
nse
>>> matrices multiplication for my task.
>>>
>>> P.S. My previous issue with nvblas was the following: it has Fortran
>>> blas functions, at the same time netlib-java uses C cblas functions. So=
,
>>> one needs cblas shared library to use nvblas through netlib-java. Fedor=
a
>>> does not have cblas (but Debian and Ubuntu have), so I needed to compil=
e
>>> it. I could not use cblas from Atlas or Openblas because they link to t=
heir
>>> implementation and not to Fortran blas.
>>>
>>> Best regards, Alexander
>>>
>>> -----Original Message-----
>>> From: Ulanov, Alexander
>>> Sent: Tuesday, March 24, 2015 6:57 PM
>>> To: Sam Halliday
>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>
>>> Hi,
>>>
>>> I am trying to use nvblas with netlib-java from Spark. nvblas functions
>>> should replace current blas functions calls after executing LD_PRELOAD =
as
>>> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
>>> changes to netlib-java. It seems to work for simple Java example, but I
>>> cannot make it work with Spark. I run the following:
>>> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
>>> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
>>> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>>>
>>> +----------------------------------------------------------------------=
-------+
>>> | Processes:                                                       GPU
>>> Memory |
>>> |  GPU       PID  Type  Process name
>>>  Usage      |
>>>
>>> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D|
>>> |    0      8873    C   bash
>>> 39MiB |
>>> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
>>> 39MiB |
>>>
>>> +----------------------------------------------------------------------=
-------+
>>>
>>> In Spark shell I do matrix multiplication and see the following:
>>> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
>>> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>>> So I am sure that netlib-native is loaded and cblas supposedly used.
>>> However, matrix multiplication does executes on CPU since I see 16% of =
CPU
>>> used and 0% of GPU used. I also checked different matrix sizes, from
>>> 100x100 to 12000x12000
>>>
>>> Could you suggest might the LD_PRELOAD not affect Spark shell?
>>>
>>> Best regards, Alexander
>>>
>>>
>>>
>>> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>>> Sent: Monday, March 09, 2015 6:01 PM
>>> To: Ulanov, Alexander
>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>
>>>
>>> Thanks so much for following up on this!
>>>
>>> Hmm, I wonder if we should have a concerted effort to chart performance
>>> on various pieces of hardware...
>>> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com
>>> <mailto:alexander.ulanov@hp.com>> wrote:
>>> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added th=
e
>>> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see th=
e
>>> support of Double in the current source code), did the test with BIDMat=
 and
>>> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>>>
>>>
>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T=
9J5r7kwKSPkY/edit?usp=3Dsharing
>>>
>>> Best regards, Alexander
>>>
>>> -----Original Message-----
>>> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
>>> sam.halliday@gmail.com>]
>>> Sent: Tuesday, March 03, 2015 1:54 PM
>>> To: Xiangrui Meng; Joseph Bradley
>>> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
>>> dev@spark.apache.org>
>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>
>>> BTW, is anybody on this list going to the London Meetup in a few weeks?
>>>
>>>
>>> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapr=
educe-world#community
>>>
>>> Would be nice to meet other people working on the guts of Spark! :-)
>>>
>>>
>>> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>>>
>>> > Hey Alexander,
>>> >
>>> > I don't quite understand the part where netlib-cublas is about 20x
>>> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
>>> > with netlib-java?
>>> >
>>> > CC'ed Sam, the author of netlib-java.
>>> >
>>> > Best,
>>> > Xiangrui
>>> >
>>> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.co=
m
>>> <mailto:joseph@databricks.com>> wrote:
>>> >> Better documentation for linking would be very helpful!  Here's a
>>> JIRA:
>>> >> https://issues.apache.org/jira/browse/SPARK-6019
>>> >>
>>> >>
>>> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>>> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>>> >> wrote:
>>> >>
>>> >>> Thanks for compiling all the data and running these benchmarks,
>>> >>> Alex. The big takeaways here can be seen with this chart:
>>> >>>
>>> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50u=
Z
>>> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>>> >>>
>>> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>>> >>> BIDMat+GPU) can provide substantial (but less than an order of
>>> >>> BIDMat+magnitude)
>>> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>>> >>> netlib-java+openblas-compiled).
>>> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>>> >>> worse than a well-tuned CPU implementation, particularly for larger
>>> matrices.
>>> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>>> >>> basically agrees with the authors own benchmarks (
>>> >>> https://github.com/fommil/netlib-java)
>>> >>>
>>> >>> I think that most of our users are in a situation where using GPUs
>>> >>> may not be practical - although we could consider having a good GPU
>>> >>> backend available as an option. However, *ALL* users of MLlib could
>>> >>> benefit (potentially tremendously) from using a well-tuned CPU-base=
d
>>> >>> BLAS implementation. Perhaps we should consider updating the mllib
>>> >>> guide with a more complete section for enabling high performance
>>> >>> binaries on OSX and Linux? Or better, figure out a way for the
>>> >>> system to fetch these automatically.
>>> >>>
>>> >>> - Evan
>>> >>>
>>> >>>
>>> >>>
>>> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>>> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>> >>>
>>> >>>> Just to summarize this thread, I was finally able to make all
>>> >>>> performance comparisons that we discussed. It turns out that:
>>> >>>> BIDMat-cublas>>BIDMat
>>> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-=
yum-repo=3D
>>> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
>>> >>>>
>>> >>>> Below is the link to the spreadsheet with full results.
>>> >>>>
>>> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUM=
x
>>> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
>>> >>>>
>>> >>>> One thing still needs exploration: does BIDMat-cublas perform
>>> >>>> copying to/from machine=E2=80=99s RAM?
>>> >>>>
>>> >>>> -----Original Message-----
>>> >>>> From: Ulanov, Alexander
>>> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>> >>>> To: Evan R. Sparks
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>>> >>>> the original one discusses slightly different topic. I was able to
>>> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>>> >>>> statically linked inside a 60MB library.
>>> >>>>
>>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>> >>>>
>>> +----------------------------------------------------------------------=
-+
>>> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>> >>>> |1,638475459 |
>>> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 =
|
>>> >>>> 1569,233228 |
>>> >>>>
>>> >>>> It turn out that pre-compiled MKL is faster than precompiled
>>> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more column=
s with
>>> >>>> locally compiled openblas and cuda.
>>> >>>>
>>> >>>> Alexander
>>> >>>>
>>> >>>> From: Evan R. Sparks
>>> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>>> >>>> Sent: Monday, February 09, 2015 6:06 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Great - perhaps we can move this discussion off-list and onto a
>>> >>>> JIRA ticket? (Here's one:
>>> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>>> >>>>
>>> >>>> It seems like this is going to be somewhat exploratory for a while
>>> >>>> (and there's probably only a handful of us who really care about
>>> >>>> fast linear
>>> >>>> algebra!)
>>> >>>>
>>> >>>> - Evan
>>> >>>>
>>> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Hi Evan,
>>> >>>>
>>> >>>> Thank you for explanation and useful link. I am going to build
>>> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>>> >>>>
>>> >>>> Do I understand correctly that BIDMat binaries contain statically
>>> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>>> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, =
I
>>> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
>>> >>>> it seems that in my case precompiled MKL BLAS performs better than
>>> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are suppose=
d
>>> to be on par with JNI overheads.
>>> >>>>
>>> >>>> Though, it might be interesting to link Netlib-java with Intel MKL=
,
>>> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>>> >>>> Halliday
>>> >>>> (Netlib-java) interested to compare their libraries.
>>> >>>>
>>> >>>> Best regards, Alexander
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> >>>> Sent: Friday, February 06, 2015 5:58 PM
>>> >>>>
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
>>> >>>> from getting cache sizes, etc. set up correctly for your particula=
r
>>> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>>> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>>> >>>> quickly and yields performance competitive with MKL.
>>> >>>>
>>> >>>> To make sure the right library is getting used, you have to make
>>> >>>> sure it's first on the search path - export
>>> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
>>> >>>>
>>> >>>> For some examples of getting netlib-java setup on an ec2 node and
>>> >>>> some example benchmarking code we ran a while back, see:
>>> >>>> https://github.com/shivaram/matrix-bench
>>> >>>>
>>> >>>> In particular - build-openblas-ec2.sh shows you how to build the
>>> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>>> >>>> shows you how to get the path setup and get that library picked up
>>> by netlib-java.
>>> >>>>
>>> >>>> In this way - you could probably get cuBLAS set up to be used by
>>> >>>> netlib-java as well.
>>> >>>>
>>> >>>> - Evan
>>> >>>>
>>> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java t=
o
>>> >>>> force loading the right blas? For netlib, I there are few JVM
>>> >>>> flags, such as
>>> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS=
,
>>> >>>> so I can force it to use Java implementation. Not sure I understan=
d
>>> how to force use a specific blas (not specific wrapper for blas).
>>> >>>>
>>> >>>> Btw. I have installed openblas (yum install openblas), so I suppos=
e
>>> >>>> that netlib is using it.
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> >>>> Sent: Friday, February 06, 2015 5:19 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Joseph Bradley;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Getting breeze to pick up the right blas library is critical for
>>> >>>> performance. I recommend using OpenBLAS (or MKL, if you already
>>> have it).
>>> >>>> It might make sense to force BIDMat to use the same underlying BLA=
S
>>> >>>> library as well.
>>> >>>>
>>> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Hi Evan, Joseph
>>> >>>>
>>> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>>> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
>>> >>>>
>>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>>> >>>> |native_system_linux_x86-64|
>>> >>>> Breeze+Netlib-java f2jblas |
>>> >>>>
>>> +----------------------------------------------------------------------=
-+
>>> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>>> >>>> ||
>>> >>>>
>>> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedor=
a
>>> >>>> 19 Linux, Scala 2.11.
>>> >>>>
>>> >>>> Later I will make tests with Cuda. I need to install new Cuda
>>> >>>> version for this purpose.
>>> >>>>
>>> >>>> Do you have any ideas why breeze-netlib with native blas is so muc=
h
>>> >>>> slower than BIDMat MKL?
>>> >>>>
>>> >>>> Best regards, Alexander
>>> >>>>
>>> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>>> joseph@databricks.com><mailto:
>>> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>>> >>>> Sent: Thursday, February 05, 2015 5:29 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: Evan R. Sparks;
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> Hi Alexander,
>>> >>>>
>>> >>>> Using GPUs with Spark would be very exciting.  Small comment:
>>> >>>> Concerning your question earlier about keeping data stored on the
>>> >>>> GPU rather than having to move it between main memory and GPU
>>> >>>> memory on each iteration, I would guess this would be critical to
>>> >>>> getting good performance.  If you could do multiple local
>>> >>>> iterations before aggregating results, then the cost of data
>>> >>>> movement to the GPU could be amortized (and I believe that is done
>>> >>>> in practice).  Having Spark be aware of the GPU and using it as
>>> another part of memory sounds like a much bigger undertaking.
>>> >>>>
>>> >>>> Joseph
>>> >>>>
>>> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach presen=
tation by
>>> >>>> John Canny and I am really inspired by his talk and comparisons
>>> with Spark MLlib.
>>> >>>>
>>> >>>> I am very interested to find out what will be better within Spark:
>>> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>>> >>>> fair way to benchmark them? Currently I do benchmarks on artificia=
l
>>> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=
=80=9D test of
>>> >>>> linear algebra, it involves some other things that are essential t=
o
>>> machine learning.
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>> >>>> Sent: Thursday, February 05, 2015 1:29 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc:
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due t=
o
>>> >>>> netlib-java+data
>>> >>>> layout and fewer levels of indirection - it's definitely a
>>> >>>> worthwhile experiment to run. The main speedups I've seen from
>>> >>>> using it come from highly optimized GPU code for linear algebra. I
>>> >>>> know that in the past Canny has gone as far as to write custom GPU
>>> >>>> kernels for performance-critical regions of code.[1]
>>> >>>>
>>> >>>> BIDMach is highly optimized for single node performance or
>>> >>>> performance on small clusters.[2] Once data doesn't fit easily in
>>> >>>> GPU memory (or can be batched in that way) the performance tends t=
o
>>> >>>> fall off. Canny argues for hardware/software codesign and as such
>>> >>>> prefers machine configurations that are quite different than what
>>> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels an=
d
>>> 4 GPUs.
>>> >>>>
>>> >>>> In contrast, MLlib was designed for horizontal scalability on
>>> >>>> commodity clusters and works best on very big datasets - order of
>>> terabytes.
>>> >>>>
>>> >>>> For the most part, these projects developed concurrently to addres=
s
>>> >>>> slightly different use cases. That said, there may be bits of
>>> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>>> >>>> careful about maintaining cross-language compatibility for our Jav=
a
>>> >>>> and Python-users, though.
>>> >>>>
>>> >>>> - Evan
>>> >>>>
>>> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>>> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>> >>>>
>>> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> >>>> Hi Evan,
>>> >>>>
>>> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>>> >>>> you know what makes them faster than netlib-java?
>>> >>>>
>>> >>>> The same group has BIDMach library that implements machine
>>> >>>> learning. For some examples they use Caffe convolutional neural
>>> >>>> network library owned by another group in Berkeley. Could you
>>> >>>> elaborate on how these all might be connected with Spark Mllib? If
>>> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDM=
ach for
>>> optimization and learning?
>>> >>>>
>>> >>>> Best regards, Alexander
>>> >>>>
>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>> evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>> >>>> Sent: Thursday, February 05, 2015 12:09 PM
>>> >>>> To: Ulanov, Alexander
>>> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>>> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark=
.
>>> >>>> apache.org<mailto:dev@spark.apache.org>>>
>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>> >>>>
>>> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>>> >>>> blas in many cases.
>>> >>>>
>>> >>>> You might consider taking a look at the codepaths that BIDMat (
>>> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>>> >>>> optimizing to make this work really fast from Scala. I've run it o=
n
>>> >>>> my laptop and compared to MKL and in certain cases it's 10x faster
>>> at matrix multiply.
>>> >>>> There are a lot of layers of indirection here and you really want
>>> >>>> to avoid data copying as much as possible.
>>> >>>>
>>> >>>> We could also consider swapping out BIDMat for Breeze, but that
>>> >>>> would be a big project and if we can figure out how to get
>>> >>>> breeze+cublas to comparable performance that would be a big win.
>>> >>>>
>>> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>> >>>> Dear Spark developers,
>>> >>>>
>>> >>>> I am exploring how to make linear algebra operations faster within
>>> Spark.
>>> >>>> One way of doing this is to use Scala Breeze library that is
>>> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>>> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms=
)
>>> >>>> and LAPACK native binaries if they are available on the worker
>>> >>>> node. It also has its own optimized Java implementation of BLAS. I=
t
>>> >>>> is worth mentioning, that native binaries provide better
>>> performance only for BLAS level 3, i.e.
>>> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>>> >>>> This is confirmed by GEMM test on Netlib-java page
>>> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>>> >>>> experiments with training of artificial neural network
>>> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>> >>>> However, I would like to boost performance more.
>>> >>>>
>>> >>>> GPU is supposed to work fast with linear algebra and there is
>>> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linu=
x
>>> >>>> server with Nvidia GPU and I was able to do the following. I linke=
d
>>> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and pu=
t
>>> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>>> >>>> performance measurements with regards to artificial neural network
>>> >>>> batch learning in Spark MLlib that involves matrix-matrix
>>> >>>> multiplications. It turns out that for matrices of size less than
>>> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas become=
s
>>> >>>> slower for bigger matrices. It worth mentioning that it is was not
>>> a test for ONLY multiplication since there are other operations involve=
d.
>>> >>>> One of the reasons for slowdown might be the overhead of copying
>>> >>>> the matrices from computer memory to graphic card memory and back.
>>> >>>>
>>> >>>> So, few questions:
>>> >>>> 1) Do these results with CUDA make sense?
>>> >>>> 2) If the problem is with copy overhead, are there any libraries
>>> >>>> that allow to force intermediate results to stay in graphic card
>>> >>>> memory thus removing the overhead?
>>> >>>> 3) Any other options to speed-up linear algebra in Spark?
>>> >>>>
>>> >>>> Thank you, Alexander
>>> >>>>
>>> >>>> ------------------------------------------------------------------=
-
>>> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto=
:
>>> dev-unsubscribe@spark.apache.org><mailto:
>>> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apac=
h
>>> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@s=
p
>>> >>>> ark.apac> he.org<http://he.org>
>>> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@sp=
a
>>> >>>> rk.apache.org>>> For additional commands, e-mail:
>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto=
:
>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org
>>> >><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org
>>> ><mailto:
>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>>
>>> >>>
>>>
>>> --
>>> Best regards,
>>> Sam
>>>
>>
>>

--bcaec511e11669ac9305122c5459--

From dev-return-12207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 09:31:43 2015
Return-Path: <dev-return-12207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B31A8173BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 09:31:43 +0000 (UTC)
Received: (qmail 76095 invoked by uid 500); 26 Mar 2015 09:31:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76009 invoked by uid 500); 26 Mar 2015 09:31:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75997 invoked by uid 99); 26 Mar 2015 09:31:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 09:31:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [95.143.172.231] (HELO bootes.uberspace.de) (95.143.172.231)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 09:31:16 +0000
Received: (qmail 23734 invoked from network); 26 Mar 2015 09:30:12 -0000
Received: from localhost (HELO webmail.bootes.uberspace.de) (127.0.0.1)
  by ::1 with SMTP; 26 Mar 2015 09:30:12 -0000
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8;
 format=flowed
Content-Transfer-Encoding: 8bit
Date: Thu, 26 Mar 2015 10:30:12 +0100
From: Karlson <ksonspark@siberie.de>
To: Davies Liu <davies@databricks.com>
Cc: dev@spark.apache.org
Subject: Re: functools.partial as UserDefinedFunction
In-Reply-To: <7461D7919B3047C5821FC90B78F82254@databricks.com>
References: <3560e95f48681fccd9266e86bbff0e51@siberie.de>
 <7461D7919B3047C5821FC90B78F82254@databricks.com>
Message-ID: <dc20a0f3627f1f685cd31f397e086b09@siberie.de>
X-Sender: ksonspark@siberie.de
User-Agent: Roundcube Webmail/1.0.5
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I've filed a JIRA (https://issues.apache.org/jira/browse/SPARK-6553) and 
suggested a fix (https://github.com/apache/spark/pull/5206).


On 2015-03-25 19:49, Davies Liu wrote:
> It’s good to support functools.partial, could you file a JIRA for it?
> 
> 
> On Wednesday, March 25, 2015 at 5:42 AM, Karlson wrote:
> 
>> 
>> Hi all,
>> 
>> passing a functools.partial-function as a UserDefinedFunction to
>> DataFrame.select raises an AttributeException, because 
>> functools.partial
>> does not have the attribute __name__. Is there any alternative to
>> relying on __name__ in pyspark/sql/functions.py:126 ?
>> 
>> 
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org 
>> (mailto:dev-unsubscribe@spark.apache.org)
>> For additional commands, e-mail: dev-help@spark.apache.org 
>> (mailto:dev-help@spark.apache.org)
>> 
>> 

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 16:10:28 2015
Return-Path: <dev-return-12208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C4AD173B5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 16:10:28 +0000 (UTC)
Received: (qmail 12399 invoked by uid 500); 26 Mar 2015 16:10:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12322 invoked by uid 500); 26 Mar 2015 16:10:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12298 invoked by uid 99); 26 Mar 2015 16:10:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:10:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.160.179 as permitted sender)
Received: from [209.85.160.179] (HELO mail-yk0-f179.google.com) (209.85.160.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:09:47 +0000
Received: by ykef74 with SMTP id f74so7117232yke.1
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 09:07:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=6m0pKlYs/4kn15RB/FowRkSKTjp+cPUrwdgLd9u1HcA=;
        b=P1RerJjt13/+rfWVjq7J/bzUKRhBOkCd9c+sS5gUwUYLrx1jb8eLTaUfCUE3/wlGu0
         fzrTrWXo3jQW1I4H6R16kwVxQw4YpO97/NKjWiZ2q0cNs8vW5YOWywYByGKhDkRcphZn
         pc2HbtAQls6w1/SwU5jRSSZQbA/ZlCC2ukveIFIvECOce1XA8O+ethNnssjJtvkTQgwT
         Ih92kvTe3FusfrFf3NK8Irg2YiSyeXg/ZUe8Iu+deyGIF9KsQi1bMsjQjpWbBtk62EA7
         REQ9DUQJUFMjW/aRKK+d9wMi2m9WTPyMT4EjSNT+bUN36IkUyoYuIivEFjU7xn0AhDPz
         w3gA==
X-Received: by 10.52.103.48 with SMTP id ft16mr17789892vdb.81.1427386050027;
 Thu, 26 Mar 2015 09:07:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Thu, 26 Mar 2015 09:07:09 -0700 (PDT)
In-Reply-To: <CALR_T9C4mBOy0T6_K9+RQZtoeNGvhZOtmvz=ij0fhyO8SDbKUw@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
 <CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
 <CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
 <CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
 <CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
 <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
 <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
 <CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
 <CALR_T9AtsmcXd8oO8yDohKmkUpZLREX3_MBhXqBiQUvrcZYe2w@mail.gmail.com> <CALR_T9C4mBOy0T6_K9+RQZtoeNGvhZOtmvz=ij0fhyO8SDbKUw@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Thu, 26 Mar 2015 09:07:09 -0700
Message-ID: <CABjXkq798dF1ARtkS+oh+oFA7F03nQ+CVCMQ8qEpGy6HFSLgdA@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
To: Sam Halliday <sam.halliday@gmail.com>
Cc: Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, "Ulanov, Alexander" <alexander.ulanov@hp.com>, 
	jfcanny <canny@berkeley.edu>
Content-Type: multipart/alternative; boundary=047d7b5dbd3a78286c05123336e6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5dbd3a78286c05123336e6
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Alright Sam - you are the expert here. If the GPL issues are unavoidable,
that's fine - what is the exact bit of code that is GPL?

The suggestion to use OpenBLAS is not to say it's the best option, but that
it's a *free, reasonable default* for many users - keep in mind the most
common deployment for Spark/MLlib is on 64-bit linux on EC2[1].
Additionally, for many of the problems we're targeting, this reasonable
default can provide a 1-2 orders of magnitude improvement in performance
over the f2jblas implementation that netlib-java falls back on.

The JVM issues are trickier, I agree - so it sounds like a good user guide
explaining the tradeoffs and configurations procedures as they relate to
spark is a reasonable way forward.

[1] -
https://gigaom.com/2015/01/27/a-few-interesting-numbers-about-apache-spark/

On Thu, Mar 26, 2015 at 12:54 AM, Sam Halliday <sam.halliday@gmail.com>
wrote:

> Btw, OpenBLAS requires GPL runtime binaries which are typically considere=
d
> "system libraries" (and these fall under something similar to the Java
> classpath exception rule)... so it's basically impossible to distribute
> OpenBLAS the way you're suggesting, sorry. Indeed, there is work ongoing =
in
> Spark right now to clear up something of this nature.
>
> On a more technical level, I'd recommend watching my talk at ScalaX which
> explains in detail why high performance only comes from machine optimised
> binaries, which requires DevOps buy-in (and, I'd recommend using MKL anyw=
ay
> on the CPU, not OpenBLAS).
>
> On an even deeper level, using natives has consequences to JIT and GC
> which isn't suitable for everybody and we'd really like people to go into
> that with their eyes wide open.
> On 26 Mar 2015 07:43, "Sam Halliday" <sam.halliday@gmail.com> wrote:
>
>> I'm not at all surprised ;-) I fully expect the GPU performance to get
>> better automatically as the hardware improves.
>>
>> Netlib natives still need to be shipped separately. I'd also oppose any
>> move to make Open BLAS the default - is not always better and I think
>> natives really need DevOps buy-in. It's not the right solution for
>> everybody.
>> On 26 Mar 2015 01:23, "Evan R. Sparks" <evan.sparks@gmail.com> wrote:
>>
>>> Yeah, much more reasonable - nice to know that we can get full GPU
>>> performance from breeze/netlib-java - meaning there's no compelling
>>> performance reason to switch out our current linear algebra library (at
>>> least as far as this benchmark is concerned).
>>>
>>> Instead, it looks like a user guide for configuring Spark/MLlib to use
>>> the right BLAS library will get us most of the way there. Or, would it =
make
>>> sense to finally ship openblas compiled for some common platforms (64-b=
it
>>> linux, windows, mac) directly with Spark - hopefully eliminating the jb=
las
>>> warnings once and for all for most users? (Licensing is BSD) Or am I
>>> missing something?
>>>
>>> On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <
>>> alexander.ulanov@hp.com> wrote:
>>>
>>>> As everyone suggested, the results were too good to be true, so I
>>>> double-checked them. It turns that nvblas did not do multiplication du=
e to
>>>> parameter NVBLAS_TILE_DIM from "nvblas.conf" and returned zero matrix.=
 My
>>>> previously posted results with nvblas are matrices copying only. The
>>>> default NVBLAS_TILE_DIM=3D=3D2048 is too big for my graphic card/matri=
x size. I
>>>> handpicked other values that worked. As a result, netlib+nvblas is on =
par
>>>> with BIDMat-cuda. As promised, I am going to post a how-to for nvblas
>>>> configuration.
>>>>
>>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378=
T9J5r7kwKSPkY/edit?usp=3Dsharing
>>>>
>>>>
>>>>
>>>> -----Original Message-----
>>>> From: Ulanov, Alexander
>>>> Sent: Wednesday, March 25, 2015 2:31 PM
>>>> To: Sam Halliday
>>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R.
>>>> Sparks; jfcanny
>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Hi again,
>>>>
>>>> I finally managed to use nvblas within Spark+netlib-java. It has
>>>> exceptional performance for big matrices with Double, faster than
>>>> BIDMat-cuda with Float. But for smaller matrices, if you will copy the=
m
>>>> to/from GPU, OpenBlas or MKL might be a better choice. This correlates=
 with
>>>> original nvblas presentation on GPU conf 2013 (slide 21):
>>>> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC31=
08-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>>>>
>>>> My results:
>>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378=
T9J5r7kwKSPkY/edit?usp=3Dsharing
>>>>
>>>> Just in case, these tests are not for generalization of performance of
>>>> different libraries. I just want to pick a library that does at best d=
ense
>>>> matrices multiplication for my task.
>>>>
>>>> P.S. My previous issue with nvblas was the following: it has Fortran
>>>> blas functions, at the same time netlib-java uses C cblas functions. S=
o,
>>>> one needs cblas shared library to use nvblas through netlib-java. Fedo=
ra
>>>> does not have cblas (but Debian and Ubuntu have), so I needed to compi=
le
>>>> it. I could not use cblas from Atlas or Openblas because they link to =
their
>>>> implementation and not to Fortran blas.
>>>>
>>>> Best regards, Alexander
>>>>
>>>> -----Original Message-----
>>>> From: Ulanov, Alexander
>>>> Sent: Tuesday, March 24, 2015 6:57 PM
>>>> To: Sam Halliday
>>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Spark=
s
>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> Hi,
>>>>
>>>> I am trying to use nvblas with netlib-java from Spark. nvblas function=
s
>>>> should replace current blas functions calls after executing LD_PRELOAD=
 as
>>>> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
>>>> changes to netlib-java. It seems to work for simple Java example, but =
I
>>>> cannot make it work with Spark. I run the following:
>>>> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
>>>> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
>>>> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>>>>
>>>> +---------------------------------------------------------------------=
--------+
>>>> | Processes:                                                       GPU
>>>> Memory |
>>>> |  GPU       PID  Type  Process name
>>>>  Usage      |
>>>>
>>>> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D|
>>>> |    0      8873    C   bash
>>>> 39MiB |
>>>> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
>>>> 39MiB |
>>>>
>>>> +---------------------------------------------------------------------=
--------+
>>>>
>>>> In Spark shell I do matrix multiplication and see the following:
>>>> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
>>>> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>>>> So I am sure that netlib-native is loaded and cblas supposedly used.
>>>> However, matrix multiplication does executes on CPU since I see 16% of=
 CPU
>>>> used and 0% of GPU used. I also checked different matrix sizes, from
>>>> 100x100 to 12000x12000
>>>>
>>>> Could you suggest might the LD_PRELOAD not affect Spark shell?
>>>>
>>>> Best regards, Alexander
>>>>
>>>>
>>>>
>>>> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>>>> Sent: Monday, March 09, 2015 6:01 PM
>>>> To: Ulanov, Alexander
>>>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Spark=
s
>>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>>
>>>>
>>>> Thanks so much for following up on this!
>>>>
>>>> Hmm, I wonder if we should have a concerted effort to chart performanc=
e
>>>> on various pieces of hardware...
>>>> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com
>>>> <mailto:alexander.ulanov@hp.com>> wrote:
>>>> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added
>>>> the comment that BIDMat 0.9.7 uses Float matrices in GPU (although I s=
ee
>>>> the support of Double in the current source code), did the test with B=
IDMat
>>>> and CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>>>>
>>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378=
T9J5r7kwKSPkY/edit?usp=3Dsharing
>>>>
>>>> Best regards, Alexander
>>>>
>>>> -----Original Message-----
>>>> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
>>>> sam.halliday@gmail.com>]
>>>> Sent: Tuesday, March 03, 2015 1:54 PM
>>>> To: Xiangrui Meng; Joseph Bradley
>>>> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
>>>> dev@spark.apache.org>
>>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>>
>>>> BTW, is anybody on this list going to the London Meetup in a few weeks=
?
>>>>
>>>>
>>>> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-map=
reduce-world#community
>>>>
>>>> Would be nice to meet other people working on the guts of Spark! :-)
>>>>
>>>>
>>>> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>>>>
>>>> > Hey Alexander,
>>>> >
>>>> > I don't quite understand the part where netlib-cublas is about 20x
>>>> > slower than netlib-openblas. What is the overhead of using a GPU BLA=
S
>>>> > with netlib-java?
>>>> >
>>>> > CC'ed Sam, the author of netlib-java.
>>>> >
>>>> > Best,
>>>> > Xiangrui
>>>> >
>>>> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <
>>>> joseph@databricks.com<mailto:joseph@databricks.com>> wrote:
>>>> >> Better documentation for linking would be very helpful!  Here's a
>>>> JIRA:
>>>> >> https://issues.apache.org/jira/browse/SPARK-6019
>>>> >>
>>>> >>
>>>> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>>>> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>>>> >> wrote:
>>>> >>
>>>> >>> Thanks for compiling all the data and running these benchmarks,
>>>> >>> Alex. The big takeaways here can be seen with this chart:
>>>> >>>
>>>> >>>
>>>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>>>> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>>>> >>>
>>>> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>>>> >>> BIDMat+GPU) can provide substantial (but less than an order of
>>>> >>> BIDMat+magnitude)
>>>> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>>>> >>> netlib-java+openblas-compiled).
>>>> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitud=
e
>>>> >>> worse than a well-tuned CPU implementation, particularly for large=
r
>>>> matrices.
>>>> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - thi=
s
>>>> >>> basically agrees with the authors own benchmarks (
>>>> >>> https://github.com/fommil/netlib-java)
>>>> >>>
>>>> >>> I think that most of our users are in a situation where using GPUs
>>>> >>> may not be practical - although we could consider having a good GP=
U
>>>> >>> backend available as an option. However, *ALL* users of MLlib coul=
d
>>>> >>> benefit (potentially tremendously) from using a well-tuned CPU-bas=
ed
>>>> >>> BLAS implementation. Perhaps we should consider updating the mllib
>>>> >>> guide with a more complete section for enabling high performance
>>>> >>> binaries on OSX and Linux? Or better, figure out a way for the
>>>> >>> system to fetch these automatically.
>>>> >>>
>>>> >>> - Evan
>>>> >>>
>>>> >>>
>>>> >>>
>>>> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>>>> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>>>> >>>
>>>> >>>> Just to summarize this thread, I was finally able to make all
>>>> >>>> performance comparisons that we discussed. It turns out that:
>>>> >>>> BIDMat-cublas>>BIDMat
>>>> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas=
-yum-repo=3D
>>>> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
>>>> >>>>
>>>> >>>> Below is the link to the spreadsheet with full results.
>>>> >>>>
>>>> >>>>
>>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>>>> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
>>>> >>>>
>>>> >>>> One thing still needs exploration: does BIDMat-cublas perform
>>>> >>>> copying to/from machine=E2=80=99s RAM?
>>>> >>>>
>>>> >>>> -----Original Message-----
>>>> >>>> From: Ulanov, Alexander
>>>> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>>>> >>>> To: Evan R. Sparks
>>>> >>>> Cc: Joseph Bradley;
>>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>>> >>>>
>>>> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>>>> >>>> the original one discusses slightly different topic. I was able t=
o
>>>> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>>>> >>>> statically linked inside a 60MB library.
>>>> >>>>
>>>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>>>> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>>>> >>>>
>>>> +---------------------------------------------------------------------=
--+
>>>> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 =
|
>>>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>>>> >>>> |1,638475459 |
>>>> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211=
 |
>>>> >>>> 1569,233228 |
>>>> >>>>
>>>> >>>> It turn out that pre-compiled MKL is faster than precompiled
>>>> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more colum=
ns with
>>>> >>>> locally compiled openblas and cuda.
>>>> >>>>
>>>> >>>> Alexander
>>>> >>>>
>>>> >>>> From: Evan R. Sparks
>>>> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>>>> >>>> Sent: Monday, February 09, 2015 6:06 PM
>>>> >>>> To: Ulanov, Alexander
>>>> >>>> Cc: Joseph Bradley;
>>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>> >>>>
>>>> >>>> Great - perhaps we can move this discussion off-list and onto a
>>>> >>>> JIRA ticket? (Here's one:
>>>> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>>>> >>>>
>>>> >>>> It seems like this is going to be somewhat exploratory for a whil=
e
>>>> >>>> (and there's probably only a handful of us who really care about
>>>> >>>> fast linear
>>>> >>>> algebra!)
>>>> >>>>
>>>> >>>> - Evan
>>>> >>>>
>>>> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> >>>> Hi Evan,
>>>> >>>>
>>>> >>>> Thank you for explanation and useful link. I am going to build
>>>> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>>>> >>>>
>>>> >>>> Do I understand correctly that BIDMat binaries contain statically
>>>> >>>> linked Intel MKL BLAS? It might be the reason why I am able to ru=
n
>>>> >>>> BIDMat not having MKL BLAS installed on my server. If it is true,=
 I
>>>> >>>> wonder if it is OK because Intel sells this library. Nevertheless=
,
>>>> >>>> it seems that in my case precompiled MKL BLAS performs better tha=
n
>>>> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are
>>>> supposed to be on par with JNI overheads.
>>>> >>>>
>>>> >>>> Though, it might be interesting to link Netlib-java with Intel MK=
L,
>>>> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>>>> >>>> Halliday
>>>> >>>> (Netlib-java) interested to compare their libraries.
>>>> >>>>
>>>> >>>> Best regards, Alexander
>>>> >>>>
>>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com><mailto:
>>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> >>>> Sent: Friday, February 06, 2015 5:58 PM
>>>> >>>>
>>>> >>>> To: Ulanov, Alexander
>>>> >>>> Cc: Joseph Bradley;
>>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spar=
k
>>>> .
>>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>> >>>>
>>>> >>>> I would build OpenBLAS yourself, since good BLAS performance come=
s
>>>> >>>> from getting cache sizes, etc. set up correctly for your particul=
ar
>>>> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>>>> >>>> but we found that on relatively modern Xeon chips, OpenBLAS build=
s
>>>> >>>> quickly and yields performance competitive with MKL.
>>>> >>>>
>>>> >>>> To make sure the right library is getting used, you have to make
>>>> >>>> sure it's first on the search path - export
>>>> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here=
.
>>>> >>>>
>>>> >>>> For some examples of getting netlib-java setup on an ec2 node and
>>>> >>>> some example benchmarking code we ran a while back, see:
>>>> >>>> https://github.com/shivaram/matrix-bench
>>>> >>>>
>>>> >>>> In particular - build-openblas-ec2.sh shows you how to build the
>>>> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>>>> >>>> shows you how to get the path setup and get that library picked u=
p
>>>> by netlib-java.
>>>> >>>>
>>>> >>>> In this way - you could probably get cuBLAS set up to be used by
>>>> >>>> netlib-java as well.
>>>> >>>>
>>>> >>>> - Evan
>>>> >>>>
>>>> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java =
to
>>>> >>>> force loading the right blas? For netlib, I there are few JVM
>>>> >>>> flags, such as
>>>> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLA=
S,
>>>> >>>> so I can force it to use Java implementation. Not sure I
>>>> understand how to force use a specific blas (not specific wrapper for =
blas).
>>>> >>>>
>>>> >>>> Btw. I have installed openblas (yum install openblas), so I suppo=
se
>>>> >>>> that netlib is using it.
>>>> >>>>
>>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com><mailto:
>>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> >>>> Sent: Friday, February 06, 2015 5:19 PM
>>>> >>>> To: Ulanov, Alexander
>>>> >>>> Cc: Joseph Bradley;
>>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spar=
k
>>>> .
>>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>>> >>>>
>>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>> >>>>
>>>> >>>> Getting breeze to pick up the right blas library is critical for
>>>> >>>> performance. I recommend using OpenBLAS (or MKL, if you already
>>>> have it).
>>>> >>>> It might make sense to force BIDMat to use the same underlying BL=
AS
>>>> >>>> library as well.
>>>> >>>>
>>>> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> >>>> Hi Evan, Joseph
>>>> >>>>
>>>> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>>>> >>>> faster than netlib-java+breeze (sorry for weird table formatting)=
:
>>>> >>>>
>>>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>>>> >>>> |native_system_linux_x86-64|
>>>> >>>> Breeze+Netlib-java f2jblas |
>>>> >>>>
>>>> +---------------------------------------------------------------------=
--+
>>>> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>>>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>>>> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,23322=
8
>>>> >>>> ||
>>>> >>>>
>>>> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedo=
ra
>>>> >>>> 19 Linux, Scala 2.11.
>>>> >>>>
>>>> >>>> Later I will make tests with Cuda. I need to install new Cuda
>>>> >>>> version for this purpose.
>>>> >>>>
>>>> >>>> Do you have any ideas why breeze-netlib with native blas is so mu=
ch
>>>> >>>> slower than BIDMat MKL?
>>>> >>>>
>>>> >>>> Best regards, Alexander
>>>> >>>>
>>>> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>>>> joseph@databricks.com><mailto:
>>>> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>>>> >>>> Sent: Thursday, February 05, 2015 5:29 PM
>>>> >>>> To: Ulanov, Alexander
>>>> >>>> Cc: Evan R. Sparks;
>>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spar=
k
>>>> .
>>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>> >>>>
>>>> >>>> Hi Alexander,
>>>> >>>>
>>>> >>>> Using GPUs with Spark would be very exciting.  Small comment:
>>>> >>>> Concerning your question earlier about keeping data stored on the
>>>> >>>> GPU rather than having to move it between main memory and GPU
>>>> >>>> memory on each iteration, I would guess this would be critical to
>>>> >>>> getting good performance.  If you could do multiple local
>>>> >>>> iterations before aggregating results, then the cost of data
>>>> >>>> movement to the GPU could be amortized (and I believe that is don=
e
>>>> >>>> in practice).  Having Spark be aware of the GPU and using it as
>>>> another part of memory sounds like a much bigger undertaking.
>>>> >>>>
>>>> >>>> Joseph
>>>> >>>>
>>>> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>>>> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach prese=
ntation by
>>>> >>>> John Canny and I am really inspired by his talk and comparisons
>>>> with Spark MLlib.
>>>> >>>>
>>>> >>>> I am very interested to find out what will be better within Spark=
:
>>>> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest =
a
>>>> >>>> fair way to benchmark them? Currently I do benchmarks on artifici=
al
>>>> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=
=80=9D test of
>>>> >>>> linear algebra, it involves some other things that are essential
>>>> to machine learning.
>>>> >>>>
>>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com><mailto:
>>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>>>> >>>> Sent: Thursday, February 05, 2015 1:29 PM
>>>> >>>> To: Ulanov, Alexander
>>>> >>>> Cc:
>>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spar=
k
>>>> .
>>>> >>>> apache.org<mailto:dev@spark.apache.org>>
>>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>> >>>>
>>>> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>>>> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due =
to
>>>> >>>> netlib-java+data
>>>> >>>> layout and fewer levels of indirection - it's definitely a
>>>> >>>> worthwhile experiment to run. The main speedups I've seen from
>>>> >>>> using it come from highly optimized GPU code for linear algebra. =
I
>>>> >>>> know that in the past Canny has gone as far as to write custom GP=
U
>>>> >>>> kernels for performance-critical regions of code.[1]
>>>> >>>>
>>>> >>>> BIDMach is highly optimized for single node performance or
>>>> >>>> performance on small clusters.[2] Once data doesn't fit easily in
>>>> >>>> GPU memory (or can be batched in that way) the performance tends =
to
>>>> >>>> fall off. Canny argues for hardware/software codesign and as such
>>>> >>>> prefers machine configurations that are quite different than what
>>>> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels
>>>> and 4 GPUs.
>>>> >>>>
>>>> >>>> In contrast, MLlib was designed for horizontal scalability on
>>>> >>>> commodity clusters and works best on very big datasets - order of
>>>> terabytes.
>>>> >>>>
>>>> >>>> For the most part, these projects developed concurrently to addre=
ss
>>>> >>>> slightly different use cases. That said, there may be bits of
>>>> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>>>> >>>> careful about maintaining cross-language compatibility for our Ja=
va
>>>> >>>> and Python-users, though.
>>>> >>>>
>>>> >>>> - Evan
>>>> >>>>
>>>> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>>>> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>>>> >>>>
>>>> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> >>>> Hi Evan,
>>>> >>>>
>>>> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>>>> >>>> you know what makes them faster than netlib-java?
>>>> >>>>
>>>> >>>> The same group has BIDMach library that implements machine
>>>> >>>> learning. For some examples they use Caffe convolutional neural
>>>> >>>> network library owned by another group in Berkeley. Could you
>>>> >>>> elaborate on how these all might be connected with Spark Mllib? I=
f
>>>> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BID=
Mach for
>>>> optimization and learning?
>>>> >>>>
>>>> >>>> Best regards, Alexander
>>>> >>>>
>>>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>>>> evan.sparks@gmail.com><mailto:
>>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>>>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>>>> >>>> Sent: Thursday, February 05, 2015 12:09 PM
>>>> >>>> To: Ulanov, Alexander
>>>> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>>>> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>>>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spar=
k
>>>> .
>>>> >>>> apache.org<mailto:dev@spark.apache.org>>>
>>>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>>> >>>>
>>>> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>>>> >>>> blas in many cases.
>>>> >>>>
>>>> >>>> You might consider taking a look at the codepaths that BIDMat (
>>>> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>>>> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>>>> >>>> optimizing to make this work really fast from Scala. I've run it =
on
>>>> >>>> my laptop and compared to MKL and in certain cases it's 10x faste=
r
>>>> at matrix multiply.
>>>> >>>> There are a lot of layers of indirection here and you really want
>>>> >>>> to avoid data copying as much as possible.
>>>> >>>>
>>>> >>>> We could also consider swapping out BIDMat for Breeze, but that
>>>> >>>> would be a big project and if we can figure out how to get
>>>> >>>> breeze+cublas to comparable performance that would be a big win.
>>>> >>>>
>>>> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>>>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>>>> >>>> Dear Spark developers,
>>>> >>>>
>>>> >>>> I am exploring how to make linear algebra operations faster withi=
n
>>>> Spark.
>>>> >>>> One way of doing this is to use Scala Breeze library that is
>>>> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>>>> >>>> that has a Java wrapper for BLAS (basic linear algebra subprogram=
s)
>>>> >>>> and LAPACK native binaries if they are available on the worker
>>>> >>>> node. It also has its own optimized Java implementation of BLAS. =
It
>>>> >>>> is worth mentioning, that native binaries provide better
>>>> performance only for BLAS level 3, i.e.
>>>> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>>>> >>>> This is confirmed by GEMM test on Netlib-java page
>>>> >>>> https://github.com/fommil/netlib-java. I also confirmed it with m=
y
>>>> >>>> experiments with training of artificial neural network
>>>> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>>>> >>>> However, I would like to boost performance more.
>>>> >>>>
>>>> >>>> GPU is supposed to work fast with linear algebra and there is
>>>> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Lin=
ux
>>>> >>>> server with Nvidia GPU and I was able to do the following. I link=
ed
>>>> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and p=
ut
>>>> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>>>> >>>> performance measurements with regards to artificial neural networ=
k
>>>> >>>> batch learning in Spark MLlib that involves matrix-matrix
>>>> >>>> multiplications. It turns out that for matrices of size less than
>>>> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becom=
es
>>>> >>>> slower for bigger matrices. It worth mentioning that it is was no=
t
>>>> a test for ONLY multiplication since there are other operations involv=
ed.
>>>> >>>> One of the reasons for slowdown might be the overhead of copying
>>>> >>>> the matrices from computer memory to graphic card memory and back=
.
>>>> >>>>
>>>> >>>> So, few questions:
>>>> >>>> 1) Do these results with CUDA make sense?
>>>> >>>> 2) If the problem is with copy overhead, are there any libraries
>>>> >>>> that allow to force intermediate results to stay in graphic card
>>>> >>>> memory thus removing the overhead?
>>>> >>>> 3) Any other options to speed-up linear algebra in Spark?
>>>> >>>>
>>>> >>>> Thank you, Alexander
>>>> >>>>
>>>> >>>> -----------------------------------------------------------------=
--
>>>> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>>> <mailto:dev-unsubscribe@spark.apache.org><mailto:
>>>> >>>> dev-unsubscribe@spark.apache.org<mailto:
>>>> dev-unsubscribe@spark.apach
>>>> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:
>>>> dev-unsubscribe@sp
>>>> >>>> ark.apac> he.org<http://he.org>
>>>> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:
>>>> dev-unsubscribe@spa
>>>> >>>> rk.apache.org>>> For additional commands, e-mail:
>>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org
>>>> ><mailto:
>>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org
>>>> >><mailto:dev-help@spark.apache.org<mailto:dev-help@spark.apache.org
>>>> ><mailto:
>>>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>>
>>>> >>>
>>>>
>>>> --
>>>> Best regards,
>>>> Sam
>>>>
>>>
>>>

--047d7b5dbd3a78286c05123336e6--

From dev-return-12209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 16:22:20 2015
Return-Path: <dev-return-12209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB3FA174CC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 16:22:20 +0000 (UTC)
Received: (qmail 54621 invoked by uid 500); 26 Mar 2015 16:22:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54530 invoked by uid 500); 26 Mar 2015 16:22:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54519 invoked by uid 99); 26 Mar 2015 16:22:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:22:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of canny@berkeley.edu designates 209.85.220.53 as permitted sender)
Received: from [209.85.220.53] (HELO mail-pa0-f53.google.com) (209.85.220.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:22:11 +0000
Received: by pacwe9 with SMTP id we9so67377564pac.1
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 09:20:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:message-id:date:from:user-agent:mime-version:to
         :cc:subject:references:in-reply-to:content-type;
        bh=1Czb+NHMABWAaWg9D8OmiKMQoRDC8qJ84UsuZDwuWdY=;
        b=hfV5ek+RmFOL4Kny4xTs69qReVzW6wwE7qYdDzAN0p4rgsy324pbDCZojqS7iIxKXy
         yWkzNJ3px2/MUtDPM8OvK3YNRWWoYfcqpMJMcfTiN6e9VaT2P7RxGfIkgphKzJXWb8nC
         C8aG/9B6b6NQzG8W3mN4FpAH15h8a4Bq85f/GrQtvpjh6yn7t4aQT+qTMhUQv6ChGAje
         4ui7PGZlf3jQX3xgqqm18Dm8kfYzEEFks1zCjuX8z/owZzf5c2bWMFWFODs0bXAOcU4G
         BniJu9ZJ2E7xhSh4PwgXG3YwjEihzT2aMYOzMNIFIC3JTCKSe/sJdx7cW+/dwigWVWEP
         r2cQ==
X-Gm-Message-State: ALoCoQlyC/8IvXw+tjphziBWqaFC9T85Jjxkr/77J1ONfbKXEU+UxTH3HVuImKdsUq55BB+G7RY9
X-Received: by 10.68.96.33 with SMTP id dp1mr27974715pbb.4.1427386821011;
        Thu, 26 Mar 2015 09:20:21 -0700 (PDT)
Received: from UNKNOWN-10-87-10-X.yahoo.com (nat-dip5.cfw-a-gci.corp.yahoo.com. [209.131.62.114])
        by mx.google.com with ESMTPSA id c2sm6008746pbu.73.2015.03.26.09.20.18
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 26 Mar 2015 09:20:18 -0700 (PDT)
Message-ID: <551431C0.4000403@berkeley.edu>
Date: Thu, 26 Mar 2015 09:20:16 -0700
From: John Canny <canny@berkeley.edu>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.10; rv:24.0) Gecko/20100101 Thunderbird/24.4.0
MIME-Version: 1.0
To: "Evan R. Sparks" <evan.sparks@gmail.com>, 
 "Ulanov, Alexander" <alexander.ulanov@hp.com>
CC: Sam Halliday <sam.halliday@gmail.com>, 
 "dev@spark.apache.org" <dev@spark.apache.org>,
 Xiangrui Meng <mengxr@gmail.com>, Joseph Bradley <joseph@databricks.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net> <CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net> <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com> <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com> <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com> <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net> <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net> <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net> <CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
In-Reply-To: <CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------010307020406000000080709"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------010307020406000000080709
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit

I mentioned this earlier in the thread, but I'll put it out again. Dense 
BLAS are not very important for most machine learning workloads: at 
least for non-image workloads in industry (and for image processing you 
would probably want a deep learning/SGD solution with convolution 
kernels). e.g. it was only relevant for 1/7 of our recent benchmarks, 
which should be a reasonable sample. What really matters is sparse BLAS 
performance. BIDMat is still an order of magnitude faster there. Those 
kernels are only in BIDMat, since NVIDIAs sparse BLAS dont perform well 
on power-law data.

Its also the case that the overall performance of an algorithm is 
determined by the slowest kernel, not the fastest. If the goal is to get 
closer to BIDMach's performance on typical problems, you need to make 
sure that every kernel goes at comparable speed. So the real question is 
how much faster MLLib routines do on a complete problem with/without GPU 
acceleration. For BIDMach, its close to a factor of 10. But that 
required running entirely on the GPU, and making sure every kernel is 
close to its limit.

-John

If you think nvblas would be helpful, you should try it in some 
end-to-end benchmarks.
On 3/25/15, 6:23 PM, Evan R. Sparks wrote:
> Yeah, much more reasonable - nice to know that we can get full GPU 
> performance from breeze/netlib-java - meaning there's no compelling 
> performance reason to switch out our current linear algebra library 
> (at least as far as this benchmark is concerned).
>
> Instead, it looks like a user guide for configuring Spark/MLlib to use 
> the right BLAS library will get us most of the way there. Or, would it 
> make sense to finally ship openblas compiled for some common platforms 
> (64-bit linux, windows, mac) directly with Spark - hopefully 
> eliminating the jblas warnings once and for all for most users? 
> (Licensing is BSD) Or am I missing something?
>
> On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander 
> <alexander.ulanov@hp.com <mailto:alexander.ulanov@hp.com>> wrote:
>
>     As everyone suggested, the results were too good to be true, so I
>     double-checked them. It turns that nvblas did not do
>     multiplication due to parameter NVBLAS_TILE_DIM from "nvblas.conf"
>     and returned zero matrix. My previously posted results with nvblas
>     are matrices copying only. The default NVBLAS_TILE_DIM==2048 is
>     too big for my graphic card/matrix size. I handpicked other values
>     that worked. As a result, netlib+nvblas is on par with
>     BIDMat-cuda. As promised, I am going to post a how-to for nvblas
>     configuration.
>
>     https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
>
>
>
>     -----Original Message-----
>     From: Ulanov, Alexander
>     Sent: Wednesday, March 25, 2015 2:31 PM
>     To: Sam Halliday
>     Cc: dev@spark.apache.org <mailto:dev@spark.apache.org>; Xiangrui
>     Meng; Joseph Bradley; Evan R. Sparks; jfcanny
>     Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>     Hi again,
>
>     I finally managed to use nvblas within Spark+netlib-java. It has
>     exceptional performance for big matrices with Double, faster than
>     BIDMat-cuda with Float. But for smaller matrices, if you will copy
>     them to/from GPU, OpenBlas or MKL might be a better choice. This
>     correlates with original nvblas presentation on GPU conf 2013
>     (slide 21):
>     http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>
>     My results:
>     https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
>
>     Just in case, these tests are not for generalization of
>     performance of different libraries. I just want to pick a library
>     that does at best dense matrices multiplication for my task.
>
>     P.S. My previous issue with nvblas was the following: it has
>     Fortran blas functions, at the same time netlib-java uses C cblas
>     functions. So, one needs cblas shared library to use nvblas
>     through netlib-java. Fedora does not have cblas (but Debian and
>     Ubuntu have), so I needed to compile it. I could not use cblas
>     from Atlas or Openblas because they link to their implementation
>     and not to Fortran blas.
>
>     Best regards, Alexander
>
>     -----Original Message-----
>     From: Ulanov, Alexander
>     Sent: Tuesday, March 24, 2015 6:57 PM
>     To: Sam Halliday
>     Cc: dev@spark.apache.org <mailto:dev@spark.apache.org>; Xiangrui
>     Meng; Joseph Bradley; Evan R. Sparks
>     Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>     Hi,
>
>     I am trying to use nvblas with netlib-java from Spark. nvblas
>     functions should replace current blas functions calls after
>     executing LD_PRELOAD as suggested in
>     http://docs.nvidia.com/cuda/nvblas/#Usage without any changes to
>     netlib-java. It seems to work for simple Java example, but I
>     cannot make it work with Spark. I run the following:
>     export LD_LIBRARY_PATH=/usr/local/cuda-6.5/lib64
>     env LD_PRELOAD=/usr/local/cuda-6.5/lib64/libnvblas.so
>     ./spark-shell --driver-memory 4G In nvidia-smi I observe that Java
>     is to use GPU:
>     +-----------------------------------------------------------------------------+
>     | Processes:            GPU Memory |
>     |  GPU       PID  Type  Process name            Usage      |
>     |=============================================================================|
>     |    0      8873    C   bash                 39MiB |
>     |    0      8910    C  /usr/lib/jvm/java-1.7.0/bin/java           
>         39MiB |
>     +-----------------------------------------------------------------------------+
>
>     In Spark shell I do matrix multiplication and see the following:
>     15/03/25 06:48:01 INFO JniLoader: successfully loaded
>     /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>     So I am sure that netlib-native is loaded and cblas supposedly
>     used. However, matrix multiplication does executes on CPU since I
>     see 16% of CPU used and 0% of GPU used. I also checked different
>     matrix sizes, from 100x100 to 12000x12000
>
>     Could you suggest might the LD_PRELOAD not affect Spark shell?
>
>     Best regards, Alexander
>
>
>
>     From: Sam Halliday [mailto:sam.halliday@gmail.com
>     <mailto:sam.halliday@gmail.com>]
>     Sent: Monday, March 09, 2015 6:01 PM
>     To: Ulanov, Alexander
>     Cc: dev@spark.apache.org <mailto:dev@spark.apache.org>; Xiangrui
>     Meng; Joseph Bradley; Evan R. Sparks
>     Subject: RE: Using CUDA within Spark / boosting linear algebra
>
>
>     Thanks so much for following up on this!
>
>     Hmm, I wonder if we should have a concerted effort to chart
>     performance on various pieces of hardware...
>     On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>> wrote:
>     Hi Everyone, I've updated the benchmark as Xiangrui suggested.
>     Added the comment that BIDMat 0.9.7 uses Float matrices in GPU
>     (although I see the support of Double in the current source code),
>     did the test with BIDMat and CPU Double matrices. BIDMat MKL is
>     indeed on par with netlib MKL.
>
>     https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9J5r7kwKSPkY/edit?usp=sharing
>
>     Best regards, Alexander
>
>     -----Original Message-----
>     From: Sam Halliday [mailto:sam.halliday@gmail.com
>     <mailto:sam.halliday@gmail.com><mailto:sam.halliday@gmail.com
>     <mailto:sam.halliday@gmail.com>>]
>     Sent: Tuesday, March 03, 2015 1:54 PM
>     To: Xiangrui Meng; Joseph Bradley
>     Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>
>     Subject: Re: Using CUDA within Spark / boosting linear algebra
>
>     BTW, is anybody on this list going to the London Meetup in a few
>     weeks?
>
>     https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapreduce-world#community
>
>     Would be nice to meet other people working on the guts of Spark! :-)
>
>
>     Xiangrui Meng <mengxr@gmail.com
>     <mailto:mengxr@gmail.com><mailto:mengxr@gmail.com
>     <mailto:mengxr@gmail.com>>> writes:
>
>     > Hey Alexander,
>     >
>     > I don't quite understand the part where netlib-cublas is about 20x
>     > slower than netlib-openblas. What is the overhead of using a GPU
>     BLAS
>     > with netlib-java?
>     >
>     > CC'ed Sam, the author of netlib-java.
>     >
>     > Best,
>     > Xiangrui
>     >
>     > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley
>     <joseph@databricks.com
>     <mailto:joseph@databricks.com><mailto:joseph@databricks.com
>     <mailto:joseph@databricks.com>>> wrote:
>     >> Better documentation for linking would be very helpful!  Here's
>     a JIRA:
>     >> https://issues.apache.org/jira/browse/SPARK-6019
>     >>
>     >>
>     >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>     >> <evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>>>
>     >> wrote:
>     >>
>     >>> Thanks for compiling all the data and running these benchmarks,
>     >>> Alex. The big takeaways here can be seen with this chart:
>     >>>
>     >>>
>     https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>     >>> Hl6kmAJeaZZggr0/pubchart?oid=1899767119&format=interactive
>     >>>
>     >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>     >>> BIDMat+GPU) can provide substantial (but less than an order of
>     >>> BIDMat+magnitude)
>     >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>     >>> netlib-java+openblas-compiled).
>     >>> 2) A poorly tuned CPU implementation can be 1-2 orders of
>     magnitude
>     >>> worse than a well-tuned CPU implementation, particularly for
>     larger matrices.
>     >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib -
>     this
>     >>> basically agrees with the authors own benchmarks (
>     >>> https://github.com/fommil/netlib-java)
>     >>>
>     >>> I think that most of our users are in a situation where using GPUs
>     >>> may not be practical - although we could consider having a
>     good GPU
>     >>> backend available as an option. However, *ALL* users of MLlib
>     could
>     >>> benefit (potentially tremendously) from using a well-tuned
>     CPU-based
>     >>> BLAS implementation. Perhaps we should consider updating the mllib
>     >>> guide with a more complete section for enabling high performance
>     >>> binaries on OSX and Linux? Or better, figure out a way for the
>     >>> system to fetch these automatically.
>     >>>
>     >>> - Evan
>     >>>
>     >>>
>     >>>
>     >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>     >>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>> wrote:
>     >>>
>     >>>> Just to summarize this thread, I was finally able to make all
>     >>>> performance comparisons that we discussed. It turns out that:
>     >>>> BIDMat-cublas>>BIDMat
>     >>>>
>     MKL==netlib-mkl==netlib-openblas-compiled>netlib-openblas-yum-repo=
>     >>>> =netlib-cublas>netlib-blas>f2jblas
>     >>>>
>     >>>> Below is the link to the spreadsheet with full results.
>     >>>>
>     >>>>
>     https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>     >>>> 378T9J5r7kwKSPkY/edit?usp=sharing
>     >>>>
>     >>>> One thing still needs exploration: does BIDMat-cublas perform
>     >>>> copying to/from machine’s RAM?
>     >>>>
>     >>>> -----Original Message-----
>     >>>> From: Ulanov, Alexander
>     >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>     >>>> To: Evan R. Sparks
>     >>>> Cc: Joseph Bradley;
>     >>>> dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>
>     >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>     >>>>
>     >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>     >>>> the original one discusses slightly different topic. I was
>     able to
>     >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>     >>>> statically linked inside a 60MB library.
>     >>>>
>     >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>     >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>     >>>>
>     +-----------------------------------------------------------------------+
>     >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 |
>     0,002556 |
>     >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>     >>>> |1,638475459 |
>     >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697
>     |445,0935211 |
>     >>>> 1569,233228 |
>     >>>>
>     >>>> It turn out that pre-compiled MKL is faster than precompiled
>     >>>> OpenBlas on my machine. Probably, I’ll add two more columns with
>     >>>> locally compiled openblas and cuda.
>     >>>>
>     >>>> Alexander
>     >>>>
>     >>>> From: Evan R. Sparks
>     >>>> [mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>>]
>     >>>> Sent: Monday, February 09, 2015 6:06 PM
>     >>>> To: Ulanov, Alexander
>     >>>> Cc: Joseph Bradley;
>     >>>> dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>
>     >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>     >>>>
>     >>>> Great - perhaps we can move this discussion off-list and onto a
>     >>>> JIRA ticket? (Here's one:
>     >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>     >>>>
>     >>>> It seems like this is going to be somewhat exploratory for a
>     while
>     >>>> (and there's probably only a handful of us who really care about
>     >>>> fast linear
>     >>>> algebra!)
>     >>>>
>     >>>> - Evan
>     >>>>
>     >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>>> wrote:
>     >>>> Hi Evan,
>     >>>>
>     >>>> Thank you for explanation and useful link. I am going to build
>     >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>     >>>>
>     >>>> Do I understand correctly that BIDMat binaries contain statically
>     >>>> linked Intel MKL BLAS? It might be the reason why I am able
>     to run
>     >>>> BIDMat not having MKL BLAS installed on my server. If it is
>     true, I
>     >>>> wonder if it is OK because Intel sells this library.
>     Nevertheless,
>     >>>> it seems that in my case precompiled MKL BLAS performs better
>     than
>     >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are
>     supposed to be on par with JNI overheads.
>     >>>>
>     >>>> Though, it might be interesting to link Netlib-java with
>     Intel MKL,
>     >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>     >>>> Halliday
>     >>>> (Netlib-java) interested to compare their libraries.
>     >>>>
>     >>>> Best regards, Alexander
>     >>>>
>     >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>><mailto:
>     >>>> evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>>>]
>     >>>> Sent: Friday, February 06, 2015 5:58 PM
>     >>>>
>     >>>> To: Ulanov, Alexander
>     >>>> Cc: Joseph Bradley;
>     >>>> dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
>     >>>> apache.org <http://apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>>
>     >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>     >>>>
>     >>>> I would build OpenBLAS yourself, since good BLAS performance
>     comes
>     >>>> from getting cache sizes, etc. set up correctly for your
>     particular
>     >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>     >>>> but we found that on relatively modern Xeon chips, OpenBLAS
>     builds
>     >>>> quickly and yields performance competitive with MKL.
>     >>>>
>     >>>> To make sure the right library is getting used, you have to make
>     >>>> sure it's first on the search path - export
>     >>>> LD_LIBRARY_PATH=/path/to/blas/library.so will do the trick here.
>     >>>>
>     >>>> For some examples of getting netlib-java setup on an ec2 node and
>     >>>> some example benchmarking code we ran a while back, see:
>     >>>> https://github.com/shivaram/matrix-bench
>     >>>>
>     >>>> In particular - build-openblas-ec2.sh shows you how to build the
>     >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>     >>>> shows you how to get the path setup and get that library
>     picked up by netlib-java.
>     >>>>
>     >>>> In this way - you could probably get cuBLAS set up to be used by
>     >>>> netlib-java as well.
>     >>>>
>     >>>> - Evan
>     >>>>
>     >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>>> wrote:
>     >>>> Evan, could you elaborate on how to force BIDMat and
>     netlib-java to
>     >>>> force loading the right blas? For netlib, I there are few JVM
>     >>>> flags, such as
>     >>>> -Dcom.github.fommil.netlib.BLAS=com.github.fommil.netlib.F2jBLAS,
>     >>>> so I can force it to use Java implementation. Not sure I
>     understand how to force use a specific blas (not specific wrapper
>     for blas).
>     >>>>
>     >>>> Btw. I have installed openblas (yum install openblas), so I
>     suppose
>     >>>> that netlib is using it.
>     >>>>
>     >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>><mailto:
>     >>>> evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>>>]
>     >>>> Sent: Friday, February 06, 2015 5:19 PM
>     >>>> To: Ulanov, Alexander
>     >>>> Cc: Joseph Bradley;
>     >>>> dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
>     >>>> apache.org <http://apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>>
>     >>>>
>     >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>     >>>>
>     >>>> Getting breeze to pick up the right blas library is critical for
>     >>>> performance. I recommend using OpenBLAS (or MKL, if you
>     already have it).
>     >>>> It might make sense to force BIDMat to use the same
>     underlying BLAS
>     >>>> library as well.
>     >>>>
>     >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>>> wrote:
>     >>>> Hi Evan, Joseph
>     >>>>
>     >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>     >>>> faster than netlib-java+breeze (sorry for weird table
>     formatting):
>     >>>>
>     >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>     >>>> |native_system_linux_x86-64|
>     >>>> Breeze+Netlib-java f2jblas |
>     >>>>
>     +-----------------------------------------------------------------------+
>     >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>     >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>     >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 |
>     1569,233228
>     >>>> ||
>     >>>>
>     >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM,
>     Fedora
>     >>>> 19 Linux, Scala 2.11.
>     >>>>
>     >>>> Later I will make tests with Cuda. I need to install new Cuda
>     >>>> version for this purpose.
>     >>>>
>     >>>> Do you have any ideas why breeze-netlib with native blas is
>     so much
>     >>>> slower than BIDMat MKL?
>     >>>>
>     >>>> Best regards, Alexander
>     >>>>
>     >>>> From: Joseph Bradley [mailto:joseph@databricks.com
>     <mailto:joseph@databricks.com><mailto:joseph@databricks.com
>     <mailto:joseph@databricks.com>><mailto:
>     >>>> joseph@databricks.com
>     <mailto:joseph@databricks.com><mailto:joseph@databricks.com
>     <mailto:joseph@databricks.com>>>]
>     >>>> Sent: Thursday, February 05, 2015 5:29 PM
>     >>>> To: Ulanov, Alexander
>     >>>> Cc: Evan R. Sparks;
>     >>>> dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
>     >>>> apache.org <http://apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>>
>     >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>     >>>>
>     >>>> Hi Alexander,
>     >>>>
>     >>>> Using GPUs with Spark would be very exciting.  Small comment:
>     >>>> Concerning your question earlier about keeping data stored on the
>     >>>> GPU rather than having to move it between main memory and GPU
>     >>>> memory on each iteration, I would guess this would be critical to
>     >>>> getting good performance.  If you could do multiple local
>     >>>> iterations before aggregating results, then the cost of data
>     >>>> movement to the GPU could be amortized (and I believe that is
>     done
>     >>>> in practice).  Having Spark be aware of the GPU and using it
>     as another part of memory sounds like a much bigger undertaking.
>     >>>>
>     >>>> Joseph
>     >>>>
>     >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>>> wrote:
>     >>>> Thank you for explanation! I’ve watched the BIDMach
>     presentation by
>     >>>> John Canny and I am really inspired by his talk and
>     comparisons with Spark MLlib.
>     >>>>
>     >>>> I am very interested to find out what will be better within
>     Spark:
>     >>>> BIDMat or netlib-java with CPU or GPU natives. Could you
>     suggest a
>     >>>> fair way to benchmark them? Currently I do benchmarks on
>     artificial
>     >>>> neural networks in batch mode. While it is not a “pure” test of
>     >>>> linear algebra, it involves some other things that are
>     essential to machine learning.
>     >>>>
>     >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>><mailto:
>     >>>> evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>>>]
>     >>>> Sent: Thursday, February 05, 2015 1:29 PM
>     >>>> To: Ulanov, Alexander
>     >>>> Cc:
>     >>>> dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
>     >>>> apache.org <http://apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>>
>     >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>     >>>>
>     >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>     >>>> netlib-java+OpenBLAS, but if it is much faster it's probably
>     due to
>     >>>> netlib-java+data
>     >>>> layout and fewer levels of indirection - it's definitely a
>     >>>> worthwhile experiment to run. The main speedups I've seen from
>     >>>> using it come from highly optimized GPU code for linear
>     algebra. I
>     >>>> know that in the past Canny has gone as far as to write
>     custom GPU
>     >>>> kernels for performance-critical regions of code.[1]
>     >>>>
>     >>>> BIDMach is highly optimized for single node performance or
>     >>>> performance on small clusters.[2] Once data doesn't fit easily in
>     >>>> GPU memory (or can be batched in that way) the performance
>     tends to
>     >>>> fall off. Canny argues for hardware/software codesign and as such
>     >>>> prefers machine configurations that are quite different than what
>     >>>> we find in most commodity cluster nodes - e.g. 10 disk
>     cahnnels and 4 GPUs.
>     >>>>
>     >>>> In contrast, MLlib was designed for horizontal scalability on
>     >>>> commodity clusters and works best on very big datasets -
>     order of terabytes.
>     >>>>
>     >>>> For the most part, these projects developed concurrently to
>     address
>     >>>> slightly different use cases. That said, there may be bits of
>     >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>     >>>> careful about maintaining cross-language compatibility for
>     our Java
>     >>>> and Python-users, though.
>     >>>>
>     >>>> - Evan
>     >>>>
>     >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>     >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>     <http://eecs.berkeley.edu/%7Ehzhao/papers/BD.pdf>
>     >>>>
>     >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>><mailto:
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>>>> wrote:
>     >>>> Hi Evan,
>     >>>>
>     >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>     >>>> you know what makes them faster than netlib-java?
>     >>>>
>     >>>> The same group has BIDMach library that implements machine
>     >>>> learning. For some examples they use Caffe convolutional neural
>     >>>> network library owned by another group in Berkeley. Could you
>     >>>> elaborate on how these all might be connected with Spark
>     Mllib? If
>     >>>> you take BIDMat for linear algebra why don’t you take BIDMach
>     for optimization and learning?
>     >>>>
>     >>>> Best regards, Alexander
>     >>>>
>     >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>><mailto:
>     >>>> evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>>><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>><mailto:
>     >>>> evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com><mailto:evan.sparks@gmail.com
>     <mailto:evan.sparks@gmail.com>>>>]
>     >>>> Sent: Thursday, February 05, 2015 12:09 PM
>     >>>> To: Ulanov, Alexander
>     >>>> Cc: dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>><mailto:
>     >>>> dev@spark.apache.org
>     <mailto:dev@spark.apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>><mailto:dev@spark <mailto:dev@spark>.
>     >>>> apache.org <http://apache.org><mailto:dev@spark.apache.org
>     <mailto:dev@spark.apache.org>>>>
>     >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>     >>>>
>     >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>     >>>> blas in many cases.
>     >>>>
>     >>>> You might consider taking a look at the codepaths that BIDMat (
>     >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>     >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>     >>>> optimizing to make this work really fast from Scala. I've run
>     it on
>     >>>> my laptop and compared to MKL and in certain cases it's 10x
>     faster at matrix multiply.
>     >>>> There are a lot of layers of indirection here and you really want
>     >>>> to avoid data copying as much as possible.
>     >>>>
>     >>>> We could also consider swapping out BIDMat for Breeze, but that
>     >>>> would be a big project and if we can figure out how to get
>     >>>> breeze+cublas to comparable performance that would be a big win.
>     >>>>
>     >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>><mailto:
>     >>>> alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com
>     <mailto:alexander.ulanov@hp.com>>>>> wrote:
>     >>>> Dear Spark developers,
>     >>>>
>     >>>> I am exploring how to make linear algebra operations faster
>     within Spark.
>     >>>> One way of doing this is to use Scala Breeze library that is
>     >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>     >>>> that has a Java wrapper for BLAS (basic linear algebra
>     subprograms)
>     >>>> and LAPACK native binaries if they are available on the worker
>     >>>> node. It also has its own optimized Java implementation of
>     BLAS. It
>     >>>> is worth mentioning, that native binaries provide better
>     performance only for BLAS level 3, i.e.
>     >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>     >>>> This is confirmed by GEMM test on Netlib-java page
>     >>>> https://github.com/fommil/netlib-java. I also confirmed it
>     with my
>     >>>> experiments with training of artificial neural network
>     >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>     >>>> However, I would like to boost performance more.
>     >>>>
>     >>>> GPU is supposed to work fast with linear algebra and there is
>     >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one
>     Linux
>     >>>> server with Nvidia GPU and I was able to do the following. I
>     linked
>     >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper
>     and put
>     >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>     >>>> performance measurements with regards to artificial neural
>     network
>     >>>> batch learning in Spark MLlib that involves matrix-matrix
>     >>>> multiplications. It turns out that for matrices of size less than
>     >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas
>     becomes
>     >>>> slower for bigger matrices. It worth mentioning that it is
>     was not a test for ONLY multiplication since there are other
>     operations involved.
>     >>>> One of the reasons for slowdown might be the overhead of copying
>     >>>> the matrices from computer memory to graphic card memory and
>     back.
>     >>>>
>     >>>> So, few questions:
>     >>>> 1) Do these results with CUDA make sense?
>     >>>> 2) If the problem is with copy overhead, are there any libraries
>     >>>> that allow to force intermediate results to stay in graphic card
>     >>>> memory thus removing the overhead?
>     >>>> 3) Any other options to speed-up linear algebra in Spark?
>     >>>>
>     >>>> Thank you, Alexander
>     >>>>
>     >>>>
>     -------------------------------------------------------------------
>     >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org>><mailto:
>     >>>> dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spark.apach
>     <mailto:dev-unsubscribe@spark.apach>
>     >>>> e.org <http://e.org>>><mailto:dev-unsubscribe@spark.apac
>     <mailto:dev-unsubscribe@spark.apac><mailto:dev-unsubscribe@sp
>     <mailto:dev-unsubscribe@sp>
>     >>>> ark.apac> he.org <http://he.org><http://he.org>
>     >>>> <mailto:dev-unsubscribe@spark.apache.org
>     <mailto:dev-unsubscribe@spark.apache.org><mailto:dev-unsubscribe@spa
>     <mailto:dev-unsubscribe@spa>
>     >>>> rk.apache.org <http://rk.apache.org>>>> For additional
>     commands, e-mail:
>     >>>> dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>><mailto:
>     >>>> dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>>><mailto:dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>><mailto:
>     >>>> dev-help@spark.apache.org
>     <mailto:dev-help@spark.apache.org><mailto:dev-help@spark.apache.org <mailto:dev-help@spark.apache.org>>>>
>     >>>>
>     >>>>
>     >>>>
>     >>>>
>     >>>
>
>     --
>     Best regards,
>     Sam
>
>


--------------010307020406000000080709--

From dev-return-12210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 16:27:20 2015
Return-Path: <dev-return-12210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 508E117513
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 16:27:20 +0000 (UTC)
Received: (qmail 78610 invoked by uid 500); 26 Mar 2015 16:27:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78536 invoked by uid 500); 26 Mar 2015 16:27:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78524 invoked by uid 99); 26 Mar 2015 16:27:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:27:18 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sam.halliday@gmail.com designates 209.85.213.173 as permitted sender)
Received: from [209.85.213.173] (HELO mail-ig0-f173.google.com) (209.85.213.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:27:13 +0000
Received: by igcau2 with SMTP id au2so16367343igc.1
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 09:26:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=cGH3/Cfvt6rPpBVUdFV4BpDDzdvNM2K5kFi/VJIwPxw=;
        b=VlHYSxbSxTCG7fWr3nsXV+uT7uMlrsEG5BBwXGKkDLHP1Uze0g8Uhlbipx5K6qkavz
         rjHTj1zMIdX+5w/y8MfOSHmiO5+ivMurwLQfa24ACsVB453vpRvwYV5SFESEOyrUN0AQ
         2y2OjMUerTwyppCETwURePh2Zr8F5JHwaA2eNJ/INWEhdQqjv8lAGl4w+O29X0gk35uY
         b/eqmdfTNV1px6wSlIpuV9qnFoySy8tEbsMJjjBd1NOuAe1HNoFFfxv85K1Rya8GLaYX
         TvmKy7+y15IbPseIqd0D+Qw3tSCSLCtu/H0/lLd3plRJ6dHRRcVEpNtOPETWHeXymYsU
         ZaeQ==
MIME-Version: 1.0
X-Received: by 10.50.129.9 with SMTP id ns9mr37704601igb.24.1427387213093;
 Thu, 26 Mar 2015 09:26:53 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Thu, 26 Mar 2015 09:26:52 -0700 (PDT)
Received: by 10.36.39.69 with HTTP; Thu, 26 Mar 2015 09:26:52 -0700 (PDT)
In-Reply-To: <551431C0.4000403@berkeley.edu>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
	<CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
	<551431C0.4000403@berkeley.edu>
Date: Thu, 26 Mar 2015 16:26:52 +0000
Message-ID: <CALR_T9CuUNfHkGqZYObfVqxHPCC5WOYzE8B8Xfh5iVKMq=X4Cg@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Sam Halliday <sam.halliday@gmail.com>
To: John Canny <canny@berkeley.edu>
Cc: Xiangrui Meng <mengxr@gmail.com>, dev@spark.apache.org, 
	Joseph Bradley <joseph@databricks.com>, "Evan R. Sparks" <evan.sparks@gmail.com>, 
	"Ulanov, Alexander" <alexander.ulanov@hp.com>
Content-Type: multipart/alternative; boundary=047d7b343c9ccb223d0512337bd8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b343c9ccb223d0512337bd8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

John, I have to disagree with you there. Dense matrices come up a lot in
industry,  although your personal experience may be different.
On 26 Mar 2015 16:20, "John Canny" <canny@berkeley.edu> wrote:

>  I mentioned this earlier in the thread, but I'll put it out again. Dense
> BLAS are not very important for most machine learning workloads: at least
> for non-image workloads in industry (and for image processing you would
> probably want a deep learning/SGD solution with convolution kernels). e.g=
.
> it was only relevant for 1/7 of our recent benchmarks, which should be a
> reasonable sample. What really matters is sparse BLAS performance. BIDMat
> is still an order of magnitude faster there. Those kernels are only in
> BIDMat, since NVIDIAs sparse BLAS dont perform well on power-law data.
>
> Its also the case that the overall performance of an algorithm is
> determined by the slowest kernel, not the fastest. If the goal is to get
> closer to BIDMach's performance on typical problems, you need to make sur=
e
> that every kernel goes at comparable speed. So the real question is how
> much faster MLLib routines do on a complete problem with/without GPU
> acceleration. For BIDMach, its close to a factor of 10. But that required
> running entirely on the GPU, and making sure every kernel is close to its
> limit.
>
> -John
>
> If you think nvblas would be helpful, you should try it in some end-to-en=
d
> benchmarks.
> On 3/25/15, 6:23 PM, Evan R. Sparks wrote:
>
> Yeah, much more reasonable - nice to know that we can get full GPU
> performance from breeze/netlib-java - meaning there's no compelling
> performance reason to switch out our current linear algebra library (at
> least as far as this benchmark is concerned).
>
>  Instead, it looks like a user guide for configuring Spark/MLlib to use
> the right BLAS library will get us most of the way there. Or, would it ma=
ke
> sense to finally ship openblas compiled for some common platforms (64-bit
> linux, windows, mac) directly with Spark - hopefully eliminating the jbla=
s
> warnings once and for all for most users? (Licensing is BSD) Or am I
> missing something?
>
> On Wed, Mar 25, 2015 at 6:03 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
>
>> As everyone suggested, the results were too good to be true, so I
>> double-checked them. It turns that nvblas did not do multiplication due =
to
>> parameter NVBLAS_TILE_DIM from "nvblas.conf" and returned zero matrix. M=
y
>> previously posted results with nvblas are matrices copying only. The
>> default NVBLAS_TILE_DIM=3D=3D2048 is too big for my graphic card/matrix =
size. I
>> handpicked other values that worked. As a result, netlib+nvblas is on pa=
r
>> with BIDMat-cuda. As promised, I am going to post a how-to for nvblas
>> configuration.
>>
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>>
>>
>> -----Original Message-----
>> From: Ulanov, Alexander
>> Sent: Wednesday, March 25, 2015 2:31 PM
>> To: Sam Halliday
>>  Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R.
>> Sparks; jfcanny
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>> Hi again,
>>
>> I finally managed to use nvblas within Spark+netlib-java. It has
>> exceptional performance for big matrices with Double, faster than
>> BIDMat-cuda with Float. But for smaller matrices, if you will copy them
>> to/from GPU, OpenBlas or MKL might be a better choice. This correlates w=
ith
>> original nvblas presentation on GPU conf 2013 (slide 21):
>> http://on-demand.gputechconf.com/supercomputing/2013/presentation/SC3108=
-New-Features-CUDA%206%20-GPU-Acceleration.pdf
>>
>> My results:
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Just in case, these tests are not for generalization of performance of
>> different libraries. I just want to pick a library that does at best den=
se
>> matrices multiplication for my task.
>>
>> P.S. My previous issue with nvblas was the following: it has Fortran bla=
s
>> functions, at the same time netlib-java uses C cblas functions. So, one
>> needs cblas shared library to use nvblas through netlib-java. Fedora doe=
s
>> not have cblas (but Debian and Ubuntu have), so I needed to compile it. =
I
>> could not use cblas from Atlas or Openblas because they link to their
>> implementation and not to Fortran blas.
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Ulanov, Alexander
>> Sent: Tuesday, March 24, 2015 6:57 PM
>> To: Sam Halliday
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>> Hi,
>>
>> I am trying to use nvblas with netlib-java from Spark. nvblas functions
>> should replace current blas functions calls after executing LD_PRELOAD a=
s
>> suggested in http://docs.nvidia.com/cuda/nvblas/#Usage without any
>> changes to netlib-java. It seems to work for simple Java example, but I
>> cannot make it work with Spark. I run the following:
>> export LD_LIBRARY_PATH=3D/usr/local/cuda-6.5/lib64
>> env LD_PRELOAD=3D/usr/local/cuda-6.5/lib64/libnvblas.so ./spark-shell
>> --driver-memory 4G In nvidia-smi I observe that Java is to use GPU:
>>
>> +-----------------------------------------------------------------------=
------+
>> | Processes:                                                       GPU
>> Memory |
>> |  GPU       PID  Type  Process name                               Usage
>>     |
>>
>> |=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D|
>> |    0      8873    C   bash
>> 39MiB |
>> |    0      8910    C   /usr/lib/jvm/java-1.7.0/bin/java
>> 39MiB |
>>
>> +-----------------------------------------------------------------------=
------+
>>
>> In Spark shell I do matrix multiplication and see the following:
>> 15/03/25 06:48:01 INFO JniLoader: successfully loaded
>> /tmp/jniloader8192964377009965483netlib-native_system-linux-x86_64.so
>> So I am sure that netlib-native is loaded and cblas supposedly used.
>> However, matrix multiplication does executes on CPU since I see 16% of C=
PU
>> used and 0% of GPU used. I also checked different matrix sizes, from
>> 100x100 to 12000x12000
>>
>> Could you suggest might the LD_PRELOAD not affect Spark shell?
>>
>> Best regards, Alexander
>>
>>
>>
>> From: Sam Halliday [mailto:sam.halliday@gmail.com]
>> Sent: Monday, March 09, 2015 6:01 PM
>> To: Ulanov, Alexander
>> Cc: dev@spark.apache.org; Xiangrui Meng; Joseph Bradley; Evan R. Sparks
>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>>
>>
>> Thanks so much for following up on this!
>>
>> Hmm, I wonder if we should have a concerted effort to chart performance
>> on various pieces of hardware...
>> On 9 Mar 2015 21:08, "Ulanov, Alexander" <alexander.ulanov@hp.com<mailto=
:
>> alexander.ulanov@hp.com>> wrote:
>> Hi Everyone, I've updated the benchmark as Xiangrui suggested. Added the
>> comment that BIDMat 0.9.7 uses Float matrices in GPU (although I see the
>> support of Double in the current source code), did the test with BIDMat =
and
>> CPU Double matrices. BIDMat MKL is indeed on par with netlib MKL.
>>
>>
>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx378T9=
J5r7kwKSPkY/edit?usp=3Dsharing
>>
>> Best regards, Alexander
>>
>> -----Original Message-----
>> From: Sam Halliday [mailto:sam.halliday@gmail.com<mailto:
>> sam.halliday@gmail.com>]
>> Sent: Tuesday, March 03, 2015 1:54 PM
>> To: Xiangrui Meng; Joseph Bradley
>> Cc: Evan R. Sparks; Ulanov, Alexander; dev@spark.apache.org<mailto:
>> dev@spark.apache.org>
>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>>
>> BTW, is anybody on this list going to the London Meetup in a few weeks?
>>
>>
>> https://skillsmatter.com/meetups/6987-apache-spark-living-the-post-mapre=
duce-world#community
>>
>> Would be nice to meet other people working on the guts of Spark! :-)
>>
>>
>> Xiangrui Meng <mengxr@gmail.com<mailto:mengxr@gmail.com>> writes:
>>
>> > Hey Alexander,
>> >
>> > I don't quite understand the part where netlib-cublas is about 20x
>> > slower than netlib-openblas. What is the overhead of using a GPU BLAS
>> > with netlib-java?
>> >
>> > CC'ed Sam, the author of netlib-java.
>> >
>> > Best,
>> > Xiangrui
>> >
>> > On Wed, Feb 25, 2015 at 3:36 PM, Joseph Bradley <joseph@databricks.com
>> <mailto:joseph@databricks.com>> wrote:
>> >> Better documentation for linking would be very helpful!  Here's a JIR=
A:
>> >> https://issues.apache.org/jira/browse/SPARK-6019
>> >>
>> >>
>> >> On Wed, Feb 25, 2015 at 2:53 PM, Evan R. Sparks
>> >> <evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>
>> >> wrote:
>> >>
>> >>> Thanks for compiling all the data and running these benchmarks,
>> >>> Alex. The big takeaways here can be seen with this chart:
>> >>>
>> >>> https://docs.google.com/spreadsheets/d/1aRm2IADRfXQV7G2vrcVh4StF50uZ
>> >>> Hl6kmAJeaZZggr0/pubchart?oid=3D1899767119&format=3Dinteractive
>> >>>
>> >>> 1) A properly configured GPU matrix multiply implementation (e.g.
>> >>> BIDMat+GPU) can provide substantial (but less than an order of
>> >>> BIDMat+magnitude)
>> >>> benefit over a well-tuned CPU implementation (e.g. BIDMat+MKL or
>> >>> netlib-java+openblas-compiled).
>> >>> 2) A poorly tuned CPU implementation can be 1-2 orders of magnitude
>> >>> worse than a well-tuned CPU implementation, particularly for larger
>> matrices.
>> >>> (netlib-f2jblas or netlib-ref) This is not to pick on netlib - this
>> >>> basically agrees with the authors own benchmarks (
>> >>> https://github.com/fommil/netlib-java)
>> >>>
>> >>> I think that most of our users are in a situation where using GPUs
>> >>> may not be practical - although we could consider having a good GPU
>> >>> backend available as an option. However, *ALL* users of MLlib could
>> >>> benefit (potentially tremendously) from using a well-tuned CPU-based
>> >>> BLAS implementation. Perhaps we should consider updating the mllib
>> >>> guide with a more complete section for enabling high performance
>> >>> binaries on OSX and Linux? Or better, figure out a way for the
>> >>> system to fetch these automatically.
>> >>>
>> >>> - Evan
>> >>>
>> >>>
>> >>>
>> >>> On Thu, Feb 12, 2015 at 4:18 PM, Ulanov, Alexander <
>> >>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
>> >>>
>> >>>> Just to summarize this thread, I was finally able to make all
>> >>>> performance comparisons that we discussed. It turns out that:
>> >>>> BIDMat-cublas>>BIDMat
>> >>>> MKL=3D=3Dnetlib-mkl=3D=3Dnetlib-openblas-compiled>netlib-openblas-y=
um-repo=3D
>> >>>> =3Dnetlib-cublas>netlib-blas>f2jblas
>> >>>>
>> >>>> Below is the link to the spreadsheet with full results.
>> >>>>
>> >>>> https://docs.google.com/spreadsheets/d/1lWdVSuSragOobb0A_oeouQgHUMx
>> >>>> 378T9J5r7kwKSPkY/edit?usp=3Dsharing
>> >>>>
>> >>>> One thing still needs exploration: does BIDMat-cublas perform
>> >>>> copying to/from machine=E2=80=99s RAM?
>> >>>>
>> >>>> -----Original Message-----
>> >>>> From: Ulanov, Alexander
>> >>>> Sent: Tuesday, February 10, 2015 2:12 PM
>> >>>> To: Evan R. Sparks
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: RE: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Thanks, Evan! It seems that ticket was marked as duplicate though
>> >>>> the original one discusses slightly different topic. I was able to
>> >>>> link netlib with MKL from BIDMat binaries. Indeed, MKL is
>> >>>> statically linked inside a 60MB library.
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-MKL  from BIDMat|
>> >>>> Breeze+Netlib-OpenBlas(native system)| Breeze+Netlib-f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,000381 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,038316857 | 0,51803557
>> >>>> |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 32,94546697 |445,0935211 |
>> >>>> 1569,233228 |
>> >>>>
>> >>>> It turn out that pre-compiled MKL is faster than precompiled
>> >>>> OpenBlas on my machine. Probably, I=E2=80=99ll add two more columns=
 with
>> >>>> locally compiled openblas and cuda.
>> >>>>
>> >>>> Alexander
>> >>>>
>> >>>> From: Evan R. Sparks
>> >>>> [mailto:evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>]
>> >>>> Sent: Monday, February 09, 2015 6:06 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Great - perhaps we can move this discussion off-list and onto a
>> >>>> JIRA ticket? (Here's one:
>> >>>> https://issues.apache.org/jira/browse/SPARK-5705)
>> >>>>
>> >>>> It seems like this is going to be somewhat exploratory for a while
>> >>>> (and there's probably only a handful of us who really care about
>> >>>> fast linear
>> >>>> algebra!)
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Mon, Feb 9, 2015 at 4:48 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for explanation and useful link. I am going to build
>> >>>> OpenBLAS, link it with Netlib-java and perform benchmark again.
>> >>>>
>> >>>> Do I understand correctly that BIDMat binaries contain statically
>> >>>> linked Intel MKL BLAS? It might be the reason why I am able to run
>> >>>> BIDMat not having MKL BLAS installed on my server. If it is true, I
>> >>>> wonder if it is OK because Intel sells this library. Nevertheless,
>> >>>> it seems that in my case precompiled MKL BLAS performs better than
>> >>>> precompiled OpenBLAS given that BIDMat and Netlib-java are supposed
>> to be on par with JNI overheads.
>> >>>>
>> >>>> Though, it might be interesting to link Netlib-java with Intel MKL,
>> >>>> as you suggested. I wonder, are John Canny (BIDMat) and Sam
>> >>>> Halliday
>> >>>> (Netlib-java) interested to compare their libraries.
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:58 PM
>> >>>>
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I would build OpenBLAS yourself, since good BLAS performance comes
>> >>>> from getting cache sizes, etc. set up correctly for your particular
>> >>>> hardware - this is often a very tricky process (see, e.g. ATLAS),
>> >>>> but we found that on relatively modern Xeon chips, OpenBLAS builds
>> >>>> quickly and yields performance competitive with MKL.
>> >>>>
>> >>>> To make sure the right library is getting used, you have to make
>> >>>> sure it's first on the search path - export
>> >>>> LD_LIBRARY_PATH=3D/path/to/blas/library.so will do the trick here.
>> >>>>
>> >>>> For some examples of getting netlib-java setup on an ec2 node and
>> >>>> some example benchmarking code we ran a while back, see:
>> >>>> https://github.com/shivaram/matrix-bench
>> >>>>
>> >>>> In particular - build-openblas-ec2.sh shows you how to build the
>> >>>> library and set up symlinks correctly, and scala/run-netlib.sh
>> >>>> shows you how to get the path setup and get that library picked up
>> by netlib-java.
>> >>>>
>> >>>> In this way - you could probably get cuBLAS set up to be used by
>> >>>> netlib-java as well.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 5:43 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Evan, could you elaborate on how to force BIDMat and netlib-java to
>> >>>> force loading the right blas? For netlib, I there are few JVM
>> >>>> flags, such as
>> >>>> -Dcom.github.fommil.netlib.BLAS=3Dcom.github.fommil.netlib.F2jBLAS,
>> >>>> so I can force it to use Java implementation. Not sure I understand
>> how to force use a specific blas (not specific wrapper for blas).
>> >>>>
>> >>>> Btw. I have installed openblas (yum install openblas), so I suppose
>> >>>> that netlib is using it.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Friday, February 06, 2015 5:19 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Joseph Bradley;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Getting breeze to pick up the right blas library is critical for
>> >>>> performance. I recommend using OpenBLAS (or MKL, if you already hav=
e
>> it).
>> >>>> It might make sense to force BIDMat to use the same underlying BLAS
>> >>>> library as well.
>> >>>>
>> >>>> On Fri, Feb 6, 2015 at 4:42 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Hi Evan, Joseph
>> >>>>
>> >>>> I did few matrix multiplication test and BIDMat seems to be ~10x
>> >>>> faster than netlib-java+breeze (sorry for weird table formatting):
>> >>>>
>> >>>> |A*B  size | BIDMat MKL | Breeze+Netlib-java
>> >>>> |native_system_linux_x86-64|
>> >>>> Breeze+Netlib-java f2jblas |
>> >>>>
>> +-----------------------------------------------------------------------=
+
>> >>>> |100x100*100x100 | 0,00205596 | 0,03810324 | 0,002556 |
>> >>>> |1000x1000*1000x1000 | 0,018320947 | 0,51803557 |1,638475459 |
>> >>>> |10000x10000*10000x10000 | 23,78046632 | 445,0935211 | 1569,233228
>> >>>> ||
>> >>>>
>> >>>> Configuration: Intel(R) Xeon(R) CPU E31240 3.3 GHz, 6GB RAM, Fedora
>> >>>> 19 Linux, Scala 2.11.
>> >>>>
>> >>>> Later I will make tests with Cuda. I need to install new Cuda
>> >>>> version for this purpose.
>> >>>>
>> >>>> Do you have any ideas why breeze-netlib with native blas is so much
>> >>>> slower than BIDMat MKL?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Joseph Bradley [mailto:joseph@databricks.com<mailto:
>> joseph@databricks.com><mailto:
>> >>>> joseph@databricks.com<mailto:joseph@databricks.com>>]
>> >>>> Sent: Thursday, February 05, 2015 5:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: Evan R. Sparks;
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> Hi Alexander,
>> >>>>
>> >>>> Using GPUs with Spark would be very exciting.  Small comment:
>> >>>> Concerning your question earlier about keeping data stored on the
>> >>>> GPU rather than having to move it between main memory and GPU
>> >>>> memory on each iteration, I would guess this would be critical to
>> >>>> getting good performance.  If you could do multiple local
>> >>>> iterations before aggregating results, then the cost of data
>> >>>> movement to the GPU could be amortized (and I believe that is done
>> >>>> in practice).  Having Spark be aware of the GPU and using it as
>> another part of memory sounds like a much bigger undertaking.
>> >>>>
>> >>>> Joseph
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 4:59 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
>> >>>> Thank you for explanation! I=E2=80=99ve watched the BIDMach present=
ation by
>> >>>> John Canny and I am really inspired by his talk and comparisons wit=
h
>> Spark MLlib.
>> >>>>
>> >>>> I am very interested to find out what will be better within Spark:
>> >>>> BIDMat or netlib-java with CPU or GPU natives. Could you suggest a
>> >>>> fair way to benchmark them? Currently I do benchmarks on artificial
>> >>>> neural networks in batch mode. While it is not a =E2=80=9Cpure=E2=
=80=9D test of
>> >>>> linear algebra, it involves some other things that are essential to
>> machine learning.
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>]
>> >>>> Sent: Thursday, February 05, 2015 1:29 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd be surprised of BIDMat+OpenBLAS was significantly faster than
>> >>>> netlib-java+OpenBLAS, but if it is much faster it's probably due to
>> >>>> netlib-java+data
>> >>>> layout and fewer levels of indirection - it's definitely a
>> >>>> worthwhile experiment to run. The main speedups I've seen from
>> >>>> using it come from highly optimized GPU code for linear algebra. I
>> >>>> know that in the past Canny has gone as far as to write custom GPU
>> >>>> kernels for performance-critical regions of code.[1]
>> >>>>
>> >>>> BIDMach is highly optimized for single node performance or
>> >>>> performance on small clusters.[2] Once data doesn't fit easily in
>> >>>> GPU memory (or can be batched in that way) the performance tends to
>> >>>> fall off. Canny argues for hardware/software codesign and as such
>> >>>> prefers machine configurations that are quite different than what
>> >>>> we find in most commodity cluster nodes - e.g. 10 disk cahnnels and
>> 4 GPUs.
>> >>>>
>> >>>> In contrast, MLlib was designed for horizontal scalability on
>> >>>> commodity clusters and works best on very big datasets - order of
>> terabytes.
>> >>>>
>> >>>> For the most part, these projects developed concurrently to address
>> >>>> slightly different use cases. That said, there may be bits of
>> >>>> BIDMach we could repurpose for MLlib - keep in mind we need to be
>> >>>> careful about maintaining cross-language compatibility for our Java
>> >>>> and Python-users, though.
>> >>>>
>> >>>> - Evan
>> >>>>
>> >>>> [1] - http://arxiv.org/abs/1409.5402 [2] -
>> >>>> http://eecs.berkeley.edu/~hzhao/papers/BD.pdf
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 1:00 PM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Hi Evan,
>> >>>>
>> >>>> Thank you for suggestion! BIDMat seems to have terrific speed. Do
>> >>>> you know what makes them faster than netlib-java?
>> >>>>
>> >>>> The same group has BIDMach library that implements machine
>> >>>> learning. For some examples they use Caffe convolutional neural
>> >>>> network library owned by another group in Berkeley. Could you
>> >>>> elaborate on how these all might be connected with Spark Mllib? If
>> >>>> you take BIDMat for linear algebra why don=E2=80=99t you take BIDMa=
ch for
>> optimization and learning?
>> >>>>
>> >>>> Best regards, Alexander
>> >>>>
>> >>>> From: Evan R. Sparks [mailto:evan.sparks@gmail.com<mailto:
>> evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>><mailto:
>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com><mailto:
>> >>>> evan.sparks@gmail.com<mailto:evan.sparks@gmail.com>>>]
>> >>>> Sent: Thursday, February 05, 2015 12:09 PM
>> >>>> To: Ulanov, Alexander
>> >>>> Cc: dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:
>> dev@spark.apache.org<mailto:dev@spark.apache.org>><mailto:
>> >>>> dev@spark.apache.org<mailto:dev@spark.apache.org><mailto:dev@spark.
>> >>>> apache.org<mailto:dev@spark.apache.org>>>
>> >>>> Subject: Re: Using CUDA within Spark / boosting linear algebra
>> >>>>
>> >>>> I'd expect that we can make GPU-accelerated BLAS faster than CPU
>> >>>> blas in many cases.
>> >>>>
>> >>>> You might consider taking a look at the codepaths that BIDMat (
>> >>>> https://github.com/BIDData/BIDMat) takes and comparing them to
>> >>>> netlib-java/breeze. John Canny et. al. have done a bunch of work
>> >>>> optimizing to make this work really fast from Scala. I've run it on
>> >>>> my laptop and compared to MKL and in certain cases it's 10x faster
>> at matrix multiply.
>> >>>> There are a lot of layers of indirection here and you really want
>> >>>> to avoid data copying as much as possible.
>> >>>>
>> >>>> We could also consider swapping out BIDMat for Breeze, but that
>> >>>> would be a big project and if we can figure out how to get
>> >>>> breeze+cublas to comparable performance that would be a big win.
>> >>>>
>> >>>> On Thu, Feb 5, 2015 at 11:55 AM, Ulanov, Alexander <
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>><mailto:
>> >>>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:
>> alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>>> wrote:
>> >>>> Dear Spark developers,
>> >>>>
>> >>>> I am exploring how to make linear algebra operations faster within
>> Spark.
>> >>>> One way of doing this is to use Scala Breeze library that is
>> >>>> bundled with Spark. For matrix operations, it employs Netlib-java
>> >>>> that has a Java wrapper for BLAS (basic linear algebra subprograms)
>> >>>> and LAPACK native binaries if they are available on the worker
>> >>>> node. It also has its own optimized Java implementation of BLAS. It
>> >>>> is worth mentioning, that native binaries provide better performanc=
e
>> only for BLAS level 3, i.e.
>> >>>> matrix-matrix operations or general matrix multiplication (GEMM).
>> >>>> This is confirmed by GEMM test on Netlib-java page
>> >>>> https://github.com/fommil/netlib-java. I also confirmed it with my
>> >>>> experiments with training of artificial neural network
>> >>>> https://github.com/apache/spark/pull/1290#issuecomment-70313952.
>> >>>> However, I would like to boost performance more.
>> >>>>
>> >>>> GPU is supposed to work fast with linear algebra and there is
>> >>>> Nvidia CUDA implementation of BLAS, called cublas. I have one Linux
>> >>>> server with Nvidia GPU and I was able to do the following. I linked
>> >>>> cublas (instead of cpu-based blas) with Netlib-java wrapper and put
>> >>>> it into Spark, so Breeze/Netlib is using it. Then I did some
>> >>>> performance measurements with regards to artificial neural network
>> >>>> batch learning in Spark MLlib that involves matrix-matrix
>> >>>> multiplications. It turns out that for matrices of size less than
>> >>>> ~1000x780 GPU cublas has the same speed as CPU blas. Cublas becomes
>> >>>> slower for bigger matrices. It worth mentioning that it is was not =
a
>> test for ONLY multiplication since there are other operations involved.
>> >>>> One of the reasons for slowdown might be the overhead of copying
>> >>>> the matrices from computer memory to graphic card memory and back.
>> >>>>
>> >>>> So, few questions:
>> >>>> 1) Do these results with CUDA make sense?
>> >>>> 2) If the problem is with copy overhead, are there any libraries
>> >>>> that allow to force intermediate results to stay in graphic card
>> >>>> memory thus removing the overhead?
>> >>>> 3) Any other options to speed-up linear algebra in Spark?
>> >>>>
>> >>>> Thank you, Alexander
>> >>>>
>> >>>> -------------------------------------------------------------------
>> >>>> -- To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:
>> dev-unsubscribe@spark.apache.org><mailto:
>> >>>> dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apach
>> >>>> e.org>><mailto:dev-unsubscribe@spark.apac<mailto:dev-unsubscribe@sp
>> >>>> ark.apac> he.org<http://he.org>
>> >>>> <mailto:dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spa
>> >>>> rk.apache.org>>> For additional commands, e-mail:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>><mailto=
:
>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org><mailto:
>> >>>> dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>>
>> >>>
>>
>> --
>> Best regards,
>> Sam
>>
>
>
>

--047d7b343c9ccb223d0512337bd8--

From dev-return-12211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 16:57:23 2015
Return-Path: <dev-return-12211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7094E17715
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 16:57:23 +0000 (UTC)
Received: (qmail 86256 invoked by uid 500); 26 Mar 2015 16:57:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86174 invoked by uid 500); 26 Mar 2015 16:57:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86163 invoked by uid 99); 26 Mar 2015 16:57:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:57:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 16:56:49 +0000
Received: by wibgn9 with SMTP id gn9so94291018wib.1
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 09:55:18 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=6aGDYacIjL/ExrOsDMogc4VAwrftj5VOQV7E7/NJnbU=;
        b=RoSJz6cWKmsRso/s9G4B5sH6GFU7xbm0GzrL7h1NsJo2Vmrmns4ihD7drAjbdV/d0m
         tUrBe56DyxKFCV3pRoyrOTQGS90G5UANa2BNY7qTvomoCLMh/8OUiiCbcawCktwcxbdW
         J64AyK1k2ieUq0N56JJN5AO6YSljYgJ+YT429HCUUXSrPpiqvENvp87Xcl7s2ApJ7hKN
         HvnSrozLKSQF0R9FKwPa8P+ua2vqWPtTTi9pOtVQBaiG3dbzBgfMMqPrG730LhgzzFhR
         5TuNs+/I6+fI4udTOATgPm063bseKuu0NmTFoV1y9NNFNgjx+MLu69vEPG61mhtPbjdS
         xaLQ==
X-Gm-Message-State: ALoCoQmmtP0cSWN7dS3vUNhY+kRbWk3tJGJ20LjaqfDY5OGx2cbzO6wjvc1jwx6iaZurkg8SbQRS
X-Received: by 10.181.25.225 with SMTP id it1mr9754306wid.8.1427388918321;
 Thu, 26 Mar 2015 09:55:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Thu, 26 Mar 2015 09:54:57 -0700 (PDT)
In-Reply-To: <CABjXkq798dF1ARtkS+oh+oFA7F03nQ+CVCMQ8qEpGy6HFSLgdA@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
 <CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
 <CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
 <CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
 <CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
 <CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
 <CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
 <CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
 <CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
 <87ioehu4qv.fsf@gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
 <CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
 <CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
 <CALR_T9AtsmcXd8oO8yDohKmkUpZLREX3_MBhXqBiQUvrcZYe2w@mail.gmail.com>
 <CALR_T9C4mBOy0T6_K9+RQZtoeNGvhZOtmvz=ij0fhyO8SDbKUw@mail.gmail.com> <CABjXkq798dF1ARtkS+oh+oFA7F03nQ+CVCMQ8qEpGy6HFSLgdA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 26 Mar 2015 16:54:57 +0000
Message-ID: <CAMAsSdKUE=xRkhx08Ymxp_1TcUZkKfk+r5ZSwMg26cZH+jy5NA@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
To: "Evan R. Sparks" <evan.sparks@gmail.com>
Cc: Sam Halliday <sam.halliday@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>, 
	"Ulanov, Alexander" <alexander.ulanov@hp.com>, jfcanny <canny@berkeley.edu>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

The license issue is with libgfortran, rather than OpenBLAS.

(FWIW I am going through the motions to get OpenBLAS set up by default
on CDH in the near future, and the hard part is just handling
libgfortran.)

On Thu, Mar 26, 2015 at 4:07 PM, Evan R. Sparks <evan.sparks@gmail.com> wrote:
> Alright Sam - you are the expert here. If the GPL issues are unavoidable,
> that's fine - what is the exact bit of code that is GPL?
>
> The suggestion to use OpenBLAS is not to say it's the best option, but that
> it's a *free, reasonable default* for many users - keep in mind the most
> common deployment for Spark/MLlib is on 64-bit linux on EC2[1].
> Additionally, for many of the problems we're targeting, this reasonable
> default can provide a 1-2 orders of magnitude improvement in performance
> over the f2jblas implementation that netlib-java falls back on.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:19:00 2015
Return-Path: <dev-return-12212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B61DA17C9F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:19:00 +0000 (UTC)
Received: (qmail 23048 invoked by uid 500); 26 Mar 2015 21:18:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22973 invoked by uid 500); 26 Mar 2015 21:18:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22960 invoked by uid 99); 26 Mar 2015 21:18:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:18:54 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.240.92.66] (HELO g9t5008.houston.hp.com) (15.240.92.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:18:46 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5008.houston.hp.com (Postfix) with ESMTPS id 1E407186
	for <dev@spark.apache.org>; Thu, 26 Mar 2015 21:17:55 +0000 (UTC)
Received: from G4W6303.americas.hpqcorp.net (16.210.26.228) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 26 Mar 2015 21:16:27 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G4W6303.americas.hpqcorp.net ([16.210.26.228]) with mapi id 14.03.0169.001;
 Thu, 26 Mar 2015 21:16:28 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Storing large data for MLlib machine learning
Thread-Topic: Storing large data for MLlib machine learning
Thread-Index: AdBoCSE6m7tKsbMVTNeZooiWf0Dn7A==
Date: Thu, 26 Mar 2015 21:16:26 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.21]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2BG4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2BG4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

Could you suggest what would be the reasonable file format to store feature=
 vector data for machine learning in Spark MLlib? Are there any best practi=
ces for Spark?

My data is dense feature vectors with labels. Some of the requirements are =
that the format should be easy loaded/serialized, randomly accessible, with=
 a small footprint (binary). I am considering Parquet, hdf5, protocol buffe=
r (protobuf), but I have little to no experience with them, so any suggesti=
ons would be really appreciated.

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2BG4W3292americas_--

From dev-return-12213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:27:08 2015
Return-Path: <dev-return-12213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6BD8D17D14
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:27:08 +0000 (UTC)
Received: (qmail 52751 invoked by uid 500); 26 Mar 2015 21:27:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52677 invoked by uid 500); 26 Mar 2015 21:27:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52663 invoked by uid 99); 26 Mar 2015 21:26:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:26:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.213.173 as permitted sender)
Received: from [209.85.213.173] (HELO mail-ig0-f173.google.com) (209.85.213.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:26:54 +0000
Received: by igcxg11 with SMTP id xg11so4155900igc.0
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 14:26:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=4g1FONFA1NgPhUaDZM60UcrjrwBlgfhYls92NRDT+7c=;
        b=0vThmFFG+m9NLhsYCSl83iT8AWNWL5m59haW9dQiZMhVipHJalBQYRqM/ulzLSe5cA
         JyzJDS32Lshi7s0LPZPcr4e9Wb7QxmMDKiNjzmEhM59I+lPOqo6NX/I5E23busgo27W7
         N6uEpo/o0eUiP99G14+MdjDvZ1s53KL/CJUlJCVmB77ummqveu3cq78MeiteTeNaL6fu
         z4qrIfSzwXDf/o6ZBO80q4uud9S27vv4vpgfoZIesmJj4F8SCjjGennnBYuQuZ5lqc8m
         95NX53fVvlg3Ju3dde748U8zVi3qj6Oo/QFSaSvSZ4hZ34jqOomc1I5nv2zDyWrMGGwO
         SHhA==
MIME-Version: 1.0
X-Received: by 10.50.18.49 with SMTP id t17mr39932330igd.3.1427405193661; Thu,
 26 Mar 2015 14:26:33 -0700 (PDT)
Received: by 10.107.155.143 with HTTP; Thu, 26 Mar 2015 14:26:33 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
Date: Thu, 26 Mar 2015 14:26:33 -0700
Message-ID: <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
Subject: Re: Storing large data for MLlib machine learning
From: Stephen Boesch <javadba@gmail.com>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149c0a084d314051237ab2b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c0a084d314051237ab2b
Content-Type: text/plain; charset=UTF-8

There are some convenience methods you might consider including:

           MLUtils.loadLibSVMFile

and   MLUtils.loadLabeledPoint

2015-03-26 14:16 GMT-07:00 Ulanov, Alexander <alexander.ulanov@hp.com>:

> Hi,
>
> Could you suggest what would be the reasonable file format to store
> feature vector data for machine learning in Spark MLlib? Are there any best
> practices for Spark?
>
> My data is dense feature vectors with labels. Some of the requirements are
> that the format should be easy loaded/serialized, randomly accessible, with
> a small footprint (binary). I am considering Parquet, hdf5, protocol buffer
> (protobuf), but I have little to no experience with them, so any
> suggestions would be really appreciated.
>
> Best regards, Alexander
>

--089e0149c0a084d314051237ab2b--

From dev-return-12214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:34:22 2015
Return-Path: <dev-return-12214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3462217DA0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:34:22 +0000 (UTC)
Received: (qmail 91773 invoked by uid 500); 26 Mar 2015 21:34:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91697 invoked by uid 500); 26 Mar 2015 21:34:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91682 invoked by uid 99); 26 Mar 2015 21:34:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:34:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.160.170 as permitted sender)
Received: from [209.85.160.170] (HELO mail-yk0-f170.google.com) (209.85.160.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:34:15 +0000
Received: by ykef74 with SMTP id f74so11078649yke.1
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 14:33:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=wi1X0g9DZE7OgSmbjCELteBFQWHUeyy02RolDwC5ycg=;
        b=P8NqkyQ2fEltOp8fdIE6xnueSs1mGOyaJCZNhRXWI/8Z/2Zxb4w6q8G/WGBmURH1KG
         LB4rOxzTSuHKBoRtL68PIrzaWq7/cEDGzWX1cDlh/M4t9G8V/NUAeH85uxjvcBlqhqmm
         qjJQui85UBHSbcEq8pZ8llj6cVWWciCRTb5nAFOfG+6speiuoYsuGBvZ4tbcsGGj+aTG
         nIWeV/GElPzLF5U6qAF5MKAAyGDHsjBmIhlUvMx3SbGWksQy+c15M/1CGmqsaRa+uFQE
         eVTaSuA5CBCY+mCfQp86o/WScubYxV55VZYMwZ5JxxmLp3/KtCEpYwYHNBxFxxFXr/XJ
         Rk1Q==
X-Received: by 10.52.103.48 with SMTP id ft16mr19306138vdb.81.1427405634502;
 Thu, 26 Mar 2015 14:33:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Thu, 26 Mar 2015 14:33:34 -0700 (PDT)
In-Reply-To: <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
 <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Thu, 26 Mar 2015 14:33:34 -0700
Message-ID: <CABjXkq5pfLnKy59HJ7mrEgvxXKeTM=a_f4M_4R7MkduksZFuhw@mail.gmail.com>
Subject: Re: Storing large data for MLlib machine learning
To: Stephen Boesch <javadba@gmail.com>
Cc: "Ulanov, Alexander" <alexander.ulanov@hp.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b5dbd3acbb35f051237c560
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5dbd3acbb35f051237c560
Content-Type: text/plain; charset=UTF-8

On binary file formats - I looked at HDF5+Spark a couple of years ago and
found it barely JVM-friendly and very Hadoop-unfriendly (e.g. the APIs
needed filenames as input, you couldn't pass it anything like an
InputStream). I don't know if it has gotten any better.

Parquet plays much more nicely and there are lots of spark-related projects
using it already. Keep in mind that it's column-oriented which might impact
performance - but basically you're going to want your features in a byte
array and deser should be pretty straightforward.

On Thu, Mar 26, 2015 at 2:26 PM, Stephen Boesch <javadba@gmail.com> wrote:

> There are some convenience methods you might consider including:
>
>            MLUtils.loadLibSVMFile
>
> and   MLUtils.loadLabeledPoint
>
> 2015-03-26 14:16 GMT-07:00 Ulanov, Alexander <alexander.ulanov@hp.com>:
>
> > Hi,
> >
> > Could you suggest what would be the reasonable file format to store
> > feature vector data for machine learning in Spark MLlib? Are there any
> best
> > practices for Spark?
> >
> > My data is dense feature vectors with labels. Some of the requirements
> are
> > that the format should be easy loaded/serialized, randomly accessible,
> with
> > a small footprint (binary). I am considering Parquet, hdf5, protocol
> buffer
> > (protobuf), but I have little to no experience with them, so any
> > suggestions would be really appreciated.
> >
> > Best regards, Alexander
> >
>

--047d7b5dbd3acbb35f051237c560--

From dev-return-12215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:35:00 2015
Return-Path: <dev-return-12215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF7FF17DBF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:35:00 +0000 (UTC)
Received: (qmail 573 invoked by uid 500); 26 Mar 2015 21:34:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 499 invoked by uid 500); 26 Mar 2015 21:34:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 487 invoked by uid 99); 26 Mar 2015 21:34:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:34:58 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:34:51 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id 8AE197D;
	Thu, 26 Mar 2015 21:34:30 +0000 (UTC)
Received: from G4W6306.americas.hpqcorp.net (16.210.26.231) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 26 Mar 2015 21:33:09 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G4W6306.americas.hpqcorp.net ([16.210.26.231]) with mapi id 14.03.0169.001;
 Thu, 26 Mar 2015 21:33:09 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Stephen Boesch <javadba@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Storing large data for MLlib machine learning
Thread-Topic: Storing large data for MLlib machine learning
Thread-Index: AdBoCSE6m7tKsbMVTNeZooiWf0Dn7AAAmbOAAAAFE3A=
Date: Thu, 26 Mar 2015 21:33:08 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD7E@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
 <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
In-Reply-To: <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.21]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD7EG4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD7EG4W3292americas_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64

VGhhbmtzIGZvciBzdWdnZXN0aW9uLCBidXQgbGlic3ZtIGlzIGEgZm9ybWF0IGZvciBzcGFyc2Ug
ZGF0YSBzdG9yaW5nIGluIHRleHQgZmlsZSBhbmQgSSBoYXZlIGRlbnNlIHZlY3RvcnMuIEluIG15
IG9waW5pb24sIHRleHQgZm9ybWF0IGlzIG5vdCBhcHByb3ByaWF0ZSBmb3Igc3RvcmluZyBsYXJn
ZSBkZW5zZSB2ZWN0b3JzIGR1ZSB0byBvdmVyaGVhZCByZWxhdGVkIHRvIHBhcnNpbmcgZnJvbSBz
dHJpbmcgdG8gZGlnaXRzIGFuZCBhbHNvIHN0b3JpbmcgZGlnaXRzIGFzIHN0cmluZ3MgaXMgbm90
IGVmZmljaWVudC4NCg0KRnJvbTogU3RlcGhlbiBCb2VzY2ggW21haWx0bzpqYXZhZGJhQGdtYWls
LmNvbV0NClNlbnQ6IFRodXJzZGF5LCBNYXJjaCAyNiwgMjAxNSAyOjI3IFBNDQpUbzogVWxhbm92
LCBBbGV4YW5kZXINCkNjOiBkZXZAc3BhcmsuYXBhY2hlLm9yZw0KU3ViamVjdDogUmU6IFN0b3Jp
bmcgbGFyZ2UgZGF0YSBmb3IgTUxsaWIgbWFjaGluZSBsZWFybmluZw0KDQpUaGVyZSBhcmUgc29t
ZSBjb252ZW5pZW5jZSBtZXRob2RzIHlvdSBtaWdodCBjb25zaWRlciBpbmNsdWRpbmc6DQoNCiAg
ICAgICAgICAgTUxVdGlscy5sb2FkTGliU1ZNRmlsZQ0KDQphbmQgICBNTFV0aWxzLmxvYWRMYWJl
bGVkUG9pbnQNCg0KMjAxNS0wMy0yNiAxNDoxNiBHTVQtMDc6MDAgVWxhbm92LCBBbGV4YW5kZXIg
PGFsZXhhbmRlci51bGFub3ZAaHAuY29tPG1haWx0bzphbGV4YW5kZXIudWxhbm92QGhwLmNvbT4+
Og0KSGksDQoNCkNvdWxkIHlvdSBzdWdnZXN0IHdoYXQgd291bGQgYmUgdGhlIHJlYXNvbmFibGUg
ZmlsZSBmb3JtYXQgdG8gc3RvcmUgZmVhdHVyZSB2ZWN0b3IgZGF0YSBmb3IgbWFjaGluZSBsZWFy
bmluZyBpbiBTcGFyayBNTGxpYj8gQXJlIHRoZXJlIGFueSBiZXN0IHByYWN0aWNlcyBmb3IgU3Bh
cms/DQoNCk15IGRhdGEgaXMgZGVuc2UgZmVhdHVyZSB2ZWN0b3JzIHdpdGggbGFiZWxzLiBTb21l
IG9mIHRoZSByZXF1aXJlbWVudHMgYXJlIHRoYXQgdGhlIGZvcm1hdCBzaG91bGQgYmUgZWFzeSBs
b2FkZWQvc2VyaWFsaXplZCwgcmFuZG9tbHkgYWNjZXNzaWJsZSwgd2l0aCBhIHNtYWxsIGZvb3Rw
cmludCAoYmluYXJ5KS4gSSBhbSBjb25zaWRlcmluZyBQYXJxdWV0LCBoZGY1LCBwcm90b2NvbCBi
dWZmZXIgKHByb3RvYnVmKSwgYnV0IEkgaGF2ZSBsaXR0bGUgdG8gbm8gZXhwZXJpZW5jZSB3aXRo
IHRoZW0sIHNvIGFueSBzdWdnZXN0aW9ucyB3b3VsZCBiZSByZWFsbHkgYXBwcmVjaWF0ZWQuDQoN
CkJlc3QgcmVnYXJkcywgQWxleGFuZGVyDQoNCg==

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD7EG4W3292americas_--

From dev-return-12216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:45:26 2015
Return-Path: <dev-return-12216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B254617E54
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:45:26 +0000 (UTC)
Received: (qmail 44931 invoked by uid 500); 26 Mar 2015 21:45:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44808 invoked by uid 500); 26 Mar 2015 21:45:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44182 invoked by uid 99); 26 Mar 2015 21:45:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:45:14 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zzhang@hortonworks.com designates 64.78.56.46 as permitted sender)
Received: from [64.78.56.46] (HELO relayvx11a.securemail.intermedia.net) (64.78.56.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:44:47 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id AA6DF53EE6;
	Thu, 26 Mar 2015 14:44:24 -0700 (PDT)
Subject: RDD.map does not allowed to preservesPartitioning?
MIME-Version: 1.0
x-echoworx-emg-received: Thu, 26 Mar 2015 14:44:24.285 -0700
x-echoworx-msg-id: 5e3da10e-9e00-4a17-8b4a-2e0d8678c384
x-echoworx-action: delivered
Received: from emg-ca-1-1.securemail.intermedia.net ([10.254.155.11])
          by emg-ca-1-1 (JAMES SMTP Server 2.3.2) with SMTP ID 273;
          Thu, 26 Mar 2015 14:44:24 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (unknown [10.224.117.101])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id 0660253EE6;
	Thu, 26 Mar 2015 14:44:24 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Thu, 26 Mar 2015 14:44:23 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Thu, 26 Mar 2015 14:44:23 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: dev <dev@spark.apache.org>, user <user@spark.apache.org>
Thread-Topic: RDD.map does not allowed to preservesPartitioning?
Thread-Index: AQHQaA4FKzFFG4KFl0ilRY10oh8Cmw==
Date: Thu, 26 Mar 2015 21:44:23 +0000
Message-ID: <82069530-27EB-4FC3-8223-19C45BFB5758@hortonworks.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [192.175.27.13]
x-source-routing-agent: Processed
Content-Type: text/plain; charset="us-ascii"
Content-ID: <5209999AF65A8F4891CCDEE8C038C088@exch080.serverpod.net>
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Folks,

Does anybody know what is the reason not allowing preserverPartitioning in =
RDD.map? Do I miss something here?

Following example involves two shuffles. I think if preservePartitioning is=
 allowed, we can avoid the second one, right?

 val r1 =3D sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4))
 val r2 =3D r1.map((_, 1))
 val r3 =3D r2.reduceByKey(_+_)
 val r4 =3D r3.map(x=3D>(x._1, x._2 + 1))
 val r5 =3D r4.reduceByKey(_+_)
 r5.collect.foreach(println)

scala> r5.toDebugString
res2: String =3D
(8) ShuffledRDD[4] at reduceByKey at <console>:29 []
 +-(8) MapPartitionsRDD[3] at map at <console>:27 []
    |  ShuffledRDD[2] at reduceByKey at <console>:25 []
    +-(8) MapPartitionsRDD[1] at map at <console>:23 []
       |  ParallelCollectionRDD[0] at parallelize at <console>:21 []

Thanks.

Zhan Zhang

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:50:46 2015
Return-Path: <dev-return-12217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B34D817ECB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:50:46 +0000 (UTC)
Received: (qmail 70207 invoked by uid 500); 26 Mar 2015 21:50:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70047 invoked by uid 500); 26 Mar 2015 21:50:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69305 invoked by uid 99); 26 Mar 2015 21:50:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:50:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jcoveney@gmail.com designates 209.85.213.178 as permitted sender)
Received: from [209.85.213.178] (HELO mail-ig0-f178.google.com) (209.85.213.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:50:36 +0000
Received: by igcau2 with SMTP id au2so4620672igc.0;
        Thu, 26 Mar 2015 14:49:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=BeTYAkPceAArtoscwzOy2QqvV2zdWRzreJd4pjRGLy8=;
        b=iB1y5rd4f4mfxdOB0YWRcjy2piTzXDLthcLD8GAzt+yIpHthvLy6s8v1hupWgGWqZ/
         ldVT/UxxAZlfRZWExa9ueW+XSkDEgttnLymujqSHZO1jcfiHQ1ZLpcL4sYuWmsF9m9f4
         /LuZFhjhPfjCUcTSMLlCF2qcmYMbwVl0Dtd26H+qEpg/ptfIAMInr+Xp/irqWhAR9mxS
         09Ha0oOKPI2j6qvB/GYbRap0XUtitdmoDAcl3ES5gvroS6ve+7EemMV2NtAHEjhJp7kx
         We7MuWtP5dB5tlep68sCy0dkur/hwzQwhVbrMVVW0wifvlpAJWP5a+eiZAP3iimOIQwO
         BMlQ==
MIME-Version: 1.0
X-Received: by 10.43.181.130 with SMTP id pi2mr28581113icc.21.1427406570883;
 Thu, 26 Mar 2015 14:49:30 -0700 (PDT)
Received: by 10.64.32.229 with HTTP; Thu, 26 Mar 2015 14:49:30 -0700 (PDT)
In-Reply-To: <82069530-27EB-4FC3-8223-19C45BFB5758@hortonworks.com>
References: <82069530-27EB-4FC3-8223-19C45BFB5758@hortonworks.com>
Date: Thu, 26 Mar 2015 17:49:30 -0400
Message-ID: <CAKne9Z6T3ACs18XtFKfm288OdvkVnW9qXK0Q9puQag9pBWjPyA@mail.gmail.com>
Subject: Re: RDD.map does not allowed to preservesPartitioning?
From: Jonathan Coveney <jcoveney@gmail.com>
To: Zhan Zhang <zzhang@hortonworks.com>
Cc: dev <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3e6549b937f051237fdf8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3e6549b937f051237fdf8
Content-Type: text/plain; charset=UTF-8

I believe if you do the following:

sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4)).map((_,1)).reduceByKey(_+_).mapValues(_+1).reduceByKey(_+_).toDebugString

(8) MapPartitionsRDD[34] at reduceByKey at <console>:23 []
 |  MapPartitionsRDD[33] at mapValues at <console>:23 []
 |  ShuffledRDD[32] at reduceByKey at <console>:23 []
 +-(8) MapPartitionsRDD[31] at map at <console>:23 []
    |  ParallelCollectionRDD[30] at parallelize at <console>:23 []

The difference is that spark has no way to know that your map closure
doesn't change the key. if you only use mapValues, it does. Pretty cool
that they optimized that :)

2015-03-26 17:44 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com>:

> Hi Folks,
>
> Does anybody know what is the reason not allowing preserverPartitioning in
> RDD.map? Do I miss something here?
>
> Following example involves two shuffles. I think if preservePartitioning
> is allowed, we can avoid the second one, right?
>
>  val r1 = sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4))
>  val r2 = r1.map((_, 1))
>  val r3 = r2.reduceByKey(_+_)
>  val r4 = r3.map(x=>(x._1, x._2 + 1))
>  val r5 = r4.reduceByKey(_+_)
>  r5.collect.foreach(println)
>
> scala> r5.toDebugString
> res2: String =
> (8) ShuffledRDD[4] at reduceByKey at <console>:29 []
>  +-(8) MapPartitionsRDD[3] at map at <console>:27 []
>     |  ShuffledRDD[2] at reduceByKey at <console>:25 []
>     +-(8) MapPartitionsRDD[1] at map at <console>:23 []
>        |  ParallelCollectionRDD[0] at parallelize at <console>:21 []
>
> Thanks.
>
> Zhan Zhang
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
> For additional commands, e-mail: user-help@spark.apache.org
>
>

--001a11c3e6549b937f051237fdf8--

From dev-return-12218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:54:19 2015
Return-Path: <dev-return-12218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3514817F0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:54:19 +0000 (UTC)
Received: (qmail 86958 invoked by uid 500); 26 Mar 2015 21:54:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86881 invoked by uid 500); 26 Mar 2015 21:54:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86870 invoked by uid 99); 26 Mar 2015 21:54:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:54:10 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:54:04 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id CA0387A;
	Thu, 26 Mar 2015 21:52:43 +0000 (UTC)
Received: from G9W3614.americas.hpqcorp.net (16.216.186.49) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 26 Mar 2015 21:51:13 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G9W3614.americas.hpqcorp.net ([16.216.186.49]) with mapi id 14.03.0169.001;
 Thu, 26 Mar 2015 21:51:13 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "Evan R. Sparks" <evan.sparks@gmail.com>, Stephen Boesch
	<javadba@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Storing large data for MLlib machine learning
Thread-Topic: Storing large data for MLlib machine learning
Thread-Index: AdBoCSE6m7tKsbMVTNeZooiWf0Dn7AAAmbOAAAA+vAAAAH5mUA==
Date: Thu, 26 Mar 2015 21:51:12 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3ADE9@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
 <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
 <CABjXkq5pfLnKy59HJ7mrEgvxXKeTM=a_f4M_4R7MkduksZFuhw@mail.gmail.com>
In-Reply-To: <CABjXkq5pfLnKy59HJ7mrEgvxXKeTM=a_f4M_4R7MkduksZFuhw@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.21]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3ADE9G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3ADE9G4W3292americas_
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
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--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3ADE9G4W3292americas_--

From dev-return-12219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 21:55:19 2015
Return-Path: <dev-return-12219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0B7BC17F2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 21:55:19 +0000 (UTC)
Received: (qmail 96417 invoked by uid 500); 26 Mar 2015 21:55:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96288 invoked by uid 500); 26 Mar 2015 21:55:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95647 invoked by uid 99); 26 Mar 2015 21:55:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:55:14 +0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zzhang@hortonworks.com designates 64.78.56.46 as permitted sender)
Received: from [64.78.56.46] (HELO relayvx11a.securemail.intermedia.net) (64.78.56.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 21:54:48 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id 4138653E88;
	Thu, 26 Mar 2015 14:54:46 -0700 (PDT)
Subject: Re: RDD.map does not allowed to preservesPartitioning?
MIME-Version: 1.0
x-echoworx-emg-received: Thu, 26 Mar 2015 14:54:46.238 -0700
x-echoworx-msg-id: ba290c40-77d0-4b92-8483-3722e24305d5
x-echoworx-action: delivered
Received: from emg-ca-1-1.securemail.intermedia.net ([10.254.155.11])
          by emg-ca-1-1 (JAMES SMTP Server 2.3.2) with SMTP ID 54;
          Thu, 26 Mar 2015 14:54:46 -0700 (PDT)
Received: from MBX080-W4-CO-2.exch080.serverpod.net (unknown [10.224.117.102])
	by emg-ca-1-1.localdomain (Postfix) with ESMTP id EED7A53E88;
	Thu, 26 Mar 2015 14:54:45 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-2.exch080.serverpod.net (10.224.117.102) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Thu, 26 Mar 2015 14:54:45 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Thu, 26 Mar 2015 14:54:44 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: Jonathan Coveney <jcoveney@gmail.com>
CC: dev <dev@spark.apache.org>, user <user@spark.apache.org>
Thread-Topic: RDD.map does not allowed to preservesPartitioning?
Thread-Index: AQHQaA4FKzFFG4KFl0ilRY10oh8Cmw==
Date: Thu, 26 Mar 2015 21:54:44 +0000
Message-ID: <28037853-21D9-42AB-831A-0DEA63A75D2B@hortonworks.com>
References: <82069530-27EB-4FC3-8223-19C45BFB5758@hortonworks.com>
 <CAKne9Z6T3ACs18XtFKfm288OdvkVnW9qXK0Q9puQag9pBWjPyA@mail.gmail.com>
In-Reply-To: <CAKne9Z6T3ACs18XtFKfm288OdvkVnW9qXK0Q9puQag9pBWjPyA@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [192.175.27.13]
x-source-routing-agent: Processed
Content-Type: multipart/alternative;
	boundary="_000_2803785321D942AB831A0DEA63A75D2Bhortonworkscom_"
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_2803785321D942AB831A0DEA63A75D2Bhortonworkscom_
Content-Type: text/plain; charset="Windows-1252"
Content-Transfer-Encoding: quoted-printable

Thanks Jonathan. You are right regarding rewrite the example.

I mean providing such option to developer so that it is controllable. The e=
xample may seems silly, and I don=92t know the use cases.

But for example, if I also want to operate both the key and value part to g=
enerate some new value with keeping key part untouched. Then mapValues may =
not be able to  do this.

Changing the code to allow this is trivial, but I don=92t know whether ther=
e is some special reason behind this.

Thanks.

Zhan Zhang



On Mar 26, 2015, at 2:49 PM, Jonathan Coveney <jcoveney@gmail.com<mailto:jc=
oveney@gmail.com>> wrote:

I believe if you do the following:

sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4)).map((_,1)).reduceByKey(_=
+_).mapValues(_+1).reduceByKey(_+_).toDebugString

(8) MapPartitionsRDD[34] at reduceByKey at <console>:23 []
 |  MapPartitionsRDD[33] at mapValues at <console>:23 []
 |  ShuffledRDD[32] at reduceByKey at <console>:23 []
 +-(8) MapPartitionsRDD[31] at map at <console>:23 []
    |  ParallelCollectionRDD[30] at parallelize at <console>:23 []

The difference is that spark has no way to know that your map closure doesn=
't change the key. if you only use mapValues, it does. Pretty cool that the=
y optimized that :)

2015-03-26 17:44 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com<mailto:zzhang=
@hortonworks.com>>:
Hi Folks,

Does anybody know what is the reason not allowing preserverPartitioning in =
RDD.map? Do I miss something here?

Following example involves two shuffles. I think if preservePartitioning is=
 allowed, we can avoid the second one, right?

 val r1 =3D sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4))
 val r2 =3D r1.map((_, 1))
 val r3 =3D r2.reduceByKey(_+_)
 val r4 =3D r3.map(x=3D>(x._1, x._2 + 1))
 val r5 =3D r4.reduceByKey(_+_)
 r5.collect.foreach(println)

scala> r5.toDebugString
res2: String =3D
(8) ShuffledRDD[4] at reduceByKey at <console>:29 []
 +-(8) MapPartitionsRDD[3] at map at <console>:27 []
    |  ShuffledRDD[2] at reduceByKey at <console>:25 []
    +-(8) MapPartitionsRDD[1] at map at <console>:23 []
       |  ParallelCollectionRDD[0] at parallelize at <console>:21 []

Thanks.

Zhan Zhang

---------------------------------------------------------------------
To unsubscribe, e-mail: user-unsubscribe@spark.apache.org<mailto:user-unsub=
scribe@spark.apache.org>
For additional commands, e-mail: user-help@spark.apache.org<mailto:user-hel=
p@spark.apache.org>




--_000_2803785321D942AB831A0DEA63A75D2Bhortonworkscom_--

From dev-return-12220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 22:03:15 2015
Return-Path: <dev-return-12220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DCD6D17FCD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 22:03:15 +0000 (UTC)
Received: (qmail 21258 invoked by uid 500); 26 Mar 2015 22:03:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21178 invoked by uid 500); 26 Mar 2015 22:03:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21166 invoked by uid 99); 26 Mar 2015 22:03:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:03:11 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of freeman.jeremy@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:03:05 +0000
Received: by pdnc3 with SMTP id c3so75673764pdn.0
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 15:01:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=Dpvwp0nliYnpqbQQLz3eU26u24wAhiIZPwhLfO+Aemw=;
        b=A5PuIx9EzGyXZ73z9exSiBSlmjhq2hvcsQdRAHR0cd1eMIDyZaeJveFiGROp2YkAXt
         L4nVMeGw3nH6+4BnPqTKXyGtDbHVzu72z/i0bb9NeLzsBslPSlgYNurNp+VZ1d/+02E8
         iSkthfFpATo0gN2kj+tBY8mG6UhVwofScZ22tQfLs9z547w/b/NlVZxRto18Ruml+X2l
         OoTFqwOsB8heaBnaddgeGsHPWBlokpBlvb/ALvUaUEHBGvfBPUDHd7/B0JIErmTU09ZR
         fCKlCqzUvhnuu0FqXdCCBwIkpBcMdRZ8nEzlsyssIfHRpPOtBdxWuut6CXl8vvltlCLx
         evUQ==
X-Received: by 10.66.66.196 with SMTP id h4mr29463821pat.127.1427407275548;
        Thu, 26 Mar 2015 15:01:15 -0700 (PDT)
Received: from [192.168.10.167] (DATABRICKS.bar1.SanFrancisco1.Level3.net. [4.15.73.18])
        by mx.google.com with ESMTPSA id hp6sm31987pbc.71.2015.03.26.15.01.14
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 26 Mar 2015 15:01:14 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_4578455F-16BD-422B-8830-F6261334D7E3"
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Storing large data for MLlib machine learning
From: Jeremy Freeman <freeman.jeremy@gmail.com>
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD7E@G4W3292.americas.hpqcorp.net>
Date: Thu, 26 Mar 2015 15:01:12 -0700
Cc: Stephen Boesch <javadba@gmail.com>,
 "dev@spark.apache.org" <dev@spark.apache.org>
Message-Id: <8F2BF3F2-471C-4B28-95E2-597D1D2F6675@gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net> <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD7E@G4W3292.americas.hpqcorp.net>
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_4578455F-16BD-422B-8830-F6261334D7E3
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

Hi Ulvanov, great question, we=92ve encountered it frequently with =
scientific data (e.g. time series). Agreed text is inefficient for dense =
arrays, and we also found HDF5+Spark to be a pain.
=20
Our strategy has been flat binary files with fixed length records. =
Loading these is now supported in Spark via the binaryRecords method, =
which wraps a custom Hadoop InputFormat we wrote.

An example (in python):

> # write data from an array
> from numpy import random
> dat =3D random.randn(100,5)
> f =3D open('test.bin', 'w')
> f.write(dat)
> f.close()

> # load the data back in
> from numpy import frombuffer
> nrecords =3D 5
> bytesize =3D 8
> recordsize =3D nrecords * bytesize
> data =3D sc.binaryRecords('test.bin', recordsize)
> parsed =3D data.map(lambda v: frombuffer(buffer(v, 0, recordsize), =
'float'))

> # these should be equal
> parsed.first()
> dat[0,:]

Compared to something like Parquet, this is a little lighter-weight, and =
plays nicer with non-distributed data science tools (e.g. numpy). It =
also scales great (we use it routinely to process TBs of time series). =
And handles single files or directories. But it's extremely simple!

-------------------------
jeremyfreeman.net
@thefreemanlab

On Mar 26, 2015, at 2:33 PM, Ulanov, Alexander <alexander.ulanov@hp.com> =
wrote:

> Thanks for suggestion, but libsvm is a format for sparse data storing =
in text file and I have dense vectors. In my opinion, text format is not =
appropriate for storing large dense vectors due to overhead related to =
parsing from string to digits and also storing digits as strings is not =
efficient.
>=20
> From: Stephen Boesch [mailto:javadba@gmail.com]
> Sent: Thursday, March 26, 2015 2:27 PM
> To: Ulanov, Alexander
> Cc: dev@spark.apache.org
> Subject: Re: Storing large data for MLlib machine learning
>=20
> There are some convenience methods you might consider including:
>=20
>           MLUtils.loadLibSVMFile
>=20
> and   MLUtils.loadLabeledPoint
>=20
> 2015-03-26 14:16 GMT-07:00 Ulanov, Alexander =
<alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>:
> Hi,
>=20
> Could you suggest what would be the reasonable file format to store =
feature vector data for machine learning in Spark MLlib? Are there any =
best practices for Spark?
>=20
> My data is dense feature vectors with labels. Some of the requirements =
are that the format should be easy loaded/serialized, randomly =
accessible, with a small footprint (binary). I am considering Parquet, =
hdf5, protocol buffer (protobuf), but I have little to no experience =
with them, so any suggestions would be really appreciated.
>=20
> Best regards, Alexander
>=20


--Apple-Mail=_4578455F-16BD-422B-8830-F6261334D7E3--

From dev-return-12221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 22:05:23 2015
Return-Path: <dev-return-12221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D980117FEE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 22:05:23 +0000 (UTC)
Received: (qmail 28136 invoked by uid 500); 26 Mar 2015 22:05:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28057 invoked by uid 500); 26 Mar 2015 22:05:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28044 invoked by uid 99); 26 Mar 2015 22:05:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:05:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.213.45 as permitted sender)
Received: from [209.85.213.45] (HELO mail-yh0-f45.google.com) (209.85.213.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:05:11 +0000
Received: by yhch68 with SMTP id h68so32586417yhc.1
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 15:02:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=rsbO8U71iZ3f8BPv11uXQr5gyybQELu5m1wneDlv/X4=;
        b=dRtSuCIuqmpyetP75pWE+ChuNCjBnY4NBgct7MSCRcmtPZqFXiZEsX1s/O+YvQyYEj
         B0j44Bb8F6wOmL3aRU8Ik3ZNL8vc4dPNXvwoj9JFUBSHzdnyZem29VpbDXfVfVUXyUT3
         OLemy/j/9nu+/OrFf5GaGg4fMaHQqtbZmCvIfbphqWhfWiY6/nJyngBbNDfdy4NKiIuc
         owxGj+jQCnE8i5qHRguPuJxHKtnI8EYTeIUjpo/dYZpHg+lGtZ11At/pMuhWfq+N1eTP
         D8tGexV1w8WyHgAUYosIMC0feYqgzsUTD3SGgusOBzWbot5Yg6zK38W+p+QZYyGxnDhD
         qoCQ==
X-Received: by 10.52.116.197 with SMTP id jy5mr19450604vdb.13.1427407356195;
 Thu, 26 Mar 2015 15:02:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.243.107 with HTTP; Thu, 26 Mar 2015 15:02:15 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3ADE9@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
 <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
 <CABjXkq5pfLnKy59HJ7mrEgvxXKeTM=a_f4M_4R7MkduksZFuhw@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FE3ADE9@G4W3292.americas.hpqcorp.net>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Thu, 26 Mar 2015 15:02:15 -0700
Message-ID: <CABjXkq5hrBO79r9K+oTPT4=RcFFJXKS18A_kPEDFmFrDk4UnKg@mail.gmail.com>
Subject: Re: Storing large data for MLlib machine learning
To: "Ulanov, Alexander" <alexander.ulanov@hp.com>
Cc: Stephen Boesch <javadba@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=bcaec5485ade6a7a8c0512382cc6
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec5485ade6a7a8c0512382cc6
Content-Type: text/plain; charset=UTF-8

Protobufs are great for serializing individual records - but parquet is
good for efficiently storing a whole bunch of these objects.

Matt Massie has a good (slightly dated) blog post on using
Spark+Parquet+Avro (and you can pretty much s/Avro/Protobuf/) describing
how they all work together here:
http://zenfractal.com/2013/08/21/a-powerful-big-data-trio/

Your use case (storing dense features, presumably as a single column) is
pretty straightforward and the extra layers of indirection are maybe
overkill.

Lastly - you might consider using some of SparkSQL/DataFrame's built-in
features for persistence, which support lots of storage backends.
https://spark.apache.org/docs/1.3.0/sql-programming-guide.html#data-sources

On Thu, Mar 26, 2015 at 2:51 PM, Ulanov, Alexander <alexander.ulanov@hp.com>
wrote:

>  Thanks, Evan. What do you think about Protobuf? Twitter has a library to
> manage protobuf files in hdfs https://github.com/twitter/elephant-bird
>
>
>
>
>
> *From:* Evan R. Sparks [mailto:evan.sparks@gmail.com]
> *Sent:* Thursday, March 26, 2015 2:34 PM
> *To:* Stephen Boesch
> *Cc:* Ulanov, Alexander; dev@spark.apache.org
> *Subject:* Re: Storing large data for MLlib machine learning
>
>
>
> On binary file formats - I looked at HDF5+Spark a couple of years ago and
> found it barely JVM-friendly and very Hadoop-unfriendly (e.g. the APIs
> needed filenames as input, you couldn't pass it anything like an
> InputStream). I don't know if it has gotten any better.
>
>
>
> Parquet plays much more nicely and there are lots of spark-related
> projects using it already. Keep in mind that it's column-oriented which
> might impact performance - but basically you're going to want your features
> in a byte array and deser should be pretty straightforward.
>
>
>
> On Thu, Mar 26, 2015 at 2:26 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
> There are some convenience methods you might consider including:
>
>            MLUtils.loadLibSVMFile
>
> and   MLUtils.loadLabeledPoint
>
> 2015-03-26 14:16 GMT-07:00 Ulanov, Alexander <alexander.ulanov@hp.com>:
>
>
> > Hi,
> >
> > Could you suggest what would be the reasonable file format to store
> > feature vector data for machine learning in Spark MLlib? Are there any
> best
> > practices for Spark?
> >
> > My data is dense feature vectors with labels. Some of the requirements
> are
> > that the format should be easy loaded/serialized, randomly accessible,
> with
> > a small footprint (binary). I am considering Parquet, hdf5, protocol
> buffer
> > (protobuf), but I have little to no experience with them, so any
> > suggestions would be really appreciated.
> >
> > Best regards, Alexander
> >
>
>
>

--bcaec5485ade6a7a8c0512382cc6--

From dev-return-12222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 22:07:35 2015
Return-Path: <dev-return-12222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0472517215
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 22:07:35 +0000 (UTC)
Received: (qmail 42592 invoked by uid 500); 26 Mar 2015 22:07:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42453 invoked by uid 500); 26 Mar 2015 22:07:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41966 invoked by uid 99); 26 Mar 2015 22:07:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:07:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jcoveney@gmail.com designates 209.85.213.176 as permitted sender)
Received: from [209.85.213.176] (HELO mail-ig0-f176.google.com) (209.85.213.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:07:25 +0000
Received: by igcau2 with SMTP id au2so4975494igc.0;
        Thu, 26 Mar 2015 15:07:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=rwo/IFcfSyJvlYnphCwNX1YWyB5hacOuqAJzY+cdEII=;
        b=Fqu6acAyd4/OlnxAnCUGeIZGZZ1wmqGBkiXIyFqviSfy3pTg28n7yAEcmt/ZCzdOpH
         y5qOP9xL2kQIwKsBpCBkYH9KniwTg3da73aEo8CDZ927nwt6230w1xz8s4hT+WbW1VO9
         lnZ+IxkrbQwV3MvasJpUjRVAHS75qmae8kCvF58Y4F+r+moIHe+vHefM3lf/rbYht8/r
         TKS/USZfZWnY8CrqftPuLRBec5aDHmhwz9ob8z/t7tcT0v/ayhTE75Oezm4JO/WVw4Lp
         Om9MaZRuHOJr+iwgQRHf75hGBdC1/YbNbRc/byMa8R8rI1RdKo1PwwsrHsMnPR91uaCr
         qUzg==
MIME-Version: 1.0
X-Received: by 10.50.60.72 with SMTP id f8mr40774423igr.31.1427407625486; Thu,
 26 Mar 2015 15:07:05 -0700 (PDT)
Received: by 10.64.32.229 with HTTP; Thu, 26 Mar 2015 15:07:05 -0700 (PDT)
In-Reply-To: <28037853-21D9-42AB-831A-0DEA63A75D2B@hortonworks.com>
References: <82069530-27EB-4FC3-8223-19C45BFB5758@hortonworks.com>
	<CAKne9Z6T3ACs18XtFKfm288OdvkVnW9qXK0Q9puQag9pBWjPyA@mail.gmail.com>
	<28037853-21D9-42AB-831A-0DEA63A75D2B@hortonworks.com>
Date: Thu, 26 Mar 2015 18:07:05 -0400
Message-ID: <CAKne9Z4XAK8W4fcbDsCoP=6RrzVEFQk2XiT5h1OMnS0Ypy+U_g@mail.gmail.com>
Subject: Re: RDD.map does not allowed to preservesPartitioning?
From: Jonathan Coveney <jcoveney@gmail.com>
To: Zhan Zhang <zzhang@hortonworks.com>
Cc: dev <dev@spark.apache.org>, user <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b10d0e3778a0a0512383ceb
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10d0e3778a0a0512383ceb
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

This is just a deficiency of the api, imo. I agree: mapValues could
definitely be a function (K, V)=3D>V1. The option isn't set by the function=
,
it's on the RDD. So you could look at the code and do this.
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/=
spark/rdd/RDD.scala

 def mapValues[U](f: V =3D> U): RDD[(K, U)] =3D {
    val cleanF =3D self.context.clean(f)
    new MapPartitionsRDD[(K, U), (K, V)](self,
      (context, pid, iter) =3D> iter.map { case (k, v) =3D> (k, cleanF(v)) =
},
      preservesPartitioning =3D true)
  }

What you want:

 def mapValues[U](f: (K, V) =3D> U): RDD[(K, U)] =3D {
    val cleanF =3D self.context.clean(f)
    new MapPartitionsRDD[(K, U), (K, V)](self,
      (context, pid, iter) =3D> iter.map { case t@(k, _) =3D> (k, cleanF(t)=
) },
      preservesPartitioning =3D true)
  }

One of the nice things about spark is that making such new operators is
very easy :)

2015-03-26 17:54 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com>:

>  Thanks Jonathan. You are right regarding rewrite the example.
>
>  I mean providing such option to developer so that it is controllable.
> The example may seems silly, and I don=E2=80=99t know the use cases.
>
> But for example, if I also want to operate both the key and value part to
> generate some new value with keeping key part untouched. Then mapValues m=
ay
> not be able to  do this.
>
>  Changing the code to allow this is trivial, but I don=E2=80=99t know whe=
ther
> there is some special reason behind this.
>
>  Thanks.
>
>  Zhan Zhang
>
>
>
>
>  On Mar 26, 2015, at 2:49 PM, Jonathan Coveney <jcoveney@gmail.com> wrote=
:
>
>  I believe if you do the following:
>
>
> sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4)).map((_,1)).reduceByKey=
(_+_).mapValues(_+1).reduceByKey(_+_).toDebugString
>
>  (8) MapPartitionsRDD[34] at reduceByKey at <console>:23 []
>  |  MapPartitionsRDD[33] at mapValues at <console>:23 []
>  |  ShuffledRDD[32] at reduceByKey at <console>:23 []
>  +-(8) MapPartitionsRDD[31] at map at <console>:23 []
>     |  ParallelCollectionRDD[30] at parallelize at <console>:23 []
>
>  The difference is that spark has no way to know that your map closure
> doesn't change the key. if you only use mapValues, it does. Pretty cool
> that they optimized that :)
>
> 2015-03-26 17:44 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com>:
>
>> Hi Folks,
>>
>> Does anybody know what is the reason not allowing preserverPartitioning
>> in RDD.map? Do I miss something here?
>>
>> Following example involves two shuffles. I think if preservePartitioning
>> is allowed, we can avoid the second one, right?
>>
>>  val r1 =3D sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4))
>>  val r2 =3D r1.map((_, 1))
>>  val r3 =3D r2.reduceByKey(_+_)
>>  val r4 =3D r3.map(x=3D>(x._1, x._2 + 1))
>>  val r5 =3D r4.reduceByKey(_+_)
>>  r5.collect.foreach(println)
>>
>> scala> r5.toDebugString
>> res2: String =3D
>> (8) ShuffledRDD[4] at reduceByKey at <console>:29 []
>>  +-(8) MapPartitionsRDD[3] at map at <console>:27 []
>>     |  ShuffledRDD[2] at reduceByKey at <console>:25 []
>>     +-(8) MapPartitionsRDD[1] at map at <console>:23 []
>>        |  ParallelCollectionRDD[0] at parallelize at <console>:21 []
>>
>> Thanks.
>>
>> Zhan Zhang
>>
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>> For additional commands, e-mail: user-help@spark.apache.org
>>
>>
>
>

--047d7b10d0e3778a0a0512383ceb--

From dev-return-12223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 22:15:12 2015
Return-Path: <dev-return-12223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4A34F172AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 22:15:12 +0000 (UTC)
Received: (qmail 68068 invoked by uid 500); 26 Mar 2015 22:15:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67929 invoked by uid 500); 26 Mar 2015 22:15:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67092 invoked by uid 99); 26 Mar 2015 22:15:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:15:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:14:42 +0000
Received: by oigz129 with SMTP id z129so16996242oig.1;
        Thu, 26 Mar 2015 15:14:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=nJO63M5ZBskRSnWuz+RNrW5HGbpRI4ZAJanRWb2bEZs=;
        b=IPPy5cErnRpb6Gs8XmL98lNqQFsYiVvlonFA7rbuikbvqMxS76uPaWW2piPJQnbTDS
         qZjEtMiWisTUrFoaNG6CqBuxLASnFLnBg6f84iCiGAFw/s1ZGQo79hj+NCBWpUYJpXdV
         9sUXhU+O8akSjulqWddWnrjcOfaZ2d3lHrwxLz7gllbF+e6STm5L/Uz2Qnaj1sT8/Zwy
         AjnbABgEn2u215cb/G7inlL25hmLnuBOY75nMaYa3Zr07M1ov4kbNY9evdGsh3C1Ulae
         USyNhscLZsoAOYzK31E5jeNuZWGtj9JWP3CTMorH0K3G9GB97IGb9D7tJS/5tn/nLvwb
         ichQ==
MIME-Version: 1.0
X-Received: by 10.202.229.201 with SMTP id c192mr5415312oih.44.1427408080920;
 Thu, 26 Mar 2015 15:14:40 -0700 (PDT)
Received: by 10.202.71.22 with HTTP; Thu, 26 Mar 2015 15:14:40 -0700 (PDT)
In-Reply-To: <CAKne9Z4XAK8W4fcbDsCoP=6RrzVEFQk2XiT5h1OMnS0Ypy+U_g@mail.gmail.com>
References: <82069530-27EB-4FC3-8223-19C45BFB5758@hortonworks.com>
	<CAKne9Z6T3ACs18XtFKfm288OdvkVnW9qXK0Q9puQag9pBWjPyA@mail.gmail.com>
	<28037853-21D9-42AB-831A-0DEA63A75D2B@hortonworks.com>
	<CAKne9Z4XAK8W4fcbDsCoP=6RrzVEFQk2XiT5h1OMnS0Ypy+U_g@mail.gmail.com>
Date: Thu, 26 Mar 2015 15:14:40 -0700
Message-ID: <CABPQxstge2e1NEpH=8gFEgAUiNOGnFiMXx6A3p42uov8uTyScg@mail.gmail.com>
Subject: Re: RDD.map does not allowed to preservesPartitioning?
From: Patrick Wendell <pwendell@gmail.com>
To: Jonathan Coveney <jcoveney@gmail.com>
Cc: Zhan Zhang <zzhang@hortonworks.com>, dev <dev@spark.apache.org>, 
	user <user@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I think we have a version of mapPartitions that allows you to tell
Spark the partitioning is preserved:

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L639

We could also add a map function that does same. Or you can just write
your map using an iterator.

- Patrick

On Thu, Mar 26, 2015 at 3:07 PM, Jonathan Coveney <jcoveney@gmail.com> wrote:
> This is just a deficiency of the api, imo. I agree: mapValues could
> definitely be a function (K, V)=>V1. The option isn't set by the function,
> it's on the RDD. So you could look at the code and do this.
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rdd/RDD.scala
>
>  def mapValues[U](f: V => U): RDD[(K, U)] = {
>     val cleanF = self.context.clean(f)
>     new MapPartitionsRDD[(K, U), (K, V)](self,
>       (context, pid, iter) => iter.map { case (k, v) => (k, cleanF(v)) },
>       preservesPartitioning = true)
>   }
>
> What you want:
>
>  def mapValues[U](f: (K, V) => U): RDD[(K, U)] = {
>     val cleanF = self.context.clean(f)
>     new MapPartitionsRDD[(K, U), (K, V)](self,
>       (context, pid, iter) => iter.map { case t@(k, _) => (k, cleanF(t)) },
>       preservesPartitioning = true)
>   }
>
> One of the nice things about spark is that making such new operators is very
> easy :)
>
> 2015-03-26 17:54 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com>:
>
>> Thanks Jonathan. You are right regarding rewrite the example.
>>
>> I mean providing such option to developer so that it is controllable. The
>> example may seems silly, and I don't know the use cases.
>>
>> But for example, if I also want to operate both the key and value part to
>> generate some new value with keeping key part untouched. Then mapValues may
>> not be able to  do this.
>>
>> Changing the code to allow this is trivial, but I don't know whether there
>> is some special reason behind this.
>>
>> Thanks.
>>
>> Zhan Zhang
>>
>>
>>
>>
>> On Mar 26, 2015, at 2:49 PM, Jonathan Coveney <jcoveney@gmail.com> wrote:
>>
>> I believe if you do the following:
>>
>>
>> sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4)).map((_,1)).reduceByKey(_+_).mapValues(_+1).reduceByKey(_+_).toDebugString
>>
>> (8) MapPartitionsRDD[34] at reduceByKey at <console>:23 []
>>  |  MapPartitionsRDD[33] at mapValues at <console>:23 []
>>  |  ShuffledRDD[32] at reduceByKey at <console>:23 []
>>  +-(8) MapPartitionsRDD[31] at map at <console>:23 []
>>     |  ParallelCollectionRDD[30] at parallelize at <console>:23 []
>>
>> The difference is that spark has no way to know that your map closure
>> doesn't change the key. if you only use mapValues, it does. Pretty cool that
>> they optimized that :)
>>
>> 2015-03-26 17:44 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com>:
>>>
>>> Hi Folks,
>>>
>>> Does anybody know what is the reason not allowing preserverPartitioning
>>> in RDD.map? Do I miss something here?
>>>
>>> Following example involves two shuffles. I think if preservePartitioning
>>> is allowed, we can avoid the second one, right?
>>>
>>>  val r1 = sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4))
>>>  val r2 = r1.map((_, 1))
>>>  val r3 = r2.reduceByKey(_+_)
>>>  val r4 = r3.map(x=>(x._1, x._2 + 1))
>>>  val r5 = r4.reduceByKey(_+_)
>>>  r5.collect.foreach(println)
>>>
>>> scala> r5.toDebugString
>>> res2: String =
>>> (8) ShuffledRDD[4] at reduceByKey at <console>:29 []
>>>  +-(8) MapPartitionsRDD[3] at map at <console>:27 []
>>>     |  ShuffledRDD[2] at reduceByKey at <console>:25 []
>>>     +-(8) MapPartitionsRDD[1] at map at <console>:23 []
>>>        |  ParallelCollectionRDD[0] at parallelize at <console>:21 []
>>>
>>> Thanks.
>>>
>>> Zhan Zhang
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: user-help@spark.apache.org
>>>
>>
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 22:20:33 2015
Return-Path: <dev-return-12224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DEC3A172E6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 22:20:32 +0000 (UTC)
Received: (qmail 83391 invoked by uid 500); 26 Mar 2015 22:20:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83259 invoked by uid 500); 26 Mar 2015 22:20:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 82447 invoked by uid 99); 26 Mar 2015 22:20:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:20:27 +0000
X-ASF-Spam-Status: No, hits=1.0 required=5.0
	tests=FSL_HELO_BARE_IP_2,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zzhang@hortonworks.com designates 64.78.52.187 as permitted sender)
Received: from [64.78.52.187] (HELO relayvx12c.securemail.intermedia.net) (64.78.52.187)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:20:22 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id 8863553DCE;
	Thu, 26 Mar 2015 15:20:02 -0700 (PDT)
Subject: Re: RDD.map does not allowed to preservesPartitioning?
MIME-Version: 1.0
x-echoworx-emg-received: Thu, 26 Mar 2015 15:20:02.488 -0700
x-echoworx-msg-id: 03c98cd9-c6bf-42fb-9aea-d9feebd85e98
x-echoworx-action: delivered
Received: from 10.254.155.17 ([10.254.155.17])
          by emg-ca-1-2 (JAMES SMTP Server 2.3.2) with SMTP ID 240;
          Thu, 26 Mar 2015 15:20:02 -0700 (PDT)
Received: from MBX080-W4-CO-2.exch080.serverpod.net (unknown [10.224.117.102])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id 40D2953DCE;
	Thu, 26 Mar 2015 15:20:02 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-2.exch080.serverpod.net (10.224.117.102) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Thu, 26 Mar 2015 15:20:01 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Thu, 26 Mar 2015 15:20:01 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: Patrick Wendell <pwendell@gmail.com>
CC: Jonathan Coveney <jcoveney@gmail.com>, dev <dev@spark.apache.org>, user
	<user@spark.apache.org>
Thread-Topic: RDD.map does not allowed to preservesPartitioning?
Thread-Index: AQHQaA4FKzFFG4KFl0ilRY10oh8Cmw==
Date: Thu, 26 Mar 2015 22:20:00 +0000
Message-ID: <9FE46A42-8467-4501-9A41-E1F5E361D50F@hortonworks.com>
References: <82069530-27EB-4FC3-8223-19C45BFB5758@hortonworks.com>
 <CAKne9Z6T3ACs18XtFKfm288OdvkVnW9qXK0Q9puQag9pBWjPyA@mail.gmail.com>
 <28037853-21D9-42AB-831A-0DEA63A75D2B@hortonworks.com>
 <CAKne9Z4XAK8W4fcbDsCoP=6RrzVEFQk2XiT5h1OMnS0Ypy+U_g@mail.gmail.com>
 <CABPQxstge2e1NEpH=8gFEgAUiNOGnFiMXx6A3p42uov8uTyScg@mail.gmail.com>
In-Reply-To: <CABPQxstge2e1NEpH=8gFEgAUiNOGnFiMXx6A3p42uov8uTyScg@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [192.175.27.13]
x-source-routing-agent: Processed
Content-Type: text/plain; charset="iso-8859-1"
Content-ID: <CBFAA79576C4344699865AA636D5F4E0@exch080.serverpod.net>
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks all for the quick response.

Thanks.

Zhan Zhang

On Mar 26, 2015, at 3:14 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> I think we have a version of mapPartitions that allows you to tell
> Spark the partitioning is preserved:
>=20
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apach=
e/spark/rdd/RDD.scala#L639
>=20
> We could also add a map function that does same. Or you can just write
> your map using an iterator.
>=20
> - Patrick
>=20
> On Thu, Mar 26, 2015 at 3:07 PM, Jonathan Coveney <jcoveney@gmail.com> wr=
ote:
>> This is just a deficiency of the api, imo. I agree: mapValues could
>> definitely be a function (K, V)=3D>V1. The option isn't set by the funct=
ion,
>> it's on the RDD. So you could look at the code and do this.
>> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apac=
he/spark/rdd/RDD.scala
>>=20
>> def mapValues[U](f: V =3D> U): RDD[(K, U)] =3D {
>>    val cleanF =3D self.context.clean(f)
>>    new MapPartitionsRDD[(K, U), (K, V)](self,
>>      (context, pid, iter) =3D> iter.map { case (k, v) =3D> (k, cleanF(v)=
) },
>>      preservesPartitioning =3D true)
>>  }
>>=20
>> What you want:
>>=20
>> def mapValues[U](f: (K, V) =3D> U): RDD[(K, U)] =3D {
>>    val cleanF =3D self.context.clean(f)
>>    new MapPartitionsRDD[(K, U), (K, V)](self,
>>      (context, pid, iter) =3D> iter.map { case t@(k, _) =3D> (k, cleanF(=
t)) },
>>      preservesPartitioning =3D true)
>>  }
>>=20
>> One of the nice things about spark is that making such new operators is =
very
>> easy :)
>>=20
>> 2015-03-26 17:54 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com>:
>>=20
>>> Thanks Jonathan. You are right regarding rewrite the example.
>>>=20
>>> I mean providing such option to developer so that it is controllable. T=
he
>>> example may seems silly, and I don't know the use cases.
>>>=20
>>> But for example, if I also want to operate both the key and value part =
to
>>> generate some new value with keeping key part untouched. Then mapValues=
 may
>>> not be able to  do this.
>>>=20
>>> Changing the code to allow this is trivial, but I don't know whether th=
ere
>>> is some special reason behind this.
>>>=20
>>> Thanks.
>>>=20
>>> Zhan Zhang
>>>=20
>>>=20
>>>=20
>>>=20
>>> On Mar 26, 2015, at 2:49 PM, Jonathan Coveney <jcoveney@gmail.com> wrot=
e:
>>>=20
>>> I believe if you do the following:
>>>=20
>>>=20
>>> sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4)).map((_,1)).reduceByK=
ey(_+_).mapValues(_+1).reduceByKey(_+_).toDebugString
>>>=20
>>> (8) MapPartitionsRDD[34] at reduceByKey at <console>:23 []
>>> |  MapPartitionsRDD[33] at mapValues at <console>:23 []
>>> |  ShuffledRDD[32] at reduceByKey at <console>:23 []
>>> +-(8) MapPartitionsRDD[31] at map at <console>:23 []
>>>    |  ParallelCollectionRDD[30] at parallelize at <console>:23 []
>>>=20
>>> The difference is that spark has no way to know that your map closure
>>> doesn't change the key. if you only use mapValues, it does. Pretty cool=
 that
>>> they optimized that :)
>>>=20
>>> 2015-03-26 17:44 GMT-04:00 Zhan Zhang <zzhang@hortonworks.com>:
>>>>=20
>>>> Hi Folks,
>>>>=20
>>>> Does anybody know what is the reason not allowing preserverPartitionin=
g
>>>> in RDD.map? Do I miss something here?
>>>>=20
>>>> Following example involves two shuffles. I think if preservePartitioni=
ng
>>>> is allowed, we can avoid the second one, right?
>>>>=20
>>>> val r1 =3D sc.parallelize(List(1,2,3,4,5,5,6,6,7,8,9,10,2,4))
>>>> val r2 =3D r1.map((_, 1))
>>>> val r3 =3D r2.reduceByKey(_+_)
>>>> val r4 =3D r3.map(x=3D>(x._1, x._2 + 1))
>>>> val r5 =3D r4.reduceByKey(_+_)
>>>> r5.collect.foreach(println)
>>>>=20
>>>> scala> r5.toDebugString
>>>> res2: String =3D
>>>> (8) ShuffledRDD[4] at reduceByKey at <console>:29 []
>>>> +-(8) MapPartitionsRDD[3] at map at <console>:27 []
>>>>    |  ShuffledRDD[2] at reduceByKey at <console>:25 []
>>>>    +-(8) MapPartitionsRDD[1] at map at <console>:23 []
>>>>       |  ParallelCollectionRDD[0] at parallelize at <console>:21 []
>>>>=20
>>>> Thanks.
>>>>=20
>>>> Zhan Zhang
>>>>=20
>>>> ---------------------------------------------------------------------
>>>> To unsubscribe, e-mail: user-unsubscribe@spark.apache.org
>>>> For additional commands, e-mail: user-help@spark.apache.org
>>>>=20
>>>=20
>>>=20
>>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 22:29:29 2015
Return-Path: <dev-return-12225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 889171734F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 22:29:29 +0000 (UTC)
Received: (qmail 16220 invoked by uid 500); 26 Mar 2015 22:29:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16139 invoked by uid 500); 26 Mar 2015 22:29:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16128 invoked by uid 99); 26 Mar 2015 22:29:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:29:28 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:29:21 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id BD8CD6D;
	Thu, 26 Mar 2015 22:29:00 +0000 (UTC)
Received: from G9W3615.americas.hpqcorp.net (16.216.186.50) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 26 Mar 2015 22:27:24 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G9W3615.americas.hpqcorp.net ([16.216.186.50]) with mapi id 14.03.0169.001;
 Thu, 26 Mar 2015 22:27:24 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Jeremy Freeman <freeman.jeremy@gmail.com>
CC: Stephen Boesch <javadba@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Subject: RE: Storing large data for MLlib machine learning
Thread-Topic: Storing large data for MLlib machine learning
Thread-Index: AdBoCSE6m7tKsbMVTNeZooiWf0Dn7AAAmbOAAAAFE3AAATC5AAAAl0+g
Date: Thu, 26 Mar 2015 22:27:23 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AEB9@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD2B@G4W3292.americas.hpqcorp.net>
 <CACkSZy2z5FXiXtsO_O_r7xOdHF1tQ+-2RyWWWGO9oSxhw4ydUA@mail.gmail.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FE3AD7E@G4W3292.americas.hpqcorp.net>
 <8F2BF3F2-471C-4B28-95E2-597D1D2F6675@gmail.com>
In-Reply-To: <8F2BF3F2-471C-4B28-95E2-597D1D2F6675@gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.21]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AEB9G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AEB9G4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Thanks, Jeremy! I also work with time series data right now, so your sugges=
tions are really relevant. However, we want to handle not the raw data, but=
 already processed and prepared for machine learning.

Initially, we also wanted to have our own simple binary format, but we coul=
d not agree on handling little/big endian. We did not agree if we have to s=
tick to a specific endian or to ship this information in metadata file. And=
 metadata file sounds like another data format engineering (aka inventing t=
he bicycle). Does this make sense to you?

From: Jeremy Freeman [mailto:freeman.jeremy@gmail.com]
Sent: Thursday, March 26, 2015 3:01 PM
To: Ulanov, Alexander
Cc: Stephen Boesch; dev@spark.apache.org
Subject: Re: Storing large data for MLlib machine learning

Hi Ulvanov, great question, we've encountered it frequently with scientific=
 data (e.g. time series). Agreed text is inefficient for dense arrays, and =
we also found HDF5+Spark to be a pain.

Our strategy has been flat binary files with fixed length records. Loading =
these is now supported in Spark via the binaryRecords method, which wraps a=
 custom Hadoop InputFormat we wrote.

An example (in python):

# write data from an array
from numpy import random
dat =3D random.randn(100,5)
f =3D open('test.bin', 'w')
f.write(dat)
f.close()

# load the data back in
from numpy import frombuffer
nrecords =3D 5
bytesize =3D 8
recordsize =3D nrecords * bytesize
data =3D sc.binaryRecords('test.bin', recordsize)
parsed =3D data.map(lambda v: frombuffer(buffer(v, 0, recordsize), 'float')=
)

# these should be equal
parsed.first()
dat[0,:]

Compared to something like Parquet, this is a little lighter-weight, and pl=
ays nicer with non-distributed data science tools (e.g. numpy). It also sca=
les great (we use it routinely to process TBs of time series). And handles =
single files or directories. But it's extremely simple!

-------------------------
jeremyfreeman.net<http://jeremyfreeman.net>
@thefreemanlab

On Mar 26, 2015, at 2:33 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mai=
lto:alexander.ulanov@hp.com>> wrote:


Thanks for suggestion, but libsvm is a format for sparse data storing in te=
xt file and I have dense vectors. In my opinion, text format is not appropr=
iate for storing large dense vectors due to overhead related to parsing fro=
m string to digits and also storing digits as strings is not efficient.

From: Stephen Boesch [mailto:javadba@gmail.com]
Sent: Thursday, March 26, 2015 2:27 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Storing large data for MLlib machine learning

There are some convenience methods you might consider including:

          MLUtils.loadLibSVMFile

and   MLUtils.loadLabeledPoint

2015-03-26 14:16 GMT-07:00 Ulanov, Alexander <alexander.ulanov@hp.com<mailt=
o:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com>>:
Hi,

Could you suggest what would be the reasonable file format to store feature=
 vector data for machine learning in Spark MLlib? Are there any best practi=
ces for Spark?

My data is dense feature vectors with labels. Some of the requirements are =
that the format should be easy loaded/serialized, randomly accessible, with=
 a small footprint (binary). I am considering Parquet, hdf5, protocol buffe=
r (protobuf), but I have little to no experience with them, so any suggesti=
ons would be really appreciated.

Best regards, Alexander


--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE3AEB9G4W3292americas_--

From dev-return-12226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Mar 26 22:49:34 2015
Return-Path: <dev-return-12226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A198C1743D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 26 Mar 2015 22:49:34 +0000 (UTC)
Received: (qmail 83127 invoked by uid 500); 26 Mar 2015 22:49:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83047 invoked by uid 500); 26 Mar 2015 22:49:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83032 invoked by uid 99); 26 Mar 2015 22:49:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:49:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mchettiar@rocketfuelinc.com designates 209.85.223.182 as permitted sender)
Received: from [209.85.223.182] (HELO mail-ie0-f182.google.com) (209.85.223.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 26 Mar 2015 22:49:07 +0000
Received: by ieclw3 with SMTP id lw3so58468430iec.2
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 15:48:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=rocketfuelinc.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=T3Qd8QFdbsWHcWace7k16PF2gGm678NfMvpqyr6JErY=;
        b=F/+zF+Nm7cQNtEBueWLESYZMMrWp3upxBYiBPV1WG2y06y/t2PO/IQEH5JIaS5RWx3
         pFWn4szFYxaMym0QVPCB+p9VOmDc37YVkBVQD4Qv3jZliU433qm+fP6kZJhxiL8ldT0t
         vHE0GERJQ25QMpBy8hFbRoIvoSig/+qn//Z+A=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=T3Qd8QFdbsWHcWace7k16PF2gGm678NfMvpqyr6JErY=;
        b=kdPVaJMxouiHj/ABnp1LGCcajwH/jsr1uKQvd6EI5EQ/yi9XLI38PZ3cyMG6Weziwa
         SHTbpBMdk2iYulyqEeMwdT08sR8oj92GNPrR9FS02xu1um34KMmzc2UcpGmbxZhzSvYE
         LgzKoQXTHoDoH5Fc7l0Ce4n6PdKwrwjcCupk1+dPbbdzI+Bir5kfe1tD3X6KJiPnRsfh
         i9zoGxpDvUXHwx3GejKxNEJh84pt9vYvOwpY0xy9HBnkJWvW0mS6DlYxLGuEfSF6+Tyc
         HgzLwWbK+8Dh5ZD/82atlxMe76JGctgPKiRvEL/J+GRJayjlwGQX3MDqXi6CTkI+7k81
         EFbQ==
X-Gm-Message-State: ALoCoQksKdWvV75lX6N0n79v3oAxoW27xRw9U1Tf0+tVaG/NKLtWjd137K9pc0Z1vL8fV3qr++mf
MIME-Version: 1.0
X-Received: by 10.42.167.8 with SMTP id q8mr41369082icy.94.1427410101015; Thu,
 26 Mar 2015 15:48:21 -0700 (PDT)
Received: by 10.51.15.47 with HTTP; Thu, 26 Mar 2015 15:48:20 -0700 (PDT)
In-Reply-To: <CALte62zMEuuxKTz4X8E+aLGU33anqgTO+Y9DNqVSeARC-18YKA@mail.gmail.com>
References: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
	<CALte62zMEuuxKTz4X8E+aLGU33anqgTO+Y9DNqVSeARC-18YKA@mail.gmail.com>
Date: Thu, 26 Mar 2015 15:48:20 -0700
Message-ID: <CAG6Lhyep5KbTt1i57_g7qt__FwQh0Gg=G_VxL0UNf065KY2Q+A@mail.gmail.com>
Subject: Re: Building spark 1.2 from source requires more dependencies
From: Pala M Muthaia <mchettiar@rocketfuelinc.com>
To: Ted Yu <yuzhihong@gmail.com>, dev@spark.apache.org
Cc: "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba6e83c405441a051238d0c9
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6e83c405441a051238d0c9
Content-Type: text/plain; charset=UTF-8

+spark-dev

Yes, the dependencies are there. I guess my question is how come the build
is succeeding in the mainline then, without adding these dependencies?

On Thu, Mar 26, 2015 at 3:44 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> Looking at output from dependency:tree, servlet-api is brought in by the
> following:
>
> [INFO] +- org.apache.cassandra:cassandra-all:jar:1.2.6:compile
> [INFO] |  +- org.antlr:antlr:jar:3.2:compile
> [INFO] |  +- com.googlecode.json-simple:json-simple:jar:1.1:compile
> [INFO] |  +- org.yaml:snakeyaml:jar:1.6:compile
> [INFO] |  +- edu.stanford.ppl:snaptree:jar:0.1:compile
> [INFO] |  +- org.mindrot:jbcrypt:jar:0.3m:compile
> [INFO] |  +- org.apache.thrift:libthrift:jar:0.7.0:compile
> [INFO] |  |  \- javax.servlet:servlet-api:jar:2.5:compile
>
> FYI
>
> On Thu, Mar 26, 2015 at 3:36 PM, Pala M Muthaia <
> mchettiar@rocketfuelinc.com> wrote:
>
>> Hi,
>>
>> We are trying to build spark 1.2 from source (tip of the branch-1.2 at
>> the moment). I tried to build spark using the following command:
>>
>> mvn -U -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive
>> -Phive-thriftserver -DskipTests clean package
>>
>> I encountered various missing class definition exceptions (e.g: class
>> javax.servlet.ServletException not found).
>>
>> I eventually got the build to succeed after adding the following set of
>> dependencies to the spark-core's pom.xml:
>>
>>     <dependency>
>>       <groupId>javax.servlet</groupId>
>>       <artifactId>*servlet-api*</artifactId>
>>       <version>3.0</version>
>>     </dependency>
>>
>>     <dependency>
>>       <groupId>org.eclipse.jetty</groupId>
>>       <artifactId>*jetty-io*</artifactId>
>>     </dependency>
>>
>>     <dependency>
>>       <groupId>org.eclipse.jetty</groupId>
>>       <artifactId>*jetty-http*</artifactId>
>>     </dependency>
>>
>>     <dependency>
>>       <groupId>org.eclipse.jetty</groupId>
>>       <artifactId>*jetty-servlet*</artifactId>
>>     </dependency>
>>
>> Pretty much all of the missing class definition errors came up while
>> building HttpServer.scala, and went away after the above dependencies were
>> included.
>>
>> My guess is official build for spark 1.2 is working already. My question
>> is what is wrong with my environment or setup, that requires me to add
>> dependencies to pom.xml in this manner, to get this build to succeed.
>>
>> Also, i am not sure if this build would work at runtime for us, i am
>> still testing this out.
>>
>>
>> Thanks,
>> pala
>>
>
>

--90e6ba6e83c405441a051238d0c9--

From dev-return-12227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 00:32:37 2015
Return-Path: <dev-return-12227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2812717A90
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 00:32:37 +0000 (UTC)
Received: (qmail 44788 invoked by uid 500); 27 Mar 2015 00:32:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44703 invoked by uid 500); 27 Mar 2015 00:32:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44689 invoked by uid 99); 27 Mar 2015 00:32:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 00:32:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of krajah@maprtech.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 00:32:10 +0000
Received: by qgh3 with SMTP id 3so105454152qgh.2
        for <dev@spark.apache.org>; Thu, 26 Mar 2015 17:29:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=oUi63rt+zbd+C3oUUy6A2PCDvrA1QhwbiguvwMvDbWo=;
        b=EdsiPug5MuLWRU1MOxSlk1/iis1lp5WHDStbRUSUP560ugE5cCmycDUgyOyiFfVqFy
         LKQp6lneJVxgr2SoYUAnYK1AP3fCpn+/EyD88Cl4gzhHYSGgF97f1OadBoQ6hdJ1LVHc
         FiAUceuEaRUMGVqP1Ir7d3yC/qDpenbimrAUvnlSk8MAKwcKu4lELnpbmH/Lw3HfjEWJ
         rw9JQJ7flYdDucsihBiL8GvWbDcuMulZPucQ6J6nqtQzKFkbvHtcd6I6K9zZY4nmBpCs
         A5x+7OBRxKOPmOPvJQHoSRmwim6q7QC2n7YoAwRBvhsB7ImnFADc+gCr1jJAlGlO1nNx
         vUlg==
X-Gm-Message-State: ALoCoQne/5thYfEn63H0/JGJVCTFqg1C0v702EdTk4+BMCSvrzkyp49HXE3yLWOxCDl48qKdbU9N
MIME-Version: 1.0
X-Received: by 10.140.148.20 with SMTP id 20mr21251808qhu.67.1427416193543;
 Thu, 26 Mar 2015 17:29:53 -0700 (PDT)
Received: by 10.140.104.132 with HTTP; Thu, 26 Mar 2015 17:29:53 -0700 (PDT)
In-Reply-To: <5512C755.2080509@gmail.com>
References: <CALH4WSPuGD-oMgogr0KO9mkJUbiDgkLjSJdTFk_-ArYGRMyg8Q@mail.gmail.com>
	<CANvfmP_1RUZXqzvLZ2b50AQYnJYQumYXBUri6nStp82U4XZ5oA@mail.gmail.com>
	<55129EA3.20000@gmail.com>
	<64474308D680D540A4D8151B0F7C03F7027C42F2@SHSMSX104.ccr.corp.intel.com>
	<5512C755.2080509@gmail.com>
Date: Thu, 26 Mar 2015 17:29:53 -0700
Message-ID: <CALH4WSNu6RC=9ejB92=k3VVDxQu-E=EU6LB6zSU2EwYBXt-5fg@mail.gmail.com>
Subject: Re: Understanding shuffle file name conflicts
From: Kannan Rajah <krajah@maprtech.com>
To: Cheng Lian <lian.cs.zju@gmail.com>
Cc: "Shao, Saisai" <saisai.shao@intel.com>, Saisai Shao <sai.sai.shao@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113bb4cc29c70205123a3ba6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113bb4cc29c70205123a3ba6
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Thanks folks. I understood the workflow. I noticed there is some code in
Worker.scala that creates app specific local dir.


--
Kannan

On Wed, Mar 25, 2015 at 7:33 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:

> Ah, I see where I'm wrong here. What are reused here are the shuffle map
> output files themselves, rather than the file paths. No new shuffle map
> output files are generated for the 2nd job. Thanks! Really need to walk
> through Spark core code again :)
>
> Cheng
>
>
> On 3/25/15 9:31 PM, Shao, Saisai wrote:
>
>> Hi Cheng,
>>
>> I think your scenario is acceptable for Spark's shuffle mechanism and
>> will not occur shuffle file name conflicts.
>>
>>  From my understanding I think the code snippet you mentioned is the sam=
e
>> RDD graph, just running twice, these two jobs will generate 3 stages, ma=
p
>> stage and collect stage for the first job, only collect stage for the
>> second job (map stage is the same as previous job). So these two jobs wi=
ll
>> only generate one copy of shuffle files in the first job, and fetch the
>> shuffle data twice for each job. So name conflicts will not be occurred,
>> since these two jobs rely on the same ShuffledRDD.
>>
>> I think only shuffle write which generates shuffle files will have chanc=
e
>> to meet name conflicts, multiple times of shuffle read is acceptable as =
the
>> code snippet shows.
>>
>> Thanks
>> Jerry
>>
>>
>>
>> -----Original Message-----
>> From: Cheng Lian [mailto:lian.cs.zju@gmail.com]
>> Sent: Wednesday, March 25, 2015 7:40 PM
>> To: Saisai Shao; Kannan Rajah
>> Cc: dev@spark.apache.org
>> Subject: Re: Understanding shuffle file name conflicts
>>
>> Hi Jerry & Josh
>>
>> It has been a while since the last time I looked into Spark core shuffle
>> code, maybe I=E2=80=99m wrong here. But the shuffle ID is created along =
with
>> ShuffleDependency, which is part of the RDD DAG. So if we submit multipl=
e
>> jobs over the same RDD DAG, I think the shuffle IDs in these jobs should
>> duplicate. For example:
>>
>> |val  dag  =3D  sc.parallelize(Array(1,2,3)).map(i =3D> i ->
>> |i).reduceByKey(_ + _)
>> dag.collect()
>> dag.collect()
>> |
>>
>>   From the debug log output, I did see duplicated shuffle IDs in both
>> jobs. Something like this:
>>
>> |# Job 1
>> 15/03/25 19:26:34 DEBUG BlockStoreShuffleFetcher: Fetching outputs for
>> shuffle 0, reduce 2
>>
>> # Job 2
>> 15/03/25 19:26:36 DEBUG BlockStoreShuffleFetcher: Fetching outputs for
>> shuffle 0, reduce 5
>> |
>>
>> So it=E2=80=99s also possible that some shuffle output files get reused =
in
>> different jobs. But Kannan, did you submit separate jobs over the same R=
DD
>> DAG as I did above? If not, I=E2=80=99d agree with Jerry and Josh.
>>
>> (Did I miss something here?)
>>
>> Cheng
>>
>> On 3/25/15 10:35 AM, Saisai Shao wrote:
>>
>>  Hi Kannan,
>>>
>>> As I know the shuffle Id in ShuffleDependency will be increased, so
>>> even if you run the same job twice, the shuffle dependency as well as
>>> shuffle id is different, so the shuffle file name which is combined by
>>> (shuffleId+mapId+reduceId) will be changed, so there's no name
>>> conflict even in the same directory as I know.
>>>
>>> Thanks
>>> Jerry
>>>
>>>
>>> 2015-03-25 1:56 GMT+08:00 Kannan Rajah <krajah@maprtech.com>:
>>>
>>>  I am working on SPARK-1529. I ran into an issue with my change, where
>>>> the same shuffle file was being reused across 2 jobs. Please note
>>>> this only happens when I use a hard coded location to use for shuffle
>>>> files, say "/tmp". It does not happen with normal code path that uses
>>>> DiskBlockManager to pick different directories for each run. So I
>>>> want to understand how DiskBlockManager guarantees that such a conflic=
t
>>>> will never happen.
>>>>
>>>> Let's say the shuffle block id has a value of shuffle_0_0_0. So the
>>>> data file name is shuffle_0_0_0.data and index file name is
>>>> shuffle_0_0_0.index.
>>>> If I run a spark job twice, one after another, these files get
>>>> created under different directories because of the hashing logic in
>>>> DiskBlockManager. But the hash is based off the file name, so how are
>>>> we sure that there won't be a conflict ever?
>>>>
>>>> --
>>>> Kannan
>>>>
>>>>  =E2=80=8B
>>
>
>

--001a113bb4cc29c70205123a3ba6--

From dev-return-12228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 08:49:26 2015
Return-Path: <dev-return-12228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 289DA17B3F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 08:49:26 +0000 (UTC)
Received: (qmail 66949 invoked by uid 500); 27 Mar 2015 08:49:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66870 invoked by uid 500); 27 Mar 2015 08:49:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66859 invoked by uid 99); 27 Mar 2015 08:49:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 08:49:24 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [96.236.214.3] (HELO dugos.com) (96.236.214.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Fri, 27 Mar 2015 08:48:59 +0000
Received: (qmail 14748 invoked from network); 27 Mar 2015 08:48:56 -0000
Received: from macpro.dugos.com (10.7.42.38)
  by dugos.com with SMTP; 27 Mar 2015 08:48:56 -0000
From: Doug Balog <doug.sparkdev@dugos.com>
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: quoted-printable
Subject: Support for Hive 0.14 in secure mode on hadoop 2.6.0
Message-Id: <B023E7E0-CBB2-4D1C-BD56-57EE6F807CEC@dugos.com>
Date: Fri, 27 Mar 2015 04:48:56 -0400
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
X-Mailer: Apple Mail (2.2070.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,=20
  I'm just wondering if anybody is working on supporting Hive 0.14 in =
secure mode on hadoop 2.6.0 ?
I see once Jira referring to it  =
https://issues.apache.org/jira/browse/SPARK-5111
but it mentions no effort to move to 0.14.

Thanks,

Doug



---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 10:14:21 2015
Return-Path: <dev-return-12229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9AE4217F22
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 10:14:21 +0000 (UTC)
Received: (qmail 74921 invoked by uid 500); 27 Mar 2015 10:14:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74849 invoked by uid 500); 27 Mar 2015 10:14:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74832 invoked by uid 99); 27 Mar 2015 10:14:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 10:14:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.42 as permitted sender)
Received: from [209.85.220.42] (HELO mail-pa0-f42.google.com) (209.85.220.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 10:14:00 +0000
Received: by pabxg6 with SMTP id xg6so91736675pab.0
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 03:13:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=F4x823q83pQC9Gv7uyn5lVXj1SnmUI3CYYL/eHcNbHk=;
        b=NrlYSh0c7gVw6A4pHrRsaHv+yjnk9oxgT3TwoomLtAs2BXdMRIe2NSbDU3PeBjQX0y
         +Z/+Gl5rC02YfvXNHW7Q+O62z9KnlwLUZumE2FYGdSuJaT6n5m1oIVc425fCOOupz6K5
         /wiY5iNONBJRHubU3slIZaYqkrz0qY+bWDOMto/OVNiPodXw8gVzL1SL/cmNJLMC0aH0
         eyYznwRFl6Lj1ouyX9s8vX2HInRgV85mv7BxO/3TrPoZfDxXHLfLoRHIwJ1dI35lq1Ph
         DGb0eN6XLivSacQTR1yvI2RLoF+jvC2tZCjha2pa1JZ+DyMH8NfDc5omwePsTvEP/fta
         S5EA==
X-Received: by 10.68.230.34 with SMTP id sv2mr33825877pbc.157.1427451220433;
        Fri, 27 Mar 2015 03:13:40 -0700 (PDT)
Received: from [10.10.0.3] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id di10sm1676058pad.41.2015.03.27.03.13.38
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Fri, 27 Mar 2015 03:13:39 -0700 (PDT)
Message-ID: <55152D4E.1090608@gmail.com>
Date: Fri, 27 Mar 2015 18:13:34 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Doug Balog <doug.sparkdev@dugos.com>, dev@spark.apache.org
Subject: Re: Support for Hive 0.14 in secure mode on hadoop 2.6.0
References: <B023E7E0-CBB2-4D1C-BD56-57EE6F807CEC@dugos.com>
In-Reply-To: <B023E7E0-CBB2-4D1C-BD56-57EE6F807CEC@dugos.com>
Content-Type: text/plain; charset=windows-1252; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

We're planning to replace the current Hive version profiles and shim 
layer with an adaption layer in Spark SQL in 1.4. This adaption layer 
allows Spark SQL to connect to arbitrary Hive version greater than or 
equal to 0.12.0 (or maybe 0.13.1, not decided yet).

However, it's not a promise yet, since this requires major refactoring 
of the current Spark SQL Hive support.

Cheng

On 3/27/15 4:48 PM, Doug Balog wrote:
> Hi,
>    I'm just wondering if anybody is working on supporting Hive 0.14 in secure mode on hadoop 2.6.0 ?
> I see once Jira referring to it  https://issues.apache.org/jira/browse/SPARK-5111
> but it mentions no effort to move to 0.14.
>
> Thanks,
>
> Doug
>
>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 12:26:26 2015
Return-Path: <dev-return-12230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 964201778F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 12:26:26 +0000 (UTC)
Received: (qmail 93995 invoked by uid 500); 27 Mar 2015 12:26:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93914 invoked by uid 500); 27 Mar 2015 12:26:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93897 invoked by uid 99); 27 Mar 2015 12:26:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 12:26:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 74.125.82.54 as permitted sender)
Received: from [74.125.82.54] (HELO mail-wg0-f54.google.com) (74.125.82.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 12:26:20 +0000
Received: by wgra20 with SMTP id a20so97120526wgr.3
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 05:23:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=SMSO9hqZUvMzN1hKaBs5YTw0VESMJurNTKlyxfVHm5k=;
        b=HSJdP2tY9ISt7Rmx9dGm6caa9qf75P15wb6tlitnmR8rfp8QUoygKgjt+XvV3VfBZw
         qaYYosOQ1NvzX/1cTuDuqPqEmZOGU+uTN1NtTyqjHwYvMDvZNTP5H3AqCUJtVmT9tEHK
         SyaI/eCAtoX7gM/PXQXr/6AtsOKggvZkafijLswmh84IfgjrgebMblDGcD0rWuujedeg
         WDYBq9FdFQXEr6TM9Q6Yow7aFxbfLcX6anWhlkEeL5Yyn+Z+HCthglLw1dQcGpmfKL+i
         ppKXaMmQWAuYOhnSCeKOkFQgbmWAT6E48q+5W/ksmrXkLkAw7A9BEWjKZ4287WCYlh0w
         motg==
X-Gm-Message-State: ALoCoQlr0BC/AGQXym8qc8W/Qud2/XDHR1/l1EeYX467QDcu/ZlWwseynck/c3u69o1EstJ/kymr
X-Received: by 10.194.200.166 with SMTP id jt6mr36751451wjc.66.1427459024549;
 Fri, 27 Mar 2015 05:23:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 27 Mar 2015 05:23:24 -0700 (PDT)
In-Reply-To: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
References: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 27 Mar 2015 12:23:24 +0000
Message-ID: <CAMAsSdJoswwE0BOA114FqoEQTEXbusvtB7ken_8wLnKzNRLQ8w@mail.gmail.com>
Subject: Re: Building spark 1.2 from source requires more dependencies
To: Pala M Muthaia <mchettiar@rocketfuelinc.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I built from the head of branch-1.2 and spark-core compiled correctly
with your exact command. You have something wrong with how you are
building. For example, you're not trying to run this from the core
subdirectory are you?

On Thu, Mar 26, 2015 at 10:36 PM, Pala M Muthaia
<mchettiar@rocketfuelinc.com> wrote:
> Hi,
>
> We are trying to build spark 1.2 from source (tip of the branch-1.2 at the
> moment). I tried to build spark using the following command:
>
> mvn -U -Pyarn -Phadoop-2.4 -Dhadoop.version=2.4.0 -Phive -Phive-thriftserver
> -DskipTests clean package
>
> I encountered various missing class definition exceptions (e.g: class
> javax.servlet.ServletException not found).
>
> I eventually got the build to succeed after adding the following set of
> dependencies to the spark-core's pom.xml:
>
>     <dependency>
>       <groupId>javax.servlet</groupId>
>       <artifactId>servlet-api</artifactId>
>       <version>3.0</version>
>     </dependency>
>
>     <dependency>
>       <groupId>org.eclipse.jetty</groupId>
>       <artifactId>jetty-io</artifactId>
>     </dependency>
>
>     <dependency>
>       <groupId>org.eclipse.jetty</groupId>
>       <artifactId>jetty-http</artifactId>
>     </dependency>
>
>     <dependency>
>       <groupId>org.eclipse.jetty</groupId>
>       <artifactId>jetty-servlet</artifactId>
>     </dependency>
>
> Pretty much all of the missing class definition errors came up while
> building HttpServer.scala, and went away after the above dependencies were
> included.
>
> My guess is official build for spark 1.2 is working already. My question is
> what is wrong with my environment or setup, that requires me to add
> dependencies to pom.xml in this manner, to get this build to succeed.
>
> Also, i am not sure if this build would work at runtime for us, i am still
> testing this out.
>
>
> Thanks,
> pala

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 17:05:11 2015
Return-Path: <dev-return-12231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 86F88177A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 17:05:11 +0000 (UTC)
Received: (qmail 86440 invoked by uid 500); 27 Mar 2015 17:05:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86378 invoked by uid 500); 27 Mar 2015 17:05:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86366 invoked by uid 99); 27 Mar 2015 17:05:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:05:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.213.171 as permitted sender)
Received: from [209.85.213.171] (HELO mail-ig0-f171.google.com) (209.85.213.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:04:44 +0000
Received: by igbqf9 with SMTP id qf9so25324387igb.1
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 10:02:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=GwQegPeO806rurUY6qBmvq5vBNr1wu/i3F4JBVQkT+k=;
        b=0KCbMpJ7qQzs/8UNI0zHYdQeXa/BAvvMHoR3dQDaqDu6ESJ/qFPsktQqoGDBo1kPkr
         rL02UXxS9LMgHdcjQ0cma+9nHS7mhK6df5adMv4jvo3Z2JjsU+rtkg4FeuWpvbpkvLQN
         XtR1N2rW6dpToQohutHbFuPtkIu/6DhjXbEifxbzACGTTTjhp2SHOD4FUlPz5EcukKCn
         /FH7se1nuxrpmSYjEZRgWk0Zjp6JgcUtTjz9KGZSU69FxBN+x3AKb972/50tYf/5++5w
         Zh5npna2njwUNi9J+Cr2rFlyUJPV/0WEc7zZGYmJxspHxbDf9adZZIKViI5RlmXdnz00
         Nnjw==
MIME-Version: 1.0
X-Received: by 10.107.135.75 with SMTP id j72mr31172649iod.0.1427475747426;
 Fri, 27 Mar 2015 10:02:27 -0700 (PDT)
Received: by 10.107.155.143 with HTTP; Fri, 27 Mar 2015 10:02:27 -0700 (PDT)
Date: Fri, 27 Mar 2015 10:02:27 -0700
Message-ID: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
Subject: Iterative pyspark / scala codebase development
From: Stephen Boesch <javadba@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113fb98ed9d9970512481869
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113fb98ed9d9970512481869
Content-Type: text/plain; charset=UTF-8

I am iteratively making changes to the scala side of some new pyspark code
and re-testing from the python/pyspark side.

Presently my only solution is to rebuild completely

      sbt assembly

after any scala side change - no matter how small.

Any better / expedited way for pyspark to see small scala side updates?

--001a113fb98ed9d9970512481869--

From dev-return-12232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 17:12:16 2015
Return-Path: <dev-return-12232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2EB9E17855
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 17:12:16 +0000 (UTC)
Received: (qmail 17361 invoked by uid 500); 27 Mar 2015 17:12:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17284 invoked by uid 500); 27 Mar 2015 17:12:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17273 invoked by uid 99); 27 Mar 2015 17:12:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:12:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.216.182] (HELO mail-qc0-f182.google.com) (209.85.216.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:12:09 +0000
Received: by qcbjx9 with SMTP id jx9so22099501qcb.0
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 10:11:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=1HThBXHY0nqlKUOFmlNuTHMvpNVo4gFfuKoxnGcXR/k=;
        b=ilUqNGpCsLBUMC3jdOXHqBoDUpp7ziijD+lK+tTZ0IHT6yxGfOHKg2yhxf/rqd8F4+
         FQHpef2onSwvOn8aumlrh/iKfdQ4YBkqeIhmpdq0FsMnDBTOxy2Qv5rM+yN/iKvcsc3D
         q7UHB4UC+g6/TYdo102E4zgDIdqYRUQ1JuX7FsR+5C1bhJkiM3OuW+qg0nV/Mp9XBn0f
         PZ1FR8QJTjzW0A1zRtxshee9xrtIVeCDqNLnutmNnfqS/dJJR87KuMatFRxzhIfoiVhH
         oiK5l2Xak+hXxKN7h5AElVa3m9uc/xxElVeMdWL+2+S3r2/pbyNBNfau9csfYnnJ7pQm
         248A==
X-Gm-Message-State: ALoCoQmHXJO5B/CYk2ttw9gYJDguwR150IC9qu1SPQD8vLlN4gxDzUqDRYWcCauHscv9wqyJnOm5
X-Received: by 10.140.148.216 with SMTP id 207mr26855932qhu.62.1427476287366;
 Fri, 27 Mar 2015 10:11:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Fri, 27 Mar 2015 10:11:06 -0700 (PDT)
In-Reply-To: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
References: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 27 Mar 2015 10:11:06 -0700
Message-ID: <CAPh_B=a35OD=BVZQeaWvEMhuaE5T97MKEJRQHP1k9aJd3tcWQQ@mail.gmail.com>
Subject: Re: Iterative pyspark / scala codebase development
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113769d008bb7705124839b6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113769d008bb7705124839b6
Content-Type: text/plain; charset=UTF-8

Python is tough if you need to change Scala at the same time.

sbt/sbt assembly/assembly

can be slightly faster than just assembly.


On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch <javadba@gmail.com> wrote:

> I am iteratively making changes to the scala side of some new pyspark code
> and re-testing from the python/pyspark side.
>
> Presently my only solution is to rebuild completely
>
>       sbt assembly
>
> after any scala side change - no matter how small.
>
> Any better / expedited way for pyspark to see small scala side updates?
>

--001a113769d008bb7705124839b6--

From dev-return-12233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 17:14:43 2015
Return-Path: <dev-return-12233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9712317873
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 17:14:43 +0000 (UTC)
Received: (qmail 29813 invoked by uid 500); 27 Mar 2015 17:14:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29757 invoked by uid 500); 27 Mar 2015 17:14:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29746 invoked by uid 99); 27 Mar 2015 17:14:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:14:38 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.217.174] (HELO mail-lb0-f174.google.com) (209.85.217.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:14:13 +0000
Received: by lbbug6 with SMTP id ug6so68314480lbb.3
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 10:13:06 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=TjClc1QH0/naMgbsF7iVz2hInXAOTZdHV7KsMmUNflI=;
        b=R7+WUbK741ZLRDNh91gzG/JxUU+nSHREid6bM8I0C9dG41PForTl/qOvAbYTDP/lS2
         /C2FHquTw6IxbaBEVJ8KRIh+gVkY7jTX5CEbnGk7Ex0//wQSdHTRFzVD8Gv69ECtE4yv
         90HX/yYD7ob2xCVV8TEqUWp7YiKAu/7MZgeyOH+XrLOwU559ugPKl2axIfMZETrkCGaf
         h9FnwspRfyaQUmCQFD8OTMHFyuDnHJ3/zmhbe6WXZ6Km7H4riHMo+LgxYkqLuK3i3Z+R
         x5vWhrZwb5BftLBkrhwctm8HVe5T+Q7KPWJrED7yNVvnMVCKT/xPw2dTivfEYxH3RY3A
         tYTA==
X-Gm-Message-State: ALoCoQmlsOFBivV2ZQh3BWw+P7hLLMnszyERDz6q6DWnoYWxMSuOE/zlrYIwk7RwPOPWm7wQUAbp
MIME-Version: 1.0
X-Received: by 10.152.198.8 with SMTP id iy8mr18754897lac.13.1427476386793;
 Fri, 27 Mar 2015 10:13:06 -0700 (PDT)
Received: by 10.25.31.68 with HTTP; Fri, 27 Mar 2015 10:13:06 -0700 (PDT)
In-Reply-To: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
References: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
Date: Fri, 27 Mar 2015 10:13:06 -0700
Message-ID: <CA+2Pv=gO4ZkaBCDMmiCfEWsv59DFrpgOiN-A+YFWF4aq0x2jaQ@mail.gmail.com>
Subject: Re: Iterative pyspark / scala codebase development
From: Davies Liu <davies@databricks.com>
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

see https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools

On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch <javadba@gmail.com> wrote:
> I am iteratively making changes to the scala side of some new pyspark code
> and re-testing from the python/pyspark side.
>
> Presently my only solution is to rebuild completely
>
>       sbt assembly
>
> after any scala side change - no matter how small.
>
> Any better / expedited way for pyspark to see small scala side updates?

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 17:17:04 2015
Return-Path: <dev-return-12234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F28CC178A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 17:17:03 +0000 (UTC)
Received: (qmail 37548 invoked by uid 500); 27 Mar 2015 17:17:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37471 invoked by uid 500); 27 Mar 2015 17:17:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37460 invoked by uid 99); 27 Mar 2015 17:17:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:17:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.53] (HELO mail-la0-f53.google.com) (209.85.215.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:16:37 +0000
Received: by labe2 with SMTP id e2so75439528lab.3
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 10:16:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=2kBXrFrj9lf8iTqakM4Jvf7ofWwU8p6rFEGzh2AQUR4=;
        b=kc1/XPFisYBI5b1wGLldcKS2YsIe90FcosOm9/1h1Y4dmkN9uy8o9C3rkCdK4s4la9
         TZmI9VALd3njeQizCVG9AIgw+0ofbfkfXXGUgYjTctwaOSCVfhjb5E2qtCDEx9cZvrmE
         mdxtWIcMqOHjTj59WZiC6YPfzeu18xNVyKk+QR7fOXqlxSup0gdt+nmq8j8pUt/cYDEd
         YXCF/SMY7ZiDTl5K72uoS2Gy18Cds7fxBVRMTt72RlKapmyGU7E7VyPBNdhwm+uXJOKt
         lyWkYzr8LTG2psCQqXdctYK3icPIlAfqVHxhLKN58ypKA3UPg1YmGlNuo8JxhstpE7Rp
         aiig==
X-Gm-Message-State: ALoCoQmJNSf0Rr4xi1MwLrTsweDFmHdzpjArAbNkMRn9gsGEbrQxZxroKGQmkGpV3GWNE51AAKWq
MIME-Version: 1.0
X-Received: by 10.152.178.197 with SMTP id da5mr19071629lac.56.1427476575265;
 Fri, 27 Mar 2015 10:16:15 -0700 (PDT)
Received: by 10.25.31.68 with HTTP; Fri, 27 Mar 2015 10:16:15 -0700 (PDT)
In-Reply-To: <CAPh_B=a35OD=BVZQeaWvEMhuaE5T97MKEJRQHP1k9aJd3tcWQQ@mail.gmail.com>
References: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
	<CAPh_B=a35OD=BVZQeaWvEMhuaE5T97MKEJRQHP1k9aJd3tcWQQ@mail.gmail.com>
Date: Fri, 27 Mar 2015 10:16:15 -0700
Message-ID: <CA+2Pv=jG7RUBEavhGx2vXK_gYMEAr8q=BBz+EtUXkBQUM=yhLw@mail.gmail.com>
Subject: Re: Iterative pyspark / scala codebase development
From: Davies Liu <davies@databricks.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Stephen Boesch <javadba@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I usually just open a terminal to do `build/sbt ~compile`, coding in
IntelliJ, then run python tests in another terminal once it compiled
successfully.

On Fri, Mar 27, 2015 at 10:11 AM, Reynold Xin <rxin@databricks.com> wrote:
> Python is tough if you need to change Scala at the same time.
>
> sbt/sbt assembly/assembly
>
> can be slightly faster than just assembly.
>
>
> On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch <javadba@gmail.com> wrote:
>
>> I am iteratively making changes to the scala side of some new pyspark code
>> and re-testing from the python/pyspark side.
>>
>> Presently my only solution is to rebuild completely
>>
>>       sbt assembly
>>
>> after any scala side change - no matter how small.
>>
>> Any better / expedited way for pyspark to see small scala side updates?
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 17:18:31 2015
Return-Path: <dev-return-12235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C864178B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 17:18:31 +0000 (UTC)
Received: (qmail 42167 invoked by uid 500); 27 Mar 2015 17:18:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42083 invoked by uid 500); 27 Mar 2015 17:18:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42038 invoked by uid 99); 27 Mar 2015 17:18:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:18:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:18:25 +0000
Received: by igbqf9 with SMTP id qf9so25710066igb.1
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 10:18:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=sBMFWw1BKaedfMkpouUa/6dxrExkwYDtE3CuIidzwyU=;
        b=fDLLSTCyBuMtJQQNWI+wxjS+aUjUh9DrspeEIKlgsl68o2sFm1MgjpLfVj4zJXVKgm
         OvYk+D01d7xkTB4lWpgtp0GUPT5kJco3jZlAkjXZdey6bkxsVXb8cOMQKmtoPgoxPMb0
         xbC+Mzz2QAxirmk9enu446O2J4kQCpR2c6hd5xsR5zBhqc0YOfrYaILCCkmfj7ZlPiWB
         XcfNFAnxOS7Xsvdggm46Im8CA1bKpunsNPK10SADlNw5WA8wu7W5Tqq4StgvLb0jqiqd
         +mJxRoh3TZNF6si3FEDrx/6Iaj8lYj9fu2wSZlsyKpvIWr+0BhlZG/iBh1p/9jPKsnKt
         46Ug==
MIME-Version: 1.0
X-Received: by 10.50.18.49 with SMTP id t17mr46246744igd.3.1427476684960; Fri,
 27 Mar 2015 10:18:04 -0700 (PDT)
Received: by 10.107.155.143 with HTTP; Fri, 27 Mar 2015 10:18:04 -0700 (PDT)
In-Reply-To: <CA+2Pv=jG7RUBEavhGx2vXK_gYMEAr8q=BBz+EtUXkBQUM=yhLw@mail.gmail.com>
References: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
	<CAPh_B=a35OD=BVZQeaWvEMhuaE5T97MKEJRQHP1k9aJd3tcWQQ@mail.gmail.com>
	<CA+2Pv=jG7RUBEavhGx2vXK_gYMEAr8q=BBz+EtUXkBQUM=yhLw@mail.gmail.com>
Date: Fri, 27 Mar 2015 10:18:04 -0700
Message-ID: <CACkSZy2jg-QrTZvE8FmKmB7hYDA-hCQKyLrEsWNOsnqRMVgDig@mail.gmail.com>
Subject: Re: Iterative pyspark / scala codebase development
From: Stephen Boesch <javadba@gmail.com>
To: Davies Liu <davies@databricks.com>
Cc: Reynold Xin <rxin@databricks.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0149c0a0bb7c21051248501b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149c0a0bb7c21051248501b
Content-Type: text/plain; charset=UTF-8

Compile alone did not show the scala code changes AFAICT. I will reverify.

2015-03-27 10:16 GMT-07:00 Davies Liu <davies@databricks.com>:

> I usually just open a terminal to do `build/sbt ~compile`, coding in
> IntelliJ, then run python tests in another terminal once it compiled
> successfully.
>
> On Fri, Mar 27, 2015 at 10:11 AM, Reynold Xin <rxin@databricks.com> wrote:
> > Python is tough if you need to change Scala at the same time.
> >
> > sbt/sbt assembly/assembly
> >
> > can be slightly faster than just assembly.
> >
> >
> > On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch <javadba@gmail.com>
> wrote:
> >
> >> I am iteratively making changes to the scala side of some new pyspark
> code
> >> and re-testing from the python/pyspark side.
> >>
> >> Presently my only solution is to rebuild completely
> >>
> >>       sbt assembly
> >>
> >> after any scala side change - no matter how small.
> >>
> >> Any better / expedited way for pyspark to see small scala side updates?
> >>
>

--089e0149c0a0bb7c21051248501b--

From dev-return-12236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 17:22:18 2015
Return-Path: <dev-return-12236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 170D0178EB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 17:22:18 +0000 (UTC)
Received: (qmail 59544 invoked by uid 500); 27 Mar 2015 17:22:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59465 invoked by uid 500); 27 Mar 2015 17:22:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59454 invoked by uid 99); 27 Mar 2015 17:22:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:22:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 17:22:11 +0000
Received: by lagg8 with SMTP id g8so75650808lag.1
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 10:21:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=vXdIxAvVd47OLhqoDiBLttaQKEgeo1B2u0Ky8aypYJg=;
        b=d0SBE30vV0b8yTycsaYe8eotcgE16CIy1UNVLVAObMYs9KXr6FAC9TWqWgLfjanhLt
         i/tqol4kArX5Z95Oqw4+cEbS/Qx4IkuuxN2Z7N1opb9dEBJtiV+pvv8pyOzpL7iiY3JO
         xE6wVJ3OO1mC3mroHNJxYLm/7YqDIzv6ANs3AJF9Hoas6Cxpl5Ki/pWWkHAKSUvOa0m6
         OvXg/y7W86c6Iw/adAsVa+If+vuDEBefkzrn6i7b6xfELLqWtrS8ob8Pi5o0R/07ZmIc
         yXYqPziY4GsZOlyE+UYx/4RWqw4ZXjbY5xPLQARhh42mxEAIoTc3ichEDn1+yuwJYSeX
         xZ1A==
X-Gm-Message-State: ALoCoQltP4Oe2QE+uEXylVtSnCd3YZ/5Th2usrd74HhaK5AAbbrIlW8xxjqyCh0WYH1vd6LR3cOb
MIME-Version: 1.0
X-Received: by 10.152.9.98 with SMTP id y2mr18508743laa.94.1427476890221; Fri,
 27 Mar 2015 10:21:30 -0700 (PDT)
Received: by 10.25.31.68 with HTTP; Fri, 27 Mar 2015 10:21:30 -0700 (PDT)
In-Reply-To: <CACkSZy00=i5U3=08nbRsQ3kxdyVUMXfNrz1P=1Fhtjtk7p761A@mail.gmail.com>
References: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
	<CA+2Pv=gO4ZkaBCDMmiCfEWsv59DFrpgOiN-A+YFWF4aq0x2jaQ@mail.gmail.com>
	<CACkSZy00=i5U3=08nbRsQ3kxdyVUMXfNrz1P=1Fhtjtk7p761A@mail.gmail.com>
Date: Fri, 27 Mar 2015 10:21:30 -0700
Message-ID: <CA+2Pv=i2GorzekE1yW7pm7DOJ1Z-EJRAZhoHCsVsivbMpEc7Zw@mail.gmail.com>
Subject: Re: Iterative pyspark / scala codebase development
From: Davies Liu <davies@databricks.com>
To: Stephen Boesch <javadba@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

put these lines in your ~/.bash_profile

export SPARK_PREPEND_CLASSES=true
export SPARK_HOME=path_to_spark
export PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip:${SPARK_HOME}/python:${PYTHONPATH}"

$ source ~/.bash_profile
$ build/sbt assembly
$ build/sbt ~compile  # do not stop this

Then in another terminal you could run python tests as
$ cd python/pyspark/
$  python rdd.py


cc to dev list


On Fri, Mar 27, 2015 at 10:15 AM, Stephen Boesch <javadba@gmail.com> wrote:
> Which aspect of that page are you suggesting provides a more optimized
> alternative?
>
> 2015-03-27 10:13 GMT-07:00 Davies Liu <davies@databricks.com>:
>
>> see
>> https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools
>>
>> On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch <javadba@gmail.com>
>> wrote:
>> > I am iteratively making changes to the scala side of some new pyspark
>> > code
>> > and re-testing from the python/pyspark side.
>> >
>> > Presently my only solution is to rebuild completely
>> >
>> >       sbt assembly
>> >
>> > after any scala side change - no matter how small.
>> >
>> > Any better / expedited way for pyspark to see small scala side updates?
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 19:09:31 2015
Return-Path: <dev-return-12237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 337A317EED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 19:09:31 +0000 (UTC)
Received: (qmail 44100 invoked by uid 500); 27 Mar 2015 19:09:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44024 invoked by uid 500); 27 Mar 2015 19:09:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44012 invoked by uid 99); 27 Mar 2015 19:09:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 19:09:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,NORMAL_HTTP_TO_IP,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mchettiar@rocketfuelinc.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 19:09:23 +0000
Received: by obbgh1 with SMTP id gh1so16311008obb.1
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 12:09:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=rocketfuelinc.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=KWCieN/LvkqXafRY5UPZcyxUZQSEWGHtghPHBj8jajU=;
        b=OSa+So9f+MYfUzYu+oJUGk0xr5ZjHRLH8fSi4wJ2te3FzGoidJIImWu6SEcvXymM6N
         uUlQz0rIDZLgCvzc+0wxWn26i8692c1x3xn3ofH47PaM7tRCBtreEAkHITWL942ppm8G
         g07me5kWObFUx3sQ1BFO6eH/Y5mBtK8nWiubQ=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=KWCieN/LvkqXafRY5UPZcyxUZQSEWGHtghPHBj8jajU=;
        b=Lk0a1YtMbuByFVS5U1pCMDxpmxOqG8turX4O43OarRIraF9QgC1PTRTDe8EfuJmukg
         Huw89BcqaLXEu+USyy5ihG5gfOO9TnGXJNNWkjaGjd1INTMor9rUjVm8cejavGFC/tgU
         LX8gLlTqhjPemtgsgwhFHrzLAdNCtN8TJwcbH7WGpidsMFHIypukWwkBZx6KjLgHUwst
         TfeJ6YAJC6pG/OsHN58PzJGyTuFA5NJ+d885thl6929eE6g786ac/kvNEaw4TcFZfPk6
         YJUhZJXDgnCt5dSEOfKy7VM/ktq5FkoAvt3o8af/wAR9yC6bJ0PBlqOgXol0T9XynWMf
         Sx5g==
X-Gm-Message-State: ALoCoQlc1LLzK3ClHOs5nztRuoR+GtqC2rRrppg2VycI0b6sqlF5vz0P5NZnOEgiPo8p7sJtjs8Z
MIME-Version: 1.0
X-Received: by 10.60.116.68 with SMTP id ju4mr17258886oeb.48.1427483342463;
 Fri, 27 Mar 2015 12:09:02 -0700 (PDT)
Received: by 10.76.158.195 with HTTP; Fri, 27 Mar 2015 12:09:02 -0700 (PDT)
In-Reply-To: <CAMAsSdJoswwE0BOA114FqoEQTEXbusvtB7ken_8wLnKzNRLQ8w@mail.gmail.com>
References: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
	<CAMAsSdJoswwE0BOA114FqoEQTEXbusvtB7ken_8wLnKzNRLQ8w@mail.gmail.com>
Date: Fri, 27 Mar 2015 12:09:02 -0700
Message-ID: <CAG6LhydcVBUQF6-yJoFRQVzYZaCqJtbREt_3MOPfed5+8WfRhA@mail.gmail.com>
Subject: Re: Building spark 1.2 from source requires more dependencies
From: Pala M Muthaia <mchettiar@rocketfuelinc.com>
To: Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0115f2308d004c051249dd03
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115f2308d004c051249dd03
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

No, i am running from the root directory, parent of core.

Here is the first set of errors that i see when i compile from source
(sorry the error message is very long, but adding it in case it helps in
diagnosis). After i manually add javax.servlet dependency for  version 3.0,
these set of errors go away and i get the next set of errors about missing
classes under eclipse-jetty.

I am on maven 3.2.5 and java 1.7.

Error:

[INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
spark-core_2.10 ---
[WARNING] Zinc server is not available at port 3030 - reverting to normal
incremental compile
[INFO] Using incremental compilation
[INFO] compiler plugin:
BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
[INFO] Compiling 403 Scala sources and 33 Java sources to
/Users/mchettiar/code/spark/core/target/scala-2.10/classes...
[WARNING] Class javax.servlet.ServletException not found - continuing with
a stub.
[ERROR]
     while compiling:
/Users/mchettiar/code/spark/core/src/main/scala/org/apache/spark/HttpServer=
.scala
        during phase: typer
     library version: version 2.10.4
    compiler version: version 2.10.4
  reconstructed args: -deprecation -feature
-Xplugin:/Users/mchettiar/.m2/repository/org/scalamacros/paradise_2.10.4/2.=
0.1/paradise_2.10.4-2.0.1.jar
-bootclasspath
/Library/Java/JavaVirtualMachines/jdk1.7.0_75.jdk/Contents/Home/jre/lib/res=
ources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_75.jdk/Contents/Home/=
jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_75.jdk/Contents/H=
ome/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_75.jd=
k/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0=
_75.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk=
1.7.0_75.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMa=
chines/jdk1.7.0_75.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirt=
ualMachines/jdk1.7.0_75.jdk/Contents/Home/jre/classes:/Users/mchettiar/.m2/=
repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
-classpath
/Users/mchettiar/code/spark/core/target/scala-2.10/classes:/Users/mchettiar=
/.m2/repository/com/twitter/chill_2.10/0.5.0/chill_2.10-0.5.0.jar:/Users/mc=
hettiar/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/U=
sers/mchettiar/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.=
07/reflectasm-1.07-shaded.jar:/Users/mchettiar/.m2/repository/com/esoterics=
oftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/mchettiar/.m2/repository/or=
g/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/mchettiar/.m2/repository=
/com/twitter/chill-java/0.5.0/chill-java-0.5.0.jar:/Users/mchettiar/.m2/rep=
ository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.4.0.jar:/User=
s/mchettiar/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/hadoop-com=
mon-2.4.0.jar:/Users/mchettiar/.m2/repository/commons-cli/commons-cli/1.2/c=
ommons-cli-1.2.jar:/Users/mchettiar/.m2/repository/xmlenc/xmlenc/0.52/xmlen=
c-0.52.jar:/Users/mchettiar/.m2/repository/commons-httpclient/commons-httpc=
lient/3.1/commons-httpclient-3.1.jar:/Users/mchettiar/.m2/repository/common=
s-io/commons-io/2.4/commons-io-2.4.jar:/Users/mchettiar/.m2/repository/comm=
ons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Us=
ers/mchettiar/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6=
.jar:/Users/mchettiar/.m2/repository/commons-configuration/commons-configur=
ation/1.6/commons-configuration-1.6.jar:/Users/mchettiar/.m2/repository/com=
mons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/mchettia=
r/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutil=
s-1.7.0.jar:/Users/mchettiar/.m2/repository/commons-beanutils/commons-beanu=
tils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/mchettiar/.m2/repos=
itory/org/codehaus/jackson/jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.ja=
r:/Users/mchettiar/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1=
.8.8/jackson-mapper-asl-1.8.8.jar:/Users/mchettiar/.m2/repository/org/apach=
e/avro/avro/1.7.6/avro-1.7.6.jar:/Users/mchettiar/.m2/repository/com/google=
/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/mchettiar/.m2/=
repository/org/apache/hadoop/hadoop-auth/2.4.0/hadoop-auth-2.4.0.jar:/Users=
/mchettiar/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons=
-compress-1.4.1.jar:/Users/mchettiar/.m2/repository/org/tukaani/xz/1.0/xz-1=
.0.jar:/Users/mchettiar/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.4.0/=
hadoop-hdfs-2.4.0.jar:/Users/mchettiar/.m2/repository/org/mortbay/jetty/jet=
ty-util/6.1.26/jetty-util-6.1.26.jar:/Users/mchettiar/.m2/repository/org/ap=
ache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-2=
.4.0.jar:/Users/mchettiar/.m2/repository/org/apache/hadoop/hadoop-mapreduce=
-client-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/mchett=
iar/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-c=
lient-2.4.0.jar:/Users/mchettiar/.m2/repository/com/sun/jersey/jersey-clien=
t/1.9/jersey-client-1.9.jar:/Users/mchettiar/.m2/repository/org/apache/hado=
op/hadoop-yarn-server-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Use=
rs/mchettiar/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuff=
le/2.4.0/hadoop-mapreduce-client-shuffle-2.4.0.jar:/Users/mchettiar/.m2/rep=
ository/org/apache/hadoop/hadoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/=
Users/mchettiar/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-co=
re/2.4.0/hadoop-mapreduce-client-core-2.4.0.jar:/Users/mchettiar/.m2/reposi=
tory/org/apache/hadoop/hadoop-yarn-common/2.4.0/hadoop-yarn-common-2.4.0.ja=
r:/Users/mchettiar/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.=
2.2.jar:/Users/mchettiar/.m2/repository/javax/xml/stream/stax-api/1.0-2/sta=
x-api-1.0-2.jar:/Users/mchettiar/.m2/repository/javax/activation/activation=
/1.1/activation-1.1.jar:/Users/mchettiar/.m2/repository/com/sun/jersey/jers=
ey-core/1.9/jersey-core-1.9.jar:/Users/mchettiar/.m2/repository/org/apache/=
hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-client-jobc=
lient-2.4.0.jar:/Users/mchettiar/.m2/repository/org/apache/hadoop/hadoop-an=
notations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/mchettiar/code/spark/ne=
twork/common/target/spark-network-common_2.10-1.2.2-SNAPSHOT.jar:/Users/mch=
ettiar/code/spark/network/shuffle/target/spark-network-shuffle_2.10-1.2.2-S=
NAPSHOT.jar:/Users/mchettiar/.m2/repository/net/java/dev/jets3t/jets3t/0.9.=
0/jets3t-0.9.0.jar:/Users/mchettiar/.m2/repository/commons-codec/commons-co=
dec/1.5/commons-codec-1.5.jar:/Users/mchettiar/.m2/repository/org/apache/ht=
tpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/Users/mchettiar/.m2/rep=
ository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-4.1.2.jar:/Users/=
mchettiar/.m2/repository/com/jamesmurty/utils/java-xmlbuilder/0.4/java-xmlb=
uilder-0.4.jar:/Users/mchettiar/.m2/repository/org/apache/curator/curator-r=
ecipes/2.4.0/curator-recipes-2.4.0.jar:/Users/mchettiar/.m2/repository/org/=
apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/m=
chettiar/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-cli=
ent-2.4.0.jar:/Users/mchettiar/.m2/repository/org/apache/zookeeper/zookeepe=
r/3.4.5/zookeeper-3.4.5.jar:/Users/mchettiar/.m2/repository/jline/jline/0.9=
.94/jline-0.9.94.jar:/Users/mchettiar/.m2/repository/org/eclipse/jetty/jett=
y-plus/8.1.14.v20131031/jetty-plus-8.1.14.v20131031.jar:/Users/mchettiar/.m=
2/repository/org/eclipse/jetty/orbit/javax.transaction/1.1.1.v201105210645/=
javax.transaction-1.1.1.v201105210645.jar:/Users/mchettiar/.m2/repository/o=
rg/eclipse/jetty/jetty-webapp/8.1.14.v20131031/jetty-webapp-8.1.14.v2013103=
1.jar:/Users/mchettiar/.m2/repository/org/eclipse/jetty/jetty-jndi/8.1.14.v=
20131031/jetty-jndi-8.1.14.v20131031.jar:/Users/mchettiar/.m2/repository/or=
g/eclipse/jetty/orbit/javax.mail.glassfish/1.4.1.v201005082020/javax.mail.g=
lassfish-1.4.1.v201005082020.jar:/Users/mchettiar/.m2/repository/org/eclips=
e/jetty/orbit/javax.activation/1.1.0.v201105071233/javax.activation-1.1.0.v=
201105071233.jar:/Users/mchettiar/.m2/repository/org/eclipse/jetty/jetty-se=
curity/8.1.14.v20131031/jetty-security-8.1.14.v20131031.jar:/Users/mchettia=
r/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20131031/jetty-util-8=
.1.14.v20131031.jar:/Users/mchettiar/.m2/repository/org/eclipse/jetty/jetty=
-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/Users/mchettiar=
/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/mchet=
tiar/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.=
3.2.jar:/Users/mchettiar/.m2/repository/org/apache/commons/commons-math3/3.=
1.1/commons-math3-3.1.1.jar:/Users/mchettiar/.m2/repository/com/google/code=
/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/mchettiar/.m2/repository/org=
/slf4j/slf4j-api/1.7.5/slf4j-api-1.7.5.jar:/Users/mchettiar/.m2/repository/=
org/slf4j/jul-to-slf4j/1.7.5/jul-to-slf4j-1.7.5.jar:/Users/mchettiar/.m2/re=
pository/org/slf4j/jcl-over-slf4j/1.7.5/jcl-over-slf4j-1.7.5.jar:/Users/mch=
ettiar/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/mchettiar/=
.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf4j-log4j12-1.7.5.jar:/Users=
/mchettiar/.m2/repository/com/ning/compress-lzf/1.0.0/compress-lzf-1.0.0.ja=
r:/Users/mchettiar/.m2/repository/org/xerial/snappy/snappy-java/
1.1.1.6/snappy-java-1.1.1.6.jar:/Users/mchettiar/.m2/repository/net/jpountz=
/lz4/lz4/1.2.0/lz4-1.2.0.jar:/Users/mchettiar/.m2/repository/org/roaringbit=
map/RoaringBitmap/0.4.5/RoaringBitmap-0.4.5.jar:/Users/mchettiar/.m2/reposi=
tory/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/mchettiar/.m2/r=
epository/org/spark-project/akka/akka-remote_2.10/2.3.4-spark/akka-remote_2=
.10-2.3.4-spark.jar:/Users/mchettiar/.m2/repository/org/spark-project/akka/=
akka-actor_2.10/2.3.4-spark/akka-actor_2.10-2.3.4-spark.jar:/Users/mchettia=
r/.m2/repository/com/typesafe/config/1.2.1/config-1.2.1.jar:/Users/mchettia=
r/.m2/repository/io/netty/netty/3.8.0.Final/netty-3.8.0.Final.jar:/Users/mc=
hettiar/.m2/repository/org/spark-project/protobuf/protobuf-java/2.5.0-spark=
/protobuf-java-2.5.0-spark.jar:/Users/mchettiar/.m2/repository/org/uncommon=
s/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/Users/mchettiar/=
.m2/repository/org/spark-project/akka/akka-slf4j_2.10/2.3.4-spark/akka-slf4=
j_2.10-2.3.4-spark.jar:/Users/mchettiar/.m2/repository/org/json4s/json4s-ja=
ckson_2.10/3.2.10/json4s-jackson_2.10-3.2.10.jar:/Users/mchettiar/.m2/repos=
itory/org/json4s/json4s-core_2.10/3.2.10/json4s-core_2.10-3.2.10.jar:/Users=
/mchettiar/.m2/repository/org/json4s/json4s-ast_2.10/3.2.10/json4s-ast_2.10=
-3.2.10.jar:/Users/mchettiar/.m2/repository/com/thoughtworks/paranamer/para=
namer/2.6/paranamer-2.6.jar:/Users/mchettiar/.m2/repository/org/scala-lang/=
scalap/2.10.4/scalap-2.10.4.jar:/Users/mchettiar/.m2/repository/org/scala-l=
ang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/mchettiar/.m2/re=
pository/com/fasterxml/jackson/core/jackson-databind/2.3.1/jackson-databind=
-2.3.1.jar:/Users/mchettiar/.m2/repository/com/fasterxml/jackson/core/jacks=
on-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/mchettiar/.m2/rep=
ository/com/fasterxml/jackson/core/jackson-core/2.3.1/jackson-core-2.3.1.ja=
r:/Users/mchettiar/.m2/repository/org/apache/mesos/mesos/0.18.1/mesos-0.18.=
1-shaded-protobuf.jar:/Users/mchettiar/.m2/repository/io/netty/netty-all/4.=
0.23.Final/netty-all-4.0.23.Final.jar:/Users/mchettiar/.m2/repository/com/c=
learspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/mchettiar/.m2/rep=
ository/com/codahale/metrics/metrics-core/3.0.0/metrics-core-3.0.0.jar:/Use=
rs/mchettiar/.m2/repository/com/codahale/metrics/metrics-jvm/3.0.0/metrics-=
jvm-3.0.0.jar:/Users/mchettiar/.m2/repository/com/codahale/metrics/metrics-=
json/3.0.0/metrics-json-3.0.0.jar:/Users/mchettiar/.m2/repository/com/codah=
ale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0.0.jar:/Users/mchett=
iar/.m2/repository/org/tachyonproject/tachyon-client/0.5.0/tachyon-client-0=
.5.0.jar:/Users/mchettiar/.m2/repository/org/tachyonproject/tachyon/0.5.0/t=
achyon-0.5.0.jar:/Users/mchettiar/.m2/repository/org/scala-lang/scala-refle=
ct/2.10.4/scala-reflect-2.10.4.jar:/Users/mchettiar/.m2/repository/org/spar=
k-project/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/mchettiar/.m2/repository=
/net/sf/py4j/py4j/0.8.2.1/py4j-0.8.2.1.jar:/Users/mchettiar/.m2/repository/=
org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/mchettiar/.m2/=
repository/org/codehaus/groovy/groovy-all/2.3.7/groovy-all-2.3.7.jar
-unchecked

  last tree to typer: Ident(Server)
              symbol: <none> (flags: )
   symbol definition: <none>
       symbol owners:
      context owners: variable server -> class HttpServer -> package spark

=3D=3D Enclosing template or block =3D=3D

Template( // val <local HttpServer>: <notype> in class HttpServer
  "org.apache.spark.Logging" // parents
  ValDef(
    private
    "_"
    <tpt>
    <empty>
  )
  // 13 statements
  ValDef( // private[this] val conf: <?> in class HttpServer
    private <local> <paramaccessor>
    "conf"
    "SparkConf"
    <empty>
  )
  ValDef( // private[this] val resourceBase: <?> in class HttpServer
    private <local> <paramaccessor>
    "resourceBase"
    "File"
    <empty>
  )
  ValDef( // private[this] val securityManager: <?> in class HttpServer
    private <local> <paramaccessor>
    "securityManager"
    "SecurityManager"
    <empty>
  )
  ValDef( // private[this] val requestedPort: <?> in class HttpServer
    private <local> <paramaccessor>
    "requestedPort"
    "Int"
    <empty>
  )
  ValDef( // private[this] val serverName: <?> in class HttpServer
    private <local> <paramaccessor>
    "serverName"
    "String"
    <empty>
  )
  DefDef( // def <init>(conf: org.apache.spark.SparkConf,resourceBase:
java.io.File,securityManager:
org.apache.spark.SecurityManager,requestedPort: Int,serverName: String):
org.apache.spark.HttpServer in class HttpServer
    <method> <triedcooking>
    "<init>"
    []
    // 1 parameter list
    ValDef( // conf: org.apache.spark.SparkConf
      <param> <paramaccessor>
      "conf"
      "SparkConf" // class SparkConf extends Cloneable with Logging in
package spark, tree.tpe=3Dorg.apache.spark.SparkConf
      <empty>
    )
    ValDef( // resourceBase: java.io.File
      <param> <paramaccessor>
      "resourceBase"
      "File"
      <empty>
    )
    ValDef( // securityManager: org.apache.spark.SecurityManager
      <param> <paramaccessor>
      "securityManager"
      "SecurityManager" // private[package spark] class SecurityManager
extends Logging with SecretKeyHolder in package spark,
tree.tpe=3Dorg.apache.spark.SecurityManager
      <empty>
    )
    ValDef( // requestedPort: Int
      <param> <defaultparam/trait> <paramaccessor>
      "requestedPort"
      "Int"
      0
    )
    ValDef( // serverName: String
      <param> <defaultparam/trait> <paramaccessor>
      "serverName"
      "String"
      "HTTP server"
    )
    <tpt> // tree.tpe=3Dorg.apache.spark.HttpServer
    Block(
      Apply(
        super."<init>"
        Nil
      )
      ()
    )
  )
  ValDef( // private[this] var server: <?> in class HttpServer
    private <mutable> <local>
    "server"
    "Server"
    null
  )
  ValDef( // private[this] var port: <?> in class HttpServer
    private <mutable> <local>
    "port"
    "Int"
    "requestedPort"
  )
  DefDef( // def start(): Unit in class HttpServer
    <method> <triedcooking>
    "start"
    []
    List(Nil)
    "scala"."Unit" // final abstract class Unit extends AnyVal in package
scala, tree.tpe=3DUnit
    If(
      Apply(
        "server"."$bang$eq"
        null
      )
      Throw(
        Apply(
          new ServerStateException."<init>"
          "Server is already started"
        )
      )
      Block(
        // 5 statements
        Apply(
          "logInfo"
          "Starting HTTP Server"
        )
        ValDef(
          private <local> <synthetic>
          "x$1"
          <tpt>
          Match(
            Annotated(
              Apply(
                new scala.unchecked."<init>"
                Nil
              )
              Apply(
                TypeApply(
                  "Utils"."startServiceOnPort"
                  "Server"
                )
                // 4 arguments
                "requestedPort"
                "doStart"
                "conf"
                "serverName"
              )
            )
            CaseDef(
              Apply(
                "scala"."Tuple2"
                // 2 arguments
                Bind(
                  "actualServer"
                  "_"
                )
                Bind(
                  "actualPort"
                  "_"
                )
              )
              Apply(
                "scala"."Tuple2"
                // 2 arguments
                "actualServer"
                "actualPort"
              )
            )
          )
        )
        ValDef(
          0
          "actualServer"
          <tpt>
          "x$1"."_1"
        )
        ValDef(
          0
          "actualPort"
          <tpt>
          "x$1"."_2"
        )
        Assign(
          "server"
          "actualServer"
        )
        Assign(
          "port"
          "actualPort"
        )
      )
    )
  )
  DefDef( // private def doStart: <?> in class HttpServer
    <method> private
    "doStart"
    []
    // 1 parameter list
    ValDef(
      <param>
      "startPort"
      "Int"
      <empty>
    )
    AppliedTypeTree(
      "scala"."Tuple2"
      // 2 arguments
      "Server"
      "Int"
    )
    Block(
      // 16 statements
      ValDef(
        0
        "server"
        <tpt>
        Apply(
          new Server."<init>"
          Nil
        )
      )
      ValDef(
        0
        "connector"
        <tpt>
        Apply(
          new SocketConnector."<init>"
          Nil
        )
      )
      Apply(
        "connector"."setMaxIdleTime"
        Apply(
          60."$times"
          1000
        )
      )
      Apply(
        "connector"."setSoLingerTime"
        -1
      )
      Apply(
        "connector"."setPort"
        "startPort"
      )
      Apply(
        "server"."addConnector"
        "connector"
      )
      ValDef(
        0
        "threadPool"
        <tpt>
        Apply(
          new QueuedThreadPool."<init>"
          Nil
        )
      )
      Apply(
        "threadPool"."setDaemon"
        true
      )
      Apply(
        "server"."setThreadPool"
        "threadPool"
      )
      ValDef(
        0
        "resHandler"
        <tpt>
        Apply(
          new ResourceHandler."<init>"
          Nil
        )
      )
      Apply(
        "resHandler"."setResourceBase"
        "resourceBase"."getAbsolutePath"
      )
      ValDef(
        0
        "handlerList"
        <tpt>
        Apply(
          new HandlerList."<init>"
          Nil
        )
      )
      Apply(
        "handlerList"."setHandlers"
        Apply(
          "Array"
          // 2 arguments
          "resHandler"
          Apply(
            new DefaultHandler."<init>"
            Nil
          )
        )
      )
      If(
        Apply(
          "securityManager"."isAuthenticationEnabled"
          Nil
        )
        Block(
          // 3 statements
          Apply(
            "logDebug"
            "HttpServer is using security"
          )
          ValDef(
            0
            "sh"
            <tpt>
            Apply(
              "setupSecurityHandler"
              "securityManager"
            )
          )
          Apply(
            "sh"."setHandler"
            "handlerList"
          )
          Apply(
            "server"."setHandler"
            "sh"
          )
        )
        Block(
          Apply(
            "logDebug"
            "HttpServer is not using security"
          )
          Apply(
            "server"."setHandler"
            "handlerList"
          )
        )
      )
      Apply(
        "server"."start"
        Nil
      )
      ValDef(
        0
        "actualPort"
        <tpt>
        server.getConnectors()(0)."getLocalPort"
      )
      Apply(
        "scala"."Tuple2"
        // 2 arguments
        "server"
        "actualPort"
      )
    )
  )
  DefDef( // private def setupSecurityHandler: <?> in class HttpServer
    <method> private
    "setupSecurityHandler"
    []
    // 1 parameter list
    ValDef(
      <param>
      "securityMgr"
      "SecurityManager"
      <empty>
    )
    "ConstraintSecurityHandler"
    Block(
      // 16 statements
      ValDef(
        0
        "constraint"
        <tpt>
        Apply(
          new Constraint."<init>"
          Nil
        )
      )
      Apply(
        "constraint"."setName"
        "Constraint"."__DIGEST_AUTH"
      )
      Apply(
        "constraint"."setRoles"
        Apply(
          "Array"
          "user"
        )
      )
      Apply(
        "constraint"."setAuthenticate"
        true
      )
      Apply(
        "constraint"."setDataConstraint"
        "Constraint"."DC_NONE"
      )
      ValDef(
        0
        "cm"
        <tpt>
        Apply(
          new ConstraintMapping."<init>"
          Nil
        )
      )
      Apply(
        "cm"."setConstraint"
        "constraint"
      )
      Apply(
        "cm"."setPathSpec"
        "/*"
      )
      ValDef(
        0
        "sh"
        <tpt>
        Apply(
          new ConstraintSecurityHandler."<init>"
          Nil
        )
      )
      ValDef(
        0
        "hashLogin"
        <tpt>
        Apply(
          new HashLoginService."<init>"
          Nil
        )
      )
      ValDef(
        0
        "userCred"
        <tpt>
        Apply(
          new Password."<init>"
          Apply(
            "securityMgr"."getSecretKey"
            Nil
          )
        )
      )
      If(
        Apply(
          "userCred"."$eq$eq"
          null
        )
        Throw(
          Apply(
            new Exception."<init>"
            "Error: secret key is null with authentication on"
          )
        )
        ()
      )
      Apply(
        "hashLogin"."putUser"
        // 3 arguments
        Apply(
          "securityMgr"."getHttpUser"
          Nil
        )
        "userCred"
        Apply(
          "Array"
          "user"
        )
      )
      Apply(
        "sh"."setLoginService"
        "hashLogin"
      )
      Apply(
        "sh"."setAuthenticator"
        Apply(
          new DigestAuthenticator."<init>"
          Nil
        )
      )
      Apply(
        "sh"."setConstraintMappings"
        Apply(
          "Array"
          "cm"
        )
      )
      "sh"
    )
  )
  DefDef( // def stop(): Unit in class HttpServer
    <method> <triedcooking>
    "stop"
    []
    List(Nil)
    "scala"."Unit" // final abstract class Unit extends AnyVal in package
scala, tree.tpe=3DUnit
    If(
      Apply(
        "server"."$eq$eq"
        null
      )
      Throw(
        Apply(
          new ServerStateException."<init>"
          "Server is already stopped"
        )
      )
      Block(
        // 2 statements
        Apply(
          "server"."stop"
          Nil
        )
        Assign(
          "port"
          -1
        )
        Assign(
          "server"
          null
        )
      )
    )
  )
  DefDef( // def uri: String in class HttpServer
    <method> <triedcooking>
    "uri"
    []
    Nil
    "String"
    If(
      Apply(
        "server"."$eq$eq"
        null
      )
      Throw(
        Apply(
          new ServerStateException."<init>"
          "Server is not started"
        )
      )
      Apply(
        "http://".$plus(Utils.localIpAddress).$plus(":")."$plus"
        "port"
      )
    )
  )
)

uncaught exception during compilation: java.lang.AssertionError
[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [
 4.306 s]
[INFO] Spark Project Networking ........................... SUCCESS [
 6.978 s]
[INFO] Spark Project Shuffle Streaming Service ............ SUCCESS [
 5.273 s]
[INFO] Spark Project Core ................................. SKIPPED
[INFO] Spark Project Bagel ................................ SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project YARN Parent POM ...................... SKIPPED
[INFO] Spark Project YARN Stable API ...................... SKIPPED
[INFO] Spark Project Hive Thrift Server ................... SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Spark Project External Twitter ..................... SKIPPED
[INFO] Spark Project External Flume Sink .................. SKIPPED
[INFO] Spark Project External Flume ....................... SKIPPED
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Project YARN Shuffle Service ................. SKIPPED
[INFO]
------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 33.464 s
[INFO] Finished at: 2015-03-27T12:01:56-07:00
[INFO] Final Memory: 58M/541M
[INFO]
------------------------------------------------------------------------
---------------------------------------------------
constituent[0]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/aether-api-1.0.0.v20140518.j=
ar
constituent[1]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/aether-connector-basic-1.0.0=
.v20140518.jar
constituent[2]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/aether-impl-1.0.0.v20140518.=
jar
constituent[3]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/aether-spi-1.0.0.v20140518.j=
ar
constituent[4]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/aether-transport-wagon-1.0.0=
.v20140518.jar
constituent[5]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/aether-util-1.0.0.v20140518.=
jar
constituent[6]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/aopalliance-1.0.jar
constituent[7]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/cdi-api-1.0.jar
constituent[8]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/commons-cli-1.2.jar
constituent[9]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/commons-io-2.2.jar
constituent[10]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/commons-lang-2.6.jar
constituent[11]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/guava-18.0.jar
constituent[12]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/javax.inject-1.jar
constituent[13]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/jsoup-1.7.2.jar
constituent[14]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/jsr250-api-1.0.jar
constituent[15]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-aether-provider-3.2.5.=
jar
constituent[16]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-artifact-3.2.5.jar
constituent[17]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-compat-3.2.5.jar
constituent[18]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-core-3.2.5.jar
constituent[19]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-embedder-3.2.5.jar
constituent[20]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-model-3.2.5.jar
constituent[21]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-model-builder-3.2.5.ja=
r
constituent[22]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-plugin-api-3.2.5.jar
constituent[23]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-repository-metadata-3.=
2.5.jar
constituent[24]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-settings-3.2.5.jar
constituent[25]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/maven-settings-builder-3.2.5=
.jar
constituent[26]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/org.eclipse.sisu.inject-0.3.=
0.M1.jar
constituent[27]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/org.eclipse.sisu.plexus-0.3.=
0.M1.jar
constituent[28]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/plexus-cipher-1.7.jar
constituent[29]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/plexus-component-annotations=
-1.5.5.jar
constituent[30]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/plexus-interpolation-1.21.ja=
r
constituent[31]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/plexus-sec-dispatcher-1.3.ja=
r
constituent[32]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/plexus-utils-3.0.20.jar
constituent[33]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/sisu-guice-3.2.3-no_aop.jar
constituent[34]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/slf4j-api-1.7.5.jar
constituent[35]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/slf4j-simple-1.7.5.jar
constituent[36]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/wagon-file-2.8.jar
constituent[37]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/wagon-http-2.8-shaded.jar
constituent[38]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/wagon-http-shared-2.8.jar
constituent[39]:
file:/usr/local/Cellar/maven/3.2.5/libexec/lib/wagon-provider-api-2.8.jar
constituent[40]: file:/usr/local/Cellar/maven/3.2.5/libexec/conf/logging/
---------------------------------------------------
Exception in thread "main" java.lang.AssertionError: assertion failed:
javax.servlet.ServletException
at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)
at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseExceptions$1(Classfil=
eParser.scala:1051)
at
scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$cla=
ssfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseAttributes(ClassfileP=
arser.scala:1080)
at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseMethod(ClassfileParse=
r.scala:666)
at
scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$cla=
ssfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
at
scala.tools.nsc.symtab.classfile.ClassfileParser$$anonfun$parseClass$1.appl=
y$mcV$sp(ClassfileParser.scala:567)
at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser=
.scala:572)
at
scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scal=
a:88)
at
scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoade=
rs.scala:261)
at
scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.sc=
ala:194)
at
scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:=
210)
at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:893)
at scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:5064)
at
scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.scal=
a:5218)
at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5561)
at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyz=
er.scala:19)
at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Type=
rs.scala:44)
at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala=
:19)
at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5769)
at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5772)
at scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1317)
at scala.tools.nsc.typechecker.Namers$Namer.getSig$1(Namers.scala:1457)
at scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1466)
at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anon=
fun$apply$1.apply$mcV$sp(Namers.scala:731)
at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anon=
fun$apply$1.apply(Namers.scala:730)
at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anon=
fun$apply$1.apply(Namers.scala:730)
at
scala.tools.nsc.typechecker.Namers$Namer.scala$tools$nsc$typechecker$Namers=
$Namer$$logAndValidate(Namers.scala:1499)
at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.apply=
(Namers.scala:730)
at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.apply=
(Namers.scala:729)
at
org.scalamacros.paradise.typechecker.Namers$$anon$3.completeImpl(Namers.sca=
la:743)
at
scala.tools.nsc.typechecker.Namers$LockingTypeCompleter$class.complete(Name=
rs.scala:1622)
at
org.scalamacros.paradise.typechecker.Namers$$anon$3.complete(Namers.scala:7=
41)
at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
at
scala.tools.nsc.typechecker.MethodSynthesis$MethodSynth$class.addDerivedTre=
es(MethodSynthesis.scala:225)
at scala.tools.nsc.typechecker.Namers$Namer.addDerivedTrees(Namers.scala:55=
)
at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:191=
7)
at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:191=
7)
at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$1.=
apply(Typers.scala:1856)
at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$1.=
apply(Typers.scala:1853)
at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.s=
cala:251)
at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.s=
cala:251)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251=
)
at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
at scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1917=
)
at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typedTemplate=
(Analyzer.scala:19)
at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typedTempla=
te(Typers.scala:51)
at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typedTemplate(Analyze=
r.scala:19)
at scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:1759=
)
at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typedClassDef=
(Analyzer.scala:19)
at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typedClassD=
ef(Typers.scala:62)
at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typedClassDef(Analyze=
r.scala:19)
at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5583)
at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyz=
er.scala:19)
at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Type=
rs.scala:44)
at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala=
:19)
at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers=
$Typer$$typedStat$1(Typers.scala:2928)
at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:303=
2)
at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:303=
2)
at scala.collection.immutable.List.loop$1(List.scala:170)
at scala.collection.immutable.List.mapConserve(List.scala:186)
at scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032)
at
scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:530=
1)
at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5587)
at org.scalamacros.paradise.typechecker.Analyzer$$anon$1.org
$scalamacros$paradise$typechecker$Typers$ParadiseTyper$$super$typed1(Analyz=
er.scala:19)
at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typedPackag=
eDef$1(Typers.scala:35)
at
org.scalamacros.paradise.typechecker.Typers$ParadiseTyper$class.typed1(Type=
rs.scala:43)
at
org.scalamacros.paradise.typechecker.Analyzer$$anon$1.typed1(Analyzer.scala=
:19)
at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5704)
at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.sc=
ala:99)
at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.ap=
ply(Analyzer.scala:91)
at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.ap=
ply(Analyzer.scala:91)
at scala.collection.Iterator$class.foreach(Iterator.scala:727)
at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.scal=
a:91)
at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:5=
7)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImp=
l.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$m=
cV$sp(AggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(A=
ggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(A=
ggressiveCompile.scala:99)
at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(Aggres=
siveCompile.scala:166)
at
sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.=
scala:98)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143=
)
at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
at sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
at sbt.inc.Incremental$.compile(Incremental.scala:37)
at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
at com.typesafe.zinc.Compiler.compile(Compiler.scala:164)
at sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:92)
at
scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.ja=
va:303)
at scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:119)
at scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:99)
at scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:482)
at
org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildP=
luginManager.java:132)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:=
208)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:=
153)
at
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:=
145)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(Lif=
ecycleModuleBuilder.java:116)
at
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(Lif=
ecycleModuleBuilder.java:80)
at
org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBu=
ilder.build(SingleThreadedBuilder.java:51)
at
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStart=
er.java:120)
at org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:355)
at org.apache.maven.DefaultMaven.execute(DefaultMaven.java:155)
at org.apache.maven.cli.MavenCli.execute(MavenCli.java:584)
at org.apache.maven.cli.MavenCli.doMain(MavenCli.java:216)
at org.apache.maven.cli.MavenCli.main(MavenCli.java:160)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:5=
7)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImp=
l.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.j=
ava:289)
at
org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:229)
at
org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher=
.java:415)
at org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:356=
)

On Fri, Mar 27, 2015 at 5:23 AM, Sean Owen <sowen@cloudera.com> wrote:

> I built from the head of branch-1.2 and spark-core compiled correctly
> with your exact command. You have something wrong with how you are
> building. For example, you're not trying to run this from the core
> subdirectory are you?
>
> On Thu, Mar 26, 2015 at 10:36 PM, Pala M Muthaia
> <mchettiar@rocketfuelinc.com> wrote:
> > Hi,
> >
> > We are trying to build spark 1.2 from source (tip of the branch-1.2 at
> the
> > moment). I tried to build spark using the following command:
> >
> > mvn -U -Pyarn -Phadoop-2.4 -Dhadoop.version=3D2.4.0 -Phive
> -Phive-thriftserver
> > -DskipTests clean package
> >
> > I encountered various missing class definition exceptions (e.g: class
> > javax.servlet.ServletException not found).
> >
> > I eventually got the build to succeed after adding the following set of
> > dependencies to the spark-core's pom.xml:
> >
> >     <dependency>
> >       <groupId>javax.servlet</groupId>
> >       <artifactId>servlet-api</artifactId>
> >       <version>3.0</version>
> >     </dependency>
> >
> >     <dependency>
> >       <groupId>org.eclipse.jetty</groupId>
> >       <artifactId>jetty-io</artifactId>
> >     </dependency>
> >
> >     <dependency>
> >       <groupId>org.eclipse.jetty</groupId>
> >       <artifactId>jetty-http</artifactId>
> >     </dependency>
> >
> >     <dependency>
> >       <groupId>org.eclipse.jetty</groupId>
> >       <artifactId>jetty-servlet</artifactId>
> >     </dependency>
> >
> > Pretty much all of the missing class definition errors came up while
> > building HttpServer.scala, and went away after the above dependencies
> were
> > included.
> >
> > My guess is official build for spark 1.2 is working already. My questio=
n
> is
> > what is wrong with my environment or setup, that requires me to add
> > dependencies to pom.xml in this manner, to get this build to succeed.
> >
> > Also, i am not sure if this build would work at runtime for us, i am
> still
> > testing this out.
> >
> >
> > Thanks,
> > pala
>

--089e0115f2308d004c051249dd03--

From dev-return-12238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 19:52:25 2015
Return-Path: <dev-return-12238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06337172D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 19:52:25 +0000 (UTC)
Received: (qmail 83569 invoked by uid 500); 27 Mar 2015 19:52:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83492 invoked by uid 500); 27 Mar 2015 19:52:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83476 invoked by uid 99); 27 Mar 2015 19:52:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 19:52:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 19:51:58 +0000
Received: by iedfl3 with SMTP id fl3so88527002ied.1
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 12:49:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=ia6WY2FHOB24Rgz/nN4s9K02mnZRk0TkPae2OGZbQz8=;
        b=VNbSD67iZJQlN+GwTpzhbBfgOfN/7uEufqqoILLINY9T0ifsqsmM03BDWVoH5M8fb5
         ogJZWPz+FBnga/6kZuIku9OmkH84YktMozq2CM382jkQDoJQud4QPYxcdPJ3E4Pr5EPY
         ijWItQecUmovGRvV/XFSFFVzEfi9AwXa3kSCiQ8SDJ1EUDM0ZtazrXQPT/EVsxg/kjah
         Yu1cd2HuGli3eOeM5eeLHNGZshci0qDHU6EVrIBKzxPl8ULGFD16iQopwCQuA8dtvAZc
         c7r4ISEDTTdrXse/3H+G7+oZy9wVi2gPzq1S+YkLz1MeKeRwnKzMM8tjXt5Je/WSjGaA
         g16g==
X-Gm-Message-State: ALoCoQnS1wAwQleRLkS8KBittChq8QdLebH+eb2WGFgquPN9wQ0kVZoI9GJeGPxR18L4QQevuiei
MIME-Version: 1.0
X-Received: by 10.50.119.229 with SMTP id kx5mr759322igb.42.1427485791777;
 Fri, 27 Mar 2015 12:49:51 -0700 (PDT)
Received: by 10.36.58.2 with HTTP; Fri, 27 Mar 2015 12:49:51 -0700 (PDT)
In-Reply-To: <CA+B-+fywj3w_174h4nYGqVsYBvxfqCpWd1Avsa6Os4t9cy01JA@mail.gmail.com>
References: <CA+B-+fwZ50HOuAKJjkcy87Rym++kdh4A_Gj32N6cZZy-7WwZZA@mail.gmail.com>
	<CAF7ADNrcC-w+ZH9S_0R-y_iEi8GKPV7ybHsmP4SF2JD=HynnSg@mail.gmail.com>
	<CAEYYnxZjN8vCQEuUzADYH8TTa3haBSP3P=JAJGy5Z39x0hdMaA@mail.gmail.com>
	<CA+B-+fywj3w_174h4nYGqVsYBvxfqCpWd1Avsa6Os4t9cy01JA@mail.gmail.com>
Date: Fri, 27 Mar 2015 12:49:51 -0700
Message-ID: <CAF7ADNq1cQFtbqRWj4iwo71Wbj1m__aOEXK5S7UVN8Td=jKZhw@mail.gmail.com>
Subject: Re: LogisticGradient Design
From: Joseph Bradley <joseph@databricks.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: DB Tsai <dbtsai@dbtsai.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113494208a8ab605124a6ff6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113494208a8ab605124a6ff6
Content-Type: text/plain; charset=UTF-8

Makes sense!

On Wed, Mar 25, 2015 at 2:46 PM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Cool...Thanks...It will be great if they move in two code paths just for
> the sake of code clean-up
>
> On Wed, Mar 25, 2015 at 2:37 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
>
>> I did the benchmark when I used the if-else statement to switch the
>> binary & multinomial logistic loss and gradient, and there is no
>> performance hit at all. However, I'm refactoring the LogisticGradient
>> code so the addBias and scaling can be done in LogisticGradient
>> instead of the input dataset to avoid the second cache. In this case,
>> the code will be more complicated, so I will split the code into two
>> paths. Will be done in another PR.
>>
>> Sincerely,
>>
>> DB Tsai
>> -------------------------------------------------------
>> Blog: https://www.dbtsai.com
>>
>>
>> On Wed, Mar 25, 2015 at 11:57 AM, Joseph Bradley <joseph@databricks.com>
>> wrote:
>> > It would be nice to see how big a performance hit we take from combining
>> > binary & multiclass logistic loss/gradient.  If it's not a big hit,
>> then it
>> > might be simpler from an outside API perspective to keep them in 1 class
>> > (even if it's more complicated within).
>> > Joseph
>> >
>> > On Wed, Mar 25, 2015 at 8:15 AM, Debasish Das <debasish.das83@gmail.com
>> >
>> > wrote:
>> >
>> >> Hi,
>> >>
>> >> Right now LogisticGradient implements both binary and multi-class in
>> the
>> >> same class using an if-else statement which is a bit convoluted.
>> >>
>> >> For Generalized matrix factorization, if the data has distinct ratings
>> I
>> >> want to use LeastSquareGradient (regression has given best results to
>> date)
>> >> but if the data has binary labels 0/1 based on domain knowledge
>> (implicit
>> >> for example, visits no-visits) I want to use a LogisticGradient
>> without any
>> >> overhead for multi-class if-else...
>> >>
>> >> I can compare the performance of LeastSquareGradient and multi-class
>> >> LogisticGradient on the recommendation metrics but it will be great if
>> we
>> >> can separate binary and multi-class in Separate
>> >> classes....MultiClassLogistic can extend BinaryLogistic but mixing
>> them in
>> >> the same class is an overhead for users (like me) who wants to use
>> >> BinaryLogistic for his application..
>> >>
>> >> Thanks.
>> >> Deb
>> >>
>>
>
>

--001a113494208a8ab605124a6ff6--

From dev-return-12239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 19:58:18 2015
Return-Path: <dev-return-12239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0618F172EF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 19:58:18 +0000 (UTC)
Received: (qmail 94375 invoked by uid 500); 27 Mar 2015 19:58:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94291 invoked by uid 500); 27 Mar 2015 19:58:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94280 invoked by uid 99); 27 Mar 2015 19:58:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 19:58:16 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [96.236.214.3] (HELO dugos.com) (96.236.214.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Fri, 27 Mar 2015 19:58:11 +0000
Received: (qmail 6479 invoked from network); 27 Mar 2015 19:57:50 -0000
Received: from dhcp075.dugos.com (10.7.42.75)
  by dugos.com with SMTP; 27 Mar 2015 19:57:50 -0000
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 8.2 \(2070.6\))
Subject: Re: Support for Hive 0.14 in secure mode on hadoop 2.6.0
From: Doug Balog <doug.sparkdev@dugos.com>
In-Reply-To: <55152D4E.1090608@gmail.com>
Date: Fri, 27 Mar 2015 15:57:49 -0400
Cc: dev@spark.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <EAD2B959-34D3-49C5-AA76-7C9DB62C7C6B@dugos.com>
References: <B023E7E0-CBB2-4D1C-BD56-57EE6F807CEC@dugos.com> <55152D4E.1090608@gmail.com>
To: Cheng Lian <lian.cs.zju@gmail.com>
X-Mailer: Apple Mail (2.2070.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Is there a JIRA for this adaption layer ? It sounds like a better long =
term solution.

If anybody knows what is require to get the current Shim layer working =
with Hive 0.14, please post what you know.
I=92m willing to spend some time on it, but I=92m still learning how =
things fit together and it might take me a while.
I=92ve been looking at the pr associated with SPARK-5111 for hints.

Thanks,

Doug


> On Mar 27, 2015, at 6:13 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:
>=20
> We're planning to replace the current Hive version profiles and shim =
layer with an adaption layer in Spark SQL in 1.4. This adaption layer =
allows Spark SQL to connect to arbitrary Hive version greater than or =
equal to 0.12.0 (or maybe 0.13.1, not decided yet).
>=20
> However, it's not a promise yet, since this requires major refactoring =
of the current Spark SQL Hive support.
>=20
> Cheng
>=20
> On 3/27/15 4:48 PM, Doug Balog wrote:
>> Hi,
>>   I'm just wondering if anybody is working on supporting Hive 0.14 in =
secure mode on hadoop 2.6.0 ?
>> I see once Jira referring to it  =
https://issues.apache.org/jira/browse/SPARK-5111
>> but it mentions no effort to move to 0.14.
>>=20
>> Thanks,
>>=20
>> Doug
>>=20
>>=20
>>=20
>> ---------------------------------------------------------------------
>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>> For additional commands, e-mail: dev-help@spark.apache.org
>>=20
>>=20
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 23:19:25 2015
Return-Path: <dev-return-12240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1D6917AE1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 23:19:25 +0000 (UTC)
Received: (qmail 2780 invoked by uid 500); 27 Mar 2015 23:19:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2698 invoked by uid 500); 27 Mar 2015 23:19:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2686 invoked by uid 99); 27 Mar 2015 23:19:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 23:19:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.213.176 as permitted sender)
Received: from [209.85.213.176] (HELO mail-ig0-f176.google.com) (209.85.213.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 23:18:58 +0000
Received: by ignm3 with SMTP id m3so40277099ign.0
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 16:16:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=80vKxec69FFnxlCNZxuPdQLqQmdX9343pInqOcr/eXQ=;
        b=AbLZA9G5UgJi2Uq7fQF5FVearM0RdLwSAbAfK8qFEA23lKoCat2kmuKT2X6xTLlzWh
         /gjv0KtalBl3xkpFZUvoDWQBnRh7xeXJVdds3yXBnLY2S8aLzhyy12qcQ+UmMURRf82k
         wNuC66FU+hVIaFuxdPCsF45wJZI0vaNaICFkol0UM0K9IdJbaRsGtnMww+DQFVpaMKCK
         48x94dWGKUsTb+xer7Jf1nZaJALafE3f7MboXSjHuGax7yhuJDCjDlvC4YqIdkCPnCgE
         mBji4fs9bAGA29MhbsfvO0Lrshf6xDxuGyxxO2qZBmCx6WtRpr3ddckDAtozKKmsXzBI
         slNw==
MIME-Version: 1.0
X-Received: by 10.50.131.196 with SMTP id oo4mr1897748igb.2.1427498201983;
 Fri, 27 Mar 2015 16:16:41 -0700 (PDT)
Received: by 10.107.155.143 with HTTP; Fri, 27 Mar 2015 16:16:41 -0700 (PDT)
In-Reply-To: <CA+2Pv=i2GorzekE1yW7pm7DOJ1Z-EJRAZhoHCsVsivbMpEc7Zw@mail.gmail.com>
References: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
	<CA+2Pv=gO4ZkaBCDMmiCfEWsv59DFrpgOiN-A+YFWF4aq0x2jaQ@mail.gmail.com>
	<CACkSZy00=i5U3=08nbRsQ3kxdyVUMXfNrz1P=1Fhtjtk7p761A@mail.gmail.com>
	<CA+2Pv=i2GorzekE1yW7pm7DOJ1Z-EJRAZhoHCsVsivbMpEc7Zw@mail.gmail.com>
Date: Fri, 27 Mar 2015 16:16:41 -0700
Message-ID: <CACkSZy06KiL6iG0U0hS-oSBc3-zsF6MZib_34Pb5ryX8nVB=pg@mail.gmail.com>
Subject: Re: Iterative pyspark / scala codebase development
From: Stephen Boesch <javadba@gmail.com>
To: Davies Liu <davies@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b2e14653f333005124d5332
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e14653f333005124d5332
Content-Type: text/plain; charset=UTF-8

Thx much!  This works.

My workflow is making changes to files in Intelij and running ipython to
execute pyspark.

Is there any way for ipython to "see the updated class files without first
exiting?

2015-03-27 10:21 GMT-07:00 Davies Liu <davies@databricks.com>:

> put these lines in your ~/.bash_profile
>
> export SPARK_PREPEND_CLASSES=true
> export SPARK_HOME=path_to_spark
> export
> PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip:${SPARK_HOME}/python:${PYTHONPATH}"
>
> $ source ~/.bash_profile
> $ build/sbt assembly
> $ build/sbt ~compile  # do not stop this
>
> Then in another terminal you could run python tests as
> $ cd python/pyspark/
> $  python rdd.py
>
>
> cc to dev list
>
>
> On Fri, Mar 27, 2015 at 10:15 AM, Stephen Boesch <javadba@gmail.com>
> wrote:
> > Which aspect of that page are you suggesting provides a more optimized
> > alternative?
> >
> > 2015-03-27 10:13 GMT-07:00 Davies Liu <davies@databricks.com>:
> >
> >> see
> >>
> https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools
> >>
> >> On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch <javadba@gmail.com>
> >> wrote:
> >> > I am iteratively making changes to the scala side of some new pyspark
> >> > code
> >> > and re-testing from the python/pyspark side.
> >> >
> >> > Presently my only solution is to rebuild completely
> >> >
> >> >       sbt assembly
> >> >
> >> > after any scala side change - no matter how small.
> >> >
> >> > Any better / expedited way for pyspark to see small scala side
> updates?
> >
> >
>

--047d7b2e14653f333005124d5332--

From dev-return-12241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Mar 27 23:21:43 2015
Return-Path: <dev-return-12241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 918D017AEB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 27 Mar 2015 23:21:43 +0000 (UTC)
Received: (qmail 6554 invoked by uid 500); 27 Mar 2015 23:21:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6472 invoked by uid 500); 27 Mar 2015 23:21:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6461 invoked by uid 99); 27 Mar 2015 23:21:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 23:21:36 +0000
X-ASF-Spam-Status: No, hits=1.0 required=10.0
	tests=FSL_HELO_BARE_IP_2,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zzhang@hortonworks.com designates 64.78.52.187 as permitted sender)
Received: from [64.78.52.187] (HELO relayvx12c.securemail.intermedia.net) (64.78.52.187)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 27 Mar 2015 23:21:31 +0000
Received: from securemail.intermedia.net (localhost [127.0.0.1])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id E9F3653E43;
	Fri, 27 Mar 2015 16:20:49 -0700 (PDT)
Subject: Re: Support for Hive 0.14 in secure mode on hadoop 2.6.0
MIME-Version: 1.0
x-echoworx-emg-received: Fri, 27 Mar 2015 16:20:48.985 -0700
x-echoworx-msg-id: e9fc65e8-b383-4156-ab98-8d2d25c1acba
x-echoworx-action: delivered
Received: from 10.254.155.17 ([10.254.155.17])
          by emg-ca-1-2 (JAMES SMTP Server 2.3.2) with SMTP ID 753;
          Fri, 27 Mar 2015 16:20:48 -0700 (PDT)
Received: from MBX080-W4-CO-2.exch080.serverpod.net (unknown [10.224.117.102])
	by emg-ca-1-2.localdomain (Postfix) with ESMTP id B64D853E5A;
	Fri, 27 Mar 2015 16:20:48 -0700 (PDT)
Received: from MBX080-W4-CO-1.exch080.serverpod.net (10.224.117.101) by
 MBX080-W4-CO-2.exch080.serverpod.net (10.224.117.102) with Microsoft SMTP
 Server (TLS) id 15.0.1044.25; Fri, 27 Mar 2015 16:20:47 -0700
Received: from MBX080-W4-CO-1.exch080.serverpod.net ([10.224.117.101]) by
 mbx080-w4-co-1.exch080.serverpod.net ([10.224.117.101]) with mapi id
 15.00.1044.021; Fri, 27 Mar 2015 16:20:47 -0700
From: Zhan Zhang <zzhang@hortonworks.com>
To: Doug Balog <doug.sparkdev@dugos.com>
CC: Cheng Lian <lian.cs.zju@gmail.com>, "dev@spark.apache.org"
	<dev@spark.apache.org>
Thread-Topic: Support for Hive 0.14 in secure mode on hadoop 2.6.0
Thread-Index: AQHQaOSn4C2O0iPIlE6Y6qGjTCPQjQ==
Date: Fri, 27 Mar 2015 23:20:47 +0000
Message-ID: <A425C764-915D-4D9D-AEC5-E79643F656F0@hortonworks.com>
References: <B023E7E0-CBB2-4D1C-BD56-57EE6F807CEC@dugos.com>
 <55152D4E.1090608@gmail.com> <EAD2B959-34D3-49C5-AA76-7C9DB62C7C6B@dugos.com>
In-Reply-To: <EAD2B959-34D3-49C5-AA76-7C9DB62C7C6B@dugos.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-ms-exchange-transport-fromentityheader: Hosted
x-originating-ip: [67.161.7.189]
x-source-routing-agent: Processed
Content-Type: text/plain; charset="Windows-1252"
Content-ID: <0502425084C50E4DAFDC34F8FD42DD04@exch080.serverpod.net>
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Doug,

Spark-5111 is to make spark work with security hadoop cluster in 2.6. There=
 is some compatibility issue which need the fix Spark-5111 patch.
In insecure cluster, current spark can connect to hive-0.14 without problem=
s.

By the way, I am really glad to hear that "an adaption layer in Spark SQL i=
n 1.4=94 and "allows Spark SQL to connect to arbitrary Hive version=94

Thanks.

Zhan Zhang

On Mar 27, 2015, at 12:57 PM, Doug Balog <doug.sparkdev@dugos.com> wrote:

> Is there a JIRA for this adaption layer ? It sounds like a better long te=
rm solution.
>=20
> If anybody knows what is require to get the current Shim layer working wi=
th Hive 0.14, please post what you know.
> I=92m willing to spend some time on it, but I=92m still learning how thin=
gs fit together and it might take me a while.
> I=92ve been looking at the pr associated with SPARK-5111 for hints.
>=20
> Thanks,
>=20
> Doug
>=20
>=20
>> On Mar 27, 2015, at 6:13 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:
>>=20
>> We're planning to replace the current Hive version profiles and shim lay=
er with an adaption layer in Spark SQL in 1.4. This adaption layer allows S=
park SQL to connect to arbitrary Hive version greater than or equal to 0.12=
.0 (or maybe 0.13.1, not decided yet).
>>=20
>> However, it's not a promise yet, since this requires major refactoring o=
f the current Spark SQL Hive support.
>>=20
>> Cheng
>>=20
>> On 3/27/15 4:48 PM, Doug Balog wrote:
>>> Hi,
>>>  I'm just wondering if anybody is working on supporting Hive 0.14 in se=
cure mode on hadoop 2.6.0 ?
>>> I see once Jira referring to it  https://issues.apache.org/jira/browse/=
SPARK-5111
>>> but it mentions no effort to move to 0.14.
>>>=20
>>> Thanks,
>>>=20
>>> Doug
>>>=20
>>>=20
>>>=20
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
>>> For additional commands, e-mail: dev-help@spark.apache.org
>>>=20
>>>=20
>>=20
>=20
>=20
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>=20


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 01:01:01 2015
Return-Path: <dev-return-12242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 71D8017EBC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 01:01:01 +0000 (UTC)
Received: (qmail 16860 invoked by uid 500); 28 Mar 2015 01:00:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16770 invoked by uid 500); 28 Mar 2015 01:00:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 12713 invoked by uid 99); 28 Mar 2015 00:55:57 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jimfcarroll@gmail.com does not designate 162.253.133.43 as permitted sender)
Date: Fri, 27 Mar 2015 17:54:58 -0700 (MST)
From: jimfcarroll <jimfcarroll@gmail.com>
To: dev@spark.apache.org
Message-ID: <1427504098515-11298.post@n3.nabble.com>
Subject: RDD.count
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I was wondering why the RDD.count call recomputes the RDD in all cases? In
most cases it can simply ask the next dependent RDD. I have several RDD
implementations and was surprised to see a call like the following never
call my RDD's count method but instead recompute/traverse the entire
dataset:

   val myRDD: MyRDD = ...
   myRDD.map({ ... }).count()

Unless I'm mistaken, a MappedRDD never needs to do more than call 'count' on
the underlying RDD. The underlying RDD's count method (in all of my cases)
know their count without a recompute (e.g. one of them selects the count
from a DB). This is MUCH less expensive than recomputing the RDD.

Thanks.
Jim




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-count-tp11298.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 04:05:08 2015
Return-Path: <dev-return-12243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EC6D41743F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 04:05:07 +0000 (UTC)
Received: (qmail 10263 invoked by uid 500); 28 Mar 2015 04:05:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10178 invoked by uid 500); 28 Mar 2015 04:05:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10167 invoked by uid 99); 28 Mar 2015 04:05:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 04:05:06 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 04:05:01 +0000
Received: by wibg7 with SMTP id g7so51902978wib.1
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 21:04:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=V8RMgbBnL4dV6zzLPPCMkwZqqLRgbxnt+9pDE4AhyrE=;
        b=g1luMLIQlCDxaFH5wqKq8dZwrs8bbnFVqKjnTpLhBMmaD8MnlD3AwAZj9JYJ8NAzmz
         ELUKUHyAN5DvIGbQU39eclSq6/kv7rP4AAI2WAUxra2lSyQcgty3X5xWts23LuhlvE6U
         TEO4T/TSpuyyAVxHWuetVvZrAVZp5yCW17iArpB6KBqiqPV9zGdF6ljTy+5lw1wZQ4sk
         wd4kh1eyw3CN55oYp4zCv5V3mRTawBZIJGGtC+etdVJETS/RCG4dr9G4bM3G3JvR7rlA
         /+zI4m0pOiVgUYKBFu4YVyWqvdEF/o0OyLH/yWMqoI4PNmxHoFGK97B38dKkwXxBZ2m/
         Ibtw==
X-Gm-Message-State: ALoCoQn1uFoZ5XZQZ7R41D2uH0Qo9LLaVL3PVnIE27Dzhx1fhY1ztEAzMk2K4p2JjEtLoR66Uq+D
MIME-Version: 1.0
X-Received: by 10.194.193.99 with SMTP id hn3mr43566813wjc.148.1427515480715;
 Fri, 27 Mar 2015 21:04:40 -0700 (PDT)
Received: by 10.27.81.195 with HTTP; Fri, 27 Mar 2015 21:04:40 -0700 (PDT)
Received: by 10.27.81.195 with HTTP; Fri, 27 Mar 2015 21:04:40 -0700 (PDT)
In-Reply-To: <1427504098515-11298.post@n3.nabble.com>
References: <1427504098515-11298.post@n3.nabble.com>
Date: Sat, 28 Mar 2015 04:04:40 +0000
Message-ID: <CAMAsSdJ+UmXmAfpBrPOH-DOBwroMLVb2gEawyR-xSuabFKsfbQ@mail.gmail.com>
Subject: Re: RDD.count
From: Sean Owen <sowen@cloudera.com>
To: Jim Carroll <jimfcarroll@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b874e5e23cc460512515999
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b874e5e23cc460512515999
Content-Type: text/plain; charset=UTF-8

I assume because map() could have side effects? Even if that's not
generally a good idea. The expectation or contract is that it is still
invoked. In this program the caller could also call count() on the parent.
On Mar 28, 2015 1:00 AM, "jimfcarroll" <jimfcarroll@gmail.com> wrote:

> Hi all,
>
> I was wondering why the RDD.count call recomputes the RDD in all cases? In
> most cases it can simply ask the next dependent RDD. I have several RDD
> implementations and was surprised to see a call like the following never
> call my RDD's count method but instead recompute/traverse the entire
> dataset:
>
>    val myRDD: MyRDD = ...
>    myRDD.map({ ... }).count()
>
> Unless I'm mistaken, a MappedRDD never needs to do more than call 'count'
> on
> the underlying RDD. The underlying RDD's count method (in all of my cases)
> know their count without a recompute (e.g. one of them selects the count
> from a DB). This is MUCH less expensive than recomputing the RDD.
>
> Thanks.
> Jim
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-count-tp11298.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--047d7b874e5e23cc460512515999--

From dev-return-12244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 05:18:38 2015
Return-Path: <dev-return-12244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 661A917551
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 05:18:38 +0000 (UTC)
Received: (qmail 72019 invoked by uid 500); 28 Mar 2015 05:18:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71945 invoked by uid 500); 28 Mar 2015 05:18:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71932 invoked by uid 99); 28 Mar 2015 05:18:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 05:18:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.218.46] (HELO mail-oi0-f46.google.com) (209.85.218.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 05:18:12 +0000
Received: by oiag65 with SMTP id g65so91998866oia.2
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 22:17:04 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=OUFTgbiS2taVkYzB3eyBNrnC7IQUH7AMBO0p4EJXTV4=;
        b=WZ2yzAfhhtSqbaEoz4O89ecpWwl4d5i4Gq3f1pwuX/iurxJzeGMtj59dP0FiB4zYk9
         t1zMrQ4Wi2IyK+w+WItcRY+WtmxFk6Y8gcR1fcb7+AshSELRLr/hyI0caQiE3IEE6s5N
         Ba2JUb0QxIZHmj88A9bOdZxiaXmXy4lhfdXJXLQw+gDoXQv/msLunl9950A4v9xmgPSW
         3OCfkFI298HArdeSPQH6BA+GMPOWjL21BBYxQ+0vO5lCRxK9fBlm9Eo9mGikramq+BNV
         U18yhfaLFPQ8Wl/3l6Lke/M0m0LJpz5iPzhL+wPD0UciR8nHFzi7eWlDsGxqOFKPQGVU
         CBaw==
X-Gm-Message-State: ALoCoQl7V55F8pfmP04txIFdxzPZdpsp3e0sehEDs/Tok6OJNOlkxQyXMh9vBhrGZeYYSieLctOP
MIME-Version: 1.0
X-Received: by 10.202.204.2 with SMTP id c2mr5450495oig.64.1427519824561; Fri,
 27 Mar 2015 22:17:04 -0700 (PDT)
Received: by 10.76.108.17 with HTTP; Fri, 27 Mar 2015 22:17:04 -0700 (PDT)
In-Reply-To: <CACkSZy06KiL6iG0U0hS-oSBc3-zsF6MZib_34Pb5ryX8nVB=pg@mail.gmail.com>
References: <CACkSZy1ooS=+YsDvpxkycq=fs6a-W04p5CBhh6L6BdfO9z0Rzw@mail.gmail.com>
	<CA+2Pv=gO4ZkaBCDMmiCfEWsv59DFrpgOiN-A+YFWF4aq0x2jaQ@mail.gmail.com>
	<CACkSZy00=i5U3=08nbRsQ3kxdyVUMXfNrz1P=1Fhtjtk7p761A@mail.gmail.com>
	<CA+2Pv=i2GorzekE1yW7pm7DOJ1Z-EJRAZhoHCsVsivbMpEc7Zw@mail.gmail.com>
	<CACkSZy06KiL6iG0U0hS-oSBc3-zsF6MZib_34Pb5ryX8nVB=pg@mail.gmail.com>
Date: Fri, 27 Mar 2015 22:17:04 -0700
Message-ID: <CA+2Pv=gMh1TP6+DTyyTJPVOgGD=y90HwwHj_ku5uRcjCf1Aq+Q@mail.gmail.com>
Subject: Re: Iterative pyspark / scala codebase development
From: Davies Liu <davies@databricks.com>
To: Stephen Boesch <javadba@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Fri, Mar 27, 2015 at 4:16 PM, Stephen Boesch <javadba@gmail.com> wrote:
> Thx much!  This works.
>
> My workflow is making changes to files in Intelij and running ipython to
> execute pyspark.
>
> Is there any way for ipython to "see the updated class files without first
> exiting?

No, iPython shell is statefull, it will have unexpected behavior when
you reload the library.

> 2015-03-27 10:21 GMT-07:00 Davies Liu <davies@databricks.com>:
>
>> put these lines in your ~/.bash_profile
>>
>> export SPARK_PREPEND_CLASSES=true
>> export SPARK_HOME=path_to_spark
>> export
>> PYTHONPATH="${SPARK_HOME}/python/lib/py4j-0.8.2.1-src.zip:${SPARK_HOME}/python:${PYTHONPATH}"
>>
>> $ source ~/.bash_profile
>> $ build/sbt assembly
>> $ build/sbt ~compile  # do not stop this
>>
>> Then in another terminal you could run python tests as
>> $ cd python/pyspark/
>> $  python rdd.py
>>
>>
>> cc to dev list
>>
>>
>> On Fri, Mar 27, 2015 at 10:15 AM, Stephen Boesch <javadba@gmail.com>
>> wrote:
>> > Which aspect of that page are you suggesting provides a more optimized
>> > alternative?
>> >
>> > 2015-03-27 10:13 GMT-07:00 Davies Liu <davies@databricks.com>:
>> >
>> >> see
>> >>
>> >> https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools
>> >>
>> >> On Fri, Mar 27, 2015 at 10:02 AM, Stephen Boesch <javadba@gmail.com>
>> >> wrote:
>> >> > I am iteratively making changes to the scala side of some new pyspark
>> >> > code
>> >> > and re-testing from the python/pyspark side.
>> >> >
>> >> > Presently my only solution is to rebuild completely
>> >> >
>> >> >       sbt assembly
>> >> >
>> >> > after any scala side change - no matter how small.
>> >> >
>> >> > Any better / expedited way for pyspark to see small scala side
>> >> > updates?
>> >
>> >
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 05:22:53 2015
Return-Path: <dev-return-12245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53D2A1755B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 05:22:53 +0000 (UTC)
Received: (qmail 75204 invoked by uid 500); 28 Mar 2015 05:22:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75117 invoked by uid 500); 28 Mar 2015 05:22:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75105 invoked by uid 99); 28 Mar 2015 05:22:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 05:22:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 74.125.82.53 as permitted sender)
Received: from [74.125.82.53] (HELO mail-wg0-f53.google.com) (74.125.82.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 05:22:27 +0000
Received: by wgdm6 with SMTP id m6so119143522wgd.2
        for <dev@spark.apache.org>; Fri, 27 Mar 2015 22:20:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=xHSUGZaTK5Cjy3D/TtSepS1GJM7JYK76e6CY3BE3J88=;
        b=EOqmA6Pyt77rNzwW5yZxkQDqK8Hjy5P/rqA1U9qMYa6HjH80ly8FIdi3yfHN27So8p
         RLGx8g38MzFL/6/wHg0o10fS0WVvcvKc3Q+3SYKQbBN+ugj2EXv4vurBOEB9mqd2x0ic
         XocsPbwaAoElrneGWQ+pSxtH/G75Np/LSDB/hiWS4W5wZcJ2tFaZqpV2cewyMZcbGmSM
         5DoqtWoHVE1bmW5t43MiDwf+u5dw7meISeX1s4ABbMHvK3wPxv7cGdAftz4Rkya4skkG
         LNHvVq3rhYYRJSub+gVWzNlWyilqwsu+3AfFBXOLlyoPysmB/yC2WqTpd9zpHg2oItDG
         7gbQ==
X-Gm-Message-State: ALoCoQn7rDjkqz5hgaunwCE1s8ZzmU9Ejixj7x1cedovVoHnZtd7Tb4sEaKpbwx0d9p7Ep5rj+sW
X-Received: by 10.194.175.39 with SMTP id bx7mr44556184wjc.22.1427520055829;
 Fri, 27 Mar 2015 22:20:55 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Fri, 27 Mar 2015 22:20:35 -0700 (PDT)
In-Reply-To: <CAG6LhydcVBUQF6-yJoFRQVzYZaCqJtbREt_3MOPfed5+8WfRhA@mail.gmail.com>
References: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
 <CAMAsSdJoswwE0BOA114FqoEQTEXbusvtB7ken_8wLnKzNRLQ8w@mail.gmail.com> <CAG6LhydcVBUQF6-yJoFRQVzYZaCqJtbREt_3MOPfed5+8WfRhA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 28 Mar 2015 05:20:35 +0000
Message-ID: <CAMAsSd+Ko=QFuSg2OkQ0b_KTuopGCg1CcJd0P0DEAoOky1su9w@mail.gmail.com>
Subject: Re: Building spark 1.2 from source requires more dependencies
To: Pala M Muthaia <mchettiar@rocketfuelinc.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

This is not a compile error, but an error from the scalac compiler.
That is, the code and build are fine, but scalac is not compiling it.
Usually when this happens, a clean build fixes it.

On Fri, Mar 27, 2015 at 7:09 PM, Pala M Muthaia
<mchettiar@rocketfuelinc.com> wrote:
> No, i am running from the root directory, parent of core.
>
> Here is the first set of errors that i see when i compile from source (sorry
> the error message is very long, but adding it in case it helps in
> diagnosis). After i manually add javax.servlet dependency for  version 3.0,
> these set of errors go away and i get the next set of errors about missing
> classes under eclipse-jetty.
>
> I am on maven 3.2.5 and java 1.7.
>
> Error:
>
> [INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
> spark-core_2.10 ---
> [WARNING] Zinc server is not available at port 3030 - reverting to normal
> incremental compile
> [INFO] Using incremental compilation
> [INFO] compiler plugin:
> BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
> [INFO] Compiling 403 Scala sources and 33 Java sources to
> /Users/mchettiar/code/spark/core/target/scala-2.10/classes...
> [WARNING] Class javax.servlet.ServletException not found - continuing with a
> stub.
> [ERROR]
>      while compiling:
> /Users/mchettiar/code/spark/core/src/main/scala/org/apache/spark/HttpServer.scala
>         during phase: typer
>      library version: version 2.10.4
>     compiler version: version 2.10.4
>   reconstructed args: -deprecation -feature
> -

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 13:07:31 2015
Return-Path: <dev-return-12246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B765417C57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 13:07:31 +0000 (UTC)
Received: (qmail 63457 invoked by uid 500); 28 Mar 2015 13:07:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63373 invoked by uid 500); 28 Mar 2015 13:07:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63361 invoked by uid 99); 28 Mar 2015 13:07:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 13:07:30 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of jimfcarroll@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 13:07:04 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 5110F18F82D4
	for <dev@spark.apache.org>; Sat, 28 Mar 2015 06:05:51 -0700 (PDT)
Date: Sat, 28 Mar 2015 06:05:32 -0700 (MST)
From: jimfcarroll <jimfcarroll@gmail.com>
To: dev@spark.apache.org
Message-ID: <1427547932037-11302.post@n3.nabble.com>
In-Reply-To: <CAMAsSdJ+UmXmAfpBrPOH-DOBwroMLVb2gEawyR-xSuabFKsfbQ@mail.gmail.com>
References: <1427504098515-11298.post@n3.nabble.com> <CAMAsSdJ+UmXmAfpBrPOH-DOBwroMLVb2gEawyR-xSuabFKsfbQ@mail.gmail.com>
Subject: Re: RDD.count
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Sean,

Thanks for the response.

I can't imagine a case (though my imagination may be somewhat limited) where
even map side effects could change the number of elements in the resulting
map.

I guess "count" wouldn't officially be an 'action' if it were implemented
this way. At least it wouldn't ALWAYS be one.

My example was contrived. We're passing RDDs to functions. If that RDD is an
instance of my class, then its count() may take a shortcut. If I
map/zip/zipWithIndex/mapPartition/etc. first then I'm stuck with a call that
literally takes 100s to 1000s of times longer (seconds vs hours on some of
our datasets) and since my custom RDDs are immutable they cache the count
call so a second invocation is the cost of a method call's overhead.

I could fix this in Spark if there's any interest in that change. Otherwise
I'll need to overload more RDD methods for my own purposes (like all of the
transformations). Of course, that will be more difficult because those
intermediate classes (like MappedRDD) are private, so I can't extend them.

Jim




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-count-tp11298p11302.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 13:11:23 2015
Return-Path: <dev-return-12247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0E28917C7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 13:11:23 +0000 (UTC)
Received: (qmail 71901 invoked by uid 500); 28 Mar 2015 13:11:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71795 invoked by uid 500); 28 Mar 2015 13:11:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71782 invoked by uid 99); 28 Mar 2015 13:11:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 13:11:21 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 13:11:17 +0000
Received: by wibg7 with SMTP id g7so54674540wib.1
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 06:10:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=mNT+jux6CzmCjENjRfC9C7eX9wj3hEjnMJw0lJULUqQ=;
        b=lVy7HVdpJEGPTP2SKhRGcRfWMGiJGyUbfIwckXALosN9ZRlMJgGhfQw+Fli87fvAwY
         pR7BB4eEui2UDVHGtMWYp5EZYqxU2p4Op7dV2p5T1zkXzEAlbrMVPi/nfkm5ZVvOFGTn
         oWfhW5Qi137OdZMEV3uh8DOzZNGJ1fe9RWTDuLRBGvbv9ZnUCD84UrD5I4fJCTjj+x68
         datmrKu0brdRfNeTX2D0PhPDyoEfsoVI4eY9rbY0C0ZwVJnKfh4Vfi78pkcs1L4/zTxT
         da4COr20kvtFeQh8i3z6Gpx6KZA+qfp5lxKCMSmgTwNNZRA0631c0iMf/5tgp623eRgk
         +E6g==
X-Gm-Message-State: ALoCoQkvQyAlQZd6dY/DNKOgfaKDquMLP9r5v0RViHJ3DRJMKCLzczEE+SXsVPl4LcGZpyy7G4L/
X-Received: by 10.180.89.227 with SMTP id br3mr6127301wib.67.1427548256212;
 Sat, 28 Mar 2015 06:10:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.195 with HTTP; Sat, 28 Mar 2015 06:10:35 -0700 (PDT)
In-Reply-To: <1427547932037-11302.post@n3.nabble.com>
References: <1427504098515-11298.post@n3.nabble.com> <CAMAsSdJ+UmXmAfpBrPOH-DOBwroMLVb2gEawyR-xSuabFKsfbQ@mail.gmail.com>
 <1427547932037-11302.post@n3.nabble.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sat, 28 Mar 2015 13:10:35 +0000
Message-ID: <CAMAsSdL7bi2CODfKvY_oyrJ_cxx5+YR17Kd40qhpgqhZx8huBg@mail.gmail.com>
Subject: Re: RDD.count
To: jimfcarroll <jimfcarroll@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

No, I'm not saying side effects change the count. But not executing
the map() function at all certainly has an effect on the side effects
of that function: the side effects which should take place never do. I
am not sure that is something to be 'fixed'; it's a legitimate
question.

You can persist an RDD if you do not want to compute it twice.

On Sat, Mar 28, 2015 at 1:05 PM, jimfcarroll <jimfcarroll@gmail.com> wrote:
> Hi Sean,
>
> Thanks for the response.
>
> I can't imagine a case (though my imagination may be somewhat limited) where
> even map side effects could change the number of elements in the resulting
> map.
>
> I guess "count" wouldn't officially be an 'action' if it were implemented
> this way. At least it wouldn't ALWAYS be one.
>
> My example was contrived. We're passing RDDs to functions. If that RDD is an
> instance of my class, then its count() may take a shortcut. If I
> map/zip/zipWithIndex/mapPartition/etc. first then I'm stuck with a call that
> literally takes 100s to 1000s of times longer (seconds vs hours on some of
> our datasets) and since my custom RDDs are immutable they cache the count
> call so a second invocation is the cost of a method call's overhead.
>
> I could fix this in Spark if there's any interest in that change. Otherwise
> I'll need to overload more RDD methods for my own purposes (like all of the
> transformations). Of course, that will be more difficult because those
> intermediate classes (like MappedRDD) are private, so I can't extend them.
>
> Jim
>
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-count-tp11298p11302.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 14:44:47 2015
Return-Path: <dev-return-12248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ABDA117ED0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 14:44:47 +0000 (UTC)
Received: (qmail 9062 invoked by uid 500); 28 Mar 2015 14:44:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8988 invoked by uid 500); 28 Mar 2015 14:44:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8977 invoked by uid 99); 28 Mar 2015 14:44:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 14:44:45 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.213.178 as permitted sender)
Received: from [209.85.213.178] (HELO mail-ig0-f178.google.com) (209.85.213.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 14:44:41 +0000
Received: by igbud6 with SMTP id ud6so42699195igb.1
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 07:42:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=o9VUnaR0E2bkPAHdogWF/FbGPvIo48rkUvDatQOAW4g=;
        b=gIdYhJhfG1FM9tp8hbwhgTEsr/s9LHZWt5m0jHetllTQrPAQhhgtiAlX0ik57X0QOv
         rI62J7+ntVnmAN/fs0d+VIHiqWgKfATEG+kLBdMwvcQBdN4GYpauUR4/a4AeO5r2Vq4R
         3z634YE6tRZpC2ynLClFncEpxWjGuMmyBlMrpNJ1Si21YKU60sD+K7cPeFQn5kDUilB3
         F6reJswx8ow1oRgUuHAPsuVH9vOKt8xKzLwYJ8Je1hesIiAmYZJUZBZ2VY7NyIWue3+9
         dTfO0fA0XitM4m8VuI7yzZMC4296qZphGoFnPk8Ce+f3J2vITt7tQ9Zz/evYJwJU9GUW
         AJWQ==
X-Gm-Message-State: ALoCoQmow+ZqwLSBMptv7TQN/dUJuXlSYn69XjrDqr1LIpQymfJqBzV8slpPUQ4Q6nzp8GciLyQm
MIME-Version: 1.0
X-Received: by 10.107.138.88 with SMTP id m85mr35460525iod.35.1427553771076;
 Sat, 28 Mar 2015 07:42:51 -0700 (PDT)
Received: by 10.36.90.208 with HTTP; Sat, 28 Mar 2015 07:42:51 -0700 (PDT)
In-Reply-To: <CAMAsSdL7bi2CODfKvY_oyrJ_cxx5+YR17Kd40qhpgqhZx8huBg@mail.gmail.com>
References: <1427504098515-11298.post@n3.nabble.com>
	<CAMAsSdJ+UmXmAfpBrPOH-DOBwroMLVb2gEawyR-xSuabFKsfbQ@mail.gmail.com>
	<1427547932037-11302.post@n3.nabble.com>
	<CAMAsSdL7bi2CODfKvY_oyrJ_cxx5+YR17Kd40qhpgqhZx8huBg@mail.gmail.com>
Date: Sat, 28 Mar 2015 07:42:51 -0700
Message-ID: <CACBYxKKocZa0WTR7g9cAHqpvodzpcYHZ6sadCzKGq+uPYisi4Q@mail.gmail.com>
Subject: Re: RDD.count
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: Sean Owen <sowen@cloudera.com>
Cc: jimfcarroll <jimfcarroll@gmail.com>, dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113fa3f46c7b7305125a43de
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113fa3f46c7b7305125a43de
Content-Type: text/plain; charset=UTF-8

I definitely see the value in this.  However, I think at this point it
would be an incompatible behavioral change.  People often use count in
Spark to exercise their DAG.  Omitting processing steps that were
previously included would likely mislead many users into thinking their
pipeline was running faster.

It's possible there might be room for something like a new smartCount API
or a new argument to count that allows it to avoid unnecessary
transformations.

-Sandy

On Sat, Mar 28, 2015 at 6:10 AM, Sean Owen <sowen@cloudera.com> wrote:

> No, I'm not saying side effects change the count. But not executing
> the map() function at all certainly has an effect on the side effects
> of that function: the side effects which should take place never do. I
> am not sure that is something to be 'fixed'; it's a legitimate
> question.
>
> You can persist an RDD if you do not want to compute it twice.
>
> On Sat, Mar 28, 2015 at 1:05 PM, jimfcarroll <jimfcarroll@gmail.com>
> wrote:
> > Hi Sean,
> >
> > Thanks for the response.
> >
> > I can't imagine a case (though my imagination may be somewhat limited)
> where
> > even map side effects could change the number of elements in the
> resulting
> > map.
> >
> > I guess "count" wouldn't officially be an 'action' if it were implemented
> > this way. At least it wouldn't ALWAYS be one.
> >
> > My example was contrived. We're passing RDDs to functions. If that RDD
> is an
> > instance of my class, then its count() may take a shortcut. If I
> > map/zip/zipWithIndex/mapPartition/etc. first then I'm stuck with a call
> that
> > literally takes 100s to 1000s of times longer (seconds vs hours on some
> of
> > our datasets) and since my custom RDDs are immutable they cache the count
> > call so a second invocation is the cost of a method call's overhead.
> >
> > I could fix this in Spark if there's any interest in that change.
> Otherwise
> > I'll need to overload more RDD methods for my own purposes (like all of
> the
> > transformations). Of course, that will be more difficult because those
> > intermediate classes (like MappedRDD) are private, so I can't extend
> them.
> >
> > Jim
> >
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-count-tp11298p11302.html
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a113fa3f46c7b7305125a43de--

From dev-return-12249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 15:28:03 2015
Return-Path: <dev-return-12249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C2691022E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 15:28:03 +0000 (UTC)
Received: (qmail 66037 invoked by uid 500); 28 Mar 2015 15:27:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65953 invoked by uid 500); 28 Mar 2015 15:27:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65941 invoked by uid 99); 28 Mar 2015 15:27:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 15:27:56 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of patrick.woody1@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 15:27:30 +0000
Received: by ierf6 with SMTP id f6so23059212ier.2
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 08:26:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=lAp+1544v5s0F6jswQINkklTR8wjvl3ap+vupWeVyJw=;
        b=pQIld8C/Xlj/Uh7DGTcOnZ2a5XiJkuGVAPbv1qdoe9U/wMxE2jjSPsfxWwrPfCnUT2
         Ajlc6N3Q/UjtXni11GjiO4GOvol4lT8eTKBk2iYg+Da/Ol6FKSWPwdxcgJ0qocx8W6CY
         cl3jDFKW+dZ6dYteH1hYEnmo0y9xdZpKvGKUpf7Gz1hqndhf3i0YxSIJ9ufO6TXtOxiM
         dyMVtLyjYeC+g9tcPTLNjJ5wG29LXZOG9WnuUMnKU23M4LUKA8Pzw9Bi3D5BlOAiU/QQ
         KBCieNHbxOwqu6Z3uFin1gu5O7MUPJfrZ4vj1ka8ThgDY+FA5RvanoXl/P9/g5l5Ae61
         zGqw==
MIME-Version: 1.0
X-Received: by 10.107.151.73 with SMTP id z70mr38042042iod.41.1427556404062;
 Sat, 28 Mar 2015 08:26:44 -0700 (PDT)
Received: by 10.107.31.206 with HTTP; Sat, 28 Mar 2015 08:26:44 -0700 (PDT)
Date: Sat, 28 Mar 2015 11:26:44 -0400
Message-ID: <CAFGcCdVPPoDNnvB+81CJ-wZDwX5+PgJYdUk4PZfcySP=ROuBdQ@mail.gmail.com>
Subject: Lazy casting with Catalyst
From: Patrick Woody <patrick.woody1@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1140f04a5c92f805125ae0e4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1140f04a5c92f805125ae0e4
Content-Type: text/plain; charset=UTF-8

Hi all,

In my application, we take input from Parquet files where BigDecimals are
written as Strings to maintain arbitrary precision.

I was hoping to convert these back over to Decimal with Unlimited
precision, but I'd still like to maintain the Parquet column pruning (all
my attempts thus far seem to bring in the whole Row). Is it possible to do
this lazily through catalyst?

Basically I'd want to do Cast(col, DecimalType()) whenever col is actually
referenced. Any tips on how to approach this would be appreciated.

Thanks!
-Pat

--001a1140f04a5c92f805125ae0e4--

From dev-return-12250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 15:37:15 2015
Return-Path: <dev-return-12250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BBF4D102D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 15:37:15 +0000 (UTC)
Received: (qmail 79780 invoked by uid 500); 28 Mar 2015 15:37:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79692 invoked by uid 500); 28 Mar 2015 15:37:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79680 invoked by uid 99); 28 Mar 2015 15:37:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 15:37:14 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.173 as permitted sender)
Received: from [209.85.192.173] (HELO mail-pd0-f173.google.com) (209.85.192.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 15:37:07 +0000
Received: by pdcp1 with SMTP id p1so35428909pdc.3
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 08:35:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=3hjvONAdt6iT2iGvjoeCENkPqIWaFb9u5oQpP+08yZY=;
        b=HKiRsU3HGqPH1jXQfb7u9kBi4UzDAzPz3k6WM/SUO8sh8YqFkRVJewcfY6/AxqS0DT
         Aghvs0Rn0GVMeltjs9fHl5HtRSi1awIfkgKh6axEZI02prft52/7TVhr1YAnGNDotmGG
         MeoLgF9PMQI0sW5W3cBJGbRR2CtwKYPMq9Xy0MzIRBtZL3QlL0bEl7OApdk2RpUmzCrc
         IrmjMy+NkQxISveEf2mLeTUc42uAFBNxi7AO0ZDar6qTzt38cq59za61fp7YgKK8rikN
         NaD3SjQHjE+IR0qRgY8Cqfty3Et18gA2cINEBPZGZdnWYeJ8CRGEJj+mA/LrKMbJU+eB
         rY4Q==
X-Received: by 10.66.217.198 with SMTP id pa6mr2748005pac.49.1427556917628;
        Sat, 28 Mar 2015 08:35:17 -0700 (PDT)
Received: from [10.10.0.33] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id iv3sm5380548pbb.32.2015.03.28.08.35.14
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sat, 28 Mar 2015 08:35:16 -0700 (PDT)
Message-ID: <5516CA31.50001@gmail.com>
Date: Sat, 28 Mar 2015 23:35:13 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Patrick Woody <patrick.woody1@gmail.com>, dev@spark.apache.org
Subject: Re: Lazy casting with Catalyst
References: <CAFGcCdVPPoDNnvB+81CJ-wZDwX5+PgJYdUk4PZfcySP=ROuBdQ@mail.gmail.com>
In-Reply-To: <CAFGcCdVPPoDNnvB+81CJ-wZDwX5+PgJYdUk4PZfcySP=ROuBdQ@mail.gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Pat,

I don't understand what "lazy casting" mean here. Why do you think 
current Catalyst casting is "eager"? Casting happens at runtime, and 
doesn't disable column pruning.

Cheng

On 3/28/15 11:26 PM, Patrick Woody wrote:
> Hi all,
>
> In my application, we take input from Parquet files where BigDecimals are
> written as Strings to maintain arbitrary precision.
>
> I was hoping to convert these back over to Decimal with Unlimited
> precision, but I'd still like to maintain the Parquet column pruning (all
> my attempts thus far seem to bring in the whole Row). Is it possible to do
> this lazily through catalyst?
>
> Basically I'd want to do Cast(col, DecimalType()) whenever col is actually
> referenced. Any tips on how to approach this would be appreciated.
>
> Thanks!
> -Pat
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 16:28:47 2015
Return-Path: <dev-return-12251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB2CD10691
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 16:28:47 +0000 (UTC)
Received: (qmail 58952 invoked by uid 500); 28 Mar 2015 16:28:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58867 invoked by uid 500); 28 Mar 2015 16:28:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58855 invoked by uid 99); 28 Mar 2015 16:28:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 16:28:46 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of patrick.woody1@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 16:28:19 +0000
Received: by igbud6 with SMTP id ud6so43905198igb.1
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 09:26:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=3IWn8eB+pW4tIf2xoxLbhtWxHJsWbNqF/4dZ5ThesK4=;
        b=CCiKjqzm9j7dydCoTpxQf80Y1EUjwWDT0N7Sol7GG4hvniN7XOOH1iQttL8VtxkP9+
         XBwmgkFZDBDWJP1M1uYWDGx1XLBI/TeZ+5Vny81zBL72h2DdwbG/BDJ9flg95zFB0P/A
         58UAZy7NSNAg/LVDq3oEQttlOUSIrWxML6sY0jMpNkY9yTYvLAh1TLNvONpvUX00iFXh
         vmrk1SrZ8sbpRVnGRqCBbV57SL3CcCdV+t1TdtWUABf9h19x/QrNDNFmt+tfEWqpcrDY
         axb/hdqAXQere2fCuR1lmhzialrWpBIW2DvrYZcO2Pv0qqna3OXMRyey6rn7X1anQtoq
         IMNQ==
MIME-Version: 1.0
X-Received: by 10.42.226.69 with SMTP id iv5mr43493571icb.58.1427560008144;
 Sat, 28 Mar 2015 09:26:48 -0700 (PDT)
Received: by 10.107.31.206 with HTTP; Sat, 28 Mar 2015 09:26:48 -0700 (PDT)
In-Reply-To: <5516CA31.50001@gmail.com>
References: <CAFGcCdVPPoDNnvB+81CJ-wZDwX5+PgJYdUk4PZfcySP=ROuBdQ@mail.gmail.com>
	<5516CA31.50001@gmail.com>
Date: Sat, 28 Mar 2015 12:26:48 -0400
Message-ID: <CAFGcCdXa-019_giRAeQffUiQxsEJOENDV7-dYUGNBWEsZQNW_g@mail.gmail.com>
Subject: Re: Lazy casting with Catalyst
From: Patrick Woody <patrick.woody1@gmail.com>
To: Cheng Lian <lian.cs.zju@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c319a62e809705125bb745
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c319a62e809705125bb745
Content-Type: text/plain; charset=UTF-8

Hey Cheng,

I didn't meant that catalyst casting was eager, just that my approaches
thus far seem to have been. Maybe I should give a concrete example?

I have columns A, B, C where B is saved as a String but I'd like all
references to B to go through a Cast to decimal regardless of the code used
on the SchemaRDD. So if someone does a min(B) it uses Decimal ordering
instead of String.

One approach that I had taken was to do a select of everything with the
casts on certain columns, but then when I did a count(literal(1)) on top of
that RDD it seemed to bring in the whole row.

Thanks!
-Pat

On Sat, Mar 28, 2015 at 11:35 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:

> Hi Pat,
>
> I don't understand what "lazy casting" mean here. Why do you think current
> Catalyst casting is "eager"? Casting happens at runtime, and doesn't
> disable column pruning.
>
> Cheng
>
>
> On 3/28/15 11:26 PM, Patrick Woody wrote:
>
>> Hi all,
>>
>> In my application, we take input from Parquet files where BigDecimals are
>> written as Strings to maintain arbitrary precision.
>>
>> I was hoping to convert these back over to Decimal with Unlimited
>> precision, but I'd still like to maintain the Parquet column pruning (all
>> my attempts thus far seem to bring in the whole Row). Is it possible to do
>> this lazily through catalyst?
>>
>> Basically I'd want to do Cast(col, DecimalType()) whenever col is actually
>> referenced. Any tips on how to approach this would be appreciated.
>>
>> Thanks!
>> -Pat
>>
>>
>

--001a11c319a62e809705125bb745--

From dev-return-12252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 16:37:23 2015
Return-Path: <dev-return-12252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3EF30106D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 16:37:23 +0000 (UTC)
Received: (qmail 72888 invoked by uid 500); 28 Mar 2015 16:37:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72800 invoked by uid 500); 28 Mar 2015 16:37:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72788 invoked by uid 99); 28 Mar 2015 16:37:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 16:37:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.220.51 as permitted sender)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 16:37:15 +0000
Received: by pacwe9 with SMTP id we9so124833928pac.1
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 09:34:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:cc:subject
         :references:in-reply-to:content-type;
        bh=LH8kLZGYyVf3i3mKzuxl8O4B+PoWy42wUvg2OHDuBkU=;
        b=Mq6goPeWmybIJjyuIzqtwY8TnkVw7CkR1mudKcmDgpzZKQEpX3RypgHYHA7jzWgssZ
         XoBgQYLHegn6rVdV0x4GjuPOP6Evx6vMb7U/81wXnLpfWrOE9b0K6BsEuodH6uQEcqV2
         wPpj9F751IVIY4LrAkTVObMlWKX5SngTEIEcYYLRVdxowJnJAOTjy5sc+JnzCdUMBtRe
         l7ZXSBzXs6tgylHF1NLzZURj1B/q9XDVL3MHiqDRgia5YTf4Ipc6U1llh4vxyzqBNDkN
         VRbFmLOqrmFYIjhLNgrgVxQoi9zzfsBJCxm0TBemd4LmwcrSDBJmPXXN7l093pA8S0r/
         +oJQ==
X-Received: by 10.68.65.100 with SMTP id w4mr20553945pbs.95.1427560479803;
        Sat, 28 Mar 2015 09:34:39 -0700 (PDT)
Received: from [10.10.0.33] (li751-165.members.linode.com. [106.185.40.165])
        by mx.google.com with ESMTPSA id cj2sm5503281pbb.10.2015.03.28.09.34.37
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Sat, 28 Mar 2015 09:34:39 -0700 (PDT)
Message-ID: <5516D81C.7090803@gmail.com>
Date: Sun, 29 Mar 2015 00:34:36 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Patrick Woody <patrick.woody1@gmail.com>
CC: dev@spark.apache.org
Subject: Re: Lazy casting with Catalyst
References: <CAFGcCdVPPoDNnvB+81CJ-wZDwX5+PgJYdUk4PZfcySP=ROuBdQ@mail.gmail.com>	<5516CA31.50001@gmail.com> <CAFGcCdXa-019_giRAeQffUiQxsEJOENDV7-dYUGNBWEsZQNW_g@mail.gmail.com>
In-Reply-To: <CAFGcCdXa-019_giRAeQffUiQxsEJOENDV7-dYUGNBWEsZQNW_g@mail.gmail.com>
Content-Type: multipart/alternative;
 boundary="------------000302080103090606070408"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------000302080103090606070408
Content-Type: text/plain; charset=windows-1252; format=flowed
Content-Transfer-Encoding: 7bit


On 3/29/15 12:26 AM, Patrick Woody wrote:
> Hey Cheng,
>
> I didn't meant that catalyst casting was eager, just that my 
> approaches thus far seem to have been. Maybe I should give a concrete 
> example?
>
> I have columns A, B, C where B is saved as a String but I'd like all 
> references to B to go through a Cast to decimal regardless of the code 
> used on the SchemaRDD. So if someone does a min(B) it uses Decimal 
> ordering instead of String.
>
> One approach that I had taken was to do a select of everything with 
> the casts on certain columns, but then when I did a count(literal(1)) 
> on top of that RDD it seemed to bring in the whole row.
What version of Spark SQL are you using? Would you mind to provide a 
brief snippet that can reproduce this issue? This might be a bug 
depending on your concrete usage. Thanks in advance!
>
> Thanks!
> -Pat
>
> On Sat, Mar 28, 2015 at 11:35 AM, Cheng Lian <lian.cs.zju@gmail.com 
> <mailto:lian.cs.zju@gmail.com>> wrote:
>
>     Hi Pat,
>
>     I don't understand what "lazy casting" mean here. Why do you think
>     current Catalyst casting is "eager"? Casting happens at runtime,
>     and doesn't disable column pruning.
>
>     Cheng
>
>
>     On 3/28/15 11:26 PM, Patrick Woody wrote:
>
>         Hi all,
>
>         In my application, we take input from Parquet files where
>         BigDecimals are
>         written as Strings to maintain arbitrary precision.
>
>         I was hoping to convert these back over to Decimal with Unlimited
>         precision, but I'd still like to maintain the Parquet column
>         pruning (all
>         my attempts thus far seem to bring in the whole Row). Is it
>         possible to do
>         this lazily through catalyst?
>
>         Basically I'd want to do Cast(col, DecimalType()) whenever col
>         is actually
>         referenced. Any tips on how to approach this would be appreciated.
>
>         Thanks!
>         -Pat
>
>
>


--------------000302080103090606070408--

From dev-return-12253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 18:39:41 2015
Return-Path: <dev-return-12253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC1B510ADB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 18:39:41 +0000 (UTC)
Received: (qmail 72033 invoked by uid 500); 28 Mar 2015 18:39:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71944 invoked by uid 500); 28 Mar 2015 18:39:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71932 invoked by uid 99); 28 Mar 2015 18:39:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 18:39:39 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of patrick.woody1@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 18:39:13 +0000
Received: by igcau2 with SMTP id au2so49480230igc.1
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 11:37:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=1B+eel0q2Bv3jeAD/hud5Fvh2AGD93FcvNt+qOt4+AE=;
        b=n7M6LgLZeJN7CncKRk2JK9SITCekv4BriSzcnk6ul0GZ17SVu98jnoE9ojf3o0Z+2L
         mArJHm8S3dTmdCFLVDh7jFK4qtmBO6r+MTckK3gV77MBAbgjFpNzOC5hIve1hc/F51mB
         2wv3y2L8FpQHMfUHK9r8FNcIruDTHAufyEh843ED5nlUo4NC44D6DGjMMgLhhOcoZvjb
         75EBLkAZfpT/eNhq2C4CTMSuhhPV15d/6KolYwHmowlowZgb+cIxSn9KjXclLaLronjH
         BDOrgkL5a6viOx5POyNIoFCajPSa8MMPTNBVYhDN4BKcpj8H97MBdE8mN57VbEHV7wqq
         XRtg==
MIME-Version: 1.0
X-Received: by 10.50.29.52 with SMTP id g20mr6919269igh.27.1427567861558; Sat,
 28 Mar 2015 11:37:41 -0700 (PDT)
Received: by 10.107.31.206 with HTTP; Sat, 28 Mar 2015 11:37:41 -0700 (PDT)
In-Reply-To: <5516D81C.7090803@gmail.com>
References: <CAFGcCdVPPoDNnvB+81CJ-wZDwX5+PgJYdUk4PZfcySP=ROuBdQ@mail.gmail.com>
	<5516CA31.50001@gmail.com>
	<CAFGcCdXa-019_giRAeQffUiQxsEJOENDV7-dYUGNBWEsZQNW_g@mail.gmail.com>
	<5516D81C.7090803@gmail.com>
Date: Sat, 28 Mar 2015 14:37:41 -0400
Message-ID: <CAFGcCdXHD9sAfp2PgJJ7vNfSSr8L-ZzJMiP-8kMgutv1Z39Byw@mail.gmail.com>
Subject: Re: Lazy casting with Catalyst
From: Patrick Woody <patrick.woody1@gmail.com>
To: Cheng Lian <lian.cs.zju@gmail.com>
Cc: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd74dd848163905125d8bf0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd74dd848163905125d8bf0
Content-Type: text/plain; charset=UTF-8

So it looks like this was actually a combination of using out of date
artifacts and further debugging needed on my part. Ripping the logic out
and testing in spark-shell works fine, so it is likely something upstream
in my application that causes it to take the whole Row.

Thanks!
-Pat





On Sat, Mar 28, 2015 at 12:34 PM, Cheng Lian <lian.cs.zju@gmail.com> wrote:

>
> On 3/29/15 12:26 AM, Patrick Woody wrote:
>
>  Hey Cheng,
>
>  I didn't meant that catalyst casting was eager, just that my approaches
> thus far seem to have been. Maybe I should give a concrete example?
>
> I have columns A, B, C where B is saved as a String but I'd like all
> references to B to go through a Cast to decimal regardless of the code used
> on the SchemaRDD. So if someone does a min(B) it uses Decimal ordering
> instead of String.
>
>  One approach that I had taken was to do a select of everything with the
> casts on certain columns, but then when I did a count(literal(1)) on top of
> that RDD it seemed to bring in the whole row.
>
> What version of Spark SQL are you using? Would you mind to provide a brief
> snippet that can reproduce this issue? This might be a bug depending on
> your concrete usage. Thanks in advance!
>
>
>  Thanks!
> -Pat
>
> On Sat, Mar 28, 2015 at 11:35 AM, Cheng Lian <lian.cs.zju@gmail.com>
> wrote:
>
>> Hi Pat,
>>
>> I don't understand what "lazy casting" mean here. Why do you think
>> current Catalyst casting is "eager"? Casting happens at runtime, and
>> doesn't disable column pruning.
>>
>> Cheng
>>
>>
>> On 3/28/15 11:26 PM, Patrick Woody wrote:
>>
>>> Hi all,
>>>
>>> In my application, we take input from Parquet files where BigDecimals are
>>> written as Strings to maintain arbitrary precision.
>>>
>>> I was hoping to convert these back over to Decimal with Unlimited
>>> precision, but I'd still like to maintain the Parquet column pruning (all
>>> my attempts thus far seem to bring in the whole Row). Is it possible to
>>> do
>>> this lazily through catalyst?
>>>
>>> Basically I'd want to do Cast(col, DecimalType()) whenever col is
>>> actually
>>> referenced. Any tips on how to approach this would be appreciated.
>>>
>>> Thanks!
>>> -Pat
>>>
>>>
>>
>
>

--047d7bd74dd848163905125d8bf0--

From dev-return-12254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Mar 28 23:36:42 2015
Return-Path: <dev-return-12254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8B093172BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 28 Mar 2015 23:36:42 +0000 (UTC)
Received: (qmail 22059 invoked by uid 500); 28 Mar 2015 23:36:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21982 invoked by uid 500); 28 Mar 2015 23:36:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21971 invoked by uid 99); 28 Mar 2015 23:36:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 23:36:35 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 28 Mar 2015 23:36:31 +0000
Received: by qgfa8 with SMTP id a8so154627240qgf.0
        for <dev@spark.apache.org>; Sat, 28 Mar 2015 16:35:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=HpjlXyQ9Sq0/ijjgtBwzrZ9OdG7x7jxnxZSJtaPqaqo=;
        b=b1ZT7IaEabMFK62fSyfHJH/DPrDIWnnh/SliYZXPmvB7LB5rCyZvxeTCj8EFV27q12
         1d6c5O/j90LkdKSyXwHc6iy2q3GJoLJXqH1envsmk5dEEP5qmUjzQQnCW4MyGe8Yd/7A
         geh0M7aGqz037oIj7iE//WWONAMhmswMq5PVR3jQLJzLbanxrsDLHIS+nJo/HP3D7qck
         6DtYNBkhm0FH8VwFNbuTPJQohRaydvf65Ij6xxws29zbGX7CWH36vzdFfUXhJpCTtuje
         tF7V7t12nZ7zoHrVh1qX3VP2iq538ojAszlG1k9zNC5HblEEkAZ5DEmwcJFdz0/eoxA8
         NF1A==
X-Gm-Message-State: ALoCoQmUHcqVRmWZqPGLRM5lxJCQx4S+bbpcqLHF9HnJ/VJxcigBmI3NdX1bBqdrQd9t8kYY9WEW
X-Received: by 10.140.93.199 with SMTP id d65mr31567276qge.104.1427585705441;
 Sat, 28 Mar 2015 16:35:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Sat, 28 Mar 2015 16:34:45 -0700 (PDT)
In-Reply-To: <CACBYxKKocZa0WTR7g9cAHqpvodzpcYHZ6sadCzKGq+uPYisi4Q@mail.gmail.com>
References: <1427504098515-11298.post@n3.nabble.com> <CAMAsSdJ+UmXmAfpBrPOH-DOBwroMLVb2gEawyR-xSuabFKsfbQ@mail.gmail.com>
 <1427547932037-11302.post@n3.nabble.com> <CAMAsSdL7bi2CODfKvY_oyrJ_cxx5+YR17Kd40qhpgqhZx8huBg@mail.gmail.com>
 <CACBYxKKocZa0WTR7g9cAHqpvodzpcYHZ6sadCzKGq+uPYisi4Q@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sat, 28 Mar 2015 16:34:45 -0700
Message-ID: <CAPh_B=YzPzGqY0Y-Fd8-c9YZPs-0OK5XAsr6FxKwYtupcPn3Kw@mail.gmail.com>
Subject: Re: RDD.count
To: Sandy Ryza <sandy.ryza@cloudera.com>
Cc: Sean Owen <sowen@cloudera.com>, jimfcarroll <jimfcarroll@gmail.com>, 
	dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11395a30dc12b8051261b2f1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11395a30dc12b8051261b2f1
Content-Type: text/plain; charset=UTF-8

I think the worry here is that people often use count() to force execution,
and when coupled with transformations with side-effect, it is no longer
safe to not run it.

However, maybe we can add a new lazy val .size that doesn't require
recomputation.


On Sat, Mar 28, 2015 at 7:42 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> I definitely see the value in this.  However, I think at this point it
> would be an incompatible behavioral change.  People often use count in
> Spark to exercise their DAG.  Omitting processing steps that were
> previously included would likely mislead many users into thinking their
> pipeline was running faster.
>
> It's possible there might be room for something like a new smartCount API
> or a new argument to count that allows it to avoid unnecessary
> transformations.
>
> -Sandy
>
> On Sat, Mar 28, 2015 at 6:10 AM, Sean Owen <sowen@cloudera.com> wrote:
>
> > No, I'm not saying side effects change the count. But not executing
> > the map() function at all certainly has an effect on the side effects
> > of that function: the side effects which should take place never do. I
> > am not sure that is something to be 'fixed'; it's a legitimate
> > question.
> >
> > You can persist an RDD if you do not want to compute it twice.
> >
> > On Sat, Mar 28, 2015 at 1:05 PM, jimfcarroll <jimfcarroll@gmail.com>
> > wrote:
> > > Hi Sean,
> > >
> > > Thanks for the response.
> > >
> > > I can't imagine a case (though my imagination may be somewhat limited)
> > where
> > > even map side effects could change the number of elements in the
> > resulting
> > > map.
> > >
> > > I guess "count" wouldn't officially be an 'action' if it were
> implemented
> > > this way. At least it wouldn't ALWAYS be one.
> > >
> > > My example was contrived. We're passing RDDs to functions. If that RDD
> > is an
> > > instance of my class, then its count() may take a shortcut. If I
> > > map/zip/zipWithIndex/mapPartition/etc. first then I'm stuck with a call
> > that
> > > literally takes 100s to 1000s of times longer (seconds vs hours on some
> > of
> > > our datasets) and since my custom RDDs are immutable they cache the
> count
> > > call so a second invocation is the cost of a method call's overhead.
> > >
> > > I could fix this in Spark if there's any interest in that change.
> > Otherwise
> > > I'll need to overload more RDD methods for my own purposes (like all of
> > the
> > > transformations). Of course, that will be more difficult because those
> > > intermediate classes (like MappedRDD) are private, so I can't extend
> > them.
> > >
> > > Jim
> > >
> > >
> > >
> > >
> > > --
> > > View this message in context:
> >
> http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-count-tp11298p11302.html
> > > Sent from the Apache Spark Developers List mailing list archive at
> > Nabble.com.
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
> >
>

--001a11395a30dc12b8051261b2f1--

From dev-return-12255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 29 01:14:34 2015
Return-Path: <dev-return-12255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B697E173EF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 29 Mar 2015 01:14:34 +0000 (UTC)
Received: (qmail 86570 invoked by uid 500); 29 Mar 2015 01:14:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86501 invoked by uid 500); 29 Mar 2015 01:14:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86490 invoked by uid 99); 29 Mar 2015 01:14:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 01:14:32 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of jimfcarroll@gmail.com does not designate 162.253.133.43 as permitted sender)
Received: from [162.253.133.43] (HELO mwork.nabble.com) (162.253.133.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 01:14:27 +0000
Received: from mben.nabble.com (unknown [162.253.133.72])
	by mwork.nabble.com (Postfix) with ESMTP id 9C3781904BC7
	for <dev@spark.apache.org>; Sat, 28 Mar 2015 18:14:26 -0700 (PDT)
Date: Sat, 28 Mar 2015 18:14:06 -0700 (MST)
From: jimfcarroll <jimfcarroll@gmail.com>
To: dev@spark.apache.org
Message-ID: <1427591646968-11311.post@n3.nabble.com>
In-Reply-To: <CAPh_B=YzPzGqY0Y-Fd8-c9YZPs-0OK5XAsr6FxKwYtupcPn3Kw@mail.gmail.com>
References: <1427504098515-11298.post@n3.nabble.com> <CAMAsSdJ+UmXmAfpBrPOH-DOBwroMLVb2gEawyR-xSuabFKsfbQ@mail.gmail.com> <1427547932037-11302.post@n3.nabble.com> <CAMAsSdL7bi2CODfKvY_oyrJ_cxx5+YR17Kd40qhpgqhZx8huBg@mail.gmail.com> <CACBYxKKocZa0WTR7g9cAHqpvodzpcYHZ6sadCzKGq+uPYisi4Q@mail.gmail.com> <CAPh_B=YzPzGqY0Y-Fd8-c9YZPs-0OK5XAsr6FxKwYtupcPn3Kw@mail.gmail.com>
Subject: Re: RDD.count
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello all,

I worked around this for now using the class (that I already had) that
inherits from RDD and is the one all of our custom RDDs inherit from. I did
the following:

1) Overload all of the transformations (that get used in our app) that don't
change the RDD size wrapping the results with a proxy rdd that intercepts
the count() call returning a cached version or calling an abstract
"calculateSize" if it doesn't already know the count.

2) piggyback a count calculation on all actions that we use (aggregate,
reduce, fold, foreach) so that as a side effect of calling any of these, if
the count isn't already known, it's calculated and stored.

The one thing I couldn't do (at least yet) was get zipWithIndex to calculate
the count because it's implementation is too opaque inside of the RDD.

If anyone wants to see the code I can post it.

Thanks for the responses.

Jim




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RDD-count-tp11298p11311.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 29 20:48:07 2015
Return-Path: <dev-return-12256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E78610762
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 29 Mar 2015 20:48:07 +0000 (UTC)
Received: (qmail 88990 invoked by uid 500); 29 Mar 2015 20:48:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88910 invoked by uid 500); 29 Mar 2015 20:48:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88899 invoked by uid 99); 29 Mar 2015 20:48:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 20:48:05 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dale__r@hotmail.com designates 65.54.190.89 as permitted sender)
Received: from [65.54.190.89] (HELO BAY004-OMC2S14.hotmail.com) (65.54.190.89)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 20:47:39 +0000
Received: from BAY180-W52 ([65.54.190.123]) by BAY004-OMC2S14.hotmail.com over TLS secured channel with Microsoft SMTPSVC(7.5.7601.22751);
	 Sun, 29 Mar 2015 13:47:15 -0700
X-TMN: [km8Tj/RnUl0edbIkDxmGAdJKBbOTx5GFtup4sBB0obU=]
X-Originating-Email: [dale__r@hotmail.com]
Message-ID: <BAY180-W52FC83687C17E905CF0EF0B1F60@phx.gbl>
Content-Type: multipart/alternative;
	boundary="_2fa97c9c-48bf-4448-a80f-299d7b0bac0f_"
From: Dale Richardson <dale__r@hotmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: One corrupt gzip in a directory of 100s
Date: Sun, 29 Mar 2015 20:47:14 +0000
Importance: Normal
MIME-Version: 1.0
X-OriginalArrivalTime: 29 Mar 2015 20:47:15.0612 (UTC) FILETIME=[8A29EDC0:01D06A61]
X-Virus-Checked: Checked by ClamAV on apache.org

--_2fa97c9c-48bf-4448-a80f-299d7b0bac0f_
Content-Type: text/plain; charset="iso-8859-1"
Content-Transfer-Encoding: quoted-printable

Recently had an incident reported to me where somebody was analysing a dire=
ctory of gzipped log files=2C and was struggling to load them into spark be=
cause one of the files was corrupted - calling sc.textFiles('hdfs:///logs/*=
.gz') caused an IOException on the particular executor that was reading tha=
t file=2C which caused the entire job to be cancelled after the retry count=
 was exceeded=2C without any way of catching and recovering from the error.=
  While normally I think it is entirely appropriate to stop execution if so=
mething is wrong with your input=2C sometimes it is useful to analyse what =
you can get (as long as you are aware that input has been skipped)=2C and t=
reat corrupt files as acceptable losses.
To cater for this particular case I've added SPARK-6593 (PR at https://gith=
ub.com/apache/spark/pull/5250). Which adds an option (spark.hadoop.ignoreIn=
putErrors) to log exceptions raised by the hadoop Input format=2C but to co=
ntinue on with the next task.
Ideally in this case you would want to report the corrupt file paths back t=
o the master so they could be dealt with in a particular way (eg moved to a=
 separate directory)=2C but that would require a public API change/addition=
. I was pondering on an addition to Spark's hadoop API that could report pr=
ocessing status back to the master via an optional accumulator that collect=
s filepath/Option(exception message) tuples so the user has some idea of wh=
at files are being processed=2C and what files are being skipped.
Regards=2CDale. 		 	   		  =

--_2fa97c9c-48bf-4448-a80f-299d7b0bac0f_--

From dev-return-12257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 29 21:29:44 2015
Return-Path: <dev-return-12257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 112E6108B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 29 Mar 2015 21:29:44 +0000 (UTC)
Received: (qmail 24895 invoked by uid 500); 29 Mar 2015 21:29:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24814 invoked by uid 500); 29 Mar 2015 21:29:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24802 invoked by uid 99); 29 Mar 2015 21:29:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 21:29:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mchettiar@rocketfuelinc.com designates 209.85.214.170 as permitted sender)
Received: from [209.85.214.170] (HELO mail-ob0-f170.google.com) (209.85.214.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 21:29:37 +0000
Received: by obcjt1 with SMTP id jt1so106400850obc.2
        for <dev@spark.apache.org>; Sun, 29 Mar 2015 14:29:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=rocketfuelinc.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=PvVE/4Qx/XbWEYm2IUMTuVo9r+DvjKu/B4ZSbipBFsk=;
        b=Xtuy0W4tyu4HngrxUMUL6HlFT9CwG2XuS8UFSkwlagcQiKUy34hzR2MgsS4Cd77E1R
         j2wA1HTcN5slbIQRRajXZ4z9snMGIWiPY2EFf3JOPEo3ZCdiUq4L4SJ1fjEMuBOHkeKl
         DCM1Ik+DE6dsu/BO0twDosONEQ3gVXDauNE8E=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=PvVE/4Qx/XbWEYm2IUMTuVo9r+DvjKu/B4ZSbipBFsk=;
        b=N5Ofymb+L73H0V5Jvfhtr94s/VdBfOdbD6K4D6Vv8JGU9fnAjY5Ax+ZYwAm1d261R5
         a6nfIpSsMmHRdqKPQEVxo4eV9q7Eb1Ke90fsm0PgedkXCn4TATZvCzF8UBJ8RztAUweq
         +OC2OLanpx2D7GWnQPEdMQ700Vn1g0rbmlVlo4VXFnTJJRH20E7wVDh7WmjtM3WUYbRa
         NRei17qlFwv3qxtPrLhBep6JSa51D7vOoyeyrM4KVyHGtJHkfRTA1JW3EtYDC5nO0pOg
         35djUuX4mWYqnyBsGkhW4hLoK51DUwhH/+au84yJI7EQPpQz9NFGxiVB3qtTbaYgEO0p
         Ki4w==
X-Gm-Message-State: ALoCoQmOSlIdM+PUQl0MK3XtkKPIZBwLHBbeLCwi9x7T1AW3QtvFobm6EE3OAMUK0Y+cSRerYrzZ
MIME-Version: 1.0
X-Received: by 10.202.211.11 with SMTP id k11mr23347418oig.132.1427664556450;
 Sun, 29 Mar 2015 14:29:16 -0700 (PDT)
Received: by 10.76.158.195 with HTTP; Sun, 29 Mar 2015 14:29:16 -0700 (PDT)
In-Reply-To: <CAMAsSd+Ko=QFuSg2OkQ0b_KTuopGCg1CcJd0P0DEAoOky1su9w@mail.gmail.com>
References: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
	<CAMAsSdJoswwE0BOA114FqoEQTEXbusvtB7ken_8wLnKzNRLQ8w@mail.gmail.com>
	<CAG6LhydcVBUQF6-yJoFRQVzYZaCqJtbREt_3MOPfed5+8WfRhA@mail.gmail.com>
	<CAMAsSd+Ko=QFuSg2OkQ0b_KTuopGCg1CcJd0P0DEAoOky1su9w@mail.gmail.com>
Date: Sun, 29 Mar 2015 14:29:16 -0700
Message-ID: <CAG6LhydibT-FDdBkhxb6yKZKYN_baOY-c_jDNiRE5BOzA2QpeA@mail.gmail.com>
Subject: Re: Building spark 1.2 from source requires more dependencies
From: Pala M Muthaia <mchettiar@rocketfuelinc.com>
To: Sean Owen <sowen@cloudera.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113b1acebf20660512740e03
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b1acebf20660512740e03
Content-Type: text/plain; charset=UTF-8

Sean,

I did a mvn clean and then build, it produces the same error. I also did a
fresh git clone of spark and invoked the same build command and it resulted
in identical error (I also had a colleague do a same thing, lest there was
some machine specific issue, and saw the same error). Unless i
misunderstood something, it doesn't look like clean build fixes this.

On Fri, Mar 27, 2015 at 10:20 PM, Sean Owen <sowen@cloudera.com> wrote:

> This is not a compile error, but an error from the scalac compiler.
> That is, the code and build are fine, but scalac is not compiling it.
> Usually when this happens, a clean build fixes it.
>
> On Fri, Mar 27, 2015 at 7:09 PM, Pala M Muthaia
> <mchettiar@rocketfuelinc.com> wrote:
> > No, i am running from the root directory, parent of core.
> >
> > Here is the first set of errors that i see when i compile from source
> (sorry
> > the error message is very long, but adding it in case it helps in
> > diagnosis). After i manually add javax.servlet dependency for  version
> 3.0,
> > these set of errors go away and i get the next set of errors about
> missing
> > classes under eclipse-jetty.
> >
> > I am on maven 3.2.5 and java 1.7.
> >
> > Error:
> >
> > [INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
> > spark-core_2.10 ---
> > [WARNING] Zinc server is not available at port 3030 - reverting to normal
> > incremental compile
> > [INFO] Using incremental compilation
> > [INFO] compiler plugin:
> > BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
> > [INFO] Compiling 403 Scala sources and 33 Java sources to
> > /Users/mchettiar/code/spark/core/target/scala-2.10/classes...
> > [WARNING] Class javax.servlet.ServletException not found - continuing
> with a
> > stub.
> > [ERROR]
> >      while compiling:
> >
> /Users/mchettiar/code/spark/core/src/main/scala/org/apache/spark/HttpServer.scala
> >         during phase: typer
> >      library version: version 2.10.4
> >     compiler version: version 2.10.4
> >   reconstructed args: -deprecation -feature
> > -
>

--001a113b1acebf20660512740e03--

From dev-return-12258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Mar 29 21:37:33 2015
Return-Path: <dev-return-12258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06B8A108DD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 29 Mar 2015 21:37:33 +0000 (UTC)
Received: (qmail 30584 invoked by uid 500); 29 Mar 2015 21:37:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30500 invoked by uid 500); 29 Mar 2015 21:37:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30489 invoked by uid 99); 29 Mar 2015 21:37:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 21:37:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 29 Mar 2015 21:37:22 +0000
Received: by wibgn9 with SMTP id gn9so100611893wib.1
        for <dev@spark.apache.org>; Sun, 29 Mar 2015 14:36:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=CyX3Q33yzG4eitdOyHMvyv534oCqH7VcjdsQDGWZu78=;
        b=ZXqYBI56G/jOofmFTmb0roSYch4ZMLkdCMP/s5p8sgRNl97LCUiOGfzdILW8kgpULO
         NbXV/0B/YiVt3OScICO30B8iBMT0ao1Ox2TzIjuxl5Kh+8fLWTk5bHZlNy4Mdl8h/PL+
         368YxprYb2HHtbm5mfRJh2n28/FD+L6wuN0h9+yuDFYHoh2KNyW8e/uNjautPhu6cLza
         ekPzARnpfq61rE3Q4YcoPmAeJvwMHozsDaIpU/+doxX+R0kvs1qS2fRDSa3PPDU7X1h9
         +y2BXrIiqPAxSMoWYCezCfx38hSr8vjobkTSlIdv0tndI5c5nmzwm82P7mV8YvMgZAfj
         2gjA==
X-Gm-Message-State: ALoCoQmvjXJ9lsRrrZHE90UrHLFLrrrO63YJmDG8eMh2fZizpVSF4ASGTvtdPp06oQLEo+FxyA6F
X-Received: by 10.180.221.71 with SMTP id qc7mr4429649wic.39.1427664976700;
 Sun, 29 Mar 2015 14:36:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.27.81.86 with HTTP; Sun, 29 Mar 2015 14:35:56 -0700 (PDT)
In-Reply-To: <CAG6LhydibT-FDdBkhxb6yKZKYN_baOY-c_jDNiRE5BOzA2QpeA@mail.gmail.com>
References: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
 <CAMAsSdJoswwE0BOA114FqoEQTEXbusvtB7ken_8wLnKzNRLQ8w@mail.gmail.com>
 <CAG6LhydcVBUQF6-yJoFRQVzYZaCqJtbREt_3MOPfed5+8WfRhA@mail.gmail.com>
 <CAMAsSd+Ko=QFuSg2OkQ0b_KTuopGCg1CcJd0P0DEAoOky1su9w@mail.gmail.com> <CAG6LhydibT-FDdBkhxb6yKZKYN_baOY-c_jDNiRE5BOzA2QpeA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 29 Mar 2015 22:35:56 +0100
Message-ID: <CAMAsSd+ey=UACjKxooGgAMO8pDdrt-2bdaE84A2L5UxGsW-tBQ@mail.gmail.com>
Subject: Re: Building spark 1.2 from source requires more dependencies
To: Pala M Muthaia <mchettiar@rocketfuelinc.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Given that's it's an internal error from scalac, I think it may be
something to take up with the Scala folks to really fix. We can just
look for workarounds. Try blowing away your .m2 and .ivy cache for
example. FWIW I was running on Linux with Java 8u31, latest scala 2.11
AFAIK.

On Sun, Mar 29, 2015 at 10:29 PM, Pala M Muthaia
<mchettiar@rocketfuelinc.com> wrote:
> Sean,
>
> I did a mvn clean and then build, it produces the same error. I also did a
> fresh git clone of spark and invoked the same build command and it resulted
> in identical error (I also had a colleague do a same thing, lest there was
> some machine specific issue, and saw the same error). Unless i misunderstood
> something, it doesn't look like clean build fixes this.
>
> On Fri, Mar 27, 2015 at 10:20 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> This is not a compile error, but an error from the scalac compiler.
>> That is, the code and build are fine, but scalac is not compiling it.
>> Usually when this happens, a clean build fixes it.
>>
>> On Fri, Mar 27, 2015 at 7:09 PM, Pala M Muthaia
>> <mchettiar@rocketfuelinc.com> wrote:
>> > No, i am running from the root directory, parent of core.
>> >
>> > Here is the first set of errors that i see when i compile from source
>> > (sorry
>> > the error message is very long, but adding it in case it helps in
>> > diagnosis). After i manually add javax.servlet dependency for  version
>> > 3.0,
>> > these set of errors go away and i get the next set of errors about
>> > missing
>> > classes under eclipse-jetty.
>> >
>> > I am on maven 3.2.5 and java 1.7.
>> >
>> > Error:
>> >
>> > [INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
>> > spark-core_2.10 ---
>> > [WARNING] Zinc server is not available at port 3030 - reverting to
>> > normal
>> > incremental compile
>> > [INFO] Using incremental compilation
>> > [INFO] compiler plugin:
>> > BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
>> > [INFO] Compiling 403 Scala sources and 33 Java sources to
>> > /Users/mchettiar/code/spark/core/target/scala-2.10/classes...
>> > [WARNING] Class javax.servlet.ServletException not found - continuing
>> > with a
>> > stub.
>> > [ERROR]
>> >      while compiling:
>> >
>> > /Users/mchettiar/code/spark/core/src/main/scala/org/apache/spark/HttpServer.scala
>> >         during phase: typer
>> >      library version: version 2.10.4
>> >     compiler version: version 2.10.4
>> >   reconstructed args: -deprecation -feature
>> > -
>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 04:09:51 2015
Return-Path: <dev-return-12259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8EE621720E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 04:09:51 +0000 (UTC)
Received: (qmail 95493 invoked by uid 500); 30 Mar 2015 04:09:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95213 invoked by uid 500); 30 Mar 2015 04:09:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94398 invoked by uid 99); 30 Mar 2015 04:09:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 04:09:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of niranda.perera@gmail.com designates 209.85.218.41 as permitted sender)
Received: from [209.85.218.41] (HELO mail-oi0-f41.google.com) (209.85.218.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 04:09:40 +0000
Received: by oifl3 with SMTP id l3so117656894oif.0;
        Sun, 29 Mar 2015 21:09:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=5iWPOZzAUR8Bt3+kUEOaMi7IwcViM3bzW/iwYZ4DEQg=;
        b=eDeOcZXWMV4QxKgMHOUm7ufGnr9ekpacIyAHnUwkCXHXfRbJAD+R58G7JSvJvscGY7
         B3MiN/mOHm8+7SknV5MUyXLYdrbU9gjFQ4T0mNukzmKpm3LRHMwtbrUZELfjddWNRRYq
         W62ohtmOM3HiVFbd+EjQhhcddSwY+jhLc2yi2UNqezOt8PQQTgxQsMAv0hLp0TPyhqno
         JG7yCu75sc4za6Xupl+rWA9PdsTTG42pbyETgK+AD+XDx4vbAYIXa9F14RZVbo5qvTtd
         uCtnFGksISV/6Hbn+eu9G48h0MmtbFd8/xO/L6CttxnJ1Wsf4f88HBmmWZwlytZ0t9i6
         7nhw==
MIME-Version: 1.0
X-Received: by 10.202.169.140 with SMTP id s134mr2933644oie.94.1427688560301;
 Sun, 29 Mar 2015 21:09:20 -0700 (PDT)
Received: by 10.182.171.65 with HTTP; Sun, 29 Mar 2015 21:09:20 -0700 (PDT)
Date: Mon, 30 Mar 2015 09:39:20 +0530
Message-ID: <CANCoaU5sX6B7EA4134kC8rfWgTdgMJxfq74am0eXpCLTqqPNcQ@mail.gmail.com>
Subject: What is the meaning to of 'STATE' in a worker/ an executor?
From: Niranda Perera <niranda.perera@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113cd4f67c9b6d051279a56e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113cd4f67c9b6d051279a56e
Content-Type: text/plain; charset=UTF-8

Hi,

I have noticed in the Spark UI, workers and executors run on several
states, ALIVE, LOADING, RUNNING, DEAD etc?

What exactly are these states mean and what is the effect it has on working
with those executor?
ex: whether an executor can not be used in the loading state, etc

cheers

-- 
Niranda

--001a113cd4f67c9b6d051279a56e--

From dev-return-12260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 04:18:01 2015
Return-Path: <dev-return-12260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A1421722B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 04:18:01 +0000 (UTC)
Received: (qmail 9659 invoked by uid 500); 30 Mar 2015 04:17:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9298 invoked by uid 500); 30 Mar 2015 04:17:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9287 invoked by uid 99); 30 Mar 2015 04:17:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 04:17:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 04:17:48 +0000
Received: by pdcp1 with SMTP id p1so71131652pdc.3
        for <dev@spark.apache.org>; Sun, 29 Mar 2015 21:17:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:references:mime-version:in-reply-to:content-type
         :content-transfer-encoding:message-id:cc:from:subject:date:to;
        bh=dli7HXl0DxR7jt+ARHndoLDUqBGnoWwm75ZvHwewKtI=;
        b=dvejg5JCabcjxK5IRnGdhXlMdFRX5LvwAu8rd86HvF18x3X/myak0r3EZpnXe8a8eT
         GT0hQ8Ztr4e5op++h3qujqClqeWDVZOOmdugamtJS5K/hybLIPXQTkJgLCAhho0J/x4U
         61DWrOA9uKJRW+2UtQOwKn/4Q7o3I4NRLv8z2KHAugP430+/9HU2P37AgUMqq9LpfeN+
         PunCeP+8scAmMURazjpoNDrQgwTj34J8Anmd8MhU/hNGunI/I0cWLnolDkJ5zsQwFCt5
         mY51+dTSVzTHwpVSSJOtv2WX11z1sKazfWFm+oOJrBsMRXU0gE4MA2F1Ej32UnznREMF
         GADw==
X-Gm-Message-State: ALoCoQkrI4ZrUVjUWJcGZknIobFvjJCTnM2GnyuzTTCOCtJ04tViiiZKbytcUMNFa+cQ203Pwq5k
X-Received: by 10.66.154.162 with SMTP id vp2mr54429869pab.73.1427689047832;
        Sun, 29 Mar 2015 21:17:27 -0700 (PDT)
Received: from [10.10.1.109] (50-0-150-227.dsl.static.fusionbroadband.com. [50.0.150.227])
        by mx.google.com with ESMTPSA id d13sm8899016pdj.92.2015.03.29.21.17.25
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 29 Mar 2015 21:17:26 -0700 (PDT)
References: <CANCoaU5sX6B7EA4134kC8rfWgTdgMJxfq74am0eXpCLTqqPNcQ@mail.gmail.com>
Mime-Version: 1.0 (1.0)
In-Reply-To: <CANCoaU5sX6B7EA4134kC8rfWgTdgMJxfq74am0eXpCLTqqPNcQ@mail.gmail.com>
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: quoted-printable
Message-Id: <F191B52B-9E76-4ECD-8C4C-6357F82C70F7@clearstorydata.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>,
 "user@spark.apache.org" <user@spark.apache.org>
X-Mailer: iPad Mail (12D508)
From: Mark Hamstra <mark@clearstorydata.com>
Subject: Re: What is the meaning to of 'STATE' in a worker/ an executor?
Date: Sun, 29 Mar 2015 21:17:24 -0700
To: Niranda Perera <niranda.perera@gmail.com>
X-Virus-Checked: Checked by ClamAV on apache.org

A LOADING Executor is on the way to RUNNING, but hasn't yet been registered w=
ith the Master, so it isn't quite ready to do useful work.


> On Mar 29, 2015, at 9:09 PM, Niranda Perera <niranda.perera@gmail.com> wro=
te:
>=20
> Hi,=20
>=20
> I have noticed in the Spark UI, workers and executors run on several state=
s, ALIVE, LOADING, RUNNING, DEAD etc?
>=20
> What exactly are these states mean and what is the effect it has on workin=
g with those executor?
> ex: whether an executor can not be used in the loading state, etc
>=20
> cheers
>=20
> --=20
> Niranda=20

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 07:54:09 2015
Return-Path: <dev-return-12261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A0EFB176E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 07:54:09 +0000 (UTC)
Received: (qmail 31140 invoked by uid 500); 30 Mar 2015 07:53:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31071 invoked by uid 500); 30 Mar 2015 07:53:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31058 invoked by uid 99); 30 Mar 2015 07:53:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 07:53:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of saucam@gmail.com designates 209.85.220.42 as permitted sender)
Received: from [209.85.220.42] (HELO mail-pa0-f42.google.com) (209.85.220.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 07:53:08 +0000
Received: by pacgg7 with SMTP id gg7so30500297pac.0
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 00:51:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bEtSvl+D1ZVHdM3Kbt+MQfwu8qXX9VoSC9hYjHMnDl8=;
        b=qaESPzE6DC/cg7H7eDFJJuiECDAAuWvoTvEicYJoleT9JqCZklionf+9HZ0HYOg6ip
         tLGGCdDfWKj1kb6PwjnEhofMafMyjSQw1FZHncESNdeFspzJbGuSKI7hPYaYXYrbYhXK
         cedOId2hns2SPelFX9wRiBV1ApUQcNMOZUhCP0XL2ALyoO/Vor8IbBgqb0OYB1I1JksT
         5s9yBF4W+ga/IwX89K6HPfnMv4Z3+B2F5JzXP1MBRIiIDEUy0gG7m0nWgq9bP9zDuAS5
         mt/IQvI/oVLiY/A+xnlaZNAFdgE96fph/BMNuaJnDcRHuEpNQee2pc4lrFgR9kifErli
         4fJQ==
MIME-Version: 1.0
X-Received: by 10.66.221.135 with SMTP id qe7mr55288438pac.97.1427701896552;
 Mon, 30 Mar 2015 00:51:36 -0700 (PDT)
Received: by 10.70.19.35 with HTTP; Mon, 30 Mar 2015 00:51:36 -0700 (PDT)
In-Reply-To: <CAMAsSd+ey=UACjKxooGgAMO8pDdrt-2bdaE84A2L5UxGsW-tBQ@mail.gmail.com>
References: <CAG6LhydpTX3jWVtk6XuoW=fD=sVsxADKKcTHx+THcF0M8+u3Tw@mail.gmail.com>
	<CAMAsSdJoswwE0BOA114FqoEQTEXbusvtB7ken_8wLnKzNRLQ8w@mail.gmail.com>
	<CAG6LhydcVBUQF6-yJoFRQVzYZaCqJtbREt_3MOPfed5+8WfRhA@mail.gmail.com>
	<CAMAsSd+Ko=QFuSg2OkQ0b_KTuopGCg1CcJd0P0DEAoOky1su9w@mail.gmail.com>
	<CAG6LhydibT-FDdBkhxb6yKZKYN_baOY-c_jDNiRE5BOzA2QpeA@mail.gmail.com>
	<CAMAsSd+ey=UACjKxooGgAMO8pDdrt-2bdaE84A2L5UxGsW-tBQ@mail.gmail.com>
Date: Mon, 30 Mar 2015 13:21:36 +0530
Message-ID: <CAJdVUwWb702naRAtEToo_J+m8jFqii+5SoHwMy5-h07GYH0z3A@mail.gmail.com>
Subject: Re: Building spark 1.2 from source requires more dependencies
From: yash datta <saucam@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b5d86d363ab6205127cc090
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d86d363ab6205127cc090
Content-Type: text/plain; charset=UTF-8

Hi all,


When selecting large data in sparksql (Select * query) , I see Buffer
overflow exception from kryo :


15/03/27 10:32:19 WARN scheduler.TaskSetManager: Lost task 6.0 in stage 3.0
(TID 30, machine159): com.esotericsoftware.kryo.KryoException: Buffer
overflow. Available: 1, required: 2
Serialization trace:
values (org.apache.spark.sql.catalyst.expressions.GenericRow)
        at com.esotericsoftware.kryo.io.Output.require(Output.java:138)
        at com.esotericsoftware.kryo.io.Output.writeInt(Output.java:247)
        at
com.esotericsoftware.kryo.serializers.DefaultSerializers$IntSerializer.write(DefaultSerializers.java:95)
        at
com.esotericsoftware.kryo.serializers.DefaultSerializers$IntSerializer.write(DefaultSerializers.java:89)
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568)
        at
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:318)
        at
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:293)
        at com.esotericsoftware.kryo.Kryo.writeObject(Kryo.java:501)
        at
com.esotericsoftware.kryo.serializers.FieldSerializer$ObjectField.write(FieldSerializer.java:564)
        at
com.esotericsoftware.kryo.serializers.FieldSerializer.write(FieldSerializer.java:213)
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568)
        at
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:318)
        at
com.esotericsoftware.kryo.serializers.DefaultArraySerializers$ObjectArraySerializer.write(DefaultArraySerializers.java:293)
        at com.esotericsoftware.kryo.Kryo.writeClassAndObject(Kryo.java:568)
        at
org.apache.spark.serializer.KryoSerializerInstance.serialize(KryoSerializer.scala:167)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:210)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown
Source)
        at java.lang.Thread.run(Unknown Source)



I thought maybe increasing these would resolve the problem, but the same
exception is seen :

set spark.kryoserializer.buffer.mb=4;
set spark.kryoserializer.buffer.max.mb=1024;


I have a parquet table with 5 Int columns , 100 million rows.

Can somebody guide why this exception is seen, am I missing some
configuration ?

Thanks
Yash


On Mon, Mar 30, 2015 at 3:05 AM, Sean Owen <sowen@cloudera.com> wrote:

> Given that's it's an internal error from scalac, I think it may be
> something to take up with the Scala folks to really fix. We can just
> look for workarounds. Try blowing away your .m2 and .ivy cache for
> example. FWIW I was running on Linux with Java 8u31, latest scala 2.11
> AFAIK.
>
> On Sun, Mar 29, 2015 at 10:29 PM, Pala M Muthaia
> <mchettiar@rocketfuelinc.com> wrote:
> > Sean,
> >
> > I did a mvn clean and then build, it produces the same error. I also did
> a
> > fresh git clone of spark and invoked the same build command and it
> resulted
> > in identical error (I also had a colleague do a same thing, lest there
> was
> > some machine specific issue, and saw the same error). Unless i
> misunderstood
> > something, it doesn't look like clean build fixes this.
> >
> > On Fri, Mar 27, 2015 at 10:20 PM, Sean Owen <sowen@cloudera.com> wrote:
> >>
> >> This is not a compile error, but an error from the scalac compiler.
> >> That is, the code and build are fine, but scalac is not compiling it.
> >> Usually when this happens, a clean build fixes it.
> >>
> >> On Fri, Mar 27, 2015 at 7:09 PM, Pala M Muthaia
> >> <mchettiar@rocketfuelinc.com> wrote:
> >> > No, i am running from the root directory, parent of core.
> >> >
> >> > Here is the first set of errors that i see when i compile from source
> >> > (sorry
> >> > the error message is very long, but adding it in case it helps in
> >> > diagnosis). After i manually add javax.servlet dependency for  version
> >> > 3.0,
> >> > these set of errors go away and i get the next set of errors about
> >> > missing
> >> > classes under eclipse-jetty.
> >> >
> >> > I am on maven 3.2.5 and java 1.7.
> >> >
> >> > Error:
> >> >
> >> > [INFO] --- scala-maven-plugin:3.2.0:compile (scala-compile-first) @
> >> > spark-core_2.10 ---
> >> > [WARNING] Zinc server is not available at port 3030 - reverting to
> >> > normal
> >> > incremental compile
> >> > [INFO] Using incremental compilation
> >> > [INFO] compiler plugin:
> >> > BasicArtifact(org.scalamacros,paradise_2.10.4,2.0.1,null)
> >> > [INFO] Compiling 403 Scala sources and 33 Java sources to
> >> > /Users/mchettiar/code/spark/core/target/scala-2.10/classes...
> >> > [WARNING] Class javax.servlet.ServletException not found - continuing
> >> > with a
> >> > stub.
> >> > [ERROR]
> >> >      while compiling:
> >> >
> >> >
> /Users/mchettiar/code/spark/core/src/main/scala/org/apache/spark/HttpServer.scala
> >> >         during phase: typer
> >> >      library version: version 2.10.4
> >> >     compiler version: version 2.10.4
> >> >   reconstructed args: -deprecation -feature
> >> > -
> >
> >
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>


-- 
When events unfold with calm and ease
When the winds that blow are merely breeze
Learn from nature, from birds and bees
Live your life in love, and let joy not cease.

--047d7b5d86d363ab6205127cc090--

From dev-return-12262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 12:03:10 2015
Return-Path: <dev-return-12262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 109B617E8C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 12:03:10 +0000 (UTC)
Received: (qmail 64699 invoked by uid 500); 30 Mar 2015 12:03:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64612 invoked by uid 500); 30 Mar 2015 12:03:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64600 invoked by uid 99); 30 Mar 2015 12:03:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 12:03:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of petro.rudenko@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 12:03:03 +0000
Received: by wibg7 with SMTP id g7so100227642wib.1
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 05:02:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=to:subject:from:message-id:date:user-agent:mime-version
         :content-type;
        bh=NF4HEiec1dHw6kTfMZjL3CXNlPrqO5LnrLB6zRHe3XQ=;
        b=BuNOyM5f85Xi4nllwvRLhI5Eez1oD8QzQfNBDAszJlN4MsMbEi2retigjfJOHcEL1k
         Cx6FfOvDUi0dXt21aQ3njAVaA/BNDN3p0gBp+vNI6v+kQVvK5bN7CorfWCO9wvA/QNmw
         tEKi4NLsihN1tqmE/g0LBsWFeBtLi9zRHVbGKXKYE4crSVJGDmOaYIF6Mn1n8pRdsYrd
         acTvfl09e9GxGd3i2wDkpmdc70SIf7sq7XHwr2Ful6/ITraxc1u4QJwgNRw4/04clGXk
         lKyUAmMo1WpTXIr3qmoE0BSCyzX8mHLSAoyOcdaRYdYHscVBBhJIEtmFkyqFeqZy0gRc
         1L/A==
X-Received: by 10.181.23.200 with SMTP id ic8mr22198522wid.53.1427716962331;
        Mon, 30 Mar 2015 05:02:42 -0700 (PDT)
Received: from [192.168.1.126] (130-61-207-82.pool.ukrtel.net. [82.207.61.130])
        by mx.google.com with ESMTPSA id ff4sm5471681wib.9.2015.03.30.05.02.40
        for <dev@spark.apache.org>
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 30 Mar 2015 05:02:41 -0700 (PDT)
To: dev@spark.apache.org
Subject: [sql] How to uniquely identify Dataframe?
From: Peter Rudenko <petro.rudenko@gmail.com>
message-id: <55193B5D.5060801@gmail.com>
Date: Mon, 30 Mar 2015 15:02:37 +0300
user-agent:
 Mozilla/5.0 (X11; Linux x86_64; rv:37.0) Gecko/20100101 Thunderbird/37.0a2
mime-version: 1.0
Content-Type: multipart/alternative;
 boundary="------------000404090606050701050601"
X-Virus-Checked: Checked by ClamAV on apache.org

--------------000404090606050701050601
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit

Hi i have some custom caching logic in my application. I need to 
identify somehow Dataframe, to check whether i saw it previously. Here’s 
a problem:

|scala> val data = sc.parallelize(1 to 1000) data: 
org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize 
at <console>:21 scala> data.id res0: Int = 0 scala> data.id res1: Int = 
0 scala> val dataDF = data.toDF dataDF: org.apache.spark.sql.DataFrame = 
[_1: int] scala> dataDF.rdd.id res3: Int = 2 scala> dataDF.rdd.id res4: 
Int = 3 |

For some reason it generates a new ID on each call. With schemaRDD i was 
able to call SchemaRDD.id.

Thanks,
Peter Rudenko

​

--------------000404090606050701050601--

From dev-return-12263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 12:40:03 2015
Return-Path: <dev-return-12263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8887217213
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 12:40:03 +0000 (UTC)
Received: (qmail 78612 invoked by uid 500); 30 Mar 2015 12:39:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78534 invoked by uid 500); 30 Mar 2015 12:39:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78522 invoked by uid 99); 30 Mar 2015 12:39:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 12:39:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 12:39:47 +0000
Received: by pdnc3 with SMTP id c3so174742391pdn.0
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 05:38:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=message-id:date:from:user-agent:mime-version:to:subject:references
         :in-reply-to:content-type:content-transfer-encoding;
        bh=/49cBkKw/KuYW419kOXuHdoepX2F6TIuatMK60cM0cM=;
        b=hV2Olk9pC/9JqUr07bqh9LkSGGv4mtYAVJGhF9wRxkPodFpcaRWu/bLTD5OPaUcyx3
         ftEUQZ9zBJsxfW2dLzgjIdR4RaYTXFoOpdDUgGr5T2JFt9iAfz95XdlYKarqVEgBjflP
         SeqqcrZS45b/BC4B61knXynboOxvMJENyvEkrVNGsHZ73dMgo8IOidOQGtoXem8Tl+xX
         Azvvaokrmb+9GUCJDEBlBdkZKhkT4YRJDkqX8AQtHDgF65cRtiG1MOT4QVd3gh02OvEJ
         N/NZ9buCGqhYwGbiZ/6DfoPEcKZsLklTuqAgOLf6/+AZvZ+NpewHPJWMkwfv+vRT1cNN
         hJMA==
X-Received: by 10.68.68.202 with SMTP id y10mr13712321pbt.34.1427719122789;
        Mon, 30 Mar 2015 05:38:42 -0700 (PDT)
Received: from [192.168.10.2] (c-50-131-222-227.hsd1.ca.comcast.net. [50.131.222.227])
        by mx.google.com with ESMTPSA id qh9sm7626849pbc.24.2015.03.30.05.38.39
        (version=TLSv1.2 cipher=ECDHE-RSA-AES128-GCM-SHA256 bits=128/128);
        Mon, 30 Mar 2015 05:38:42 -0700 (PDT)
Message-ID: <551943CD.6070706@gmail.com>
Date: Mon, 30 Mar 2015 20:38:37 +0800
From: Cheng Lian <lian.cs.zju@gmail.com>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:31.0) Gecko/20100101 Thunderbird/31.5.0
MIME-Version: 1.0
To: Peter Rudenko <petro.rudenko@gmail.com>, dev@spark.apache.org
Subject: Re: [sql] How to uniquely identify Dataframe?
References: <55193B5D.5060801@gmail.com>
In-Reply-To: <55193B5D.5060801@gmail.com>
Content-Type: text/plain; charset=utf-8; format=flowed
Content-Transfer-Encoding: 8bit
X-Virus-Checked: Checked by ClamAV on apache.org

This is because unlike SchemaRDD, DataFrame itself is no longer an RDD 
now. In the meanwhile, DataFrame.rdd is a function, which always returns 
a new RDD. I think you may use DataFrame.queryExecution.logical (the 
logical plan) as an ID. Maybe we should make it a "lazy val" rather than 
a "def". Personally I don't find a good reason that it has to be a 
"def", but maybe I miss something here.

Filed JIRA ticket and PR for this:

- https://issues.apache.org/jira/browse/SPARK-6608
- https://github.com/apache/spark/pull/5265

Cheng

On 3/30/15 8:02 PM, Peter Rudenko wrote:
> Hi i have some custom caching logic in my application. I need to 
> identify somehow Dataframe, to check whether i saw it previously. 
> Here’s a problem:
>
> |scala> val data = sc.parallelize(1 to 1000) data: 
> org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at 
> parallelize at <console>:21 scala> data.id res0: Int = 0 scala> 
> data.id res1: Int = 0 scala> val dataDF = data.toDF dataDF: 
> org.apache.spark.sql.DataFrame = [_1: int] scala> dataDF.rdd.id res3: 
> Int = 2 scala> dataDF.rdd.id res4: Int = 3 |
>
> For some reason it generates a new ID on each call. With schemaRDD i 
> was able to call SchemaRDD.id.
>
> Thanks,
> Peter Rudenko
>
> ​
>


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 16:33:57 2015
Return-Path: <dev-return-12264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 64E6917D6D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 16:33:57 +0000 (UTC)
Received: (qmail 14762 invoked by uid 500); 30 Mar 2015 16:33:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14686 invoked by uid 500); 30 Mar 2015 16:33:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14672 invoked by uid 99); 30 Mar 2015 16:33:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 16:33:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 16:33:09 +0000
Received: by qgep97 with SMTP id p97so193510048qge.1
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 09:32:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ea5FIrGt+kU+4XZnVzJ+VjszU5zq8JdA69WMj2uBne0=;
        b=Ik++YvPwImjeibyZGPOc1s25ROyUSggj+NR1WgRv6+Tc/9rIkUeEgIOH3KRCP7ISj2
         hDYDJ6aS1Wz0BtOBG8RdUcKHvd3wUVUN99W61dyO84TE/AY6eimK32B2PvlsQzPUetOL
         KOV5SAyRvoXBQN3+LmPeLc5D5/GRIhxkRs/mmZTvSNPJ+ADD8S7QNXldlhPlNL6aRfit
         Rn8VIkpHApAhBJEB9/6SmjRe0ezKDVsqi5AF+ZYG6F5oB+PCKwZyVlFq0TwMlfc0qhSn
         p8Y+acDjosiXFfm4gGYI+pxoYzEFjbVyBYuqutXogFxo2ykaLEijQ75mjdOoohs7R1FX
         WIXA==
X-Gm-Message-State: ALoCoQkMAqqCnFq5UCaDWJw2RhPTTJ0VvXd6MmQJpuym+KH7YRBmPKMZ/plW6eJ5U4RXogHFjwaP
X-Received: by 10.55.52.77 with SMTP id b74mr12122289qka.78.1427733121896;
 Mon, 30 Mar 2015 09:32:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Mon, 30 Mar 2015 09:31:41 -0700 (PDT)
In-Reply-To: <551943CD.6070706@gmail.com>
References: <55193B5D.5060801@gmail.com> <551943CD.6070706@gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 30 Mar 2015 09:31:41 -0700
Message-ID: <CAPh_B=YZ05dw+d4iH-DYMyetE1NQzGeG0pyvBG-KQ5ARK+pemw@mail.gmail.com>
Subject: Re: [sql] How to uniquely identify Dataframe?
To: Cheng Lian <lian.cs.zju@gmail.com>
Cc: Peter Rudenko <petro.rudenko@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11490e4290a95f051284059a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11490e4290a95f051284059a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The only reason I can think of right now is that you might want to change
the config parameter to change the behavior of the optimizer and regenerate
the plan. However, maybe that's not a strong enough reasons to regenerate
the RDD everytime.


On Mon, Mar 30, 2015 at 5:38 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:

> This is because unlike SchemaRDD, DataFrame itself is no longer an RDD
> now. In the meanwhile, DataFrame.rdd is a function, which always returns =
a
> new RDD. I think you may use DataFrame.queryExecution.logical (the
> logical plan) as an ID. Maybe we should make it a "lazy val" rather than =
a
> "def". Personally I don't find a good reason that it has to be a "def", b=
ut
> maybe I miss something here.
>
> Filed JIRA ticket and PR for this:
>
> - https://issues.apache.org/jira/browse/SPARK-6608
> - https://github.com/apache/spark/pull/5265
>
> Cheng
>
>
> On 3/30/15 8:02 PM, Peter Rudenko wrote:
>
>> Hi i have some custom caching logic in my application. I need to identif=
y
>> somehow Dataframe, to check whether i saw it previously. Here=E2=80=99s =
a problem:
>>
>> |scala> val data =3D sc.parallelize(1 to 1000) data:
>> org.apache.spark.rdd.RDD[Int] =3D ParallelCollectionRDD[0] at paralleliz=
e at
>> <console>:21 scala> data.id res0: Int =3D 0 scala> data.id res1: Int =3D=
 0
>> scala> val dataDF =3D data.toDF dataDF: org.apache.spark.sql.DataFrame =
=3D [_1:
>> int] scala> dataDF.rdd.id res3: Int =3D 2 scala> dataDF.rdd.id res4: Int=
 =3D
>> 3 |
>>
>> For some reason it generates a new ID on each call. With schemaRDD i was
>> able to call SchemaRDD.id.
>>
>> Thanks,
>> Peter Rudenko
>>
>> =E2=80=8B
>>
>>
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>
>

--001a11490e4290a95f051284059a--

From dev-return-12265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 21:32:35 2015
Return-Path: <dev-return-12265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE63B17E96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 21:32:35 +0000 (UTC)
Received: (qmail 82051 invoked by uid 500); 30 Mar 2015 21:32:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81982 invoked by uid 500); 30 Mar 2015 21:32:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81967 invoked by uid 99); 30 Mar 2015 21:32:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 21:32:24 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 21:32:00 +0000
Received: by iecvj10 with SMTP id vj10so1219682iec.0
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 14:31:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=KevssJsz/mZlZllXrVBztrjbbknBlNUMrxfslTJ6H8w=;
        b=vi2PzpdTaXvH8bWtCAblnNVqqcCJPd5+MVlNd8/y/9yrUxkCTqJIxzKTe+3Bk6qdLD
         3Xjt6NxnLmNrooUFMH38Nf6oZRxgq27c1XgEY7IBzChMkgCsBUfvhsHa5c6A84XTH+6J
         MlPpygxBoy/aDimiZk6PT19em2DhYJ8Ly14Py7RY4JZHRkWRsExfSu5ghuqeTaxw+j+F
         Lgfhdkt45RE1fw7DLvAd9snlyKGCgDm7WylSmk2VIKJPhyX8GLqsuulGY/zcOCifzzqt
         VHzh3le9rFQHUJ8s/qV+vETiov0cQ5mEKJSHsfnoeWibkiqDs9yopvsvVHiimOZzNBtC
         ENWA==
MIME-Version: 1.0
X-Received: by 10.50.66.141 with SMTP id f13mr20904916igt.9.1427751118286;
 Mon, 30 Mar 2015 14:31:58 -0700 (PDT)
Received: by 10.36.117.139 with HTTP; Mon, 30 Mar 2015 14:31:58 -0700 (PDT)
In-Reply-To: <CA+B-+fwq1-SE4ntnRkJYagovn+8x66pGsBpEyLm80sgjUQ=6Mw@mail.gmail.com>
References: <CA+B-+fwL-t9sGDZ7yYfxwXDJUM6bcLiy1UYdP=NRKpfebCaBhA@mail.gmail.com>
	<CAJgQjQ-oCe+OHqX-4BBy_Le6g61A3nuZJwxLmj21_Ux2f0cH+Q@mail.gmail.com>
	<CA+B-+fx_1tZNoJQ5b1BreQCKGUR9VvBiQU3Q1_iE4iuB9obzmw@mail.gmail.com>
	<CA+B-+fwq1-SE4ntnRkJYagovn+8x66pGsBpEyLm80sgjUQ=6Mw@mail.gmail.com>
Date: Mon, 30 Mar 2015 14:31:58 -0700
Message-ID: <CAJgQjQ9_bUzwBOmPmzPiPC=TNXJ_iqdndERB7w4V_fXBQiZD1w@mail.gmail.com>
Subject: Re: mllib.recommendation Design
From: Xiangrui Meng <mengxr@gmail.com>
To: Debasish Das <debasish.das83@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Wed, Mar 25, 2015 at 7:59 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Xiangrui,
>
> I am facing some minor issues in implementing Alternating Nonlinear
> Minimization as documented in this JIRA due to the ALS code being in ml
> package: https://issues.apache.org/jira/browse/SPARK-6323
>
> I need to use Vectors.fromBreeze / Vectors.toBreeze but they are package
> private on mllib. For now I removed private but not sure that's the correct
> way...

We don't expose 3rd-party types in our public APIs. You can either
implement your algorithm under org.apache.spark or copy the
fromBreeze/toBreeze code over.

>
> I also need to re-use lot of building blocks from ml.ALS and so I am writing
> ALM in ml package...
>

That sounds okay.

> I thought the plan was to still write core algorithms in mllib and pipeline
> integration in ml...It will be great if you can move the ALS object from ml
> to mllib and that way I can also move ALM to mllib (which I feel is the
> right place)...Of course the Pipeline based flow will stay in ml package...
>

It breaks compatibility if we move it. I think it should be quite
flexible about where we put the implementation.

> We can decide later if ALM needs to be in recommendation or a better place
> is package called factorization but the idea is that ALM will support MAP
> (and may be KL divergence loss) with sparsity constraints (probability
> simplex and bounds are fine for what I am focused at right now)...
>

I'm really sorry about the late response on this. It is partially
because that I'm still not sure about whether there exist many
applications that need this feature. Please do list some public work
and help us to understand the need.

> Thanks.
> Deb
>
> On Tue, Feb 17, 2015 at 4:40 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>>
>> There is a usability difference...I am not sure if recommendation.ALS
>> would like to add both userConstraint and productConstraint ? GraphLab CF
>> for example has it and we are ready to support all the features for modest
>> ranks where gram matrices can be made...
>>
>> For large ranks I am still working on the code
>>
>> On Tue, Feb 17, 2015 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>
>>> The current ALS implementation allow pluggable solvers for
>>> NormalEquation, where we put CholeskeySolver and NNLS solver. Please
>>> check the current implementation and let us know how your constraint
>>> solver would fit. For a general matrix factorization package, let's
>>> make a JIRA and move our discussion there. -Xiangrui
>>>
>>> On Fri, Feb 13, 2015 at 7:46 AM, Debasish Das <debasish.das83@gmail.com>
>>> wrote:
>>> > Hi,
>>> >
>>> > I am bit confused on the mllib design in the master. I thought that
>>> > core
>>> > algorithms will stay in mllib and ml will define the pipelines over the
>>> > core algorithm but looks like in master ALS is moved from mllib to
>>> > ml...
>>> >
>>> > I am refactoring my PR to a factorization package and I want to build
>>> > it on
>>> > top of ml.recommendation.ALS (possibly extend from
>>> > ml.recommendation.ALS
>>> > since first version will use very similar RDD handling as ALS and a
>>> > proximal solver that's being added to breeze)
>>> >
>>> > https://issues.apache.org/jira/browse/SPARK-2426
>>> > https://github.com/scalanlp/breeze/pull/321
>>> >
>>> > Basically I am not sure if we should merge it with recommendation.ALS
>>> > since
>>> > this is more generic than recommendation. I am considering calling it
>>> > ConstrainedALS where user can specify different constraint for user and
>>> > product factors (Similar to GraphLab CF structure).
>>> >
>>> > I am also working on ConstrainedALM where the underlying algorithm is
>>> > no
>>> > longer ALS but nonlinear alternating minimization with constraints.
>>> > https://github.com/scalanlp/breeze/pull/364
>>> > This will let us do large rank matrix completion where there is no need
>>> > to
>>> > construct gram matrices. I will open up the JIRA soon after getting
>>> > initial
>>> > results
>>> >
>>> > I am bit confused that where should I add the factorization package. It
>>> > will use the current ALS test-cases and I have to construct more
>>> > test-cases
>>> > for sparse coding and PLSA formulations.
>>> >
>>> > Thanks.
>>> > Deb
>>
>>
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 21:42:59 2015
Return-Path: <dev-return-12266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47EDA17F00
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 21:42:59 +0000 (UTC)
Received: (qmail 18809 invoked by uid 500); 30 Mar 2015 21:42:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18734 invoked by uid 500); 30 Mar 2015 21:42:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18716 invoked by uid 99); 30 Mar 2015 21:42:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 21:42:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 21:42:51 +0000
Received: by igcau2 with SMTP id au2so1877810igc.0
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 14:42:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=/jciVcoFBzYhUtxGhVMZJITdccsWmo0+T5gVZW/kCqk=;
        b=ydIVbsW0OdchYzaCpxQtPaJquVDnhRkJSRzQ+E75Sv6aufHukR9T8xUTuW4ufUGJ0d
         asPhw3d8mB9DRGLL7YdjNf8dRHnCT4b4tgcGhO1RHnb70F3jOrlUUP42feZZ+AS+ySgf
         sqw1YamSgFz2v33LQYUIgPIlG9Zgi7/iekwY60sFrg4r04LKdVrMWJTHpM9yW5Tb0TzY
         JQiv7fn5Ii01GElSpk8eRJu1F7Pcx3YyDsPlmVVA1223Gy8mnKO+45xb/Cr+KloeNRzq
         lHQXxuXXlzoCSEYHO/BLGWukqcIxQQ0p//PeRIQfwSZoy7WV2LlcK0L0V15hqJOEiA0J
         kCNg==
MIME-Version: 1.0
X-Received: by 10.42.167.8 with SMTP id q8mr63907181icy.94.1427751750729; Mon,
 30 Mar 2015 14:42:30 -0700 (PDT)
Received: by 10.36.117.139 with HTTP; Mon, 30 Mar 2015 14:42:30 -0700 (PDT)
In-Reply-To: <CAMAsSdKUE=xRkhx08Ymxp_1TcUZkKfk+r5ZSwMg26cZH+jy5NA@mail.gmail.com>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
	<CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
	<CALR_T9AtsmcXd8oO8yDohKmkUpZLREX3_MBhXqBiQUvrcZYe2w@mail.gmail.com>
	<CALR_T9C4mBOy0T6_K9+RQZtoeNGvhZOtmvz=ij0fhyO8SDbKUw@mail.gmail.com>
	<CABjXkq798dF1ARtkS+oh+oFA7F03nQ+CVCMQ8qEpGy6HFSLgdA@mail.gmail.com>
	<CAMAsSdKUE=xRkhx08Ymxp_1TcUZkKfk+r5ZSwMg26cZH+jy5NA@mail.gmail.com>
Date: Mon, 30 Mar 2015 14:42:30 -0700
Message-ID: <CAJgQjQ-+S-79qBPU7KO6VLAaLypQmDnSNEsmWnxYNgfQt1jPJQ@mail.gmail.com>
Subject: Re: Using CUDA within Spark / boosting linear algebra
From: Xiangrui Meng <mengxr@gmail.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "Evan R. Sparks" <evan.sparks@gmail.com>, Sam Halliday <sam.halliday@gmail.com>, 
	"dev@spark.apache.org" <dev@spark.apache.org>, "Ulanov, Alexander" <alexander.ulanov@hp.com>, 
	jfcanny <canny@berkeley.edu>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Alex,

Since it is non-trivial to make nvblas work with netlib-java, it would
be great if you can send the instructions to netlib-java as part of
the README. Hopefully we don't need to modify netlib-java code to use
nvblas.

Best,
Xiangrui

On Thu, Mar 26, 2015 at 9:54 AM, Sean Owen <sowen@cloudera.com> wrote:
> The license issue is with libgfortran, rather than OpenBLAS.
>
> (FWIW I am going through the motions to get OpenBLAS set up by default
> on CDH in the near future, and the hard part is just handling
> libgfortran.)
>
> On Thu, Mar 26, 2015 at 4:07 PM, Evan R. Sparks <evan.sparks@gmail.com> wrote:
>> Alright Sam - you are the expert here. If the GPL issues are unavoidable,
>> that's fine - what is the exact bit of code that is GPL?
>>
>> The suggestion to use OpenBLAS is not to say it's the best option, but that
>> it's a *free, reasonable default* for many users - keep in mind the most
>> common deployment for Spark/MLlib is on 64-bit linux on EC2[1].
>> Additionally, for many of the problems we're targeting, this reasonable
>> default can provide a 1-2 orders of magnitude improvement in performance
>> over the f2jblas implementation that netlib-java falls back on.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org


From dev-return-12267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Mar 30 23:42:07 2015
Return-Path: <dev-return-12267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00FA31760B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 30 Mar 2015 23:42:07 +0000 (UTC)
Received: (qmail 43687 invoked by uid 500); 30 Mar 2015 23:42:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43615 invoked by uid 500); 30 Mar 2015 23:42:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43601 invoked by uid 99); 30 Mar 2015 23:42:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 23:42:05 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Ilya.Ganelin@capitalone.com designates 199.244.214.13 as permitted sender)
Received: from [199.244.214.13] (HELO komail03.capitalone.com) (199.244.214.13)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 30 Mar 2015 23:41:40 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=simple/simple;
  d=capitalone.com; l=4792; q=dns/txt; s=SM2048Apr2013K;
  t=1427758919; x=1427845319;
  h=from:to:date:subject:message-id:mime-version;
  bh=ZJHUxoJoHF5JcZ+fiUSBc06611WuUFY+V20uXgnLgeg=;
  b=C6u8q5u/7Dn+0J+tE5qYD2dY+7ItkRrpTTIQE/L8k+WHCjKy03RIE4v+
   qOjqjuj8PPcqWp4lDwsrHk1aLTh2et+iDGvYObW3LtFtlK/arC1Msu+g0
   XzibsRMLPZoDZgEFpGaupfEG4s3zZHSoBCRzh2Inswig0f/P7Gu9dRX8v
   q8xwvr2MyXHZ1STxWp8eJydqL3WspQk+ogE/MlHS5fphsqTQ1fFbiteqB
   nAmnOL9Xzs0RGXC0DOD6byVKdyeV51h2K5z1DXqpVjGiPSk4/ZMIyfyqO
   JknkVKxt2ugDajcdcKZ5pv4pxk6aW3BE8PpjqeQkNdjH2iPtV1alpDb9w
   A==;
X-IronPort-AV: E=McAfee;i="5700,7163,7756"; a="214646060"
X-IronPort-AV: E=Sophos;i="5.11,497,1422939600"; 
   d="scan'208,217";a="214646060"
X-HTML-Disclaimer: True
Received: from kdcpexcasht03.cof.ds.capitalone.com ([10.37.194.13])
  by komail03.kdc.capitalone.com with ESMTP; 30 Mar 2015 19:40:38 -0400
Received: from KDCPEXCMB01.cof.ds.capitalone.com ([169.254.1.118]) by
 kdcpexcasht03.cof.ds.capitalone.com ([10.37.194.13]) with mapi; Mon, 30 Mar
 2015 19:40:37 -0400
From: "Ganelin, Ilya" <Ilya.Ganelin@capitalone.com>
To: dev <dev@spark.apache.org>
Date: Mon, 30 Mar 2015 19:40:31 -0400
Subject: Problems with cleanup throughout code base
Thread-Topic: Problems with cleanup throughout code base
Thread-Index: AdBrQuw6y9M5YfcFSiGUz1dwjTs3fg==
Message-ID: <D13F2CFF.20C23%ilya.ganelin@capitalone.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
user-agent: Microsoft-MacOutlook/14.4.7.141117
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_D13F2CFF20C23ilyaganelincapitalonecom_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_D13F2CFF20C23ilyaganelincapitalonecom_
MIME-Version: 1.0
Content-Type: text/plain; charset="windows-1252"
Content-Transfer-Encoding: quoted-printable

Hi all, when looking into a fix for a deadlock in the SparkContext shutdown=
 code for https://issues.apache.org/jira/browse/SPARK-6492, I noticed that =
the =93isStopped=94 flag is set to true before executing the actual shutdow=
n code. This is a problem since it means that if the shutdown sequence does=
n=92t complete successfully that parts of the code will never get shut down=
. I looked deeper and found examples of this in other places as well. I=92v=
e created a JIRA for it here:

https://issues.apache.org/jira/browse/SPARK-6616

Is there a reason that this exists in its present state? I know that it=92s=
 a significant effort to fix this since there are definitely instances thro=
ughout the code where a double cleanup would break at the moment (e.g. Asyn=
chronousBusListener.stop()). I would, however, be strongly in favor of fixi=
ng this, and I would be happy to do it since it=92s a lot easier to solve t=
he more obvious problems caused by an exception from a double cleanup than =
to find the insidious errors from an incomplete cleanup.

I would appreciate any feedback.
Thank you,
Ilya Ganelin

________________________________________________________

The information contained in this e-mail is confidential and/or proprietary=
 to Capital One and/or its affiliates. The information transmitted herewith=
 is intended only for use by the individual or entity to which it is addres=
sed.  If the reader of this message is not the intended recipient, you are =
hereby notified that any review, retransmission, dissemination, distributio=
n, copying or other use of, or taking of any action in reliance upon this i=
nformation is strictly prohibited. If you have received this communication =
in error, please contact the sender and delete the material from your compu=
ter.

--_000_D13F2CFF20C23ilyaganelincapitalonecom_--


From dev-return-12268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 31 00:03:20 2015
Return-Path: <dev-return-12268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0C13176E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 31 Mar 2015 00:03:20 +0000 (UTC)
Received: (qmail 3749 invoked by uid 500); 31 Mar 2015 00:03:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3661 invoked by uid 500); 31 Mar 2015 00:03:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3648 invoked by uid 99); 31 Mar 2015 00:03:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 00:03:19 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.240.92.66] (HELO g9t5008.houston.hp.com) (15.240.92.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 00:02:50 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5008.houston.hp.com (Postfix) with ESMTPS id 7A49597
	for <dev@spark.apache.org>; Tue, 31 Mar 2015 00:01:17 +0000 (UTC)
Received: from G9W3617.americas.hpqcorp.net (16.216.186.52) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Mon, 30 Mar 2015 23:59:50 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G9W3617.americas.hpqcorp.net ([16.216.186.52]) with mapi id 14.03.0169.001;
 Mon, 30 Mar 2015 23:59:50 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Stochastic gradient descent performance
Thread-Topic: Stochastic gradient descent performance
Thread-Index: AdBrRAcrHT3WxbdlTtOI1YXFy9w5hw==
Date: Mon, 30 Mar 2015 23:59:49 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE40857@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.216.65.175]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE40857G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE40857G4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

It seems to me that there is an overhead in "runMiniBatchSGD" function of M=
Llib's "GradientDescent". In particular, "sample" and "treeAggregate" might=
 take time that is order of magnitude greater than the actual gradient comp=
utation. In particular, for mnist dataset of 60K instances, minibatch size =
=3D 0.001 (i.e. 60 samples) it take 0.15 s to sample and 0.3 to aggregate i=
n local mode with 1 data partition on Core i5 processor. The actual gradien=
t computation takes 0.002 s. I searched through Spark Jira and found that t=
here was recently an update for more efficient sampling (SPARK-3250) that i=
s already included in Spark codebase. Is there a way to reduce the sampling=
 time and local treeRedeuce by order of magnitude?

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FE40857G4W3292americas_--

From dev-return-12269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 31 00:13:51 2015
Return-Path: <dev-return-12269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4D2C1777A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 31 Mar 2015 00:13:51 +0000 (UTC)
Received: (qmail 42560 invoked by uid 500); 31 Mar 2015 00:13:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42483 invoked by uid 500); 31 Mar 2015 00:13:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42472 invoked by uid 99); 31 Mar 2015 00:13:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 00:13:50 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.240.92.66] (HELO g9t5008.houston.hp.com) (15.240.92.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 00:13:44 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g9t5008.houston.hp.com (Postfix) with ESMTPS id 06356D3
	for <dev@spark.apache.org>; Tue, 31 Mar 2015 00:12:54 +0000 (UTC)
Received: from G4W6304.americas.hpqcorp.net (16.210.26.229) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Tue, 31 Mar 2015 00:11:35 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.138]) by
 G4W6304.americas.hpqcorp.net ([16.210.26.229]) with mapi id 14.03.0169.001;
 Tue, 31 Mar 2015 00:11:35 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: Xiangrui Meng <mengxr@gmail.com>, Sam Halliday <sam.halliday@gmail.com>
CC: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Using CUDA within Spark / boosting linear algebra
Thread-Topic: Using CUDA within Spark / boosting linear algebra
Thread-Index: AdBBfWhuKPqoaEklS3C36BE9QomgGQAAhtEAAAGfVLAAASz4gAAHItZAAAFGYoAAMDL08AABuXqAAAC0q0AAAKwxgACUAsfwAAMhugAAKb0RoABpRQ1wAorxuwAAAXuuAAAtZPiAAPzG0wABK/otkAAISqwAAvQQdDAAKEa4gAAH6PcwAAE1XQAADUFLAAAAaQ6AABEyGoAAAatdgADTNVAAAAUaAEA=
Date: Tue, 31 Mar 2015 00:11:34 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FE4086E@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FDEFB6F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6CWqUJkfrxmymVFzdsyrJTdHnnuFMka0dUYnaDaLvirg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFC2D@G4W3292.americas.hpqcorp.net>
	<CABjXkq61RorkGRJQMhnfHsAvQTOnBn6eGyiG=XsdX=q8uhVL9Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDEFDE2@G4W3292.americas.hpqcorp.net>
	<CAF7ADNq=LdkP14EjJNgYx=eWWwcpALw+B47WfEqV9PdYfNRgfg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF039F@G4W3292.americas.hpqcorp.net>
	<CABjXkq6eKSkb7iHV5Z7v=GMhMCJB_bwjyKFFHc-Y0i_5QTzeWg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF0451@G4W3292.americas.hpqcorp.net>
	<CABjXkq5oXFus=wYRQyA42UM2XUC8FVV1iLpTiOnbrtwUGO2RFA@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF16BF@G4W3292.americas.hpqcorp.net>
	<CABjXkq47H8+4NqweLvq95hr0-CNZ74tM7TAkqwfH_phTMg7HOg@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF1D26@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FDF2B99@G4W3292.americas.hpqcorp.net>
	<CABjXkq5wrLT1Z-aT3mwsU9qpcHVw2fKq5XvUaXVJbJZpHHMg1g@mail.gmail.com>
	<CAF7ADNoG3R_G5Y3ESy6=L2Ck_b+KwKD1P08OUHEYRbjNiZUJPA@mail.gmail.com>
	<CAJgQjQ_AwWRWgy_nPs1+Z4MqB=HHwTGmFw7_2S+GvQw3n7SzJg@mail.gmail.com>
	<87ioehu4qv.fsf@gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE17CA3@G4W3292.americas.hpqcorp.net>
	<CALR_T9CfniRQeObvVKzW6bTfsgzSF=XaeyYrUM==3t9gTBXj=Q@mail.gmail.com>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE39BB7@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A5BC@G4W3292.americas.hpqcorp.net>
	<9D5B00849D2CDA4386BDA89E83F69E6C0FE3A87B@G4W3292.americas.hpqcorp.net>
	<CABjXkq5H2Lj1uFLKwkcnRq+u=WAnjTa8+syCHDPuS9AZdtkTrQ@mail.gmail.com>
	<CALR_T9AtsmcXd8oO8yDohKmkUpZLREX3_MBhXqBiQUvrcZYe2w@mail.gmail.com>
	<CALR_T9C4mBOy0T6_K9+RQZtoeNGvhZOtmvz=ij0fhyO8SDbKUw@mail.gmail.com>
	<CABjXkq798dF1ARtkS+oh+oFA7F03nQ+CVCMQ8qEpGy6HFSLgdA@mail.gmail.com>
	<CAMAsSdKUE=xRkhx08Ymxp_1TcUZkKfk+r5ZSwMg26cZH+jy5NA@mail.gmail.com>
 <CAJgQjQ-+S-79qBPU7KO6VLAaLypQmDnSNEsmWnxYNgfQt1jPJQ@mail.gmail.com>
In-Reply-To: <CAJgQjQ-+S-79qBPU7KO6VLAaLypQmDnSNEsmWnxYNgfQt1jPJQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.216.65.175]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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DQotLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0tLS0t
LS0tLS0tLS0tLS0tLS0tLS0NClRvIHVuc3Vic2NyaWJlLCBlLW1haWw6IGRldi11bnN1YnNj
cmliZUBzcGFyay5hcGFjaGUub3JnDQpGb3IgYWRkaXRpb25hbCBjb21tYW5kcywgZS1tYWls
OiBkZXYtaGVscEBzcGFyay5hcGFjaGUub3JnDQoN

From dev-return-12270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 31 02:09:23 2015
Return-Path: <dev-return-12270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3A4DB17B86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 31 Mar 2015 02:09:23 +0000 (UTC)
Received: (qmail 89008 invoked by uid 500); 31 Mar 2015 02:09:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88923 invoked by uid 500); 31 Mar 2015 02:09:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88911 invoked by uid 99); 31 Mar 2015 02:09:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 02:09:21 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.217.177 as permitted sender)
Received: from [209.85.217.177] (HELO mail-lb0-f177.google.com) (209.85.217.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 02:08:56 +0000
Received: by lboc7 with SMTP id c7so2274835lbo.1
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 19:08:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=gutYIA4cDoMD2DVPVbqfnfaHogJUaD64iW870VCaT/0=;
        b=DdaV6931Mi5fn6Bb4JJozdEGpoYQeKEPYeA4XC14XfgFGDelNwfisQOK5WLR5HlgRa
         aYL4k4/2JfTiIym+Rtcqr+ZSLFaXhs/98LMpJU9Atn1HSJC4S2kh+UscBPsmHVMYGwQv
         csK2dRD1hTC5OPM7pm53a8s4+0NZZKvmcLmTWbqx6HZ+2V3B/Ay3G16tygcV4w3uObkw
         yRgkCmFVzOD5+dXwtvjusqe5tYa4m8z94BWzhloIZEUWzzU0yOytcSa8W71eq0DSfMyv
         2FoUjPxRAxCCs1UyA1mSa34CG5A4KJPkNqPOTSNSwUzS+riBSbXmYkl84OloZA6a0IBu
         iVWQ==
MIME-Version: 1.0
X-Received: by 10.112.212.106 with SMTP id nj10mr28901689lbc.36.1427767689981;
 Mon, 30 Mar 2015 19:08:09 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Mon, 30 Mar 2015 19:08:09 -0700 (PDT)
Received: by 10.25.82.20 with HTTP; Mon, 30 Mar 2015 19:08:09 -0700 (PDT)
In-Reply-To: <CAJgQjQ9_bUzwBOmPmzPiPC=TNXJ_iqdndERB7w4V_fXBQiZD1w@mail.gmail.com>
References: <CA+B-+fwL-t9sGDZ7yYfxwXDJUM6bcLiy1UYdP=NRKpfebCaBhA@mail.gmail.com>
	<CAJgQjQ-oCe+OHqX-4BBy_Le6g61A3nuZJwxLmj21_Ux2f0cH+Q@mail.gmail.com>
	<CA+B-+fx_1tZNoJQ5b1BreQCKGUR9VvBiQU3Q1_iE4iuB9obzmw@mail.gmail.com>
	<CA+B-+fwq1-SE4ntnRkJYagovn+8x66pGsBpEyLm80sgjUQ=6Mw@mail.gmail.com>
	<CAJgQjQ9_bUzwBOmPmzPiPC=TNXJ_iqdndERB7w4V_fXBQiZD1w@mail.gmail.com>
Date: Mon, 30 Mar 2015 19:08:09 -0700
Message-ID: <CA+B-+fyPbG3JESmwmizRYr30+rQ2+HK_O7nQ8RNEm6bKpE3WfQ@mail.gmail.com>
Subject: Re: mllib.recommendation Design
From: Debasish Das <debasish.das83@gmail.com>
To: Xiangrui Meng <mengxr@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11346ddafbb56d05128c11e8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11346ddafbb56d05128c11e8
Content-Type: text/plain; charset=UTF-8

For alm I have started experimenting with the following:

1. rmse and map improvement from loglikelihood loss over least square loss.

2. Factorization for datasets that are not ratings (basically improvement
over implicit ratings)

3. Sparse topic generation using plsa. We are directly optimizing
likelihood under constraints here and so I feel it will improve upon EM
algorithm. Also the current LDA does not produce sparse topics and ALM
results can augment LDA flow. I am understanding LDA flow to see if the
sparsity and loglikelihood optimization can be added there.

I will understand more as I see the result. I am not sure if it is
supported by public packages like graphlab or scikit but the plsa papers
show interesting results.
 On Mar 30, 2015 2:31 PM, "Xiangrui Meng" <mengxr@gmail.com> wrote:

> On Wed, Mar 25, 2015 at 7:59 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi Xiangrui,
> >
> > I am facing some minor issues in implementing Alternating Nonlinear
> > Minimization as documented in this JIRA due to the ALS code being in ml
> > package: https://issues.apache.org/jira/browse/SPARK-6323
> >
> > I need to use Vectors.fromBreeze / Vectors.toBreeze but they are package
> > private on mllib. For now I removed private but not sure that's the
> correct
> > way...
>
> We don't expose 3rd-party types in our public APIs. You can either
> implement your algorithm under org.apache.spark or copy the
> fromBreeze/toBreeze code over.
>
> >
> > I also need to re-use lot of building blocks from ml.ALS and so I am
> writing
> > ALM in ml package...
> >
>
> That sounds okay.
>
> > I thought the plan was to still write core algorithms in mllib and
> pipeline
> > integration in ml...It will be great if you can move the ALS object from
> ml
> > to mllib and that way I can also move ALM to mllib (which I feel is the
> > right place)...Of course the Pipeline based flow will stay in ml
> package...
> >
>
> It breaks compatibility if we move it. I think it should be quite
> flexible about where we put the implementation.
>
> > We can decide later if ALM needs to be in recommendation or a better
> place
> > is package called factorization but the idea is that ALM will support MAP
> > (and may be KL divergence loss) with sparsity constraints (probability
> > simplex and bounds are fine for what I am focused at right now)...
> >
>
> I'm really sorry about the late response on this. It is partially
> because that I'm still not sure about whether there exist many
> applications that need this feature. Please do list some public work
> and help us to understand the need.
>
> > Thanks.
> > Deb
> >
> > On Tue, Feb 17, 2015 at 4:40 PM, Debasish Das <debasish.das83@gmail.com>
> > wrote:
> >>
> >> There is a usability difference...I am not sure if recommendation.ALS
> >> would like to add both userConstraint and productConstraint ? GraphLab
> CF
> >> for example has it and we are ready to support all the features for
> modest
> >> ranks where gram matrices can be made...
> >>
> >> For large ranks I am still working on the code
> >>
> >> On Tue, Feb 17, 2015 at 3:19 PM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >>>
> >>> The current ALS implementation allow pluggable solvers for
> >>> NormalEquation, where we put CholeskeySolver and NNLS solver. Please
> >>> check the current implementation and let us know how your constraint
> >>> solver would fit. For a general matrix factorization package, let's
> >>> make a JIRA and move our discussion there. -Xiangrui
> >>>
> >>> On Fri, Feb 13, 2015 at 7:46 AM, Debasish Das <
> debasish.das83@gmail.com>
> >>> wrote:
> >>> > Hi,
> >>> >
> >>> > I am bit confused on the mllib design in the master. I thought that
> >>> > core
> >>> > algorithms will stay in mllib and ml will define the pipelines over
> the
> >>> > core algorithm but looks like in master ALS is moved from mllib to
> >>> > ml...
> >>> >
> >>> > I am refactoring my PR to a factorization package and I want to build
> >>> > it on
> >>> > top of ml.recommendation.ALS (possibly extend from
> >>> > ml.recommendation.ALS
> >>> > since first version will use very similar RDD handling as ALS and a
> >>> > proximal solver that's being added to breeze)
> >>> >
> >>> > https://issues.apache.org/jira/browse/SPARK-2426
> >>> > https://github.com/scalanlp/breeze/pull/321
> >>> >
> >>> > Basically I am not sure if we should merge it with recommendation.ALS
> >>> > since
> >>> > this is more generic than recommendation. I am considering calling it
> >>> > ConstrainedALS where user can specify different constraint for user
> and
> >>> > product factors (Similar to GraphLab CF structure).
> >>> >
> >>> > I am also working on ConstrainedALM where the underlying algorithm is
> >>> > no
> >>> > longer ALS but nonlinear alternating minimization with constraints.
> >>> > https://github.com/scalanlp/breeze/pull/364
> >>> > This will let us do large rank matrix completion where there is no
> need
> >>> > to
> >>> > construct gram matrices. I will open up the JIRA soon after getting
> >>> > initial
> >>> > results
> >>> >
> >>> > I am bit confused that where should I add the factorization package.
> It
> >>> > will use the current ALS test-cases and I have to construct more
> >>> > test-cases
> >>> > for sparse coding and PLSA formulations.
> >>> >
> >>> > Thanks.
> >>> > Deb
> >>
> >>
> >
>

--001a11346ddafbb56d05128c11e8--

From dev-return-12271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 31 02:48:18 2015
Return-Path: <dev-return-12271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B2A217C58
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 31 Mar 2015 02:48:18 +0000 (UTC)
Received: (qmail 41424 invoked by uid 500); 31 Mar 2015 02:48:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41339 invoked by uid 500); 31 Mar 2015 02:48:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41321 invoked by uid 99); 31 Mar 2015 02:48:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 02:48:16 +0000
X-ASF-Spam-Status: No, hits=2.4 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wyphao.2007@163.com designates 220.181.13.42 as permitted sender)
Received: from [220.181.13.42] (HELO m13-42.163.com) (220.181.13.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 02:48:09 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=anlEN
	Zmwi9eUl0SuYcIIZ0ZvBDIVm3aAXZV7O4oM7k8=; b=jmMQUoX78vLc8Tu2uraVl
	l0UGE6rPhzNGlfevPGS8IwvomabmZrlgd7qJIG14aIiMn7US9li16tlQyEoMeoZN
	sWXkaAECPcl6v5b2Fvwe6oFxlR/qNdPnEtROZCJlLIFHM7/Fxv1kq3V//fiEpUQS
	nFgJDUQ4P7JwVeYCVBAtQk=
Received: from wyphao.2007$163.com ( [211.151.238.52] ) by
 ajax-webmail-wmsvr42 (Coremail) ; Tue, 31 Mar 2015 10:47:46 +0800 (CST)
X-Originating-IP: [211.151.238.52]
Date: Tue, 31 Mar 2015 10:47:46 +0800 (CST)
From: "wyphao.2007" <wyphao.2007@163.com>
To: user@spark.apache.org, dev@spark.apache.org
Subject: =?GBK?Q?How_to_get_removed_RDD_from_windows=A3=BF?=
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20150119(59087.7062) Copyright (c) 2002-2015 www.mailtech.cn 163com
X-CM-CTRLDATA: RF0qGGZvb3Rlcl9odG09MjU5MDo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_69113_1176034132.1427770066957"
MIME-Version: 1.0
Message-ID: <4087ef54.41e.14c6dba480d.Coremail.wyphao.2007@163.com>
X-CM-TRANSID:KsGowGC5qUDUChpVuWozAA--.13791W
X-CM-SenderInfo: xz1sxtbrosiiqx6rljoofrz/1tbiXRHNKFEAMirsbQAEsl
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_69113_1176034132.1427770066957
Content-Type: text/plain; charset=GBK
Content-Transfer-Encoding: base64

SSB3YW50IHRvIGdldCByZW1vdmVkIFJERCBmcm9tIHdpbmRvd3MgYXMgZm9sbG93LCBUaGUgb2xk
IFJERHMgd2lsbCByZW1vdmVkIGZyb20gY3VycmVudCB3aW5kb3csIAovLyAgX19fX19fX19fX19f
X19fX19fX19fX19fX19fX18KLy8gfCAgcHJldmlvdXMgd2luZG93ICAgX19fX19fX19ffF9fX19f
X19fX19fX19fX19fX18KLy8gfF9fX19fX19fX19fX19fX19fX198ICAgICAgIGN1cnJlbnQgd2lu
ZG93ICAgICAgICB8ICAtLS0tLS0tLS0tLS0tLT4gVGltZQovLyAgICAgICAgICAgICAgICAgICAg
IHxfX19fX19fX19fX19fX19fX19fX19fX19fX19fX3wKLy8KLy8gfF9fX19fX19fIF9fX19fX19f
X3wgICAgICAgICAgfF9fX19fX19fIF9fX19fX19fX3wKLy8gICAgICAgICAgfCAgICAgICAgICAg
ICAgICAgICAgICAgICAgICAgfAovLyAgICAgICAgICBWICAgICAgICAgICAgICAgICAgICAgICAg
ICAgICBWCi8vICAgICAgIG9sZCBSRERzICAgICAgICAgICAgICAgICAgICAgbmV3IFJERHMKLy8K
SSBmaW5kICB0aGUgc2xpY2UgZnVuY3Rpb24gaW4gRFN0cmVhbSBjbGFzcyBjYW4gcmV0dXJuIHRo
ZSBEU3RyZWFtIGJldHdlZW4gZnJvbVRpbWUgdG8gIHRvVGltZS4gQnV0IHdoZW4gSSB1c2UgdGhl
IGZ1bmN0aW9uIGFzIGZvbGxvdzoKCgogICAgdmFsIG5vdyA9IFN5c3RlbS5jdXJyZW50VGltZU1p
bGxpcygpCiAgICByZXN1bHQuc2xpY2UobmV3IFRpbWUobm93IC0gMzAgKiAxMDAwKSwgbmV3IFRp
bWUobm93IC0gMzAgKiAxMDAwICsgcmVzdWx0LnNsaWRlRHVyYXRpb24ubWlsbGlzZWNvbmRzKSku
Zm9yZWFjaChpdGVtID0+IHByaW50bG4oInh4eCIgKyBpdGVtKSkKICAgIHNzYy5zdGFydCgpCgoK
MzAgaXMgdGhlIHdpbmRvdydzIGR1cmF0aW9uLFRoZW4gSSBnb3QgemVyb1RpbWUgaGFzIG5vdCBi
ZWVuIGluaXRpYWxpemVkIGV4Y2VwdGlvbi4gCgoKSXMgYW55b25lIGNhbiBoZWxwIG1lPyB0aHgh
CgoKCgoKCgoKCgo=
------=_Part_69113_1176034132.1427770066957--


From dev-return-12272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 31 02:48:38 2015
Return-Path: <dev-return-12272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 15D9F17C5A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 31 Mar 2015 02:48:38 +0000 (UTC)
Received: (qmail 43765 invoked by uid 500); 31 Mar 2015 02:48:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43690 invoked by uid 500); 31 Mar 2015 02:48:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43672 invoked by uid 99); 31 Mar 2015 02:48:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 02:48:36 +0000
X-ASF-Spam-Status: No, hits=2.4 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wyphao.2007@163.com designates 220.181.13.42 as permitted sender)
Received: from [220.181.13.42] (HELO m13-42.163.com) (220.181.13.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 02:48:29 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=163.com;
	s=s110527; h=Date:From:Subject:MIME-Version:Message-ID; bh=tVC3k
	84vm3TLX/RwALGgfMtGSgqNYSkCZdjKi0KVdvo=; b=EFGnCwbQRwzRIScDp501o
	OeWUm4ft5K0TIT3gVDZ/ylo6h604JqDdbvTtkErxuhQyXKBhkvrR8RY4XNNAXq+A
	pRikV2YW8RbfvhRMD1aRlL3CF6zmBv7xTSbWYGQ8ZFtiDxctIHEjmIkgvlfPq0Lr
	Tq1L4KFJhZYnWJXFStYa5k=
Received: from wyphao.2007$163.com ( [211.151.238.52] ) by
 ajax-webmail-wmsvr42 (Coremail) ; Tue, 31 Mar 2015 10:46:27 +0800 (CST)
X-Originating-IP: [211.151.238.52]
Date: Tue, 31 Mar 2015 10:46:27 +0800 (CST)
From: "wyphao.2007" <wyphao.2007@163.com>
To: user@spark.apache.org, dev@spark.apache.org
Subject: =?GBK?Q?How_to_get_removed_RDD_from_windows=A3=BF?=
X-Priority: 3
X-Mailer: Coremail Webmail Server Version SP_ntes V3.5 build
 20150119(59087.7062) Copyright (c) 2002-2015 www.mailtech.cn 163com
X-CM-CTRLDATA: YIx0JGZvb3Rlcl9odG09MjQyNzo4MQ==
Content-Type: multipart/alternative; 
	boundary="----=_Part_68637_20509827.1427769987068"
MIME-Version: 1.0
Message-ID: <23af07ac.417.14c6db90ffd.Coremail.wyphao.2007@163.com>
X-CM-TRANSID:KsGowGAZCkGDChpV+GkzAA--.1890W
X-CM-SenderInfo: xz1sxtbrosiiqx6rljoofrz/1tbiXRHNKFEAMirsbQADsi
X-Coremail-Antispam: 1U5529EdanIXcx71UUUUU7vcSsGvfC2KfnxnUU==
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_68637_20509827.1427769987068
Content-Type: text/plain; charset=GBK
Content-Transfer-Encoding: base64

SSB3YW50IHRvIGdldCByZW1vdmVkIFJERCBmcm9tIHdpbmRvd3MgYXMgZm9sbG93LCBUaGUgb2xk
IFJERHMgd2lsbCByZW1vdmVkIGZyb20gY3VycmVudCB3aW5kb3csIAovLyAgX19fX19fX19fX19f
X19fX19fX19fX19fX19fX18KLy8gfCAgcHJldmlvdXMgd2luZG93ICAgX19fX19fX19ffF9fX19f
X19fX19fX19fX19fX18KLy8gfF9fX19fX19fX19fX19fX19fX198ICAgICAgIGN1cnJlbnQgd2lu
ZG93ICAgICAgICB8ICAtLS0tLS0tLS0tLS0tLT4gVGltZQovLyAgICAgICAgICAgICAgICAgICAg
IHxfX19fX19fX19fX19fX19fX19fX19fX19fX19fX3wKLy8KLy8gfF9fX19fX19fIF9fX19fX19f
X3wgICAgICAgICAgfF9fX19fX19fIF9fX19fX19fX3wKLy8gICAgICAgICAgfCAgICAgICAgICAg
ICAgICAgICAgICAgICAgICAgfAovLyAgICAgICAgICBWICAgICAgICAgICAgICAgICAgICAgICAg
ICAgICBWCi8vICAgICAgIG9sZCBSRERzICAgICAgICAgICAgICAgICAgICAgbmV3IFJERHMKLy8K
SSBmaW5kICB0aGUgc2xpY2UgZnVuY3Rpb24gaW4gRFN0cmVhbSBjbGFzcyBjYW4gcmV0dXJuIHRo
ZSBEU3RyZWFtIGJldHdlZW4gZnJvbVRpbWUgdG8gIHRvVGltZS4gQnV0IHdoZW4gSSB1c2UgdGhl
IGZ1bmN0aW9uIGFzIGZvbGxvdzoKCgogICAgdmFsIG5vdyA9IFN5c3RlbS5jdXJyZW50VGltZU1p
bGxpcygpCiAgICByZXN1bHQuc2xpY2UobmV3IFRpbWUobm93IC0gMzAgKiAxMDAwKSwgbmV3IFRp
bWUobm93IC0gMzAgKiAxMDAwICsgcmVzdWx0LnNsaWRlRHVyYXRpb24ubWlsbGlzZWNvbmRzKSku
Zm9yZWFjaChpdGVtID0+IHByaW50bG4oInh4eCIgKyBpdGVtKSkKICAgIHNzYy5zdGFydCgpCgoK
MzAgaXMgdGhlIHdpbmRvdydzIGR1cmF0aW9uLFRoZW4gSSBnb3QgemVyb1RpbWUgaGFzIG5vdCBi
ZWVuIGluaXRpYWxpemVkIGV4Y2VwdGlvbi4gCgoKSXMgYW55b25lIGNhbiBoZWxwIG1lPyB0aHgh
CgoKCgoKCgo=
------=_Part_68637_20509827.1427769987068--


From dev-return-12273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 31 06:39:49 2015
Return-Path: <dev-return-12273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 62DD4174BD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 31 Mar 2015 06:39:49 +0000 (UTC)
Received: (qmail 19067 invoked by uid 500); 31 Mar 2015 06:39:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18987 invoked by uid 500); 31 Mar 2015 06:39:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18976 invoked by uid 99); 31 Mar 2015 06:39:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 06:39:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 06:39:43 +0000
Received: by qgh3 with SMTP id 3so6638958qgh.2
        for <dev@spark.apache.org>; Mon, 30 Mar 2015 23:38:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=ODk9cwYOUPAAapWp9O6BvR7LtJrYMztWv5+6YoptPT4=;
        b=gAjKEeeKkUQ2RronFObC6NkuvfkIj5PzHuTGMIimIzO6kvTopbax91GBzCkHqleyct
         d/70rvkoVYwD8h1mibmnO1873niCDRHzy5JrcEo3JuPLciJnNhwIwcE3qgAVlC/+cSNE
         eFycbYQwNLFHM4FW5SodAjmRQL8ZZh/4SNkCuhEdfebFQA5r5kRA8HyJPARrCBxeoc2F
         18a4yG/8IzwXu1HC0zwnWIv4/8uTIRtyC39r1v1D4kDGurZvhKTsdXcj4W4prM0oiTPd
         SP1p1fc9DmRWM1AoJazvOIAp11JC1qSv0Qw+jM5aveg9QuGnG3LGqYFa/qR1Vnv5RFeQ
         Qodg==
X-Gm-Message-State: ALoCoQn53suXOcUU4vo1IQhh7DEHniii4bd5+gwWeDYM9PuY7Wut3oeK5RzKzzwf8j1ivsC7QBo/
X-Received: by 10.140.151.8 with SMTP id 8mr47845445qhx.65.1427783896899; Mon,
 30 Mar 2015 23:38:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.93.101 with HTTP; Mon, 30 Mar 2015 23:37:56 -0700 (PDT)
In-Reply-To: <BAY180-W51C42605342232B7571A72B1040@phx.gbl>
References: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl> <CAJiQeYJ7k4Obu+TBzfTeU3wgz=ou5+vix0GLBTq_nN045mM0RQ@mail.gmail.com>
 <BAY180-W19B8A71D6FB18B714BCAF1B1070@phx.gbl> <CAJiQeYJv-x27FtvPtopLh=B3t2dzJRfaQYqVVH_xbZJqKr0sUg@mail.gmail.com>
 <BAY180-W51C42605342232B7571A72B1040@phx.gbl>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 30 Mar 2015 23:37:56 -0700
Message-ID: <CAPh_B=bOMJSC5J1ABBTtUEsEFDgi=zsUtFbqOM7no6XtL_GMcA@mail.gmail.com>
Subject: Re: Spark config option 'expression language' feedback request
To: Dale Richardson <dale__r@hotmail.com>
Cc: Mridul Muralidharan <mridul@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11353862fdb73405128fd756
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11353862fdb73405128fd756
Content-Type: text/plain; charset=UTF-8

Reviving this to see if others would like to chime in about this
"expression language" for config options.


On Fri, Mar 13, 2015 at 7:57 PM, Dale Richardson <dale__r@hotmail.com>
wrote:

> Mridul,I may have added some confusion by giving examples in completely
> different areas. For example the number of cores available for tasking on
> each worker machine is a resource-controller level configuration variable.
> In standalone mode (ie using Spark's home-grown resource manager) the
> configuration variable SPARK_WORKER_CORES is an item that spark admins can
> set (and we can use expressions for). The equivalent variable for YARN
> (Yarn.nodemanager.resource.cpu-vcores) is only used by Yarn's node manager
> setup and is set by Yarn administrators and outside of control of spark
> (and most users).  If you are not a cluster administrator then both
> variables are irrelevant to you. The same goes for SPARK_WORKER_MEMORY.
>
> As for spark.executor.memory,  As there is no way to know the attributes
> of a machine before a task is allocated to it, we cannot use any of the
> JVMInfo functions. For options like that the expression parser can easily
> be limited to supporting different byte units of scale (kb/mb/gb etc) and
> other configuration variables only.
> Regards,Dale.
>
>
>
>
> > Date: Fri, 13 Mar 2015 17:30:51 -0700
> > Subject: Re: Spark config option 'expression language' feedback request
> > From: mridul@gmail.com
> > To: dale__r@hotmail.com
> > CC: dev@spark.apache.org
> >
> > Let me try to rephrase my query.
> > How can a user specify, for example, what the executor memory should
> > be or number of cores should be.
> >
> > I dont want a situation where some variables can be specified using
> > one set of idioms (from this PR for example) and another set cannot
> > be.
> >
> >
> > Regards,
> > Mridul
> >
> >
> >
> >
> > On Fri, Mar 13, 2015 at 4:06 PM, Dale Richardson <dale__r@hotmail.com>
> wrote:
> > >
> > >
> > >
> > > Thanks for your questions Mridul.
> > > I assume you are referring to how the functionality to query system
> state works in Yarn and Mesos?
> > > The API's used are the standard JVM API's so the functionality will
> work without change. There is no real use case for using
> 'physicalMemoryBytes' in these cases though, as the JVM size has already
> been limited by the resource manager.
> > > Regards,Dale.
> > >> Date: Fri, 13 Mar 2015 08:20:33 -0700
> > >> Subject: Re: Spark config option 'expression language' feedback
> request
> > >> From: mridul@gmail.com
> > >> To: dale__r@hotmail.com
> > >> CC: dev@spark.apache.org
> > >>
> > >> I am curious how you are going to support these over mesos and yarn.
> > >> Any configure change like this should be applicable to all of them,
> not
> > >> just local and standalone modes.
> > >>
> > >> Regards
> > >> Mridul
> > >>
> > >> On Friday, March 13, 2015, Dale Richardson <dale__r@hotmail.com>
> wrote:
> > >>
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> >
> > >> > PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature
> to
> > >> > allow for Spark configuration options (whether on command line,
> environment
> > >> > variable or a configuration file) to be specified via a simple
> expression
> > >> > language.
> > >> >
> > >> >
> > >> > Such a feature has the following end-user benefits:
> > >> > - Allows for the flexibility in specifying time intervals or byte
> > >> > quantities in appropriate and easy to follow units e.g. 1 week
> rather
> > >> > rather then 604800 seconds
> > >> >
> > >> > - Allows for the scaling of a configuration option in relation to a
> system
> > >> > attributes. e.g.
> > >> >
> > >> > SPARK_WORKER_CORES = numCores - 1
> > >> >
> > >> > SPARK_WORKER_MEMORY = physicalMemoryBytes - 1.5 GB
> > >> >
> > >> > - Gives the ability to scale multiple configuration options
> together eg:
> > >> >
> > >> > spark.driver.memory = 0.75 * physicalMemoryBytes
> > >> >
> > >> > spark.driver.maxResultSize = spark.driver.memory * 0.8
> > >> >
> > >> >
> > >> > The following functions are currently supported by this PR:
> > >> > NumCores:             Number of cores assigned to the JVM (usually
> ==
> > >> > Physical machine cores)
> > >> > PhysicalMemoryBytes:  Memory size of hosting machine
> > >> >
> > >> > JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
> > >> >
> > >> > JVMMaxMemoryBytes:    Maximum number of bytes of memory available
> to the
> > >> > JVM
> > >> >
> > >> > JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
> > >> >
> > >> >
> > >> > I was wondering if anybody on the mailing list has any further
> ideas on
> > >> > other functions that could be useful to have when specifying spark
> > >> > configuration options?
> > >> > Regards,Dale.
> > >> >
> > >
> > >
> >
> > ---------------------------------------------------------------------
> > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > For additional commands, e-mail: dev-help@spark.apache.org
> >
>
>

--001a11353862fdb73405128fd756--

From dev-return-12274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Mar 31 13:07:03 2015
Return-Path: <dev-return-12274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8CD1A17363
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 31 Mar 2015 13:07:03 +0000 (UTC)
Received: (qmail 77797 invoked by uid 500); 31 Mar 2015 13:07:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77715 invoked by uid 500); 31 Mar 2015 13:07:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77703 invoked by uid 99); 31 Mar 2015 13:07:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 13:07:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of 91mbbh@gmail.com designates 209.85.192.44 as permitted sender)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 31 Mar 2015 13:06:36 +0000
Received: by qgf60 with SMTP id 60so13254941qgf.3
        for <dev@spark.apache.org>; Tue, 31 Mar 2015 06:06:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=LYNra03AihMN4FjiEJOEHz+x2FPKULfMycP+jvt2t64=;
        b=ommQnTauAzUDIlUZ0Geyj0i/nAk3XqDc5bB2X3+iD/hEj9MxxmNGwuLmrHCtx+9G4W
         MdfAt158bRp6GewMEuVnoGMNoSWg0ag3nG09V9S41HiQW5AgsYpll0anjWfPSnEmR4wb
         1B1T6vfaZDe97E9U1vhPZIPXZ3vMOpL0hDDMZb/K4H/eaxJ32ogobEHakFcU1FbxWa8e
         NogDGWNpQKy8FSFvImNavG3aMtXma5g1V96wRY2DSQ/k34R0amebCraEV38il+FG6+Wn
         0T9tMPzph0VggJdBfv+PkcSlHLvvVtIOizeeusZVxme/CRD0j169PTH3n8KUgcPt9fe8
         +DZQ==
MIME-Version: 1.0
X-Received: by 10.55.21.140 with SMTP id 12mr78109855qkv.95.1427807194827;
 Tue, 31 Mar 2015 06:06:34 -0700 (PDT)
Received: by 10.140.90.51 with HTTP; Tue, 31 Mar 2015 06:06:34 -0700 (PDT)
Received: by 10.140.90.51 with HTTP; Tue, 31 Mar 2015 06:06:34 -0700 (PDT)
In-Reply-To: <CAPh_B=bOMJSC5J1ABBTtUEsEFDgi=zsUtFbqOM7no6XtL_GMcA@mail.gmail.com>
References: <BAY180-W6325454BBA7C9649C4678CB1070@phx.gbl>
	<CAJiQeYJ7k4Obu+TBzfTeU3wgz=ou5+vix0GLBTq_nN045mM0RQ@mail.gmail.com>
	<BAY180-W19B8A71D6FB18B714BCAF1B1070@phx.gbl>
	<CAJiQeYJv-x27FtvPtopLh=B3t2dzJRfaQYqVVH_xbZJqKr0sUg@mail.gmail.com>
	<BAY180-W51C42605342232B7571A72B1040@phx.gbl>
	<CAPh_B=bOMJSC5J1ABBTtUEsEFDgi=zsUtFbqOM7no6XtL_GMcA@mail.gmail.com>
Date: Tue, 31 Mar 2015 09:06:34 -0400
Message-ID: <CABX=+Lo02otOxxSb7=FsrnkkSKqS1imMY++Vzufy0S+jsW4YdA@mail.gmail.com>
Subject: Re: Spark config option 'expression language' feedback request
From: Mike Hynes <91mbbh@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Mridul Muralidharan <mridul@gmail.com>, dev <dev@spark.apache.org>, 
	Dale Richardson <dale__r@hotmail.com>
Content-Type: multipart/alternative; boundary=001a11473a7ca7e74c05129544b1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11473a7ca7e74c05129544b1
Content-Type: text/plain; charset=ISO-8859-1

Hi,
This is just a thought from my experience setting up Spark to run on a
linux cluster. I found it a bit unusual that some parameters could be
specified as command line args to spark-submit, others as env variables,
and some in a configuration file. What I ended up doing was writing my own
bash script that exported all the variables and other scripts to call
spark-submit with the arguments I wanted.

I think that the "expressive language" idea would be doable by using an
entirely env variable based approach, or as commandline parameters. That
way there is only one configuration, which is easily scriptable,  and you
are still able to express relations like:
spark.driver.maxResultSize = spark.driver.memory * 0.8
in your config as
export SPARK_DRIVER_MAXRESULTSIZE = $(bc -l <<< "0.8 *
$SPARK_DRIVER_MEMORY")

It may not look as nice, but it does allow for everything to be in one
place, and to have separate config files for certain jobs. Admittedly, if
you want something like 0.8 * 2G, you first write a bash function to expand
all the "G M k" symbols,  but that's not too painful.
On Mar 31, 2015 2:39 AM, "Reynold Xin" <rxin@databricks.com> wrote:

> Reviving this to see if others would like to chime in about this
> "expression language" for config options.
>
>
> On Fri, Mar 13, 2015 at 7:57 PM, Dale Richardson <dale__r@hotmail.com>
> wrote:
>
> > Mridul,I may have added some confusion by giving examples in completely
> > different areas. For example the number of cores available for tasking on
> > each worker machine is a resource-controller level configuration
> variable.
> > In standalone mode (ie using Spark's home-grown resource manager) the
> > configuration variable SPARK_WORKER_CORES is an item that spark admins
> can
> > set (and we can use expressions for). The equivalent variable for YARN
> > (Yarn.nodemanager.resource.cpu-vcores) is only used by Yarn's node
> manager
> > setup and is set by Yarn administrators and outside of control of spark
> > (and most users).  If you are not a cluster administrator then both
> > variables are irrelevant to you. The same goes for SPARK_WORKER_MEMORY.
> >
> > As for spark.executor.memory,  As there is no way to know the attributes
> > of a machine before a task is allocated to it, we cannot use any of the
> > JVMInfo functions. For options like that the expression parser can easily
> > be limited to supporting different byte units of scale (kb/mb/gb etc) and
> > other configuration variables only.
> > Regards,Dale.
> >
> >
> >
> >
> > > Date: Fri, 13 Mar 2015 17:30:51 -0700
> > > Subject: Re: Spark config option 'expression language' feedback request
> > > From: mridul@gmail.com
> > > To: dale__r@hotmail.com
> > > CC: dev@spark.apache.org
> > >
> > > Let me try to rephrase my query.
> > > How can a user specify, for example, what the executor memory should
> > > be or number of cores should be.
> > >
> > > I dont want a situation where some variables can be specified using
> > > one set of idioms (from this PR for example) and another set cannot
> > > be.
> > >
> > >
> > > Regards,
> > > Mridul
> > >
> > >
> > >
> > >
> > > On Fri, Mar 13, 2015 at 4:06 PM, Dale Richardson <dale__r@hotmail.com>
> > wrote:
> > > >
> > > >
> > > >
> > > > Thanks for your questions Mridul.
> > > > I assume you are referring to how the functionality to query system
> > state works in Yarn and Mesos?
> > > > The API's used are the standard JVM API's so the functionality will
> > work without change. There is no real use case for using
> > 'physicalMemoryBytes' in these cases though, as the JVM size has already
> > been limited by the resource manager.
> > > > Regards,Dale.
> > > >> Date: Fri, 13 Mar 2015 08:20:33 -0700
> > > >> Subject: Re: Spark config option 'expression language' feedback
> > request
> > > >> From: mridul@gmail.com
> > > >> To: dale__r@hotmail.com
> > > >> CC: dev@spark.apache.org
> > > >>
> > > >> I am curious how you are going to support these over mesos and yarn.
> > > >> Any configure change like this should be applicable to all of them,
> > not
> > > >> just local and standalone modes.
> > > >>
> > > >> Regards
> > > >> Mridul
> > > >>
> > > >> On Friday, March 13, 2015, Dale Richardson <dale__r@hotmail.com>
> > wrote:
> > > >>
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> >
> > > >> > PR#4937 ( https://github.com/apache/spark/pull/4937) is a feature
> > to
> > > >> > allow for Spark configuration options (whether on command line,
> > environment
> > > >> > variable or a configuration file) to be specified via a simple
> > expression
> > > >> > language.
> > > >> >
> > > >> >
> > > >> > Such a feature has the following end-user benefits:
> > > >> > - Allows for the flexibility in specifying time intervals or byte
> > > >> > quantities in appropriate and easy to follow units e.g. 1 week
> > rather
> > > >> > rather then 604800 seconds
> > > >> >
> > > >> > - Allows for the scaling of a configuration option in relation to
> a
> > system
> > > >> > attributes. e.g.
> > > >> >
> > > >> > SPARK_WORKER_CORES = numCores - 1
> > > >> >
> > > >> > SPARK_WORKER_MEMORY = physicalMemoryBytes - 1.5 GB
> > > >> >
> > > >> > - Gives the ability to scale multiple configuration options
> > together eg:
> > > >> >
> > > >> > spark.driver.memory = 0.75 * physicalMemoryBytes
> > > >> >
> > > >> > spark.driver.maxResultSize = spark.driver.memory * 0.8
> > > >> >
> > > >> >
> > > >> > The following functions are currently supported by this PR:
> > > >> > NumCores:             Number of cores assigned to the JVM (usually
> > ==
> > > >> > Physical machine cores)
> > > >> > PhysicalMemoryBytes:  Memory size of hosting machine
> > > >> >
> > > >> > JVMTotalMemoryBytes:  Current bytes of memory allocated to the JVM
> > > >> >
> > > >> > JVMMaxMemoryBytes:    Maximum number of bytes of memory available
> > to the
> > > >> > JVM
> > > >> >
> > > >> > JVMFreeMemoryBytes:   maxMemoryBytes - totalMemoryBytes
> > > >> >
> > > >> >
> > > >> > I was wondering if anybody on the mailing list has any further
> > ideas on
> > > >> > other functions that could be useful to have when specifying spark
> > > >> > configuration options?
> > > >> > Regards,Dale.
> > > >> >
> > > >
> > > >
> > >
> > > ---------------------------------------------------------------------
> > > To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> > > For additional commands, e-mail: dev-help@spark.apache.org
> > >
> >
> >
>

--001a11473a7ca7e74c05129544b1--

