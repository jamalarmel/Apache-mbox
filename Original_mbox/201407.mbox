From dev-return-8154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 02:59:18 2014
Return-Path: <dev-return-8154-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D5E9A11B6A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 02:59:18 +0000 (UTC)
Received: (qmail 17392 invoked by uid 500); 1 Jul 2014 02:59:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17322 invoked by uid 500); 1 Jul 2014 02:59:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17310 invoked by uid 99); 1 Jul 2014 02:59:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 02:59:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 02:59:14 +0000
Received: by mail-qc0-f174.google.com with SMTP id x13so7720292qcv.5
        for <dev@spark.apache.org>; Mon, 30 Jun 2014 19:58:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=b7yfUOYqPhzm4hO+hsoWH2zHlh5IIr6DLMv0/pcWD0o=;
        b=K89omy92Yq7j2Ul+YGlmt+v5Q1zEey9JLf7jzoYfkNn0cjLwsl62nzByQkm3YmuBcs
         /Hon/NSwAc58vhXvB0gTUcZJSGgmF73hcCMzNjaaMzZ+zYlB196DXSrFbgcvJkJ108Mg
         WOCPapIS1UDdbn9gel5ocGpHilPxSSCr27UGChY+/McQpZki3LypZ/+z7TPiJ0YFbyDW
         VfvjLAZFvUa586NH8lNvkkFvqvnwHdRXPkO9U4cvY0hZ24hKcftc+hin4fURjiTOxrSC
         OnGSMUy87qP7Xvb2nNYBYk7xW0hUoWbO+2huwjmP49/LwN3PyIyKn58lI9ZG6/Ywsznn
         tIBg==
X-Received: by 10.140.44.101 with SMTP id f92mr61326817qga.89.1404183530110;
 Mon, 30 Jun 2014 19:58:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 30 Jun 2014 19:58:30 -0700 (PDT)
In-Reply-To: <CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
 <CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 30 Jun 2014 19:58:30 -0700
Message-ID: <CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a9d5a8307e104fd18f417
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a9d5a8307e104fd18f417
Content-Type: text/plain; charset=UTF-8

I don't know of any way to avoid Akka doing a copy, but I would like to
mention that it's on the priority list to piggy-back only the map statuses
relevant to a particular map task on the task itself, thus reducing the
total amount of data sent over the wire by a factor of N for N physical
machines in your cluster. Ideally we would also avoid Akka entirely when
sending the tasks, as these can get somewhat large and Akka doesn't work
well with large messages.

Do note that your solution of using broadcast to send the map tasks is very
similar to how the executor returns the result of a task when it's too big
for akka. We were thinking of refactoring this too, as using the block
manager has much higher latency than a direct TCP send.


On Mon, Jun 30, 2014 at 12:13 PM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> Our current hack is to use Broadcast variables when serialized
> statuses are above some (configurable) size : and have the workers
> directly pull them from master.
> This is a workaround : so would be great if there was a
> better/principled solution.
>
> Please note that the responses are going to different workers
> requesting for the output statuses for shuffle (after map) - so not
> sure if back pressure buffers, etc would help.
>
>
> Regards,
> Mridul
>
>
> On Mon, Jun 30, 2014 at 11:07 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
> > Hi,
> >
> >   While sending map output tracker result, the same serialized byte
> > array is sent multiple times - but the akka implementation copies it
> > to a private byte array within ByteString for each send.
> > Caching a ByteString instead of Array[Byte] did not help, since akka
> > does not support special casing ByteString : serializes the
> > ByteString, and copies the result out to an array before creating
> > ByteString out of it (in Array[Byte] serializing is thankfully simply
> > returning same array - so one copy only).
> >
> >
> > Given the need to send immutable data large number of times, is there
> > any way to do it in akka without copying internally in akka ?
> >
> >
> > To see how expensive it is, for 200 nodes withi large number of
> > mappers and reducers, the status becomes something like 30 mb for us -
> > and pulling this about 200 to 300 times results in OOM due to the
> > large number of copies sent out.
> >
> >
> > Thanks,
> > Mridul
>

--001a113a9d5a8307e104fd18f417--

From dev-return-8155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 03:14:17 2014
Return-Path: <dev-return-8155-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4598411BF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 03:14:17 +0000 (UTC)
Received: (qmail 39893 invoked by uid 500); 1 Jul 2014 03:14:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39833 invoked by uid 500); 1 Jul 2014 03:14:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39822 invoked by uid 99); 1 Jul 2014 03:14:15 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 03:14:15 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Bert.Greevenbosch@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 03:14:11 +0000
Received: from 172.24.2.119 (EHLO SZXEMA408-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id ARA28260;
	Tue, 01 Jul 2014 11:13:47 +0800 (CST)
Received: from SZXEMA510-MBX.china.huawei.com ([169.254.3.190]) by
 SZXEMA408-HUB.china.huawei.com ([10.82.72.40]) with mapi id 14.03.0158.001;
 Tue, 1 Jul 2014 11:13:43 +0800
From: Bert Greevenbosch <Bert.Greevenbosch@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Artificial Neural Network in Spark?
Thread-Topic: Artificial Neural Network in Spark?
Thread-Index: Ac+RoTqSDV42paQFQ+OUNd1O9igqiAAKqVEX//984wD/+nPFgA==
Date: Tue, 1 Jul 2014 03:13:42 +0000
Message-ID: <46A1DF3F04371240B504290A071B4DB63E6395A3@SZXEMA510-MBX.china.huawei.com>
References: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
	<092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
 <CA+B-+fy7fw7RVvvvWUOeQTFtZzvSiW2HXPz7TNxGWUFM5UOfug@mail.gmail.com>
In-Reply-To: <CA+B-+fy7fw7RVvvvWUOeQTFtZzvSiW2HXPz7TNxGWUFM5UOfug@mail.gmail.com>
Accept-Language: en-GB, zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.162.63]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A02020A.53B2276C.0017,ss=1,re=0.000,fgs=0,
	ip=169.254.3.190,
	so=2013-05-26 15:14:31,
	dmn=2011-05-27 18:58:46
X-Mirapoint-Loop-Id: 4ea57a73c0f113db63700ca14ab4de43
X-Virus-Checked: Checked by ClamAV on apache.org
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=

From dev-return-8156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 03:18:14 2014
Return-Path: <dev-return-8156-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 922A411C0F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 03:18:14 +0000 (UTC)
Received: (qmail 48376 invoked by uid 500); 1 Jul 2014 03:18:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48312 invoked by uid 500); 1 Jul 2014 03:18:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48301 invoked by uid 99); 1 Jul 2014 03:18:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 03:18:13 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [219.142.118.228] (HELO SINA-HUB02.staff.sina.com.cn) (219.142.118.228)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 03:18:09 +0000
Received: from [10.221.12.94] (10.221.12.94) by mail.staff.sina.com.cn
 (10.210.97.52) with Microsoft SMTP Server (TLS) id 14.2.247.3; Tue, 1 Jul
 2014 11:17:46 +0800
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: Contributing to MLlib on GLM
From: Gang Bai <baigang@staff.sina.com.cn>
In-Reply-To: <B3E02C88-8C42-497B-8D78-3B58186A26BB@gmail.com>
Date: Tue, 1 Jul 2014 11:17:42 +0800
Content-Transfer-Encoding: quoted-printable
Message-ID: <C8CA2150-009B-485C-8DD1-CE1689E75885@staff.sina.com.cn>
References: <CFC6248F.577%xwei@palantir.com> <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com> <1403831040483-7088.post@n3.nabble.com> <D350552E-A930-43C0-AB12-FC7E1957EB78@staff.sina.com.cn> <B3E02C88-8C42-497B-8D78-3B58186A26BB@gmail.com>
To: <dev@spark.apache.org>
X-Mailer: Apple Mail (2.1878.2)
X-Originating-IP: [10.221.12.94]
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks Xiaokai,

I=E2=80=99ve created a pull request to merge features in my PR to your =
repo. Please take a review here =
https://github.com/xwei-datageek/spark/pull/2 .

As for GLMs, here at Sina, we are solving the problem of predicting the =
num of visitors who read a particular news article or watch an online =
sports live stream in a particular period. I=E2=80=99m trying to improve =
the prediction results by tuning features and incorporating new models. =
So I=E2=80=99ll try Gamma regression later. Thanks for the =
implementation.

Cheers,
-Gang

On Jun 29, 2014, at 8:17 AM, xwei <weixiaokai@gmail.com> wrote:

> Hi Gang,
>=20
> No worries!=20
>=20
> I agree LBFGS would converge faster and your test suite is more =
comprehensive. I'd like to merge my branch with yours.
>=20
> I also agree with your viewpoint on the redundancy issue. For =
different GLMs, usually they only differ in gradient calculation but the =
****regression.scala files are quite similar. For example, =
linearRegressionSGD, logisticRegressionSGD, RidgeRegressionSGD, =
poissonRegressionSGD all share quite a bit of common code in their class =
implementations. Since such redundancy is already there in the legacy =
code, simply merging Poisson and Gamma does not seem to help much. So I =
suggest we just leave them as separate classes for the time being.=20
>=20
>=20
> Best regards,
>=20
> Xiaokai
>=20
> On Jun 27, 2014, at 6:45 PM, Gang Bai [via Apache Spark Developers =
List] wrote:
>=20
>> Hi Xiaokai,=20
>>=20
>> My bad. I didn't notice this before I created another PR for Poisson =
regression. The mails were buried in junk by the corp mail master. Also, =
thanks for considering my comments and advice in your PR.=20
>>=20
>> Adding my two cents here:=20
>>=20
>> * PoissonRegressionModel and GammaRegressionModel have the same =
fields and prediction method. Shall we use one instead of two redundant =
classes? Say, a LogLinearModel.=20
>> * The LBFGS optimizer takes fewer iterations and results in better =
convergence than SGD. I implemented two GeneralizedLinearAlgorithm =
classes using LBFGS and SGD respectively. You may take a look into it. =
If it's OK to you, I'd be happy to send a PR to your branch.=20
>> * In addition to the generated test data, We may use some real-world =
data for testing. In my implementation, I added the test data from =
https://onlinecourses.science.psu.edu/stat504/node/223. Please check my =
test suite.=20
>>=20
>> -Gang=20
>> Sent from my iPad=20
>>=20
>>> On 2014=E5=B9=B46=E6=9C=8827=E6=97=A5, at =E4=B8=8B=E5=8D=886:03, =
"xwei" <[hidden email]> wrote:=20
>>>=20
>>>=20
>>> Yes, that's what we did: adding two gradient functions to =
Gradient.scala and=20
>>> create PoissonRegression and GammaRegression using these gradients. =
We made=20
>>> a PR on this.=20
>>>=20
>>>=20
>>>=20
>>> --=20
>>> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7088.html
>>> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.=20
>>=20
>>=20
>> If you reply to this email, your message will be added to the =
discussion below:
>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7107.html
>> To unsubscribe from Contributing to MLlib on GLM, click here.
>> NAML
>=20
>=20
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7117.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.


From dev-return-8157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 04:21:08 2014
Return-Path: <dev-return-8157-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF7FE11D9E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 04:21:08 +0000 (UTC)
Received: (qmail 39453 invoked by uid 500); 1 Jul 2014 04:21:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39384 invoked by uid 500); 1 Jul 2014 04:21:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39367 invoked by uid 99); 1 Jul 2014 04:21:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 04:21:07 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 04:21:03 +0000
Received: by mail-qg0-f53.google.com with SMTP id i50so2889877qgf.12
        for <dev@spark.apache.org>; Mon, 30 Jun 2014 21:20:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=1Sh9ACdJTSVWmML0j9qo1WqHHQpx6ufCIuXptmSAB9U=;
        b=jTlcBnYasgAbqewMwfoS8y0qxhHY3hwfl3WuI1763GwGHGIK4LCFsyataRHCk6HcrS
         vq5g6q/E6o+PJPjscxdkl2Pz7bZIv0rNXxfdHMP5Euj2tIgqEJ4jZa7+1xwvVfUTXUMj
         2bJolpk3ttBZXBpJfwkJ1LU8g+mL9uIXi2N9tVTCFdVEBTuJhnFcxIys1U40VKjvlmys
         DGBDFnH4ThIpH6F8ZDEjQG3mhsKyBrasP/7nTngBmeXoS9SGp9rvr1of+6nq4pUqhj2c
         99T/nFi5cEo1zmAHMV+3mT0klfMxNNRzpzJmSzA/UNMuJzBPr7oVi833WWjSdbX+UDSv
         CdXQ==
MIME-Version: 1.0
X-Received: by 10.140.102.15 with SMTP id v15mr6101490qge.93.1404188442979;
 Mon, 30 Jun 2014 21:20:42 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Mon, 30 Jun 2014 21:20:42 -0700 (PDT)
In-Reply-To: <46A1DF3F04371240B504290A071B4DB63E6395A3@SZXEMA510-MBX.china.huawei.com>
References: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
	<092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
	<CA+B-+fy7fw7RVvvvWUOeQTFtZzvSiW2HXPz7TNxGWUFM5UOfug@mail.gmail.com>
	<46A1DF3F04371240B504290A071B4DB63E6395A3@SZXEMA510-MBX.china.huawei.com>
Date: Mon, 30 Jun 2014 21:20:42 -0700
Message-ID: <CA+B-+fxMxTuMNGYR8r2q0dBBwe+G1hVrZVO0oT0gPybNVMoY0Q@mail.gmail.com>
Subject: Re: Artificial Neural Network in Spark?
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1721a57748304fd1a1962
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1721a57748304fd1a1962
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I will let Xiangrui to comment on the PR process to add the code in mllib
but I would love to look into your initial version if you push it to
github...

As far as I remember Quoc got his best ANN results using back-propagation
algorithm and solved using CG...do you have those features or you are using
SGD style update....



On Mon, Jun 30, 2014 at 8:13 PM, Bert Greevenbosch <
Bert.Greevenbosch@huawei.com> wrote:

> Hi Debasish, Alexander, all,
>
> Indeed I found the OpenDL project through the Powered by Spark page. I'll
> need some time to look into the code, but on the first sight it looks qui=
te
> well-developed. I'll contact the author about this too.
>
> My own implementation (in Scala) works for multiple inputs and multiple
> outputs. It implements a single hidden layer, the number of nodes in it c=
an
> be specified.
>
> The implementation is a general ANN implementation. As such, it should be
> useable for an autoencoder too, since that is just an ANN with some speci=
al
> input/output constraints.
>
> As said before, the implementation is built upon the linear regression
> model and gradient descent implementation. However it did require some
> tweaks:
>
> - The linear regression model only supports a single output "label" (as
> Double). Since the ANN can have multiple outputs, it ignores the "label"
> attribute, but for training divides the input vector into two parts, the
> first part being the genuine input vector, the second the target output
> vector.
>
> - The concatenation of input and target output vectors is only internally=
,
> the training function takes as input an RDD with tuples of two Vectors, o=
ne
> for each input and output.
>
> - The GradientDescend optimizer is re-used without modification.
>
> - I have made an even simpler updater than the SimpleUpdater, leaving out
> the division by the square root of the number of iterations. The
> SimpleUpdater can also be used, but I created this simpler one because I
> like to plot the result every now and then, and then continue the
> calculations. For this, I also wrote a training function with as input th=
e
> weights from the previous training session.
>
> - I created a ParallelANNModel similar to the LinearRegressionModel.
>
> - I created a new GeneralizedSteepestDescendAlgorithm class similar to th=
e
> GeneralizedLinearAlgorithm class.
>
> - Created some example code to test with 2D (1 input 1 output), 3D (2
> inputs 1 output) and 4D (1 input 3 outputs) functions.
>
> If there is interest, I would be happy to release the code. What would be
> the best way to do this? Is there some kind of review process?
>
> Best regards,
> Bert
>
>
> > -----Original Message-----
> > From: Debasish Das [mailto:debasish.das83@gmail.com]
> > Sent: 27 June 2014 14:02
> > To: dev@spark.apache.org
> > Subject: Re: Artificial Neural Network in Spark?
> >
> > Look into Powered by Spark page...I found a project there which used
> > autoencoder functions...It's not updated for a long time now !
> >
> > On Thu, Jun 26, 2014 at 10:51 PM, Ulanov, Alexander
> > <alexander.ulanov@hp.com
> > > wrote:
> >
> > > Hi Bert,
> > >
> > > It would be extremely interesting. Do you plan to implement
> > autoencoder as
> > > well? It would be great to have deep learning in Spark.
> > >
> > > Best regards, Alexander
> > >
> > > 27.06.2014, =D0=B2 4:47, "Bert Greevenbosch" <Bert.Greevenbosch@huawe=
i.com>
> > > =D0=BD=D0=B0=D0=BF=D0=B8=D1=81=D0=B0=D0=BB(=D0=B0):
> > >
> > > > Hello all,
> > > >
> > > > I was wondering whether Spark/mllib supports Artificial Neural
> > Networks
> > > (ANNs)?
> > > >
> > > > If not, I am currently working on an implementation of it. I re-use
> > the
> > > code for linear regression and gradient descent as much as possible.
> > > >
> > > > Would the community be interested in such implementation? Or maybe
> > > somebody is already working on it?
> > > >
> > > > Best regards,
> > > > Bert
> > >
>

--001a11c1721a57748304fd1a1962--

From dev-return-8158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 08:00:47 2014
Return-Path: <dev-return-8158-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E06B810474
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 08:00:46 +0000 (UTC)
Received: (qmail 44114 invoked by uid 500); 1 Jul 2014 08:00:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44050 invoked by uid 500); 1 Jul 2014 08:00:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44031 invoked by uid 99); 1 Jul 2014 08:00:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:00:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of taka.epsilon@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:00:36 +0000
Received: by mail-oa0-f44.google.com with SMTP id i7so10163572oag.3
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 01:00:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=xOBS1revLnXgmLglrtR5K5Gy/urnNX8VTbHqRwNETR4=;
        b=Uya/T5OFr3KQOwk1QIvnQt0IQ2mfLuUAVkpz+q8n0YL6H9xLWjIBJRHTtoeehHKEIb
         0/M8B+jmFnyZof3boKu27mZXRSi2r2xg1a0SPZ9ZCA4aZippKiq2l/RBZX1Bz9FKREHC
         y3X/qU3Gm5AXwni+26xGgYZLk08wkMSn2b8q+Uh78dkfm5r77G8ObP9YXBi4hXN0ipIX
         Cy/ZOaX8v1XfAigTtJkPYZnc6LW4yAq/6XwQDa6ahpor2fcBM5BpsEpzPbymlitXG63A
         SADGM1P9J7kPNjML5BFY5IBWwYH20mJUClNlXBEeV5rAYneiaGLaFUETB4JiAikHP6Dp
         UlWw==
MIME-Version: 1.0
X-Received: by 10.182.191.106 with SMTP id gx10mr14082076obc.69.1404201611017;
 Tue, 01 Jul 2014 01:00:11 -0700 (PDT)
Received: by 10.182.128.232 with HTTP; Tue, 1 Jul 2014 01:00:10 -0700 (PDT)
Date: Tue, 1 Jul 2014 01:00:10 -0700
Message-ID: <CALkvKbnEbSfMVfJp6NCM-q6X8UQq-JHDdMpiw04190p4wCe0EQ@mail.gmail.com>
Subject: Errors from Sbt Test
From: Taka Shinagawa <taka.epsilon@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/mixed; boundary=089e0158b20e3866a204fd1d2ae2
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158b20e3866a204fd1d2ae2
Content-Type: multipart/alternative; boundary=089e0158b20e38669d04fd1d2ae0

--089e0158b20e38669d04fd1d2ae0
Content-Type: text/plain; charset=UTF-8

Since Spark 1.0.0, I've been seeing multiple errors when running sbt test.

I ran the following commands from Spark 1.0.1 RC1 on Mac OSX 10.9.2.

$ sbt/sbt clean
$ SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly
$ sbt/sbt test


I'm attaching the log file generated by the sbt test.

Here's the summary part of the test.

[info] Run completed in 30 minutes, 57 seconds.
[info] Total number of tests run: 605
[info] Suites: completed 83, aborted 0
[info] Tests: succeeded 600, failed 5, canceled 0, ignored 5, pending 0
[info] *** 5 TESTS FAILED ***
[error] Failed: Total 653, Failed 5, Errors 0, Passed 648, Ignored 5
[error] Failed tests:
[error] org.apache.spark.ShuffleNettySuite
[error] org.apache.spark.ShuffleSuite
[error] org.apache.spark.FileServerSuite
[error] org.apache.spark.DistributedSuite
[error] (core/test:test) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 2033 s, completed Jul 1, 2014 12:08:03 AM

Is anyone else seeing errors like this?


Thanks,
Taka

--089e0158b20e38669d04fd1d2ae0
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Since Spark 1.0.0, I&#39;ve been seeing multiple errors wh=
en running sbt test.<div><br></div><div>I ran the following commands from S=
park 1.0.1 RC1 on Mac OSX 10.9.2.=C2=A0</div><div><br></div><div>$ sbt/sbt =
clean</div>
<div>$=C2=A0SPARK_HADOOP_VERSION=3D1.2.1 sbt/sbt assembly</div><div>$ sbt/s=
bt test</div><div><br></div><div><br></div><div>I&#39;m attaching the log f=
ile generated by the sbt test.</div><div><br></div><div>Here&#39;s the summ=
ary part of the test.</div>
<div><br></div><div><div>[info] Run completed in 30 minutes, 57 seconds.</d=
iv><div>[info] Total number of tests run: 605</div><div>[info] Suites: comp=
leted 83, aborted 0</div><div>[info] Tests: succeeded 600, failed 5, cancel=
ed 0, ignored 5, pending 0</div>
<div>[info] *** 5 TESTS FAILED ***</div><div>[error] Failed: Total 653, Fai=
led 5, Errors 0, Passed 648, Ignored 5</div><div>[error] Failed tests:</div=
><div>[error] <span class=3D"" style=3D"white-space:pre">	</span>org.apache=
.spark.ShuffleNettySuite</div>
<div>[error] <span class=3D"" style=3D"white-space:pre">	</span>org.apache.=
spark.ShuffleSuite</div><div>[error] <span class=3D"" style=3D"white-space:=
pre">	</span>org.apache.spark.FileServerSuite</div><div>[error] <span class=
=3D"" style=3D"white-space:pre">	</span>org.apache.spark.DistributedSuite</=
div>
<div>[error] (core/test:test) sbt.TestsFailedException: Tests unsuccessful<=
/div><div>[error] Total time: 2033 s, completed Jul 1, 2014 12:08:03 AM</di=
v></div><div><br></div><div>Is anyone else seeing errors like this?</div>
<div><br></div><div><br></div><div>Thanks,</div><div>Taka</div></div>

--089e0158b20e38669d04fd1d2ae0--
--089e0158b20e3866a204fd1d2ae2--

From dev-return-8159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 08:04:50 2014
Return-Path: <dev-return-8159-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 626451049F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 08:04:50 +0000 (UTC)
Received: (qmail 55301 invoked by uid 500); 1 Jul 2014 08:04:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55236 invoked by uid 500); 1 Jul 2014 08:04:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55218 invoked by uid 99); 1 Jul 2014 08:04:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:04:48 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.179 as permitted sender)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:04:42 +0000
Received: by mail-ob0-f179.google.com with SMTP id uz6so10116904obc.38
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 01:04:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=4R+pGddP4bQAk6jqM1OIT9nfxvKwHIoLawTJJua8I20=;
        b=ysZZaAJe3Vl+72lh3cqNJf3DfGWk5MVRMvr1eb8h8UGM6a/xv4jVSrTQbGdDtW1hiY
         aL4W/Y/UoMsoZAcsnw2s7xX/W8SDhm/JzlVPbz7AHdkODox4KbMk7YpQdanxIRK0q3by
         UtVT0uqL6dz3nzoqmVqS81snq8PiFH3olCrqwiSjRvBXssSG6ckCKh+7BJrIKPj+qXm/
         Xwi9Wspaew8fFOxqia7AmAy6zlhgaa4sWKjerEqjJPz4ck8SWXPQTrzMedMG5zZVmQ+/
         PCUiNT0BvUdD4LhbOz8STMDPFD9ePpXjcQAKqyzaUk54RAwE5vCqtNtFatIPeF5hhrJ/
         AsYA==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr8529355obc.75.1404201862342;
 Tue, 01 Jul 2014 01:04:22 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 1 Jul 2014 01:04:22 -0700 (PDT)
In-Reply-To: <CALkvKbnEbSfMVfJp6NCM-q6X8UQq-JHDdMpiw04190p4wCe0EQ@mail.gmail.com>
References: <CALkvKbnEbSfMVfJp6NCM-q6X8UQq-JHDdMpiw04190p4wCe0EQ@mail.gmail.com>
Date: Tue, 1 Jul 2014 01:04:22 -0700
Message-ID: <CABPQxssPy8jYy+Nebzde1h0TZE2gr5JU8T3e9aeF=3-Cg=VRXg@mail.gmail.com>
Subject: Re: Errors from Sbt Test
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Do those also happen if you run other hadoop versions (e.g. try 1.0.4)?

On Tue, Jul 1, 2014 at 1:00 AM, Taka Shinagawa <taka.epsilon@gmail.com> wrote:
> Since Spark 1.0.0, I've been seeing multiple errors when running sbt test.
>
> I ran the following commands from Spark 1.0.1 RC1 on Mac OSX 10.9.2.
>
> $ sbt/sbt clean
> $ SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly
> $ sbt/sbt test
>
>
> I'm attaching the log file generated by the sbt test.
>
> Here's the summary part of the test.
>
> [info] Run completed in 30 minutes, 57 seconds.
> [info] Total number of tests run: 605
> [info] Suites: completed 83, aborted 0
> [info] Tests: succeeded 600, failed 5, canceled 0, ignored 5, pending 0
> [info] *** 5 TESTS FAILED ***
> [error] Failed: Total 653, Failed 5, Errors 0, Passed 648, Ignored 5
> [error] Failed tests:
> [error] org.apache.spark.ShuffleNettySuite
> [error] org.apache.spark.ShuffleSuite
> [error] org.apache.spark.FileServerSuite
> [error] org.apache.spark.DistributedSuite
> [error] (core/test:test) sbt.TestsFailedException: Tests unsuccessful
> [error] Total time: 2033 s, completed Jul 1, 2014 12:08:03 AM
>
> Is anyone else seeing errors like this?
>
>
> Thanks,
> Taka

From dev-return-8160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 08:06:54 2014
Return-Path: <dev-return-8160-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F2BA104A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 08:06:54 +0000 (UTC)
Received: (qmail 59162 invoked by uid 500); 1 Jul 2014 08:06:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59100 invoked by uid 500); 1 Jul 2014 08:06:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59088 invoked by uid 99); 1 Jul 2014 08:06:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:06:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:06:47 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so10052753oag.19
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 01:06:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=sVhioM2MYCBbYcrxF5MfzbOGsRyPp0hConK/gP5de3M=;
        b=e1YNFxO19BRjmq6CDW+mgJW5/J3fsStCdOvjdskQn1NkD6g0l/1dUqmhj7g/qLjd/b
         9HwoIaSwmt2ZFgeAUi3mM4iKZp2tnkpDO2l9NajAd0F8p7W+2GA9Z+sWFdH6ZCu7SE04
         OXY0+APLwd2ZLIcFw0em1PxFog8VpmUOvwU3/C4AItAjioN7srb0ngmEhUJPPXhSzf3M
         EH0QVBBofbeLygZdVvrs68v/9wKiz+Z4WJi4tD7KQ8dNtwzvzPkdHZMRlGAAGjT1zi+v
         yb9bRAafszwPsUNoLKkoFN0KcZsstUffoc+pFqFQTCsRU8tJiZNBKvTFo1XWArFF3aE5
         oDvw==
MIME-Version: 1.0
X-Received: by 10.182.232.135 with SMTP id to7mr8791393obc.73.1404201987439;
 Tue, 01 Jul 2014 01:06:27 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 1 Jul 2014 01:06:27 -0700 (PDT)
In-Reply-To: <CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
	<CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
	<CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
Date: Tue, 1 Jul 2014 01:06:27 -0700
Message-ID: <CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah I created a JIRA a while back to piggy-back the map status info
on top of the task (I honestly think it will be a small change). There
isn't a good reason to broadcast the entire array and it can be an
issue during large shuffles.

- Patrick

On Mon, Jun 30, 2014 at 7:58 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> I don't know of any way to avoid Akka doing a copy, but I would like to
> mention that it's on the priority list to piggy-back only the map statuses
> relevant to a particular map task on the task itself, thus reducing the
> total amount of data sent over the wire by a factor of N for N physical
> machines in your cluster. Ideally we would also avoid Akka entirely when
> sending the tasks, as these can get somewhat large and Akka doesn't work
> well with large messages.
>
> Do note that your solution of using broadcast to send the map tasks is very
> similar to how the executor returns the result of a task when it's too big
> for akka. We were thinking of refactoring this too, as using the block
> manager has much higher latency than a direct TCP send.
>
>
> On Mon, Jun 30, 2014 at 12:13 PM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>
>> Our current hack is to use Broadcast variables when serialized
>> statuses are above some (configurable) size : and have the workers
>> directly pull them from master.
>> This is a workaround : so would be great if there was a
>> better/principled solution.
>>
>> Please note that the responses are going to different workers
>> requesting for the output statuses for shuffle (after map) - so not
>> sure if back pressure buffers, etc would help.
>>
>>
>> Regards,
>> Mridul
>>
>>
>> On Mon, Jun 30, 2014 at 11:07 PM, Mridul Muralidharan <mridul@gmail.com>
>> wrote:
>> > Hi,
>> >
>> >   While sending map output tracker result, the same serialized byte
>> > array is sent multiple times - but the akka implementation copies it
>> > to a private byte array within ByteString for each send.
>> > Caching a ByteString instead of Array[Byte] did not help, since akka
>> > does not support special casing ByteString : serializes the
>> > ByteString, and copies the result out to an array before creating
>> > ByteString out of it (in Array[Byte] serializing is thankfully simply
>> > returning same array - so one copy only).
>> >
>> >
>> > Given the need to send immutable data large number of times, is there
>> > any way to do it in akka without copying internally in akka ?
>> >
>> >
>> > To see how expensive it is, for 200 nodes withi large number of
>> > mappers and reducers, the status becomes something like 30 mb for us -
>> > and pulling this about 200 to 300 times results in OOM due to the
>> > large number of copies sent out.
>> >
>> >
>> > Thanks,
>> > Mridul
>>

From dev-return-8161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 08:11:21 2014
Return-Path: <dev-return-8161-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B7AD5104C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 08:11:21 +0000 (UTC)
Received: (qmail 66364 invoked by uid 500); 1 Jul 2014 08:11:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66296 invoked by uid 500); 1 Jul 2014 08:11:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66280 invoked by uid 99); 1 Jul 2014 08:11:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:11:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of taka.epsilon@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:11:15 +0000
Received: by mail-ob0-f182.google.com with SMTP id nu7so10084034obb.13
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 01:10:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=vG/bWPN239PeVG/YRV6sUFB7ihD73IZ1EWxuWxXkA5g=;
        b=IjHkPfVYqT6Odstm4lFrk/1nwGAcQpMZH4ZQhhm7JIPy8W7LToBvfJuTeiUuEbU78P
         Iagoul7m80hasCm9tLbzFZKDo8zRUMbSv2M/Fnv+EhhWAKlbCZxlhS5Rx2EkBBjCiCdH
         ZfU3+lFUr9p+2EgRL+I1l7fpbpJk8EuW7lHtu/Fv4SmTrsWCmVrHZZ7dfgEG0mKLMSyM
         ClLGO5zfArb5dVVCFUmlJTEiK/T/gUdUnIwEuphIDaiqpW7e0e9mQVe47x4i5tlmpd3W
         +ek9175Ju7/WQuREnFxxG2ISmtyqt0tTIjk388z/oAv8NO9JN+aFBsSS63syNOBvU5gx
         dlLg==
MIME-Version: 1.0
X-Received: by 10.182.228.163 with SMTP id sj3mr8843304obc.72.1404202254818;
 Tue, 01 Jul 2014 01:10:54 -0700 (PDT)
Received: by 10.182.128.232 with HTTP; Tue, 1 Jul 2014 01:10:54 -0700 (PDT)
In-Reply-To: <CABPQxssPy8jYy+Nebzde1h0TZE2gr5JU8T3e9aeF=3-Cg=VRXg@mail.gmail.com>
References: <CALkvKbnEbSfMVfJp6NCM-q6X8UQq-JHDdMpiw04190p4wCe0EQ@mail.gmail.com>
	<CABPQxssPy8jYy+Nebzde1h0TZE2gr5JU8T3e9aeF=3-Cg=VRXg@mail.gmail.com>
Date: Tue, 1 Jul 2014 01:10:54 -0700
Message-ID: <CALkvKb=A89WEzy4-60fmGKpYmo3-mng=fqp+E4T7Tai6DFeJdA@mail.gmail.com>
Subject: Re: Errors from Sbt Test
From: Taka Shinagawa <taka.epsilon@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1136358a97625e04fd1d504c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136358a97625e04fd1d504c
Content-Type: text/plain; charset=UTF-8

If I remember correctly, similar/same errors happened with other hadoop
versions. I need to rebuild it with those and compare the logs.


On Tue, Jul 1, 2014 at 1:04 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Do those also happen if you run other hadoop versions (e.g. try 1.0.4)?
>
> On Tue, Jul 1, 2014 at 1:00 AM, Taka Shinagawa <taka.epsilon@gmail.com>
> wrote:
> > Since Spark 1.0.0, I've been seeing multiple errors when running sbt
> test.
> >
> > I ran the following commands from Spark 1.0.1 RC1 on Mac OSX 10.9.2.
> >
> > $ sbt/sbt clean
> > $ SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly
> > $ sbt/sbt test
> >
> >
> > I'm attaching the log file generated by the sbt test.
> >
> > Here's the summary part of the test.
> >
> > [info] Run completed in 30 minutes, 57 seconds.
> > [info] Total number of tests run: 605
> > [info] Suites: completed 83, aborted 0
> > [info] Tests: succeeded 600, failed 5, canceled 0, ignored 5, pending 0
> > [info] *** 5 TESTS FAILED ***
> > [error] Failed: Total 653, Failed 5, Errors 0, Passed 648, Ignored 5
> > [error] Failed tests:
> > [error] org.apache.spark.ShuffleNettySuite
> > [error] org.apache.spark.ShuffleSuite
> > [error] org.apache.spark.FileServerSuite
> > [error] org.apache.spark.DistributedSuite
> > [error] (core/test:test) sbt.TestsFailedException: Tests unsuccessful
> > [error] Total time: 2033 s, completed Jul 1, 2014 12:08:03 AM
> >
> > Is anyone else seeing errors like this?
> >
> >
> > Thanks,
> > Taka
>

--001a1136358a97625e04fd1d504c--

From dev-return-8162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 08:25:33 2014
Return-Path: <dev-return-8162-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5B8CF1059F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 08:25:33 +0000 (UTC)
Received: (qmail 91178 invoked by uid 500); 1 Jul 2014 08:25:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91117 invoked by uid 500); 1 Jul 2014 08:25:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91089 invoked by uid 99); 1 Jul 2014 08:25:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:25:32 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 08:25:27 +0000
Received: by mail-wg0-f46.google.com with SMTP id y10so9331728wgg.29
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 01:25:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=oqJyk5cvo7wCNwISna06Im5NdKx4t3GgHQY+ziM138Q=;
        b=Jrqnhuazt1jLwZDRj4OflltUSCTDxM26IU0ByHquLasHXtzmMaGZ501LNZM8UfrPbB
         xepG2bzbC5FYyUQV0lQmCZLnFQA4Gu8fYx8BWcJ4dBkspqp0nZIu/yElEygM6qwLye5M
         5YhM0ID1z5ryoBSW4T1uWsgmBpwGrbQ5u8cxkXbV78SmDKApxmD4hP6Tvfq47DtWvbEY
         EOzKZpdAlQYujerefXAH7NSBZWi2D9l5cN7ydGpBcQyARwtK7UC/iAXDUR47oM374jhX
         j+JxPPUAo3GslwM61VitM0LnGoH5I8qviKpu1dsBLrp31HWN47F8Q9JN0DAcQhPaSBW+
         pXbg==
MIME-Version: 1.0
X-Received: by 10.180.73.106 with SMTP id k10mr34041991wiv.11.1404203106002;
 Tue, 01 Jul 2014 01:25:06 -0700 (PDT)
Received: by 10.194.86.166 with HTTP; Tue, 1 Jul 2014 01:25:05 -0700 (PDT)
Date: Tue, 1 Jul 2014 16:25:05 +0800
Message-ID: <CABDsqqZ3vsH-ebfjZWJHKhfFQoFiJZYko-eLEP_RT3Z278g0xQ@mail.gmail.com>
Subject: task always lost
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043749c7536a3c04fd1d83d0
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043749c7536a3c04fd1d83d0
Content-Type: text/plain; charset=UTF-8

i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started, when I
using spark-shell to submit one job, the tasks always lost.  here is the
log:
----------
14/07/01 16:24:27 INFO DAGScheduler: Host gained which was in lost list
earlier: bigdata005
14/07/01 16:24:27 INFO TaskSetManager: Starting task 0.0:1 as TID 4042 on
executor 20140616-143932-1694607552-5050-4080-2: bigdata005 (PROCESS_LOCAL)
14/07/01 16:24:27 INFO TaskSetManager: Serialized task 0.0:1 as 1570 bytes
in 0 ms
14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
20140616-104524-1694607552-5050-26919-1 from TaskSet 0.0
14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4041 (task 0.0:0)
14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
20140616-104524-1694607552-5050-26919-1 (epoch 3427)
14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
14/07/01 16:24:28 INFO BlockManagerMaster: Removed
20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
20140616-143932-1694607552-5050-4080-2 from TaskSet 0.0
14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4042 (task 0.0:1)
14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
20140616-143932-1694607552-5050-4080-2 (epoch 3428)
14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
20140616-143932-1694607552-5050-4080-2 from BlockManagerMaster.
14/07/01 16:24:28 INFO BlockManagerMaster: Removed
20140616-143932-1694607552-5050-4080-2 successfully in removeExecutor
14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
earlier: bigdata005
14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
earlier: bigdata001
14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:1 as TID 4043 on
executor 20140616-143932-1694607552-5050-4080-2: bigdata005 (PROCESS_LOCAL)
14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:1 as 1570 bytes
in 0 ms
14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:0 as TID 4044 on
executor 20140616-104524-1694607552-5050-26919-1: bigdata001 (PROCESS_LOCAL)
14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:0 as 1570 bytes
in 0 ms


it seems other guy has also encountered such problem,
http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201305.mbox/%3C201305161047069952830@nfs.iscas.ac.cn%3E

--f46d043749c7536a3c04fd1d83d0--

From dev-return-8163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 09:51:51 2014
Return-Path: <dev-return-8163-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D3C231084C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 09:51:51 +0000 (UTC)
Received: (qmail 75377 invoked by uid 500); 1 Jul 2014 09:51:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75323 invoked by uid 500); 1 Jul 2014 09:51:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75306 invoked by uid 99); 1 Jul 2014 09:51:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 09:51:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.170 as permitted sender)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 09:51:45 +0000
Received: by mail-qc0-f170.google.com with SMTP id l6so8332097qcy.29
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 02:51:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=CUAzOInLCS3TumLnibNfaYk0LnYZBdWycmk/p7BrlKI=;
        b=buUwqOAxAs3mcN1L4NP9AmMygH3xaesFqhOEE9WPd/EOcxTP+sRSA82u9k4/naW0a1
         CHivjZv0cnkWR9y5V1aai+Bqe99lGJLWE7M29/xmlrbJFvWzC4jqcfuvB0IXutLt35OQ
         1V9YK5vKw+69IP5PDarrzvM+Pmx+HfdwCrXH6cierwWbcfMyC+MQGjTDXyPdG1RNtfTE
         j9wvinGec8UInlh5o0HWT99eBI56eZ7o2JSuif76VHpKGrRMwADxYNIv8RT9b2Sicp+I
         AEWkZ0bJ0wD5qCCnjFabMZGUSGeKHWDJe+WCYAU7QYRL3LXwyFMlEPieKGxDX+t8xXsj
         FeGg==
MIME-Version: 1.0
X-Received: by 10.140.91.66 with SMTP id y60mr65515799qgd.58.1404208284874;
 Tue, 01 Jul 2014 02:51:24 -0700 (PDT)
Received: by 10.140.38.149 with HTTP; Tue, 1 Jul 2014 02:51:24 -0700 (PDT)
In-Reply-To: <CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
	<CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
	<CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
	<CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
Date: Tue, 1 Jul 2014 15:21:24 +0530
Message-ID: <CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

We had considered both approaches (if I understood the suggestions right) :
a) Pulling only map output states for tasks which run on the reducer
by modifying the Actor. (Probably along lines of what Aaron described
?)
The performance implication of this was bad :
1) We cant cache serialized result anymore, (caching it makes no sense rather).
2) The number requests to master will go from num_executors to
num_reducers - the latter can be orders of magnitude higher than
former.

b) Instead of pulling this information, push it to executors as part
of task submission. (What Patrick mentioned ?)
(1) a.1 from above is still an issue for this.
(2) Serialized task size is also a concern : we have already seen
users hitting akka limits for task size - this will be an additional
vector which might exacerbate it.
Our jobs are not hitting this yet though !

I was hoping there might be something in akka itself to alleviate this
- but if not, we can solve it within context of spark.

Currently, we have worked around it by using broadcast variable when
serialized size is above some threshold - so that our immediate
concerns are unblocked :-)
But a better solution should be greatly welcomed !
Maybe we can unify it with large serialized task as well ...


Btw, I am not sure what the higher cost of BlockManager referred to is
Aaron - do you mean the cost of persisting the serialized map outputs
to disk ?




Regards,
Mridul


On Tue, Jul 1, 2014 at 1:36 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Yeah I created a JIRA a while back to piggy-back the map status info
> on top of the task (I honestly think it will be a small change). There
> isn't a good reason to broadcast the entire array and it can be an
> issue during large shuffles.
>
> - Patrick
>
> On Mon, Jun 30, 2014 at 7:58 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
>> I don't know of any way to avoid Akka doing a copy, but I would like to
>> mention that it's on the priority list to piggy-back only the map statuses
>> relevant to a particular map task on the task itself, thus reducing the
>> total amount of data sent over the wire by a factor of N for N physical
>> machines in your cluster. Ideally we would also avoid Akka entirely when
>> sending the tasks, as these can get somewhat large and Akka doesn't work
>> well with large messages.
>>
>> Do note that your solution of using broadcast to send the map tasks is very
>> similar to how the executor returns the result of a task when it's too big
>> for akka. We were thinking of refactoring this too, as using the block
>> manager has much higher latency than a direct TCP send.
>>
>>
>> On Mon, Jun 30, 2014 at 12:13 PM, Mridul Muralidharan <mridul@gmail.com>
>> wrote:
>>
>>> Our current hack is to use Broadcast variables when serialized
>>> statuses are above some (configurable) size : and have the workers
>>> directly pull them from master.
>>> This is a workaround : so would be great if there was a
>>> better/principled solution.
>>>
>>> Please note that the responses are going to different workers
>>> requesting for the output statuses for shuffle (after map) - so not
>>> sure if back pressure buffers, etc would help.
>>>
>>>
>>> Regards,
>>> Mridul
>>>
>>>
>>> On Mon, Jun 30, 2014 at 11:07 PM, Mridul Muralidharan <mridul@gmail.com>
>>> wrote:
>>> > Hi,
>>> >
>>> >   While sending map output tracker result, the same serialized byte
>>> > array is sent multiple times - but the akka implementation copies it
>>> > to a private byte array within ByteString for each send.
>>> > Caching a ByteString instead of Array[Byte] did not help, since akka
>>> > does not support special casing ByteString : serializes the
>>> > ByteString, and copies the result out to an array before creating
>>> > ByteString out of it (in Array[Byte] serializing is thankfully simply
>>> > returning same array - so one copy only).
>>> >
>>> >
>>> > Given the need to send immutable data large number of times, is there
>>> > any way to do it in akka without copying internally in akka ?
>>> >
>>> >
>>> > To see how expensive it is, for 200 nodes withi large number of
>>> > mappers and reducers, the status becomes something like 30 mb for us -
>>> > and pulling this about 200 to 300 times results in OOM due to the
>>> > large number of copies sent out.
>>> >
>>> >
>>> > Thanks,
>>> > Mridul
>>>

From dev-return-8164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 10:18:08 2014
Return-Path: <dev-return-8164-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 72B17108F6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 10:18:08 +0000 (UTC)
Received: (qmail 31100 invoked by uid 500); 1 Jul 2014 10:18:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31041 invoked by uid 500); 1 Jul 2014 10:18:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31030 invoked by uid 99); 1 Jul 2014 10:18:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 10:18:07 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 10:18:01 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id 7FF57225
	for <dev@spark.apache.org>; Tue,  1 Jul 2014 10:17:40 +0000 (UTC)
Received: from G4W6306.americas.hpqcorp.net (16.210.26.231) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Tue, 1 Jul 2014 10:16:55 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.241]) by
 G4W6306.americas.hpqcorp.net ([16.210.26.231]) with mapi id 14.03.0169.001;
 Tue, 1 Jul 2014 10:16:55 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Artificial Neural Network in Spark?
Thread-Topic: Artificial Neural Network in Spark?
Thread-Index: Ac+RoTqSDV42paQFQ+OUNd1O9igqiAAKqVEXAABgAgAAw0WGAAAOhS/w
Date: Tue, 1 Jul 2014 10:16:54 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCA9253@G4W3292.americas.hpqcorp.net>
References: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
	<092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
 <CA+B-+fy7fw7RVvvvWUOeQTFtZzvSiW2HXPz7TNxGWUFM5UOfug@mail.gmail.com>
 <46A1DF3F04371240B504290A071B4DB63E6395A3@SZXEMA510-MBX.china.huawei.com>
In-Reply-To: <46A1DF3F04371240B504290A071B4DB63E6395A3@SZXEMA510-MBX.china.huawei.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.18]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-8165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 13:48:01 2014
Return-Path: <dev-return-8165-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0943610F7C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 13:48:01 +0000 (UTC)
Received: (qmail 64706 invoked by uid 500); 1 Jul 2014 13:48:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64650 invoked by uid 500); 1 Jul 2014 13:48:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 14034 invoked by uid 99); 1 Jul 2014 12:48:00 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ajayydv@gmail.com designates 209.85.216.53 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=YY7/sFFvai2phyMOc5jLOm7lqyo9UYZ7zaWxlTheAmg=;
        b=eUmBqyAVoO0aHAWSPY/Ew6tz75cX6N7MoevDmdNbG5RzFdp0pxCVvE3Bsqyl1b5Umh
         TDRhKHmMNZLi4CA2Ic6zxF5VOShi4PmZo4gvC8Zzh3htgGymTCUdBkNXLzqGcJCxZef1
         fe0nEtTn8BAtuhrv651XEEaSMjKmfNVeNQ5+hzb5OOG9f+1HW2XNsWjA8zJgR25mLo39
         3copinLdXfJaeVUXwy3J9bAWxuShDJ+T1kJrMZsRenPJ/Qta3KR2yeFEVIa6klLCHRnG
         MMA/UMUfOnOHWRQ6H0dpn9v4gDBEkBgRWxcLFQGe139h/Wu9wyaDX7d5+pcC2+8RjezK
         azLw==
X-Received: by 10.224.3.201 with SMTP id 9mr70955412qao.73.1404218854496; Tue,
 01 Jul 2014 05:47:34 -0700 (PDT)
MIME-Version: 1.0
From: ajay yadav <ajayydv@gmail.com>
Date: Tue, 1 Jul 2014 18:17:04 +0530
Message-ID: <CALQU-Trigm=Jdnks+fAPv2FbiPSYZmEFLRhSLbcU2T9rQbX9bA@mail.gmail.com>
Subject: send this email to subscribe
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c24cfc0260ab04fd212efe
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24cfc0260ab04fd212efe
Content-Type: text/plain; charset=UTF-8



--001a11c24cfc0260ab04fd212efe--

From dev-return-8166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 17:46:30 2014
Return-Path: <dev-return-8166-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB4D911951
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 17:46:30 +0000 (UTC)
Received: (qmail 97072 invoked by uid 500); 1 Jul 2014 17:46:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97011 invoked by uid 500); 1 Jul 2014 17:46:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96993 invoked by uid 99); 1 Jul 2014 17:46:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 17:46:29 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 17:46:24 +0000
Received: by mail-qg0-f50.google.com with SMTP id j5so3583421qga.23
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 10:46:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=xsTdeWqej/dHIeLqmBH2infBgcSzd5zS6mS233ToSYg=;
        b=fSL3CULcEFs4RYfY/nSiIqfs4fBGTACxstBpeTBatnCmlFeOoAOsZRXFm9rrUEOsnd
         61pX3R+d8/Vo5rYNi+9m7IjzikbVTonhs95EuU1X5/61TldPgve3EheZLSONdTkpcy2F
         Kgmf/Tmi3XFimC1w8B6UnOnacyRyX/v9Czl0BozCSHukTgPnrVD1R6KlxvHw6YCJpfsE
         aqeMTiyl930b1hQtH1K/yyTlbypyBUrpsJqBCcWYvHai80ij9jGYo0+3VvzM6ulZq/fV
         GD8pzzTvFzKVjfJYOdIFOMmXwIlDbqEnUzZbZbUvmAyTS6FvzoX67y5IARixSW3xPlRj
         cf/w==
X-Received: by 10.229.51.201 with SMTP id e9mr72454382qcg.2.1404236763852;
 Tue, 01 Jul 2014 10:46:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Tue, 1 Jul 2014 10:45:43 -0700 (PDT)
In-Reply-To: <CABDsqqZ3vsH-ebfjZWJHKhfFQoFiJZYko-eLEP_RT3Z278g0xQ@mail.gmail.com>
References: <CABDsqqZ3vsH-ebfjZWJHKhfFQoFiJZYko-eLEP_RT3Z278g0xQ@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Tue, 1 Jul 2014 10:45:43 -0700
Message-ID: <CANGvG8qFVu2Z1SNE0_UqwXrWv39fNt7CTVR-rCH8xTadBGm3NQ@mail.gmail.com>
Subject: Re: task always lost
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0168133e7d75a204fd25590a
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0168133e7d75a204fd25590a
Content-Type: text/plain; charset=UTF-8

Can you post the logs from any of the dying executors?


On Tue, Jul 1, 2014 at 1:25 AM, qingyang li <liqingyang1985@gmail.com>
wrote:

> i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started, when I
> using spark-shell to submit one job, the tasks always lost.  here is the
> log:
> ----------
> 14/07/01 16:24:27 INFO DAGScheduler: Host gained which was in lost list
> earlier: bigdata005
> 14/07/01 16:24:27 INFO TaskSetManager: Starting task 0.0:1 as TID 4042 on
> executor 20140616-143932-1694607552-5050-4080-2: bigdata005 (PROCESS_LOCAL)
> 14/07/01 16:24:27 INFO TaskSetManager: Serialized task 0.0:1 as 1570 bytes
> in 0 ms
> 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
> 20140616-104524-1694607552-5050-26919-1 from TaskSet 0.0
> 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4041 (task 0.0:0)
> 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
> 20140616-104524-1694607552-5050-26919-1 (epoch 3427)
> 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
> 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
> 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
> 20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
> 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
> 20140616-143932-1694607552-5050-4080-2 from TaskSet 0.0
> 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4042 (task 0.0:1)
> 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
> 20140616-143932-1694607552-5050-4080-2 (epoch 3428)
> 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
> 20140616-143932-1694607552-5050-4080-2 from BlockManagerMaster.
> 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
> 20140616-143932-1694607552-5050-4080-2 successfully in removeExecutor
> 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
> earlier: bigdata005
> 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
> earlier: bigdata001
> 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:1 as TID 4043 on
> executor 20140616-143932-1694607552-5050-4080-2: bigdata005 (PROCESS_LOCAL)
> 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:1 as 1570 bytes
> in 0 ms
> 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:0 as TID 4044 on
> executor 20140616-104524-1694607552-5050-26919-1: bigdata001
> (PROCESS_LOCAL)
> 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:0 as 1570 bytes
> in 0 ms
>
>
> it seems other guy has also encountered such problem,
>
> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201305.mbox/%3C201305161047069952830@nfs.iscas.ac.cn%3E
>

--089e0168133e7d75a204fd25590a--

From dev-return-8167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  1 18:32:07 2014
Return-Path: <dev-return-8167-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BAB711B04
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  1 Jul 2014 18:32:07 +0000 (UTC)
Received: (qmail 24765 invoked by uid 500); 1 Jul 2014 18:32:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24689 invoked by uid 500); 1 Jul 2014 18:32:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23600 invoked by uid 99); 1 Jul 2014 18:32:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 18:32:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.128.181 as permitted sender)
Received: from [209.85.128.181] (HELO mail-ve0-f181.google.com) (209.85.128.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 01 Jul 2014 18:32:03 +0000
Received: by mail-ve0-f181.google.com with SMTP id db11so10103339veb.12
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 11:31:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=mLAdnxUvoF1KguhQTpOJsIl6LDU3vRhVZuRViDI4GLo=;
        b=e6B3Ano/kBa43Mf4lqxD22smIPkqUFwuYSTqNOXqg6TzGmGTIQAuAxlF3g53uGXNjo
         p6j2e2ZUqksYnhMPkKEr0oLoTAQ8llWVZP0L5BVs/foGb0MCqURuoWVDD8/gip8NwWYp
         klXALOslH1qkhRIC/c9pALOM6TOzK1YR/DTXm3Fyxvw/7R2GSnP6mr4QodGEu+i1HS70
         Y4FlWv3OuQ0WcZDUyq5uoOriDxNe7kiW4DBqtGMwWooc0MJ6HMjEniUV/IIUODlbIPm7
         OvaUli7srbFXq6CT1ew2+DuntpYNHjb7W41//7xsMZ/axiyVFxUGq9S96rrKPpy8DKPo
         ijVA==
X-Gm-Message-State: ALoCoQk925WQlKFL930B1fLfs9BhlGW8LLH0okDK5a2lRBEvNzGC7oshsYKcbfjPj//A0PTz55GJ
MIME-Version: 1.0
X-Received: by 10.52.34.46 with SMTP id w14mr1606566vdi.64.1404239498780; Tue,
 01 Jul 2014 11:31:38 -0700 (PDT)
Received: by 10.58.137.197 with HTTP; Tue, 1 Jul 2014 11:31:38 -0700 (PDT)
Date: Tue, 1 Jul 2014 14:31:38 -0400
Message-ID: <CALWDz_s9xXLMqDK6X=4boqG6GHteKRpjN5ZW1Od_39jLbyuOTg@mail.gmail.com>
Subject: PySpark Driver from Jython
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: "user@spark.incubator.apache.org" <user@spark.incubator.apache.org>, 
	"dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf307ca3b88138d804fd25fc4a
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307ca3b88138d804fd25fc4a
Content-Type: text/plain; charset=UTF-8

Has anyone tried running pyspark driver code in Jython, preferably by
calling python code within Java code?

I know CPython is the only interpreter tested because of the need to
support C extensions.

But in my case, C extensions would be called on the worker, not in the
driver.

And being able to execute the python driver from within my JVM is an
advantage in my current use case.

-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--20cf307ca3b88138d804fd25fc4a--

From dev-return-8168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 03:47:01 2014
Return-Path: <dev-return-8168-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9218311DB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 03:47:01 +0000 (UTC)
Received: (qmail 19860 invoked by uid 500); 2 Jul 2014 03:47:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19801 invoked by uid 500); 2 Jul 2014 03:47:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19787 invoked by uid 99); 2 Jul 2014 03:47:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 03:47:00 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 03:46:57 +0000
Received: by mail-wi0-f173.google.com with SMTP id cc10so8852384wib.12
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 20:46:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=KXSGCQ5tNOONmM6RcgoblDviOB35VuFWO3tqnl2V3ho=;
        b=E3r0AfblnndPq7nFuMHW42tRtYVRH1t9Lkf5k6xTiePopeeQubOUy2ki8FbbyrrNVJ
         Jmxy0TpIkaZkn3IqUz26ZgQILV/vpUiJc7pU0SF/EW5pcIMYHHrZxop+py0r6Kqg+EaI
         bB7mw7fmYaDIVgv7A6OXOrsKZlZp78pTMlTEZDiFPsVIv5cZS2DIqIAS1jSCRR1TRD+y
         Jy5ndAbJvPbOQU4zvsMZWCk9aICDVw+Zv2fYOS2M+rwPcgJuRecjKpDCuBFWLTlChJKx
         TUFc8YXbAJpwNRsoaTppequp7HuH1fyFatcOG9M9bnpi1lp9ophYIHw2Qm/j1DJnma9H
         Gweg==
MIME-Version: 1.0
X-Received: by 10.194.85.78 with SMTP id f14mr57101473wjz.36.1404272793533;
 Tue, 01 Jul 2014 20:46:33 -0700 (PDT)
Received: by 10.194.86.166 with HTTP; Tue, 1 Jul 2014 20:46:33 -0700 (PDT)
In-Reply-To: <CANGvG8qFVu2Z1SNE0_UqwXrWv39fNt7CTVR-rCH8xTadBGm3NQ@mail.gmail.com>
References: <CABDsqqZ3vsH-ebfjZWJHKhfFQoFiJZYko-eLEP_RT3Z278g0xQ@mail.gmail.com>
	<CANGvG8qFVu2Z1SNE0_UqwXrWv39fNt7CTVR-rCH8xTadBGm3NQ@mail.gmail.com>
Date: Wed, 2 Jul 2014 11:46:33 +0800
Message-ID: <CABDsqqZfE6SJBjuYVbJBdYgPQDC-9Yd=FZLc5oYJFcctL0G1Aw@mail.gmail.com>
Subject: Re: task always lost
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfcfd2606c2b704fd2dbd33
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfd2606c2b704fd2dbd33
Content-Type: text/plain; charset=UTF-8

Here is the log:

E0702 10:32:07.599364 14915 slave.cpp:2686] Failed to unmonitor container
for executor 20140616-104524-1694607552-5050-26919-1 of framework
20140702-102939-1694607552-5050-14846-0000: Not monitored


2014-07-02 1:45 GMT+08:00 Aaron Davidson <ilikerps@gmail.com>:

> Can you post the logs from any of the dying executors?
>
>
> On Tue, Jul 1, 2014 at 1:25 AM, qingyang li <liqingyang1985@gmail.com>
> wrote:
>
> > i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started,
> when I
> > using spark-shell to submit one job, the tasks always lost.  here is the
> > log:
> > ----------
> > 14/07/01 16:24:27 INFO DAGScheduler: Host gained which was in lost list
> > earlier: bigdata005
> > 14/07/01 16:24:27 INFO TaskSetManager: Starting task 0.0:1 as TID 4042 on
> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
> (PROCESS_LOCAL)
> > 14/07/01 16:24:27 INFO TaskSetManager: Serialized task 0.0:1 as 1570
> bytes
> > in 0 ms
> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
> > 20140616-104524-1694607552-5050-26919-1 from TaskSet 0.0
> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4041 (task 0.0:0)
> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
> > 20140616-104524-1694607552-5050-26919-1 (epoch 3427)
> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
> > 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
> > 20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
> > 20140616-143932-1694607552-5050-4080-2 from TaskSet 0.0
> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4042 (task 0.0:1)
> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
> > 20140616-143932-1694607552-5050-4080-2 (epoch 3428)
> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove executor
> > 20140616-143932-1694607552-5050-4080-2 from BlockManagerMaster.
> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
> > 20140616-143932-1694607552-5050-4080-2 successfully in removeExecutor
> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
> > earlier: bigdata005
> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
> > earlier: bigdata001
> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:1 as TID 4043 on
> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
> (PROCESS_LOCAL)
> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:1 as 1570
> bytes
> > in 0 ms
> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:0 as TID 4044 on
> > executor 20140616-104524-1694607552-5050-26919-1: bigdata001
> > (PROCESS_LOCAL)
> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:0 as 1570
> bytes
> > in 0 ms
> >
> >
> > it seems other guy has also encountered such problem,
> >
> >
> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201305.mbox/%3C201305161047069952830@nfs.iscas.ac.cn%3E
> >
>

--047d7bfcfd2606c2b704fd2dbd33--

From dev-return-8169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 04:01:46 2014
Return-Path: <dev-return-8169-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33A9D11DEE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 04:01:46 +0000 (UTC)
Received: (qmail 47620 invoked by uid 500); 2 Jul 2014 04:01:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47529 invoked by uid 500); 2 Jul 2014 04:01:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46933 invoked by uid 99); 2 Jul 2014 04:01:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 04:01:43 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 04:01:39 +0000
Received: by mail-wg0-f45.google.com with SMTP id l18so10397353wgh.4
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 21:01:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0MisPfN4nl4EVOc+mG7gE2GGyfTPzeuBa6z3wlerMN4=;
        b=QWyo1g+Lghx+Nja6KNbl5jfTLvUq0R+fTHlRo4/jHT//JcS01xxnazzvr9E2FaCtRa
         wSyMESIWuDz8OoYIUAQceMjkGLg3isPuOSyE4Q1kZZoCRm6qBnqw5W9U3R5uRIPp3lbq
         JoU1DEF9hTXsVg0hPmOCldlUy3dHntbngksAxbmpdLHacZ2ShXuGaU7dEzPB5fAZAaLj
         3bs0ftnA+WKjFt+G+piXU6OZr9ML01UKNS4r+Ld6GbOxJwwljl8JE7Hhz+NxKmotqtT8
         3dNFsk6JmuF9MeLDiQj7+KUjt8B0jR+RWMsgYw0sOWwZy6hHGnernCyAS8ywoYfJqNry
         qPAw==
MIME-Version: 1.0
X-Received: by 10.180.84.226 with SMTP id c2mr1630866wiz.50.1404273675809;
 Tue, 01 Jul 2014 21:01:15 -0700 (PDT)
Received: by 10.194.86.166 with HTTP; Tue, 1 Jul 2014 21:01:15 -0700 (PDT)
In-Reply-To: <CABDsqqZfE6SJBjuYVbJBdYgPQDC-9Yd=FZLc5oYJFcctL0G1Aw@mail.gmail.com>
References: <CABDsqqZ3vsH-ebfjZWJHKhfFQoFiJZYko-eLEP_RT3Z278g0xQ@mail.gmail.com>
	<CANGvG8qFVu2Z1SNE0_UqwXrWv39fNt7CTVR-rCH8xTadBGm3NQ@mail.gmail.com>
	<CABDsqqZfE6SJBjuYVbJBdYgPQDC-9Yd=FZLc5oYJFcctL0G1Aw@mail.gmail.com>
Date: Wed, 2 Jul 2014 12:01:15 +0800
Message-ID: <CABDsqqau77QOd1zN6ScvPencRHtQEnk15rno6bnd-_6yV=3XOw@mail.gmail.com>
Subject: Re: task always lost
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d044280de9d397904fd2df161
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044280de9d397904fd2df161
Content-Type: text/plain; charset=UTF-8

also this one in warning log:

E0702 11:35:08.869998 17840 slave.cpp:2310] Container
'af557235-2d5f-4062-aaf3-a747cb3cd0d1' for executor
'20140616-104524-1694607552-5050-26919-1' of framework
'20140702-113428-1694607552-5050-17766-0000' failed to start: Failed to
fetch URIs for container 'af557235-2d5f-4062-aaf3-a747cb3cd0d1': exit
status 32512


2014-07-02 11:46 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:

> Here is the log:
>
> E0702 10:32:07.599364 14915 slave.cpp:2686] Failed to unmonitor container
> for executor 20140616-104524-1694607552-5050-26919-1 of framework
> 20140702-102939-1694607552-5050-14846-0000: Not monitored
>
>
> 2014-07-02 1:45 GMT+08:00 Aaron Davidson <ilikerps@gmail.com>:
>
> Can you post the logs from any of the dying executors?
>>
>>
>> On Tue, Jul 1, 2014 at 1:25 AM, qingyang li <liqingyang1985@gmail.com>
>> wrote:
>>
>> > i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started,
>> when I
>> > using spark-shell to submit one job, the tasks always lost.  here is the
>> > log:
>> > ----------
>> > 14/07/01 16:24:27 INFO DAGScheduler: Host gained which was in lost list
>> > earlier: bigdata005
>> > 14/07/01 16:24:27 INFO TaskSetManager: Starting task 0.0:1 as TID 4042
>> on
>> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
>> (PROCESS_LOCAL)
>> > 14/07/01 16:24:27 INFO TaskSetManager: Serialized task 0.0:1 as 1570
>> bytes
>> > in 0 ms
>> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
>> > 20140616-104524-1694607552-5050-26919-1 from TaskSet 0.0
>> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4041 (task 0.0:0)
>> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
>> > 20140616-104524-1694607552-5050-26919-1 (epoch 3427)
>> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove
>> executor
>> > 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
>> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
>> > 20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
>> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
>> > 20140616-143932-1694607552-5050-4080-2 from TaskSet 0.0
>> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4042 (task 0.0:1)
>> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
>> > 20140616-143932-1694607552-5050-4080-2 (epoch 3428)
>> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove
>> executor
>> > 20140616-143932-1694607552-5050-4080-2 from BlockManagerMaster.
>> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
>> > 20140616-143932-1694607552-5050-4080-2 successfully in removeExecutor
>> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
>> > earlier: bigdata005
>> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
>> > earlier: bigdata001
>> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:1 as TID 4043
>> on
>> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
>> (PROCESS_LOCAL)
>> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:1 as 1570
>> bytes
>> > in 0 ms
>> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:0 as TID 4044
>> on
>> > executor 20140616-104524-1694607552-5050-26919-1: bigdata001
>> > (PROCESS_LOCAL)
>> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:0 as 1570
>> bytes
>> > in 0 ms
>> >
>> >
>> > it seems other guy has also encountered such problem,
>> >
>> >
>> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201305.mbox/%3C201305161047069952830@nfs.iscas.ac.cn%3E
>> >
>>
>
>

--f46d044280de9d397904fd2df161--

From dev-return-8170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 05:22:42 2014
Return-Path: <dev-return-8170-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4892811F86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 05:22:42 +0000 (UTC)
Received: (qmail 81376 invoked by uid 500); 2 Jul 2014 05:22:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81313 invoked by uid 500); 2 Jul 2014 05:22:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81287 invoked by uid 99); 2 Jul 2014 05:22:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 05:22:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 05:22:37 +0000
Received: by mail-ob0-f182.google.com with SMTP id nu7so11857216obb.27
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 22:22:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=XADe0uRlkoZd8Pu20P9tgxEs180CK3HYFEsBol7Csi0=;
        b=sJjjwonQKHxO+67XQJIqNffsrKtqL60GzbdJ5uMlz9RJFbmRVH0UXmq+AY6DPzrVNR
         8uTYzBhMfIbPeXyEq7xdlpZNSsD9RDnZyzj4izdNzsy6tiSeBHRy1OB24IoM7PDtAn1s
         RCo1nHcT7SDFimHDnlFP+TrPARWLVxsmt/fQYASJTa6sn0kBoiDF3oZNM40PUAmJKyJ+
         yGtXQSwoCeUsMBJGlZVFjaGYg5qlP0Atj4ZKEhgOHonTjBZT2t5lkehTmuQH4zOUnMHk
         cuZkWShaFxTSHOQcFwWmrxKLADCn0rbM0XMxmtoCino+oOB+0tss+MkdhdrdjzQUrIZX
         pgbw==
MIME-Version: 1.0
X-Received: by 10.60.155.167 with SMTP id vx7mr16288366oeb.50.1404278532940;
 Tue, 01 Jul 2014 22:22:12 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 1 Jul 2014 22:22:12 -0700 (PDT)
In-Reply-To: <CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
	<CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
	<CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
	<CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
	<CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
Date: Tue, 1 Jul 2014 22:22:12 -0700
Message-ID: <CABPQxsuWs0QV27QEBiLDdf7REf1Fwesz6TGdqMJOZFNqvcK=gQ@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

> b) Instead of pulling this information, push it to executors as part
> of task submission. (What Patrick mentioned ?)
> (1) a.1 from above is still an issue for this.

I don't understand problem a.1 is. In this case, we don't need to do
caching, right?

> (2) Serialized task size is also a concern : we have already seen
> users hitting akka limits for task size - this will be an additional
> vector which might exacerbate it.

This would add only a small, constant amount of data to the task. It's
strictly better than before. Before if the map output status array was
size M x R, we send a single akka message to every node of size M x
R... this basically scales quadratically with the size of the RDD. The
new approach is constant... it's much better. And the total amount of
data send over the wire is likely much less.

- Patrick

From dev-return-8171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 05:28:36 2014
Return-Path: <dev-return-8171-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD58D11F9E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 05:28:36 +0000 (UTC)
Received: (qmail 94897 invoked by uid 500); 2 Jul 2014 05:28:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94841 invoked by uid 500); 2 Jul 2014 05:28:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94830 invoked by uid 99); 2 Jul 2014 05:28:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 05:28:35 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 05:28:32 +0000
Received: by mail-qa0-f46.google.com with SMTP id i13so8456707qae.33
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 22:28:06 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=xJlPP0VGBUw3HsfsuavDo7ugy1SMyphnyvIHaVpoJnM=;
        b=WJvvcnVEPE39nCmcLVRr0+H1RhtySNAL1Z7bKyPprkKhIf1kav21tqlpDMFManTXaL
         Pe85wZiwgGx7Ju0RuRHyocrQcRgMCWRcAFqKhq+bKvmiKEkUsPJZKFcdB/VgVXRCIKE3
         46geulOrmYXeKdYxuVsm/P9gLUs96rD9AsMofOA7f4BGx7VGBPrqUVyyUGo/HreL4mlG
         zuR45aRDph0Nt0S+JDhY5ZiOuzDodrcYZ0B3ekrN2GpPQYb0GW1C/PXME5hFSaTuxDN8
         KCTV/jKy0/wb9hZLEqIfn870OWfhQizZ75x1p3nN81Tdjb/antX2GziwBIwF6s+ssIoK
         +ueQ==
X-Gm-Message-State: ALoCoQn4TDMrvtbFlP+1tgDkavYIVIZXSJAxB8wlrnd4vtoW/ake9qBrm26uSX4X9NbOncf4u1aw
X-Received: by 10.140.95.215 with SMTP id i81mr30070163qge.6.1404278886733;
 Tue, 01 Jul 2014 22:28:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Tue, 1 Jul 2014 22:27:46 -0700 (PDT)
In-Reply-To: <CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
 <CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
 <CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
 <CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com> <CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 1 Jul 2014 22:27:46 -0700
Message-ID: <CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c15de635b10604fd2f28e6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c15de635b10604fd2f28e6
Content-Type: text/plain; charset=UTF-8

I was actually talking to tgraves today at the summit about this.

Based on my understanding, the sizes we track and send (which is
unfortunately O(M*R) regardless of how we change the implementation --
whether we send via task or send via MapOutputTracker) is only used to
compute maxBytesInFlight so we can throttle the fetching speed to not
result in oom. Perhaps for very large shuffles, we don't need to send the
bytes for each block, and we can send whether they are zero or not (which
can be tracked via a compressed bitmap that can be tiny).

The other thing we do need is the location of blocks. This is actually just
O(n) because we just need to know where the map was run.


On Tue, Jul 1, 2014 at 2:51 AM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> We had considered both approaches (if I understood the suggestions right) :
> a) Pulling only map output states for tasks which run on the reducer
> by modifying the Actor. (Probably along lines of what Aaron described
> ?)
> The performance implication of this was bad :
> 1) We cant cache serialized result anymore, (caching it makes no sense
> rather).
> 2) The number requests to master will go from num_executors to
> num_reducers - the latter can be orders of magnitude higher than
> former.
>
> b) Instead of pulling this information, push it to executors as part
> of task submission. (What Patrick mentioned ?)
> (1) a.1 from above is still an issue for this.
> (2) Serialized task size is also a concern : we have already seen
> users hitting akka limits for task size - this will be an additional
> vector which might exacerbate it.
> Our jobs are not hitting this yet though !
>
> I was hoping there might be something in akka itself to alleviate this
> - but if not, we can solve it within context of spark.
>
> Currently, we have worked around it by using broadcast variable when
> serialized size is above some threshold - so that our immediate
> concerns are unblocked :-)
> But a better solution should be greatly welcomed !
> Maybe we can unify it with large serialized task as well ...
>
>
> Btw, I am not sure what the higher cost of BlockManager referred to is
> Aaron - do you mean the cost of persisting the serialized map outputs
> to disk ?
>
>
>
>
> Regards,
> Mridul
>
>
> On Tue, Jul 1, 2014 at 1:36 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Yeah I created a JIRA a while back to piggy-back the map status info
> > on top of the task (I honestly think it will be a small change). There
> > isn't a good reason to broadcast the entire array and it can be an
> > issue during large shuffles.
> >
> > - Patrick
> >
> > On Mon, Jun 30, 2014 at 7:58 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >> I don't know of any way to avoid Akka doing a copy, but I would like to
> >> mention that it's on the priority list to piggy-back only the map
> statuses
> >> relevant to a particular map task on the task itself, thus reducing the
> >> total amount of data sent over the wire by a factor of N for N physical
> >> machines in your cluster. Ideally we would also avoid Akka entirely when
> >> sending the tasks, as these can get somewhat large and Akka doesn't work
> >> well with large messages.
> >>
> >> Do note that your solution of using broadcast to send the map tasks is
> very
> >> similar to how the executor returns the result of a task when it's too
> big
> >> for akka. We were thinking of refactoring this too, as using the block
> >> manager has much higher latency than a direct TCP send.
> >>
> >>
> >> On Mon, Jun 30, 2014 at 12:13 PM, Mridul Muralidharan <mridul@gmail.com
> >
> >> wrote:
> >>
> >>> Our current hack is to use Broadcast variables when serialized
> >>> statuses are above some (configurable) size : and have the workers
> >>> directly pull them from master.
> >>> This is a workaround : so would be great if there was a
> >>> better/principled solution.
> >>>
> >>> Please note that the responses are going to different workers
> >>> requesting for the output statuses for shuffle (after map) - so not
> >>> sure if back pressure buffers, etc would help.
> >>>
> >>>
> >>> Regards,
> >>> Mridul
> >>>
> >>>
> >>> On Mon, Jun 30, 2014 at 11:07 PM, Mridul Muralidharan <
> mridul@gmail.com>
> >>> wrote:
> >>> > Hi,
> >>> >
> >>> >   While sending map output tracker result, the same serialized byte
> >>> > array is sent multiple times - but the akka implementation copies it
> >>> > to a private byte array within ByteString for each send.
> >>> > Caching a ByteString instead of Array[Byte] did not help, since akka
> >>> > does not support special casing ByteString : serializes the
> >>> > ByteString, and copies the result out to an array before creating
> >>> > ByteString out of it (in Array[Byte] serializing is thankfully simply
> >>> > returning same array - so one copy only).
> >>> >
> >>> >
> >>> > Given the need to send immutable data large number of times, is there
> >>> > any way to do it in akka without copying internally in akka ?
> >>> >
> >>> >
> >>> > To see how expensive it is, for 200 nodes withi large number of
> >>> > mappers and reducers, the status becomes something like 30 mb for us
> -
> >>> > and pulling this about 200 to 300 times results in OOM due to the
> >>> > large number of copies sent out.
> >>> >
> >>> >
> >>> > Thanks,
> >>> > Mridul
> >>>
>

--001a11c15de635b10604fd2f28e6--

From dev-return-8172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 06:05:46 2014
Return-Path: <dev-return-8172-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6B0AD1106B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 06:05:46 +0000 (UTC)
Received: (qmail 78107 invoked by uid 500); 2 Jul 2014 06:05:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78050 invoked by uid 500); 2 Jul 2014 06:05:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78032 invoked by uid 99); 2 Jul 2014 06:05:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 06:05:44 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 06:05:41 +0000
Received: by mail-qc0-f169.google.com with SMTP id c9so9594619qcz.0
        for <dev@spark.apache.org>; Tue, 01 Jul 2014 23:05:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=jbaY5oJRxbcz9etHi2GRvhBkj+xbaJHsUJvmzG0ZB/4=;
        b=vl4CUpd54a9qKldbrDHDYLBgX+QcSP4mvciyVht5yy6tRTUJwqhBXBiicIx62CQxAY
         G0fPH4XTrLwDbWQjCO4E10nlpafo1Hf8cUDn01VJa1Pqp/S9w2ut2teivUYGL6Cq1vUg
         Yan/XzX8QwzDfdaP7O/kgf/l5SXn2DvETf3mIubJbVUUeOjG3JFCkHHrcMdlcT9tMvRa
         zGCFMsGAH21KifZvOTGAMFnm3XSJEg7F1HYw73UCbf4iowNU53GklcwtJjr8HzH7tzP6
         23j3Rgibxd6k9XxI3IqRgGGbqD//yjDBioQSTvqnRz9StA2rNIiQBJX5YZwXEtjFt30e
         Cvvw==
MIME-Version: 1.0
X-Received: by 10.229.53.65 with SMTP id l1mr77114078qcg.19.1404281116783;
 Tue, 01 Jul 2014 23:05:16 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Tue, 1 Jul 2014 23:05:16 -0700 (PDT)
In-Reply-To: <CAJgQjQ-wvbaE+QJ2Mngzpy=g5i7=4ym59JVrAMtijwv4uTbfPw@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
	<CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
	<CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
	<CAJgQjQ-wvbaE+QJ2Mngzpy=g5i7=4ym59JVrAMtijwv4uTbfPw@mail.gmail.com>
Date: Tue, 1 Jul 2014 23:05:16 -0700
Message-ID: <CA+B-+fxYLfEWx_+jzdfNO3DFa5dc4VXfEYpffpOT0xCV=yV4Ug@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134bc6a217ad204fd2fad87
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134bc6a217ad204fd2fad87
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

Could you please point to the IPM solver that you have positive results
with ? I was planning to compare with CVX, KNITRO from Professor Nocedal
and MOSEK probably...I don't have CPLEX license so I won't be able to do
that comparison...

My experiments so far tells me that ADMM based solver is faster than IPM
for simpler constraints but then perhaps I did not choose the correct
IPM....

Proximal algorithm paper also shows very similar results compared to CVX:

http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf

Thanks.
Deb

On Wed, Jun 11, 2014 at 3:21 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> You idea is close to what implicit feedback does. You can check the
> paper, which is short and concise. In the ALS setting, all subproblems
> are independent in each iteration. This is part of the reason why ALS
> is scalable. If you have some global constraints that make the
> subproblems no longer decoupled, that would certainly affects
> scalability. -Xiangrui
>
> On Wed, Jun 11, 2014 at 2:20 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > I got it...ALS formulation is solving the matrix completion problem....
> >
> > To convert the problem to matrix factorization or take user feedback
> > (missing entries means the user hate the site ?), we should put 0 to the
> > missing entries (or may be -1)...in that case we have to use computeYtY
> and
> > accumulate over users in each block to generate full gram matrix...and
> > after that while computing userXy(index) we have to be careful in putting
> > 0/-1 for rest of the features...
> >
> > Is implicit feedback trying to do something like this ?
> >
> > Basically I am trying to see if it is possible to cache the gram matrix
> and
> > it's cholesky factorization, and then call the QpSolver multiple times
> with
> > updated gradient term...I am expecting better runtimes than dposv when
> > ranks are high...
> >
> > But seems like that's not possible without a broadcast step which might
> > kill all the runtime gain...
> >
> >
> > On Wed, Jun 11, 2014 at 12:21 AM, Xiangrui Meng <mengxr@gmail.com>
> wrote:
> >
> >> For explicit feedback, ALS uses only observed ratings for computation.
> >> So XtXs are not the same. -Xiangrui
> >>
> >> On Tue, Jun 10, 2014 at 8:58 PM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >> > Sorry last one went out by mistake:
> >> >
> >> > Is not for users (0 to numUsers), fullXtX is same ? In the ALS
> >> formulation
> >> > this is W^TW or H^TH which should be same for all the users ? Why we
> are
> >> > reading userXtX(index) and adding it to fullXtX in the loop over all
> >> > numUsers ?
> >> >
> >> > // Solve the least-squares problem for each user and return the new
> >> feature
> >> > vectors
> >> >
> >> >     Array.range(0, numUsers).map { index =>
> >> >
> >> >       // Compute the full XtX matrix from the lower-triangular part we
> >> got
> >> > above
> >> >
> >> >       fillFullMatrix(userXtX(index), fullXtX)
> >> >
> >> >       // Add regularization
> >> >
> >> >       var i = 0
> >> >
> >> >       while (i < rank) {
> >> >
> >> >         fullXtX.data(i * rank + i) += lambda
> >> >
> >> >         i += 1
> >> >
> >> >       }
> >> >
> >> >       // Solve the resulting matrix, which is symmetric and
> >> > positive-definite
> >> >
> >> >       algo match {
> >> >
> >> >         case ALSAlgo.Implicit =>
> >> > Solve.solvePositive(fullXtX.addi(YtY.get.value),
> >> > userXy(index)).data
> >> >
> >> >         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
> >> > (index)).data
> >> >
> >> >       }
> >> >
> >> >     }
> >> >
> >> >
> >> > On Tue, Jun 10, 2014 at 8:56 PM, Debasish Das <
> debasish.das83@gmail.com>
> >> > wrote:
> >> >
> >> >> Hi,
> >> >>
> >> >> I am bit confused wiht the code here:
> >> >>
> >> >> // Solve the least-squares problem for each user and return the new
> >> >> feature vectors
> >> >>
> >> >>     Array.range(0, numUsers).map { index =>
> >> >>
> >> >>       // Compute the full XtX matrix from the lower-triangular part
> we
> >> >> got above
> >> >>
> >> >>       fillFullMatrix(userXtX(index), fullXtX)
> >> >>
> >> >>       // Add regularization
> >> >>
> >> >>       var i = 0
> >> >>
> >> >>       while (i < rank) {
> >> >>
> >> >>         fullXtX.data(i * rank + i) += lambda
> >> >>
> >> >>         i += 1
> >> >>
> >> >>       }
> >> >>
> >> >>       // Solve the resulting matrix, which is symmetric and
> >> >> positive-definite
> >> >>
> >> >>       algo match {
> >> >>
> >> >>         case ALSAlgo.Implicit =>
> >> Solve.solvePositive(fullXtX.addi(YtY.get.value),
> >> >> userXy(index)).data
> >> >>
> >> >>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
> >> >> (index)).data
> >> >>
> >> >>       }
> >> >>
> >> >>     }
> >> >>
> >> >>
> >> >> On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <
> debasish.das83@gmail.com
> >> >
> >> >> wrote:
> >> >>
> >> >>> Hi Xiangrui,
> >> >>>
> >> >>> It's not the linear constraint, It is quadratic inequality with
> slack,
> >> >>> first order taylor approximation of off diagonal cross terms and a
> >> cyclic
> >> >>> coordinate descent, which we think will yield orthogonality....It's
> >> still
> >> >>> under works...
> >> >>>
> >> >>> Also we want to put a L1 constraint as set of linear equations when
> >> >>> solving for ALS...
> >> >>>
> >> >>> I will create the JIRA...as I see it, this will evolve to a generic
> >> >>> constraint solver for machine learning problems that has a QP
> >> >>> structure....ALS is one example....another example is kernel SVMs...
> >> >>>
> >> >>> I did not know that lgpl solver can be added to the classpath....if
> it
> >> >>> can be then definitely we should add these in ALS.scala...
> >> >>>
> >> >>> Thanks.
> >> >>> Deb
> >> >>>
> >> >>>
> >> >>>
> >> >>> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >>>
> >> >>>> I don't quite understand why putting linear constraints can promote
> >> >>>> orthogonality. For the interfaces, if the subproblem is determined
> by
> >> >>>> Y^T Y and Y^T b for each iteration, then the least squares solver,
> the
> >> >>>> non-negative least squares solver, or your convex solver is simply
> a
> >> >>>> function
> >> >>>>
> >> >>>> (A, b) -> x.
> >> >>>>
> >> >>>> You can define it as an interface, and make the solver pluggable by
> >> >>>> adding a setter to ALS. If you want to use your lgpl solver, just
> >> >>>> include it in the classpath. Creating two separate files still
> seems
> >> >>>> unnecessary to me. Could you create a JIRA and we can move our
> >> >>>> discussion there? Thanks!
> >> >>>>
> >> >>>> Best,
> >> >>>> Xiangrui
> >> >>>>
> >> >>>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <
> >> debasish.das83@gmail.com>
> >> >>>> wrote:
> >> >>>> > Hi Xiangrui,
> >> >>>> >
> >> >>>> > For orthogonality properties in the factors we need a constraint
> >> solver
> >> >>>> > other than the usuals (l1, upper and lower bounds, l2 etc)
> >> >>>> >
> >> >>>> > The interface of constraint solver is standard and I can add it
> in
> >> >>>> mllib
> >> >>>> > optimization....
> >> >>>> >
> >> >>>> > But I am not sure how will I call the gpl licensed ipm solver
> from
> >> >>>> > mllib....assume the solver interface is as follows:
> >> >>>> >
> >> >>>> > Qpsolver (densematrix h, array [double] f, int linearEquality,
> int
> >> >>>> > linearInequality, bool lb, bool ub)
> >> >>>> >
> >> >>>> > And then I have functions to update equalities, inequalities,
> bounds
> >> >>>> etc
> >> >>>> > followed by the run which generates the solution....
> >> >>>> >
> >> >>>> > For l1 constraints I have to use epigraph formulation which
> needs a
> >> >>>> > variable transformation before the solve....
> >> >>>> >
> >> >>>> > I was thinking that for the problems that does not need
> constraints
> >> >>>> people
> >> >>>> > will use ALS.scala and ConstrainedALS.scala will have the
> >> constrained
> >> >>>> > formulations....
> >> >>>> >
> >> >>>> > I can point you to the code once it is ready and then you can
> guide
> >> me
> >> >>>> how
> >> >>>> > to refactor it to mllib als ?
> >> >>>> >
> >> >>>> > Thanks.
> >> >>>> > Deb
> >> >>>> > Hi Deb,
> >> >>>> >
> >> >>>> > Why do you want to make those methods public? If you only need to
> >> >>>> > replace the solver for subproblems. You can try to make the
> solver
> >> >>>> > pluggable. Now it supports least squares and non-negative least
> >> >>>> > squares. You can define an interface for the subproblem solvers
> and
> >> >>>> > maintain the IPM solver at your own code base, if the only
> >> information
> >> >>>> > you need is Y^T Y and Y^T b.
> >> >>>> >
> >> >>>> > Btw, just curious, what is the use case for quadratic
> constraints?
> >> >>>> >
> >> >>>> > Best,
> >> >>>> > Xiangrui
> >> >>>> >
> >> >>>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <
> >> debasish.das83@gmail.com
> >> >>>> >
> >> >>>> > wrote:
> >> >>>> >> Hi,
> >> >>>> >>
> >> >>>> >> We are adding a constrained ALS solver in Spark to solve matrix
> >> >>>> >> factorization use-cases which needs additional constraints
> (bounds,
> >> >>>> >> equality, inequality, quadratic constraints)
> >> >>>> >>
> >> >>>> >> We are using a native version of a primal dual SOCP solver due
> to
> >> its
> >> >>>> > small
> >> >>>> >> memory footprint and sparse ccs matrix computation it uses...The
> >> >>>> solver
> >> >>>> >> depends on AMD and LDL packages from Timothy Davis for sparse
> ccs
> >> >>>> matrix
> >> >>>> >> algebra (released under lgpl)...
> >> >>>> >>
> >> >>>> >> Due to GPL dependencies, it won't be possible to release the
> code
> >> as
> >> >>>> > Apache
> >> >>>> >> license for now...If we get good results on our use-cases, we
> will
> >> >>>> plan to
> >> >>>> >> write a version in breeze/modify joptimizer for sparse ccs
> >> >>>> operations...
> >> >>>> >>
> >> >>>> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing
> >> the
> >> >>>> >> performance with default ALS and non-negative ALS as baseline.
> Plan
> >> >>>> is to
> >> >>>> >> release the code as GPL license for community review...I have
> kept
> >> the
> >> >>>> >> package structure as org.apache.spark.mllib.recommendation
> >> >>>> >>
> >> >>>> >> There are some private functions defined in ALS, which I would
> >> like to
> >> >>>> >> reuse....Is it possible to take the private out from the
> following
> >> >>>> >> functions:
> >> >>>> >>
> >> >>>> >> 1. makeLinkRDDs
> >> >>>> >> 2. makeInLinkBlock
> >> >>>> >> 3. makeOutLinkBlock
> >> >>>> >> 4. randomFactor
> >> >>>> >> 5. unblockFactors
> >> >>>> >>
> >> >>>> >> I don't want to copy any code.... I can ask for a PR to make
> these
> >> >>>> >> changes...
> >> >>>> >>
> >> >>>> >> Thanks.
> >> >>>> >> Deb
> >> >>>>
> >> >>>
> >> >>>
> >> >>
> >>
>

--001a1134bc6a217ad204fd2fad87--

From dev-return-8173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 07:09:57 2014
Return-Path: <dev-return-8173-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D2981127F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 07:09:57 +0000 (UTC)
Received: (qmail 94041 invoked by uid 500); 2 Jul 2014 07:09:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93979 invoked by uid 500); 2 Jul 2014 07:09:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93967 invoked by uid 99); 2 Jul 2014 07:09:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 07:09:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,WEIRD_PORT
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of taka.epsilon@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 07:09:51 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so11859176oag.33
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 00:09:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=j2W4Woy8RUCz7m0pTqUUbYRc4B/IDIXMKOGE0/0BaN0=;
        b=kImnZ2nvrOxx5FPj2ONixVaGFkDw+p7s9wO2WR5cx3Sk4R6xwzewB1DuvIv5Kdrb7P
         tf8lRvoP+LMIkyHnZeOe0Dio/20Kj1Fwz2FNY2pd+gvJC2IryZkwGTk+G3k4GBvjLVFO
         bRGOSr8OHM+jHrFbSGINWsZHSUhDkzI+UuyR4qF21zxmsZtreck/U99ex72cupBafGZb
         7/9w36CIi7FpWH2iVNJMQ84ooX08mXKWGvjPB3sGeHAySdy4QMIjfHyaeziLD6v7IP8q
         dvShbb2mwt5RFrYEOtfO6/MwpqjmBHnwoskqfHFG1LsqK0Za0byTvDzPSxv5DXM+EMfs
         iyAQ==
MIME-Version: 1.0
X-Received: by 10.60.62.174 with SMTP id z14mr22248695oer.61.1404284970183;
 Wed, 02 Jul 2014 00:09:30 -0700 (PDT)
Received: by 10.182.128.232 with HTTP; Wed, 2 Jul 2014 00:09:30 -0700 (PDT)
In-Reply-To: <CABPQxssPy8jYy+Nebzde1h0TZE2gr5JU8T3e9aeF=3-Cg=VRXg@mail.gmail.com>
References: <CALkvKbnEbSfMVfJp6NCM-q6X8UQq-JHDdMpiw04190p4wCe0EQ@mail.gmail.com>
	<CABPQxssPy8jYy+Nebzde1h0TZE2gr5JU8T3e9aeF=3-Cg=VRXg@mail.gmail.com>
Date: Wed, 2 Jul 2014 00:09:30 -0700
Message-ID: <CALkvKbmZWNb2KcuoVLYKShMOAOnMBZ-D_RH-2hH7u1adj5fE5A@mail.gmail.com>
Subject: Re: Errors from Sbt Test
From: Taka Shinagawa <taka.epsilon@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c21b5ccfaf8404fd309206
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c21b5ccfaf8404fd309206
Content-Type: text/plain; charset=UTF-8

>> Do those also happen if you run other hadoop versions (e.g. try 1.0.4)?
With Hadoop 1.0.4, the sbt test completed with fewer errors than with
Hadoop 1.2.1. I'll run the test for other Hadoop versions and report back
later.

--------------------------------
sbt test errors with Hadoop 1.0.4

[info] FlumeStreamSuite:

2014-07-01 23:18:55.057 java[90699:5903] Unable to load realm info from
SCDynamicStore

[info] - flume input stream *** FAILED ***

[info]   java.io.IOException: Error connecting to localhost/127.0.0.1:9999

[info]   at
org.apache.avro.ipc.NettyTransceiver.getChannel(NettyTransceiver.java:261)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:203)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:152)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:120)

[info]   at
org.apache.avro.ipc.NettyTransceiver.<init>(NettyTransceiver.java:107)

[info]   at
org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$1.apply$mcV$sp(FlumeStreamSuite.scala:54)

[info]   at
org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$1.apply(FlumeStreamSuite.scala:40)

[info]   at
org.apache.spark.streaming.flume.FlumeStreamSuite$$anonfun$1.apply(FlumeStreamSuite.scala:40)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   ...

[info]   Cause: java.net.ConnectException: Connection refused: localhost/
127.0.0.1:9999

[info]   at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)

[info]   at
sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.connect(NioClientBoss.java:150)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.processSelectedKeys(NioClientBoss.java:105)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.process(NioClientBoss.java:79)

[info]   at
org.jboss.netty.channel.socket.nio.AbstractNioSelector.run(AbstractNioSelector.java:312)

[info]   at
org.jboss.netty.channel.socket.nio.NioClientBoss.run(NioClientBoss.java:42)

[info]   at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

[info]   at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

[info]   at java.lang.Thread.run(Thread.java:744)

[info]   ...

[info] - cached post-shuffle

[ERROR] [07/01/2014 23:24:58.083] [test-akka.actor.default-dispatcher-3]
[akka://test/user/dagSupervisor/$a] error

org.apache.spark.SparkException: error

at
org.apache.spark.scheduler.BuggyDAGEventProcessActor$$anonfun$receive$1.applyOrElse(DAGSchedulerSuite.scala:36)

at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)

at akka.actor.ActorCell.invoke(ActorCell.scala:456)

at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)

at akka.dispatch.Mailbox.run(Mailbox.scala:219)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


[info] - DAGSchedulerActorSupervisor closes the SparkContext when
EventProcessActor crashes

[ERROR] [07/01/2014 23:24:58.115]
[DAGSchedulerSuite-akka.actor.default-dispatcher-6]
[akka://DAGSchedulerSuite/user/$$a] Job cancelled because SparkContext was
shut down

org.apache.spark.SparkException: Job cancelled because SparkContext was
shut down

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:639)

at
org.apache.spark.scheduler.DAGScheduler$$anonfun$cleanUpAfterSchedulerStop$1.apply(DAGScheduler.scala:638)

at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)

at
org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:638)

at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor.postStop(DAGScheduler.scala:1227)

at
akka.actor.dungeon.FaultHandling$class.akka$actor$dungeon$FaultHandling$$finishTerminate(FaultHandling.scala:201)

at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:163)

at akka.actor.ActorCell.terminate(ActorCell.scala:338)

at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431)

at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)

at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)

at
akka.testkit.CallingThreadDispatcher.process$1(CallingThreadDispatcher.scala:244)

at
akka.testkit.CallingThreadDispatcher.runQueue(CallingThreadDispatcher.scala:284)

at
akka.testkit.CallingThreadDispatcher.systemDispatch(CallingThreadDispatcher.scala:192)

at akka.actor.dungeon.Dispatch$class.stop(Dispatch.scala:106)

at akka.actor.ActorCell.stop(ActorCell.scala:338)

at akka.actor.LocalActorRef.stop(ActorRef.scala:340)

at akka.actor.dungeon.Children$class.stop(Children.scala:66)

at akka.actor.ActorCell.stop(ActorCell.scala:338)

at
akka.actor.dungeon.FaultHandling$$anonfun$terminate$1.apply(FaultHandling.scala:149)

at
akka.actor.dungeon.FaultHandling$$anonfun$terminate$1.apply(FaultHandling.scala:149)

at scala.collection.Iterator$class.foreach(Iterator.scala:727)

at
akka.util.Collections$PartialImmutableValuesIterable$$anon$1.foreach(Collections.scala:27)

at
akka.util.Collections$PartialImmutableValuesIterable.foreach(Collections.scala:52)

at akka.actor.dungeon.FaultHandling$class.terminate(FaultHandling.scala:149)

at akka.actor.ActorCell.terminate(ActorCell.scala:338)

at akka.actor.ActorCell.invokeAll$1(ActorCell.scala:431)

at akka.actor.ActorCell.systemInvoke(ActorCell.scala:447)

at akka.dispatch.Mailbox.processAllSystemMessages(Mailbox.scala:262)

at akka.dispatch.Mailbox.run(Mailbox.scala:218)

at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)

at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)

at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)

at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)

at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)










On Tue, Jul 1, 2014 at 1:04 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Do those also happen if you run other hadoop versions (e.g. try 1.0.4)?
>
> On Tue, Jul 1, 2014 at 1:00 AM, Taka Shinagawa <taka.epsilon@gmail.com>
> wrote:
> > Since Spark 1.0.0, I've been seeing multiple errors when running sbt
> test.
> >
> > I ran the following commands from Spark 1.0.1 RC1 on Mac OSX 10.9.2.
> >
> > $ sbt/sbt clean
> > $ SPARK_HADOOP_VERSION=1.2.1 sbt/sbt assembly
> > $ sbt/sbt test
> >
> >
> > I'm attaching the log file generated by the sbt test.
> >
> > Here's the summary part of the test.
> >
> > [info] Run completed in 30 minutes, 57 seconds.
> > [info] Total number of tests run: 605
> > [info] Suites: completed 83, aborted 0
> > [info] Tests: succeeded 600, failed 5, canceled 0, ignored 5, pending 0
> > [info] *** 5 TESTS FAILED ***
> > [error] Failed: Total 653, Failed 5, Errors 0, Passed 648, Ignored 5
> > [error] Failed tests:
> > [error] org.apache.spark.ShuffleNettySuite
> > [error] org.apache.spark.ShuffleSuite
> > [error] org.apache.spark.FileServerSuite
> > [error] org.apache.spark.DistributedSuite
> > [error] (core/test:test) sbt.TestsFailedException: Tests unsuccessful
> > [error] Total time: 2033 s, completed Jul 1, 2014 12:08:03 AM
> >
> > Is anyone else seeing errors like this?
> >
> >
> > Thanks,
> > Taka
>

--001a11c21b5ccfaf8404fd309206--

From dev-return-8174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 08:02:05 2014
Return-Path: <dev-return-8174-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A0C3211498
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 08:02:05 +0000 (UTC)
Received: (qmail 10200 invoked by uid 500); 2 Jul 2014 08:02:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10152 invoked by uid 500); 2 Jul 2014 08:02:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10140 invoked by uid 99); 2 Jul 2014 08:02:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 08:02:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of oddskool@gmail.com designates 209.85.219.47 as permitted sender)
Received: from [209.85.219.47] (HELO mail-oa0-f47.google.com) (209.85.219.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 08:02:02 +0000
Received: by mail-oa0-f47.google.com with SMTP id n16so11826319oag.6
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 01:01:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:date:message-id:subject:from:to:content-type;
        bh=qmgDx9+YEaJmSvv09rK3FHQXD6R8nPSiXq+0l9NLmgQ=;
        b=JgK4vzYzW7p1IIVBHW1bRzj4dkTR50roYXo5YqzuBJZo7g4xCEJBLIjqhMeMEykIa6
         jlSzKfLJZ3q9meq3i/Zx+liwmVJK8fQJs4sPzubO27PIDddd3iM8CGfWLjDzFdVJ09tb
         7I7rGlUV85urrPP9ku6pPhVA0VnLkgfkgbzZ4+eoP0k1jTq66zqY9gUvZdGqgHC8a+sg
         POq6zMt2g1n7slvZ6caxb/XTaZAzVuA4hwB1DZjvYcJ+t5mY8Roqhc4Ueas8T09wVTql
         jfJRaPn2cD98EOVDs0gSLiMmQzQQHiZ89+7uuLFQ7BEFPF+iYHSdIw5/cQYeqXTuMMDg
         ivrQ==
MIME-Version: 1.0
X-Received: by 10.60.59.4 with SMTP id v4mr22088569oeq.63.1404288098069; Wed,
 02 Jul 2014 01:01:38 -0700 (PDT)
Sender: oddskool@gmail.com
Received: by 10.60.178.210 with HTTP; Wed, 2 Jul 2014 01:01:38 -0700 (PDT)
Date: Wed, 2 Jul 2014 10:01:38 +0200
X-Google-Sender-Auth: ZW7FehELfGKI4fRt_J7Xq3zALt4
Message-ID: <CA+Onq3iCmv6NrrkMc842-gqW3mWOrwOVSG2neer9D7C2EqiGUw@mail.gmail.com>
Subject: process for contributing to mllib
From: Eustache DIEMERT <eustache@diemert.fr>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0129458c3f748804fd314d56
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0129458c3f748804fd314d56
Content-Type: text/plain; charset=UTF-8

Hi there,

I just created an issue [1] for MLlib on Jira. I also want to contribute a
fix, is it a good idea to submit a PR on github [2] ?

Should I also mention the issue on this list ?

Thanks

Eustache

[1] https://issues.apache.org/jira/browse/SPARK-2341
[2] https://github.com/apache/spark/pulls

--089e0129458c3f748804fd314d56--

From dev-return-8175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 08:11:00 2014
Return-Path: <dev-return-8175-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2861811508
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 08:11:00 +0000 (UTC)
Received: (qmail 33937 invoked by uid 500); 2 Jul 2014 08:10:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33873 invoked by uid 500); 2 Jul 2014 08:10:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33861 invoked by uid 99); 2 Jul 2014 08:10:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 08:10:59 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 08:10:55 +0000
Received: by mail-qc0-f180.google.com with SMTP id r5so9288637qcx.25
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 01:10:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=ykSTXWAQ+xJmgNC8U8TkGhMKqO9t3DeQFxMs8QPP5yQ=;
        b=XB5gbsOJP+N7AMYzYCzHKyAgrKuH8q8gqDo92qH7YdFK8yunO1wn23AS0dU+XqABEe
         +GJwlcOQIT8JJ+GLuRHpdGrbnHD3PSXrd6fGejCAreB75hfAp6Q2Wroi5tUknzPavQZ/
         de6cPq9MzbQhqmB+V7h2y6GvDbJJaycLP5lymVXb7TX39M5J/E9b9oNDnEtVVsO/UEPl
         CTnSTo24Cbgxs/uLniJmYRoT5nQgIiAsgh5pmVB55fAU69fp3zsRciKhw8JdPzyn6my/
         T6/rXy3eNNwnD+kqmgZS+DYdJ5/kzxqopv3+ImwuWXlVdH8TwfHMZnmYexq/6/qQvMk7
         CxJw==
X-Gm-Message-State: ALoCoQmIEwKhuGpAAcX+s27IaPqs1cdifKC60QMshAIcu9kSctTNwlEnuJop7gb9RwL103u28Fbn
X-Received: by 10.140.88.200 with SMTP id t66mr345670qgd.47.1404288633874;
 Wed, 02 Jul 2014 01:10:33 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 2 Jul 2014 01:10:12 -0700 (PDT)
In-Reply-To: <CA+Onq3iCmv6NrrkMc842-gqW3mWOrwOVSG2neer9D7C2EqiGUw@mail.gmail.com>
References: <CA+Onq3iCmv6NrrkMc842-gqW3mWOrwOVSG2neer9D7C2EqiGUw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 2 Jul 2014 01:10:12 -0700
Message-ID: <CAPh_B=ZBZh44730PS2HeYr9zvRy-PnLzzpUmxoeRAp5Oksq4gw@mail.gmail.com>
Subject: Re: process for contributing to mllib
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c11ea62f43a604fd316d9d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c11ea62f43a604fd316d9d
Content-Type: text/plain; charset=UTF-8

Yes it would be great to mention the JIRA ticket number on the pull
request. Thanks!



On Wed, Jul 2, 2014 at 1:01 AM, Eustache DIEMERT <eustache@diemert.fr>
wrote:

> Hi there,
>
> I just created an issue [1] for MLlib on Jira. I also want to contribute a
> fix, is it a good idea to submit a PR on github [2] ?
>
> Should I also mention the issue on this list ?
>
> Thanks
>
> Eustache
>
> [1] https://issues.apache.org/jira/browse/SPARK-2341
> [2] https://github.com/apache/spark/pulls
>

--001a11c11ea62f43a604fd316d9d--

From dev-return-8176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 08:53:20 2014
Return-Path: <dev-return-8176-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D52311806
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 08:53:20 +0000 (UTC)
Received: (qmail 49497 invoked by uid 500); 2 Jul 2014 08:53:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49447 invoked by uid 500); 2 Jul 2014 08:53:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49435 invoked by uid 99); 2 Jul 2014 08:53:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 08:53:19 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 08:53:14 +0000
Received: by mail-wi0-f178.google.com with SMTP id n15so53180wiw.5
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 01:52:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=co9CF3UjRAS8/gfg8x+P911pafNVBonevO7hr5Qyrik=;
        b=rhbmCNBbfWzoEq64+HW9jNlDIhR5O7BL7nS9j1urn4o5yPAnjir5PQOlIUZ/783fTV
         5xDyFJ4psX+BzAEDrC5pT7TvrXWE4yKoTg23RbIMnnCsW393NUyL8+NM+Qh9ajnApwpA
         iKHRUh7kzXxxvdP6ajJJtpst+KCsLnQYejFS01H6oE/Re2m+YIY7b6NInWxAmRsK08Fn
         FXcLJ8ID7qokbBQMMlo123a7JQfIKQSa27fdfpKTmW6DDXhShrae2geBeUAMFlKODw/i
         raHo5HZvD6qLNJrP9dFG7hicwhdZ2GIQ0cWHbKRMZ0T1mGqhNVr/ks5ez/GjAOydMpw1
         s0XA==
MIME-Version: 1.0
X-Received: by 10.194.48.103 with SMTP id k7mr57335229wjn.68.1404291169318;
 Wed, 02 Jul 2014 01:52:49 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Wed, 2 Jul 2014 01:52:49 -0700 (PDT)
In-Reply-To: <CA+B-+fxYLfEWx_+jzdfNO3DFa5dc4VXfEYpffpOT0xCV=yV4Ug@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
	<CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
	<CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
	<CAJgQjQ-wvbaE+QJ2Mngzpy=g5i7=4ym59JVrAMtijwv4uTbfPw@mail.gmail.com>
	<CA+B-+fxYLfEWx_+jzdfNO3DFa5dc4VXfEYpffpOT0xCV=yV4Ug@mail.gmail.com>
Date: Wed, 2 Jul 2014 01:52:49 -0700
Message-ID: <CAJgQjQ8KQRswOkedse2SXByU6r-XCgVZK2ftda-JKnjSu_r+QA@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Deb,

KNITRO and MOSEK are both commercial. If you are looking for
open-source ones, you can take a look at PDCO from SOL:

http://web.stanford.edu/group/SOL/software/pdco/

Each subproblem is really just a small QP. ADMM is designed for the
cases when data is distributively stored or the objective function is
complex but splittable. Neither applies to this case.

Best,
Xiangrui

On Tue, Jul 1, 2014 at 11:05 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi Xiangrui,
>
> Could you please point to the IPM solver that you have positive results
> with ? I was planning to compare with CVX, KNITRO from Professor Nocedal
> and MOSEK probably...I don't have CPLEX license so I won't be able to do
> that comparison...
>
> My experiments so far tells me that ADMM based solver is faster than IPM
> for simpler constraints but then perhaps I did not choose the correct
> IPM....
>
> Proximal algorithm paper also shows very similar results compared to CVX:
>
> http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf
>
> Thanks.
> Deb
>
> On Wed, Jun 11, 2014 at 3:21 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> You idea is close to what implicit feedback does. You can check the
>> paper, which is short and concise. In the ALS setting, all subproblems
>> are independent in each iteration. This is part of the reason why ALS
>> is scalable. If you have some global constraints that make the
>> subproblems no longer decoupled, that would certainly affects
>> scalability. -Xiangrui
>>
>> On Wed, Jun 11, 2014 at 2:20 AM, Debasish Das <debasish.das83@gmail.com>
>> wrote:
>> > I got it...ALS formulation is solving the matrix completion problem....
>> >
>> > To convert the problem to matrix factorization or take user feedback
>> > (missing entries means the user hate the site ?), we should put 0 to the
>> > missing entries (or may be -1)...in that case we have to use computeYtY
>> and
>> > accumulate over users in each block to generate full gram matrix...and
>> > after that while computing userXy(index) we have to be careful in putting
>> > 0/-1 for rest of the features...
>> >
>> > Is implicit feedback trying to do something like this ?
>> >
>> > Basically I am trying to see if it is possible to cache the gram matrix
>> and
>> > it's cholesky factorization, and then call the QpSolver multiple times
>> with
>> > updated gradient term...I am expecting better runtimes than dposv when
>> > ranks are high...
>> >
>> > But seems like that's not possible without a broadcast step which might
>> > kill all the runtime gain...
>> >
>> >
>> > On Wed, Jun 11, 2014 at 12:21 AM, Xiangrui Meng <mengxr@gmail.com>
>> wrote:
>> >
>> >> For explicit feedback, ALS uses only observed ratings for computation.
>> >> So XtXs are not the same. -Xiangrui
>> >>
>> >> On Tue, Jun 10, 2014 at 8:58 PM, Debasish Das <debasish.das83@gmail.com
>> >
>> >> wrote:
>> >> > Sorry last one went out by mistake:
>> >> >
>> >> > Is not for users (0 to numUsers), fullXtX is same ? In the ALS
>> >> formulation
>> >> > this is W^TW or H^TH which should be same for all the users ? Why we
>> are
>> >> > reading userXtX(index) and adding it to fullXtX in the loop over all
>> >> > numUsers ?
>> >> >
>> >> > // Solve the least-squares problem for each user and return the new
>> >> feature
>> >> > vectors
>> >> >
>> >> >     Array.range(0, numUsers).map { index =>
>> >> >
>> >> >       // Compute the full XtX matrix from the lower-triangular part we
>> >> got
>> >> > above
>> >> >
>> >> >       fillFullMatrix(userXtX(index), fullXtX)
>> >> >
>> >> >       // Add regularization
>> >> >
>> >> >       var i = 0
>> >> >
>> >> >       while (i < rank) {
>> >> >
>> >> >         fullXtX.data(i * rank + i) += lambda
>> >> >
>> >> >         i += 1
>> >> >
>> >> >       }
>> >> >
>> >> >       // Solve the resulting matrix, which is symmetric and
>> >> > positive-definite
>> >> >
>> >> >       algo match {
>> >> >
>> >> >         case ALSAlgo.Implicit =>
>> >> > Solve.solvePositive(fullXtX.addi(YtY.get.value),
>> >> > userXy(index)).data
>> >> >
>> >> >         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
>> >> > (index)).data
>> >> >
>> >> >       }
>> >> >
>> >> >     }
>> >> >
>> >> >
>> >> > On Tue, Jun 10, 2014 at 8:56 PM, Debasish Das <
>> debasish.das83@gmail.com>
>> >> > wrote:
>> >> >
>> >> >> Hi,
>> >> >>
>> >> >> I am bit confused wiht the code here:
>> >> >>
>> >> >> // Solve the least-squares problem for each user and return the new
>> >> >> feature vectors
>> >> >>
>> >> >>     Array.range(0, numUsers).map { index =>
>> >> >>
>> >> >>       // Compute the full XtX matrix from the lower-triangular part
>> we
>> >> >> got above
>> >> >>
>> >> >>       fillFullMatrix(userXtX(index), fullXtX)
>> >> >>
>> >> >>       // Add regularization
>> >> >>
>> >> >>       var i = 0
>> >> >>
>> >> >>       while (i < rank) {
>> >> >>
>> >> >>         fullXtX.data(i * rank + i) += lambda
>> >> >>
>> >> >>         i += 1
>> >> >>
>> >> >>       }
>> >> >>
>> >> >>       // Solve the resulting matrix, which is symmetric and
>> >> >> positive-definite
>> >> >>
>> >> >>       algo match {
>> >> >>
>> >> >>         case ALSAlgo.Implicit =>
>> >> Solve.solvePositive(fullXtX.addi(YtY.get.value),
>> >> >> userXy(index)).data
>> >> >>
>> >> >>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX, userXy
>> >> >> (index)).data
>> >> >>
>> >> >>       }
>> >> >>
>> >> >>     }
>> >> >>
>> >> >>
>> >> >> On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <
>> debasish.das83@gmail.com
>> >> >
>> >> >> wrote:
>> >> >>
>> >> >>> Hi Xiangrui,
>> >> >>>
>> >> >>> It's not the linear constraint, It is quadratic inequality with
>> slack,
>> >> >>> first order taylor approximation of off diagonal cross terms and a
>> >> cyclic
>> >> >>> coordinate descent, which we think will yield orthogonality....It's
>> >> still
>> >> >>> under works...
>> >> >>>
>> >> >>> Also we want to put a L1 constraint as set of linear equations when
>> >> >>> solving for ALS...
>> >> >>>
>> >> >>> I will create the JIRA...as I see it, this will evolve to a generic
>> >> >>> constraint solver for machine learning problems that has a QP
>> >> >>> structure....ALS is one example....another example is kernel SVMs...
>> >> >>>
>> >> >>> I did not know that lgpl solver can be added to the classpath....if
>> it
>> >> >>> can be then definitely we should add these in ALS.scala...
>> >> >>>
>> >> >>> Thanks.
>> >> >>> Deb
>> >> >>>
>> >> >>>
>> >> >>>
>> >> >>> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com>
>> >> wrote:
>> >> >>>
>> >> >>>> I don't quite understand why putting linear constraints can promote
>> >> >>>> orthogonality. For the interfaces, if the subproblem is determined
>> by
>> >> >>>> Y^T Y and Y^T b for each iteration, then the least squares solver,
>> the
>> >> >>>> non-negative least squares solver, or your convex solver is simply
>> a
>> >> >>>> function
>> >> >>>>
>> >> >>>> (A, b) -> x.
>> >> >>>>
>> >> >>>> You can define it as an interface, and make the solver pluggable by
>> >> >>>> adding a setter to ALS. If you want to use your lgpl solver, just
>> >> >>>> include it in the classpath. Creating two separate files still
>> seems
>> >> >>>> unnecessary to me. Could you create a JIRA and we can move our
>> >> >>>> discussion there? Thanks!
>> >> >>>>
>> >> >>>> Best,
>> >> >>>> Xiangrui
>> >> >>>>
>> >> >>>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <
>> >> debasish.das83@gmail.com>
>> >> >>>> wrote:
>> >> >>>> > Hi Xiangrui,
>> >> >>>> >
>> >> >>>> > For orthogonality properties in the factors we need a constraint
>> >> solver
>> >> >>>> > other than the usuals (l1, upper and lower bounds, l2 etc)
>> >> >>>> >
>> >> >>>> > The interface of constraint solver is standard and I can add it
>> in
>> >> >>>> mllib
>> >> >>>> > optimization....
>> >> >>>> >
>> >> >>>> > But I am not sure how will I call the gpl licensed ipm solver
>> from
>> >> >>>> > mllib....assume the solver interface is as follows:
>> >> >>>> >
>> >> >>>> > Qpsolver (densematrix h, array [double] f, int linearEquality,
>> int
>> >> >>>> > linearInequality, bool lb, bool ub)
>> >> >>>> >
>> >> >>>> > And then I have functions to update equalities, inequalities,
>> bounds
>> >> >>>> etc
>> >> >>>> > followed by the run which generates the solution....
>> >> >>>> >
>> >> >>>> > For l1 constraints I have to use epigraph formulation which
>> needs a
>> >> >>>> > variable transformation before the solve....
>> >> >>>> >
>> >> >>>> > I was thinking that for the problems that does not need
>> constraints
>> >> >>>> people
>> >> >>>> > will use ALS.scala and ConstrainedALS.scala will have the
>> >> constrained
>> >> >>>> > formulations....
>> >> >>>> >
>> >> >>>> > I can point you to the code once it is ready and then you can
>> guide
>> >> me
>> >> >>>> how
>> >> >>>> > to refactor it to mllib als ?
>> >> >>>> >
>> >> >>>> > Thanks.
>> >> >>>> > Deb
>> >> >>>> > Hi Deb,
>> >> >>>> >
>> >> >>>> > Why do you want to make those methods public? If you only need to
>> >> >>>> > replace the solver for subproblems. You can try to make the
>> solver
>> >> >>>> > pluggable. Now it supports least squares and non-negative least
>> >> >>>> > squares. You can define an interface for the subproblem solvers
>> and
>> >> >>>> > maintain the IPM solver at your own code base, if the only
>> >> information
>> >> >>>> > you need is Y^T Y and Y^T b.
>> >> >>>> >
>> >> >>>> > Btw, just curious, what is the use case for quadratic
>> constraints?
>> >> >>>> >
>> >> >>>> > Best,
>> >> >>>> > Xiangrui
>> >> >>>> >
>> >> >>>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <
>> >> debasish.das83@gmail.com
>> >> >>>> >
>> >> >>>> > wrote:
>> >> >>>> >> Hi,
>> >> >>>> >>
>> >> >>>> >> We are adding a constrained ALS solver in Spark to solve matrix
>> >> >>>> >> factorization use-cases which needs additional constraints
>> (bounds,
>> >> >>>> >> equality, inequality, quadratic constraints)
>> >> >>>> >>
>> >> >>>> >> We are using a native version of a primal dual SOCP solver due
>> to
>> >> its
>> >> >>>> > small
>> >> >>>> >> memory footprint and sparse ccs matrix computation it uses...The
>> >> >>>> solver
>> >> >>>> >> depends on AMD and LDL packages from Timothy Davis for sparse
>> ccs
>> >> >>>> matrix
>> >> >>>> >> algebra (released under lgpl)...
>> >> >>>> >>
>> >> >>>> >> Due to GPL dependencies, it won't be possible to release the
>> code
>> >> as
>> >> >>>> > Apache
>> >> >>>> >> license for now...If we get good results on our use-cases, we
>> will
>> >> >>>> plan to
>> >> >>>> >> write a version in breeze/modify joptimizer for sparse ccs
>> >> >>>> operations...
>> >> >>>> >>
>> >> >>>> >> I derived ConstrainedALS from Spark mllib ALS and I am comparing
>> >> the
>> >> >>>> >> performance with default ALS and non-negative ALS as baseline.
>> Plan
>> >> >>>> is to
>> >> >>>> >> release the code as GPL license for community review...I have
>> kept
>> >> the
>> >> >>>> >> package structure as org.apache.spark.mllib.recommendation
>> >> >>>> >>
>> >> >>>> >> There are some private functions defined in ALS, which I would
>> >> like to
>> >> >>>> >> reuse....Is it possible to take the private out from the
>> following
>> >> >>>> >> functions:
>> >> >>>> >>
>> >> >>>> >> 1. makeLinkRDDs
>> >> >>>> >> 2. makeInLinkBlock
>> >> >>>> >> 3. makeOutLinkBlock
>> >> >>>> >> 4. randomFactor
>> >> >>>> >> 5. unblockFactors
>> >> >>>> >>
>> >> >>>> >> I don't want to copy any code.... I can ask for a PR to make
>> these
>> >> >>>> >> changes...
>> >> >>>> >>
>> >> >>>> >> Thanks.
>> >> >>>> >> Deb
>> >> >>>>
>> >> >>>
>> >> >>>
>> >> >>
>> >>
>>

From dev-return-8177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 09:12:59 2014
Return-Path: <dev-return-8177-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C6B1711887
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 09:12:59 +0000 (UTC)
Received: (qmail 96314 invoked by uid 500); 2 Jul 2014 09:12:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96252 invoked by uid 500); 2 Jul 2014 09:12:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96236 invoked by uid 99); 2 Jul 2014 09:12:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 09:12:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.177 as permitted sender)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 09:12:52 +0000
Received: by mail-qc0-f177.google.com with SMTP id r5so9366299qcx.22
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 02:12:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=K9ttynNIBhuhTmyeHYub1TdwSPjvJN1PjDXJczPwoqY=;
        b=WgD86ZVMdY1fRnDjQFSS7VG9UdAiq0bM4VJJebPZgz2TUlNsEXUjAyI46wHKVzSDps
         tj03RyLewW4YFnq2k20rGnh+czsfv9AlBExbBaibs3L3goHR4xe/p4SPjkFU+7POAySc
         8N8bv66JFTgl6fqchNJMpwhVC/XU2q09972TCTBt+ZSFTWNlwB7yymuCQ2wQwRzvtW4r
         OPhUX+plk+zK5BasTGOO4Si1iX9eaTcRCs3wlM7iPNL7/a+u4sFE60LCKCIYhlfOJXS3
         IY2PRYQeejcU1+K2uDU2ON0uBRdnVdEwIQclJD+msKSUc/vqQvztDu0oEPivTaKGMkW0
         /UmA==
MIME-Version: 1.0
X-Received: by 10.140.84.168 with SMTP id l37mr77423717qgd.104.1404292352022;
 Wed, 02 Jul 2014 02:12:32 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Wed, 2 Jul 2014 02:12:31 -0700 (PDT)
In-Reply-To: <CABPQxsuWs0QV27QEBiLDdf7REf1Fwesz6TGdqMJOZFNqvcK=gQ@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
	<CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
	<CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
	<CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
	<CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
	<CABPQxsuWs0QV27QEBiLDdf7REf1Fwesz6TGdqMJOZFNqvcK=gQ@mail.gmail.com>
Date: Wed, 2 Jul 2014 14:42:31 +0530
Message-ID: <CAJiQeY+jG-+H1hAESNEE1MSwaVkABDWDaqSvYBDQTD4RO02NjA@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

  Please see inline.

Regards,
Mridul


On Wed, Jul 2, 2014 at 10:52 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>> b) Instead of pulling this information, push it to executors as part
>> of task submission. (What Patrick mentioned ?)
>> (1) a.1 from above is still an issue for this.
>
> I don't understand problem a.1 is. In this case, we don't need to do
> caching, right?


To rephrase in this context, attempting to cache wont help since it is
reducer specific and benefits are minimal (other than for reexecution
for failures and speculative tasks).


>
>> (2) Serialized task size is also a concern : we have already seen
>> users hitting akka limits for task size - this will be an additional
>> vector which might exacerbate it.
>
> This would add only a small, constant amount of data to the task. It's
> strictly better than before. Before if the map output status array was
> size M x R, we send a single akka message to every node of size M x
> R... this basically scales quadratically with the size of the RDD. The
> new approach is constant... it's much better. And the total amount of
> data send over the wire is likely much less.


It would be a function of the number of mappers - and an overhead for each task.


Regards,
Mridul

>
> - Patrick

From dev-return-8178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 09:44:56 2014
Return-Path: <dev-return-8178-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8A57E11927
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 09:44:56 +0000 (UTC)
Received: (qmail 49977 invoked by uid 500); 2 Jul 2014 09:44:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49919 invoked by uid 500); 2 Jul 2014 09:44:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49907 invoked by uid 99); 2 Jul 2014 09:44:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 09:44:55 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 09:44:50 +0000
Received: by mail-wi0-f171.google.com with SMTP id n15so9282711wiw.16
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 02:44:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=y01YrViz7JWw+MAKH0Gvr3jdCmpLjyQK+egSj2PsBwM=;
        b=viXJLuJdeUvf1D3GUqVK9zsohvQSmO39cE7Q6iSIIqHSqwaaILFXrHKZ0L5RfW56jd
         KAlViPeXN6QzQA41QJ8VL+QHf6VFa5xZK//ql6aqdaoSrUs1tCP1CykaNZ+XyQ8Kulx8
         XKKiGPn/1EeTWl6+L9W/oKaQJFCscrM8IxLrYCkRmyzcC8b5MFZKPffOF0vDXRZ0cL2c
         qmOhf9nodTNPFD0k5SsmCoDE02jqIdRtU6ZnIfUoXE7uh0A+hlFq8gfy0RlUGkv/Qr8M
         HwmNFG5RBLsTmIck8y0/aV1moYJjp1mOUlCNXyJ5HDzIepfpWlmqzw9kbloh3VZd86dR
         gQUA==
MIME-Version: 1.0
X-Received: by 10.180.84.226 with SMTP id c2mr3423735wiz.50.1404294265747;
 Wed, 02 Jul 2014 02:44:25 -0700 (PDT)
Received: by 10.194.86.166 with HTTP; Wed, 2 Jul 2014 02:44:25 -0700 (PDT)
In-Reply-To: <CABDsqqau77QOd1zN6ScvPencRHtQEnk15rno6bnd-_6yV=3XOw@mail.gmail.com>
References: <CABDsqqZ3vsH-ebfjZWJHKhfFQoFiJZYko-eLEP_RT3Z278g0xQ@mail.gmail.com>
	<CANGvG8qFVu2Z1SNE0_UqwXrWv39fNt7CTVR-rCH8xTadBGm3NQ@mail.gmail.com>
	<CABDsqqZfE6SJBjuYVbJBdYgPQDC-9Yd=FZLc5oYJFcctL0G1Aw@mail.gmail.com>
	<CABDsqqau77QOd1zN6ScvPencRHtQEnk15rno6bnd-_6yV=3XOw@mail.gmail.com>
Date: Wed, 2 Jul 2014 17:44:25 +0800
Message-ID: <CABDsqqY39hUXDvae4Fkybq1LBZaB-Wkuh0uNAXF1z+z0yPwh5Q@mail.gmail.com>
Subject: Re: task always lost
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d044280dedec38304fd32bcad
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044280dedec38304fd32bcad
Content-Type: text/plain; charset=UTF-8

executor always been removed.

someone encountered same issue
https://groups.google.com/forum/#!topic/spark-users/-mYn6BF-Y5Y

-------------
14/07/02 17:41:16 INFO storage.BlockManagerMasterActor: Trying to remove
executor 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
14/07/02 17:41:16 INFO storage.BlockManagerMaster: Removed
20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
14/07/02 17:41:16 DEBUG spark.MapOutputTrackerMaster: Increasing epoch to 10
14/07/02 17:41:16 INFO scheduler.DAGScheduler: Host gained which was in
lost list earlier: bigdata001
14/07/02 17:41:16 DEBUG scheduler.TaskSchedulerImpl: parentName: , name:
TaskSet_0, runningTasks: 0
14/07/02 17:41:16 DEBUG scheduler.TaskSchedulerImpl: parentName: , name:
TaskSet_0, runningTasks: 0
14/07/02 17:41:16 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID
12 on executor 20140616-143932-1694607552-5050-4080-3: bigdata004
(NODE_LOCAL)
14/07/02 17:41:16 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as
10785 bytes in 1 ms
14/07/02 17:41:16 INFO scheduler.TaskSetManager: Starting task 0.0:1 as TID
13 on executor 20140616-104524-1694607552-5050-26919-3: bigdata002
(NODE_LOCAL


2014-07-02 12:01 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:

> also this one in warning log:
>
> E0702 11:35:08.869998 17840 slave.cpp:2310] Container
> 'af557235-2d5f-4062-aaf3-a747cb3cd0d1' for executor
> '20140616-104524-1694607552-5050-26919-1' of framework
> '20140702-113428-1694607552-5050-17766-0000' failed to start: Failed to
> fetch URIs for container 'af557235-2d5f-4062-aaf3-a747cb3cd0d1': exit
> status 32512
>
>
> 2014-07-02 11:46 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:
>
> Here is the log:
>>
>> E0702 10:32:07.599364 14915 slave.cpp:2686] Failed to unmonitor container
>> for executor 20140616-104524-1694607552-5050-26919-1 of framework
>> 20140702-102939-1694607552-5050-14846-0000: Not monitored
>>
>>
>> 2014-07-02 1:45 GMT+08:00 Aaron Davidson <ilikerps@gmail.com>:
>>
>> Can you post the logs from any of the dying executors?
>>>
>>>
>>> On Tue, Jul 1, 2014 at 1:25 AM, qingyang li <liqingyang1985@gmail.com>
>>> wrote:
>>>
>>> > i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started,
>>> when I
>>> > using spark-shell to submit one job, the tasks always lost.  here is
>>> the
>>> > log:
>>> > ----------
>>> > 14/07/01 16:24:27 INFO DAGScheduler: Host gained which was in lost list
>>> > earlier: bigdata005
>>> > 14/07/01 16:24:27 INFO TaskSetManager: Starting task 0.0:1 as TID 4042
>>> on
>>> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
>>> (PROCESS_LOCAL)
>>> > 14/07/01 16:24:27 INFO TaskSetManager: Serialized task 0.0:1 as 1570
>>> bytes
>>> > in 0 ms
>>> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
>>> > 20140616-104524-1694607552-5050-26919-1 from TaskSet 0.0
>>> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4041 (task 0.0:0)
>>> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
>>> > 20140616-104524-1694607552-5050-26919-1 (epoch 3427)
>>> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove
>>> executor
>>> > 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
>>> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
>>> > 20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
>>> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
>>> > 20140616-143932-1694607552-5050-4080-2 from TaskSet 0.0
>>> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4042 (task 0.0:1)
>>> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
>>> > 20140616-143932-1694607552-5050-4080-2 (epoch 3428)
>>> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove
>>> executor
>>> > 20140616-143932-1694607552-5050-4080-2 from BlockManagerMaster.
>>> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
>>> > 20140616-143932-1694607552-5050-4080-2 successfully in removeExecutor
>>> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
>>> > earlier: bigdata005
>>> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost list
>>> > earlier: bigdata001
>>> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:1 as TID 4043
>>> on
>>> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
>>> (PROCESS_LOCAL)
>>> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:1 as 1570
>>> bytes
>>> > in 0 ms
>>> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:0 as TID 4044
>>> on
>>> > executor 20140616-104524-1694607552-5050-26919-1: bigdata001
>>> > (PROCESS_LOCAL)
>>> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:0 as 1570
>>> bytes
>>> > in 0 ms
>>> >
>>> >
>>> > it seems other guy has also encountered such problem,
>>> >
>>> >
>>> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201305.mbox/%3C201305161047069952830@nfs.iscas.ac.cn%3E
>>> >
>>>
>>
>>
>

--f46d044280dedec38304fd32bcad--

From dev-return-8179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 10:44:52 2014
Return-Path: <dev-return-8179-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 781AB11AB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 10:44:52 +0000 (UTC)
Received: (qmail 19880 invoked by uid 500); 2 Jul 2014 10:44:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19831 invoked by uid 500); 2 Jul 2014 10:44:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19819 invoked by uid 99); 2 Jul 2014 10:44:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 10:44:51 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 10:44:47 +0000
Received: by mail-qg0-f50.google.com with SMTP id j5so4410563qga.23
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 03:44:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=SnuM1lJxyW6oQmUOXlX4/WCjjsT0mKYyOpFKtIbEhmA=;
        b=GoifBUVryJPfy5Yi2gfRTM8hZv7HiALEC0oHzK+Td/5ejxANboq/lWq2bzN32NXtNz
         zPjWKEZ7qBr2I1uegc8notpzBPjfBLing8wsnm9e9bU4H39kCLOfFGTX0bYGYn2mI9iA
         ZQYsFXzapppgETXUCCf3r4Ge6cB5kxVQECV3/r1RLaQHZBlamRNyXST9Joo4mSmAUzVB
         VKW1v48wJg1059Mij5n7p6WTURs32BJUy4b6qeXz9aMX9fEO/ZCeIaCaq1Qyb49I1vNQ
         owyR6zRjH2csQl1/k/i0rSJy9TjwPPljtx4v7zhyZiUx85CtyymuE9mvkxuvScP55Tpl
         vliw==
MIME-Version: 1.0
X-Received: by 10.224.34.73 with SMTP id k9mr84306814qad.11.1404297866388;
 Wed, 02 Jul 2014 03:44:26 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Wed, 2 Jul 2014 03:44:26 -0700 (PDT)
In-Reply-To: <CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
	<CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
	<CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
	<CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
	<CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
	<CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com>
Date: Wed, 2 Jul 2014 16:14:26 +0530
Message-ID: <CAJiQeYKoo-6r_4q75FmcryKqy_iLcT929LKurP8AEAGgMFJrjg@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Reynold,

  Please see inline.

Regards,
Mridul

On Wed, Jul 2, 2014 at 10:57 AM, Reynold Xin <rxin@databricks.com> wrote:
> I was actually talking to tgraves today at the summit about this.
>
> Based on my understanding, the sizes we track and send (which is
> unfortunately O(M*R) regardless of how we change the implementation --
> whether we send via task or send via MapOutputTracker) is only used to
> compute maxBytesInFlight so we can throttle the fetching speed to not
> result in oom. Perhaps for very large shuffles, we don't need to send the
> bytes for each block, and we can send whether they are zero or not (which
> can be tracked via a compressed bitmap that can be tiny).

You are right, currently for large blocks, we just need to know where
the block exists.
I was not sure if there was any possible future extension on that -
for this reason, in order to preserve functionality, we moved to using
Short from Byte for MapOutputTracker.compressedSize (to ensure large
sizes can be represented with 0.7% error).

Within a MapStatus, we moved to holding compressed data to save on
space within master/workers (particularly for large number of
reducers).

If we do not anticipate any other reason for "size", we can move back
to using Byte instead of Short to compress size (which will reduce
required space by some factor less than 2) : since error in computed
size for blocks larger than maxBytesInFlight does not really matter :
we will split them into different FetchRequest's.


>
> The other thing we do need is the location of blocks. This is actually just
> O(n) because we just need to know where the map was run.

For well partitioned data, wont this not involve a lot of unwanted
requests to nodes which are not hosting data for a reducer (and lack
of ability to throttle).


Regards,
Mridul

>
>
> On Tue, Jul 1, 2014 at 2:51 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>
>> We had considered both approaches (if I understood the suggestions right) :
>> a) Pulling only map output states for tasks which run on the reducer
>> by modifying the Actor. (Probably along lines of what Aaron described
>> ?)
>> The performance implication of this was bad :
>> 1) We cant cache serialized result anymore, (caching it makes no sense
>> rather).
>> 2) The number requests to master will go from num_executors to
>> num_reducers - the latter can be orders of magnitude higher than
>> former.
>>
>> b) Instead of pulling this information, push it to executors as part
>> of task submission. (What Patrick mentioned ?)
>> (1) a.1 from above is still an issue for this.
>> (2) Serialized task size is also a concern : we have already seen
>> users hitting akka limits for task size - this will be an additional
>> vector which might exacerbate it.
>> Our jobs are not hitting this yet though !
>>
>> I was hoping there might be something in akka itself to alleviate this
>> - but if not, we can solve it within context of spark.
>>
>> Currently, we have worked around it by using broadcast variable when
>> serialized size is above some threshold - so that our immediate
>> concerns are unblocked :-)
>> But a better solution should be greatly welcomed !
>> Maybe we can unify it with large serialized task as well ...
>>
>>
>> Btw, I am not sure what the higher cost of BlockManager referred to is
>> Aaron - do you mean the cost of persisting the serialized map outputs
>> to disk ?
>>
>>
>>
>>
>> Regards,
>> Mridul
>>
>>
>> On Tue, Jul 1, 2014 at 1:36 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> > Yeah I created a JIRA a while back to piggy-back the map status info
>> > on top of the task (I honestly think it will be a small change). There
>> > isn't a good reason to broadcast the entire array and it can be an
>> > issue during large shuffles.
>> >
>> > - Patrick
>> >
>> > On Mon, Jun 30, 2014 at 7:58 PM, Aaron Davidson <ilikerps@gmail.com>
>> wrote:
>> >> I don't know of any way to avoid Akka doing a copy, but I would like to
>> >> mention that it's on the priority list to piggy-back only the map
>> statuses
>> >> relevant to a particular map task on the task itself, thus reducing the
>> >> total amount of data sent over the wire by a factor of N for N physical
>> >> machines in your cluster. Ideally we would also avoid Akka entirely when
>> >> sending the tasks, as these can get somewhat large and Akka doesn't work
>> >> well with large messages.
>> >>
>> >> Do note that your solution of using broadcast to send the map tasks is
>> very
>> >> similar to how the executor returns the result of a task when it's too
>> big
>> >> for akka. We were thinking of refactoring this too, as using the block
>> >> manager has much higher latency than a direct TCP send.
>> >>
>> >>
>> >> On Mon, Jun 30, 2014 at 12:13 PM, Mridul Muralidharan <mridul@gmail.com
>> >
>> >> wrote:
>> >>
>> >>> Our current hack is to use Broadcast variables when serialized
>> >>> statuses are above some (configurable) size : and have the workers
>> >>> directly pull them from master.
>> >>> This is a workaround : so would be great if there was a
>> >>> better/principled solution.
>> >>>
>> >>> Please note that the responses are going to different workers
>> >>> requesting for the output statuses for shuffle (after map) - so not
>> >>> sure if back pressure buffers, etc would help.
>> >>>
>> >>>
>> >>> Regards,
>> >>> Mridul
>> >>>
>> >>>
>> >>> On Mon, Jun 30, 2014 at 11:07 PM, Mridul Muralidharan <
>> mridul@gmail.com>
>> >>> wrote:
>> >>> > Hi,
>> >>> >
>> >>> >   While sending map output tracker result, the same serialized byte
>> >>> > array is sent multiple times - but the akka implementation copies it
>> >>> > to a private byte array within ByteString for each send.
>> >>> > Caching a ByteString instead of Array[Byte] did not help, since akka
>> >>> > does not support special casing ByteString : serializes the
>> >>> > ByteString, and copies the result out to an array before creating
>> >>> > ByteString out of it (in Array[Byte] serializing is thankfully simply
>> >>> > returning same array - so one copy only).
>> >>> >
>> >>> >
>> >>> > Given the need to send immutable data large number of times, is there
>> >>> > any way to do it in akka without copying internally in akka ?
>> >>> >
>> >>> >
>> >>> > To see how expensive it is, for 200 nodes withi large number of
>> >>> > mappers and reducers, the status becomes something like 30 mb for us
>> -
>> >>> > and pulling this about 200 to 300 times results in OOM due to the
>> >>> > large number of copies sent out.
>> >>> >
>> >>> >
>> >>> > Thanks,
>> >>> > Mridul
>> >>>
>>

From dev-return-8180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 18:02:57 2014
Return-Path: <dev-return-8180-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75DB41104A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 18:02:57 +0000 (UTC)
Received: (qmail 9000 invoked by uid 500); 2 Jul 2014 18:02:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8940 invoked by uid 500); 2 Jul 2014 18:02:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8928 invoked by uid 99); 2 Jul 2014 18:02:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:02:56 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of salexln@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:02:51 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <salexln@gmail.com>)
	id 1X2OrX-0004ky-9Z
	for dev@spark.incubator.apache.org; Wed, 02 Jul 2014 11:02:31 -0700
Date: Wed, 2 Jul 2014 11:02:31 -0700 (PDT)
From: salexln <salexln@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1404324151284-7155.post@n3.nabble.com>
In-Reply-To: <1404143698646-7125.post@n3.nabble.com>
References: <1404143698646-7125.post@n3.nabble.com>
Subject: Re: Contributing to MLlib
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

guys??? anyone???



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7155.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 18:11:10 2014
Return-Path: <dev-return-8181-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8FCC51109C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 18:11:10 +0000 (UTC)
Received: (qmail 23431 invoked by uid 500); 2 Jul 2014 18:11:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23366 invoked by uid 500); 2 Jul 2014 18:11:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23346 invoked by uid 99); 2 Jul 2014 18:11:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:11:09 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:11:05 +0000
Received: by mail-ve0-f178.google.com with SMTP id oy12so11657585veb.37
        for <dev@spark.incubator.apache.org>; Wed, 02 Jul 2014 11:10:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=QlnaHynbtrnGTjLFjqqXEzjOao8ufKTrEW5ERCfAs7s=;
        b=w3HqxjoOHyhaJd/px1Gd/siv0s1y16PLc/lrf1JV0V3wcZXrN2x8KkBJwvGeo69yyO
         sUTl4dMZRH+tTTZUNywcaPmn2CkKzBDNFIrmH5A+MQOkYPomXVsjGkqzaAGoaUsMDNCD
         suU3mV6EzylHyqJ8Rtjj2O+uHBrcpiBmJuN6+5oTOG7vnBWl2ohYpgnww7zIFOzQmyQg
         aylNNoabtAwUJQPU1unOsCf0Vfq15eg1rxQxnKR5S9qdzNTu+RS18PazByC+OStpYq1n
         yjh1AzlOU0ArL6IlH1h9LPN2JTnGhmrITe6p9tQy2fmuDtvv+Pudp3pVeMn+x/j8+OOL
         A3hQ==
X-Received: by 10.58.234.199 with SMTP id ug7mr1417418vec.40.1404324644350;
 Wed, 02 Jul 2014 11:10:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.79.162 with HTTP; Wed, 2 Jul 2014 11:10:24 -0700 (PDT)
In-Reply-To: <1404324151284-7155.post@n3.nabble.com>
References: <1404143698646-7125.post@n3.nabble.com> <1404324151284-7155.post@n3.nabble.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Wed, 2 Jul 2014 11:10:24 -0700
Message-ID: <CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com>
Subject: Re: Contributing to MLlib
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6da66893734204fd39cfab
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6da66893734204fd39cfab
Content-Type: text/plain; charset=UTF-8

Hi there,

Generally we try to avoid duplicating logic if possible, particularly for
algorithms that share a great deal of algorithmic similarity. See, for
example, the way we implement Logistic regression vs. Linear regression vs.
Linear SVM with different gradient functions all on top of SGD or L-BFGS.

Based on my (brief) look at the FCM algorithm, it appears that the main
difference is the ability to assign a weight vector associating the degree
of relationship of a given point to some centroid. My guess is that you can
figure out a way to inherit much of the K-Means logic in an algorithm for
FCM.

Regardless, if you'd like to add an algorithm, please create a JIRA ticket
for it and then send a pull request which references that JIRA where we can
discuss the specifics of that implementation and whether it is of broad
enough interest to warrant inclusion in the library.

- Evan


On Wed, Jul 2, 2014 at 11:02 AM, salexln <salexln@gmail.com> wrote:

> guys??? anyone???
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7155.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b6da66893734204fd39cfab--

From dev-return-8182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 18:11:12 2014
Return-Path: <dev-return-8182-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5CF291109D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 18:11:12 +0000 (UTC)
Received: (qmail 24345 invoked by uid 500); 2 Jul 2014 18:11:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24287 invoked by uid 500); 2 Jul 2014 18:11:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24272 invoked by uid 99); 2 Jul 2014 18:11:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:11:11 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of evan.sparks@gmail.com designates 209.85.128.178 as permitted sender)
Received: from [209.85.128.178] (HELO mail-ve0-f178.google.com) (209.85.128.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:11:09 +0000
Received: by mail-ve0-f178.google.com with SMTP id oy12so11582130veb.23
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 11:10:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=QlnaHynbtrnGTjLFjqqXEzjOao8ufKTrEW5ERCfAs7s=;
        b=w3HqxjoOHyhaJd/px1Gd/siv0s1y16PLc/lrf1JV0V3wcZXrN2x8KkBJwvGeo69yyO
         sUTl4dMZRH+tTTZUNywcaPmn2CkKzBDNFIrmH5A+MQOkYPomXVsjGkqzaAGoaUsMDNCD
         suU3mV6EzylHyqJ8Rtjj2O+uHBrcpiBmJuN6+5oTOG7vnBWl2ohYpgnww7zIFOzQmyQg
         aylNNoabtAwUJQPU1unOsCf0Vfq15eg1rxQxnKR5S9qdzNTu+RS18PazByC+OStpYq1n
         yjh1AzlOU0ArL6IlH1h9LPN2JTnGhmrITe6p9tQy2fmuDtvv+Pudp3pVeMn+x/j8+OOL
         A3hQ==
X-Received: by 10.58.234.199 with SMTP id ug7mr1417418vec.40.1404324644350;
 Wed, 02 Jul 2014 11:10:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.79.162 with HTTP; Wed, 2 Jul 2014 11:10:24 -0700 (PDT)
In-Reply-To: <1404324151284-7155.post@n3.nabble.com>
References: <1404143698646-7125.post@n3.nabble.com> <1404324151284-7155.post@n3.nabble.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Wed, 2 Jul 2014 11:10:24 -0700
Message-ID: <CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com>
Subject: Re: Contributing to MLlib
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6da66893734204fd39cfab
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6da66893734204fd39cfab
Content-Type: text/plain; charset=UTF-8

Hi there,

Generally we try to avoid duplicating logic if possible, particularly for
algorithms that share a great deal of algorithmic similarity. See, for
example, the way we implement Logistic regression vs. Linear regression vs.
Linear SVM with different gradient functions all on top of SGD or L-BFGS.

Based on my (brief) look at the FCM algorithm, it appears that the main
difference is the ability to assign a weight vector associating the degree
of relationship of a given point to some centroid. My guess is that you can
figure out a way to inherit much of the K-Means logic in an algorithm for
FCM.

Regardless, if you'd like to add an algorithm, please create a JIRA ticket
for it and then send a pull request which references that JIRA where we can
discuss the specifics of that implementation and whether it is of broad
enough interest to warrant inclusion in the library.

- Evan


On Wed, Jul 2, 2014 at 11:02 AM, salexln <salexln@gmail.com> wrote:

> guys??? anyone???
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7155.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--047d7b6da66893734204fd39cfab--

From dev-return-8183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 18:21:59 2014
Return-Path: <dev-return-8183-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B8A011155
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 18:21:59 +0000 (UTC)
Received: (qmail 56842 invoked by uid 500); 2 Jul 2014 18:21:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56793 invoked by uid 500); 2 Jul 2014 18:21:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56782 invoked by uid 99); 2 Jul 2014 18:21:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:21:57 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of salexln@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 18:21:53 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <salexln@gmail.com>)
	id 1X2P9w-0005tN-Dm
	for dev@spark.incubator.apache.org; Wed, 02 Jul 2014 11:21:32 -0700
Date: Wed, 2 Jul 2014 11:21:32 -0700 (PDT)
From: salexln <salexln@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1404325292417-7157.post@n3.nabble.com>
In-Reply-To: <CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com>
References: <1404143698646-7125.post@n3.nabble.com> <1404324151284-7155.post@n3.nabble.com> <CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com>
Subject: Re: Contributing to MLlib
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

thanks for the response !

that's is exactly the way i wanted to implement it :)

I will create JIRA ticket and a request.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7157.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 19:07:35 2014
Return-Path: <dev-return-8184-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0156E1133D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 19:07:35 +0000 (UTC)
Received: (qmail 65198 invoked by uid 500); 2 Jul 2014 19:07:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65137 invoked by uid 500); 2 Jul 2014 19:07:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65126 invoked by uid 99); 2 Jul 2014 19:07:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 19:07:33 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of salexln@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 19:07:29 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <salexln@gmail.com>)
	id 1X2Ps5-0001hN-5J
	for dev@spark.incubator.apache.org; Wed, 02 Jul 2014 12:07:09 -0700
Date: Wed, 2 Jul 2014 12:07:09 -0700 (PDT)
From: salexln <salexln@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1404328029153-7158.post@n3.nabble.com>
In-Reply-To: <1404325292417-7157.post@n3.nabble.com>
References: <1404143698646-7125.post@n3.nabble.com> <1404324151284-7155.post@n3.nabble.com> <CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com> <1404325292417-7157.post@n3.nabble.com>
Subject: Re: Contributing to MLlib
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I opened a JIRA (https://issues.apache.org/jira/browse/SPARK-2344)

and a pull request for this (https://github.com/salexln/spark/pull/1)



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7158.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  2 19:42:00 2014
Return-Path: <dev-return-8185-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F24A511474
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  2 Jul 2014 19:41:59 +0000 (UTC)
Received: (qmail 37704 invoked by uid 500); 2 Jul 2014 19:41:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37654 invoked by uid 500); 2 Jul 2014 19:41:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37629 invoked by uid 99); 2 Jul 2014 19:41:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 19:41:58 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 02 Jul 2014 19:41:57 +0000
Received: by mail-wi0-f179.google.com with SMTP id cc10so1080543wib.0
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 12:41:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=NYTKUGhhiOaIV7dRNZfh7dBnzl83NgMUWBsgcFOEea4=;
        b=VpKmF8OQgSYyZL2mDyJD67Pcd0gtkF3S7ubg2hrBqoY3TNs9E4Q/XDASeONbZ09PO0
         XOeoT6LFzEj0HqRTR11gdrmAUJZ2slOqhQbIjvfazr1K2+LD3qq69veljXlXTxdmS2Yn
         ekidhYoj4Q8djuX03YNF5Eadbn/wodWn9MKo1aiKGFuVkd7qzoWa4j8UDk09O3E2afI1
         fgPh4qBWQXnJ9ARuk3EaNsw5ijk7kyiZuza//0bxhRZckajJKDUTB5wJjiHCp5GFtq9V
         MVBES3uwLY8RoPzFKKGr10kSeHIHQuFS6/X/ZgZfTDXsKbCSlQ3L/nn3xmGD92LX3qUH
         BMyw==
MIME-Version: 1.0
X-Received: by 10.194.76.212 with SMTP id m20mr83269wjw.30.1404330093238; Wed,
 02 Jul 2014 12:41:33 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Wed, 2 Jul 2014 12:41:33 -0700 (PDT)
In-Reply-To: <1404328029153-7158.post@n3.nabble.com>
References: <1404143698646-7125.post@n3.nabble.com>
	<1404324151284-7155.post@n3.nabble.com>
	<CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com>
	<1404325292417-7157.post@n3.nabble.com>
	<1404328029153-7158.post@n3.nabble.com>
Date: Wed, 2 Jul 2014 15:41:33 -0400
Message-ID: <CADtDQQJJswVN=0TLd+JHtJk54gxC-_RVVqjM8LzchnZnHXzhVw@mail.gmail.com>
Subject: Re: Contributing to MLlib
From: RJ Nowling <rnowling@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Alex,

I'm also a new contributor.  I created a pull request for the KMeans
MiniBatch implementation here:

https://github.com/apache/spark/pull/1248

I also created a JIRA here:

https://issues.apache.org/jira/browse/SPARK-2308

As part of my work, I started to refactor the common code to create a
KMeansCommons file containing traits for the KMeans classes and KMeans
objects.

We should probability coordinate a bit on this.

RJ
rnowling@gmail.com

On Wed, Jul 2, 2014 at 3:07 PM, salexln <salexln@gmail.com> wrote:
> I opened a JIRA (https://issues.apache.org/jira/browse/SPARK-2344)
>
> and a pull request for this (https://github.com/salexln/spark/pull/1)
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7158.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.



-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 06:03:04 2014
Return-Path: <dev-return-8186-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6988511603
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 06:03:04 +0000 (UTC)
Received: (qmail 52177 invoked by uid 500); 3 Jul 2014 06:03:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52114 invoked by uid 500); 3 Jul 2014 06:03:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52091 invoked by uid 99); 3 Jul 2014 06:03:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 06:03:03 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.49] (HELO mail-qa0-f49.google.com) (209.85.216.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 06:03:00 +0000
Received: by mail-qa0-f49.google.com with SMTP id w8so9715842qac.8
        for <dev@spark.apache.org>; Wed, 02 Jul 2014 23:02:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=vQARY4/YoHrneYddpXqgLWi6bfAmbCHSuaDBcDzUd50=;
        b=moIxK3awwynWXPtxntQN7MxOIQON6zm8ePd9/NkV5Zf93KbXrWaLkrvVmtw0OOw5/X
         49ZqNAJCJVLb1VpWsF9/fAlk1L+jl6QCqyONw/HQJkHYLULGk2l3joid3TD0PCo2PuW+
         fuqRro6EqvKRNFFRIR1dbWszyqE6/ao9W/3blaD2+UJsXEv6b+CLUFZoHzsH0xEJ4YTh
         /75U+358U6/0VIqZOPc3ZaZEJU15m/xJdtQq5mWX3LTR9dfSUYDqp/OGIjQ0HRvPR/Ja
         bJbq1UGnfqe+ESnT4zTyiffWyJf5ock9FwrGmKBCNFAJ2sFfkKVsJodJVcrSY0i3Jwvj
         xjCA==
X-Gm-Message-State: ALoCoQmJYwI8NPGI8+WPh3rcFblVTi9o3Fe4pypqIqeSmdYkM1Y10mbh2CktIAulf165HLq059OH
X-Received: by 10.224.115.3 with SMTP id g3mr3958104qaq.9.1404367355082; Wed,
 02 Jul 2014 23:02:35 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 2 Jul 2014 23:02:14 -0700 (PDT)
In-Reply-To: <CAJiQeYKoo-6r_4q75FmcryKqy_iLcT929LKurP8AEAGgMFJrjg@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
 <CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
 <CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
 <CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
 <CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
 <CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com> <CAJiQeYKoo-6r_4q75FmcryKqy_iLcT929LKurP8AEAGgMFJrjg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 2 Jul 2014 23:02:14 -0700
Message-ID: <CAPh_B=aG=oXBLZ=iFhaLumdE0-HT+GKZgvHXQCu4ZOjDJOpEAA@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc96b055887704fd43c122
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc96b055887704fd43c122
Content-Type: text/plain; charset=UTF-8

On Wed, Jul 2, 2014 at 3:44 AM, Mridul Muralidharan <mridul@gmail.com>
wrote:

>
> >
> > The other thing we do need is the location of blocks. This is actually
> just
> > O(n) because we just need to know where the map was run.
>
> For well partitioned data, wont this not involve a lot of unwanted
> requests to nodes which are not hosting data for a reducer (and lack
> of ability to throttle).
>

Was that a question? (I'm guessing it is). What do you mean exactly?

--047d7bdc96b055887704fd43c122--

From dev-return-8187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 07:01:16 2014
Return-Path: <dev-return-8187-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 316AC11757
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 07:01:16 +0000 (UTC)
Received: (qmail 84886 invoked by uid 500); 3 Jul 2014 07:01:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84830 invoked by uid 500); 3 Jul 2014 07:01:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84818 invoked by uid 99); 3 Jul 2014 07:01:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 07:01:15 +0000
X-ASF-Spam-Status: No, hits=1.6 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 07:01:13 +0000
Received: by mail-wi0-f177.google.com with SMTP id r20so1682102wiv.4
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 00:00:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=fALOrPoGUSoFLhw1k/h1zihQY+PtUgMrsloV9GQ5pxM=;
        b=xNWDiYBJxDuT8YJU+EWgXIoKycGWpKnu5p7xEPlgL/fgnZscfvv6TDXVJ4JNDvgBYd
         Kuft8SopdpQj8UnoU5WlLo7/yuw4qTlkOrivrSXpURsoYB1O+LPAEnzmlhPV9VEfSb27
         o2yss6K+gAploQhp4YtLbRcA7pOnCdONllQsG23+z0QDPpB97R44d3eLmb0tr2xtAU3b
         2UxZ6f9bPdaS2KqB+qtRl9WFF4m+GX2VjKmRUPUXz7i3CMaJNuw5bifirEwomEk82IvC
         hLyq1Mz5/PM/mH870TI6JhYSmveylG9W/be2mJ5Mr3vxt4ZeX0SF3SYTECW4Nhs6b3nf
         KXSw==
MIME-Version: 1.0
X-Received: by 10.180.74.11 with SMTP id p11mr47317636wiv.68.1404370849742;
 Thu, 03 Jul 2014 00:00:49 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Thu, 3 Jul 2014 00:00:49 -0700 (PDT)
In-Reply-To: <CADtDQQJJswVN=0TLd+JHtJk54gxC-_RVVqjM8LzchnZnHXzhVw@mail.gmail.com>
References: <1404143698646-7125.post@n3.nabble.com>
	<1404324151284-7155.post@n3.nabble.com>
	<CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com>
	<1404325292417-7157.post@n3.nabble.com>
	<1404328029153-7158.post@n3.nabble.com>
	<CADtDQQJJswVN=0TLd+JHtJk54gxC-_RVVqjM8LzchnZnHXzhVw@mail.gmail.com>
Date: Thu, 3 Jul 2014 00:00:49 -0700
Message-ID: <CAJgQjQ-Z7w6Pxp9h0WT8mNU4y=edPMt8hDw3ZrCFNXjGJxZ3RQ@mail.gmail.com>
Subject: Re: Contributing to MLlib
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Alex, please send the pull request to apache/spark instead of your own
repo, following the instructions in

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Thanks,
Xiangrui

On Wed, Jul 2, 2014 at 12:41 PM, RJ Nowling <rnowling@gmail.com> wrote:
> Hey Alex,
>
> I'm also a new contributor.  I created a pull request for the KMeans
> MiniBatch implementation here:
>
> https://github.com/apache/spark/pull/1248
>
> I also created a JIRA here:
>
> https://issues.apache.org/jira/browse/SPARK-2308
>
> As part of my work, I started to refactor the common code to create a
> KMeansCommons file containing traits for the KMeans classes and KMeans
> objects.
>
> We should probability coordinate a bit on this.
>
> RJ
> rnowling@gmail.com
>
> On Wed, Jul 2, 2014 at 3:07 PM, salexln <salexln@gmail.com> wrote:
>> I opened a JIRA (https://issues.apache.org/jira/browse/SPARK-2344)
>>
>> and a pull request for this (https://github.com/salexln/spark/pull/1)
>>
>>
>>
>> --
>> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7158.html
>> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314

From dev-return-8188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 07:04:56 2014
Return-Path: <dev-return-8188-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 21B1A1175C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 07:04:56 +0000 (UTC)
Received: (qmail 86902 invoked by uid 500); 3 Jul 2014 07:04:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86835 invoked by uid 500); 3 Jul 2014 07:04:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86824 invoked by uid 99); 3 Jul 2014 07:04:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 07:04:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Bert.Greevenbosch@huawei.com designates 119.145.14.65 as permitted sender)
Received: from [119.145.14.65] (HELO szxga02-in.huawei.com) (119.145.14.65)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 07:04:50 +0000
Received: from 172.24.2.119 (EHLO SZXEMA405-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id BWE50917;
	Thu, 03 Jul 2014 15:04:29 +0800 (CST)
Received: from SZXEMA510-MBX.china.huawei.com ([169.254.3.190]) by
 SZXEMA405-HUB.china.huawei.com ([10.82.72.37]) with mapi id 14.03.0158.001;
 Thu, 3 Jul 2014 15:04:24 +0800
From: Bert Greevenbosch <Bert.Greevenbosch@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Artificial Neural Network in Spark?
Thread-Topic: Artificial Neural Network in Spark?
Thread-Index: Ac+RoTqSDV42paQFQ+OUNd1O9igqiAAKqVEX//984wD/+nPFgIALuSAA//w63bA=
Date: Thu, 3 Jul 2014 07:04:23 +0000
Message-ID: <46A1DF3F04371240B504290A071B4DB63E63D449@SZXEMA510-MBX.china.huawei.com>
References: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
	<092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
	<CA+B-+fy7fw7RVvvvWUOeQTFtZzvSiW2HXPz7TNxGWUFM5UOfug@mail.gmail.com>
	<46A1DF3F04371240B504290A071B4DB63E6395A3@SZXEMA510-MBX.china.huawei.com>
 <CA+B-+fxMxTuMNGYR8r2q0dBBwe+G1hVrZVO0oT0gPybNVMoY0Q@mail.gmail.com>
In-Reply-To: <CA+B-+fxMxTuMNGYR8r2q0dBBwe+G1hVrZVO0oT0gPybNVMoY0Q@mail.gmail.com>
Accept-Language: en-GB, zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.162.63]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-8189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 07:07:48 2014
Return-Path: <dev-return-8189-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6376511769
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 07:07:48 +0000 (UTC)
Received: (qmail 93804 invoked by uid 500); 3 Jul 2014 07:07:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93749 invoked by uid 500); 3 Jul 2014 07:07:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93738 invoked by uid 99); 3 Jul 2014 07:07:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 07:07:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Bert.Greevenbosch@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 07:07:43 +0000
Received: from 172.24.2.119 (EHLO SZXEMA401-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id ARD47462;
	Thu, 03 Jul 2014 15:07:20 +0800 (CST)
Received: from SZXEMA510-MBX.china.huawei.com ([169.254.3.190]) by
 SZXEMA401-HUB.china.huawei.com ([10.82.72.33]) with mapi id 14.03.0158.001;
 Thu, 3 Jul 2014 15:07:16 +0800
From: Bert Greevenbosch <Bert.Greevenbosch@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Artificial Neural Network in Spark?
Thread-Topic: Artificial Neural Network in Spark?
Thread-Index: Ac+RoTqSDV42paQFQ+OUNd1O9igqiAAKqVEX//984wD/+nPFgIAMHKUA//yK7iA=
Date: Thu, 3 Jul 2014 07:07:15 +0000
Message-ID: <46A1DF3F04371240B504290A071B4DB63E63D45C@SZXEMA510-MBX.china.huawei.com>
References: <46A1DF3F04371240B504290A071B4DB63E632EDC@SZXEMA510-MBX.china.huawei.com>
	<092662BE-0168-4260-B6D4-2EF5E9E3F42D@hp.com>
 <CA+B-+fy7fw7RVvvvWUOeQTFtZzvSiW2HXPz7TNxGWUFM5UOfug@mail.gmail.com>
 <46A1DF3F04371240B504290A071B4DB63E6395A3@SZXEMA510-MBX.china.huawei.com>
 <9D5B00849D2CDA4386BDA89E83F69E6C0FCA9253@G4W3292.americas.hpqcorp.net>
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FCA9253@G4W3292.americas.hpqcorp.net>
Accept-Language: en-GB, zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.162.63]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A020205.53B50129.00D0,ss=1,re=0.000,fgs=0,
	ip=169.254.3.190,
	so=2013-05-26 15:14:31,
	dmn=2011-05-27 18:58:46
X-Mirapoint-Loop-Id: 4ea57a73c0f113db63700ca14ab4de43
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-8190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 10:31:16 2014
Return-Path: <dev-return-8190-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7158B11D58
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 10:31:16 +0000 (UTC)
Received: (qmail 99799 invoked by uid 500); 3 Jul 2014 10:31:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99740 invoked by uid 500); 3 Jul 2014 10:31:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99724 invoked by uid 99); 3 Jul 2014 10:31:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 10:31:15 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 10:31:12 +0000
Received: by mail-qc0-f178.google.com with SMTP id c9so782qcz.23
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 03:30:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=QTUF6pJ4lcU6vQJ1qWHLnFboCMJpipOiYvre5qYDQ+8=;
        b=plNjkvhuhzK+64xSSS3z0uFMKFJi7awVAkkKYvmqq0p7EFs5AcofriMuPnXU2D2AOS
         5iO+DDkASvFF7PxZDeYYblisS32DtfEqDiyhutzcC54K3N6TCLy27bWGUMSdNRtU0mDS
         3B3cxxBb3ktsfwKLNCCLh8TQVHnFqfJ+lvL/ZweEBZko5qFeI52cPeriiKMUYLjFKkKm
         o2LQwTaQfq8tUGhozCZTrpLK0THo4UsPb1uADm7mFWcBZ0FVR/x6vjL13fX5qoO5AvoN
         Bmqi9aPG3phw6j35zFqHZCcK/MWhk20zvcoPB7GSYRPM4WWrPl6CxLXpTR8xX+fNYX5w
         nLhw==
MIME-Version: 1.0
X-Received: by 10.140.28.37 with SMTP id 34mr5723576qgy.28.1404383447347; Thu,
 03 Jul 2014 03:30:47 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Thu, 3 Jul 2014 03:30:47 -0700 (PDT)
In-Reply-To: <CAJgQjQ8KQRswOkedse2SXByU6r-XCgVZK2ftda-JKnjSu_r+QA@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
	<CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
	<CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
	<CAJgQjQ-wvbaE+QJ2Mngzpy=g5i7=4ym59JVrAMtijwv4uTbfPw@mail.gmail.com>
	<CA+B-+fxYLfEWx_+jzdfNO3DFa5dc4VXfEYpffpOT0xCV=yV4Ug@mail.gmail.com>
	<CAJgQjQ8KQRswOkedse2SXByU6r-XCgVZK2ftda-JKnjSu_r+QA@mail.gmail.com>
Date: Thu, 3 Jul 2014 03:30:47 -0700
Message-ID: <CA+B-+fz1DMVZz=vOU3GteUVUBNuFmo-O2gtcVm1-T-_L3u9Tow@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11393d4281fdfc04fd478068
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11393d4281fdfc04fd478068
Content-Type: text/plain; charset=UTF-8

Hi Xiangrui,

I did some out-of-box comparisons with ECOS and PDCO from SOL.

Both of them seems to be running at par but I will do more detailed
analysis.

I used pdco's testQP randomized problem generation. pdcotestQP(m, n) means
m constraints and n variables

For ECOS runtime reference here is the paper
http://web.stanford.edu/~boyd/papers/pdf/ecos_ecc.pdf

It runs at par with gurobi but slower than MOSEK. Note that MOSEK is also a
SOCP solver.

K>> pdcotestQP(50, 100)

ECOSQP: Converting QP to SOCP...

ECOSQP: Time for Cholesky: 0.00 seconds

Conversion completed. Calling ECOS...

ECOS - (c) A. Domahidi, Automatic Control Laboratory, ETH Zurich, 2012-2014.

OPTIMAL (within feastol=1.0e-05, reltol=1.0e-06, abstol=1.0e-06).

Runtime: 0.010340 seconds.

--------------------------------------------------------

   pdco.m                            Version of 23 Nov 2013

   Primal-dual barrier method to minimize a convex function

   subject to linear constraints Ax + r = b,  bl <= x <= bu



   Michael Saunders       SOL and ICME, Stanford University

   Contributors:     Byunggyoo Kim (SOL), Chris Maes (ICME)

                     Santiago Akle (ICME), Matt Zahr (ICME)

   --------------------------------------------------------

m        =       50     n        =      100      nnz(A)  =      483

Method   =       21     (1=chol  2=QR  3=LSMR  4=MINRES  21=SQD(LU)
22=SQD(MA57))

Elapsed time is 0.050226 seconds.

2. With a larger problem with 50 equality and 1000 variables:

K>> pdcotestQP(50, 1000)

ECOSQP: Converting QP to SOCP...

ECOSQP: Time for Cholesky: 0.05 seconds

Conversion completed. Calling ECOS...

ECOS - (c) A. Domahidi, Automatic Control Laboratory, ETH Zurich, 2012-2014.

OPTIMAL (within feastol=1.0e-05, reltol=1.0e-06, abstol=1.0e-06).

Runtime: 6.333036 seconds.


   --------------------------------------------------------

   pdco.m                            Version of 23 Nov 2013

   Primal-dual barrier method to minimize a convex function

   subject to linear constraints Ax + r = b,  bl <= x <= bu



   Michael Saunders       SOL and ICME, Stanford University

   Contributors:     Byunggyoo Kim (SOL), Chris Maes (ICME)

                     Santiago Akle (ICME), Matt Zahr (ICME)

   --------------------------------------------------------


The objective is defined by a function handle:

   @(x)deal(0.5*(x'*H*x)+c'*x,H*x+c,H)

The matrix A is an explicit sparse matrix

m        =       50     n        =     1000      nnz(A)  =     4842

Method   =       21     (1=chol  2=QR  3=LSMR  4=MINRES  21=SQD(LU)
22=SQD(MA57))

Elapsed time is 7.531934 seconds.

I will change the Method = 21 (LU) to 1 (chol) and that should help PDCO.
If both the IPMs are at par it's still a good idea to choose ECOS as the
generic IPM since it can solve conic programs which are a superset of
quadratic programs (robust portfolio optimization from quantitative finance
is an example of standard conic program).

For the runtime comparisons with ADMM based decomposition for simpler
constraints, I am doing further profiling and see if the jnilib is causing
any performance issues for ECOS calls...

Please look at the proximal algorithm references
http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf. For many problems
like L1 constraint / positivity / bounds / huber / hyperplane projection
etc, the proximal operator is simple to evaluate and for these cases ADMM
decomposition has been shown to run faster than standard constraint solvers
like IPM. I am not very surprised that Sparse NMF runs in 4X runtime of
least squares using ADMM decomposition.

Distributed consensus is another ADMM decomposition which we are working
with right now. We will have some results on that soon. There the idea is
to use ADMM so that accumulator need not collect dense gradient vectors
from each worker. This development will further help the treeReduce work.

Should I open up Spark JIRA's so that we can document Quadratic
Minimization related runtime experiments/benchmarks and share the code for
review ?

Most likely the core solvers will go to breeze and in Spark mllib
optimization, I will add a QpSolver object which will call the underlying
breeze solvers based on the problem complexity....the ecos jnilib can be
part of breeze natives as it is GPL licensed (same as netlib-java
jniloader). I will push the jnilib as a PR to ecos repository
https://github.com/ifa-ethz/ecos

Thanks.

Deb
On Wed, Jul 2, 2014 at 1:52 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> Hi Deb,
>
> KNITRO and MOSEK are both commercial. If you are looking for
> open-source ones, you can take a look at PDCO from SOL:
>
> http://web.stanford.edu/group/SOL/software/pdco/
>
> Each subproblem is really just a small QP. ADMM is designed for the
> cases when data is distributively stored or the objective function is
> complex but splittable. Neither applies to this case.
>
> Best,
> Xiangrui
>
> On Tue, Jul 1, 2014 at 11:05 PM, Debasish Das <debasish.das83@gmail.com>
> wrote:
> > Hi Xiangrui,
> >
> > Could you please point to the IPM solver that you have positive results
> > with ? I was planning to compare with CVX, KNITRO from Professor Nocedal
> > and MOSEK probably...I don't have CPLEX license so I won't be able to do
> > that comparison...
> >
> > My experiments so far tells me that ADMM based solver is faster than IPM
> > for simpler constraints but then perhaps I did not choose the correct
> > IPM....
> >
> > Proximal algorithm paper also shows very similar results compared to CVX:
> >
> > http://web.stanford.edu/~boyd/papers/pdf/prox_algs.pdf
> >
> > Thanks.
> > Deb
> >
> > On Wed, Jun 11, 2014 at 3:21 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> >
> >> You idea is close to what implicit feedback does. You can check the
> >> paper, which is short and concise. In the ALS setting, all subproblems
> >> are independent in each iteration. This is part of the reason why ALS
> >> is scalable. If you have some global constraints that make the
> >> subproblems no longer decoupled, that would certainly affects
> >> scalability. -Xiangrui
> >>
> >> On Wed, Jun 11, 2014 at 2:20 AM, Debasish Das <debasish.das83@gmail.com
> >
> >> wrote:
> >> > I got it...ALS formulation is solving the matrix completion
> problem....
> >> >
> >> > To convert the problem to matrix factorization or take user feedback
> >> > (missing entries means the user hate the site ?), we should put 0 to
> the
> >> > missing entries (or may be -1)...in that case we have to use
> computeYtY
> >> and
> >> > accumulate over users in each block to generate full gram matrix...and
> >> > after that while computing userXy(index) we have to be careful in
> putting
> >> > 0/-1 for rest of the features...
> >> >
> >> > Is implicit feedback trying to do something like this ?
> >> >
> >> > Basically I am trying to see if it is possible to cache the gram
> matrix
> >> and
> >> > it's cholesky factorization, and then call the QpSolver multiple times
> >> with
> >> > updated gradient term...I am expecting better runtimes than dposv when
> >> > ranks are high...
> >> >
> >> > But seems like that's not possible without a broadcast step which
> might
> >> > kill all the runtime gain...
> >> >
> >> >
> >> > On Wed, Jun 11, 2014 at 12:21 AM, Xiangrui Meng <mengxr@gmail.com>
> >> wrote:
> >> >
> >> >> For explicit feedback, ALS uses only observed ratings for
> computation.
> >> >> So XtXs are not the same. -Xiangrui
> >> >>
> >> >> On Tue, Jun 10, 2014 at 8:58 PM, Debasish Das <
> debasish.das83@gmail.com
> >> >
> >> >> wrote:
> >> >> > Sorry last one went out by mistake:
> >> >> >
> >> >> > Is not for users (0 to numUsers), fullXtX is same ? In the ALS
> >> >> formulation
> >> >> > this is W^TW or H^TH which should be same for all the users ? Why
> we
> >> are
> >> >> > reading userXtX(index) and adding it to fullXtX in the loop over
> all
> >> >> > numUsers ?
> >> >> >
> >> >> > // Solve the least-squares problem for each user and return the new
> >> >> feature
> >> >> > vectors
> >> >> >
> >> >> >     Array.range(0, numUsers).map { index =>
> >> >> >
> >> >> >       // Compute the full XtX matrix from the lower-triangular
> part we
> >> >> got
> >> >> > above
> >> >> >
> >> >> >       fillFullMatrix(userXtX(index), fullXtX)
> >> >> >
> >> >> >       // Add regularization
> >> >> >
> >> >> >       var i = 0
> >> >> >
> >> >> >       while (i < rank) {
> >> >> >
> >> >> >         fullXtX.data(i * rank + i) += lambda
> >> >> >
> >> >> >         i += 1
> >> >> >
> >> >> >       }
> >> >> >
> >> >> >       // Solve the resulting matrix, which is symmetric and
> >> >> > positive-definite
> >> >> >
> >> >> >       algo match {
> >> >> >
> >> >> >         case ALSAlgo.Implicit =>
> >> >> > Solve.solvePositive(fullXtX.addi(YtY.get.value),
> >> >> > userXy(index)).data
> >> >> >
> >> >> >         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX,
> userXy
> >> >> > (index)).data
> >> >> >
> >> >> >       }
> >> >> >
> >> >> >     }
> >> >> >
> >> >> >
> >> >> > On Tue, Jun 10, 2014 at 8:56 PM, Debasish Das <
> >> debasish.das83@gmail.com>
> >> >> > wrote:
> >> >> >
> >> >> >> Hi,
> >> >> >>
> >> >> >> I am bit confused wiht the code here:
> >> >> >>
> >> >> >> // Solve the least-squares problem for each user and return the
> new
> >> >> >> feature vectors
> >> >> >>
> >> >> >>     Array.range(0, numUsers).map { index =>
> >> >> >>
> >> >> >>       // Compute the full XtX matrix from the lower-triangular
> part
> >> we
> >> >> >> got above
> >> >> >>
> >> >> >>       fillFullMatrix(userXtX(index), fullXtX)
> >> >> >>
> >> >> >>       // Add regularization
> >> >> >>
> >> >> >>       var i = 0
> >> >> >>
> >> >> >>       while (i < rank) {
> >> >> >>
> >> >> >>         fullXtX.data(i * rank + i) += lambda
> >> >> >>
> >> >> >>         i += 1
> >> >> >>
> >> >> >>       }
> >> >> >>
> >> >> >>       // Solve the resulting matrix, which is symmetric and
> >> >> >> positive-definite
> >> >> >>
> >> >> >>       algo match {
> >> >> >>
> >> >> >>         case ALSAlgo.Implicit =>
> >> >> Solve.solvePositive(fullXtX.addi(YtY.get.value),
> >> >> >> userXy(index)).data
> >> >> >>
> >> >> >>         case ALSAlgo.Explicit => Solve.solvePositive(fullXtX,
> userXy
> >> >> >> (index)).data
> >> >> >>
> >> >> >>       }
> >> >> >>
> >> >> >>     }
> >> >> >>
> >> >> >>
> >> >> >> On Fri, Jun 6, 2014 at 10:42 AM, Debasish Das <
> >> debasish.das83@gmail.com
> >> >> >
> >> >> >> wrote:
> >> >> >>
> >> >> >>> Hi Xiangrui,
> >> >> >>>
> >> >> >>> It's not the linear constraint, It is quadratic inequality with
> >> slack,
> >> >> >>> first order taylor approximation of off diagonal cross terms and
> a
> >> >> cyclic
> >> >> >>> coordinate descent, which we think will yield
> orthogonality....It's
> >> >> still
> >> >> >>> under works...
> >> >> >>>
> >> >> >>> Also we want to put a L1 constraint as set of linear equations
> when
> >> >> >>> solving for ALS...
> >> >> >>>
> >> >> >>> I will create the JIRA...as I see it, this will evolve to a
> generic
> >> >> >>> constraint solver for machine learning problems that has a QP
> >> >> >>> structure....ALS is one example....another example is kernel
> SVMs...
> >> >> >>>
> >> >> >>> I did not know that lgpl solver can be added to the
> classpath....if
> >> it
> >> >> >>> can be then definitely we should add these in ALS.scala...
> >> >> >>>
> >> >> >>> Thanks.
> >> >> >>> Deb
> >> >> >>>
> >> >> >>>
> >> >> >>>
> >> >> >>> On Thu, Jun 5, 2014 at 11:31 PM, Xiangrui Meng <mengxr@gmail.com
> >
> >> >> wrote:
> >> >> >>>
> >> >> >>>> I don't quite understand why putting linear constraints can
> promote
> >> >> >>>> orthogonality. For the interfaces, if the subproblem is
> determined
> >> by
> >> >> >>>> Y^T Y and Y^T b for each iteration, then the least squares
> solver,
> >> the
> >> >> >>>> non-negative least squares solver, or your convex solver is
> simply
> >> a
> >> >> >>>> function
> >> >> >>>>
> >> >> >>>> (A, b) -> x.
> >> >> >>>>
> >> >> >>>> You can define it as an interface, and make the solver
> pluggable by
> >> >> >>>> adding a setter to ALS. If you want to use your lgpl solver,
> just
> >> >> >>>> include it in the classpath. Creating two separate files still
> >> seems
> >> >> >>>> unnecessary to me. Could you create a JIRA and we can move our
> >> >> >>>> discussion there? Thanks!
> >> >> >>>>
> >> >> >>>> Best,
> >> >> >>>> Xiangrui
> >> >> >>>>
> >> >> >>>> On Thu, Jun 5, 2014 at 7:20 PM, Debasish Das <
> >> >> debasish.das83@gmail.com>
> >> >> >>>> wrote:
> >> >> >>>> > Hi Xiangrui,
> >> >> >>>> >
> >> >> >>>> > For orthogonality properties in the factors we need a
> constraint
> >> >> solver
> >> >> >>>> > other than the usuals (l1, upper and lower bounds, l2 etc)
> >> >> >>>> >
> >> >> >>>> > The interface of constraint solver is standard and I can add
> it
> >> in
> >> >> >>>> mllib
> >> >> >>>> > optimization....
> >> >> >>>> >
> >> >> >>>> > But I am not sure how will I call the gpl licensed ipm solver
> >> from
> >> >> >>>> > mllib....assume the solver interface is as follows:
> >> >> >>>> >
> >> >> >>>> > Qpsolver (densematrix h, array [double] f, int linearEquality,
> >> int
> >> >> >>>> > linearInequality, bool lb, bool ub)
> >> >> >>>> >
> >> >> >>>> > And then I have functions to update equalities, inequalities,
> >> bounds
> >> >> >>>> etc
> >> >> >>>> > followed by the run which generates the solution....
> >> >> >>>> >
> >> >> >>>> > For l1 constraints I have to use epigraph formulation which
> >> needs a
> >> >> >>>> > variable transformation before the solve....
> >> >> >>>> >
> >> >> >>>> > I was thinking that for the problems that does not need
> >> constraints
> >> >> >>>> people
> >> >> >>>> > will use ALS.scala and ConstrainedALS.scala will have the
> >> >> constrained
> >> >> >>>> > formulations....
> >> >> >>>> >
> >> >> >>>> > I can point you to the code once it is ready and then you can
> >> guide
> >> >> me
> >> >> >>>> how
> >> >> >>>> > to refactor it to mllib als ?
> >> >> >>>> >
> >> >> >>>> > Thanks.
> >> >> >>>> > Deb
> >> >> >>>> > Hi Deb,
> >> >> >>>> >
> >> >> >>>> > Why do you want to make those methods public? If you only
> need to
> >> >> >>>> > replace the solver for subproblems. You can try to make the
> >> solver
> >> >> >>>> > pluggable. Now it supports least squares and non-negative
> least
> >> >> >>>> > squares. You can define an interface for the subproblem
> solvers
> >> and
> >> >> >>>> > maintain the IPM solver at your own code base, if the only
> >> >> information
> >> >> >>>> > you need is Y^T Y and Y^T b.
> >> >> >>>> >
> >> >> >>>> > Btw, just curious, what is the use case for quadratic
> >> constraints?
> >> >> >>>> >
> >> >> >>>> > Best,
> >> >> >>>> > Xiangrui
> >> >> >>>> >
> >> >> >>>> > On Thu, Jun 5, 2014 at 3:38 PM, Debasish Das <
> >> >> debasish.das83@gmail.com
> >> >> >>>> >
> >> >> >>>> > wrote:
> >> >> >>>> >> Hi,
> >> >> >>>> >>
> >> >> >>>> >> We are adding a constrained ALS solver in Spark to solve
> matrix
> >> >> >>>> >> factorization use-cases which needs additional constraints
> >> (bounds,
> >> >> >>>> >> equality, inequality, quadratic constraints)
> >> >> >>>> >>
> >> >> >>>> >> We are using a native version of a primal dual SOCP solver
> due
> >> to
> >> >> its
> >> >> >>>> > small
> >> >> >>>> >> memory footprint and sparse ccs matrix computation it
> uses...The
> >> >> >>>> solver
> >> >> >>>> >> depends on AMD and LDL packages from Timothy Davis for sparse
> >> ccs
> >> >> >>>> matrix
> >> >> >>>> >> algebra (released under lgpl)...
> >> >> >>>> >>
> >> >> >>>> >> Due to GPL dependencies, it won't be possible to release the
> >> code
> >> >> as
> >> >> >>>> > Apache
> >> >> >>>> >> license for now...If we get good results on our use-cases, we
> >> will
> >> >> >>>> plan to
> >> >> >>>> >> write a version in breeze/modify joptimizer for sparse ccs
> >> >> >>>> operations...
> >> >> >>>> >>
> >> >> >>>> >> I derived ConstrainedALS from Spark mllib ALS and I am
> comparing
> >> >> the
> >> >> >>>> >> performance with default ALS and non-negative ALS as
> baseline.
> >> Plan
> >> >> >>>> is to
> >> >> >>>> >> release the code as GPL license for community review...I have
> >> kept
> >> >> the
> >> >> >>>> >> package structure as org.apache.spark.mllib.recommendation
> >> >> >>>> >>
> >> >> >>>> >> There are some private functions defined in ALS, which I
> would
> >> >> like to
> >> >> >>>> >> reuse....Is it possible to take the private out from the
> >> following
> >> >> >>>> >> functions:
> >> >> >>>> >>
> >> >> >>>> >> 1. makeLinkRDDs
> >> >> >>>> >> 2. makeInLinkBlock
> >> >> >>>> >> 3. makeOutLinkBlock
> >> >> >>>> >> 4. randomFactor
> >> >> >>>> >> 5. unblockFactors
> >> >> >>>> >>
> >> >> >>>> >> I don't want to copy any code.... I can ask for a PR to make
> >> these
> >> >> >>>> >> changes...
> >> >> >>>> >>
> >> >> >>>> >> Thanks.
> >> >> >>>> >> Deb
> >> >> >>>>
> >> >> >>>
> >> >> >>>
> >> >> >>
> >> >>
> >>
>

--001a11393d4281fdfc04fd478068--

From dev-return-8191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 11:27:29 2014
Return-Path: <dev-return-8191-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B50F711EEE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 11:27:29 +0000 (UTC)
Received: (qmail 84660 invoked by uid 500); 3 Jul 2014 11:27:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84597 invoked by uid 500); 3 Jul 2014 11:27:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84585 invoked by uid 99); 3 Jul 2014 11:27:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 11:27:28 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 11:27:23 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id 469C927F
	for <dev@spark.apache.org>; Thu,  3 Jul 2014 11:26:57 +0000 (UTC)
Received: from G4W6305.americas.hpqcorp.net (16.210.26.230) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 3 Jul 2014 11:24:37 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.241]) by
 G4W6305.americas.hpqcorp.net ([16.210.26.230]) with mapi id 14.03.0169.001;
 Thu, 3 Jul 2014 11:24:37 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Pass parameters to RDD functions
Thread-Topic: Pass parameters to RDD functions
Thread-Index: Ac+WsV3jdqQ+R5QgQEmMgp6AuOgZ5Q==
Date: Thu, 3 Jul 2014 11:24:35 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.18]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225G4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

I wonder how I can pass parameters to RDD functions with closures. If I do =
it in a following way, Spark crashes with NotSerializableException:

class TextToWordVector(csvData:RDD[Array[String]]) {

  val n =3D 1
  lazy val x =3D csvData.map{ stringArr =3D> stringArr(n)}.collect()
}

Exception:
Job aborted due to stage failure: Task not serializable: java.io.NotSeriali=
zableException: org.apache.spark.mllib.util.TextToWordVector
org.apache.spark.SparkException: Job aborted due to stage failure: Task not=
 serializable: java.io.NotSerializableException: org.apache.spark.mllib.uti=
l.TextToWordVector
                at org.apache.spark.scheduler.DAGScheduler.org$apache$spark=
$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:103=
8)


This message proposes a workaround, but it didn't work for me:
http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CCAA_qdLr=
xXzwXd5=3D6SXLOgSmTTorpOADHjnOXn=3DtMrOLEJM=3DFrw@mail.gmail.com%3E

What is the best practice?

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225G4W3292americas_--

From dev-return-8192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 11:31:28 2014
Return-Path: <dev-return-8192-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C54111EFE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 11:31:28 +0000 (UTC)
Received: (qmail 91208 invoked by uid 500); 3 Jul 2014 11:31:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91156 invoked by uid 500); 3 Jul 2014 11:31:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90918 invoked by uid 99); 3 Jul 2014 11:31:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 11:31:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.128.177 as permitted sender)
Received: from [209.85.128.177] (HELO mail-ve0-f177.google.com) (209.85.128.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 11:31:24 +0000
Received: by mail-ve0-f177.google.com with SMTP id i13so68266veh.22
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 04:30:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=PRm8htn5eJqU2AZ0HNEe2dQs0WQfeuSqSBoMOsW7wyI=;
        b=F55nNE8rHUuD5ShXjuOhu74PWPFvm0nvAmeKdRTtVpF8RWbFEwqJ/qdILsbhtsrziS
         JD14PN3ORcVR9/pnYrD49yDXg6XmTWk5IilnCwxulhvldMFWIpNAHIRk5jdPXYXd/urN
         sYADVWTtm4JfUxFMnSTOvUKJWuOSrOVgNeGwUbpvVjgh+o1om5XwsSYsxhGDE1sjdN7N
         /CEZ8a8YAnS8F1vbbYg95WWwvNyK0XKGnmsGwjfTfRrq2g+Dlf5oi4KeWGvcxksQWI7T
         sdI0ucaqP4agyN3uT0gTWUqZn7NR8OBpuD1j9Olu0KdECsLOLmAiVyPNa29M2sWZRegi
         KIzw==
X-Gm-Message-State: ALoCoQm9YNg70/FRBU9kQ6LFTqvGHfNyWFOw2L10RJn8aSue0avz2q8jRkMr8vRWQsmsNGhDT0ze
X-Received: by 10.58.188.199 with SMTP id gc7mr3257794vec.4.1404387059644;
 Thu, 03 Jul 2014 04:30:59 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.243.4 with HTTP; Thu, 3 Jul 2014 04:30:38 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225@G4W3292.americas.hpqcorp.net>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 3 Jul 2014 12:30:38 +0100
Message-ID: <CAMAsSd+dDppUF4EhUDoDUBK2EB0hDY0_eUqbUh=tb3sJCvh3+Q@mail.gmail.com>
Subject: Re: Pass parameters to RDD functions
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Declare this class with "extends Serializable", meaning java.io.Serializable?

On Thu, Jul 3, 2014 at 12:24 PM, Ulanov, Alexander
<alexander.ulanov@hp.com> wrote:
> Hi,
>
> I wonder how I can pass parameters to RDD functions with closures. If I do it in a following way, Spark crashes with NotSerializableException:
>
> class TextToWordVector(csvData:RDD[Array[String]]) {
>
>   val n = 1
>   lazy val x = csvData.map{ stringArr => stringArr(n)}.collect()
> }
>
> Exception:
> Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.spark.mllib.util.TextToWordVector
> org.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: org.apache.spark.mllib.util.TextToWordVector
>                 at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1038)
>
>
> This message proposes a workaround, but it didn't work for me:
> http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CCAA_qdLrxXzwXd5=6SXLOgSmTTorpOADHjnOXn=tMrOLEJM=Frw@mail.gmail.com%3E
>
> What is the best practice?
>
> Best regards, Alexander

From dev-return-8193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 12:23:36 2014
Return-Path: <dev-return-8193-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4B7AA11094
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 12:23:36 +0000 (UTC)
Received: (qmail 11616 invoked by uid 500); 3 Jul 2014 12:23:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11560 invoked by uid 500); 3 Jul 2014 12:23:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11549 invoked by uid 99); 3 Jul 2014 12:23:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 12:23:34 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [15.201.208.55] (HELO g4t3427.houston.hp.com) (15.201.208.55)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 12:23:26 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3427.houston.hp.com (Postfix) with ESMTPS id EA836136
	for <dev@spark.apache.org>; Thu,  3 Jul 2014 12:23:05 +0000 (UTC)
Received: from G4W6301.americas.hpqcorp.net (16.210.26.226) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 3 Jul 2014 12:21:53 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.241]) by
 G4W6301.americas.hpqcorp.net ([16.210.26.226]) with mapi id 14.03.0169.001;
 Thu, 3 Jul 2014 12:21:53 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Pass parameters to RDD functions
Thread-Topic: Pass parameters to RDD functions
Thread-Index: Ac+WsV3jdqQ+R5QgQEmMgp6AuOgZ5QAANigAAAGkW8A=
Date: Thu, 3 Jul 2014 12:21:52 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA257@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225@G4W3292.americas.hpqcorp.net>
 <CAMAsSd+dDppUF4EhUDoDUBK2EB0hDY0_eUqbUh=tb3sJCvh3+Q@mail.gmail.com>
In-Reply-To: <CAMAsSd+dDppUF4EhUDoDUBK2EB0hDY0_eUqbUh=tb3sJCvh3+Q@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.18]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-8194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 15:02:18 2014
Return-Path: <dev-return-8194-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F1B8411913
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 15:02:17 +0000 (UTC)
Received: (qmail 27205 invoked by uid 500); 3 Jul 2014 15:02:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27141 invoked by uid 500); 3 Jul 2014 15:02:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27126 invoked by uid 99); 3 Jul 2014 15:02:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 15:02:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 15:02:15 +0000
Received: by mail-qa0-f50.google.com with SMTP id m5so285578qaj.23
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 08:01:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=BNfPEn0p6QRh9LhtnexI3vFEsDfj27BYGYgv4xfaLIA=;
        b=MUaeEaVfC6+NnHn9h5mxBjENXaufyEcZQ4BAR/tLr6cxSu9lOJH8q8Kb2HUfu/tqUQ
         JhkOB82W6XiG9nqFgLj2pBa2GKPn3HG+PTZu0tArWvzj6kwH9RQ7JUJHdtYbcO50L6eA
         eCotnLhCy8KyWaA4NL4fkn5NysMJDYM1/2fcFezM3Elf2aTt8wOFG+Jsztv+vtmllgLl
         9a4YJsNPQXWqkwxZDqyrem91zu5tawP2txMp9IgzEHPugyoeeFoRrHxDB7g3/GJwOHo6
         YXPDNxqlM8la2wPLljRRCxbax4voYfpkuf6D5na/7LsFDShWeFjmZxYfYrMKx15aGmIZ
         DFFg==
MIME-Version: 1.0
X-Received: by 10.224.80.67 with SMTP id s3mr8197344qak.92.1404399710380; Thu,
 03 Jul 2014 08:01:50 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Thu, 3 Jul 2014 08:01:50 -0700 (PDT)
In-Reply-To: <CAPh_B=aG=oXBLZ=iFhaLumdE0-HT+GKZgvHXQCu4ZOjDJOpEAA@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
	<CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
	<CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
	<CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
	<CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
	<CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com>
	<CAJiQeYKoo-6r_4q75FmcryKqy_iLcT929LKurP8AEAGgMFJrjg@mail.gmail.com>
	<CAPh_B=aG=oXBLZ=iFhaLumdE0-HT+GKZgvHXQCu4ZOjDJOpEAA@mail.gmail.com>
Date: Thu, 3 Jul 2014 20:31:50 +0530
Message-ID: <CAJiQeYJ+FRP9WdyvE2yMqde8a8LW=R5+C2537Wbpa1OaGq-6nA@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Thu, Jul 3, 2014 at 11:32 AM, Reynold Xin <rxin@databricks.com> wrote:
> On Wed, Jul 2, 2014 at 3:44 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>
>>
>> >
>> > The other thing we do need is the location of blocks. This is actually
>> just
>> > O(n) because we just need to know where the map was run.
>>
>> For well partitioned data, wont this not involve a lot of unwanted
>> requests to nodes which are not hosting data for a reducer (and lack
>> of ability to throttle).
>>
>
> Was that a question? (I'm guessing it is). What do you mean exactly?


I was not sure if I understood the proposal correctly - hence the
query : if I understood it right - the number of wasted requests goes
up by num_reducers * avg_nodes_not_hosting data.

Ofcourse, if avg_nodes_not_hosting data == 0, then we are fine !

Regards,
Mridul

From dev-return-8195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 15:45:51 2014
Return-Path: <dev-return-8195-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7290111BA2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 15:45:51 +0000 (UTC)
Received: (qmail 59698 invoked by uid 500); 3 Jul 2014 15:45:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59637 invoked by uid 500); 3 Jul 2014 15:45:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59626 invoked by uid 99); 3 Jul 2014 15:45:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 15:45:50 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of salexln@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 15:45:48 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <salexln@gmail.com>)
	id 1X2jCO-00066x-2K
	for dev@spark.incubator.apache.org; Thu, 03 Jul 2014 08:45:24 -0700
Date: Thu, 3 Jul 2014 08:45:24 -0700 (PDT)
From: salexln <salexln@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1404402324061-7169.post@n3.nabble.com>
In-Reply-To: <CAJgQjQ-Z7w6Pxp9h0WT8mNU4y=edPMt8hDw3ZrCFNXjGJxZ3RQ@mail.gmail.com>
References: <1404143698646-7125.post@n3.nabble.com> <1404324151284-7155.post@n3.nabble.com> <CABjXkq5fOWhJT7jCqctN7JUOb0bb0q8F7WLWCPEW2D5kiVz7rg@mail.gmail.com> <1404325292417-7157.post@n3.nabble.com> <1404328029153-7158.post@n3.nabble.com> <CADtDQQJJswVN=0TLd+JHtJk54gxC-_RVVqjM8LzchnZnHXzhVw@mail.gmail.com> <CAJgQjQ-Z7w6Pxp9h0WT8mNU4y=edPMt8hDw3ZrCFNXjGJxZ3RQ@mail.gmail.com>
Subject: Re: Contributing to MLlib
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

thanks for the input.
at the moment , I don't have any code commits yet.

I wanted to discuss the algorithm implementation prior to the code
submission.

(never work with Git\ GutHub - so I hope this isn't very basic stuff....)








--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-tp7125p7169.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 15:50:12 2014
Return-Path: <dev-return-8196-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0AA4711BB8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 15:50:12 +0000 (UTC)
Received: (qmail 66230 invoked by uid 500); 3 Jul 2014 15:50:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66175 invoked by uid 500); 3 Jul 2014 15:50:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66143 invoked by uid 99); 3 Jul 2014 15:50:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 15:50:10 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 15:50:09 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <turdakov@ispras.ru>)
	id 1X2jGa-0006ME-Pp
	for dev@spark.incubator.apache.org; Thu, 03 Jul 2014 08:49:44 -0700
Date: Thu, 3 Jul 2014 08:49:44 -0700 (PDT)
From: Denis Turdakov <turdakov@ispras.ru>
To: dev@spark.incubator.apache.org
Message-ID: <1404402584790-7170.post@n3.nabble.com>
Subject: PLSA
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hello guys,

We made pull request with PLSA and its modifications:
- https://github.com/apache/spark/pull/1269
- JIRA issue SPARK-2199
Could somebody look at the code and provide some feedback what we should
improve.

Best regards,
Denis Turdakov




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/PLSA-tp7170.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 16:28:48 2014
Return-Path: <dev-return-8197-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 47C3A11CF4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 16:28:48 +0000 (UTC)
Received: (qmail 86753 invoked by uid 500); 3 Jul 2014 16:28:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86699 invoked by uid 500); 3 Jul 2014 16:28:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86686 invoked by uid 99); 3 Jul 2014 16:28:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 16:28:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 16:28:42 +0000
Received: by mail-qa0-f45.google.com with SMTP id v10so386664qac.4
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 09:28:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ippo4TiOsoKo/VCS4Z3trNuOJtvX/z27SI9SnbSv47k=;
        b=yFKlPPNVvRwojc+C981kHbeq0bNc7AWLYNZPRDLJhZQ7EIsZ18OesN8jkE6imuJvnp
         XOY6yzNKZ1z0v20lPmn6d6zQQh0f2AkQ4g7drcM3gR0KluOz7ECm2Sj2NW65LJ0iGUEA
         zpOjnAOFRa4EzLEQI+pB/beGuri/te+vujZUPS+8VE22QXTCmvXVrQAXWAW/XV+A9RIh
         Ax2Wg7xWWFhyXp5pdSkkcTEaJPSafGTDbwHWZbOB8q+eU3XGJ61bB7fSXa3EAjx/S8kG
         QzEvKDFmun2G+3yhaUy+uQmOCk/JoipqaGKlkAlfM19s3qvx7unTRFIgPSO33lLYRa7b
         tbdQ==
X-Received: by 10.224.129.130 with SMTP id o2mr9327910qas.64.1404404901577;
 Thu, 03 Jul 2014 09:28:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Thu, 3 Jul 2014 09:28:01 -0700 (PDT)
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA257@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA225@G4W3292.americas.hpqcorp.net>
 <CAMAsSd+dDppUF4EhUDoDUBK2EB0hDY0_eUqbUh=tb3sJCvh3+Q@mail.gmail.com> <9D5B00849D2CDA4386BDA89E83F69E6C0FCAA257@G4W3292.americas.hpqcorp.net>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Thu, 3 Jul 2014 09:28:01 -0700
Message-ID: <CANGvG8q5kjyEuqe1aYofZGiKSRS5x3Md7uSS5h5z9QAQCXg=2w@mail.gmail.com>
Subject: Re: Pass parameters to RDD functions
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1132ef0c478bb604fd4c7f24
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ef0c478bb604fd4c7f24
Content-Type: text/plain; charset=UTF-8

Either Serializable works, scala Serializable extends Java's (originally
intended a common interface for people who didn't want to run Scala on a
JVM).

Class fields require the class be serialized along with the object to
access. If you declared "val n" inside a method's scope instead, though, we
wouldn't need the class. E.g.:

class TextToWordVector(csvData:RDD[Array[String]]) {
  def computeX() = {
    val n = 1
    csvData.map{ stringArr => stringArr(n)}.collect()
  }
  lazy val x = computeX()
}

Note that if the class itself doesn't actually contain many (large) fields,
though, it may not be an issue to actually transfer it around.



On Thu, Jul 3, 2014 at 5:21 AM, Ulanov, Alexander <alexander.ulanov@hp.com>
wrote:

> Thanks, this works both with Scala and Java Serializable. Which one should
> I use?
>
> Related question: I would like only the particular val to be used instead
> of the whole class, what should I do?
> As far as I understand, the whole class is serialized and transferred
> between nodes (am I right?)
>
> Alexander
>
> -----Original Message-----
> From: Sean Owen [mailto:sowen@cloudera.com]
> Sent: Thursday, July 03, 2014 3:31 PM
> To: dev@spark.apache.org
> Subject: Re: Pass parameters to RDD functions
>
> Declare this class with "extends Serializable", meaning
> java.io.Serializable?
>
> On Thu, Jul 3, 2014 at 12:24 PM, Ulanov, Alexander <
> alexander.ulanov@hp.com> wrote:
> > Hi,
> >
> > I wonder how I can pass parameters to RDD functions with closures. If I
> do it in a following way, Spark crashes with NotSerializableException:
> >
> > class TextToWordVector(csvData:RDD[Array[String]]) {
> >
> >   val n = 1
> >   lazy val x = csvData.map{ stringArr => stringArr(n)}.collect() }
> >
> > Exception:
> > Job aborted due to stage failure: Task not serializable:
> > java.io.NotSerializableException:
> > org.apache.spark.mllib.util.TextToWordVector
> > org.apache.spark.SparkException: Job aborted due to stage failure: Task
> not serializable: java.io.NotSerializableException:
> org.apache.spark.mllib.util.TextToWordVector
> >                 at
> > org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAG
> > Scheduler$$failJobAndIndependentStages(DAGScheduler.scala:1038)
> >
> >
> > This message proposes a workaround, but it didn't work for me:
> > http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CCAA
> > _qdLrxXzwXd5=6SXLOgSmTTorpOADHjnOXn=tMrOLEJM=Frw@mail.gmail.com%3E
> >
> > What is the best practice?
> >
> > Best regards, Alexander
>

--001a1132ef0c478bb604fd4c7f24--

From dev-return-8198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 16:38:11 2014
Return-Path: <dev-return-8198-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA99811D3C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 16:38:11 +0000 (UTC)
Received: (qmail 8420 invoked by uid 500); 3 Jul 2014 16:38:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8360 invoked by uid 500); 3 Jul 2014 16:38:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8347 invoked by uid 99); 3 Jul 2014 16:38:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 16:38:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 16:38:05 +0000
Received: by mail-qa0-f47.google.com with SMTP id hw13so398443qab.34
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 09:37:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=8E2Qs7USLh8sl+YDbXC7hBh+fnHRLN5QmyJFn6TYdbs=;
        b=pJ/wgIsboFwYq6R+/EooIrWZbiBR7tzdy2+VKUfBE6u63ld+nqHGdQu/cRJp3VORAy
         bbztKBTIgQ089AEOqJKRM4RWLoHQbe5W0JHioH8PeVXLEGFD5BUJ4dKaRij48BS3Sef2
         +I5Aa8b+wTiqA0Nq6eV3+oZjLLVpW8pxW4xwgmReb8ledIiVsug+p3ClDfQwHmSAgH70
         GiBhEi+hq9xLUQyl12awGeApLMFu9xd530PD8BC9JU8eDUCcqYxrvAhUnvAHx2Stomxa
         btoSyrHpVJ0/4Z06koRM0F+nEdTSrhuZwsd39ySly1Bbe6n4nayGbcsK5Q0JhBEl61S0
         045w==
X-Received: by 10.140.35.232 with SMTP id n95mr8517239qgn.82.1404405464666;
 Thu, 03 Jul 2014 09:37:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Thu, 3 Jul 2014 09:37:24 -0700 (PDT)
In-Reply-To: <CABDsqqY39hUXDvae4Fkybq1LBZaB-Wkuh0uNAXF1z+z0yPwh5Q@mail.gmail.com>
References: <CABDsqqZ3vsH-ebfjZWJHKhfFQoFiJZYko-eLEP_RT3Z278g0xQ@mail.gmail.com>
 <CANGvG8qFVu2Z1SNE0_UqwXrWv39fNt7CTVR-rCH8xTadBGm3NQ@mail.gmail.com>
 <CABDsqqZfE6SJBjuYVbJBdYgPQDC-9Yd=FZLc5oYJFcctL0G1Aw@mail.gmail.com>
 <CABDsqqau77QOd1zN6ScvPencRHtQEnk15rno6bnd-_6yV=3XOw@mail.gmail.com> <CABDsqqY39hUXDvae4Fkybq1LBZaB-Wkuh0uNAXF1z+z0yPwh5Q@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Thu, 3 Jul 2014 09:37:24 -0700
Message-ID: <CANGvG8rvJnh2Tz8EWc+9R5CZSbNQkfMxdD3JTrR_jCxgzLnnFw@mail.gmail.com>
Subject: Re: task always lost
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c03f34d7c52904fd4ca087
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c03f34d7c52904fd4ca087
Content-Type: text/plain; charset=UTF-8

The issue you're seeing is not the same as the one you linked to -- your
serialized task sizes are very small, and Mesos fine-grained mode doesn't
use Akka anyway.

The error log you printed seems to be from some sort of Mesos logs, but do
you happen to have the logs from the actual executors themselves? These
should be Spark logs which hopefully show the actual Exception (or lack
thereof) before the executors die.

The tasks are dying very quickly, so this is probably either related to
your application logic throwing some sort of fatal JVM error or due to your
Mesos setup. I'm not sure if that "Failed to fetch URIs for container" is
fatal or not.


On Wed, Jul 2, 2014 at 2:44 AM, qingyang li <liqingyang1985@gmail.com>
wrote:

> executor always been removed.
>
> someone encountered same issue
> https://groups.google.com/forum/#!topic/spark-users/-mYn6BF-Y5Y
>
> -------------
> 14/07/02 17:41:16 INFO storage.BlockManagerMasterActor: Trying to remove
> executor 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
> 14/07/02 17:41:16 INFO storage.BlockManagerMaster: Removed
> 20140616-104524-1694607552-5050-26919-1 successfully in removeExecutor
> 14/07/02 17:41:16 DEBUG spark.MapOutputTrackerMaster: Increasing epoch to
> 10
> 14/07/02 17:41:16 INFO scheduler.DAGScheduler: Host gained which was in
> lost list earlier: bigdata001
> 14/07/02 17:41:16 DEBUG scheduler.TaskSchedulerImpl: parentName: , name:
> TaskSet_0, runningTasks: 0
> 14/07/02 17:41:16 DEBUG scheduler.TaskSchedulerImpl: parentName: , name:
> TaskSet_0, runningTasks: 0
> 14/07/02 17:41:16 INFO scheduler.TaskSetManager: Starting task 0.0:0 as TID
> 12 on executor 20140616-143932-1694607552-5050-4080-3: bigdata004
> (NODE_LOCAL)
> 14/07/02 17:41:16 INFO scheduler.TaskSetManager: Serialized task 0.0:0 as
> 10785 bytes in 1 ms
> 14/07/02 17:41:16 INFO scheduler.TaskSetManager: Starting task 0.0:1 as TID
> 13 on executor 20140616-104524-1694607552-5050-26919-3: bigdata002
> (NODE_LOCAL
>
>
> 2014-07-02 12:01 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:
>
> > also this one in warning log:
> >
> > E0702 11:35:08.869998 17840 slave.cpp:2310] Container
> > 'af557235-2d5f-4062-aaf3-a747cb3cd0d1' for executor
> > '20140616-104524-1694607552-5050-26919-1' of framework
> > '20140702-113428-1694607552-5050-17766-0000' failed to start: Failed to
> > fetch URIs for container 'af557235-2d5f-4062-aaf3-a747cb3cd0d1': exit
> > status 32512
> >
> >
> > 2014-07-02 11:46 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:
> >
> > Here is the log:
> >>
> >> E0702 10:32:07.599364 14915 slave.cpp:2686] Failed to unmonitor
> container
> >> for executor 20140616-104524-1694607552-5050-26919-1 of framework
> >> 20140702-102939-1694607552-5050-14846-0000: Not monitored
> >>
> >>
> >> 2014-07-02 1:45 GMT+08:00 Aaron Davidson <ilikerps@gmail.com>:
> >>
> >> Can you post the logs from any of the dying executors?
> >>>
> >>>
> >>> On Tue, Jul 1, 2014 at 1:25 AM, qingyang li <liqingyang1985@gmail.com>
> >>> wrote:
> >>>
> >>> > i am using mesos0.19 and spark0.9.0 ,  the mesos cluster is started,
> >>> when I
> >>> > using spark-shell to submit one job, the tasks always lost.  here is
> >>> the
> >>> > log:
> >>> > ----------
> >>> > 14/07/01 16:24:27 INFO DAGScheduler: Host gained which was in lost
> list
> >>> > earlier: bigdata005
> >>> > 14/07/01 16:24:27 INFO TaskSetManager: Starting task 0.0:1 as TID
> 4042
> >>> on
> >>> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
> >>> (PROCESS_LOCAL)
> >>> > 14/07/01 16:24:27 INFO TaskSetManager: Serialized task 0.0:1 as 1570
> >>> bytes
> >>> > in 0 ms
> >>> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
> >>> > 20140616-104524-1694607552-5050-26919-1 from TaskSet 0.0
> >>> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4041 (task 0.0:0)
> >>> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
> >>> > 20140616-104524-1694607552-5050-26919-1 (epoch 3427)
> >>> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove
> >>> executor
> >>> > 20140616-104524-1694607552-5050-26919-1 from BlockManagerMaster.
> >>> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
> >>> > 20140616-104524-1694607552-5050-26919-1 successfully in
> removeExecutor
> >>> > 14/07/01 16:24:28 INFO TaskSetManager: Re-queueing tasks for
> >>> > 20140616-143932-1694607552-5050-4080-2 from TaskSet 0.0
> >>> > 14/07/01 16:24:28 WARN TaskSetManager: Lost TID 4042 (task 0.0:1)
> >>> > 14/07/01 16:24:28 INFO DAGScheduler: Executor lost:
> >>> > 20140616-143932-1694607552-5050-4080-2 (epoch 3428)
> >>> > 14/07/01 16:24:28 INFO BlockManagerMasterActor: Trying to remove
> >>> executor
> >>> > 20140616-143932-1694607552-5050-4080-2 from BlockManagerMaster.
> >>> > 14/07/01 16:24:28 INFO BlockManagerMaster: Removed
> >>> > 20140616-143932-1694607552-5050-4080-2 successfully in removeExecutor
> >>> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost
> list
> >>> > earlier: bigdata005
> >>> > 14/07/01 16:24:28 INFO DAGScheduler: Host gained which was in lost
> list
> >>> > earlier: bigdata001
> >>> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:1 as TID
> 4043
> >>> on
> >>> > executor 20140616-143932-1694607552-5050-4080-2: bigdata005
> >>> (PROCESS_LOCAL)
> >>> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:1 as 1570
> >>> bytes
> >>> > in 0 ms
> >>> > 14/07/01 16:24:28 INFO TaskSetManager: Starting task 0.0:0 as TID
> 4044
> >>> on
> >>> > executor 20140616-104524-1694607552-5050-26919-1: bigdata001
> >>> > (PROCESS_LOCAL)
> >>> > 14/07/01 16:24:28 INFO TaskSetManager: Serialized task 0.0:0 as 1570
> >>> bytes
> >>> > in 0 ms
> >>> >
> >>> >
> >>> > it seems other guy has also encountered such problem,
> >>> >
> >>> >
> >>>
> http://mail-archives.apache.org/mod_mbox/incubator-mesos-dev/201305.mbox/%3C201305161047069952830@nfs.iscas.ac.cn%3E
> >>> >
> >>>
> >>
> >>
> >
>

--001a11c03f34d7c52904fd4ca087--

From dev-return-8199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 17:57:32 2014
Return-Path: <dev-return-8199-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 660261108C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 17:57:32 +0000 (UTC)
Received: (qmail 3027 invoked by uid 500); 3 Jul 2014 17:57:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2972 invoked by uid 500); 3 Jul 2014 17:57:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2956 invoked by uid 99); 3 Jul 2014 17:57:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 17:57:31 +0000
X-ASF-Spam-Status: No, hits=3.8 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.47 as permitted sender)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 17:57:29 +0000
Received: by mail-qg0-f47.google.com with SMTP id q108so535632qgd.20
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 10:57:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JxMG8uoI6Syb6ZfjXeEW/7Zs1b7S1Iy8h2NLjgrO1QY=;
        b=Yfc+XLSwAvgvuLh4cIDeYXxTSva1s4eMYnEIErdml+LkFVG4sv0QeyRKz6qPInsBby
         rescejaNBrXFMSmuBrnB80NUUN2VhcRQ+FFsfuZbQz+OPYdBf0Idi54/YnXcvc8pSpHx
         3Ls+NZbOXjf15k37CvmdS18J27DdiAGuEDolB/+zCXG9hq0MGoQ89j4X/XoQwPqxeeRN
         WRW0qBWhb3jSZWXvhOHQ/vKWkDzzqI10rnPsnkNLJHcmh7Ir9u510hD5bj2AC81QGoWp
         dY0GmneE9cFvMUAuDHFQwdFxCF5CgoFBVc+FlEaLUU/G/L8R4LQH8ai4J+2JL8ims7LR
         +UsQ==
MIME-Version: 1.0
X-Received: by 10.224.7.202 with SMTP id e10mr10081704qae.15.1404410225093;
 Thu, 03 Jul 2014 10:57:05 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Thu, 3 Jul 2014 10:57:05 -0700 (PDT)
In-Reply-To: <1404402584790-7170.post@n3.nabble.com>
References: <1404402584790-7170.post@n3.nabble.com>
Date: Thu, 3 Jul 2014 10:57:05 -0700
Message-ID: <CA+B-+fwbuTmWNa3y+9w9RZyR3_MJ4jPDBUkfkvr6rbP_z0+29Q@mail.gmail.com>
Subject: Re: PLSA
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c229e896097604fd4dbcc7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c229e896097604fd4dbcc7
Content-Type: text/plain; charset=UTF-8

Hi Denis,

Are you using matrix factorization to generate the latent factors ?

Thanks.
Deb



On Thu, Jul 3, 2014 at 8:49 AM, Denis Turdakov <turdakov@ispras.ru> wrote:

> Hello guys,
>
> We made pull request with PLSA and its modifications:
> - https://github.com/apache/spark/pull/1269
> - JIRA issue SPARK-2199
> Could somebody look at the code and provide some feedback what we should
> improve.
>
> Best regards,
> Denis Turdakov
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/PLSA-tp7170.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c229e896097604fd4dbcc7--

From dev-return-8200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 20:38:08 2014
Return-Path: <dev-return-8200-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 68B291157F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 20:38:08 +0000 (UTC)
Received: (qmail 46464 invoked by uid 500); 3 Jul 2014 20:38:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46400 invoked by uid 500); 3 Jul 2014 20:38:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46387 invoked by uid 99); 3 Jul 2014 20:38:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 20:38:07 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 20:38:03 +0000
Received: by mail-oa0-f44.google.com with SMTP id i7so847624oag.17
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 13:37:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=REXaoM6r8DVPxanY1Relvff4rfQpn7u9Nsv1liuIUJg=;
        b=qT+IG3L+7rPY5WyzRjlGYwSMhnrpcbhpP90C9jzPKEfjePaTuvSMlDMDP4G/xojIpp
         TjRCQ4sCN7i9ZHGCHi8b2U0KVRZdeRkBCqJ/AOIxIv0Tw+z43EvNoCP7goEMM49K7nn7
         XuDiSdeMLznzcXQXzYpPUb1CX5eeXDEc2LuilWoH897c95QnUaB/PQu07NRnGnhpYphe
         COBfVfp4cIZYjx5YPOkVCy3fhj0+3+4Cn6nsEdpcv6r4ghsFxk0x/tHVlMaLYvX1akz+
         ogp/TxAslHZU/FLcUwPlS0vO+TVkyvybfjFxAFM9+1cKMQFxVWjqkBci2sO+OQq1YK6y
         MoyQ==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr7389884obc.75.1404419862822;
 Thu, 03 Jul 2014 13:37:42 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Thu, 3 Jul 2014 13:37:42 -0700 (PDT)
In-Reply-To: <CAAsvFP=OS7L8O5wxThHpVNdPtmViySk2rJ3w=uxivHiTyGL54A@mail.gmail.com>
References: <CABPQxsvUMnvU7ZLkGArVyTTV0xo-Vfmk5NAcpEJhxXJKcp5a+A@mail.gmail.com>
	<CAAsvFP=OS7L8O5wxThHpVNdPtmViySk2rJ3w=uxivHiTyGL54A@mail.gmail.com>
Date: Thu, 3 Jul 2014 13:37:42 -0700
Message-ID: <CABPQxsvbRzCUAC4jdTbtBHMHRZfhfxqcMxZwksMG+=8o5aUZcw@mail.gmail.com>
Subject: Re: Assorted project updates (tests, build, etc)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Just a reminder here - we'll soon be merging a patch that changes the
SBT build internals significantly. We've tried to make this fully
backwards compatible, but there may be issues (which we'll resolve as
they arrive).

https://github.com/apache/spark/pull/77

- Patrick

On Sun, Jun 22, 2014 at 10:27 AM, Mark Hamstra <mark@clearstorydata.com> wrote:
> Just a couple of FYI notes: With Zinc and the scala-maven-plugin, repl and
> incremental builds are also available to those doing day-to-day development
> using Maven.  As long as you don't have to delve into the extra boilerplate
> and verbosity of Maven's POMs relative to an SBT build file, there is
> little day-to-day functional difference between the two -- if anything, I
> find that Maven supports faster development cycles.
>
>
> On Sun, Jun 22, 2014 at 12:24 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Hey All,
>>
>> 1. The original test infrastructure hosted by the AMPLab has been
>> fully restored and also expanded with many more executor slots for
>> tests. Thanks to Matt Massie at the Amplab for helping with this.
>>
>> 2. We now have a nightly build matrix across different Hadoop
>> versions. It appears that the Maven build is failing tests with some
>> of the newer Hadoop versions. If people from the community are
>> interested, diagnosing and fixing test issues would be welcome patches
>> (they are all dependency related).
>>
>> https://issues.apache.org/jira/browse/SPARK-2232
>>
>> 3. Prashant Sharma has spent a lot of time to make it possible for our
>> sbt build to read dependencies from Maven. This will save us a huge
>> amount of headache keeping the builds consistent. I just wanted to
>> give a heads up to users about this - we should retain compatibility
>> with features of the sbt build, but if you are e.g. hooking into deep
>> internals of our build it may affect you. I'm hoping this can be
>> updated and merged in the next week:
>>
>> https://github.com/apache/spark/pull/77
>>
>> 4. We've moved most of the documentation over to recommending users
>> build with Maven when creating official packages. This is just to
>> provide a single "reference build" of Spark since it's the one we test
>> and package for releases, we make sure all recursive dependencies are
>> correct, etc. I'd recommend that all downstream packagers use this
>> build.
>>
>> For day-to-day development I imagine sbt will remain more popular
>> (repl, incremental builds, etc). Prashant's work allows us to get the
>> "best of both worlds" which is great.
>>
>> - Patrick
>>

From dev-return-8201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 22:13:38 2014
Return-Path: <dev-return-8201-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D41F11838
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 22:13:38 +0000 (UTC)
Received: (qmail 58759 invoked by uid 500); 3 Jul 2014 22:13:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58705 invoked by uid 500); 3 Jul 2014 22:13:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58694 invoked by uid 99); 3 Jul 2014 22:13:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 22:13:36 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 22:13:32 +0000
Received: by mail-qa0-f42.google.com with SMTP id dc16so737170qab.1
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 15:13:11 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=uiBDr2fBuCct/p4o2Zf6u+ivEJTV8i9rFesIK7ROHLg=;
        b=b/pIY9uOYdsCSWV3reHxXd842fXfoxG+4rBag/VigbzroSBGGWFLszehjLGnalOVf1
         OEdd26xdGzbt6JdSyxQ357Wr6gkdhpzEVuCwfyIFBjmQaRFYBWDk+NJxBV9tjeVBbzuE
         zHy8ZxP6nTXHXnkqyGVeHqxgBxF+k5T9TgARrCKW6OXgmpqhDt8k9YMmP5AGHir8Tt7e
         qQNWm5NjB/eNad7CYUm5zagYkuOolIbzW7BfGkwfgu4gy7A4zzJrKnWARW6WDEtAm7tW
         ofVhFxu/qwQ974Y7B1mLr8Ak+JcfgVOaaF5imtL9CCA2Mh1Nqz4f3k1XeBo13n+ejoew
         mPFA==
X-Gm-Message-State: ALoCoQkaQM93Lo2NvlvSJsEKe4KwY3UpYEyv2unf5S9gMiaG9RnXauRCMKDFto1RO8FP9k0UdJcY
X-Received: by 10.140.95.215 with SMTP id i81mr11647614qge.6.1404425591776;
 Thu, 03 Jul 2014 15:13:11 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Thu, 3 Jul 2014 15:12:50 -0700 (PDT)
In-Reply-To: <CAJiQeYJ+FRP9WdyvE2yMqde8a8LW=R5+C2537Wbpa1OaGq-6nA@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
 <CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
 <CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
 <CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
 <CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
 <CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com>
 <CAJiQeYKoo-6r_4q75FmcryKqy_iLcT929LKurP8AEAGgMFJrjg@mail.gmail.com>
 <CAPh_B=aG=oXBLZ=iFhaLumdE0-HT+GKZgvHXQCu4ZOjDJOpEAA@mail.gmail.com> <CAJiQeYJ+FRP9WdyvE2yMqde8a8LW=R5+C2537Wbpa1OaGq-6nA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 3 Jul 2014 15:12:50 -0700
Message-ID: <CAPh_B=aJUm3fb06a9mNPtz+ec17YKyXELdvHW-1SwpOnz+6vsA@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c15de683020f04fd5150d1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c15de683020f04fd5150d1
Content-Type: text/plain; charset=UTF-8

Yes, that number is likely == 0 in any real workload ...


On Thu, Jul 3, 2014 at 8:01 AM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> On Thu, Jul 3, 2014 at 11:32 AM, Reynold Xin <rxin@databricks.com> wrote:
> > On Wed, Jul 2, 2014 at 3:44 AM, Mridul Muralidharan <mridul@gmail.com>
> > wrote:
> >
> >>
> >> >
> >> > The other thing we do need is the location of blocks. This is actually
> >> just
> >> > O(n) because we just need to know where the map was run.
> >>
> >> For well partitioned data, wont this not involve a lot of unwanted
> >> requests to nodes which are not hosting data for a reducer (and lack
> >> of ability to throttle).
> >>
> >
> > Was that a question? (I'm guessing it is). What do you mean exactly?
>
>
> I was not sure if I understood the proposal correctly - hence the
> query : if I understood it right - the number of wasted requests goes
> up by num_reducers * avg_nodes_not_hosting data.
>
> Ofcourse, if avg_nodes_not_hosting data == 0, then we are fine !
>
> Regards,
> Mridul
>

--001a11c15de683020f04fd5150d1--

From dev-return-8202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 22:14:17 2014
Return-Path: <dev-return-8202-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0D8D811839
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 22:14:17 +0000 (UTC)
Received: (qmail 59826 invoked by uid 500); 3 Jul 2014 22:14:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59768 invoked by uid 500); 3 Jul 2014 22:14:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59754 invoked by uid 99); 3 Jul 2014 22:14:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 22:14:16 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 22:14:13 +0000
Received: by mail-qg0-f44.google.com with SMTP id j107so793755qga.17
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 15:13:49 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=a23Yh4aC4tHPtF6/x1D/HJlEigHdWM0iSNH+OELs01I=;
        b=hp4QG3Od+HLOyl90HMm1NtMADdXNlQ5Hd8toNYV7SdLxPD4DHaMNjHpXz7LP1QHHPh
         sSm7AVzYBxo5Y/uTnnI3JBbskPNY+D5VTL4zu1Exo8JBieJMjw2HtbFDA5FEMIAp1FAF
         CVAfmE8w+GOq2GeZ1AjzQ6+Ptn3dYRkGmaCbWFCVCk0vbsEdDdF29Eu7S5RHp1ru/1m1
         Ic3O5PeN/IMSbMLOZuZRnQR4o1nAPeuiubpfbt85v6T0dmWDVmGfoNTkWKovG2EwEbcC
         2RbSiWXMAlnPqP+NnsRbXOOfnyiuddUrQowzv+wIE1RohaFyhboYqa85tCW9cSTC+zD+
         m+0A==
X-Gm-Message-State: ALoCoQlilojbj8R9Xj1ptyEoXZYqJTgLg9YeLCGBTHaO7N8sPsPjjzegk7kr2MVe+UZbm0XU5LhL
X-Received: by 10.140.88.230 with SMTP id t93mr602293qgd.47.1404425628904;
 Thu, 03 Jul 2014 15:13:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Thu, 3 Jul 2014 15:13:28 -0700 (PDT)
In-Reply-To: <CAPh_B=aJUm3fb06a9mNPtz+ec17YKyXELdvHW-1SwpOnz+6vsA@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
 <CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
 <CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
 <CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
 <CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
 <CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com>
 <CAJiQeYKoo-6r_4q75FmcryKqy_iLcT929LKurP8AEAGgMFJrjg@mail.gmail.com>
 <CAPh_B=aG=oXBLZ=iFhaLumdE0-HT+GKZgvHXQCu4ZOjDJOpEAA@mail.gmail.com>
 <CAJiQeYJ+FRP9WdyvE2yMqde8a8LW=R5+C2537Wbpa1OaGq-6nA@mail.gmail.com> <CAPh_B=aJUm3fb06a9mNPtz+ec17YKyXELdvHW-1SwpOnz+6vsA@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 3 Jul 2014 15:13:28 -0700
Message-ID: <CAPh_B=bZGzU=zRgNApP5nkdOuh=L36FqJU9zuaU_ZD2jnBAPKw@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c13d12b97dea04fd5152bd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c13d12b97dea04fd5152bd
Content-Type: text/plain; charset=UTF-8

Note that in my original proposal, I was suggesting we could track whether
block size = 0 using a compressed bitmap. That way we can still avoid
requests for zero-sized blocks.



On Thu, Jul 3, 2014 at 3:12 PM, Reynold Xin <rxin@databricks.com> wrote:

> Yes, that number is likely == 0 in any real workload ...
>
>
> On Thu, Jul 3, 2014 at 8:01 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>
>> On Thu, Jul 3, 2014 at 11:32 AM, Reynold Xin <rxin@databricks.com> wrote:
>> > On Wed, Jul 2, 2014 at 3:44 AM, Mridul Muralidharan <mridul@gmail.com>
>> > wrote:
>> >
>> >>
>> >> >
>> >> > The other thing we do need is the location of blocks. This is
>> actually
>> >> just
>> >> > O(n) because we just need to know where the map was run.
>> >>
>> >> For well partitioned data, wont this not involve a lot of unwanted
>> >> requests to nodes which are not hosting data for a reducer (and lack
>> >> of ability to throttle).
>> >>
>> >
>> > Was that a question? (I'm guessing it is). What do you mean exactly?
>>
>>
>> I was not sure if I understood the proposal correctly - hence the
>> query : if I understood it right - the number of wasted requests goes
>> up by num_reducers * avg_nodes_not_hosting data.
>>
>> Ofcourse, if avg_nodes_not_hosting data == 0, then we are fine !
>>
>> Regards,
>> Mridul
>>
>
>

--001a11c13d12b97dea04fd5152bd--

From dev-return-8203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul  3 22:49:12 2014
Return-Path: <dev-return-8203-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B952118F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  3 Jul 2014 22:49:12 +0000 (UTC)
Received: (qmail 7268 invoked by uid 500); 3 Jul 2014 22:49:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7214 invoked by uid 500); 3 Jul 2014 22:49:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7194 invoked by uid 99); 3 Jul 2014 22:49:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 22:49:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 03 Jul 2014 22:49:07 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so986319oag.33
        for <dev@spark.apache.org>; Thu, 03 Jul 2014 15:48:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TwqPkHuT3bnPaijnOphCWo5gadWvhxZ91rlu3a14L0M=;
        b=btrNqOxwL6dT6V+wlt9Mop1RLnfJgAsbnFSUkYdIcRGpTn8RI0ohXGpa2GqxwgiVmk
         4PAeGdwG232Jk5tEvUTUEeI78HFOR4YCmOVOOS1H+wUUBrzMU85UYa9+TLPPn9yalpF3
         Y4nKKOTDLyLnHzU85kO0gDVQUWyEMr4i0OH0LHEaC/iZLjg5CNyTm4xwJfJ1aKgl3WfS
         xOlBbB4/H/c3PA0nWi3S+FPmmfr/n0oe8qK36/VfhqBXFR2dKHeDLc3CnydQ8txzycPH
         BRMNm119HZw5bzsMr6bQG3cHkX9KwS9MlvlLjjNBa8dBTEBqra/T/DNqmASYwTUgEM1f
         V8Jg==
MIME-Version: 1.0
X-Received: by 10.60.120.98 with SMTP id lb2mr8168017oeb.52.1404427726749;
 Thu, 03 Jul 2014 15:48:46 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Thu, 3 Jul 2014 15:48:46 -0700 (PDT)
In-Reply-To: <CABPQxsvbRzCUAC4jdTbtBHMHRZfhfxqcMxZwksMG+=8o5aUZcw@mail.gmail.com>
References: <CABPQxsvUMnvU7ZLkGArVyTTV0xo-Vfmk5NAcpEJhxXJKcp5a+A@mail.gmail.com>
	<CAAsvFP=OS7L8O5wxThHpVNdPtmViySk2rJ3w=uxivHiTyGL54A@mail.gmail.com>
	<CABPQxsvbRzCUAC4jdTbtBHMHRZfhfxqcMxZwksMG+=8o5aUZcw@mail.gmail.com>
Date: Thu, 3 Jul 2014 15:48:46 -0700
Message-ID: <CABPQxsv58JmPtt65x0rnru2iizgsiTbhZcV07zt1afeNbRYdSw@mail.gmail.com>
Subject: Re: Assorted project updates (tests, build, etc)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry all, I sent the wrong pull request to refer to Prashant's work:

https://github.com/apache/spark/pull/772

On Thu, Jul 3, 2014 at 1:37 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Just a reminder here - we'll soon be merging a patch that changes the
> SBT build internals significantly. We've tried to make this fully
> backwards compatible, but there may be issues (which we'll resolve as
> they arrive).
>
> https://github.com/apache/spark/pull/77
>
> - Patrick
>
> On Sun, Jun 22, 2014 at 10:27 AM, Mark Hamstra <mark@clearstorydata.com> wrote:
>> Just a couple of FYI notes: With Zinc and the scala-maven-plugin, repl and
>> incremental builds are also available to those doing day-to-day development
>> using Maven.  As long as you don't have to delve into the extra boilerplate
>> and verbosity of Maven's POMs relative to an SBT build file, there is
>> little day-to-day functional difference between the two -- if anything, I
>> find that Maven supports faster development cycles.
>>
>>
>> On Sun, Jun 22, 2014 at 12:24 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> Hey All,
>>>
>>> 1. The original test infrastructure hosted by the AMPLab has been
>>> fully restored and also expanded with many more executor slots for
>>> tests. Thanks to Matt Massie at the Amplab for helping with this.
>>>
>>> 2. We now have a nightly build matrix across different Hadoop
>>> versions. It appears that the Maven build is failing tests with some
>>> of the newer Hadoop versions. If people from the community are
>>> interested, diagnosing and fixing test issues would be welcome patches
>>> (they are all dependency related).
>>>
>>> https://issues.apache.org/jira/browse/SPARK-2232
>>>
>>> 3. Prashant Sharma has spent a lot of time to make it possible for our
>>> sbt build to read dependencies from Maven. This will save us a huge
>>> amount of headache keeping the builds consistent. I just wanted to
>>> give a heads up to users about this - we should retain compatibility
>>> with features of the sbt build, but if you are e.g. hooking into deep
>>> internals of our build it may affect you. I'm hoping this can be
>>> updated and merged in the next week:
>>>
>>> https://github.com/apache/spark/pull/77
>>>
>>> 4. We've moved most of the documentation over to recommending users
>>> build with Maven when creating official packages. This is just to
>>> provide a single "reference build" of Spark since it's the one we test
>>> and package for releases, we make sure all recursive dependencies are
>>> correct, etc. I'd recommend that all downstream packagers use this
>>> build.
>>>
>>> For day-to-day development I imagine sbt will remain more popular
>>> (repl, incremental builds, etc). Prashant's work allows us to get the
>>> "best of both worlds" which is great.
>>>
>>> - Patrick
>>>

From dev-return-8204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 09:29:13 2014
Return-Path: <dev-return-8204-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8DCEA11592
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 09:29:13 +0000 (UTC)
Received: (qmail 43158 invoked by uid 500); 4 Jul 2014 09:29:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43109 invoked by uid 500); 4 Jul 2014 09:29:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43097 invoked by uid 99); 4 Jul 2014 09:29:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 09:29:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 09:29:08 +0000
Received: by mail-qa0-f54.google.com with SMTP id v10so1145588qac.27
        for <dev@spark.apache.org>; Fri, 04 Jul 2014 02:28:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0sjnqXaSyAGjzV4hbWmV+miqdWm/sPIo35OQi7czE0o=;
        b=P8x5H1KprGz92JdA78lk+ZjzgheMiecAV8kaPf/lpnMVRB+O5n7UuOStqYwwoGCjeP
         nsSD2r3idosuqv7vg9APkgZOWuTPx8NN9TzY9+5Fd1dzgSL8UO8SjZvzFVjGbMiOIrqv
         zO8Vcbd2NSuNTE/CymztqdWpHut5oxkKH8898VQhRCH0+OWUQNnDEXVI9nueGi8D7lkp
         4dzhh7WpnqqPXNhlKTygxjXRlAprGfD9LOgZLSwg/+G7eNXxtPcPNG7QAH2OJ6g2q2au
         C6Cwa0Y2RbLxaeoqiNkzfWA6R0YCBjzhFrv/g8MTm7rsg+bCI8J+GYzLCtQTbzZjWLH5
         7Xmw==
MIME-Version: 1.0
X-Received: by 10.140.31.119 with SMTP id e110mr15505434qge.74.1404466127429;
 Fri, 04 Jul 2014 02:28:47 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Fri, 4 Jul 2014 02:28:47 -0700 (PDT)
In-Reply-To: <CAPh_B=bZGzU=zRgNApP5nkdOuh=L36FqJU9zuaU_ZD2jnBAPKw@mail.gmail.com>
References: <CAJiQeYJ=UvuOownStpX+fTxKrZS9Sg7=hSCeD_RJ=-SAX1snWQ@mail.gmail.com>
	<CAJiQeYL9xXFzHKZ-3VjO8xT2DnQR0yfY5Ls9em9JXb2HXZZenw@mail.gmail.com>
	<CANGvG8oR9D40oZGSXPzZ7+M=bE+3zdTCmy+HKNXgTvvRHYnRdQ@mail.gmail.com>
	<CABPQxsuREAO284UkSgK0EoprmPwWqjcACpLOyvC1cnRJnAe6xQ@mail.gmail.com>
	<CAJiQeY+K7Z97hhYFuEuh9VU8_xKs=uNq-NMMXA32752EBBEfew@mail.gmail.com>
	<CAPh_B=bQ3fU3OHKW7+7RN_+cMZrZD1MOidcWsZytKga36VwxSw@mail.gmail.com>
	<CAJiQeYKoo-6r_4q75FmcryKqy_iLcT929LKurP8AEAGgMFJrjg@mail.gmail.com>
	<CAPh_B=aG=oXBLZ=iFhaLumdE0-HT+GKZgvHXQCu4ZOjDJOpEAA@mail.gmail.com>
	<CAJiQeYJ+FRP9WdyvE2yMqde8a8LW=R5+C2537Wbpa1OaGq-6nA@mail.gmail.com>
	<CAPh_B=aJUm3fb06a9mNPtz+ec17YKyXELdvHW-1SwpOnz+6vsA@mail.gmail.com>
	<CAPh_B=bZGzU=zRgNApP5nkdOuh=L36FqJU9zuaU_ZD2jnBAPKw@mail.gmail.com>
Date: Fri, 4 Jul 2014 14:58:47 +0530
Message-ID: <CAJiQeYJuF79Dt4usg6OH-FAa8ozA+82dOrHVG4Oty48eFXh75w@mail.gmail.com>
Subject: Re: Eliminate copy while sending data : any Akka experts here ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

In our clusters, number of containers we can get is high but memory
per container is low : which is why avg_nodes_not_hosting data is
rarely zero for ML tasks :-)

To update - to unblock our current implementation efforts, we went
with broadcast - since it is intutively easier and minimal change; and
compress the array as bytes in TaskResult.
This is then stored in disk backed maps - to remove memory pressure on
master and workers (else MapOutputTracker becomes a memory hog).

But I agree, compressed bitmap to represent 'large' blocks (anything
larger that maxBytesInFlight actually) and probably existing to track
non zero should be fine (we should not really track zero output for
reducer - just waste of space).


Regards,
Mridul

On Fri, Jul 4, 2014 at 3:43 AM, Reynold Xin <rxin@databricks.com> wrote:
> Note that in my original proposal, I was suggesting we could track whether
> block size = 0 using a compressed bitmap. That way we can still avoid
> requests for zero-sized blocks.
>
>
>
> On Thu, Jul 3, 2014 at 3:12 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Yes, that number is likely == 0 in any real workload ...
>>
>>
>> On Thu, Jul 3, 2014 at 8:01 AM, Mridul Muralidharan <mridul@gmail.com>
>> wrote:
>>
>>> On Thu, Jul 3, 2014 at 11:32 AM, Reynold Xin <rxin@databricks.com> wrote:
>>> > On Wed, Jul 2, 2014 at 3:44 AM, Mridul Muralidharan <mridul@gmail.com>
>>> > wrote:
>>> >
>>> >>
>>> >> >
>>> >> > The other thing we do need is the location of blocks. This is
>>> actually
>>> >> just
>>> >> > O(n) because we just need to know where the map was run.
>>> >>
>>> >> For well partitioned data, wont this not involve a lot of unwanted
>>> >> requests to nodes which are not hosting data for a reducer (and lack
>>> >> of ability to throttle).
>>> >>
>>> >
>>> > Was that a question? (I'm guessing it is). What do you mean exactly?
>>>
>>>
>>> I was not sure if I understood the proposal correctly - hence the
>>> query : if I understood it right - the number of wasted requests goes
>>> up by num_reducers * avg_nodes_not_hosting data.
>>>
>>> Ofcourse, if avg_nodes_not_hosting data == 0, then we are fine !
>>>
>>> Regards,
>>> Mridul
>>>
>>
>>

From dev-return-8205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 11:27:29 2014
Return-Path: <dev-return-8205-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87F9011AA9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 11:27:29 +0000 (UTC)
Received: (qmail 29773 invoked by uid 500); 4 Jul 2014 11:27:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29720 invoked by uid 500); 4 Jul 2014 11:27:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29709 invoked by uid 99); 4 Jul 2014 11:27:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 11:27:28 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 11:27:27 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <turdakov@ispras.ru>)
	id 1X31du-0003V7-Gu
	for dev@spark.incubator.apache.org; Fri, 04 Jul 2014 04:27:02 -0700
Date: Fri, 4 Jul 2014 04:27:02 -0700 (PDT)
From: Denis Turdakov <turdakov@ispras.ru>
To: dev@spark.incubator.apache.org
Message-ID: <1404473222501-7179.post@n3.nabble.com>
In-Reply-To: <CA+B-+fwbuTmWNa3y+9w9RZyR3_MJ4jPDBUkfkvr6rbP_z0+29Q@mail.gmail.com>
References: <1404402584790-7170.post@n3.nabble.com> <CA+B-+fwbuTmWNa3y+9w9RZyR3_MJ4jPDBUkfkvr6rbP_z0+29Q@mail.gmail.com>
Subject: Re: PLSA
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, Deb.

I don't quite understand the question. PLSA is an instance of matrix
factorization problem. 

If you are asking about inference algorithm, we use EM-algorithm.
Description of this approach is, for example, here:
http://www.machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf


Best, Denis.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/PLSA-tp7170p7179.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 15:48:46 2014
Return-Path: <dev-return-8206-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 16DD711097
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 15:48:46 +0000 (UTC)
Received: (qmail 45596 invoked by uid 500); 4 Jul 2014 15:48:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45544 invoked by uid 500); 4 Jul 2014 15:48:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45523 invoked by uid 99); 4 Jul 2014 15:48:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 15:48:44 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.182 as permitted sender)
Received: from [209.85.216.182] (HELO mail-qc0-f182.google.com) (209.85.216.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 15:48:40 +0000
Received: by mail-qc0-f182.google.com with SMTP id m20so1603940qcx.41
        for <dev@spark.apache.org>; Fri, 04 Jul 2014 08:48:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=QvpU7MEOrVJfWBh/cP4x8XS1gfbkeqK87h0aXokdSds=;
        b=dZf/EPUEUbMtZHMFCCmI6CG0jmfxLjq3W3hjAKj4jZsgFuQ48FVPw74d95d75B4gGP
         0YsB5JGS/RsKrikdkyrg4jeFwfW1pLCB/r7ks1ebDCE6qGEdUTRfheZeiYTG+cpaDu79
         F0/8emhD90ttEhAoYeePyC66ChkZtDREi1fGGqztxE4vCpCPb5obYy2SFyeqw2VbyOT4
         AC78hj2hCXJHxibwiHt4/2YuXv+r079iA4YwoESwfoYW3RJOQk7/Z/AVTjQVU1D4Q2Ti
         VGELMcsZ4Qvbv3N15++AuEIYoOkJFcZ5MN2NFnTizuad05AmO0jK4Quu3PhZgdEzhxP7
         tChg==
MIME-Version: 1.0
X-Received: by 10.140.16.67 with SMTP id 61mr3616294qga.28.1404488900101; Fri,
 04 Jul 2014 08:48:20 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Fri, 4 Jul 2014 08:48:20 -0700 (PDT)
In-Reply-To: <1404473222501-7179.post@n3.nabble.com>
References: <1404402584790-7170.post@n3.nabble.com>
	<CA+B-+fwbuTmWNa3y+9w9RZyR3_MJ4jPDBUkfkvr6rbP_z0+29Q@mail.gmail.com>
	<1404473222501-7179.post@n3.nabble.com>
Date: Fri, 4 Jul 2014 08:48:20 -0700
Message-ID: <CA+B-+fzykSsxdaJfJQ=izydBHBVOyD4Q6-MLff3dz1oO4HvvGA@mail.gmail.com>
Subject: Re: PLSA
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c068fefb4b8b04fd600d21
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c068fefb4b8b04fd600d21
Content-Type: text/plain; charset=UTF-8

Thanks for the pointer...

Looks like you are using EM algorithm for factorization which looks similar
to multiplicative update rules

Do you think using mllib ALS implicit feedback, you can scale the problem
further ?

We can handle L1, L2, equality and positivity constraints in ALS now...As
long as you can find the gradient and hessian from the KL divergence loss,
you can use that in place of gram matrix that is used in ALS right now

If you look in topic modeling work in Solr (Carrot is the package), they
use ALS to generate the topics...that algorithm looks like a simplified
version of what you are attempting here...

May be the EM algorithm for topic modeling is efficient than ALS but from
looking at it I don't see how...I see lot of broadcasts...while in implicit
feedback you need one broadcast of gram matrix...

On Fri, Jul 4, 2014 at 4:27 AM, Denis Turdakov <turdakov@ispras.ru> wrote:

> Hi, Deb.
>
> I don't quite understand the question. PLSA is an instance of matrix
> factorization problem.
>
> If you are asking about inference algorithm, we use EM-algorithm.
> Description of this approach is, for example, here:
> http://www.machinelearning.ru/wiki/images/1/1f/Voron14aist.pdf
>
>
> Best, Denis.
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/PLSA-tp7170p7179.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c068fefb4b8b04fd600d21--

From dev-return-8207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 17:19:35 2014
Return-Path: <dev-return-8207-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 088BB112AA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 17:19:35 +0000 (UTC)
Received: (qmail 96272 invoked by uid 500); 4 Jul 2014 17:19:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96219 invoked by uid 500); 4 Jul 2014 17:19:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96173 invoked by uid 99); 4 Jul 2014 17:19:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 17:19:34 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 17:19:31 +0000
Received: by mail-qg0-f42.google.com with SMTP id e89so1677328qgf.15
        for <dev@spark.apache.org>; Fri, 04 Jul 2014 10:19:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=80gtCuKmZiFaDls9nRgQnDB+pX/bnVm7e5NMG2ipoKk=;
        b=s7ZqNYqtCj+UcLJmryVvGCuGKTckcZ3F0pVs9tD/3xdZ1O1e1OJZQMdICSSDOjcLzb
         zWNOzMHAaxuut7j6RvnH+Ixxx+bh5SqT9sCfmdxVSND4F1qMK3wvqtf9FE/+NnW6ijDp
         GZactQL2EX/AslxMPeTGHplr1UfaP2z53kLhkDP6OpZzQa0Var4PB956m5yFS6IbHG+G
         3jIF1wxB04J/4kWTVntuyde4TDvYY/1/uvUDHW5bWRiTyuE7bSlnaYGT5CDnqUX00kv3
         Uo6FE5yLRJu02pd97eKq6+Gx9olkCrwirW6614/XdtMQRj6YTnFPrFFDGE5E+BlAc1PI
         5YSw==
MIME-Version: 1.0
X-Received: by 10.140.29.66 with SMTP id a60mr19904356qga.76.1404494346781;
 Fri, 04 Jul 2014 10:19:06 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Fri, 4 Jul 2014 10:19:06 -0700 (PDT)
In-Reply-To: <CA+B-+fz1DMVZz=vOU3GteUVUBNuFmo-O2gtcVm1-T-_L3u9Tow@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
	<CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
	<CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
	<CAJgQjQ-wvbaE+QJ2Mngzpy=g5i7=4ym59JVrAMtijwv4uTbfPw@mail.gmail.com>
	<CA+B-+fxYLfEWx_+jzdfNO3DFa5dc4VXfEYpffpOT0xCV=yV4Ug@mail.gmail.com>
	<CAJgQjQ8KQRswOkedse2SXByU6r-XCgVZK2ftda-JKnjSu_r+QA@mail.gmail.com>
	<CA+B-+fz1DMVZz=vOU3GteUVUBNuFmo-O2gtcVm1-T-_L3u9Tow@mail.gmail.com>
Date: Fri, 4 Jul 2014 10:19:06 -0700
Message-ID: <CA+B-+fzod5i6dsJWaf7oz6758DY6i+Q0ui0J2TBOYwW4zQs+cw@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113ac052a10a3004fd615219
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ac052a10a3004fd615219
Content-Type: text/plain; charset=UTF-8

I looked further and realized that ECOS used a mex file while PDCO is using
pure Matlab code. So the out-of-box runtime comparison is not fair.

I am trying to generate PDCO C port. Like ECOS, PDCO also makes use of
sparse support from Tim Davis.

Thanks.
Deb

--001a113ac052a10a3004fd615219--

From dev-return-8208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 18:15:48 2014
Return-Path: <dev-return-8208-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EF9C01137C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 18:15:48 +0000 (UTC)
Received: (qmail 59701 invoked by uid 500); 4 Jul 2014 18:15:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59631 invoked by uid 500); 4 Jul 2014 18:15:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59620 invoked by uid 99); 4 Jul 2014 18:15:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 18:15:48 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [49.212.34.109] (HELO oss.nttdata.co.jp) (49.212.34.109)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 18:15:41 +0000
Received: from [10.2.2.202] (192.169.20.154.static.etheric.net [192.169.20.154])
	by oss.nttdata.co.jp (Postfix) with ESMTP id 9478917EE07
	for <dev@spark.apache.org>; Sat,  5 Jul 2014 03:14:58 +0900 (JST)
From: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
Content-Type: multipart/alternative; boundary="Apple-Mail=_857A6885-DA66-4F06-9204-167C62A5288C"
Subject: Invalid link for Spark 1.0.0 in Official Web Site
Message-Id: <EC8B47A8-886A-40EB-BB2F-AF7C8F299E91@oss.nttdata.co.jp>
Date: Fri, 4 Jul 2014 11:14:27 -0700
To: dev@spark.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Scanned: clamav-milter 0.98.3 at oss.nttdata.co.jp
X-Virus-Status: Clean
X-Spam-Checker-Version: SpamAssassin 3.2.5 (2008-06-10) on oss.nttdata.co.jp
X-Virus-Checked: Checked by ClamAV on apache.org
X-Old-Spam-Status: No, score=-97.2 required=13.0 tests=CONTENT_TYPE_PRESENT,
	HTML_MESSAGE,MIMEQENC,MULTIPART_ALTERNATIVE,ONLY1HOPDIRECT,RDNS_NONE,
	TOOLONGSTR,USER_IN_WHITELIST autolearn=no version=3.2.5

--Apple-Mail=_857A6885-DA66-4F06-9204-167C62A5288C
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Hi,

I found there is a invalid link in =
<http://spark.apache.org/downloads.html> .
The link for release note of Spark 1.0.0 indicates =
http://spark.apache.org/releases/spark-release-1.0.0.html but this link =
is invalid.
I think that is mistake for =
<http://spark.apache.org/releases/spark-release-1-0-0.html>.

Thanks,
Kousuke



--Apple-Mail=_857A6885-DA66-4F06-9204-167C62A5288C--

From dev-return-8209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 19:39:41 2014
Return-Path: <dev-return-8209-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D87CE114DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 19:39:41 +0000 (UTC)
Received: (qmail 43957 invoked by uid 500); 4 Jul 2014 19:39:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43903 invoked by uid 500); 4 Jul 2014 19:39:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43891 invoked by uid 99); 4 Jul 2014 19:39:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 19:39:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 19:39:39 +0000
Received: by mail-ob0-f178.google.com with SMTP id wn1so2188771obc.37
        for <dev@spark.apache.org>; Fri, 04 Jul 2014 12:39:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=PK7i9mDZ2/t5Y7+B9p19eX/BiKzZBxgaoaYj17pEoAI=;
        b=Ozb9LoowB5WEDda7JtDyJ0oU0n5bvWWf36I6ObDeeZ8wUVnTkHl4T+ELHjjICw78P8
         1JX5u1eRA04Sr0q5eDXkHLtpS/7T5h4aGHnh2MhK/thWu5NyNGmTrYr4auo7LBKlxuyY
         lr70uTGvHYUyywOAXyoQFtCOB8RSHeJ0X6boNFfCmQ6iD5avWHIkBSoylkZW6ykQXkKx
         YFKi67Wp6i3q9b/KM32NMMD5EwfliuFAUUd0+jY8fSb4iNGtu3n3sLrrNJAxKYr4XNs6
         UIIt2pgrHDt11bEw0Y/FGor7ka0cj8y1Bf4id9DPFa15qVGvwaN+dfcYg64qgi4+jY8W
         xMmg==
MIME-Version: 1.0
X-Received: by 10.182.232.135 with SMTP id to7mr14576956obc.73.1404502754471;
 Fri, 04 Jul 2014 12:39:14 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 4 Jul 2014 12:39:14 -0700 (PDT)
Date: Fri, 4 Jul 2014 12:39:14 -0700
Message-ID: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.1!

The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1021/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/

Please vote on releasing this package as Apache Spark 1.0.1!

The vote is open until Monday, July 07, at 20:45 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

=== Differences from RC1 ===
This release includes only one "blocking" patch from rc1:
https://github.com/apache/spark/pull/1255

There are also smaller fixes which came in over the last week.

=== About this release ===
This release fixes a few high-priority bugs in 1.0 and has a variety
of smaller fixes. The full list is here: http://s.apache.org/b45. Some
of the more visible patches are:

SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
SPARK-1790: Support r3 instance types on EC2.

This is the first maintenance release on the 1.0 line. We plan to make
additional maintenance releases as new fixes come in.

From dev-return-8210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 19:40:21 2014
Return-Path: <dev-return-8210-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20FE0114DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 19:40:21 +0000 (UTC)
Received: (qmail 45167 invoked by uid 500); 4 Jul 2014 19:40:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45104 invoked by uid 500); 4 Jul 2014 19:40:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45046 invoked by uid 99); 4 Jul 2014 19:40:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 19:40:20 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 19:40:18 +0000
Received: by mail-oa0-f52.google.com with SMTP id j17so2120497oag.25
        for <dev@spark.apache.org>; Fri, 04 Jul 2014 12:39:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Nopz/4jA7YJ/SaZyg8fceea9UhI6l7rCexEqqHQZyRE=;
        b=E3601MHMSty8VjqJi+8/69PzlpJ3m73Mtd8/CpsZAE/oLD9pLsWWIs8RKfEyQDRPTY
         caSx7Ey2L9dpxQG9V0FIvjXuA04KxViubpw08wiUYoJEc6ZXYf2kcdTEsJ0T+sGFEDcd
         zcPRLteQtr33+9zmYHubQK4tlDndUaZ+CVAuOl4zJyOKNEVRLuwHovaLZP1u2YyNKdf7
         9F/EUskwkQiabI+2F9OnlbYwH8VFUGFwuRqrEU5BNXoKFkvPjwz88V1usYrt/vif9udz
         eSaInp9jiH3VVsRanRhmKBKSIwFDmBd5FKBLr2etlr6IfYNsM/UAlm2RgsMnS9lIh+/B
         AY4g==
MIME-Version: 1.0
X-Received: by 10.60.155.167 with SMTP id vx7mr14670652oeb.50.1404502793621;
 Fri, 04 Jul 2014 12:39:53 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 4 Jul 2014 12:39:53 -0700 (PDT)
Date: Fri, 4 Jul 2014 12:39:53 -0700
Message-ID: <CABPQxssKSsS4z0ObHuu4Ho7G4zu96MRcwxKtq5pZO1N6dsRJRA@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.0.1 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote is cancelled in favor of RC2. Thanks to everyone who voted.

On Sun, Jun 29, 2014 at 11:23 PM, Andrew Ash <andrew@andrewash.com> wrote:
> Ok that's reasonable -- it's certainly more of an enhancement than a
> critical bug-fix.  I would like to get this in for 1.1.0 though, so let's
> talk through the right way to do that on the PR.
>
> In the meantime the best alternative is running with lax firewall settings,
> which can be somewhat mitigated by modifying the ephemeral port range.
>
> Thanks!
> Andrew
>
>
> On Sun, Jun 29, 2014 at 11:14 PM, Reynold Xin <rxin@databricks.com> wrote:
>
>> Hi Andrew,
>>
>> The port stuff is great to have, but they are pretty big changes to the
>> core that are introducing new features and are not exactly fixing important
>> bugs. For this reason, it probably can't block a release (I'm not even sure
>> if it should go into a maintenance release where we fix critical bugs for
>> Spark core).
>>
>> We should definitely include them for 1.1.0 though (~Aug).
>>
>>
>>
>>
>> On Sun, Jun 29, 2014 at 11:09 PM, Andrew Ash <andrew@andrewash.com> wrote:
>>
>> > Thanks for helping shepherd the voting on 1.0.1 Patrick.
>> >
>> > I'd like to call attention to
>> > https://issues.apache.org/jira/browse/SPARK-2157 and
>> > https://github.com/apache/spark/pull/1107 -- "Ability to write tight
>> > firewall rules for Spark"
>> >
>> > I'm currently unable to run Spark on some projects because our cloud ops
>> > team is uncomfortable with the firewall situation around Spark at the
>> > moment.  Currently Spark starts listening on random ephemeral ports and
>> > does server to server communication on them.  This keeps the team from
>> > writing tight firewall rules between the services -- they get real queasy
>> > when asked to open inbound connections to the entire ephemeral port range
>> > of a cluster.  We can tighten the size of the ephemeral range using
>> kernel
>> > settings to mitigate the issue, but it doesn't actually solve the
>> problem.
>> >
>> > The PR above aims to make every listening port on JVMs in a Spark
>> > standalone cluster configurable with an option.  If not set, the current
>> > behavior stands (start listening on an ephemeral port).  Is this
>> something
>> > the Spark team would consider merging into 1.0.1?
>> >
>> > Thanks!
>> > Andrew
>> >
>> >
>> >
>> > On Sun, Jun 29, 2014 at 10:54 PM, Patrick Wendell <pwendell@gmail.com>
>> > wrote:
>> >
>> > > Hey All,
>> > >
>> > > We're going to move onto another rc because of this vote.
>> > > Unfortunately with the summit activities I haven't been able to usher
>> > > in the necessary patches and cut the RC. I will do so as soon as
>> > > possible and we can commence official voting.
>> > >
>> > > - Patrick
>> > >
>> > > On Sun, Jun 29, 2014 at 4:56 PM, Reynold Xin <rxin@databricks.com>
>> > wrote:
>> > > > We should make sure we include the following two patches:
>> > > >
>> > > > https://github.com/apache/spark/pull/1264
>> > > >
>> > > > https://github.com/apache/spark/pull/1263
>> > > >
>> > > >
>> > > >
>> > > >
>> > > > On Fri, Jun 27, 2014 at 8:39 PM, Krishna Sankar <ksankar42@gmail.com
>> >
>> > > wrote:
>> > > >
>> > > >> +1
>> > > >> Compiled for CentOS 6.5, deployed in our 4 node cluster (Hadoop 2.2,
>> > > YARN)
>> > > >> Smoke Tests (sparkPi,spark-shell, web UI) successful
>> > > >>
>> > > >> Cheers
>> > > >> <k/>
>> > > >>
>> > > >>
>> > > >> On Thu, Jun 26, 2014 at 7:06 PM, Patrick Wendell <
>> pwendell@gmail.com>
>> > > >> wrote:
>> > > >>
>> > > >> > Please vote on releasing the following candidate as Apache Spark
>> > > version
>> > > >> > 1.0.1!
>> > > >> >
>> > > >> > The tag to be voted on is v1.0.1-rc1 (commit 7feeda3):
>> > > >> >
>> > > >> >
>> > > >>
>> > >
>> >
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7feeda3d729f9397aa15ee8750c01ef5aa601962
>> > > >> >
>> > > >> > The release files, including signatures, digests, etc. can be
>> found
>> > > at:
>> > > >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1/
>> > > >> >
>> > > >> > Release artifacts are signed with the following key:
>> > > >> > https://people.apache.org/keys/committer/pwendell.asc
>> > > >> >
>> > > >> > The staging repository for this release can be found at:
>> > > >> >
>> > >
>> https://repository.apache.org/content/repositories/orgapachespark-1020/
>> > > >> >
>> > > >> > The documentation corresponding to this release can be found at:
>> > > >> > http://people.apache.org/~pwendell/spark-1.0.1-rc1-docs/
>> > > >> >
>> > > >> > Please vote on releasing this package as Apache Spark 1.0.1!
>> > > >> >
>> > > >> > The vote is open until Monday, June 30, at 03:00 UTC and passes if
>> > > >> > a majority of at least 3 +1 PMC votes are cast.
>> > > >> >
>> > > >> > [ ] +1 Release this package as Apache Spark 1.0.1
>> > > >> > [ ] -1 Do not release this package because ...
>> > > >> >
>> > > >> > To learn more about Apache Spark, please see
>> > > >> > http://spark.apache.org/
>> > > >> >
>> > > >> > === About this release ===
>> > > >> > This release fixes a few high-priority bugs in 1.0 and has a
>> variety
>> > > >> > of smaller fixes. The full list is here: http://s.apache.org/b45.
>> > > Some
>> > > >> > of the more visible patches are:
>> > > >> >
>> > > >> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching
>> keys
>> > > >> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka
>> > frame
>> > > >> size.
>> > > >> > SPARK-1790: Support r3 instance types on EC2.
>> > > >> >
>> > > >> > This is the first maintenance release on the 1.0 line. We plan to
>> > make
>> > > >> > additional maintenance releases as new fixes come in.
>> > > >> >
>> > > >> > - Patrick
>> > > >> >
>> > > >>
>> > >
>> >
>>

From dev-return-8211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 19:41:04 2014
Return-Path: <dev-return-8211-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C203C114E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 19:41:04 +0000 (UTC)
Received: (qmail 46515 invoked by uid 500); 4 Jul 2014 19:41:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46475 invoked by uid 500); 4 Jul 2014 19:41:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46462 invoked by uid 99); 4 Jul 2014 19:41:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 19:41:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 19:40:59 +0000
Received: by mail-oa0-f50.google.com with SMTP id n16so2183537oag.9
        for <dev@spark.apache.org>; Fri, 04 Jul 2014 12:40:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=wZdDtcjIjbsUt1zlR3VGp2jHqWjhvVeyBVHS0HjxFUo=;
        b=x5Wy8o8pfZSQDUm8X9UCJwdyT7KXdYlZHWq4bgGtxRj2pPgS8ca3Q+DqsIvSIley2P
         0QIm6m2/992pN+DQQbhRoJ8eEQvCD2b17V2I2zpUmcpQBEaES9JxfP6gf4NkqNwCDnSn
         qhHivroj5WfgeDJJgCmb7p5gCgSCN+fvN8eBSpkYWd4JtfyIuQKEbfuslGgWHEeH6Rhp
         39xBfafUELPlcnSLOZ+a4BytBYwzZ82tY88fRpng3qXm1m1mOp8PBz5WEV60SiUcaCiV
         gTEsvFUsZLekdLV3w0MIyzCecUqzbsCkYlG2gmHM9fTYUpoKBhtC4czo76tAQUQJWrLd
         50IQ==
MIME-Version: 1.0
X-Received: by 10.182.112.134 with SMTP id iq6mr14315286obb.34.1404502838783;
 Fri, 04 Jul 2014 12:40:38 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 4 Jul 2014 12:40:38 -0700 (PDT)
In-Reply-To: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
Date: Fri, 4 Jul 2014 12:40:38 -0700
Message-ID: <CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'll start the voting with a +1 - ran tests on the release candidate
and ran some basic programs. RC1 passed our performance regression
suite, and there are no major changes from that RC.

On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.1!
>
> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1021/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.1!
>
> The vote is open until Monday, July 07, at 20:45 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.1
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> === Differences from RC1 ===
> This release includes only one "blocking" patch from rc1:
> https://github.com/apache/spark/pull/1255
>
> There are also smaller fixes which came in over the last week.
>
> === About this release ===
> This release fixes a few high-priority bugs in 1.0 and has a variety
> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> of the more visible patches are:
>
> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
> SPARK-1790: Support r3 instance types on EC2.
>
> This is the first maintenance release on the 1.0 line. We plan to make
> additional maintenance releases as new fixes come in.

From dev-return-8212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul  4 21:03:06 2014
Return-Path: <dev-return-8212-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C628611625
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  4 Jul 2014 21:03:06 +0000 (UTC)
Received: (qmail 28431 invoked by uid 500); 4 Jul 2014 21:03:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28119 invoked by uid 500); 4 Jul 2014 21:03:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27914 invoked by uid 99); 4 Jul 2014 21:03:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 21:03:04 +0000
X-ASF-Spam-Status: No, hits=0.2 required=5.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,SUSPICIOUS_RECIPS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chris.a.mattmann@jpl.nasa.gov designates 128.149.139.109 as permitted sender)
Received: from [128.149.139.109] (HELO mail.jpl.nasa.gov) (128.149.139.109)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 04 Jul 2014 21:03:00 +0000
Received: from mail.jpl.nasa.gov (ap-ehub-sp01.jpl.nasa.gov [128.149.137.148])
	by smtp.jpl.nasa.gov (Sentrion-MTA-4.3.1/Sentrion-MTA-4.3.1) with ESMTP id s64L2dUB022285
	(using TLSv1/SSLv3 with cipher AES128-SHA (128 bits) verified NO);
	Fri, 4 Jul 2014 14:02:39 -0700
Received: from AP-EMBX-SP40.RES.AD.JPL ([169.254.7.217]) by
 ap-ehub-sp01.RES.AD.JPL ([169.254.3.182]) with mapi id 14.03.0174.001; Fri, 4
 Jul 2014 14:02:39 -0700
From: "Mattmann, Chris A (3980)" <chris.a.mattmann@jpl.nasa.gov>
To: "dev@oodt.apache.org" <dev@oodt.apache.org>,
        "dev@gora.apache.org"
	<dev@gora.apache.org>,
        "dev@tika.apache.org" <dev@tika.apache.org>
CC: "dev@ctakes.apache.org" <dev@ctakes.apache.org>,
        "dev@airavata.apache.org"
	<dev@airavata.apache.org>,
        "dev@tajo.apache.org" <dev@tajo.apache.org>,
        "dev@spark.apache.org" <dev@spark.apache.org>
Subject: 2nd Workshop on Sustainable Software for Science: Practice and
 Experiences (WSSSPE2)
Thread-Topic: 2nd Workshop on Sustainable Software for Science: Practice and
 Experiences (WSSSPE2)
Thread-Index: AQHPl8tJgmZJI1URF0qB/0w9UXjWTg==
Date: Fri, 4 Jul 2014 21:02:38 +0000
Message-ID: <CFDC646D.167066%chris.a.mattmann@jpl.nasa.gov>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
user-agent: Microsoft-MacOutlook/14.4.3.140616
x-originating-ip: [128.149.137.113]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <CA12BF51E462214BBC17A01612D40584@ad.jpl>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Source-Sender: chris.a.mattmann@jpl.nasa.gov
X-AUTH: Authorized
X-Virus-Checked: Checked by ClamAV on apache.org

(apologies for Cross Posting)

2nd Workshop on Sustainable Software for Science: Practice and Experiences
(WSSSPE2)
http://wssspe.researchcomputing.org.uk/wssspe2/
(to be held in conjunction with SC14, Sunday, 16 November 2014, New
Orleans, LA, USA)

Progress in scientific research is dependent on the quality and
accessibility of software at all levels and it is critical to address
challenges related to the development, deployment, and maintenance of
reusable software as well as education around software practices. These
challenges can be technological, policy based, organizational, and
educational, and are of interest to developers (the software community),
users (science disciplines), and researchers studying the conduct of
science (science of team science, science of organizations, science of
science and innovation policy, and social science communities).

The WSSSPE1 workshop (http://wssspe.researchcomputing.org.uk/WSSSPE1)
engaged the broad scientific community to identify challenges and best
practices in areas of interest for sustainable scientific software. At
WSSSPE2, we invite the community to propose and discuss specific
mechanisms to move towards an imagined future practice of software
development and usage in science and engineering. The workshop will
include multiple mechanisms for participation, encourage team building
around solutions, and identify risky solutions with potentially
transformative outcomes. Participation by early career students and
postdoctoral researchers is strongly encouraged.

We invite short (4-page) actionable papers that will lead to improvements
for sustainable software science. These papers could be a call to action,
or could provide position or experience reports on sustainable software
activities. The papers will be used by the organizing committee to design
sessions that will be highly interactive and targeted towards facilitating
action. Submitted papers should be archived by a third-party service that
provides DOIs. We encourage submitters to license their papers under a
Creative Commons license that encourages sharing and remixing, as we will
combine ideas (with attribution) into the outcomes of the workshop.

The organizers will invite one or more submitters of provocative papers to
start the workshop by presenting highlights of their papers in a keynote
presentation to initiate active discussion that will continue throughout
the day.

Areas of interest for WSSSPE2, include, but are not limited to:

=3D80 defining software sustainability in the context of science and
engineering software
=3D80 how to evaluate software sustainability
=3D80 improving the development process that leads to new software
=3D80 methods to develop sustainable software from the outset
=3D80 effective approaches to reusable software created as a by-product of
research
=3D80 impact of computer science research on the development of scientific
software
=3D80 recommendations for the support and maintenance of existing software
=3D80 software engineering best practices
=3D80 governance, business, and sustainability models
=3D80 the role of community software repositories, their operation and
sustainability
=3D80 reproducibility, transparency needs that may be unique to science
=3D80 successful open source software implementations
=3D80 incentives for using and contributing to open source software
=3D80 transitioning users into contributing developers
=3D80 building large and engaged user communities
=3D80 developing strong advocates
=3D80 measurement of usage and impact
=3D80 encouraging industry=3DB9s role in sustainability
=3D80 engagement of industry with volunteer communities
=3D80 incentives for industry
=3D80 incentives for community to contribute to industry-driven projects
=3D80 recommending policy changes
=3D80 software credit, attribution, incentive, and reward
=3D80 issues related to multiple organizations and multiple countries, such
a=3D
s
intellectual property, licensing, etc.
=3D80 mechanisms and venues for publishing software, and the role of
publishe=3D
rs
=3D80 improving education and training
=3D80 best practices for providing graduate students and postdoctoral
researchers in domain communities with sufficient training in software
development
=3D80 novel uses of sustainable software in education (K-20)
=3D80 case studies from students on issues around software development in t=
he
undergraduate or graduate curricula
=3D80 careers and profession
=3D80 successful examples of career paths for developers
=3D80 institutional changes to support sustainable software such as promoti=
on
and tenure metrics, job categories, etc.

Submissions:

Submissions of up to four pages should be formatted to be easily readable
and submitted to an open access repository that provides unique
identifiers (e.g., DOIs) that can be cited, for example http://arXiv.org
<http://arxiv.org/>
or http://figshare.com <http://figshare.com/>.

Once you have received an identifier for your self-published paper from a
repository, submit it to WSSSPE2 by creating a new submission at
https://www.easychair.org/conferences/?conf=3D3Dwssspe2, and entering:

=3D80 author information for all authors
=3D80 title
=3D80 abstract (with the identifier as the first line of the abstract, for
example, http://dx.doi.org/10.6084/m9.figshare.791606 or
http://arxiv.org/abs/1404.7414 or alternative)
=3D80 at least three keywords
=3D80 tick the abstract only box
Do not submit the paper itself through EasyChair; the identifier in the
abstract that points to the paper is sufficient.

Deadline for Submission:

14 July 2014 (any time of day, no extensions)

Travel Support

Funds are available to support participation in WSSSPE2 by 1) US-based
students, early-career researchers, and members of underrepresented
groups; and 2) participants who would not otherwise attend the SC14
conference. Priority will be given to those who have submitted papers and
can make a compelling case for how their participation will strengthen the
overall workshop and/or positively impact their future research or
educational activities.

Submissions for travel support will be accepted from September 1st to
September 15th 2014 following instructions posted on the workshop web site.

Financial support to enable this has been generously provided by 1) the
National Science Foundation and 2) the Gordon and Betty Moore Foundation.

Important Dates:

July 14, 2014 Paper submission deadline
September 1, 2014 Author notification
September 15, 2014  Funding request submission deadline
September 22, 2014  Funding decision notification
November 16, 2014 WSSSPE2 Workshop

Organizers:

=3D80 Daniel S. Katz, d.katz@ieee.org, National Science Foundation, USA
=3D80 Gabrielle Allen, gdallen@illinois.edu, University of Illinois
Urbana-Champaign, USA
=3D80 Neil Chue Hong, N.ChueHong@software.ac.uk, Software Sustainability
Institute, University of Edinburgh, UK
=3D80 Karen Cranston, karen.cranston@nescent.org, National Evolutionary
Synthesis Center (NESCent), USA
=3D80 Manish Parashar, parashar@rutgers.edu, Rutgers University, USA
=3D80 David Proctor, djproctor@gmail.com, National Science Foundation, USA
=3D80 Matthew Turk, matthewturk@gmail.com, Columbia University, USA
=3D80 Colin C. Venters, colin.venters@googlemail.com, University of
Huddersfield, UK
=3D80 Nancy Wilkins-Diehr, wilkinsn@sdsc.edu, San Diego Supercomputer Cente=
r,
University of California, San Diego, USA

Program Committee:

=3D80 Aron Ahmadia, U.S. Army Engineer Research and Development Center, USA
=3D80 Liz Allen, Wellcome Trust, UK
=3D80 Lorena A. Barba, The George Washington University, USA
=3D80 C. Titus Brown, Michigan State University, USA
=3D80 Coral Calero, Universidad Castilla La Mancha, Spain
=3D80 Jeffrey Carver, University of Alabama, USA
=3D80 Ewa Deelman, University of Southern California, USA
=3D80 Gabriel A. Devenyi, McMaster University, Canada
=3D80 Charlie E. Dibsdale, O-Sys, Rolls Royce PLC, UK
=3D80 Alberto Di Meglio, CERN, Switzerland
=3D80 Anshu Dubey, Lawrence Berkeley National Laboratory, USA
=3D80 David Gavaghan, University of Oxford, UK
=3D80 Paul Ginsparg, Cornell University, USA
=3D80 Josh Greenberg, Alfred P. Sloan Foundation, USA
=3D80 Sarah Harris, University of Leeds, UK
=3D80 James Herbsleb, Carnegie Mellon University, USA
=3D80 James Howison, University of Texas at Austin, USA
=3D80 Caroline Jay, University of Manchester, UK
=3D80 Matthew B. Jones, National Center for Ecological Analysis and Synthes=
is
(NCEAS), University of California, Santa Barbara, USA
=3D80 Jong-Suk Ruth Lee, National Institute of Supercomputing and Networkin=
g,
KISTI (Korea Institute of Science and Technology Information), Korea
=3D80 James Lin, Shanghai Jiao Tong University, China
=3D80 Frank L=3DF6ffler, Louisiana State University, USA
=3D80 Chris A. Mattmann, NASA JPL & University of Southern California, USA
=3D80 Robert H. McDonald, Indiana University, USA
=3D80 Lois Curfman McInnes, Argonne National Laboratory, USA
=3D80 Chris Mentzel, Gordon and Betty Moore Foundation, USA
=3D80 Kenneth M. Merz, Jr., Michigan State University, USA
=3D80 Marek T. Michalewicz, A*STAR Computational Resource Centre, Singapore
=3D80 Peter E. Murray, LYRASIS, USA
=3D80 Kenjo Nakajima, University of Tokyo, Japan
=3D80 Cameron Neylon, PLOS, UK
=3D80 Aleksandra Pawlik, Software Sustainability Institute, Manchester
University, UK
=3D80 Birgit Penzenstadler, University of California, Irvine, USA
=3D80 Marian Petre, The Open University, UK
=3D80 Mark D. Plumbley, Queen Mary University of London, UK
=3D80 Andreas Prlic, University of California, San Diego, USA
=3D80 Victoria Stodden, Columbia University, USA
=3D80 Kaitlin Thaney, Mozilla Science Lab, USA
=3D80 Greg Watson, IBM, USA
=3D80 Theresa Windus, Iowa State University and Ames Laboratory, USA





++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 168-519, Mailstop: 168-527
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++





From dev-return-8213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul  5 16:31:10 2014
Return-Path: <dev-return-8213-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8BFFC11472
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Jul 2014 16:31:10 +0000 (UTC)
Received: (qmail 15022 invoked by uid 500); 5 Jul 2014 16:31:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14966 invoked by uid 500); 5 Jul 2014 16:31:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14955 invoked by uid 99); 5 Jul 2014 16:31:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Jul 2014 16:31:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Jul 2014 16:31:05 +0000
Received: by mail-we0-f170.google.com with SMTP id w61so2696914wes.29
        for <dev@spark.apache.org>; Sat, 05 Jul 2014 09:30:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=7aXfwKYiCsFn4+u7FmNvZJECNakOow6R+Q4/Lr5t5FY=;
        b=Oz4WhL7g82M1nyHo3yFyhLx4ykRDUtpGNBRPKipG6QtGA9MvAmMNn+TaQnUVzAAaDb
         ykFhR+L6fVedCS+khWq69V8knLquz7QJ58GXxuD/S8UaycoS9DXjpMsKQUl09D6OlBv8
         sW6nPjGTVcthPKpG/9PMPLIotM6vP/ZeYRq0A15LJ47p/JcSSUGVT9iWQvPR139ZPF56
         8sBO19FmIMmhbkd9StFnEHkfnwF/QpyRbrOw/ERAz8Ppcj2WKKghlhAT+st/zo3/G1O0
         6Offq5V2xWy1vTSdAZ+mf/4ZYUpSMIeupaeEC0fVQYhUgOyFfjQS4FrarR4CqEJlcsYl
         7/Og==
X-Gm-Message-State: ALoCoQnBIh2iQi5P0IUJJ1ODJVaRLsZV8RzSwqpOyjfWYji2tOXrj/FvHYFcIxGCYnzRf9h+lf4O
MIME-Version: 1.0
X-Received: by 10.180.189.210 with SMTP id gk18mr64880795wic.66.1404577843278;
 Sat, 05 Jul 2014 09:30:43 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Sat, 5 Jul 2014 09:30:43 -0700 (PDT)
In-Reply-To: <CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
Date: Sat, 5 Jul 2014 09:30:43 -0700
Message-ID: <CAAsvFPmYju6-WKVHZmj7UQn_us5Zkjr__F9K5sNxz+hTChs4tg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c34d7468926a04fd74c35f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c34d7468926a04fd74c35f
Content-Type: text/plain; charset=UTF-8

+1


On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> I'll start the voting with a +1 - ran tests on the release candidate
> and ran some basic programs. RC1 passed our performance regression
> suite, and there are no major changes from that RC.
>
> On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Please vote on releasing the following candidate as Apache Spark version
> 1.0.1!
> >
> > The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc2/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1021/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.1!
> >
> > The vote is open until Monday, July 07, at 20:45 UTC and passes if
> > a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.1
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > === Differences from RC1 ===
> > This release includes only one "blocking" patch from rc1:
> > https://github.com/apache/spark/pull/1255
> >
> > There are also smaller fixes which came in over the last week.
> >
> > === About this release ===
> > This release fixes a few high-priority bugs in 1.0 and has a variety
> > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> > of the more visible patches are:
> >
> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> size.
> > SPARK-1790: Support r3 instance types on EC2.
> >
> > This is the first maintenance release on the 1.0 line. We plan to make
> > additional maintenance releases as new fixes come in.
>

--001a11c34d7468926a04fd74c35f--

From dev-return-8214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul  5 20:39:00 2014
Return-Path: <dev-return-8214-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C39C117C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Jul 2014 20:39:00 +0000 (UTC)
Received: (qmail 2806 invoked by uid 500); 5 Jul 2014 20:38:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2735 invoked by uid 500); 5 Jul 2014 20:38:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2724 invoked by uid 99); 5 Jul 2014 20:38:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Jul 2014 20:38:59 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Jul 2014 20:38:55 +0000
Received: by mail-qa0-f41.google.com with SMTP id cm18so2354097qab.28
        for <dev@spark.apache.org>; Sat, 05 Jul 2014 13:38:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=1Z1v2NW2WrbniL7EGt9UlKI+uS6NzyPoC8ocw6xZHO0=;
        b=bNiB/ViatoiUbSBSN8O+dWOfyyUhnUShYt4dZUYxwHsRkrqAezYot3nkqNS9ZrXmIr
         9CDqFo32vzLwZJyxVrZ1sohygBE99rZzi1ZfGHq8E4TmzG3EU1bT2J+F9c4L9cRcKxpl
         zHK46rHm1myVbR/1P6WaXWU7U4QR/J9OP0XpRbIDTVliOUj2S9TMkCrj9aXP++8pGa3A
         e1M6DUkUoDs5bfEod93hoyaELHVA1Zjj/xt7CvkMg+/6ypBlNONKh7b3FASCkYgvITFY
         ALjQmzCbhmLfY5NM+zJAn8dl2rcELBAAfL4e8w7pnoyFfoqsxx3OTphMwcqCOc89JlLU
         vvqA==
X-Gm-Message-State: ALoCoQlMF0IWuieliQQF20Wy7N3Hwqpz/CHzGN/pdgwwKK+cSEMHQTu+cXD+xL7SyQl/ASE6omdz
X-Received: by 10.224.112.131 with SMTP id w3mr33371864qap.68.1404592714000;
 Sat, 05 Jul 2014 13:38:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.3 with HTTP; Sat, 5 Jul 2014 13:38:13 -0700 (PDT)
In-Reply-To: <CAAsvFPmYju6-WKVHZmj7UQn_us5Zkjr__F9K5sNxz+hTChs4tg@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
 <CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com> <CAAsvFPmYju6-WKVHZmj7UQn_us5Zkjr__F9K5sNxz+hTChs4tg@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Sat, 5 Jul 2014 13:38:13 -0700
Message-ID: <CAAswR-4Yk1W9Hnbx0w+2mYNi2FBjDhg22szo6AUa5r7cHe6PjQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2580cc5c54b04fd783926
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2580cc5c54b04fd783926
Content-Type: text/plain; charset=UTF-8

+1

I tested sql/hive functionality.


On Sat, Jul 5, 2014 at 9:30 AM, Mark Hamstra <mark@clearstorydata.com>
wrote:

> +1
>
>
> On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > I'll start the voting with a +1 - ran tests on the release candidate
> > and ran some basic programs. RC1 passed our performance regression
> > suite, and there are no major changes from that RC.
> >
> > On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > > Please vote on releasing the following candidate as Apache Spark
> version
> > 1.0.1!
> > >
> > > The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
> > >
> > > The release files, including signatures, digests, etc. can be found at:
> > > http://people.apache.org/~pwendell/spark-1.0.1-rc2/
> > >
> > > Release artifacts are signed with the following key:
> > > https://people.apache.org/keys/committer/pwendell.asc
> > >
> > > The staging repository for this release can be found at:
> > >
> https://repository.apache.org/content/repositories/orgapachespark-1021/
> > >
> > > The documentation corresponding to this release can be found at:
> > > http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
> > >
> > > Please vote on releasing this package as Apache Spark 1.0.1!
> > >
> > > The vote is open until Monday, July 07, at 20:45 UTC and passes if
> > > a majority of at least 3 +1 PMC votes are cast.
> > >
> > > [ ] +1 Release this package as Apache Spark 1.0.1
> > > [ ] -1 Do not release this package because ...
> > >
> > > To learn more about Apache Spark, please see
> > > http://spark.apache.org/
> > >
> > > === Differences from RC1 ===
> > > This release includes only one "blocking" patch from rc1:
> > > https://github.com/apache/spark/pull/1255
> > >
> > > There are also smaller fixes which came in over the last week.
> > >
> > > === About this release ===
> > > This release fixes a few high-priority bugs in 1.0 and has a variety
> > > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> > > of the more visible patches are:
> > >
> > > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> > size.
> > > SPARK-1790: Support r3 instance types on EC2.
> > >
> > > This is the first maintenance release on the 1.0 line. We plan to make
> > > additional maintenance releases as new fixes come in.
> >
>

--001a11c2580cc5c54b04fd783926--

From dev-return-8215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul  5 21:02:46 2014
Return-Path: <dev-return-8215-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 449AA117F8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  5 Jul 2014 21:02:46 +0000 (UTC)
Received: (qmail 19017 invoked by uid 500); 5 Jul 2014 21:02:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18952 invoked by uid 500); 5 Jul 2014 21:02:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18941 invoked by uid 99); 5 Jul 2014 21:02:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Jul 2014 21:02:45 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 05 Jul 2014 21:02:41 +0000
Received: by mail-qg0-f41.google.com with SMTP id i50so2501391qgf.28
        for <dev@spark.apache.org>; Sat, 05 Jul 2014 14:02:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=zygYk5Du20rEpKyGSXNTWnuOOwNpL9wJPpfp1G193kA=;
        b=cfDlyY/WyXiXjJwoAudMpwLzXW6xPXMdU4aMpoPWjWekR63w3vL1/RAuTmyFqSVnlV
         9Vn6ty65Zq+DfHE91UGJttvWLBQ7FtEeEIFV0MROAbaW03jyBDGGi3r/wGnf+TyWPops
         wGA4sl16C5nC84Buhh+Vk/LYMhnSkahxChsUxouLREs/gIqNGJYiMSKJJRfH82HJ9P6h
         EuQmDx4bFM5XTdgcgQytBQ187v5WZjwLjvNgqGB3Tn2fnqao1kDZH5X4VJK7vyO0EbiI
         2rYV9C4Xk+B7x/I/h4wJ+mAfe+Yw4RIvqWaxeBOsuJjPU6UywqpfI9ZJjyv9Qms4IwU4
         grRA==
X-Gm-Message-State: ALoCoQnSzhztzjP3tJFkmKZIRTSaarCY2Wv7D3AnIbh6jxRktw2wy10vR+E9C0fIYrIt/br+a8Ux
MIME-Version: 1.0
X-Received: by 10.229.127.199 with SMTP id h7mr30965036qcs.21.1404594140187;
 Sat, 05 Jul 2014 14:02:20 -0700 (PDT)
Received: by 10.229.160.19 with HTTP; Sat, 5 Jul 2014 14:02:20 -0700 (PDT)
Received: by 10.229.160.19 with HTTP; Sat, 5 Jul 2014 14:02:20 -0700 (PDT)
In-Reply-To: <CAAswR-4Yk1W9Hnbx0w+2mYNi2FBjDhg22szo6AUa5r7cHe6PjQ@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
	<CAAsvFPmYju6-WKVHZmj7UQn_us5Zkjr__F9K5sNxz+hTChs4tg@mail.gmail.com>
	<CAAswR-4Yk1W9Hnbx0w+2mYNi2FBjDhg22szo6AUa5r7cHe6PjQ@mail.gmail.com>
Date: Sat, 5 Jul 2014 14:02:20 -0700
Message-ID: <CAEYYnxabQSJcbuNitqCaFehyVB+HL3_M8cuy_o36+XZ6Zd2j4Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1132f770c7c2f104fd788e12
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132f770c7c2f104fd788e12
Content-Type: text/plain; charset=UTF-8

+1
On Jul 5, 2014 1:39 PM, "Michael Armbrust" <michael@databricks.com> wrote:

> +1
>
> I tested sql/hive functionality.
>
>
> On Sat, Jul 5, 2014 at 9:30 AM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
>
> > +1
> >
> >
> > On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> > > I'll start the voting with a +1 - ran tests on the release candidate
> > > and ran some basic programs. RC1 passed our performance regression
> > > suite, and there are no major changes from that RC.
> > >
> > > On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com>
> > > wrote:
> > > > Please vote on releasing the following candidate as Apache Spark
> > version
> > > 1.0.1!
> > > >
> > > > The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> > > >
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
> > > >
> > > > The release files, including signatures, digests, etc. can be found
> at:
> > > > http://people.apache.org/~pwendell/spark-1.0.1-rc2/
> > > >
> > > > Release artifacts are signed with the following key:
> > > > https://people.apache.org/keys/committer/pwendell.asc
> > > >
> > > > The staging repository for this release can be found at:
> > > >
> > https://repository.apache.org/content/repositories/orgapachespark-1021/
> > > >
> > > > The documentation corresponding to this release can be found at:
> > > > http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
> > > >
> > > > Please vote on releasing this package as Apache Spark 1.0.1!
> > > >
> > > > The vote is open until Monday, July 07, at 20:45 UTC and passes if
> > > > a majority of at least 3 +1 PMC votes are cast.
> > > >
> > > > [ ] +1 Release this package as Apache Spark 1.0.1
> > > > [ ] -1 Do not release this package because ...
> > > >
> > > > To learn more about Apache Spark, please see
> > > > http://spark.apache.org/
> > > >
> > > > === Differences from RC1 ===
> > > > This release includes only one "blocking" patch from rc1:
> > > > https://github.com/apache/spark/pull/1255
> > > >
> > > > There are also smaller fixes which came in over the last week.
> > > >
> > > > === About this release ===
> > > > This release fixes a few high-priority bugs in 1.0 and has a variety
> > > > of smaller fixes. The full list is here: http://s.apache.org/b45.
> Some
> > > > of the more visible patches are:
> > > >
> > > > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > > > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> > > size.
> > > > SPARK-1790: Support r3 instance types on EC2.
> > > >
> > > > This is the first maintenance release on the 1.0 line. We plan to
> make
> > > > additional maintenance releases as new fixes come in.
> > >
> >
>

--001a1132f770c7c2f104fd788e12--

From dev-return-8216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul  6 02:41:50 2014
Return-Path: <dev-return-8216-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8A82511AE6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Jul 2014 02:41:50 +0000 (UTC)
Received: (qmail 535 invoked by uid 500); 6 Jul 2014 02:41:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 423 invoked by uid 500); 6 Jul 2014 02:41:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 410 invoked by uid 99); 6 Jul 2014 02:41:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 02:41:49 +0000
X-ASF-Spam-Status: No, hits=2.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ksankar42@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 02:41:46 +0000
Received: by mail-pd0-f172.google.com with SMTP id w10so3570104pde.17
        for <dev@spark.apache.org>; Sat, 05 Jul 2014 19:41:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ag8Tv3dSfk5Gn6rUZi5DGz5AOmgMwjZBtihbfYR9D8g=;
        b=rbRv4+zMmQiTYYFD/i6lHnxBXGqC3Fft5TTaNKGCSlaWlHVLv6MSjYtbFEib/w+qgV
         kbhM6Adu1uiyK/mt7vC+PnGunsyI4m8fwmoIBExH1hrg46JnzyDWTBxfSu5VIE3Si1dp
         +FFsMbKv7o9HxxmsgAfBl1eN9tNDNFo0a/T7DWlszXpdwmp2uQPPByQOUMO1/syZ+Ju1
         7TsytE7ZSiEQEqt44YPhdQ140neKRZRnZ1Kppw9MnWRqoHVT3/6ze+gbdWSJQL+KNHzF
         46rfoaw2AMbf0A6EMmTonoVhCuscZLFSmQHtQVt1Dbthk4I8XdOi6v7kls5aoipi1nQT
         95lQ==
MIME-Version: 1.0
X-Received: by 10.68.186.130 with SMTP id fk2mr20077227pbc.60.1404614481578;
 Sat, 05 Jul 2014 19:41:21 -0700 (PDT)
Received: by 10.70.40.174 with HTTP; Sat, 5 Jul 2014 19:41:21 -0700 (PDT)
In-Reply-To: <CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
Date: Sat, 5 Jul 2014 19:41:21 -0700
Message-ID: <CAOTBr2=O6Qjw_XFch8Xmy2hhDw65HSC_vAMbbZq99S4C=1pqRg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Krishna Sankar <ksankar42@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd6b6ee38911f04fd7d4b00
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd6b6ee38911f04fd7d4b00
Content-Type: text/plain; charset=UTF-8

+1

   - Compiled rc2 w/ CentOS 6.5, Yarn,Hadoop 2.2.0 - successful
   - Smoke Test (scala,python) (distributed cluster) - successful
   - We had ran Java/SparkSQL (count, distinct et al) ~250M records RDD
   over HBase 0.98.3 over last build (rc1) - successful
   - Stand alone multi-node cluster is working better for us than Yarn

Cheers
<k/>


On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> I'll start the voting with a +1 - ran tests on the release candidate
> and ran some basic programs. RC1 passed our performance regression
> suite, and there are no major changes from that RC.
>
> On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Please vote on releasing the following candidate as Apache Spark version
> 1.0.1!
> >
> > The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
> >
> > The release files, including signatures, digests, etc. can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc2/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/pwendell.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1021/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.1!
> >
> > The vote is open until Monday, July 07, at 20:45 UTC and passes if
> > a majority of at least 3 +1 PMC votes are cast.
> >
> > [ ] +1 Release this package as Apache Spark 1.0.1
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
> > === Differences from RC1 ===
> > This release includes only one "blocking" patch from rc1:
> > https://github.com/apache/spark/pull/1255
> >
> > There are also smaller fixes which came in over the last week.
> >
> > === About this release ===
> > This release fixes a few high-priority bugs in 1.0 and has a variety
> > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> > of the more visible patches are:
> >
> > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> size.
> > SPARK-1790: Support r3 instance types on EC2.
> >
> > This is the first maintenance release on the 1.0 line. We plan to make
> > additional maintenance releases as new fixes come in.
>

--047d7bd6b6ee38911f04fd7d4b00--

From dev-return-8217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul  6 03:02:05 2014
Return-Path: <dev-return-8217-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48E2811B34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Jul 2014 03:02:05 +0000 (UTC)
Received: (qmail 13594 invoked by uid 500); 6 Jul 2014 03:02:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13524 invoked by uid 500); 6 Jul 2014 03:02:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13473 invoked by uid 99); 6 Jul 2014 03:02:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 03:02:03 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 03:02:01 +0000
Received: by mail-ob0-f180.google.com with SMTP id vb8so3180330obc.11
        for <dev@spark.apache.org>; Sat, 05 Jul 2014 20:01:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=PSZ2hD/JnAtjNMmvyP/DuJxTEPyG4PGkgg+4A4+8Ddw=;
        b=N1cXehd4Zb3+KHwfIr0JKasA9vx6ZkhpQpV2xSBz+cpHoBQD70yoIiUbdMg+AI7i4V
         FtK0WRcUiPeUlDLLh/PF1C2e5lgIs7xMX/EZXQBymyrbVtiOhJs7O+qd3Qp+ULypXyBk
         6FWuL2Q/7DYWW8hHipKKhvI6IQ4qJ5tMu92kKSZBc1EhfHMgavjK5/pkSQcJ1nGLBBAR
         52FBy5xjN6FlEUcMXd9dPOo09zR0DmmSBEPt7Vnj20BuBwxB8FXBlCvyrjlZqX5aWUwS
         zE54hhdepu6IrGn465Aq2bsDHDslguO9egJRUknYEOHMGq9bykTAvNmKaykcMOznjsrs
         7qaA==
X-Gm-Message-State: ALoCoQk+ROti4XWBfvEMrB8PmaEt/FIh0TMta+tgFvtV+Y6BpCdlwiMBdYbI0DhWc4fgPqk6dRen
MIME-Version: 1.0
X-Received: by 10.60.73.129 with SMTP id l1mr22581051oev.2.1404615694342; Sat,
 05 Jul 2014 20:01:34 -0700 (PDT)
Received: by 10.182.194.75 with HTTP; Sat, 5 Jul 2014 20:01:34 -0700 (PDT)
In-Reply-To: <CAOTBr2=O6Qjw_XFch8Xmy2hhDw65HSC_vAMbbZq99S4C=1pqRg@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
	<CAOTBr2=O6Qjw_XFch8Xmy2hhDw65HSC_vAMbbZq99S4C=1pqRg@mail.gmail.com>
Date: Sat, 5 Jul 2014 20:01:34 -0700
Message-ID: <CAJ3iqPQ7qSGJisFzoqwNtWJs9yB1LrZcbuYpX-+YBzG9FRy7Kg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Soren Macbeth <soren@yieldbot.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1136001081f12704fd7d930b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136001081f12704fd7d930b
Content-Type: text/plain; charset=UTF-8

+1


On Sat, Jul 5, 2014 at 7:41 PM, Krishna Sankar <ksankar42@gmail.com> wrote:

> +1
>
>    - Compiled rc2 w/ CentOS 6.5, Yarn,Hadoop 2.2.0 - successful
>    - Smoke Test (scala,python) (distributed cluster) - successful
>    - We had ran Java/SparkSQL (count, distinct et al) ~250M records RDD
>    over HBase 0.98.3 over last build (rc1) - successful
>    - Stand alone multi-node cluster is working better for us than Yarn
>
> Cheers
> <k/>
>
>
> On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > I'll start the voting with a +1 - ran tests on the release candidate
> > and ran some basic programs. RC1 passed our performance regression
> > suite, and there are no major changes from that RC.
> >
> > On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> > > Please vote on releasing the following candidate as Apache Spark
> version
> > 1.0.1!
> > >
> > > The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
> > >
> > > The release files, including signatures, digests, etc. can be found at:
> > > http://people.apache.org/~pwendell/spark-1.0.1-rc2/
> > >
> > > Release artifacts are signed with the following key:
> > > https://people.apache.org/keys/committer/pwendell.asc
> > >
> > > The staging repository for this release can be found at:
> > >
> https://repository.apache.org/content/repositories/orgapachespark-1021/
> > >
> > > The documentation corresponding to this release can be found at:
> > > http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
> > >
> > > Please vote on releasing this package as Apache Spark 1.0.1!
> > >
> > > The vote is open until Monday, July 07, at 20:45 UTC and passes if
> > > a majority of at least 3 +1 PMC votes are cast.
> > >
> > > [ ] +1 Release this package as Apache Spark 1.0.1
> > > [ ] -1 Do not release this package because ...
> > >
> > > To learn more about Apache Spark, please see
> > > http://spark.apache.org/
> > >
> > > === Differences from RC1 ===
> > > This release includes only one "blocking" patch from rc1:
> > > https://github.com/apache/spark/pull/1255
> > >
> > > There are also smaller fixes which came in over the last week.
> > >
> > > === About this release ===
> > > This release fixes a few high-priority bugs in 1.0 and has a variety
> > > of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> > > of the more visible patches are:
> > >
> > > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> > size.
> > > SPARK-1790: Support r3 instance types on EC2.
> > >
> > > This is the first maintenance release on the 1.0 line. We plan to make
> > > additional maintenance releases as new fixes come in.
> >
>

--001a1136001081f12704fd7d930b--

From dev-return-8218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul  6 05:54:54 2014
Return-Path: <dev-return-8218-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9FEBD11C7C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Jul 2014 05:54:53 +0000 (UTC)
Received: (qmail 95899 invoked by uid 500); 6 Jul 2014 05:54:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95860 invoked by uid 500); 6 Jul 2014 05:54:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95849 invoked by uid 99); 6 Jul 2014 05:54:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 05:54:50 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.171] (HELO mail-pd0-f171.google.com) (209.85.192.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 05:54:48 +0000
Received: by mail-pd0-f171.google.com with SMTP id fp1so3695961pdb.30
        for <dev@spark.apache.org>; Sat, 05 Jul 2014 22:54:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=wwnXOpNmqFLGSEXsQu/qJR2L51Y5ILTykiq2KPaVw28=;
        b=CozWixqzGBctWQXlhvnWEqvxesgf/ufzY18QbZ74J99/MsSNXqBW49TZ/mQJp7j5oQ
         fpkWO6zsTSYn5WFnCbrOsoFq5VgDEwb5V7n6hXOP79SCtfQMvzYwY1HvNo086Tzxlw+n
         YYbCPSO++w1G3uc9FrpkxKA4j1CLs2BjW4+eLOjopxwx60TB2VZ0m0njRdC/LD5gRh0J
         zdaduOJqGR3lZMWvaN8Ioya7mhzMwFEkbx7jezDWPnmFR93u1Yw8sAqo1QKgnKKrCGTr
         iltQIYIuUlIy7I19EZ40cytJQUiYOjBI7cjBSY3tOIfsLvI4YJc11CzcaOPkHSQ47lMb
         XIEg==
X-Gm-Message-State: ALoCoQmNqu4Gd5ap4GZmr8ZGLkuvouPH7muwraYwGTkUP7bgLoUFBjYS3DfYYxUyMOM1qOJ+NFuB
MIME-Version: 1.0
X-Received: by 10.66.184.79 with SMTP id es15mr17513780pac.57.1404626063062;
 Sat, 05 Jul 2014 22:54:23 -0700 (PDT)
Received: by 10.70.53.8 with HTTP; Sat, 5 Jul 2014 22:54:22 -0700 (PDT)
In-Reply-To: <CAJ3iqPQ7qSGJisFzoqwNtWJs9yB1LrZcbuYpX-+YBzG9FRy7Kg@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
	<CAOTBr2=O6Qjw_XFch8Xmy2hhDw65HSC_vAMbbZq99S4C=1pqRg@mail.gmail.com>
	<CAJ3iqPQ7qSGJisFzoqwNtWJs9yB1LrZcbuYpX-+YBzG9FRy7Kg@mail.gmail.com>
Date: Sat, 5 Jul 2014 22:54:22 -0700
Message-ID: <CAMJOb8k1Xa0O_eVagWeB25f1OEfBoh0T=nSHYQgOAgV_QjFaOA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf0c984880d0b04fd7ffd3d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0c984880d0b04fd7ffd3d
Content-Type: text/plain; charset=UTF-8

+1, verified that the UI bug is in fact fixed in
https://github.com/apache/spark/pull/1255.


2014-07-05 20:01 GMT-07:00 Soren Macbeth <soren@yieldbot.com>:

> +1
>
>
> On Sat, Jul 5, 2014 at 7:41 PM, Krishna Sankar <ksankar42@gmail.com>
> wrote:
>
> > +1
> >
> >    - Compiled rc2 w/ CentOS 6.5, Yarn,Hadoop 2.2.0 - successful
> >    - Smoke Test (scala,python) (distributed cluster) - successful
> >    - We had ran Java/SparkSQL (count, distinct et al) ~250M records RDD
> >    over HBase 0.98.3 over last build (rc1) - successful
> >    - Stand alone multi-node cluster is working better for us than Yarn
> >
> > Cheers
> > <k/>
> >
> >
> > On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> > > I'll start the voting with a +1 - ran tests on the release candidate
> > > and ran some basic programs. RC1 passed our performance regression
> > > suite, and there are no major changes from that RC.
> > >
> > > On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com>
> > > wrote:
> > > > Please vote on releasing the following candidate as Apache Spark
> > version
> > > 1.0.1!
> > > >
> > > > The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> > > >
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
> > > >
> > > > The release files, including signatures, digests, etc. can be found
> at:
> > > > http://people.apache.org/~pwendell/spark-1.0.1-rc2/
> > > >
> > > > Release artifacts are signed with the following key:
> > > > https://people.apache.org/keys/committer/pwendell.asc
> > > >
> > > > The staging repository for this release can be found at:
> > > >
> > https://repository.apache.org/content/repositories/orgapachespark-1021/
> > > >
> > > > The documentation corresponding to this release can be found at:
> > > > http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
> > > >
> > > > Please vote on releasing this package as Apache Spark 1.0.1!
> > > >
> > > > The vote is open until Monday, July 07, at 20:45 UTC and passes if
> > > > a majority of at least 3 +1 PMC votes are cast.
> > > >
> > > > [ ] +1 Release this package as Apache Spark 1.0.1
> > > > [ ] -1 Do not release this package because ...
> > > >
> > > > To learn more about Apache Spark, please see
> > > > http://spark.apache.org/
> > > >
> > > > === Differences from RC1 ===
> > > > This release includes only one "blocking" patch from rc1:
> > > > https://github.com/apache/spark/pull/1255
> > > >
> > > > There are also smaller fixes which came in over the last week.
> > > >
> > > > === About this release ===
> > > > This release fixes a few high-priority bugs in 1.0 and has a variety
> > > > of smaller fixes. The full list is here: http://s.apache.org/b45.
> Some
> > > > of the more visible patches are:
> > > >
> > > > SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> > > > SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> > > size.
> > > > SPARK-1790: Support r3 instance types on EC2.
> > > >
> > > > This is the first maintenance release on the 1.0 line. We plan to
> make
> > > > additional maintenance releases as new fixes come in.
> > >
> >
>

--047d7bf0c984880d0b04fd7ffd3d--

From dev-return-8219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul  6 20:22:07 2014
Return-Path: <dev-return-8219-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 09F51116C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  6 Jul 2014 20:22:07 +0000 (UTC)
Received: (qmail 79854 invoked by uid 500); 6 Jul 2014 20:22:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79788 invoked by uid 500); 6 Jul 2014 20:22:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79772 invoked by uid 99); 6 Jul 2014 20:22:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 20:22:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.223.170 as permitted sender)
Received: from [209.85.223.170] (HELO mail-ie0-f170.google.com) (209.85.223.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 06 Jul 2014 20:22:01 +0000
Received: by mail-ie0-f170.google.com with SMTP id lx4so2028845iec.15
        for <dev@spark.apache.org>; Sun, 06 Jul 2014 13:21:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=c0+YI/oqIioXjWGhAZ59uBDKVOhNLJBYr7JiAIsPEZ0=;
        b=a5ClvPg6ESiawy4M4P0HegieLNtOrEhPWx+RualedsmWTJURXaGEd/lcT9huhu8Tr/
         gaPEvVdSe5NuED+pA9ejog3f/tdVPL9+JpDh88QpfJcUBk29wXTBlH5AyxbZ2cUQC8QH
         P72wORGaSKD7bg/42fiWNqZHKnpjkv1pOBp/eMTnh6UVKsFrRFzhea6YV5sW3oWhTIce
         QHh4a94IF0QVwoYOWU29KUDFydKHhwWVst2xxE3uoShH7dwp+/jqq55uv44vRO5j/RfE
         mS6GcxxrVKPIQRgdmwBSh2lUURD4HYJckdCb+7katidKnZoXFLrvtxSszE2OguO1qE2E
         VYSQ==
X-Received: by 10.42.93.84 with SMTP id w20mr28926568icm.49.1404678101014;
        Sun, 06 Jul 2014 13:21:41 -0700 (PDT)
Received: from [192.168.1.109] (CPE68b6fc3fbad3-CM68b6fc3fbad0.cpe.net.cable.rogers.com. [99.226.46.122])
        by mx.google.com with ESMTPSA id hj18sm20530238igb.1.2014.07.06.13.21.38
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 06 Jul 2014 13:21:38 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMJOb8k1Xa0O_eVagWeB25f1OEfBoh0T=nSHYQgOAgV_QjFaOA@mail.gmail.com>
Date: Sun, 6 Jul 2014 16:21:37 -0400
Content-Transfer-Encoding: quoted-printable
Message-Id: <515BE677-F7B2-4CFF-8289-57E3B0B8E262@gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com> <CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com> <CAOTBr2=O6Qjw_XFch8Xmy2hhDw65HSC_vAMbbZq99S4C=1pqRg@mail.gmail.com> <CAJ3iqPQ7qSGJisFzoqwNtWJs9yB1LrZcbuYpX-+YBzG9FRy7Kg@mail.gmail.com> <CAMJOb8k1Xa0O_eVagWeB25f1OEfBoh0T=nSHYQgOAgV_QjFaOA@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.2)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested on Mac OS X.

Matei

On Jul 6, 2014, at 1:54 AM, Andrew Or <andrew@databricks.com> wrote:

> +1, verified that the UI bug is in fact fixed in
> https://github.com/apache/spark/pull/1255.
>=20
>=20
> 2014-07-05 20:01 GMT-07:00 Soren Macbeth <soren@yieldbot.com>:
>=20
>> +1
>>=20
>>=20
>> On Sat, Jul 5, 2014 at 7:41 PM, Krishna Sankar <ksankar42@gmail.com>
>> wrote:
>>=20
>>> +1
>>>=20
>>>   - Compiled rc2 w/ CentOS 6.5, Yarn,Hadoop 2.2.0 - successful
>>>   - Smoke Test (scala,python) (distributed cluster) - successful
>>>   - We had ran Java/SparkSQL (count, distinct et al) ~250M records =
RDD
>>>   over HBase 0.98.3 over last build (rc1) - successful
>>>   - Stand alone multi-node cluster is working better for us than =
Yarn
>>>=20
>>> Cheers
>>> <k/>
>>>=20
>>>=20
>>> On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell =
<pwendell@gmail.com>
>>> wrote:
>>>=20
>>>> I'll start the voting with a +1 - ran tests on the release =
candidate
>>>> and ran some basic programs. RC1 passed our performance regression
>>>> suite, and there are no major changes from that RC.
>>>>=20
>>>> On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell =
<pwendell@gmail.com>
>>>> wrote:
>>>>> Please vote on releasing the following candidate as Apache Spark
>>> version
>>>> 1.0.1!
>>>>>=20
>>>>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>>>>>=20
>>>>=20
>>>=20
>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D7d104=
3c99303b87aef8ee19873629c2bfba4cc78
>>>>>=20
>>>>> The release files, including signatures, digests, etc. can be =
found
>> at:
>>>>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>>>>>=20
>>>>> Release artifacts are signed with the following key:
>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>=20
>>>>> The staging repository for this release can be found at:
>>>>>=20
>>> =
https://repository.apache.org/content/repositories/orgapachespark-1021/
>>>>>=20
>>>>> The documentation corresponding to this release can be found at:
>>>>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>>>>>=20
>>>>> Please vote on releasing this package as Apache Spark 1.0.1!
>>>>>=20
>>>>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>=20
>>>>> [ ] +1 Release this package as Apache Spark 1.0.1
>>>>> [ ] -1 Do not release this package because ...
>>>>>=20
>>>>> To learn more about Apache Spark, please see
>>>>> http://spark.apache.org/
>>>>>=20
>>>>> =3D=3D=3D Differences from RC1 =3D=3D=3D
>>>>> This release includes only one "blocking" patch from rc1:
>>>>> https://github.com/apache/spark/pull/1255
>>>>>=20
>>>>> There are also smaller fixes which came in over the last week.
>>>>>=20
>>>>> =3D=3D=3D About this release =3D=3D=3D
>>>>> This release fixes a few high-priority bugs in 1.0 and has a =
variety
>>>>> of smaller fixes. The full list is here: http://s.apache.org/b45.
>> Some
>>>>> of the more visible patches are:
>>>>>=20
>>>>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching =
keys
>>>>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka =
frame
>>>> size.
>>>>> SPARK-1790: Support r3 instance types on EC2.
>>>>>=20
>>>>> This is the first maintenance release on the 1.0 line. We plan to
>> make
>>>>> additional maintenance releases as new fixes come in.
>>>>=20
>>>=20
>>=20


From dev-return-8220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul  7 15:34:32 2014
Return-Path: <dev-return-8220-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D69011E91
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Jul 2014 15:34:32 +0000 (UTC)
Received: (qmail 62308 invoked by uid 500); 7 Jul 2014 15:34:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62255 invoked by uid 500); 7 Jul 2014 15:34:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62239 invoked by uid 99); 7 Jul 2014 15:34:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 15:34:30 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 15:34:27 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <turdakov@ispras.ru>)
	id 1X4Ava-0000z3-Ge
	for dev@spark.incubator.apache.org; Mon, 07 Jul 2014 08:34:02 -0700
Date: Mon, 7 Jul 2014 08:34:02 -0700 (PDT)
From: Denis Turdakov <turdakov@ispras.ru>
To: dev@spark.incubator.apache.org
Message-ID: <1404747242509-7194.post@n3.nabble.com>
In-Reply-To: <CA+B-+fzykSsxdaJfJQ=izydBHBVOyD4Q6-MLff3dz1oO4HvvGA@mail.gmail.com>
References: <1404402584790-7170.post@n3.nabble.com> <CA+B-+fwbuTmWNa3y+9w9RZyR3_MJ4jPDBUkfkvr6rbP_z0+29Q@mail.gmail.com> <1404473222501-7179.post@n3.nabble.com> <CA+B-+fzykSsxdaJfJQ=izydBHBVOyD4Q6-MLff3dz1oO4HvvGA@mail.gmail.com>
Subject: Re: PLSA
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi, Deb.

Thanks for your idea to use ALS for PLSA training. I discussed it with our
engineers and it seems it's better to use EM. We have the following points:

1. We have some doubts that ALS is applicable to the problem. By its
definition, PLSA is a matrix decomposition with respect to Kullback=E2=80=
=93Leibler
divergence. Unlike matrix decomposition with respect to Frobenius' distance=
,
this problem lacks convexity. Actually, PLSA has multiple solutions and
EM-algorithm is experimentally proven to obtain "good" local maxima. By the
way, Kullback=E2=80=93Leibler divergence is not symmetric, does not satisfy=
 to
triangle inequality and the objects it's defined on do not form the linear
space -- would not it be a problem for ALS? For instance, does not ALS rely
on the fact that L_2 is defined on the object that form self-dual space? =
=20

2. Using EM-algorithm is a common way for PLSA training described in most
known papers. "Contributing to Spark" page says, "a new algorithm" "should
be used and accepted" and "widely known". We have found no publications
describing PLSA training via ALS. So, we are not sure if it will provide
results of comparable quality to EM-algorithm. That could be an issue for
research.

3. PLSA objective function is non-differentiable in some points (e.g.
\phi_{wt} =3D 0 \forall t), EM-algorithm is theoretically proven to avoid s=
uch
points. We are afraid, ALS is likely to fall into this trap.

4. We also suppose that EM is faster for this problem. PLSA has D*T + W*T
parameters (where D stand for the number of documents, T - for the number o=
f
topics, W - for the size of the alphabet). Thus, Hessian matrix has size
(D*T + W*T)^2 -- that's a lot. Note, EM-algorithm has O(D*T*W) complexity
for every iteration. Probably, it's possible to use a diagonalized Hessian,
but it will increase the number of iterations needed for convergence and we
think EM-algorithm will outperform it due to the fact that we have a very
simple analytical solution for E-step. (There was a story in 80's about
EM-algorithms and second-order methods. Second-order methods were believed
to outperform EM-algorithm unless someone has proven that it's not necessar=
y
to conduct precise optimization during E-step -- it's enough to increase th=
e
objective function. This idea allowed to speed up E-step and EM-algorithm
became superior to second-order methods. Once again, we have a very simple
analytical solution for E-step -- it's very fast).

5. As far as we can understand, RALS handles only quadratic regularizers=20
(maybe it's possible to substitute a quadratic approximation at every
iteration, but we've no idea why it must work). In PSLA we want to allow
user to define every regularizer she wants.

6. There are only a few broadcasts in our implementation. Only \phi matrix
is  broadcasted (yes, we have to call broadcast(...) method in three
different places, but we broadcast only once in iteration).=20



--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/PLSA-tp7170p7194.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

From dev-return-8221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul  7 16:24:25 2014
Return-Path: <dev-return-8221-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1CED51103A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Jul 2014 16:24:25 +0000 (UTC)
Received: (qmail 92862 invoked by uid 500); 7 Jul 2014 16:24:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92815 invoked by uid 500); 7 Jul 2014 16:24:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92627 invoked by uid 99); 7 Jul 2014 16:24:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 16:24:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.172 as permitted sender)
Received: from [74.125.82.172] (HELO mail-we0-f172.google.com) (74.125.82.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 16:24:18 +0000
Received: by mail-we0-f172.google.com with SMTP id u57so4687903wes.17
        for <dev@spark.apache.org>; Mon, 07 Jul 2014 09:23:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=DJQZUX8z7t9Mvs86vtVw+DEJzATlWc92M3P7yQATbEM=;
        b=ev9FfINOfhNwHycIZlw8jJYoL8Kk0fUw5UuAiKcgcuf2vcqBjBckZKblTem1ExIOHc
         qjGJM9x/VjfV1K+qXZ5KheCuyAd3pnWGIykYdwt4WBhFEv9xUmLh4+jiWKpHdQkH2dZj
         P8kBLXyoNCt0JrxuLz+Gcj9208lUGkaMfGOUvX2Z+7DnAQEYyXgZFXWEVnVRpRGuextq
         yo/GdagmLEXynWO4izR9Wsf4p6bxsmeWYnX6CQAB1ATfNBhOtGo9gfN4CjN1ZGqZ2SPA
         yLMcvC6WXVNkOYdobnJWJrmiqBMXo/DhNUt2ChUhFRRUCXHy2qKuDHNTD9oDG8BPJqBU
         hu+Q==
MIME-Version: 1.0
X-Received: by 10.194.6.134 with SMTP id b6mr34444217wja.64.1404750237108;
 Mon, 07 Jul 2014 09:23:57 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Mon, 7 Jul 2014 09:23:56 -0700 (PDT)
In-Reply-To: <CA+B-+fzod5i6dsJWaf7oz6758DY6i+Q0ui0J2TBOYwW4zQs+cw@mail.gmail.com>
References: <CA+B-+fw8EkK1ECg=GPACiJUCMRS6C4vVmQC9dwAfpxvcoPDZtA@mail.gmail.com>
	<CAJgQjQ9y_B1LATjt6c1-PmmytBzf2R1XyFVhMayVK2Hay5stMQ@mail.gmail.com>
	<CA+B-+fxE2rMOf_uURn7OtF_HXpjgZFSRZWD4QS=ZDyyvT-HTAQ@mail.gmail.com>
	<CAJgQjQ9UWjURVD2U0uTUUc2DHH5-_VXQ=uH8mFYZvjLgCLSr9w@mail.gmail.com>
	<CA+B-+fx64WwiSK_ymuvEXMnw2sd+F5fCL3-mQiJbexqyJgzbww@mail.gmail.com>
	<CA+B-+fxircx6nrzHnhoMjDRzGFnY+R7xvjwU49GjkfT39vVs8A@mail.gmail.com>
	<CA+B-+fyO10rcr5bXUaxQOodotmy7njwJujzukzj-ZU8zT-oAuA@mail.gmail.com>
	<CAJgQjQ-b+me5O_ZRqty_OniGFLPai0AFTu5mnZrhGYG9s-ekbg@mail.gmail.com>
	<CA+B-+fyx_aACX5igiY+ibmk6y1OOCY0QmoRO7qpo2SY-+FqYcw@mail.gmail.com>
	<CAJgQjQ-wvbaE+QJ2Mngzpy=g5i7=4ym59JVrAMtijwv4uTbfPw@mail.gmail.com>
	<CA+B-+fxYLfEWx_+jzdfNO3DFa5dc4VXfEYpffpOT0xCV=yV4Ug@mail.gmail.com>
	<CAJgQjQ8KQRswOkedse2SXByU6r-XCgVZK2ftda-JKnjSu_r+QA@mail.gmail.com>
	<CA+B-+fz1DMVZz=vOU3GteUVUBNuFmo-O2gtcVm1-T-_L3u9Tow@mail.gmail.com>
	<CA+B-+fzod5i6dsJWaf7oz6758DY6i+Q0ui0J2TBOYwW4zQs+cw@mail.gmail.com>
Date: Mon, 7 Jul 2014 09:23:56 -0700
Message-ID: <CAJgQjQ__bt67q7NTKh=vr8tTvbSkw1VXt_kB6RtpRWtt+i_-zQ@mail.gmail.com>
Subject: Re: Constraint Solver for Spark
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Deb,

If your goal is to solve the subproblems in ALS, exploring sparsity
doesn't give you much benefit because the data is small and dense.
Porting either ECOS's or PDCO's implementation but using dense
representation should be sufficient. Feel free to open a JIRA and we
can move our discussion there.

Best,
Xiangrui

On Fri, Jul 4, 2014 at 10:19 AM, Debasish Das <debasish.das83@gmail.com> wrote:
> I looked further and realized that ECOS used a mex file while PDCO is using
> pure Matlab code. So the out-of-box runtime comparison is not fair.
>
> I am trying to generate PDCO C port. Like ECOS, PDCO also makes use of
> sparse support from Tim Davis.
>
> Thanks.
> Deb

From dev-return-8222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul  7 16:32:45 2014
Return-Path: <dev-return-8222-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 188E61108E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Jul 2014 16:32:45 +0000 (UTC)
Received: (qmail 19498 invoked by uid 500); 7 Jul 2014 16:32:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19439 invoked by uid 500); 7 Jul 2014 16:32:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19427 invoked by uid 99); 7 Jul 2014 16:32:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 16:32:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.175 as permitted sender)
Received: from [74.125.82.175] (HELO mail-we0-f175.google.com) (74.125.82.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 16:32:41 +0000
Received: by mail-we0-f175.google.com with SMTP id k48so4635148wev.6
        for <dev@spark.apache.org>; Mon, 07 Jul 2014 09:32:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/tzc7BDrBqPy/E/AfojnRxWdLokz4az9l0IgrEBAxnw=;
        b=ocgBhoFJVJzj0V/xvN9oW4EfIDCE6FB/8SwjlXxzdcLeoBSpz/fS9jK3AVylTcyK5M
         zx05djN3wkP5owBGxdqUb0EAbjitOEcskwpNetrr9+0OWQf+3RkpC9QkXG/zJyIL8G+B
         LiyO8e240JEMWLvUcOrmSAbMYhOeYLxMsuF+kQdEE+gDlAR+qRidEvVx4wXAosRrhfyV
         rb6v8TlQpeiySCEHbQ5k+yli6OYlNXaUSEyoK5kvs/ffcPhr2g95aJpm08NFAZ1xsQjW
         NH/29O+K1DfBgYiGdObG9vw6w7Z0wgIVLIEvJJ8YldbV2K7rKuirB8x7fu4ugs0g+csB
         0TuA==
MIME-Version: 1.0
X-Received: by 10.180.74.11 with SMTP id p11mr75902991wiv.68.1404750736764;
 Mon, 07 Jul 2014 09:32:16 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Mon, 7 Jul 2014 09:32:16 -0700 (PDT)
In-Reply-To: <515BE677-F7B2-4CFF-8289-57E3B0B8E262@gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<CABPQxsvAypb9JNU=RD49=6vEu2Rpd23DhhzynHN=gxNz_f4c2Q@mail.gmail.com>
	<CAOTBr2=O6Qjw_XFch8Xmy2hhDw65HSC_vAMbbZq99S4C=1pqRg@mail.gmail.com>
	<CAJ3iqPQ7qSGJisFzoqwNtWJs9yB1LrZcbuYpX-+YBzG9FRy7Kg@mail.gmail.com>
	<CAMJOb8k1Xa0O_eVagWeB25f1OEfBoh0T=nSHYQgOAgV_QjFaOA@mail.gmail.com>
	<515BE677-F7B2-4CFF-8289-57E3B0B8E262@gmail.com>
Date: Mon, 7 Jul 2014 09:32:16 -0700
Message-ID: <CAJgQjQ9mqQo87hTK7jUk6UrWsNLN8+nSj+srf7j+b16bDzoatg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Ran mllib examples.

On Sun, Jul 6, 2014 at 1:21 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> +1
>
> Tested on Mac OS X.
>
> Matei
>
> On Jul 6, 2014, at 1:54 AM, Andrew Or <andrew@databricks.com> wrote:
>
>> +1, verified that the UI bug is in fact fixed in
>> https://github.com/apache/spark/pull/1255.
>>
>>
>> 2014-07-05 20:01 GMT-07:00 Soren Macbeth <soren@yieldbot.com>:
>>
>>> +1
>>>
>>>
>>> On Sat, Jul 5, 2014 at 7:41 PM, Krishna Sankar <ksankar42@gmail.com>
>>> wrote:
>>>
>>>> +1
>>>>
>>>>   - Compiled rc2 w/ CentOS 6.5, Yarn,Hadoop 2.2.0 - successful
>>>>   - Smoke Test (scala,python) (distributed cluster) - successful
>>>>   - We had ran Java/SparkSQL (count, distinct et al) ~250M records RDD
>>>>   over HBase 0.98.3 over last build (rc1) - successful
>>>>   - Stand alone multi-node cluster is working better for us than Yarn
>>>>
>>>> Cheers
>>>> <k/>
>>>>
>>>>
>>>> On Fri, Jul 4, 2014 at 12:40 PM, Patrick Wendell <pwendell@gmail.com>
>>>> wrote:
>>>>
>>>>> I'll start the voting with a +1 - ran tests on the release candidate
>>>>> and ran some basic programs. RC1 passed our performance regression
>>>>> suite, and there are no major changes from that RC.
>>>>>
>>>>> On Fri, Jul 4, 2014 at 12:39 PM, Patrick Wendell <pwendell@gmail.com>
>>>>> wrote:
>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>> version
>>>>> 1.0.1!
>>>>>>
>>>>>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>>>>>>
>>>>>
>>>>
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
>>>>>>
>>>>>> The release files, including signatures, digests, etc. can be found
>>> at:
>>>>>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>>>>>>
>>>>>> Release artifacts are signed with the following key:
>>>>>> https://people.apache.org/keys/committer/pwendell.asc
>>>>>>
>>>>>> The staging repository for this release can be found at:
>>>>>>
>>>> https://repository.apache.org/content/repositories/orgapachespark-1021/
>>>>>>
>>>>>> The documentation corresponding to this release can be found at:
>>>>>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>>>>>>
>>>>>> Please vote on releasing this package as Apache Spark 1.0.1!
>>>>>>
>>>>>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>
>>>>>> [ ] +1 Release this package as Apache Spark 1.0.1
>>>>>> [ ] -1 Do not release this package because ...
>>>>>>
>>>>>> To learn more about Apache Spark, please see
>>>>>> http://spark.apache.org/
>>>>>>
>>>>>> === Differences from RC1 ===
>>>>>> This release includes only one "blocking" patch from rc1:
>>>>>> https://github.com/apache/spark/pull/1255
>>>>>>
>>>>>> There are also smaller fixes which came in over the last week.
>>>>>>
>>>>>> === About this release ===
>>>>>> This release fixes a few high-priority bugs in 1.0 and has a variety
>>>>>> of smaller fixes. The full list is here: http://s.apache.org/b45.
>>> Some
>>>>>> of the more visible patches are:
>>>>>>
>>>>>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>>>>>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>>>>> size.
>>>>>> SPARK-1790: Support r3 instance types on EC2.
>>>>>>
>>>>>> This is the first maintenance release on the 1.0 line. We plan to
>>> make
>>>>>> additional maintenance releases as new fixes come in.
>>>>>
>>>>
>>>
>

From dev-return-8223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul  7 19:01:23 2014
Return-Path: <dev-return-8223-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6C65F1191D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Jul 2014 19:01:23 +0000 (UTC)
Received: (qmail 27325 invoked by uid 500); 7 Jul 2014 19:01:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27203 invoked by uid 500); 7 Jul 2014 19:01:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26470 invoked by uid 99); 7 Jul 2014 19:01:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 19:01:21 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of weixiaokai@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 19:01:17 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <weixiaokai@gmail.com>)
	id 1X4E9p-0001nx-AM
	for dev@spark.incubator.apache.org; Mon, 07 Jul 2014 12:00:57 -0700
Date: Mon, 7 Jul 2014 12:00:57 -0700 (PDT)
From: xwei <weixiaokai@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAHpkSEqmECQF_FC4XSru6uzYNdp+UbaC2dd=M6JSJ290gpxiMQ@mail.gmail.com>
In-Reply-To: <C8CA2150-009B-485C-8DD1-CE1689E75885@staff.sina.com.cn>
References: <CFC6248F.577%xwei@palantir.com> <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com> <1403831040483-7088.post@n3.nabble.com> <D350552E-A930-43C0-AB12-FC7E1957EB78@staff.sina.com.cn> <B3E02C88-8C42-497B-8D78-3B58186A26BB@gmail.com> <C8CA2150-009B-485C-8DD1-CE1689E75885@staff.sina.com.cn>
Subject: Re: Contributing to MLlib on GLM
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_98216_13759024.1404759657313"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_98216_13759024.1404759657313
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Gang,

No admin is looking at our patch:( do you have some suggestions so that our
patch can get noticed by the admin?

Best regards,

Xiaokai


On Mon, Jun 30, 2014 at 8:18 PM, Gang Bai [via Apache Spark Developers
List] <ml-node+s1001551n7131h2@n3.nabble.com> wrote:

> Thanks Xiaokai,
>
> I=E2=80=99ve created a pull request to merge features in my PR to your re=
po.
> Please take a review here https://github.com/xwei-datageek/spark/pull/2 .
>
> As for GLMs, here at Sina, we are solving the problem of predicting the
> num of visitors who read a particular news article or watch an online
> sports live stream in a particular period. I=E2=80=99m trying to improve =
the
> prediction results by tuning features and incorporating new models. So I=
=E2=80=99ll
> try Gamma regression later. Thanks for the implementation.
>
> Cheers,
> -Gang
>
> On Jun 29, 2014, at 8:17 AM, xwei <[hidden email]
> <http://user/SendEmail.jtp?type=3Dnode&node=3D7131&i=3D0>> wrote:
>
> > Hi Gang,
> >
> > No worries!
> >
> > I agree LBFGS would converge faster and your test suite is more
> comprehensive. I'd like to merge my branch with yours.
> >
> > I also agree with your viewpoint on the redundancy issue. For different
> GLMs, usually they only differ in gradient calculation but the
> ****regression.scala files are quite similar. For example,
> linearRegressionSGD, logisticRegressionSGD, RidgeRegressionSGD,
> poissonRegressionSGD all share quite a bit of common code in their class
> implementations. Since such redundancy is already there in the legacy cod=
e,
> simply merging Poisson and Gamma does not seem to help much. So I suggest
> we just leave them as separate classes for the time being.
> >
> >
> > Best regards,
> >
> > Xiaokai
> >
> > On Jun 27, 2014, at 6:45 PM, Gang Bai [via Apache Spark Developers List=
]
> wrote:
> >
> >> Hi Xiaokai,
> >>
> >> My bad. I didn't notice this before I created another PR for Poisson
> regression. The mails were buried in junk by the corp mail master. Also,
> thanks for considering my comments and advice in your PR.
> >>
> >> Adding my two cents here:
> >>
> >> * PoissonRegressionModel and GammaRegressionModel have the same fields
> and prediction method. Shall we use one instead of two redundant classes?
> Say, a LogLinearModel.
> >> * The LBFGS optimizer takes fewer iterations and results in better
> convergence than SGD. I implemented two GeneralizedLinearAlgorithm classe=
s
> using LBFGS and SGD respectively. You may take a look into it. If it's OK
> to you, I'd be happy to send a PR to your branch.
> >> * In addition to the generated test data, We may use some real-world
> data for testing. In my implementation, I added the test data from
> https://onlinecourses.science.psu.edu/stat504/node/223. Please check my
> test suite.
> >>
> >> -Gang
> >> Sent from my iPad
> >>
> >>> On 2014=E5=B9=B46=E6=9C=8827=E6=97=A5, at =E4=B8=8B=E5=8D=886:03, "xw=
ei" <[hidden email]> wrote:
> >>>
> >>>
> >>> Yes, that's what we did: adding two gradient functions to
> Gradient.scala and
> >>> create PoissonRegression and GammaRegression using these gradients. W=
e
> made
> >>> a PR on this.
> >>>
> >>>
> >>>
> >>> --
> >>> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to=
-MLlib-on-GLM-tp7033p7088.html
> >>> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
> >>
> >>
> >> If you reply to this email, your message will be added to the
> discussion below:
> >>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to=
-MLlib-on-GLM-tp7033p7107.html
> >> To unsubscribe from Contributing to MLlib on GLM, click here.
> >> NAML
> >
> >
> >
> >
> >
> > --
> > View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to=
-MLlib-on-GLM-tp7033p7117.html
>
> > Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to=
-MLlib-on-GLM-tp7033p7131.html
>  To unsubscribe from Contributing to MLlib on GLM, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dunsubscribe_by_code&node=3D7033&code=3Dd2VpeGlhb2thaUBnb=
WFpbC5jb218NzAzM3w2NTc5NDUzMzA=3D>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlS=
ervlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&bas=
e=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNa=
mespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subscri=
bers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instan=
t_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/Contributing-to-MLlib-on-GLM-tp7033p7197.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.
------=_Part_98216_13759024.1404759657313--

From dev-return-8224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul  7 21:18:11 2014
Return-Path: <dev-return-8224-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4E3611F09
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Jul 2014 21:18:11 +0000 (UTC)
Received: (qmail 48534 invoked by uid 500); 7 Jul 2014 21:18:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48478 invoked by uid 500); 7 Jul 2014 21:18:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48465 invoked by uid 99); 7 Jul 2014 21:18:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 21:18:10 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tgraves_cs@yahoo.com designates 72.30.239.213 as permitted sender)
Received: from [72.30.239.213] (HELO nm40-vm5.bullet.mail.bf1.yahoo.com) (72.30.239.213)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 21:18:04 +0000
Received: from [66.196.81.172] by nm40.bullet.mail.bf1.yahoo.com with NNFMP; 07 Jul 2014 21:17:43 -0000
Received: from [98.139.212.209] by tm18.bullet.mail.bf1.yahoo.com with NNFMP; 07 Jul 2014 21:17:43 -0000
Received: from [127.0.0.1] by omp1018.mail.bf1.yahoo.com with NNFMP; 07 Jul 2014 21:17:43 -0000
X-Yahoo-Newman-Property: ymail-5
X-Yahoo-Newman-Id: 491376.91349.bm@omp1018.mail.bf1.yahoo.com
Received: (qmail 75096 invoked by uid 60001); 7 Jul 2014 21:17:43 -0000
X-YMail-OSG: Q6SmxpEVM1nwgqVzChbJwGooraBDpVL.KdhlKdIfgZrBJHk
 On_YKN2ZzauTX7Il1CM9WgU3ViZdxGcj1CKN98Ph8nd7WEmW1S83YaaNqHrV
 UbZij6nIusf7VzfMxAjI1icPLlruSh6pURdMMaOYyjHbb_0U.bhH1bQVTmy2
 qRWiMsHsdhrU4TaIdkIJF3Z2rUxhZMik0zAaMzxhVjNRYWsLhr4gtHMItVMO
 H1iZ94Oh4_4use6HNVX_3o.hgU5RABxX60MrIkFqS_9VCJzR2sVoZ3_WKVqP
 Iii2ZIAIpkdNr_dIBqaU2YEwVi7ntpsTdl9BE1oC3W4nINAYpdEuTWnY4QaM
 oHwuGiVyaeY2GSYVmBIbkywdAM7qI.ZV_oyokofbwRf0CcbyCBs3YKfzAgD3
 JSbxrLlCQgInUQsXip_5QfZRBPnDainWRWkxwkbuZJhxTer64cNDuxBX17DX
 7xZ3hCEhnGP6g.9j7nGN3qu9NBoW0K2Swecg8BfOHZt2oo1JYfffRq4w6SJ0
 2at_zb_Fj8zsRjk0P24kTKR5b.e7DVsoWr_fq1VqFGHVid8BwsFFR8Zy1.dk
 4Rj6T.PWxLOIRPcDjDVnSLZfX1if1mn7gC5XeO1btaVYlYxcLESodZgO.Vbn
 epDmCIl0TQsQcB50msnOp6RLLdf0G9DeeXLHf526lCgBHz0QNnY7QpRtuFsr
 8NIbU_vpuHMWuyA--
Received: from [204.11.79.50] by web140103.mail.bf1.yahoo.com via HTTP; Mon, 07 Jul 2014 14:17:43 PDT
X-Rocket-MIMEInfo: 002.001,KzEuIFJhbiBzb21lIFNwYXJrIG9uIHlhcm4gam9icyBvbiBhIGhhZG9vcCAyLjQgY2x1c3RlciB3aXRoIGF1dGhlbnRpY2F0aW9uIG9uLgoKVG9tCgoKT24gRnJpZGF5LCBKdWx5IDQsIDIwMTQgMjozOSBQTSwgUGF0cmljayBXZW5kZWxsIDxwd2VuZGVsbEBnbWFpbC5jb20.IHdyb3RlOgogCgoKUGxlYXNlIHZvdGUgb24gcmVsZWFzaW5nIHRoZSBmb2xsb3dpbmcgY2FuZGlkYXRlIGFzIEFwYWNoZSBTcGFyayB2ZXJzaW9uIDEuMC4xIQoKVGhlIHRhZyB0byBiZSB2b3RlZCBvbiBpcyB2MS4wLjEtcmMxIChjb20BMAEBAQE-
X-Mailer: YahooMailWebService/0.8.191.1
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
Message-ID: <1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
Date: Mon, 7 Jul 2014 14:17:43 -0700
From: Tom Graves <tgraves_cs@yahoo.com.INVALID>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
To: "dev@spark.apache.org" <dev@spark.apache.org>
In-Reply-To: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-916770207-912570624-1404767863=:71288"
X-Virus-Checked: Checked by ClamAV on apache.org

---916770207-912570624-1404767863=:71288
Content-Type: text/plain; charset=us-ascii

+1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with authentication on.

Tom


On Friday, July 4, 2014 2:39 PM, Patrick Wendell <pwendell@gmail.com> wrote:
 


Please vote on releasing the following candidate as Apache Spark version 1.0.1!

The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1021/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/

Please vote on releasing this package as Apache Spark 1.0.1!

The vote is open until Monday, July 07, at 20:45 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.0.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

=== Differences from RC1 ===
This release includes only one "blocking" patch from rc1:
https://github.com/apache/spark/pull/1255

There are also smaller fixes which came in over the last week.

=== About this release ===
This release fixes a few high-priority bugs in 1.0 and has a variety
of smaller fixes. The full list is here: http://s.apache.org/b45. Some
of the more visible patches are:

SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
SPARK-1790: Support r3 instance types on EC2.

This is the first maintenance release on the 1.0 line. We plan to make
additional maintenance releases as new fixes come in.
---916770207-912570624-1404767863=:71288--

From dev-return-8225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul  7 22:48:28 2014
Return-Path: <dev-return-8225-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF0B211227
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  7 Jul 2014 22:48:28 +0000 (UTC)
Received: (qmail 52815 invoked by uid 500); 7 Jul 2014 22:48:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52755 invoked by uid 500); 7 Jul 2014 22:48:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52744 invoked by uid 99); 7 Jul 2014 22:48:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 22:48:28 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 07 Jul 2014 22:48:26 +0000
Received: by mail-qg0-f46.google.com with SMTP id q107so4252288qgd.33
        for <dev@spark.apache.org>; Mon, 07 Jul 2014 15:48:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=BI8FJM223ltG4WUVsh2O+4eME5WRK7T3QUoaAsiWEG4=;
        b=F0LuH97KJ7uIm1b1Hl3IxQu4SV5qqREnOAelB+75aQJvbdG4fTQkkDon3u0GdmiWbb
         MoXniIPSwS00IUd1+QzmuGOUNyh+CmAmBcjtF9ok0U3ByFzlYbAEOfwYk4v+O5OlJ+3A
         sRv6kJg2xNWLXLboB8L7ITXiJ3HoF02iTdX7E+6CJULK3RljE6pwqbj9HZ2BDnM4wDYF
         Io0ba5Lmgzgb80dqpgbJOl+PpxmcF5WRst/uLdBThiksgY/tFsePj+kHNY34sZ3P3K4L
         H7S9ILLshf4MFjMnQiCdWT167UFTzgu1T6asb6o95uIi1jDZdL3/A3nUqf502rgcuxrq
         j3GA==
X-Gm-Message-State: ALoCoQl65ebME28Mx5Wm51i5UemzxkCtP6Od00csl2VOeqdC+tZjcvBQyyEB6j0FO85wMLAirUho
X-Received: by 10.140.26.149 with SMTP id 21mr21511704qgv.51.1404773280754;
 Mon, 07 Jul 2014 15:48:00 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 7 Jul 2014 15:47:40 -0700 (PDT)
In-Reply-To: <EC8B47A8-886A-40EB-BB2F-AF7C8F299E91@oss.nttdata.co.jp>
References: <EC8B47A8-886A-40EB-BB2F-AF7C8F299E91@oss.nttdata.co.jp>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 7 Jul 2014 15:47:40 -0700
Message-ID: <CAPh_B=bdtUO1JF-e-skVHshzp_qCqGzXL9EEybb1Qf2C-83F9w@mail.gmail.com>
Subject: Re: Invalid link for Spark 1.0.0 in Official Web Site
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c032c063c2d604fda244e0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c032c063c2d604fda244e0
Content-Type: text/plain; charset=UTF-8

Thanks for reporting this. I just fixed it.



On Fri, Jul 4, 2014 at 11:14 AM, Kousuke Saruta <sarutak@oss.nttdata.co.jp>
wrote:

> Hi,
>
> I found there is a invalid link in <http://spark.apache.org/downloads.html>
> .
> The link for release note of Spark 1.0.0 indicates
> http://spark.apache.org/releases/spark-release-1.0.0.html but this link
> is invalid.
> I think that is mistake for <
> http://spark.apache.org/releases/spark-release-1-0-0.html>.
>
> Thanks,
> Kousuke
>
>
>

--001a11c032c063c2d604fda244e0--

From dev-return-8226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 02:33:21 2014
Return-Path: <dev-return-8226-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F46B118C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 02:33:21 +0000 (UTC)
Received: (qmail 57045 invoked by uid 500); 8 Jul 2014 02:33:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56792 invoked by uid 500); 8 Jul 2014 02:33:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56781 invoked by uid 99); 8 Jul 2014 02:33:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 02:33:20 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [61.135.152.212] (HELO SINA-HUB02.staff.sina.com.cn) (61.135.152.212)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 02:33:18 +0000
Received: from [10.221.12.120] (10.221.12.120) by mail.staff.sina.com.cn
 (10.210.97.52) with Microsoft SMTP Server (TLS) id 14.2.247.3; Tue, 8 Jul
 2014 10:32:50 +0800
Content-Type: text/plain; charset="utf-8"
MIME-Version: 1.0 (Mac OS X Mail 7.3 \(1878.2\))
Subject: Re: Contributing to MLlib on GLM
From: Gang Bai <baigang@staff.sina.com.cn>
In-Reply-To: <CAHpkSEqmECQF_FC4XSru6uzYNdp+UbaC2dd=M6JSJ290gpxiMQ@mail.gmail.com>
Date: Tue, 8 Jul 2014 10:32:46 +0800
Content-Transfer-Encoding: quoted-printable
Message-ID: <A6A6A78B-CA07-4C3A-991E-DB66F6C9F0A6@staff.sina.com.cn>
References: <CFC6248F.577%xwei@palantir.com> <CAP7bEL1W+B=qdj8bWhDMRApJb5QVqxE1rfxt23wTKSYJbt27Fg@mail.gmail.com> <1403831040483-7088.post@n3.nabble.com> <D350552E-A930-43C0-AB12-FC7E1957EB78@staff.sina.com.cn> <B3E02C88-8C42-497B-8D78-3B58186A26BB@gmail.com> <C8CA2150-009B-485C-8DD1-CE1689E75885@staff.sina.com.cn> <CAHpkSEqmECQF_FC4XSru6uzYNdp+UbaC2dd=M6JSJ290gpxiMQ@mail.gmail.com>
To: <dev@spark.apache.org>
X-Mailer: Apple Mail (2.1878.2)
X-Originating-IP: [10.221.12.120]
X-Virus-Checked: Checked by ClamAV on apache.org

Poisson and Gamma regressions for modeling count data are definitely =
important in spark.mllib.regression. So don=E2=80=99t worry. Let=E2=80=99s=
 change the updater to SquaredL2Updater as we discussed in the PR. Then =
we can ask Jenkins to run the test.

On Jul 8, 2014, at 3:00 AM, xwei <weixiaokai@gmail.com> wrote:

> Hi Gang,
>=20
> No admin is looking at our patch:( do you have some suggestions so =
that our
> patch can get noticed by the admin?
>=20
> Best regards,
>=20
> Xiaokai
>=20
>=20
> On Mon, Jun 30, 2014 at 8:18 PM, Gang Bai [via Apache Spark Developers
> List] <ml-node+s1001551n7131h2@n3.nabble.com> wrote:
>=20
>> Thanks Xiaokai,
>>=20
>> I=E2=80=99ve created a pull request to merge features in my PR to =
your repo.
>> Please take a review here =
https://github.com/xwei-datageek/spark/pull/2 .
>>=20
>> As for GLMs, here at Sina, we are solving the problem of predicting =
the
>> num of visitors who read a particular news article or watch an online
>> sports live stream in a particular period. I=E2=80=99m trying to =
improve the
>> prediction results by tuning features and incorporating new models. =
So I=E2=80=99ll
>> try Gamma regression later. Thanks for the implementation.
>>=20
>> Cheers,
>> -Gang
>>=20
>> On Jun 29, 2014, at 8:17 AM, xwei <[hidden email]
>> <http://user/SendEmail.jtp?type=3Dnode&node=3D7131&i=3D0>> wrote:
>>=20
>>> Hi Gang,
>>>=20
>>> No worries!
>>>=20
>>> I agree LBFGS would converge faster and your test suite is more
>> comprehensive. I'd like to merge my branch with yours.
>>>=20
>>> I also agree with your viewpoint on the redundancy issue. For =
different
>> GLMs, usually they only differ in gradient calculation but the
>> ****regression.scala files are quite similar. For example,
>> linearRegressionSGD, logisticRegressionSGD, RidgeRegressionSGD,
>> poissonRegressionSGD all share quite a bit of common code in their =
class
>> implementations. Since such redundancy is already there in the legacy =
code,
>> simply merging Poisson and Gamma does not seem to help much. So I =
suggest
>> we just leave them as separate classes for the time being.
>>>=20
>>>=20
>>> Best regards,
>>>=20
>>> Xiaokai
>>>=20
>>> On Jun 27, 2014, at 6:45 PM, Gang Bai [via Apache Spark Developers =
List]
>> wrote:
>>>=20
>>>> Hi Xiaokai,
>>>>=20
>>>> My bad. I didn't notice this before I created another PR for =
Poisson
>> regression. The mails were buried in junk by the corp mail master. =
Also,
>> thanks for considering my comments and advice in your PR.
>>>>=20
>>>> Adding my two cents here:
>>>>=20
>>>> * PoissonRegressionModel and GammaRegressionModel have the same =
fields
>> and prediction method. Shall we use one instead of two redundant =
classes?
>> Say, a LogLinearModel.
>>>> * The LBFGS optimizer takes fewer iterations and results in better
>> convergence than SGD. I implemented two GeneralizedLinearAlgorithm =
classes
>> using LBFGS and SGD respectively. You may take a look into it. If =
it's OK
>> to you, I'd be happy to send a PR to your branch.
>>>> * In addition to the generated test data, We may use some =
real-world
>> data for testing. In my implementation, I added the test data from
>> https://onlinecourses.science.psu.edu/stat504/node/223. Please check =
my
>> test suite.
>>>>=20
>>>> -Gang
>>>> Sent from my iPad
>>>>=20
>>>>> On 2014=E5=B9=B46=E6=9C=8827=E6=97=A5, at =E4=B8=8B=E5=8D=886:03, =
"xwei" <[hidden email]> wrote:
>>>>>=20
>>>>>=20
>>>>> Yes, that's what we did: adding two gradient functions to
>> Gradient.scala and
>>>>> create PoissonRegression and GammaRegression using these =
gradients. We
>> made
>>>>> a PR on this.
>>>>>=20
>>>>>=20
>>>>>=20
>>>>> --
>>>>> View this message in context:
>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7088.html
>>>>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>>>=20
>>>>=20
>>>> If you reply to this email, your message will be added to the
>> discussion below:
>>>>=20
>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7107.html
>>>> To unsubscribe from Contributing to MLlib on GLM, click here.
>>>> NAML
>>>=20
>>>=20
>>>=20
>>>=20
>>>=20
>>> --
>>> View this message in context:
>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7117.html
>>=20
>>> Sent from the Apache Spark Developers List mailing list archive at
>> Nabble.com.
>>=20
>>=20
>>=20
>> ------------------------------
>> If you reply to this email, your message will be added to the =
discussion
>> below:
>>=20
>> =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7131.html
>> To unsubscribe from Contributing to MLlib on GLM, click here
>> =
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dunsubscribe_by_code&node=3D7033&code=3Dd2VpeGlhb2thaUBnb=
WFpbC5jb218NzAzM3w2NTc5NDUzMzA=3D>
>> .
>> NAML
>> =
<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlSe=
rvlet.jtp?macro=3Dmacro_viewer&id=3Dinstant_html%21nabble%3Aemail.naml&bas=
e=3Dnabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleN=
amespace-nabble.view.web.template.NodeNamespace&breadcrumbs=3Dnotify_subsc=
ribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_ins=
tant_email%21nabble%3Aemail.naml>
>>=20
>=20
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-=
MLlib-on-GLM-tp7033p7197.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.


From dev-return-8227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 06:30:37 2014
Return-Path: <dev-return-8227-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4DA711D42
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 06:30:37 +0000 (UTC)
Received: (qmail 32346 invoked by uid 500); 8 Jul 2014 06:30:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32283 invoked by uid 500); 8 Jul 2014 06:30:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32266 invoked by uid 99); 8 Jul 2014 06:30:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 06:30:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhengbing.li@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 06:30:32 +0000
Received: from 172.24.2.119 (EHLO SZXEMA405-HUB.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id ARJ21695;
	Tue, 08 Jul 2014 14:30:01 +0800 (CST)
Received: from SZXEMA501-MBX.china.huawei.com ([169.254.1.119]) by
 SZXEMA405-HUB.china.huawei.com ([10.82.72.37]) with mapi id 14.03.0158.001;
 Tue, 8 Jul 2014 14:29:57 +0800
From: "Lizhengbing (bing, BIPA)" <zhengbing.li@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Could the function MLUtils.loadLibSVMFile be modified to support
 zero-based-index data?
Thread-Topic: Could the function MLUtils.loadLibSVMFile be modified to
 support zero-based-index data?
Thread-Index: Ac+adggi05OEn/WeSyOhp2PV2jetYg==
Date: Tue, 8 Jul 2014 06:29:55 +0000
Message-ID: <49229E870391FC49BBBED818C268753D70531377@SZXEMA501-MBX.china.huawei.com>
Accept-Language: zh-CN, en-US
Content-Language: zh-CN
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.66.170.84]
Content-Type: multipart/alternative;
	boundary="_000_49229E870391FC49BBBED818C268753D70531377SZXEMA501MBXchi_"
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A020205.53BB8FEA.0014,ss=1,re=0.000,fgs=0,
	ip=169.254.1.119,
	so=2013-05-26 15:14:31,
	dmn=2011-05-27 18:58:46
X-Mirapoint-Loop-Id: ddfe95186e88efe44304f1dcb7d79d02
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_49229E870391FC49BBBED818C268753D70531377SZXEMA501MBXchi_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable


1)  I download the imdb data from http://komarix.org/ac/ds/Blanc__Mel.txt.b=
z2 and use this data to test LBFGS
When I run examples referencing http://spark.apache.org/docs/latest/mllib-o=
ptimization.html,  an error occus.
4/07/07 08:37:27 ERROR Executor: Exception in task ID 2
java.lang.ArrayIndexOutOfBoundsException: -1
         at breeze.linalg.operators.DenseVector_SparseVector_Ops$$anon$129.=
apply(SparseVectorOps.scala:231)
         at breeze.linalg.operators.DenseVector_SparseVector_Ops$$anon$129.=
apply(SparseVectorOps.scala:216)
         at breeze.linalg.operators.BinaryRegistry$class.apply(BinaryOp.sca=
la:60)
         at breeze.linalg.VectorOps$$anon$178.apply(Vector.scala:391)
         at breeze.linalg.NumericOps$class.dot(NumericOps.scala:83)
         at breeze.linalg.DenseVector.dot(DenseVector.scala:47)
..................

2)  I find the imdb data are zero-based-index data
0 0:1 3:1 6208:1 8936:1 8959:1 16434:1 29840:1 29843:1 30274:1 32092:1 6372=
7:1 109302:1 114311:1 114336:1 119637:1 121867:1 143744:1 145106:1 186951:1=
 216401:1 228548:1 248919:1 251691:1 294713:1 302316:1 307685:1 316421:1 31=
6556:1 317062:1 321771:1 327174:1 364381:1 384514:1 404531:1 414947:1 43423=
5:1 434250:1 462625:1 471013:1 503923:1 511725:1 514582:1 514635:1 519251:1=
 524274:1 540734:1 556018:1 559036:1 559037:1 559039:1 559341:1 609032:1 64=
4534:1 650763:1 659114:1 666864:1 669778:1 669783:1 669787:1 673083:1

3) If change code "val index =3D indexAndValue(0).toInt - 1" to "val index =
=3D indexAndValue(0).toInt - offset" (offset equals 0 or 1 based on user's =
selection), then MLUtils.loadLibSVMFile will support both zero-based-index =
data and one-based-index data.
  That also means the interface of MLUtils.loadLibSVMFile will be changed



--_000_49229E870391FC49BBBED818C268753D70531377SZXEMA501MBXchi_--


From dev-return-8228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 07:23:07 2014
Return-Path: <dev-return-8228-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DE41711E34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 07:23:06 +0000 (UTC)
Received: (qmail 9991 invoked by uid 500); 8 Jul 2014 07:23:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9954 invoked by uid 500); 8 Jul 2014 07:23:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9943 invoked by uid 99); 8 Jul 2014 07:23:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 07:23:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.128.169 as permitted sender)
Received: from [209.85.128.169] (HELO mail-ve0-f169.google.com) (209.85.128.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 07:23:01 +0000
Received: by mail-ve0-f169.google.com with SMTP id pa12so5363227veb.0
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 00:22:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=0RbWCd0kvz0X8gUu67lfQ9LFtV2mDiNYL9jopgOO7Q0=;
        b=bFiVZ4kNgiitT0b1A4NBnAXcOc+q3uQCK1OPpxjq4RGt4AEf/pX7Ha9Y/YPdu6sqB2
         gfeWzZ6BXUh9VPYtmuFoUSfSKZSr9laFrxb1AFoJtiwh/HFkzLnDxr0AspF+vqB2CO8G
         ENrCUhjfV8b9OFdpspLbjh5qWKzlYf2s6gFnrFdfECUxo9/y29LKDlrnppPNS+4Nhd2b
         2qdg4n0qEjw/uMKHtupAXbuGteQBiQZNQnXulTrydpt1T5Zg0ldwW64mt5gN2WSN0FqY
         oT9Ho3EyMY3B4Hw+W+ryOcAEd6vIadEyvmjTpRFI6PgxXe4nuMUFcgmwag5GYQVN+3aX
         xzJg==
X-Gm-Message-State: ALoCoQkJ4hPeXXtkfkA+pybUgEVoGlDy9IOgeYNAdjBMAf+gDLEcsJTyff6hIpF8iNw2InS8i5t2
X-Received: by 10.221.64.80 with SMTP id xh16mr90677vcb.35.1404804160689; Tue,
 08 Jul 2014 00:22:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Tue, 8 Jul 2014 00:22:20 -0700 (PDT)
In-Reply-To: <49229E870391FC49BBBED818C268753D70531377@SZXEMA501-MBX.china.huawei.com>
References: <49229E870391FC49BBBED818C268753D70531377@SZXEMA501-MBX.china.huawei.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 8 Jul 2014 08:22:20 +0100
Message-ID: <CAMAsSdLZyP6H-68+7qKC+m5wYL1HwdV7eLfbZT4oG7if8UntDw@mail.gmail.com>
Subject: Re: Could the function MLUtils.loadLibSVMFile be modified to support
 zero-based-index data?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113351b2fa346004fda97489
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113351b2fa346004fda97489
Content-Type: text/plain; charset=UTF-8

On Tue, Jul 8, 2014 at 7:29 AM, Lizhengbing (bing, BIPA) <
zhengbing.li@huawei.com> wrote:

>
> 1)  I download the imdb data from
> http://komarix.org/ac/ds/Blanc__Mel.txt.bz2 and use this data to test
> LBFGS
> 2)  I find the imdb data are zero-based-index data
>

Since the method is for parsing the LIBSVM format, and its labels are
always 1-indexed IIUC, I don't think it would make sense to read 0-indexed
labels. It sounds like that input is not properly formatted, unless anyone
knows to the contrary?

--001a113351b2fa346004fda97489--

From dev-return-8229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 08:19:19 2014
Return-Path: <dev-return-8229-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CF65611FEF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 08:19:19 +0000 (UTC)
Received: (qmail 47637 invoked by uid 500); 8 Jul 2014 08:19:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47579 invoked by uid 500); 8 Jul 2014 08:19:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47567 invoked by uid 99); 8 Jul 2014 08:19:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 08:19:19 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.175 as permitted sender)
Received: from [74.125.82.175] (HELO mail-we0-f175.google.com) (74.125.82.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 08:19:16 +0000
Received: by mail-we0-f175.google.com with SMTP id k48so5443848wev.20
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 01:18:52 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=vz84ZXLWWRmFXCcYO2EO5D1Y2jBulDHcNSzeg6bs5Hc=;
        b=JNI1+WtFHi3wtXkoeTVmRiGo7wABcD359sEzU2OSSxw8k92XELHt59Tj3AaRXzy82Q
         tLkumib9bXkpvqZdfu7N15uD2OItFREr+Jrr2km/RDcyhmJXljJvsPQH0mJoV5S5uZMB
         VkZzfTj7eV9Ys5GHYh5UP4/RkVvroUHWe11iJPLq9LGBdrweh9gzqHj6vx4T1fi/8GTy
         d4U+zEgaLI26szKEKlpMkzMxN3ejOJP5z7cvCYjmGlrv0XdFG7e8WkEF98x6Blq1frwa
         HgN0nPO4E1lM2doD0GwLkQAa4SLHPpA5w4PYhZnQDUy2F+f6e6cIXEz/jU1X/2B8fMoy
         ewUg==
MIME-Version: 1.0
X-Received: by 10.180.14.33 with SMTP id m1mr2101602wic.50.1404807531918; Tue,
 08 Jul 2014 01:18:51 -0700 (PDT)
Received: by 10.194.6.74 with HTTP; Tue, 8 Jul 2014 01:18:51 -0700 (PDT)
Date: Tue, 8 Jul 2014 16:18:51 +0800
Message-ID: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
Subject: on shark, is tachyon less efficient than memory_only cache strategy ?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d040fa1eeeafbfd04fdaa3df2
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d040fa1eeeafbfd04fdaa3df2
Content-Type: text/plain; charset=UTF-8

hi, when i create a table, i can point the cache strategy using shark.cache,
i think "shark.cache=memory_only"  means data are managed by spark, and
data are in the same jvm with excutor;   while  "shark.cache=tachyon"
 means  data are managed by tachyon which is off heap, and data are not in
the same jvm with excutor,  so spark will load data from tachyon for each
query sql , so,  is  tachyon less efficient than memory_only cache strategy
 ?
if yes, can we let spark load all data once from tachyon  for all sql query
 if i want to use tachyon cache strategy since tachyon is more HA than
memory_only ?

--f46d040fa1eeeafbfd04fdaa3df2--

From dev-return-8230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 11:17:58 2014
Return-Path: <dev-return-8230-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F810118A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 11:17:58 +0000 (UTC)
Received: (qmail 60240 invoked by uid 500); 8 Jul 2014 11:17:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60177 invoked by uid 500); 8 Jul 2014 11:17:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60164 invoked by uid 99); 8 Jul 2014 11:17:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 11:17:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of leoncamel@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 11:17:52 +0000
Received: by mail-ie0-f180.google.com with SMTP id tr6so2761284ieb.25
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 04:17:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=Uz8xorQTP+moK+s3MWXAq6qmbePy/ODus38OPzpl7QQ=;
        b=HWpPbyjSiG5OmzP+AQN88TjG2FPHEXkecW4HP8MaYK9diaIdp9i3hf7wAB8ILnLhDP
         dq2FA0lNWQ+asZTi3yVGjrwHJxPtDsf9XAk3ng5mhOG+mnzu5R+cqTU1b1+R+lNCtQrm
         YhV9rPHZAqq/UG5uZjjqn7wu/bwviW+nuRN+QhpiJCpExndV+MxqqWtUVXiWZp0tym6U
         VLxkg45pAObQDTPbCBEZLDNIsL22L+yFO9uiwUiyUVpckeAHZRlzCU6YvXz3XluHf8Gw
         4rQ1LOnFoxUmRtMJx4pcS1V3MHY+PnOWVHOLvcs7kA85iSjx37zYtYpX3aU2fo2gusCS
         dkOg==
X-Received: by 10.42.37.13 with SMTP id w13mr31800349icd.13.1404818251655;
 Tue, 08 Jul 2014 04:17:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.50.179.104 with HTTP; Tue, 8 Jul 2014 04:17:11 -0700 (PDT)
From: Leon Zhang <leoncamel@gmail.com>
Date: Tue, 8 Jul 2014 19:17:11 +0800
Message-ID: <CALtac6h3Mo7Tbbu3iJ_8MAmMMVOf2UFANt_19fpuBNsZOSc0fg@mail.gmail.com>
Subject: (send this email to subscribe)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=90e6ba6e8e1edd30da04fdacbcf1
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6e8e1edd30da04fdacbcf1
Content-Type: text/plain; charset=UTF-8



--90e6ba6e8e1edd30da04fdacbcf1--

From dev-return-8231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 11:44:13 2014
Return-Path: <dev-return-8231-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1E5CC11924
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 11:44:13 +0000 (UTC)
Received: (qmail 6708 invoked by uid 500); 8 Jul 2014 11:44:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6649 invoked by uid 500); 8 Jul 2014 11:44:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6622 invoked by uid 99); 8 Jul 2014 11:44:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 11:44:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.220.41 as permitted sender)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 11:44:08 +0000
Received: by mail-pa0-f41.google.com with SMTP id fb1so7262438pad.0
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 04:43:43 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=references:mime-version:in-reply-to:content-type
         :content-transfer-encoding:message-id:cc:from:subject:date:to;
        bh=TNprv2qFzuSGvs1Hm4HEX7ObsWuVIOm78UONmHSQagU=;
        b=TELhXKXvJgYN9i9Kfyx1MYFeTF3Q24dfUHBX/wgH/ZMNICqbcwNXqzUaunPbyre13b
         4qA/0oIFwux2sRs/Fdw6Ycb/Vc8pd4VRiZ4ETM9ZM74XzBVir07+YGIv67bTJUMlSdp1
         BATYpSqMQxppUsjm0AzZhGBoQo1ACij8mfmatwRPLUU6fNCBUCOyA77GFLDVclFGDC4E
         s2cW5vgDmELJ4OZBO62u1NNj7tWWs8AD8muS/NQdy/oOIGM7bm9QUanhzcUW4HE2xIgi
         2PFENR8Fz1DqN5ir1VZyt3oU4svOWnykj9LKf0YrBRaW2mlW9t6Hi7/5r/AUO1YZnqEp
         6d7A==
X-Received: by 10.68.232.33 with SMTP id tl1mr161461pbc.162.1404819822979;
        Tue, 08 Jul 2014 04:43:42 -0700 (PDT)
Received: from [192.168.0.11] (c-24-130-236-83.hsd1.ca.comcast.net. [24.130.236.83])
        by mx.google.com with ESMTPSA id hk5sm55518497pbb.86.2014.07.08.04.43.40
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 08 Jul 2014 04:43:41 -0700 (PDT)
References: <CALtac6h3Mo7Tbbu3iJ_8MAmMMVOf2UFANt_19fpuBNsZOSc0fg@mail.gmail.com>
Mime-Version: 1.0 (1.0)
In-Reply-To: <CALtac6h3Mo7Tbbu3iJ_8MAmMMVOf2UFANt_19fpuBNsZOSc0fg@mail.gmail.com>
Content-Type: multipart/alternative;
	boundary=Apple-Mail-36FEB3EF-ABF5-4D32-A3D0-342D857C7078
Content-Transfer-Encoding: 7bit
Message-Id: <ECDC5B2F-6F18-4D9E-AB8B-19FD806EA93C@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
X-Mailer: iPhone Mail (10B146)
From: Ted Yu <yuzhihong@gmail.com>
Subject: Re: (send this email to subscribe)
Date: Tue, 8 Jul 2014 04:43:40 -0700
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-36FEB3EF-ABF5-4D32-A3D0-342D857C7078
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: 7bit

See http://spark.apache.org/news/spark-mailing-lists-moving-to-apache.html

Cheers

On Jul 8, 2014, at 4:17 AM, Leon Zhang <leoncamel@gmail.com> wrote:

> 

--Apple-Mail-36FEB3EF-ABF5-4D32-A3D0-342D857C7078--

From dev-return-8232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 11:50:28 2014
Return-Path: <dev-return-8232-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6DACD1194D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 11:50:28 +0000 (UTC)
Received: (qmail 17756 invoked by uid 500); 8 Jul 2014 11:50:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17698 invoked by uid 500); 8 Jul 2014 11:50:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17683 invoked by uid 99); 8 Jul 2014 11:50:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 11:50:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,MIME_QP_LONG_LINE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.220.54 as permitted sender)
Received: from [209.85.220.54] (HELO mail-pa0-f54.google.com) (209.85.220.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 11:50:23 +0000
Received: by mail-pa0-f54.google.com with SMTP id et14so7146896pad.41
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 04:49:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=references:mime-version:in-reply-to:content-type
         :content-transfer-encoding:message-id:cc:from:subject:date:to;
        bh=CjmE9/sMiAMTH4nJ6Uc2UM/RfA7AEmiVpuUE7DOYBXQ=;
        b=hfbZ73NXnfrpTnJOnOzhU5NKgK89CZNawRJYT0L4pHMceZvDTmlklipjZpuyorbkQW
         Eg11A4/EQ8waGNMdX0xqEq5/gCSli9bb3NxSQhO0u5XvUUe7B4FaGzX79Sbbhl81lPYI
         0s/Fy0CsFqog7SXw1QCavAEnlwCzqyINROFSFhqZQVR9vBncMXP7ic5dJQ0fClvaL7tB
         zHTrUYUgoidTI43JhLQe/Wigelyfv69K77fBl0LF11o8ywNV67jLtYtMq7qe4gjV51sn
         b8usCRoKaQlQthhAQz49HkXNhWRNJ8S6kunkJOh2PQJ6FxAHSQcYyRU2To/7xF7eLfmb
         qmPw==
X-Received: by 10.66.147.131 with SMTP id tk3mr8582894pab.111.1404820197834;
        Tue, 08 Jul 2014 04:49:57 -0700 (PDT)
Received: from [192.168.0.11] (c-24-130-236-83.hsd1.ca.comcast.net. [24.130.236.83])
        by mx.google.com with ESMTPSA id w16sm24105037pdl.36.2014.07.08.04.49.55
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 08 Jul 2014 04:49:56 -0700 (PDT)
References: <CALtac6h3Mo7Tbbu3iJ_8MAmMMVOf2UFANt_19fpuBNsZOSc0fg@mail.gmail.com> <ECDC5B2F-6F18-4D9E-AB8B-19FD806EA93C@gmail.com>
Mime-Version: 1.0 (1.0)
In-Reply-To: <ECDC5B2F-6F18-4D9E-AB8B-19FD806EA93C@gmail.com>
Content-Type: multipart/alternative;
	boundary=Apple-Mail-21ECEC74-D9FA-4D21-ADA3-2960E4C46ADB
Content-Transfer-Encoding: 7bit
Message-Id: <9CFB3015-9D4E-4EB9-9359-11AC17478AD2@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
X-Mailer: iPhone Mail (10B146)
From: Ted Yu <yuzhihong@gmail.com>
Subject: Re: (send this email to subscribe)
Date: Tue, 8 Jul 2014 04:49:55 -0700
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail-21ECEC74-D9FA-4D21-ADA3-2960E4C46ADB
Content-Type: text/plain;
	charset=us-ascii
Content-Transfer-Encoding: 7bit

This is the correct page: http://spark.apache.org/community.html

Cheers

On Jul 8, 2014, at 4:43 AM, Ted Yu <yuzhihong@gmail.com> wrote:

> See http://spark.apache.org/news/spark-mailing-lists-moving-to-apache.html
> 
> Cheers
> 
> On Jul 8, 2014, at 4:17 AM, Leon Zhang <leoncamel@gmail.com> wrote:
> 
>> 

--Apple-Mail-21ECEC74-D9FA-4D21-ADA3-2960E4C46ADB--

From dev-return-8233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 15:19:41 2014
Return-Path: <dev-return-8233-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A163D112D1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 15:19:41 +0000 (UTC)
Received: (qmail 84855 invoked by uid 500); 8 Jul 2014 15:19:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84793 invoked by uid 500); 8 Jul 2014 15:19:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84780 invoked by uid 99); 8 Jul 2014 15:19:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 15:19:40 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 15:19:35 +0000
Received: by mail-qg0-f54.google.com with SMTP id q107so5133151qgd.41
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 08:19:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=TXWS2c0kKmtI8Mi7yEJydTYHjvZG5Z/VhNbYdzVIV+Y=;
        b=Vpxy7/oyZ0clGqk9QTIVbtPEew1MbBoHJjmhX4QmKA+JQ/QnrYO0d6aSqvBpnRkJep
         a0Ko9gRGcUc8okquBrfSJDZDXTkErmkCPclMCMNnLbarA5ixCL7xvhSxZ0VeP7ekw5zQ
         6EUBEEOf3syVwoULboqffxlNJDoTjE3FrKCf67fZpoQ2SemTKfUqlw4rk1M5floTq5Pe
         hWQ0kMIv397UsYSSc0M7UaLion3L9t6AFezjYNfI+V/LldGvfHiS+3OA7Z/EnnfDHc2g
         WUx/cMOVScka9xSBKuY8w0tGRtCwCQG3TRFupQs2NfqXkCnmgWDmYl8GaMjjRlrB7xxo
         jT1g==
X-Received: by 10.140.101.115 with SMTP id t106mr55724378qge.91.1404832754274;
 Tue, 08 Jul 2014 08:19:14 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Tue, 8 Jul 2014 08:18:54 -0700 (PDT)
In-Reply-To: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Tue, 8 Jul 2014 08:18:54 -0700
Message-ID: <CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1721e499a4b04fdb01da7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1721e499a4b04fdb01da7
Content-Type: text/plain; charset=UTF-8

Tachyon should only be marginally less performant than memory_only, because
we mmap the data from Tachyon's ramdisk. We do not have to, say, transfer
the data over a pipe from Tachyon; we can directly read from the buffers in
the same way that Shark reads from its in-memory columnar format.



On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <liqingyang1985@gmail.com>
wrote:

> hi, when i create a table, i can point the cache strategy using
> shark.cache,
> i think "shark.cache=memory_only"  means data are managed by spark, and
> data are in the same jvm with excutor;   while  "shark.cache=tachyon"
>  means  data are managed by tachyon which is off heap, and data are not in
> the same jvm with excutor,  so spark will load data from tachyon for each
> query sql , so,  is  tachyon less efficient than memory_only cache strategy
>  ?
> if yes, can we let spark load all data once from tachyon  for all sql query
>  if i want to use tachyon cache strategy since tachyon is more HA than
> memory_only ?
>

--001a11c1721e499a4b04fdb01da7--

From dev-return-8234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 15:26:14 2014
Return-Path: <dev-return-8234-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7AA51137F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 15:26:14 +0000 (UTC)
Received: (qmail 95039 invoked by uid 500); 8 Jul 2014 15:26:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94981 invoked by uid 500); 8 Jul 2014 15:26:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94964 invoked by uid 99); 8 Jul 2014 15:26:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 15:26:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.128.179 as permitted sender)
Received: from [209.85.128.179] (HELO mail-ve0-f179.google.com) (209.85.128.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 15:26:09 +0000
Received: by mail-ve0-f179.google.com with SMTP id sa20so5890705veb.10
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 08:25:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=NaxKrRcqeki/41TJIMbZfvgHF+cRfDeH0gcSeWWaHHg=;
        b=IrBIiB7evIcyDVIHVg7ZlqXueFa7TotD+WP+cy1JJOHFe1qmxqEzlSaO2qHGDMRX7j
         DN7XHLF6n5WulDWz+XCn/FNZpiOTcKgMndXmWdY+vTwt3gfLUg9+09NiX2E2m0fBMbhY
         7buC0VbuNq5otB+o4lBfuwz0RCSjaZ+9PsWFWBUfsCUcat1hhTfu+dQGiig+PjrztoR4
         HH+9sLeETTWmDnnwGX2vqTy+KSMQCtIoawyQSiOKYe8U4zDC3VXwAxz7y7WBMdCj4tBb
         ki+nfDzi0MpDnR4GXGvxK0W5pVvmnta68Vf+DpKBOXKPEuaecBZ7qkhZ92TnbFwNbeAi
         7zVA==
X-Received: by 10.220.68.140 with SMTP id v12mr34384190vci.13.1404833149014;
 Tue, 08 Jul 2014 08:25:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.188.133 with HTTP; Tue, 8 Jul 2014 08:25:28 -0700 (PDT)
In-Reply-To: <CAMAsSdLZyP6H-68+7qKC+m5wYL1HwdV7eLfbZT4oG7if8UntDw@mail.gmail.com>
References: <49229E870391FC49BBBED818C268753D70531377@SZXEMA501-MBX.china.huawei.com>
 <CAMAsSdLZyP6H-68+7qKC+m5wYL1HwdV7eLfbZT4oG7if8UntDw@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Tue, 8 Jul 2014 08:25:28 -0700
Message-ID: <CABjXkq5mnrZLJW3aJeofYRM2xcP-e1eewr1tdg2r02Wr0gWCMw@mail.gmail.com>
Subject: Re: Could the function MLUtils.loadLibSVMFile be modified to support
 zero-based-index data?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a8506d0dada04fdb03463
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8506d0dada04fdb03463
Content-Type: text/plain; charset=UTF-8

As Sean mentions, if you can change the data to the standard format, that's
probably a good idea. If you'd rather read the data raw, then writing your
own version of loadLibSVMFile - then you could make your own loader
function which is very similar to the existing one with a few characters
removed:

https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/util/MLUtils.scala#L81

You will also likely need to change the logic where it determines the
number of features (currently line 95)


On Tue, Jul 8, 2014 at 12:22 AM, Sean Owen <sowen@cloudera.com> wrote:

> On Tue, Jul 8, 2014 at 7:29 AM, Lizhengbing (bing, BIPA) <
> zhengbing.li@huawei.com> wrote:
>
> >
> > 1)  I download the imdb data from
> > http://komarix.org/ac/ds/Blanc__Mel.txt.bz2 and use this data to test
> > LBFGS
> > 2)  I find the imdb data are zero-based-index data
> >
>
> Since the method is for parsing the LIBSVM format, and its labels are
> always 1-indexed IIUC, I don't think it would make sense to read 0-indexed
> labels. It sounds like that input is not properly formatted, unless anyone
> knows to the contrary?
>

--047d7b3a8506d0dada04fdb03463--

From dev-return-8235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 16:51:07 2014
Return-Path: <dev-return-8235-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4641B1171D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 16:51:07 +0000 (UTC)
Received: (qmail 14601 invoked by uid 500); 8 Jul 2014 16:51:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14543 invoked by uid 500); 8 Jul 2014 16:51:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14531 invoked by uid 99); 8 Jul 2014 16:51:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 16:51:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.41 as permitted sender)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 16:51:02 +0000
Received: by mail-qa0-f41.google.com with SMTP id cm18so5254636qab.28
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 09:50:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TKNECMFvr25tf5VklooTiOn4iBv28h9U9sVCHYUnM1M=;
        b=HnmWZ2kWh/2+PQO3ozw82Pm82Ew8eHrLpuQC0emkcXzokuMXOcjCeghiMvLx/WQLjd
         qbu0wyQF0oSQe1hXAHmwkzkl+CA7vXJAeFqpiG0IL1lBoS4XTkYZd64sYWrNUgU8SAPX
         4cCGYM7NFA04xYkb26VbkPWPUcGrTcc4F3jI3b90bge9Kv43/M8cWWHvjyaKyFqYmffk
         ew1zvMvL9ku1CUc04YfKCtpMfQoI4mSAw8cx8OGgan7RFmQfzotxRQGfZXrhlHKpLwl4
         2Mt03IBqE8f/C5GbXGoFMama+uV9DC+ZU7+G/JCmKgp+mf96cMq0KtSytzkFgJKL5YAF
         TC5Q==
MIME-Version: 1.0
X-Received: by 10.229.65.200 with SMTP id k8mr58901860qci.4.1404838238312;
 Tue, 08 Jul 2014 09:50:38 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Tue, 8 Jul 2014 09:50:38 -0700 (PDT)
In-Reply-To: <CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
	<CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
Date: Tue, 8 Jul 2014 22:20:38 +0530
Message-ID: <CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

You are ignoring serde costs :-)

- Mridul

On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> Tachyon should only be marginally less performant than memory_only, because
> we mmap the data from Tachyon's ramdisk. We do not have to, say, transfer
> the data over a pipe from Tachyon; we can directly read from the buffers in
> the same way that Shark reads from its in-memory columnar format.
>
>
>
> On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <liqingyang1985@gmail.com>
> wrote:
>
>> hi, when i create a table, i can point the cache strategy using
>> shark.cache,
>> i think "shark.cache=memory_only"  means data are managed by spark, and
>> data are in the same jvm with excutor;   while  "shark.cache=tachyon"
>>  means  data are managed by tachyon which is off heap, and data are not in
>> the same jvm with excutor,  so spark will load data from tachyon for each
>> query sql , so,  is  tachyon less efficient than memory_only cache strategy
>>  ?
>> if yes, can we let spark load all data once from tachyon  for all sql query
>>  if i want to use tachyon cache strategy since tachyon is more HA than
>> memory_only ?
>>

From dev-return-8236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 16:59:04 2014
Return-Path: <dev-return-8236-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 25AE011756
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 16:59:04 +0000 (UTC)
Received: (qmail 41994 invoked by uid 500); 8 Jul 2014 16:59:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41943 invoked by uid 500); 8 Jul 2014 16:59:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41926 invoked by uid 99); 8 Jul 2014 16:59:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 16:59:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.179 as permitted sender)
Received: from [209.85.216.179] (HELO mail-qc0-f179.google.com) (209.85.216.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 16:58:57 +0000
Received: by mail-qc0-f179.google.com with SMTP id x3so5433454qcv.38
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 09:58:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=FSwTQlRpTxwPDF38LENz9pUAJu6LfKefZev/mpAG/KI=;
        b=cK+603o45d6nNp7LIWFmFdcqFtKV0Ju0dLZfUCsNXdNS1eZT/ay5aYYQL+Q2iXnM9Z
         ZtBmeqJE41oCTjh8kXS747b9BgNqS3IXart0rXk8GHFN10umP/g/JoYcBnhiJG/OezFl
         bkCrly+ZETnB4xZRvc/3cS0aGROx6LPVL4rA5cZvV5aPWLe7DQOwmDB4Fl0dvOC/kvNV
         5ADiM9MhzxQ5NAwj8LD5NqvpsY+FRXlzcnRd0udfc8P+3WMLwGdUkdTt5NhYMQKz8tTN
         Vb4MKNAL64Kmq951OkxnHSWajz4R1DXoOA1P+5VIYNGXTyOfjI3O7Qjax0jEKOUhL0fE
         4eLw==
X-Received: by 10.140.101.115 with SMTP id t106mr56667317qge.91.1404838716984;
 Tue, 08 Jul 2014 09:58:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Tue, 8 Jul 2014 09:58:16 -0700 (PDT)
In-Reply-To: <CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
 <CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com> <CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Tue, 8 Jul 2014 09:58:16 -0700
Message-ID: <CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1721eb16d6b04fdb1809c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1721eb16d6b04fdb1809c
Content-Type: text/plain; charset=UTF-8

Shark's in-memory format is already serialized (it's compressed and
column-based).


On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> You are ignoring serde costs :-)
>
> - Mridul
>
> On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
> > Tachyon should only be marginally less performant than memory_only,
> because
> > we mmap the data from Tachyon's ramdisk. We do not have to, say, transfer
> > the data over a pipe from Tachyon; we can directly read from the buffers
> in
> > the same way that Shark reads from its in-memory columnar format.
> >
> >
> >
> > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <liqingyang1985@gmail.com>
> > wrote:
> >
> >> hi, when i create a table, i can point the cache strategy using
> >> shark.cache,
> >> i think "shark.cache=memory_only"  means data are managed by spark, and
> >> data are in the same jvm with excutor;   while  "shark.cache=tachyon"
> >>  means  data are managed by tachyon which is off heap, and data are not
> in
> >> the same jvm with excutor,  so spark will load data from tachyon for
> each
> >> query sql , so,  is  tachyon less efficient than memory_only cache
> strategy
> >>  ?
> >> if yes, can we let spark load all data once from tachyon  for all sql
> query
> >>  if i want to use tachyon cache strategy since tachyon is more HA than
> >> memory_only ?
> >>
>

--001a11c1721eb16d6b04fdb1809c--

From dev-return-8237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 17:19:53 2014
Return-Path: <dev-return-8237-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 21478118F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 17:19:53 +0000 (UTC)
Received: (qmail 40636 invoked by uid 500); 8 Jul 2014 17:19:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40576 invoked by uid 500); 8 Jul 2014 17:19:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40552 invoked by uid 99); 8 Jul 2014 17:19:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 17:19:52 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of haoyuan.li@gmail.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 17:19:48 +0000
Received: by mail-we0-f169.google.com with SMTP id t60so6353613wes.0
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 10:19:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=Eiaee36kcqoPehLTQpXrKsuZH0ga/FpwIelnpQ40a/M=;
        b=m6uBqSjyShkKV8Y49RiXd2R9WPJ4GTqzYJwyfy/g3u9ZdnD2yuCDK3cJxnLNnEvT5p
         SuLKHrHaFS3bguHQ/NSlIfEX/TiuulEq8WriHa/+pRG75qRcydw7Ny1R3E1BqHVURnQd
         QbFUUzzsbbRTPf03+VZfbsW9GzL+WdNZ8AHGEqYiBrwJrXPsZBWXLYwONbNS0F9XoxRR
         hbtB6ugQ1rZ8u4FP7WcOWutksxya6fGpZIscKpKpLKfLrTE6EnWguKNCZlwOuhVV9YlD
         g1WxCUW8ESMdII4Ic38/PdEWB34QFP7EEOtAFel8t5ekiAoXEcQHzeTHq8N6ClY1/PKU
         /rDQ==
X-Received: by 10.180.189.234 with SMTP id gl10mr5433262wic.56.1404839967147;
 Tue, 08 Jul 2014 10:19:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.204.6 with HTTP; Tue, 8 Jul 2014 10:19:06 -0700 (PDT)
In-Reply-To: <CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
 <CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
 <CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com> <CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
From: Haoyuan Li <haoyuan.li@gmail.com>
Date: Tue, 8 Jul 2014 10:19:06 -0700
Message-ID: <CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3253a354e0c04fdb1cbbc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3253a354e0c04fdb1cbbc
Content-Type: text/plain; charset=UTF-8

Yes. For Shark, two modes, "shark.cache=tachyon" and "shark.cache=memory",
have the same ser/de overhead. Shark loads data from outsize of the process
in Tachyon mode with the following benefits:


   - In-memory data sharing across multiple Shark instances (i.e. stronger
   isolation)
   - Instant recovery of in-memory tables
   - Reduce heap size => faster GC in shark
   - If the table is larger than the memory size, only the hot columns will
   be cached in memory

from http://tachyon-project.org/master/Running-Shark-on-Tachyon.html and
https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon

Haoyuan


On Tue, Jul 8, 2014 at 9:58 AM, Aaron Davidson <ilikerps@gmail.com> wrote:

> Shark's in-memory format is already serialized (it's compressed and
> column-based).
>
>
> On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <mridul@gmail.com>
> wrote:
>
> > You are ignoring serde costs :-)
> >
> > - Mridul
> >
> > On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> > > Tachyon should only be marginally less performant than memory_only,
> > because
> > > we mmap the data from Tachyon's ramdisk. We do not have to, say,
> transfer
> > > the data over a pipe from Tachyon; we can directly read from the
> buffers
> > in
> > > the same way that Shark reads from its in-memory columnar format.
> > >
> > >
> > >
> > > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <liqingyang1985@gmail.com>
> > > wrote:
> > >
> > >> hi, when i create a table, i can point the cache strategy using
> > >> shark.cache,
> > >> i think "shark.cache=memory_only"  means data are managed by spark,
> and
> > >> data are in the same jvm with excutor;   while  "shark.cache=tachyon"
> > >>  means  data are managed by tachyon which is off heap, and data are
> not
> > in
> > >> the same jvm with excutor,  so spark will load data from tachyon for
> > each
> > >> query sql , so,  is  tachyon less efficient than memory_only cache
> > strategy
> > >>  ?
> > >> if yes, can we let spark load all data once from tachyon  for all sql
> > query
> > >>  if i want to use tachyon cache strategy since tachyon is more HA than
> > >> memory_only ?
> > >>
> >
>



-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/

--001a11c3253a354e0c04fdb1cbbc--

From dev-return-8238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 17:54:27 2014
Return-Path: <dev-return-8238-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B91EB11A55
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 17:54:27 +0000 (UTC)
Received: (qmail 37241 invoked by uid 500); 8 Jul 2014 17:54:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37184 invoked by uid 500); 8 Jul 2014 17:54:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37172 invoked by uid 99); 8 Jul 2014 17:54:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 17:54:26 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 17:54:25 +0000
Received: by mail-wg0-f52.google.com with SMTP id b13so6228742wgh.11
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 10:54:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=qdNkAC2oH2l6h6lSR5fPiFYOSYf6ppIk/wSc70vbb3E=;
        b=D0oF8ffqJ0GZlzUXGfNeLTvxqtARdaNKP/5vzbAW/HUoZqrcHUqJnti472v3geAZn9
         IDNPDzoK4W2A2M/x8imANv93JGWQ/KNlqk6WDnF6GgWqg8YthbiYj0g+tyRFq+CGrUuW
         qv4vFGNa7NtM1NieYPzbRbQ8W+Yqm+iNwJNWUhXCkCxLKk5jtB4sMxGMVuGqSqDfWqhX
         kF1ZbRni3u3vT2RqKY9g94Huy27yEkMjya/wOHnK6+I3fzaFS51MPudDh/YVpSBoT0XX
         XkOemQHymRsWqEGtVooCpxKl1UlEkjPOR2QiqjPiS4gE2WxJzVNwwE+GyLXsMU0tPclZ
         fTvA==
MIME-Version: 1.0
X-Received: by 10.180.109.168 with SMTP id ht8mr5571920wib.68.1404842040951;
 Tue, 08 Jul 2014 10:54:00 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Tue, 8 Jul 2014 10:54:00 -0700 (PDT)
Date: Tue, 8 Jul 2014 13:54:00 -0400
Message-ID: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
Subject: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

MLlib currently has one clustering algorithm implementation, KMeans.
It would benefit from having implementations of other clustering
algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
Clustering, and Affinity Propagation.

I recently submitted a PR [1] for a MiniBatch KMeans implementation,
and I saw an email on this list about interest in implementing Fuzzy
C-Means.

Based on Sean Owen's review of my MiniBatch KMeans code, it became
apparent that before I implement more clustering algorithms, it would
be useful to hammer out a framework to reduce code duplication and
implement a consistent API.

I'd like to gauge the interest and goals of the MLlib community:

1. Are you interested in having more clustering algorithms available?

2. Is the community interested in specifying a common framework?

Thanks!
RJ

[1] - https://github.com/apache/spark/pull/1248


-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 19:24:59 2014
Return-Path: <dev-return-8239-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1804011E60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 19:24:59 +0000 (UTC)
Received: (qmail 5780 invoked by uid 500); 8 Jul 2014 19:24:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5725 invoked by uid 500); 8 Jul 2014 19:24:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5713 invoked by uid 99); 8 Jul 2014 19:24:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 19:24:57 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [106.10.151.118] (HELO nm26-vm7.bullet.mail.sg3.yahoo.com) (106.10.151.118)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 19:24:52 +0000
Received: from [106.10.166.125] by nm26.bullet.mail.sg3.yahoo.com with NNFMP; 08 Jul 2014 19:24:30 -0000
Received: from [106.10.150.23] by tm14.bullet.mail.sg3.yahoo.com with NNFMP; 08 Jul 2014 19:24:30 -0000
Received: from [127.0.0.1] by omp1024.mail.sg3.yahoo.com with NNFMP; 08 Jul 2014 19:24:29 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 992258.51695.bm@omp1024.mail.sg3.yahoo.com
Received: (qmail 5594 invoked by uid 60001); 8 Jul 2014 19:24:29 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.co.in; s=s1024; t=1404847469; bh=XWr/dBVozevPArhfMtHV5L9JOx8luvPgQRu86hVDL6k=; h=Message-ID:Date:From:Subject:To:MIME-Version:Content-Type; b=yn252uEx9Kq3zYMA8QdjQqEsjWHtQaDSfJBjN39HR2FTkrXM+SdkHTVcUrIYNUVYIngih/EUoJPT9fLm+h/7K0E6MVs26H63jPUYItQBJGwBHYKB2lka4YPy1PyHXK1K0xRmKcV/JvVSyzNAJQCs9ZarmwVW7Bd7JRYnbdt1bYE=
X-YMail-OSG: oLW.OhQVM1mxoeqJn.GhPDrNTZqplGUJhfyhv.7ufW68BTZ
 Wj78yD2WAmykJT1hRHEDZZpHfmE8pA_mWUKz3MiflTB3fNXvLSpPSMFKWN3Z
 y.UmUTCS09N2kNlxYNN8.sWGYPOh9XaSUloBdYXykuXOr2nZ_a3CNrVfPPq4
 tIEUu7NVih2yuZG6ZHajoVCIowJ12EVriZAwCuRv94ODEZMapCYYiyc.3EeE
 3pdFqApFI._zScICHY1s9hQExfm8LH3i8z5B03Roi35EeNYsMcjQue2wsK_m
 ak.NA3WtwDRoiIR7ZK.KVWVD7HYMG.ihKw0Vl.Q1d8o7_B5L5gMEyJHbq.qd
 508.DLGfbDXBv28bvr1civiO7QSmEcGF3v1e7QLY5fLBLmvNN4GNdZS9NCHC
 uquEfS.UX1mLF3506eFlakejx8qvD34j_9DqNzGorAXU6.cMVxeuyXQZ_1VB
 IurCuC27mz4Mf_Gyj7L9ViSV8v0kKEQlL0.2QKYrxUKz4kyA3PozoWovqwf4
 sEn.9r1qXYZb59N7_x8Uo0_irxGmLA3XJ_e00Cj4dgKtFVx4jNp79yGM_WEY
 xgvtS8W7Ual4BdiYRfdaQPRq8Gyl4RlqkFIn3HN4p..ry85CVnxEcFi21Ija
 WVWMqhy2iesyXAtn8cwX2LXFbO2o-
Received: from [122.176.213.65] by web194602.mail.sg3.yahoo.com via HTTP; Wed, 09 Jul 2014 03:24:29 SGT
X-Rocket-MIMEInfo: 002.001,SGkgQWxsCgpJIHJlYWQgc29tZXdoZXJlIHRoYXQgQ2xvdWRlcmEgYW5ub3VuY2VkIEhpdmUgb24gU3BhcmssIHNpbmNlIEFtcExhYiBhbHJlYWR5IGhhdmUgU2hhcmsuIEkgd2FzIHRyeWluZyB0byB1bmRlcnN0YW5kIGlzIGl0IHJlYnJhbmRpbmcgb2YgU2hhcmsgb3IgdGhleSBhcmUgcGxhbm5pbmcgc29tZXRoaW5nIG5ldyBhbHRvZ2V0aGVyLgoKUGxlYXNlIHN1Z2dlc3QuCgotLcKgCkFuaXNoIFNuZWgKIkV4cGVyaWVuY2UgaXMgdGhlIGJlc3QgdGVhY2hlci4iCmh0dHA6Ly9pbi5saW5rZWRpbi5jb20vaW4BMAEBAQE-
X-Mailer: YahooMailAndroidMobile/4.0.2 YahooMailWebService/0.8.191.1
Message-ID: <1404847469.78391.YahooMailAndroidMobile@web194602.mail.sg3.yahoo.com>
Date: Wed, 9 Jul 2014 03:24:29 +0800
From: "anishsneh@yahoo.co.in" <anishsneh@yahoo.co.in>
Subject: Cloudera's Hive on Spark vs AmpLab's Shark
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="937119039-1449588825-1404847469=:78391"
X-Virus-Checked: Checked by ClamAV on apache.org

--937119039-1449588825-1404847469=:78391
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Hi All=0A=0AI read somewhere that Cloudera announced Hive on Spark, since A=
mpLab already have Shark. I was trying to understand is it rebranding of Sh=
ark or they are planning something new altogether.=0A=0APlease suggest.=0A=
=0A--=A0=0AAnish Sneh=0A"Experience is the best teacher."=0Ahttp://in.linke=
din.com/in/anishsneh=0A=0A
--937119039-1449588825-1404847469=:78391--

From dev-return-8240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 19:27:44 2014
Return-Path: <dev-return-8240-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E4B611E7A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 19:27:44 +0000 (UTC)
Received: (qmail 13456 invoked by uid 500); 8 Jul 2014 19:27:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13393 invoked by uid 500); 8 Jul 2014 19:27:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13381 invoked by uid 99); 8 Jul 2014 19:27:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 19:27:42 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [106.10.151.17] (HELO nm22.bullet.mail.sg3.yahoo.com) (106.10.151.17)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 19:27:39 +0000
Received: from [106.10.166.114] by nm22.bullet.mail.sg3.yahoo.com with NNFMP; 08 Jul 2014 19:27:12 -0000
Received: from [106.10.151.203] by tm3.bullet.mail.sg3.yahoo.com with NNFMP; 08 Jul 2014 19:27:12 -0000
Received: from [127.0.0.1] by omp1015.mail.sg3.yahoo.com with NNFMP; 08 Jul 2014 19:27:12 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 937256.7628.bm@omp1015.mail.sg3.yahoo.com
Received: (qmail 48265 invoked by uid 60001); 8 Jul 2014 19:27:12 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.co.in; s=s1024; t=1404847632; bh=1r9rRErRsa2Wmt4A2+27N965AhRjjaFkDjhySZUNNg0=; h=Message-ID:Date:From:Subject:To:MIME-Version:Content-Type; b=sKBLUBg67QxPH1R6qWNJOAh2G8Hk95/WOFgsYvu4BRA7aE2QUWIwRyj0ZhntwinTwzdxCYtg1C+HaZnEHLM9U91Bc6zlZs+uVlSyQnBvJc8oHhww32D2vmjuXqc2Y9bD8f+MDjmHukJwBr0ADK/+/89Ty0lbyWa5CdYBjZwEz4w=
X-YMail-OSG: 6NtIS1MVM1kmm0AzsetLkQIf5isYn1RMMnRvfNGlJV3dbZL
 gO_0MufG_jIDv3MSrULZNfNDpZGwBWie4htsZdccTSOo3tk36UXRG43jhOFS
 X29H4qAKbmzSiRxkVpG48JnYNu5LCMFePZw9O1HY214.XXmC3zdDPOPKU9kd
 sHEoPS8IswHaPmIUk5ZXe3HWeeRlcg97qhQnkCYgmBXPyDejecdbgQSTIrrn
 7YbTIfRx7Pht3PuzNwIDX02S7PhJ7MHB1.uB_lqKxxTminOjLhqJvgfpSTxR
 EMTVoKH1ZHI6LBTngp4d73kSpVx9672q81veGsRGnOzbuDQavZH5gBMJ2GeD
 OOovzz2wvR5f_vqXg_SZOx8VCnUmBL_ObEQPsJNQzNKJ5NJ2bh9uHPfkrJ09
 NiLaYFYpOCwFeGudUhDk74pKIX0NvBF2sfYJPlGmRH3aOhBJ0H4LhCJgUX3Q
 XA4WObfOtZfhyjp7.dk_F6Ip6mjYrnMw5zFxoAc3w8bHrVwvmMVZLBJQN7zp
 L9yKpFMaXxsEOd9KazAj_3bNGP6NnR9gnXiPn3HVOulfYb_CxfZRurfRRudv
 Lc6jFCkv53xE4i7POrfrmpZCxU7t73v851lotuXbv1Bi7Y6UJf7o555gZWh4
 qzK0rSlLgPSivdF_AeCMVrVhn8Q--
Received: from [122.176.213.65] by web194605.mail.sg3.yahoo.com via HTTP; Wed, 09 Jul 2014 03:27:12 SGT
X-Rocket-MIMEInfo: 002.001,SGkgQWxsCgpNeSBhcG9sb2dpZXMgZm9yIHZlcnkgYmFzaWMgcXVlc3Rpb24sIGRvIHdlIGhhdmUgZnVsbCBzdXBwb3J0IG9mIGRhdGEgbG9jYWxpdHkgaW4gU3BhcmsgTWFwUmVkdWNlLgoKUGxlYXNlIHN1Z2dlc3QuCgotLcKgCkFuaXNoIFNuZWgKIkV4cGVyaWVuY2UgaXMgdGhlIGJlc3QgdGVhY2hlci4iCmh0dHA6Ly9pbi5saW5rZWRpbi5jb20vaW4vYW5pc2hzbmVoCgoBMAEBAQE-
X-Mailer: YahooMailAndroidMobile/4.0.2 YahooMailWebService/0.8.191.1
Message-ID: <1404847632.48111.YahooMailAndroidMobile@web194605.mail.sg3.yahoo.com>
Date: Wed, 9 Jul 2014 03:27:12 +0800
From: "anishsneh@yahoo.co.in" <anishsneh@yahoo.co.in>
Subject: Data Locality In Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="1564683395-81434112-1404847632=:48111"
X-Virus-Checked: Checked by ClamAV on apache.org

--1564683395-81434112-1404847632=:48111
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Hi All=0A=0AMy apologies for very basic question, do we have full support o=
f data locality in Spark MapReduce.=0A=0APlease suggest.=0A=0A--=A0=0AAnish=
 Sneh=0A"Experience is the best teacher."=0Ahttp://in.linkedin.com/in/anish=
sneh=0A=0A
--1564683395-81434112-1404847632=:48111--

From dev-return-8241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 19:30:24 2014
Return-Path: <dev-return-8241-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B0ED11E96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 19:30:24 +0000 (UTC)
Received: (qmail 19894 invoked by uid 500); 8 Jul 2014 19:30:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19856 invoked by uid 500); 8 Jul 2014 19:30:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19845 invoked by uid 99); 8 Jul 2014 19:30:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 19:30:23 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 19:30:22 +0000
Received: by mail-qa0-f45.google.com with SMTP id s7so1253590qap.4
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 12:29:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=JoXTCVHv25wFeIPIaFE8A0OYUuDErCCovvMkqFmYHj4=;
        b=CTGCEa7fGQ9gRNBSNfkMSl/yFlHLxmOWBLBIr3+kHszH4tSdNZZDSyStizda7UuTkS
         G+9aoNi7AblOaKb5tyyL8Lw9ZE2pgDafc/Xm5XFCkkOJdJ+N5X3VQXFUkmDlJ+sA+CRo
         FxiL/MXqHC6ZTJ3xYb8fdukEqy+IyvCXr4gLqjs6j6gdD5ySC2GsW8VuhNzYmXd6jtYS
         6gjY9Yt3aX754gVB3CE5WbKp8y5Ga2bVBg2cVXBVMmnI1YnmG0/CZD1DfBHb/ajx0huw
         F/27AhcDCzVcyL1x4Frsec7qhavQ1hIp87Xp75K5Uj8mLtxLrvvGwUsJ8e7RDpc75aTN
         hDrw==
X-Gm-Message-State: ALoCoQmxKeIkZIPLsP9SBe8Cw+Sr5QAAH4f7ggPPlwGKnG64BL9IUo3qSbXPoaYqdR27TXjT8tge
X-Received: by 10.140.104.161 with SMTP id a30mr60092528qgf.19.1404847797396;
 Tue, 08 Jul 2014 12:29:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Tue, 8 Jul 2014 12:29:37 -0700 (PDT)
In-Reply-To: <1404847469.78391.YahooMailAndroidMobile@web194602.mail.sg3.yahoo.com>
References: <1404847469.78391.YahooMailAndroidMobile@web194602.mail.sg3.yahoo.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 8 Jul 2014 12:29:37 -0700
Message-ID: <CAPh_B=ZP2ge_mWzfW9D9gaNjCCvZx-QMNAEcoWD4EEs9Fv+5zw@mail.gmail.com>
Subject: Re: Cloudera's Hive on Spark vs AmpLab's Shark
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11354864ed774504fdb39d91
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11354864ed774504fdb39d91
Content-Type: text/plain; charset=UTF-8

This blog post probably clarifies a lot of things:
http://databricks.com/blog/2014/07/01/shark-spark-sql-hive-on-spark-and-the-future-of-sql-on-spark.html




On Tue, Jul 8, 2014 at 12:24 PM, anishsneh@yahoo.co.in <
anishsneh@yahoo.co.in> wrote:

> Hi All
>
> I read somewhere that Cloudera announced Hive on Spark, since AmpLab
> already have Shark. I was trying to understand is it rebranding of Shark or
> they are planning something new altogether.
>
> Please suggest.
>
> --
> Anish Sneh
> "Experience is the best teacher."
> http://in.linkedin.com/in/anishsneh
>
>

--001a11354864ed774504fdb39d91--

From dev-return-8242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 20:02:07 2014
Return-Path: <dev-return-8242-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E2C611071
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 20:02:07 +0000 (UTC)
Received: (qmail 11802 invoked by uid 500); 8 Jul 2014 20:02:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11746 invoked by uid 500); 8 Jul 2014 20:02:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11733 invoked by uid 99); 8 Jul 2014 20:02:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:02:06 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hector.yee@gmail.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:02:04 +0000
Received: by mail-we0-f179.google.com with SMTP id w62so6440006wes.24
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 13:01:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=Y2pqdEroTNgGI6xODYJRlTsoJ5jcqx4Vd/5AsX5vO40=;
        b=jBpBvJCkBEITsy7Wj1SYVCVU34jrlzxavHl4U/uw5MKl9vUoTpubAP2jgJl1jQFa/1
         FSKd87VCiIvH6BNRmVNVWnJcsoXzE5V2ZMyYA3yX2JV258kSSsSTO5pBN14seCHUfMCg
         3R/N4hHuUV6amzPOzVOWBExkNqUOFbgY20WRGkHE1ussEhOUmtkfZuk6Lf4jVVqJy7O8
         k2V8gFcPDnvpcPqyIUMtdyoW09JZWH4SuFU2Ze6gCBTC/D9ZC983pAmKXac17FbbbGdZ
         D9Bz6sunJceYgCoAq/OU0SnPDYlmvikXB6LUuDmSFz1CFeFOiGszLPvg5I4ndbkzu+KJ
         ZMFw==
X-Received: by 10.195.17.164 with SMTP id gf4mr42521133wjd.45.1404849700030;
 Tue, 08 Jul 2014 13:01:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.30.79 with HTTP; Tue, 8 Jul 2014 13:01:19 -0700 (PDT)
In-Reply-To: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
From: Hector Yee <hector.yee@gmail.com>
Date: Tue, 8 Jul 2014 13:01:19 -0700
Message-ID: <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e016817de55529304fdb40fa9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e016817de55529304fdb40fa9
Content-Type: text/plain; charset=UTF-8

I would say for bigdata applications the most useful would be hierarchical
k-means with back tracking and the ability to support k nearest centroids.


On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com> wrote:

> Hi all,
>
> MLlib currently has one clustering algorithm implementation, KMeans.
> It would benefit from having implementations of other clustering
> algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> Clustering, and Affinity Propagation.
>
> I recently submitted a PR [1] for a MiniBatch KMeans implementation,
> and I saw an email on this list about interest in implementing Fuzzy
> C-Means.
>
> Based on Sean Owen's review of my MiniBatch KMeans code, it became
> apparent that before I implement more clustering algorithms, it would
> be useful to hammer out a framework to reduce code duplication and
> implement a consistent API.
>
> I'd like to gauge the interest and goals of the MLlib community:
>
> 1. Are you interested in having more clustering algorithms available?
>
> 2. Is the community interested in specifying a common framework?
>
> Thanks!
> RJ
>
> [1] - https://github.com/apache/spark/pull/1248
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>



-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*

--089e016817de55529304fdb40fa9--

From dev-return-8243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 20:14:13 2014
Return-Path: <dev-return-8243-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B3303110F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 20:14:13 +0000 (UTC)
Received: (qmail 46604 invoked by uid 500); 8 Jul 2014 20:14:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46540 invoked by uid 500); 8 Jul 2014 20:14:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46528 invoked by uid 99); 8 Jul 2014 20:14:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:14:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:14:08 +0000
Received: by mail-qc0-f174.google.com with SMTP id x13so5524871qcv.19
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 13:13:47 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Mbb3wYDS3chCr0mje5uzL7+pZLjEoIywk4R0uKXRUVQ=;
        b=ImreJ963V0JEB4kCw/NgItb0Y0WLskr/3Hxq97hOS0a+xmxo288R29YChnOS6JVRXX
         +uobJYY/HYP/mtDO0rnNpFQCFGhB5fLXRYODYngHMAIIrc0xLpu/5hgBE8R6+b69/0/h
         4TkscWcnlJM2t2n/c3EGijsbnqT/E2tCte5K6u8B8Jx6GPStZVhsmfSxolVhRFCoCFAP
         enafKfTTFaesU0pBQLnVh59Br/plE+059MKaUrzTSx+5ewfBzKB8WdTydd4EKr+mxNyF
         YP0NK1ARaYvNcrmToLcLAwXYd/H13ebl5CVR4BqeEdeak/lha6eYScrAf/mz4oweYWsI
         dvzw==
X-Gm-Message-State: ALoCoQnyIS0fm0qcSFrZzer66fhSAqPeOBuEwWZTlgSIg93GBN4v5WOdtRPe8ECcdPjEL9LfCQTY
MIME-Version: 1.0
X-Received: by 10.140.48.161 with SMTP id o30mr60122971qga.68.1404850427849;
 Tue, 08 Jul 2014 13:13:47 -0700 (PDT)
Received: by 10.140.30.53 with HTTP; Tue, 8 Jul 2014 13:13:47 -0700 (PDT)
In-Reply-To: <1404847632.48111.YahooMailAndroidMobile@web194605.mail.sg3.yahoo.com>
References: <1404847632.48111.YahooMailAndroidMobile@web194605.mail.sg3.yahoo.com>
Date: Tue, 8 Jul 2014 13:13:47 -0700
Message-ID: <CACBYxKJcLfAKiifWHpgJP1qQ0t=nZfCASnoh2kCn-k3wsO8zKQ@mail.gmail.com>
Subject: Re: Data Locality In Spark
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135086eb71edc04fdb43a80
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135086eb71edc04fdb43a80
Content-Type: text/plain; charset=UTF-8

Hi Anish,

Spark, like MapReduce, makes an effort to schedule tasks on the same nodes
and racks that the input blocks reside on.

-Sandy


On Tue, Jul 8, 2014 at 12:27 PM, anishsneh@yahoo.co.in <
anishsneh@yahoo.co.in> wrote:

> Hi All
>
> My apologies for very basic question, do we have full support of data
> locality in Spark MapReduce.
>
> Please suggest.
>
> --
> Anish Sneh
> "Experience is the best teacher."
> http://in.linkedin.com/in/anishsneh
>
>

--001a1135086eb71edc04fdb43a80--

From dev-return-8244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 20:19:36 2014
Return-Path: <dev-return-8244-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E394611180
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 20:19:36 +0000 (UTC)
Received: (qmail 72841 invoked by uid 500); 8 Jul 2014 20:19:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72784 invoked by uid 500); 8 Jul 2014 20:19:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72772 invoked by uid 99); 8 Jul 2014 20:19:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:19:36 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:19:33 +0000
Received: by mail-we0-f182.google.com with SMTP id q59so6547763wes.27
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 13:19:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=qHQ1eQrUeSTMMbhaJyumeX60ZrgXvD57oyTZrtpSJ7E=;
        b=C+wZhHsSZZ3IwtwLyhA5lMRY8sIaARybGP0cB+Q8YiumKU8+u/aXHIsMzwDLpaZFQx
         gYeP/WtHUtLAxIdu24DFmpaLuFiU19jws+/+16F8X4QATNVR8wRF+fSwJC4WaVcdFSbp
         OYwotaW6er9OuJIr6Qhx1pUceS5r8ftygu9apinaaiTaVqrJyRDufGEkNxe15c6ONew7
         Kz4T70d/tZQHjRC6UVA5CByvenug1zz4jk3NnEWKv8Dk63Cp38J8EO0WESVcXIF76lv0
         mLwMNDVsqbina1487ZSwetVGoIC0XzAXYlh98RopJkMWJhp6phXllIE4hB/V0gSKBJHo
         5IWg==
MIME-Version: 1.0
X-Received: by 10.194.48.8 with SMTP id h8mr6617524wjn.106.1404850749753; Tue,
 08 Jul 2014 13:19:09 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Tue, 8 Jul 2014 13:19:09 -0700 (PDT)
In-Reply-To: <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
	<CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
Date: Tue, 8 Jul 2014 16:19:09 -0400
Message-ID: <CADtDQQLJZg+dnnW=zpj3XkZf1xMvu9WJ2h=mLYpyiPFj33yzPg@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba97554e6cf0304fdb44d71
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba97554e6cf0304fdb44d71
Content-Type: text/plain; charset=UTF-8

Thanks, Hector! Your feedback is useful.

On Tuesday, July 8, 2014, Hector Yee <hector.yee@gmail.com> wrote:

> I would say for bigdata applications the most useful would be hierarchical
> k-means with back tracking and the ability to support k nearest centroids.
>
>
> On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com
> <javascript:;>> wrote:
>
> > Hi all,
> >
> > MLlib currently has one clustering algorithm implementation, KMeans.
> > It would benefit from having implementations of other clustering
> > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> > Clustering, and Affinity Propagation.
> >
> > I recently submitted a PR [1] for a MiniBatch KMeans implementation,
> > and I saw an email on this list about interest in implementing Fuzzy
> > C-Means.
> >
> > Based on Sean Owen's review of my MiniBatch KMeans code, it became
> > apparent that before I implement more clustering algorithms, it would
> > be useful to hammer out a framework to reduce code duplication and
> > implement a consistent API.
> >
> > I'd like to gauge the interest and goals of the MLlib community:
> >
> > 1. Are you interested in having more clustering algorithms available?
> >
> > 2. Is the community interested in specifying a common framework?
> >
> > Thanks!
> > RJ
> >
> > [1] - https://github.com/apache/spark/pull/1248
> >
> >
> > --
> > em rnowling@gmail.com <javascript:;>
> > c 954.496.2314
> >
>
>
>
> --
> Yee Yang Li Hector <http://google.com/+HectorYee>
> *google.com/+HectorYee <http://google.com/+HectorYee>*
>


-- 
em rnowling@gmail.com
c 954.496.2314

--047d7ba97554e6cf0304fdb44d71--

From dev-return-8245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 20:24:33 2014
Return-Path: <dev-return-8245-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8F047111C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 20:24:33 +0000 (UTC)
Received: (qmail 94655 invoked by uid 500); 8 Jul 2014 20:24:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94588 invoked by uid 500); 8 Jul 2014 20:24:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94575 invoked by uid 99); 8 Jul 2014 20:24:32 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:24:32 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of dlieu.7@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:24:30 +0000
Received: by mail-oa0-f45.google.com with SMTP id o6so7079432oag.18
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 13:24:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=rsNBdw1NK/K1V0zS2VFXawoA77j/+NT1zutI/ak1xw8=;
        b=YZ3Wt9XCZqNyC3OxRPjgI7nSskXlVtYqFU9RLZ2g+LrU/qLFni4kRm+Jf5zVKev7rW
         jBXg0KZ8GhCB9VPnjq6vFJhAm6c1VnqEUedkwwTzPxx22IdBgoIFmlKfTsFqvwzIcYCm
         7sf0fINXGS7Dt1P5Tn1kzePBYPcKuFlQqMMomP5qg7yIndHIGZaFnJMMla6R9nAHx/AC
         8mcmK5aATdkj2pmcX/XVn4D46dTvshLGOgydziGtPgg5bLPCgqxHfN1awcFyi2xfzvNz
         d6Hs8EIvkDvk0EjAbSSZZ+tXOvsUKG1QNekabZ+dH7Wa6WAfojPKmq0npGGM7HtxUERQ
         l4aA==
MIME-Version: 1.0
X-Received: by 10.182.241.130 with SMTP id wi2mr42136749obc.27.1404851044285;
 Tue, 08 Jul 2014 13:24:04 -0700 (PDT)
Received: by 10.76.35.134 with HTTP; Tue, 8 Jul 2014 13:24:04 -0700 (PDT)
In-Reply-To: <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
	<CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
Date: Tue, 8 Jul 2014 13:24:04 -0700
Message-ID: <CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1f7e2753c9b04fdb45f09
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1f7e2753c9b04fdb45f09
Content-Type: text/plain; charset=UTF-8

Hector, could you share the references for hierarchical K-means? thanks.


On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com> wrote:

> I would say for bigdata applications the most useful would be hierarchical
> k-means with back tracking and the ability to support k nearest centroids.
>
>
> On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com> wrote:
>
> > Hi all,
> >
> > MLlib currently has one clustering algorithm implementation, KMeans.
> > It would benefit from having implementations of other clustering
> > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> > Clustering, and Affinity Propagation.
> >
> > I recently submitted a PR [1] for a MiniBatch KMeans implementation,
> > and I saw an email on this list about interest in implementing Fuzzy
> > C-Means.
> >
> > Based on Sean Owen's review of my MiniBatch KMeans code, it became
> > apparent that before I implement more clustering algorithms, it would
> > be useful to hammer out a framework to reduce code duplication and
> > implement a consistent API.
> >
> > I'd like to gauge the interest and goals of the MLlib community:
> >
> > 1. Are you interested in having more clustering algorithms available?
> >
> > 2. Is the community interested in specifying a common framework?
> >
> > Thanks!
> > RJ
> >
> > [1] - https://github.com/apache/spark/pull/1248
> >
> >
> > --
> > em rnowling@gmail.com
> > c 954.496.2314
> >
>
>
>
> --
> Yee Yang Li Hector <http://google.com/+HectorYee>
> *google.com/+HectorYee <http://google.com/+HectorYee>*
>

--001a11c1f7e2753c9b04fdb45f09--

From dev-return-8246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 20:25:05 2014
Return-Path: <dev-return-8246-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 41884111C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 20:25:05 +0000 (UTC)
Received: (qmail 95812 invoked by uid 500); 8 Jul 2014 20:25:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95754 invoked by uid 500); 8 Jul 2014 20:25:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95741 invoked by uid 99); 8 Jul 2014 20:25:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:25:04 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:25:01 +0000
Received: by mail-qg0-f50.google.com with SMTP id j5so5596356qga.9
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 13:24:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=1N56ehzxhcriJSi4/tUwMHPIqiTFyaU7Bgp/Nib9K18=;
        b=KDSlws/AEmD8pp6bYz+Fh6bf1ehO67mR83rP3haJbksOoev9z+6vKcoBk+lhompgC8
         iQi5UgswB3BW6q+X2/1Cz+9GwqNk1/J6bqIkQPECQAppjK2nq5p5EMRbAsDUXZsAcTpV
         XRB/CBlP6grAOrje78OJ6MtL28n3c2293DYkw11jXqwcnmtOV7nw6DCzgGhIWEeUHFEt
         X766doeiUcc8FOaBMQrxqz0fJ5jsYXCOOtSAPWWZHPRs82v4pxjdK2PsinBFfk0+HqXn
         oqq4vzSsUU3QnhU7C4cRa0HrANrBn6pEDoRKYo/oYmI1HpwZKHDlEpQG+GmwwT9Ifrgc
         DCiQ==
X-Gm-Message-State: ALoCoQm2jJ8RpoCnAe1WxW/gbLmCPPhdeCQe7y2cl/iIpZvIdknBFDXUKIQ5zjAdk0hSGSqRseZe
MIME-Version: 1.0
X-Received: by 10.140.40.81 with SMTP id w75mr60062419qgw.112.1404851077206;
 Tue, 08 Jul 2014 13:24:37 -0700 (PDT)
Received: by 10.140.30.53 with HTTP; Tue, 8 Jul 2014 13:24:37 -0700 (PDT)
In-Reply-To: <CADtDQQLJZg+dnnW=zpj3XkZf1xMvu9WJ2h=mLYpyiPFj33yzPg@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
	<CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
	<CADtDQQLJZg+dnnW=zpj3XkZf1xMvu9WJ2h=mLYpyiPFj33yzPg@mail.gmail.com>
Date: Tue, 8 Jul 2014 13:24:37 -0700
Message-ID: <CACBYxKKmFZaLcFUQPtOt+ntPNs=jKyx1etP=+aG0vtHc=jBchA@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c151546b668604fdb461a1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c151546b668604fdb461a1
Content-Type: text/plain; charset=UTF-8

Having a common framework for clustering makes sense to me.  While we
should be careful about what algorithms we include, having solid
implementations of minibatch clustering and hierarchical clustering seems
like a worthwhile goal, and we should reuse as much code and APIs as
reasonable.


On Tue, Jul 8, 2014 at 1:19 PM, RJ Nowling <rnowling@gmail.com> wrote:

> Thanks, Hector! Your feedback is useful.
>
> On Tuesday, July 8, 2014, Hector Yee <hector.yee@gmail.com> wrote:
>
> > I would say for bigdata applications the most useful would be
> hierarchical
> > k-means with back tracking and the ability to support k nearest
> centroids.
> >
> >
> > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com
> > <javascript:;>> wrote:
> >
> > > Hi all,
> > >
> > > MLlib currently has one clustering algorithm implementation, KMeans.
> > > It would benefit from having implementations of other clustering
> > > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> > > Clustering, and Affinity Propagation.
> > >
> > > I recently submitted a PR [1] for a MiniBatch KMeans implementation,
> > > and I saw an email on this list about interest in implementing Fuzzy
> > > C-Means.
> > >
> > > Based on Sean Owen's review of my MiniBatch KMeans code, it became
> > > apparent that before I implement more clustering algorithms, it would
> > > be useful to hammer out a framework to reduce code duplication and
> > > implement a consistent API.
> > >
> > > I'd like to gauge the interest and goals of the MLlib community:
> > >
> > > 1. Are you interested in having more clustering algorithms available?
> > >
> > > 2. Is the community interested in specifying a common framework?
> > >
> > > Thanks!
> > > RJ
> > >
> > > [1] - https://github.com/apache/spark/pull/1248
> > >
> > >
> > > --
> > > em rnowling@gmail.com <javascript:;>
> > > c 954.496.2314
> > >
> >
> >
> >
> > --
> > Yee Yang Li Hector <http://google.com/+HectorYee>
> > *google.com/+HectorYee <http://google.com/+HectorYee>*
> >
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>

--001a11c151546b668604fdb461a1--

From dev-return-8247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 20:32:39 2014
Return-Path: <dev-return-8247-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A364E11236
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 20:32:39 +0000 (UTC)
Received: (qmail 22738 invoked by uid 500); 8 Jul 2014 20:32:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22682 invoked by uid 500); 8 Jul 2014 20:32:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22669 invoked by uid 99); 8 Jul 2014 20:32:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:32:38 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hector.yee@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:32:34 +0000
Received: by mail-wi0-f172.google.com with SMTP id hi2so1657575wib.5
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 13:32:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=kec1D8Lblagu2eqqfb7oRI/NyvLbSbogpXGLch+wrDg=;
        b=dInBJuSUPvKRNMhVzSqObKHWMVYwmuly4kFZPuv93bubvR8iiichEoa4rltBpjWFKF
         PT6r3W0MB416YrNEYfevOkAjALPDRYkThM5ueRESRPAgsNAKLD+33wgNmQuUQuCZ6Ae7
         XGfB6GjhAllROLlw1JSZ4uzGWONj7nkXkiJ/R+APS1eMloLgz6zB5wxf0KkE1HNcjVjk
         tNi3KLXD4dZ2TU8QBZp47SiET4+2hVnbAyGREeI2jzkXCO0RtyOFhjQsoT49qun8n5ZR
         Vev4IJ8674GkKM1/Ob7WK5hxPSwLOCRpLGV7kpH6l8wF2SL14/IRY8U/0gdNFIfczcdK
         lTNA==
X-Received: by 10.180.198.116 with SMTP id jb20mr6344065wic.59.1404851533331;
 Tue, 08 Jul 2014 13:32:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.30.79 with HTTP; Tue, 8 Jul 2014 13:31:53 -0700 (PDT)
In-Reply-To: <CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
 <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com> <CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
From: Hector Yee <hector.yee@gmail.com>
Date: Tue, 8 Jul 2014 13:31:53 -0700
Message-ID: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b66f9199b459f04fdb47c64
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b66f9199b459f04fdb47c64
Content-Type: text/plain; charset=UTF-8

No idea, never looked it up. Always just implemented it as doing k-means
again on each cluster.

FWIW standard k-means with euclidean distance has problems too with some
dimensionality reduction methods. Swapping out the distance metric with
negative dot or cosine may help.

Other more useful clustering would be hierarchical SVD. The reason why I
like hierarchical clustering is it makes for faster inference especially
over billions of users.


On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:

> Hector, could you share the references for hierarchical K-means? thanks.
>
>
> On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com> wrote:
>
> > I would say for bigdata applications the most useful would be
> hierarchical
> > k-means with back tracking and the ability to support k nearest
> centroids.
> >
> >
> > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com> wrote:
> >
> > > Hi all,
> > >
> > > MLlib currently has one clustering algorithm implementation, KMeans.
> > > It would benefit from having implementations of other clustering
> > > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> > > Clustering, and Affinity Propagation.
> > >
> > > I recently submitted a PR [1] for a MiniBatch KMeans implementation,
> > > and I saw an email on this list about interest in implementing Fuzzy
> > > C-Means.
> > >
> > > Based on Sean Owen's review of my MiniBatch KMeans code, it became
> > > apparent that before I implement more clustering algorithms, it would
> > > be useful to hammer out a framework to reduce code duplication and
> > > implement a consistent API.
> > >
> > > I'd like to gauge the interest and goals of the MLlib community:
> > >
> > > 1. Are you interested in having more clustering algorithms available?
> > >
> > > 2. Is the community interested in specifying a common framework?
> > >
> > > Thanks!
> > > RJ
> > >
> > > [1] - https://github.com/apache/spark/pull/1248
> > >
> > >
> > > --
> > > em rnowling@gmail.com
> > > c 954.496.2314
> > >
> >
> >
> >
> > --
> > Yee Yang Li Hector <http://google.com/+HectorYee>
> > *google.com/+HectorYee <http://google.com/+HectorYee>*
> >
>



-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*

--047d7b66f9199b459f04fdb47c64--

From dev-return-8248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 20:50:25 2014
Return-Path: <dev-return-8248-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF6D7112B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 20:50:25 +0000 (UTC)
Received: (qmail 67438 invoked by uid 500); 8 Jul 2014 20:50:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67380 invoked by uid 500); 8 Jul 2014 20:50:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67359 invoked by uid 99); 8 Jul 2014 20:50:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:50:24 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of dlieu.7@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 20:50:20 +0000
Received: by mail-oa0-f45.google.com with SMTP id o6so7057559oag.4
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 13:50:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=vMoaEomOSJEXpIU4kNQxQ/OxU/ZsR6cOyaqNxbgiX/A=;
        b=nDd89vmX80YTkUACg0TjJhWQ1t/7HUK89BedsuPsjizGtckn4gxSzja2T/P95F1uZM
         snMAK1GB+7eynSmKEwjyIaHPBHRdW36ywCMzoxMh3NpEtC4+ToYHqVFNX9xBfbXFDHtg
         +SVoGXZ+k+ZWlQQJXCcsfDJMdgUnOcFfFzMte9DIXmUkLCzdhANWR6YecRgSWekhNEzB
         e7wdkt1okcEpOz8oHZYh4+WeE4pZvTTY+3eStsHe1dTaHW3zuJ9N17xM6ak/+/DeTw8s
         ILiyVyupE7KCq78hKqfnUcsS5Ep6ed6Ht0aMrSbMfdQghfGyLlIBC0lequGj9eCGJx/G
         SEaA==
MIME-Version: 1.0
X-Received: by 10.182.133.69 with SMTP id pa5mr20039774obb.2.1404852600055;
 Tue, 08 Jul 2014 13:50:00 -0700 (PDT)
Received: by 10.76.35.134 with HTTP; Tue, 8 Jul 2014 13:50:00 -0700 (PDT)
In-Reply-To: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
	<CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
	<CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
	<CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
Date: Tue, 8 Jul 2014 13:50:00 -0700
Message-ID: <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: Dmitriy Lyubimov <dlieu.7@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8ff1ce12302c7904fdb4bcc1
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1ce12302c7904fdb4bcc1
Content-Type: text/plain; charset=UTF-8

sure. more interesting problem here is choosing k at each level. Kernel
methods seem to be most promising.


On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com> wrote:

> No idea, never looked it up. Always just implemented it as doing k-means
> again on each cluster.
>
> FWIW standard k-means with euclidean distance has problems too with some
> dimensionality reduction methods. Swapping out the distance metric with
> negative dot or cosine may help.
>
> Other more useful clustering would be hierarchical SVD. The reason why I
> like hierarchical clustering is it makes for faster inference especially
> over billions of users.
>
>
> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> wrote:
>
> > Hector, could you share the references for hierarchical K-means? thanks.
> >
> >
> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com> wrote:
> >
> > > I would say for bigdata applications the most useful would be
> > hierarchical
> > > k-means with back tracking and the ability to support k nearest
> > centroids.
> > >
> > >
> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com>
> wrote:
> > >
> > > > Hi all,
> > > >
> > > > MLlib currently has one clustering algorithm implementation, KMeans.
> > > > It would benefit from having implementations of other clustering
> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> > > > Clustering, and Affinity Propagation.
> > > >
> > > > I recently submitted a PR [1] for a MiniBatch KMeans implementation,
> > > > and I saw an email on this list about interest in implementing Fuzzy
> > > > C-Means.
> > > >
> > > > Based on Sean Owen's review of my MiniBatch KMeans code, it became
> > > > apparent that before I implement more clustering algorithms, it would
> > > > be useful to hammer out a framework to reduce code duplication and
> > > > implement a consistent API.
> > > >
> > > > I'd like to gauge the interest and goals of the MLlib community:
> > > >
> > > > 1. Are you interested in having more clustering algorithms available?
> > > >
> > > > 2. Is the community interested in specifying a common framework?
> > > >
> > > > Thanks!
> > > > RJ
> > > >
> > > > [1] - https://github.com/apache/spark/pull/1248
> > > >
> > > >
> > > > --
> > > > em rnowling@gmail.com
> > > > c 954.496.2314
> > > >
> > >
> > >
> > >
> > > --
> > > Yee Yang Li Hector <http://google.com/+HectorYee>
> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
> > >
> >
>
>
>
> --
> Yee Yang Li Hector <http://google.com/+HectorYee>
> *google.com/+HectorYee <http://google.com/+HectorYee>*
>

--e89a8ff1ce12302c7904fdb4bcc1--

From dev-return-8249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 21:00:26 2014
Return-Path: <dev-return-8249-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0BD6511337
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 21:00:26 +0000 (UTC)
Received: (qmail 3397 invoked by uid 500); 8 Jul 2014 21:00:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3342 invoked by uid 500); 8 Jul 2014 21:00:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3330 invoked by uid 99); 8 Jul 2014 21:00:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:00:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.174 as permitted sender)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:00:21 +0000
Received: by mail-we0-f174.google.com with SMTP id u57so6520794wes.5
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 14:00:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=3fxf6G09oSSGnDcABp67ioOfTHNWeNUyBCDzPLeVB1M=;
        b=aURMFlLBhJiodPbh5Cqhu2vDKBhLLb+wJ0a4kOowswyDveX2MP0G96fy9dFQdoQLsc
         dbYcCcn9U1ybRCd5gaWjHSh4bau9VHhKp1iMqdUJbmFkE7QKTDTl+qrTMrI7KFR95RcB
         pHEf8s5IV81BSGxZknRVsNqf65RHFVujCKvKkDoFsJJ6MGVKYqyDydx0NlrfRNDTB5e+
         Nz0wxiYsD7EUwvhc+WrHpCtk61mD8wQk4pyXeQZnM1n7Kgt1AVZ4UrZzBLEz5ypo319l
         OTB8Rt6lqbgdWicDIuIO3zTaTI8wowAn45JCYQT5heU6CLwcojB4DG5syrt3cVGv1voe
         HOqg==
MIME-Version: 1.0
X-Received: by 10.194.48.8 with SMTP id h8mr6827552wjn.106.1404853199962; Tue,
 08 Jul 2014 13:59:59 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Tue, 8 Jul 2014 13:59:59 -0700 (PDT)
In-Reply-To: <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
	<CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
	<CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
	<CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
Date: Tue, 8 Jul 2014 16:59:59 -0400
Message-ID: <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

The scikit-learn implementation may be of interest:

http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.html#sklearn.cluster.Ward

It's a bottom up approach.  The pair of clusters for merging are
chosen to minimize variance.

Their code is under a BSD license so it can be used as a template.

Is something like that you were thinking Hector?

On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:
> sure. more interesting problem here is choosing k at each level. Kernel
> methods seem to be most promising.
>
>
> On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com> wrote:
>
>> No idea, never looked it up. Always just implemented it as doing k-means
>> again on each cluster.
>>
>> FWIW standard k-means with euclidean distance has problems too with some
>> dimensionality reduction methods. Swapping out the distance metric with
>> negative dot or cosine may help.
>>
>> Other more useful clustering would be hierarchical SVD. The reason why I
>> like hierarchical clustering is it makes for faster inference especially
>> over billions of users.
>>
>>
>> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
>> wrote:
>>
>> > Hector, could you share the references for hierarchical K-means? thanks.
>> >
>> >
>> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com> wrote:
>> >
>> > > I would say for bigdata applications the most useful would be
>> > hierarchical
>> > > k-means with back tracking and the ability to support k nearest
>> > centroids.
>> > >
>> > >
>> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com>
>> wrote:
>> > >
>> > > > Hi all,
>> > > >
>> > > > MLlib currently has one clustering algorithm implementation, KMeans.
>> > > > It would benefit from having implementations of other clustering
>> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
>> > > > Clustering, and Affinity Propagation.
>> > > >
>> > > > I recently submitted a PR [1] for a MiniBatch KMeans implementation,
>> > > > and I saw an email on this list about interest in implementing Fuzzy
>> > > > C-Means.
>> > > >
>> > > > Based on Sean Owen's review of my MiniBatch KMeans code, it became
>> > > > apparent that before I implement more clustering algorithms, it would
>> > > > be useful to hammer out a framework to reduce code duplication and
>> > > > implement a consistent API.
>> > > >
>> > > > I'd like to gauge the interest and goals of the MLlib community:
>> > > >
>> > > > 1. Are you interested in having more clustering algorithms available?
>> > > >
>> > > > 2. Is the community interested in specifying a common framework?
>> > > >
>> > > > Thanks!
>> > > > RJ
>> > > >
>> > > > [1] - https://github.com/apache/spark/pull/1248
>> > > >
>> > > >
>> > > > --
>> > > > em rnowling@gmail.com
>> > > > c 954.496.2314
>> > > >
>> > >
>> > >
>> > >
>> > > --
>> > > Yee Yang Li Hector <http://google.com/+HectorYee>
>> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
>> > >
>> >
>>
>>
>>
>> --
>> Yee Yang Li Hector <http://google.com/+HectorYee>
>> *google.com/+HectorYee <http://google.com/+HectorYee>*
>>



-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 21:01:27 2014
Return-Path: <dev-return-8250-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 12C9311345
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 21:01:27 +0000 (UTC)
Received: (qmail 5407 invoked by uid 500); 8 Jul 2014 21:01:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5354 invoked by uid 500); 8 Jul 2014 21:01:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5323 invoked by uid 99); 8 Jul 2014 21:01:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:01:26 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of hector.yee@gmail.com designates 74.125.82.177 as permitted sender)
Received: from [74.125.82.177] (HELO mail-we0-f177.google.com) (74.125.82.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:01:24 +0000
Received: by mail-we0-f177.google.com with SMTP id u56so6421170wes.8
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 14:01:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=XegdrDrrA4TeKUSjRTRH8oZVi2pnG8FNOIwUDjLj5Ug=;
        b=FYI3JQwN30ZvV177woRZDyV9RqzKGx+OJXUT4gs04MO7Q69IyfPZi8hxeM7NbAmVpA
         Kj3qkh1FueWEht97dgF6ZW9clKUwVOuuwbDlTv+Nwon11z8bw+Ufhw5IR+3WgC2hKahI
         mu1AC8Syo22zuodeueqSvGrq847m/zCjcOwClj5Snu6f+Np7ajAk3uWRzT32bxLWs4Op
         YBPDTE55KIZNW2lP/ZwNRZFxlVEte7FWKZqwrlC9iXEsgJxSzYvKc4catC047okH1S2e
         BMVMrHhpS7L7U4g3KS10BrZZ4gm6BjxEE0lvnKbXSjz1RXPwh0flkb+7sjMmfQII58ji
         L0TQ==
X-Received: by 10.180.104.40 with SMTP id gb8mr6477365wib.59.1404853260194;
 Tue, 08 Jul 2014 14:01:00 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.30.79 with HTTP; Tue, 8 Jul 2014 14:00:40 -0700 (PDT)
In-Reply-To: <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
 <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
 <CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
 <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com> <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
From: Hector Yee <hector.yee@gmail.com>
Date: Tue, 8 Jul 2014 14:00:40 -0700
Message-ID: <CAPi87hfys_M=2KmeoXCMG5FiRjC_XWi7KvTJbWp_BuHwB-8hCQ@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0418270c891aec04fdb4e310
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0418270c891aec04fdb4e310
Content-Type: text/plain; charset=UTF-8

K doesn't matter much I've tried anything from 2^10 to 10^3 and the
performance
doesn't change much as measured by precision @ K. (see table 1
http://machinelearning.wustl.edu/mlpapers/papers/weston13). Although 10^3
kmeans did outperform 2^10 hierarchical SVD slightly in terms of the
metrics, 2^10 SVD was much faster in terms of inference time.

I found the thing that affected performance most was adding in back
tracking to fix mistakes made at higher levels rather than how the K is
picked per level.



On Tue, Jul 8, 2014 at 1:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.com> wrote:

> sure. more interesting problem here is choosing k at each level. Kernel
> methods seem to be most promising.
>
>
> On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com> wrote:
>
> > No idea, never looked it up. Always just implemented it as doing k-means
> > again on each cluster.
> >
> > FWIW standard k-means with euclidean distance has problems too with some
> > dimensionality reduction methods. Swapping out the distance metric with
> > negative dot or cosine may help.
> >
> > Other more useful clustering would be hierarchical SVD. The reason why I
> > like hierarchical clustering is it makes for faster inference especially
> > over billions of users.
> >
> >
> > On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> > wrote:
> >
> > > Hector, could you share the references for hierarchical K-means?
> thanks.
> > >
> > >
> > > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com>
> wrote:
> > >
> > > > I would say for bigdata applications the most useful would be
> > > hierarchical
> > > > k-means with back tracking and the ability to support k nearest
> > > centroids.
> > > >
> > > >
> > > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com>
> > wrote:
> > > >
> > > > > Hi all,
> > > > >
> > > > > MLlib currently has one clustering algorithm implementation,
> KMeans.
> > > > > It would benefit from having implementations of other clustering
> > > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> > > > > Clustering, and Affinity Propagation.
> > > > >
> > > > > I recently submitted a PR [1] for a MiniBatch KMeans
> implementation,
> > > > > and I saw an email on this list about interest in implementing
> Fuzzy
> > > > > C-Means.
> > > > >
> > > > > Based on Sean Owen's review of my MiniBatch KMeans code, it became
> > > > > apparent that before I implement more clustering algorithms, it
> would
> > > > > be useful to hammer out a framework to reduce code duplication and
> > > > > implement a consistent API.
> > > > >
> > > > > I'd like to gauge the interest and goals of the MLlib community:
> > > > >
> > > > > 1. Are you interested in having more clustering algorithms
> available?
> > > > >
> > > > > 2. Is the community interested in specifying a common framework?
> > > > >
> > > > > Thanks!
> > > > > RJ
> > > > >
> > > > > [1] - https://github.com/apache/spark/pull/1248
> > > > >
> > > > >
> > > > > --
> > > > > em rnowling@gmail.com
> > > > > c 954.496.2314
> > > > >
> > > >
> > > >
> > > >
> > > > --
> > > > Yee Yang Li Hector <http://google.com/+HectorYee>
> > > > *google.com/+HectorYee <http://google.com/+HectorYee>*
> > > >
> > >
> >
> >
> >
> > --
> > Yee Yang Li Hector <http://google.com/+HectorYee>
> > *google.com/+HectorYee <http://google.com/+HectorYee>*
> >
>



-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*

--f46d0418270c891aec04fdb4e310--

From dev-return-8251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 21:07:06 2014
Return-Path: <dev-return-8251-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C47DC11373
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 21:07:06 +0000 (UTC)
Received: (qmail 23031 invoked by uid 500); 8 Jul 2014 21:07:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22977 invoked by uid 500); 8 Jul 2014 21:07:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22965 invoked by uid 99); 8 Jul 2014 21:07:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:07:02 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hector.yee@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:06:58 +0000
Received: by mail-wi0-f181.google.com with SMTP id n3so1694134wiv.8
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 14:06:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=sFagzT7rvFx84Mn8reVO1swwY0a7ryw59UCVEcCK8xo=;
        b=oShV8quhpJ/7zMTx7qeXEI1D/8roaHGqIN9sJdEtIc9pqvr0GkcwxQqvMXQMBw9rT7
         FfXY8iI2PCoTM52mbz4Fhv3sNgxLZDCjLmXeihMr7v8W5ABYXpgD6Bx8Wfw2t0u7HPQM
         KzPwoj94C9AB+73G7t0U7F75dkb5KdM6i9lo/jbKmwMQMuhAK/5xljQq/e6Xomz1Sf61
         s1E6DUNRJCamHgc2fn/9RV4jCaBLRDG7GyCQlpnnE7pdcw1e3eQjT8D9iis0W6YN0Fjk
         gf+n+1bOcSrEytmy/02qqB/RsGq8nEA6PXBkEQtyeTqb/3GIXDDy5yyYdALvSxqhy8B3
         i8nw==
X-Received: by 10.194.186.178 with SMTP id fl18mr42175671wjc.83.1404853597762;
 Tue, 08 Jul 2014 14:06:37 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.30.79 with HTTP; Tue, 8 Jul 2014 14:06:17 -0700 (PDT)
In-Reply-To: <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
 <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
 <CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
 <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
 <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com> <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
From: Hector Yee <hector.yee@gmail.com>
Date: Tue, 8 Jul 2014 14:06:17 -0700
Message-ID: <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bb04dd2a7fb6e04fdb4f733
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb04dd2a7fb6e04fdb4f733
Content-Type: text/plain; charset=UTF-8

No was thinking more top-down:

assuming a distributed kmeans system already existing, recursively apply
the kmeans algorithm on data already partitioned by the previous level of
kmeans.

I haven't been much of a fan of bottom up approaches like HAC mainly
because they assume there is already a distance metric for items to items.
This makes it hard to cluster new content. The distances between sibling
clusters is also hard to compute (if you have thrown away the similarity
matrix), do you count paths to same parent node if you are computing
distances between items in two adjacent nodes for example. It is also a bit
harder to distribute the computation for bottom up approaches as you have
to already find the nearest neighbor to an item to begin the process.


On Tue, Jul 8, 2014 at 1:59 PM, RJ Nowling <rnowling@gmail.com> wrote:

> The scikit-learn implementation may be of interest:
>
>
> http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.html#sklearn.cluster.Ward
>
> It's a bottom up approach.  The pair of clusters for merging are
> chosen to minimize variance.
>
> Their code is under a BSD license so it can be used as a template.
>
> Is something like that you were thinking Hector?
>
> On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> wrote:
> > sure. more interesting problem here is choosing k at each level. Kernel
> > methods seem to be most promising.
> >
> >
> > On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com> wrote:
> >
> >> No idea, never looked it up. Always just implemented it as doing k-means
> >> again on each cluster.
> >>
> >> FWIW standard k-means with euclidean distance has problems too with some
> >> dimensionality reduction methods. Swapping out the distance metric with
> >> negative dot or cosine may help.
> >>
> >> Other more useful clustering would be hierarchical SVD. The reason why I
> >> like hierarchical clustering is it makes for faster inference especially
> >> over billions of users.
> >>
> >>
> >> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> >> wrote:
> >>
> >> > Hector, could you share the references for hierarchical K-means?
> thanks.
> >> >
> >> >
> >> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com>
> wrote:
> >> >
> >> > > I would say for bigdata applications the most useful would be
> >> > hierarchical
> >> > > k-means with back tracking and the ability to support k nearest
> >> > centroids.
> >> > >
> >> > >
> >> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com>
> >> wrote:
> >> > >
> >> > > > Hi all,
> >> > > >
> >> > > > MLlib currently has one clustering algorithm implementation,
> KMeans.
> >> > > > It would benefit from having implementations of other clustering
> >> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> >> > > > Clustering, and Affinity Propagation.
> >> > > >
> >> > > > I recently submitted a PR [1] for a MiniBatch KMeans
> implementation,
> >> > > > and I saw an email on this list about interest in implementing
> Fuzzy
> >> > > > C-Means.
> >> > > >
> >> > > > Based on Sean Owen's review of my MiniBatch KMeans code, it became
> >> > > > apparent that before I implement more clustering algorithms, it
> would
> >> > > > be useful to hammer out a framework to reduce code duplication and
> >> > > > implement a consistent API.
> >> > > >
> >> > > > I'd like to gauge the interest and goals of the MLlib community:
> >> > > >
> >> > > > 1. Are you interested in having more clustering algorithms
> available?
> >> > > >
> >> > > > 2. Is the community interested in specifying a common framework?
> >> > > >
> >> > > > Thanks!
> >> > > > RJ
> >> > > >
> >> > > > [1] - https://github.com/apache/spark/pull/1248
> >> > > >
> >> > > >
> >> > > > --
> >> > > > em rnowling@gmail.com
> >> > > > c 954.496.2314
> >> > > >
> >> > >
> >> > >
> >> > >
> >> > > --
> >> > > Yee Yang Li Hector <http://google.com/+HectorYee>
> >> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
> >> > >
> >> >
> >>
> >>
> >>
> >> --
> >> Yee Yang Li Hector <http://google.com/+HectorYee>
> >> *google.com/+HectorYee <http://google.com/+HectorYee>*
> >>
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>



-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*

--047d7bb04dd2a7fb6e04fdb4f733--

From dev-return-8252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 21:13:24 2014
Return-Path: <dev-return-8252-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD647113B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 21:13:24 +0000 (UTC)
Received: (qmail 43636 invoked by uid 500); 8 Jul 2014 21:13:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43581 invoked by uid 500); 8 Jul 2014 21:13:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43569 invoked by uid 99); 8 Jul 2014 21:13:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:13:22 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of evan.sparks@gmail.com designates 209.85.128.182 as permitted sender)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:13:18 +0000
Received: by mail-ve0-f182.google.com with SMTP id oy12so6441092veb.27
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 14:12:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=0ryqlx/Yr1NdyAYgeGUmT3TpgnPYY/J1Hb4kcGP04Kg=;
        b=bzj/6YHNSioBa58+d9uN/C6B2uIzVsClJtqJ/AxYgwp/GY47RrXNRS8tgnIPgpRzMf
         SpKr1Mgg43zOX0RUMnddvu54yx7ycGEhGk79JjiaTpH/YgCLEkkBdIQ/jOP+3t6OJhn+
         tHyq0kGN/8MmqAlQVcNCOGagtei7uEiO14aoS/ThrJaSvRT7sR1ajm+Tekdr3wQEO6VM
         wE8H9bxDJwIQM+W2vqBVbSHUs1IKSmIRaZVWI6M30KTBrqBgyxUr8Tb5HP/dzZElfS/+
         8rbGMU6LHWSxOk/BDinLJUD1cExtWNSR2KK1lU9rxg/FPFYtXGDjnprwcOSILKEfosTO
         bwbA==
X-Received: by 10.52.121.197 with SMTP id lm5mr15834vdb.74.1404853977317; Tue,
 08 Jul 2014 14:12:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.52.188.133 with HTTP; Tue, 8 Jul 2014 14:12:37 -0700 (PDT)
In-Reply-To: <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
 <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
 <CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
 <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
 <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
 <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com> <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
From: "Evan R. Sparks" <evan.sparks@gmail.com>
Date: Tue, 8 Jul 2014 14:12:37 -0700
Message-ID: <CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1135ea104789ed04fdb50eb7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1135ea104789ed04fdb50eb7
Content-Type: text/plain; charset=UTF-8

If you're thinking along these lines, have a look at the DecisionTree
implementation in MLlib. It uses the same idea and is optimized to prevent
multiple passes over the data by computing several splits at each level of
tree building. The tradeoff is increased model state and computation per
pass over the data, but fewer total passes and hopefully lower
communication overheads than, say, shuffling data around that belongs to
one cluster or another. Something like that could work here as well.

I'm not super-familiar with hierarchical K-Means so perhaps there's a more
efficient way to implement it, though.


On Tue, Jul 8, 2014 at 2:06 PM, Hector Yee <hector.yee@gmail.com> wrote:

> No was thinking more top-down:
>
> assuming a distributed kmeans system already existing, recursively apply
> the kmeans algorithm on data already partitioned by the previous level of
> kmeans.
>
> I haven't been much of a fan of bottom up approaches like HAC mainly
> because they assume there is already a distance metric for items to items.
> This makes it hard to cluster new content. The distances between sibling
> clusters is also hard to compute (if you have thrown away the similarity
> matrix), do you count paths to same parent node if you are computing
> distances between items in two adjacent nodes for example. It is also a bit
> harder to distribute the computation for bottom up approaches as you have
> to already find the nearest neighbor to an item to begin the process.
>
>
> On Tue, Jul 8, 2014 at 1:59 PM, RJ Nowling <rnowling@gmail.com> wrote:
>
> > The scikit-learn implementation may be of interest:
> >
> >
> >
> http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.html#sklearn.cluster.Ward
> >
> > It's a bottom up approach.  The pair of clusters for merging are
> > chosen to minimize variance.
> >
> > Their code is under a BSD license so it can be used as a template.
> >
> > Is something like that you were thinking Hector?
> >
> > On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> > wrote:
> > > sure. more interesting problem here is choosing k at each level. Kernel
> > > methods seem to be most promising.
> > >
> > >
> > > On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com>
> wrote:
> > >
> > >> No idea, never looked it up. Always just implemented it as doing
> k-means
> > >> again on each cluster.
> > >>
> > >> FWIW standard k-means with euclidean distance has problems too with
> some
> > >> dimensionality reduction methods. Swapping out the distance metric
> with
> > >> negative dot or cosine may help.
> > >>
> > >> Other more useful clustering would be hierarchical SVD. The reason
> why I
> > >> like hierarchical clustering is it makes for faster inference
> especially
> > >> over billions of users.
> > >>
> > >>
> > >> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> > >> wrote:
> > >>
> > >> > Hector, could you share the references for hierarchical K-means?
> > thanks.
> > >> >
> > >> >
> > >> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com>
> > wrote:
> > >> >
> > >> > > I would say for bigdata applications the most useful would be
> > >> > hierarchical
> > >> > > k-means with back tracking and the ability to support k nearest
> > >> > centroids.
> > >> > >
> > >> > >
> > >> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com>
> > >> wrote:
> > >> > >
> > >> > > > Hi all,
> > >> > > >
> > >> > > > MLlib currently has one clustering algorithm implementation,
> > KMeans.
> > >> > > > It would benefit from having implementations of other clustering
> > >> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means, Hierarchical
> > >> > > > Clustering, and Affinity Propagation.
> > >> > > >
> > >> > > > I recently submitted a PR [1] for a MiniBatch KMeans
> > implementation,
> > >> > > > and I saw an email on this list about interest in implementing
> > Fuzzy
> > >> > > > C-Means.
> > >> > > >
> > >> > > > Based on Sean Owen's review of my MiniBatch KMeans code, it
> became
> > >> > > > apparent that before I implement more clustering algorithms, it
> > would
> > >> > > > be useful to hammer out a framework to reduce code duplication
> and
> > >> > > > implement a consistent API.
> > >> > > >
> > >> > > > I'd like to gauge the interest and goals of the MLlib community:
> > >> > > >
> > >> > > > 1. Are you interested in having more clustering algorithms
> > available?
> > >> > > >
> > >> > > > 2. Is the community interested in specifying a common framework?
> > >> > > >
> > >> > > > Thanks!
> > >> > > > RJ
> > >> > > >
> > >> > > > [1] - https://github.com/apache/spark/pull/1248
> > >> > > >
> > >> > > >
> > >> > > > --
> > >> > > > em rnowling@gmail.com
> > >> > > > c 954.496.2314
> > >> > > >
> > >> > >
> > >> > >
> > >> > >
> > >> > > --
> > >> > > Yee Yang Li Hector <http://google.com/+HectorYee>
> > >> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
> > >> > >
> > >> >
> > >>
> > >>
> > >>
> > >> --
> > >> Yee Yang Li Hector <http://google.com/+HectorYee>
> > >> *google.com/+HectorYee <http://google.com/+HectorYee>*
> > >>
> >
> >
> >
> > --
> > em rnowling@gmail.com
> > c 954.496.2314
> >
>
>
>
> --
> Yee Yang Li Hector <http://google.com/+HectorYee>
> *google.com/+HectorYee <http://google.com/+HectorYee>*
>

--001a1135ea104789ed04fdb50eb7--

From dev-return-8253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul  8 21:44:47 2014
Return-Path: <dev-return-8253-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 570091149B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  8 Jul 2014 21:44:47 +0000 (UTC)
Received: (qmail 1475 invoked by uid 500); 8 Jul 2014 21:44:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1405 invoked by uid 500); 8 Jul 2014 21:44:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1392 invoked by uid 99); 8 Jul 2014 21:44:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:44:46 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hector.yee@gmail.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 08 Jul 2014 21:44:42 +0000
Received: by mail-wi0-f179.google.com with SMTP id cc10so1735491wib.6
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 14:44:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=K9MOK0bRwAl3yAUYQnDNAl0C9oK2ivaTf3+fJFZwfxg=;
        b=JQKmJWS9A2Kj2D1HtTYy8tw35O4ssDIyivMfCyOsMKSvuicNunWWmugyPMRZFIzl4M
         Q6DqKfA/NBIcjBMnfiz8DgM73Y9MuNNS76b25b9P+k5iwHbZ/IBISm3rRoq3TKUNuQi/
         vzsoVH5CXGy8xdcw3yVce6c8zEvNOmyowvHKL+fuU8axkX6qUPjkgr6WvbYF/iMfC+VS
         liRe2/0KX3APSeA2EXvifR9NTyNKHzwtRCr3VE0xnZ8OITePzTQH0s2rTzuTLu9a3B1H
         rk7bhbusPHuvI91Zz8lKy0VvOot9ZZgXQhJLv9vbH9ckHXPr6Ex5iMQUCVzRZ6Sl67Bd
         bZmA==
X-Received: by 10.180.14.33 with SMTP id m1mr7021744wic.50.1404855860890; Tue,
 08 Jul 2014 14:44:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.30.79 with HTTP; Tue, 8 Jul 2014 14:44:00 -0700 (PDT)
In-Reply-To: <CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
 <CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
 <CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
 <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
 <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
 <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
 <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com> <CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
From: Hector Yee <hector.yee@gmail.com>
Date: Tue, 8 Jul 2014 14:44:00 -0700
Message-ID: <CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d040fa1ee8c91fd04fdb57e7f
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d040fa1ee8c91fd04fdb57e7f
Content-Type: text/plain; charset=UTF-8

Yeah if one were to replace the objective function in decision tree with
minimizing the variance of the leaf nodes it would be a hierarchical
clusterer.


On Tue, Jul 8, 2014 at 2:12 PM, Evan R. Sparks <evan.sparks@gmail.com>
wrote:

> If you're thinking along these lines, have a look at the DecisionTree
> implementation in MLlib. It uses the same idea and is optimized to prevent
> multiple passes over the data by computing several splits at each level of
> tree building. The tradeoff is increased model state and computation per
> pass over the data, but fewer total passes and hopefully lower
> communication overheads than, say, shuffling data around that belongs to
> one cluster or another. Something like that could work here as well.
>
> I'm not super-familiar with hierarchical K-Means so perhaps there's a more
> efficient way to implement it, though.
>
>
> On Tue, Jul 8, 2014 at 2:06 PM, Hector Yee <hector.yee@gmail.com> wrote:
>
> > No was thinking more top-down:
> >
> > assuming a distributed kmeans system already existing, recursively apply
> > the kmeans algorithm on data already partitioned by the previous level of
> > kmeans.
> >
> > I haven't been much of a fan of bottom up approaches like HAC mainly
> > because they assume there is already a distance metric for items to
> items.
> > This makes it hard to cluster new content. The distances between sibling
> > clusters is also hard to compute (if you have thrown away the similarity
> > matrix), do you count paths to same parent node if you are computing
> > distances between items in two adjacent nodes for example. It is also a
> bit
> > harder to distribute the computation for bottom up approaches as you have
> > to already find the nearest neighbor to an item to begin the process.
> >
> >
> > On Tue, Jul 8, 2014 at 1:59 PM, RJ Nowling <rnowling@gmail.com> wrote:
> >
> > > The scikit-learn implementation may be of interest:
> > >
> > >
> > >
> >
> http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.html#sklearn.cluster.Ward
> > >
> > > It's a bottom up approach.  The pair of clusters for merging are
> > > chosen to minimize variance.
> > >
> > > Their code is under a BSD license so it can be used as a template.
> > >
> > > Is something like that you were thinking Hector?
> > >
> > > On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
> > > wrote:
> > > > sure. more interesting problem here is choosing k at each level.
> Kernel
> > > > methods seem to be most promising.
> > > >
> > > >
> > > > On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com>
> > wrote:
> > > >
> > > >> No idea, never looked it up. Always just implemented it as doing
> > k-means
> > > >> again on each cluster.
> > > >>
> > > >> FWIW standard k-means with euclidean distance has problems too with
> > some
> > > >> dimensionality reduction methods. Swapping out the distance metric
> > with
> > > >> negative dot or cosine may help.
> > > >>
> > > >> Other more useful clustering would be hierarchical SVD. The reason
> > why I
> > > >> like hierarchical clustering is it makes for faster inference
> > especially
> > > >> over billions of users.
> > > >>
> > > >>
> > > >> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com
> >
> > > >> wrote:
> > > >>
> > > >> > Hector, could you share the references for hierarchical K-means?
> > > thanks.
> > > >> >
> > > >> >
> > > >> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com>
> > > wrote:
> > > >> >
> > > >> > > I would say for bigdata applications the most useful would be
> > > >> > hierarchical
> > > >> > > k-means with back tracking and the ability to support k nearest
> > > >> > centroids.
> > > >> > >
> > > >> > >
> > > >> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com
> >
> > > >> wrote:
> > > >> > >
> > > >> > > > Hi all,
> > > >> > > >
> > > >> > > > MLlib currently has one clustering algorithm implementation,
> > > KMeans.
> > > >> > > > It would benefit from having implementations of other
> clustering
> > > >> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means,
> Hierarchical
> > > >> > > > Clustering, and Affinity Propagation.
> > > >> > > >
> > > >> > > > I recently submitted a PR [1] for a MiniBatch KMeans
> > > implementation,
> > > >> > > > and I saw an email on this list about interest in implementing
> > > Fuzzy
> > > >> > > > C-Means.
> > > >> > > >
> > > >> > > > Based on Sean Owen's review of my MiniBatch KMeans code, it
> > became
> > > >> > > > apparent that before I implement more clustering algorithms,
> it
> > > would
> > > >> > > > be useful to hammer out a framework to reduce code duplication
> > and
> > > >> > > > implement a consistent API.
> > > >> > > >
> > > >> > > > I'd like to gauge the interest and goals of the MLlib
> community:
> > > >> > > >
> > > >> > > > 1. Are you interested in having more clustering algorithms
> > > available?
> > > >> > > >
> > > >> > > > 2. Is the community interested in specifying a common
> framework?
> > > >> > > >
> > > >> > > > Thanks!
> > > >> > > > RJ
> > > >> > > >
> > > >> > > > [1] - https://github.com/apache/spark/pull/1248
> > > >> > > >
> > > >> > > >
> > > >> > > > --
> > > >> > > > em rnowling@gmail.com
> > > >> > > > c 954.496.2314
> > > >> > > >
> > > >> > >
> > > >> > >
> > > >> > >
> > > >> > > --
> > > >> > > Yee Yang Li Hector <http://google.com/+HectorYee>
> > > >> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
> > > >> > >
> > > >> >
> > > >>
> > > >>
> > > >>
> > > >> --
> > > >> Yee Yang Li Hector <http://google.com/+HectorYee>
> > > >> *google.com/+HectorYee <http://google.com/+HectorYee>*
> > > >>
> > >
> > >
> > >
> > > --
> > > em rnowling@gmail.com
> > > c 954.496.2314
> > >
> >
> >
> >
> > --
> > Yee Yang Li Hector <http://google.com/+HectorYee>
> > *google.com/+HectorYee <http://google.com/+HectorYee>*
> >
>



-- 
Yee Yang Li Hector <http://google.com/+HectorYee>
*google.com/+HectorYee <http://google.com/+HectorYee>*

--f46d040fa1ee8c91fd04fdb57e7f--

From dev-return-8254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 02:58:39 2014
Return-Path: <dev-return-8254-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A2DA711C35
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 02:58:39 +0000 (UTC)
Received: (qmail 91024 invoked by uid 500); 9 Jul 2014 02:58:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90960 invoked by uid 500); 9 Jul 2014 02:58:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90949 invoked by uid 99); 9 Jul 2014 02:58:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 02:58:38 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wibenton@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 02:58:33 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s692wCqU014341
	for <dev@spark.apache.org>; Tue, 8 Jul 2014 22:58:12 -0400
Date: Tue, 8 Jul 2014 22:58:11 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <2064037878.8677989.1404874691888.JavaMail.zimbra@redhat.com>
In-Reply-To: <1429800275.8620003.1404873664466.JavaMail.zimbra@redhat.com>
Subject: odd test suite failures while adding functions to Catalyst
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF29 (Mac)/8.0.6_GA_5922)
Thread-Topic: odd test suite failures while adding functions to Catalyst
Thread-Index: BGBKv2lXK7tdegPbm1UFBcnhku+yrA==
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I was testing an addition to Catalyst today (reimplementing a Hive UDF) and ran into some odd failures in the test suite.  In particular, it seems that what most of these have in common is that an array is spuriously reversed somewhere.  For example, the stddev tests in the HiveCompatibilitySuite all failed this way (note reversed synonyms list; note also that stddev isn't the function I reimplemented):

   [info] - udf_std *** FAILED ***
   [info]   Results do not match for udf_std:
   [info]   DESCRIBE FUNCTION EXTENDED std
   [info]   == Logical Plan ==
   [info]   NativeCommand DESCRIBE FUNCTION EXTENDED std
   [info]   
   [info]   == Optimized Logical Plan ==
   [info]   NativeCommand DESCRIBE FUNCTION EXTENDED std
   [info]   
   [info]   == Physical Plan ==
   [info]   NativeCommand DESCRIBE FUNCTION EXTENDED std, [result#38637:0]
   [info]   result
   [info]   !== HIVE - 2 row(s) ==                                         == CATALYST - 2 row(s) ==
   [info]    std(x) - Returns the standard deviation of a set of numbers   std(x) - Returns the standard deviation of a set of numbers
   [info]   !Synonyms: stddev_pop, stddev                                  Synonyms: stddev, stddev_pop (HiveComparisonTest.scala:372)

I also saw a reversed array (relative to the expected array) of Hive settings in HiveQuerySuite.

I'll probably be able to track down where things are going wrong after getting away from my desk for a bit, but I thought I'd send it out to the dev list in case this looks familiar to anyone.  Has anyone seen this kind of failure before?



thanks,
wb

From dev-return-8255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 04:57:52 2014
Return-Path: <dev-return-8255-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20A3411F2F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 04:57:52 +0000 (UTC)
Received: (qmail 53449 invoked by uid 500); 9 Jul 2014 04:57:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53387 invoked by uid 500); 9 Jul 2014 04:57:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53375 invoked by uid 99); 9 Jul 2014 04:57:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 04:57:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of usman@platfora.com designates 74.125.82.169 as permitted sender)
Received: from [74.125.82.169] (HELO mail-we0-f169.google.com) (74.125.82.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 04:57:45 +0000
Received: by mail-we0-f169.google.com with SMTP id t60so6887211wes.28
        for <dev@spark.apache.org>; Tue, 08 Jul 2014 21:57:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=4qMoQDwrnkUmc16Bg4FJYCS1beJGC/9kHSAJEKtGjAc=;
        b=E9nmxtBCrx2kKMoX/TZ64zd77+GiLVPTWfYud+/hoJoW8VOngDxwkDiadQ6DLjC3uh
         dun4PteQB7LlMdQrseso2b/lVnc2htJ1rYrcIUszSFkESbvf+87ZWyEF8kD2QwGAQiv2
         ZI/SSRNUlMRG51aTX/RNer6ocAaatXpX3mWZsBbKkRootAbl7RawU2ACmCb95lT+4oTn
         +jRlgazXl//8ZN6D9EspT8Al+JgybzFn2gTkMdL55k9a+yyZW5bC4OVlK8N/dDeDbZeX
         XWsbgbA8+Ko5IgPJZva4Hzv2vJU4LH2YWCwIVM7rUaf07Q9kIbKFSU7IuHw9ThFDf+0I
         YPwA==
X-Gm-Message-State: ALoCoQkf91NlKDLBdQrP43TP92WJFBdnWBZLNnzoTf0EXO7IG7vmFXlhPUpyNTm0Az2nxPtKYPtA
MIME-Version: 1.0
X-Received: by 10.180.13.47 with SMTP id e15mr8900512wic.28.1404881843855;
 Tue, 08 Jul 2014 21:57:23 -0700 (PDT)
Received: by 10.216.188.6 with HTTP; Tue, 8 Jul 2014 21:57:23 -0700 (PDT)
Date: Tue, 8 Jul 2014 21:57:23 -0700
Message-ID: <CAG_e43zOi1QqXt2YYVMHGyV+8fa5yxf+T5jUniXo++TfpE=Vng@mail.gmail.com>
Subject: Not starting the Web ui in the driver
From: Usman Ghani <usman@platfora.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c24e42413b5404fdbb8be9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24e42413b5404fdbb8be9
Content-Type: text/plain; charset=UTF-8

Is there a way to run the spark driver program without starting the
monitoring web UI in-process? I didn't see any config setting around it.

--001a11c24e42413b5404fdbb8be9--

From dev-return-8256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 05:54:55 2014
Return-Path: <dev-return-8256-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4849A1103A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 05:54:55 +0000 (UTC)
Received: (qmail 45253 invoked by uid 500); 9 Jul 2014 05:54:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45192 invoked by uid 500); 9 Jul 2014 05:54:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45179 invoked by uid 99); 9 Jul 2014 05:54:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 05:54:54 +0000
X-ASF-Spam-Status: No, hits=0.7 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [76.96.27.228] (HELO qmta15.emeryville.ca.mail.comcast.net) (76.96.27.228)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 05:54:48 +0000
Received: from omta23.emeryville.ca.mail.comcast.net ([76.96.30.90])
	by qmta15.emeryville.ca.mail.comcast.net with comcast
	id Q5rF1o0011wfjNsAF5uTwb; Wed, 09 Jul 2014 05:54:27 +0000
Received: from fs ([24.130.135.131])
	by omta23.emeryville.ca.mail.comcast.net with comcast
	id Q5uS1o00e2qGB608j5uTd5; Wed, 09 Jul 2014 05:54:27 +0000
Received: from localhost (tpx.boudnik.org [192.168.102.148])
	by fs (8.14.3/8.14.3/Debian-5+lenny1) with ESMTP id s695sPDq024158
	for <dev@spark.apache.org>; Tue, 8 Jul 2014 22:54:26 -0700
Date: Tue, 8 Jul 2014 22:54:24 -0700
From: Konstantin Boudnik <cos@apache.org>
To: dev@spark.apache.org
Subject: Meetup invitation: Consensus based replication in Hadoop
Message-ID: <20140709055424.GA3686@tpx>
References: <CAKtuutE1=Sbj33y=SXz1fW6iZ-9Cx82yc_PVb2EjBYsAF=RD+g@mail.gmail.com>
 <20140619034555.GP27913@boudnik.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Disposition: inline
In-Reply-To: <20140619034555.GP27913@boudnik.org>
User-Agent: Mutt/1.5.21 (2010-09-15)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=comcast.net;
	s=q20140121; t=1404885268;
	bh=x7uvY9ZknfLOtZyMyHscRBCfKmHmQDOXRcq6W5k/Un8=;
	h=Received:Received:Received:Date:From:To:Subject:Message-ID:
	 MIME-Version:Content-Type;
	b=K9OPWJZofcHPzQJDJ9fG0ieI7JMnX7NCpNTBlDG4c0/QeM8d4qz60si/szHhAGtnG
	 wr9J6uB3KzutymGzrJ1tRyi6KNby4civ0wcJj/6e+IExGVpfB0kNiOhyez0YsMzIxP
	 O0AwoRMU/g6/t/oJHO5ia8dnHvKpH1FxiCghpahsO9HJMYJlvEETNxsYuiDYKQyt9a
	 34bG8MQzeQjwvqWy66Dw/xaYKyw0Nb8SJmVkwA0FLIi8bsnTDHf0WkQ149ieISpF75
	 ysTy9WZkd/Ep68d1QN0aeSj0NlcYRodeuwVvXTKmQWmgSwlR3h5vNeTpur6rdnAi3a
	 nCvzh5K8fY5oA==
X-Virus-Checked: Checked by ClamAV on apache.org

[cross-posted from hdfs-dev@hadoop, common-dev@hadoop]

We'd like to invite you to the 
    Consensus based replication in Hadoop: A deep dive
event that we are happy to hold in our San Ramon office on July 15th at noon.
We'd like to accommodate as many people as possible, but I think are physically
limited to 30 (+/- a few), so please RSVP to this Eventbrite invitation:

https://www.eventbrite.co.uk/e/consensus-based-replication-in-hadoop-a-deep-dive-tickets-12158236613

We'll provide pizza and beverages (feel free to express your special dietary
requirements if any).

See you soon!
With regards,
  Cos

On Wed, Jun 18, 2014 at 08:45PM, Konstantin Boudnik wrote:
> Guys,
> 
> In the last a couple of weeks, we had a very good and productive initial round
> of discussions on the JIRAs. I think it is worthy to keep the momentum going
> and have a more detailed conversation. For that, we'd like to host s Hadoop
> developers meetup to get into the bowls of the consensus-based coordination
> implementation for HDFS. The proposed venue is our office in San Ramon, CA.
> 
> Considering that it is already a mid week and the following one looks short
> because of the holidays, how would the week of July 7th looks for yall?
> Tuesday or Thursday look pretty good on our end.
> 
> Please chime in on your preference either here or reach of directly to me.
> Once I have a few RSVPs I will setup an event on Eventbrite or similar.
> 
> Looking forward to your input. Regards,
>   Cos
> 
> On Thu, May 29, 2014 at 02:09PM, Konstantin Shvachko wrote:
> > Hello hadoop developers,
> > 
> > I just opened two jiras proposing to introduce ConsensusNode into HDFS and
> > a Coordination Engine into Hadoop Common. The latter should benefit HDFS
> > and  HBase as well as potentially other projects. See HDFS-6469 and
> > HADOOP-10641 for details.
> > The effort is based on the system we built at Wandisco with my colleagues,
> > who are glad to contribute it to Apache, as quite a few people in the
> > community expressed interest in this ideas and their potential applications.
> > 
> > We should probably keep technical discussions in the jiras. Here on the dev
> > list I wanted to touch-base on any logistic issues / questions.
> > - First of all, any ideas and help are very much welcome.
> > - We would like to set up a meetup to discuss this if people are
> > interested. Hadoop Summit next week may be a potential time-place to meet.
> > Not sure in what form. If not, we can organize one in our San Ramon office
> > later on.
> > - The effort may take a few months depending on the contributors schedules.
> > Would it make sense to open a branch for the ConsensusNode work?
> > - APIs and the implementation of the Coordination Engine should be a fairly
> > independent, so it may be reasonable to add it directly to Hadoop Common
> > trunk.
> > 
> > Thanks,
> > --Konstantin

From dev-return-8257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 11:05:51 2014
Return-Path: <dev-return-8257-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 824B511AE2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 11:05:51 +0000 (UTC)
Received: (qmail 36049 invoked by uid 500); 9 Jul 2014 11:05:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35994 invoked by uid 500); 9 Jul 2014 11:05:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35982 invoked by uid 99); 9 Jul 2014 11:05:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 11:05:50 +0000
X-ASF-Spam-Status: No, hits=3.4 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLYTO_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [106.10.148.115] (HELO nm3-vm4.bullet.mail.sg3.yahoo.com) (106.10.148.115)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 11:05:45 +0000
Received: from [106.10.166.126] by nm3.bullet.mail.sg3.yahoo.com with NNFMP; 09 Jul 2014 11:05:22 -0000
Received: from [106.10.151.123] by tm15.bullet.mail.sg3.yahoo.com with NNFMP; 09 Jul 2014 11:05:22 -0000
Received: from [127.0.0.1] by omp1005.mail.sg3.yahoo.com with NNFMP; 09 Jul 2014 11:05:22 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 797458.5174.bm@omp1005.mail.sg3.yahoo.com
Received: (qmail 13234 invoked by uid 60001); 9 Jul 2014 11:05:22 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.co.in; s=s1024; t=1404903922; bh=lbP58rEaJaws3rGOWFFBb++mCokMGHC6frebWFtpJt8=; h=Message-ID:Date:From:Reply-To:Subject:To:MIME-Version:Content-Type; b=Aw4B8Z/RVYZXMS4VYbeMxQw4GpFZPwVM1IOv/h70AkJFoDmAMvFZFHFnE6/NDkheow5YTwchLJIuzygdh2g02WqeAV+JeowvEu7Myt6hB68rGXE2nHxHRHRxepVSGTVsg25/rZ5MxvW8SeOSGSn/jJKH344Ve9vbK/UD6wIDa8o=
X-YMail-OSG: lkVbDVwVM1ks4ncqqQ132PR3ZVfnTiQXYVDUCuDvWg7OIfe
 ryFUaEDPF3Ly47TTkUlsQPzf4_Y6mKJs6EBpts.5IBOOxwOecVTnr1pck9Ps
 kRSPtYVAyXWzWCqnIe6fOhPPg_k6EbuVt.ZtojowLUiym8ZtHqTEXbZyjtQp
 yEwycBz2PtNQDnBPBgv6roc.483OMOpbhyLx7T63k5XjgGU_rqOBn9lUANJx
 m3Mdm7CSvCzSXctn.OjIoRJb0kFO4YY2aGQ_37BBonbAiYhHKzoTlvWzT_pN
 8SpIj4F1sJkCLipf3h6n0DXRhuXI_uCvsA8otgf__Mp5cc5yA46n86UVKb_R
 1F83y23f.4QXwhz2RANSViEbgx5dHBEJPZhHAAuQTJSbsea2B6D9o.WBPbak
 qnR2JrAA4X52NDCSwLn3_8JS8BO6Pm6kNAbuPL5OI20lAcqERZ11huLJR1AX
 6B1sYYuukwLU600YweQDyVmNeJY9JRQ3Ya.KltDOEipn5ESfi.Vg9dTXIr2r
 5XjLM1Tr0KyjREhXoXbNtx4y9bPErjpYtckNTOxmhKH3e88dxEikp.0DQ75s
 1LymjwR7BsMvgPOp8mfo_kvtEaP6LRodrz9ADhR7gXiC54j.YJuXD2FdMRTY
 WUhN9kkx24qq.YcjWwDL6S13Ryib8d3cjwrBV8A--
Received: from [125.17.228.30] by web190403.mail.sg3.yahoo.com via HTTP; Wed, 09 Jul 2014 19:05:22 SGT
X-Rocket-MIMEInfo: 002.001,SGksCgpJIGFtIGludGVyZXN0ZWQgaW4gY29udHJpYnV0aW5nIGEgY2x1c3RlcmluZyBhbGdvcml0aG0gdG93YXJkcyBNTGxpYiBvZiBTcGFyay5JIGFtIGZvY3VzaW5nIG9uIEdhdXNzaWFuIE1peHR1cmUgTW9kZWwuCkJ1dCBJIHNhdyBhIEpJUkEgQCBodHRwczovL3NwYXJrLXByb2plY3QuYXRsYXNzaWFuLm5ldC9icm93c2UvU1BBUkstOTUyIHJlZ3JhZGluZyB0aGUgc2FtZS5JIHdvdWxkIGxpa2UgdG8ga25vdyB3aGV0aGVywqBHYXVzc2lhbiBNaXh0dXJlIE1vZGVsIGlzwqBhbHJlYWR5IGltcGxlbWVudGUBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.191.1
Message-ID: <1404903922.95325.YahooMailNeo@web190403.mail.sg3.yahoo.com>
Date: Wed, 9 Jul 2014 19:05:22 +0800
From: MEETHU MATHEW <meethu2006@yahoo.co.in>
Reply-To: MEETHU MATHEW <meethu2006@yahoo.co.in>
Subject: Contribution to MLlib
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="-1873340658-476876293-1404903922=:95325"
X-Virus-Checked: Checked by ClamAV on apache.org

---1873340658-476876293-1404903922=:95325
Content-Type: text/plain; charset=iso-8859-1
Content-Transfer-Encoding: quoted-printable

Hi,=0A=0AI am interested in contributing a clustering algorithm towards MLl=
ib of Spark.I am focusing on Gaussian Mixture Model.=0ABut I saw a JIRA @ h=
ttps://spark-project.atlassian.net/browse/SPARK-952 regrading the same.I wo=
uld like to know whether=A0Gaussian Mixture Model is=A0already implemented =
or not.=0A=0A=0A=0AThanks & Regards, =0AMeethu M
---1873340658-476876293-1404903922=:95325--

From dev-return-8258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 11:56:48 2014
Return-Path: <dev-return-8258-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A631811C9F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 11:56:48 +0000 (UTC)
Received: (qmail 40018 invoked by uid 500); 9 Jul 2014 11:56:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39961 invoked by uid 500); 9 Jul 2014 11:56:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39947 invoked by uid 99); 9 Jul 2014 11:56:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 11:56:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.176 as permitted sender)
Received: from [74.125.82.176] (HELO mail-we0-f176.google.com) (74.125.82.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 11:56:43 +0000
Received: by mail-we0-f176.google.com with SMTP id u56so7305499wes.21
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 04:56:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JGd45s1kDZglcUveyfEDGL0qW0PyWlYzwo7DDVIVwC4=;
        b=tXy+GVpXt7x9vO8/s9knKbJK+AqHrPIUqcf1y8qbcFkux2MMQzqVuMdZH9NSIbw8rA
         HzYstVnXzrTY/vZP9mmQlLF1IV7qmhCfs80HkfRmoQnwcDmLNFzVvPbqzohxSCjfPaJf
         GnWHR5Bin7lDixg/7gbT0kePTsM2MVPBxtKvm1Hd8R4ccC8XSwqYWWPK2G6QP4MyZZpm
         BFTT0d+FDD40lQL+I5sE5dZt+MeX1z5TCDPlkUvv2FZhy5G5pmJp+ar4rKUEicDXJ/l2
         XSamsVb8ZhroBKg/yJSqT6AjhIpTAypE+bNN1IA89F+m2G7XtiU/Ba5dq84lGIaDiO/P
         vq+g==
MIME-Version: 1.0
X-Received: by 10.180.94.104 with SMTP id db8mr10753116wib.23.1404906982488;
 Wed, 09 Jul 2014 04:56:22 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Wed, 9 Jul 2014 04:56:22 -0700 (PDT)
In-Reply-To: <1404903922.95325.YahooMailNeo@web190403.mail.sg3.yahoo.com>
References: <1404903922.95325.YahooMailNeo@web190403.mail.sg3.yahoo.com>
Date: Wed, 9 Jul 2014 07:56:22 -0400
Message-ID: <CADtDQQL96NZVVDUQx4yLgMtU96BY7_TyB1k7YZyKngvsju50Yg@mail.gmail.com>
Subject: Re: Contribution to MLlib
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, MEETHU MATHEW <meethu2006@yahoo.co.in>
Content-Type: multipart/alternative; boundary=f46d04428484a28acc04fdc165a6
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04428484a28acc04fdc165a6
Content-Type: text/plain; charset=UTF-8

Hi Meethu,

There is no code for a Gaussian Mixture Model clustering algorithm in the
repository, but I don't know if anyone is working on it.

RJ

On Wednesday, July 9, 2014, MEETHU MATHEW <meethu2006@yahoo.co.in> wrote:

> Hi,
>
> I am interested in contributing a clustering algorithm towards MLlib of
> Spark.I am focusing on Gaussian Mixture Model.
> But I saw a JIRA @ https://spark-project.atlassian.net/browse/SPARK-952
> regrading the same.I would like to know whether Gaussian Mixture Model
> is already implemented or not.
>
>
>
> Thanks & Regards,
> Meethu M



-- 
em rnowling@gmail.com
c 954.496.2314

--f46d04428484a28acc04fdc165a6--

From dev-return-8259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 12:15:47 2014
Return-Path: <dev-return-8259-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B704611D8A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 12:15:47 +0000 (UTC)
Received: (qmail 89498 invoked by uid 500); 9 Jul 2014 12:15:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89439 invoked by uid 500); 9 Jul 2014 12:15:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89395 invoked by uid 99); 9 Jul 2014 12:15:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 12:15:46 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 12:15:42 +0000
Received: by mail-wi0-f169.google.com with SMTP id hi2so2680381wib.4
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 05:15:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=6wqEuHg0/6ldRD6d/U2wjKFNF1TixNerzn8dc0qRPPU=;
        b=I5Zdj/IawNsWt0ioFdd6gIV/H0sCxbJGKWSohByrSdbMnaQzZUBfyfaIUrtVVt7h/r
         krk98XrAszpB8J/rTEA5DMXbu1kFk5RmOq9ytsDn6iOaDJyo9ry3774soQ3ciqKNzznb
         feNzXHCd2d02ptMgodFRvnBwAiwOoqxOInkfKM+SaVC8Um5xNjWewu+EHf42S32/pVAX
         INborB9paJp169wRZKF9Uy86qO9DZ5C19ph6PTFwqeC02HO/mqwLKtY9fyanN8ymWGQP
         zv0lonKD8PlCRUS0U2iHiREK/roxCZD+NwNhago0Jqir4TJouV57vz3/5RNGfnxULjlU
         nwWw==
MIME-Version: 1.0
X-Received: by 10.180.109.168 with SMTP id ht8mr11004215wib.68.1404908121037;
 Wed, 09 Jul 2014 05:15:21 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Wed, 9 Jul 2014 05:15:20 -0700 (PDT)
In-Reply-To: <CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
References: <CADtDQQ+dEVcaQyZ1WXuyyijNJ+i=O3k0D+W6oWnzpPxP-oTNCw@mail.gmail.com>
	<CAPi87hdGKYsjyw2LrPXh=3RZZSbJ=cEW3TsFppWM6Jp5CnM_qQ@mail.gmail.com>
	<CAPud8To2vY5QeGffxkpP-snE4Pt-TmHAfsbeA8RQhqO98ZDBfg@mail.gmail.com>
	<CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
	<CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
	<CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
	<CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
	<CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
Date: Wed, 9 Jul 2014 08:15:20 -0400
Message-ID: <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks everyone for the input.

So it seems what people want is:

* Implement MiniBatch KMeans and Hierarchical KMeans (Divide and
conquer approach, look at DecisionTree implementation as a reference)
* Restructure 3 Kmeans clustering algorithm implementations to prevent
code duplication and conform to a consistent API where possible

If this is correct, I'll start work on that.  How would it be best to
structure it? Should I submit separate JIRAs / PRs for refactoring of
current code, MiniBatch KMeans, and Hierarchical or keep my current
JIRA and PR for MiniBatch KMeans open and submit a second JIRA and PR
for Hierarchical KMeans that builds on top of that?

Thanks!


On Tue, Jul 8, 2014 at 5:44 PM, Hector Yee <hector.yee@gmail.com> wrote:
> Yeah if one were to replace the objective function in decision tree with
> minimizing the variance of the leaf nodes it would be a hierarchical
> clusterer.
>
>
> On Tue, Jul 8, 2014 at 2:12 PM, Evan R. Sparks <evan.sparks@gmail.com>
> wrote:
>
>> If you're thinking along these lines, have a look at the DecisionTree
>> implementation in MLlib. It uses the same idea and is optimized to prevent
>> multiple passes over the data by computing several splits at each level of
>> tree building. The tradeoff is increased model state and computation per
>> pass over the data, but fewer total passes and hopefully lower
>> communication overheads than, say, shuffling data around that belongs to
>> one cluster or another. Something like that could work here as well.
>>
>> I'm not super-familiar with hierarchical K-Means so perhaps there's a more
>> efficient way to implement it, though.
>>
>>
>> On Tue, Jul 8, 2014 at 2:06 PM, Hector Yee <hector.yee@gmail.com> wrote:
>>
>> > No was thinking more top-down:
>> >
>> > assuming a distributed kmeans system already existing, recursively apply
>> > the kmeans algorithm on data already partitioned by the previous level of
>> > kmeans.
>> >
>> > I haven't been much of a fan of bottom up approaches like HAC mainly
>> > because they assume there is already a distance metric for items to
>> items.
>> > This makes it hard to cluster new content. The distances between sibling
>> > clusters is also hard to compute (if you have thrown away the similarity
>> > matrix), do you count paths to same parent node if you are computing
>> > distances between items in two adjacent nodes for example. It is also a
>> bit
>> > harder to distribute the computation for bottom up approaches as you have
>> > to already find the nearest neighbor to an item to begin the process.
>> >
>> >
>> > On Tue, Jul 8, 2014 at 1:59 PM, RJ Nowling <rnowling@gmail.com> wrote:
>> >
>> > > The scikit-learn implementation may be of interest:
>> > >
>> > >
>> > >
>> >
>> http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.html#sklearn.cluster.Ward
>> > >
>> > > It's a bottom up approach.  The pair of clusters for merging are
>> > > chosen to minimize variance.
>> > >
>> > > Their code is under a BSD license so it can be used as a template.
>> > >
>> > > Is something like that you were thinking Hector?
>> > >
>> > > On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.com>
>> > > wrote:
>> > > > sure. more interesting problem here is choosing k at each level.
>> Kernel
>> > > > methods seem to be most promising.
>> > > >
>> > > >
>> > > > On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com>
>> > wrote:
>> > > >
>> > > >> No idea, never looked it up. Always just implemented it as doing
>> > k-means
>> > > >> again on each cluster.
>> > > >>
>> > > >> FWIW standard k-means with euclidean distance has problems too with
>> > some
>> > > >> dimensionality reduction methods. Swapping out the distance metric
>> > with
>> > > >> negative dot or cosine may help.
>> > > >>
>> > > >> Other more useful clustering would be hierarchical SVD. The reason
>> > why I
>> > > >> like hierarchical clustering is it makes for faster inference
>> > especially
>> > > >> over billions of users.
>> > > >>
>> > > >>
>> > > >> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.com
>> >
>> > > >> wrote:
>> > > >>
>> > > >> > Hector, could you share the references for hierarchical K-means?
>> > > thanks.
>> > > >> >
>> > > >> >
>> > > >> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.com>
>> > > wrote:
>> > > >> >
>> > > >> > > I would say for bigdata applications the most useful would be
>> > > >> > hierarchical
>> > > >> > > k-means with back tracking and the ability to support k nearest
>> > > >> > centroids.
>> > > >> > >
>> > > >> > >
>> > > >> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.com
>> >
>> > > >> wrote:
>> > > >> > >
>> > > >> > > > Hi all,
>> > > >> > > >
>> > > >> > > > MLlib currently has one clustering algorithm implementation,
>> > > KMeans.
>> > > >> > > > It would benefit from having implementations of other
>> clustering
>> > > >> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means,
>> Hierarchical
>> > > >> > > > Clustering, and Affinity Propagation.
>> > > >> > > >
>> > > >> > > > I recently submitted a PR [1] for a MiniBatch KMeans
>> > > implementation,
>> > > >> > > > and I saw an email on this list about interest in implementing
>> > > Fuzzy
>> > > >> > > > C-Means.
>> > > >> > > >
>> > > >> > > > Based on Sean Owen's review of my MiniBatch KMeans code, it
>> > became
>> > > >> > > > apparent that before I implement more clustering algorithms,
>> it
>> > > would
>> > > >> > > > be useful to hammer out a framework to reduce code duplication
>> > and
>> > > >> > > > implement a consistent API.
>> > > >> > > >
>> > > >> > > > I'd like to gauge the interest and goals of the MLlib
>> community:
>> > > >> > > >
>> > > >> > > > 1. Are you interested in having more clustering algorithms
>> > > available?
>> > > >> > > >
>> > > >> > > > 2. Is the community interested in specifying a common
>> framework?
>> > > >> > > >
>> > > >> > > > Thanks!
>> > > >> > > > RJ
>> > > >> > > >
>> > > >> > > > [1] - https://github.com/apache/spark/pull/1248
>> > > >> > > >
>> > > >> > > >
>> > > >> > > > --
>> > > >> > > > em rnowling@gmail.com
>> > > >> > > > c 954.496.2314
>> > > >> > > >
>> > > >> > >
>> > > >> > >
>> > > >> > >
>> > > >> > > --
>> > > >> > > Yee Yang Li Hector <http://google.com/+HectorYee>
>> > > >> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
>> > > >> > >
>> > > >> >
>> > > >>
>> > > >>
>> > > >>
>> > > >> --
>> > > >> Yee Yang Li Hector <http://google.com/+HectorYee>
>> > > >> *google.com/+HectorYee <http://google.com/+HectorYee>*
>> > > >>
>> > >
>> > >
>> > >
>> > > --
>> > > em rnowling@gmail.com
>> > > c 954.496.2314
>> > >
>> >
>> >
>> >
>> > --
>> > Yee Yang Li Hector <http://google.com/+HectorYee>
>> > *google.com/+HectorYee <http://google.com/+HectorYee>*
>> >
>>
>
>
>
> --
> Yee Yang Li Hector <http://google.com/+HectorYee>
> *google.com/+HectorYee <http://google.com/+HectorYee>*



-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 12:39:57 2014
Return-Path: <dev-return-8260-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 757D811EC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 12:39:57 +0000 (UTC)
Received: (qmail 36961 invoked by uid 500); 9 Jul 2014 12:39:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 36908 invoked by uid 500); 9 Jul 2014 12:39:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 36896 invoked by uid 99); 9 Jul 2014 12:39:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 12:39:56 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.216.178 as permitted sender)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 12:39:53 +0000
Received: by mail-qc0-f178.google.com with SMTP id i17so2293118qcy.23
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 05:39:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:subject
         :content-type;
        bh=Eo/OFssIgUniijIAX5+LBhX1y1Mi2Cj7PLXH1Z0Lot4=;
        b=vadoVMmUqTl1ftJTuePQiAel/74GJICsyBprI8hAwuyyJqFoptng0h83MBwCENcHpg
         +NwN3VAzYYo/G37qgUSAxDFNisvrTK9tPtUGRVSMJ+3mGxwjSXiVkRKECFqiD/exDsOS
         A+dSco63L2vVj3Cg8A/UHYBCn4tIEZzwHXFpZRhtd8ww3cMS5EUWFz+1GZuockL0oTjS
         0ZX2kCOVY646oqyiQKko2BTDRVJkeYHJblWxinmgbU/WKUIyKA0f3u9rQQAJekBEB/fw
         yrF6F5XBjtlYpU8wvnCPQxwP4tRU+nwXpjDmfmZliIWLtGXVDbSMyhpwc9nQfgzTIhTk
         VImQ==
X-Received: by 10.140.96.38 with SMTP id j35mr64535141qge.5.1404909568893;
        Wed, 09 Jul 2014 05:39:28 -0700 (PDT)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id p15sm86240413qar.34.2014.07.09.05.39.28
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 09 Jul 2014 05:39:28 -0700 (PDT)
Date: Wed, 09 Jul 2014 05:39:28 -0700 (PDT)
X-Google-Original-Date: Wed, 09 Jul 2014 12:39:27 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1404909567855.c2a8fd87@Nodemailer>
In-Reply-To: <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
References: <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
X-Orchestra-Oid: 58973EC7-5C76-4903-BA9D-7B3D237CB981
X-Orchestra-Sig: 512ad2d4619f5c90eec91581f80b31b58cc2f1a8
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1473083676001909825
X-Orchestra-Thrid-Sig: d641d9f7482d5ef02a493b882d0784668a217d30
X-Orchestra-Account: c6e8f09d92b344e417f7a5f181944421bfead5e5
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1404909568185"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1404909568185
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Cool seems like a god initiative. Adding a couple extra high quality =
clustering implantations will be great.


I'd say it would make most sense to submit a PR for the Standardised API =
first, agree that with everyone and then build on it for the specific =
implementations.
=E2=80=94
Sent from Mailbox

On Wed, Jul 9, 2014 at 2:15 PM, RJ Nowling <rnowling@gmail.com> wrote:

> Thanks everyone for the input.
> So it seems what people want is:
> * Implement MiniBatch KMeans and Hierarchical KMeans (Divide and
> conquer approach, look at DecisionTree implementation as a reference)
> * Restructure 3 Kmeans clustering algorithm implementations to prevent
> code duplication and conform to a consistent API where possible
> If this is correct, I'll start work on that.  How would it be best to
> structure it=3F Should I submit separate JIRAs / PRs for refactoring of
> current code, MiniBatch KMeans, and Hierarchical or keep my current
> JIRA and PR for MiniBatch KMeans open and submit a second JIRA and PR
> for Hierarchical KMeans that builds on top of that=3F
> Thanks!
> On Tue, Jul 8, 2014 at 5:44 PM, Hector Yee <hector.yee@gmail.com> wrote:
>> Yeah if one were to replace the objective function in decision tree =
with
>> minimizing the variance of the leaf nodes it would be a hierarchical
>> clusterer.
>>
>>
>> On Tue, Jul 8, 2014 at 2:12 PM, Evan R. Sparks <evan.sparks@gmail.com>
>> wrote:
>>
>>> If you're thinking along these lines, have a look at the DecisionTree
>>> implementation in MLlib. It uses the same idea and is optimized to =
prevent
>>> multiple passes over the data by computing several splits at each level=
 of
>>> tree building. The tradeoff is increased model state and computation =
per
>>> pass over the data, but fewer total passes and hopefully lower
>>> communication overheads than, say, shuffling data around that belongs =
to
>>> one cluster or another. Something like that could work here as well.
>>>
>>> I'm not super-familiar with hierarchical K-Means so perhaps there's a =
more
>>> efficient way to implement it, though.
>>>
>>>
>>> On Tue, Jul 8, 2014 at 2:06 PM, Hector Yee <hector.yee@gmail.com> =
wrote:
>>>
>>> > No was thinking more top-down:
>>> >
>>> > assuming a distributed kmeans system already existing, recursively =
apply
>>> > the kmeans algorithm on data already partitioned by the previous =
level of
>>> > kmeans.
>>> >
>>> > I haven't been much of a fan of bottom up approaches like HAC mainly
>>> > because they assume there is already a distance metric for items to
>>> items.
>>> > This makes it hard to cluster new content. The distances between =
sibling
>>> > clusters is also hard to compute (if you have thrown away the =
similarity
>>> > matrix), do you count paths to same parent node if you are computing
>>> > distances between items in two adjacent nodes for example. It is also=
 a
>>> bit
>>> > harder to distribute the computation for bottom up approaches as you =
have
>>> > to already find the nearest neighbor to an item to begin the process.=

>>> >
>>> >
>>> > On Tue, Jul 8, 2014 at 1:59 PM, RJ Nowling <rnowling@gmail.com> =
wrote:
>>> >
>>> > > The scikit-learn implementation may be of interest:
>>> > >
>>> > >
>>> > >
>>> >
>>> http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.=
html#sklearn.cluster.Ward
>>> > >
>>> > > It's a bottom up approach.  The pair of clusters for merging are
>>> > > chosen to minimize variance.
>>> > >
>>> > > Their code is under a BSD license so it can be used as a template.
>>> > >
>>> > > Is something like that you were thinking Hector=3F
>>> > >
>>> > > On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.=
com>
>>> > > wrote:
>>> > > > sure. more interesting problem here is choosing k at each level.
>>> Kernel
>>> > > > methods seem to be most promising.
>>> > > >
>>> > > >
>>> > > > On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.=
com>
>>> > wrote:
>>> > > >
>>> > > >> No idea, never looked it up. Always just implemented it as =
doing
>>> > k-means
>>> > > >> again on each cluster.
>>> > > >>
>>> > > >> FWIW standard k-means with euclidean distance has problems too =
with
>>> > some
>>> > > >> dimensionality reduction methods. Swapping out the distance =
metric
>>> > with
>>> > > >> negative dot or cosine may help.
>>> > > >>
>>> > > >> Other more useful clustering would be hierarchical SVD. The =
reason
>>> > why I
>>> > > >> like hierarchical clustering is it makes for faster inference
>>> > especially
>>> > > >> over billions of users.
>>> > > >>
>>> > > >>
>>> > > >> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail.=
com
>>> >
>>> > > >> wrote:
>>> > > >>
>>> > > >> > Hector, could you share the references for hierarchical =
K-means=3F
>>> > > thanks.
>>> > > >> >
>>> > > >> >
>>> > > >> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.=
com>
>>> > > wrote:
>>> > > >> >
>>> > > >> > > I would say for bigdata applications the most useful would =
be
>>> > > >> > hierarchical
>>> > > >> > > k-means with back tracking and the ability to support k =
nearest
>>> > > >> > centroids.
>>> > > >> > >
>>> > > >> > >
>>> > > >> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail.=
com
>>> >
>>> > > >> wrote:
>>> > > >> > >
>>> > > >> > > > Hi all,
>>> > > >> > > >
>>> > > >> > > > MLlib currently has one clustering algorithm =
implementation,
>>> > > KMeans.
>>> > > >> > > > It would benefit from having implementations of other
>>> clustering
>>> > > >> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means,
>>> Hierarchical
>>> > > >> > > > Clustering, and Affinity Propagation.
>>> > > >> > > >
>>> > > >> > > > I recently submitted a PR [1] for a MiniBatch KMeans
>>> > > implementation,
>>> > > >> > > > and I saw an email on this list about interest in =
implementing
>>> > > Fuzzy
>>> > > >> > > > C-Means.
>>> > > >> > > >
>>> > > >> > > > Based on Sean Owen's review of my MiniBatch KMeans code, =
it
>>> > became
>>> > > >> > > > apparent that before I implement more clustering =
algorithms,
>>> it
>>> > > would
>>> > > >> > > > be useful to hammer out a framework to reduce code =
duplication
>>> > and
>>> > > >> > > > implement a consistent API.
>>> > > >> > > >
>>> > > >> > > > I'd like to gauge the interest and goals of the MLlib
>>> community:
>>> > > >> > > >
>>> > > >> > > > 1. Are you interested in having more clustering =
algorithms
>>> > > available=3F
>>> > > >> > > >
>>> > > >> > > > 2. Is the community interested in specifying a common
>>> framework=3F
>>> > > >> > > >
>>> > > >> > > > Thanks!
>>> > > >> > > > RJ
>>> > > >> > > >
>>> > > >> > > > [1] - https://github.com/apache/spark/pull/1248
>>> > > >> > > >
>>> > > >> > > >
>>> > > >> > > > --
>>> > > >> > > > em rnowling@gmail.com
>>> > > >> > > > c 954.496.2314
>>> > > >> > > >
>>> > > >> > >
>>> > > >> > >
>>> > > >> > >
>>> > > >> > > --
>>> > > >> > > Yee Yang Li Hector <http://google.com/+HectorYee>
>>> > > >> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
>>> > > >> > >
>>> > > >> >
>>> > > >>
>>> > > >>
>>> > > >>
>>> > > >> --
>>> > > >> Yee Yang Li Hector <http://google.com/+HectorYee>
>>> > > >> *google.com/+HectorYee <http://google.com/+HectorYee>*
>>> > > >>
>>> > >
>>> > >
>>> > >
>>> > > --
>>> > > em rnowling@gmail.com
>>> > > c 954.496.2314
>>> > >
>>> >
>>> >
>>> >
>>> > --
>>> > Yee Yang Li Hector <http://google.com/+HectorYee>
>>> > *google.com/+HectorYee <http://google.com/+HectorYee>*
>>> >
>>>
>>
>>
>>
>> --
>> Yee Yang Li Hector <http://google.com/+HectorYee>
>> *google.com/+HectorYee <http://google.com/+HectorYee>*
> --=20
> em rnowling@gmail.com
> c 954.496.2314
------Nodemailer-0.5.0-?=_1-1404909568185--

From dev-return-8261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 17:17:49 2014
Return-Path: <dev-return-8261-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E5BF010E40
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 17:17:49 +0000 (UTC)
Received: (qmail 68096 invoked by uid 500); 9 Jul 2014 17:17:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68044 invoked by uid 500); 9 Jul 2014 17:17:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68031 invoked by uid 99); 9 Jul 2014 17:17:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 17:17:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mridul@gmail.com designates 209.85.160.182 as permitted sender)
Received: from [209.85.160.182] (HELO mail-yk0-f182.google.com) (209.85.160.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 17:17:43 +0000
Received: by mail-yk0-f182.google.com with SMTP id 19so2932661ykq.41
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 10:17:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=3NI1HIS5iT9XPXC86PuyaeId3hBhV+n1MAbXapYphCA=;
        b=T83gdAqh/O//CrTxyT6jUrOtr74MsFKR2nqHN0m9QM4CypJ8NNUSMKU22P9DONnqZm
         QCJA734qm0jNYOMoe6tpYb0jS/CttPBCxgBbDKM1TTlcQEx06rkbK4LbFTxzO2aIK3HL
         vwrKrMHHe189vztXCgy4n8kV8VrsRX9oTZ/b3O0cYcBn4KzzNiNHK1mrXPTRX2qk67KK
         7g2VoAePmH8P/akcXMievc0IAphma7X41fBgpB6nd7xBCaIgDh82pp5JxOeb9Qwd9Xoc
         VbNDm+4uCW75oef9bIUkNMO4wKOKb1G+9qBZ+C57HVIEBGfiO0aDXl/7RAiwOXyE59fx
         5ECA==
MIME-Version: 1.0
X-Received: by 10.58.245.194 with SMTP id xq2mr41330225vec.26.1404926242989;
 Wed, 09 Jul 2014 10:17:22 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Wed, 9 Jul 2014 10:17:22 -0700 (PDT)
Date: Wed, 9 Jul 2014 22:47:22 +0530
Message-ID: <CAJiQeY+XDSjc_8ejWPAJeXJKhV54zSwDRYXfsAXyRvOcMBh-KA@mail.gmail.com>
Subject: Unresponsive to PR/jira changes
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,


  I noticed today that gmail has been marking most of the mails from
spark github/jira I was receiving to spam folder; and I was assuming
it was lull in activity due to spark summit for past few weeks !

In case I have commented on specific PR/JIRA issues and not followed
up, apologies for the same - please do reach out in case it is still
pending something from my end.



Regards,
Mridul

From dev-return-8262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 17:32:45 2014
Return-Path: <dev-return-8262-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6176A10EE0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 17:32:45 +0000 (UTC)
Received: (qmail 9444 invoked by uid 500); 9 Jul 2014 17:32:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9386 invoked by uid 500); 9 Jul 2014 17:32:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9374 invoked by uid 99); 9 Jul 2014 17:32:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 17:32:44 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.170 as permitted sender)
Received: from [209.85.212.170] (HELO mail-wi0-f170.google.com) (209.85.212.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 17:32:43 +0000
Received: by mail-wi0-f170.google.com with SMTP id cc10so3192062wib.3
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 10:32:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=ww1Q5ZeMkbHkmdbX6lg2DC1QauUyore++gkApkCua0I=;
        b=RLqfjWOXHs6uNfDTxby159QtbpIwg7lNXMrMqGsi1+b62jhkyoQGAQ9Fd9CS7NT0hK
         ZdzEKte0+WmiJ/HmlvAjuXuod8W95Jd+Gjyn/dWOQTgerIw0PQtw/034uQ8gXI7MbOhU
         Z10dKOXDx48JUehWfn6MtE7jmQRXnU3h3NFQ4Wo0SFYVl0ZH33/FHnQeX5cCVHuHCDAU
         X9sDtewX56FIQ428wsAOSoDPdXxs9pGKc1XuRKWgtORrQR4KmXgKL/Xs4HdyOj/Vi98X
         hF8gfVRu1EGa6N8dK65g9uIvZAJpAgo4dTXnnY+tmBvuFm2z0DSIazNUiDrbh0dq7Pe8
         UPOA==
MIME-Version: 1.0
X-Received: by 10.180.96.97 with SMTP id dr1mr13168962wib.19.1404927139129;
 Wed, 09 Jul 2014 10:32:19 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Wed, 9 Jul 2014 10:32:19 -0700 (PDT)
In-Reply-To: <CADtDQQL96NZVVDUQx4yLgMtU96BY7_TyB1k7YZyKngvsju50Yg@mail.gmail.com>
References: <1404903922.95325.YahooMailNeo@web190403.mail.sg3.yahoo.com>
	<CADtDQQL96NZVVDUQx4yLgMtU96BY7_TyB1k7YZyKngvsju50Yg@mail.gmail.com>
Date: Wed, 9 Jul 2014 10:32:19 -0700
Message-ID: <CAJgQjQ-a9mu4buYLvY0oNnxVLtKwNqGU+xTymu1EANNShCxhWQ@mail.gmail.com>
Subject: Re: Contribution to MLlib
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Cc: MEETHU MATHEW <meethu2006@yahoo.co.in>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I don't know if anyone is working on it either. If that JIRA is not
moved to Apache JIRA, feel free to create a new one and make a note
that you are working on it. Thanks! -Xiangrui

On Wed, Jul 9, 2014 at 4:56 AM, RJ Nowling <rnowling@gmail.com> wrote:
> Hi Meethu,
>
> There is no code for a Gaussian Mixture Model clustering algorithm in the
> repository, but I don't know if anyone is working on it.
>
> RJ
>
> On Wednesday, July 9, 2014, MEETHU MATHEW <meethu2006@yahoo.co.in> wrote:
>
>> Hi,
>>
>> I am interested in contributing a clustering algorithm towards MLlib of
>> Spark.I am focusing on Gaussian Mixture Model.
>> But I saw a JIRA @ https://spark-project.atlassian.net/browse/SPARK-952
>> regrading the same.I would like to know whether Gaussian Mixture Model
>> is already implemented or not.
>>
>>
>>
>> Thanks & Regards,
>> Meethu M
>
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314

From dev-return-8263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 18:43:54 2014
Return-Path: <dev-return-8263-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4272111E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 18:43:54 +0000 (UTC)
Received: (qmail 93468 invoked by uid 500); 9 Jul 2014 18:43:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93403 invoked by uid 500); 9 Jul 2014 18:43:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93389 invoked by uid 99); 9 Jul 2014 18:43:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 18:43:53 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of michaelmalak@yahoo.com designates 98.138.90.47 as permitted sender)
Received: from [98.138.90.47] (HELO nm9-vm1.bullet.mail.ne1.yahoo.com) (98.138.90.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 18:43:47 +0000
Received: from [98.138.100.116] by nm9.bullet.mail.ne1.yahoo.com with NNFMP; 09 Jul 2014 18:43:26 -0000
Received: from [98.139.212.248] by tm107.bullet.mail.ne1.yahoo.com with NNFMP; 09 Jul 2014 18:43:26 -0000
Received: from [127.0.0.1] by omp1057.mail.bf1.yahoo.com with NNFMP; 09 Jul 2014 18:43:26 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 341487.16068.bm@omp1057.mail.bf1.yahoo.com
Received: (qmail 99332 invoked by uid 60001); 9 Jul 2014 18:43:26 -0000
X-YMail-OSG: ajXopLsVM1lxoyVKP.nEECD0FdaEA2lS.m5sJ5kHbISdKQ3
 46KAvjwe8E_QpmfhyG4mlSC7aCDGq1kJWy.FYI2bNGwvnOpUFJ_IL07vmnB0
 fBqI_hRftalSkvY5EbrSvPskUZ0FBYwBK6qhktbdk0vAMUZMz.FD_kqgTStD
 RP9RE8mRTU15j_9OOyaI13qU.tik.noLGN3AHGFfnwjykCE0HejgO0MmYmMf
 Bu5uJPn8B6TAaE4a9fxWMKfnT7aCRUV.sMcX5LBCbT27g5hQb7LiyvVomjx8
 sQm2waU9sPHJROOqwvNVyaWW5l6R_uRf4yYKLQwxe1NW9Uu4o1kDkUsKPCo3
 b.P20pvMLzJOQsNFP.bSZbk_aDiPTLcyDHB_elEiqsudz3G4wV_629H7zS24
 vcD9xQFw.HflsfghcRAorQeLlaSmU9lM_NdgV8ho_jUcq5nBjiry9LwpcAzc
 DP0TCzb7pwbrd7QxERUQLAukCBYcuPSsawrJ7HTdSCJbFWpriJnZqFkEXpyr
 PlqToq3TOvQ.egsbNZ8ngzBZxCksn6WCeSDwKhO1q7tDbH843Nb6l2Vq.3Kd
 4jrTeHQqC7gYeuuQ18pZXtfUIQlMxARAgR1Y5cc7mKSnCzP._UemIfl11X1x
 FlDGzc_nfY3SJphnBS2GmseMkxUQRCZLkF.Zc8i_S12a6nBvFgIYAWwPdNOe
 FSxKBRmr3RX4KxPe5YxqDMg5LNOry11P1CQ89bmhTFfcP1reOdr9bztR0SL9
 oZ3IiWz4cAE6VEOaX8TvDnvap6dhCqsqPeydX1LPABRsrF0fxeSdzymOmfgo
 NGIQwY6pFc.9NO6rW.9TU5Ufqh8VwMqyRCnLBdKOpZpksIFm6vMCw2ACkmHJ
 9ZN2YDDHuKIGFB3RxquZ5yB3yrOw0JrpIpg.B6hMfGWZ5.rVU5ySe3PDo8SA
 qI0BhHKZOmZaOc9zQ7GGu.4RHCtKD0yL6FiFpPtJ.azIZ1UdqJeGxPrw0Uen
 dn1me1FdwmaaQ4UWuDNoXguTRa_Z6mhzT_cwjcu3m4B1hdIyB7eMN_o4FDnY
 N_8vIJA--
Received: from [148.87.67.202] by web140401.mail.bf1.yahoo.com via HTTP; Wed, 09 Jul 2014 11:43:26 PDT
X-Rocket-MIMEInfo: 002.001,QXQgU3BhcmsgU3VtbWl0LCBQYXRyaWNrIFdlbmRlbGwgaW5kaWNhdGVkIHRoZSBudW1iZXIgb2YgTUxsaWIgYWxnb3JpdGhtcyB3b3VsZCAicm91Z2hseSBkb3VibGUiIGluIDEuMSBmcm9tIHRoZSBjdXJyZW50IGFwcHJveC4gMTUuCmh0dHA6Ly9zcGFyay1zdW1taXQub3JnL3dwLWNvbnRlbnQvdXBsb2Fkcy8yMDE0LzA3L0Z1dHVyZS1vZi1TcGFyay1QYXRyaWNrLVdlbmRlbGwucGRmCgpXaGF0IGFyZSB0aGUgcGxhbm5lZCBhZGRpdGlvbmFsIGFsZ29yaXRobXM_CgpJbiBKaXJhLCBJIG9ubHkgc2VlIHR3byABMAEBAQE-
X-Mailer: YahooMailWebService/0.8.191.1
Message-ID: <1404931406.35334.YahooMailNeo@web140401.mail.bf1.yahoo.com>
Date: Wed, 9 Jul 2014 11:43:26 -0700
From: Michael Malak <michaelmalak@yahoo.com.INVALID>
Reply-To: Michael Malak <michaelmalak@yahoo.com>
Subject: 15 new MLlib algorithms
To: "dev@spark.apache.org" <dev@spark.apache.org>
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
X-Virus-Checked: Checked by ClamAV on apache.org

At Spark Summit, Patrick Wendell indicated the number of MLlib algorithms would "roughly double" in 1.1 from the current approx. 15.
http://spark-summit.org/wp-content/uploads/2014/07/Future-of-Spark-Patrick-Wendell.pdf

What are the planned additional algorithms?

In Jira, I only see two when filtering on version 1.1, component MLlib: one on multi-label and another on high dimensionality.

https://issues.apache.org/jira/browse/SPARK-2329?jql=issuetype%20in%20(Brainstorming%2C%20Epic%2C%20%22New%20Feature%22%2C%20Story)%20AND%20fixVersion%20%3D%201.1.0%20AND%20component%20%3D%20MLlib

http://tinyurl.com/ku7sehu

From dev-return-8264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 19:23:46 2014
Return-Path: <dev-return-8264-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B40D21144C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 19:23:46 +0000 (UTC)
Received: (qmail 86381 invoked by uid 500); 9 Jul 2014 19:23:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86320 invoked by uid 500); 9 Jul 2014 19:23:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86303 invoked by uid 99); 9 Jul 2014 19:23:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 19:23:45 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.147 as permitted sender)
Received: from [169.229.218.147] (HELO cm06fe.IST.Berkeley.EDU) (169.229.218.147)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 19:23:40 +0000
Received: from mail-vc0-f180.google.com ([209.85.220.180])
	by cm06fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1X4xSY-0003IH-LI
	for dev@spark.apache.org; Wed, 09 Jul 2014 12:23:19 -0700
Received: by mail-vc0-f180.google.com with SMTP id im17so7888109vcb.25
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 12:23:17 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.220.252.198 with SMTP id mx6mr24429341vcb.15.1404933797630;
 Wed, 09 Jul 2014 12:23:17 -0700 (PDT)
Received: by 10.220.240.144 with HTTP; Wed, 9 Jul 2014 12:23:17 -0700 (PDT)
Date: Wed, 9 Jul 2014 12:23:17 -0700
Message-ID: <CAKJXNjHHXjnvKyxk+8-5aT_tJaoBeUUT=Adc2BR0AAN+TAAsHg@mail.gmail.com>
Subject: CPU/Disk/network performance instrumentation
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e013a044af0de3704fdc7a35e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a044af0de3704fdc7a35e
Content-Type: text/plain; charset=UTF-8

Hi all,

I've been doing a bunch of performance measurement of Spark and, as part of
doing this, added metrics that record the average CPU utilization, disk
throughput and utilization for each block device, and network throughput
while each task is running.  These metrics are collected by reading the
/proc filesystem so work only on Linux.  I'm happy to submit a pull request
with the appropriate changes but first wanted to see if sufficiently many
people think this would be useful.  I know the metrics reported by Spark
(and in the UI) are already overwhelming to some folks so don't want to add
more instrumentation if it's not widely useful.

These metrics are slightly more difficult to interpret for Spark than
similar metrics reported by Hadoop because, with Spark, multiple tasks run
in the same JVM and therefore as part of the same process.  This means
that, for example, the CPU utilization metrics reflect the CPU use across
all tasks in the JVM, rather than only the CPU time used by the particular
task.  This is a pro and a con -- it makes it harder to determine why
utilization is high (it may be from a different task) but it also makes the
metrics useful for diagnosing straggler problems.  Just wanted to clarify
this before asking folks to weigh in on whether the added metrics would be
useful.

-Kay

(if you're curious, the instrumentation code is on a very messy branch
here:
https://github.com/kayousterhout/spark-1/tree/proc_logging_perf_minimal_temp/core/src/main/scala/org/apache/spark/performance_logging
)

--089e013a044af0de3704fdc7a35e--

From dev-return-8265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 19:26:05 2014
Return-Path: <dev-return-8265-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7554711464
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 19:26:05 +0000 (UTC)
Received: (qmail 92206 invoked by uid 500); 9 Jul 2014 19:26:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92150 invoked by uid 500); 9 Jul 2014 19:26:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92134 invoked by uid 99); 9 Jul 2014 19:26:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 19:26:04 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.172] (HELO mail-vc0-f172.google.com) (209.85.220.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 19:26:01 +0000
Received: by mail-vc0-f172.google.com with SMTP id hy10so8140229vcb.3
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 12:25:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=o+kUYihLc5pf6+xILcOVgfctyBBNb6IPb00AqrZHqmQ=;
        b=UR1rbrUB7d0jDPVVDgA+tyL1PsWLDmMA7Qz7d48WINUl1V7xJHAwcV4kVvzKe/3A1U
         4LIeC3JCs/lcvRIR7LgjzwEP/ZGH4RMc63lezDL5bsfiu7+UmIz7eOfyzhWEK9FpZnst
         TJU+ZvV2hEopMyoMd0+ZYIBorRzv/g4qrSCzrJgqfAAAu9dK38dcGwzxm5V5FHj1lquu
         HbZJqzpy/ftbo+9xwvSdkPmR6BFWcjV5qoRy6IjSCTq+5s+JBW+0MRUX7iG8InGg7cCi
         8Hx6vVFg8QnOnTEHAVRV/yxeohREj6OYgBpYtjc9Z5+A3EskglqqJAT2aIXEtE+xzJZt
         S86Q==
X-Gm-Message-State: ALoCoQn7sj67Nd7Mv1+dWU3hV97BtOfeXtaJcehrp19jwRSyceSiCdyObrHvSFumB+vC+czsI/gy
X-Received: by 10.220.81.194 with SMTP id y2mr26230671vck.29.1404933936715;
 Wed, 09 Jul 2014 12:25:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 9 Jul 2014 12:25:16 -0700 (PDT)
In-Reply-To: <CAKJXNjHHXjnvKyxk+8-5aT_tJaoBeUUT=Adc2BR0AAN+TAAsHg@mail.gmail.com>
References: <CAKJXNjHHXjnvKyxk+8-5aT_tJaoBeUUT=Adc2BR0AAN+TAAsHg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 9 Jul 2014 12:25:16 -0700
Message-ID: <CAPh_B=aWUrRjW9SH9c7H_Q5duWnQbuvCoyVzGujr=UBV-h3s4w@mail.gmail.com>
Subject: Re: CPU/Disk/network performance instrumentation
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2d8603b3dec04fdc7ac5c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d8603b3dec04fdc7ac5c
Content-Type: text/plain; charset=UTF-8

Maybe it's time to create an advanced mode in the ui.


On Wed, Jul 9, 2014 at 12:23 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
wrote:

> Hi all,
>
> I've been doing a bunch of performance measurement of Spark and, as part of
> doing this, added metrics that record the average CPU utilization, disk
> throughput and utilization for each block device, and network throughput
> while each task is running.  These metrics are collected by reading the
> /proc filesystem so work only on Linux.  I'm happy to submit a pull request
> with the appropriate changes but first wanted to see if sufficiently many
> people think this would be useful.  I know the metrics reported by Spark
> (and in the UI) are already overwhelming to some folks so don't want to add
> more instrumentation if it's not widely useful.
>
> These metrics are slightly more difficult to interpret for Spark than
> similar metrics reported by Hadoop because, with Spark, multiple tasks run
> in the same JVM and therefore as part of the same process.  This means
> that, for example, the CPU utilization metrics reflect the CPU use across
> all tasks in the JVM, rather than only the CPU time used by the particular
> task.  This is a pro and a con -- it makes it harder to determine why
> utilization is high (it may be from a different task) but it also makes the
> metrics useful for diagnosing straggler problems.  Just wanted to clarify
> this before asking folks to weigh in on whether the added metrics would be
> useful.
>
> -Kay
>
> (if you're curious, the instrumentation code is on a very messy branch
> here:
>
> https://github.com/kayousterhout/spark-1/tree/proc_logging_perf_minimal_temp/core/src/main/scala/org/apache/spark/performance_logging
> )
>

--001a11c2d8603b3dec04fdc7ac5c--

From dev-return-8266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 19:31:59 2014
Return-Path: <dev-return-8266-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 87840114A4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 19:31:59 +0000 (UTC)
Received: (qmail 15169 invoked by uid 500); 9 Jul 2014 19:31:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15108 invoked by uid 500); 9 Jul 2014 19:31:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15097 invoked by uid 99); 9 Jul 2014 19:31:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 19:31:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of byavuz@stanford.edu designates 171.67.219.81 as permitted sender)
Received: from [171.67.219.81] (HELO smtp.stanford.edu) (171.67.219.81)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 19:31:55 +0000
Received: from codegreen2.stanford.edu (codegreen2.Stanford.EDU [171.67.224.3])
	(using TLSv1 with cipher DHE-RSA-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by smtp.stanford.edu (Postfix) with ESMTPS id 3C05B21C3C;
	Wed,  9 Jul 2014 12:31:30 -0700 (PDT)
Received: from codegreen2.stanford.edu (localhost.localdomain [127.0.0.1])
	by codegreen2.stanford.edu (Postfix) with ESMTP id 1D82E65;
	Wed,  9 Jul 2014 12:31:30 -0700 (PDT)
Received: from smtp.stanford.edu (smtp2.Stanford.EDU [171.67.219.82])
	(using TLSv1 with cipher ADH-AES256-SHA (256/256 bits))
	(No client certificate requested)
	by codegreen2.stanford.edu (Postfix) with ESMTP id 090DC65;
	Wed,  9 Jul 2014 12:31:30 -0700 (PDT)
Received: from smtp.stanford.edu (localhost [127.0.0.1])
	by localhost (Postfix) with SMTP id D13443414BF;
	Wed,  9 Jul 2014 12:31:29 -0700 (PDT)
Received: from zm01.stanford.edu (zm01.Stanford.EDU [171.67.219.145])
	by smtp.stanford.edu (Postfix) with ESMTP id 7795634159A;
	Wed,  9 Jul 2014 12:31:29 -0700 (PDT)
Date: Wed, 9 Jul 2014 12:31:29 -0700 (PDT)
From: Burak Yavuz <byavuz@stanford.edu>
To: dev@spark.apache.org, Michael Malak <michaelmalak@yahoo.com>
Message-ID: <1756836064.4169024.1404934289291.JavaMail.zimbra@stanford.edu>
In-Reply-To: <1404931406.35334.YahooMailNeo@web140401.mail.bf1.yahoo.com>
References: <1404931406.35334.YahooMailNeo@web140401.mail.bf1.yahoo.com>
Subject: Re: 15 new MLlib algorithms
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [67.164.94.237]
X-Mailer: Zimbra 8.0.7_GA_6021 (ZimbraWebClient - GC35 (Mac)/8.0.7_GA_6021)
X-Authenticated-User: byavuz@stanford.edu
Thread-Topic: 15 new MLlib algorithms
Thread-Index: ZM/lV0jhUzQm4DYgrXvgsM+QZWpuig==
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

The roadmap for the 1.1 release and MLLib includes algorithms such as:

Non-negative matrix factorization, Sparse SVD, Multiclass 
decision tree, Random Forests (?)

and optimizers such as:
ADMM, Accelerated gradient methods

also a statistical toolbox that includes:
descriptive statistics, sampling, hypothesis testing

and hopefully Parallel model training for autotuning.

Source:
https://databricks-training.s3.amazonaws.com/slides/Spark_Summit_MLlib_070214_v2.pdf

Best,
Burak



----- Original Message -----
From: "Michael Malak" <michaelmalak@yahoo.com.INVALID>
To: dev@spark.apache.org
Sent: Wednesday, July 9, 2014 11:43:26 AM
Subject: 15 new MLlib algorithms

At Spark Summit, Patrick Wendell indicated the number of MLlib algorithms would "roughly double" in 1.1 from the current approx. 15.
http://spark-summit.org/wp-content/uploads/2014/07/Future-of-Spark-Patrick-Wendell.pdf

What are the planned additional algorithms?

In Jira, I only see two when filtering on version 1.1, component MLlib: one on multi-label and another on high dimensionality.

https://issues.apache.org/jira/browse/SPARK-2329?jql=issuetype%20in%20(Brainstorming%2C%20Epic%2C%20%22New%20Feature%22%2C%20Story)%20AND%20fixVersion%20%3D%201.1.0%20AND%20component%20%3D%20MLlib

http://tinyurl.com/ku7sehu


From dev-return-8267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 21:18:18 2014
Return-Path: <dev-return-8267-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 083E311A38
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 21:18:18 +0000 (UTC)
Received: (qmail 72936 invoked by uid 500); 9 Jul 2014 21:18:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72868 invoked by uid 500); 9 Jul 2014 21:18:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72857 invoked by uid 99); 9 Jul 2014 21:18:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:18:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 209.85.212.177 as permitted sender)
Received: from [209.85.212.177] (HELO mail-wi0-f177.google.com) (209.85.212.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:18:13 +0000
Received: by mail-wi0-f177.google.com with SMTP id ho1so3482787wib.16
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 14:17:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=dUIHWhHvBx6yVPZNp/rJxRQWTM/E3jF7+5iDbNb/hc0=;
        b=F2ShmozDmb9FJ22BeA41iQAcoFnEgmHjGNJ70CAiPM9bA6WFTLXTZal/wIas+qWzRB
         11EHsUvUWJIIPa01GHaI3hct8iVXGxkmxcK65JduY+dkyqiPP4xTSoxYKIbvHl/OfFFs
         p+B8nq9JyjoYV769dyvVzKWIMIVhkT3BGKNZqqpc1yzzx5r3EhIGG0zFwXFW/776jmd5
         3ldBlCYNby7RDiNPVwkjXwLKPWLl0Hv8Z6WL0jl2VKZBIl5gzJeYvqXAYVumL82nzLKR
         HsB2IqUqdWvNEupgTb1LkDbtmJAyBZ5btzz1JT8xtNHmukU8v/gNwlatrzanaFf0npsQ
         wNlA==
X-Gm-Message-State: ALoCoQmoIZ6llMNqw4HDXQhEhpV4KajZylMiQ+qcu1c9WOC5BOFMLoB5lTUX7gWXT9r/khPW5CeM
MIME-Version: 1.0
X-Received: by 10.180.94.5 with SMTP id cy5mr13844997wib.11.1404940671927;
 Wed, 09 Jul 2014 14:17:51 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.216.94.195 with HTTP; Wed, 9 Jul 2014 14:17:51 -0700 (PDT)
In-Reply-To: <CAPh_B=aWUrRjW9SH9c7H_Q5duWnQbuvCoyVzGujr=UBV-h3s4w@mail.gmail.com>
References: <CAKJXNjHHXjnvKyxk+8-5aT_tJaoBeUUT=Adc2BR0AAN+TAAsHg@mail.gmail.com>
	<CAPh_B=aWUrRjW9SH9c7H_Q5duWnQbuvCoyVzGujr=UBV-h3s4w@mail.gmail.com>
Date: Wed, 9 Jul 2014 14:17:51 -0700
Message-ID: <CAKx7Bf_rwd5CpLCiA0_MuTVbPE+N__eQX7=jPnbdNdV0sgH54Q@mail.gmail.com>
Subject: Re: CPU/Disk/network performance instrumentation
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04440356ae546504fdc93dc4
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04440356ae546504fdc93dc4
Content-Type: text/plain; charset=UTF-8

I think it would be very useful to have this. We could put the ui display
either behind a flag or a url parameter

Shivaram


On Wed, Jul 9, 2014 at 12:25 PM, Reynold Xin <rxin@databricks.com> wrote:

> Maybe it's time to create an advanced mode in the ui.
>
>
> On Wed, Jul 9, 2014 at 12:23 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
> wrote:
>
> > Hi all,
> >
> > I've been doing a bunch of performance measurement of Spark and, as part
> of
> > doing this, added metrics that record the average CPU utilization, disk
> > throughput and utilization for each block device, and network throughput
> > while each task is running.  These metrics are collected by reading the
> > /proc filesystem so work only on Linux.  I'm happy to submit a pull
> request
> > with the appropriate changes but first wanted to see if sufficiently many
> > people think this would be useful.  I know the metrics reported by Spark
> > (and in the UI) are already overwhelming to some folks so don't want to
> add
> > more instrumentation if it's not widely useful.
> >
> > These metrics are slightly more difficult to interpret for Spark than
> > similar metrics reported by Hadoop because, with Spark, multiple tasks
> run
> > in the same JVM and therefore as part of the same process.  This means
> > that, for example, the CPU utilization metrics reflect the CPU use across
> > all tasks in the JVM, rather than only the CPU time used by the
> particular
> > task.  This is a pro and a con -- it makes it harder to determine why
> > utilization is high (it may be from a different task) but it also makes
> the
> > metrics useful for diagnosing straggler problems.  Just wanted to clarify
> > this before asking folks to weigh in on whether the added metrics would
> be
> > useful.
> >
> > -Kay
> >
> > (if you're curious, the instrumentation code is on a very messy branch
> > here:
> >
> >
> https://github.com/kayousterhout/spark-1/tree/proc_logging_perf_minimal_temp/core/src/main/scala/org/apache/spark/performance_logging
> > )
> >
>

--f46d04440356ae546504fdc93dc4--

From dev-return-8268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 21:20:37 2014
Return-Path: <dev-return-8268-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 57DD711A4F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 21:20:37 +0000 (UTC)
Received: (qmail 87308 invoked by uid 500); 9 Jul 2014 21:20:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87249 invoked by uid 500); 9 Jul 2014 21:20:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87231 invoked by uid 99); 9 Jul 2014 21:20:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:20:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:20:35 +0000
Received: by mail-vc0-f174.google.com with SMTP id hy4so8600063vcb.33
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 14:20:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bFFse1Z5nzum+Xt4Ld2ld9YYpvTe9ekwVIU/0g8YtbY=;
        b=sdaRn7LiyBFnX8dDuFWJ+kK13/Yb28mnjDd7hxP7VDpkGZyAuqw7v6O7p565yJJ34E
         0cPPuni6cxAcUa1K+g2OKgCaH4T0ttoYTB40GYrXa/vUDw917FhVbmUKkaA/Q+V+4UAM
         bgVUoSjVPoM2sSwpzsLACqSZgPu4yVMsz4g7BSlfUdo//g+JsKtTjeNjUwe+nl0Pq3JU
         y1WcTvKUlREvjtoXNX9OGirKEv+J7pUlKhZG0JHep664OGvZ5c0HcYi/t2chlPk+Sq8v
         7b6pVHTsNGm83+X0QWqXaRjEwxCwPa28f/a1v/n5WZSOfMIR0KjYpGsn7lTGGSmCuiN5
         wFvQ==
MIME-Version: 1.0
X-Received: by 10.52.12.229 with SMTP id b5mr1980031vdc.52.1404940810388; Wed,
 09 Jul 2014 14:20:10 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Wed, 9 Jul 2014 14:20:10 -0700 (PDT)
In-Reply-To: <CAPh_B=aWUrRjW9SH9c7H_Q5duWnQbuvCoyVzGujr=UBV-h3s4w@mail.gmail.com>
References: <CAKJXNjHHXjnvKyxk+8-5aT_tJaoBeUUT=Adc2BR0AAN+TAAsHg@mail.gmail.com>
	<CAPh_B=aWUrRjW9SH9c7H_Q5duWnQbuvCoyVzGujr=UBV-h3s4w@mail.gmail.com>
Date: Thu, 10 Jul 2014 02:50:10 +0530
Message-ID: <CAJiQeY+cTnZcsfXK90vTGx_6x5mbRjeAo5iuy2p0yQSGqtnBZQ@mail.gmail.com>
Subject: Re: CPU/Disk/network performance instrumentation
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1 on advanced mode !

Regards.
Mridul

On Thu, Jul 10, 2014 at 12:55 AM, Reynold Xin <rxin@databricks.com> wrote:
> Maybe it's time to create an advanced mode in the ui.
>
>
> On Wed, Jul 9, 2014 at 12:23 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
> wrote:
>
>> Hi all,
>>
>> I've been doing a bunch of performance measurement of Spark and, as part of
>> doing this, added metrics that record the average CPU utilization, disk
>> throughput and utilization for each block device, and network throughput
>> while each task is running.  These metrics are collected by reading the
>> /proc filesystem so work only on Linux.  I'm happy to submit a pull request
>> with the appropriate changes but first wanted to see if sufficiently many
>> people think this would be useful.  I know the metrics reported by Spark
>> (and in the UI) are already overwhelming to some folks so don't want to add
>> more instrumentation if it's not widely useful.
>>
>> These metrics are slightly more difficult to interpret for Spark than
>> similar metrics reported by Hadoop because, with Spark, multiple tasks run
>> in the same JVM and therefore as part of the same process.  This means
>> that, for example, the CPU utilization metrics reflect the CPU use across
>> all tasks in the JVM, rather than only the CPU time used by the particular
>> task.  This is a pro and a con -- it makes it harder to determine why
>> utilization is high (it may be from a different task) but it also makes the
>> metrics useful for diagnosing straggler problems.  Just wanted to clarify
>> this before asking folks to weigh in on whether the added metrics would be
>> useful.
>>
>> -Kay
>>
>> (if you're curious, the instrumentation code is on a very messy branch
>> here:
>>
>> https://github.com/kayousterhout/spark-1/tree/proc_logging_perf_minimal_temp/core/src/main/scala/org/apache/spark/performance_logging
>> )
>>

From dev-return-8269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 21:37:10 2014
Return-Path: <dev-return-8269-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EAAFD11B31
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 21:37:10 +0000 (UTC)
Received: (qmail 29950 invoked by uid 500); 9 Jul 2014 21:37:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29895 invoked by uid 500); 9 Jul 2014 21:37:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 29884 invoked by uid 99); 9 Jul 2014 21:37:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:37:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:37:04 +0000
Received: by mail-wi0-f173.google.com with SMTP id cc10so3500752wib.0
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 14:36:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=DNx0CLVazyYXJ/gjqqk8FEuwqCecGKGM59TB0SWcIzY=;
        b=TE2FFf3X8z7VjjbHX49M1NCIU34AKt9n/H3on0MX82ExYUbDXPL/1sX6LlZ/ILAI7p
         VBZJPAm2AlJ4/R6hTJ9LNwArIGI2xcAr+zWOzxCqCd4C+PPWrFeam0tjSFPEHk8fVus+
         j+6BscJP6+SX3YRRcSRfaB1CiK7gZJKaM2glKyhor2sUDi3938eWI4HPyNK05Ae/2mSW
         RW1HZjd9bW5C2g38sEqqwHD/Y7M2wjJHFmg3qzdaobKMH5edUwNgyNg6e8Zd9Bpt9NkM
         21txQamFdPhelDIySoOHvBz1YnwAxcT1AQz9fspbHcj3FjUJ5BURXjjn5gqUxhjB5j5Y
         OYGQ==
X-Gm-Message-State: ALoCoQnIazsJDJCBv3Wip0zt/rHTwslM1cdvsQf+cAU1rBX2fIqOR2ql5IEQXA3u62AA8gcE8YFW
MIME-Version: 1.0
X-Received: by 10.194.78.141 with SMTP id b13mr14826098wjx.111.1404941803264;
 Wed, 09 Jul 2014 14:36:43 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Wed, 9 Jul 2014 14:36:43 -0700 (PDT)
Date: Wed, 9 Jul 2014 14:36:43 -0700
Message-ID: <CAAsvFPmyXCGd_fmiVH9d4-nKWwEj13cHO=mz-G0Usaz=pR12ow@mail.gmail.com>
Subject: ExecutorState.LOADING?
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfcfc821d827a04fdc981b3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfc821d827a04fdc981b3
Content-Type: text/plain; charset=UTF-8

Doesn't look to me like this is used.  Does anybody recall what it was
intended for?

--047d7bfcfc821d827a04fdc981b3--

From dev-return-8270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 21:44:29 2014
Return-Path: <dev-return-8270-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2901211B5B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 21:44:29 +0000 (UTC)
Received: (qmail 42950 invoked by uid 500); 9 Jul 2014 21:44:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42891 invoked by uid 500); 9 Jul 2014 21:44:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42879 invoked by uid 99); 9 Jul 2014 21:44:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:44:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of suren.hiraman@sociocast.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:44:26 +0000
Received: by mail-vc0-f173.google.com with SMTP id lf12so8690441vcb.32
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 14:44:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=1IevjaNw1UkV6VIjyJZEkQ4IvzpFQvvbOqxxBWcFTGQ=;
        b=M3zAGR/vbf4TwjhMG9Tb/q85QgMgmGnyKsX3wGdFwqp6OM9DuHVRRaQDOoVRMu+/7G
         JSF9EWRG96ys5uTaWnsq6ts/r0Tz3ptXrF/JxpWMwJBchKrEQ7nrJeWPwvIHrvhKcoEf
         wXvQwevbkejYk+zA9fV8lL8e2arWWdYBr5NKgYzyHT3eI1CPdgWREfUVhzatnJKtUGkL
         cxiRoVAfTgeWu9/5zHUYSQKzTObUPJ1hkjAMBWCR9ij+9Uc/7OwRdJGc2NRBBoqElPEG
         AjAxRRpaMTRyDC752MuwNusy/COgtMrif6TE7SyJEnxK7wvtfBV4vmPuY2lnPa3U27XV
         VSlA==
X-Gm-Message-State: ALoCoQnavhW1kO2PIN9vt34MzKY313BZ+EiSwr50yDZSjfqJiyXhye9QTLNbo8u8wAQgEoHEwin1
MIME-Version: 1.0
X-Received: by 10.52.25.228 with SMTP id f4mr476376vdg.62.1404942241430; Wed,
 09 Jul 2014 14:44:01 -0700 (PDT)
Received: by 10.58.137.197 with HTTP; Wed, 9 Jul 2014 14:44:01 -0700 (PDT)
In-Reply-To: <CAJiQeY+cTnZcsfXK90vTGx_6x5mbRjeAo5iuy2p0yQSGqtnBZQ@mail.gmail.com>
References: <CAKJXNjHHXjnvKyxk+8-5aT_tJaoBeUUT=Adc2BR0AAN+TAAsHg@mail.gmail.com>
	<CAPh_B=aWUrRjW9SH9c7H_Q5duWnQbuvCoyVzGujr=UBV-h3s4w@mail.gmail.com>
	<CAJiQeY+cTnZcsfXK90vTGx_6x5mbRjeAo5iuy2p0yQSGqtnBZQ@mail.gmail.com>
Date: Wed, 9 Jul 2014 17:44:01 -0400
Message-ID: <CALWDz_uepE9BU2GyZBtJqBazRnnDmj336PMH_xm7OiqaNOanSw@mail.gmail.com>
Subject: Re: CPU/Disk/network performance instrumentation
From: Surendranauth Hiraman <suren.hiraman@velos.io>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113697643b194d04fdc99bbc
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113697643b194d04fdc99bbc
Content-Type: text/plain; charset=UTF-8

+1 on advanced tab.



On Wed, Jul 9, 2014 at 5:20 PM, Mridul Muralidharan <mridul@gmail.com>
wrote:

> +1 on advanced mode !
>
> Regards.
> Mridul
>
> On Thu, Jul 10, 2014 at 12:55 AM, Reynold Xin <rxin@databricks.com> wrote:
> > Maybe it's time to create an advanced mode in the ui.
> >
> >
> > On Wed, Jul 9, 2014 at 12:23 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
> > wrote:
> >
> >> Hi all,
> >>
> >> I've been doing a bunch of performance measurement of Spark and, as
> part of
> >> doing this, added metrics that record the average CPU utilization, disk
> >> throughput and utilization for each block device, and network throughput
> >> while each task is running.  These metrics are collected by reading the
> >> /proc filesystem so work only on Linux.  I'm happy to submit a pull
> request
> >> with the appropriate changes but first wanted to see if sufficiently
> many
> >> people think this would be useful.  I know the metrics reported by Spark
> >> (and in the UI) are already overwhelming to some folks so don't want to
> add
> >> more instrumentation if it's not widely useful.
> >>
> >> These metrics are slightly more difficult to interpret for Spark than
> >> similar metrics reported by Hadoop because, with Spark, multiple tasks
> run
> >> in the same JVM and therefore as part of the same process.  This means
> >> that, for example, the CPU utilization metrics reflect the CPU use
> across
> >> all tasks in the JVM, rather than only the CPU time used by the
> particular
> >> task.  This is a pro and a con -- it makes it harder to determine why
> >> utilization is high (it may be from a different task) but it also makes
> the
> >> metrics useful for diagnosing straggler problems.  Just wanted to
> clarify
> >> this before asking folks to weigh in on whether the added metrics would
> be
> >> useful.
> >>
> >> -Kay
> >>
> >> (if you're curious, the instrumentation code is on a very messy branch
> >> here:
> >>
> >>
> https://github.com/kayousterhout/spark-1/tree/proc_logging_perf_minimal_temp/core/src/main/scala/org/apache/spark/performance_logging
> >> )
> >>
>



-- 

SUREN HIRAMAN, VP TECHNOLOGY
Velos
Accelerating Machine Learning

440 NINTH AVENUE, 11TH FLOOR
NEW YORK, NY 10001
O: (917) 525-2466 ext. 105
F: 646.349.4063
E: suren.hiraman@v <suren.hiraman@sociocast.com>elos.io
W: www.velos.io

--001a113697643b194d04fdc99bbc--

From dev-return-8271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 21:49:35 2014
Return-Path: <dev-return-8271-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E151111BC0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 21:49:34 +0000 (UTC)
Received: (qmail 59077 invoked by uid 500); 9 Jul 2014 21:49:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59016 invoked by uid 500); 9 Jul 2014 21:49:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59005 invoked by uid 99); 9 Jul 2014 21:49:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:49:34 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.142 as permitted sender)
Received: from [169.229.218.142] (HELO cm01fe.IST.Berkeley.EDU) (169.229.218.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 21:49:29 +0000
Received: from mail-vc0-f169.google.com ([209.85.220.169])
	by cm01fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1X4zjg-0005Le-4V
	for dev@spark.apache.org; Wed, 09 Jul 2014 14:49:09 -0700
Received: by mail-vc0-f169.google.com with SMTP id la4so8887989vcb.0
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 14:49:07 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.52.182.163 with SMTP id ef3mr34804224vdc.22.1404942547437;
 Wed, 09 Jul 2014 14:49:07 -0700 (PDT)
Received: by 10.220.240.144 with HTTP; Wed, 9 Jul 2014 14:49:07 -0700 (PDT)
In-Reply-To: <CAAsvFPmyXCGd_fmiVH9d4-nKWwEj13cHO=mz-G0Usaz=pR12ow@mail.gmail.com>
References: <CAAsvFPmyXCGd_fmiVH9d4-nKWwEj13cHO=mz-G0Usaz=pR12ow@mail.gmail.com>
Date: Wed, 9 Jul 2014 14:49:07 -0700
Message-ID: <CAKJXNjEcPK3zirQXuXEEP-wCt+GLaEkHJCyUhpTygoptpAtXbQ@mail.gmail.com>
Subject: Re: ExecutorState.LOADING?
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec548a7a378506b04fdc9adb7
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec548a7a378506b04fdc9adb7
Content-Type: text/plain; charset=UTF-8

Git history to the rescue!  It seems to have been added by Matei way back
in July 2012:
https://github.com/apache/spark/commit/5d1a887bed8423bd6c25660910d18d91880e01fe

and then was removed a few months later (replaced by RUNNING) by the same
Mr. Zaharia:
https://github.com/apache/spark/commit/bb1bce79240da22c2677d9f8159683cdf73158c2#diff-776a630ac2b2ec5fe85c07ca20a58fc0

So I'd say it's safe to delete it.


On Wed, Jul 9, 2014 at 2:36 PM, Mark Hamstra <mark@clearstorydata.com>
wrote:

> Doesn't look to me like this is used.  Does anybody recall what it was
> intended for?
>

--bcaec548a7a378506b04fdc9adb7--

From dev-return-8272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 22:08:31 2014
Return-Path: <dev-return-8272-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63A8A11D22
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 22:08:31 +0000 (UTC)
Received: (qmail 99290 invoked by uid 500); 9 Jul 2014 22:08:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99222 invoked by uid 500); 9 Jul 2014 22:08:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99210 invoked by uid 99); 9 Jul 2014 22:08:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 22:08:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 22:08:26 +0000
Received: by mail-we0-f170.google.com with SMTP id w61so8147376wes.1
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 15:08:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=NJdTdCWffnWlQNtb76+qecrTqSmgnDz8ZwRw3HUJ+Tg=;
        b=XnjMEjdLyQSbEJBfMslthfib6yWOxIxfFxhsAXf7eayBTsckoiYCZIRi4S59H8NmqV
         mWjb6vj80vC5RmuvyesmPbv+NK39Pc+xPkxL4pw5zRqcaYSwx/DSHjcTW4NdshasNALB
         OQtWGu3etKM2IrFtnDxnOq9nBjJdUkAGhieuMhhXOSVtKqR323Wd6B67Ymetha46pKLI
         VQdgg/MkVH9BQxkqOcQ9HHi9fwGFo6xDKQ8/GjZPQlmwSO2IDwUp6QA6rw7OJ/JlWbbN
         zOoMHbX1rRCTtaTQ4a0b2FJqwC12UzUSvlBeyc8hvtw6HVey2/RChL86PfBsoRJAOFJ6
         RF+g==
X-Gm-Message-State: ALoCoQkJfL7tUPtFv3hzaf1KyytmNqHsgR7Pi+LumlQU0hIvrXKhVY2WByP/aoyzMpk28fVO85AT
MIME-Version: 1.0
X-Received: by 10.180.211.101 with SMTP id nb5mr14542035wic.53.1404943685484;
 Wed, 09 Jul 2014 15:08:05 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Wed, 9 Jul 2014 15:08:05 -0700 (PDT)
In-Reply-To: <CAKJXNjEcPK3zirQXuXEEP-wCt+GLaEkHJCyUhpTygoptpAtXbQ@mail.gmail.com>
References: <CAAsvFPmyXCGd_fmiVH9d4-nKWwEj13cHO=mz-G0Usaz=pR12ow@mail.gmail.com>
	<CAKJXNjEcPK3zirQXuXEEP-wCt+GLaEkHJCyUhpTygoptpAtXbQ@mail.gmail.com>
Date: Wed, 9 Jul 2014 15:08:05 -0700
Message-ID: <CAAsvFPmXBkHyyVQZDkfyLuVZcOd0sQ9OFVrMMaYHuo5G339vyA@mail.gmail.com>
Subject: Re: ExecutorState.LOADING?
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c37cc64d9b3d04fdc9f112
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37cc64d9b3d04fdc9f112
Content-Type: text/plain; charset=UTF-8

Actually, I'm thinking about re-purposing it.  There's a nasty behavior
that I'll open a JIRA for soon, and that I'm thinking about addressing by
introducing/using another ExecutorState transition.  The basic problem is
that Master can be overly aggressive in calling removeApplication on
ExecutorStateChanged.  For example, say you have a working, long-running
Spark stand-alone-mode application and then try to add some more worker
nodes, but manage to misconfigure the new nodes so that on the new nodes
Executors never successfully start.  In that scenario, you will repeatedly
end up in the !normalExit branch of Master's receive ExecutorStateChanged,
quickly exceed ApplicationState.MAX_NUM_RETRY (a non-configurable 10, which
is another irritation), and end up having your application killed off even
though it is still running successfully on the old worker nodes.



On Wed, Jul 9, 2014 at 2:49 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
wrote:

> Git history to the rescue!  It seems to have been added by Matei way back
> in July 2012:
>
> https://github.com/apache/spark/commit/5d1a887bed8423bd6c25660910d18d91880e01fe
>
> and then was removed a few months later (replaced by RUNNING) by the same
> Mr. Zaharia:
>
> https://github.com/apache/spark/commit/bb1bce79240da22c2677d9f8159683cdf73158c2#diff-776a630ac2b2ec5fe85c07ca20a58fc0
>
> So I'd say it's safe to delete it.
>
>
> On Wed, Jul 9, 2014 at 2:36 PM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
>
> > Doesn't look to me like this is used.  Does anybody recall what it was
> > intended for?
> >
>

--001a11c37cc64d9b3d04fdc9f112--

From dev-return-8273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul  9 22:51:06 2014
Return-Path: <dev-return-8273-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 132D711E1D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  9 Jul 2014 22:51:06 +0000 (UTC)
Received: (qmail 76881 invoked by uid 500); 9 Jul 2014 22:51:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76816 invoked by uid 500); 9 Jul 2014 22:51:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76804 invoked by uid 99); 9 Jul 2014 22:51:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 22:51:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.220.172 as permitted sender)
Received: from [209.85.220.172] (HELO mail-vc0-f172.google.com) (209.85.220.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 09 Jul 2014 22:51:00 +0000
Received: by mail-vc0-f172.google.com with SMTP id hy10so8884763vcb.31
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 15:50:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=FfYqpcldMk/4Vs7JqYjLfc471WUp+zt7YB57GTMuWTo=;
        b=db14L2Hxo/VWLJ/9UkkM2LMZlYTalhpJZGZg0kGrx/ggjWToDsgaZpZ0OZC8GnK7I1
         G65YHQS6M4eBfe67455yQA9Wakfw5I+AZy6anNB9uOem1ob8CDjDoYVq7YH9RrmEVM1n
         YsVghwJTsN1bdGivey8yFvW8rehu7r3KWnjwkK2HYLi9MDJXCzAqFQmpTlstkUCMTrSA
         Q5tWZP+a6HUYt4CkvuGPIeKoHH2Zz/CAsa0v6pEI/QcqrIgEWlx7aszEw0l641imxkon
         1M6IC4I/bqyown8CVx2WOanpzGWmgfEn79xfEKG4ABvimbEKkt2JxDVEUWOmomucFE5z
         ZoBw==
X-Received: by 10.58.198.67 with SMTP id ja3mr42221524vec.10.1404946240013;
 Wed, 09 Jul 2014 15:50:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.69.169 with HTTP; Wed, 9 Jul 2014 15:50:19 -0700 (PDT)
In-Reply-To: <CAAsvFPmXBkHyyVQZDkfyLuVZcOd0sQ9OFVrMMaYHuo5G339vyA@mail.gmail.com>
References: <CAAsvFPmyXCGd_fmiVH9d4-nKWwEj13cHO=mz-G0Usaz=pR12ow@mail.gmail.com>
 <CAKJXNjEcPK3zirQXuXEEP-wCt+GLaEkHJCyUhpTygoptpAtXbQ@mail.gmail.com> <CAAsvFPmXBkHyyVQZDkfyLuVZcOd0sQ9OFVrMMaYHuo5G339vyA@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Wed, 9 Jul 2014 15:50:19 -0700
Message-ID: <CANGvG8qLZJUDPhMzb4YK2nTA6ozYBC4UzJ4uky=v33iVCLUgQQ@mail.gmail.com>
Subject: Re: ExecutorState.LOADING?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b6dc84c908f4504fdca8993
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dc84c908f4504fdca8993
Content-Type: text/plain; charset=UTF-8

Agreed that the behavior of the Master killing off an Application when
Executors from the same set of nodes repeatedly die is silly. This can also
strike if a single node enters a state where any Executor created on it
quickly dies (e.g., a block device becomes faulty). This prevents the
Application from launching despite only one node being bad.


On Wed, Jul 9, 2014 at 3:08 PM, Mark Hamstra <mark@clearstorydata.com>
wrote:

> Actually, I'm thinking about re-purposing it.  There's a nasty behavior
> that I'll open a JIRA for soon, and that I'm thinking about addressing by
> introducing/using another ExecutorState transition.  The basic problem is
> that Master can be overly aggressive in calling removeApplication on
> ExecutorStateChanged.  For example, say you have a working, long-running
> Spark stand-alone-mode application and then try to add some more worker
> nodes, but manage to misconfigure the new nodes so that on the new nodes
> Executors never successfully start.  In that scenario, you will repeatedly
> end up in the !normalExit branch of Master's receive ExecutorStateChanged,
> quickly exceed ApplicationState.MAX_NUM_RETRY (a non-configurable 10, which
> is another irritation), and end up having your application killed off even
> though it is still running successfully on the old worker nodes.
>
>
>
> On Wed, Jul 9, 2014 at 2:49 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
> wrote:
>
> > Git history to the rescue!  It seems to have been added by Matei way back
> > in July 2012:
> >
> >
> https://github.com/apache/spark/commit/5d1a887bed8423bd6c25660910d18d91880e01fe
> >
> > and then was removed a few months later (replaced by RUNNING) by the same
> > Mr. Zaharia:
> >
> >
> https://github.com/apache/spark/commit/bb1bce79240da22c2677d9f8159683cdf73158c2#diff-776a630ac2b2ec5fe85c07ca20a58fc0
> >
> > So I'd say it's safe to delete it.
> >
> >
> > On Wed, Jul 9, 2014 at 2:36 PM, Mark Hamstra <mark@clearstorydata.com>
> > wrote:
> >
> > > Doesn't look to me like this is used.  Does anybody recall what it was
> > > intended for?
> > >
> >
>

--047d7b6dc84c908f4504fdca8993--

From dev-return-8274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 02:31:57 2014
Return-Path: <dev-return-8274-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EBC0D11484
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 02:31:57 +0000 (UTC)
Received: (qmail 37552 invoked by uid 500); 10 Jul 2014 02:31:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37502 invoked by uid 500); 10 Jul 2014 02:31:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37490 invoked by uid 99); 10 Jul 2014 02:31:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:31:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:31:54 +0000
Received: by mail-ob0-f169.google.com with SMTP id nu7so2071269obb.28
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 19:31:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=1P6Pk+GBFHNAISjWo+e2zvah+kJ3XWOeZZWReTtEzGI=;
        b=JXxZDqaQo6cKnRI5EUlem+HG7DLF8zr8sPtHgfZr0AYm0jHsWsoH2s4cPuj862RiR4
         uQYQoqbrsrtJYxwUwQua0cCWtMqlfwmRBQ2K0wWyScocO2l2y9KvIcgY+ZJtizSdeY2g
         DoKp2NsXuKRpQDHnhv8E5EUlwLOaS1yw7qVl06eo/HftifgzoY1wjAckVS4CluJ2aA0Y
         B62GWfTZlyxZHMu9qE9Fh+aQRg+OTdUyyqqfG9KjJSPsheAnjIG82bAMit4ApBO/vqsZ
         x6nxEpnFU4tG7EUMhwnSXUNx+lcg0UMTIB02gHcd1euOaMy/fr7L25pYUBsmOn2Zof4Z
         5jew==
MIME-Version: 1.0
X-Received: by 10.60.160.71 with SMTP id xi7mr51181924oeb.51.1404959489117;
 Wed, 09 Jul 2014 19:31:29 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Wed, 9 Jul 2014 19:31:29 -0700 (PDT)
Date: Wed, 9 Jul 2014 19:31:29 -0700
Message-ID: <CABPQxss+5VVgrPGOuWuV+5LJ1fhgmY2D3fVgXAinSzkh7biTwg@mail.gmail.com>
Subject: Testing period for better jenkins integration
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Just a heads up - I've added some better Jenkins integration that
posts more useful messages on pull requests. We'll run this
side-by-side with the current Jenkins messages for a while to make
sure it's working well. Things may be a bit chatty while we are
testing this - we can migrate over as soon as we feel it's stable.

- Patrick

From dev-return-8275-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 02:35:28 2014
Return-Path: <dev-return-8275-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AE08411496
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 02:35:28 +0000 (UTC)
Received: (qmail 41634 invoked by uid 500); 10 Jul 2014 02:35:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41592 invoked by uid 500); 10 Jul 2014 02:35:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41573 invoked by uid 99); 10 Jul 2014 02:35:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:35:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of taka.epsilon@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:35:23 +0000
Received: by mail-oa0-f45.google.com with SMTP id o6so9221086oag.32
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 19:35:03 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=I/5RAi1uLl6bRPsF8hYL1kZ0L7RTUBlEisxtbCQwzxw=;
        b=wKM2qBJivm1JISVSGrBSr/xwXrKvuUZubAKdJb9lDR46OfFI9jK9TveeZjvEWiRQse
         plUCYxZTxNopqZTAZSPPvLh0KmBkvLv6x2LJp1qMeAq48hgdlHDS2Ceq3TkZbv0AQHKj
         3IzB1Y5MIwtS+fKqkd/Lh3KBi6bJmz/qSHY+c0f+taGPdGxiITm45KewQbwapykHbCmr
         kbvTx4j95AP+fYdLhWhmN2G0GijOZRJDTc8EnfhecMm7iS9OhthOXVnIuo65O0GtYIli
         pUTTtfRT9qoRrrkYm3CmYZopF4+KbwIUdBksKJcf1bFhYr6dEuEPP0da9VLkRaeSIUgN
         xgDw==
MIME-Version: 1.0
X-Received: by 10.60.133.203 with SMTP id pe11mr51439005oeb.24.1404959702950;
 Wed, 09 Jul 2014 19:35:02 -0700 (PDT)
Received: by 10.182.128.232 with HTTP; Wed, 9 Jul 2014 19:35:02 -0700 (PDT)
Date: Wed, 9 Jul 2014 19:35:02 -0700
Message-ID: <CALkvKb=Mj6JRbJPiPzHX4JGi1G8xQ94BfXKkchrQ8ekJfaCJ1Q@mail.gmail.com>
Subject: libgfortran Dependency
From: Taka Shinagawa <taka.epsilon@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b41baea04a6e704fdcdac04
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b41baea04a6e704fdcdac04
Content-Type: text/plain; charset=UTF-8

Hi,

After testing Spark 1.0.1-RC2 on EC2 instances from the standard Ubuntu and
Amazon Linux AMIs,
I've noticed the MLlib's dependancy on gfortran library (libgfortran.so.3).

"sbt assembly" succeeds without this library installed, but "sbt test"
fails as follows.

I'm wondering if documenting this dependency in README and online doc might
a good idea.

-------------
[info] ALSSuite:
-- org.jblas ERROR Couldn't load copied link file:
java.lang.UnsatisfiedLinkError:
/tmp/jblas8312335435391185287libjblas_arch_flavor.so: libgfortran.so.3:
cannot open shared object file: No such file or directory.

On Linux 64bit, you need additional support libraries.
You need to install libgfortran3.

For example for debian or Ubuntu, type "sudo apt-get install libgfortran3"

For more information, see
https://github.com/mikiobraun/jblas/wiki/Missing-Libraries
[info] Exception encountered when attempting to run a suite with class
name: org.apache.spark.mllib.recommendation.ALSSuite *** ABORTED ***
[info]   java.lang.UnsatisfiedLinkError:
org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V
[info]   at org.jblas.NativeBlas.dgemm(Native Method)
[info]   at org.jblas.SimpleBlas.gemm(SimpleBlas.java:251)
[info]   at org.jblas.DoubleMatrix.mmuli(DoubleMatrix.java:1697)
[info]   at org.jblas.DoubleMatrix.mmul(DoubleMatrix.java:3054)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$.generateRatings(ALSSuite.scala:67)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite.testALS(ALSSuite.scala:167)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply$mcV$sp(ALSSuite.scala:83)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
[info]   at
org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
[info]   ...

-------------

--047d7b41baea04a6e704fdcdac04--

From dev-return-8276-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 02:41:06 2014
Return-Path: <dev-return-8276-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D9F4F114D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 02:41:06 +0000 (UTC)
Received: (qmail 50198 invoked by uid 500); 10 Jul 2014 02:41:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50142 invoked by uid 500); 10 Jul 2014 02:41:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50127 invoked by uid 99); 10 Jul 2014 02:41:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:41:06 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:41:04 +0000
Received: by mail-we0-f179.google.com with SMTP id w62so8274331wes.38
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 19:40:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=a0szBlUE1s+NKZe6Y99lvVAitezbvRlY7OiylrrRcTw=;
        b=SOC1jggGeoCvT6gW0azTDlDp/xlx6zAsIw1XZ0ek6Pd4QRz3b/n7VCH70zVTl73KWW
         tgYHbdD0HtpOEhAcnneAVw8quD8LchfY5UrKTarELxyFLXrwPyXJBTaJ6JtqoHKy5uAp
         DsD4iXDkxWgrze/cLH/WnWrBIxPr5TLoTwENU7Ewx0XpNmtSGG3vIatiBMVL4yr3n6yU
         dACUtGvI/YWqUsqk+tLpg/q5qKEWeb62MloiIH7C2BMW8kk9xJyn/xfZoul7hDMx8hYD
         MEHAwUPfZIZzIfDLkj0aDnjrlYfmxgtKCyat4ebR8HVWBd29KxkqDGUUoBJh2V2rZnnb
         BJ9g==
MIME-Version: 1.0
X-Received: by 10.180.73.106 with SMTP id k10mr15169079wiv.11.1404960040164;
 Wed, 09 Jul 2014 19:40:40 -0700 (PDT)
Received: by 10.194.6.74 with HTTP; Wed, 9 Jul 2014 19:40:40 -0700 (PDT)
In-Reply-To: <CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
	<CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
	<CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
	<CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
	<CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com>
Date: Thu, 10 Jul 2014 10:40:40 +0800
Message-ID: <CABDsqqarh4OO2fdLEB49yXjroBoCHh7_oVuRdsGZx-jNFdb1Lw@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043749c71e214004fdcdc03c
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043749c71e214004fdcdc03c
Content-Type: text/plain; charset=UTF-8

could i set some cache policy to let spark load data from tachyon only one
time for all sql query?  for example by using CacheAllPolicy
FIFOCachePolicy LRUCachePolicy.  But I have tried that three policy, they
are not useful.
I think , if spark always load data for each sql query,  it will impact the
query speed , it will take more time than the case that data are managed by
spark itself.




2014-07-09 1:19 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:

> Yes. For Shark, two modes, "shark.cache=tachyon" and "shark.cache=memory",
> have the same ser/de overhead. Shark loads data from outsize of the process
> in Tachyon mode with the following benefits:
>
>
>    - In-memory data sharing across multiple Shark instances (i.e. stronger
>    isolation)
>    - Instant recovery of in-memory tables
>    - Reduce heap size => faster GC in shark
>    - If the table is larger than the memory size, only the hot columns will
>    be cached in memory
>
> from http://tachyon-project.org/master/Running-Shark-on-Tachyon.html and
> https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon
>
> Haoyuan
>
>
> On Tue, Jul 8, 2014 at 9:58 AM, Aaron Davidson <ilikerps@gmail.com> wrote:
>
> > Shark's in-memory format is already serialized (it's compressed and
> > column-based).
> >
> >
> > On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <mridul@gmail.com>
> > wrote:
> >
> > > You are ignoring serde costs :-)
> > >
> > > - Mridul
> > >
> > > On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <ilikerps@gmail.com>
> > wrote:
> > > > Tachyon should only be marginally less performant than memory_only,
> > > because
> > > > we mmap the data from Tachyon's ramdisk. We do not have to, say,
> > transfer
> > > > the data over a pipe from Tachyon; we can directly read from the
> > buffers
> > > in
> > > > the same way that Shark reads from its in-memory columnar format.
> > > >
> > > >
> > > >
> > > > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <
> liqingyang1985@gmail.com>
> > > > wrote:
> > > >
> > > >> hi, when i create a table, i can point the cache strategy using
> > > >> shark.cache,
> > > >> i think "shark.cache=memory_only"  means data are managed by spark,
> > and
> > > >> data are in the same jvm with excutor;   while
>  "shark.cache=tachyon"
> > > >>  means  data are managed by tachyon which is off heap, and data are
> > not
> > > in
> > > >> the same jvm with excutor,  so spark will load data from tachyon for
> > > each
> > > >> query sql , so,  is  tachyon less efficient than memory_only cache
> > > strategy
> > > >>  ?
> > > >> if yes, can we let spark load all data once from tachyon  for all
> sql
> > > query
> > > >>  if i want to use tachyon cache strategy since tachyon is more HA
> than
> > > >> memory_only ?
> > > >>
> > >
> >
>
>
>
> --
> Haoyuan Li
> AMPLab, EECS, UC Berkeley
> http://www.cs.berkeley.edu/~haoyuan/
>

--f46d043749c71e214004fdcdc03c--

From dev-return-8277-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 02:45:53 2014
Return-Path: <dev-return-8277-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 40C93114E5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 02:45:53 +0000 (UTC)
Received: (qmail 52625 invoked by uid 500); 10 Jul 2014 02:45:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52566 invoked by uid 500); 10 Jul 2014 02:45:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52549 invoked by uid 99); 10 Jul 2014 02:45:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:45:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.171 as permitted sender)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:45:48 +0000
Received: by mail-we0-f171.google.com with SMTP id q58so8396040wes.2
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 19:45:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=aQRPGw0bF5u/7fJO20qMWKEhAOE6u97TaAWjkh7EgWE=;
        b=HYsFe4CncVCzrIMwrOrhodpQ3Xe+mtiD9NcqC75+cF24EP8IOhvFPiKcEtztKsuH1T
         3pF7L+YXlsfX4zgZAxjfEKPOJCW9yBnaeG2yspBpSfkkbI3ZpW+/2cn01KFUEkxi5Gtl
         L4NJzMyl98mj6aJ9uEM7U97NZ+ygUsnNx8r0X0ZJS7l4dhJ6Zq+iHQeLFdAhfv0tlmG6
         1HsJEcSexrTWbTDjVm1xaankF37q/BZlaHouEIWrGHYft8s8+d9esmEQ0keeMxCXBPxK
         wQjdrLmbv+jMP9HeNOvCQ4K/a7ThijDua/OcjNrSrt2MtKuJTChOC2pgyIKRmHF5kRZ6
         StMQ==
MIME-Version: 1.0
X-Received: by 10.194.222.197 with SMTP id qo5mr54362518wjc.78.1404960326839;
 Wed, 09 Jul 2014 19:45:26 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Wed, 9 Jul 2014 19:45:26 -0700 (PDT)
In-Reply-To: <CALkvKb=Mj6JRbJPiPzHX4JGi1G8xQ94BfXKkchrQ8ekJfaCJ1Q@mail.gmail.com>
References: <CALkvKb=Mj6JRbJPiPzHX4JGi1G8xQ94BfXKkchrQ8ekJfaCJ1Q@mail.gmail.com>
Date: Wed, 9 Jul 2014 19:45:26 -0700
Message-ID: <CAJgQjQ9zFaP2eO+OPjZHiNW8p8frwJpREojA4BwmrUcHBKuAkQ@mail.gmail.com>
Subject: Re: libgfortran Dependency
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

It is documented in the official doc:
http://spark.apache.org/docs/latest/mllib-guide.html

On Wed, Jul 9, 2014 at 7:35 PM, Taka Shinagawa <taka.epsilon@gmail.com> wrote:
> Hi,
>
> After testing Spark 1.0.1-RC2 on EC2 instances from the standard Ubuntu and
> Amazon Linux AMIs,
> I've noticed the MLlib's dependancy on gfortran library (libgfortran.so.3).
>
> "sbt assembly" succeeds without this library installed, but "sbt test"
> fails as follows.
>
> I'm wondering if documenting this dependency in README and online doc might
> a good idea.
>
> -------------
> [info] ALSSuite:
> -- org.jblas ERROR Couldn't load copied link file:
> java.lang.UnsatisfiedLinkError:
> /tmp/jblas8312335435391185287libjblas_arch_flavor.so: libgfortran.so.3:
> cannot open shared object file: No such file or directory.
>
> On Linux 64bit, you need additional support libraries.
> You need to install libgfortran3.
>
> For example for debian or Ubuntu, type "sudo apt-get install libgfortran3"
>
> For more information, see
> https://github.com/mikiobraun/jblas/wiki/Missing-Libraries
> [info] Exception encountered when attempting to run a suite with class
> name: org.apache.spark.mllib.recommendation.ALSSuite *** ABORTED ***
> [info]   java.lang.UnsatisfiedLinkError:
> org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V
> [info]   at org.jblas.NativeBlas.dgemm(Native Method)
> [info]   at org.jblas.SimpleBlas.gemm(SimpleBlas.java:251)
> [info]   at org.jblas.DoubleMatrix.mmuli(DoubleMatrix.java:1697)
> [info]   at org.jblas.DoubleMatrix.mmul(DoubleMatrix.java:3054)
> [info]   at
> org.apache.spark.mllib.recommendation.ALSSuite$.generateRatings(ALSSuite.scala:67)
> [info]   at
> org.apache.spark.mllib.recommendation.ALSSuite.testALS(ALSSuite.scala:167)
> [info]   at
> org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply$mcV$sp(ALSSuite.scala:83)
> [info]   at
> org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
> [info]   at
> org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
> [info]   at
> org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
> [info]   ...
>
> -------------

From dev-return-8278-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 02:58:01 2014
Return-Path: <dev-return-8278-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2AF3911506
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 02:58:01 +0000 (UTC)
Received: (qmail 63185 invoked by uid 500); 10 Jul 2014 02:58:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63039 invoked by uid 500); 10 Jul 2014 02:58:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63027 invoked by uid 99); 10 Jul 2014 02:58:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:58:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of taka.epsilon@gmail.com designates 209.85.219.43 as permitted sender)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 02:57:56 +0000
Received: by mail-oa0-f43.google.com with SMTP id o6so9207122oag.16
        for <dev@spark.apache.org>; Wed, 09 Jul 2014 19:57:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=FI/6BOhz/OugZsf9WeAGMt/WCg2sOLuEKFjLuLy3MDY=;
        b=Crdr3qOoXKLy+lVdOqDUL34Fg6ELjcO5DJNs1TjzHGk+3mlvaJOlI+XjQi1U5flIAP
         3c/WfZKEbJNsNLed9gBsmxaetWs5zuAOzriGW/m6Yi9IDLHVEYrb3bcMDttdUdX5iXaj
         IiMz+6ZCtfxA4E4OL7oGCJbCOIpsIBop2QMAgwJkS2XCgc2Hup1u7c5pQp9TeOsqyOcM
         7X8cr/u9as7R3pXssgdWwZUkVXKhEwA8JDV1/E0lzcaDPbpApqPJiS1lbHwK+1JFh8ra
         aYqh/eXAt/HZ2ce3oYa5kNGweUmTNgPHyhDBChFuqsoQIAI+tRFxJObzmCcd56VTyT9x
         yy2Q==
MIME-Version: 1.0
X-Received: by 10.60.97.230 with SMTP id ed6mr85608oeb.81.1404961055564; Wed,
 09 Jul 2014 19:57:35 -0700 (PDT)
Received: by 10.182.128.232 with HTTP; Wed, 9 Jul 2014 19:57:35 -0700 (PDT)
In-Reply-To: <CAJgQjQ9zFaP2eO+OPjZHiNW8p8frwJpREojA4BwmrUcHBKuAkQ@mail.gmail.com>
References: <CALkvKb=Mj6JRbJPiPzHX4JGi1G8xQ94BfXKkchrQ8ekJfaCJ1Q@mail.gmail.com>
	<CAJgQjQ9zFaP2eO+OPjZHiNW8p8frwJpREojA4BwmrUcHBKuAkQ@mail.gmail.com>
Date: Wed, 9 Jul 2014 19:57:35 -0700
Message-ID: <CALkvKbmuW15zWViC6WWQf7+4sD6+7grNHEUNsYcgrUG82_7J4A@mail.gmail.com>
Subject: Re: libgfortran Dependency
From: Taka Shinagawa <taka.epsilon@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0115f46ea3e82604fdcdfc32
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115f46ea3e82604fdcdfc32
Content-Type: text/plain; charset=UTF-8

Thanks for point me to the MLlib guide. I was looking at only README and
Spark docs.

Also found it's already filed in JIRA
https://spark-project.atlassian.net/browse/SPARK-797


On Wed, Jul 9, 2014 at 7:45 PM, Xiangrui Meng <mengxr@gmail.com> wrote:

> It is documented in the official doc:
> http://spark.apache.org/docs/latest/mllib-guide.html
>
> On Wed, Jul 9, 2014 at 7:35 PM, Taka Shinagawa <taka.epsilon@gmail.com>
> wrote:
> > Hi,
> >
> > After testing Spark 1.0.1-RC2 on EC2 instances from the standard Ubuntu
> and
> > Amazon Linux AMIs,
> > I've noticed the MLlib's dependancy on gfortran library
> (libgfortran.so.3).
> >
> > "sbt assembly" succeeds without this library installed, but "sbt test"
> > fails as follows.
> >
> > I'm wondering if documenting this dependency in README and online doc
> might
> > a good idea.
> >
> > -------------
> > [info] ALSSuite:
> > -- org.jblas ERROR Couldn't load copied link file:
> > java.lang.UnsatisfiedLinkError:
> > /tmp/jblas8312335435391185287libjblas_arch_flavor.so: libgfortran.so.3:
> > cannot open shared object file: No such file or directory.
> >
> > On Linux 64bit, you need additional support libraries.
> > You need to install libgfortran3.
> >
> > For example for debian or Ubuntu, type "sudo apt-get install
> libgfortran3"
> >
> > For more information, see
> > https://github.com/mikiobraun/jblas/wiki/Missing-Libraries
> > [info] Exception encountered when attempting to run a suite with class
> > name: org.apache.spark.mllib.recommendation.ALSSuite *** ABORTED ***
> > [info]   java.lang.UnsatisfiedLinkError:
> > org.jblas.NativeBlas.dgemm(CCIIID[DII[DIID[DII)V
> > [info]   at org.jblas.NativeBlas.dgemm(Native Method)
> > [info]   at org.jblas.SimpleBlas.gemm(SimpleBlas.java:251)
> > [info]   at org.jblas.DoubleMatrix.mmuli(DoubleMatrix.java:1697)
> > [info]   at org.jblas.DoubleMatrix.mmul(DoubleMatrix.java:3054)
> > [info]   at
> >
> org.apache.spark.mllib.recommendation.ALSSuite$.generateRatings(ALSSuite.scala:67)
> > [info]   at
> >
> org.apache.spark.mllib.recommendation.ALSSuite.testALS(ALSSuite.scala:167)
> > [info]   at
> >
> org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply$mcV$sp(ALSSuite.scala:83)
> > [info]   at
> >
> org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
> > [info]   at
> >
> org.apache.spark.mllib.recommendation.ALSSuite$$anonfun$3.apply(ALSSuite.scala:83)
> > [info]   at
> > org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
> > [info]   ...
> >
> > -------------
>

--089e0115f46ea3e82604fdcdfc32--

From dev-return-8279-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 07:21:46 2014
Return-Path: <dev-return-8279-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AE50C11A83
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 07:21:46 +0000 (UTC)
Received: (qmail 85179 invoked by uid 500); 10 Jul 2014 07:21:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85124 invoked by uid 500); 10 Jul 2014 07:21:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85105 invoked by uid 99); 10 Jul 2014 07:21:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 07:21:45 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of raymond.liu@intel.com designates 192.55.52.93 as permitted sender)
Received: from [192.55.52.93] (HELO mga11.intel.com) (192.55.52.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 07:21:42 +0000
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by fmsmga102.fm.intel.com with ESMTP; 10 Jul 2014 00:21:16 -0700
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="5.01,636,1400050800"; 
   d="scan'208";a="567758094"
Received: from fmsmsx103.amr.corp.intel.com ([10.19.9.34])
  by fmsmga002.fm.intel.com with ESMTP; 10 Jul 2014 00:21:03 -0700
Received: from fmsmsx152.amr.corp.intel.com (10.19.17.221) by
 FMSMSX103.amr.corp.intel.com (10.19.9.34) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Thu, 10 Jul 2014 00:21:02 -0700
Received: from shsmsx151.ccr.corp.intel.com (10.239.6.50) by
 fmsmsx152.amr.corp.intel.com (10.19.17.221) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Thu, 10 Jul 2014 00:21:02 -0700
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.81]) by
 SHSMSX151.ccr.corp.intel.com ([169.254.3.188]) with mapi id 14.03.0123.003;
 Thu, 10 Jul 2014 15:21:00 +0800
From: "Liu, Raymond" <raymond.liu@intel.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: MIMA Compatiblity Checks
Thread-Topic: MIMA Compatiblity Checks
Thread-Index: AQHPg1GKsoqEpV16oUS90Us5AKV555uZFgxw
Date: Thu, 10 Jul 2014 07:21:00 +0000
Message-ID: <391D65D0EBFC9B4B95E117F72A360F1A0E938CFF@SHSMSX101.ccr.corp.intel.com>
References: <CABPQxstGPL85DAiXLgC9x7aPq42vvaGDmDngAwwMMbOtoVtXvA@mail.gmail.com>
In-Reply-To: <CABPQxstGPL85DAiXLgC9x7aPq42vvaGDmDngAwwMMbOtoVtXvA@mail.gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

so how to run the check locally?

On master tree, sbt mimaReportBinaryIssues Seems to lead to a lot of errors=
 reported. Do we need to modify SparkBuilder.scala etc to run it locally? C=
ould not figure out how Jekins run the check on its console outputs.


Best Regards,
Raymond Liu

-----Original Message-----
From: Patrick Wendell [mailto:pwendell@gmail.com]=20
Sent: Monday, June 09, 2014 3:40 AM
To: dev@spark.apache.org
Subject: MIMA Compatiblity Checks

Hey All,

Some people may have noticed PR failures due to binary compatibility checks=
. We've had these enabled in several of the sub-modules since the 0.9.0 rel=
ease but we've turned them on in Spark core post 1.0.0 which has much highe=
r churn.

The checks are based on the "migration manager" tool from Typesafe.
One issue is that tool doesn't support package-private declarations of clas=
ses or methods. Prashant Sharma has built instrumentation that adds partial=
 support for package-privacy (via a workaround) but since there isn't reall=
y native support for this in MIMA we are still finding cases in which we tr=
igger false positives.

In the next week or two we'll make it a priority to handle more of these fa=
lse-positive cases. In the mean time users can add manual excludes to:

project/MimaExcludes.scala

to avoid triggering warnings for certain issues.

This is definitely annoying - sorry about that. Unfortunately we are the fi=
rst open source Scala project to ever do this, so we are dealing with uncha=
rted territory.

Longer term I'd actually like to see us just write our own sbt-based tool t=
o do this in a better way (we've had trouble trying to extend MIMA itself, =
it e.g. has copy-pasted code in it from an old version of the scala compile=
r). If someone in the community is a Scala fan and wants to take that on, I=
'm happy to give more details.

- Patrick

From dev-return-8280-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 07:33:55 2014
Return-Path: <dev-return-8280-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1445A11AB8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 07:33:55 +0000 (UTC)
Received: (qmail 8308 invoked by uid 500); 10 Jul 2014 07:33:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8246 invoked by uid 500); 10 Jul 2014 07:33:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8234 invoked by uid 99); 10 Jul 2014 07:33:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 07:33:54 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 07:33:49 +0000
Received: by mail-qg0-f44.google.com with SMTP id j107so7327881qga.31
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 00:33:29 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=xyrnFjTeraaEN8yrEcVgM1DICrVr43yCwe/leAWYwuI=;
        b=VU0rrJqxw+ELEMPyb7aPkogeoLNB9Txuf3G2l2N5miZeaSIx7sVJUvn2GW8/GIyDpR
         XUB4T7ikvnWMf7yrgAxUlMuLwrOI+2l7haLQzxSD7fSgKLcuyAEL5QW8fUQBfp5r3o6o
         f0rWLSJcAgIeTz/3YyAWy44nLIRUpkLKuSvUVnNbYsZ7NAcTuk31R8gdvSypyeq+b/sB
         XyRr4LhoybfKCIvQmbm/y4TJxdFcUqrotRQp6SxLrcpk9MEn4jdZRA80ReZd+ummvxpL
         rVAQo5pMecmN1xx5HUhD9uJoz/8/+PCVNknSRU4U6H/EV3FDWsYyEwXB+XX19UFpZLgd
         27gQ==
X-Gm-Message-State: ALoCoQm/h5gf5wHshmzPgPw5Re+aeHnxJ9wBVYNY9bgTKo1d4uqRHJzbgI2BTE4VPHi+B9m0zsSG
X-Received: by 10.140.90.7 with SMTP id w7mr75179324qgd.52.1404977609062; Thu,
 10 Jul 2014 00:33:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Thu, 10 Jul 2014 00:33:09 -0700 (PDT)
In-Reply-To: <391D65D0EBFC9B4B95E117F72A360F1A0E938CFF@SHSMSX101.ccr.corp.intel.com>
References: <CABPQxstGPL85DAiXLgC9x7aPq42vvaGDmDngAwwMMbOtoVtXvA@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A0E938CFF@SHSMSX101.ccr.corp.intel.com>
From: Reynold Xin <rxin@databricks.com>
Date: Thu, 10 Jul 2014 00:33:09 -0700
Message-ID: <CAPh_B=bdR1vmqM9rkimx9R5fEVtZn3WnHP=ZRHEXpj77vx3ZuA@mail.gmail.com>
Subject: Re: MIMA Compatiblity Checks
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c124ce4e567004fdd1d7c0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c124ce4e567004fdd1d7c0
Content-Type: text/plain; charset=UTF-8

You can take a look at

https://github.com/apache/spark/blob/master/dev/run-tests

dev/mima


On Thu, Jul 10, 2014 at 12:21 AM, Liu, Raymond <raymond.liu@intel.com>
wrote:

> so how to run the check locally?
>
> On master tree, sbt mimaReportBinaryIssues Seems to lead to a lot of
> errors reported. Do we need to modify SparkBuilder.scala etc to run it
> locally? Could not figure out how Jekins run the check on its console
> outputs.
>
>
> Best Regards,
> Raymond Liu
>
> -----Original Message-----
> From: Patrick Wendell [mailto:pwendell@gmail.com]
> Sent: Monday, June 09, 2014 3:40 AM
> To: dev@spark.apache.org
> Subject: MIMA Compatiblity Checks
>
> Hey All,
>
> Some people may have noticed PR failures due to binary compatibility
> checks. We've had these enabled in several of the sub-modules since the
> 0.9.0 release but we've turned them on in Spark core post 1.0.0 which has
> much higher churn.
>
> The checks are based on the "migration manager" tool from Typesafe.
> One issue is that tool doesn't support package-private declarations of
> classes or methods. Prashant Sharma has built instrumentation that adds
> partial support for package-privacy (via a workaround) but since there
> isn't really native support for this in MIMA we are still finding cases in
> which we trigger false positives.
>
> In the next week or two we'll make it a priority to handle more of these
> false-positive cases. In the mean time users can add manual excludes to:
>
> project/MimaExcludes.scala
>
> to avoid triggering warnings for certain issues.
>
> This is definitely annoying - sorry about that. Unfortunately we are the
> first open source Scala project to ever do this, so we are dealing with
> uncharted territory.
>
> Longer term I'd actually like to see us just write our own sbt-based tool
> to do this in a better way (we've had trouble trying to extend MIMA itself,
> it e.g. has copy-pasted code in it from an old version of the scala
> compiler). If someone in the community is a Scala fan and wants to take
> that on, I'm happy to give more details.
>
> - Patrick
>

--001a11c124ce4e567004fdd1d7c0--

From dev-return-8281-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 09:52:00 2014
Return-Path: <dev-return-8281-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D313911E69
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 09:52:00 +0000 (UTC)
Received: (qmail 19311 invoked by uid 500); 10 Jul 2014 09:52:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19254 invoked by uid 500); 10 Jul 2014 09:52:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19242 invoked by uid 99); 10 Jul 2014 09:51:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 09:51:59 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of liqingyang1985@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 09:51:57 +0000
Received: by mail-ig0-f177.google.com with SMTP id r10so2870745igi.10
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 02:51:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=wbp3tLlnLNpAD+QRb+7+09LXWKpWL0hp0aFhrJVo3GI=;
        b=FrbZsOaLrr8sjk7mLbBz2Wr+An86SkAjRko6yHj8Pwuh1aGqDXbTLPsREOAusj5QBw
         yH3HdENRrdWaOlNJUVNoS5TSPid/NtJrXCGrOirjbOEkx61WuRG/bjuQRs9clbjcqCIZ
         9h04i5silDJWHaVBzewXWyv529vPYy/lOjpcKaLV5c3W4gxhj+YBWtiGSdf0NLrg/9M5
         peQmF0koelaWAM5nDM2TDajjICckm6tBNRese1kRA1/HHvy7T7jQH6b/k9vQQYe6VC29
         ZgwdYVHPPBNZj9cwDGaVKJNXUQOTveRuE9Mi8tGfA7I/GRPHB3d6pgtXckJuQwnwxQoQ
         Uo8w==
MIME-Version: 1.0
X-Received: by 10.50.111.19 with SMTP id ie19mr20723177igb.4.1404985892112;
 Thu, 10 Jul 2014 02:51:32 -0700 (PDT)
Received: by 10.64.78.201 with HTTP; Thu, 10 Jul 2014 02:51:32 -0700 (PDT)
Date: Thu, 10 Jul 2014 17:51:32 +0800
Message-ID: <CABDsqqayTZvbTox0==94zF6SEiwVh8xgy+Nke34q59Dvm=RL_w@mail.gmail.com>
Subject: when insert data into one table which is on tachyon, how can i
 control the data position?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01493924038e1b04fdd3c53e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01493924038e1b04fdd3c53e
Content-Type: text/plain; charset=UTF-8

when insert data (the data is small, it will not be partitioned
automatically)into one table which is on tachyon, how can i control the
data position,  i mean how can i point which machine the data should exist
on?
if we can not control, what is the data assign strategy of tachyon or spark?

--089e01493924038e1b04fdd3c53e--

From dev-return-8282-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 14:25:11 2014
Return-Path: <dev-return-8282-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB9BA1150F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 14:25:11 +0000 (UTC)
Received: (qmail 91836 invoked by uid 500); 10 Jul 2014 14:25:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91775 invoked by uid 500); 10 Jul 2014 14:25:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91762 invoked by uid 99); 10 Jul 2014 14:25:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 14:25:10 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 14:25:06 +0000
Received: by mail-wi0-f178.google.com with SMTP id f8so4147083wiw.17
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 07:24:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=WNRAZiEntbhxuou910D4WbYuL4J8w1Xb5qbiREXdJac=;
        b=TXI4emQgQxvhhA+ExvOL3DwcoeRJI5hHPb2S6UTYoo8NOl2xNNrNaifehgLF545iZp
         nX8afjGXlXfFCna5bzb0xFtMYW/rj174lJSajtpPjMyIGoJgInyykdQbbxkUownrHrk6
         9GXVkjofLmPlp5tpq0/bg0phvoN4GuzwLap6Z0TfhEmWMztnNoN7lL+FWGb831x+5de4
         3nsJKGj8xvSWhPOtMllN3Bp+ZyvnmkiwprjjikRZ1aB+x9ubOsozpQBCf7olESQ9ADQ8
         zefoNh2lmJThs4+a6HPi2ZhyF6u3/UjpgwOv1YcuikToYmm56w7c24Kn2O7HpvGKuVtD
         /wUA==
MIME-Version: 1.0
X-Received: by 10.180.105.68 with SMTP id gk4mr19339421wib.24.1405002285710;
 Thu, 10 Jul 2014 07:24:45 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Thu, 10 Jul 2014 07:24:45 -0700 (PDT)
In-Reply-To: <1404909567855.c2a8fd87@Nodemailer>
References: <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
	<1404909567855.c2a8fd87@Nodemailer>
Date: Thu, 10 Jul 2014 10:24:45 -0400
Message-ID: <CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

I went ahead and created JIRAs.

JIRA for Hierarchical Clustering:
https://issues.apache.org/jira/browse/SPARK-2429

JIRA for Standarized Clustering APIs:
https://issues.apache.org/jira/browse/SPARK-2430

Before submitting a PR for the standardized API, I want to implement a
few clustering algorithms for myself to get a good "feel" for how to
structure such an API.  If others with more experience want to dive
into designing the API, though, that would allow us to get moving more
quickly.


On Wed, Jul 9, 2014 at 8:39 AM, Nick Pentreath <nick.pentreath@gmail.com> w=
rote:
> Cool seems like a god initiative. Adding a couple extra high quality clus=
tering implantations will be great.
>
>
> I'd say it would make most sense to submit a PR for the Standardised API =
first, agree that with everyone and then build on it for the specific imple=
mentations.
> =E2=80=94
> Sent from Mailbox
>
> On Wed, Jul 9, 2014 at 2:15 PM, RJ Nowling <rnowling@gmail.com> wrote:
>
>> Thanks everyone for the input.
>> So it seems what people want is:
>> * Implement MiniBatch KMeans and Hierarchical KMeans (Divide and
>> conquer approach, look at DecisionTree implementation as a reference)
>> * Restructure 3 Kmeans clustering algorithm implementations to prevent
>> code duplication and conform to a consistent API where possible
>> If this is correct, I'll start work on that.  How would it be best to
>> structure it? Should I submit separate JIRAs / PRs for refactoring of
>> current code, MiniBatch KMeans, and Hierarchical or keep my current
>> JIRA and PR for MiniBatch KMeans open and submit a second JIRA and PR
>> for Hierarchical KMeans that builds on top of that?
>> Thanks!
>> On Tue, Jul 8, 2014 at 5:44 PM, Hector Yee <hector.yee@gmail.com> wrote:
>>> Yeah if one were to replace the objective function in decision tree wit=
h
>>> minimizing the variance of the leaf nodes it would be a hierarchical
>>> clusterer.
>>>
>>>
>>> On Tue, Jul 8, 2014 at 2:12 PM, Evan R. Sparks <evan.sparks@gmail.com>
>>> wrote:
>>>
>>>> If you're thinking along these lines, have a look at the DecisionTree
>>>> implementation in MLlib. It uses the same idea and is optimized to pre=
vent
>>>> multiple passes over the data by computing several splits at each leve=
l of
>>>> tree building. The tradeoff is increased model state and computation p=
er
>>>> pass over the data, but fewer total passes and hopefully lower
>>>> communication overheads than, say, shuffling data around that belongs =
to
>>>> one cluster or another. Something like that could work here as well.
>>>>
>>>> I'm not super-familiar with hierarchical K-Means so perhaps there's a =
more
>>>> efficient way to implement it, though.
>>>>
>>>>
>>>> On Tue, Jul 8, 2014 at 2:06 PM, Hector Yee <hector.yee@gmail.com> wrot=
e:
>>>>
>>>> > No was thinking more top-down:
>>>> >
>>>> > assuming a distributed kmeans system already existing, recursively a=
pply
>>>> > the kmeans algorithm on data already partitioned by the previous lev=
el of
>>>> > kmeans.
>>>> >
>>>> > I haven't been much of a fan of bottom up approaches like HAC mainly
>>>> > because they assume there is already a distance metric for items to
>>>> items.
>>>> > This makes it hard to cluster new content. The distances between sib=
ling
>>>> > clusters is also hard to compute (if you have thrown away the simila=
rity
>>>> > matrix), do you count paths to same parent node if you are computing
>>>> > distances between items in two adjacent nodes for example. It is als=
o a
>>>> bit
>>>> > harder to distribute the computation for bottom up approaches as you=
 have
>>>> > to already find the nearest neighbor to an item to begin the process=
.
>>>> >
>>>> >
>>>> > On Tue, Jul 8, 2014 at 1:59 PM, RJ Nowling <rnowling@gmail.com> wrot=
e:
>>>> >
>>>> > > The scikit-learn implementation may be of interest:
>>>> > >
>>>> > >
>>>> > >
>>>> >
>>>> http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward.=
html#sklearn.cluster.Ward
>>>> > >
>>>> > > It's a bottom up approach.  The pair of clusters for merging are
>>>> > > chosen to minimize variance.
>>>> > >
>>>> > > Their code is under a BSD license so it can be used as a template.
>>>> > >
>>>> > > Is something like that you were thinking Hector?
>>>> > >
>>>> > > On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.co=
m>
>>>> > > wrote:
>>>> > > > sure. more interesting problem here is choosing k at each level.
>>>> Kernel
>>>> > > > methods seem to be most promising.
>>>> > > >
>>>> > > >
>>>> > > > On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.com=
>
>>>> > wrote:
>>>> > > >
>>>> > > >> No idea, never looked it up. Always just implemented it as doin=
g
>>>> > k-means
>>>> > > >> again on each cluster.
>>>> > > >>
>>>> > > >> FWIW standard k-means with euclidean distance has problems too =
with
>>>> > some
>>>> > > >> dimensionality reduction methods. Swapping out the distance met=
ric
>>>> > with
>>>> > > >> negative dot or cosine may help.
>>>> > > >>
>>>> > > >> Other more useful clustering would be hierarchical SVD. The rea=
son
>>>> > why I
>>>> > > >> like hierarchical clustering is it makes for faster inference
>>>> > especially
>>>> > > >> over billions of users.
>>>> > > >>
>>>> > > >>
>>>> > > >> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.7@gmail=
.com
>>>> >
>>>> > > >> wrote:
>>>> > > >>
>>>> > > >> > Hector, could you share the references for hierarchical K-mea=
ns?
>>>> > > thanks.
>>>> > > >> >
>>>> > > >> >
>>>> > > >> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail.=
com>
>>>> > > wrote:
>>>> > > >> >
>>>> > > >> > > I would say for bigdata applications the most useful would =
be
>>>> > > >> > hierarchical
>>>> > > >> > > k-means with back tracking and the ability to support k nea=
rest
>>>> > > >> > centroids.
>>>> > > >> > >
>>>> > > >> > >
>>>> > > >> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling <rnowling@gmail=
.com
>>>> >
>>>> > > >> wrote:
>>>> > > >> > >
>>>> > > >> > > > Hi all,
>>>> > > >> > > >
>>>> > > >> > > > MLlib currently has one clustering algorithm implementati=
on,
>>>> > > KMeans.
>>>> > > >> > > > It would benefit from having implementations of other
>>>> clustering
>>>> > > >> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means,
>>>> Hierarchical
>>>> > > >> > > > Clustering, and Affinity Propagation.
>>>> > > >> > > >
>>>> > > >> > > > I recently submitted a PR [1] for a MiniBatch KMeans
>>>> > > implementation,
>>>> > > >> > > > and I saw an email on this list about interest in impleme=
nting
>>>> > > Fuzzy
>>>> > > >> > > > C-Means.
>>>> > > >> > > >
>>>> > > >> > > > Based on Sean Owen's review of my MiniBatch KMeans code, =
it
>>>> > became
>>>> > > >> > > > apparent that before I implement more clustering algorith=
ms,
>>>> it
>>>> > > would
>>>> > > >> > > > be useful to hammer out a framework to reduce code duplic=
ation
>>>> > and
>>>> > > >> > > > implement a consistent API.
>>>> > > >> > > >
>>>> > > >> > > > I'd like to gauge the interest and goals of the MLlib
>>>> community:
>>>> > > >> > > >
>>>> > > >> > > > 1. Are you interested in having more clustering algorithm=
s
>>>> > > available?
>>>> > > >> > > >
>>>> > > >> > > > 2. Is the community interested in specifying a common
>>>> framework?
>>>> > > >> > > >
>>>> > > >> > > > Thanks!
>>>> > > >> > > > RJ
>>>> > > >> > > >
>>>> > > >> > > > [1] - https://github.com/apache/spark/pull/1248
>>>> > > >> > > >
>>>> > > >> > > >
>>>> > > >> > > > --
>>>> > > >> > > > em rnowling@gmail.com
>>>> > > >> > > > c 954.496.2314
>>>> > > >> > > >
>>>> > > >> > >
>>>> > > >> > >
>>>> > > >> > >
>>>> > > >> > > --
>>>> > > >> > > Yee Yang Li Hector <http://google.com/+HectorYee>
>>>> > > >> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
>>>> > > >> > >
>>>> > > >> >
>>>> > > >>
>>>> > > >>
>>>> > > >>
>>>> > > >> --
>>>> > > >> Yee Yang Li Hector <http://google.com/+HectorYee>
>>>> > > >> *google.com/+HectorYee <http://google.com/+HectorYee>*
>>>> > > >>
>>>> > >
>>>> > >
>>>> > >
>>>> > > --
>>>> > > em rnowling@gmail.com
>>>> > > c 954.496.2314
>>>> > >
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > Yee Yang Li Hector <http://google.com/+HectorYee>
>>>> > *google.com/+HectorYee <http://google.com/+HectorYee>*
>>>> >
>>>>
>>>
>>>
>>>
>>> --
>>> Yee Yang Li Hector <http://google.com/+HectorYee>
>>> *google.com/+HectorYee <http://google.com/+HectorYee>*
>> --
>> em rnowling@gmail.com
>> c 954.496.2314



--=20
em rnowling@gmail.com
c 954.496.2314

From dev-return-8283-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 14:48:25 2014
Return-Path: <dev-return-8283-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C22A5115A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 14:48:25 +0000 (UTC)
Received: (qmail 41685 invoked by uid 500); 10 Jul 2014 14:48:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41628 invoked by uid 500); 10 Jul 2014 14:48:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41616 invoked by uid 99); 10 Jul 2014 14:48:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 14:48:23 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 14:48:21 +0000
Received: by mail-qc0-f169.google.com with SMTP id i17so3705437qcy.14
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 07:47:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:subject
         :content-type;
        bh=BvJYIkdHfQ5TyT6u61Mdyf3Wh+8eWpyPOvGbzEKx1j4=;
        b=MW3mLNwlGHYM5VvowjJElbMCWyLoJ5DUJqASkbSnR4C+pyXnfnN8xPnTtYngD02v5N
         aXszA/DpYoM0L2eDlIE8Ams5k9usuF2BtmLZezVZ+/Wt0ApAJS+SwDPTdJdgQFKyg1fZ
         EBFHV2n0wf9SpN7V2KvCX0G1QdqQW6NITxQZFWTUu2u9aVpuHLRaYhnaVJJVljD53rhA
         +Lugjuf1QL7NK0i+lMg0RT1m5lovyeKrNlDYIkXVpZvLj2Mek0h5vmxGNCLhmVt+xHco
         LyLX1IHSDivyEgUfgjtVzS4jrY+UzYpErxQxJdcL6lOxxhDMFGKxsabr7/9GW20Q6Vyj
         8l9g==
X-Received: by 10.140.109.135 with SMTP id l7mr34933328qgf.72.1405003675718;
        Thu, 10 Jul 2014 07:47:55 -0700 (PDT)
Received: from hedwig-24.prd.orcali.com (ec2-54-85-253-245.compute-1.amazonaws.com. [54.85.253.245])
        by mx.google.com with ESMTPSA id t75sm35394134qga.16.2014.07.10.07.47.54
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 10 Jul 2014 07:47:54 -0700 (PDT)
Date: Thu, 10 Jul 2014 07:47:54 -0700 (PDT)
X-Google-Original-Date: Thu, 10 Jul 2014 14:47:54 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1405003674571.192c3983@Nodemailer>
In-Reply-To: <CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
References: <CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
X-Orchestra-Oid: BE24DE40-533A-436B-8AD5-66B58C73F958
X-Orchestra-Sig: 2cae0480734aac6f7a6a3722e50510b95da58763
X-Orchestra-Thrid: TEDF7BF56-A3FD-4BD3-A0FA-706CB3088F7E_1473083676001909825
X-Orchestra-Thrid-Sig: d641d9f7482d5ef02a493b882d0784668a217d30
X-Orchestra-Account: ef70a72d339ad7dfbc1fe96ab1345fe332a3cbd9
From: "Nick Pentreath" <nick.pentreath@gmail.com>
To: dev@spark.apache.org
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1405003674879"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1405003674879
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Might be worth checking out scikit-learn and mahout to get some broad =
ideas=E2=80=94
Sent from Mailbox

On Thu, Jul 10, 2014 at 4:25 PM, RJ Nowling <rnowling@gmail.com> wrote:

> I went ahead and created JIRAs.
> JIRA for Hierarchical Clustering:
> https://issues.apache.org/jira/browse/SPARK-2429
> JIRA for Standarized Clustering APIs:
> https://issues.apache.org/jira/browse/SPARK-2430
> Before submitting a PR for the standardized API, I want to implement a
> few clustering algorithms for myself to get a good =22feel=22 for how to
> structure such an API.  If others with more experience want to dive
> into designing the API, though, that would allow us to get moving more
> quickly.
> On Wed, Jul 9, 2014 at 8:39 AM, Nick Pentreath <nick.pentreath@gmail.com>=
 wrote:
>> Cool seems like a god initiative. Adding a couple extra high quality =
clustering implantations will be great.
>>
>>
>> I'd say it would make most sense to submit a PR for the Standardised API=
 first, agree that with everyone and then build on it for the specific =
implementations.
>> =E2=80=94
>> Sent from Mailbox
>>
>> On Wed, Jul 9, 2014 at 2:15 PM, RJ Nowling <rnowling@gmail.com> wrote:
>>
>>> Thanks everyone for the input.
>>> So it seems what people want is:
>>> * Implement MiniBatch KMeans and Hierarchical KMeans (Divide and
>>> conquer approach, look at DecisionTree implementation as a reference)
>>> * Restructure 3 Kmeans clustering algorithm implementations to prevent
>>> code duplication and conform to a consistent API where possible
>>> If this is correct, I'll start work on that.  How would it be best to
>>> structure it=3F Should I submit separate JIRAs / PRs for refactoring =
of
>>> current code, MiniBatch KMeans, and Hierarchical or keep my current
>>> JIRA and PR for MiniBatch KMeans open and submit a second JIRA and PR
>>> for Hierarchical KMeans that builds on top of that=3F
>>> Thanks!
>>> On Tue, Jul 8, 2014 at 5:44 PM, Hector Yee <hector.yee@gmail.com> =
wrote:
>>>> Yeah if one were to replace the objective function in decision tree =
with
>>>> minimizing the variance of the leaf nodes it would be a hierarchical
>>>> clusterer.
>>>>
>>>>
>>>> On Tue, Jul 8, 2014 at 2:12 PM, Evan R. Sparks <evan.sparks@gmail.=
com>
>>>> wrote:
>>>>
>>>>> If you're thinking along these lines, have a look at the =
DecisionTree
>>>>> implementation in MLlib. It uses the same idea and is optimized to =
prevent
>>>>> multiple passes over the data by computing several splits at each =
level of
>>>>> tree building. The tradeoff is increased model state and computation =
per
>>>>> pass over the data, but fewer total passes and hopefully lower
>>>>> communication overheads than, say, shuffling data around that belongs=
 to
>>>>> one cluster or another. Something like that could work here as well.
>>>>>
>>>>> I'm not super-familiar with hierarchical K-Means so perhaps there's a=
 more
>>>>> efficient way to implement it, though.
>>>>>
>>>>>
>>>>> On Tue, Jul 8, 2014 at 2:06 PM, Hector Yee <hector.yee@gmail.com> =
wrote:
>>>>>
>>>>> > No was thinking more top-down:
>>>>> >
>>>>> > assuming a distributed kmeans system already existing, recursively =
apply
>>>>> > the kmeans algorithm on data already partitioned by the previous =
level of
>>>>> > kmeans.
>>>>> >
>>>>> > I haven't been much of a fan of bottom up approaches like HAC =
mainly
>>>>> > because they assume there is already a distance metric for items =
to
>>>>> items.
>>>>> > This makes it hard to cluster new content. The distances between =
sibling
>>>>> > clusters is also hard to compute (if you have thrown away the =
similarity
>>>>> > matrix), do you count paths to same parent node if you are =
computing
>>>>> > distances between items in two adjacent nodes for example. It is =
also a
>>>>> bit
>>>>> > harder to distribute the computation for bottom up approaches as =
you have
>>>>> > to already find the nearest neighbor to an item to begin the =
process.
>>>>> >
>>>>> >
>>>>> > On Tue, Jul 8, 2014 at 1:59 PM, RJ Nowling <rnowling@gmail.com> =
wrote:
>>>>> >
>>>>> > > The scikit-learn implementation may be of interest:
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> >
>>>>> http://scikit-learn.org/stable/modules/generated/sklearn.cluster.Ward=
.html#sklearn.cluster.Ward
>>>>> > >
>>>>> > > It's a bottom up approach.  The pair of clusters for merging are
>>>>> > > chosen to minimize variance.
>>>>> > >
>>>>> > > Their code is under a BSD license so it can be used as a template=
.
>>>>> > >
>>>>> > > Is something like that you were thinking Hector=3F
>>>>> > >
>>>>> > > On Tue, Jul 8, 2014 at 4:50 PM, Dmitriy Lyubimov <dlieu.7@gmail.=
com>
>>>>> > > wrote:
>>>>> > > > sure. more interesting problem here is choosing k at each level=
.
>>>>> Kernel
>>>>> > > > methods seem to be most promising.
>>>>> > > >
>>>>> > > >
>>>>> > > > On Tue, Jul 8, 2014 at 1:31 PM, Hector Yee <hector.yee@gmail.=
com>
>>>>> > wrote:
>>>>> > > >
>>>>> > > >> No idea, never looked it up. Always just implemented it as =
doing
>>>>> > k-means
>>>>> > > >> again on each cluster.
>>>>> > > >>
>>>>> > > >> FWIW standard k-means with euclidean distance has problems too=
 with
>>>>> > some
>>>>> > > >> dimensionality reduction methods. Swapping out the distance =
metric
>>>>> > with
>>>>> > > >> negative dot or cosine may help.
>>>>> > > >>
>>>>> > > >> Other more useful clustering would be hierarchical SVD. The =
reason
>>>>> > why I
>>>>> > > >> like hierarchical clustering is it makes for faster inference
>>>>> > especially
>>>>> > > >> over billions of users.
>>>>> > > >>
>>>>> > > >>
>>>>> > > >> On Tue, Jul 8, 2014 at 1:24 PM, Dmitriy Lyubimov <dlieu.=
7@gmail.com
>>>>> >
>>>>> > > >> wrote:
>>>>> > > >>
>>>>> > > >> > Hector, could you share the references for hierarchical =
K-means=3F
>>>>> > > thanks.
>>>>> > > >> >
>>>>> > > >> >
>>>>> > > >> > On Tue, Jul 8, 2014 at 1:01 PM, Hector Yee <hector.yee@gmail=
.com>
>>>>> > > wrote:
>>>>> > > >> >
>>>>> > > >> > > I would say for bigdata applications the most useful would=
 be
>>>>> > > >> > hierarchical
>>>>> > > >> > > k-means with back tracking and the ability to support k =
nearest
>>>>> > > >> > centroids.
>>>>> > > >> > >
>>>>> > > >> > >
>>>>> > > >> > > On Tue, Jul 8, 2014 at 10:54 AM, RJ Nowling =
<rnowling@gmail.com
>>>>> >
>>>>> > > >> wrote:
>>>>> > > >> > >
>>>>> > > >> > > > Hi all,
>>>>> > > >> > > >
>>>>> > > >> > > > MLlib currently has one clustering algorithm =
implementation,
>>>>> > > KMeans.
>>>>> > > >> > > > It would benefit from having implementations of other
>>>>> clustering
>>>>> > > >> > > > algorithms such as MiniBatch KMeans, Fuzzy C-Means,
>>>>> Hierarchical
>>>>> > > >> > > > Clustering, and Affinity Propagation.
>>>>> > > >> > > >
>>>>> > > >> > > > I recently submitted a PR [1] for a MiniBatch KMeans
>>>>> > > implementation,
>>>>> > > >> > > > and I saw an email on this list about interest in =
implementing
>>>>> > > Fuzzy
>>>>> > > >> > > > C-Means.
>>>>> > > >> > > >
>>>>> > > >> > > > Based on Sean Owen's review of my MiniBatch KMeans code,=
 it
>>>>> > became
>>>>> > > >> > > > apparent that before I implement more clustering =
algorithms,
>>>>> it
>>>>> > > would
>>>>> > > >> > > > be useful to hammer out a framework to reduce code =
duplication
>>>>> > and
>>>>> > > >> > > > implement a consistent API.
>>>>> > > >> > > >
>>>>> > > >> > > > I'd like to gauge the interest and goals of the MLlib
>>>>> community:
>>>>> > > >> > > >
>>>>> > > >> > > > 1. Are you interested in having more clustering =
algorithms
>>>>> > > available=3F
>>>>> > > >> > > >
>>>>> > > >> > > > 2. Is the community interested in specifying a common
>>>>> framework=3F
>>>>> > > >> > > >
>>>>> > > >> > > > Thanks!
>>>>> > > >> > > > RJ
>>>>> > > >> > > >
>>>>> > > >> > > > [1] - https://github.com/apache/spark/pull/1248
>>>>> > > >> > > >
>>>>> > > >> > > >
>>>>> > > >> > > > --
>>>>> > > >> > > > em rnowling@gmail.com
>>>>> > > >> > > > c 954.496.2314
>>>>> > > >> > > >
>>>>> > > >> > >
>>>>> > > >> > >
>>>>> > > >> > >
>>>>> > > >> > > --
>>>>> > > >> > > Yee Yang Li Hector <http://google.com/+HectorYee>
>>>>> > > >> > > *google.com/+HectorYee <http://google.com/+HectorYee>*
>>>>> > > >> > >
>>>>> > > >> >
>>>>> > > >>
>>>>> > > >>
>>>>> > > >>
>>>>> > > >> --
>>>>> > > >> Yee Yang Li Hector <http://google.com/+HectorYee>
>>>>> > > >> *google.com/+HectorYee <http://google.com/+HectorYee>*
>>>>> > > >>
>>>>> > >
>>>>> > >
>>>>> > >
>>>>> > > --
>>>>> > > em rnowling@gmail.com
>>>>> > > c 954.496.2314
>>>>> > >
>>>>> >
>>>>> >
>>>>> >
>>>>> > --
>>>>> > Yee Yang Li Hector <http://google.com/+HectorYee>
>>>>> > *google.com/+HectorYee <http://google.com/+HectorYee>*
>>>>> >
>>>>>
>>>>
>>>>
>>>>
>>>> --
>>>> Yee Yang Li Hector <http://google.com/+HectorYee>
>>>> *google.com/+HectorYee <http://google.com/+HectorYee>*
>>> --
>>> em rnowling@gmail.com
>>> c 954.496.2314
> --=20
> em rnowling@gmail.com
> c 954.496.2314
------Nodemailer-0.5.0-?=_1-1405003674879--

From dev-return-8284-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 17:40:10 2014
Return-Path: <dev-return-8284-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F73111D74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 17:40:10 +0000 (UTC)
Received: (qmail 26462 invoked by uid 500); 10 Jul 2014 17:40:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26403 invoked by uid 500); 10 Jul 2014 17:40:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26392 invoked by uid 99); 10 Jul 2014 17:40:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 17:40:09 +0000
X-ASF-Spam-Status: No, hits=-0.1 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.54] (HELO g4t3426.houston.hp.com) (15.201.208.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 17:40:03 +0000
Received: from G4W6310.americas.hpqcorp.net (g4w6310.houston.hp.com [16.210.26.217])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3426.houston.hp.com (Postfix) with ESMTPS id 2BE48285
	for <dev@spark.apache.org>; Thu, 10 Jul 2014 17:39:38 +0000 (UTC)
Received: from G4W6301.americas.hpqcorp.net (16.210.26.226) by
 G4W6310.americas.hpqcorp.net (16.210.26.217) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Thu, 10 Jul 2014 17:38:35 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.241]) by
 G4W6301.americas.hpqcorp.net ([16.210.26.226]) with mapi id 14.03.0169.001;
 Thu, 10 Jul 2014 17:38:35 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Feature selection interface
Thread-Topic: Feature selection interface
Thread-Index: Ac+cYtczmLagap9pRNythu+fMYQelw==
Date: Thu, 10 Jul 2014 17:38:33 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCADD05@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [16.210.48.18]
Content-Type: multipart/alternative;
	boundary="_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCADD05G4W3292americas_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCADD05G4W3292americas_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

I've implemented a class that does Chi-squared feature selection for RDD[La=
beledPoint]. It also computes basic class/feature occurrence statistics and=
 other methods like mutual information or information gain can be easily im=
plemented. I would like to make a pull request. However, MLlib master branc=
h doesn't have any feature selection methods implemented. So, I need to cre=
ate a proper interface that my class will extend or mix. It should be easy =
to use from developers and users prospective.

I was thinking that there should be FeatureEvaluator that for each feature =
from RDD[LabeledPoint] returns RDD[((featureIndex: Int, label: Double), val=
ue: Double)].
Then there should be FeatureSelector that selects top N features or top N f=
eatures group by class etc.
And the simplest one, FeatureFilter that filters the data based on set of f=
eature indices.

Additionally, there should be the interface for FeatureEvaluators that don'=
t use class labels, i.e. for RDD[Vector].

I am concerned that such design looks rather "disconnected" because there a=
re 3 disconnected objects.

As a result of use, I would like to see something like "val filteredData =
=3D Filter(data, ChiSquared(data).selectTop(100))".

Any ideas or suggestions?

Best regards, Alexander

--_000_9D5B00849D2CDA4386BDA89E83F69E6C0FCADD05G4W3292americas_--

From dev-return-8285-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 18:16:11 2014
Return-Path: <dev-return-8285-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8BED411F33
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 18:16:11 +0000 (UTC)
Received: (qmail 20650 invoked by uid 500); 10 Jul 2014 18:16:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20601 invoked by uid 500); 10 Jul 2014 18:16:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20576 invoked by uid 99); 10 Jul 2014 18:16:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 18:16:09 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 18:16:06 +0000
Received: by mail-vc0-f170.google.com with SMTP id hy10so11266358vcb.15
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 11:15:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=zQtwUcPeWflMk2YxR7IC0DEfnAwJVz6Ve/bdf8UIzFA=;
        b=HPajClDCIVTv25MfBIRDl41zpJZyRbNlHbqXXDDGGQ0aVW6YycusYpOKBXYb1vOYnZ
         Q8BI4oJocHR0FVzpideunoyt2PWtMye1d9sSl9UxknEWTHJ5Vvfj9l9WFi0b8UCjNnM6
         yGsMZHIcuHRPFZfbTUaENgbRxDQAziRUD0zfVzvKkrQpurf3Fov/aBP6Zx9tOdSLjuez
         LEaZPxIO/Pbi4l77132dqoccOiLTzcG45YHMDeckk9QR/W6UZy17e+0m7TEJSg5kuvr4
         rcaF3rdWrbsk6WHCDdvfh2CjaesXnyAwGt1PFnce57pUa5sT8LQs2cwNDRCka1TAK7fz
         IJdA==
X-Gm-Message-State: ALoCoQk8ebxX6ZTTpDuvALiB7UdiRDfN8VALToH1vEICccf+80afRftt6XYaLpgcopmwE07OfcyH
MIME-Version: 1.0
X-Received: by 10.53.13.200 with SMTP id fa8mr1505672vdd.57.1405016141193;
 Thu, 10 Jul 2014 11:15:41 -0700 (PDT)
Received: by 10.220.98.194 with HTTP; Thu, 10 Jul 2014 11:15:41 -0700 (PDT)
Date: Thu, 10 Jul 2014 11:15:41 -0700
Message-ID: <CAM6vJxGyz7SzhuOkpGWOks=+XkXmEkQj_Jsq1mVn-DiY16e1xA@mail.gmail.com>
Subject: Changes to sbt build have been merged
From: Patrick Wendell <patrick@databricks.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134a1da00456904fddad026
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134a1da00456904fddad026
Content-Type: text/plain; charset=UTF-8

Just a heads up, we merged Prashant's work on having the sbt build read all
dependencies from Maven. Please report any issues you find on the dev list
or on JIRA.

One note here for developers, going forward the sbt build will use the same
configuration style as the maven build (-D for options and -P for maven
profiles). So this will be a change for developers:

sbt/sbt -Dhadoop.version=2.2.0 -Pyarn assembly

For now, we'll continue to support the old env-var options with a
deprecation warning.

- Patrick

--001a1134a1da00456904fddad026--

From dev-return-8286-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 20:30:17 2014
Return-Path: <dev-return-8286-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 37BA611549
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 20:30:17 +0000 (UTC)
Received: (qmail 50336 invoked by uid 500); 10 Jul 2014 20:30:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50263 invoked by uid 500); 10 Jul 2014 20:30:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50252 invoked by uid 99); 10 Jul 2014 20:30:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 20:30:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.171 as permitted sender)
Received: from [209.85.216.171] (HELO mail-qc0-f171.google.com) (209.85.216.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 20:30:11 +0000
Received: by mail-qc0-f171.google.com with SMTP id w7so125416qcr.16
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 13:29:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=WvEDtycKji1niIGx8MqrnYEOXeHBCpsG9SP5caxlO+4=;
        b=TTurqzzgdjEiTSqM3tQ7VlqbPePZ7V/pwNX2d1zZ46IJ1QpHZiTS8syiCmsjcSnsOC
         DMfb/5a+DpGhRqXFsgEhNyFPDu7LBY5BYZ0wcd9T3Qna3KgHCZMHMMvQgJixS101Yplr
         3CKkEec1tWGSYe9oS+cmyQ+LUrBggSiuAQn/JU0kmgSPew1/Vl40THWT9hu3pCCWthXZ
         P/y+UY81kL/2qw/c1CTmNBZdKjq7sJdptQFPxTjs8i1yr3QPIMQwDJP/vSlk4dAIr8E6
         iKzxmiNePENkdrg9AYFuQLVNcZxJpDLQGucf8JCi8Sy+N0bQJiXAy19Bm6xC6q8EbqRZ
         RRPA==
X-Gm-Message-State: ALoCoQmjgjcbGRyAG4hYhPXzV3P8mfMty2XJBVdwUW/3frvPfEsZtDr1T4yfwW1KzqQit8GAdjEE
MIME-Version: 1.0
X-Received: by 10.224.12.138 with SMTP id x10mr3065848qax.36.1405024191353;
 Thu, 10 Jul 2014 13:29:51 -0700 (PDT)
Received: by 10.140.30.53 with HTTP; Thu, 10 Jul 2014 13:29:51 -0700 (PDT)
In-Reply-To: <CAM6vJxGyz7SzhuOkpGWOks=+XkXmEkQj_Jsq1mVn-DiY16e1xA@mail.gmail.com>
References: <CAM6vJxGyz7SzhuOkpGWOks=+XkXmEkQj_Jsq1mVn-DiY16e1xA@mail.gmail.com>
Date: Thu, 10 Jul 2014 13:29:51 -0700
Message-ID: <CACBYxKJhknNdU-v2pgFN6cMYG-yZX9z50BzX-wpvcpgR2UGa+w@mail.gmail.com>
Subject: Re: Changes to sbt build have been merged
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01494f44d3a29e04fddcafdf
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01494f44d3a29e04fddcafdf
Content-Type: text/plain; charset=UTF-8

Woot!


On Thu, Jul 10, 2014 at 11:15 AM, Patrick Wendell <patrick@databricks.com>
wrote:

> Just a heads up, we merged Prashant's work on having the sbt build read all
> dependencies from Maven. Please report any issues you find on the dev list
> or on JIRA.
>
> One note here for developers, going forward the sbt build will use the same
> configuration style as the maven build (-D for options and -P for maven
> profiles). So this will be a change for developers:
>
> sbt/sbt -Dhadoop.version=2.2.0 -Pyarn assembly
>
> For now, we'll continue to support the old env-var options with a
> deprecation warning.
>
> - Patrick
>

--089e01494f44d3a29e04fddcafdf--

From dev-return-8287-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 20:33:09 2014
Return-Path: <dev-return-8287-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06228115D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 20:33:09 +0000 (UTC)
Received: (qmail 63710 invoked by uid 500); 10 Jul 2014 20:33:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63654 invoked by uid 500); 10 Jul 2014 20:33:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63640 invoked by uid 99); 10 Jul 2014 20:33:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 20:33:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yaoshengzhe@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 20:33:06 +0000
Received: by mail-wg0-f47.google.com with SMTP id y10so111964wgg.30
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 13:32:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=g9zoeg3H3QpmJAKMriEibV1FE5euZihbO/0yF9WIqn4=;
        b=0dHnfHZVKuDpR0hycvZY2AGTxZ6v3wDvJRHA76vVxbjuPnIMDbffUn2+wSVFZKGNAq
         K0njHDUqTEAziRjnkD+gejWwkw1FjTqtiwTQVNoqG8gNlxvRuYUWMqBR0E+eYPY0/N9W
         Gqc51HE20e8m+6XfNd/j2NCwkrc62a7YVv/wvhBAsHLRUaRp+KCAtT281gDJVw4bZpNP
         ahy6NzNySK9hVHo9Pkt/TGmngMaa1iPER4sCiF34uLJPPC5rIfAVHaKiLSgK6Q2cSezg
         3M9GyUi8OJr+Rs8DJDc4qzCGiOwPMWOHCzKhlq7rFd99NmO1DBjb6SxhKEU8J7qwt/+y
         hGRw==
MIME-Version: 1.0
X-Received: by 10.194.104.200 with SMTP id gg8mr59694924wjb.93.1405024360963;
 Thu, 10 Jul 2014 13:32:40 -0700 (PDT)
Received: by 10.194.24.102 with HTTP; Thu, 10 Jul 2014 13:32:40 -0700 (PDT)
In-Reply-To: <CACBYxKJhknNdU-v2pgFN6cMYG-yZX9z50BzX-wpvcpgR2UGa+w@mail.gmail.com>
References: <CAM6vJxGyz7SzhuOkpGWOks=+XkXmEkQj_Jsq1mVn-DiY16e1xA@mail.gmail.com>
	<CACBYxKJhknNdU-v2pgFN6cMYG-yZX9z50BzX-wpvcpgR2UGa+w@mail.gmail.com>
Date: Thu, 10 Jul 2014 13:32:40 -0700
Message-ID: <CA+FETEKd6HLTCUtAYhgTDHsELo5DJDk5jW-kmftmot4MgshaGQ@mail.gmail.com>
Subject: Re: Changes to sbt build have been merged
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e010d8212efa5b104fddcb9fc
X-Virus-Checked: Checked by ClamAV on apache.org

--089e010d8212efa5b104fddcb9fc
Content-Type: text/plain; charset=UTF-8

Cool~


On Thu, Jul 10, 2014 at 1:29 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> Woot!
>
>
> On Thu, Jul 10, 2014 at 11:15 AM, Patrick Wendell <patrick@databricks.com>
> wrote:
>
> > Just a heads up, we merged Prashant's work on having the sbt build read
> all
> > dependencies from Maven. Please report any issues you find on the dev
> list
> > or on JIRA.
> >
> > One note here for developers, going forward the sbt build will use the
> same
> > configuration style as the maven build (-D for options and -P for maven
> > profiles). So this will be a change for developers:
> >
> > sbt/sbt -Dhadoop.version=2.2.0 -Pyarn assembly
> >
> > For now, we'll continue to support the old env-var options with a
> > deprecation warning.
> >
> > - Patrick
> >
>

--089e010d8212efa5b104fddcb9fc--

From dev-return-8288-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 22:06:56 2014
Return-Path: <dev-return-8288-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A45CC119F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 22:06:56 +0000 (UTC)
Received: (qmail 99744 invoked by uid 500); 10 Jul 2014 22:06:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99684 invoked by uid 500); 10 Jul 2014 22:06:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99667 invoked by uid 99); 10 Jul 2014 22:06:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 22:06:55 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.174 as permitted sender)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 22:06:50 +0000
Received: by mail-we0-f174.google.com with SMTP id u57so215967wes.33
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 15:06:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=CpUYwLFhkO5a54gFOQnarSq15Liw+nGmNsomBjyNxRA=;
        b=jTB664FHT/kHF2Z217YRn6u2pmUI03ZEdc/CpM+k0Srw/UYr3zf0WJxRcdTe9Us7Io
         ZuqcZ8Y8JnBhpcg4ArVixZNni46nPmWDLJX6Jvoyr/E8bBnZ2PFWjrWGircYQOMr4hXF
         2ZMMDKXQ/8Q67g9ZqgIryUsIh6ecG5hyE1kfFJaifUdRO7xNfau7h1F14YdcLrm9R9qp
         IFo/sriXnKG7lwyFkbge0M6Y2xiF0VbC9neYAbMeUWprjUEsElbHs8VpO1VqwGLS2vOA
         PNyE38ckUznaH7XCEAQBig/AhJGto9q86mhuPCfqUSJa4czIp8R5kPy6LWYwUSX8vXT7
         32BA==
X-Received: by 10.180.208.13 with SMTP id ma13mr249374wic.45.1405029988583;
 Thu, 10 Jul 2014 15:06:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.24.168 with HTTP; Thu, 10 Jul 2014 15:05:48 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 10 Jul 2014 18:05:48 -0400
Message-ID: <CAOhmDzch1e=zOaqhi7VNNX+doGuNSi3PQCBSHZKXFRW8E9B+uA@mail.gmail.com>
Subject: EC2 clusters ready in launch time + 30 seconds
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c38d8a5e536d04fdde09cd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c38d8a5e536d04fdde09cd
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi devs!

Right now it takes a non-trivial amount of time to launch EC2 clusters.
Part of this time is spent starting the EC2 instances, which is out of our
control. Another part of this time is spent installing stuff on and
configuring the instances. This, we can control.

I=E2=80=99d like to explore approaches to upgrading spark-ec2 so that launc=
hing a
cluster of any size generally takes only 30 seconds on top of the time to
launch the base EC2 instances. Since Amazon can launch instances
concurrently, I believe this means we should be able to launch a fully
operational Spark cluster of any size in constant time. Is that correct?

Do we already have an idea of what it would take to get to that point?

Nick
=E2=80=8B

--001a11c38d8a5e536d04fdde09cd--

From dev-return-8289-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 22:16:33 2014
Return-Path: <dev-return-8289-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8886A11A26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 22:16:33 +0000 (UTC)
Received: (qmail 16290 invoked by uid 500); 10 Jul 2014 22:16:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16223 invoked by uid 500); 10 Jul 2014 22:16:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16211 invoked by uid 99); 10 Jul 2014 22:16:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 22:16:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ianoconnell@gmail.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 22:16:29 +0000
Received: by mail-ig0-f175.google.com with SMTP id uq10so1144754igb.8
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 15:16:05 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=ndtEIelgsPD7n2BAy5tb0FGPwwc5jIHL+7deXlqxPXo=;
        b=hxHrZr9uN1FMARFeWIPBAbv2v7fy0rIAktgCASrLWVViyx0K8ylAj1uNmXdPZSO5z5
         aHynFVsNn2cVW0jrPSpqM1kXiJSh5SA/CAfl/lgyh7WPM5ub07HyAgfAE+uIhxV5Pwma
         0ItDSVbW+UdNTRwvRKmFXHaQtGsZwEZjS33e2lFV4g0s1D2LcEODbQ40DRD6xXszn646
         u/UxWuTgtoutxmaHFFIc4SXzdL70Q0cJahR7Elr2ZsfnLnBRqq3/VhM/HURxYUrS7pUo
         HChYfiYysV/DTvoyVAzw7WH15gnlGDUQbZbpzX+Il0B4/StUFJSRiiFhyo6sCEfrwUKP
         YPoQ==
X-Received: by 10.51.16.197 with SMTP id fy5mr25016474igd.47.1405030564938;
 Thu, 10 Jul 2014 15:16:04 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.223.135 with HTTP; Thu, 10 Jul 2014 15:15:44 -0700 (PDT)
From: "Ian O'Connell" <ianoconnell@gmail.com>
Date: Thu, 10 Jul 2014 15:15:44 -0700
Message-ID: <CAMDxJTEy8-2o=MW6Z96sWMZOZnDDJS79bmd05Bo1WK8aMzvXig@mail.gmail.com>
Subject: sparkSQL thread safe?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134cf1eb8c99104fdde2ba7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134cf1eb8c99104fdde2ba7
Content-Type: text/plain; charset=UTF-8

Had a few quick questions...

Just wondering if right now spark sql is expected to be thread safe on
master?

doing a simple hadoop file -> RDD -> schema RDD -> write parquet

will fail in reflection code if i run these in a thread pool.

The SparkSqlSerializer, seems to create a new Kryo instance each time it
wants to serialize anything. I got a huge speedup when I had any
non-primitive type in my SchemaRDD using the ResourcePool's from Chill for
providing the KryoSerializer to it. (I can open an RB if there is some
reason not to re-use them?)

====

With the Distinct Count operator there is no map-side operations, and a
test to check for this. Is there any reason not to do a map side combine
into a set and then merge the sets later? (similar to the approximate
distinct count operator)

===

Another thing while i'm mailing.. the 1.0.1 docs have a section like:
"
// Note: Case classes in Scala 2.10 can support only up to 22 fields. To
work around this limit, // you can use custom classes that implement the
Product interface.
"

Which sounds great, we have lots of data in thrift.. so via scrooge (
https://github.com/twitter/scrooge), we end up with ultimately instances of
traits which implement product. Though the reflection code appears to look
for the constructor of the class and base the types based on those
parameters?


Ian.

--001a1134cf1eb8c99104fdde2ba7--

From dev-return-8290-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 10 23:51:44 2014
Return-Path: <dev-return-8290-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6588611D4B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 10 Jul 2014 23:51:44 +0000 (UTC)
Received: (qmail 92550 invoked by uid 500); 10 Jul 2014 23:51:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92490 invoked by uid 500); 10 Jul 2014 23:51:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92479 invoked by uid 99); 10 Jul 2014 23:51:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 23:51:43 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 10 Jul 2014 23:51:41 +0000
Received: by mail-qg0-f54.google.com with SMTP id q107so294749qgd.41
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 16:51:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=1JC6GWrItoyz8NX0pNGRjF1fPx9GMnXQOoyZrkiAom4=;
        b=iWuyRzEcdI9O5Fme2IGb6fCQEQDuArByRwL0QzCDeDTh3EfGHwr158a9OH7mkubiL7
         Jjd12t/CFZqCrG6mj0mHvEUbJZcxQq6+hnhIP3y9OlSXxrl72wStxZO5vd6rNZO0yGyy
         K/py43R+p6mi548AXupV2Hlk/sWUDF29KOSTQsg/2yfNlOO7RfL2E5I4u7iepPqt2YJH
         JrwN0art5V4sXSzqZ6DW5v8jWqiZrq407UNWAIih0cgCwkkQFgxEIjbhReRRgTAQs0d2
         ldi0k23jbfvTEGiSCLwsoYL+UxkoxdZEXoEtqvShYr1gpyetoH5FpdgZ/8c4N/HrJorH
         GpfQ==
X-Gm-Message-State: ALoCoQnjFMInj2KBQYxBYx5cwqlM5Ea12j7e87Wb71iq28wY8qZQ9zAudW4v6jpt44RfB9o5gnIT
X-Received: by 10.140.30.73 with SMTP id c67mr80688884qgc.16.1405036276895;
 Thu, 10 Jul 2014 16:51:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.3 with HTTP; Thu, 10 Jul 2014 16:50:56 -0700 (PDT)
In-Reply-To: <CAMDxJTEy8-2o=MW6Z96sWMZOZnDDJS79bmd05Bo1WK8aMzvXig@mail.gmail.com>
References: <CAMDxJTEy8-2o=MW6Z96sWMZOZnDDJS79bmd05Bo1WK8aMzvXig@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 10 Jul 2014 16:50:56 -0700
Message-ID: <CAAswR-7MEswRBLgAqU0fJQ2rJkKqJ9U82R9HVYD6Hw69ve2POw@mail.gmail.com>
Subject: Re: sparkSQL thread safe?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a35a42e64f404fddf80f4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a35a42e64f404fddf80f4
Content-Type: text/plain; charset=UTF-8

Hey Ian,

Thanks for bringing these up!  Responses in-line:

Just wondering if right now spark sql is expected to be thread safe on
> master?
> doing a simple hadoop file -> RDD -> schema RDD -> write parquet
> will fail in reflection code if i run these in a thread pool.
>

You are probably hitting SPARK-2178
<https://issues.apache.org/jira/browse/SPARK-2178> which is caused by
SI-6240 <https://issues.scala-lang.org/browse/SI-6240>.  We have a plan to
fix this by moving the schema introspection to compile time, using macros.


> The SparkSqlSerializer, seems to create a new Kryo instance each time it
> wants to serialize anything. I got a huge speedup when I had any
> non-primitive type in my SchemaRDD using the ResourcePool's from Chill for
> providing the KryoSerializer to it. (I can open an RB if there is some
> reason not to re-use them?)
>

Sounds like SPARK-2102 <https://issues.apache.org/jira/browse/SPARK-2102>.
 There is no reason AFAIK to not reuse the instance. A PR would be greatly
appreciated!


> With the Distinct Count operator there is no map-side operations, and a
> test to check for this. Is there any reason not to do a map side combine
> into a set and then merge the sets later? (similar to the approximate
> distinct count operator)
>

Thats just not an optimization that we had implemented yet... but I've just
done it here <https://github.com/apache/spark/pull/1366> and it'll be in
master soon :)


> Another thing while i'm mailing.. the 1.0.1 docs have a section like:
> "
> // Note: Case classes in Scala 2.10 can support only up to 22 fields. To
> work around this limit, // you can use custom classes that implement the
> Product interface.
> "
>
> Which sounds great, we have lots of data in thrift.. so via scrooge (
> https://github.com/twitter/scrooge), we end up with ultimately instances
> of
> traits which implement product. Though the reflection code appears to look
> for the constructor of the class and base the types based on those
> parameters?


Yeah, thats true that we only look in the constructor at the moment, but I
don't think there is a really good reason for that (other than I guess we
will need to add code to make sure we skip builtin object methods).  If you
want to open a JIRA, we can try fixing this.

Michael

--001a113a35a42e64f404fddf80f4--

From dev-return-8291-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 00:10:53 2014
Return-Path: <dev-return-8291-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B7CA11DE0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 00:10:53 +0000 (UTC)
Received: (qmail 24632 invoked by uid 500); 11 Jul 2014 00:10:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24573 invoked by uid 500); 11 Jul 2014 00:10:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24561 invoked by uid 99); 11 Jul 2014 00:10:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 00:10:51 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [216.70.64.77] (HELO n26.mail01.mtsvc.net) (216.70.64.77)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 00:10:47 +0000
Received: from 173-11-124-254-sfba.hfc.comcastbusiness.net ([173.11.124.254]:59235 helo=NODBOT)
	by n26.mail01.mtsvc.net with esmtpa (Exim 4.72)
	(envelope-from <nate@reactor8.com>)
	id 1X5OPu-0007Ol-43
	for dev@spark.apache.org; Thu, 10 Jul 2014 20:10:22 -0400
From: "Nate D'Amico" <nate@reactor8.com>
To: <dev@spark.apache.org>
References: <CAOhmDzch1e=zOaqhi7VNNX+doGuNSi3PQCBSHZKXFRW8E9B+uA@mail.gmail.com>
In-Reply-To: <CAOhmDzch1e=zOaqhi7VNNX+doGuNSi3PQCBSHZKXFRW8E9B+uA@mail.gmail.com>
Subject: RE: EC2 clusters ready in launch time + 30 seconds
Date: Thu, 10 Jul 2014 17:10:15 -0700
Message-ID: <057601cf9c9c$7e798c80$7b6ca580$@reactor8.com>
MIME-Version: 1.0
Content-Type: text/plain;
	charset="UTF-8"
Content-Transfer-Encoding: quoted-printable
X-Mailer: Microsoft Outlook 14.0
Thread-Index: AQILQtTfQ5EDkfN70khtIOqVZr1+X5sisFyA
Content-Language: en-us
X-Authenticated-User: 917868 nate@reactor8.com
X-MT-ID: 3CB1E6102B94980D10544FA93FD5F01BC02E0A63
X-Virus-Checked: Checked by ClamAV on apache.org

You are partially correct.

It's not terribly complex, but also not easy to accomplish.  Sounds like =
you want to manage some partially/fully baked AMI's with the core spark =
libs and dependencies already on the image.  Main issues that crop up =
are:

1) image sprawl, as libs/config/defaults/etc change, images need to be =
"rebuilt" and references updated
2) cross region support (not too huge deal now with copy functionality, =
just more complex image mgmt.)

If you don=E2=80=99t want to restrict which instance types/sizes one can =
use, you also have uptick in image mgmt. complexity with:

3) instance type (need both standard and hvm)

Starting to work through some automation/config stuff for spark stack on =
EC2 with a project, will be focusing the work through the apache bigtop =
effort to start, can then share with spark community directly as things =
progress if people are interested

Nate


-----Original Message-----
From: Nicholas Chammas [mailto:nicholas.chammas@gmail.com]=20
Sent: Thursday, July 10, 2014 3:06 PM
To: dev
Subject: EC2 clusters ready in launch time + 30 seconds

Hi devs!

Right now it takes a non-trivial amount of time to launch EC2 clusters.
Part of this time is spent starting the EC2 instances, which is out of =
our control. Another part of this time is spent installing stuff on and =
configuring the instances. This, we can control.

I=E2=80=99d like to explore approaches to upgrading spark-ec2 so that =
launching a cluster of any size generally takes only 30 seconds on top =
of the time to launch the base EC2 instances. Since Amazon can launch =
instances concurrently, I believe this means we should be able to launch =
a fully operational Spark cluster of any size in constant time. Is that =
correct?

Do we already have an idea of what it would take to get to that point?

Nick
=E2=80=8B


From dev-return-8292-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 01:28:41 2014
Return-Path: <dev-return-8292-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4092311FCE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 01:28:41 +0000 (UTC)
Received: (qmail 34661 invoked by uid 500); 11 Jul 2014 01:28:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34604 invoked by uid 500); 11 Jul 2014 01:28:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34591 invoked by uid 99); 11 Jul 2014 01:28:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 01:28:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.41 as permitted sender)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 01:28:38 +0000
Received: by mail-qa0-f41.google.com with SMTP id cm18so358833qab.14
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 18:28:13 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=998DwRzyFsxvA3D5hUPFeUBIsLaPHZyM2HT8o1bd9eg=;
        b=lLte9mXn93L6FQCoBUhILOw5Bq0R9m95duicsT/nGBcwKnJNrbQIFT5OaTzeGNv0nV
         8KnnuaxYIInRlKjyFbW+CYJQEa4wX/r/K0rKTmZop6Lx3xOMv+Df5neS1MfuzEjgIfFn
         RP8Xe+qlYkFUMNDy4jBrav/5qnTar8a6viW4C5tzqSc1QSEvvbW+Nnk23u+7T/hpb/YZ
         gNxAlbcW67p4WI5dLF1tyF5Hhdcf5Wm8O9Yz1uYsSYBKPpe4K68L77d8KbdxQQN/ohWz
         XDo/PHJ49isV7Ku0wiaqg1F2LyLP8Oi8j1rw37igQTn8m0ED1hjNQbrmCtlNwwCtjhdn
         ea1g==
MIME-Version: 1.0
X-Received: by 10.140.38.112 with SMTP id s103mr82192399qgs.40.1405042093062;
 Thu, 10 Jul 2014 18:28:13 -0700 (PDT)
Received: by 10.140.49.236 with HTTP; Thu, 10 Jul 2014 18:28:12 -0700 (PDT)
In-Reply-To: <1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
Date: Thu, 10 Jul 2014 21:28:12 -0400
Message-ID: <CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c13c8cda02bd04fde0da0f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c13c8cda02bd04fde0da0f
Content-Type: text/plain; charset=UTF-8

-1 I honestly do not know the voting rules for the Spark community, so
please excuse me if I am out of line or if Mesos compatibility is not a
concern at this point.

We just tried to run this version built against 2.3.0-cdh5.0.2 on mesos
0.18.2.  All of our jobs with data above a few gigabytes hung indefinitely.
 Downgrading back to the 1.0.0 stable release of Spark built the same way
worked for us.


On Mon, Jul 7, 2014 at 5:17 PM, Tom Graves <tgraves_cs@yahoo.com.invalid>
wrote:

> +1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with
> authentication on.
>
> Tom
>
>
> On Friday, July 4, 2014 2:39 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>
>
> Please vote on releasing the following candidate as Apache Spark version
> 1.0.1!
>
> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/pwendell.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1021/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.1!
>
> The vote is open until Monday, July 07, at 20:45 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 1.0.1
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> === Differences from RC1 ===
> This release includes only one "blocking" patch from rc1:
> https://github.com/apache/spark/pull/1255
>
> There are also smaller fixes which came in over the last week.
>
> === About this release ===
> This release fixes a few high-priority bugs in 1.0 and has a variety
> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> of the more visible patches are:
>
> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
> SPARK-1790: Support r3 instance types on EC2.
>
> This is the first maintenance release on the 1.0 line. We plan to make
> additional maintenance releases as new fixes come in.
>

--001a11c13c8cda02bd04fde0da0f--

From dev-return-8293-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 01:36:57 2014
Return-Path: <dev-return-8293-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD5FE11FFC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 01:36:57 +0000 (UTC)
Received: (qmail 45353 invoked by uid 500); 11 Jul 2014 01:36:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45290 invoked by uid 500); 11 Jul 2014 01:36:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45274 invoked by uid 99); 11 Jul 2014 01:36:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 01:36:56 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 01:36:54 +0000
Received: by mail-qg0-f42.google.com with SMTP id e89so391830qgf.1
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 18:36:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0iEH1dj93rgvPhsSsU1PJyMi67bCCo45iQxo7W3RBHI=;
        b=LmRVOZKQm6q1GZz37DVWVjvQOwIyrf7NjPS1mTNFgos5KvMg4aUaRN7SuNYEiOIrYO
         rGRWzXQTZJ6dINqub/QXK681f0fVEkz7/N/X2z1TlK+4Y1WQZSJWEMf0JGw3hlAdFw42
         qXXr+zFAnJr+E1A2W8DcOiFWFFZYFbE3PYcX4CgNJYaTelm7mbrP/u2TcvmmcaGscO5d
         TZyb1X1WNsuc+d+BBRRowLQ2J+Kau+I05xP4DE05Yl5eluwHQTH4wMHrxag66Ew3bYxs
         otxZpEyryhla9caLhb1PjWQYS8SpD72SowQomAJuTbmpwThYIndQfU7JrwXB+QRK3a0w
         OIxw==
MIME-Version: 1.0
X-Received: by 10.140.20.247 with SMTP id 110mr82510373qgj.49.1405042590008;
 Thu, 10 Jul 2014 18:36:30 -0700 (PDT)
Received: by 10.140.49.236 with HTTP; Thu, 10 Jul 2014 18:36:29 -0700 (PDT)
In-Reply-To: <CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
	<CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
Date: Thu, 10 Jul 2014 21:36:29 -0400
Message-ID: <CAGOvqioKCuR3qnPA5twAC-11Jo2c2peNmc1pMUybBrxP9O9=6Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c12d3478f49304fde0f8a0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12d3478f49304fde0f8a0
Content-Type: text/plain; charset=UTF-8

Just realized the deadline was Monday, my apologies.  The issue
nevertheless stands.


On Thu, Jul 10, 2014 at 9:28 PM, Gary Malouf <malouf.gary@gmail.com> wrote:

> -1 I honestly do not know the voting rules for the Spark community, so
> please excuse me if I am out of line or if Mesos compatibility is not a
> concern at this point.
>
> We just tried to run this version built against 2.3.0-cdh5.0.2 on mesos
> 0.18.2.  All of our jobs with data above a few gigabytes hung indefinitely.
>  Downgrading back to the 1.0.0 stable release of Spark built the same way
> worked for us.
>
>
> On Mon, Jul 7, 2014 at 5:17 PM, Tom Graves <tgraves_cs@yahoo.com.invalid>
> wrote:
>
>> +1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with
>> authentication on.
>>
>> Tom
>>
>>
>> On Friday, July 4, 2014 2:39 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>
>>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.0.1!
>>
>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1021/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.1!
>>
>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.0.1
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> === Differences from RC1 ===
>> This release includes only one "blocking" patch from rc1:
>> https://github.com/apache/spark/pull/1255
>>
>> There are also smaller fixes which came in over the last week.
>>
>> === About this release ===
>> This release fixes a few high-priority bugs in 1.0 and has a variety
>> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
>> of the more visible patches are:
>>
>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>> size.
>> SPARK-1790: Support r3 instance types on EC2.
>>
>> This is the first maintenance release on the 1.0 line. We plan to make
>> additional maintenance releases as new fixes come in.
>>
>
>

--001a11c12d3478f49304fde0f8a0--

From dev-return-8294-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 01:40:44 2014
Return-Path: <dev-return-8294-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E1CA61001B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 01:40:43 +0000 (UTC)
Received: (qmail 52347 invoked by uid 500); 11 Jul 2014 01:40:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52272 invoked by uid 500); 11 Jul 2014 01:40:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52260 invoked by uid 99); 11 Jul 2014 01:40:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 01:40:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 01:40:40 +0000
Received: by mail-ob0-f172.google.com with SMTP id wn1so462235obc.17
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 18:40:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=MJPcKe9tWPf4xIwiIzpc59dCEBEwPaeOLa11pRMxupc=;
        b=KPh91JmftMSde19VMX9+VVjovRemy+NirAmTfuEHb63gDpr1/A+36Zg3kKTxnoX4Pp
         iKTkDPm0/k1Y05eM/EJy5IKmpPFArslR05jSAUUSL28kxMataABOBhDY8psrvFJAUcn6
         dZvvnma4qkWjCrtOqByvXdK3jyHmZYHxVngMQ91aM9UH+74PWq+qBHkN4OAjvr/Rb0lI
         rNCD8PRRyX+SasTVg4geAGCAUC/Wr4uYg3rujF0SDTDhlQjMgchIB9bvPfXkNvdoTWIj
         vfRjy5c/dXNOZOjLAPmfbLpJNGwjvbukg7rNf0AXewExP2yVxqHaOS1MQ47CdpG03Gc0
         xrTw==
MIME-Version: 1.0
X-Received: by 10.182.27.225 with SMTP id w1mr59362014obg.64.1405042815498;
 Thu, 10 Jul 2014 18:40:15 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Thu, 10 Jul 2014 18:40:15 -0700 (PDT)
In-Reply-To: <CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
	<CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
Date: Thu, 10 Jul 2014 18:40:15 -0700
Message-ID: <CABPQxsthhkXZUpPQkX4UPb=J2unPJFkwECYo0aznOXghx5hhLw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Gary,

The vote technically doesn't close until I send the vote summary
e-mail, but I was planning to close and package this tonight. It's too
bad if there is a regression, it might be worth holding the release
but it really requires narrowing down the issue to get more
information about the scope and severity. Could you fork another
thread for this?

- Patrick

On Thu, Jul 10, 2014 at 6:28 PM, Gary Malouf <malouf.gary@gmail.com> wrote:
> -1 I honestly do not know the voting rules for the Spark community, so
> please excuse me if I am out of line or if Mesos compatibility is not a
> concern at this point.
>
> We just tried to run this version built against 2.3.0-cdh5.0.2 on mesos
> 0.18.2.  All of our jobs with data above a few gigabytes hung indefinitely.
>  Downgrading back to the 1.0.0 stable release of Spark built the same way
> worked for us.
>
>
> On Mon, Jul 7, 2014 at 5:17 PM, Tom Graves <tgraves_cs@yahoo.com.invalid>
> wrote:
>
>> +1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with
>> authentication on.
>>
>> Tom
>>
>>
>> On Friday, July 4, 2014 2:39 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>
>>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.0.1!
>>
>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
>>
>> The release files, including signatures, digests, etc. can be found at:
>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/pwendell.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1021/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.1!
>>
>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>
>> [ ] +1 Release this package as Apache Spark 1.0.1
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>
>> === Differences from RC1 ===
>> This release includes only one "blocking" patch from rc1:
>> https://github.com/apache/spark/pull/1255
>>
>> There are also smaller fixes which came in over the last week.
>>
>> === About this release ===
>> This release fixes a few high-priority bugs in 1.0 and has a variety
>> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
>> of the more visible patches are:
>>
>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size.
>> SPARK-1790: Support r3 instance types on EC2.
>>
>> This is the first maintenance release on the 1.0 line. We plan to make
>> additional maintenance releases as new fixes come in.
>>

From dev-return-8295-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 06:18:15 2014
Return-Path: <dev-return-8295-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D3DEB107BD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 06:18:15 +0000 (UTC)
Received: (qmail 4680 invoked by uid 500); 11 Jul 2014 06:18:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4617 invoked by uid 500); 11 Jul 2014 06:18:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4602 invoked by uid 99); 11 Jul 2014 06:18:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 06:18:15 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of davies.liu@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 06:18:13 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <davies.liu@gmail.com>)
	id 1X5U9U-0002qY-DG
	for dev@spark.incubator.apache.org; Thu, 10 Jul 2014 23:17:48 -0700
Date: Thu, 10 Jul 2014 23:17:48 -0700 (PDT)
From: davies <davies.liu@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1405059468378-7269.post@n3.nabble.com>
In-Reply-To: <CALWDz_s9xXLMqDK6X=4boqG6GHteKRpjN5ZW1Od_39jLbyuOTg@mail.gmail.com>
References: <CALWDz_s9xXLMqDK6X=4boqG6GHteKRpjN5ZW1Od_39jLbyuOTg@mail.gmail.com>
Subject: Re: PySpark Driver from Jython
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

The function run in worker is serialized in driver, so the driver and worker
should be run in the same Python interpreter.

If you do not need c extension support, then Jython will be better than
CPython, because of the cost of serialization is much lower.

Davies



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-tp7142p7269.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8296-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 06:30:30 2014
Return-Path: <dev-return-8296-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3643610808
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 06:30:30 +0000 (UTC)
Received: (qmail 24802 invoked by uid 500); 11 Jul 2014 06:30:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24741 invoked by uid 500); 11 Jul 2014 06:30:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24721 invoked by uid 99); 11 Jul 2014 06:30:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 06:30:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wangfei1@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 06:30:27 +0000
Received: from 172.24.2.119 (EHLO szxeml418-hub.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id ARM67121;
	Fri, 11 Jul 2014 14:29:57 +0800 (CST)
Received: from [127.0.0.1] (10.177.17.18) by szxeml418-hub.china.huawei.com
 (10.82.67.157) with Microsoft SMTP Server id 14.3.158.1; Fri, 11 Jul 2014
 14:29:53 +0800
Message-ID: <53BF8464.5020509@huawei.com>
Date: Fri, 11 Jul 2014 14:29:56 +0800
From: kingfly <wangfei1@huawei.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:17.0) Gecko/20130509 Thunderbird/17.0.6
MIME-Version: 1.0
To: <dev@spark.apache.org>
Subject: what is the difference between org.spark-project.hive and org.apache.hadoop.hive
Content-Type: text/plain; charset="ISO-8859-1"; format=flowed
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.177.17.18]
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A020208.53BF8465.00BF,ss=1,re=0.000,fgs=0,
	ip=0.0.0.0,
	so=2013-05-26 15:14:31,
	dmn=2011-05-27 18:58:46
X-Mirapoint-Loop-Id: 99726cc900e1eb6379eaf2bdd7b0f70b
X-Virus-Checked: Checked by ClamAV on apache.org


-- 

Best Regards
Frank Wang | Software Engineer

Mobile: +86 18505816792
Phone: +86 571 63547
Fax:
Email: wangfei1@huawei.com
--------------------------------------------------------------------------------
Huawei Technologies Co., Ltd.
Hangzhou R&D Center
NO.410, JiangHong Road, Binjiang Area, Hangzhou, 310052, P. R. China



From dev-return-8297-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 06:51:23 2014
Return-Path: <dev-return-8297-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D0E22108BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 06:51:23 +0000 (UTC)
Received: (qmail 76076 invoked by uid 500); 11 Jul 2014 06:51:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76016 invoked by uid 500); 11 Jul 2014 06:51:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76002 invoked by uid 99); 11 Jul 2014 06:51:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 06:51:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 06:51:18 +0000
Received: by mail-oa0-f50.google.com with SMTP id g18so723669oah.37
        for <dev@spark.apache.org>; Thu, 10 Jul 2014 23:50:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=BN46nuAQ1IEEc3wArNcPbhiDyzhwG+Hf8/7OyG6TAw4=;
        b=ANdSUezeEdjmiX2d5EO5vO+l8PR0+H9YF8uye4p6VT1An+LfqxOTXC6FzELqMnNJZZ
         5acFetRXqR8sHjDbaY56d7QV9mpXvDreJjEDpg2FXMdwXjPagjO9lI8knw134tBCPMlt
         UNhI4eIf/a+uM6orXVEljfGAC8zKrBCzWXdJUs+fE8QIIcA4CJl8M40K3gxvlVblu4CL
         cCegQ6+uUGA7U/7QrtybbLrIjDlJrVEzcGEY4/xQi4giTsnlmjDr28u5RxSf/rHZ5hq+
         A2a4ozx4eZoetpFxwsihzfAz10pa7wfiWaDiUeAiE7zZSBksUc1DmwoTaPemTbTGvMUs
         XeUw==
MIME-Version: 1.0
X-Received: by 10.60.132.171 with SMTP id ov11mr60498284oeb.46.1405061457971;
 Thu, 10 Jul 2014 23:50:57 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Thu, 10 Jul 2014 23:50:57 -0700 (PDT)
In-Reply-To: <53BF8464.5020509@huawei.com>
References: <53BF8464.5020509@huawei.com>
Date: Thu, 10 Jul 2014 23:50:57 -0700
Message-ID: <CABPQxsshySgdp-SyEmPZ=PJ8092rEuQ0voxyYVuA7JGDEc6pSQ@mail.gmail.com>
Subject: Re: what is the difference between org.spark-project.hive and org.apache.hadoop.hive
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

There are two differences:

1. We publish hive with a shaded protobuf dependency to avoid
conflicts with some Hadoop versions.
2. We publish a proper hive-exec jar that only includes hive packages.
The upstream version of hive-exec bundles a bunch of other random
dependencies in it which makes it really hard for third-party projects
to use it.

On Thu, Jul 10, 2014 at 11:29 PM, kingfly <wangfei1@huawei.com> wrote:
>
> --
>
> Best Regards
> Frank Wang | Software Engineer
>
> Mobile: +86 18505816792
> Phone: +86 571 63547
> Fax:
> Email: wangfei1@huawei.com
> --------------------------------------------------------------------------------
> Huawei Technologies Co., Ltd.
> Hangzhou R&D Center
> NO.410, JiangHong Road, Binjiang Area, Hangzhou, 310052, P. R. China
>
>

From dev-return-8298-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 12:50:54 2014
Return-Path: <dev-return-8298-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00337113C8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 12:50:54 +0000 (UTC)
Received: (qmail 81302 invoked by uid 500); 11 Jul 2014 12:50:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81242 invoked by uid 500); 11 Jul 2014 12:50:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81224 invoked by uid 99); 11 Jul 2014 12:50:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 12:50:53 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pahomov.egor@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 12:50:47 +0000
Received: by mail-qg0-f54.google.com with SMTP id q107so864440qgd.13
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 05:50:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=amcvtURlH4ctkQGp0Ip+Y6Mn2S+7gMSP9m0GGNiWGTU=;
        b=gjC5D3x4C60dYyqML9Dxlk6mNRkZdkxP8Ii/qNMkoIAbZyYYXqXsb+ZsGGugcM2w8f
         6+bYcIOqmaGPjSHe2QI3b993F9Y+uQ88Gohw5Mq0emDw6Str5YEuOmmwqHCfUdb9yCch
         NOx/h2gkVww+kVXU6JaWmxKXCuPKy4Q7etKv4BcWgpqXiXrj5qQRI7CRE56SnroDXQYj
         31HnlvR7ob+3FGeH7WbM6zPWAdo9geb1ymVn5xThmTp1Bmhz4yypWZBO6odzXCoCrC04
         PQXfPaTMG7UfvdxdONbL9aGsqD2tWgzm0GGJ8Wf8bIgqIiobqfw6odffh+zYQCLs3BXO
         +Wjg==
MIME-Version: 1.0
X-Received: by 10.140.103.74 with SMTP id x68mr84087836qge.34.1405083026970;
 Fri, 11 Jul 2014 05:50:26 -0700 (PDT)
Received: by 10.140.29.11 with HTTP; Fri, 11 Jul 2014 05:50:26 -0700 (PDT)
Date: Fri, 11 Jul 2014 16:50:26 +0400
Message-ID: <CAMrx5DyJLCaQ6EsFpHOSGLVpYS48Cp4_ho+438GQ-sA+qZKvsA@mail.gmail.com>
Subject: How pySpark works?
From: Egor Pahomov <pahomov.egor@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134eda8b3e56804fdea625e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134eda8b3e56804fdea625e
Content-Type: text/plain; charset=UTF-8

Hi, I want to use pySpark, but can't understand how it works. Documentation
doesn't provide enough information.

1) How python shipped to cluster? Should machines in cluster already have
python?
2) What happens when I write some python code in "map" function - is it
shipped to cluster and just executed on it? How it understand all
dependencies, which my code need and ship it there? If I use Math in my
code in "map" does it mean, that I would ship Math class or some python
Math on cluster would be used?
3) I have c++ compiled code. Can I ship this executable with "addPyFile"
and just use "exec" function from python? Would it work?

-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*

--001a1134eda8b3e56804fdea625e--

From dev-return-8299-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 13:03:28 2014
Return-Path: <dev-return-8299-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 877AB1141C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 13:03:28 +0000 (UTC)
Received: (qmail 3970 invoked by uid 500); 11 Jul 2014 13:03:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3912 invoked by uid 500); 11 Jul 2014 13:03:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3900 invoked by uid 99); 11 Jul 2014 13:03:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 13:03:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pahomov.egor@gmail.com designates 209.85.216.169 as permitted sender)
Received: from [209.85.216.169] (HELO mail-qc0-f169.google.com) (209.85.216.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 13:03:18 +0000
Received: by mail-qc0-f169.google.com with SMTP id i17so942484qcy.14
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 06:02:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=/HWKAWv1UJ2H7mRKARqh4iuDBE2RGiSx6alQr7ijhIY=;
        b=AeEstR6G17w3NDmgr77nZmfSNdC+OFFi9VIrBaZVQllkyLhEnRqNUV19J1jBTjQMFQ
         OJ7VQrukd8CjNohO5zPa3iblmPin0fI5I/TN1XVhFLuvKMUyyC7odrIRkFZMy8fD553l
         4cKkhmiDhYv3k0xa9EIbkwGFLKUndtOBr2fNZXsC0JFvOwKlj/Wu+za4cohAi2nwNjUy
         UlxsKfrz2naVk4NykM5pfmii3tJFRNm2z1/5Ry+CSdRAkdkcNxFom1J4gtJXG1yKJGu0
         Hc5N//0AmLxHmdh2Um6DkBaDXTYXG2V2KEoMut28F9d/k2XT9nRYxqE8fEpPjYlV1zkj
         +8og==
MIME-Version: 1.0
X-Received: by 10.140.86.116 with SMTP id o107mr73458696qgd.14.1405083777492;
 Fri, 11 Jul 2014 06:02:57 -0700 (PDT)
Received: by 10.140.29.11 with HTTP; Fri, 11 Jul 2014 06:02:57 -0700 (PDT)
Date: Fri, 11 Jul 2014 17:02:57 +0400
Message-ID: <CAMrx5Dz3JB=xO0W5VRmjzhOS1RJ1RrHaKtnDGpQ2qDH9PASb-A@mail.gmail.com>
Subject: Random forest - is it under implementation?
From: Egor Pahomov <pahomov.egor@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c13f086fedfb04fdea8f29
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c13f086fedfb04fdea8f29
Content-Type: text/plain; charset=UTF-8

Hi, I have intern, who wants to implement some ML algorithm for spark.
Which algorithm would be good idea to implement(it should be not very
difficult)? I heard someone already working on random forest, but couldn't
find proof of that.

I'm aware of new politics, where we should implement stable, good quality,
popular ML or do not do it at all.

-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*

--001a11c13f086fedfb04fdea8f29--

From dev-return-8300-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 13:23:52 2014
Return-Path: <dev-return-8300-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CF0FE114A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 13:23:52 +0000 (UTC)
Received: (qmail 32248 invoked by uid 500); 11 Jul 2014 13:23:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32184 invoked by uid 500); 11 Jul 2014 13:23:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32173 invoked by uid 99); 11 Jul 2014 13:23:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 13:23:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.220.46 as permitted sender)
Received: from [209.85.220.46] (HELO mail-pa0-f46.google.com) (209.85.220.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 13:23:45 +0000
Received: by mail-pa0-f46.google.com with SMTP id eu11so1460168pac.5
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 06:23:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:references:in-reply-to:mime-version
         :content-transfer-encoding:content-type:message-id:cc:from:subject
         :date:to;
        bh=7YyyCzb7mHspDj64RLtHjrRYaNJoRUiEuhUhHeN/o+k=;
        b=brhegAfknivinHh79qe+UwLU4I5GmnOxDQjcI91sVAah9pA3f4cHA8XtS+mIQBpf23
         Gr8rchf6Q7dnjZFBh6Cf5sPCuoHdzR9AWckWQ4/63/Vn31P9EqOGI7mkjYd21IY4vbYW
         o6SD5ztEYM04WCXm2jateMmq5u4Sbq0+LPLNTQwh59TsBbgKjAyB/iE0Qt5fPRgoeIz2
         QhwGolFzB8c4u5v3CktpWd1zNZ66ek2x17Ld7o5J+Reutdd4Ov1M9vnH0g5PpJ5epQ1d
         2iK0OTGIfNoP1+aWWEAjUNFMOIHDKYFtrlij+K8gAT7ivSMKd64ypYgP2PHwjFtZkUxN
         Yu0A==
X-Gm-Message-State: ALoCoQlz90UWqiOEGzbPHAzkEnphkd3+iqY3QkVHfjktAEJt1IdGR2Ofj0mSTZafL+JM1BsY1xx4
X-Received: by 10.66.252.35 with SMTP id zp3mr53931328pac.40.1405085004496;
        Fri, 11 Jul 2014 06:23:24 -0700 (PDT)
Received: from [192.168.2.5] ([24.5.225.89])
        by mx.google.com with ESMTPSA id sv10sm9424035pab.32.2014.07.11.06.23.22
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 11 Jul 2014 06:23:22 -0700 (PDT)
References: <CAMrx5Dz3JB=xO0W5VRmjzhOS1RJ1RrHaKtnDGpQ2qDH9PASb-A@mail.gmail.com>
In-Reply-To: <CAMrx5Dz3JB=xO0W5VRmjzhOS1RJ1RrHaKtnDGpQ2qDH9PASb-A@mail.gmail.com>
Mime-Version: 1.0 (iPad Mail 8F191)
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii
Message-Id: <EC456996-967D-414B-8088-6938C40ADBA2@alpinenow.com>
Cc: Sung Chung <schung@alpinenow.com>
X-Mailer: iPad Mail (8F191)
From: Chester At Work <chester@alpinenow.com>
Subject: Re: Random forest - is it under implementation?
Date: Fri, 11 Jul 2014 06:43:34 -0700
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Virus-Checked: Checked by ClamAV on apache.org

Sung chung from alpine data labs presented the random Forrest implementation=
 at Spark summit 2014. The work will be open sourced and contributed back to=
 MLLib.

Stay tuned=20



Sent from my iPad

On Jul 11, 2014, at 6:02 AM, Egor Pahomov <pahomov.egor@gmail.com> wrote:

> Hi, I have intern, who wants to implement some ML algorithm for spark.
> Which algorithm would be good idea to implement(it should be not very
> difficult)? I heard someone already working on random forest, but couldn't=

> find proof of that.
>=20
> I'm aware of new politics, where we should implement stable, good quality,=

> popular ML or do not do it at all.
>=20
> --=20
>=20
>=20
>=20
> *Sincerely yoursEgor PakhomovScala Developer, Yandex*

From dev-return-8301-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 13:44:36 2014
Return-Path: <dev-return-8301-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC0531151E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 13:44:35 +0000 (UTC)
Received: (qmail 72521 invoked by uid 500); 11 Jul 2014 13:44:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72461 invoked by uid 500); 11 Jul 2014 13:44:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72443 invoked by uid 99); 11 Jul 2014 13:44:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 13:44:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pahomov.egor@gmail.com designates 209.85.216.41 as permitted sender)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 13:44:32 +0000
Received: by mail-qa0-f41.google.com with SMTP id j7so230988qaq.28
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 06:44:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=TIROco8j7W1p/ia35wcNWVTPMtCIRDpI1vl5eh6M72g=;
        b=NrgcDOtzFPKY5qcUf4mahX4YTd5KMtYAc8y5n5/PGXj1nLfjUZaRVS1H+pPvSGmn8i
         LdgahfQY8fTEkspN/aakAAumFGPjfr7M31fC6vHe+Cum9V977w1beEfkLMX+pNgconmb
         dCFObBjOy9V6hQEsoyTSxNiZMOiapG/J3ntECERuZ94KJ32thb51z1ZpDuqoMIdvK0oz
         EbLH10guNMI/gt6HyQxHSNd3IZTvOIMwjrPSsIRlDbE8tRc8NOORgZz38O1YHD3sc5Mo
         50VPoN8sfua52s/GlSKFKklbbM9Wqsc5L6162x7NiYrDPaJc5jM47wMKmioImz0qS41Q
         0NWg==
MIME-Version: 1.0
X-Received: by 10.224.171.197 with SMTP id i5mr49943197qaz.55.1405086247893;
 Fri, 11 Jul 2014 06:44:07 -0700 (PDT)
Received: by 10.140.29.11 with HTTP; Fri, 11 Jul 2014 06:44:07 -0700 (PDT)
In-Reply-To: <EC456996-967D-414B-8088-6938C40ADBA2@alpinenow.com>
References: <CAMrx5Dz3JB=xO0W5VRmjzhOS1RJ1RrHaKtnDGpQ2qDH9PASb-A@mail.gmail.com>
	<EC456996-967D-414B-8088-6938C40ADBA2@alpinenow.com>
Date: Fri, 11 Jul 2014 17:44:07 +0400
Message-ID: <CAMrx5DwCt-RP2GFTutpgwxNrZZX7Q=j5DE-_7HuSt=ARecRaSw@mail.gmail.com>
Subject: Re: Random forest - is it under implementation?
From: Egor Pahomov <pahomov.egor@gmail.com>
To: dev@spark.apache.org
Cc: Sung Chung <schung@alpinenow.com>
Content-Type: multipart/alternative; boundary=001a11c2c83aaf44dd04fdeb22cb
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c83aaf44dd04fdeb22cb
Content-Type: text/plain; charset=UTF-8

Great. Then one question left:
what would you recommend for implementation?



2014-07-11 17:43 GMT+04:00 Chester At Work <chester@alpinenow.com>:

> Sung chung from alpine data labs presented the random Forrest
> implementation at Spark summit 2014. The work will be open sourced and
> contributed back to MLLib.
>
> Stay tuned
>
>
>
> Sent from my iPad
>
> On Jul 11, 2014, at 6:02 AM, Egor Pahomov <pahomov.egor@gmail.com> wrote:
>
> > Hi, I have intern, who wants to implement some ML algorithm for spark.
> > Which algorithm would be good idea to implement(it should be not very
> > difficult)? I heard someone already working on random forest, but
> couldn't
> > find proof of that.
> >
> > I'm aware of new politics, where we should implement stable, good
> quality,
> > popular ML or do not do it at all.
> >
> > --
> >
> >
> >
> > *Sincerely yoursEgor PakhomovScala Developer, Yandex*
>



-- 



*Sincerely yoursEgor PakhomovScala Developer, Yandex*

--001a11c2c83aaf44dd04fdeb22cb--

From dev-return-8302-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 15:21:13 2014
Return-Path: <dev-return-8302-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 302CB117E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 15:21:13 +0000 (UTC)
Received: (qmail 10437 invoked by uid 500); 11 Jul 2014 15:21:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10386 invoked by uid 500); 11 Jul 2014 15:21:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 21442 invoked by uid 99); 11 Jul 2014 12:05:27 -0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=JGL74U//S3C3vbEwl4n2TJCEJOBMocAYmXZT59KFMsw=;
        b=ZXx2rmogSz8+D0mW7D0Dmyh/XZZRsfuuUmshRPgY4QRbo/GL9hUScDsbOsEB/xaSAL
         uVlP9MK4eWXa9Vl+p8tYFyGXGzb0cV7fmaQzxqJhGnKIFftkCnMfXQyVPM2mNfM0VpQP
         Y59hH3TY3uiWXpDxu8zb4dEWU1bZWWO3sNC8uChjI4jaSlyPwq5tGXtZf6FPrVmK+BI2
         E278rCR/Vmb5v+4tBb7hQ9LcUd525YM9Dw3kxFZgbZMHjr8SWOCfSaz2CB2vlwhOO3sQ
         Z0q0Zd2XdC60+kQiS9cQGfAkV0HXDmdZc1HeINkgboXvEH6P10w44VWKQ226xBJyLdtn
         IHdQ==
X-Gm-Message-State: ALoCoQkAardO8aXSNz+sQ/hmdtiKFRyLmRqIt+WC1GQsWXRWaclYgy9LIuIR2/AxRO3IPhVjWOP3
MIME-Version: 1.0
X-Received: by 10.140.90.7 with SMTP id w7mr88563082qgd.52.1405080299380; Fri,
 11 Jul 2014 05:04:59 -0700 (PDT)
X-Originating-IP: [103.14.2.1]
Date: Fri, 11 Jul 2014 17:34:59 +0530
Message-ID: <CAHmnFJ7zYO6Mk1+RodhfXR8xO-=mimsbd2WzHV2ULrZuJG_FNw@mail.gmail.com>
Subject: Calling Scala/Java methods which operates on RDD
From: Jai Kumar Singh <flukebox@flukebox.in>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c124ce203bba04fde9c0f1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c124ce203bba04fde9c0f1
Content-Type: text/plain; charset=UTF-8

HI,
  I want to write some common utility function in Scala and want to call
the same from Java/Python Spark API ( may be add some wrapper code around
scala calls). Calling Scala functions from Java works fine. I was reading
pyspark rdd code and find out that pyspark is able to call JavaRDD function
like union/zip to get same for pyspark RDD and deserializing the output and
everything works fine. But somehow I am
not able to work out really simple example. I think I am missing some
serialization/deserialization.

Can someone confirm that is it even possible to do so? Or, would it be much
easier to pass RDD data files around instead of RDD directly (from pyspark
to java/scala)?

For example, below code just add 1 to each element of RDD containing
Integers.

package flukebox.test;

object TestClass{

def testFunc(data:RDD[Int])={

  data.map(x => x+1)

}

}

Calling from python,

from pyspark import RDD

from py4j.java_gateway import java_import

java_import(sc._gateway.jvm, "flukebox.test")


data = sc.parallelize([1,2,3,4,5,6,7,8,9])

sc._jvm.flukebox.test.TestClass.testFunc(data._jrdd.rdd())


*This fails because testFunc get any RDD of type Byte Array.*


Any help/pointer would be highly appreciated.


Thanks & Regards,

Jai K Singh

--001a11c124ce203bba04fde9c0f1--

From dev-return-8303-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 16:08:43 2014
Return-Path: <dev-return-8303-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2EA09118E9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 16:08:43 +0000 (UTC)
Received: (qmail 97361 invoked by uid 500); 11 Jul 2014 16:08:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97289 invoked by uid 500); 11 Jul 2014 16:08:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97275 invoked by uid 99); 11 Jul 2014 16:08:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:08:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.213.178 as permitted sender)
Received: from [209.85.213.178] (HELO mail-ig0-f178.google.com) (209.85.213.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:08:39 +0000
Received: by mail-ig0-f178.google.com with SMTP id uq10so214772igb.11
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 09:08:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=MH2W9eK84YMK6SNl197Ct6e4mNbChAY4hMlFQ0Xh8pY=;
        b=Vud7U1ddIHaIa00EiiAlTuvigX2B6UlvHMpMPpgxE1vyZeGj4JRMOo9kwcmCSCRtl0
         CtVGMox0JzsLdPafAPtPzMmPKMiibrO1SVfFgJjWTK067fwwen+OKlaupnnAnxhetI6y
         sYr+3Gy475ppeuhVbTCZnYOOocWhcAMScJpTfqyO2oKjEt6pgTVaADYSo1kvEuqteh/U
         ii7370Wf/OuW0UNgPXFANwLViLFpCRGf2ZK543SrPFdVDfXHcpbyPTIDwrFJTRhDL6J1
         YoCm7/myPVSQhbpt7g5Fg+NKJWEK0R5094mZ50jRuLrPo7aQpJS/XqH3XcN0AYMcAM0P
         8hgg==
X-Received: by 10.50.124.227 with SMTP id ml3mr5711724igb.46.1405094894597;
        Fri, 11 Jul 2014 09:08:14 -0700 (PDT)
Received: from [192.168.1.109] (CPE68b6fc3fbad3-CM68b6fc3fbad0.cpe.net.cable.rogers.com. [99.226.46.122])
        by mx.google.com with ESMTPSA id t1sm7110180igh.9.2014.07.11.09.08.09
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 11 Jul 2014 09:08:10 -0700 (PDT)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxsthhkXZUpPQkX4UPb=J2unPJFkwECYo0aznOXghx5hhLw@mail.gmail.com>
Date: Fri, 11 Jul 2014 12:08:07 -0400
Content-Transfer-Encoding: quoted-printable
Message-Id: <039203BF-0092-4F64-B34D-A448D89F5CE2@gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com> <1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com> <CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com> <CABPQxsthhkXZUpPQkX4UPb=J2unPJFkwECYo0aznOXghx5hhLw@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Unless you can diagnose the problem quickly, Gary, I think we need to go =
ahead with this release as is. This release didn't touch the Mesos =
support as far as I know, so the problem might be a nondeterministic =
issue with your application. But on the other hand the release does fix =
some critical bugs that affect all users. We can always do 1.0.2 later =
if we discover a problem.

Matei

On Jul 10, 2014, at 9:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Gary,
>=20
> The vote technically doesn't close until I send the vote summary
> e-mail, but I was planning to close and package this tonight. It's too
> bad if there is a regression, it might be worth holding the release
> but it really requires narrowing down the issue to get more
> information about the scope and severity. Could you fork another
> thread for this?
>=20
> - Patrick
>=20
> On Thu, Jul 10, 2014 at 6:28 PM, Gary Malouf <malouf.gary@gmail.com> =
wrote:
>> -1 I honestly do not know the voting rules for the Spark community, =
so
>> please excuse me if I am out of line or if Mesos compatibility is not =
a
>> concern at this point.
>>=20
>> We just tried to run this version built against 2.3.0-cdh5.0.2 on =
mesos
>> 0.18.2.  All of our jobs with data above a few gigabytes hung =
indefinitely.
>> Downgrading back to the 1.0.0 stable release of Spark built the same =
way
>> worked for us.
>>=20
>>=20
>> On Mon, Jul 7, 2014 at 5:17 PM, Tom Graves =
<tgraves_cs@yahoo.com.invalid>
>> wrote:
>>=20
>>> +1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with
>>> authentication on.
>>>=20
>>> Tom
>>>=20
>>>=20
>>> On Friday, July 4, 2014 2:39 PM, Patrick Wendell =
<pwendell@gmail.com>
>>> wrote:
>>>=20
>>>=20
>>>=20
>>> Please vote on releasing the following candidate as Apache Spark =
version
>>> 1.0.1!
>>>=20
>>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>>>=20
>>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D7d104=
3c99303b87aef8ee19873629c2bfba4cc78
>>>=20
>>> The release files, including signatures, digests, etc. can be found =
at:
>>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>>>=20
>>> Release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/pwendell.asc
>>>=20
>>> The staging repository for this release can be found at:
>>> =
https://repository.apache.org/content/repositories/orgapachespark-1021/
>>>=20
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>>>=20
>>> Please vote on releasing this package as Apache Spark 1.0.1!
>>>=20
>>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
>>> a majority of at least 3 +1 PMC votes are cast.
>>>=20
>>> [ ] +1 Release this package as Apache Spark 1.0.1
>>> [ ] -1 Do not release this package because ...
>>>=20
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>=20
>>> =3D=3D=3D Differences from RC1 =3D=3D=3D
>>> This release includes only one "blocking" patch from rc1:
>>> https://github.com/apache/spark/pull/1255
>>>=20
>>> There are also smaller fixes which came in over the last week.
>>>=20
>>> =3D=3D=3D About this release =3D=3D=3D
>>> This release fixes a few high-priority bugs in 1.0 and has a variety
>>> of smaller fixes. The full list is here: http://s.apache.org/b45. =
Some
>>> of the more visible patches are:
>>>=20
>>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka =
frame size.
>>> SPARK-1790: Support r3 instance types on EC2.
>>>=20
>>> This is the first maintenance release on the 1.0 line. We plan to =
make
>>> additional maintenance releases as new fixes come in.
>>>=20


From dev-return-8304-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 16:31:54 2014
Return-Path: <dev-return-8304-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 02C40119C6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 16:31:54 +0000 (UTC)
Received: (qmail 69757 invoked by uid 500); 11 Jul 2014 16:31:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69696 invoked by uid 500); 11 Jul 2014 16:31:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69681 invoked by uid 99); 11 Jul 2014 16:31:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:31:52 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.49 as permitted sender)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:31:49 +0000
Received: by mail-qg0-f49.google.com with SMTP id 63so1158447qgz.36
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 09:31:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ORiNYagYNwLGojqeUkukX/L+nteD1pm1H2ybIut3if8=;
        b=rWf+sVLmBPB85P0LMLhUN4Tyz9wULkl9fx2FfIHU6hYKxg5zi8jWmc2jAq65hwC12+
         RENaCVnMkf9R0VEs1RhUzvG7gbsYWoZOPWPqg182Pmjj6tlGBq2LeEQMC+yDml31X4mj
         bklCb5tl3QvF6ZwqYecrhD+Qa1vVbVESypqVKwFzGcXaf0pN8dztGePvDqw9SpaDl5Lh
         oNbwMRS4zxWqGIjVr++IAUpLQ3FPMSsuWap/oyJk4WZBOV4zY6+MEsOs//Q8Z6Z51SVQ
         Nyc92bVIRXZvHE+9GyEYf9yG9FL2WxOxKfIGSBG5sozu9OkejeA2wCQ2ynHsqAVImw9t
         TRxg==
MIME-Version: 1.0
X-Received: by 10.140.20.247 with SMTP id 110mr89397250qgj.49.1405096284975;
 Fri, 11 Jul 2014 09:31:24 -0700 (PDT)
Received: by 10.140.49.236 with HTTP; Fri, 11 Jul 2014 09:31:24 -0700 (PDT)
In-Reply-To: <039203BF-0092-4F64-B34D-A448D89F5CE2@gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
	<CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
	<CABPQxsthhkXZUpPQkX4UPb=J2unPJFkwECYo0aznOXghx5hhLw@mail.gmail.com>
	<039203BF-0092-4F64-B34D-A448D89F5CE2@gmail.com>
Date: Fri, 11 Jul 2014 12:31:24 -0400
Message-ID: <CAGOvqiqbzuqRfEbbRoTJPwKhc9D+jrkTqCdaBip4OaSV04O1LA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c12d34f0f8ac04fded7820
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12d34f0f8ac04fded7820
Content-Type: text/plain; charset=UTF-8

Hi Matei,

We have not had time to re-deploy the rc today, but one thing that jumps
out is the shrinking of the default akka frame size from 10MB to around
128KB by default.  That is my first suspicion for our issue - could imagine
that biting others as well.

I'll try to re-test that today - either way, understand moving forward at
this point.

Gary


On Fri, Jul 11, 2014 at 12:08 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Unless you can diagnose the problem quickly, Gary, I think we need to go
> ahead with this release as is. This release didn't touch the Mesos support
> as far as I know, so the problem might be a nondeterministic issue with
> your application. But on the other hand the release does fix some critical
> bugs that affect all users. We can always do 1.0.2 later if we discover a
> problem.
>
> Matei
>
> On Jul 10, 2014, at 9:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
> > Hey Gary,
> >
> > The vote technically doesn't close until I send the vote summary
> > e-mail, but I was planning to close and package this tonight. It's too
> > bad if there is a regression, it might be worth holding the release
> > but it really requires narrowing down the issue to get more
> > information about the scope and severity. Could you fork another
> > thread for this?
> >
> > - Patrick
> >
> > On Thu, Jul 10, 2014 at 6:28 PM, Gary Malouf <malouf.gary@gmail.com>
> wrote:
> >> -1 I honestly do not know the voting rules for the Spark community, so
> >> please excuse me if I am out of line or if Mesos compatibility is not a
> >> concern at this point.
> >>
> >> We just tried to run this version built against 2.3.0-cdh5.0.2 on mesos
> >> 0.18.2.  All of our jobs with data above a few gigabytes hung
> indefinitely.
> >> Downgrading back to the 1.0.0 stable release of Spark built the same way
> >> worked for us.
> >>
> >>
> >> On Mon, Jul 7, 2014 at 5:17 PM, Tom Graves <tgraves_cs@yahoo.com.invalid
> >
> >> wrote:
> >>
> >>> +1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with
> >>> authentication on.
> >>>
> >>> Tom
> >>>
> >>>
> >>> On Friday, July 4, 2014 2:39 PM, Patrick Wendell <pwendell@gmail.com>
> >>> wrote:
> >>>
> >>>
> >>>
> >>> Please vote on releasing the following candidate as Apache Spark
> version
> >>> 1.0.1!
> >>>
> >>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
> >>>
> >>>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
> >>>
> >>> The release files, including signatures, digests, etc. can be found at:
> >>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
> >>>
> >>> Release artifacts are signed with the following key:
> >>> https://people.apache.org/keys/committer/pwendell.asc
> >>>
> >>> The staging repository for this release can be found at:
> >>>
> https://repository.apache.org/content/repositories/orgapachespark-1021/
> >>>
> >>> The documentation corresponding to this release can be found at:
> >>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
> >>>
> >>> Please vote on releasing this package as Apache Spark 1.0.1!
> >>>
> >>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
> >>> a majority of at least 3 +1 PMC votes are cast.
> >>>
> >>> [ ] +1 Release this package as Apache Spark 1.0.1
> >>> [ ] -1 Do not release this package because ...
> >>>
> >>> To learn more about Apache Spark, please see
> >>> http://spark.apache.org/
> >>>
> >>> === Differences from RC1 ===
> >>> This release includes only one "blocking" patch from rc1:
> >>> https://github.com/apache/spark/pull/1255
> >>>
> >>> There are also smaller fixes which came in over the last week.
> >>>
> >>> === About this release ===
> >>> This release fixes a few high-priority bugs in 1.0 and has a variety
> >>> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
> >>> of the more visible patches are:
> >>>
> >>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> >>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> size.
> >>> SPARK-1790: Support r3 instance types on EC2.
> >>>
> >>> This is the first maintenance release on the 1.0 line. We plan to make
> >>> additional maintenance releases as new fixes come in.
> >>>
>
>

--001a11c12d34f0f8ac04fded7820--

From dev-return-8305-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 16:38:11 2014
Return-Path: <dev-return-8305-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8289B119EA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 16:38:11 +0000 (UTC)
Received: (qmail 87288 invoked by uid 500); 11 Jul 2014 16:38:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87236 invoked by uid 500); 11 Jul 2014 16:38:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87223 invoked by uid 99); 11 Jul 2014 16:38:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:38:10 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:38:06 +0000
Received: by mail-oa0-f41.google.com with SMTP id l6so1488490oag.0
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 09:37:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=B0i3eZkN95ElNpqc8gxb3M79/63cGiEspe5bH3Lk5Xs=;
        b=s658JptrxZyOzXlk8Ur6gNdK/nXo+uHAhy3cdmWeKEJk0ExQGlWEEi2diGmrFqB4II
         EnBJuAc2qKyqdvkLaJPM/WjKyOZMblLydLHx/fvTdipYd6pNPVIXy1bjirhPchsYUvjW
         4i84EcXxP+iBTFBzIS6rsVvdQ54AB6UMCJftaqh07+kvyGPQRqNbs4xqocUh40uQ6oEr
         zcuDvjC6tnBnEcKtHub4DBDlWX99XOnxIE2jPpXMxiCpzhGLIl5kbP3rwS7r5dj2HSPD
         oHGIB7Pe09pMIq8zO0a5YcgCu4fs4DLWCYdnU2x+lB4ykajXbfhtyIf7zOF2zn4KK7EA
         mZdw==
MIME-Version: 1.0
X-Received: by 10.60.132.171 with SMTP id ov11mr31810oeb.46.1405096665303;
 Fri, 11 Jul 2014 09:37:45 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 11 Jul 2014 09:37:45 -0700 (PDT)
In-Reply-To: <CAGOvqiqbzuqRfEbbRoTJPwKhc9D+jrkTqCdaBip4OaSV04O1LA@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
	<CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
	<CABPQxsthhkXZUpPQkX4UPb=J2unPJFkwECYo0aznOXghx5hhLw@mail.gmail.com>
	<039203BF-0092-4F64-B34D-A448D89F5CE2@gmail.com>
	<CAGOvqiqbzuqRfEbbRoTJPwKhc9D+jrkTqCdaBip4OaSV04O1LA@mail.gmail.com>
Date: Fri, 11 Jul 2014 09:37:45 -0700
Message-ID: <CABPQxsuKe+HjVqcdhs3kzaMvCYkiDnN0to=0k4AdL9_b3cNA3A@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Gary,

Why do you think the akka frame size changed? It didn't change - we
added some fixes for cases where users were setting non-default
values.

On Fri, Jul 11, 2014 at 9:31 AM, Gary Malouf <malouf.gary@gmail.com> wrote:
> Hi Matei,
>
> We have not had time to re-deploy the rc today, but one thing that jumps
> out is the shrinking of the default akka frame size from 10MB to around
> 128KB by default.  That is my first suspicion for our issue - could imagine
> that biting others as well.
>
> I'll try to re-test that today - either way, understand moving forward at
> this point.
>
> Gary
>
>
> On Fri, Jul 11, 2014 at 12:08 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
>> Unless you can diagnose the problem quickly, Gary, I think we need to go
>> ahead with this release as is. This release didn't touch the Mesos support
>> as far as I know, so the problem might be a nondeterministic issue with
>> your application. But on the other hand the release does fix some critical
>> bugs that affect all users. We can always do 1.0.2 later if we discover a
>> problem.
>>
>> Matei
>>
>> On Jul 10, 2014, at 9:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>
>> > Hey Gary,
>> >
>> > The vote technically doesn't close until I send the vote summary
>> > e-mail, but I was planning to close and package this tonight. It's too
>> > bad if there is a regression, it might be worth holding the release
>> > but it really requires narrowing down the issue to get more
>> > information about the scope and severity. Could you fork another
>> > thread for this?
>> >
>> > - Patrick
>> >
>> > On Thu, Jul 10, 2014 at 6:28 PM, Gary Malouf <malouf.gary@gmail.com>
>> wrote:
>> >> -1 I honestly do not know the voting rules for the Spark community, so
>> >> please excuse me if I am out of line or if Mesos compatibility is not a
>> >> concern at this point.
>> >>
>> >> We just tried to run this version built against 2.3.0-cdh5.0.2 on mesos
>> >> 0.18.2.  All of our jobs with data above a few gigabytes hung
>> indefinitely.
>> >> Downgrading back to the 1.0.0 stable release of Spark built the same way
>> >> worked for us.
>> >>
>> >>
>> >> On Mon, Jul 7, 2014 at 5:17 PM, Tom Graves <tgraves_cs@yahoo.com.invalid
>> >
>> >> wrote:
>> >>
>> >>> +1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with
>> >>> authentication on.
>> >>>
>> >>> Tom
>> >>>
>> >>>
>> >>> On Friday, July 4, 2014 2:39 PM, Patrick Wendell <pwendell@gmail.com>
>> >>> wrote:
>> >>>
>> >>>
>> >>>
>> >>> Please vote on releasing the following candidate as Apache Spark
>> version
>> >>> 1.0.1!
>> >>>
>> >>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>> >>>
>> >>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
>> >>>
>> >>> The release files, including signatures, digests, etc. can be found at:
>> >>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>> >>>
>> >>> Release artifacts are signed with the following key:
>> >>> https://people.apache.org/keys/committer/pwendell.asc
>> >>>
>> >>> The staging repository for this release can be found at:
>> >>>
>> https://repository.apache.org/content/repositories/orgapachespark-1021/
>> >>>
>> >>> The documentation corresponding to this release can be found at:
>> >>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>> >>>
>> >>> Please vote on releasing this package as Apache Spark 1.0.1!
>> >>>
>> >>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
>> >>> a majority of at least 3 +1 PMC votes are cast.
>> >>>
>> >>> [ ] +1 Release this package as Apache Spark 1.0.1
>> >>> [ ] -1 Do not release this package because ...
>> >>>
>> >>> To learn more about Apache Spark, please see
>> >>> http://spark.apache.org/
>> >>>
>> >>> === Differences from RC1 ===
>> >>> This release includes only one "blocking" patch from rc1:
>> >>> https://github.com/apache/spark/pull/1255
>> >>>
>> >>> There are also smaller fixes which came in over the last week.
>> >>>
>> >>> === About this release ===
>> >>> This release fixes a few high-priority bugs in 1.0 and has a variety
>> >>> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
>> >>> of the more visible patches are:
>> >>>
>> >>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>> >>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>> size.
>> >>> SPARK-1790: Support r3 instance types on EC2.
>> >>>
>> >>> This is the first maintenance release on the 1.0 line. We plan to make
>> >>> additional maintenance releases as new fixes come in.
>> >>>
>>
>>

From dev-return-8306-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 16:46:37 2014
Return-Path: <dev-return-8306-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 20D7711A26
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 16:46:37 +0000 (UTC)
Received: (qmail 2392 invoked by uid 500); 11 Jul 2014 16:46:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2337 invoked by uid 500); 11 Jul 2014 16:46:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2324 invoked by uid 99); 11 Jul 2014 16:46:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:46:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:46:34 +0000
Received: by mail-oa0-f44.google.com with SMTP id eb12so1478312oac.17
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 09:46:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=xEZc8ATJ6yfzr0U93JtY45excpH95CtNj7qmPPLRzgU=;
        b=C5WagX8EKCOsfi1vmtpipQ5Wgwet74ri1AzjpBAeTsmL/vG53T3QmDxPosCGlis6+9
         YQEUC3jd8J4UJN4ls2sy2rhDWJk9dp3TKRrBfueJ20C3nq4QEkEHxiIYJ79HmaPGLhfL
         3qQlpvm846q9W0WGM0N6oP1FxAzn3ztAUvhERNxGdZt9cMso1dRAdbEixDlfHrdbMjbG
         aWaHFGuZRldt3gzSMk1Y1lPoQZ6hCq4BN1m0yogx09zPzLpTFrcro6aygg56zEd69uNH
         cQsDOGOz7KiI6BqKjgsaSrghkCgaGV2n8x2HVT1xheMKYmygaKwgqdz3bam07tGF3UJD
         +o4w==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr42918811obc.75.1405097169090;
 Fri, 11 Jul 2014 09:46:09 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 11 Jul 2014 09:46:09 -0700 (PDT)
In-Reply-To: <CABPQxsuKe+HjVqcdhs3kzaMvCYkiDnN0to=0k4AdL9_b3cNA3A@mail.gmail.com>
References: <CABPQxss7Cf+YaUuxCk0jnusH4207hCP4dkWn3BWFSvdnD86HHQ@mail.gmail.com>
	<1404767863.71288.YahooMailNeo@web140103.mail.bf1.yahoo.com>
	<CAGOvqiohehignhERWqMEz6OYwr5cpbMMxiQaTpa8Qun2k4CP_w@mail.gmail.com>
	<CABPQxsthhkXZUpPQkX4UPb=J2unPJFkwECYo0aznOXghx5hhLw@mail.gmail.com>
	<039203BF-0092-4F64-B34D-A448D89F5CE2@gmail.com>
	<CAGOvqiqbzuqRfEbbRoTJPwKhc9D+jrkTqCdaBip4OaSV04O1LA@mail.gmail.com>
	<CABPQxsuKe+HjVqcdhs3kzaMvCYkiDnN0to=0k4AdL9_b3cNA3A@mail.gmail.com>
Date: Fri, 11 Jul 2014 09:46:09 -0700
Message-ID: <CABPQxstDnxHEO3u7EQ2o-ErCE1KuP-GmvVw8E826uq1SH9BYxw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Okay just FYI - I'm closing this vote since many people are waiting on
the release and I was hoping to package it today. If we find a
reproducible Mesos issue here, we can definitely spin the fix into a
subsequent release.



On Fri, Jul 11, 2014 at 9:37 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Gary,
>
> Why do you think the akka frame size changed? It didn't change - we
> added some fixes for cases where users were setting non-default
> values.
>
> On Fri, Jul 11, 2014 at 9:31 AM, Gary Malouf <malouf.gary@gmail.com> wrote:
>> Hi Matei,
>>
>> We have not had time to re-deploy the rc today, but one thing that jumps
>> out is the shrinking of the default akka frame size from 10MB to around
>> 128KB by default.  That is my first suspicion for our issue - could imagine
>> that biting others as well.
>>
>> I'll try to re-test that today - either way, understand moving forward at
>> this point.
>>
>> Gary
>>
>>
>> On Fri, Jul 11, 2014 at 12:08 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> wrote:
>>
>>> Unless you can diagnose the problem quickly, Gary, I think we need to go
>>> ahead with this release as is. This release didn't touch the Mesos support
>>> as far as I know, so the problem might be a nondeterministic issue with
>>> your application. But on the other hand the release does fix some critical
>>> bugs that affect all users. We can always do 1.0.2 later if we discover a
>>> problem.
>>>
>>> Matei
>>>
>>> On Jul 10, 2014, at 9:40 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>>>
>>> > Hey Gary,
>>> >
>>> > The vote technically doesn't close until I send the vote summary
>>> > e-mail, but I was planning to close and package this tonight. It's too
>>> > bad if there is a regression, it might be worth holding the release
>>> > but it really requires narrowing down the issue to get more
>>> > information about the scope and severity. Could you fork another
>>> > thread for this?
>>> >
>>> > - Patrick
>>> >
>>> > On Thu, Jul 10, 2014 at 6:28 PM, Gary Malouf <malouf.gary@gmail.com>
>>> wrote:
>>> >> -1 I honestly do not know the voting rules for the Spark community, so
>>> >> please excuse me if I am out of line or if Mesos compatibility is not a
>>> >> concern at this point.
>>> >>
>>> >> We just tried to run this version built against 2.3.0-cdh5.0.2 on mesos
>>> >> 0.18.2.  All of our jobs with data above a few gigabytes hung
>>> indefinitely.
>>> >> Downgrading back to the 1.0.0 stable release of Spark built the same way
>>> >> worked for us.
>>> >>
>>> >>
>>> >> On Mon, Jul 7, 2014 at 5:17 PM, Tom Graves <tgraves_cs@yahoo.com.invalid
>>> >
>>> >> wrote:
>>> >>
>>> >>> +1. Ran some Spark on yarn jobs on a hadoop 2.4 cluster with
>>> >>> authentication on.
>>> >>>
>>> >>> Tom
>>> >>>
>>> >>>
>>> >>> On Friday, July 4, 2014 2:39 PM, Patrick Wendell <pwendell@gmail.com>
>>> >>> wrote:
>>> >>>
>>> >>>
>>> >>>
>>> >>> Please vote on releasing the following candidate as Apache Spark
>>> version
>>> >>> 1.0.1!
>>> >>>
>>> >>> The tag to be voted on is v1.0.1-rc1 (commit 7d1043c):
>>> >>>
>>> >>>
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=7d1043c99303b87aef8ee19873629c2bfba4cc78
>>> >>>
>>> >>> The release files, including signatures, digests, etc. can be found at:
>>> >>> http://people.apache.org/~pwendell/spark-1.0.1-rc2/
>>> >>>
>>> >>> Release artifacts are signed with the following key:
>>> >>> https://people.apache.org/keys/committer/pwendell.asc
>>> >>>
>>> >>> The staging repository for this release can be found at:
>>> >>>
>>> https://repository.apache.org/content/repositories/orgapachespark-1021/
>>> >>>
>>> >>> The documentation corresponding to this release can be found at:
>>> >>> http://people.apache.org/~pwendell/spark-1.0.1-rc2-docs/
>>> >>>
>>> >>> Please vote on releasing this package as Apache Spark 1.0.1!
>>> >>>
>>> >>> The vote is open until Monday, July 07, at 20:45 UTC and passes if
>>> >>> a majority of at least 3 +1 PMC votes are cast.
>>> >>>
>>> >>> [ ] +1 Release this package as Apache Spark 1.0.1
>>> >>> [ ] -1 Do not release this package because ...
>>> >>>
>>> >>> To learn more about Apache Spark, please see
>>> >>> http://spark.apache.org/
>>> >>>
>>> >>> === Differences from RC1 ===
>>> >>> This release includes only one "blocking" patch from rc1:
>>> >>> https://github.com/apache/spark/pull/1255
>>> >>>
>>> >>> There are also smaller fixes which came in over the last week.
>>> >>>
>>> >>> === About this release ===
>>> >>> This release fixes a few high-priority bugs in 1.0 and has a variety
>>> >>> of smaller fixes. The full list is here: http://s.apache.org/b45. Some
>>> >>> of the more visible patches are:
>>> >>>
>>> >>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>>> >>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>>> size.
>>> >>> SPARK-1790: Support r3 instance types on EC2.
>>> >>>
>>> >>> This is the first maintenance release on the 1.0 line. We plan to make
>>> >>> additional maintenance releases as new fixes come in.
>>> >>>
>>>
>>>

From dev-return-8307-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 16:48:35 2014
Return-Path: <dev-return-8307-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3747611A3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 16:48:35 +0000 (UTC)
Received: (qmail 8348 invoked by uid 500); 11 Jul 2014 16:48:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8290 invoked by uid 500); 11 Jul 2014 16:48:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8262 invoked by uid 99); 11 Jul 2014 16:48:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:48:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 16:48:31 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so1456888oag.21
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 09:48:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=VTUz9MutPAlmcFuvDjezFHHgDgkEssg0EYswWUFCtBQ=;
        b=NZgMIJeLDnmscdLQpNPj8k+TmFtCyJULasxqe5zjPPlQXSzDB32Y9jiDJiPACEqiNo
         MwOJLvI9rjPgQsSPBrp+pW2esLtTiXiwxEA13oEYajz9cKfj0fXeuXz5yn+i6e4xDJSj
         k5PlRokc8t1rFtuSr8rPZ1OX+oR/bIB6EpCAQsplztrCN1wyDbR+VJBUIn2+QK/J9Wi8
         ZEK2RFxlKatm1csiX9G84nOinl554EFPjU4F7lEzOZ4YZbZH491pqTARm5Wynaz7sECq
         I+hXwFtgu64wTksLY1igxSTMPijs1RkB1vccMbDe/lew9XiYo5PEZW/NqEzcWNYVeRpW
         9jZg==
MIME-Version: 1.0
X-Received: by 10.60.132.171 with SMTP id ov11mr113317oeb.46.1405097285979;
 Fri, 11 Jul 2014 09:48:05 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 11 Jul 2014 09:48:05 -0700 (PDT)
Date: Fri, 11 Jul 2014 09:48:05 -0700
Message-ID: <CABPQxss1eQQZO0RDo0L3GYS1N0nZwG4YmdDTo_SuFi9_77GK0A@mail.gmail.com>
Subject: [RESULT] [VOTE] Release Apache Spark 1.0.1 (RC2)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

This vote has passed with 9 +1 votes (5 binding) and 1 -1 vote (0 binding).

+1:
Patrick Wendell*
Mark Hamstra*
DB Tsai
Krishna Sankar
Soren Macbeth
Andrew Or
Matei Zaharia*
Xiangrui Meng*
Tom Graves*

0:

-1:
Gary Malouf

From dev-return-8308-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 17:29:45 2014
Return-Path: <dev-return-8308-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9289111BC0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 17:29:45 +0000 (UTC)
Received: (qmail 99974 invoked by uid 500); 11 Jul 2014 17:29:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99915 invoked by uid 500); 11 Jul 2014 17:29:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99903 invoked by uid 99); 11 Jul 2014 17:29:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 17:29:44 +0000
X-ASF-Spam-Status: No, hits=2.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 17:29:41 +0000
Received: by mail-pd0-f170.google.com with SMTP id z10so1744813pdj.29
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 10:29:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=/Q0+7RncwuDzvUY2ymweUNF1/uMwNes+8NvPcrv2o+g=;
        b=DGtACRGTUzENm1J+kz9qWKBE0G7IORTKzJCK5HQsw+PbBoGtqildU2PvgD4lRbO9oK
         Zlx2lAWJc1cO9h3X+TfJb+4ZQI7b9K9YUFiToe0ir7RTEVu9g/bJZfabVbZ/oeyCMGUD
         d8hJLLSccO1EuhsCPtrtCM/OTFlvktNyYhFXvyWbMy/jhwIbH6XDOzcxlBSexFzoEImf
         3w62K6g4OtyaXjMOrZVh9gYLEmWUfxuGx4+pgoHM8u8axuy8c1D2RgBGL77jLvKp8ddm
         SvcpZsV+oX6c6Qx0yVJ4f5vbLuY6+AfYK7Xw0IsSqrAnh9F7biS8Y3ODLJyxe5gwj6Ol
         WMbQ==
X-Gm-Message-State: ALoCoQm8zg5DPAUv7sLKO4kLZsi8PhLOfYoJa7PXax2M7oFpQ514mZKaljYxZJMjIpbYW12GTxH0
MIME-Version: 1.0
X-Received: by 10.68.68.196 with SMTP id y4mr348416pbt.60.1405099755630; Fri,
 11 Jul 2014 10:29:15 -0700 (PDT)
Received: by 10.70.5.227 with HTTP; Fri, 11 Jul 2014 10:29:15 -0700 (PDT)
In-Reply-To: <CAMrx5DyJLCaQ6EsFpHOSGLVpYS48Cp4_ho+438GQ-sA+qZKvsA@mail.gmail.com>
References: <CAMrx5DyJLCaQ6EsFpHOSGLVpYS48Cp4_ho+438GQ-sA+qZKvsA@mail.gmail.com>
Date: Fri, 11 Jul 2014 10:29:15 -0700
Message-ID: <CAMJOb8nfaBYv=8avPLTjqaqudSu6XJiFeCDXgRQVm+xp3F6w=A@mail.gmail.com>
Subject: Re: How pySpark works?
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11380e24cf0dae04fdee47c2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11380e24cf0dae04fdee47c2
Content-Type: text/plain; charset=UTF-8

Hi Egor,

Here are a few answers to your questions:

1) Python needs to be installed on all machines, but not pyspark. The way
the executors get the pyspark code depends on which cluster manager you
use. In standalone mode, your executors need to have the actual python
files in their working directory. In yarn mode, python files are included
in the assembly jar, which is then shipped to your executor containers
through a distributed cache.

2) Pyspark is just a thin wrapper around Spark. When you write a closure in
python, it is shipped to the executors within the task itself the same way
scala closures are shipped. If you use a special library, then all of the
nodes will need to have that library pre-installed.

3) Are you trying to run your c++ code inside the "map" function? If so,
you need to make sure the compiled code is present in the working directory
on all the executors before-hand for python to "exec" it. I haven't done
this before, but maybe there are a few gotchas in doing this.

Maybe others can add more information?

Andrew


2014-07-11 5:50 GMT-07:00 Egor Pahomov <pahomov.egor@gmail.com>:

> Hi, I want to use pySpark, but can't understand how it works. Documentation
> doesn't provide enough information.
>
> 1) How python shipped to cluster? Should machines in cluster already have
> python?
> 2) What happens when I write some python code in "map" function - is it
> shipped to cluster and just executed on it? How it understand all
> dependencies, which my code need and ship it there? If I use Math in my
> code in "map" does it mean, that I would ship Math class or some python
> Math on cluster would be used?
> 3) I have c++ compiled code. Can I ship this executable with "addPyFile"
> and just use "exec" function from python? Would it work?
>
> --
>
>
>
> *Sincerely yoursEgor PakhomovScala Developer, Yandex*
>

--001a11380e24cf0dae04fdee47c2--

From dev-return-8309-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 17:53:12 2014
Return-Path: <dev-return-8309-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B71F511CDB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 17:53:12 +0000 (UTC)
Received: (qmail 65355 invoked by uid 500); 11 Jul 2014 17:53:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65292 invoked by uid 500); 11 Jul 2014 17:53:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65279 invoked by uid 99); 11 Jul 2014 17:53:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 17:53:11 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 17:53:09 +0000
Received: by mail-qa0-f47.google.com with SMTP id i13so1164912qae.6
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 10:52:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Ibi5bbPnvjdqlkeaqZykm+gbg7OuR7bRL56iHJAwwlA=;
        b=hlbGPH/dM/6wZLrZb+S1b3vmF661dHNHz/Hbmlq+rLsoS6P8RNrzT8udnSO6rmsdcx
         udhEb9sRlDDYyxu6Ax7XUGqkkJPjo6EFFpH0EVxXdkr9zgtymK9IqyceZhs8eUnSVLA5
         x7wkFbtzj/4E9dmqTYgwxXVajVfI2Z4OAJqdpBVDiZSYbQYMdwOzGm0Zdl/cgVc6/Ck2
         s3qt6mBvNyANOIgXsqzvDqXEj88FMNROAr/ctVI1dIcS/BC341gmhc4s2oz8lw5OaC7V
         K47BLu328vcipOOUxtctsNX8yNL2jg/631x0hr1cLVelDWGwGuPEM8I3LWfLRIUG7GPq
         eC5Q==
X-Gm-Message-State: ALoCoQnCwUqII56zUj91ZQnGEHJ15UkxVr0arHvTVGf/erTFuKgWVuR3vn8nnXUzw9UOzQrEHuUb
X-Received: by 10.224.115.3 with SMTP id g3mr622394qaq.9.1405101164571; Fri,
 11 Jul 2014 10:52:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Fri, 11 Jul 2014 10:52:23 -0700 (PDT)
In-Reply-To: <CAMJOb8nfaBYv=8avPLTjqaqudSu6XJiFeCDXgRQVm+xp3F6w=A@mail.gmail.com>
References: <CAMrx5DyJLCaQ6EsFpHOSGLVpYS48Cp4_ho+438GQ-sA+qZKvsA@mail.gmail.com>
 <CAMJOb8nfaBYv=8avPLTjqaqudSu6XJiFeCDXgRQVm+xp3F6w=A@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 11 Jul 2014 10:52:23 -0700
Message-ID: <CAPh_B=YKDJD4qouOxVuZaEDXGa=zFLOdb8=baio6P0vNHgWHhA@mail.gmail.com>
Subject: Re: How pySpark works?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc96b0c9c72f04fdee9b0d
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc96b0c9c72f04fdee9b0d
Content-Type: text/plain; charset=UTF-8

Also take a look at this:
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals


On Fri, Jul 11, 2014 at 10:29 AM, Andrew Or <andrew@databricks.com> wrote:

> Hi Egor,
>
> Here are a few answers to your questions:
>
> 1) Python needs to be installed on all machines, but not pyspark. The way
> the executors get the pyspark code depends on which cluster manager you
> use. In standalone mode, your executors need to have the actual python
> files in their working directory. In yarn mode, python files are included
> in the assembly jar, which is then shipped to your executor containers
> through a distributed cache.
>
> 2) Pyspark is just a thin wrapper around Spark. When you write a closure in
> python, it is shipped to the executors within the task itself the same way
> scala closures are shipped. If you use a special library, then all of the
> nodes will need to have that library pre-installed.
>
> 3) Are you trying to run your c++ code inside the "map" function? If so,
> you need to make sure the compiled code is present in the working directory
> on all the executors before-hand for python to "exec" it. I haven't done
> this before, but maybe there are a few gotchas in doing this.
>
> Maybe others can add more information?
>
> Andrew
>
>
> 2014-07-11 5:50 GMT-07:00 Egor Pahomov <pahomov.egor@gmail.com>:
>
> > Hi, I want to use pySpark, but can't understand how it works.
> Documentation
> > doesn't provide enough information.
> >
> > 1) How python shipped to cluster? Should machines in cluster already have
> > python?
> > 2) What happens when I write some python code in "map" function - is it
> > shipped to cluster and just executed on it? How it understand all
> > dependencies, which my code need and ship it there? If I use Math in my
> > code in "map" does it mean, that I would ship Math class or some python
> > Math on cluster would be used?
> > 3) I have c++ compiled code. Can I ship this executable with "addPyFile"
> > and just use "exec" function from python? Would it work?
> >
> > --
> >
> >
> >
> > *Sincerely yoursEgor PakhomovScala Developer, Yandex*
> >
>

--047d7bdc96b0c9c72f04fdee9b0d--

From dev-return-8310-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 11 22:40:34 2014
Return-Path: <dev-return-8310-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 80D6D11844
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 11 Jul 2014 22:40:34 +0000 (UTC)
Received: (qmail 49893 invoked by uid 500); 11 Jul 2014 22:40:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49838 invoked by uid 500); 11 Jul 2014 22:40:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49826 invoked by uid 99); 11 Jul 2014 22:40:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 22:40:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kanzhangemail@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 11 Jul 2014 22:40:29 +0000
Received: by mail-ig0-f182.google.com with SMTP id c1so312328igq.15
        for <dev@spark.apache.org>; Fri, 11 Jul 2014 15:40:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:reply-to:sender:in-reply-to:references:date:message-id
         :subject:from:to:content-type;
        bh=KBJoaipXkLk4wLN5Ywd5EUHvWD57hqwmzjuwSmKQb1E=;
        b=pm2CK5Nicidl86FXz24ituiEHQrUd2638vxrXO2u5K3YQvKAar74X4cll30W6R3nqh
         p+MmBONUWauuJwyQyfYPRJ4fjlAcWMlzmnD5Z4oL3MUJk9bBA7DqFO47k39hP/6OPfIq
         Hw3aaKACkmYP//itxa7Q/vZ0YaPFnHLTUaAT4+ht4UsnhXhHdm3prSfhBu88gYjV2dCZ
         Z2CVOFIFGg1s6xdV0oSjOeybL5pOgxfeYiH7ZfB1t29J3Qm3xPZ69mVVI1/nzH3WsamH
         /8o+5Q6X+4naep7+l95LJM2RIEuBYivetQe81zJsCYuPbkk8MZWLmNWYsWRucaTta6kr
         GDwg==
MIME-Version: 1.0
X-Received: by 10.42.84.141 with SMTP id m13mr5396063icl.38.1405118408476;
 Fri, 11 Jul 2014 15:40:08 -0700 (PDT)
Reply-To: kzhang@apache.org
Sender: kanzhangemail@gmail.com
Received: by 10.64.70.130 with HTTP; Fri, 11 Jul 2014 15:40:08 -0700 (PDT)
In-Reply-To: <CAHmnFJ7zYO6Mk1+RodhfXR8xO-=mimsbd2WzHV2ULrZuJG_FNw@mail.gmail.com>
References: <CAHmnFJ7zYO6Mk1+RodhfXR8xO-=mimsbd2WzHV2ULrZuJG_FNw@mail.gmail.com>
Date: Fri, 11 Jul 2014 15:40:08 -0700
X-Google-Sender-Auth: W_j-MJo7IsbPVo9Iqadt7IZZAQc
Message-ID: <CALRHqP9RfYA=q5NqXunOAkVyv4o=WD5PofFBJ7jwJZXrzc9E8A@mail.gmail.com>
Subject: Re: Calling Scala/Java methods which operates on RDD
From: Kan Zhang <kzhang@apache.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf30334ee99acbda04fdf29f6b
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30334ee99acbda04fdf29f6b
Content-Type: text/plain; charset=UTF-8

Hi Jai,

Your suspicion is correct. In general, Python RDDs are pickled into byte
arrays and stored in Java land as RDDs of byte arrays. union/zip operates
on byte arrays directly without deserializing. Currently, Python byte
arrays only get unpickled into Java objects in special cases, like SQL
functions or saving to Sequence Files (upcoming).

Hope it helps.

Kan


On Fri, Jul 11, 2014 at 5:04 AM, Jai Kumar Singh <flukebox@flukebox.in>
wrote:

> HI,
>   I want to write some common utility function in Scala and want to call
> the same from Java/Python Spark API ( may be add some wrapper code around
> scala calls). Calling Scala functions from Java works fine. I was reading
> pyspark rdd code and find out that pyspark is able to call JavaRDD function
> like union/zip to get same for pyspark RDD and deserializing the output and
> everything works fine. But somehow I am
> not able to work out really simple example. I think I am missing some
> serialization/deserialization.
>
> Can someone confirm that is it even possible to do so? Or, would it be much
> easier to pass RDD data files around instead of RDD directly (from pyspark
> to java/scala)?
>
> For example, below code just add 1 to each element of RDD containing
> Integers.
>
> package flukebox.test;
>
> object TestClass{
>
> def testFunc(data:RDD[Int])={
>
>   data.map(x => x+1)
>
> }
>
> }
>
> Calling from python,
>
> from pyspark import RDD
>
> from py4j.java_gateway import java_import
>
> java_import(sc._gateway.jvm, "flukebox.test")
>
>
> data = sc.parallelize([1,2,3,4,5,6,7,8,9])
>
> sc._jvm.flukebox.test.TestClass.testFunc(data._jrdd.rdd())
>
>
> *This fails because testFunc get any RDD of type Byte Array.*
>
>
> Any help/pointer would be highly appreciated.
>
>
> Thanks & Regards,
>
> Jai K Singh
>

--20cf30334ee99acbda04fdf29f6b--

From dev-return-8311-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 12 01:35:47 2014
Return-Path: <dev-return-8311-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 04B9911C65
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 12 Jul 2014 01:35:47 +0000 (UTC)
Received: (qmail 96713 invoked by uid 500); 12 Jul 2014 01:35:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96655 invoked by uid 500); 12 Jul 2014 01:35:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95566 invoked by uid 99); 12 Jul 2014 01:35:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 01:35:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.179 as permitted sender)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 01:35:39 +0000
Received: by mail-ob0-f179.google.com with SMTP id wn1so62930obc.38
        for <multiple recipients>; Fri, 11 Jul 2014 18:35:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=/y7C+YWIoU0mP6EG0vBcKEVco7R1x82bPF1qnayN4m0=;
        b=nOs9QSSqAtC+zMpCCa8nYqUj5/rUdikZyr5TsjnQIimOz1SfmREhx2nzmUZCxhYI4N
         Q3YmUqT+LUgXw/2PRk50dcxZe866V5NuARkuAj4up3+Z1iVAdaNckTk7FpRaHCzIq4HK
         YWH+35yxTuko5T+bnED/hwOalpEbKaHbif5SKVsDgdeU4aI4L/PN4Jjsog/wUVG7q27I
         M7GeahpcEPIuDOiZUFOKMLafWzRoT2PkyFQ7DAf36HAtdP7ZomwEKRErLfmVcbyub112
         /Zfa9CsESsVDbyElpJVwK2CXsM76MbScefSY4hjh0BsX8iPRZJF9a/rfADffytU6Ko6d
         TZTg==
MIME-Version: 1.0
X-Received: by 10.60.97.66 with SMTP id dy2mr3307883oeb.50.1405128918546; Fri,
 11 Jul 2014 18:35:18 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 11 Jul 2014 18:35:18 -0700 (PDT)
Date: Fri, 11 Jul 2014 18:35:18 -0700
Message-ID: <CABPQxsu80FB-3YcGrgyXBPge-smHiMMRgpt2LomM8Pf=yzBa3Q@mail.gmail.com>
Subject: Announcing Spark 1.0.1
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, user@spark.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I am happy to announce the availability of Spark 1.0.1! This release
includes contributions from 70 developers. Spark 1.0.0 includes fixes
across several areas of Spark, including the core API, PySpark, and
MLlib. It also includes new features in Spark's (alpha) SQL library,
including support for JSON data and performance and stability fixes.

Visit the release notes[1] to read about this release or download[2]
the release today.

[1] http://spark.apache.org/releases/spark-release-1-0-1.html
[2] http://spark.apache.org/downloads.html

From dev-return-8312-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 12 03:44:45 2014
Return-Path: <dev-return-8312-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7A25411E87
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 12 Jul 2014 03:44:45 +0000 (UTC)
Received: (qmail 14783 invoked by uid 500); 12 Jul 2014 03:44:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14724 invoked by uid 500); 12 Jul 2014 03:44:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13605 invoked by uid 99); 12 Jul 2014 03:44:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 03:44:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 03:44:39 +0000
Received: by mail-we0-f179.google.com with SMTP id p10so1578634wes.38
        for <multiple recipients>; Fri, 11 Jul 2014 20:44:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=brWueYmsSOsh7hxtiaKsZNY1mb0vwTSH/zFPXNrK1I8=;
        b=tMiDwmhOXVZ6ZwH8G53CECz0C49WpKYpUNyChSorwJpMn6btIu5lAURDa3CnpOV2vX
         D3z4U+M/PtSG+48V0dcprRJ9BPG63biOa3ZVV+i6kMPcd1DqQ0FZyZgSiWHU8OrKYQ9d
         k+FnUmE6n2HyccwyNs9gHvHAh8DyKiGlb6I1o0VdHx2glyM9JuhqgPaf2whV13cFMn7V
         LhMgHnbAU3VJ8Qn84ILn/SNDQvV0YCKncwcwa0d2Y7Gr5HabTZSPa+OLmCf28OK1SdwA
         TY2AHk2UCtTDFHrxGj2Az7DZKuv6T/qtqDP6BGndiHD2QPFcS7O9iTiEs354qju/2I3c
         gYww==
MIME-Version: 1.0
X-Received: by 10.180.73.106 with SMTP id k10mr9764979wiv.11.1405136655497;
 Fri, 11 Jul 2014 20:44:15 -0700 (PDT)
Received: by 10.217.119.72 with HTTP; Fri, 11 Jul 2014 20:44:15 -0700 (PDT)
In-Reply-To: <CABPQxsu80FB-3YcGrgyXBPge-smHiMMRgpt2LomM8Pf=yzBa3Q@mail.gmail.com>
References: <CABPQxsu80FB-3YcGrgyXBPge-smHiMMRgpt2LomM8Pf=yzBa3Q@mail.gmail.com>
Date: Fri, 11 Jul 2014 20:44:15 -0700
Message-ID: <CALuGr6Zp-HPm_j+_nrcKsqqSgNA4CQW0MTXdSbEekHdfoNg7Yg@mail.gmail.com>
Subject: Re: Announcing Spark 1.0.1
From: Henry Saputra <henry.saputra@gmail.com>
To: "user@spark.apache.org" <user@spark.apache.org>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043749c736408f04fdf6dfbd
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043749c736408f04fdf6dfbd
Content-Type: text/plain; charset=UTF-8

Congrats to the Spark community !

On Friday, July 11, 2014, Patrick Wendell <pwendell@gmail.com> wrote:

> I am happy to announce the availability of Spark 1.0.1! This release
> includes contributions from 70 developers. Spark 1.0.0 includes fixes
> across several areas of Spark, including the core API, PySpark, and
> MLlib. It also includes new features in Spark's (alpha) SQL library,
> including support for JSON data and performance and stability fixes.
>
> Visit the release notes[1] to read about this release or download[2]
> the release today.
>
> [1] http://spark.apache.org/releases/spark-release-1-0-1.html
> [2] http://spark.apache.org/downloads.html
>

--f46d043749c736408f04fdf6dfbd--

From dev-return-8313-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 12 14:18:04 2014
Return-Path: <dev-return-8313-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2D3281184B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 12 Jul 2014 14:18:04 +0000 (UTC)
Received: (qmail 84051 invoked by uid 500); 12 Jul 2014 14:18:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84012 invoked by uid 500); 12 Jul 2014 14:18:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83999 invoked by uid 99); 12 Jul 2014 14:18:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 14:18:02 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [49.212.34.109] (HELO oss.nttdata.co.jp) (49.212.34.109)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 14:17:59 +0000
Received: from Kousuke-no-MacBook-Pro.local (173-164-244-209-SFBA.hfc.comcastbusiness.net [173.164.244.209])
	by oss.nttdata.co.jp (Postfix) with ESMTP id BCBFE17EE07
	for <dev@spark.apache.org>; Sat, 12 Jul 2014 23:17:22 +0900 (JST)
Message-ID: <53C14371.1050909@oss.nttdata.co.jp>
Date: Sat, 12 Jul 2014 07:17:21 -0700
From: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Miss-link for the document of Spark 1.0.1
Content-Type: text/plain; charset=ISO-2022-JP
Content-Transfer-Encoding: 7bit
X-Virus-Scanned: clamav-milter 0.98.4 at oss.nttdata.co.jp
X-Virus-Status: Clean
X-Spam-Checker-Version: SpamAssassin 3.2.5 (2008-06-10) on oss.nttdata.co.jp
X-Virus-Checked: Checked by ClamAV on apache.org
X-Old-Spam-Status: No, score=-99.7 required=13.0 tests=CONTENT_TYPE_PRESENT,
	ISO2022JP_CHARSET,UNPARSEABLE_RELAY,USER_IN_WHITELIST autolearn=failed
	version=3.2.5

Hi developpers

Conguraturations on the release of Spark 1.0.1!

I found a miss-link for the document of Spark 1.0.1.

I checked out the overview from the link
http://spark.apache.org/docs/latest/ but
this link points to the page for 1.0.0.

I also visited the link http://spark.apache.org/docs/latest/index.html
(added "index.html") and I could accessed the documet for 1.0.1.

Best Regards,
Kousuke



From dev-return-8314-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 12 14:22:31 2014
Return-Path: <dev-return-8314-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA83C1186A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 12 Jul 2014 14:22:31 +0000 (UTC)
Received: (qmail 88902 invoked by uid 500); 12 Jul 2014 14:22:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88842 invoked by uid 500); 12 Jul 2014 14:22:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88831 invoked by uid 99); 12 Jul 2014 14:22:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 14:22:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 14:22:26 +0000
Received: by mail-vc0-f180.google.com with SMTP id im17so4182229vcb.25
        for <dev@spark.apache.org>; Sat, 12 Jul 2014 07:22:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=t1yadGoGJTozaRXZcsHRvBeh6GwZA9OKTGgrjdUokdw=;
        b=JzXQvW7p4Hlwd3woKhVes6Ey7KXyF1yW2n855NQ9t1YrKgBvmleTzK3i5CuR8OmzAy
         ayBZ4KiSmlxK2ab+5di8WxiEdmII3BEgjgA9hc5S64Dsds4gFwLpbT2toP4q4nZl1I/t
         Ns4h/8P9RAu0OygPbu6baA28ncwUIGNbnRTDb9nK3iBRowVAq4CGfXw4TVRdlLDrCx4Q
         k3AsjZ9q4Sm95cs2sP37pWuBZDbpIGcyFJop6KUYFnaXlJxhmFxyjRoYFovOoG40u2U6
         oirMnvmceU3YE9d1q3bWCcS4vgY1yUmQAKYK4bLvkxUh6wQL20rys1VYh6Tal2pxyN0K
         InVw==
X-Gm-Message-State: ALoCoQm6yTzShF/YV6M0/r7Y17cmq2Sd1K6++hnAWwlDQ+WIIrJ5sXS27ouF2Y54Rs/RuMkANlOu
MIME-Version: 1.0
X-Received: by 10.221.20.133 with SMTP id qo5mr26991vcb.53.1405174925597; Sat,
 12 Jul 2014 07:22:05 -0700 (PDT)
Received: by 10.58.247.97 with HTTP; Sat, 12 Jul 2014 07:22:05 -0700 (PDT)
Received: by 10.58.247.97 with HTTP; Sat, 12 Jul 2014 07:22:05 -0700 (PDT)
In-Reply-To: <53C14371.1050909@oss.nttdata.co.jp>
References: <53C14371.1050909@oss.nttdata.co.jp>
Date: Sat, 12 Jul 2014 15:22:05 +0100
Message-ID: <CAMAsSdKopkXKb2xDheWTvyQDsOyCrc-XZY8_RkPDq2h1L0ef+w@mail.gmail.com>
Subject: Re: Miss-link for the document of Spark 1.0.1
From: Sean Owen <sowen@cloudera.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133812c49bb9c04fdffc8d6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133812c49bb9c04fdffc8d6
Content-Type: text/plain; charset=UTF-8

It correctly points to 1.0.1 but you may need to refresh in your browser to
see the update.
On Jul 12, 2014 3:18 PM, "Kousuke Saruta" <sarutak@oss.nttdata.co.jp> wrote:

> Hi developpers
>
> Conguraturations on the release of Spark 1.0.1!
>
> I found a miss-link for the document of Spark 1.0.1.
>
> I checked out the overview from the link
> http://spark.apache.org/docs/latest/ but
> this link points to the page for 1.0.0.
>
> I also visited the link http://spark.apache.org/docs/latest/index.html
> (added "index.html") and I could accessed the documet for 1.0.1.
>
> Best Regards,
> Kousuke
>
>
>

--001a1133812c49bb9c04fdffc8d6--

From dev-return-8315-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 12 14:25:04 2014
Return-Path: <dev-return-8315-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB82511877
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 12 Jul 2014 14:25:04 +0000 (UTC)
Received: (qmail 91316 invoked by uid 500); 12 Jul 2014 14:25:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91249 invoked by uid 500); 12 Jul 2014 14:25:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91238 invoked by uid 99); 12 Jul 2014 14:25:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 14:25:04 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [49.212.34.109] (HELO oss.nttdata.co.jp) (49.212.34.109)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 12 Jul 2014 14:24:58 +0000
Received: from Kousuke-no-MacBook-Pro.local (173-164-244-209-SFBA.hfc.comcastbusiness.net [173.164.244.209])
	by oss.nttdata.co.jp (Postfix) with ESMTP id A736717EE07
	for <dev@spark.apache.org>; Sat, 12 Jul 2014 23:24:28 +0900 (JST)
Message-ID: <53C1451B.1050006@oss.nttdata.co.jp>
Date: Sat, 12 Jul 2014 07:24:27 -0700
From: Kousuke Saruta <sarutak@oss.nttdata.co.jp>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: Miss-link for the document of Spark 1.0.1
References: <53C14371.1050909@oss.nttdata.co.jp> <CAMAsSdKopkXKb2xDheWTvyQDsOyCrc-XZY8_RkPDq2h1L0ef+w@mail.gmail.com>
In-Reply-To: <CAMAsSdKopkXKb2xDheWTvyQDsOyCrc-XZY8_RkPDq2h1L0ef+w@mail.gmail.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Scanned: clamav-milter 0.98.4 at oss.nttdata.co.jp
X-Virus-Status: Clean
X-Spam-Checker-Version: SpamAssassin 3.2.5 (2008-06-10) on oss.nttdata.co.jp
X-Virus-Checked: Checked by ClamAV on apache.org
X-Old-Spam-Status: No, score=-99.6 required=13.0 tests=CONTENT_TYPE_PRESENT,
	UNPARSEABLE_RELAY,USER_IN_WHITELIST autolearn=failed version=3.2.5

Sorry, it may be my bad.

Now I confirmed the link points to the page for 1.0.1.

Thanks
Kousuke

(2014/07/12 7:22), Sean Owen wrote:
> It correctly points to 1.0.1 but you may need to refresh in your browser to
> see the update.
> On Jul 12, 2014 3:18 PM, "Kousuke Saruta" <sarutak@oss.nttdata.co.jp> wrote:
>
>> Hi developpers
>>
>> Conguraturations on the release of Spark 1.0.1!
>>
>> I found a miss-link for the document of Spark 1.0.1.
>>
>> I checked out the overview from the link
>> http://spark.apache.org/docs/latest/ but
>> this link points to the page for 1.0.0.
>>
>> I also visited the link http://spark.apache.org/docs/latest/index.html
>> (added "index.html") and I could accessed the documet for 1.0.1.
>>
>> Best Regards,
>> Kousuke
>>
>>
>>


From dev-return-8316-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 13 02:03:59 2014
Return-Path: <dev-return-8316-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 45AEE114EA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 13 Jul 2014 02:03:59 +0000 (UTC)
Received: (qmail 19308 invoked by uid 500); 13 Jul 2014 02:03:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19252 invoked by uid 500); 13 Jul 2014 02:03:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19238 invoked by uid 99); 13 Jul 2014 02:03:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 02:03:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 02:03:55 +0000
Received: by mail-wg0-f51.google.com with SMTP id b13so552439wgh.10
        for <dev@spark.apache.org>; Sat, 12 Jul 2014 19:03:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=0TH8tBKCO0bNJPXHfFiSdGSdf/he4z8W5nDx012QqnQ=;
        b=ZQE5CqXT6qnh4awF38ABE69hQ/DcptWN7rpCKNtmdD+8AHsu5oRJ8h4d/UT06W5/PP
         ikCOd4dB3p0LN3NUey8nMnjMtrbEyctOzogGsVx2Fd8JCBJskJE7OT0QK8i+Ayh64jaV
         FzSN3jt9Ai24Xegc4mnWU1qqPZbvt0KkgaErz7kCvmpxIUbZyMeC+IKLgblwSIb76rw6
         W+xy2rC4FuZplLarJeTZFhhkR0GP4JIRhfYqBE3BeM2GCemyvhivcT87GHogUwqOvDxO
         RS3ktftBovfdyEeKLRjbS5TPZs0jeUcnRLow62RUmpLViHTiMh5Kwxk/J5tlU6acSltO
         DUXg==
X-Received: by 10.180.208.13 with SMTP id ma13mr15413083wic.45.1405217011782;
 Sat, 12 Jul 2014 19:03:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.24.168 with HTTP; Sat, 12 Jul 2014 19:02:51 -0700 (PDT)
In-Reply-To: <057601cf9c9c$7e798c80$7b6ca580$@reactor8.com>
References: <CAOhmDzch1e=zOaqhi7VNNX+doGuNSi3PQCBSHZKXFRW8E9B+uA@mail.gmail.com>
 <057601cf9c9c$7e798c80$7b6ca580$@reactor8.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sat, 12 Jul 2014 22:02:51 -0400
Message-ID: <CAOhmDzceR8E+ij6R185Q0tOqWRiMgtEQ_saO1H3Ne2SW8tXZOg@mail.gmail.com>
Subject: Re: EC2 clusters ready in launch time + 30 seconds
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c38d8ad1d8a304fe099462
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c38d8ad1d8a304fe099462
Content-Type: text/plain; charset=UTF-8

On Thu, Jul 10, 2014 at 8:10 PM, Nate D'Amico <nate@reactor8.com> wrote:

> Starting to work through some automation/config stuff for spark stack on
> EC2 with a project, will be focusing the work through the apache bigtop
> effort to start, can then share with spark community directly as things
> progress if people are interested


Let us know how that goes. I'm definitely interested in hearing more.

Nick

--001a11c38d8ad1d8a304fe099462--

From dev-return-8317-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 13 03:00:12 2014
Return-Path: <dev-return-8317-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B8679115D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 13 Jul 2014 03:00:12 +0000 (UTC)
Received: (qmail 54949 invoked by uid 500); 13 Jul 2014 03:00:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54879 invoked by uid 500); 13 Jul 2014 03:00:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54867 invoked by uid 99); 13 Jul 2014 03:00:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 03:00:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ianoconnell@gmail.com designates 209.85.223.173 as permitted sender)
Received: from [209.85.223.173] (HELO mail-ie0-f173.google.com) (209.85.223.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 03:00:07 +0000
Received: by mail-ie0-f173.google.com with SMTP id tr6so2202645ieb.4
        for <dev@spark.apache.org>; Sat, 12 Jul 2014 19:59:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:from:date:message-id
         :subject:to:content-type;
        bh=daBmFbgaFuUQ5kUD7g05T7CQFlxD+RWBqCWu9h1sMHM=;
        b=qI4NBsoY1QxrNPDMO2jeXVv8GTiAvtASPhwoDAJazCjQHZhPrNSWjP8neQEsSXFW2W
         gfpc8QS28bJxYZwhoYsWxhndjAR4KVtqR8enhKegWE6eHAVNtbk5pWax++wJM/WOGyPy
         xYntHSMcQ5qTfJ/+vzxThIhaMZWs3CFHx3o9TroKARBhZPZfOqpPjfuN8Q9jPvKuMv+Z
         fDurELajU12Q86riCRB4wWuo7nnZuBG2lG0+NjD+ZkHDRP1Gy9XysseGsWumvWMkgERh
         Dr1SQa2p50iIjg5L09nUWBT0FZizgwO3ZTIMxXkwsF5ea8yXqoanfI8EaaSPWmIOVFYv
         kVIw==
X-Received: by 10.50.12.105 with SMTP id x9mr15229912igb.10.1405220387141;
 Sat, 12 Jul 2014 19:59:47 -0700 (PDT)
MIME-Version: 1.0
Sender: ianoconnell@gmail.com
Received: by 10.64.223.135 with HTTP; Sat, 12 Jul 2014 19:59:27 -0700 (PDT)
In-Reply-To: <CAAswR-7MEswRBLgAqU0fJQ2rJkKqJ9U82R9HVYD6Hw69ve2POw@mail.gmail.com>
References: <CAMDxJTEy8-2o=MW6Z96sWMZOZnDDJS79bmd05Bo1WK8aMzvXig@mail.gmail.com>
 <CAAswR-7MEswRBLgAqU0fJQ2rJkKqJ9U82R9HVYD6Hw69ve2POw@mail.gmail.com>
From: "Ian O'Connell" <ian@ianoconnell.com>
Date: Sat, 12 Jul 2014 19:59:27 -0700
X-Google-Sender-Auth: 3SYglpqRdC_-pnJwwj9BHccMaAA
Message-ID: <CAMDxJTG+2X5GPD-9kAXyGtu3_ucXGk2wu-NNgT6T-R1-TpL_xg@mail.gmail.com>
Subject: Re: sparkSQL thread safe?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0118363601be2404fe0a5e94
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0118363601be2404fe0a5e94
Content-Type: text/plain; charset=UTF-8

Thanks for the response Michael

On the first i'm following the JIRA now thanks, not blocker for me but
would be great to see.

I opened up a PR with the resource pool usage around it. I didn't include
it in the PR, but a few classes we should probably add as registered in
kryo for good perf/size:
    classOf[org.apache.spark.sql.catalyst.expressions.GenericRow],
    classOf[org.apache.spark.sql.catalyst.expressions.GenericMutableRow],
    classOf[org.apache.spark.sql.catalyst.expressions.Row],
    classOf[Array[Object]],
    scala.collection.immutable.Nil.getClass,
    scala.collection.immutable.::.getClass,
    classOf[scala.collection.immutable.::[Any]]

Thanks for adding that distinct btw, great to have it scale more.

On the last, opened the JIRA thanks.

Also more of a sparkCore thing that you might already be aware of, but I
haven't seen mentioned somewhere and was hitting me(Also if any part of
this seems wrong to you I'd love to know):

I was getting out of memory doing a bunch of ops against medium(~1TB
compressed) input sizes with simple things that should spill nicely
(distinct, reduceByKey(_ + _) ).

Anyway what I came back with(copied from an internal email):

I looked through some heap dumps from the OOM's in spark and found there
were >10k instances of DiskBlockObjectWriter's each of which were up to
300kb in size per active executor. At up to 12 concurrent tasks per host is
about 33gb of space topping out. The nodes of course were failing before
this(max mem on our ts cluster per jvm is 25gb).

The memory usage primarily comes from two places, a byte array in
LZFOutputStream and a byte array in BufferedOutputStream. These are both
output buffers along the way to disk(so when we are using the former we can
turn down/disable the latter). These are configured to be 65kb and 100kb
respectively by default. The former is not a configurable option but is
static in that library's code.

These come from the ShuffleBlockWriter, that is we get an input stream with
>10k chunks. When we do operations which require partitioning (say
distinct, reduceByKey, etc..) it maintains the existing partition count. So
each task basically opens >10k files, each file handle of which has these
buffers in place for that task to write to.

Solution put in place(maybe there's a better one?):

Given:
X: The heap size for an executors JVM
Y: The number of threads/cores allowed for concurrent execution per host
Z: The expected overhead of these output streams (currently estimated at
65k + size of the output buffer * 1.1 for overheads)
K: The fraction of memory to allow be used for this overhead (configurable
parameter, default @ 0.2)

Then, the number of partitions: P = (X / Y / Z) * K

Then inside some of our root sources now:
-> After assembling the RDD, if numPartitions > P
-> coalesce to P.
This won't trigger another shuffle phase, so can easily sit inline to
source definitions.

The only real down side of this approach i've seen is that it limits the
number of tasks in this initial map phase which may not be ideal for
parallelism when loading a large dataset and then filtering heavily. It
would be more efficient to pass P into the first distinct/reduceByKey call,
but the user code would have to reference P.





On Thu, Jul 10, 2014 at 4:50 PM, Michael Armbrust <michael@databricks.com>
wrote:

> Hey Ian,
>
> Thanks for bringing these up!  Responses in-line:
>
> Just wondering if right now spark sql is expected to be thread safe on
> > master?
> > doing a simple hadoop file -> RDD -> schema RDD -> write parquet
> > will fail in reflection code if i run these in a thread pool.
> >
>
> You are probably hitting SPARK-2178
> <https://issues.apache.org/jira/browse/SPARK-2178> which is caused by
> SI-6240 <https://issues.scala-lang.org/browse/SI-6240>.  We have a plan to
> fix this by moving the schema introspection to compile time, using macros.
>
>
> > The SparkSqlSerializer, seems to create a new Kryo instance each time it
> > wants to serialize anything. I got a huge speedup when I had any
> > non-primitive type in my SchemaRDD using the ResourcePool's from Chill
> for
> > providing the KryoSerializer to it. (I can open an RB if there is some
> > reason not to re-use them?)
> >
>
> Sounds like SPARK-2102 <https://issues.apache.org/jira/browse/SPARK-2102>.
>  There is no reason AFAIK to not reuse the instance. A PR would be greatly
> appreciated!
>
>
> > With the Distinct Count operator there is no map-side operations, and a
> > test to check for this. Is there any reason not to do a map side combine
> > into a set and then merge the sets later? (similar to the approximate
> > distinct count operator)
> >
>
> Thats just not an optimization that we had implemented yet... but I've just
> done it here <https://github.com/apache/spark/pull/1366> and it'll be in
> master soon :)
>
>
> > Another thing while i'm mailing.. the 1.0.1 docs have a section like:
> > "
> > // Note: Case classes in Scala 2.10 can support only up to 22 fields. To
> > work around this limit, // you can use custom classes that implement the
> > Product interface.
> > "
> >
> > Which sounds great, we have lots of data in thrift.. so via scrooge (
> > https://github.com/twitter/scrooge), we end up with ultimately instances
> > of
> > traits which implement product. Though the reflection code appears to
> look
> > for the constructor of the class and base the types based on those
> > parameters?
>
>
> Yeah, thats true that we only look in the constructor at the moment, but I
> don't think there is a really good reason for that (other than I guess we
> will need to add code to make sure we skip builtin object methods).  If you
> want to open a JIRA, we can try fixing this.
>
> Michael
>

--089e0118363601be2404fe0a5e94--

From dev-return-8318-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 13 06:20:19 2014
Return-Path: <dev-return-8318-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 37EA6117C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 13 Jul 2014 06:20:18 +0000 (UTC)
Received: (qmail 89576 invoked by uid 500); 13 Jul 2014 06:20:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89515 invoked by uid 500); 13 Jul 2014 06:20:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89504 invoked by uid 99); 13 Jul 2014 06:20:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 06:20:16 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 06:20:12 +0000
Received: by mail-qa0-f45.google.com with SMTP id cm18so816433qab.18
        for <dev@spark.apache.org>; Sat, 12 Jul 2014 23:19:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=gJmS3cOTJfSSbmU376hMB+PVGbgnX+IYB2B7M3Fi9AE=;
        b=QFMWoG0UFeHtCU4pKpeGzqopB0iG2rHEHWso8zWE0TpGKv6gL8QJjs0YsaBudV927L
         X1GD0Z8ki3QU6ZiadTk/quZ+A9X/D640sV8cPCK5+GuUSR3x49ygsiw85dd6DI+bmxne
         HrkjSKZ7TOYWL4gVkantBVlE/Pp9zWrn9Y7NMK0E8qDRJHPPVDv69wzOxB6eLGxced3e
         syaG/O2hXMtfY5vrTx2h4LBGH8UTvHc/qEah90VLpkjvm9dWuLNO6qyGu3I3+HfU/t7a
         sZoST/OGlNNPcY85BpPRAtiiazAoDmmra+NwH9xlmUaM7JttHDwWsRIVj2fACh5gEcmj
         cnJw==
X-Gm-Message-State: ALoCoQk2JYoSYhYJgngmtIlGx+TM/e/k3VSpn5d5SxZMobY7Y4JWsZ1aeMlA3tBMxEY8a7bi2eZz
X-Received: by 10.229.252.130 with SMTP id mw2mr13788254qcb.12.1405232391589;
 Sat, 12 Jul 2014 23:19:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sat, 12 Jul 2014 23:19:30 -0700 (PDT)
In-Reply-To: <CAMDxJTG+2X5GPD-9kAXyGtu3_ucXGk2wu-NNgT6T-R1-TpL_xg@mail.gmail.com>
References: <CAMDxJTEy8-2o=MW6Z96sWMZOZnDDJS79bmd05Bo1WK8aMzvXig@mail.gmail.com>
 <CAAswR-7MEswRBLgAqU0fJQ2rJkKqJ9U82R9HVYD6Hw69ve2POw@mail.gmail.com> <CAMDxJTG+2X5GPD-9kAXyGtu3_ucXGk2wu-NNgT6T-R1-TpL_xg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sat, 12 Jul 2014 23:19:30 -0700
Message-ID: <CAPh_B=aPef=8+ddi5T4u5Eocu-p8ws1gQg3Oar6B8SDeRNuAPA@mail.gmail.com>
Subject: Re: sparkSQL thread safe?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e920872a8e04fe0d29c4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e920872a8e04fe0d29c4
Content-Type: text/plain; charset=UTF-8

Ian,

The LZFOutputStream's large byte buffer is sort of annoying. It is much
smaller if you use the Snappy one. The downside of the Snappy one is
slightly less compression (I've seen 10 - 20% larger sizes).

If we can find a compression scheme implementation that doesn't do very
large buffers, that'd be a good idea too ... let me know if you have any
suggestions.

In the future, we plan to make shuffle write to less number of streams at
the same time.



On Sat, Jul 12, 2014 at 7:59 PM, Ian O'Connell <ian@ianoconnell.com> wrote:

> Thanks for the response Michael
>
> On the first i'm following the JIRA now thanks, not blocker for me but
> would be great to see.
>
> I opened up a PR with the resource pool usage around it. I didn't include
> it in the PR, but a few classes we should probably add as registered in
> kryo for good perf/size:
>     classOf[org.apache.spark.sql.catalyst.expressions.GenericRow],
>     classOf[org.apache.spark.sql.catalyst.expressions.GenericMutableRow],
>     classOf[org.apache.spark.sql.catalyst.expressions.Row],
>     classOf[Array[Object]],
>     scala.collection.immutable.Nil.getClass,
>     scala.collection.immutable.::.getClass,
>     classOf[scala.collection.immutable.::[Any]]
>
> Thanks for adding that distinct btw, great to have it scale more.
>
> On the last, opened the JIRA thanks.
>
> Also more of a sparkCore thing that you might already be aware of, but I
> haven't seen mentioned somewhere and was hitting me(Also if any part of
> this seems wrong to you I'd love to know):
>
> I was getting out of memory doing a bunch of ops against medium(~1TB
> compressed) input sizes with simple things that should spill nicely
> (distinct, reduceByKey(_ + _) ).
>
> Anyway what I came back with(copied from an internal email):
>
> I looked through some heap dumps from the OOM's in spark and found there
> were >10k instances of DiskBlockObjectWriter's each of which were up to
> 300kb in size per active executor. At up to 12 concurrent tasks per host is
> about 33gb of space topping out. The nodes of course were failing before
> this(max mem on our ts cluster per jvm is 25gb).
>
> The memory usage primarily comes from two places, a byte array in
> LZFOutputStream and a byte array in BufferedOutputStream. These are both
> output buffers along the way to disk(so when we are using the former we can
> turn down/disable the latter). These are configured to be 65kb and 100kb
> respectively by default. The former is not a configurable option but is
> static in that library's code.
>
> These come from the ShuffleBlockWriter, that is we get an input stream with
> >10k chunks. When we do operations which require partitioning (say
> distinct, reduceByKey, etc..) it maintains the existing partition count. So
> each task basically opens >10k files, each file handle of which has these
> buffers in place for that task to write to.
>
> Solution put in place(maybe there's a better one?):
>
> Given:
> X: The heap size for an executors JVM
> Y: The number of threads/cores allowed for concurrent execution per host
> Z: The expected overhead of these output streams (currently estimated at
> 65k + size of the output buffer * 1.1 for overheads)
> K: The fraction of memory to allow be used for this overhead (configurable
> parameter, default @ 0.2)
>
> Then, the number of partitions: P = (X / Y / Z) * K
>
> Then inside some of our root sources now:
> -> After assembling the RDD, if numPartitions > P
> -> coalesce to P.
> This won't trigger another shuffle phase, so can easily sit inline to
> source definitions.
>
> The only real down side of this approach i've seen is that it limits the
> number of tasks in this initial map phase which may not be ideal for
> parallelism when loading a large dataset and then filtering heavily. It
> would be more efficient to pass P into the first distinct/reduceByKey call,
> but the user code would have to reference P.
>
>
>
>
>
> On Thu, Jul 10, 2014 at 4:50 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
> > Hey Ian,
> >
> > Thanks for bringing these up!  Responses in-line:
> >
> > Just wondering if right now spark sql is expected to be thread safe on
> > > master?
> > > doing a simple hadoop file -> RDD -> schema RDD -> write parquet
> > > will fail in reflection code if i run these in a thread pool.
> > >
> >
> > You are probably hitting SPARK-2178
> > <https://issues.apache.org/jira/browse/SPARK-2178> which is caused by
> > SI-6240 <https://issues.scala-lang.org/browse/SI-6240>.  We have a plan
> to
> > fix this by moving the schema introspection to compile time, using
> macros.
> >
> >
> > > The SparkSqlSerializer, seems to create a new Kryo instance each time
> it
> > > wants to serialize anything. I got a huge speedup when I had any
> > > non-primitive type in my SchemaRDD using the ResourcePool's from Chill
> > for
> > > providing the KryoSerializer to it. (I can open an RB if there is some
> > > reason not to re-use them?)
> > >
> >
> > Sounds like SPARK-2102 <https://issues.apache.org/jira/browse/SPARK-2102
> >.
> >  There is no reason AFAIK to not reuse the instance. A PR would be
> greatly
> > appreciated!
> >
> >
> > > With the Distinct Count operator there is no map-side operations, and a
> > > test to check for this. Is there any reason not to do a map side
> combine
> > > into a set and then merge the sets later? (similar to the approximate
> > > distinct count operator)
> > >
> >
> > Thats just not an optimization that we had implemented yet... but I've
> just
> > done it here <https://github.com/apache/spark/pull/1366> and it'll be in
> > master soon :)
> >
> >
> > > Another thing while i'm mailing.. the 1.0.1 docs have a section like:
> > > "
> > > // Note: Case classes in Scala 2.10 can support only up to 22 fields.
> To
> > > work around this limit, // you can use custom classes that implement
> the
> > > Product interface.
> > > "
> > >
> > > Which sounds great, we have lots of data in thrift.. so via scrooge (
> > > https://github.com/twitter/scrooge), we end up with ultimately
> instances
> > > of
> > > traits which implement product. Though the reflection code appears to
> > look
> > > for the constructor of the class and base the types based on those
> > > parameters?
> >
> >
> > Yeah, thats true that we only look in the constructor at the moment, but
> I
> > don't think there is a really good reason for that (other than I guess we
> > will need to add code to make sure we skip builtin object methods).  If
> you
> > want to open a JIRA, we can try fixing this.
> >
> > Michael
> >
>

--001a11c2e920872a8e04fe0d29c4--

From dev-return-8319-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 13 12:46:27 2014
Return-Path: <dev-return-8319-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8FBCE11295
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 13 Jul 2014 12:46:27 +0000 (UTC)
Received: (qmail 95069 invoked by uid 500); 13 Jul 2014 12:46:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95010 invoked by uid 500); 13 Jul 2014 12:46:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94998 invoked by uid 99); 13 Jul 2014 12:46:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 12:46:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.213.179 as permitted sender)
Received: from [209.85.213.179] (HELO mail-ig0-f179.google.com) (209.85.213.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 12:46:22 +0000
Received: by mail-ig0-f179.google.com with SMTP id h18so899501igc.0
        for <dev@spark.apache.org>; Sun, 13 Jul 2014 05:45:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:subject:mime-version:content-type;
        bh=sIpCryqnOHKxjVFWMCCeS1nJGy7IyPffgHfv1N1f/Qg=;
        b=PzIM90Q9vo0FLu87d94Uv2qkKMLVJz49/d1/EOiGlGLda6fjBWbQepx6jdh3m90wrr
         uwK2G60EH67RiFFwwJG+tAFKmkhXX6Gyr3txzdjqqdozX0p0YSrxzFEpmq8ToH0kzaPD
         wrrRSwRfc+gzKk14SgEyj/YmkzhnQIdbW/J6e9BzynVTEn4C9tV7tCJff70+9dEoS+r5
         1YRTjoRLj7yS1kr89uczNrdDffAePa4XwytfjU6lVSPkAC+h1OK7q917RwE7Ps/t53Q/
         v60H8gi9MlRvtQeqwdOI6C7vF3giXxb72Fqqy92quIJzBAc8GAX+AasbEu0V1SFZDQYv
         oSmg==
X-Received: by 10.50.25.196 with SMTP id e4mr17738652igg.28.1405255557198;
        Sun, 13 Jul 2014 05:45:57 -0700 (PDT)
Received: from [192.168.2.13] ([69.159.114.117])
        by mx.google.com with ESMTPSA id kk10sm10660666igb.19.2014.07.13.05.45.56
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sun, 13 Jul 2014 05:45:56 -0700 (PDT)
Date: Sun, 13 Jul 2014 08:56:37 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <EB8AF045DC3E4B07A592E6226E47BA10@gmail.com>
Subject: how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53c28205_68eb2f63_1e6"
X-Virus-Checked: Checked by ClamAV on apache.org

--53c28205_68eb2f63_1e6
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi, all =20

I=E2=80=99m trying the JDBC server, so the cluster is running the version=
 compiled from branch-0.1-jdbc =20

Unfortunately (and as expected), it cannot run the programs compiled with=
 the dependency on spark 1.0 (i.e. download from maven)

1. The first error I met is the different SerializationVersionUID in Exec=
uterStatus =20

I resolved by explicitly declare SerializationVersionUID in ExecuterStatu=
s.scala and recompile branch-0.1-jdbc

2. Then I start the program compiled with spark-1.0, what I met is =20

14/07/13 05:08:11 WARN AppClient=24ClientActor: Could not connect to akka=
.tcp://sparkMaster=40172.31.*.*:*: java.util.NoSuchElementException: key =
not found: 6 =20
14/07/13 05:08:11 WARN AppClient=24ClientActor: Connection to akka.tcp://=
sparkMaster=40172.31.*.*:* failed; waiting for master to reconnect...



I don=E2=80=99t understand how =22key not found: 6=E2=80=9D comes



Also I tried to start JDBC server with spark-1.0 cluster, after resolving=
 different SerializationVersionUID, what I met is that when I use beeline=
 to run =E2=80=9Cshow tables;=E2=80=9D, it shows some executors get lost =
and tasks failed for unknown reason

Anyone can give some suggestions on how to make spark-1.0 cluster work wi=
th JDBC=3F =20

(maybe I need to have a internal maven repo and change all spark dependen=
cy to that=3F)

Best,

-- =20
Nan Zhu


--53c28205_68eb2f63_1e6--


From dev-return-8320-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 13 18:47:47 2014
Return-Path: <dev-return-8320-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C414F11B00
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 13 Jul 2014 18:47:47 +0000 (UTC)
Received: (qmail 53611 invoked by uid 500); 13 Jul 2014 18:47:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53571 invoked by uid 500); 13 Jul 2014 18:47:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53559 invoked by uid 99); 13 Jul 2014 18:47:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 18:47:46 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of haoyuan.li@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 13 Jul 2014 18:47:43 +0000
Received: by mail-wi0-f176.google.com with SMTP id bs8so1522930wib.3
        for <dev@spark.apache.org>; Sun, 13 Jul 2014 11:47:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=vc0rwaOck/62L0RI/hvXazYEK8gb0dvK2/3V4oeDVCA=;
        b=WbUXUrMQAiwptaYxKqH+NLiuGBKm5ywDA20GmFdGsGM33AekuvMgAMtqSmESU0dwcy
         As0WtbpD+yml0PMQwEzRC6S4QWAOfWIlsgKrceYmfyetGl2j/g7utyWH2Uo7YVRYL8Pv
         n75CsnpLWxa441I992t8kK5CGsMrJRy+5wTmp+l0MCFnImdMm/Gppl1JcyrLC5Xf4kQ9
         KYVpjYQZuVamW8WYbQf/WRCZTIYlhrHtxuU1LT+XXa+tnvfCpSMCDJhgg7/mnQd7ur44
         KgvJkjyDTbw1jwvEHHgokfCFY65su0NLba+fjRLlzN6lm5j98WAB/QjL79DTmt8YQi/r
         pETQ==
X-Received: by 10.194.134.70 with SMTP id pi6mr14030104wjb.1.1405277242005;
 Sun, 13 Jul 2014 11:47:22 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.204.6 with HTTP; Sun, 13 Jul 2014 11:47:01 -0700 (PDT)
In-Reply-To: <CABDsqqarh4OO2fdLEB49yXjroBoCHh7_oVuRdsGZx-jNFdb1Lw@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
 <CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
 <CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
 <CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
 <CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com> <CABDsqqarh4OO2fdLEB49yXjroBoCHh7_oVuRdsGZx-jNFdb1Lw@mail.gmail.com>
From: Haoyuan Li <haoyuan.li@gmail.com>
Date: Sun, 13 Jul 2014 11:47:01 -0700
Message-ID: <CAG2iju36syXK7yJUDgwOS4U9PTrR+M=DYPcV9AaBP=baskZgvw@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01175d9dd220cd04fe179a3d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01175d9dd220cd04fe179a3d
Content-Type: text/plain; charset=UTF-8

Qingyang,

Are you asking Spark or Shark (The first email was "Shark", the last email
was "Spark".)?

Best,

Haoyuan


On Wed, Jul 9, 2014 at 7:40 PM, qingyang li <liqingyang1985@gmail.com>
wrote:

> could i set some cache policy to let spark load data from tachyon only one
> time for all sql query?  for example by using CacheAllPolicy
> FIFOCachePolicy LRUCachePolicy.  But I have tried that three policy, they
> are not useful.
> I think , if spark always load data for each sql query,  it will impact the
> query speed , it will take more time than the case that data are managed by
> spark itself.
>
>
>
>
> 2014-07-09 1:19 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
>
> > Yes. For Shark, two modes, "shark.cache=tachyon" and
> "shark.cache=memory",
> > have the same ser/de overhead. Shark loads data from outsize of the
> process
> > in Tachyon mode with the following benefits:
> >
> >
> >    - In-memory data sharing across multiple Shark instances (i.e.
> stronger
> >    isolation)
> >    - Instant recovery of in-memory tables
> >    - Reduce heap size => faster GC in shark
> >    - If the table is larger than the memory size, only the hot columns
> will
> >    be cached in memory
> >
> > from http://tachyon-project.org/master/Running-Shark-on-Tachyon.html and
> > https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon
> >
> > Haoyuan
> >
> >
> > On Tue, Jul 8, 2014 at 9:58 AM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >
> > > Shark's in-memory format is already serialized (it's compressed and
> > > column-based).
> > >
> > >
> > > On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <mridul@gmail.com>
> > > wrote:
> > >
> > > > You are ignoring serde costs :-)
> > > >
> > > > - Mridul
> > > >
> > > > On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <ilikerps@gmail.com>
> > > wrote:
> > > > > Tachyon should only be marginally less performant than memory_only,
> > > > because
> > > > > we mmap the data from Tachyon's ramdisk. We do not have to, say,
> > > transfer
> > > > > the data over a pipe from Tachyon; we can directly read from the
> > > buffers
> > > > in
> > > > > the same way that Shark reads from its in-memory columnar format.
> > > > >
> > > > >
> > > > >
> > > > > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <
> > liqingyang1985@gmail.com>
> > > > > wrote:
> > > > >
> > > > >> hi, when i create a table, i can point the cache strategy using
> > > > >> shark.cache,
> > > > >> i think "shark.cache=memory_only"  means data are managed by
> spark,
> > > and
> > > > >> data are in the same jvm with excutor;   while
> >  "shark.cache=tachyon"
> > > > >>  means  data are managed by tachyon which is off heap, and data
> are
> > > not
> > > > in
> > > > >> the same jvm with excutor,  so spark will load data from tachyon
> for
> > > > each
> > > > >> query sql , so,  is  tachyon less efficient than memory_only cache
> > > > strategy
> > > > >>  ?
> > > > >> if yes, can we let spark load all data once from tachyon  for all
> > sql
> > > > query
> > > > >>  if i want to use tachyon cache strategy since tachyon is more HA
> > than
> > > > >> memory_only ?
> > > > >>
> > > >
> > >
> >
> >
> >
> > --
> > Haoyuan Li
> > AMPLab, EECS, UC Berkeley
> > http://www.cs.berkeley.edu/~haoyuan/
> >
>



-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/

--089e01175d9dd220cd04fe179a3d--

From dev-return-8321-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 00:53:33 2014
Return-Path: <dev-return-8321-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD10F1129C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 00:53:33 +0000 (UTC)
Received: (qmail 28391 invoked by uid 500); 14 Jul 2014 00:53:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28325 invoked by uid 500); 14 Jul 2014 00:53:33 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28314 invoked by uid 99); 14 Jul 2014 00:53:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 00:53:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.182 as permitted sender)
Received: from [74.125.82.182] (HELO mail-we0-f182.google.com) (74.125.82.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 00:53:28 +0000
Received: by mail-we0-f182.google.com with SMTP id q59so3372836wes.41
        for <dev@spark.apache.org>; Sun, 13 Jul 2014 17:53:06 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=JdeO54EeuTqWw3WZgMjo5MCeaj2dHsgdioCtfo08Jbc=;
        b=GhOs6dxA3RfB4K6D63DCoKxWDGnbSX7QHFoaNpkbQ5xdDza1Aes8AYJqvBmjvzbf9b
         5Szr8nL99FlQ3JNUvCoCX55AJKY7HIAAnlCWc1rt8RkG2wGJxRTE8zPdCxh0TXQUS+Fp
         0X080wpiFqUmTZOWxYDli3baCRlJo8Y5lTuDUf8R52u0/wF/Z70szfU1NgUmHx7U96Dv
         0Z5EnOMXAQG9X1IzEnKa5IanczntYeLUVr6dL8vX1e0xLnWlF+v5Yme2WQU6hLM/ec58
         mr+mC/bCx5ZnuJF1Bf+c8rG91pBxCIMbzVIR8lYIwCY6d4gRLkP+LnwtBiISjP7ktaXs
         kQSg==
X-Gm-Message-State: ALoCoQndIvMlIMNy6TcSKLhwxMhfORJgdHymExWARc1Eo/B7GU0p8Z6V809qh/I9KCql/FwNsGfE
MIME-Version: 1.0
X-Received: by 10.194.79.135 with SMTP id j7mr15667209wjx.56.1405299185741;
 Sun, 13 Jul 2014 17:53:05 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.217.48.72 with HTTP; Sun, 13 Jul 2014 17:53:05 -0700 (PDT)
In-Reply-To: <CAOhmDzceR8E+ij6R185Q0tOqWRiMgtEQ_saO1H3Ne2SW8tXZOg@mail.gmail.com>
References: <CAOhmDzch1e=zOaqhi7VNNX+doGuNSi3PQCBSHZKXFRW8E9B+uA@mail.gmail.com>
	<057601cf9c9c$7e798c80$7b6ca580$@reactor8.com>
	<CAOhmDzceR8E+ij6R185Q0tOqWRiMgtEQ_saO1H3Ne2SW8tXZOg@mail.gmail.com>
Date: Sun, 13 Jul 2014 17:53:05 -0700
Message-ID: <CAKx7Bf8F=bKwR-2vWcYFhL=qUWgw-TTYZiWupjdJhz6KR31WgQ@mail.gmail.com>
Subject: Re: EC2 clusters ready in launch time + 30 seconds
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bb04f04c50cfa04fe1cb624
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb04f04c50cfa04fe1cb624
Content-Type: text/plain; charset=UTF-8

It should be possible to improve cluster launch time if we are careful
about what commands we run during setup. One way to do this would be to
walk down the list of things we do for cluster initialization and see if
there is anything we can do make things faster. Unfortunately this might be
pretty time consuming, but I don't know of a better strategy. The place to
start would be the setup.sh file at
https://github.com/mesos/spark-ec2/blob/v3/setup.sh

Here are some things that take a lot of time and could be improved:
1. Creating swap partitions on all machines. We could check if there is a
way to get EC2 to always mount a swap partition
2. Copying / syncing things across slaves. The copy-dir script is called
too many times right now and each time it pauses for a few milliseconds
between slaves [1]. This could be improved by removing unnecessary copies
3. We could make less frequently used modules like Tachyon, persistent hdfs
not a part of the default setup.

[1] https://github.com/mesos/spark-ec2/blob/v3/copy-dir.sh#L42

Thanks
Shivaram




On Sat, Jul 12, 2014 at 7:02 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> On Thu, Jul 10, 2014 at 8:10 PM, Nate D'Amico <nate@reactor8.com> wrote:
>
> > Starting to work through some automation/config stuff for spark stack on
> > EC2 with a project, will be focusing the work through the apache bigtop
> > effort to start, can then share with spark community directly as things
> > progress if people are interested
>
>
> Let us know how that goes. I'm definitely interested in hearing more.
>
> Nick
>

--047d7bb04f04c50cfa04fe1cb624--

From dev-return-8322-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 05:13:36 2014
Return-Path: <dev-return-8322-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D5DA111656
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 05:13:36 +0000 (UTC)
Received: (qmail 35582 invoked by uid 500); 14 Jul 2014 05:13:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35518 invoked by uid 500); 14 Jul 2014 05:13:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35495 invoked by uid 99); 14 Jul 2014 05:13:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 05:13:35 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 05:13:31 +0000
Received: by mail-wg0-f47.google.com with SMTP id b13so1399381wgh.18
        for <dev@spark.apache.org>; Sun, 13 Jul 2014 22:13:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=fyavYS7Z4GcTn0zgEZ5652aXDACZSc1ITKj5kNQ3vg4=;
        b=BFv5ORph8fVARnwZhcaIM0LzBtJb8pMzCnb9P/hdBF1iPvg2GJ4ysdYP9FOZgsmmGX
         MZ7z8I14fh+FXv3xwUmouQyh325qvMznN281JECtUfyNbZ6FE0qZV1rrmoLVxwmOtyWF
         ijmvbe2SRRmvgbP8l6csfQL8m6yxaJWHHiKqgcIPKdtLNTLDIx2SH9Dv6EQ9jbqxZKcy
         /p5SfNTVBPFi2296/LsCg4ZhTxSbynk6KrrDfbPU8ZAS5BElzCK/MbOVKcWWb0HWdMwF
         s2vD/wgASYIub2wSLKXCYvJvZNdhMpczzIErEVRa/LiAugLDawKXLBAwkDdXONrEaX5J
         L2rw==
MIME-Version: 1.0
X-Received: by 10.194.123.105 with SMTP id lz9mr305185wjb.122.1405314790210;
 Sun, 13 Jul 2014 22:13:10 -0700 (PDT)
Received: by 10.194.6.74 with HTTP; Sun, 13 Jul 2014 22:13:10 -0700 (PDT)
In-Reply-To: <CAG2iju36syXK7yJUDgwOS4U9PTrR+M=DYPcV9AaBP=baskZgvw@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
	<CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
	<CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
	<CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
	<CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com>
	<CABDsqqarh4OO2fdLEB49yXjroBoCHh7_oVuRdsGZx-jNFdb1Lw@mail.gmail.com>
	<CAG2iju36syXK7yJUDgwOS4U9PTrR+M=DYPcV9AaBP=baskZgvw@mail.gmail.com>
Date: Mon, 14 Jul 2014 13:13:10 +0800
Message-ID: <CABDsqqYP8QG+FOs3k4Jr9RDaStV8jxgGZ2K7=Q1HJqpZXruu8w@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01177699de409904fe2058f7
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01177699de409904fe2058f7
Content-Type: text/plain; charset=UTF-8

Shark,  thanks for replying.
Let's me clear my question again.
----------------------------------------------
i create a table using " create table xxx1
tblproperties("shark.cache"="tachyon") as select * from xxx2"
when excuting some sql (for example , select * from xxx1) using shark,
 shark will read data into shark's memory  from tachyon's memory.
I think if each time we execute sql, shark always load data from tachyon,
it is less effient.
could we use some cache policy (such as,  CacheAllPolicy FIFOCachePolicy
LRUCachePolicy ) to cache data to invoid reading data from tachyon for each
sql query?
----------------------------------------------



2014-07-14 2:47 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:

> Qingyang,
>
> Are you asking Spark or Shark (The first email was "Shark", the last email
> was "Spark".)?
>
> Best,
>
> Haoyuan
>
>
> On Wed, Jul 9, 2014 at 7:40 PM, qingyang li <liqingyang1985@gmail.com>
> wrote:
>
> > could i set some cache policy to let spark load data from tachyon only
> one
> > time for all sql query?  for example by using CacheAllPolicy
> > FIFOCachePolicy LRUCachePolicy.  But I have tried that three policy, they
> > are not useful.
> > I think , if spark always load data for each sql query,  it will impact
> the
> > query speed , it will take more time than the case that data are managed
> by
> > spark itself.
> >
> >
> >
> >
> > 2014-07-09 1:19 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
> >
> > > Yes. For Shark, two modes, "shark.cache=tachyon" and
> > "shark.cache=memory",
> > > have the same ser/de overhead. Shark loads data from outsize of the
> > process
> > > in Tachyon mode with the following benefits:
> > >
> > >
> > >    - In-memory data sharing across multiple Shark instances (i.e.
> > stronger
> > >    isolation)
> > >    - Instant recovery of in-memory tables
> > >    - Reduce heap size => faster GC in shark
> > >    - If the table is larger than the memory size, only the hot columns
> > will
> > >    be cached in memory
> > >
> > > from http://tachyon-project.org/master/Running-Shark-on-Tachyon.html
> and
> > > https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon
> > >
> > > Haoyuan
> > >
> > >
> > > On Tue, Jul 8, 2014 at 9:58 AM, Aaron Davidson <ilikerps@gmail.com>
> > wrote:
> > >
> > > > Shark's in-memory format is already serialized (it's compressed and
> > > > column-based).
> > > >
> > > >
> > > > On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <
> mridul@gmail.com>
> > > > wrote:
> > > >
> > > > > You are ignoring serde costs :-)
> > > > >
> > > > > - Mridul
> > > > >
> > > > > On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <ilikerps@gmail.com
> >
> > > > wrote:
> > > > > > Tachyon should only be marginally less performant than
> memory_only,
> > > > > because
> > > > > > we mmap the data from Tachyon's ramdisk. We do not have to, say,
> > > > transfer
> > > > > > the data over a pipe from Tachyon; we can directly read from the
> > > > buffers
> > > > > in
> > > > > > the same way that Shark reads from its in-memory columnar format.
> > > > > >
> > > > > >
> > > > > >
> > > > > > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <
> > > liqingyang1985@gmail.com>
> > > > > > wrote:
> > > > > >
> > > > > >> hi, when i create a table, i can point the cache strategy using
> > > > > >> shark.cache,
> > > > > >> i think "shark.cache=memory_only"  means data are managed by
> > spark,
> > > > and
> > > > > >> data are in the same jvm with excutor;   while
> > >  "shark.cache=tachyon"
> > > > > >>  means  data are managed by tachyon which is off heap, and data
> > are
> > > > not
> > > > > in
> > > > > >> the same jvm with excutor,  so spark will load data from tachyon
> > for
> > > > > each
> > > > > >> query sql , so,  is  tachyon less efficient than memory_only
> cache
> > > > > strategy
> > > > > >>  ?
> > > > > >> if yes, can we let spark load all data once from tachyon  for
> all
> > > sql
> > > > > query
> > > > > >>  if i want to use tachyon cache strategy since tachyon is more
> HA
> > > than
> > > > > >> memory_only ?
> > > > > >>
> > > > >
> > > >
> > >
> > >
> > >
> > > --
> > > Haoyuan Li
> > > AMPLab, EECS, UC Berkeley
> > > http://www.cs.berkeley.edu/~haoyuan/
> > >
> >
>
>
>
> --
> Haoyuan Li
> AMPLab, EECS, UC Berkeley
> http://www.cs.berkeley.edu/~haoyuan/
>

--089e01177699de409904fe2058f7--

From dev-return-8323-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 07:02:05 2014
Return-Path: <dev-return-8323-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B23FB1187D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 07:02:05 +0000 (UTC)
Received: (qmail 77688 invoked by uid 500); 14 Jul 2014 07:02:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77635 invoked by uid 500); 14 Jul 2014 07:02:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77589 invoked by uid 99); 14 Jul 2014 07:02:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 07:02:04 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 07:02:01 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so1209887qap.9
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 00:01:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=7pvikK5g0fXm8WSOnbeZQ7CrycPC52NZrTpdjT9u/Pg=;
        b=hK/JRrSK2pC6D0OPEAg8rnV7yXlGAA5W/kshITVmnrpVLK/04u+mH88CrGsovUatVs
         ny8cCDwah95CE6IM4pt9ZObMhAXe/mvKNiIot85B9qX7HoqVMWyW70SYeCBbFI/QLQSc
         Wbd8sdd2Jn7ecgaJxW5fnJlQdorA0IreUaxzHbJ5mgm91xiDA0SKsxJ89l9SYlJ7CLDD
         LVo2EdyNbQF+LyBQ1GPAK3JrQNVQCR0yOoIa2h3Qz8zLczIVS6sm8loq6v0mu3Eyir/s
         XOE4375h4Ft+mnTMa3GUd/1/0JrZBB1wDkNNciiBtTSNVwDMeZNUhP5fQcNnNBUclgRR
         tQyg==
X-Gm-Message-State: ALoCoQlpBJzar2Es/yK/tMrFSUsdUYQbXDp2ADORhCCX7s4nih00wLiliBcLum1iY/Sg6ZOAi/NO
X-Received: by 10.224.22.12 with SMTP id l12mr19676055qab.88.1405321296969;
 Mon, 14 Jul 2014 00:01:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 14 Jul 2014 00:01:16 -0700 (PDT)
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 14 Jul 2014 00:01:16 -0700
Message-ID: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
Subject: better compression codecs for shuffle blocks?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bf0e59cb399e604fe21dcc2
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bf0e59cb399e604fe21dcc2
Content-Type: text/plain; charset=UTF-8

Hi Spark devs,

I was looking into the memory usage of shuffle and one annoying thing is
the default compression codec (LZF) is that the implementation we use
allocates buffers pretty generously. I did a simple experiment and found
that creating 1000 LZFOutputStream allocated 198976424 bytes (~190MB). If
we have a shuffle task that uses 10k reducers and 32 threads running
currently, the memory used by the lzf stream alone would be ~ 60GB.

In comparison, Snappy only allocates ~ 65MB for every
1k SnappyOutputStream. However, Snappy's compression is slightly lower than
LZF's. In my experience, it leads to 10 - 20% increase in size. Compression
ratio does matter here because we are sending data across the network.

In future releases we will likely change the shuffle implementation to open
less streams. Until that happens, I'm looking for compression codec
implementations that are fast, allocate small buffers, and have decent
compression ratio.

Does anybody on this list have any suggestions? If not, I will submit a
patch for 1.1 that replaces LZF with Snappy for the default compression
codec to lower memory usage.


allocation data here: https://gist.github.com/rxin/ad7217ea60e3fb36c567

--047d7bf0e59cb399e604fe21dcc2--

From dev-return-8324-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 07:34:46 2014
Return-Path: <dev-return-8324-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B59B211948
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 07:34:46 +0000 (UTC)
Received: (qmail 49373 invoked by uid 500); 14 Jul 2014 07:34:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49331 invoked by uid 500); 14 Jul 2014 07:34:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 49319 invoked by uid 99); 14 Jul 2014 07:34:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 07:34:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.42 as permitted sender)
Received: from [209.85.219.42] (HELO mail-oa0-f42.google.com) (209.85.219.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 07:34:41 +0000
Received: by mail-oa0-f42.google.com with SMTP id n16so3845590oag.29
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 00:34:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=i0fU2t5IxfNQZGfQgSgsZ0LCxJDxiU8N/tjqMueKpC4=;
        b=UOi8XuClvkyu2MSfItKhuTnuPheB6jUZ2ABQ2iavKM/SWL2IAz9J6fOzmUVvIHKQgi
         Gt9Kq0i9gdjZLTSxaG7doDrmIVimxF66VJSqJuO55gAXJJfwwL5sc0+NAROS+i2K1riF
         M5DMfuW6qkzMNNMsqjXK1aOGM/oukTgRTA6OzKyccmofgcDCUkGFu19VTS8Pa3GeHWWJ
         q5V7KTh8zTakFtaa3tNfXPsicf2PZjieBSOZVWUdO+O8+ShkrhMZf9GJIPBDXv8q3XGj
         y+g3n37WECLC1nlN1p6o0yH9d4nDBxJMUzOvUDKK0LR+EOYtnod/hRJ3uM5e0kE+rTb/
         2hjw==
MIME-Version: 1.0
X-Received: by 10.182.24.38 with SMTP id r6mr16250839obf.10.1405323256164;
 Mon, 14 Jul 2014 00:34:16 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 00:34:16 -0700 (PDT)
In-Reply-To: <EB8AF045DC3E4B07A592E6226E47BA10@gmail.com>
References: <EB8AF045DC3E4B07A592E6226E47BA10@gmail.com>
Date: Mon, 14 Jul 2014 00:34:16 -0700
Message-ID: <CABPQxst+bgk6CQBjOh9yygRMCOZFW2M+_zjdW017_1ms9+DbCA@mail.gmail.com>
Subject: Re: how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

> 1. The first error I met is the different SerializationVersionUID in ExecuterStatus
>
> I resolved by explicitly declare SerializationVersionUID in ExecuterStatus.scala and recompile branch-0.1-jdbc
>

I don't think there is a class in Spark named ExecuterStatus (sic) ...
or ExecutorStatus. Is this a class you made?

From dev-return-8325-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 10:36:35 2014
Return-Path: <dev-return-8325-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2CBA811E52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 10:36:35 +0000 (UTC)
Received: (qmail 37982 invoked by uid 500); 14 Jul 2014 10:36:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 37918 invoked by uid 500); 14 Jul 2014 10:36:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37906 invoked by uid 99); 14 Jul 2014 10:36:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 10:36:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 10:36:28 +0000
Received: by mail-qg0-f50.google.com with SMTP id j5so3162553qga.23
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 03:36:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=V3zZlJz6EDZ6NNxi+lVK0IAdRteQtF7Nx/m33/xTD3s=;
        b=K6lqdtw5BPf/NEL2ZxGFyZPM6mRIKif97JIsyh18jjL7LP1QaiXdITekfaQR/V0jYm
         GuivjgNC68Va5BPH38vvK6Gtv0Qsw63IPM/T9ILNqzpdAmz3uCzbdvS76s1Er6eNr4VG
         x9KnWKLLs0afP6QkGKzNH/pfjZ+D7njAZ2uUL7OzRdS/Jamf6U0zuTzn3DLgcWvmtMd9
         ZLRGLJZsf0+8jRRbzmV8lqZAVU2jobu3D4kLYTuCVLrZMgO2JXKmBljs2n1C2jPxVkUE
         Tepdaf/BMd7MbtKs2TZRzsl1MLAI8Y2rNmMag1FgP1VbwUrc0X8oGNZ2xEErSWK2DpYn
         YEZg==
MIME-Version: 1.0
X-Received: by 10.224.166.138 with SMTP id m10mr20989410qay.69.1405334168299;
 Mon, 14 Jul 2014 03:36:08 -0700 (PDT)
Received: by 10.140.85.100 with HTTP; Mon, 14 Jul 2014 03:36:08 -0700 (PDT)
In-Reply-To: <CABPQxst+bgk6CQBjOh9yygRMCOZFW2M+_zjdW017_1ms9+DbCA@mail.gmail.com>
References: <EB8AF045DC3E4B07A592E6226E47BA10@gmail.com>
	<CABPQxst+bgk6CQBjOh9yygRMCOZFW2M+_zjdW017_1ms9+DbCA@mail.gmail.com>
Date: Mon, 14 Jul 2014 06:36:08 -0400
Message-ID: <CAMtqZee9ZQnjxXNbgJsp1ztUrQkOsBr5A8zusZ-nx=pbjZ23nw@mail.gmail.com>
Subject: Re: how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster
From: Nan Zhu <zhunanmcgill@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2394ce46d1204fe24db18
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2394ce46d1204fe24db18
Content-Type: text/plain; charset=UTF-8

Ah, sorry, sorry

It's executorState under deploy package

On Monday, July 14, 2014, Patrick Wendell <pwendell@gmail.com> wrote:

> > 1. The first error I met is the different SerializationVersionUID in
> ExecuterStatus
> >
> > I resolved by explicitly declare SerializationVersionUID in
> ExecuterStatus.scala and recompile branch-0.1-jdbc
> >
>
> I don't think there is a class in Spark named ExecuterStatus (sic) ...
> or ExecutorStatus. Is this a class you made?
>

--001a11c2394ce46d1204fe24db18--

From dev-return-8326-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 11:34:55 2014
Return-Path: <dev-return-8326-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D392D11FA0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 11:34:55 +0000 (UTC)
Received: (qmail 22488 invoked by uid 500); 14 Jul 2014 11:34:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22426 invoked by uid 500); 14 Jul 2014 11:34:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22399 invoked by uid 99); 14 Jul 2014 11:34:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 11:34:54 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mridul@gmail.com designates 209.85.216.180 as permitted sender)
Received: from [209.85.216.180] (HELO mail-qc0-f180.google.com) (209.85.216.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 11:34:53 +0000
Received: by mail-qc0-f180.google.com with SMTP id l6so3072020qcy.39
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 04:34:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=3HA7yGp5mXJHkoxtC98SvgOKerx1mFrMO73PcWqnud4=;
        b=RsTR93MpVJMS+D1OtgQmuHf/88kROqCL9DQIKmsMtsSVy9mdqTD3JfD07QSwO7tmSJ
         voBfzv4vxtAP9I1U4+mYf1Z/oc4EJFSr1KKv6THs2mH4zsh28Do63S0uTLwTQ8JlERZ8
         qSq8FcgmyYj0PWVU3NslzD4B+0YICLfIPSNeC2OQA+BEsJkK96zK5nOKMx6hf3NAX1v4
         rrfM6LpdFQv+wAgI+SSkKcj4n3IrzeocM57/2iy7sY37POpZXt3+h/WNSFA7ZdThlAlk
         sFBfMrssr90l+zkOzC7LFuiDq7HOEkwheG1o+9ar7PfTm12pa3UgHUT/w+n/94q3UfZX
         q8Bg==
MIME-Version: 1.0
X-Received: by 10.224.34.73 with SMTP id k9mr22476810qad.11.1405337668542;
 Mon, 14 Jul 2014 04:34:28 -0700 (PDT)
Received: by 10.140.38.170 with HTTP; Mon, 14 Jul 2014 04:34:28 -0700 (PDT)
In-Reply-To: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
Date: Mon, 14 Jul 2014 17:04:28 +0530
Message-ID: <CAJiQeY+5n-uwQ93e1V14XURy_NCmoHgg8Af-Cbbc=-++1n93uA@mail.gmail.com>
Subject: Re: better compression codecs for shuffle blocks?
From: Mridul Muralidharan <mridul@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

We tried with lower block size for lzf, but it barfed all over the place.
Snappy was the way to go for our jobs.


Regards,
Mridul


On Mon, Jul 14, 2014 at 12:31 PM, Reynold Xin <rxin@databricks.com> wrote:
> Hi Spark devs,
>
> I was looking into the memory usage of shuffle and one annoying thing is
> the default compression codec (LZF) is that the implementation we use
> allocates buffers pretty generously. I did a simple experiment and found
> that creating 1000 LZFOutputStream allocated 198976424 bytes (~190MB). If
> we have a shuffle task that uses 10k reducers and 32 threads running
> currently, the memory used by the lzf stream alone would be ~ 60GB.
>
> In comparison, Snappy only allocates ~ 65MB for every
> 1k SnappyOutputStream. However, Snappy's compression is slightly lower than
> LZF's. In my experience, it leads to 10 - 20% increase in size. Compression
> ratio does matter here because we are sending data across the network.
>
> In future releases we will likely change the shuffle implementation to open
> less streams. Until that happens, I'm looking for compression codec
> implementations that are fast, allocate small buffers, and have decent
> compression ratio.
>
> Does anybody on this list have any suggestions? If not, I will submit a
> patch for 1.1 that replaces LZF with Snappy for the default compression
> codec to lower memory usage.
>
>
> allocation data here: https://gist.github.com/rxin/ad7217ea60e3fb36c567

From dev-return-8327-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 16:51:40 2014
Return-Path: <dev-return-8327-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 846CB119CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 16:51:40 +0000 (UTC)
Received: (qmail 23841 invoked by uid 500); 14 Jul 2014 16:51:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23778 invoked by uid 500); 14 Jul 2014 16:51:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23767 invoked by uid 99); 14 Jul 2014 16:51:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 16:51:39 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wibenton@redhat.com designates 209.132.183.39 as permitted sender)
Received: from [209.132.183.39] (HELO mx6-phx2.redhat.com) (209.132.183.39)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 16:51:34 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx6-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s6EGpCqp003573
	for <dev@spark.apache.org>; Mon, 14 Jul 2014 12:51:12 -0400
Date: Mon, 14 Jul 2014 12:51:11 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com>
In-Reply-To: <1790318077.16442789.1405356237238.JavaMail.zimbra@redhat.com>
Subject: Profiling Spark tests with YourKit (or something else)
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF30 (Mac)/8.0.6_GA_5922)
Thread-Topic: Profiling Spark tests with YourKit (or something else)
Thread-Index: l9tnFZRB09Vor8UVwvF5nFLoFWSLjw==
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I've been evaluating YourKit and would like to profile the heap and CPU usage of certain tests from the Spark test suite.  In particular, I'm very interested in tracking heap usage by allocation site.  Unfortunately, I get a lot of crashes running Spark tests with profiling (and thus allocation-site tracking) enabled in YourKit; just using the sampler works fine, but it appears that enabling the profiler breaks Utils.getCallSite.

Is there a way to make this combination work?  If not, what are people using to understand the memory and CPU behavior of Spark and Spark apps?


thanks,
wb

From dev-return-8328-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 18:02:36 2014
Return-Path: <dev-return-8328-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 476F711CC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 18:02:36 +0000 (UTC)
Received: (qmail 99139 invoked by uid 500); 14 Jul 2014 18:02:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99081 invoked by uid 500); 14 Jul 2014 18:02:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99069 invoked by uid 99); 14 Jul 2014 18:02:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 18:02:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.51 as permitted sender)
Received: from [209.85.220.51] (HELO mail-pa0-f51.google.com) (209.85.220.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 18:02:28 +0000
Received: by mail-pa0-f51.google.com with SMTP id ey11so2832780pad.38
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 11:02:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=FjfBHrcDFIVmcMejKOJnOIXhl5pEGMMqvCgAqDgHIIE=;
        b=Kdr5fjUvjIaVbjWlty69crw6e9oHcfjY4mwoAeDUE5IJ3e0kiuD9TEdSBQCNJTpt8V
         xNz5LrFUC5Cl8/a5ZrG4ROuhyItnGqxLfqw4/ebX/n59zosTPjur9OWoQ8bK+7spPEMl
         93T1/8GZgP4bqMv7Tzh3St5oPZ7vzZxKoXSH/0qbMvRlEPqpk3BOqH5cHF4bkpB9aI1E
         mNfeNxiNEuwennxBZ6siaeGPLyQOEvNrE+nsJqvGI8+ULB+4SUODSQryANQktU1gOHoH
         wiOS9LYvxJsiOYKmV5zMxGMJStsqg7mMOZE1dOawhxEFRY3emOac+syyYqrUqZmA4Mkt
         eVyQ==
X-Received: by 10.66.231.139 with SMTP id tg11mr17841023pac.87.1405360928498;
        Mon, 14 Jul 2014 11:02:08 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id rz4sm48064021pab.13.2014.07.14.11.02.06
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 11:02:07 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Profiling Spark tests with YourKit (or something else)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com>
Date: Mon, 14 Jul 2014 11:02:04 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <674ECB29-553B-4927-A64B-AFC6E5F97FED@gmail.com>
References: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

I haven't seen issues using the JVM's own tools (jstack, jmap, hprof and =
such), so maybe there's a problem in YourKit or in your release of the =
JVM. Otherwise I'd suggest increasing the heap size of the unit tests a =
bit (you can do this in the SBT build file). Maybe they are very close =
to full and profiling pushes them over the edge.

Matei

On Jul 14, 2014, at 9:51 AM, Will Benton <willb@redhat.com> wrote:

> Hi all,
>=20
> I've been evaluating YourKit and would like to profile the heap and =
CPU usage of certain tests from the Spark test suite.  In particular, =
I'm very interested in tracking heap usage by allocation site.  =
Unfortunately, I get a lot of crashes running Spark tests with profiling =
(and thus allocation-site tracking) enabled in YourKit; just using the =
sampler works fine, but it appears that enabling the profiler breaks =
Utils.getCallSite.
>=20
> Is there a way to make this combination work?  If not, what are people =
using to understand the memory and CPU behavior of Spark and Spark apps?
>=20
>=20
> thanks,
> wb


From dev-return-8329-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 20:05:19 2014
Return-Path: <dev-return-8329-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7A71F112A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 20:05:19 +0000 (UTC)
Received: (qmail 87269 invoked by uid 500); 14 Jul 2014 20:05:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87176 invoked by uid 500); 14 Jul 2014 20:05:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86363 invoked by uid 99); 14 Jul 2014 20:05:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 20:05:17 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 20:05:11 +0000
Received: by mail-vc0-f178.google.com with SMTP id la4so1574631vcb.9
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 13:04:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=BSkx6ZPlDrkWCSYzlCixrQdDJYeUyJBWcgY+zHlYW0I=;
        b=IJRCLs3xCZ/JyZjKOrjIaDREuni9UFYq2oT2EwjOrDYHMC89hK1t/lJuuO6Apjx1Fl
         s4+UCoS8jAtc0jvXctk8BMV1+FSeA1WDKt/jkVKm7q/rZlyKU8O7KuTRnNxErq0/aDNW
         fxwfD2bZrJ5+hx0XUao+dntYkgmA9KjXxO1tyqc2+JV8fINP7zSDnVk+Z8fo9TgzRuRr
         qmFwbDk/nPh0YS5Mk864GqGPoyOBX090+OYWWaGVYElJAtKi3A6OVwJ6OiurIovIRf+Y
         IJUJXLRLLtd0tAQ20lzYtntkHRL5zaV/YbcIh6Sy6FjQRv9gWNjyYbowyoZs8z8otBKY
         cITw==
X-Gm-Message-State: ALoCoQlrNU0TgdO7HkZ1S6MdDs0+TTrQl649PKae+xDW1LjDSDH0TO1P8JpLYvk+n6dnJC1BRLDN
X-Received: by 10.52.23.71 with SMTP id k7mr7163505vdf.27.1405368291024; Mon,
 14 Jul 2014 13:04:51 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.108.194 with HTTP; Mon, 14 Jul 2014 13:04:30 -0700 (PDT)
In-Reply-To: <CALDQvdcYe1ct_LdwmkV5tfR1_rfMMxhFyu=VdZw_Y2aAYmin-Q@mail.gmail.com>
References: <CAJOb8btWjn+pVBRngB-gA57CQ2YRw702j+=eP79tRyyPeZgcFA@mail.gmail.com>
 <CALDQvdcYe1ct_LdwmkV5tfR1_rfMMxhFyu=VdZw_Y2aAYmin-Q@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 14 Jul 2014 13:04:30 -0700
Message-ID: <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com>
Subject: Re: Catalyst dependency on Spark Core
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf3071c8d2c49edc04fe2ccd2c
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3071c8d2c49edc04fe2ccd2c
Content-Type: text/plain; charset=UTF-8

Yeah, sadly this dependency was introduced when someone consolidated the
logging infrastructure.  However, the dependency should be very small and
thus easy to remove, and I would like catalyst to be usable outside of
Spark.  A pull request to make this possible would be welcome.

Ideally, we'd create some sort of spark common package that has things like
logging.  That way catalyst could depend on that, without pulling in all of
Hadoop, etc.  Maybe others have opinions though, so I'm cc-ing the dev list.


On Mon, Jul 14, 2014 at 12:21 AM, Yanbo Liang <yanbohappy@gmail.com> wrote:

> Make Catalyst independent of Spark is the goal of Catalyst, maybe need
> time and evolution.
> I awared that package org.apache.spark.sql.catalyst.util
> embraced org.apache.spark.util.{Utils => SparkUtils},
> so that Catalyst has a dependency on Spark core.
> I'm not sure whether it will be replaced by other component independent of
> Spark in later release.
>
>
> 2014-07-14 11:51 GMT+08:00 Aniket Bhatnagar <aniket.bhatnagar@gmail.com>:
>
> As per the recent presentation given in Scala days (
>> http://people.apache.org/~marmbrus/talks/SparkSQLScalaDays2014.pdf), it
>> was mentioned that Catalyst is independent of Spark. But on inspecting
>> pom.xml of sql/catalyst module, it seems it has a dependency on Spark Core.
>> Any particular reason for the dependency? I would love to use Catalyst
>> outside Spark
>>
>> (reposted as previous email bounced. Sorry if this is a duplicate).
>>
>
>

--20cf3071c8d2c49edc04fe2ccd2c--

From dev-return-8330-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 20:43:07 2014
Return-Path: <dev-return-8330-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9B26111472
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 20:43:07 +0000 (UTC)
Received: (qmail 81391 invoked by uid 500); 14 Jul 2014 20:43:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81328 invoked by uid 500); 14 Jul 2014 20:43:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81310 invoked by uid 99); 14 Jul 2014 20:43:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 20:43:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of cody.koeninger@mediacrossing.com designates 209.85.212.171 as permitted sender)
Received: from [209.85.212.171] (HELO mail-wi0-f171.google.com) (209.85.212.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 20:43:00 +0000
Received: by mail-wi0-f171.google.com with SMTP id hi2so3271421wib.16
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 13:42:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=mediacrossing.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=B1pm0T/jmroP3EWg7ywLeY7x0MYSGUS8Q1e3Fxrb0WY=;
        b=JO1p0xl1UYe1JMZEyFMcU6jxTxL8Kfx6cdIpz97BBFhSxYBQDdlpwDsA1LXPi/lEGM
         Sxrz1JKr+T5BPY3KdhncHe/woVp0ceb9LA8kWc4bV3XKqYzpWepo3/t+Yyub1r5zILG6
         F2BzEFvP2Ol2Fr+sl6BRikoclWKfcKRcCbuuQ=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=B1pm0T/jmroP3EWg7ywLeY7x0MYSGUS8Q1e3Fxrb0WY=;
        b=NoorjZ1yBFjQF5xFC6LZNW+aFEQuHL+HKlGQvPOtqah77ubrdO6pzRefpVcZrg8k1a
         36zR87SSlrHR6BBpjo+YB3F6+qCU1DWvEkuhxHPZkz+j/LIC14B87cc8/7CH/88xNPz5
         fuj4erXE+ZQElh7ojaS1G+bPZTkWn/mVURlSMnYPY7MIr3a9xupW69/Y3DUBblskz4oC
         OVu8cMuwRqQdRwHVtTiZ+eFfKHzFXPymartsdCy/dQmMnGVHZw7IRC3kwIr30x+pOr4M
         YRrdMWIYnCmfhF+qtkNReZ3Rg5kiNdVokrdufNWqYckOqboQOXyNlv6UAxdcYz4ykzVF
         +Kjw==
X-Gm-Message-State: ALoCoQmm9XdGTE2IwGCDh4rtcxMOYcdT3zsPs/0rHipOKHgYeaCmfNX/mGBPGjj4lRqPQ2bUBSQa
MIME-Version: 1.0
X-Received: by 10.180.73.109 with SMTP id k13mr340899wiv.11.1405370559034;
 Mon, 14 Jul 2014 13:42:39 -0700 (PDT)
Received: by 10.194.15.33 with HTTP; Mon, 14 Jul 2014 13:42:38 -0700 (PDT)
Date: Mon, 14 Jul 2014 15:42:38 -0500
Message-ID: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
Subject: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Cody Koeninger <cody.koeninger@mediacrossing.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d043891d9f3104204fe2d548f
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043891d9f3104204fe2d548f
Content-Type: text/plain; charset=UTF-8

Hi all, just wanted to give a heads up that we're seeing a reproducible
deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2

If jira is a better place for this, apologies in advance - figured talking
about it on the mailing list was friendlier than randomly (re)opening jira
tickets.

I know Gary had mentioned some issues with 1.0.1 on the mailing list, once
we got a thread dump I wanted to follow up.

The thread dump shows the deadlock occurs in the synchronized block of code
that was changed in HadoopRDD.scala, for the Spark-1097 issue

Relevant portions of the thread dump are summarized below, we can provide
the whole dump if it's useful.

Found one Java-level deadlock:
=============================
"Executor task launch worker-1":
  waiting to lock monitor 0x00007f250400c520 (object 0x00000000fae7dc30, a
org.apache.hadoop.co
nf.Configuration),
  which is held by "Executor task launch worker-0"
"Executor task launch worker-0":
  waiting to lock monitor 0x00007f2520495620 (object 0x00000000faeb4fc8, a
java.lang.Class),
  which is held by "Executor task launch worker-1"


"Executor task launch worker-1":
        at
org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
        - waiting to lock <0x00000000fae7dc30> (a
org.apache.hadoop.conf.Configuration)
        at
org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
        - locked <0x00000000faca6ff8> (a java.lang.Class for
org.apache.hadoop.conf.Configurati
on)
        at
org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
        at
org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
java:57)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
        at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
java:57)
        at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
sorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
        at java.lang.Class.newInstance0(Class.java:374)
        at java.lang.Class.newInstance(Class.java:327)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
        at
org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
        - locked <0x00000000faeb4fc8> (a java.lang.Class for
org.apache.hadoop.fs.FileSystem)
        at
org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
        at
org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
        at
org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
        at
org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)



...elided...


"Executor task launch worker-0" daemon prio=10 tid=0x0000000001e71800
nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at
org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
        - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
org.apache.hadoop.fs.FileSystem)
        at
org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
        at
org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
        at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
        at
org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
        at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
        at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
        at
org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
        at
org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
        at
org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)

--f46d043891d9f3104204fe2d548f--

From dev-return-8331-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 21:16:15 2014
Return-Path: <dev-return-8331-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5B1FF1162C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 21:16:15 +0000 (UTC)
Received: (qmail 70734 invoked by uid 500); 14 Jul 2014 21:16:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70643 invoked by uid 500); 14 Jul 2014 21:16:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69673 invoked by uid 99); 14 Jul 2014 21:16:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 21:16:13 +0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.172 as permitted sender)
Received: from [209.85.192.172] (HELO mail-pd0-f172.google.com) (209.85.192.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 21:16:08 +0000
Received: by mail-pd0-f172.google.com with SMTP id w10so5912295pde.3
        for <multiple recipients>; Mon, 14 Jul 2014 14:15:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=eOJcVwemNwUB0/0LxTsb0WElTocxbLVCPd04XQfyWX0=;
        b=l4BUvFH1YaJ8vzToCQV5DS0yWH9BDpgd8PkiTbKd/9eBvwDVN9JFmFnl/NCktBUP58
         XT/ednE+C1bowPKGVvGScSWLwOyUP/GbAm4Ed+lQzQMaAAUFD4HYu/zPEhkkRxu1SR6t
         WpIbc/DLjPJGgkGWvRERGL4QDqhuITLe+HSJUUikkivKU/YR5XLXPrXCVMCjTA4qSBtE
         UZNVHJGICfmiRhfIUHvW9z5erZiH1730sJtyuCEnV1K6ANZxzjqYcvOwZb4dB8ZwcKgx
         9UC+ciQ4625GYEgr2wvNS43oOBJZWRtZzuZAVSWbZOIy+bdKqutBNBeU2QlNi3n2or7K
         hFhg==
X-Received: by 10.68.225.133 with SMTP id rk5mr10914987pbc.101.1405372548195;
        Mon, 14 Jul 2014 14:15:48 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id rz4sm49622344pab.13.2014.07.14.14.15.45
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 14:15:46 -0700 (PDT)
Content-Type: multipart/alternative; boundary="Apple-Mail=_73F5F6AC-5ABC-41B5-8C65-D3BBABCB020C"
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Catalyst dependency on Spark Core
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com>
Date: Mon, 14 Jul 2014 14:15:44 -0700
Cc: dev@spark.apache.org
Message-Id: <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com>
References: <CAJOb8btWjn+pVBRngB-gA57CQ2YRw702j+=eP79tRyyPeZgcFA@mail.gmail.com> <CALDQvdcYe1ct_LdwmkV5tfR1_rfMMxhFyu=VdZw_Y2aAYmin-Q@mail.gmail.com> <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com>
To: user@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_73F5F6AC-5ABC-41B5-8C65-D3BBABCB020C
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=us-ascii

Yeah, I'd just add a spark-util that has these things.

Matei

On Jul 14, 2014, at 1:04 PM, Michael Armbrust <michael@databricks.com> =
wrote:

> Yeah, sadly this dependency was introduced when someone consolidated =
the logging infrastructure.  However, the dependency should be very =
small and thus easy to remove, and I would like catalyst to be usable =
outside of Spark.  A pull request to make this possible would be =
welcome.
>=20
> Ideally, we'd create some sort of spark common package that has things =
like logging.  That way catalyst could depend on that, without pulling =
in all of Hadoop, etc.  Maybe others have opinions though, so I'm cc-ing =
the dev list.
>=20
>=20
> On Mon, Jul 14, 2014 at 12:21 AM, Yanbo Liang <yanbohappy@gmail.com> =
wrote:
> Make Catalyst independent of Spark is the goal of Catalyst, maybe need =
time and evolution.
> I awared that package org.apache.spark.sql.catalyst.util embraced =
org.apache.spark.util.{Utils =3D> SparkUtils},
> so that Catalyst has a dependency on Spark core.=20
> I'm not sure whether it will be replaced by other component =
independent of Spark in later release.
>=20
>=20
> 2014-07-14 11:51 GMT+08:00 Aniket Bhatnagar =
<aniket.bhatnagar@gmail.com>:
>=20
> As per the recent presentation given in Scala days =
(http://people.apache.org/~marmbrus/talks/SparkSQLScalaDays2014.pdf), it =
was mentioned that Catalyst is independent of Spark. But on inspecting =
pom.xml of sql/catalyst module, it seems it has a dependency on Spark =
Core. Any particular reason for the dependency? I would love to use =
Catalyst outside Spark
>=20
> (reposted as previous email bounced. Sorry if this is a duplicate).
>=20
>=20


--Apple-Mail=_73F5F6AC-5ABC-41B5-8C65-D3BBABCB020C--

From dev-return-8332-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 21:44:32 2014
Return-Path: <dev-return-8332-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2563611776
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 21:44:32 +0000 (UTC)
Received: (qmail 44433 invoked by uid 500); 14 Jul 2014 21:44:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44372 invoked by uid 500); 14 Jul 2014 21:44:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44360 invoked by uid 99); 14 Jul 2014 21:44:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 21:44:31 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.51 as permitted sender)
Received: from [209.85.219.51] (HELO mail-oa0-f51.google.com) (209.85.219.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 21:44:29 +0000
Received: by mail-oa0-f51.google.com with SMTP id o6so3025757oag.24
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 14:44:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=jj94E5vs0PJui+frlVLI9qTntM7Af1CTXWf3YfmS/9c=;
        b=UT1CxteZdZQOw/AaRBiQS/Iz3HPLtWEww2XQVPuvVeheGbTSLzcuSmWEyIuIauHYEF
         Mi/0QveElZ+kabRD6UiHiIcGjNqY30p/MNqrYOXvOEJDSlA/Qnf+10yaAeWsRezNqTcr
         JKHZNK6mmaepRkD1D1GqYu/EZYSd0UE2iyzDO2XzVRkbEHk0uoF7KQVvZTTRZ4U7Na9A
         y+RB10+5s1O4gWhVuwgOMoVpWzJqMLaQRNg5jGuGDU18Y8z2DBroIpEYjzgcrvyd6B2v
         SOTnRUCIKR6Z83pO2kDqxktJxbLCopXJ7kM2obRiIBxve2xVGyR66az3eB4Zmg2IASyU
         rzFA==
MIME-Version: 1.0
X-Received: by 10.182.96.71 with SMTP id dq7mr20905315obb.42.1405374244505;
 Mon, 14 Jul 2014 14:44:04 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 14:44:04 -0700 (PDT)
In-Reply-To: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
Date: Mon, 14 Jul 2014 14:44:04 -0700
Message-ID: <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Cody,

This Jstack seems truncated, would you mind giving the entire stack
trace? For the second thread, for instance, we can't see where the
lock is being acquired.

- Patrick

On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
<cody.koeninger@mediacrossing.com> wrote:
> Hi all, just wanted to give a heads up that we're seeing a reproducible
> deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
>
> If jira is a better place for this, apologies in advance - figured talking
> about it on the mailing list was friendlier than randomly (re)opening jira
> tickets.
>
> I know Gary had mentioned some issues with 1.0.1 on the mailing list, once
> we got a thread dump I wanted to follow up.
>
> The thread dump shows the deadlock occurs in the synchronized block of code
> that was changed in HadoopRDD.scala, for the Spark-1097 issue
>
> Relevant portions of the thread dump are summarized below, we can provide
> the whole dump if it's useful.
>
> Found one Java-level deadlock:
> =============================
> "Executor task launch worker-1":
>   waiting to lock monitor 0x00007f250400c520 (object 0x00000000fae7dc30, a
> org.apache.hadoop.co
> nf.Configuration),
>   which is held by "Executor task launch worker-0"
> "Executor task launch worker-0":
>   waiting to lock monitor 0x00007f2520495620 (object 0x00000000faeb4fc8, a
> java.lang.Class),
>   which is held by "Executor task launch worker-1"
>
>
> "Executor task launch worker-1":
>         at
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
>         - waiting to lock <0x00000000fae7dc30> (a
> org.apache.hadoop.conf.Configuration)
>         at
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
>         - locked <0x00000000faca6ff8> (a java.lang.Class for
> org.apache.hadoop.conf.Configurati
> on)
>         at
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
>         at
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> )
>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> Method)
>         at
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> java:57)
>         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> Method)
>         at
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> java:57)
>         at
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> sorImpl.java:45)
>         at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
>         at java.lang.Class.newInstance0(Class.java:374)
>         at java.lang.Class.newInstance(Class.java:327)
>         at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
>         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
>         at
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
>         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> org.apache.hadoop.fs.FileSystem)
>         at
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>         at
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>         at
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>         at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>         at
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>         at
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>         at
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>         at
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>         at
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>         at
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>
>
>
> ...elided...
>
>
> "Executor task launch worker-0" daemon prio=10 tid=0x0000000001e71800
> nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
>    java.lang.Thread.State: BLOCKED (on object monitor)
>         at
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
>         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
> org.apache.hadoop.fs.FileSystem)
>         at
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>         at
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>         at
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>         at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>         at
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>         at
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>         at
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>         at
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>         at
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>         at
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)

From dev-return-8333-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 21:59:31 2014
Return-Path: <dev-return-8333-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24EC1117E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 21:59:31 +0000 (UTC)
Received: (qmail 87146 invoked by uid 500); 14 Jul 2014 21:59:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87082 invoked by uid 500); 14 Jul 2014 21:59:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87071 invoked by uid 99); 14 Jul 2014 21:59:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 21:59:29 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wibenton@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 21:59:24 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s6ELx3Hj029004
	for <dev@spark.apache.org>; Mon, 14 Jul 2014 17:59:03 -0400
Date: Mon, 14 Jul 2014 17:59:01 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <643342387.16800517.1405375141069.JavaMail.zimbra@redhat.com>
In-Reply-To: <674ECB29-553B-4927-A64B-AFC6E5F97FED@gmail.com>
References: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com> <674ECB29-553B-4927-A64B-AFC6E5F97FED@gmail.com>
Subject: Re: Profiling Spark tests with YourKit (or something else)
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF30 (Mac)/8.0.6_GA_5922)
Thread-Topic: Profiling Spark tests with YourKit (or something else)
Thread-Index: 64w2/kx0FwAAdYnYr5efVqr4N28y3A==
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks, Matei; I have also had some success with jmap and friends and will probably just stick with them!


best,
wb


----- Original Message -----
> From: "Matei Zaharia" <matei.zaharia@gmail.com>
> To: dev@spark.apache.org
> Sent: Monday, July 14, 2014 1:02:04 PM
> Subject: Re: Profiling Spark tests with YourKit (or something else)
> 
> I haven't seen issues using the JVM's own tools (jstack, jmap, hprof and
> such), so maybe there's a problem in YourKit or in your release of the JVM.
> Otherwise I'd suggest increasing the heap size of the unit tests a bit (you
> can do this in the SBT build file). Maybe they are very close to full and
> profiling pushes them over the edge.
> 
> Matei
> 
> On Jul 14, 2014, at 9:51 AM, Will Benton <willb@redhat.com> wrote:
> 
> > Hi all,
> > 
> > I've been evaluating YourKit and would like to profile the heap and CPU
> > usage of certain tests from the Spark test suite.  In particular, I'm very
> > interested in tracking heap usage by allocation site.  Unfortunately, I
> > get a lot of crashes running Spark tests with profiling (and thus
> > allocation-site tracking) enabled in YourKit; just using the sampler works
> > fine, but it appears that enabling the profiler breaks Utils.getCallSite.
> > 
> > Is there a way to make this combination work?  If not, what are people
> > using to understand the memory and CPU behavior of Spark and Spark apps?
> > 
> > 
> > thanks,
> > wb
> 
> 

From dev-return-8334-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:08:44 2014
Return-Path: <dev-return-8334-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9B9D1184E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:08:44 +0000 (UTC)
Received: (qmail 22299 invoked by uid 500); 14 Jul 2014 22:08:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22252 invoked by uid 500); 14 Jul 2014 22:08:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22240 invoked by uid 99); 14 Jul 2014 22:08:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:08:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.176 as permitted sender)
Received: from [209.85.216.176] (HELO mail-qc0-f176.google.com) (209.85.216.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:08:39 +0000
Received: by mail-qc0-f176.google.com with SMTP id i17so1613145qcy.35
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:08:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ZUj3xdEIgbXJegkgLqL7eu9lY2mcq6+l+3s5iouqtZE=;
        b=CxWnK492nQ3TMjoZW6xvRlAkGR8teRTYxhOK5HXKosxDQ/8zpxhbMa0PqrBW0a0dhb
         iz3gtAcrf1mqclONOF+ik8eyVL20AkW7+fFWrfvMx0YoARgbo5ZwnPB7I+ZPB7phZaft
         LKYF4IOiBDE65E/AQUEJvoSdIR2Uwn6/v3SdHPkgYwULgvtSTIK0Wirag22Xd+Z0OJzv
         pF8YrNZY6BtdN+5x9pWzndZ567w6a38ezr7Z6mvz7DXvYBXNKE+mJ98JaY9kXfsSrSHA
         S/BCTjXAHcsg/D7q6gA3x3eX6jkspf2CvmnV9oiioHQoleZGayjYEmvbHiyHcpBsSnAi
         CqFg==
X-Received: by 10.224.41.65 with SMTP id n1mr27427243qae.75.1405375699002;
 Mon, 14 Jul 2014 15:08:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 14 Jul 2014 15:07:58 -0700 (PDT)
In-Reply-To: <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
 <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Jul 2014 15:07:58 -0700
Message-ID: <CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc89fc50abef04fe2e87d0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc89fc50abef04fe2e87d0
Content-Type: text/plain; charset=UTF-8

The full jstack would still be useful, but our current working theory is
that this is due to the fact that Configuration#loadDefaults goes through
every Configuration object that was ever created (via
Configuration.REGISTRY) and locks it, thus introducing a dependency from
new Configuration to old, otherwise unrelated, Configuration objects that
our locking did not anticipate.

I have created https://github.com/apache/spark/pull/1409 to hopefully fix
this bug.


On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Cody,
>
> This Jstack seems truncated, would you mind giving the entire stack
> trace? For the second thread, for instance, we can't see where the
> lock is being acquired.
>
> - Patrick
>
> On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> <cody.koeninger@mediacrossing.com> wrote:
> > Hi all, just wanted to give a heads up that we're seeing a reproducible
> > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> >
> > If jira is a better place for this, apologies in advance - figured
> talking
> > about it on the mailing list was friendlier than randomly (re)opening
> jira
> > tickets.
> >
> > I know Gary had mentioned some issues with 1.0.1 on the mailing list,
> once
> > we got a thread dump I wanted to follow up.
> >
> > The thread dump shows the deadlock occurs in the synchronized block of
> code
> > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> >
> > Relevant portions of the thread dump are summarized below, we can provide
> > the whole dump if it's useful.
> >
> > Found one Java-level deadlock:
> > =============================
> > "Executor task launch worker-1":
> >   waiting to lock monitor 0x00007f250400c520 (object 0x00000000fae7dc30,
> a
> > org.apache.hadoop.co
> > nf.Configuration),
> >   which is held by "Executor task launch worker-0"
> > "Executor task launch worker-0":
> >   waiting to lock monitor 0x00007f2520495620 (object 0x00000000faeb4fc8,
> a
> > java.lang.Class),
> >   which is held by "Executor task launch worker-1"
> >
> >
> > "Executor task launch worker-1":
> >         at
> >
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> >         - waiting to lock <0x00000000fae7dc30> (a
> > org.apache.hadoop.conf.Configuration)
> >         at
> >
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> > org.apache.hadoop.conf.Configurati
> > on)
> >         at
> >
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> >         at
> >
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> > )
> >         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > Method)
> >         at
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > java:57)
> >         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > Method)
> >         at
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > java:57)
> >         at
> >
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> > sorImpl.java:45)
> >         at
> java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> >         at java.lang.Class.newInstance0(Class.java:374)
> >         at java.lang.Class.newInstance(Class.java:327)
> >         at
> java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> >         at
> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> > org.apache.hadoop.fs.FileSystem)
> >         at
> > org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >         at
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >         at
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >         at
> org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >         at
> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >
> >
> >
> > ...elided...
> >
> >
> > "Executor task launch worker-0" daemon prio=10 tid=0x0000000001e71800
> > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> >    java.lang.Thread.State: BLOCKED (on object monitor)
> >         at
> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
> > org.apache.hadoop.fs.FileSystem)
> >         at
> > org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >         at
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >         at
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >         at
> org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >         at
> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>

--047d7bdc89fc50abef04fe2e87d0--

From dev-return-8335-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:21:59 2014
Return-Path: <dev-return-8335-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D1545118DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:21:59 +0000 (UTC)
Received: (qmail 50484 invoked by uid 500); 14 Jul 2014 22:21:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50428 invoked by uid 500); 14 Jul 2014 22:21:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50414 invoked by uid 99); 14 Jul 2014 22:21:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:21:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.216.42 as permitted sender)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:21:55 +0000
Received: by mail-qa0-f42.google.com with SMTP id j15so3942975qaq.1
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:21:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=TFPmFEJtntARFXuiJc/MfL9GT9b+o2lN+iS5XnYa12I=;
        b=eV5cnCM0DNmxa/g2u2STSY5jxbKjyJgKzt4xpXn/rj57qoLbRuctjEoWb5endcU80u
         hMfpS40BFbIppuTRJiIfokQ/Nt6rGmGuP6CUVDVddTv5HU2gxhOSV8XIO/ewYIkKwZ8i
         lmxgY3pHPhKJJDKbol3oxALFcoT5NBp/t6rCT90zT35e4l9B6r/LZLFhpJFChWbaX6a5
         HkTDOd1Q+aZZraaD95+6Sul2wuDG9ehu4/aKPqDoK25kGBUf09sjeJk0jJPEuwVlVqBo
         FuQ9zkZeRuUP+Mx+twPaVHeF3FIFZE9QyHE0s41afnpm6C94Q7oUjEA+RtNO6mLzeYgR
         Em8A==
X-Received: by 10.224.34.73 with SMTP id k9mr28233742qad.11.1405376491013;
 Mon, 14 Jul 2014 15:21:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 14 Jul 2014 15:21:10 -0700 (PDT)
In-Reply-To: <643342387.16800517.1405375141069.JavaMail.zimbra@redhat.com>
References: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com>
 <674ECB29-553B-4927-A64B-AFC6E5F97FED@gmail.com> <643342387.16800517.1405375141069.JavaMail.zimbra@redhat.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Jul 2014 15:21:10 -0700
Message-ID: <CANGvG8rhCbmNUGKJbSh7o8n023BVoCAV3OUTexpmEtwf90jm5Q@mail.gmail.com>
Subject: Re: Profiling Spark tests with YourKit (or something else)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2047c85d3ba04fe2eb6b3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2047c85d3ba04fe2eb6b3
Content-Type: text/plain; charset=UTF-8

Out of curiosity, what problems are you seeing with Utils.getCallSite?


On Mon, Jul 14, 2014 at 2:59 PM, Will Benton <willb@redhat.com> wrote:

> Thanks, Matei; I have also had some success with jmap and friends and will
> probably just stick with them!
>
>
> best,
> wb
>
>
> ----- Original Message -----
> > From: "Matei Zaharia" <matei.zaharia@gmail.com>
> > To: dev@spark.apache.org
> > Sent: Monday, July 14, 2014 1:02:04 PM
> > Subject: Re: Profiling Spark tests with YourKit (or something else)
> >
> > I haven't seen issues using the JVM's own tools (jstack, jmap, hprof and
> > such), so maybe there's a problem in YourKit or in your release of the
> JVM.
> > Otherwise I'd suggest increasing the heap size of the unit tests a bit
> (you
> > can do this in the SBT build file). Maybe they are very close to full and
> > profiling pushes them over the edge.
> >
> > Matei
> >
> > On Jul 14, 2014, at 9:51 AM, Will Benton <willb@redhat.com> wrote:
> >
> > > Hi all,
> > >
> > > I've been evaluating YourKit and would like to profile the heap and CPU
> > > usage of certain tests from the Spark test suite.  In particular, I'm
> very
> > > interested in tracking heap usage by allocation site.  Unfortunately, I
> > > get a lot of crashes running Spark tests with profiling (and thus
> > > allocation-site tracking) enabled in YourKit; just using the sampler
> works
> > > fine, but it appears that enabling the profiler breaks
> Utils.getCallSite.
> > >
> > > Is there a way to make this combination work?  If not, what are people
> > > using to understand the memory and CPU behavior of Spark and Spark
> apps?
> > >
> > >
> > > thanks,
> > > wb
> >
> >
>

--001a11c2047c85d3ba04fe2eb6b3--

From dev-return-8336-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:29:24 2014
Return-Path: <dev-return-8336-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43E8411916
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:29:24 +0000 (UTC)
Received: (qmail 69823 invoked by uid 500); 14 Jul 2014 22:29:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69760 invoked by uid 500); 14 Jul 2014 22:29:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69746 invoked by uid 99); 14 Jul 2014 22:29:23 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:29:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nravi@cloudera.com designates 209.85.216.170 as permitted sender)
Received: from [209.85.216.170] (HELO mail-qc0-f170.google.com) (209.85.216.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:29:20 +0000
Received: by mail-qc0-f170.google.com with SMTP id c9so4229223qcz.15
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:28:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=sjTqt7foXGjAfhNZ4BxbHn0ax0m5GpWg6X+g3Gs0hXc=;
        b=f1cq+uHWal/5PNwGqLtMvMI+OXwAdvDY4LEjNxzyxQEMVQ5h+Dm3VvnBM5ITwQsPsn
         wsjDsuWb51Uhc6jbcL+UNOQYuwF68wmiKb1hX3oZPoDy0G4DWWne9wJH+V0wxCzQTjT9
         ScQb4RaMNrHWffgr7M0sIzWdeT0Dz9b6hN0n0LU/TU+NSl70l11Hs5W0R9BaFcCJWoZB
         JP9Mpi1fM0bj/2Fhw3ekZkSLOHOU6+9u60H/P9PuzdbdCK90le5nUjU6OJ0E32hTcw/L
         qrXmEx1sQMF0pPKhoUhiOJak/KVvHutN8YbJzFVEulYXSmrrOOW6Fbsqmcoi07S7T/KW
         EEbQ==
X-Gm-Message-State: ALoCoQkE5TnV31rUHSwyXn2M0srq8UKfwdrl4mUivjdM8tSWS3682rVTHM6adwcDYWjPrHIir52T
X-Received: by 10.140.40.84 with SMTP id w78mr1763700qgw.112.1405376936121;
 Mon, 14 Jul 2014 15:28:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.31.4 with HTTP; Mon, 14 Jul 2014 15:28:36 -0700 (PDT)
In-Reply-To: <CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
 <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com> <CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Mon, 14 Jul 2014 15:28:36 -0700
Message-ID: <CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c122ba0da7d404fe2ed12f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c122ba0da7d404fe2ed12f
Content-Type: text/plain; charset=UTF-8

Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object would
help. I suspect we will start seeing the ConcurrentModificationException
again. The right fix has gone into Hadoop through 10456. Unfortunately, I
don't have any bright ideas on how to synchronize this at the Spark level
without the risk of deadlocks.


On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <ilikerps@gmail.com> wrote:

> The full jstack would still be useful, but our current working theory is
> that this is due to the fact that Configuration#loadDefaults goes through
> every Configuration object that was ever created (via
> Configuration.REGISTRY) and locks it, thus introducing a dependency from
> new Configuration to old, otherwise unrelated, Configuration objects that
> our locking did not anticipate.
>
> I have created https://github.com/apache/spark/pull/1409 to hopefully fix
> this bug.
>
>
> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Hey Cody,
> >
> > This Jstack seems truncated, would you mind giving the entire stack
> > trace? For the second thread, for instance, we can't see where the
> > lock is being acquired.
> >
> > - Patrick
> >
> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> > <cody.koeninger@mediacrossing.com> wrote:
> > > Hi all, just wanted to give a heads up that we're seeing a reproducible
> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> > >
> > > If jira is a better place for this, apologies in advance - figured
> > talking
> > > about it on the mailing list was friendlier than randomly (re)opening
> > jira
> > > tickets.
> > >
> > > I know Gary had mentioned some issues with 1.0.1 on the mailing list,
> > once
> > > we got a thread dump I wanted to follow up.
> > >
> > > The thread dump shows the deadlock occurs in the synchronized block of
> > code
> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> > >
> > > Relevant portions of the thread dump are summarized below, we can
> provide
> > > the whole dump if it's useful.
> > >
> > > Found one Java-level deadlock:
> > > =============================
> > > "Executor task launch worker-1":
> > >   waiting to lock monitor 0x00007f250400c520 (object
> 0x00000000fae7dc30,
> > a
> > > org.apache.hadoop.co
> > > nf.Configuration),
> > >   which is held by "Executor task launch worker-0"
> > > "Executor task launch worker-0":
> > >   waiting to lock monitor 0x00007f2520495620 (object
> 0x00000000faeb4fc8,
> > a
> > > java.lang.Class),
> > >   which is held by "Executor task launch worker-1"
> > >
> > >
> > > "Executor task launch worker-1":
> > >         at
> > >
> >
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> > >         - waiting to lock <0x00000000fae7dc30> (a
> > > org.apache.hadoop.conf.Configuration)
> > >         at
> > >
> >
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> > > org.apache.hadoop.conf.Configurati
> > > on)
> > >         at
> > >
> >
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> > >         at
> > >
> >
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> > > )
> > >         at
> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > > Method)
> > >         at
> > >
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > > java:57)
> > >         at
> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > > Method)
> > >         at
> > >
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > > java:57)
> > >         at
> > >
> >
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> > > sorImpl.java:45)
> > >         at
> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> > >         at java.lang.Class.newInstance0(Class.java:374)
> > >         at java.lang.Class.newInstance(Class.java:327)
> > >         at
> > java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> > >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> > >         at
> > > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> > > org.apache.hadoop.fs.FileSystem)
> > >         at
> > >
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> > >         at
> > > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> > >         at
> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> > >         at
> > > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> > >         at
> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> > >         at
> > > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> > >         at
> > >
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> > >         at
> > >
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> > >         at
> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >         at
> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >         at
> > >
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> > >
> > >
> > >
> > > ...elided...
> > >
> > >
> > > "Executor task launch worker-0" daemon prio=10 tid=0x0000000001e71800
> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> > >    java.lang.Thread.State: BLOCKED (on object monitor)
> > >         at
> > > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> > >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
> > > org.apache.hadoop.fs.FileSystem)
> > >         at
> > >
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> > >         at
> > > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> > >         at
> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> > >         at
> > > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> > >         at
> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> > >         at
> > > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> > >         at
> > >
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> > >         at
> > >
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> > >         at
> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >         at
> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >         at
> > >
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >
>

--001a11c122ba0da7d404fe2ed12f--

From dev-return-8337-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:30:58 2014
Return-Path: <dev-return-8337-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 661F111924
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:30:58 +0000 (UTC)
Received: (qmail 75849 invoked by uid 500); 14 Jul 2014 22:30:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75789 invoked by uid 500); 14 Jul 2014 22:30:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75773 invoked by uid 99); 14 Jul 2014 22:30:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:30:57 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of stephen.haberman@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:30:53 +0000
Received: by mail-oa0-f45.google.com with SMTP id i7so4066159oag.32
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:30:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:subject:message-id:in-reply-to:references
         :mime-version:content-type:content-transfer-encoding;
        bh=iL/ySZ3L1QfSTpgdYvJ6/VgTTlRwQPw5YTXwqS2wKzY=;
        b=ZySuv7i828QeNXP0jmRhq71ouXDFWR/8k0dVQgMGR5sCSNlNNfyYMCOprZ4jxIJQOI
         l8xEvDDDn3H9XkfXujR9lTfzLcSkQ8EAr1vWE/3oDv1Q22uBt2hGncoWCBGvYu3Nsf4e
         3ml6DgdiU+2auF71J51TsOlbQzVG3M7okU+3Habs5mebPED1Me4NbJrNPgJmiS9e4wHk
         q6EU/gggFPx+TAwK7BxaH+b+uc6d16km2TJqwCnNfbIyhNiPBOnW8G7oOgRYTQphaUKH
         FYN6a5pXuY9qvIbWRubW+PbIXGO0qwNNMHBsSEJgSHzVoAkJN2h+xEzAKs9o6S8ElGER
         ZG5g==
X-Received: by 10.60.124.162 with SMTP id mj2mr21148840oeb.22.1405377028782;
        Mon, 14 Jul 2014 15:30:28 -0700 (PDT)
Received: from sh9 (wsip-184-187-11-226.om.om.cox.net. [184.187.11.226])
        by mx.google.com with ESMTPSA id k9sm10207055obe.23.2014.07.14.15.30.28
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 15:30:28 -0700 (PDT)
Date: Mon, 14 Jul 2014 17:30:27 -0500
From: Stephen Haberman <stephen.haberman@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: better compression codecs for shuffle blocks?
Message-ID: <20140714173027.6e60b16f@sh9>
In-Reply-To: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
X-Mailer: Claws Mail 3.10.1 (GTK+ 2.24.23; x86_64-pc-linux-gnu)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org


Just a comment from the peanut gallery, but these buffers are a real
PITA for us as well. Probably 75% of our non-user-error job failures
are related to them.

Just naively, what about not doing compression on the fly? E.g. during
the shuffle just write straight to disk, uncompressed?

For us, we always have plenty of disk space, and if you're concerned
about network transmission, you could add a separate compress step
after the blocks have been written to disk, but before being sent over
the wire.

Granted, IANAE, so perhaps this is a bad idea; either way, awesome to
see work in this area!

- Stephen


From dev-return-8338-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:52:10 2014
Return-Path: <dev-return-8338-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 427D5119AE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:52:10 +0000 (UTC)
Received: (qmail 23209 invoked by uid 500); 14 Jul 2014 22:52:09 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23153 invoked by uid 500); 14 Jul 2014 22:52:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23142 invoked by uid 99); 14 Jul 2014 22:52:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:52:09 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:52:04 +0000
Received: by mail-qc0-f181.google.com with SMTP id w7so1289215qcr.40
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:51:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=r4Uk/oV53qRnl2LBtye4Nc7G9O2uGqDNAOew7VlXeWg=;
        b=h6q9doJ/ouvS/DjFhQATuxkVixh2agZpb01vOfTKfTiEtHbFMDzCMdHzacIrOr8fae
         i+qTxRx1vRB3pXT3t4hvLRbAYM45eDyLPjIYLdI4973uJvI7I5yYdJ0510PLNtZFFUTk
         lvECdqv+SixCWPeCagsW9zbTCpVaAO+9bf5yHpyY2g1r1qWYhSOnLTNFiVtK8LkrdOFG
         Uhs5K9qnSResPjb6JwWyVpoe/VTh3jDjzyN87j/heb+XQCmLTnNb836to/ZDHpXWwpo2
         v5z05yFXEANrh1Soj9OmgnujmvUYhr3XZjBharFXsWzAht7ixKVDlPXEgiqgVr33M8mH
         FJZw==
X-Gm-Message-State: ALoCoQlu2Uty9QLUxDOuYdH6ZYswEckk4JcM8BBaxtL/UeuVQfDinfqvoS3QhBlvgFUOTXwreviI
MIME-Version: 1.0
X-Received: by 10.224.111.196 with SMTP id t4mr26502173qap.63.1405378303322;
 Mon, 14 Jul 2014 15:51:43 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Mon, 14 Jul 2014 15:51:43 -0700 (PDT)
In-Reply-To: <20140714173027.6e60b16f@sh9>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
	<20140714173027.6e60b16f@sh9>
Date: Mon, 14 Jul 2014 15:51:43 -0700
Message-ID: <CACBYxKJqXnMFmxHaPYgHwSVbtrSbYuX5Sq21Stw8+PdrtjrSXQ@mail.gmail.com>
Subject: Re: better compression codecs for shuffle blocks?
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Reynold Xin <rxin@databricks.com>
Content-Type: multipart/alternative; boundary=001a11c2d6008b8f8a04fe2f2271
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d6008b8f8a04fe2f2271
Content-Type: text/plain; charset=UTF-8

Stephen,
Often the shuffle is bound by writes to disk, so even if disks have enough
space to store the uncompressed data, the shuffle can complete faster by
writing less data.

Reynold,
This isn't a big help in the short term, but if we switch to a sort-based
shuffle, we'll only need a single LZFOutputStream per map task.


On Mon, Jul 14, 2014 at 3:30 PM, Stephen Haberman <
stephen.haberman@gmail.com> wrote:

>
> Just a comment from the peanut gallery, but these buffers are a real
> PITA for us as well. Probably 75% of our non-user-error job failures
> are related to them.
>
> Just naively, what about not doing compression on the fly? E.g. during
> the shuffle just write straight to disk, uncompressed?
>
> For us, we always have plenty of disk space, and if you're concerned
> about network transmission, you could add a separate compress step
> after the blocks have been written to disk, but before being sent over
> the wire.
>
> Granted, IANAE, so perhaps this is a bad idea; either way, awesome to
> see work in this area!
>
> - Stephen
>
>

--001a11c2d6008b8f8a04fe2f2271--

From dev-return-8339-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:55:17 2014
Return-Path: <dev-return-8339-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F652119C3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:55:17 +0000 (UTC)
Received: (qmail 34919 invoked by uid 500); 14 Jul 2014 22:55:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34864 invoked by uid 500); 14 Jul 2014 22:55:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34852 invoked by uid 99); 14 Jul 2014 22:55:16 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:55:16 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.182 as permitted sender)
Received: from [209.85.192.182] (HELO mail-pd0-f182.google.com) (209.85.192.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:55:09 +0000
Received: by mail-pd0-f182.google.com with SMTP id fp1so3163341pdb.13
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:54:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=WXtPRRqstl0kNioCLM3GevLR7STcqpPjaZAL5NTEii0=;
        b=PSY/gRA5tPSovEBwhXI6ztWyrT4mej0fPIK867ZbU7Wttl6yWiQEmcXxZcURGtiUrq
         L6Bmb5fR91KJU0hNo1Geqzer5Kntm8z5l5FY8uQ8NGByFAdVOajhGpdzJBJqgLBcwM2a
         Rt2vbKZxg2eticgprrsGQfy8Gnr6hE6gFM6rq4GqjrTIMdpeCivcUkSRQAG9o0hnjpS4
         cgu1y1AFCWkHTSV6Mv/fS0nDogZzmiIm9JMvq54g40PcMJJnbF4xKHF/kPWEz9XaiM9s
         gkdDrdkDh3bzQCRiwuz62mp/A1gnmaH4DBR0NRJh1BaS6gDUWI4oM3DoC0IOp1EimuU6
         70/A==
X-Received: by 10.67.30.130 with SMTP id ke2mr19223225pad.44.1405378489670;
        Mon, 14 Jul 2014 15:54:49 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id tr2sm50380735pab.26.2014.07.14.15.54.45
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 15:54:46 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: better compression codecs for shuffle blocks?
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <20140714173027.6e60b16f@sh9>
Date: Mon, 14 Jul 2014 15:54:45 -0700
Cc: Reynold Xin <rxin@databricks.com>
Content-Transfer-Encoding: quoted-printable
Message-Id: <1C3442B5-637F-4DA0-BD98-F1F95E2AEFD9@gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com> <20140714173027.6e60b16f@sh9>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

You can actually turn off shuffle compression by setting =
spark.shuffle.compress to false. Try that out, there will still be some =
buffers for the various OutputStreams, but they should be smaller.

Matei

On Jul 14, 2014, at 3:30 PM, Stephen Haberman =
<stephen.haberman@gmail.com> wrote:

>=20
> Just a comment from the peanut gallery, but these buffers are a real
> PITA for us as well. Probably 75% of our non-user-error job failures
> are related to them.
>=20
> Just naively, what about not doing compression on the fly? E.g. during
> the shuffle just write straight to disk, uncompressed?
>=20
> For us, we always have plenty of disk space, and if you're concerned
> about network transmission, you could add a separate compress step
> after the blocks have been written to disk, but before being sent over
> the wire.
>=20
> Granted, IANAE, so perhaps this is a bad idea; either way, awesome to
> see work in this area!
>=20
> - Stephen
>=20


From dev-return-8340-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:56:48 2014
Return-Path: <dev-return-8340-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4CC68119D4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:56:48 +0000 (UTC)
Received: (qmail 39187 invoked by uid 500); 14 Jul 2014 22:56:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39129 invoked by uid 500); 14 Jul 2014 22:56:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39114 invoked by uid 99); 14 Jul 2014 22:56:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:56:47 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:56:45 +0000
Received: by mail-qa0-f43.google.com with SMTP id w8so3852581qac.16
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:56:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=uHrE5W9LG3WTBOhSXBDgx6x+PVrYSc2Ikh6qRnBI8bc=;
        b=lHFXALpvxmvAP3LPTz1KRBNNwjBpQunmuCtSTtfINQI7WKqb6D9a/Y4l7/Q1cUpEJA
         UhQGssc+vlWlb9eSXGrviFL+S81KRxuFFTiW9S+m9S1s2IET8r/6wJIy9kZ6Q97DlC6H
         6ztmQPIogY31xnChFwmmXYe9Fc0d96gc1GiPUb4sRDI9k8ZHNlznG7Au6yuhwyJMz1Md
         LPaXVldANSkTwcG6d30e1cvNTJO63jZXJC4B12t2VBmUhrU373I01aaXpCo6i+aGlhg7
         jB3yaXdEhIMwg+M7zfKsVlb3vTERRTRzVkdvbWSBBHd9wAWtmoZkgg1X9paaNdM3R5ih
         CXZg==
X-Gm-Message-State: ALoCoQlmqt7p6WRhwn0xtPRBAER+S3QSsoRTBet3GEOGjODfqNHQrWMgAqd9PF6HxVLbvvUd8hu6
X-Received: by 10.140.23.198 with SMTP id 64mr29712383qgp.84.1405378579810;
 Mon, 14 Jul 2014 15:56:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.209.3 with HTTP; Mon, 14 Jul 2014 15:55:59 -0700 (PDT)
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 14 Jul 2014 15:55:59 -0700
Message-ID: <CAAswR-7_zWT634JUT=xwsCaVi8qh8MPgRaLe_xsp1UxUER7CjA@mail.gmail.com>
Subject: Change when loading/storing String data using Parquet
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c12e6a06668a04fe2f3343
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12e6a06668a04fe2f3343
Content-Type: text/plain; charset=UTF-8

I just wanted to send out a quick note about a change in the handling of
strings when loading / storing data using parquet and Spark SQL.  Before,
Spark SQL did not support binary data in Parquet, so all binary blobs were
implicitly treated as Strings.  9fe693
<https://github.com/apache/spark/commit/9fe693b5b6ed6af34ee1e800ab89c8a11991ea38>
fixes
this limitation by adding support for binary data.

However, data written out with a prior version of Spark SQL will be missing
the annotation telling us to interpret a given column as a String, so old
string data will now be loaded as binary data.  If you would like to use
the data as a string, you will need to add a CAST to convert the datatype.

New string data written out after this change, will correctly be loaded in
as a string as now we will include an annotation about the desired type.
 Additionally, this should now interoperate correctly with other systems
that write Parquet data (hive, thrift, etc).

Michael

--001a11c12e6a06668a04fe2f3343--

From dev-return-8341-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 22:58:12 2014
Return-Path: <dev-return-8341-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 811D3119D7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 22:58:12 +0000 (UTC)
Received: (qmail 41609 invoked by uid 500); 14 Jul 2014 22:58:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41550 invoked by uid 500); 14 Jul 2014 22:58:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41525 invoked by uid 99); 14 Jul 2014 22:58:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:58:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 22:58:07 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so5004231oag.19
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 15:57:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=kxFWeV6YXdZ594GuGIcIrXhXlPlQtZyXOEt80k5jb2c=;
        b=jkRvI7OhZFUQ/1Wu4YenI7ZhcbQJUC3t6GldJ/xb8oalOR2+KRC7PyW/RPdxX4SEQH
         6wl6X4soSqIb0NdUD9TB1qfQv+kibdIysuU0cTUD2mv00tvnGq9A2JkaOPdpQly9JT/0
         R2rhK2WFMpo3ovF4AU5X0zMooidPTT6p6dFFSwZZXwCzShw2LREYDqnjQXL+4Aplkb/R
         nQu4ytOdoEeQwjklnsM8H4g2Al9WeCibL82UhVDVOpC0/OFESF5UNi7c8wQxHh1bOmqb
         sYSZbHNzbPZMXyG8fkQb1x9D7tpg/RgZuxfL//Od/yO/duk+SrMejhk2Qze5947Pvlso
         IHyQ==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr6382062obc.75.1405378666921;
 Mon, 14 Jul 2014 15:57:46 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 15:57:46 -0700 (PDT)
In-Reply-To: <CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
	<CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
Date: Mon, 14 Jul 2014 15:57:46 -0700
Message-ID: <CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Nishkam,

Aaron's fix should prevent two concurrent accesses to getJobConf (and
the Hadoop code therein). But if there is code elsewhere that tries to
mutate the configuration, then I could see how we might still have the
ConcurrentModificationException.

I looked at your patch for HADOOP-10456 and the only example you give
is of the data being accessed inside of getJobConf. Is it accessed
somewhere else too from Spark that you are aware of?

https://issues.apache.org/jira/browse/HADOOP-10456

- Patrick

On Mon, Jul 14, 2014 at 3:28 PM, Nishkam Ravi <nravi@cloudera.com> wrote:
> Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object would
> help. I suspect we will start seeing the ConcurrentModificationException
> again. The right fix has gone into Hadoop through 10456. Unfortunately, I
> don't have any bright ideas on how to synchronize this at the Spark level
> without the risk of deadlocks.
>
>
> On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
>
>> The full jstack would still be useful, but our current working theory is
>> that this is due to the fact that Configuration#loadDefaults goes through
>> every Configuration object that was ever created (via
>> Configuration.REGISTRY) and locks it, thus introducing a dependency from
>> new Configuration to old, otherwise unrelated, Configuration objects that
>> our locking did not anticipate.
>>
>> I have created https://github.com/apache/spark/pull/1409 to hopefully fix
>> this bug.
>>
>>
>> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > Hey Cody,
>> >
>> > This Jstack seems truncated, would you mind giving the entire stack
>> > trace? For the second thread, for instance, we can't see where the
>> > lock is being acquired.
>> >
>> > - Patrick
>> >
>> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
>> > <cody.koeninger@mediacrossing.com> wrote:
>> > > Hi all, just wanted to give a heads up that we're seeing a reproducible
>> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
>> > >
>> > > If jira is a better place for this, apologies in advance - figured
>> > talking
>> > > about it on the mailing list was friendlier than randomly (re)opening
>> > jira
>> > > tickets.
>> > >
>> > > I know Gary had mentioned some issues with 1.0.1 on the mailing list,
>> > once
>> > > we got a thread dump I wanted to follow up.
>> > >
>> > > The thread dump shows the deadlock occurs in the synchronized block of
>> > code
>> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
>> > >
>> > > Relevant portions of the thread dump are summarized below, we can
>> provide
>> > > the whole dump if it's useful.
>> > >
>> > > Found one Java-level deadlock:
>> > > =============================
>> > > "Executor task launch worker-1":
>> > >   waiting to lock monitor 0x00007f250400c520 (object
>> 0x00000000fae7dc30,
>> > a
>> > > org.apache.hadoop.co
>> > > nf.Configuration),
>> > >   which is held by "Executor task launch worker-0"
>> > > "Executor task launch worker-0":
>> > >   waiting to lock monitor 0x00007f2520495620 (object
>> 0x00000000faeb4fc8,
>> > a
>> > > java.lang.Class),
>> > >   which is held by "Executor task launch worker-1"
>> > >
>> > >
>> > > "Executor task launch worker-1":
>> > >         at
>> > >
>> >
>> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
>> > >         - waiting to lock <0x00000000fae7dc30> (a
>> > > org.apache.hadoop.conf.Configuration)
>> > >         at
>> > >
>> >
>> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
>> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
>> > > org.apache.hadoop.conf.Configurati
>> > > on)
>> > >         at
>> > >
>> >
>> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
>> > >         at
>> > >
>> >
>> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
>> > > )
>> > >         at
>> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> > > Method)
>> > >         at
>> > >
>> >
>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
>> > > java:57)
>> > >         at
>> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> > > Method)
>> > >         at
>> > >
>> >
>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
>> > > java:57)
>> > >         at
>> > >
>> >
>> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
>> > > sorImpl.java:45)
>> > >         at
>> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
>> > >         at java.lang.Class.newInstance0(Class.java:374)
>> > >         at java.lang.Class.newInstance(Class.java:327)
>> > >         at
>> > java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
>> > >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
>> > >         at
>> > > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
>> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
>> > > org.apache.hadoop.fs.FileSystem)
>> > >         at
>> > >
>> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>> > >         at
>> > > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>> > >         at
>> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>> > >         at
>> > > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>> > >         at
>> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>> > >         at
>> > > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>> > >         at
>> > >
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>> > >         at
>> > >
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>> > >         at
>> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >         at
>> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >         at
>> > >
>> >
>> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>> > >
>> > >
>> > >
>> > > ...elided...
>> > >
>> > >
>> > > "Executor task launch worker-0" daemon prio=10 tid=0x0000000001e71800
>> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
>> > >    java.lang.Thread.State: BLOCKED (on object monitor)
>> > >         at
>> > > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
>> > >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
>> > > org.apache.hadoop.fs.FileSystem)
>> > >         at
>> > >
>> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>> > >         at
>> > > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>> > >         at
>> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>> > >         at
>> > > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>> > >         at
>> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>> > >         at
>> > > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>> > >         at
>> > >
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>> > >         at
>> > >
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>> > >         at
>> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >         at
>> > > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >         at
>> > >
>> >
>> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>> >
>>

From dev-return-8342-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:06:51 2014
Return-Path: <dev-return-8342-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2251C11A2E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:06:51 +0000 (UTC)
Received: (qmail 56394 invoked by uid 500); 14 Jul 2014 23:06:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 56331 invoked by uid 500); 14 Jul 2014 23:06:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 56314 invoked by uid 99); 14 Jul 2014 23:06:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:06:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.216.172 as permitted sender)
Received: from [209.85.216.172] (HELO mail-qc0-f172.google.com) (209.85.216.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:06:47 +0000
Received: by mail-qc0-f172.google.com with SMTP id l6so4336575qcy.31
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:06:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=+8w97mKlNp75nCDyfLfZcp7POJ6cT4wOLd9eaHtiGyA=;
        b=m+10KrCAFd/waVqmMOH7mjBWDHetMr3PhIhAIEvaFvkzj28Mpyh5C7a0j1509dkBkX
         FPWO5+S6mLKX6trlNbf2EEzFjP0nPoLaD/ERlv0Gv0gqeBPyprwnuZZRRgoEUhC+/PsF
         +epNcxIrOk2jki7X9xOvjEEBfeTqRO7983vQyNLtriPk6FMQJuaZB+BCLTsUqFwZSuwA
         3ObgoeOW2TY8PUNq7D+GAh4JtlcplMFY+Gk/tLE8AHwKuYFDn+FhB2q9KcXM5ZeCSFH4
         ht0b0Mw7lyMYVXPuVgkLnDuBpmgJC3q99SgWlNSWEXGUVx4yNbI1wrhavcmFQAB465pA
         KwaQ==
MIME-Version: 1.0
X-Received: by 10.140.51.37 with SMTP id t34mr27996820qga.50.1405379182910;
 Mon, 14 Jul 2014 16:06:22 -0700 (PDT)
Received: by 10.140.49.236 with HTTP; Mon, 14 Jul 2014 16:06:22 -0700 (PDT)
In-Reply-To: <CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
	<CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
	<CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
Date: Mon, 14 Jul 2014 19:06:22 -0400
Message-ID: <CAGOvqipzPYoQy74MSetCsP7qyDcAQMjwLb6uitgQ-dSEr5hy9Q@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11351a5af8e43b04fe2f5636
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11351a5af8e43b04fe2f5636
Content-Type: text/plain; charset=UTF-8

We use the Hadoop configuration inside of our code executing on Spark as we
need to list out files in the path.  Maybe that is why it is exposed for us.


On Mon, Jul 14, 2014 at 6:57 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Nishkam,
>
> Aaron's fix should prevent two concurrent accesses to getJobConf (and
> the Hadoop code therein). But if there is code elsewhere that tries to
> mutate the configuration, then I could see how we might still have the
> ConcurrentModificationException.
>
> I looked at your patch for HADOOP-10456 and the only example you give
> is of the data being accessed inside of getJobConf. Is it accessed
> somewhere else too from Spark that you are aware of?
>
> https://issues.apache.org/jira/browse/HADOOP-10456
>
> - Patrick
>
> On Mon, Jul 14, 2014 at 3:28 PM, Nishkam Ravi <nravi@cloudera.com> wrote:
> > Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object would
> > help. I suspect we will start seeing the ConcurrentModificationException
> > again. The right fix has gone into Hadoop through 10456. Unfortunately, I
> > don't have any bright ideas on how to synchronize this at the Spark level
> > without the risk of deadlocks.
> >
> >
> > On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >
> >> The full jstack would still be useful, but our current working theory is
> >> that this is due to the fact that Configuration#loadDefaults goes
> through
> >> every Configuration object that was ever created (via
> >> Configuration.REGISTRY) and locks it, thus introducing a dependency from
> >> new Configuration to old, otherwise unrelated, Configuration objects
> that
> >> our locking did not anticipate.
> >>
> >> I have created https://github.com/apache/spark/pull/1409 to hopefully
> fix
> >> this bug.
> >>
> >>
> >> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >> > Hey Cody,
> >> >
> >> > This Jstack seems truncated, would you mind giving the entire stack
> >> > trace? For the second thread, for instance, we can't see where the
> >> > lock is being acquired.
> >> >
> >> > - Patrick
> >> >
> >> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> >> > <cody.koeninger@mediacrossing.com> wrote:
> >> > > Hi all, just wanted to give a heads up that we're seeing a
> reproducible
> >> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> >> > >
> >> > > If jira is a better place for this, apologies in advance - figured
> >> > talking
> >> > > about it on the mailing list was friendlier than randomly
> (re)opening
> >> > jira
> >> > > tickets.
> >> > >
> >> > > I know Gary had mentioned some issues with 1.0.1 on the mailing
> list,
> >> > once
> >> > > we got a thread dump I wanted to follow up.
> >> > >
> >> > > The thread dump shows the deadlock occurs in the synchronized block
> of
> >> > code
> >> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> >> > >
> >> > > Relevant portions of the thread dump are summarized below, we can
> >> provide
> >> > > the whole dump if it's useful.
> >> > >
> >> > > Found one Java-level deadlock:
> >> > > =============================
> >> > > "Executor task launch worker-1":
> >> > >   waiting to lock monitor 0x00007f250400c520 (object
> >> 0x00000000fae7dc30,
> >> > a
> >> > > org.apache.hadoop.co
> >> > > nf.Configuration),
> >> > >   which is held by "Executor task launch worker-0"
> >> > > "Executor task launch worker-0":
> >> > >   waiting to lock monitor 0x00007f2520495620 (object
> >> 0x00000000faeb4fc8,
> >> > a
> >> > > java.lang.Class),
> >> > >   which is held by "Executor task launch worker-1"
> >> > >
> >> > >
> >> > > "Executor task launch worker-1":
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> >> > >         - waiting to lock <0x00000000fae7dc30> (a
> >> > > org.apache.hadoop.conf.Configuration)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> >> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> >> > > org.apache.hadoop.conf.Configurati
> >> > > on)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> >> > > )
> >> > >         at
> >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> >> > > Method)
> >> > >         at
> >> > >
> >> >
> >>
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> >> > > java:57)
> >> > >         at
> >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> >> > > Method)
> >> > >         at
> >> > >
> >> >
> >>
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> >> > > java:57)
> >> > >         at
> >> > >
> >> >
> >>
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> >> > > sorImpl.java:45)
> >> > >         at
> >> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> >> > >         at java.lang.Class.newInstance0(Class.java:374)
> >> > >         at java.lang.Class.newInstance(Class.java:327)
> >> > >         at
> >> > java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> >> > >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> >> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> >> > > org.apache.hadoop.fs.FileSystem)
> >> > >         at
> >> > >
> >> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >> > >         at
> >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >> > >         at
> >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >> > >         at
> >> > >
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >> > >
> >> > >
> >> > >
> >> > > ...elided...
> >> > >
> >> > >
> >> > > "Executor task launch worker-0" daemon prio=10
> tid=0x0000000001e71800
> >> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> >> > >    java.lang.Thread.State: BLOCKED (on object monitor)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> >> > >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class
> for
> >> > > org.apache.hadoop.fs.FileSystem)
> >> > >         at
> >> > >
> >> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >> > >         at
> >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >> > >         at
> >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >> > >         at
> >> > >
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >> >
> >>
>

--001a11351a5af8e43b04fe2f5636--

From dev-return-8343-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:09:37 2014
Return-Path: <dev-return-8343-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B29C711A40
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:09:37 +0000 (UTC)
Received: (qmail 63288 invoked by uid 500); 14 Jul 2014 23:09:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63218 invoked by uid 500); 14 Jul 2014 23:09:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63204 invoked by uid 99); 14 Jul 2014 23:09:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:09:36 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:09:33 +0000
Received: by mail-qc0-f178.google.com with SMTP id x3so2271058qcv.37
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:09:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=dMyyqGJk+AjweJAhZZZtOE3gmX2huEbwbBnVvJPomNI=;
        b=Hr03BcraFt8XlcbybYQquvNjPh+ghWOcsF56MFOhqfM9ry4imfrNP00sRkxrCD2Rgn
         rXzB3W21r8w3qSgrpglIW6V0Hp8htWoBIkl86BPDg4ill9Y7znQntOs0OGl6xrAkg7Gx
         miWK+JN1qWXXSHvpNnrqitDqjY+jFBHBgThvmybRIKoEo9izkq2o9wEE/hwWghbzvgQi
         JgspnQVNZavtiPVMDP7tPNWle49XaoI/KqYfXJ3LDiq+HiAAVbyZ1vsK+3B6J/sky0p7
         emaEq129wGPpJoBPYeyjDpAePbXH7N2OoFaT2R+iX2XXFTb6WhlI9+G99XkvGxdxeco9
         LgRQ==
X-Gm-Message-State: ALoCoQm1FuJaTUoW06ao+sAJrVy1KGdAtD2YdgpoEXjObVQ7fFOhlguZEJyc4kY7pn47Ai6Sar3s
X-Received: by 10.140.95.241 with SMTP id i104mr28086767qge.6.1405379349018;
 Mon, 14 Jul 2014 16:09:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 14 Jul 2014 16:08:48 -0700 (PDT)
In-Reply-To: <1C3442B5-637F-4DA0-BD98-F1F95E2AEFD9@gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
 <20140714173027.6e60b16f@sh9> <1C3442B5-637F-4DA0-BD98-F1F95E2AEFD9@gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 14 Jul 2014 16:08:48 -0700
Message-ID: <CAPh_B=abZT0S94+ESrWBYx2FJw7O1_g84AGep3sRq1nkXJDBMA@mail.gmail.com>
Subject: Re: better compression codecs for shuffle blocks?
To: Matei Zaharia <matei.zaharia@gmail.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, Jon Hartlaub <jhartlaub@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c169aedf949704fe2f6017
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c169aedf949704fe2f6017
Content-Type: text/plain; charset=UTF-8

Copying Jon here since he worked on the lzf library at Ning.

Jon - any comments on this topic?


On Mon, Jul 14, 2014 at 3:54 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> You can actually turn off shuffle compression by setting
> spark.shuffle.compress to false. Try that out, there will still be some
> buffers for the various OutputStreams, but they should be smaller.
>
> Matei
>
> On Jul 14, 2014, at 3:30 PM, Stephen Haberman <stephen.haberman@gmail.com>
> wrote:
>
> >
> > Just a comment from the peanut gallery, but these buffers are a real
> > PITA for us as well. Probably 75% of our non-user-error job failures
> > are related to them.
> >
> > Just naively, what about not doing compression on the fly? E.g. during
> > the shuffle just write straight to disk, uncompressed?
> >
> > For us, we always have plenty of disk space, and if you're concerned
> > about network transmission, you could add a separate compress step
> > after the blocks have been written to disk, but before being sent over
> > the wire.
> >
> > Granted, IANAE, so perhaps this is a bad idea; either way, awesome to
> > see work in this area!
> >
> > - Stephen
> >
>
>

--001a11c169aedf949704fe2f6017--

From dev-return-8344-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:11:22 2014
Return-Path: <dev-return-8344-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0277E11A4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:11:22 +0000 (UTC)
Received: (qmail 68512 invoked by uid 500); 14 Jul 2014 23:11:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68457 invoked by uid 500); 14 Jul 2014 23:11:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68440 invoked by uid 99); 14 Jul 2014 23:11:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:11:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nravi@cloudera.com designates 209.85.214.169 as permitted sender)
Received: from [209.85.214.169] (HELO mail-ob0-f169.google.com) (209.85.214.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:11:19 +0000
Received: by mail-ob0-f169.google.com with SMTP id nu7so5003646obb.14
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:10:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=QaV1LSZILyoHE8v0hZ1krqqX4eSty5AvjNZyy2+Bhgw=;
        b=b/Pes9eRXwN9Zij0J2gf8COQt3IdI3B3f1atX/ku4BH8C0HxjmAhQJdRQfk5hkDjcI
         f9PW/Q75MedtRD93sqh2tDd7j8zRzxF/b+iJycqHYYSlpEUMwiPauPBgx2XCINAR0hk4
         YtitNcT6RS+qL4DbtDlmc8PPEDhgdi2tnRfdQkgF0RsHNY+ZFmkblhn8EFk/XeI3UQhB
         +LD1JYiY6q7Raf0uP1q+YG9fKtlhjmP3EXnT/3Ch5oI9ReyhGGCxPCV7Z8oyoRWEPI5U
         VPYIGnD8u1Aw79HgwlK/jQ6685RZsrMyxEpdqcSM6CUlaBDlYYdN+JVuCJaXZWvnDA3j
         NqRQ==
X-Gm-Message-State: ALoCoQkxjTo8uZGFN5W8UBZpJ0HfEyjOtMbCSy90gYph3mA4Il49gUHcPtef6nLBJgFlKN8upRTk
X-Received: by 10.182.81.129 with SMTP id a1mr11931568oby.65.1405379453929;
 Mon, 14 Jul 2014 16:10:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.129.194 with HTTP; Mon, 14 Jul 2014 16:10:33 -0700 (PDT)
In-Reply-To: <CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
 <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
 <CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
 <CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com> <CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Mon, 14 Jul 2014 16:10:33 -0700
Message-ID: <CACfA1zVytQYn5=PxBQaMQTuYUUPJg-YepQK-dtDhoXCozKSj-g@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b2e49fa205a2604fe2f67f0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2e49fa205a2604fe2f67f0
Content-Type: text/plain; charset=UTF-8

HI Patrick, I'm not aware of another place where the access happens, but
it's possible that it does. The original fix synchronized on the
broadcastConf object and someone reported the same exception.


On Mon, Jul 14, 2014 at 3:57 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Nishkam,
>
> Aaron's fix should prevent two concurrent accesses to getJobConf (and
> the Hadoop code therein). But if there is code elsewhere that tries to
> mutate the configuration, then I could see how we might still have the
> ConcurrentModificationException.
>
> I looked at your patch for HADOOP-10456 and the only example you give
> is of the data being accessed inside of getJobConf. Is it accessed
> somewhere else too from Spark that you are aware of?
>
> https://issues.apache.org/jira/browse/HADOOP-10456
>
> - Patrick
>
> On Mon, Jul 14, 2014 at 3:28 PM, Nishkam Ravi <nravi@cloudera.com> wrote:
> > Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object would
> > help. I suspect we will start seeing the ConcurrentModificationException
> > again. The right fix has gone into Hadoop through 10456. Unfortunately, I
> > don't have any bright ideas on how to synchronize this at the Spark level
> > without the risk of deadlocks.
> >
> >
> > On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >
> >> The full jstack would still be useful, but our current working theory is
> >> that this is due to the fact that Configuration#loadDefaults goes
> through
> >> every Configuration object that was ever created (via
> >> Configuration.REGISTRY) and locks it, thus introducing a dependency from
> >> new Configuration to old, otherwise unrelated, Configuration objects
> that
> >> our locking did not anticipate.
> >>
> >> I have created https://github.com/apache/spark/pull/1409 to hopefully
> fix
> >> this bug.
> >>
> >>
> >> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >> > Hey Cody,
> >> >
> >> > This Jstack seems truncated, would you mind giving the entire stack
> >> > trace? For the second thread, for instance, we can't see where the
> >> > lock is being acquired.
> >> >
> >> > - Patrick
> >> >
> >> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> >> > <cody.koeninger@mediacrossing.com> wrote:
> >> > > Hi all, just wanted to give a heads up that we're seeing a
> reproducible
> >> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> >> > >
> >> > > If jira is a better place for this, apologies in advance - figured
> >> > talking
> >> > > about it on the mailing list was friendlier than randomly
> (re)opening
> >> > jira
> >> > > tickets.
> >> > >
> >> > > I know Gary had mentioned some issues with 1.0.1 on the mailing
> list,
> >> > once
> >> > > we got a thread dump I wanted to follow up.
> >> > >
> >> > > The thread dump shows the deadlock occurs in the synchronized block
> of
> >> > code
> >> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> >> > >
> >> > > Relevant portions of the thread dump are summarized below, we can
> >> provide
> >> > > the whole dump if it's useful.
> >> > >
> >> > > Found one Java-level deadlock:
> >> > > =============================
> >> > > "Executor task launch worker-1":
> >> > >   waiting to lock monitor 0x00007f250400c520 (object
> >> 0x00000000fae7dc30,
> >> > a
> >> > > org.apache.hadoop.co
> >> > > nf.Configuration),
> >> > >   which is held by "Executor task launch worker-0"
> >> > > "Executor task launch worker-0":
> >> > >   waiting to lock monitor 0x00007f2520495620 (object
> >> 0x00000000faeb4fc8,
> >> > a
> >> > > java.lang.Class),
> >> > >   which is held by "Executor task launch worker-1"
> >> > >
> >> > >
> >> > > "Executor task launch worker-1":
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> >> > >         - waiting to lock <0x00000000fae7dc30> (a
> >> > > org.apache.hadoop.conf.Configuration)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> >> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> >> > > org.apache.hadoop.conf.Configurati
> >> > > on)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> >> > > )
> >> > >         at
> >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> >> > > Method)
> >> > >         at
> >> > >
> >> >
> >>
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> >> > > java:57)
> >> > >         at
> >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> >> > > Method)
> >> > >         at
> >> > >
> >> >
> >>
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> >> > > java:57)
> >> > >         at
> >> > >
> >> >
> >>
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> >> > > sorImpl.java:45)
> >> > >         at
> >> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> >> > >         at java.lang.Class.newInstance0(Class.java:374)
> >> > >         at java.lang.Class.newInstance(Class.java:327)
> >> > >         at
> >> > java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> >> > >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> >> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> >> > > org.apache.hadoop.fs.FileSystem)
> >> > >         at
> >> > >
> >> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >> > >         at
> >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >> > >         at
> >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >> > >         at
> >> > >
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >> > >
> >> > >
> >> > >
> >> > > ...elided...
> >> > >
> >> > >
> >> > > "Executor task launch worker-0" daemon prio=10
> tid=0x0000000001e71800
> >> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> >> > >    java.lang.Thread.State: BLOCKED (on object monitor)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> >> > >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class
> for
> >> > > org.apache.hadoop.fs.FileSystem)
> >> > >         at
> >> > >
> >> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >> > >         at
> >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >> > >         at
> >> > >
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >> > >         at
> >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >> > >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >> > >         at
> >> > >
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >         at
> >> > >
> >> >
> >>
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >> >
> >>
>

--047d7b2e49fa205a2604fe2f67f0--

From dev-return-8345-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:18:39 2014
Return-Path: <dev-return-8345-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4C3A11A7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:18:39 +0000 (UTC)
Received: (qmail 84357 invoked by uid 500); 14 Jul 2014 23:18:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 84293 invoked by uid 500); 14 Jul 2014 23:18:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84280 invoked by uid 99); 14 Jul 2014 23:18:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:18:38 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:18:35 +0000
Received: by mail-vc0-f182.google.com with SMTP id hq11so8918898vcb.27
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:18:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=hJbwxTDFpY9/AXQWU1oiqgyuTJVhHVJC+w1R6RTc3yQ=;
        b=K2bRSmcqNYC+A1Fg5ppJSlsPVYbsTE17RQF0BKZm5zwD92CQQwrbi71+oNBi02vbG4
         TeZXo0zps2o+SN5mOPsXnUBNiNJsmHkQzk1utV2l+eyJnHT9pQ1ZXYHRAETTwiHgEuqX
         i4nLlEdR3sCaVOxYWQrOeIzIu3c8c9EXFxPwFPXyaQZD8KQHaBd49sAkots6vfMoUz1P
         TPo5o8bzHrKZ6ys6EA8qNy/YF4oZTFCgkSLxUJYjsnWLuCTCMuffawrw2ZSE2LNKr8lZ
         LAgspepPzXHwXGlFkZMEd0nuUjIkY7MWw7LD5u0Jyxxpw+ujE0plLzAl/P6NDKi0qbe1
         59lg==
X-Gm-Message-State: ALoCoQn0Q41yqRYIa3+QYWn21sXC750QIiKoDfulXcKbovuX/aykhFb7XDkXbpBMH1Y9NRyVleFw
X-Received: by 10.58.71.230 with SMTP id y6mr2569216veu.19.1405379893382;
        Mon, 14 Jul 2014 16:18:13 -0700 (PDT)
Received: from mail-vc0-f182.google.com (mail-vc0-f182.google.com [209.85.220.182])
        by mx.google.com with ESMTPSA id d18sm5798093veh.12.2014.07.14.16.18.12
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 16:18:12 -0700 (PDT)
Received: by mail-vc0-f182.google.com with SMTP id hq11so8918858vcb.27
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:18:11 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.52.116.194 with SMTP id jy2mr2675698vdb.39.1405379891808;
 Mon, 14 Jul 2014 16:18:11 -0700 (PDT)
Received: by 10.221.24.4 with HTTP; Mon, 14 Jul 2014 16:18:11 -0700 (PDT)
Received: by 10.221.24.4 with HTTP; Mon, 14 Jul 2014 16:18:11 -0700 (PDT)
In-Reply-To: <CACfA1zVytQYn5=PxBQaMQTuYUUPJg-YepQK-dtDhoXCozKSj-g@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
	<CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
	<CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
	<CACfA1zVytQYn5=PxBQaMQTuYUUPJg-YepQK-dtDhoXCozKSj-g@mail.gmail.com>
Date: Mon, 14 Jul 2014 19:18:11 -0400
Message-ID: <CA+-p3AFjcn4OBYVwjWBcVqmjsT8vAT-nMVpBQpkpEWB7OCP9bQ@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Andrew Ash <andrew@andrewash.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec547cbb739d3de04fe2f8149
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec547cbb739d3de04fe2f8149
Content-Type: text/plain; charset=UTF-8

I observed a deadlock here when using the AvroInputFormat as well. The
short of the issue is that there's one configuration object per JVM, but
multiple threads, one for each task. If each thread attempts to add a
configuration option to the Configuration object at once you get issues
because HashMap isn't thread safe.

More details to come tonight. Thanks!
On Jul 14, 2014 4:11 PM, "Nishkam Ravi" <nravi@cloudera.com> wrote:

> HI Patrick, I'm not aware of another place where the access happens, but
> it's possible that it does. The original fix synchronized on the
> broadcastConf object and someone reported the same exception.
>
>
> On Mon, Jul 14, 2014 at 3:57 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Hey Nishkam,
> >
> > Aaron's fix should prevent two concurrent accesses to getJobConf (and
> > the Hadoop code therein). But if there is code elsewhere that tries to
> > mutate the configuration, then I could see how we might still have the
> > ConcurrentModificationException.
> >
> > I looked at your patch for HADOOP-10456 and the only example you give
> > is of the data being accessed inside of getJobConf. Is it accessed
> > somewhere else too from Spark that you are aware of?
> >
> > https://issues.apache.org/jira/browse/HADOOP-10456
> >
> > - Patrick
> >
> > On Mon, Jul 14, 2014 at 3:28 PM, Nishkam Ravi <nravi@cloudera.com>
> wrote:
> > > Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object
> would
> > > help. I suspect we will start seeing the
> ConcurrentModificationException
> > > again. The right fix has gone into Hadoop through 10456.
> Unfortunately, I
> > > don't have any bright ideas on how to synchronize this at the Spark
> level
> > > without the risk of deadlocks.
> > >
> > >
> > > On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <ilikerps@gmail.com>
> > wrote:
> > >
> > >> The full jstack would still be useful, but our current working theory
> is
> > >> that this is due to the fact that Configuration#loadDefaults goes
> > through
> > >> every Configuration object that was ever created (via
> > >> Configuration.REGISTRY) and locks it, thus introducing a dependency
> from
> > >> new Configuration to old, otherwise unrelated, Configuration objects
> > that
> > >> our locking did not anticipate.
> > >>
> > >> I have created https://github.com/apache/spark/pull/1409 to hopefully
> > fix
> > >> this bug.
> > >>
> > >>
> > >> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <pwendell@gmail.com>
> > >> wrote:
> > >>
> > >> > Hey Cody,
> > >> >
> > >> > This Jstack seems truncated, would you mind giving the entire stack
> > >> > trace? For the second thread, for instance, we can't see where the
> > >> > lock is being acquired.
> > >> >
> > >> > - Patrick
> > >> >
> > >> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> > >> > <cody.koeninger@mediacrossing.com> wrote:
> > >> > > Hi all, just wanted to give a heads up that we're seeing a
> > reproducible
> > >> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> > >> > >
> > >> > > If jira is a better place for this, apologies in advance - figured
> > >> > talking
> > >> > > about it on the mailing list was friendlier than randomly
> > (re)opening
> > >> > jira
> > >> > > tickets.
> > >> > >
> > >> > > I know Gary had mentioned some issues with 1.0.1 on the mailing
> > list,
> > >> > once
> > >> > > we got a thread dump I wanted to follow up.
> > >> > >
> > >> > > The thread dump shows the deadlock occurs in the synchronized
> block
> > of
> > >> > code
> > >> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> > >> > >
> > >> > > Relevant portions of the thread dump are summarized below, we can
> > >> provide
> > >> > > the whole dump if it's useful.
> > >> > >
> > >> > > Found one Java-level deadlock:
> > >> > > =============================
> > >> > > "Executor task launch worker-1":
> > >> > >   waiting to lock monitor 0x00007f250400c520 (object
> > >> 0x00000000fae7dc30,
> > >> > a
> > >> > > org.apache.hadoop.co
> > >> > > nf.Configuration),
> > >> > >   which is held by "Executor task launch worker-0"
> > >> > > "Executor task launch worker-0":
> > >> > >   waiting to lock monitor 0x00007f2520495620 (object
> > >> 0x00000000faeb4fc8,
> > >> > a
> > >> > > java.lang.Class),
> > >> > >   which is held by "Executor task launch worker-1"
> > >> > >
> > >> > >
> > >> > > "Executor task launch worker-1":
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> > >> > >         - waiting to lock <0x00000000fae7dc30> (a
> > >> > > org.apache.hadoop.conf.Configuration)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> > >> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> > >> > > org.apache.hadoop.conf.Configurati
> > >> > > on)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> > >> > > )
> > >> > >         at
> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > >> > > Method)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > >> > > java:57)
> > >> > >         at
> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > >> > > Method)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > >> > > java:57)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> > >> > > sorImpl.java:45)
> > >> > >         at
> > >> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> > >> > >         at java.lang.Class.newInstance0(Class.java:374)
> > >> > >         at java.lang.Class.newInstance(Class.java:327)
> > >> > >         at
> > >> > java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> > >> > >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> > >> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> > >> > > org.apache.hadoop.fs.FileSystem)
> > >> > >         at
> > >> > >
> > >>
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> > >> > >         at
> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> > >> > >         at
> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> > >> > >         at
> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> > >> > >         at
> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> > >> > >         at
> > >> > >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >         at
> > >> > >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> > >> > >
> > >> > >
> > >> > >
> > >> > > ...elided...
> > >> > >
> > >> > >
> > >> > > "Executor task launch worker-0" daemon prio=10
> > tid=0x0000000001e71800
> > >> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> > >> > >    java.lang.Thread.State: BLOCKED (on object monitor)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> > >> > >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class
> > for
> > >> > > org.apache.hadoop.fs.FileSystem)
> > >> > >         at
> > >> > >
> > >>
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> > >> > >         at
> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> > >> > >         at
> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> > >> > >         at
> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> > >> > >         at
> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> > >> > >         at
> > >> > >
> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> > >> > >         at
> > >> > >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >         at
> > >> > >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >         at
> > >> > >
> > >> >
> > >>
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> > >> >
> > >>
> >
>

--bcaec547cbb739d3de04fe2f8149--

From dev-return-8346-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:23:26 2014
Return-Path: <dev-return-8346-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E64CE11A9F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:23:26 +0000 (UTC)
Received: (qmail 91468 invoked by uid 500); 14 Jul 2014 23:23:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91403 invoked by uid 500); 14 Jul 2014 23:23:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91379 invoked by uid 99); 14 Jul 2014 23:23:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:23:25 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.51 as permitted sender)
Received: from [209.85.219.51] (HELO mail-oa0-f51.google.com) (209.85.219.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:23:24 +0000
Received: by mail-oa0-f51.google.com with SMTP id o6so3106311oag.38
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:22:59 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=t8MEyBGxdEjPeLsSvxi8v0QLZ+wQ7pcEl6AETaDz4YU=;
        b=cj3XpRPF/1evwQKKWyusQ1euQN5vNk36HMs9JDpcMwllAGSMY0+nYDiYJzlJsJlCSv
         rPTtyexp1/HY8hTJh4PunaVAML7zhh3VJqmJ4k1kvWfvW6vJDutcJ3+16tkMVChVnGaZ
         3OH7jUsF485LpS+/VBvjHycuIemIdVUyEaZB1ImPBCxgU1p4OsUdrCw/8huuPfBEru5W
         ZyPGLtJ1cY+d8ZiH11AZ2F6xPdmNK7lO6TXfZXNMf1zoSYUQ4HVo1SDCbGZnLnp+VXAA
         aDHPvrL9kVUGox9d84DYJyiW4QZx8yc/s6pybFZq2PrWMpW6y2q1+3N3f9WV6hGoQdkM
         5E3w==
MIME-Version: 1.0
X-Received: by 10.182.27.225 with SMTP id w1mr12118009obg.64.1405380179111;
 Mon, 14 Jul 2014 16:22:59 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 16:22:59 -0700 (PDT)
In-Reply-To: <CA+-p3AFjcn4OBYVwjWBcVqmjsT8vAT-nMVpBQpkpEWB7OCP9bQ@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
	<CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
	<CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
	<CACfA1zVytQYn5=PxBQaMQTuYUUPJg-YepQK-dtDhoXCozKSj-g@mail.gmail.com>
	<CA+-p3AFjcn4OBYVwjWBcVqmjsT8vAT-nMVpBQpkpEWB7OCP9bQ@mail.gmail.com>
Date: Mon, 14 Jul 2014 16:22:59 -0700
Message-ID: <CABPQxss83HFp-4RhyRXiOws8JFL15ZA+sVJNqNs8rvTe7Y5CVQ@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Andrew and Gary,

Would you guys be able to test
https://github.com/apache/spark/pull/1409/files and see if it solves
your problem?

- Patrick

On Mon, Jul 14, 2014 at 4:18 PM, Andrew Ash <andrew@andrewash.com> wrote:
> I observed a deadlock here when using the AvroInputFormat as well. The
> short of the issue is that there's one configuration object per JVM, but
> multiple threads, one for each task. If each thread attempts to add a
> configuration option to the Configuration object at once you get issues
> because HashMap isn't thread safe.
>
> More details to come tonight. Thanks!
> On Jul 14, 2014 4:11 PM, "Nishkam Ravi" <nravi@cloudera.com> wrote:
>
>> HI Patrick, I'm not aware of another place where the access happens, but
>> it's possible that it does. The original fix synchronized on the
>> broadcastConf object and someone reported the same exception.
>>
>>
>> On Mon, Jul 14, 2014 at 3:57 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > Hey Nishkam,
>> >
>> > Aaron's fix should prevent two concurrent accesses to getJobConf (and
>> > the Hadoop code therein). But if there is code elsewhere that tries to
>> > mutate the configuration, then I could see how we might still have the
>> > ConcurrentModificationException.
>> >
>> > I looked at your patch for HADOOP-10456 and the only example you give
>> > is of the data being accessed inside of getJobConf. Is it accessed
>> > somewhere else too from Spark that you are aware of?
>> >
>> > https://issues.apache.org/jira/browse/HADOOP-10456
>> >
>> > - Patrick
>> >
>> > On Mon, Jul 14, 2014 at 3:28 PM, Nishkam Ravi <nravi@cloudera.com>
>> wrote:
>> > > Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object
>> would
>> > > help. I suspect we will start seeing the
>> ConcurrentModificationException
>> > > again. The right fix has gone into Hadoop through 10456.
>> Unfortunately, I
>> > > don't have any bright ideas on how to synchronize this at the Spark
>> level
>> > > without the risk of deadlocks.
>> > >
>> > >
>> > > On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <ilikerps@gmail.com>
>> > wrote:
>> > >
>> > >> The full jstack would still be useful, but our current working theory
>> is
>> > >> that this is due to the fact that Configuration#loadDefaults goes
>> > through
>> > >> every Configuration object that was ever created (via
>> > >> Configuration.REGISTRY) and locks it, thus introducing a dependency
>> from
>> > >> new Configuration to old, otherwise unrelated, Configuration objects
>> > that
>> > >> our locking did not anticipate.
>> > >>
>> > >> I have created https://github.com/apache/spark/pull/1409 to hopefully
>> > fix
>> > >> this bug.
>> > >>
>> > >>
>> > >> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <pwendell@gmail.com>
>> > >> wrote:
>> > >>
>> > >> > Hey Cody,
>> > >> >
>> > >> > This Jstack seems truncated, would you mind giving the entire stack
>> > >> > trace? For the second thread, for instance, we can't see where the
>> > >> > lock is being acquired.
>> > >> >
>> > >> > - Patrick
>> > >> >
>> > >> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
>> > >> > <cody.koeninger@mediacrossing.com> wrote:
>> > >> > > Hi all, just wanted to give a heads up that we're seeing a
>> > reproducible
>> > >> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
>> > >> > >
>> > >> > > If jira is a better place for this, apologies in advance - figured
>> > >> > talking
>> > >> > > about it on the mailing list was friendlier than randomly
>> > (re)opening
>> > >> > jira
>> > >> > > tickets.
>> > >> > >
>> > >> > > I know Gary had mentioned some issues with 1.0.1 on the mailing
>> > list,
>> > >> > once
>> > >> > > we got a thread dump I wanted to follow up.
>> > >> > >
>> > >> > > The thread dump shows the deadlock occurs in the synchronized
>> block
>> > of
>> > >> > code
>> > >> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
>> > >> > >
>> > >> > > Relevant portions of the thread dump are summarized below, we can
>> > >> provide
>> > >> > > the whole dump if it's useful.
>> > >> > >
>> > >> > > Found one Java-level deadlock:
>> > >> > > =============================
>> > >> > > "Executor task launch worker-1":
>> > >> > >   waiting to lock monitor 0x00007f250400c520 (object
>> > >> 0x00000000fae7dc30,
>> > >> > a
>> > >> > > org.apache.hadoop.co
>> > >> > > nf.Configuration),
>> > >> > >   which is held by "Executor task launch worker-0"
>> > >> > > "Executor task launch worker-0":
>> > >> > >   waiting to lock monitor 0x00007f2520495620 (object
>> > >> 0x00000000faeb4fc8,
>> > >> > a
>> > >> > > java.lang.Class),
>> > >> > >   which is held by "Executor task launch worker-1"
>> > >> > >
>> > >> > >
>> > >> > > "Executor task launch worker-1":
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
>> > >> > >         - waiting to lock <0x00000000fae7dc30> (a
>> > >> > > org.apache.hadoop.conf.Configuration)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
>> > >> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
>> > >> > > org.apache.hadoop.conf.Configurati
>> > >> > > on)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
>> > >> > > )
>> > >> > >         at
>> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> > >> > > Method)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
>> > >> > > java:57)
>> > >> > >         at
>> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>> > >> > > Method)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
>> > >> > > java:57)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
>> > >> > > sorImpl.java:45)
>> > >> > >         at
>> > >> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
>> > >> > >         at java.lang.Class.newInstance0(Class.java:374)
>> > >> > >         at java.lang.Class.newInstance(Class.java:327)
>> > >> > >         at
>> > >> > java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
>> > >> > >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
>> > >> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
>> > >> > > org.apache.hadoop.fs.FileSystem)
>> > >> > >         at
>> > >> > >
>> > >>
>> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>> > >> > >         at
>> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>> > >> > >         at
>> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>> > >> > >         at
>> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>> > >> > >         at
>> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>> > >> > >         at
>> > >> > >
>> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >> > >         at
>> > >> > >
>> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>> > >> > >
>> > >> > >
>> > >> > >
>> > >> > > ...elided...
>> > >> > >
>> > >> > >
>> > >> > > "Executor task launch worker-0" daemon prio=10
>> > tid=0x0000000001e71800
>> > >> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
>> > >> > >    java.lang.Thread.State: BLOCKED (on object monitor)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
>> > >> > >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class
>> > for
>> > >> > > org.apache.hadoop.fs.FileSystem)
>> > >> > >         at
>> > >> > >
>> > >>
>> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>> > >> > >         at
>> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>> > >> > >         at
>> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>> > >> > >         at
>> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>> > >> > >         at
>> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>> > >> > >         at
>> > >> > >
>> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>> > >> > >         at
>> > >> > >
>> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >> > >         at
>> > >> > >
>> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>> > >> > >         at
>> > >> > >
>> > >> >
>> > >>
>> >
>> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>> > >> >
>> > >>
>> >
>>

From dev-return-8347-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:35:35 2014
Return-Path: <dev-return-8347-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7361311B0B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:35:35 +0000 (UTC)
Received: (qmail 8216 invoked by uid 500); 14 Jul 2014 23:35:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8154 invoked by uid 500); 14 Jul 2014 23:35:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8137 invoked by uid 99); 14 Jul 2014 23:35:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:35:34 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:35:31 +0000
Received: by mail-qg0-f53.google.com with SMTP id q107so1912109qgd.26
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:35:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=J4kxKjXofD2y1kwiYGAqMjU13Pr087dqzYXdDPr1+UU=;
        b=DllJSyAIDye3iGPHnWdfuLUdLE53P1uPkutGE4x/BtoKCYlLmOTqRREdntVUH6VVYi
         G0hdAHH2nXv14VQVmCBWvgeuuUQEhNKySx09aLbFkcIOoWNu86S66ImRyAsi5T597kHs
         TQzJzilfdPFA70vo2TA+f3GsrtqIS3fpVFhP8WdzSbGP6xwW0a7xcEIK2O+FbMR+N3lt
         GoGyK+RnlG+4ujtCAoVgPSsJcAF2q6hHRxAFVfwWhyhSyE8scbsz0lLxLf51dvfILjc1
         +RtAQdoNoBSzN+JMehE2147+UoON7hcYvjmN8zVeA/Sia7NJOEQE3ZrFmkjh+xFUp6r0
         3CYg==
MIME-Version: 1.0
X-Received: by 10.140.51.235 with SMTP id u98mr28413892qga.69.1405380907033;
 Mon, 14 Jul 2014 16:35:07 -0700 (PDT)
Received: by 10.140.49.236 with HTTP; Mon, 14 Jul 2014 16:35:06 -0700 (PDT)
In-Reply-To: <CABPQxss83HFp-4RhyRXiOws8JFL15ZA+sVJNqNs8rvTe7Y5CVQ@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
	<CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
	<CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
	<CACfA1zVytQYn5=PxBQaMQTuYUUPJg-YepQK-dtDhoXCozKSj-g@mail.gmail.com>
	<CA+-p3AFjcn4OBYVwjWBcVqmjsT8vAT-nMVpBQpkpEWB7OCP9bQ@mail.gmail.com>
	<CABPQxss83HFp-4RhyRXiOws8JFL15ZA+sVJNqNs8rvTe7Y5CVQ@mail.gmail.com>
Date: Mon, 14 Jul 2014 19:35:06 -0400
Message-ID: <CAGOvqiq-Z2Ae=b_rb3Lu3FDEGe6eZ6fgHHM6Xzstrtx5a9sRFA@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11351718bcedce04fe2fbdc1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11351718bcedce04fe2fbdc1
Content-Type: text/plain; charset=UTF-8

We'll try to run a build tomorrow AM.


On Mon, Jul 14, 2014 at 7:22 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Andrew and Gary,
>
> Would you guys be able to test
> https://github.com/apache/spark/pull/1409/files and see if it solves
> your problem?
>
> - Patrick
>
> On Mon, Jul 14, 2014 at 4:18 PM, Andrew Ash <andrew@andrewash.com> wrote:
> > I observed a deadlock here when using the AvroInputFormat as well. The
> > short of the issue is that there's one configuration object per JVM, but
> > multiple threads, one for each task. If each thread attempts to add a
> > configuration option to the Configuration object at once you get issues
> > because HashMap isn't thread safe.
> >
> > More details to come tonight. Thanks!
> > On Jul 14, 2014 4:11 PM, "Nishkam Ravi" <nravi@cloudera.com> wrote:
> >
> >> HI Patrick, I'm not aware of another place where the access happens, but
> >> it's possible that it does. The original fix synchronized on the
> >> broadcastConf object and someone reported the same exception.
> >>
> >>
> >> On Mon, Jul 14, 2014 at 3:57 PM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >> > Hey Nishkam,
> >> >
> >> > Aaron's fix should prevent two concurrent accesses to getJobConf (and
> >> > the Hadoop code therein). But if there is code elsewhere that tries to
> >> > mutate the configuration, then I could see how we might still have the
> >> > ConcurrentModificationException.
> >> >
> >> > I looked at your patch for HADOOP-10456 and the only example you give
> >> > is of the data being accessed inside of getJobConf. Is it accessed
> >> > somewhere else too from Spark that you are aware of?
> >> >
> >> > https://issues.apache.org/jira/browse/HADOOP-10456
> >> >
> >> > - Patrick
> >> >
> >> > On Mon, Jul 14, 2014 at 3:28 PM, Nishkam Ravi <nravi@cloudera.com>
> >> wrote:
> >> > > Hi Aaron, I'm not sure if synchronizing on an arbitrary lock object
> >> would
> >> > > help. I suspect we will start seeing the
> >> ConcurrentModificationException
> >> > > again. The right fix has gone into Hadoop through 10456.
> >> Unfortunately, I
> >> > > don't have any bright ideas on how to synchronize this at the Spark
> >> level
> >> > > without the risk of deadlocks.
> >> > >
> >> > >
> >> > > On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <ilikerps@gmail.com
> >
> >> > wrote:
> >> > >
> >> > >> The full jstack would still be useful, but our current working
> theory
> >> is
> >> > >> that this is due to the fact that Configuration#loadDefaults goes
> >> > through
> >> > >> every Configuration object that was ever created (via
> >> > >> Configuration.REGISTRY) and locks it, thus introducing a dependency
> >> from
> >> > >> new Configuration to old, otherwise unrelated, Configuration
> objects
> >> > that
> >> > >> our locking did not anticipate.
> >> > >>
> >> > >> I have created https://github.com/apache/spark/pull/1409 to
> hopefully
> >> > fix
> >> > >> this bug.
> >> > >>
> >> > >>
> >> > >> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <
> pwendell@gmail.com>
> >> > >> wrote:
> >> > >>
> >> > >> > Hey Cody,
> >> > >> >
> >> > >> > This Jstack seems truncated, would you mind giving the entire
> stack
> >> > >> > trace? For the second thread, for instance, we can't see where
> the
> >> > >> > lock is being acquired.
> >> > >> >
> >> > >> > - Patrick
> >> > >> >
> >> > >> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> >> > >> > <cody.koeninger@mediacrossing.com> wrote:
> >> > >> > > Hi all, just wanted to give a heads up that we're seeing a
> >> > reproducible
> >> > >> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> >> > >> > >
> >> > >> > > If jira is a better place for this, apologies in advance -
> figured
> >> > >> > talking
> >> > >> > > about it on the mailing list was friendlier than randomly
> >> > (re)opening
> >> > >> > jira
> >> > >> > > tickets.
> >> > >> > >
> >> > >> > > I know Gary had mentioned some issues with 1.0.1 on the mailing
> >> > list,
> >> > >> > once
> >> > >> > > we got a thread dump I wanted to follow up.
> >> > >> > >
> >> > >> > > The thread dump shows the deadlock occurs in the synchronized
> >> block
> >> > of
> >> > >> > code
> >> > >> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> >> > >> > >
> >> > >> > > Relevant portions of the thread dump are summarized below, we
> can
> >> > >> provide
> >> > >> > > the whole dump if it's useful.
> >> > >> > >
> >> > >> > > Found one Java-level deadlock:
> >> > >> > > =============================
> >> > >> > > "Executor task launch worker-1":
> >> > >> > >   waiting to lock monitor 0x00007f250400c520 (object
> >> > >> 0x00000000fae7dc30,
> >> > >> > a
> >> > >> > > org.apache.hadoop.co
> >> > >> > > nf.Configuration),
> >> > >> > >   which is held by "Executor task launch worker-0"
> >> > >> > > "Executor task launch worker-0":
> >> > >> > >   waiting to lock monitor 0x00007f2520495620 (object
> >> > >> 0x00000000faeb4fc8,
> >> > >> > a
> >> > >> > > java.lang.Class),
> >> > >> > >   which is held by "Executor task launch worker-1"
> >> > >> > >
> >> > >> > >
> >> > >> > > "Executor task launch worker-1":
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> >> > >> > >         - waiting to lock <0x00000000fae7dc30> (a
> >> > >> > > org.apache.hadoop.conf.Configuration)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> >> > >> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> >> > >> > > org.apache.hadoop.conf.Configurati
> >> > >> > > on)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> >> > >> > > )
> >> > >> > >         at
> >> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> >> > >> > > Method)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> >> > >> > > java:57)
> >> > >> > >         at
> >> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> >> > >> > > Method)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> >> > >> > > java:57)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> >> > >> > > sorImpl.java:45)
> >> > >> > >         at
> >> > >> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> >> > >> > >         at java.lang.Class.newInstance0(Class.java:374)
> >> > >> > >         at java.lang.Class.newInstance(Class.java:327)
> >> > >> > >         at
> >> > >> > java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> >> > >> > >         at
> java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> >> > >> > >         at
> >> > >> > >
> >> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> >> > >> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> >> > >> > > org.apache.hadoop.fs.FileSystem)
> >> > >> > >         at
> >> > >> > >
> >> > >>
> >> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >> > >> > >         at
> >> > >> > >
> >> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >> > >> > >         at
> >> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >> > >> > >         at
> >> > >> > >
> >> >
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >> > >> > >         at
> >> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >> > >> > >         at
> >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >> > >> > >         at
> >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >> > >> > >         at
> >> > >> > >
> >> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >> > >> > >         at
> >> > >> > >
> >> >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >> > >         at
> >> > >> > >
> >> >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >> > >> > >
> >> > >> > >
> >> > >> > >
> >> > >> > > ...elided...
> >> > >> > >
> >> > >> > >
> >> > >> > > "Executor task launch worker-0" daemon prio=10
> >> > tid=0x0000000001e71800
> >> > >> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> >> > >> > >    java.lang.Thread.State: BLOCKED (on object monitor)
> >> > >> > >         at
> >> > >> > >
> >> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> >> > >> > >         - waiting to lock <0x00000000faeb4fc8> (a
> java.lang.Class
> >> > for
> >> > >> > > org.apache.hadoop.fs.FileSystem)
> >> > >> > >         at
> >> > >> > >
> >> > >>
> >> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >> > >> > >         at
> >> > >> > >
> >> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >> > >> > >         at
> >> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >> > >> > >         at
> >> > >> > >
> >> >
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >> > >> > >         at
> >> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >> > >> > >         at
> >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >> > >> > >         at
> >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >> > >> > >         at
> >> > >> > >
> >> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >> > >> > >         at
> >> > >> > >
> >> >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >> > >         at
> >> > >> > >
> >> >
> org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >> > >> > >         at
> >> > >> > >
> >> > >> >
> >> > >>
> >> >
> >>
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >> > >> >
> >> > >>
> >> >
> >>
>

--001a11351718bcedce04fe2fbdc1--

From dev-return-8348-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:46:55 2014
Return-Path: <dev-return-8348-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BFA0A11B57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:46:55 +0000 (UTC)
Received: (qmail 27546 invoked by uid 500); 14 Jul 2014 23:46:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27492 invoked by uid 500); 14 Jul 2014 23:46:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27480 invoked by uid 99); 14 Jul 2014 23:46:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:46:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.45 as permitted sender)
Received: from [209.85.216.45] (HELO mail-qa0-f45.google.com) (209.85.216.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:46:51 +0000
Received: by mail-qa0-f45.google.com with SMTP id cm18so2433551qab.4
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:46:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=zLbblXqHHZBNaSY4FzsTQw5y266jJQYRVB2LwZqEpeI=;
        b=wBBPuyLu2E9DsBBvldhNn0WzrN6dRnmof5iarOvHKkFFGgOQb+lDu1YfvxGeTQOJHu
         N1PT/cvNnCBPBNBS/KEp9aBc/XTpTULEtFtH00mi+LMmU4ZqIcCmKLQ9uEr1abEATVBC
         +1ViVUwTgrWqvu1bQgi7k3sQXM0c7K9vdc8mIuIi7JUXh5SAp4veHYt07yKx9sLYJ7df
         uQFgcMakOaXGgXBgB9bxOjAVzsupTHNm12CQ7wi54cSpzcvdGjx1FUhWXVOGW2TR5D+0
         X06vz9LCYg5fNvniA8ABt9gJfCG5t7DyliT2FKQdVt5uJwL9b1MVdgDyfY5xc35VuZtA
         cvmg==
X-Received: by 10.140.44.99 with SMTP id f90mr10652608qga.89.1405381590297;
 Mon, 14 Jul 2014 16:46:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 14 Jul 2014 16:46:10 -0700 (PDT)
In-Reply-To: <CAGOvqiq-Z2Ae=b_rb3Lu3FDEGe6eZ6fgHHM6Xzstrtx5a9sRFA@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
 <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
 <CANGvG8oZ2LwPvEWJyA6ZuVDFTTDu4fd5oMVCidQc0XuObqC0xA@mail.gmail.com>
 <CACfA1zXzuRUwN-zFGtRfvt1PtBGBA3K2aYoWGFpypPjxyBu4cw@mail.gmail.com>
 <CABPQxsviMzV_cLBTtOG6NyOvshYW_fToXUkt8m6_geGJHWQYuw@mail.gmail.com>
 <CACfA1zVytQYn5=PxBQaMQTuYUUPJg-YepQK-dtDhoXCozKSj-g@mail.gmail.com>
 <CA+-p3AFjcn4OBYVwjWBcVqmjsT8vAT-nMVpBQpkpEWB7OCP9bQ@mail.gmail.com>
 <CABPQxss83HFp-4RhyRXiOws8JFL15ZA+sVJNqNs8rvTe7Y5CVQ@mail.gmail.com> <CAGOvqiq-Z2Ae=b_rb3Lu3FDEGe6eZ6fgHHM6Xzstrtx5a9sRFA@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Jul 2014 16:46:10 -0700
Message-ID: <CANGvG8prncBCLMtn+Xv_zxP+NRSw7FCcs2iDtzi5VhW-GRxesA@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a9a6876bbc804fe2fe667
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a9a6876bbc804fe2fe667
Content-Type: text/plain; charset=UTF-8

The patch won't solve the problem where two people try to add a
configuration option at the same time, but I think there is currently an
issue where two people can try to initialize the Configuration at the same
time and still run into a ConcurrentModificationException. This at least
reduces (slightly) the scope of the exception although eliminating it may
not be possible.


On Mon, Jul 14, 2014 at 4:35 PM, Gary Malouf <malouf.gary@gmail.com> wrote:

> We'll try to run a build tomorrow AM.
>
>
> On Mon, Jul 14, 2014 at 7:22 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > Andrew and Gary,
> >
> > Would you guys be able to test
> > https://github.com/apache/spark/pull/1409/files and see if it solves
> > your problem?
> >
> > - Patrick
> >
> > On Mon, Jul 14, 2014 at 4:18 PM, Andrew Ash <andrew@andrewash.com>
> wrote:
> > > I observed a deadlock here when using the AvroInputFormat as well. The
> > > short of the issue is that there's one configuration object per JVM,
> but
> > > multiple threads, one for each task. If each thread attempts to add a
> > > configuration option to the Configuration object at once you get issues
> > > because HashMap isn't thread safe.
> > >
> > > More details to come tonight. Thanks!
> > > On Jul 14, 2014 4:11 PM, "Nishkam Ravi" <nravi@cloudera.com> wrote:
> > >
> > >> HI Patrick, I'm not aware of another place where the access happens,
> but
> > >> it's possible that it does. The original fix synchronized on the
> > >> broadcastConf object and someone reported the same exception.
> > >>
> > >>
> > >> On Mon, Jul 14, 2014 at 3:57 PM, Patrick Wendell <pwendell@gmail.com>
> > >> wrote:
> > >>
> > >> > Hey Nishkam,
> > >> >
> > >> > Aaron's fix should prevent two concurrent accesses to getJobConf
> (and
> > >> > the Hadoop code therein). But if there is code elsewhere that tries
> to
> > >> > mutate the configuration, then I could see how we might still have
> the
> > >> > ConcurrentModificationException.
> > >> >
> > >> > I looked at your patch for HADOOP-10456 and the only example you
> give
> > >> > is of the data being accessed inside of getJobConf. Is it accessed
> > >> > somewhere else too from Spark that you are aware of?
> > >> >
> > >> > https://issues.apache.org/jira/browse/HADOOP-10456
> > >> >
> > >> > - Patrick
> > >> >
> > >> > On Mon, Jul 14, 2014 at 3:28 PM, Nishkam Ravi <nravi@cloudera.com>
> > >> wrote:
> > >> > > Hi Aaron, I'm not sure if synchronizing on an arbitrary lock
> object
> > >> would
> > >> > > help. I suspect we will start seeing the
> > >> ConcurrentModificationException
> > >> > > again. The right fix has gone into Hadoop through 10456.
> > >> Unfortunately, I
> > >> > > don't have any bright ideas on how to synchronize this at the
> Spark
> > >> level
> > >> > > without the risk of deadlocks.
> > >> > >
> > >> > >
> > >> > > On Mon, Jul 14, 2014 at 3:07 PM, Aaron Davidson <
> ilikerps@gmail.com
> > >
> > >> > wrote:
> > >> > >
> > >> > >> The full jstack would still be useful, but our current working
> > theory
> > >> is
> > >> > >> that this is due to the fact that Configuration#loadDefaults goes
> > >> > through
> > >> > >> every Configuration object that was ever created (via
> > >> > >> Configuration.REGISTRY) and locks it, thus introducing a
> dependency
> > >> from
> > >> > >> new Configuration to old, otherwise unrelated, Configuration
> > objects
> > >> > that
> > >> > >> our locking did not anticipate.
> > >> > >>
> > >> > >> I have created https://github.com/apache/spark/pull/1409 to
> > hopefully
> > >> > fix
> > >> > >> this bug.
> > >> > >>
> > >> > >>
> > >> > >> On Mon, Jul 14, 2014 at 2:44 PM, Patrick Wendell <
> > pwendell@gmail.com>
> > >> > >> wrote:
> > >> > >>
> > >> > >> > Hey Cody,
> > >> > >> >
> > >> > >> > This Jstack seems truncated, would you mind giving the entire
> > stack
> > >> > >> > trace? For the second thread, for instance, we can't see where
> > the
> > >> > >> > lock is being acquired.
> > >> > >> >
> > >> > >> > - Patrick
> > >> > >> >
> > >> > >> > On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> > >> > >> > <cody.koeninger@mediacrossing.com> wrote:
> > >> > >> > > Hi all, just wanted to give a heads up that we're seeing a
> > >> > reproducible
> > >> > >> > > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> > >> > >> > >
> > >> > >> > > If jira is a better place for this, apologies in advance -
> > figured
> > >> > >> > talking
> > >> > >> > > about it on the mailing list was friendlier than randomly
> > >> > (re)opening
> > >> > >> > jira
> > >> > >> > > tickets.
> > >> > >> > >
> > >> > >> > > I know Gary had mentioned some issues with 1.0.1 on the
> mailing
> > >> > list,
> > >> > >> > once
> > >> > >> > > we got a thread dump I wanted to follow up.
> > >> > >> > >
> > >> > >> > > The thread dump shows the deadlock occurs in the synchronized
> > >> block
> > >> > of
> > >> > >> > code
> > >> > >> > > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> > >> > >> > >
> > >> > >> > > Relevant portions of the thread dump are summarized below, we
> > can
> > >> > >> provide
> > >> > >> > > the whole dump if it's useful.
> > >> > >> > >
> > >> > >> > > Found one Java-level deadlock:
> > >> > >> > > =============================
> > >> > >> > > "Executor task launch worker-1":
> > >> > >> > >   waiting to lock monitor 0x00007f250400c520 (object
> > >> > >> 0x00000000fae7dc30,
> > >> > >> > a
> > >> > >> > > org.apache.hadoop.co
> > >> > >> > > nf.Configuration),
> > >> > >> > >   which is held by "Executor task launch worker-0"
> > >> > >> > > "Executor task launch worker-0":
> > >> > >> > >   waiting to lock monitor 0x00007f2520495620 (object
> > >> > >> 0x00000000faeb4fc8,
> > >> > >> > a
> > >> > >> > > java.lang.Class),
> > >> > >> > >   which is held by "Executor task launch worker-1"
> > >> > >> > >
> > >> > >> > >
> > >> > >> > > "Executor task launch worker-1":
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> > >> > >> > >         - waiting to lock <0x00000000fae7dc30> (a
> > >> > >> > > org.apache.hadoop.conf.Configuration)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> > >> > >> > >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> > >> > >> > > org.apache.hadoop.conf.Configurati
> > >> > >> > > on)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> > >> > >> > > )
> > >> > >> > >         at
> > >> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > >> > >> > > Method)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > >> > >> > > java:57)
> > >> > >> > >         at
> > >> > >> sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > >> > >> > > Method)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > >> > >> > > java:57)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> > >> > >> > > sorImpl.java:45)
> > >> > >> > >         at
> > >> > >> > java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> > >> > >> > >         at java.lang.Class.newInstance0(Class.java:374)
> > >> > >> > >         at java.lang.Class.newInstance(Class.java:327)
> > >> > >> > >         at
> > >> > >> >
> java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> > >> > >> > >         at
> > java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> > >> > >> > >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> > >> > >> > > org.apache.hadoop.fs.FileSystem)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >>
> > >>
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> > >> > >> > >         at
> > >> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> > >> > >> > >         at
> > >> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> > >> > >> > >         at
> > >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> > >> > >> > >         at
> > >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> > >> > >> > >
> > >> > >> > >
> > >> > >> > >
> > >> > >> > > ...elided...
> > >> > >> > >
> > >> > >> > >
> > >> > >> > > "Executor task launch worker-0" daemon prio=10
> > >> > tid=0x0000000001e71800
> > >> > >> > > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> > >> > >> > >    java.lang.Thread.State: BLOCKED (on object monitor)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> > >> > >> > >         - waiting to lock <0x00000000faeb4fc8> (a
> > java.lang.Class
> > >> > for
> > >> > >> > > org.apache.hadoop.fs.FileSystem)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >>
> > >>
> org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> > >> > >> > >         at
> > >> > >> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> > >> > >> > >         at
> > >> > >> > org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> > >> > >> > >         at
> > >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> > >> > >> > >         at
> > >> org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >> > >         at
> > >> > >> > >
> > >> >
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> > >> > >> > >         at
> > >> > >> > >
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> > >> > >> >
> > >> > >>
> > >> >
> > >>
> >
>

--001a113a9a6876bbc804fe2fe667--

From dev-return-8349-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:53:06 2014
Return-Path: <dev-return-8349-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3C4F511B74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:53:06 +0000 (UTC)
Received: (qmail 35719 invoked by uid 500); 14 Jul 2014 23:53:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35652 invoked by uid 500); 14 Jul 2014 23:53:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35622 invoked by uid 99); 14 Jul 2014 23:53:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:53:04 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [74.125.82.171] (HELO mail-we0-f171.google.com) (74.125.82.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:53:02 +0000
Received: by mail-we0-f171.google.com with SMTP id p10so2215412wes.30
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:52:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=Q4wud32JqIfJQq7Ionrloffpb1WUyboqWtpUVh8IL50=;
        b=GnTcmp9OJR94RAEaaG6DWAGR5CH4AAV4cHmp7MIkxGUt4UBBdajpP1plO2jNFZEyde
         /ArYKCeCwomRvE5JC/XKn1kTJiWg8Y3YFlQGEJbZmnx9HiTFV12wQ2F8nmLkl56yZENj
         QpzspNG1pzw/jQhnMOMnII7z2Ai8DnEyXakI2KkqtY3i8teW7W7wgqOoBlUzrBvDSXT8
         VCIPKuG3cOySKS2JBgXCL61ypHxnAq4PFmY7I5YtG2WrBKOQ5mrbsKRy2r9DHvd9UMDI
         4ueJ/yhgu43jrDcV3bn8iUbXBEpUIFsIkY7zbifNxmpbKjkrXEnanoTs/k1202fv/UBw
         G08w==
X-Gm-Message-State: ALoCoQk3DgifM5ac+MiFg0fXybPlsB4vzuxjx0zRLg1Wi4+NJgbRVprmBsBneoQf+dHDVVdOWWJT
MIME-Version: 1.0
X-Received: by 10.194.63.196 with SMTP id i4mr23282188wjs.50.1405381958395;
 Mon, 14 Jul 2014 16:52:38 -0700 (PDT)
Received: by 10.180.94.164 with HTTP; Mon, 14 Jul 2014 16:52:38 -0700 (PDT)
In-Reply-To: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
Date: Mon, 14 Jul 2014 16:52:38 -0700
Message-ID: <CA+2Pv=im_8hvthVN+09E1QTGr4kYa8FL-rPTGLcrc39Mq=AS6g@mail.gmail.com>
Subject: Re: better compression codecs for shuffle blocks?
From: Davies Liu <davies@databricks.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Maybe we could try LZ4 [1], which has better performance and smaller footprint
than LZF and Snappy. In fast scan mode, the performance is 1.5 - 2x
higher than LZF[2],
but memory used is 10x smaller than LZF (16k vs 190k).

[1] https://github.com/jpountz/lz4-java
[2] http://ning.github.io/jvm-compressor-benchmark/results/calgary/roundtrip-2013-06-06/index.html


On Mon, Jul 14, 2014 at 12:01 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> Hi Spark devs,
>
> I was looking into the memory usage of shuffle and one annoying thing is
> the default compression codec (LZF) is that the implementation we use
> allocates buffers pretty generously. I did a simple experiment and found
> that creating 1000 LZFOutputStream allocated 198976424 bytes (~190MB). If
> we have a shuffle task that uses 10k reducers and 32 threads running
> currently, the memory used by the lzf stream alone would be ~ 60GB.
>
> In comparison, Snappy only allocates ~ 65MB for every
> 1k SnappyOutputStream. However, Snappy's compression is slightly lower than
> LZF's. In my experience, it leads to 10 - 20% increase in size. Compression
> ratio does matter here because we are sending data across the network.
>
> In future releases we will likely change the shuffle implementation to open
> less streams. Until that happens, I'm looking for compression codec
> implementations that are fast, allocate small buffers, and have decent
> compression ratio.
>
> Does anybody on this list have any suggestions? If not, I will submit a
> patch for 1.1 that replaces LZF with Snappy for the default compression
> codec to lower memory usage.
>
>
> allocation data here: https://gist.github.com/rxin/ad7217ea60e3fb36c567

From dev-return-8350-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:53:48 2014
Return-Path: <dev-return-8350-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0FDCD11B7D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:53:48 +0000 (UTC)
Received: (qmail 38074 invoked by uid 500); 14 Jul 2014 23:53:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38009 invoked by uid 500); 14 Jul 2014 23:53:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 37997 invoked by uid 99); 14 Jul 2014 23:53:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:53:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jhartlaub@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:53:42 +0000
Received: by mail-ob0-f182.google.com with SMTP id wm4so4997174obc.13
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:53:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=e3XCNVUFq8lWo1ml2oS0O1rX52GAVkzLmlkT5C5auh0=;
        b=B9EjFSKcL4+615UixT6f8BW9x9OGTspBH7YijxwEeTMWxh5Q6hOEcNDvPT9ABCzuXp
         1EIeKRhn/EBCzguKldTV7+198FRFvMg/S29L8nots/3L90cOaED5lEKOpXAdfmWIjyvl
         Fda3MyaORG13fhPKfIF2BuoAbTxg5rQKA6JI3Kt+6fM4kN4Ex748KCk4w+9wOl4EmZDw
         9mbA2c6RjTALldtK1CHLmkw7bSTEn49DAUODPOOBxB0BFmjvIXlcU51dWOa+ZZnWMC+G
         IW9aOk+7LrVSW4qsdVOSCJYV9zW9JdyE9N5giTrk8LJ3WuxVlWhO4PA6Rl4Z/82W6GYu
         nf+w==
MIME-Version: 1.0
X-Received: by 10.182.65.66 with SMTP id v2mr6555371obs.74.1405382002320; Mon,
 14 Jul 2014 16:53:22 -0700 (PDT)
Received: by 10.60.175.70 with HTTP; Mon, 14 Jul 2014 16:53:22 -0700 (PDT)
In-Reply-To: <CAPh_B=abZT0S94+ESrWBYx2FJw7O1_g84AGep3sRq1nkXJDBMA@mail.gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
	<20140714173027.6e60b16f@sh9>
	<1C3442B5-637F-4DA0-BD98-F1F95E2AEFD9@gmail.com>
	<CAPh_B=abZT0S94+ESrWBYx2FJw7O1_g84AGep3sRq1nkXJDBMA@mail.gmail.com>
Date: Mon, 14 Jul 2014 16:53:22 -0700
Message-ID: <CAFnjqBf9ecoABf0u4UJfhy1iGDgkJbdhyCt1iPamYBLp61DwCQ@mail.gmail.com>
Subject: Re: better compression codecs for shuffle blocks?
From: Jon Hartlaub <jhartlaub@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: Matei Zaharia <matei.zaharia@gmail.com>, "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01538dee05ad6004fe2fff55
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01538dee05ad6004fe2fff55
Content-Type: text/plain; charset=UTF-8

Is the held memory due to just instantiating the LZFOutputStream?  If so,
I'm a surprised and I consider that a bug.

I suspect the held memory may be due to a SoftReference - memory will be
released with enough memory pressure.

Finally, is it necessary to keep 1000 (or more) decoders active?  Would it
be possible to keep an object pool of encoders and check them in and out as
needed?  I admit I have not done much homework to determine if this is
viable.

-Jon


On Mon, Jul 14, 2014 at 4:08 PM, Reynold Xin <rxin@databricks.com> wrote:

> Copying Jon here since he worked on the lzf library at Ning.
>
> Jon - any comments on this topic?
>
>
> On Mon, Jul 14, 2014 at 3:54 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
>
>> You can actually turn off shuffle compression by setting
>> spark.shuffle.compress to false. Try that out, there will still be some
>> buffers for the various OutputStreams, but they should be smaller.
>>
>> Matei
>>
>> On Jul 14, 2014, at 3:30 PM, Stephen Haberman <stephen.haberman@gmail.com>
>> wrote:
>>
>> >
>> > Just a comment from the peanut gallery, but these buffers are a real
>> > PITA for us as well. Probably 75% of our non-user-error job failures
>> > are related to them.
>> >
>> > Just naively, what about not doing compression on the fly? E.g. during
>> > the shuffle just write straight to disk, uncompressed?
>> >
>> > For us, we always have plenty of disk space, and if you're concerned
>> > about network transmission, you could add a separate compress step
>> > after the blocks have been written to disk, but before being sent over
>> > the wire.
>> >
>> > Granted, IANAE, so perhaps this is a bad idea; either way, awesome to
>> > see work in this area!
>> >
>> > - Stephen
>> >
>>
>>
>

--089e01538dee05ad6004fe2fff55--

From dev-return-8351-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 14 23:53:53 2014
Return-Path: <dev-return-8351-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3212D11B7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 14 Jul 2014 23:53:53 +0000 (UTC)
Received: (qmail 39015 invoked by uid 500); 14 Jul 2014 23:53:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38956 invoked by uid 500); 14 Jul 2014 23:53:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38945 invoked by uid 99); 14 Jul 2014 23:53:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:53:52 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 14 Jul 2014 23:53:50 +0000
Received: by mail-qc0-f174.google.com with SMTP id o8so1318003qcw.19
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 16:53:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=qkMSA4g+6UpjebGntuHkclzjAXgobAsidhgibRqdFPs=;
        b=WHliOBnUEgxz0MkO2YwWek5v0jyjA62KixFrU7vJ8tPKss7bdaf6nlesT/fxkc6sgd
         cWnSaitmdth9uPyY/Hh8FeJkGJteqJG/9YAas+iy2AKU4L2LOprg/qRoOe6l0AJbuUhp
         GLtaQL5UjLstxXtn7LT3SKfoYn1Ni7PJEhjD+BeFRIZLtFelVkUMkpeTJXb+UaXnMG0m
         dPIJcQMMxf2hwYQfPUt4xERV7S+s6nx64YBOPMR1OIhfi7kv8+fu9iUXSKTeiiKUmYhw
         cZDHXVc535FPOLM6s/vka+No45fUfCW+d3E6UDT09WVKgI3Qo8HJkBbHG1mwlbaMKREI
         dlwg==
X-Gm-Message-State: ALoCoQlsLAXaO9thXcsuw2ADQnuz0vKRfeuxEGXxdKxGrIli1sgfROXiFP9EAGExC54PvNQtwQfs
MIME-Version: 1.0
X-Received: by 10.140.16.67 with SMTP id 61mr28960926qga.28.1405382005896;
 Mon, 14 Jul 2014 16:53:25 -0700 (PDT)
Received: by 10.229.183.130 with HTTP; Mon, 14 Jul 2014 16:53:25 -0700 (PDT)
Date: Mon, 14 Jul 2014 16:53:25 -0700
Message-ID: <CAEYYnxaMOjTPWXONZgsvuyAF3bPHOCzzbN6POixsMUAmiCD7tw@mail.gmail.com>
Subject: SBT gen-idea doesn't work well after merging SPARK-1776
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I've a clean clone of spark master repository, and I generated the
intellij project file by sbt gen-idea as usual. There are two issues
we have after merging SPARK-1776 (read dependencies from Maven).

1) After SPARK-1776, sbt gen-idea will download the dependencies from
internet even those jars are in local cache. Before merging, the
second time we run gen-idea will not download anything but use the
jars in cache.

2) The tests with spark local context can not be run in the intellij.
It will show the following exception.

The current workaround we've are checking out any snapshot before
merging to gen-idea, and then switch back to current master. But this
will not work when the master deviate too much from the latest working
snapshot.

[ERROR] [07/14/2014 16:27:49.967] [ScalaTest-run] [Remoting] Remoting
error: [Startup timed out] [
akka.remote.RemoteTransportException: Startup timed out
at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)
at akka.remote.Remoting.start(Remoting.scala:191)
at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
at org.apache.spark.SparkEnv$.create(SparkEnv.scala:153)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:117)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:132)
at org.apache.spark.mllib.util.LocalSparkContext$class.beforeAll(LocalSparkContext.scala:29)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)
at org.apache.spark.mllib.optimization.LBFGSSuite.run(LBFGSSuite.scala:27)
at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
at scala.collection.immutable.List.foreach(List.scala:318)
at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
at org.scalatest.tools.Runner$.run(Runner.scala:883)
at org.scalatest.tools.Runner.run(Runner.scala)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
Caused by: java.util.concurrent.TimeoutException: Futures timed out
after [10000 milliseconds]
at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
at scala.concurrent.Await$.result(package.scala:107)
at akka.remote.Remoting.start(Remoting.scala:173)
... 35 more
]

An exception or error caused a run to abort: Futures timed out after
[10000 milliseconds]
java.util.concurrent.TimeoutException: Futures timed out after [10000
milliseconds]
at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
at scala.concurrent.Await$.result(package.scala:107)
at akka.remote.Remoting.start(Remoting.scala:173)
at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
at org.apache.spark.SparkEnv$.create(SparkEnv.scala:153)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:117)
at org.apache.spark.SparkContext.<init>(SparkContext.scala:132)
at org.apache.spark.mllib.util.LocalSparkContext$class.beforeAll(LocalSparkContext.scala:29)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)
at org.apache.spark.mllib.optimization.LBFGSSuite.run(LBFGSSuite.scala:27)
at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
at scala.collection.immutable.List.foreach(List.scala:318)
at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
at org.scalatest.tools.Runner$.run(Runner.scala:883)
at org.scalatest.tools.Runner.run(Runner.scala)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)


Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

From dev-return-8352-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 00:07:05 2014
Return-Path: <dev-return-8352-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DE6E11C0B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 00:07:05 +0000 (UTC)
Received: (qmail 62161 invoked by uid 500); 15 Jul 2014 00:07:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62111 invoked by uid 500); 15 Jul 2014 00:07:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62084 invoked by uid 99); 15 Jul 2014 00:07:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 00:07:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.216.174 as permitted sender)
Received: from [209.85.216.174] (HELO mail-qc0-f174.google.com) (209.85.216.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 00:07:02 +0000
Received: by mail-qc0-f174.google.com with SMTP id o8so1366405qcw.33
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 17:06:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=bSI5DEMYV0p/A78pFZxOoBZGbaTbroBvEpryXn44fMY=;
        b=ctZ1N0IWLrC9ME61WvoKCCMMl86oP/Lur1C8gNMOGX/yZmnecTdsRlWC6VJQGdpV7C
         vkCts8jZH6OvCwo3FxftZYMVjgJLMUVlIDFn5aKkoi+dgCgbCN4/ZSbO+CLx52UDC3ZD
         jxCj6hxFJi5amTecpE/0/XCwbCMCJ3LoGEUrrLQm+Hjy4E0QXSIGQwT2sVtEjMcyUFgL
         dm7u5Grpjegj0N5JpacgRxuGWZ1uHqzCmNz1y4etyAfckt8l9dFoqUSm8aeYGGsQpeLM
         b1RgfaQzuiTvJM2P7Y0LeLEseTgr1rhw8orT6GjJjMW7zS+yBUJkEupp7Y7ePptiSY/+
         SDEw==
X-Received: by 10.140.105.102 with SMTP id b93mr29615922qgf.3.1405382797743;
 Mon, 14 Jul 2014 17:06:37 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 14 Jul 2014 17:06:17 -0700 (PDT)
In-Reply-To: <CAFnjqBf9ecoABf0u4UJfhy1iGDgkJbdhyCt1iPamYBLp61DwCQ@mail.gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
 <20140714173027.6e60b16f@sh9> <1C3442B5-637F-4DA0-BD98-F1F95E2AEFD9@gmail.com>
 <CAPh_B=abZT0S94+ESrWBYx2FJw7O1_g84AGep3sRq1nkXJDBMA@mail.gmail.com> <CAFnjqBf9ecoABf0u4UJfhy1iGDgkJbdhyCt1iPamYBLp61DwCQ@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Jul 2014 17:06:17 -0700
Message-ID: <CANGvG8oOKFHNn6tNMaV3Oo_-A1OGAZsRj8DDS-MgPQAeMtPPJw@mail.gmail.com>
Subject: Re: better compression codecs for shuffle blocks?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113985346ee0ae04fe302e68
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113985346ee0ae04fe302e68
Content-Type: text/plain; charset=UTF-8

One of the core problems here is the number of open streams we have, which
is (# cores * # reduce partitions), which can easily climb into the tens of
thousands for large jobs. This is a more general problem that we are
planning on fixing for our largest shuffles, as even moderate buffer sizes
can explode to use huge amounts of memory at that scale.


On Mon, Jul 14, 2014 at 4:53 PM, Jon Hartlaub <jhartlaub@gmail.com> wrote:

> Is the held memory due to just instantiating the LZFOutputStream?  If so,
> I'm a surprised and I consider that a bug.
>
> I suspect the held memory may be due to a SoftReference - memory will be
> released with enough memory pressure.
>
> Finally, is it necessary to keep 1000 (or more) decoders active?  Would it
> be possible to keep an object pool of encoders and check them in and out as
> needed?  I admit I have not done much homework to determine if this is
> viable.
>
> -Jon
>
>
> On Mon, Jul 14, 2014 at 4:08 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Copying Jon here since he worked on the lzf library at Ning.
> >
> > Jon - any comments on this topic?
> >
> >
> > On Mon, Jul 14, 2014 at 3:54 PM, Matei Zaharia <matei.zaharia@gmail.com>
> > wrote:
> >
> >> You can actually turn off shuffle compression by setting
> >> spark.shuffle.compress to false. Try that out, there will still be some
> >> buffers for the various OutputStreams, but they should be smaller.
> >>
> >> Matei
> >>
> >> On Jul 14, 2014, at 3:30 PM, Stephen Haberman <
> stephen.haberman@gmail.com>
> >> wrote:
> >>
> >> >
> >> > Just a comment from the peanut gallery, but these buffers are a real
> >> > PITA for us as well. Probably 75% of our non-user-error job failures
> >> > are related to them.
> >> >
> >> > Just naively, what about not doing compression on the fly? E.g. during
> >> > the shuffle just write straight to disk, uncompressed?
> >> >
> >> > For us, we always have plenty of disk space, and if you're concerned
> >> > about network transmission, you could add a separate compress step
> >> > after the blocks have been written to disk, but before being sent over
> >> > the wire.
> >> >
> >> > Granted, IANAE, so perhaps this is a bad idea; either way, awesome to
> >> > see work in this area!
> >> >
> >> > - Stephen
> >> >
> >>
> >>
> >
>

--001a113985346ee0ae04fe302e68--

From dev-return-8353-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 01:02:13 2014
Return-Path: <dev-return-8353-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7FF8411E47
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 01:02:13 +0000 (UTC)
Received: (qmail 54315 invoked by uid 500); 15 Jul 2014 01:02:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54273 invoked by uid 500); 15 Jul 2014 01:02:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54262 invoked by uid 99); 15 Jul 2014 01:02:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:02:12 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of wibenton@redhat.com designates 209.132.183.24 as permitted sender)
Received: from [209.132.183.24] (HELO mx3-phx2.redhat.com) (209.132.183.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:02:07 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx3-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s6F11jNr000713
	for <dev@spark.apache.org>; Mon, 14 Jul 2014 21:01:45 -0400
Date: Mon, 14 Jul 2014 21:01:41 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <676330761.16923904.1405386101301.JavaMail.zimbra@redhat.com>
In-Reply-To: <CANGvG8rhCbmNUGKJbSh7o8n023BVoCAV3OUTexpmEtwf90jm5Q@mail.gmail.com>
References: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com> <674ECB29-553B-4927-A64B-AFC6E5F97FED@gmail.com> <643342387.16800517.1405375141069.JavaMail.zimbra@redhat.com> <CANGvG8rhCbmNUGKJbSh7o8n023BVoCAV3OUTexpmEtwf90jm5Q@mail.gmail.com>
Subject: Re: Profiling Spark tests with YourKit (or something else)
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF30 (Mac)/8.0.6_GA_5922)
Thread-Topic: Profiling Spark tests with YourKit (or something else)
Thread-Index: Mv/6l1QlGQ8Ckm/Q56w6ig6u+R0DUA==
X-Virus-Checked: Checked by ClamAV on apache.org

----- Original Message -----
> From: "Aaron Davidson" <ilikerps@gmail.com>
> To: dev@spark.apache.org
> Sent: Monday, July 14, 2014 5:21:10 PM
> Subject: Re: Profiling Spark tests with YourKit (or something else)
> 
> Out of curiosity, what problems are you seeing with Utils.getCallSite?

Aaron, if I enable call site tracking or CPU profiling in YourKit, many (but not all) Spark test cases will NPE on the line filtering out "getStackTrace" from the stack trace (this is Utils.scala:812 in the current master).  I'm not sure if this is a consequence of Thread#getStackTrace including bogus frames when running instrumented or if whatever instrumentation YourKit inserts relies on assumptions that don't always hold for Scala code.


best,
wb

From dev-return-8354-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 01:22:53 2014
Return-Path: <dev-return-8354-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 43F2211EAD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 01:22:53 +0000 (UTC)
Received: (qmail 1499 invoked by uid 500); 15 Jul 2014 01:22:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1440 invoked by uid 500); 15 Jul 2014 01:22:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1427 invoked by uid 99); 15 Jul 2014 01:22:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:22:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of cody.koeninger@mediacrossing.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:22:47 +0000
Received: by mail-we0-f181.google.com with SMTP id q59so4858641wes.12
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 18:22:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=mediacrossing.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=b1y4MxN5w2nIdNGiVzzBS9LmwCRe0pMxu02RMWJMZzM=;
        b=Mjr8EyS9hNYLSPH1sRHBiT99V7Q4WOpjzVpqi2HMa/ehox+dhPxJLa8uFAOr0I4dj3
         EBs1+U/7ggB+KuxjrwwiRY19lVapHPl8UDA8KNMiH31cWtvKSGBhQkWzp8pvtRpl1arC
         ucMp4LqWwFCAHPbkWMTKlbrmOuUgDaydvxA80=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=b1y4MxN5w2nIdNGiVzzBS9LmwCRe0pMxu02RMWJMZzM=;
        b=al8zjA15qAeFixYCSsCrwqWF19wGMv1wQCIjUprlisVyBxRGwj/SqU9lfBLu9SKYnl
         fpX4QkDgHBr678ikWBgL+nyk1gbQwpSIZakZqa3bABu+YQkOVyNeD4Iv9Sk6jFqBgU4t
         k4d+hhtx6rvgLpaxV51P28bBAdseBD8Ba1ha1Z35pkPM4eckxZhyjk8T6JbXIq/B9CJF
         3EVsYMHTUc/1xgBun/2SKbsFhMm64MZb8O8Um9u6/Ey1RRNs5LBwQ7ulquIa2tpIC94r
         W8SPwmDhFToK27pxXtiXTTwnwqKgI4/UQyZh0SxNrQY780h0uKWtleVwCAk386NZ1M1+
         sX+w==
X-Gm-Message-State: ALoCoQlbe/0OGulIAxuQZZYPYRG1oU1Ok0nBgBsg2EDL9TXXWcCL2mvwRVQZX8lIYUZ73GWS2pPg
MIME-Version: 1.0
X-Received: by 10.180.92.38 with SMTP id cj6mr1621728wib.64.1405387345669;
 Mon, 14 Jul 2014 18:22:25 -0700 (PDT)
Received: by 10.194.15.33 with HTTP; Mon, 14 Jul 2014 18:22:25 -0700 (PDT)
In-Reply-To: <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
Date: Mon, 14 Jul 2014 20:22:25 -0500
Message-ID: <CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Cody Koeninger <cody.koeninger@mediacrossing.com>
To: dev@spark.apache.org
Content-Type: multipart/mixed; boundary=f46d043c7faa83175204fe313d75
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c7faa83175204fe313d75
Content-Type: multipart/alternative; boundary=f46d043c7faa83174e04fe313d73

--f46d043c7faa83174e04fe313d73
Content-Type: text/plain; charset=UTF-8

Here's the entire jstack output.


On Mon, Jul 14, 2014 at 4:44 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Cody,
>
> This Jstack seems truncated, would you mind giving the entire stack
> trace? For the second thread, for instance, we can't see where the
> lock is being acquired.
>
> - Patrick
>
> On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> <cody.koeninger@mediacrossing.com> wrote:
> > Hi all, just wanted to give a heads up that we're seeing a reproducible
> > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> >
> > If jira is a better place for this, apologies in advance - figured
> talking
> > about it on the mailing list was friendlier than randomly (re)opening
> jira
> > tickets.
> >
> > I know Gary had mentioned some issues with 1.0.1 on the mailing list,
> once
> > we got a thread dump I wanted to follow up.
> >
> > The thread dump shows the deadlock occurs in the synchronized block of
> code
> > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> >
> > Relevant portions of the thread dump are summarized below, we can provide
> > the whole dump if it's useful.
> >
> > Found one Java-level deadlock:
> > =============================
> > "Executor task launch worker-1":
> >   waiting to lock monitor 0x00007f250400c520 (object 0x00000000fae7dc30,
> a
> > org.apache.hadoop.co
> > nf.Configuration),
> >   which is held by "Executor task launch worker-0"
> > "Executor task launch worker-0":
> >   waiting to lock monitor 0x00007f2520495620 (object 0x00000000faeb4fc8,
> a
> > java.lang.Class),
> >   which is held by "Executor task launch worker-1"
> >
> >
> > "Executor task launch worker-1":
> >         at
> >
> org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
> >         - waiting to lock <0x00000000fae7dc30> (a
> > org.apache.hadoop.conf.Configuration)
> >         at
> >
> org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
> >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> > org.apache.hadoop.conf.Configurati
> > on)
> >         at
> >
> org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
> >         at
> >
> org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
> > )
> >         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > Method)
> >         at
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > java:57)
> >         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
> > Method)
> >         at
> >
> sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
> > java:57)
> >         at
> >
> sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
> > sorImpl.java:45)
> >         at
> java.lang.reflect.Constructor.newInstance(Constructor.java:525)
> >         at java.lang.Class.newInstance0(Class.java:374)
> >         at java.lang.Class.newInstance(Class.java:327)
> >         at
> java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
> >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> >         at
> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
> >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> > org.apache.hadoop.fs.FileSystem)
> >         at
> > org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >         at
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >         at
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >         at
> org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >         at
> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
> >
> >
> >
> > ...elided...
> >
> >
> > "Executor task launch worker-0" daemon prio=10 tid=0x0000000001e71800
> > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> >    java.lang.Thread.State: BLOCKED (on object monitor)
> >         at
> > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
> >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
> > org.apache.hadoop.fs.FileSystem)
> >         at
> > org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
> >         at
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
> >         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
> >         at
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
> >         at
> org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
> >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
> >         at
> > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
> >         at
> >
> org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
> >         at
> >
> org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>

--f46d043c7faa83174e04fe313d73
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Here&#39;s the entire jstack output.<br></div><div class=
=3D"gmail_extra"><br><br><div class=3D"gmail_quote">On Mon, Jul 14, 2014 at=
 4:44 PM, Patrick Wendell <span dir=3D"ltr">&lt;<a href=3D"mailto:pwendell@=
gmail.com" target=3D"_blank">pwendell@gmail.com</a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">Hey Cody,<br>
<br>
This Jstack seems truncated, would you mind giving the entire stack<br>
trace? For the second thread, for instance, we can&#39;t see where the<br>
lock is being acquired.<br>
<span class=3D"HOEnZb"><font color=3D"#888888"><br>
- Patrick<br>
</font></span><div class=3D"HOEnZb"><div class=3D"h5"><br>
On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger<br>
&lt;<a href=3D"mailto:cody.koeninger@mediacrossing.com">cody.koeninger@medi=
acrossing.com</a>&gt; wrote:<br>
&gt; Hi all, just wanted to give a heads up that we&#39;re seeing a reprodu=
cible<br>
&gt; deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2<br>
&gt;<br>
&gt; If jira is a better place for this, apologies in advance - figured tal=
king<br>
&gt; about it on the mailing list was friendlier than randomly (re)opening =
jira<br>
&gt; tickets.<br>
&gt;<br>
&gt; I know Gary had mentioned some issues with 1.0.1 on the mailing list, =
once<br>
&gt; we got a thread dump I wanted to follow up.<br>
&gt;<br>
&gt; The thread dump shows the deadlock occurs in the synchronized block of=
 code<br>
&gt; that was changed in HadoopRDD.scala, for the Spark-1097 issue<br>
&gt;<br>
&gt; Relevant portions of the thread dump are summarized below, we can prov=
ide<br>
&gt; the whole dump if it&#39;s useful.<br>
&gt;<br>
&gt; Found one Java-level deadlock:<br>
&gt; =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D<br>
&gt; &quot;Executor task launch worker-1&quot;:<br>
&gt; =C2=A0 waiting to lock monitor 0x00007f250400c520 (object 0x00000000fa=
e7dc30, a<br>
&gt; <a href=3D"http://org.apache.hadoop.co" target=3D"_blank">org.apache.h=
adoop.co</a><br>
&gt; nf.Configuration),<br>
&gt; =C2=A0 which is held by &quot;Executor task launch worker-0&quot;<br>
&gt; &quot;Executor task launch worker-0&quot;:<br>
&gt; =C2=A0 waiting to lock monitor 0x00007f2520495620 (object 0x00000000fa=
eb4fc8, a<br>
&gt; java.lang.Class),<br>
&gt; =C2=A0 which is held by &quot;Executor task launch worker-1&quot;<br>
&gt;<br>
&gt;<br>
&gt; &quot;Executor task launch worker-1&quot;:<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration=
.java:791)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 - waiting to lock &lt;0x00000000fae7dc30&g=
t; (a<br>
&gt; org.apache.hadoop.conf.Configuration)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.=
java:690)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 - locked &lt;0x00000000faca6ff8&gt; (a jav=
a.lang.Class for<br>
&gt; org.apache.hadoop.conf.Configurati<br>
&gt; on)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.hdfs.HdfsConfiguration.&lt;clinit&gt;(HdfsConfigurat=
ion.java:34)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.hdfs.DistributedFileSystem.&lt;clinit&gt;(Distribute=
dFileSystem.java:110<br>
&gt; )<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at sun.reflect.NativeConstructorAccessorIm=
pl.newInstance0(Native<br>
&gt; Method)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructo=
rAccessorImpl.<br>
&gt; java:57)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at sun.reflect.NativeConstructorAccessorIm=
pl.newInstance0(Native<br>
&gt; Method)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructo=
rAccessorImpl.<br>
&gt; java:57)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingCo=
nstructorAcces<br>
&gt; sorImpl.java:45)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at java.lang.reflect.Constructor.newInstan=
ce(Constructor.java:525)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at java.lang.Class.newInstance0(Class.java=
:374)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at java.lang.Class.newInstance(Class.java:=
327)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at java.util.ServiceLoader$LazyIterator.ne=
xt(ServiceLoader.java:373)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at java.util.ServiceLoader$1.next(ServiceL=
oader.java:445)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)<=
br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 - locked &lt;0x00000000faeb4fc8&gt; (a jav=
a.lang.Class for<br>
&gt; org.apache.hadoop.fs.FileSystem)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:237=
5)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)=
<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem.access$=
200(FileSystem.java:89)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431=
)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem$Cache.g=
et(FileSystem.java:2413)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem.get(Fil=
eSystem.java:368)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem.get(Fil=
eSystem.java:167)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)=
<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat=
.java:315)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat=
.java:288)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546=
)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546=
)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.s=
cala:145)<br>
&gt;<br>
&gt;<br>
&gt;<br>
&gt; ...elided...<br>
&gt;<br>
&gt;<br>
&gt; &quot;Executor task launch worker-0&quot; daemon prio=3D10 tid=3D0x000=
0000001e71800<br>
&gt; nid=3D0x2d97 waiting for monitor entry [0x00007f24d2bf1000]<br>
&gt; =C2=A0 =C2=A0java.lang.Thread.State: BLOCKED (on object monitor)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)<=
br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 - waiting to lock &lt;0x00000000faeb4fc8&g=
t; (a java.lang.Class for<br>
&gt; org.apache.hadoop.fs.FileSystem)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:237=
5)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)=
<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem.access$=
200(FileSystem.java:89)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431=
)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem$Cache.g=
et(FileSystem.java:2413)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem.get(Fil=
eSystem.java:368)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem.get(Fil=
eSystem.java:167)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)=
<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat=
.java:315)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat=
.java:288)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546=
)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546=
)<br>
&gt; =C2=A0 =C2=A0 =C2=A0 =C2=A0 at<br>
&gt; org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.s=
cala:145)<br>
</div></div></blockquote></div><br></div>

--f46d043c7faa83174e04fe313d73--
--f46d043c7faa83175204fe313d75
Content-Type: text/plain; charset=US-ASCII; name="hung_spark_jstack2.txt"
Content-Disposition: attachment; filename="hung_spark_jstack2.txt"
Content-Transfer-Encoding: base64
X-Attachment-Id: f_hxmjbail0
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--f46d043c7faa83175204fe313d75--

From dev-return-8355-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 01:39:03 2014
Return-Path: <dev-return-8355-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BA5A211EEC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 01:39:03 +0000 (UTC)
Received: (qmail 23750 invoked by uid 500); 15 Jul 2014 01:39:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23693 invoked by uid 500); 15 Jul 2014 01:39:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23678 invoked by uid 99); 15 Jul 2014 01:39:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:39:02 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.192.51 as permitted sender)
Received: from [209.85.192.51] (HELO mail-qg0-f51.google.com) (209.85.192.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:38:57 +0000
Received: by mail-qg0-f51.google.com with SMTP id a108so4155190qge.10
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 18:38:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=drQgnQO0SId7yVFd/nm81ZcXRer4d2UouABKbEOL28E=;
        b=EjLM3bKbhX+C61Dj2zU/381gQdFZc//P5d7n2wq3fditkqUi7bMR/cv2O7nvA6XeBL
         R7X14uD+tRUQUdwfn222inGHRY63sHpIarb2xUdU+WK1C6Nwe3Xk3Q5b7fDJ/1LLQ5g/
         uUNzYAZtyPvtDgbV0/IjsstNkMCUoOUFzq9bIAwvPzUius3JFIm9jxQdKBSNC5M+3r53
         j90wyjgUxnG0ivN5z3DXVjF4NYqS70f+yLzPGQbZ26w8ZPY/QrY2+SgDZtOX8blHRhID
         /QARCcHEfusPF8y+ZGWhn5wTKg7qfnvzRT/Uz4S9BZDTDT9c51yf3S9byCAu5eyY3g3r
         CQcQ==
X-Received: by 10.140.105.102 with SMTP id b93mr30062307qgf.3.1405388316846;
 Mon, 14 Jul 2014 18:38:36 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 14 Jul 2014 18:38:16 -0700 (PDT)
In-Reply-To: <676330761.16923904.1405386101301.JavaMail.zimbra@redhat.com>
References: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com>
 <674ECB29-553B-4927-A64B-AFC6E5F97FED@gmail.com> <643342387.16800517.1405375141069.JavaMail.zimbra@redhat.com>
 <CANGvG8rhCbmNUGKJbSh7o8n023BVoCAV3OUTexpmEtwf90jm5Q@mail.gmail.com> <676330761.16923904.1405386101301.JavaMail.zimbra@redhat.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Jul 2014 18:38:16 -0700
Message-ID: <CANGvG8pesqb7w7vw5SAHWXMuK8_a25_9d7rA2EmSmdJZsNgAvQ@mail.gmail.com>
Subject: Re: Profiling Spark tests with YourKit (or something else)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1139853465b48904fe31776c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1139853465b48904fe31776c
Content-Type: text/plain; charset=UTF-8

Would you mind filing a JIRA for this? That does sound like something bogus
happening on the JVM/YourKit level, but this sort of diagnosis is
sufficiently important that we should be resilient against it.


On Mon, Jul 14, 2014 at 6:01 PM, Will Benton <willb@redhat.com> wrote:

> ----- Original Message -----
> > From: "Aaron Davidson" <ilikerps@gmail.com>
> > To: dev@spark.apache.org
> > Sent: Monday, July 14, 2014 5:21:10 PM
> > Subject: Re: Profiling Spark tests with YourKit (or something else)
> >
> > Out of curiosity, what problems are you seeing with Utils.getCallSite?
>
> Aaron, if I enable call site tracking or CPU profiling in YourKit, many
> (but not all) Spark test cases will NPE on the line filtering out
> "getStackTrace" from the stack trace (this is Utils.scala:812 in the
> current master).  I'm not sure if this is a consequence of
> Thread#getStackTrace including bogus frames when running instrumented or if
> whatever instrumentation YourKit inserts relies on assumptions that don't
> always hold for Scala code.
>
>
> best,
> wb
>

--001a1139853465b48904fe31776c--

From dev-return-8356-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 01:44:26 2014
Return-Path: <dev-return-8356-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5BAA11F19
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 01:44:26 +0000 (UTC)
Received: (qmail 50089 invoked by uid 500); 15 Jul 2014 01:44:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50029 invoked by uid 500); 15 Jul 2014 01:44:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50017 invoked by uid 99); 15 Jul 2014 01:44:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:44:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:44:21 +0000
Received: by mail-wi0-f172.google.com with SMTP id n3so3497539wiv.11
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 18:43:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=EcRnkOg3l7J2biBt+gceiJgMGR3DGWroxCxT6dEmI5I=;
        b=F5l1Qb+hOg6GGgB9kP4HusNmHfO3Ec4tt+eoaCAdhGDvMOJL5R3XVyp4SNXFUdrkWb
         l5sUaqT9f/+JPIeW/MjhJylOwDEu/IjJfmFd9qpRB7YPYfDRIJ+wBOAzHeyk/ADSSR18
         YAOjhiCNXP9kfku/PxVxoHthJJNo7BI6+Nv4+QSYu7RnNuQcjs0JQc9av/096lxH/Ob3
         cKsbdQ5U/4QIRRjf/zTNcKWOcEEloVCp5N5I9LZuxLKG0+2rk5jCvo9JBbACPn7yKtpn
         7EmcYl77ExO6RW7CUebhGKZaXPHSqa3EodQW7k0sVmezXlHiS8s22ZzH7Jk22kJiwWgI
         t6Yw==
X-Received: by 10.180.208.13 with SMTP id ma13mr1614483wic.45.1405388637477;
 Mon, 14 Jul 2014 18:43:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.24.168 with HTTP; Mon, 14 Jul 2014 18:43:17 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 14 Jul 2014 21:43:17 -0400
Message-ID: <CAOhmDzcN=ehCrms2krYX-gY2bPTu4P-KGEMaQd9vK2+jot9_vQ@mail.gmail.com>
Subject: ec2 clusters launched at 9fe693b5b6 are broken (?)
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c38d8a82229904fe318ab0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c38d8a82229904fe318ab0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Just launched an EC2 cluster from git hash
9fe693b5b6ed6af34ee1e800ab89c8a11991ea38. Calling take() on an RDD
accessing data in S3 yields the following error output.

I understand that NoClassDefFoundError errors may mean something in the
deployment was messed up. Is that correct? When I launch a cluster using
spark-ec2, I expect all critical deployment details to be taken care of by
the script.

So is something in the deployment executed by spark-ec2 borked?

Nick

java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException
    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(=
NativeS3FileSystem.java:224)
    at org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3=
FileSystem.java:214)
    at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:138=
6)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
    at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.=
java:176)
    at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.j=
ava:208)
    at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:176)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD=
.scala:32)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:71)
    at org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:7=
9)
    at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:190=
)
    at org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:188=
)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.dependencies(RDD.scala:188)
    at org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGSchedule=
r.scala:1144)
    at org.apache.spark.SparkContext.getPreferredLocs(SparkContext.scala:90=
3)
    at org.apache.spark.rdd.PartitionCoalescer.currPrefLocs(CoalescedRDD.sc=
ala:174)
    at org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$=
$anonfun$apply$2.apply(CoalescedRDD.scala:191)
    at org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$=
$anonfun$apply$2.apply(CoalescedRDD.scala:190)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
    at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
    at org.apache.spark.rdd.PartitionCoalescer$LocationIterator.<init>(Coal=
escedRDD.scala:185)
    at org.apache.spark.rdd.PartitionCoalescer.setupGroups(CoalescedRDD.sca=
la:236)
    at org.apache.spark.rdd.PartitionCoalescer.run(CoalescedRDD.scala:337)
    at org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:8=
3)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
    at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
    at scala.Option.getOrElse(Option.scala:120)
    at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
    at org.apache.spark.rdd.RDD.take(RDD.scala:1036)
    at $iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
    at $iwC$$iwC$$iwC.<init>(<console>:31)
    at $iwC$$iwC.<init>(<console>:33)
    at $iwC.<init>(<console>:35)
    at <init>(<console>:37)
    at .<init>(<console>:41)
    at .<clinit>(<console>)
    at .<init>(<console>:7)
    at .<clinit>(<console>)
    at $print(<console>)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl=
.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcce=
ssorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala=
:788)
    at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala=
:1056)
    at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:61=
4)
    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
    at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
    at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:=
796)
    at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.sc=
ala:841)
    at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
    at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
    at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
    at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(Spa=
rkILoop.scala:936)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop=
.scala:884)
    at org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop=
.scala:884)
    at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClas=
sLoader.scala:135)
    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
    at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
    at org.apache.spark.repl.Main$.main(Main.scala:31)
    at org.apache.spark.repl.Main.main(Main.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl=
.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAcce=
ssorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
    at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
    at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException:
org.jets3t.service.S3ServiceException
    at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
    at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
    at java.security.AccessController.doPrivileged(Native Method)
    at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
    ... 92 more

=E2=80=8B

--001a11c38d8a82229904fe318ab0--

From dev-return-8357-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 01:50:20 2014
Return-Path: <dev-return-8357-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7681A11F3F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 01:50:20 +0000 (UTC)
Received: (qmail 61885 invoked by uid 500); 15 Jul 2014 01:50:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61829 invoked by uid 500); 15 Jul 2014 01:50:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61818 invoked by uid 99); 15 Jul 2014 01:50:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:50:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wangfei1@huawei.com designates 119.145.14.66 as permitted sender)
Received: from [119.145.14.66] (HELO szxga03-in.huawei.com) (119.145.14.66)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 01:50:17 +0000
Received: from 172.24.2.119 (EHLO szxeml406-hub.china.huawei.com) ([172.24.2.119])
	by szxrg03-dlp.huawei.com (MOS 4.4.3-GA FastPath queued)
	with ESMTP id ARQ09689;
	Tue, 15 Jul 2014 09:49:49 +0800 (CST)
Received: from [127.0.0.1] (10.177.17.18) by szxeml406-hub.china.huawei.com
 (10.82.67.93) with Microsoft SMTP Server id 14.3.158.1; Tue, 15 Jul 2014
 09:49:44 +0800
Message-ID: <53C488B7.402@huawei.com>
Date: Tue, 15 Jul 2014 09:49:43 +0800
From: scwf <wangfei1@huawei.com>
User-Agent: Mozilla/5.0 (Windows NT 6.1; rv:17.0) Gecko/20130509 Thunderbird/17.0.6
MIME-Version: 1.0
To: <dev@spark.apache.org>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com> <CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com> <CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
In-Reply-To: <CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
Content-Type: text/plain; charset="UTF-8"; format=flowed
Content-Transfer-Encoding: 8bit
X-Originating-IP: [10.177.17.18]
X-CFilter-Loop: Reflected
X-Mirapoint-Virus-RAPID-Raw: score=unknown(0),
	refid=str=0001.0A020203.53C488BE.0068,ss=1,re=0.000,fgs=0,
	ip=0.0.0.0,
	so=2013-05-26 15:14:31,
	dmn=2011-05-27 18:58:46
X-Mirapoint-Loop-Id: ea09869ff0043e6e8695df5bf4f6374b
X-Virus-Checked: Checked by ClamAV on apache.org

hiCody
   i met this issue days before and i post a PR for this( https://github.com/apache/spark/pull/1385)
it's very strange that if i synchronize conf it will deadlock but it is ok when synchronize initLocalJobConfFuncOpt


> Here's the entire jstack output.
>
>
> On Mon, Jul 14, 2014 at 4:44 PM, Patrick Wendell <pwendell@gmail.com <mailto:pwendell@gmail.com>> wrote:
>
>     Hey Cody,
>
>     This Jstack seems truncated, would you mind giving the entire stack
>     trace? For the second thread, for instance, we can't see where the
>     lock is being acquired.
>
>     - Patrick
>
>     On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
>     <cody.koeninger@mediacrossing.com <mailto:cody.koeninger@mediacrossing.com>> wrote:
>      > Hi all, just wanted to give a heads up that we're seeing a reproducible
>      > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
>      >
>      > If jira is a better place for this, apologies in advance - figured talking
>      > about it on the mailing list was friendlier than randomly (re)opening jira
>      > tickets.
>      >
>      > I know Gary had mentioned some issues with 1.0.1 on the mailing list, once
>      > we got a thread dump I wanted to follow up.
>      >
>      > The thread dump shows the deadlock occurs in the synchronized block of code
>      > that was changed in HadoopRDD.scala, for the Spark-1097 issue
>      >
>      > Relevant portions of the thread dump are summarized below, we can provide
>      > the whole dump if it's useful.
>      >
>      > Found one Java-level deadlock:
>      > =============================
>      > "Executor task launch worker-1":
>      >   waiting to lock monitor 0x00007f250400c520 (object 0x00000000fae7dc30, a
>      > org.apache.hadoop.co <http://org.apache.hadoop.co>
>      > nf.Configuration),
>      >   which is held by "Executor task launch worker-0"
>      > "Executor task launch worker-0":
>      >   waiting to lock monitor 0x00007f2520495620 (object 0x00000000faeb4fc8, a
>      > java.lang.Class),
>      >   which is held by "Executor task launch worker-1"
>      >
>      >
>      > "Executor task launch worker-1":
>      >         at
>      > org.apache.hadoop.conf.Configuration.reloadConfiguration(Configuration.java:791)
>      >         - waiting to lock <0x00000000fae7dc30> (a
>      > org.apache.hadoop.conf.Configuration)
>      >         at
>      > org.apache.hadoop.conf.Configuration.addDefaultResource(Configuration.java:690)
>      >         - locked <0x00000000faca6ff8> (a java.lang.Class for
>      > org.apache.hadoop.conf.Configurati
>      > on)
>      >         at
>      > org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(HdfsConfiguration.java:34)
>      >         at
>      > org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>(DistributedFileSystem.java:110
>      > )
>      >         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>      > Method)
>      >         at
>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
>      > java:57)
>      >         at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
>      > Method)
>      >         at
>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.
>      > java:57)
>      >         at
>      > sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAcces
>      > sorImpl.java:45)
>      >         at java.lang.reflect.Constructor.newInstance(Constructor.java:525)
>      >         at java.lang.Class.newInstance0(Class.java:374)
>      >         at java.lang.Class.newInstance(Class.java:327)
>      >         at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:373)
>      >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
>      >         at
>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2364)
>      >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
>      > org.apache.hadoop.fs.FileSystem)
>      >         at
>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>      >         at
>      > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>      >         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>      >         at
>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>      >         at
>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>      >         at
>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>      >         at
>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>      >         at
>      > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>      >         at
>      > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>      >         at
>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>      >
>      >
>      >
>      > ...elided...
>      >
>      >
>      > "Executor task launch worker-0" daemon prio=10 tid=0x0000000001e71800
>      > nid=0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
>      >    java.lang.Thread.State: BLOCKED (on object monitor)
>      >         at
>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(FileSystem.java:2362)
>      >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class for
>      > org.apache.hadoop.fs.FileSystem)
>      >         at
>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:2375)
>      >         at
>      > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2392)
>      >         at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:89)
>      >         at
>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2431)
>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2413)
>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:368)
>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:167)
>      >         at
>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(JobConf.java:587)
>      >         at
>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:315)
>      >         at
>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:288)
>      >         at
>      > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>      >         at
>      > org.apache.spark.SparkContext$$anonfun$22.apply(SparkContext.scala:546)
>      >         at
>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$1.apply(HadoopRDD.scala:145)
>
>


-- 

Best Regards
Fei Wang

--------------------------------------------------------------------------------



From dev-return-8358-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 02:33:28 2014
Return-Path: <dev-return-8358-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8AD4F11017
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 02:33:28 +0000 (UTC)
Received: (qmail 35067 invoked by uid 500); 15 Jul 2014 02:33:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35016 invoked by uid 500); 15 Jul 2014 02:33:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34959 invoked by uid 99); 15 Jul 2014 02:33:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 02:33:27 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 02:33:21 +0000
Received: by mail-ie0-f180.google.com with SMTP id at20so3912651iec.25
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 19:33:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=gkAmBJBfHixOcBBQWguhAMXTvm5Qm8Oa98CQYbWZ4lc=;
        b=gZ82UbrlDla9xUXsqGq6DmJMpzWWTR4vXaE17dNy9oU7OvdihqawwAyDXmicmqMDas
         BalWGJmir0RltH/GvCiUwq2zwn5nAi/SACoYqS1b9H9lhmepx1z6w4/3KwIi/JFxikHJ
         F7Y1oNDrwJ6Ko9t42pbiKueMoW5+0MA7Xs5dvSEHOjlUZ3HBRr4/uHqXTLrjGltehOPn
         LAx/uMOyXlMeFCxOZcs1oqrADBc/L36JxfHTnMGA8s6/PMjVAvfJE2cZS5jXgyb9DIac
         01eEMIb9IIdJeNVCkEqNoxmr0ZuwagjfOxfntyxEq681qvjHH3fwna1Zo501MDp38kJu
         oWyg==
X-Received: by 10.50.117.42 with SMTP id kb10mr2190622igb.37.1405391580588;
        Mon, 14 Jul 2014 19:33:00 -0700 (PDT)
Received: from [192.168.2.13] ([69.159.114.117])
        by mx.google.com with ESMTPSA id u6sm30390282igs.12.2014.07.14.19.32.59
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 19:33:00 -0700 (PDT)
Date: Mon, 14 Jul 2014 22:43:49 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: "=?utf-8?Q?dev=40spark.apache.org?=" <dev@spark.apache.org>
Message-ID: <FD645677C87E40D9A3957A789891B6C5@gmail.com>
In-Reply-To: <CAMtqZee9ZQnjxXNbgJsp1ztUrQkOsBr5A8zusZ-nx=pbjZ23nw@mail.gmail.com>
References: <EB8AF045DC3E4B07A592E6226E47BA10@gmail.com>
 <CABPQxst+bgk6CQBjOh9yygRMCOZFW2M+_zjdW017_1ms9+DbCA@mail.gmail.com>
 <CAMtqZee9ZQnjxXNbgJsp1ztUrQkOsBr5A8zusZ-nx=pbjZ23nw@mail.gmail.com>
Subject: Re: how to run the program compiled with spark 1.0.0 in the
 branch-0.1-jdbc cluster
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53c49565_12200854_1f4"
X-Virus-Checked: Checked by ClamAV on apache.org

--53c49565_12200854_1f4
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

I resolved the issue by setting an internal maven repository to contain the Spark-1.0.1 jar compiled from branch-0.1-jdbc and replacing the dependency to the central repository with our own repository 

I believe there should be some more lightweight way

Best, 

-- 
Nan Zhu


On Monday, July 14, 2014 at 6:36 AM, Nan Zhu wrote:

> Ah, sorry, sorry
> 
> It's executorState under deploy package
> 
> On Monday, July 14, 2014, Patrick Wendell <pwendell@gmail.com (mailto:pwendell@gmail.com)> wrote:
> > > 1. The first error I met is the different SerializationVersionUID in ExecuterStatus
> > >
> > > I resolved by explicitly declare SerializationVersionUID in ExecuterStatus.scala and recompile branch-0.1-jdbc
> > >
> > 
> > I don't think there is a class in Spark named ExecuterStatus (sic) ...
> > or ExecutorStatus. Is this a class you made?


--53c49565_12200854_1f4--


From dev-return-8359-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 03:44:18 2014
Return-Path: <dev-return-8359-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A3CF111B3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 03:44:17 +0000 (UTC)
Received: (qmail 50359 invoked by uid 500); 15 Jul 2014 03:44:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50292 invoked by uid 500); 15 Jul 2014 03:44:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50281 invoked by uid 99); 15 Jul 2014 03:44:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 03:44:16 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.25 as permitted sender)
Received: from [209.132.183.25] (HELO mx4-phx2.redhat.com) (209.132.183.25)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 03:44:14 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx4-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s6F3hlO0026311
	for <dev@spark.apache.org>; Mon, 14 Jul 2014 23:43:47 -0400
Date: Mon, 14 Jul 2014 23:43:46 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <1916771286.17057992.1405395826584.JavaMail.zimbra@redhat.com>
In-Reply-To: <CANGvG8pesqb7w7vw5SAHWXMuK8_a25_9d7rA2EmSmdJZsNgAvQ@mail.gmail.com>
References: <304436700.16450777.1405356671607.JavaMail.zimbra@redhat.com> <674ECB29-553B-4927-A64B-AFC6E5F97FED@gmail.com> <643342387.16800517.1405375141069.JavaMail.zimbra@redhat.com> <CANGvG8rhCbmNUGKJbSh7o8n023BVoCAV3OUTexpmEtwf90jm5Q@mail.gmail.com> <676330761.16923904.1405386101301.JavaMail.zimbra@redhat.com> <CANGvG8pesqb7w7vw5SAHWXMuK8_a25_9d7rA2EmSmdJZsNgAvQ@mail.gmail.com>
Subject: Re: Profiling Spark tests with YourKit (or something else)
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF30 (Mac)/8.0.6_GA_5922)
Thread-Topic: Profiling Spark tests with YourKit (or something else)
Thread-Index: IUT0I6iApaFLuDzWDFx9vcaGRjpBkw==
X-Virus-Checked: Checked by ClamAV on apache.org

Sure thing:

   https://issues.apache.org/jira/browse/SPARK-2486
   https://github.com/apache/spark/pull/1413

best,
wb


----- Original Message -----
> From: "Aaron Davidson" <ilikerps@gmail.com>
> To: dev@spark.apache.org
> Sent: Monday, July 14, 2014 8:38:16 PM
> Subject: Re: Profiling Spark tests with YourKit (or something else)
> 
> Would you mind filing a JIRA for this? That does sound like something bogus
> happening on the JVM/YourKit level, but this sort of diagnosis is
> sufficiently important that we should be resilient against it.
> 
> 
> On Mon, Jul 14, 2014 at 6:01 PM, Will Benton <willb@redhat.com> wrote:
> 
> > ----- Original Message -----
> > > From: "Aaron Davidson" <ilikerps@gmail.com>
> > > To: dev@spark.apache.org
> > > Sent: Monday, July 14, 2014 5:21:10 PM
> > > Subject: Re: Profiling Spark tests with YourKit (or something else)
> > >
> > > Out of curiosity, what problems are you seeing with Utils.getCallSite?
> >
> > Aaron, if I enable call site tracking or CPU profiling in YourKit, many
> > (but not all) Spark test cases will NPE on the line filtering out
> > "getStackTrace" from the stack trace (this is Utils.scala:812 in the
> > current master).  I'm not sure if this is a consequence of
> > Thread#getStackTrace including bogus frames when running instrumented or if
> > whatever instrumentation YourKit inserts relies on assumptions that don't
> > always hold for Scala code.
> >
> >
> > best,
> > wb
> >
> 

From dev-return-8360-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 04:05:43 2014
Return-Path: <dev-return-8360-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 308C01121B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 04:05:43 +0000 (UTC)
Received: (qmail 79934 invoked by uid 500); 15 Jul 2014 04:05:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79872 invoked by uid 500); 15 Jul 2014 04:05:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 79861 invoked by uid 99); 15 Jul 2014 04:05:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 04:05:41 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 04:05:38 +0000
Received: by mail-vc0-f177.google.com with SMTP id hy4so2354740vcb.36
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 21:05:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ihbOIqIRY0lohN7kvddOjFwhp82knkayjAJ+VV/QeWU=;
        b=NQIIH3IfrM4/ss20dMrh75b/E8YR13eSl3r2DMKBOz9mn2WN7s6evr8zVXXhAoYgeU
         XRaQPV4a48RCu10ELmEFge4+Ev/b0Cp8r3wwIrfGOes6EbP3SVJQ9qHYm4vZrDI2FfB4
         gqFk2GQx4mH7B2gz9s73DmgAv8r5y9iKVwL5yB8R3GIgQDt0e/VNmkUk6K8NAng1Efj2
         WaSZ0q/Sf+F6hHpfeFQoODgg+Fs9URXG6pLIdPfhUi7lyKWbUANFpnfmFMAw6wIYVm3/
         D4Bv5/MYAeLBi8PasMQJtTUBuqWYH8Bp1ysF/9WQWsGiw1nI2LbGt6Fi1OfW8X/BDFVi
         QS/g==
X-Gm-Message-State: ALoCoQlxLoKLvyUdr6p3o9ZS+qxvlkkJKMWyjkgmqRELlksQYc6EarLzkE/awEDKMWDm1JtH2d0R
X-Received: by 10.58.150.136 with SMTP id ui8mr20291577veb.14.1405397112824;
        Mon, 14 Jul 2014 21:05:12 -0700 (PDT)
Received: from mail-vc0-f181.google.com (mail-vc0-f181.google.com [209.85.220.181])
        by mx.google.com with ESMTPSA id m10sm22871770vdj.28.2014.07.14.21.05.11
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 21:05:11 -0700 (PDT)
Received: by mail-vc0-f181.google.com with SMTP id lf12so5115374vcb.26
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.52.244.81 with SMTP id xe17mr16602217vdc.24.1405397110824;
 Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Mon, 14 Jul 2014 21:05:10 -0700 (PDT)
In-Reply-To: <53C488B7.402@huawei.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
	<53C488B7.402@huawei.com>
Date: Tue, 15 Jul 2014 00:05:10 -0400
Message-ID: <CA+-p3AGioD-tfQP+_n29pWnJqR_OBWnAbj2Ek_jVA_WVZLn7ag@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Andrew Ash <andrew@andrewash.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2c34a8f260604fe3383c0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c34a8f260604fe3383c0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I'm not sure either of those PRs will fix the concurrent adds to
Configuration issue I observed. I've got a stack trace and writeup I'll
share in an hour or two (traveling today).
On Jul 14, 2014 9:50 PM, "scwf" <wangfei1@huawei.com> wrote:

> hi=EF=BC=8CCody
>   i met this issue days before and i post a PR for this(
> https://github.com/apache/spark/pull/1385)
> it's very strange that if i synchronize conf it will deadlock but it is o=
k
> when synchronize initLocalJobConfFuncOpt
>
>
>  Here's the entire jstack output.
>>
>>
>> On Mon, Jul 14, 2014 at 4:44 PM, Patrick Wendell <pwendell@gmail.com
>> <mailto:pwendell@gmail.com>> wrote:
>>
>>     Hey Cody,
>>
>>     This Jstack seems truncated, would you mind giving the entire stack
>>     trace? For the second thread, for instance, we can't see where the
>>     lock is being acquired.
>>
>>     - Patrick
>>
>>     On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
>>     <cody.koeninger@mediacrossing.com <mailto:cody.koeninger@
>> mediacrossing.com>> wrote:
>>      > Hi all, just wanted to give a heads up that we're seeing a
>> reproducible
>>      > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
>>      >
>>      > If jira is a better place for this, apologies in advance - figure=
d
>> talking
>>      > about it on the mailing list was friendlier than randomly
>> (re)opening jira
>>      > tickets.
>>      >
>>      > I know Gary had mentioned some issues with 1.0.1 on the mailing
>> list, once
>>      > we got a thread dump I wanted to follow up.
>>      >
>>      > The thread dump shows the deadlock occurs in the synchronized
>> block of code
>>      > that was changed in HadoopRDD.scala, for the Spark-1097 issue
>>      >
>>      > Relevant portions of the thread dump are summarized below, we can
>> provide
>>      > the whole dump if it's useful.
>>      >
>>      > Found one Java-level deadlock:
>>      > =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D
>>      > "Executor task launch worker-1":
>>      >   waiting to lock monitor 0x00007f250400c520 (object
>> 0x00000000fae7dc30, a
>>      > org.apache.hadoop.co <http://org.apache.hadoop.co>
>>      > nf.Configuration),
>>      >   which is held by "Executor task launch worker-0"
>>      > "Executor task launch worker-0":
>>      >   waiting to lock monitor 0x00007f2520495620 (object
>> 0x00000000faeb4fc8, a
>>      > java.lang.Class),
>>      >   which is held by "Executor task launch worker-1"
>>      >
>>      >
>>      > "Executor task launch worker-1":
>>      >         at
>>      > org.apache.hadoop.conf.Configuration.reloadConfiguration(
>> Configuration.java:791)
>>      >         - waiting to lock <0x00000000fae7dc30> (a
>>      > org.apache.hadoop.conf.Configuration)
>>      >         at
>>      > org.apache.hadoop.conf.Configuration.addDefaultResource(
>> Configuration.java:690)
>>      >         - locked <0x00000000faca6ff8> (a java.lang.Class for
>>      > org.apache.hadoop.conf.Configurati
>>      > on)
>>      >         at
>>      > org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(
>> HdfsConfiguration.java:34)
>>      >         at
>>      > org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>
>> (DistributedFileSystem.java:110
>>      > )
>>      >         at sun.reflect.NativeConstructorAccessorImpl.
>> newInstance0(Native
>>      > Method)
>>      >         at
>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
>> NativeConstructorAccessorImpl.
>>      > java:57)
>>      >         at sun.reflect.NativeConstructorAccessorImpl.
>> newInstance0(Native
>>      > Method)
>>      >         at
>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
>> NativeConstructorAccessorImpl.
>>      > java:57)
>>      >         at
>>      > sun.reflect.DelegatingConstructorAccessorImpl.newInstance(
>> DelegatingConstructorAcces
>>      > sorImpl.java:45)
>>      >         at java.lang.reflect.Constructor.
>> newInstance(Constructor.java:525)
>>      >         at java.lang.Class.newInstance0(Class.java:374)
>>      >         at java.lang.Class.newInstance(Class.java:327)
>>      >         at java.util.ServiceLoader$LazyIterator.next(
>> ServiceLoader.java:373)
>>      >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
>> FileSystem.java:2364)
>>      >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
>>      > org.apache.hadoop.fs.FileSystem)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
>> FileSystem.java:2375)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
>> FileSystem.java:2392)
>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
>> FileSystem.java:89)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
>> FileSystem.java:2431)
>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
>> FileSystem.java:2413)
>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> java:368)
>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> java:167)
>>      >         at
>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
>> JobConf.java:587)
>>      >         at
>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> FileInputFormat.java:315)
>>      >         at
>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> FileInputFormat.java:288)
>>      >         at
>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> SparkContext.scala:546)
>>      >         at
>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> SparkContext.scala:546)
>>      >         at
>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
>> 1.apply(HadoopRDD.scala:145)
>>      >
>>      >
>>      >
>>      > ...elided...
>>      >
>>      >
>>      > "Executor task launch worker-0" daemon prio=3D10
>> tid=3D0x0000000001e71800
>>      > nid=3D0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
>>      >    java.lang.Thread.State: BLOCKED (on object monitor)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
>> FileSystem.java:2362)
>>      >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Class
>> for
>>      > org.apache.hadoop.fs.FileSystem)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
>> FileSystem.java:2375)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
>> FileSystem.java:2392)
>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
>> FileSystem.java:89)
>>      >         at
>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
>> FileSystem.java:2431)
>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
>> FileSystem.java:2413)
>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> java:368)
>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> java:167)
>>      >         at
>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
>> JobConf.java:587)
>>      >         at
>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> FileInputFormat.java:315)
>>      >         at
>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> FileInputFormat.java:288)
>>      >         at
>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> SparkContext.scala:546)
>>      >         at
>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> SparkContext.scala:546)
>>      >         at
>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
>> 1.apply(HadoopRDD.scala:145)
>>
>>
>>
>
> --
>
> Best Regards
> Fei Wang
>
> ------------------------------------------------------------
> --------------------
>
>
>

--001a11c2c34a8f260604fe3383c0--

From dev-return-8361-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 04:55:52 2014
Return-Path: <dev-return-8361-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 86F7A1130C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 04:55:52 +0000 (UTC)
Received: (qmail 39503 invoked by uid 500); 15 Jul 2014 04:55:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39445 invoked by uid 500); 15 Jul 2014 04:55:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39433 invoked by uid 99); 15 Jul 2014 04:55:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 04:55:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 04:55:48 +0000
Received: by mail-oa0-f41.google.com with SMTP id j17so2535180oag.14
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 21:55:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=Qjnmyne16OOkANZv/HIoUrrCscYPhUYM8TH415uF+Ao=;
        b=rlOeXbJlOpKtKcOQoD2XWgKBxhYBWUirn4QMUMqkQbJXtZVTzpkXHnOIZWpNcbJ54w
         CleY4nzy8fI4burAlPQvwuBBk2C9JgpIXkiidZusmQyjt7PTWND+lku2GNmaDr3SRIam
         fEkLWIMuAFQPBPtRQNXoDWf0E/w5hBjwsCA6PAM6SQcN6/uSKlrQLlfHcXmcjbxAoraE
         kcbZ5vBhUkPZWn7Tu3A8kYpc89ek+a/+dQ9CE2nMmdWQSfaPS6TRzQST0K0tTydOCb0p
         dgf6De33uqlYseLQqim8D9+NKj2VY3T56OPfTtWGDv7fYndkG4aFbjysXp8AXNP3II3G
         7zUQ==
MIME-Version: 1.0
X-Received: by 10.60.57.166 with SMTP id j6mr7630453oeq.77.1405400127129; Mon,
 14 Jul 2014 21:55:27 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 21:55:27 -0700 (PDT)
In-Reply-To: <CA+-p3AGioD-tfQP+_n29pWnJqR_OBWnAbj2Ek_jVA_WVZLn7ag@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
	<53C488B7.402@huawei.com>
	<CA+-p3AGioD-tfQP+_n29pWnJqR_OBWnAbj2Ek_jVA_WVZLn7ag@mail.gmail.com>
Date: Mon, 14 Jul 2014 21:55:27 -0700
Message-ID: <CABPQxsvOvmHao6ct1OH68w2E2piCcJdMXqbgJFsX4QVeBoC5sg@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Andrew is your issue also a regression from 1.0.0 to 1.0.1? The
immediate priority is addressing regressions between these two
releases.

On Mon, Jul 14, 2014 at 9:05 PM, Andrew Ash <andrew@andrewash.com> wrote:
> I'm not sure either of those PRs will fix the concurrent adds to
> Configuration issue I observed. I've got a stack trace and writeup I'll
> share in an hour or two (traveling today).
> On Jul 14, 2014 9:50 PM, "scwf" <wangfei1@huawei.com> wrote:
>
>> hi=EF=BC=8CCody
>>   i met this issue days before and i post a PR for this(
>> https://github.com/apache/spark/pull/1385)
>> it's very strange that if i synchronize conf it will deadlock but it is =
ok
>> when synchronize initLocalJobConfFuncOpt
>>
>>
>>  Here's the entire jstack output.
>>>
>>>
>>> On Mon, Jul 14, 2014 at 4:44 PM, Patrick Wendell <pwendell@gmail.com
>>> <mailto:pwendell@gmail.com>> wrote:
>>>
>>>     Hey Cody,
>>>
>>>     This Jstack seems truncated, would you mind giving the entire stack
>>>     trace? For the second thread, for instance, we can't see where the
>>>     lock is being acquired.
>>>
>>>     - Patrick
>>>
>>>     On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
>>>     <cody.koeninger@mediacrossing.com <mailto:cody.koeninger@
>>> mediacrossing.com>> wrote:
>>>      > Hi all, just wanted to give a heads up that we're seeing a
>>> reproducible
>>>      > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
>>>      >
>>>      > If jira is a better place for this, apologies in advance - figur=
ed
>>> talking
>>>      > about it on the mailing list was friendlier than randomly
>>> (re)opening jira
>>>      > tickets.
>>>      >
>>>      > I know Gary had mentioned some issues with 1.0.1 on the mailing
>>> list, once
>>>      > we got a thread dump I wanted to follow up.
>>>      >
>>>      > The thread dump shows the deadlock occurs in the synchronized
>>> block of code
>>>      > that was changed in HadoopRDD.scala, for the Spark-1097 issue
>>>      >
>>>      > Relevant portions of the thread dump are summarized below, we ca=
n
>>> provide
>>>      > the whole dump if it's useful.
>>>      >
>>>      > Found one Java-level deadlock:
>>>      > =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D
>>>      > "Executor task launch worker-1":
>>>      >   waiting to lock monitor 0x00007f250400c520 (object
>>> 0x00000000fae7dc30, a
>>>      > org.apache.hadoop.co <http://org.apache.hadoop.co>
>>>      > nf.Configuration),
>>>      >   which is held by "Executor task launch worker-0"
>>>      > "Executor task launch worker-0":
>>>      >   waiting to lock monitor 0x00007f2520495620 (object
>>> 0x00000000faeb4fc8, a
>>>      > java.lang.Class),
>>>      >   which is held by "Executor task launch worker-1"
>>>      >
>>>      >
>>>      > "Executor task launch worker-1":
>>>      >         at
>>>      > org.apache.hadoop.conf.Configuration.reloadConfiguration(
>>> Configuration.java:791)
>>>      >         - waiting to lock <0x00000000fae7dc30> (a
>>>      > org.apache.hadoop.conf.Configuration)
>>>      >         at
>>>      > org.apache.hadoop.conf.Configuration.addDefaultResource(
>>> Configuration.java:690)
>>>      >         - locked <0x00000000faca6ff8> (a java.lang.Class for
>>>      > org.apache.hadoop.conf.Configurati
>>>      > on)
>>>      >         at
>>>      > org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(
>>> HdfsConfiguration.java:34)
>>>      >         at
>>>      > org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>
>>> (DistributedFileSystem.java:110
>>>      > )
>>>      >         at sun.reflect.NativeConstructorAccessorImpl.
>>> newInstance0(Native
>>>      > Method)
>>>      >         at
>>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
>>> NativeConstructorAccessorImpl.
>>>      > java:57)
>>>      >         at sun.reflect.NativeConstructorAccessorImpl.
>>> newInstance0(Native
>>>      > Method)
>>>      >         at
>>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
>>> NativeConstructorAccessorImpl.
>>>      > java:57)
>>>      >         at
>>>      > sun.reflect.DelegatingConstructorAccessorImpl.newInstance(
>>> DelegatingConstructorAcces
>>>      > sorImpl.java:45)
>>>      >         at java.lang.reflect.Constructor.
>>> newInstance(Constructor.java:525)
>>>      >         at java.lang.Class.newInstance0(Class.java:374)
>>>      >         at java.lang.Class.newInstance(Class.java:327)
>>>      >         at java.util.ServiceLoader$LazyIterator.next(
>>> ServiceLoader.java:373)
>>>      >         at java.util.ServiceLoader$1.next(ServiceLoader.java:445=
)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
>>> FileSystem.java:2364)
>>>      >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
>>>      > org.apache.hadoop.fs.FileSystem)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
>>> FileSystem.java:2375)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
>>> FileSystem.java:2392)
>>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
>>> FileSystem.java:89)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
>>> FileSystem.java:2431)
>>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
>>> FileSystem.java:2413)
>>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>>> java:368)
>>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>>> java:167)
>>>      >         at
>>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
>>> JobConf.java:587)
>>>      >         at
>>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>>> FileInputFormat.java:315)
>>>      >         at
>>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>>> FileInputFormat.java:288)
>>>      >         at
>>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>>> SparkContext.scala:546)
>>>      >         at
>>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>>> SparkContext.scala:546)
>>>      >         at
>>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
>>> 1.apply(HadoopRDD.scala:145)
>>>      >
>>>      >
>>>      >
>>>      > ...elided...
>>>      >
>>>      >
>>>      > "Executor task launch worker-0" daemon prio=3D10
>>> tid=3D0x0000000001e71800
>>>      > nid=3D0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
>>>      >    java.lang.Thread.State: BLOCKED (on object monitor)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
>>> FileSystem.java:2362)
>>>      >         - waiting to lock <0x00000000faeb4fc8> (a java.lang.Clas=
s
>>> for
>>>      > org.apache.hadoop.fs.FileSystem)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
>>> FileSystem.java:2375)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
>>> FileSystem.java:2392)
>>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
>>> FileSystem.java:89)
>>>      >         at
>>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
>>> FileSystem.java:2431)
>>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
>>> FileSystem.java:2413)
>>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>>> java:368)
>>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>>> java:167)
>>>      >         at
>>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
>>> JobConf.java:587)
>>>      >         at
>>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>>> FileInputFormat.java:315)
>>>      >         at
>>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>>> FileInputFormat.java:288)
>>>      >         at
>>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>>> SparkContext.scala:546)
>>>      >         at
>>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>>> SparkContext.scala:546)
>>>      >         at
>>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
>>> 1.apply(HadoopRDD.scala:145)
>>>
>>>
>>>
>>
>> --
>>
>> Best Regards
>> Fei Wang
>>
>> ------------------------------------------------------------
>> --------------------
>>
>>
>>

From dev-return-8362-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:00:14 2014
Return-Path: <dev-return-8362-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F4A711325
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:00:14 +0000 (UTC)
Received: (qmail 47321 invoked by uid 500); 15 Jul 2014 05:00:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47278 invoked by uid 500); 15 Jul 2014 05:00:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46216 invoked by uid 99); 15 Jul 2014 05:00:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:00:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.182 as permitted sender)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:00:08 +0000
Received: by mail-ob0-f182.google.com with SMTP id wm4so5240638obc.13
        for <multiple recipients>; Mon, 14 Jul 2014 21:59:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=moq88wVesDI3WBLcmwrKaFXzmsPX5whcbUtzgDJOEAs=;
        b=jJu33tJLHVglnRUmAhmNzJblP7Fsbg68NpXjjihMjkoxUvtt5acdnUKZTiUr39ZqIs
         0IzoaNTi/tClWBaXVbQHeX5GaMpKv1QQ6++eDeRdfyNSFx7sWjZ360lU3dsTTZQH/H0Q
         4+dbg79tMfZOwKDGgizVgfc4zYA8TqLxR4v/HvYxz/+22kU0J8Rtz92fijgB4LvfQjq5
         r8ba+lTEzCThnKh630iE+8WITvclxdhc+e/TOp/QiLD/13EtnOk560aqxwksJKupaRnZ
         RMq0YSnFZx9tWcEPy4dqi9HV6AEsRm3YwWBNyOU+NF1ygEJXb6MrNV/TYbUKszEYGH1J
         aouw==
MIME-Version: 1.0
X-Received: by 10.182.24.38 with SMTP id r6mr22887186obf.10.1405400387410;
 Mon, 14 Jul 2014 21:59:47 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 21:59:47 -0700 (PDT)
In-Reply-To: <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com>
References: <CAJOb8btWjn+pVBRngB-gA57CQ2YRw702j+=eP79tRyyPeZgcFA@mail.gmail.com>
	<CALDQvdcYe1ct_LdwmkV5tfR1_rfMMxhFyu=VdZw_Y2aAYmin-Q@mail.gmail.com>
	<CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com>
	<55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com>
Date: Mon, 14 Jul 2014 21:59:47 -0700
Message-ID: <CABPQxssmCmQvHnxrr_zbwSs_KMkTb805hquEFnS2FbVKyzYMTg@mail.gmail.com>
Subject: Re: Catalyst dependency on Spark Core
From: Patrick Wendell <pwendell@gmail.com>
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Adding new build modules is pretty high overhead, so if this is a case
where a small amount of duplicated code could get rid of the
dependency, that could also be a good short-term option.

- Patrick

On Mon, Jul 14, 2014 at 2:15 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> Yeah, I'd just add a spark-util that has these things.
>
> Matei
>
> On Jul 14, 2014, at 1:04 PM, Michael Armbrust <michael@databricks.com>
> wrote:
>
> Yeah, sadly this dependency was introduced when someone consolidated the
> logging infrastructure.  However, the dependency should be very small and
> thus easy to remove, and I would like catalyst to be usable outside of
> Spark.  A pull request to make this possible would be welcome.
>
> Ideally, we'd create some sort of spark common package that has things like
> logging.  That way catalyst could depend on that, without pulling in all of
> Hadoop, etc.  Maybe others have opinions though, so I'm cc-ing the dev list.
>
>
> On Mon, Jul 14, 2014 at 12:21 AM, Yanbo Liang <yanbohappy@gmail.com> wrote:
>>
>> Make Catalyst independent of Spark is the goal of Catalyst, maybe need
>> time and evolution.
>> I awared that package org.apache.spark.sql.catalyst.util embraced
>> org.apache.spark.util.{Utils => SparkUtils},
>> so that Catalyst has a dependency on Spark core.
>> I'm not sure whether it will be replaced by other component independent of
>> Spark in later release.
>>
>>
>> 2014-07-14 11:51 GMT+08:00 Aniket Bhatnagar <aniket.bhatnagar@gmail.com>:
>>
>>> As per the recent presentation given in Scala days
>>> (http://people.apache.org/~marmbrus/talks/SparkSQLScalaDays2014.pdf), it was
>>> mentioned that Catalyst is independent of Spark. But on inspecting pom.xml
>>> of sql/catalyst module, it seems it has a dependency on Spark Core. Any
>>> particular reason for the dependency? I would love to use Catalyst outside
>>> Spark
>>>
>>> (reposted as previous email bounced. Sorry if this is a duplicate).
>>
>>
>
>

From dev-return-8363-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:03:18 2014
Return-Path: <dev-return-8363-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6A831133C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:03:18 +0000 (UTC)
Received: (qmail 53376 invoked by uid 500); 15 Jul 2014 05:03:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53314 invoked by uid 500); 15 Jul 2014 05:03:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53303 invoked by uid 99); 15 Jul 2014 05:03:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:03:17 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:03:13 +0000
Received: by mail-vc0-f177.google.com with SMTP id hy4so2427621vcb.36
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:02:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=I2HJke1SC/X2WoZyuEmngG44p049bOH4xB5jYjtD7Zg=;
        b=XS1oz9JIZogTaeiN5bFcwgyTqvFYKxMKeowRQPcLTVB3IP3M6fNU8T4dKX6DJm1vl7
         /P+K2vDewYdJjE8Wh4LmndKUviaiT18/ciGG2xylJAXWPQyEh2D/kZwXAtxtcH8aem/2
         dm5syobdGnSnN+PHL6z3Xjl2+N8iqXt1hnkc8x7xZ6yUzB38YauJ3SxP02dj27Z8Ukms
         N9borwNLHMsjzeKjTVBrQIcwobTvtgvJJ1sLXxC1wY/gJ35B3dWG2xsj46Zqu1DCb3lo
         o18AUHiHeG8V0nlhr5RvzzGKpD6qQTjIjzQfvEATRsh0DyFfjK6uh6PohjVVSEdmX9QW
         sajw==
X-Gm-Message-State: ALoCoQm1RaKdEfPGOJCOfpe7RLoJANBNTi8wyOJK36tjAjI8V7cYZN2prR/bY4CnnMybZ0FjhZ5K
X-Received: by 10.58.150.136 with SMTP id ui8mr20530654veb.14.1405400572673;
        Mon, 14 Jul 2014 22:02:52 -0700 (PDT)
Received: from mail-vc0-f178.google.com (mail-vc0-f178.google.com [209.85.220.178])
        by mx.google.com with ESMTPSA id xb3sm23135871vdb.14.2014.07.14.22.02.51
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 22:02:51 -0700 (PDT)
Received: by mail-vc0-f178.google.com with SMTP id la4so2489569vcb.9
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:02:50 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.58.208.170 with SMTP id mf10mr10773447vec.22.1405400570838;
 Mon, 14 Jul 2014 22:02:50 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Mon, 14 Jul 2014 22:02:50 -0700 (PDT)
Received: by 10.220.124.211 with HTTP; Mon, 14 Jul 2014 22:02:50 -0700 (PDT)
In-Reply-To: <CABPQxsvOvmHao6ct1OH68w2E2piCcJdMXqbgJFsX4QVeBoC5sg@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
	<53C488B7.402@huawei.com>
	<CA+-p3AGioD-tfQP+_n29pWnJqR_OBWnAbj2Ek_jVA_WVZLn7ag@mail.gmail.com>
	<CABPQxsvOvmHao6ct1OH68w2E2piCcJdMXqbgJFsX4QVeBoC5sg@mail.gmail.com>
Date: Tue, 15 Jul 2014 01:02:50 -0400
Message-ID: <CA+-p3AHtLVekqu=9YZKmHfN8gvv2fiVcHbfZx9-+Ti18tv9y2Q@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Andrew Ash <andrew@andrewash.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc17f6cac5fa04fe345194
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc17f6cac5fa04fe345194
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I don't believe mine is a regression. But it is related to thread safety on
Hadoop Configuration objects. Should I start a new thread?
On Jul 15, 2014 12:55 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:

> Andrew is your issue also a regression from 1.0.0 to 1.0.1? The
> immediate priority is addressing regressions between these two
> releases.
>
> On Mon, Jul 14, 2014 at 9:05 PM, Andrew Ash <andrew@andrewash.com> wrote:
> > I'm not sure either of those PRs will fix the concurrent adds to
> > Configuration issue I observed. I've got a stack trace and writeup I'll
> > share in an hour or two (traveling today).
> > On Jul 14, 2014 9:50 PM, "scwf" <wangfei1@huawei.com> wrote:
> >
> >> hi=EF=BC=8CCody
> >>   i met this issue days before and i post a PR for this(
> >> https://github.com/apache/spark/pull/1385)
> >> it's very strange that if i synchronize conf it will deadlock but it i=
s
> ok
> >> when synchronize initLocalJobConfFuncOpt
> >>
> >>
> >>  Here's the entire jstack output.
> >>>
> >>>
> >>> On Mon, Jul 14, 2014 at 4:44 PM, Patrick Wendell <pwendell@gmail.com
> >>> <mailto:pwendell@gmail.com>> wrote:
> >>>
> >>>     Hey Cody,
> >>>
> >>>     This Jstack seems truncated, would you mind giving the entire sta=
ck
> >>>     trace? For the second thread, for instance, we can't see where th=
e
> >>>     lock is being acquired.
> >>>
> >>>     - Patrick
> >>>
> >>>     On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
> >>>     <cody.koeninger@mediacrossing.com <mailto:cody.koeninger@
> >>> mediacrossing.com>> wrote:
> >>>      > Hi all, just wanted to give a heads up that we're seeing a
> >>> reproducible
> >>>      > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
> >>>      >
> >>>      > If jira is a better place for this, apologies in advance -
> figured
> >>> talking
> >>>      > about it on the mailing list was friendlier than randomly
> >>> (re)opening jira
> >>>      > tickets.
> >>>      >
> >>>      > I know Gary had mentioned some issues with 1.0.1 on the mailin=
g
> >>> list, once
> >>>      > we got a thread dump I wanted to follow up.
> >>>      >
> >>>      > The thread dump shows the deadlock occurs in the synchronized
> >>> block of code
> >>>      > that was changed in HadoopRDD.scala, for the Spark-1097 issue
> >>>      >
> >>>      > Relevant portions of the thread dump are summarized below, we
> can
> >>> provide
> >>>      > the whole dump if it's useful.
> >>>      >
> >>>      > Found one Java-level deadlock:
> >>>      > =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D
> >>>      > "Executor task launch worker-1":
> >>>      >   waiting to lock monitor 0x00007f250400c520 (object
> >>> 0x00000000fae7dc30, a
> >>>      > org.apache.hadoop.co <http://org.apache.hadoop.co>
> >>>      > nf.Configuration),
> >>>      >   which is held by "Executor task launch worker-0"
> >>>      > "Executor task launch worker-0":
> >>>      >   waiting to lock monitor 0x00007f2520495620 (object
> >>> 0x00000000faeb4fc8, a
> >>>      > java.lang.Class),
> >>>      >   which is held by "Executor task launch worker-1"
> >>>      >
> >>>      >
> >>>      > "Executor task launch worker-1":
> >>>      >         at
> >>>      > org.apache.hadoop.conf.Configuration.reloadConfiguration(
> >>> Configuration.java:791)
> >>>      >         - waiting to lock <0x00000000fae7dc30> (a
> >>>      > org.apache.hadoop.conf.Configuration)
> >>>      >         at
> >>>      > org.apache.hadoop.conf.Configuration.addDefaultResource(
> >>> Configuration.java:690)
> >>>      >         - locked <0x00000000faca6ff8> (a java.lang.Class for
> >>>      > org.apache.hadoop.conf.Configurati
> >>>      > on)
> >>>      >         at
> >>>      > org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(
> >>> HdfsConfiguration.java:34)
> >>>      >         at
> >>>      > org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>
> >>> (DistributedFileSystem.java:110
> >>>      > )
> >>>      >         at sun.reflect.NativeConstructorAccessorImpl.
> >>> newInstance0(Native
> >>>      > Method)
> >>>      >         at
> >>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
> >>> NativeConstructorAccessorImpl.
> >>>      > java:57)
> >>>      >         at sun.reflect.NativeConstructorAccessorImpl.
> >>> newInstance0(Native
> >>>      > Method)
> >>>      >         at
> >>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
> >>> NativeConstructorAccessorImpl.
> >>>      > java:57)
> >>>      >         at
> >>>      > sun.reflect.DelegatingConstructorAccessorImpl.newInstance(
> >>> DelegatingConstructorAcces
> >>>      > sorImpl.java:45)
> >>>      >         at java.lang.reflect.Constructor.
> >>> newInstance(Constructor.java:525)
> >>>      >         at java.lang.Class.newInstance0(Class.java:374)
> >>>      >         at java.lang.Class.newInstance(Class.java:327)
> >>>      >         at java.util.ServiceLoader$LazyIterator.next(
> >>> ServiceLoader.java:373)
> >>>      >         at
> java.util.ServiceLoader$1.next(ServiceLoader.java:445)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
> >>> FileSystem.java:2364)
> >>>      >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
> >>>      > org.apache.hadoop.fs.FileSystem)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
> >>> FileSystem.java:2375)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
> >>> FileSystem.java:2392)
> >>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
> >>> FileSystem.java:89)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
> >>> FileSystem.java:2431)
> >>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
> >>> FileSystem.java:2413)
> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
> >>> java:368)
> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
> >>> java:167)
> >>>      >         at
> >>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
> >>> JobConf.java:587)
> >>>      >         at
> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
> >>> FileInputFormat.java:315)
> >>>      >         at
> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
> >>> FileInputFormat.java:288)
> >>>      >         at
> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
> >>> SparkContext.scala:546)
> >>>      >         at
> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
> >>> SparkContext.scala:546)
> >>>      >         at
> >>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
> >>> 1.apply(HadoopRDD.scala:145)
> >>>      >
> >>>      >
> >>>      >
> >>>      > ...elided...
> >>>      >
> >>>      >
> >>>      > "Executor task launch worker-0" daemon prio=3D10
> >>> tid=3D0x0000000001e71800
> >>>      > nid=3D0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
> >>>      >    java.lang.Thread.State: BLOCKED (on object monitor)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
> >>> FileSystem.java:2362)
> >>>      >         - waiting to lock <0x00000000faeb4fc8> (a
> java.lang.Class
> >>> for
> >>>      > org.apache.hadoop.fs.FileSystem)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
> >>> FileSystem.java:2375)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
> >>> FileSystem.java:2392)
> >>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
> >>> FileSystem.java:89)
> >>>      >         at
> >>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
> >>> FileSystem.java:2431)
> >>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
> >>> FileSystem.java:2413)
> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
> >>> java:368)
> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
> >>> java:167)
> >>>      >         at
> >>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
> >>> JobConf.java:587)
> >>>      >         at
> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
> >>> FileInputFormat.java:315)
> >>>      >         at
> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
> >>> FileInputFormat.java:288)
> >>>      >         at
> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
> >>> SparkContext.scala:546)
> >>>      >         at
> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
> >>> SparkContext.scala:546)
> >>>      >         at
> >>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
> >>> 1.apply(HadoopRDD.scala:145)
> >>>
> >>>
> >>>
> >>
> >> --
> >>
> >> Best Regards
> >> Fei Wang
> >>
> >> ------------------------------------------------------------
> >> --------------------
> >>
> >>
> >>
>

--047d7bdc17f6cac5fa04fe345194--

From dev-return-8364-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:03:25 2014
Return-Path: <dev-return-8364-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 703781133E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:03:25 +0000 (UTC)
Received: (qmail 54479 invoked by uid 500); 15 Jul 2014 05:03:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54426 invoked by uid 500); 15 Jul 2014 05:03:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54414 invoked by uid 99); 15 Jul 2014 05:03:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:03:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:03:19 +0000
Received: by mail-qa0-f54.google.com with SMTP id k15so2709843qaq.13
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:02:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=YzuhkakMxWZTkCRP55K0Tol84AWBxxEqlvJqnwUqC/I=;
        b=oxSX11y+rIBvkI1FqVtFEY6MQh/YgaMPyGf67XtzNyGkrPSFrYvavyJO+agYP+t4K6
         J5gaa1kK6VbIvYtw1Oxnsn/NiPb2Tpdt9OJD8tFpuvnbHet4jpWnUXYdSqdSxo/fQcwo
         BkKgRjUGY5jjqdTCBKvOMRhFzYwxeAi0kmxVgIShUpGjteRsa+shF7wzIHsLPMx9O2Y4
         lGT9592Ui8VQJrGhWm98atwsymqN5i3CS9jdMkXZcsaC6OeSOmTyCZVwoRMJJV59nO3z
         XwMR+gwBnZ1+U+wR8f5de26tTo6REAfNi8lvCHZ6Ru5/oNW+r+e6oIjL49iCLRRmVKp2
         mYtQ==
X-Received: by 10.229.202.136 with SMTP id fe8mr10103719qcb.8.1405400578541;
 Mon, 14 Jul 2014 22:02:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Mon, 14 Jul 2014 22:02:38 -0700 (PDT)
In-Reply-To: <CAOhmDzcN=ehCrms2krYX-gY2bPTu4P-KGEMaQd9vK2+jot9_vQ@mail.gmail.com>
References: <CAOhmDzcN=ehCrms2krYX-gY2bPTu4P-KGEMaQd9vK2+jot9_vQ@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Mon, 14 Jul 2014 22:02:38 -0700
Message-ID: <CANGvG8psZbChNgZkUL=rK9AFXfmxFJ1oXqFPpK_Am_onMAbFJA@mail.gmail.com>
Subject: Re: ec2 clusters launched at 9fe693b5b6 are broken (?)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3061040522e04fe34524b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3061040522e04fe34524b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

This one is typically due to a mismatch between the Hadoop versions --
i.e., Spark is compiled against 1.0.4 but is running with 2.3.0 in the
classpath, or something like that. Not certain why you're seeing this with
spark-ec2, but I'm assuming this is related to the issues you posted in a
separate thread.


On Mon, Jul 14, 2014 at 6:43 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Just launched an EC2 cluster from git hash
> 9fe693b5b6ed6af34ee1e800ab89c8a11991ea38. Calling take() on an RDD
> accessing data in S3 yields the following error output.
>
> I understand that NoClassDefFoundError errors may mean something in the
> deployment was messed up. Is that correct? When I launch a cluster using
> spark-ec2, I expect all critical deployment details to be taken care of b=
y
> the script.
>
> So is something in the deployment executed by spark-ec2 borked?
>
> Nick
>
> java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException
>     at
> org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(Nativ=
eS3FileSystem.java:224)
>     at
> org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileS=
ystem.java:214)
>     at
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
>     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
>     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
>     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
>     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
>     at
> org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:=
176)
>     at
> org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:2=
08)
>     at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:176)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203=
)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201=
)
>     at scala.Option.getOrElse(Option.scala:120)
>     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>     at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203=
)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201=
)
>     at scala.Option.getOrElse(Option.scala:120)
>     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>     at
> org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scal=
a:32)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203=
)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201=
)
>     at scala.Option.getOrElse(Option.scala:120)
>     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>     at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:71)
>     at
> org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:79)
>     at
> org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:190)
>     at
> org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:188)
>     at scala.Option.getOrElse(Option.scala:120)
>     at org.apache.spark.rdd.RDD.dependencies(RDD.scala:188)
>     at
> org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGScheduler.sca=
la:1144)
>     at
> org.apache.spark.SparkContext.getPreferredLocs(SparkContext.scala:903)
>     at
> org.apache.spark.rdd.PartitionCoalescer.currPrefLocs(CoalescedRDD.scala:1=
74)
>     at
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anon=
fun$apply$2.apply(CoalescedRDD.scala:191)
>     at
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anon=
fun$apply$2.apply(CoalescedRDD.scala:190)
>     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
>     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
>     at
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator.<init>(Coalesced=
RDD.scala:185)
>     at
> org.apache.spark.rdd.PartitionCoalescer.setupGroups(CoalescedRDD.scala:23=
6)
>     at org.apache.spark.rdd.PartitionCoalescer.run(CoalescedRDD.scala:337=
)
>     at
> org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:83)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203=
)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201=
)
>     at scala.Option.getOrElse(Option.scala:120)
>     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>     at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203=
)
>     at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201=
)
>     at scala.Option.getOrElse(Option.scala:120)
>     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>     at org.apache.spark.rdd.RDD.take(RDD.scala:1036)
>     at $iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
>     at $iwC$$iwC$$iwC.<init>(<console>:31)
>     at $iwC$$iwC.<init>(<console>:33)
>     at $iwC.<init>(<console>:35)
>     at <init>(<console>:37)
>     at .<init>(<console>:41)
>     at .<clinit>(<console>)
>     at .<init>(<console>:7)
>     at .<clinit>(<console>)
>     at $print(<console>)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
>     at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
>     at java.lang.reflect.Method.invoke(Method.java:606)
>     at
> org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:788)
>     at
> org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1056=
)
>     at
> org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:614)
>     at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
>     at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
>     at
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
>     at
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:8=
41)
>     at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
>     at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:60=
1)
>     at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
>     at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
>     at
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILo=
op.scala:936)
>     at
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scal=
a:884)
>     at
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scal=
a:884)
>     at
> scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoad=
er.scala:135)
>     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
>     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
>     at org.apache.spark.repl.Main$.main(Main.scala:31)
>     at org.apache.spark.repl.Main.main(Main.scala)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
>     at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
>     at java.lang.reflect.Method.invoke(Method.java:606)
>     at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
>     at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
>     at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
> Caused by: java.lang.ClassNotFoundException:
> org.jets3t.service.S3ServiceException
>     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>     at java.security.AccessController.doPrivileged(Native Method)
>     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
>     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>     ... 92 more
>
> =E2=80=8B
>

--001a11c3061040522e04fe34524b--

From dev-return-8365-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:12:05 2014
Return-Path: <dev-return-8365-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2EAE61135D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:12:05 +0000 (UTC)
Received: (qmail 67978 invoked by uid 500); 15 Jul 2014 05:12:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67915 invoked by uid 500); 15 Jul 2014 05:12:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67904 invoked by uid 99); 15 Jul 2014 05:12:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:12:04 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:12:00 +0000
Received: by mail-wg0-f51.google.com with SMTP id b13so2932066wgh.22
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:11:36 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=3i66wZiJiinpJDJwm+1jnUXKQuACioIBqJOB1hV2MUE=;
        b=WbdwUnZoAa1VrzDsCwlm1m5m8wpOg9jkjjniLM7OLMn6cTDspIzAofhpLN7Z+O5OcD
         UvFIVXPEusvuLsKdEjid2HlvEvsBCj/a029Q3e33lNyUywDVl3ysV5RDizGJVgQAT4qb
         CrGBgMo6P1YObCf7ITGaT/rJYjtngnHQb8bwaP89QQRYYv9Ps13Ko9YN/JpM9SmuE0VB
         K5bwRdz9kZpZZ+7AkrzQW5EM6y03Ik2OqnT53RxntWmm/pD3u81hI3Ch0yHVIYEAgmkM
         L9dLK/yrXgUmgIKzz7yWLgoAsRIcHDy7y397NonrVUsOSLOJmr1SWB/v3K/k1/RHJLuF
         BljA==
X-Gm-Message-State: ALoCoQlaYOIjq0crcyaR/sDlwu4t3QVkaD6iWzcrB1p4LZ03q5BNazOcY7JXz6ywKMN2KQAKWjUX
MIME-Version: 1.0
X-Received: by 10.180.221.108 with SMTP id qd12mr2497622wic.83.1405401095697;
 Mon, 14 Jul 2014 22:11:35 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.217.48.72 with HTTP; Mon, 14 Jul 2014 22:11:35 -0700 (PDT)
In-Reply-To: <CANGvG8psZbChNgZkUL=rK9AFXfmxFJ1oXqFPpK_Am_onMAbFJA@mail.gmail.com>
References: <CAOhmDzcN=ehCrms2krYX-gY2bPTu4P-KGEMaQd9vK2+jot9_vQ@mail.gmail.com>
	<CANGvG8psZbChNgZkUL=rK9AFXfmxFJ1oXqFPpK_Am_onMAbFJA@mail.gmail.com>
Date: Mon, 14 Jul 2014 22:11:35 -0700
Message-ID: <CAKx7Bf8tE1D88r36SXHrZeN4c2ehehpAzbNSt=ZtrSmuXkteZg@mail.gmail.com>
Subject: Re: ec2 clusters launched at 9fe693b5b6 are broken (?)
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134d2421380fc04fe347142
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134d2421380fc04fe347142
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

My guess is that this is related to
https://issues.apache.org/jira/browse/SPARK-2471 where the S3 library gets
excluded from the SBT assembly jar. I am not sure if the assembly jar used
in EC2 is generated using SBT though.

Shivaram


On Mon, Jul 14, 2014 at 10:02 PM, Aaron Davidson <ilikerps@gmail.com> wrote=
:

> This one is typically due to a mismatch between the Hadoop versions --
> i.e., Spark is compiled against 1.0.4 but is running with 2.3.0 in the
> classpath, or something like that. Not certain why you're seeing this wit=
h
> spark-ec2, but I'm assuming this is related to the issues you posted in a
> separate thread.
>
>
> On Mon, Jul 14, 2014 at 6:43 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> > Just launched an EC2 cluster from git hash
> > 9fe693b5b6ed6af34ee1e800ab89c8a11991ea38. Calling take() on an RDD
> > accessing data in S3 yields the following error output.
> >
> > I understand that NoClassDefFoundError errors may mean something in the
> > deployment was messed up. Is that correct? When I launch a cluster usin=
g
> > spark-ec2, I expect all critical deployment details to be taken care of
> by
> > the script.
> >
> > So is something in the deployment executed by spark-ec2 borked?
> >
> > Nick
> >
> > java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException
> >     at
> >
> org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(Nativ=
eS3FileSystem.java:224)
> >     at
> >
> org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileS=
ystem.java:214)
> >     at
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
> >     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
> >     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
> >     at
> >
> org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:=
176)
> >     at
> >
> org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:2=
08)
> >     at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:176=
)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >     at scala.Option.getOrElse(Option.scala:120)
> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >     at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >     at scala.Option.getOrElse(Option.scala:120)
> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >     at
> >
> org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scal=
a:32)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >     at scala.Option.getOrElse(Option.scala:120)
> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >     at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:71)
> >     at
> > org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:79)
> >     at
> > org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:190)
> >     at
> > org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:188)
> >     at scala.Option.getOrElse(Option.scala:120)
> >     at org.apache.spark.rdd.RDD.dependencies(RDD.scala:188)
> >     at
> >
> org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGScheduler.sca=
la:1144)
> >     at
> > org.apache.spark.SparkContext.getPreferredLocs(SparkContext.scala:903)
> >     at
> >
> org.apache.spark.rdd.PartitionCoalescer.currPrefLocs(CoalescedRDD.scala:1=
74)
> >     at
> >
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anon=
fun$apply$2.apply(CoalescedRDD.scala:191)
> >     at
> >
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anon=
fun$apply$2.apply(CoalescedRDD.scala:190)
> >     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
> >     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
> >     at
> >
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator.<init>(Coalesced=
RDD.scala:185)
> >     at
> >
> org.apache.spark.rdd.PartitionCoalescer.setupGroups(CoalescedRDD.scala:23=
6)
> >     at
> org.apache.spark.rdd.PartitionCoalescer.run(CoalescedRDD.scala:337)
> >     at
> > org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:83)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >     at scala.Option.getOrElse(Option.scala:120)
> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >     at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >     at
> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >     at scala.Option.getOrElse(Option.scala:120)
> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >     at org.apache.spark.rdd.RDD.take(RDD.scala:1036)
> >     at $iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
> >     at $iwC$$iwC$$iwC.<init>(<console>:31)
> >     at $iwC$$iwC.<init>(<console>:33)
> >     at $iwC.<init>(<console>:35)
> >     at <init>(<console>:37)
> >     at .<init>(<console>:41)
> >     at .<clinit>(<console>)
> >     at .<init>(<console>:7)
> >     at .<clinit>(<console>)
> >     at $print(<console>)
> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >     at
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >     at
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >     at java.lang.reflect.Method.invoke(Method.java:606)
> >     at
> > org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:78=
8)
> >     at
> >
> org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1056=
)
> >     at
> > org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:614)
> >     at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
> >     at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
> >     at
> > org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796=
)
> >     at
> >
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:8=
41)
> >     at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
> >     at
> org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
> >     at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:60=
8)
> >     at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
> >     at
> >
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILo=
op.scala:936)
> >     at
> >
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scal=
a:884)
> >     at
> >
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scal=
a:884)
> >     at
> >
> scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoad=
er.scala:135)
> >     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
> >     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
> >     at org.apache.spark.repl.Main$.main(Main.scala:31)
> >     at org.apache.spark.repl.Main.main(Main.scala)
> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >     at
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >     at
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >     at java.lang.reflect.Method.invoke(Method.java:606)
> >     at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:30=
3)
> >     at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
> >     at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
> > Caused by: java.lang.ClassNotFoundException:
> > org.jets3t.service.S3ServiceException
> >     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> >     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> >     at java.security.AccessController.doPrivileged(Native Method)
> >     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> >     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
> >     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
> >     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
> >     ... 92 more
> >
> > =E2=80=8B
> >
>

--001a1134d2421380fc04fe347142--

From dev-return-8366-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:19:43 2014
Return-Path: <dev-return-8366-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6AEA21137C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:19:43 +0000 (UTC)
Received: (qmail 83190 invoked by uid 500); 15 Jul 2014 05:19:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83126 invoked by uid 500); 15 Jul 2014 05:19:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83114 invoked by uid 99); 15 Jul 2014 05:19:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:19:42 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.214.181 as permitted sender)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:19:38 +0000
Received: by mail-ob0-f181.google.com with SMTP id va2so3269459obc.26
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:19:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=yxSwG+RpJvYJVDgBBsRHOVmwW9vpH0/zZnmdHbobqLM=;
        b=J9hNPxKtZuz0pKyXuGJeehTXGUDMlB5lb68Na+KAh4B8aZCgq4bjV1CEGTBIxPnCQX
         4OKZo5nWXiDxAjtyXAqbI03Bz8mEVhlN7449FpChYp17XccXd0cPlVtqDb4nkm6bFgGZ
         tPKZ3+WZacsvlw5fPdY4c0CAANYF9e/255kpQTP6GJtQll5jZsz8cO1slqEI9vYD1oM0
         JjIeY5SdCDq5UgFd6LqhCLUGwQGqvFOUnFgsdEDj6C6YvnQQbUAB/7/ZFTKHmYnv99PL
         YSHpCXsJgmFdpAdLD/XGdM1/JbVC0uKmz5vxGYYdlEI/wrWBlufCT/JTTq7sjGxz26Op
         6/ag==
MIME-Version: 1.0
X-Received: by 10.182.24.38 with SMTP id r6mr22978620obf.10.1405401557513;
 Mon, 14 Jul 2014 22:19:17 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 22:19:17 -0700 (PDT)
In-Reply-To: <CAKx7Bf8tE1D88r36SXHrZeN4c2ehehpAzbNSt=ZtrSmuXkteZg@mail.gmail.com>
References: <CAOhmDzcN=ehCrms2krYX-gY2bPTu4P-KGEMaQd9vK2+jot9_vQ@mail.gmail.com>
	<CANGvG8psZbChNgZkUL=rK9AFXfmxFJ1oXqFPpK_Am_onMAbFJA@mail.gmail.com>
	<CAKx7Bf8tE1D88r36SXHrZeN4c2ehehpAzbNSt=ZtrSmuXkteZg@mail.gmail.com>
Date: Mon, 14 Jul 2014 22:19:17 -0700
Message-ID: <CABPQxsvj4XUtNy1csfY8yYiTT5apiVAAfxRiN7ngx0u9czJbag@mail.gmail.com>
Subject: Re: ec2 clusters launched at 9fe693b5b6 are broken (?)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah - this is likely caused by SPARK-2471.

On Mon, Jul 14, 2014 at 10:11 PM, Shivaram Venkataraman
<shivaram@eecs.berkeley.edu> wrote:
> My guess is that this is related to
> https://issues.apache.org/jira/browse/SPARK-2471 where the S3 library gets
> excluded from the SBT assembly jar. I am not sure if the assembly jar used
> in EC2 is generated using SBT though.
>
> Shivaram
>
>
> On Mon, Jul 14, 2014 at 10:02 PM, Aaron Davidson <ilikerps@gmail.com> wrote:
>
>> This one is typically due to a mismatch between the Hadoop versions --
>> i.e., Spark is compiled against 1.0.4 but is running with 2.3.0 in the
>> classpath, or something like that. Not certain why you're seeing this with
>> spark-ec2, but I'm assuming this is related to the issues you posted in a
>> separate thread.
>>
>>
>> On Mon, Jul 14, 2014 at 6:43 PM, Nicholas Chammas <
>> nicholas.chammas@gmail.com> wrote:
>>
>> > Just launched an EC2 cluster from git hash
>> > 9fe693b5b6ed6af34ee1e800ab89c8a11991ea38. Calling take() on an RDD
>> > accessing data in S3 yields the following error output.
>> >
>> > I understand that NoClassDefFoundError errors may mean something in the
>> > deployment was messed up. Is that correct? When I launch a cluster using
>> > spark-ec2, I expect all critical deployment details to be taken care of
>> by
>> > the script.
>> >
>> > So is something in the deployment executed by spark-ec2 borked?
>> >
>> > Nick
>> >
>> > java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException
>> >     at
>> >
>> org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(NativeS3FileSystem.java:224)
>> >     at
>> >
>> org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:214)
>> >     at
>> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
>> >     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
>> >     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
>> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
>> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
>> >     at
>> >
>> org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:176)
>> >     at
>> >
>> org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)
>> >     at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:176)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
>> >     at scala.Option.getOrElse(Option.scala:120)
>> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>> >     at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
>> >     at scala.Option.getOrElse(Option.scala:120)
>> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>> >     at
>> >
>> org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
>> >     at scala.Option.getOrElse(Option.scala:120)
>> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>> >     at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:71)
>> >     at
>> > org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:79)
>> >     at
>> > org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:190)
>> >     at
>> > org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:188)
>> >     at scala.Option.getOrElse(Option.scala:120)
>> >     at org.apache.spark.rdd.RDD.dependencies(RDD.scala:188)
>> >     at
>> >
>> org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGScheduler.scala:1144)
>> >     at
>> > org.apache.spark.SparkContext.getPreferredLocs(SparkContext.scala:903)
>> >     at
>> >
>> org.apache.spark.rdd.PartitionCoalescer.currPrefLocs(CoalescedRDD.scala:174)
>> >     at
>> >
>> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:191)
>> >     at
>> >
>> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:190)
>> >     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
>> >     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
>> >     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
>> >     at
>> >
>> org.apache.spark.rdd.PartitionCoalescer$LocationIterator.<init>(CoalescedRDD.scala:185)
>> >     at
>> >
>> org.apache.spark.rdd.PartitionCoalescer.setupGroups(CoalescedRDD.scala:236)
>> >     at
>> org.apache.spark.rdd.PartitionCoalescer.run(CoalescedRDD.scala:337)
>> >     at
>> > org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:83)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
>> >     at scala.Option.getOrElse(Option.scala:120)
>> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>> >     at org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
>> >     at
>> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
>> >     at scala.Option.getOrElse(Option.scala:120)
>> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
>> >     at org.apache.spark.rdd.RDD.take(RDD.scala:1036)
>> >     at $iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
>> >     at $iwC$$iwC$$iwC.<init>(<console>:31)
>> >     at $iwC$$iwC.<init>(<console>:33)
>> >     at $iwC.<init>(<console>:35)
>> >     at <init>(<console>:37)
>> >     at .<init>(<console>:41)
>> >     at .<clinit>(<console>)
>> >     at .<init>(<console>:7)
>> >     at .<clinit>(<console>)
>> >     at $print(<console>)
>> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> >     at
>> >
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>> >     at
>> >
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>> >     at java.lang.reflect.Method.invoke(Method.java:606)
>> >     at
>> > org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:788)
>> >     at
>> >
>> org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1056)
>> >     at
>> > org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:614)
>> >     at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
>> >     at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
>> >     at
>> > org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
>> >     at
>> >
>> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
>> >     at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
>> >     at
>> org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
>> >     at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
>> >     at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
>> >     at
>> >
>> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
>> >     at
>> >
>> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
>> >     at
>> >
>> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
>> >     at
>> >
>> scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
>> >     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
>> >     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
>> >     at org.apache.spark.repl.Main$.main(Main.scala:31)
>> >     at org.apache.spark.repl.Main.main(Main.scala)
>> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>> >     at
>> >
>> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
>> >     at
>> >
>> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
>> >     at java.lang.reflect.Method.invoke(Method.java:606)
>> >     at org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
>> >     at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
>> >     at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
>> > Caused by: java.lang.ClassNotFoundException:
>> > org.jets3t.service.S3ServiceException
>> >     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
>> >     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
>> >     at java.security.AccessController.doPrivileged(Native Method)
>> >     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
>> >     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
>> >     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
>> >     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
>> >     ... 92 more
>> >
>> >
>> >
>>

From dev-return-8367-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:20:32 2014
Return-Path: <dev-return-8367-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0EC1311389
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:20:32 +0000 (UTC)
Received: (qmail 85573 invoked by uid 500); 15 Jul 2014 05:20:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85520 invoked by uid 500); 15 Jul 2014 05:20:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85508 invoked by uid 99); 15 Jul 2014 05:20:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:20:31 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.172 as permitted sender)
Received: from [209.85.214.172] (HELO mail-ob0-f172.google.com) (209.85.214.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:20:29 +0000
Received: by mail-ob0-f172.google.com with SMTP id wn1so5291470obc.17
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:20:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=LS9VIqbFW8xNzD0/hFn2QAfEMnCw9x+s5OmmdQQ+gQE=;
        b=sPU6B1qXn6Bt+XwFra4wavViU7fycIuMX+RV+9RWFKtgZ5wC9/8SElGcsJ5RfMNren
         VEkMheupx+ueV2lc+KBznnDx+g0rdroXbbHwFfudxEFtpmAGp+p1WZSTreLkS8EeKTwx
         z+RIKASdH/2o60MEOREi3r3jEqiMfR81D6VQ6TbOunAskEnMzOip5QgZ/TaT1xPSzOEC
         ZxzeaLaiXhVfMoKKs/bxq9YAr3tgfBoASvmhmOqM7r+EI/2dGzZZmPKc0i79yXKoYwI1
         aletySuVhb5UmRQAowq1j0RxunOWsNzHhojz7W7kgKYsZIYXLOXLaxeA1V7z+JmjhOeR
         Xj/w==
MIME-Version: 1.0
X-Received: by 10.60.42.226 with SMTP id r2mr12813651oel.69.1405401604665;
 Mon, 14 Jul 2014 22:20:04 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 22:20:04 -0700 (PDT)
In-Reply-To: <CA+-p3AHtLVekqu=9YZKmHfN8gvv2fiVcHbfZx9-+Ti18tv9y2Q@mail.gmail.com>
References: <CAO1Ju5KfBi=Gj722RvU67bms73bFRcdJLa+QU6rNskRPpU+SLg@mail.gmail.com>
	<CABPQxsvHqFF-3WWjHgUGYTux22P5+gAkWJ0Fp407=8yT0SF6tg@mail.gmail.com>
	<CAO1Ju5KrKvHRdQxia9azUQ+MQs3ANeynMbgQEB_PV2ysMy3o_Q@mail.gmail.com>
	<53C488B7.402@huawei.com>
	<CA+-p3AGioD-tfQP+_n29pWnJqR_OBWnAbj2Ek_jVA_WVZLn7ag@mail.gmail.com>
	<CABPQxsvOvmHao6ct1OH68w2E2piCcJdMXqbgJFsX4QVeBoC5sg@mail.gmail.com>
	<CA+-p3AHtLVekqu=9YZKmHfN8gvv2fiVcHbfZx9-+Ti18tv9y2Q@mail.gmail.com>
Date: Mon, 14 Jul 2014 22:20:04 -0700
Message-ID: <CABPQxsvxHLYK2Z8y2yZRLp3uwBu476T5o6=cWiucHgQCsrX+kQ@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Andrew,

Yeah, that would be preferable. Definitely worth investigating both,
but the regression is more pressing at the moment.

- Patrick

On Mon, Jul 14, 2014 at 10:02 PM, Andrew Ash <andrew@andrewash.com> wrote:
> I don't believe mine is a regression. But it is related to thread safety =
on
> Hadoop Configuration objects. Should I start a new thread?
> On Jul 15, 2014 12:55 AM, "Patrick Wendell" <pwendell@gmail.com> wrote:
>
>> Andrew is your issue also a regression from 1.0.0 to 1.0.1? The
>> immediate priority is addressing regressions between these two
>> releases.
>>
>> On Mon, Jul 14, 2014 at 9:05 PM, Andrew Ash <andrew@andrewash.com> wrote=
:
>> > I'm not sure either of those PRs will fix the concurrent adds to
>> > Configuration issue I observed. I've got a stack trace and writeup I'l=
l
>> > share in an hour or two (traveling today).
>> > On Jul 14, 2014 9:50 PM, "scwf" <wangfei1@huawei.com> wrote:
>> >
>> >> hi=EF=BC=8CCody
>> >>   i met this issue days before and i post a PR for this(
>> >> https://github.com/apache/spark/pull/1385)
>> >> it's very strange that if i synchronize conf it will deadlock but it =
is
>> ok
>> >> when synchronize initLocalJobConfFuncOpt
>> >>
>> >>
>> >>  Here's the entire jstack output.
>> >>>
>> >>>
>> >>> On Mon, Jul 14, 2014 at 4:44 PM, Patrick Wendell <pwendell@gmail.com
>> >>> <mailto:pwendell@gmail.com>> wrote:
>> >>>
>> >>>     Hey Cody,
>> >>>
>> >>>     This Jstack seems truncated, would you mind giving the entire st=
ack
>> >>>     trace? For the second thread, for instance, we can't see where t=
he
>> >>>     lock is being acquired.
>> >>>
>> >>>     - Patrick
>> >>>
>> >>>     On Mon, Jul 14, 2014 at 1:42 PM, Cody Koeninger
>> >>>     <cody.koeninger@mediacrossing.com <mailto:cody.koeninger@
>> >>> mediacrossing.com>> wrote:
>> >>>      > Hi all, just wanted to give a heads up that we're seeing a
>> >>> reproducible
>> >>>      > deadlock with spark 1.0.1 with 2.3.0-mr1-cdh5.0.2
>> >>>      >
>> >>>      > If jira is a better place for this, apologies in advance -
>> figured
>> >>> talking
>> >>>      > about it on the mailing list was friendlier than randomly
>> >>> (re)opening jira
>> >>>      > tickets.
>> >>>      >
>> >>>      > I know Gary had mentioned some issues with 1.0.1 on the maili=
ng
>> >>> list, once
>> >>>      > we got a thread dump I wanted to follow up.
>> >>>      >
>> >>>      > The thread dump shows the deadlock occurs in the synchronized
>> >>> block of code
>> >>>      > that was changed in HadoopRDD.scala, for the Spark-1097 issue
>> >>>      >
>> >>>      > Relevant portions of the thread dump are summarized below, we
>> can
>> >>> provide
>> >>>      > the whole dump if it's useful.
>> >>>      >
>> >>>      > Found one Java-level deadlock:
>> >>>      > =3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=3D=
=3D=3D=3D=3D=3D=3D=3D=3D=3D
>> >>>      > "Executor task launch worker-1":
>> >>>      >   waiting to lock monitor 0x00007f250400c520 (object
>> >>> 0x00000000fae7dc30, a
>> >>>      > org.apache.hadoop.co <http://org.apache.hadoop.co>
>> >>>      > nf.Configuration),
>> >>>      >   which is held by "Executor task launch worker-0"
>> >>>      > "Executor task launch worker-0":
>> >>>      >   waiting to lock monitor 0x00007f2520495620 (object
>> >>> 0x00000000faeb4fc8, a
>> >>>      > java.lang.Class),
>> >>>      >   which is held by "Executor task launch worker-1"
>> >>>      >
>> >>>      >
>> >>>      > "Executor task launch worker-1":
>> >>>      >         at
>> >>>      > org.apache.hadoop.conf.Configuration.reloadConfiguration(
>> >>> Configuration.java:791)
>> >>>      >         - waiting to lock <0x00000000fae7dc30> (a
>> >>>      > org.apache.hadoop.conf.Configuration)
>> >>>      >         at
>> >>>      > org.apache.hadoop.conf.Configuration.addDefaultResource(
>> >>> Configuration.java:690)
>> >>>      >         - locked <0x00000000faca6ff8> (a java.lang.Class for
>> >>>      > org.apache.hadoop.conf.Configurati
>> >>>      > on)
>> >>>      >         at
>> >>>      > org.apache.hadoop.hdfs.HdfsConfiguration.<clinit>(
>> >>> HdfsConfiguration.java:34)
>> >>>      >         at
>> >>>      > org.apache.hadoop.hdfs.DistributedFileSystem.<clinit>
>> >>> (DistributedFileSystem.java:110
>> >>>      > )
>> >>>      >         at sun.reflect.NativeConstructorAccessorImpl.
>> >>> newInstance0(Native
>> >>>      > Method)
>> >>>      >         at
>> >>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
>> >>> NativeConstructorAccessorImpl.
>> >>>      > java:57)
>> >>>      >         at sun.reflect.NativeConstructorAccessorImpl.
>> >>> newInstance0(Native
>> >>>      > Method)
>> >>>      >         at
>> >>>      > sun.reflect.NativeConstructorAccessorImpl.newInstance(
>> >>> NativeConstructorAccessorImpl.
>> >>>      > java:57)
>> >>>      >         at
>> >>>      > sun.reflect.DelegatingConstructorAccessorImpl.newInstance(
>> >>> DelegatingConstructorAcces
>> >>>      > sorImpl.java:45)
>> >>>      >         at java.lang.reflect.Constructor.
>> >>> newInstance(Constructor.java:525)
>> >>>      >         at java.lang.Class.newInstance0(Class.java:374)
>> >>>      >         at java.lang.Class.newInstance(Class.java:327)
>> >>>      >         at java.util.ServiceLoader$LazyIterator.next(
>> >>> ServiceLoader.java:373)
>> >>>      >         at
>> java.util.ServiceLoader$1.next(ServiceLoader.java:445)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
>> >>> FileSystem.java:2364)
>> >>>      >         - locked <0x00000000faeb4fc8> (a java.lang.Class for
>> >>>      > org.apache.hadoop.fs.FileSystem)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
>> >>> FileSystem.java:2375)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
>> >>> FileSystem.java:2392)
>> >>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
>> >>> FileSystem.java:89)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
>> >>> FileSystem.java:2431)
>> >>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
>> >>> FileSystem.java:2413)
>> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> >>> java:368)
>> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> >>> java:167)
>> >>>      >         at
>> >>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
>> >>> JobConf.java:587)
>> >>>      >         at
>> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> >>> FileInputFormat.java:315)
>> >>>      >         at
>> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> >>> FileInputFormat.java:288)
>> >>>      >         at
>> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> >>> SparkContext.scala:546)
>> >>>      >         at
>> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> >>> SparkContext.scala:546)
>> >>>      >         at
>> >>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
>> >>> 1.apply(HadoopRDD.scala:145)
>> >>>      >
>> >>>      >
>> >>>      >
>> >>>      > ...elided...
>> >>>      >
>> >>>      >
>> >>>      > "Executor task launch worker-0" daemon prio=3D10
>> >>> tid=3D0x0000000001e71800
>> >>>      > nid=3D0x2d97 waiting for monitor entry [0x00007f24d2bf1000]
>> >>>      >    java.lang.Thread.State: BLOCKED (on object monitor)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem.loadFileSystems(
>> >>> FileSystem.java:2362)
>> >>>      >         - waiting to lock <0x00000000faeb4fc8> (a
>> java.lang.Class
>> >>> for
>> >>>      > org.apache.hadoop.fs.FileSystem)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem.getFileSystemClass(
>> >>> FileSystem.java:2375)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem.createFileSystem(
>> >>> FileSystem.java:2392)
>> >>>      >         at org.apache.hadoop.fs.FileSystem.access$200(
>> >>> FileSystem.java:89)
>> >>>      >         at
>> >>>      > org.apache.hadoop.fs.FileSystem$Cache.getInternal(
>> >>> FileSystem.java:2431)
>> >>>      >         at org.apache.hadoop.fs.FileSystem$Cache.get(
>> >>> FileSystem.java:2413)
>> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> >>> java:368)
>> >>>      >         at org.apache.hadoop.fs.FileSystem.get(FileSystem.
>> >>> java:167)
>> >>>      >         at
>> >>>      > org.apache.hadoop.mapred.JobConf.getWorkingDirectory(
>> >>> JobConf.java:587)
>> >>>      >         at
>> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> >>> FileInputFormat.java:315)
>> >>>      >         at
>> >>>      > org.apache.hadoop.mapred.FileInputFormat.setInputPaths(
>> >>> FileInputFormat.java:288)
>> >>>      >         at
>> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> >>> SparkContext.scala:546)
>> >>>      >         at
>> >>>      > org.apache.spark.SparkContext$$anonfun$22.apply(
>> >>> SparkContext.scala:546)
>> >>>      >         at
>> >>>      > org.apache.spark.rdd.HadoopRDD$$anonfun$getJobConf$
>> >>> 1.apply(HadoopRDD.scala:145)
>> >>>
>> >>>
>> >>>
>> >>
>> >> --
>> >>
>> >> Best Regards
>> >> Fei Wang
>> >>
>> >> ------------------------------------------------------------
>> >> --------------------
>> >>
>> >>
>> >>
>>

From dev-return-8368-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:23:48 2014
Return-Path: <dev-return-8368-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EE00E113A4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:23:48 +0000 (UTC)
Received: (qmail 92862 invoked by uid 500); 15 Jul 2014 05:23:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92806 invoked by uid 500); 15 Jul 2014 05:23:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92795 invoked by uid 99); 15 Jul 2014 05:23:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:23:48 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.175] (HELO mail-vc0-f175.google.com) (209.85.220.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:23:41 +0000
Received: by mail-vc0-f175.google.com with SMTP id hu12so4027694vcb.20
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:23:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=bubwyZwV2pLKZnwJ3DOC3VsUUnOiNnwVVgPHwmKmpmw=;
        b=Rawp+dsTMaP3Fi2dxNx1WWPuot7Z15a13eqZPgwxOxQUh9Inxkc9gSqYXmw1fM334J
         Id7MbUYVk81jLViz9dpso5tBf7H6XE5USxB+6vC6fIsvE9ot/TC45Ns5emlrVqb+QMK4
         oaUSrPTVLTOOjeSl202taJ3KkRk0Q1ytmsUuDda99bu6axmzZEFEl8DlIFfA0zZdUDQV
         xncnppggL8aoanMxZUCkmj6pXG+9P2z9G/Aiie3bG4LVPLvb3O+rN4BX7Fi2NcmWYHGC
         rN0wqt2OhlWYlVanRjW3wXbiwp37DmFJo0g8AKUOaGLYoTIFw1aHfaknkkJfq8A/glbj
         QIZQ==
X-Gm-Message-State: ALoCoQltQgJk0qUY6Jxfd/uciLO2epZXyu+R3nrYPzW9xOkfqDiSxp0SyWqBPtWDcZSgzxwzRUmG
X-Received: by 10.221.40.193 with SMTP id tr1mr4635446vcb.31.1405401795710;
        Mon, 14 Jul 2014 22:23:15 -0700 (PDT)
Received: from mail-vc0-f178.google.com (mail-vc0-f178.google.com [209.85.220.178])
        by mx.google.com with ESMTPSA id pr9sm6103744vec.1.2014.07.14.22.23.14
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 14 Jul 2014 22:23:14 -0700 (PDT)
Received: by mail-vc0-f178.google.com with SMTP id la4so2518508vcb.9
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:23:13 -0700 (PDT)
X-Received: by 10.220.166.9 with SMTP id k9mr20193556vcy.20.1405401793694;
 Mon, 14 Jul 2014 22:23:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Mon, 14 Jul 2014 22:22:53 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 15 Jul 2014 01:22:53 -0400
Message-ID: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
Subject: Hadoop's Configuration object isn't threadsafe
To: dev@spark.apache.org
Content-Type: multipart/mixed; boundary=047d7b3a8ae6ae2c4b04fe349a66
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a8ae6ae2c4b04fe349a66
Content-Type: multipart/alternative; boundary=047d7b3a8ae6ae2c4504fe349a64

--047d7b3a8ae6ae2c4504fe349a64
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Spark devs,

We discovered a very interesting bug in Spark at work last week in Spark
0.9.1 =E2=80=94 that the way Spark uses the Hadoop Configuration object is =
prone to
thread safety issues.  I believe it still applies in Spark 1.0.1 as well.
 Let me explain:


*Observations*

   - Was running a relatively simple job (read from Avro files, do a map,
   do another map, write back to Avro files)
   - 412 of 413 tasks completed, but the last task was hung in RUNNING stat=
e
   - The 412 successful tasks completed in median time 3.4s
   - The last hung task didn't finish even in 20 hours
   - The executor with the hung task was responsible for 100% of one core
   of CPU usage
   - Jstack of the executor attached (relevant thread pasted below)


*Diagnosis*

After doing some code spelunking, we determined the issue was concurrent
use of a Configuration object for each task on an executor.  In Hadoop each
task runs in its own JVM, but in Spark multiple tasks can run in the same
JVM, so the single-threaded access assumptions of the Configuration object
no longer hold in Spark.

The specific issue is that the AvroRecordReader actually _modifies_ the
JobConf it's given when it's instantiated!  It adds a key for the RPC
protocol engine in the process of connecting to the Hadoop FileSystem.
 When many tasks start at the same time (like at the start of a job), many
tasks are adding this configuration item to the one Configuration object at
once.  Internally Configuration uses a java.lang.HashMap, which isn't
threadsafe=E2=80=A6 The below post is an excellent explanation of what happ=
ens in
the situation where multiple threads insert into a HashMap at the same time=
.

http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html

The gist is that you have a thread following a cycle of linked list nodes
indefinitely.  This exactly matches our observations of the 100% CPU core
and also the final location in the stack trace.

So it seems the way Spark shares a Configuration object between task
threads in an executor is incorrect.  We need some way to prevent
concurrent access to a single Configuration object.


*Proposed fix*

We can clone the JobConf object in HadoopRDD.getJobConf() so each task gets
its own JobConf object (and thus Configuration object).  The optimization
of broadcasting the Configuration object across the cluster can remain, but
on the other side I think it needs to be cloned for each task to allow for
concurrent access.  I'm not sure the performance implications, but the
comments suggest that the Configuration object is ~10KB so I would expect a
clone on the object to be relatively speedy.

Has this been observed before?  Does my suggested fix make sense?  I'd be
happy to file a Jira ticket and continue discussion there for the right way
to fix.


Thanks!
Andrew


P.S.  For others seeing this issue, our temporary workaround is to enable
spark.speculation, which retries failed (or hung) tasks on other machines.



"Executor task launch worker-6" daemon prio=3D10 tid=3D0x00007f91f01fe000
nid=3D0x54b1 runnable [0x00007f92d74f1000]
   java.lang.Thread.State: RUNNABLE
    at java.util.HashMap.transfer(HashMap.java:601)
    at java.util.HashMap.resize(HashMap.java:581)
    at java.util.HashMap.addEntry(HashMap.java:879)
    at java.util.HashMap.put(HashMap.java:505)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
    at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
    at
org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
    at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
    at
org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(Name=
NodeProxies.java:343)
    at
org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.jav=
a:168)
    at
org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129=
)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
    at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
    at
org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSyst=
em.java:125)
    at
org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
    at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
    at
org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
    at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
    at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
    at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
    at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
    at
org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
    at
org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java=
:52)
    at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
    at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
    at org.apache.spark.scheduler.Task.run(Task.scala:53)
    at
org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(E=
xecutor.scala:211)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:4=
2)
    at
org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:4=
1)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at
org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.j=
ava:1408)
    at
org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176=
)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1=
145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:=
615)
    at java.lang.Thread.run(Thread.java:745)

--047d7b3a8ae6ae2c4504fe349a64
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div class=3D"gmail_quote"><div style=3D"word-wrap:break-w=
ord;color:rgb(0,0,0);font-size:15px"><div style=3D"font-family:Calibri,sans=
-serif">Hi Spark devs,</div><div style=3D"font-family:Calibri,sans-serif"><=
br></div>

<div style=3D"font-family:Calibri,sans-serif">We discovered a very interest=
ing bug in Spark at work last week in Spark 0.9.1 =E2=80=94 that the way Sp=
ark uses the Hadoop Configuration object is prone to thread safety issues. =
=C2=A0I believe it still applies in Spark 1.0.1 as well. =C2=A0Let me expla=
in:</div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif"><br></div><div style=3D"font-family:Calibri,sans=
-serif"><b>Observations</b></div><ul style=3D"font-family:Calibri,sans-seri=
f">
<li>
Was running a relatively simple job (read from Avro files, do a map, do ano=
ther map, write back to Avro files)</li><li>412 of 413 tasks completed, but=
 the last task was hung in RUNNING state</li><li>The 412 successful tasks c=
ompleted in median time 3.4s</li>

<li>The last hung task didn&#39;t finish even in 20 hours</li><li>The execu=
tor with the hung task was responsible for 100% of one core of CPU usage</l=
i><li>Jstack of the executor attached (relevant thread pasted below)</li>

</ul><div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"=
font-family:Calibri,sans-serif"><b>Diagnosis</b></div><div style=3D"font-fa=
mily:Calibri,sans-serif"><br></div><div style=3D"font-family:Calibri,sans-s=
erif">

After doing some code spelunking, we determined the issue was concurrent us=
e of a Configuration object for each task on an executor. =C2=A0In Hadoop e=
ach task runs in its own JVM, but in Spark multiple tasks can run in the sa=
me JVM, so the single-threaded access assumptions of the Configuration obje=
ct no longer hold in Spark.</div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif">The specific issue is that the AvroRecordReader =
actually _modifies_ the JobConf it&#39;s given when it&#39;s instantiated! =
=C2=A0It adds a key for the RPC protocol engine in the process of connectin=
g to the Hadoop FileSystem. =C2=A0When many tasks start at the same time (l=
ike at the start of a job), many tasks are adding this configuration item t=
o the one Configuration object at once. =C2=A0Internally Configuration uses=
 a java.lang.HashMap, which isn&#39;t threadsafe=E2=80=A6 The below post is=
 an excellent explanation of what happens in the situation where multiple t=
hreads insert into a HashMap at the same time.</div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif"><a href=3D"http://mailinator.blogspot.com/2009/0=
6/beautiful-race-condition.html" target=3D"_blank">http://mailinator.blogsp=
ot.com/2009/06/beautiful-race-condition.html</a></div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif">The gist is that you have a thread following a c=
ycle of linked list nodes indefinitely. =C2=A0This exactly matches our obse=
rvations of the 100% CPU core and also the final location in the stack trac=
e.</div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif">So it seems the way Spark shares a Configuration=
 object between task threads in an executor is incorrect. =C2=A0We need som=
e way to prevent concurrent access to a single Configuration object.</div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif"><br></div><div style=3D"font-family:Calibri,sans=
-serif"><b>Proposed fix</b></div><div><br></div><div>We can clone the JobCo=
nf object in HadoopRDD.getJobConf() so each task gets its own JobConf objec=
t (and thus Configuration object). =C2=A0The optimization of=C2=A0broadcast=
ing the Configuration object across the cluster can remain, but on the othe=
r side I think it needs to be cloned for each task to allow for concurrent =
access. =C2=A0I&#39;m not sure the performance implications, but the commen=
ts suggest that the Configuration object is ~10KB so I would expect a clone=
 on the object to be relatively speedy.</div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif">Has this been observed before? =C2=A0Does my sug=
gested fix make sense? =C2=A0I&#39;d be happy to file a Jira ticket and con=
tinue discussion there for the right way to fix.</div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif"><br></div><div style=3D"font-family:Calibri,sans=
-serif">Thanks!</div><div style=3D"font-family:Calibri,sans-serif">Andrew</=
div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif"><br></div><div><span style=3D"font-family:Calibr=
i,sans-serif">P.S. =C2=A0For others seeing this issue, our temporary workar=
ound is to enable </span><font face=3D"Courier New">spark.speculation</font=
><font face=3D"Calibri,sans-serif">, which retries failed (or hung) tasks o=
n other machines.</font></div>

<div style=3D"font-family:Calibri,sans-serif"><br></div><div style=3D"font-=
family:Calibri,sans-serif"><br></div><div style=3D"font-family:Calibri,sans=
-serif"><br></div><div><div><font face=3D"Courier New">&quot;Executor task =
launch worker-6&quot; daemon prio=3D10 tid=3D0x00007f91f01fe000 nid=3D0x54b=
1 runnable [0x00007f92d74f1000]</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0java.lang.Thread.State: RUNNAB=
LE</font></div><div><font face=3D"Courier New">=C2=A0 =C2=A0 at java.util.H=
ashMap.transfer(HashMap.java:601)</font></div><div><font face=3D"Courier Ne=
w">=C2=A0 =C2=A0 at java.util.HashMap.resize(HashMap.java:581)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at java.util.HashMap.addEntry=
(HashMap.java:879)</font></div><div><font face=3D"Courier New">=C2=A0 =C2=
=A0 at java.util.HashMap.put(HashMap.java:505)</font></div><div><font face=
=3D"Courier New" style=3D"background-color:rgb(255,255,0)">=C2=A0 =C2=A0 at=
 org.apache.hadoop.conf.Configuration.set(Configuration.java:803)</font></d=
iv>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.conf.Con=
figuration.set(Configuration.java:783)</font></div><div><font face=3D"Couri=
er New">=C2=A0 =C2=A0 at org.apache.hadoop.conf.Configuration.setClass(Conf=
iguration.java:1662)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.ipc.RPC.=
setProtocolEngine(RPC.java:193)</font></div><div><font face=3D"Courier New"=
>=C2=A0 =C2=A0 at org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithC=
lientProtocol(NameNodeProxies.java:343)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.hdfs.Nam=
eNodeProxies.createNonHAProxy(NameNodeProxies.java:168)</font></div><div><f=
ont face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.hdfs.NameNodePr=
oxies.createProxy(NameNodeProxies.java:129)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.hdfs.DFS=
Client.&lt;init&gt;(DFSClient.java:436)</font></div><div><font face=3D"Cour=
ier New">=C2=A0 =C2=A0 at org.apache.hadoop.hdfs.DFSClient.&lt;init&gt;(DFS=
Client.java:403)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.hdfs.Dis=
tributedFileSystem.initialize(DistributedFileSystem.java:125)</font></div><=
div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSy=
stem.createFileSystem(FileSystem.java:2262)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.fs.FileS=
ystem.access$200(FileSystem.java:86)</font></div><div><font face=3D"Courier=
 New">=C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem$Cache.getInternal(Fi=
leSystem.java:2296)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.fs.FileS=
ystem$Cache.get(FileSystem.java:2278)</font></div><div><font face=3D"Courie=
r New">=C2=A0 =C2=A0 at org.apache.hadoop.fs.FileSystem.get(FileSystem.java=
:316)</font></div><div>

<font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.fs.Path.getFi=
leSystem(Path.java:194)</font></div><div><font face=3D"Courier New">=C2=A0 =
=C2=A0 at org.apache.avro.mapred.FsInput.&lt;init&gt;(FsInput.java:37)</fon=
t></div><div><font face=3D"Courier New" style=3D"background-color:rgb(255,2=
55,0)">=C2=A0 =C2=A0 at org.apache.avro.mapred.AvroRecordReader.&lt;init&gt=
;(AvroRecordReader.java:43)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.avro.mapred.Avr=
oInputFormat.getRecordReader(AvroInputFormat.java:52)</font></div><div><fon=
t face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.rdd.HadoopRDD$$ano=
n$1.&lt;init&gt;(HadoopRDD.scala:156)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.rdd.Hadoo=
pRDD.compute(HadoopRDD.scala:149)</font></div><div><font face=3D"Courier Ne=
w">=C2=A0 =C2=A0 at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:=
64)</font></div><div>

<font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.rdd.RDD.comput=
eOrReadCheckpoint(RDD.scala:241)</font></div><div><font face=3D"Courier New=
">=C2=A0 =C2=A0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)</font><=
/div><div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.rdd.=
MappedRDD.compute(MappedRDD.scala:31)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.rdd.RDD.c=
omputeOrReadCheckpoint(RDD.scala:241)</font></div><div><font face=3D"Courie=
r New">=C2=A0 =C2=A0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)</f=
ont></div><div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark=
.rdd.MappedRDD.compute(MappedRDD.scala:31)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.rdd.RDD.c=
omputeOrReadCheckpoint(RDD.scala:241)</font></div><div><font face=3D"Courie=
r New">=C2=A0 =C2=A0 at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)</f=
ont></div><div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark=
.scheduler.ResultTask.runTask(ResultTask.scala:109)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.scheduler=
.Task.run(Task.scala:53)</font></div><div><font face=3D"Courier New">=C2=A0=
 =C2=A0 at org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.app=
ly$mcV$sp(Executor.scala:211)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.deploy.Sp=
arkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)</font></div><div><font =
face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.deploy.SparkHadoopUt=
il$$anon$1.run(SparkHadoopUtil.scala:41)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at java.security.AccessContro=
ller.doPrivileged(Native Method)</font></div><div><font face=3D"Courier New=
">=C2=A0 =C2=A0 at javax.security.auth.Subject.doAs(Subject.java:415)</font=
></div><div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.hadoop.s=
ecurity.UserGroupInformation.doAs(UserGroupInformation.java:1408)</font></d=
iv>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.deploy.Sp=
arkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)</font></div><div><font fa=
ce=3D"Courier New">=C2=A0 =C2=A0 at org.apache.spark.executor.Executor$Task=
Runner.run(Executor.scala:176)</font></div>

</div><div><div><font face=3D"Courier New">=C2=A0 =C2=A0 at java.util.concu=
rrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)</font></di=
v><div><font face=3D"Courier New">=C2=A0 =C2=A0 at java.util.concurrent.Thr=
eadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)</font></div>

<div><font face=3D"Courier New">=C2=A0 =C2=A0 at java.lang.Thread.run(Threa=
d.java:745)</font></div></div></div>


</div><br></div>

--047d7b3a8ae6ae2c4504fe349a64--
--047d7b3a8ae6ae2c4b04fe349a66
Content-Type: text/plain; charset=US-ASCII; name="jstack.txt"
Content-Disposition: attachment; filename="jstack.txt"
Content-Transfer-Encoding: base64
X-Attachment-Id: fb1efe80da84a23a_0.0.1
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--047d7b3a8ae6ae2c4b04fe349a66--

From dev-return-8369-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 05:33:23 2014
Return-Path: <dev-return-8369-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D170113D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 05:33:23 +0000 (UTC)
Received: (qmail 8353 invoked by uid 500); 15 Jul 2014 05:33:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8291 invoked by uid 500); 15 Jul 2014 05:33:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8278 invoked by uid 99); 15 Jul 2014 05:33:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:33:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 05:33:18 +0000
Received: by mail-wi0-f176.google.com with SMTP id bs8so3692212wib.15
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 22:32:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=JOFmTX2GcKTlgCXEA369VpCnzcUH7I8YGWI2IRE6f1Y=;
        b=1J0RJhCYNcMW35VbFgDRAJP/CYav7d1+6Vu05Ac4MdhVPuyBbcH2XhfuqbCBWagnCs
         id3veFknpNIRXs7i5OWxjli1prXrxCvP1nHOMRWZj6KMuTivyv7DFBbuXJhsl3fsp+R7
         n8269Ttkbf2agPvCurdv/DK/x30W2u1QehkIlWOQ5WhDPx+6xzZTvh7GTmmGE9RZ+ZWy
         ySlYAjB+olhA9A5QX78hphRKuk6NRAYpWGFbUpVdK9brLuvUgZWWm++UsaylN8Sd2TpB
         LoPTt2vL0sXXXxtIa7itmjXUk5xeQ5j0qaWZXyGqWx+6YDrLJYEo+GcYEH9GOAgamPqR
         nRww==
X-Received: by 10.194.57.132 with SMTP id i4mr4514343wjq.6.1405402376829; Mon,
 14 Jul 2014 22:32:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.24.168 with HTTP; Mon, 14 Jul 2014 22:32:16 -0700 (PDT)
In-Reply-To: <CABPQxsvj4XUtNy1csfY8yYiTT5apiVAAfxRiN7ngx0u9czJbag@mail.gmail.com>
References: <CAOhmDzcN=ehCrms2krYX-gY2bPTu4P-KGEMaQd9vK2+jot9_vQ@mail.gmail.com>
 <CANGvG8psZbChNgZkUL=rK9AFXfmxFJ1oXqFPpK_Am_onMAbFJA@mail.gmail.com>
 <CAKx7Bf8tE1D88r36SXHrZeN4c2ehehpAzbNSt=ZtrSmuXkteZg@mail.gmail.com> <CABPQxsvj4XUtNy1csfY8yYiTT5apiVAAfxRiN7ngx0u9czJbag@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 15 Jul 2014 01:32:16 -0400
Message-ID: <CAOhmDzetCwXzXh9kNirid20QaixSUHAoPLbujaiyzG7w-=McUw@mail.gmail.com>
Subject: Re: ec2 clusters launched at 9fe693b5b6 are broken (?)
To: dev <dev@spark.apache.org>
Cc: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Content-Type: multipart/alternative; boundary=047d7ba9782e70008f04fe34bdb8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9782e70008f04fe34bdb8
Content-Type: text/plain; charset=UTF-8

Okie doke--added myself as a watcher on that issue.

On a related note, what are the thoughts on automatically spinning up/down
EC2 clusters and running tests against them? It would probably be way too
cumbersome to do that for every build, but perhaps on some schedule it
could help validate that we are still deploying EC2 clusters correctly.

Would something like that be valuable?

Nick


On Tue, Jul 15, 2014 at 1:19 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Yeah - this is likely caused by SPARK-2471.
>
> On Mon, Jul 14, 2014 at 10:11 PM, Shivaram Venkataraman
> <shivaram@eecs.berkeley.edu> wrote:
> > My guess is that this is related to
> > https://issues.apache.org/jira/browse/SPARK-2471 where the S3 library
> gets
> > excluded from the SBT assembly jar. I am not sure if the assembly jar
> used
> > in EC2 is generated using SBT though.
> >
> > Shivaram
> >
> >
> > On Mon, Jul 14, 2014 at 10:02 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
> >
> >> This one is typically due to a mismatch between the Hadoop versions --
> >> i.e., Spark is compiled against 1.0.4 but is running with 2.3.0 in the
> >> classpath, or something like that. Not certain why you're seeing this
> with
> >> spark-ec2, but I'm assuming this is related to the issues you posted in
> a
> >> separate thread.
> >>
> >>
> >> On Mon, Jul 14, 2014 at 6:43 PM, Nicholas Chammas <
> >> nicholas.chammas@gmail.com> wrote:
> >>
> >> > Just launched an EC2 cluster from git hash
> >> > 9fe693b5b6ed6af34ee1e800ab89c8a11991ea38. Calling take() on an RDD
> >> > accessing data in S3 yields the following error output.
> >> >
> >> > I understand that NoClassDefFoundError errors may mean something in
> the
> >> > deployment was messed up. Is that correct? When I launch a cluster
> using
> >> > spark-ec2, I expect all critical deployment details to be taken care
> of
> >> by
> >> > the script.
> >> >
> >> > So is something in the deployment executed by spark-ec2 borked?
> >> >
> >> > Nick
> >> >
> >> > java.lang.NoClassDefFoundError: org/jets3t/service/S3ServiceException
> >> >     at
> >> >
> >>
> org.apache.hadoop.fs.s3native.NativeS3FileSystem.createDefaultStore(NativeS3FileSystem.java:224)
> >> >     at
> >> >
> >>
> org.apache.hadoop.fs.s3native.NativeS3FileSystem.initialize(NativeS3FileSystem.java:214)
> >> >     at
> >> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:1386)
> >> >     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:66)
> >> >     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:1404)
> >> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:254)
> >> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:187)
> >> >     at
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:176)
> >> >     at
> >> >
> >>
> org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:208)
> >> >     at
> org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:176)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >> >     at scala.Option.getOrElse(Option.scala:120)
> >> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >> >     at
> org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >> >     at scala.Option.getOrElse(Option.scala:120)
> >> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >> >     at scala.Option.getOrElse(Option.scala:120)
> >> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >> >     at org.apache.spark.ShuffleDependency.<init>(Dependency.scala:71)
> >> >     at
> >> > org.apache.spark.rdd.ShuffledRDD.getDependencies(ShuffledRDD.scala:79)
> >> >     at
> >> > org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:190)
> >> >     at
> >> > org.apache.spark.rdd.RDD$$anonfun$dependencies$2.apply(RDD.scala:188)
> >> >     at scala.Option.getOrElse(Option.scala:120)
> >> >     at org.apache.spark.rdd.RDD.dependencies(RDD.scala:188)
> >> >     at
> >> >
> >>
> org.apache.spark.scheduler.DAGScheduler.getPreferredLocs(DAGScheduler.scala:1144)
> >> >     at
> >> > org.apache.spark.SparkContext.getPreferredLocs(SparkContext.scala:903)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.PartitionCoalescer.currPrefLocs(CoalescedRDD.scala:174)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:191)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator$$anonfun$4$$anonfun$apply$2.apply(CoalescedRDD.scala:190)
> >> >     at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
> >> >     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
> >> >     at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:350)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.PartitionCoalescer$LocationIterator.<init>(CoalescedRDD.scala:185)
> >> >     at
> >> >
> >>
> org.apache.spark.rdd.PartitionCoalescer.setupGroups(CoalescedRDD.scala:236)
> >> >     at
> >> org.apache.spark.rdd.PartitionCoalescer.run(CoalescedRDD.scala:337)
> >> >     at
> >> > org.apache.spark.rdd.CoalescedRDD.getPartitions(CoalescedRDD.scala:83)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >> >     at scala.Option.getOrElse(Option.scala:120)
> >> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >> >     at
> org.apache.spark.rdd.MappedRDD.getPartitions(MappedRDD.scala:28)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:203)
> >> >     at
> >> org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:201)
> >> >     at scala.Option.getOrElse(Option.scala:120)
> >> >     at org.apache.spark.rdd.RDD.partitions(RDD.scala:201)
> >> >     at org.apache.spark.rdd.RDD.take(RDD.scala:1036)
> >> >     at $iwC$$iwC$$iwC$$iwC.<init>(<console>:26)
> >> >     at $iwC$$iwC$$iwC.<init>(<console>:31)
> >> >     at $iwC$$iwC.<init>(<console>:33)
> >> >     at $iwC.<init>(<console>:35)
> >> >     at <init>(<console>:37)
> >> >     at .<init>(<console>:41)
> >> >     at .<clinit>(<console>)
> >> >     at .<init>(<console>:7)
> >> >     at .<clinit>(<console>)
> >> >     at $print(<console>)
> >> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >> >     at
> >> >
> >>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> >> >     at
> >> >
> >>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> >> >     at java.lang.reflect.Method.invoke(Method.java:606)
> >> >     at
> >> >
> org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:788)
> >> >     at
> >> >
> >>
> org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1056)
> >> >     at
> >> > org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:614)
> >> >     at
> org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:645)
> >> >     at
> org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:609)
> >> >     at
> >> >
> org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:796)
> >> >     at
> >> >
> >>
> org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:841)
> >> >     at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:753)
> >> >     at
> >> org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:601)
> >> >     at
> org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:608)
> >> >     at org.apache.spark.repl.SparkILoop.loop(SparkILoop.scala:611)
> >> >     at
> >> >
> >>
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply$mcZ$sp(SparkILoop.scala:936)
> >> >     at
> >> >
> >>
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
> >> >     at
> >> >
> >>
> org.apache.spark.repl.SparkILoop$$anonfun$process$1.apply(SparkILoop.scala:884)
> >> >     at
> >> >
> >>
> scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
> >> >     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:884)
> >> >     at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:982)
> >> >     at org.apache.spark.repl.Main$.main(Main.scala:31)
> >> >     at org.apache.spark.repl.Main.main(Main.scala)
> >> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >> >     at
> >> >
> >>
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
> >> >     at
> >> >
> >>
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
> >> >     at java.lang.reflect.Method.invoke(Method.java:606)
> >> >     at
> org.apache.spark.deploy.SparkSubmit$.launch(SparkSubmit.scala:303)
> >> >     at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:55)
> >> >     at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
> >> > Caused by: java.lang.ClassNotFoundException:
> >> > org.jets3t.service.S3ServiceException
> >> >     at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
> >> >     at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
> >> >     at java.security.AccessController.doPrivileged(Native Method)
> >> >     at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
> >> >     at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
> >> >     at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
> >> >     at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
> >> >     ... 92 more
> >> >
> >> >
> >> >
> >>
>

--047d7ba9782e70008f04fe34bdb8--

From dev-return-8370-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 06:03:14 2014
Return-Path: <dev-return-8370-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB3231149F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 06:03:13 +0000 (UTC)
Received: (qmail 52786 invoked by uid 500); 15 Jul 2014 06:03:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52739 invoked by uid 500); 15 Jul 2014 06:03:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52726 invoked by uid 99); 15 Jul 2014 06:03:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 06:03:12 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 06:03:11 +0000
Received: by mail-oa0-f50.google.com with SMTP id g18so5378867oah.23
        for <dev@spark.apache.org>; Mon, 14 Jul 2014 23:02:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=N3ndas7CxOwdW/TDGRi3BivGgX7ptbIFz7XBJ1CcYts=;
        b=c++URIrD0AFbHUuxz/W2KdL4ZZRFkQHHtqcw9b5bXiRBGzNedC/Jg25JuO19+Z2Zac
         MQ+g+WwlUpSdBMII18roPevfOcJMrzdW8ufV3LVZchKZeIeTtU1vhzy3ViLy41cXLdZx
         P83i+YiHroGcxWshaOR2Nze02Zn0ThVAWPimUmfNOaOoIZ1JEB+6/+XHR/DOb2A894fL
         mXDivznNU/dwOITCbScxaklhZasQexCjokM7uzjsNSzJ4pgc+eNXjl2GKgMFL058NHVl
         SusB+HLP3zORrg/KkJJZ/RRX4BuHitkVMELc+6VoVF6MMMsbieppna7jlq6wofmcf/qO
         MRAQ==
MIME-Version: 1.0
X-Received: by 10.60.42.226 with SMTP id r2mr13012592oel.69.1405404166289;
 Mon, 14 Jul 2014 23:02:46 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 14 Jul 2014 23:02:46 -0700 (PDT)
In-Reply-To: <CAEYYnxaMOjTPWXONZgsvuyAF3bPHOCzzbN6POixsMUAmiCD7tw@mail.gmail.com>
References: <CAEYYnxaMOjTPWXONZgsvuyAF3bPHOCzzbN6POixsMUAmiCD7tw@mail.gmail.com>
Date: Mon, 14 Jul 2014 23:02:46 -0700
Message-ID: <CABPQxsu1krWEFDsb-YT-hstQBDr5CgHfMSN3h+wpyhkdY00ZNA@mail.gmail.com>
Subject: Re: SBT gen-idea doesn't work well after merging SPARK-1776
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks DB,

Feel free to file sub-jira's under:
https://issues.apache.org/jira/browse/SPARK-2487

I've been importing the Maven build into Intellij, it might be worth
trying that as well to see if it works.

- Patrick

On Mon, Jul 14, 2014 at 4:53 PM, DB Tsai <dbtsai@dbtsai.com> wrote:
> I've a clean clone of spark master repository, and I generated the
> intellij project file by sbt gen-idea as usual. There are two issues
> we have after merging SPARK-1776 (read dependencies from Maven).
>
> 1) After SPARK-1776, sbt gen-idea will download the dependencies from
> internet even those jars are in local cache. Before merging, the
> second time we run gen-idea will not download anything but use the
> jars in cache.
>
> 2) The tests with spark local context can not be run in the intellij.
> It will show the following exception.
>
> The current workaround we've are checking out any snapshot before
> merging to gen-idea, and then switch back to current master. But this
> will not work when the master deviate too much from the latest working
> snapshot.
>
> [ERROR] [07/14/2014 16:27:49.967] [ScalaTest-run] [Remoting] Remoting
> error: [Startup timed out] [
> akka.remote.RemoteTransportException: Startup timed out
> at akka.remote.Remoting.akka$remote$Remoting$$notifyError(Remoting.scala:129)
> at akka.remote.Remoting.start(Remoting.scala:191)
> at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
> at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
> at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
> at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
> at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
> at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
> at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
> at org.apache.spark.SparkEnv$.create(SparkEnv.scala:153)
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:117)
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:132)
> at org.apache.spark.mllib.util.LocalSparkContext$class.beforeAll(LocalSparkContext.scala:29)
> at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
> at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
> at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
> at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)
> at org.apache.spark.mllib.optimization.LBFGSSuite.run(LBFGSSuite.scala:27)
> at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
> at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
> at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
> at scala.collection.immutable.List.foreach(List.scala:318)
> at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
> at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
> at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
> at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
> at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
> at org.scalatest.tools.Runner$.run(Runner.scala:883)
> at org.scalatest.tools.Runner.run(Runner.scala)
> at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
> at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
> at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
> at java.lang.reflect.Method.invoke(Method.java:597)
> at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
> Caused by: java.util.concurrent.TimeoutException: Futures timed out
> after [10000 milliseconds]
> at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
> at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
> at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
> at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
> at scala.concurrent.Await$.result(package.scala:107)
> at akka.remote.Remoting.start(Remoting.scala:173)
> ... 35 more
> ]
>
> An exception or error caused a run to abort: Futures timed out after
> [10000 milliseconds]
> java.util.concurrent.TimeoutException: Futures timed out after [10000
> milliseconds]
> at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
> at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
> at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
> at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
> at scala.concurrent.Await$.result(package.scala:107)
> at akka.remote.Remoting.start(Remoting.scala:173)
> at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
> at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:579)
> at akka.actor.ActorSystemImpl._start(ActorSystem.scala:577)
> at akka.actor.ActorSystemImpl.start(ActorSystem.scala:588)
> at akka.actor.ActorSystem$.apply(ActorSystem.scala:111)
> at akka.actor.ActorSystem$.apply(ActorSystem.scala:104)
> at org.apache.spark.util.AkkaUtils$.createActorSystem(AkkaUtils.scala:104)
> at org.apache.spark.SparkEnv$.create(SparkEnv.scala:153)
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:202)
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:117)
> at org.apache.spark.SparkContext.<init>(SparkContext.scala:132)
> at org.apache.spark.mllib.util.LocalSparkContext$class.beforeAll(LocalSparkContext.scala:29)
> at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
> at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
> at org.apache.spark.mllib.optimization.LBFGSSuite.beforeAll(LBFGSSuite.scala:27)
> at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)
> at org.apache.spark.mllib.optimization.LBFGSSuite.run(LBFGSSuite.scala:27)
> at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
> at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
> at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
> at scala.collection.immutable.List.foreach(List.scala:318)
> at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
> at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
> at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
> at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
> at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
> at org.scalatest.tools.Runner$.run(Runner.scala:883)
> at org.scalatest.tools.Runner.run(Runner.scala)
> at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.runScalaTest2(ScalaTestRunner.java:141)
> at org.jetbrains.plugins.scala.testingSupport.scalaTest.ScalaTestRunner.main(ScalaTestRunner.java:32)
> at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
> at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
> at java.lang.reflect.Method.invoke(Method.java:597)
> at com.intellij.rt.execution.application.AppMain.main(AppMain.java:134)
>
>
> Sincerely,
>
> DB Tsai
> -------------------------------------------------------
> My Blog: https://www.dbtsai.com
> LinkedIn: https://www.linkedin.com/in/dbtsai

From dev-return-8371-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 08:12:59 2014
Return-Path: <dev-return-8371-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 86AC4118A2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 08:12:59 +0000 (UTC)
Received: (qmail 93904 invoked by uid 500); 15 Jul 2014 08:12:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93844 invoked by uid 500); 15 Jul 2014 08:12:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93832 invoked by uid 99); 15 Jul 2014 08:12:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 08:12:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.219.53 as permitted sender)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 08:12:54 +0000
Received: by mail-oa0-f53.google.com with SMTP id j17so2631870oag.26
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 01:12:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=VnbwooVk+0MNMYghsvtMxSCjzzSvymjGySgtj5BesbI=;
        b=ytrR3Afm/cd/biicMmFQCVeLqf5LMi+S9mAO0zslRl6MfBdHN/nukUo0CzMEy1V0be
         GabocpSni4QB+p9Kcqc4IjtQHgLbR+m3iPRYYrD0/8nkQ5hTjGzfxyDsxcrOOPxtpYHk
         cW0T0fxuEdO6+LzhTehF31vQOI0DNbNo/9y+TFf5po5GPKrbrYIUsamRvhLJORytavaz
         9dy74l+RhyM+1w0b0DDCI5q2wo6O51wktp4onAAvn5uGsBOBqnK3v036zcb9tyemJoqr
         RP7TFpz9SWRnpWwBLDv6/TwhN2qKWbUkUbn7qu5mek6Z+2ULRYD076JYP3r2DfxUf52z
         kFyw==
MIME-Version: 1.0
X-Received: by 10.182.142.42 with SMTP id rt10mr8548228obb.80.1405411953826;
 Tue, 15 Jul 2014 01:12:33 -0700 (PDT)
Received: by 10.182.95.106 with HTTP; Tue, 15 Jul 2014 01:12:33 -0700 (PDT)
Date: Tue, 15 Jul 2014 10:12:33 +0200
Message-ID: <CALD+6GPjR5EmbRA26TFyXdcVyV3X5Gnp8BX0pvJMfLc+=frKxg@mail.gmail.com>
Subject: Spark-summingbird
From: Nick Pentreath <nick.pentreath@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2f324457cc404fe36f844
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2f324457cc404fe36f844
Content-Type: text/plain; charset=UTF-8

Seems Twitter has made a bit of progress here:
https://github.com/twitter/summingbird/tree/develop/summingbird-spark

May be of interest and perhaps some devs with experience in both may be
able to help out.

N

--001a11c2f324457cc404fe36f844--

From dev-return-8372-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 09:02:20 2014
Return-Path: <dev-return-8372-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C15111A7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 09:02:20 +0000 (UTC)
Received: (qmail 4560 invoked by uid 500); 15 Jul 2014 09:02:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4501 invoked by uid 500); 15 Jul 2014 09:02:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4490 invoked by uid 99); 15 Jul 2014 09:02:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 09:02:19 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.178] (HELO mail-qc0-f178.google.com) (209.85.216.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 09:02:15 +0000
Received: by mail-qc0-f178.google.com with SMTP id x3so2695658qcv.23
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 02:01:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=pVTqZgH8kaCScEJzVu3cZazDp+PZFdlicLZRAxNeJqQ=;
        b=lKh4aTQ/BAvik/+qEI/ugqDtQeTyXRC6mrIlg+/F9Bv84z6UpsgcQjsp9L1YadGUbv
         d/f9MHLkDjoGrQGrbFdZn4f5smaDfnDx6SiiYX2zsCpK1EapQG/5+q6X5+lbUaJ6JPTj
         U8HWkyDmiFiQxPumCKvJPAowq+YpIYVmLaBe41eFiJvY00YXHkWLeoP73hmuSWImQ7MT
         W98E02Q3KFl6nKP1brWWogm54FB4QPSbtNL+pNiU8BO0nOjYDHK/TLM/07/QWVGuWCIQ
         WJEFJEq8RxZORHLt0cdZmoGriKs5XUug5YilctG0C3ILkZHpY1unSwy8KqGKS0ooyPXU
         s3kQ==
X-Gm-Message-State: ALoCoQnPbsevRJHMNu7ehUY98QPFPzL4LznIbIOvRMHbyjjnwutvenAG2qFEoWNlUzpThjrwpfNx
X-Received: by 10.229.252.130 with SMTP id mw2mr33333213qcb.12.1405414914974;
 Tue, 15 Jul 2014 02:01:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Tue, 15 Jul 2014 02:01:34 -0700 (PDT)
In-Reply-To: <CANGvG8oOKFHNn6tNMaV3Oo_-A1OGAZsRj8DDS-MgPQAeMtPPJw@mail.gmail.com>
References: <CAPh_B=aD1bbahC3E0c=d7Tm=1Dae9OZtsTekh=yRfehmB9TcMw@mail.gmail.com>
 <20140714173027.6e60b16f@sh9> <1C3442B5-637F-4DA0-BD98-F1F95E2AEFD9@gmail.com>
 <CAPh_B=abZT0S94+ESrWBYx2FJw7O1_g84AGep3sRq1nkXJDBMA@mail.gmail.com>
 <CAFnjqBf9ecoABf0u4UJfhy1iGDgkJbdhyCt1iPamYBLp61DwCQ@mail.gmail.com> <CANGvG8oOKFHNn6tNMaV3Oo_-A1OGAZsRj8DDS-MgPQAeMtPPJw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 15 Jul 2014 02:01:34 -0700
Message-ID: <CAPh_B=azfgZxpc+WJc3UeQY9z9Aj2-ndVByeX4kqygqmyEUTkQ@mail.gmail.com>
Subject: Re: better compression codecs for shuffle blocks?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2e920c4fac304fe37a81a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2e920c4fac304fe37a81a
Content-Type: text/plain; charset=UTF-8

FYI dev,

I submitted a PR making Snappy as the default compression codec:
https://github.com/apache/spark/pull/1415

Also submitted a separate PR to add lz4 support:
https://github.com/apache/spark/pull/1416


On Mon, Jul 14, 2014 at 5:06 PM, Aaron Davidson <ilikerps@gmail.com> wrote:

> One of the core problems here is the number of open streams we have, which
> is (# cores * # reduce partitions), which can easily climb into the tens of
> thousands for large jobs. This is a more general problem that we are
> planning on fixing for our largest shuffles, as even moderate buffer sizes
> can explode to use huge amounts of memory at that scale.
>
>
> On Mon, Jul 14, 2014 at 4:53 PM, Jon Hartlaub <jhartlaub@gmail.com> wrote:
>
> > Is the held memory due to just instantiating the LZFOutputStream?  If so,
> > I'm a surprised and I consider that a bug.
> >
> > I suspect the held memory may be due to a SoftReference - memory will be
> > released with enough memory pressure.
> >
> > Finally, is it necessary to keep 1000 (or more) decoders active?  Would
> it
> > be possible to keep an object pool of encoders and check them in and out
> as
> > needed?  I admit I have not done much homework to determine if this is
> > viable.
> >
> > -Jon
> >
> >
> > On Mon, Jul 14, 2014 at 4:08 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> > > Copying Jon here since he worked on the lzf library at Ning.
> > >
> > > Jon - any comments on this topic?
> > >
> > >
> > > On Mon, Jul 14, 2014 at 3:54 PM, Matei Zaharia <
> matei.zaharia@gmail.com>
> > > wrote:
> > >
> > >> You can actually turn off shuffle compression by setting
> > >> spark.shuffle.compress to false. Try that out, there will still be
> some
> > >> buffers for the various OutputStreams, but they should be smaller.
> > >>
> > >> Matei
> > >>
> > >> On Jul 14, 2014, at 3:30 PM, Stephen Haberman <
> > stephen.haberman@gmail.com>
> > >> wrote:
> > >>
> > >> >
> > >> > Just a comment from the peanut gallery, but these buffers are a real
> > >> > PITA for us as well. Probably 75% of our non-user-error job failures
> > >> > are related to them.
> > >> >
> > >> > Just naively, what about not doing compression on the fly? E.g.
> during
> > >> > the shuffle just write straight to disk, uncompressed?
> > >> >
> > >> > For us, we always have plenty of disk space, and if you're concerned
> > >> > about network transmission, you could add a separate compress step
> > >> > after the blocks have been written to disk, but before being sent
> over
> > >> > the wire.
> > >> >
> > >> > Granted, IANAE, so perhaps this is a bad idea; either way, awesome
> to
> > >> > see work in this area!
> > >> >
> > >> > - Stephen
> > >> >
> > >>
> > >>
> > >
> >
>

--001a11c2e920c4fac304fe37a81a--

From dev-return-8373-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 10:04:08 2014
Return-Path: <dev-return-8373-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4F9511CB8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 10:04:08 +0000 (UTC)
Received: (qmail 46840 invoked by uid 500); 15 Jul 2014 10:04:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46743 invoked by uid 500); 15 Jul 2014 10:04:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45873 invoked by uid 99); 15 Jul 2014 10:04:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 10:04:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 10:04:02 +0000
Received: by mail-vc0-f173.google.com with SMTP id hy10so5932876vcb.32
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 03:03:41 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=yD65UsQ9s3iyrFDoz/oEyd28nkx1NcaHVaYrND8rkks=;
        b=nNBu/tUkdv1NCWsAd5owaXLJ3gBFt0J/Dl4q+jY5OfGxn24KljuetMMLnh9TfzrE/k
         qCJqvkiDI2I46K3ZDs7NcLEwSGI8wbGjmfkWyKcPq8vohzkjkwpdcJIYK5MIJRi8gLVq
         gkKPnSfZnDQU58HH0/phwuHD+q3L7dAi4s5ilnh8UepqBrybVF0D2n58lKHXPut8g/+S
         3CZbvbahIV06EB2R3GOyrxKVxG7wWeB78s6EEUQYQmC4i2ACbSU5fWQqeaxM28WJj+1o
         vAxDwcpqp4RREh8tyv/eGVbuTXJvvbUOtaE5JwAuIW0qpsXnFDXz4vhMBCyYwxlWEw5+
         1a3w==
X-Gm-Message-State: ALoCoQn3UAe9TBCQikvqu0VM8QAezJTSXAjXXIYfi+7rUuoWPaaRMcFbvIJirmSlo5h+NQrOpxor
X-Received: by 10.220.144.10 with SMTP id x10mr9058vcu.42.1405418621523; Tue,
 15 Jul 2014 03:03:41 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Tue, 15 Jul 2014 03:03:21 -0700 (PDT)
In-Reply-To: <CABPQxssmCmQvHnxrr_zbwSs_KMkTb805hquEFnS2FbVKyzYMTg@mail.gmail.com>
References: <CAJOb8btWjn+pVBRngB-gA57CQ2YRw702j+=eP79tRyyPeZgcFA@mail.gmail.com>
 <CALDQvdcYe1ct_LdwmkV5tfR1_rfMMxhFyu=VdZw_Y2aAYmin-Q@mail.gmail.com>
 <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com>
 <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com> <CABPQxssmCmQvHnxrr_zbwSs_KMkTb805hquEFnS2FbVKyzYMTg@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Tue, 15 Jul 2014 11:03:21 +0100
Message-ID: <CAMAsSd+dB2JB0-JqruKp0QXcA-nYy6Xistz3MmwQ3K5zXSnufw@mail.gmail.com>
Subject: Re: Catalyst dependency on Spark Core
To: user@spark.apache.org
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Agree. You end up with a "core" and a "corer core" to distinguish
between and it ends up just being more complicated. This sounds like
something that doesn't need a module.

On Tue, Jul 15, 2014 at 5:59 AM, Patrick Wendell <pwendell@gmail.com> wrote:
> Adding new build modules is pretty high overhead, so if this is a case
> where a small amount of duplicated code could get rid of the
> dependency, that could also be a good short-term option.
>
> - Patrick
>
> On Mon, Jul 14, 2014 at 2:15 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
>> Yeah, I'd just add a spark-util that has these things.
>>
>> Matei
>>
>> On Jul 14, 2014, at 1:04 PM, Michael Armbrust <michael@databricks.com>
>> wrote:
>>
>> Yeah, sadly this dependency was introduced when someone consolidated the
>> logging infrastructure.  However, the dependency should be very small and
>> thus easy to remove, and I would like catalyst to be usable outside of
>> Spark.  A pull request to make this possible would be welcome.
>>
>> Ideally, we'd create some sort of spark common package that has things like
>> logging.  That way catalyst could depend on that, without pulling in all of
>> Hadoop, etc.  Maybe others have opinions though, so I'm cc-ing the dev list.
>>
>>
>> On Mon, Jul 14, 2014 at 12:21 AM, Yanbo Liang <yanbohappy@gmail.com> wrote:
>>>
>>> Make Catalyst independent of Spark is the goal of Catalyst, maybe need
>>> time and evolution.
>>> I awared that package org.apache.spark.sql.catalyst.util embraced
>>> org.apache.spark.util.{Utils => SparkUtils},
>>> so that Catalyst has a dependency on Spark core.
>>> I'm not sure whether it will be replaced by other component independent of
>>> Spark in later release.
>>>
>>>
>>> 2014-07-14 11:51 GMT+08:00 Aniket Bhatnagar <aniket.bhatnagar@gmail.com>:
>>>
>>>> As per the recent presentation given in Scala days
>>>> (http://people.apache.org/~marmbrus/talks/SparkSQLScalaDays2014.pdf), it was
>>>> mentioned that Catalyst is independent of Spark. But on inspecting pom.xml
>>>> of sql/catalyst module, it seems it has a dependency on Spark Core. Any
>>>> particular reason for the dependency? I would love to use Catalyst outside
>>>> Spark
>>>>
>>>> (reposted as previous email bounced. Sorry if this is a duplicate).
>>>
>>>
>>
>>

From dev-return-8374-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 14:15:29 2014
Return-Path: <dev-return-8374-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 61BD71161F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 14:15:29 +0000 (UTC)
Received: (qmail 793 invoked by uid 500); 15 Jul 2014 14:15:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 724 invoked by uid 500); 15 Jul 2014 14:15:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 710 invoked by uid 99); 15 Jul 2014 14:15:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 14:15:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of cody.koeninger@mediacrossing.com designates 209.85.212.179 as permitted sender)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 14:15:26 +0000
Received: by mail-wi0-f179.google.com with SMTP id f8so3458127wiw.6
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 07:15:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=mediacrossing.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=P4/QpQSpv/UqrR84xD73SZa3bU7hkH/ktwWpquSBmxY=;
        b=FNsJA5Qges8FRLETl+fE/oxKIIFN0KuJY89I76s5ePLOvNANIhT9tf6MvBYaSJSh52
         cRa6rlEW6GXf6D1L7rls6eCO4FCX8je22AaL0zsb8F0v3YpVl48eLdqH2JVBEzCKAn7O
         UTYeV+mimG/WZQQVBoX2cpNPazWYoJAZJ3Of4=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=P4/QpQSpv/UqrR84xD73SZa3bU7hkH/ktwWpquSBmxY=;
        b=J+x2NEkEGOVxIZHhchcuvsd83+n0x73pxe128yw/9tvG0lLiJqDRV2iAkxtWix61gR
         vSFxIQxhOfk3uaEjSbaAFwsVfzO+Ffl5vsyMjf42MDvJ10JHjSagafEFm+7CGprY5ZcY
         CaKfzuDTugPn5H6xTqcV0057Frg4wGyAzgQ1RL8DO/sQGrgudpSsDSh7RgFTvPjHMsbo
         mIBxIMKQnu2M/SVCtqS6g+oAFdlNxW+Ac3AwTTAvD6f6tFowcW5YzgOx4WQcePZ4t1bW
         hS2UGhgBOJ0pAyMJNhsX9KZ88mqdxdVkkBPu0FOnSHXYp2NQCKrOEbLg3aRA9CR4PKtq
         VfhA==
X-Gm-Message-State: ALoCoQm4FumnfoE8ced6hFiTjOYOraVM8iaWXyw7YeEgRaolCrqrx24pvZz7jJsW0jyxCLCnOx6w
MIME-Version: 1.0
X-Received: by 10.194.5.103 with SMTP id r7mr28256798wjr.41.1405433701685;
 Tue, 15 Jul 2014 07:15:01 -0700 (PDT)
Received: by 10.194.15.33 with HTTP; Tue, 15 Jul 2014 07:15:01 -0700 (PDT)
Date: Tue, 15 Jul 2014 09:15:01 -0500
Message-ID: <CAO1Ju5Kv7PcvvS1SJ4udhqz8jqZU1nGNEQcAS2QUwpGhpdgtDQ@mail.gmail.com>
Subject: traveling next week
From: Cody Koeninger <cody.koeninger@mediacrossing.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b4512f28b748c04fe3c0848
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4512f28b748c04fe3c0848
Content-Type: text/plain; charset=UTF-8

I'm going to be on a plane wed 23, return flight monday 28, so will miss
daily call those days.  I'll be pushing forward on projects as I can, but
skype availability may be limited, so email if you need something from me.

--047d7b4512f28b748c04fe3c0848--

From dev-return-8375-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 17:11:50 2014
Return-Path: <dev-return-8375-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4241311DE4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 17:11:50 +0000 (UTC)
Received: (qmail 68862 invoked by uid 500); 15 Jul 2014 17:11:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68800 invoked by uid 500); 15 Jul 2014 17:11:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68785 invoked by uid 99); 15 Jul 2014 17:11:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:11:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andy.petrella@gmail.com designates 209.85.217.178 as permitted sender)
Received: from [209.85.217.178] (HELO mail-lb0-f178.google.com) (209.85.217.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:11:47 +0000
Received: by mail-lb0-f178.google.com with SMTP id 10so3734617lbg.23
        for <dev@spark.incubator.apache.org>; Tue, 15 Jul 2014 10:11:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=l330wMO1wpvLPYP8ar/N/lbNH24+QEb1CR4SMtcq7sk=;
        b=gcuFC+/2wntSyrZFQHZuBDuCEBzdfuuHvU3ERP/V5OBgeLpm1NtR836rlBdIP9Y5xw
         lnKhz2+lWzlKXZ2LX8GwWvQ65sTwp8NVIb7dA0kFtpC7JmtZDGd/vxV+4WyOhigq5yo2
         0BfCCZQqYqztZtzCfgdsQCfZ848hJNQmsK7ncAB/JuIJcZ6801IM93iPRtylKU4wlRTC
         U8ay57IwGMskxac3yK46FqSUrxC1TfzejL8EEoxvDqnXIHDxOAERh/NttflVkHgP+AjF
         nFBiQZjchO1DZ/zAUUGKM9nOUhv5bd9qlwaWhnSQYt9u53ra3nas6FjHNmGwSoNaUc3L
         XX3g==
X-Received: by 10.152.4.129 with SMTP id k1mr8693798lak.11.1405444282800; Tue,
 15 Jul 2014 10:11:22 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.162.5 with HTTP; Tue, 15 Jul 2014 10:11:01 -0700 (PDT)
From: andy petrella <andy.petrella@gmail.com>
Date: Tue, 15 Jul 2014 19:11:01 +0200
Message-ID: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
Subject: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e013d100a3a603c04fe3e7f3d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d100a3a603c04fe3e7f3d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Dear Sparkers,

*[sorry for the lengthy email... =3D> head to the gist
<https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a preview
:-p**]*

I would like to share some thinking I had due to a use case I faced.
Basically, as the subject announced it, it's a generalization of the
DStream currently available in the streaming project.
First of all, I'd like to say that it's only a result of some personal
thinking, alone in the dark with a use case, the spark code, a sheet of
paper and a poor pen.


DStream is a very great concept to deal with micro-batching use cases, and
it does it very well too!
Also, it hardly relies on the elapsing time to create its internal
micro-batches.
However, there are similar use cases where we need micro-batches where this
constraint on the time doesn't hold, here are two of them:
* a micro-batch has to be created every *n* events received
* a micro-batch has to be generate based on the values of the items pushed
by the source (which might even not be a stream!).

An example of use case (mine ^^) would be
* the creation of timeseries from a cold source containing timestamped
events (like S3).
* one these timeseries have cells being the mean (sum, count, ...) of one
of the fields of the event
* the mean has to be computed over a window depending on a field *timestamp=
*.

* a timeserie is created for each type of event (the number of types is
high)
So, in this case, it'd be interesting to have an RDD for each cell, which
will generate all cells for all neede timeseries.
It's more or less what DStream does, but here it won't help due what was
stated above.

That's how I came to a raw sketch of what could be named ContinuousRDD
(CRDD) which is basically and RDD[RDD[_]]. And, for the sake of simplicity
I've stuck with the definition of a DStream to think about it. Okay, let's
go ^^.


Looking at the DStream contract, here is something that could be drafted
around CRDD.
A *CRDD* would be a generalized concept that relies on:
* a reference space/continuum (to which data can be bound)
* a binning function that can breaks the continuum into splits.
Since *Space* is a continuum we could define it as:
* a *SpacePoint* (the origin)
* a SpacePoint=3D>SpacePoint (the continuous function)
* a Ordering[SpacePoint]

DStream uses a *JobGenerator* along with a DStreamGraph, which are using
timer and clock to do their work, in the case of a CRDD we'll have to
define also a point generator, as a more generic but also adaptable
concept.


So far (so good?), these definition should work quite fine for *ordered* sp=
ace
for which:
* points are coming/fetched in order
* the space is fully filled (no gaps)
For these cases, the JobGenerator (f.i.) could be defined with two extra
functions:
* one is responsible to chop the batches even if the upper bound of the
batch hasn't been seen yet
* the other is responsible to handle outliers (and could wrap them into yet
another CRDD ?)


I created a gist here wrapping up the types and thus the skeleton of this
idea, you can find it here:
https://gist.github.com/andypetrella/12228eb24eea6b3e1389

WDYT?
*The answer can be: you're a fool!*
Actually, I already I am, but also I like to know why.... so some
explanations will help me :-D.

Thanks to read 'till this point.

Greetz,



 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>

--089e013d100a3a603c04fe3e7f3d--

From dev-return-8376-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 17:37:22 2014
Return-Path: <dev-return-8376-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 64BD011FAE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 17:37:22 +0000 (UTC)
Received: (qmail 61015 invoked by uid 500); 15 Jul 2014 17:37:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60954 invoked by uid 500); 15 Jul 2014 17:37:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60942 invoked by uid 99); 15 Jul 2014 17:37:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:37:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:37:17 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so6174804oag.7
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 10:36:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=vXOJV4nMOjmO+A69AWQh80QwUUvLeu9UdEPHLL1p7Qw=;
        b=OOxc2Y0J5e+cwZpW3vspMOknbW8nGwwbTnh/s2ocjwKzAIbA5/TBej9iMq7pOxQnRo
         WleqAqk8iy2zedulqet7OfzZN98bn79NZ79QkpoYDrLMiDI5pxspDrMhFPFpX4l4OZLl
         s+PIjkoYxkbry9yk5noaC/6TcWSspRxZcKyrbKsWwryS8dHNxjer0LGPLE95V4B/fzNN
         OvUEz9qzMoNtfobd73CaKOec7kN12ZbY86JW0mlGD1cWSpXvZ0VgJAMtpXFD6IQ9wgPk
         3xvyPKOI/Qugkp2HKJPZr6qF9KmjVz1/6+A6oVcsnmiV0/FjI7URmTqsuGUfJsw294WC
         LR7Q==
MIME-Version: 1.0
X-Received: by 10.60.62.197 with SMTP id a5mr7075788oes.78.1405445816568; Tue,
 15 Jul 2014 10:36:56 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 15 Jul 2014 10:36:56 -0700 (PDT)
In-Reply-To: <CAO1Ju5Kv7PcvvS1SJ4udhqz8jqZU1nGNEQcAS2QUwpGhpdgtDQ@mail.gmail.com>
References: <CAO1Ju5Kv7PcvvS1SJ4udhqz8jqZU1nGNEQcAS2QUwpGhpdgtDQ@mail.gmail.com>
Date: Tue, 15 Jul 2014 10:36:56 -0700
Message-ID: <CABPQxss0iP6kM_EywyG6f+O+UMAQVzu4XqVifu5GjLJ-HOq=xQ@mail.gmail.com>
Subject: Re: traveling next week
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Cody - did you mean to send this to the spark dev list?

On Tue, Jul 15, 2014 at 7:15 AM, Cody Koeninger
<cody.koeninger@mediacrossing.com> wrote:
> I'm going to be on a plane wed 23, return flight monday 28, so will miss
> daily call those days.  I'll be pushing forward on projects as I can, but
> skype availability may be limited, so email if you need something from me.

From dev-return-8377-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 17:41:15 2014
Return-Path: <dev-return-8377-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98DFF11FDA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 17:41:15 +0000 (UTC)
Received: (qmail 71074 invoked by uid 500); 15 Jul 2014 17:41:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 71016 invoked by uid 500); 15 Jul 2014 17:41:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 71004 invoked by uid 99); 15 Jul 2014 17:41:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:41:14 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.214.181] (HELO mail-ob0-f181.google.com) (209.85.214.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:41:10 +0000
Received: by mail-ob0-f181.google.com with SMTP id va2so4257296obc.12
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 10:40:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=OCTiJ/Ad5edriZfbMr5EntSlEy3D6JDDzfnjsQLkoWI=;
        b=AmNsLTyLXlUuE+Wdte1DJT/9+RM05MBn94xkFnDdM+2lfPOqyvAA1d6FHtPmjNpfGM
         idDqTbEFQ93xscKDwzA4XZEuOhvYY0dHeCrpWQNAOvj2W3CbLiJFUyFPGil2d1lub3tf
         U7XOMsh7kHdzVWtK+ay++EMYD42j5QGNhTtsOhbkfu//5NdoQ83pVH5rRSP9xLkw/VVV
         zu4MXVEbqa2s/+EzeiPvipL5hp10gIFHfETGNv3fwksLoPn3Y5+x0htaU+GqbnvZQmYg
         bTzXbX9SfMAGp2SZO+pU0NAshDodsI7BSu1E3AjePUSyY+mZZlFw1GRDRqtl7rzm/roP
         LhgA==
X-Gm-Message-State: ALoCoQl1FGVUffTRanE7D0Zl9hEvIDGdzekFQa/kW5z5DJYo2A+WcT0mSnBUEoGHkih1pGnrrpIP
MIME-Version: 1.0
X-Received: by 10.60.155.231 with SMTP id vz7mr28479890oeb.56.1405446045744;
 Tue, 15 Jul 2014 10:40:45 -0700 (PDT)
Received: by 10.76.132.2 with HTTP; Tue, 15 Jul 2014 10:40:45 -0700 (PDT)
In-Reply-To: <CABPQxss0iP6kM_EywyG6f+O+UMAQVzu4XqVifu5GjLJ-HOq=xQ@mail.gmail.com>
References: <CAO1Ju5Kv7PcvvS1SJ4udhqz8jqZU1nGNEQcAS2QUwpGhpdgtDQ@mail.gmail.com>
	<CABPQxss0iP6kM_EywyG6f+O+UMAQVzu4XqVifu5GjLJ-HOq=xQ@mail.gmail.com>
Date: Tue, 15 Jul 2014 12:40:45 -0500
Message-ID: <CAKWX9VUk-S3KccW2yQZUaM1HQiCB3GA3ZX2idssDREwn8kAWMA@mail.gmail.com>
Subject: Re: traveling next week
From: Cody Koeninger <cody@koeninger.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd7668e4ed3c204fe3ee8b3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd7668e4ed3c204fe3ee8b3
Content-Type: text/plain; charset=UTF-8

No, sorry for the mixup, it was a "helpful" autocomplete similarity between
an internal work list and the spark dev list
:(

Switched my spark mailing list subscription back to my personal email so
you guys won't be subjected to further unwanted email.


On Tue, Jul 15, 2014 at 12:36 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Cody - did you mean to send this to the spark dev list?
>
> On Tue, Jul 15, 2014 at 7:15 AM, Cody Koeninger
> <cody.koeninger@mediacrossing.com> wrote:
> > I'm going to be on a plane wed 23, return flight monday 28, so will miss
> > daily call those days.  I'll be pushing forward on projects as I can, but
> > skype availability may be limited, so email if you need something from
> me.
>

--047d7bd7668e4ed3c204fe3ee8b3--

From dev-return-8378-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 17:44:43 2014
Return-Path: <dev-return-8378-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 51D5011FFD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 17:44:43 +0000 (UTC)
Received: (qmail 81448 invoked by uid 500); 15 Jul 2014 17:44:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81386 invoked by uid 500); 15 Jul 2014 17:44:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81375 invoked by uid 99); 15 Jul 2014 17:44:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:44:42 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.214.182] (HELO mail-ob0-f182.google.com) (209.85.214.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 17:44:39 +0000
Received: by mail-ob0-f182.google.com with SMTP id wm4so6098190obc.13
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 10:44:12 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=DfWWa+AE3zLtJSFqguHn2J4BKnUfdHHK+pleXkNGRPI=;
        b=aAohHWjCdfF9CzoV70zO90l122T3nme8lalDYp2njE/RGUL98RfnMhUfimNC4xjuFR
         bYMW+PT8SeU7njScqehJiNl/DPC6OibXg4+qOnUsCv0+B6hDLM/OOzKVVxLLgD0AtgL6
         alz3L/ubB3bRYyNxppIHIznM11bu+0QyfPW8RvUSrXJMpkUf3fbVuLn+rzeHGGNRGoYn
         GvRhzUHWVwiBpzzRiqYsM4+RRDeijRWXJrcwDaSU1kOSP0lXzXdYkXjxwH8gtAHP2p1U
         odAoUVeEOjvqM/Eh8tF8eZvzL2vLpao8E4iAcr/lpd1z4p+EXy1pD7sJaHIm8fKhoGgU
         HsTQ==
X-Gm-Message-State: ALoCoQlssuHJS8egJwbajjuxDO5etH6xepsOo9vjXZD2PgEvX1XNFgMyAP/wRqxTn6ju6i7qHR8k
MIME-Version: 1.0
X-Received: by 10.60.174.3 with SMTP id bo3mr27544636oec.31.1405446251797;
 Tue, 15 Jul 2014 10:44:11 -0700 (PDT)
Received: by 10.76.132.2 with HTTP; Tue, 15 Jul 2014 10:44:11 -0700 (PDT)
Date: Tue, 15 Jul 2014 12:44:11 -0500
Message-ID: <CAKWX9VVB8XR1WTNrUUwNYr2a_8mu8TDhF+0+eFB1xU6qWcHeLw@mail.gmail.com>
Subject: Re: Reproducible deadlock in 1.0.1, possibly related to Spark-1097
From: Cody Koeninger <cody@koeninger.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011844ce97225e04fe3ef488
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011844ce97225e04fe3ef488
Content-Type: text/plain; charset=UTF-8

We tested that patch from aarondav's branch, and are no longer seeing that
deadlock.  Seems to have solved the problem, at least for us.


On Mon, Jul 14, 2014 at 7:22 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Andrew and Gary,
>
> Would you guys be able to test
> https://github.com/apache/spark/pull/1409/files and see if it solves
> your problem?
>
> - Patrick
>

--089e011844ce97225e04fe3ef488--

From dev-return-8379-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 18:44:22 2014
Return-Path: <dev-return-8379-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BA1A511480
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 18:44:22 +0000 (UTC)
Received: (qmail 9330 invoked by uid 500); 15 Jul 2014 18:44:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9278 invoked by uid 500); 15 Jul 2014 18:44:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9198 invoked by uid 99); 15 Jul 2014 18:44:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 18:44:21 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 18:44:17 +0000
Received: by mail-wi0-f182.google.com with SMTP id d1so58548wiv.9
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 11:43:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=4sBHap8rPAIaXT5hjLMXi80QLADpy6Qrc+VGJIKxrvQ=;
        b=fyqnIi12bXelY0gC70/1liGMTUTEGghNsiWPKCMw/KXZRUo+j8t9UvHRgc7K0uCk/8
         yO+W4HxgU8FLHrh2PhBk2PEj9CW7VZ1HIErB9SwqWFLe7oS560d01mwcER/e3rbIEIa5
         1Lh7H0AcKXRxe1RTAj5nmf4M2jcgYOXrxoCY9cs+77APU47ECV30LlYc6Nl1VPSMeFWI
         c8ieRMkaZ+XV5xjL/HP1m0vQmUDBecyyDXFQfnKUW1KTMp+b8qHLTYBHh2EMBbNMD+3i
         JYsFECbm3GirnLqUoUoHzaCQ6o6KtWj0++FrjY9ulT6YyUtnh9zMktOYVeUjNdjf6JU2
         Pc4w==
MIME-Version: 1.0
X-Received: by 10.194.84.101 with SMTP id x5mr29748544wjy.52.1405449836317;
 Tue, 15 Jul 2014 11:43:56 -0700 (PDT)
Received: by 10.194.24.102 with HTTP; Tue, 15 Jul 2014 11:43:56 -0700 (PDT)
In-Reply-To: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
References: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
Date: Tue, 15 Jul 2014 11:43:56 -0700
Message-ID: <CA+FETE+MMOzGTU5brjLJt8wKvAMuzq2UVnAZCT=+9sdPaZyndg@mail.gmail.com>
Subject: Re: Hadoop's Configuration object isn't threadsafe
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bb04dce3e566404fe3fca3b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb04dce3e566404fe3fca3b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Good catch Andrew. In addition to your proposed solution, is that possible
to fix Configuration class and make it thread-safe ? I think the fix should
be trivial, just use a ConcurrentHashMap, but I am not sure if we can push
this change upstream (will hadoop guys accept this change ? for them, it
seems they never expect Configuration object being accessed by multiple
threads).

-Shengzhe


On Mon, Jul 14, 2014 at 10:22 PM, Andrew Ash <andrew@andrewash.com> wrote:

> Hi Spark devs,
>
> We discovered a very interesting bug in Spark at work last week in Spark
> 0.9.1 =E2=80=94 that the way Spark uses the Hadoop Configuration object i=
s prone to
> thread safety issues.  I believe it still applies in Spark 1.0.1 as well.
>  Let me explain:
>
>
> *Observations*
>
>    - Was running a relatively simple job (read from Avro files, do a map,
>    do another map, write back to Avro files)
>    - 412 of 413 tasks completed, but the last task was hung in RUNNING
>    state
>    - The 412 successful tasks completed in median time 3.4s
>    - The last hung task didn't finish even in 20 hours
>    - The executor with the hung task was responsible for 100% of one core
>    of CPU usage
>    - Jstack of the executor attached (relevant thread pasted below)
>
>
> *Diagnosis*
>
> After doing some code spelunking, we determined the issue was concurrent
> use of a Configuration object for each task on an executor.  In Hadoop ea=
ch
> task runs in its own JVM, but in Spark multiple tasks can run in the same
> JVM, so the single-threaded access assumptions of the Configuration objec=
t
> no longer hold in Spark.
>
> The specific issue is that the AvroRecordReader actually _modifies_ the
> JobConf it's given when it's instantiated!  It adds a key for the RPC
> protocol engine in the process of connecting to the Hadoop FileSystem.
>  When many tasks start at the same time (like at the start of a job), man=
y
> tasks are adding this configuration item to the one Configuration object =
at
> once.  Internally Configuration uses a java.lang.HashMap, which isn't
> threadsafe=E2=80=A6 The below post is an excellent explanation of what ha=
ppens in
> the situation where multiple threads insert into a HashMap at the same ti=
me.
>
> http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html
>
> The gist is that you have a thread following a cycle of linked list nodes
> indefinitely.  This exactly matches our observations of the 100% CPU core
> and also the final location in the stack trace.
>
> So it seems the way Spark shares a Configuration object between task
> threads in an executor is incorrect.  We need some way to prevent
> concurrent access to a single Configuration object.
>
>
> *Proposed fix*
>
> We can clone the JobConf object in HadoopRDD.getJobConf() so each task
> gets its own JobConf object (and thus Configuration object).  The
> optimization of broadcasting the Configuration object across the cluster
> can remain, but on the other side I think it needs to be cloned for each
> task to allow for concurrent access.  I'm not sure the performance
> implications, but the comments suggest that the Configuration object is
> ~10KB so I would expect a clone on the object to be relatively speedy.
>
> Has this been observed before?  Does my suggested fix make sense?  I'd be
> happy to file a Jira ticket and continue discussion there for the right w=
ay
> to fix.
>
>
> Thanks!
> Andrew
>
>
> P.S.  For others seeing this issue, our temporary workaround is to enable
> spark.speculation, which retries failed (or hung) tasks on other machines=
.
>
>
>
> "Executor task launch worker-6" daemon prio=3D10 tid=3D0x00007f91f01fe000
> nid=3D0x54b1 runnable [0x00007f92d74f1000]
>    java.lang.Thread.State: RUNNABLE
>     at java.util.HashMap.transfer(HashMap.java:601)
>     at java.util.HashMap.resize(HashMap.java:581)
>     at java.util.HashMap.addEntry(HashMap.java:879)
>     at java.util.HashMap.put(HashMap.java:505)
>     at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
>     at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
>     at
> org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
>     at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
>     at
> org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(Na=
meNodeProxies.java:343)
>     at
> org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.j=
ava:168)
>     at
> org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:1=
29)
>     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
>     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
>     at
> org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSy=
stem.java:125)
>     at
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
>     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
>     at
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
>     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
>     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
>     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
>     at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
>     at
> org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
>     at
> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.ja=
va:52)
>     at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
>     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
>     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>     at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109=
)
>     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>     at
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp=
(Executor.scala:211)
>     at
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala=
:42)
>     at
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala=
:41)
>     at java.security.AccessController.doPrivileged(Native Method)
>     at javax.security.auth.Subject.doAs(Subject.java:415)
>     at
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation=
.java:1408)
>     at
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:4=
1)
>     at
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>     at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java=
:1145)
>     at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.jav=
a:615)
>     at java.lang.Thread.run(Thread.java:745)
>
>

--047d7bb04dce3e566404fe3fca3b--

From dev-return-8380-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 19:57:20 2014
Return-Path: <dev-return-8380-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id ED5D811838
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 19:57:19 +0000 (UTC)
Received: (qmail 13261 invoked by uid 500); 15 Jul 2014 19:57:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13198 invoked by uid 500); 15 Jul 2014 19:57:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13187 invoked by uid 99); 15 Jul 2014 19:57:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 19:57:19 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 19:57:15 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so1518706vcb.27
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 12:56:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=CI6GWa3a2MyjzpPGsE4tksHQ4rqnbrRF4q0OVpYq0L4=;
        b=gUtbTadoNgZgoVTyhmaK5lALz71OY1DdwMnRU3Nodn4DPL1aQqliMAChQU6FKKub1D
         2Dss5vu4PtsKR0YD1AexdaSnVW7f2mq8Xi667qPAv9krmq5R7v6f8T2ds3uALHqyPp+d
         5zb21+NqKmYp9NIKCj0RThDzIULf4F8laphZNZoQJuBSfRi9tX/1O/iSuApe4PoZdLkB
         dOYGY8SF3TIuVds++OVQ+xoWrQjDsjjyiSLBq1KKF1Nw3nc076Ge5x+/CI3np3rHAVzY
         1/ibIVB3BQOFsM9ly/ZZ8BlYLTxp3oR45o/PlsAyc6n7TV5kOVyerzAPWBZKnku94YG2
         KfOQ==
X-Gm-Message-State: ALoCoQl/WHWEuVNw9yhYLcavw/ue3KLwOYtvW/NpSQSKMK/YjfZw6QMouU1Y3Do6hID20QUgbqAH
X-Received: by 10.221.64.80 with SMTP id xh16mr3085434vcb.35.1405454210566;
        Tue, 15 Jul 2014 12:56:50 -0700 (PDT)
Received: from mail-vc0-f171.google.com (mail-vc0-f171.google.com [209.85.220.171])
        by mx.google.com with ESMTPSA id r8sm7519659vet.7.2014.07.15.12.56.49
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 15 Jul 2014 12:56:49 -0700 (PDT)
Received: by mail-vc0-f171.google.com with SMTP id id10so11317734vcb.16
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 12:56:49 -0700 (PDT)
X-Received: by 10.52.120.83 with SMTP id la19mr2483159vdb.68.1405454209059;
 Tue, 15 Jul 2014 12:56:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Tue, 15 Jul 2014 12:56:29 -0700 (PDT)
In-Reply-To: <CA+FETE+MMOzGTU5brjLJt8wKvAMuzq2UVnAZCT=+9sdPaZyndg@mail.gmail.com>
References: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
 <CA+FETE+MMOzGTU5brjLJt8wKvAMuzq2UVnAZCT=+9sdPaZyndg@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Tue, 15 Jul 2014 15:56:29 -0400
Message-ID: <CA+-p3AEKYARqtBgj+aKYWN7gx8Fxm37nvpEx=goRqh2QcQJHxw@mail.gmail.com>
Subject: Re: Hadoop's Configuration object isn't threadsafe
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0122f12ee1121704fe40cef6
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122f12ee1121704fe40cef6
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Shengzhe,

Even if we did make Configuration threadsafe, it'd take quite some time for
that to trickle down to a Hadoop release that we could actually rely on
Spark users having installed.  I agree we should consider whether making
Configuration threadsafe is something that Hadoop should do, but for the
short term I think Spark needs to be able to handle the common scenario of
Configuration being single-threaded.

Thanks!
Andrew


On Tue, Jul 15, 2014 at 2:43 PM, yao <yaoshengzhe@gmail.com> wrote:

> Good catch Andrew. In addition to your proposed solution, is that possibl=
e
> to fix Configuration class and make it thread-safe ? I think the fix shou=
ld
> be trivial, just use a ConcurrentHashMap, but I am not sure if we can pus=
h
> this change upstream (will hadoop guys accept this change ? for them, it
> seems they never expect Configuration object being accessed by multiple
> threads).
>
> -Shengzhe
>
>
> On Mon, Jul 14, 2014 at 10:22 PM, Andrew Ash <andrew@andrewash.com> wrote=
:
>
> > Hi Spark devs,
> >
> > We discovered a very interesting bug in Spark at work last week in Spar=
k
> > 0.9.1 =E2=80=94 that the way Spark uses the Hadoop Configuration object=
 is prone
> to
> > thread safety issues.  I believe it still applies in Spark 1.0.1 as wel=
l.
> >  Let me explain:
> >
> >
> > *Observations*
> >
> >    - Was running a relatively simple job (read from Avro files, do a ma=
p,
> >    do another map, write back to Avro files)
> >    - 412 of 413 tasks completed, but the last task was hung in RUNNING
> >    state
> >    - The 412 successful tasks completed in median time 3.4s
> >    - The last hung task didn't finish even in 20 hours
> >    - The executor with the hung task was responsible for 100% of one co=
re
> >    of CPU usage
> >    - Jstack of the executor attached (relevant thread pasted below)
> >
> >
> > *Diagnosis*
> >
> > After doing some code spelunking, we determined the issue was concurren=
t
> > use of a Configuration object for each task on an executor.  In Hadoop
> each
> > task runs in its own JVM, but in Spark multiple tasks can run in the sa=
me
> > JVM, so the single-threaded access assumptions of the Configuration
> object
> > no longer hold in Spark.
> >
> > The specific issue is that the AvroRecordReader actually _modifies_ the
> > JobConf it's given when it's instantiated!  It adds a key for the RPC
> > protocol engine in the process of connecting to the Hadoop FileSystem.
> >  When many tasks start at the same time (like at the start of a job),
> many
> > tasks are adding this configuration item to the one Configuration objec=
t
> at
> > once.  Internally Configuration uses a java.lang.HashMap, which isn't
> > threadsafe=E2=80=A6 The below post is an excellent explanation of what =
happens in
> > the situation where multiple threads insert into a HashMap at the same
> time.
> >
> > http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html
> >
> > The gist is that you have a thread following a cycle of linked list nod=
es
> > indefinitely.  This exactly matches our observations of the 100% CPU co=
re
> > and also the final location in the stack trace.
> >
> > So it seems the way Spark shares a Configuration object between task
> > threads in an executor is incorrect.  We need some way to prevent
> > concurrent access to a single Configuration object.
> >
> >
> > *Proposed fix*
> >
> > We can clone the JobConf object in HadoopRDD.getJobConf() so each task
> > gets its own JobConf object (and thus Configuration object).  The
> > optimization of broadcasting the Configuration object across the cluste=
r
> > can remain, but on the other side I think it needs to be cloned for eac=
h
> > task to allow for concurrent access.  I'm not sure the performance
> > implications, but the comments suggest that the Configuration object is
> > ~10KB so I would expect a clone on the object to be relatively speedy.
> >
> > Has this been observed before?  Does my suggested fix make sense?  I'd =
be
> > happy to file a Jira ticket and continue discussion there for the right
> way
> > to fix.
> >
> >
> > Thanks!
> > Andrew
> >
> >
> > P.S.  For others seeing this issue, our temporary workaround is to enab=
le
> > spark.speculation, which retries failed (or hung) tasks on other
> machines.
> >
> >
> >
> > "Executor task launch worker-6" daemon prio=3D10 tid=3D0x00007f91f01fe0=
00
> > nid=3D0x54b1 runnable [0x00007f92d74f1000]
> >    java.lang.Thread.State: RUNNABLE
> >     at java.util.HashMap.transfer(HashMap.java:601)
> >     at java.util.HashMap.resize(HashMap.java:581)
> >     at java.util.HashMap.addEntry(HashMap.java:879)
> >     at java.util.HashMap.put(HashMap.java:505)
> >     at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
> >     at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
> >     at
> > org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
> >     at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
> >     at
> >
> org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(Na=
meNodeProxies.java:343)
> >     at
> >
> org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.j=
ava:168)
> >     at
> >
> org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:1=
29)
> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
> >     at
> >
> org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSy=
stem.java:125)
> >     at
> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
> >     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
> >     at
> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
> >     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
> >     at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
> >     at
> > org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43=
)
> >     at
> >
> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.ja=
va:52)
> >     at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:15=
6)
> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >     at
> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
> >     at
> >
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp=
(Executor.scala:211)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala=
:42)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala=
:41)
> >     at java.security.AccessController.doPrivileged(Native Method)
> >     at javax.security.auth.Subject.doAs(Subject.java:415)
> >     at
> >
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation=
.java:1408)
> >     at
> >
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:4=
1)
> >     at
> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >     at
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java=
:1145)
> >     at
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.jav=
a:615)
> >     at java.lang.Thread.run(Thread.java:745)
> >
> >
>

--089e0122f12ee1121704fe40cef6--

From dev-return-8381-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 21:20:46 2014
Return-Path: <dev-return-8381-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D727011B7B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 21:20:46 +0000 (UTC)
Received: (qmail 63631 invoked by uid 500); 15 Jul 2014 21:20:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63579 invoked by uid 500); 15 Jul 2014 21:20:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63562 invoked by uid 99); 15 Jul 2014 21:20:45 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 21:20:45 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 21:20:42 +0000
Received: by mail-oa0-f45.google.com with SMTP id i7so5606074oag.18
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 14:20:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Dxp+8jGfbvFs/uocADhpNmtrXcxk37cg0t6nX+4C+QQ=;
        b=klI/mII+ysqU1V/BLJUG6WoMgFa0FpjqkNozcKqvu2MGvNCEmOXvO/02ZfF7lBOYK0
         7C1oz0Qhol2wSns3dyvMK9I6c6QH3W2trjx5MhiG79IseKkY3PL4pUwRVbcEWSoqpBPN
         AKt+07F3P1rSUHJ8qUseb7oAOC0x2OnS1ILJV2OvuQSXBwFVy0pfd9iJ2zlHWYgwukIg
         TYTEoEO+nSxW1f9La8yoh9nkEssk+RNvbAL92O9tLGXn/wzgvaXYPSlMw3BlFGy6Tz00
         Rw2hSCSgEoDAzoJJ/pd4yAY4Ro4OWCE1opFP74MeYp8JrPokLo8a4OiHdJOjEnzpzyz1
         kDMA==
MIME-Version: 1.0
X-Received: by 10.60.62.197 with SMTP id a5mr8707870oes.78.1405459217436; Tue,
 15 Jul 2014 14:20:17 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 15 Jul 2014 14:20:17 -0700 (PDT)
In-Reply-To: <CA+-p3AEKYARqtBgj+aKYWN7gx8Fxm37nvpEx=goRqh2QcQJHxw@mail.gmail.com>
References: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
	<CA+FETE+MMOzGTU5brjLJt8wKvAMuzq2UVnAZCT=+9sdPaZyndg@mail.gmail.com>
	<CA+-p3AEKYARqtBgj+aKYWN7gx8Fxm37nvpEx=goRqh2QcQJHxw@mail.gmail.com>
Date: Tue, 15 Jul 2014 14:20:17 -0700
Message-ID: <CABPQxsuDbTA95hjNW9b+AwuivA3GxcW5KLheHfdJNcs2UqHZMA@mail.gmail.com>
Subject: Re: Hadoop's Configuration object isn't threadsafe
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Andrew,

Cloning the conf this might be a good/simple fix for this particular
problem. It's definitely worth looking into.

There are a few things we can probably do in Spark to deal with
non-thread-safety inside of the Hadoop FileSystem and Configuration
classes. One thing we can do in general is to add barriers around the
locations where we knowingly access Hadoop FileSystem and
Configuration state from multiple threads (e.g. during our own calls
to getRecordReader in this case). But this will only deal with "writer
writer" conflicts where we had multiple calls mutating the same object
at the same time. It won't deal with "reader writer" conflicts where
some of our initialization code touches state that is needed during
normal execution of other tasks.

- Patrick

On Tue, Jul 15, 2014 at 12:56 PM, Andrew Ash <andrew@andrewash.com> wrote:
> Hi Shengzhe,
>
> Even if we did make Configuration threadsafe, it'd take quite some time for
> that to trickle down to a Hadoop release that we could actually rely on
> Spark users having installed.  I agree we should consider whether making
> Configuration threadsafe is something that Hadoop should do, but for the
> short term I think Spark needs to be able to handle the common scenario of
> Configuration being single-threaded.
>
> Thanks!
> Andrew
>
>
> On Tue, Jul 15, 2014 at 2:43 PM, yao <yaoshengzhe@gmail.com> wrote:
>
>> Good catch Andrew. In addition to your proposed solution, is that possible
>> to fix Configuration class and make it thread-safe ? I think the fix should
>> be trivial, just use a ConcurrentHashMap, but I am not sure if we can push
>> this change upstream (will hadoop guys accept this change ? for them, it
>> seems they never expect Configuration object being accessed by multiple
>> threads).
>>
>> -Shengzhe
>>
>>
>> On Mon, Jul 14, 2014 at 10:22 PM, Andrew Ash <andrew@andrewash.com> wrote:
>>
>> > Hi Spark devs,
>> >
>> > We discovered a very interesting bug in Spark at work last week in Spark
>> > 0.9.1 -- that the way Spark uses the Hadoop Configuration object is prone
>> to
>> > thread safety issues.  I believe it still applies in Spark 1.0.1 as well.
>> >  Let me explain:
>> >
>> >
>> > *Observations*
>> >
>> >    - Was running a relatively simple job (read from Avro files, do a map,
>> >    do another map, write back to Avro files)
>> >    - 412 of 413 tasks completed, but the last task was hung in RUNNING
>> >    state
>> >    - The 412 successful tasks completed in median time 3.4s
>> >    - The last hung task didn't finish even in 20 hours
>> >    - The executor with the hung task was responsible for 100% of one core
>> >    of CPU usage
>> >    - Jstack of the executor attached (relevant thread pasted below)
>> >
>> >
>> > *Diagnosis*
>> >
>> > After doing some code spelunking, we determined the issue was concurrent
>> > use of a Configuration object for each task on an executor.  In Hadoop
>> each
>> > task runs in its own JVM, but in Spark multiple tasks can run in the same
>> > JVM, so the single-threaded access assumptions of the Configuration
>> object
>> > no longer hold in Spark.
>> >
>> > The specific issue is that the AvroRecordReader actually _modifies_ the
>> > JobConf it's given when it's instantiated!  It adds a key for the RPC
>> > protocol engine in the process of connecting to the Hadoop FileSystem.
>> >  When many tasks start at the same time (like at the start of a job),
>> many
>> > tasks are adding this configuration item to the one Configuration object
>> at
>> > once.  Internally Configuration uses a java.lang.HashMap, which isn't
>> > threadsafe... The below post is an excellent explanation of what happens in
>> > the situation where multiple threads insert into a HashMap at the same
>> time.
>> >
>> > http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html
>> >
>> > The gist is that you have a thread following a cycle of linked list nodes
>> > indefinitely.  This exactly matches our observations of the 100% CPU core
>> > and also the final location in the stack trace.
>> >
>> > So it seems the way Spark shares a Configuration object between task
>> > threads in an executor is incorrect.  We need some way to prevent
>> > concurrent access to a single Configuration object.
>> >
>> >
>> > *Proposed fix*
>> >
>> > We can clone the JobConf object in HadoopRDD.getJobConf() so each task
>> > gets its own JobConf object (and thus Configuration object).  The
>> > optimization of broadcasting the Configuration object across the cluster
>> > can remain, but on the other side I think it needs to be cloned for each
>> > task to allow for concurrent access.  I'm not sure the performance
>> > implications, but the comments suggest that the Configuration object is
>> > ~10KB so I would expect a clone on the object to be relatively speedy.
>> >
>> > Has this been observed before?  Does my suggested fix make sense?  I'd be
>> > happy to file a Jira ticket and continue discussion there for the right
>> way
>> > to fix.
>> >
>> >
>> > Thanks!
>> > Andrew
>> >
>> >
>> > P.S.  For others seeing this issue, our temporary workaround is to enable
>> > spark.speculation, which retries failed (or hung) tasks on other
>> machines.
>> >
>> >
>> >
>> > "Executor task launch worker-6" daemon prio=10 tid=0x00007f91f01fe000
>> > nid=0x54b1 runnable [0x00007f92d74f1000]
>> >    java.lang.Thread.State: RUNNABLE
>> >     at java.util.HashMap.transfer(HashMap.java:601)
>> >     at java.util.HashMap.resize(HashMap.java:581)
>> >     at java.util.HashMap.addEntry(HashMap.java:879)
>> >     at java.util.HashMap.put(HashMap.java:505)
>> >     at org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
>> >     at org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
>> >     at
>> > org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
>> >     at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
>> >     at
>> >
>> org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
>> >     at
>> >
>> org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
>> >     at
>> >
>> org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
>> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
>> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
>> >     at
>> >
>> org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
>> >     at
>> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
>> >     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
>> >     at
>> > org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
>> >     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
>> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
>> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
>> >     at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
>> >     at
>> > org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
>> >     at
>> >
>> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
>> >     at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
>> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
>> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >     at
>> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>> >     at
>> >
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
>> >     at java.security.AccessController.doPrivileged(Native Method)
>> >     at javax.security.auth.Subject.doAs(Subject.java:415)
>> >     at
>> >
>> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
>> >     at
>> >
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
>> >     at
>> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>> >     at
>> >
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> >     at
>> >
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> >     at java.lang.Thread.run(Thread.java:745)
>> >
>> >
>>

From dev-return-8382-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 22:34:18 2014
Return-Path: <dev-return-8382-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 03AE211F06
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 22:34:18 +0000 (UTC)
Received: (qmail 8043 invoked by uid 500); 15 Jul 2014 22:34:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7985 invoked by uid 500); 15 Jul 2014 22:34:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7958 invoked by uid 99); 15 Jul 2014 22:34:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 22:34:16 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.171 as permitted sender)
Received: from [209.85.220.171] (HELO mail-vc0-f171.google.com) (209.85.220.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 22:34:14 +0000
Received: by mail-vc0-f171.google.com with SMTP id id10so136617vcb.16
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 15:33:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=uxT3EqeMxguarFeLnL1x7kwKjJVfAOjNEMkkLfXmHjc=;
        b=UT9/yxHwWbPCpCx69+jjW9FA32+x2MP6oIG7icGMwGbfQNR+JmqhST/FuS42PEThPF
         AO7lU6BHYm4XrAwb6aU3+Bn3tXkwXDomTANgfhw19w3iU16PB6iE74wMkJxA2HM4LobM
         lLMukeHFsyoN6ZVONqS2Cqea2NcARLVVSTi325PMBj0xSTqsAAlK3taX4OlIbucGflTq
         /M+sTl0cfxqIkEHTxAPQZSfv1KvBu/7ZE7uJRKpRxkbyAwYXWC6mn1zASnBo95a/LuO0
         5hI8/opiTZ9pB1rlmEwLJ4STq+Eys19U0YK7ANGn9/T4V/QjgAKpr4Fyz4UMryxQTNr+
         jRrQ==
X-Received: by 10.52.135.7 with SMTP id po7mr3072786vdb.50.1405463629923; Tue,
 15 Jul 2014 15:33:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.131.39 with HTTP; Tue, 15 Jul 2014 15:33:19 -0700 (PDT)
In-Reply-To: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Tue, 15 Jul 2014 15:33:19 -0700
Message-ID: <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=bcaec52c637d680bb104fe4300d0
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec52c637d680bb104fe4300d0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Very interesting ideas Andy!

Conceptually i think it makes sense. In fact, it is true that dealing with
time series data, windowing over application time, windowing over number of
events, are things that DStream does not natively support. The real
challenge is actually mapping the conceptual windows with the underlying
RDD model. On aspect you correctly observed in the ordering of events
within the RDDs of the DStream. Another fundamental aspect is the fact that
RDDs as parallel collections, with no well-defined ordering in the records
in the RDDs. If you want to process the records in an RDD as a ordered
stream of events, you kind of have to process the stream sequentially,
which means you have to process each RDD partition one-by-one, and
therefore lose the parallelism. So implementing all these functionality may
mean adding functionality at the cost of performance. Whether that is okay
for Spark Streaming to have these OR this tradeoff is not-intuitive for
end-users and therefore should not come out-of-the-box with Spark Streaming
-- that is a definitely a question worth debating upon.

That said, for some limited usecases, like windowing over N events, can be
implemented using custom RDDs like SlidingRDD
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apach=
e/spark/mllib/rdd/SlidingRDD.scala>
without
losing parallelism. For things like app time based windows, and
random-application-event based windows, its much harder.

Interesting ideas nonetheless. I am curious to see how far we can push
using the RDD model underneath, without losing parallelism and performance.

TD



On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Dear Sparkers,
>
> *[sorry for the lengthy email... =3D> head to the gist
> <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a preview
> :-p**]*
>
> I would like to share some thinking I had due to a use case I faced.
> Basically, as the subject announced it, it's a generalization of the
> DStream currently available in the streaming project.
> First of all, I'd like to say that it's only a result of some personal
> thinking, alone in the dark with a use case, the spark code, a sheet of
> paper and a poor pen.
>
>
> DStream is a very great concept to deal with micro-batching use cases, an=
d
> it does it very well too!
> Also, it hardly relies on the elapsing time to create its internal
> micro-batches.
> However, there are similar use cases where we need micro-batches where th=
is
> constraint on the time doesn't hold, here are two of them:
> * a micro-batch has to be created every *n* events received
> * a micro-batch has to be generate based on the values of the items pushe=
d
> by the source (which might even not be a stream!).
>
> An example of use case (mine ^^) would be
> * the creation of timeseries from a cold source containing timestamped
> events (like S3).
> * one these timeseries have cells being the mean (sum, count, ...) of one
> of the fields of the event
> * the mean has to be computed over a window depending on a field
> *timestamp*.
>
> * a timeserie is created for each type of event (the number of types is
> high)
> So, in this case, it'd be interesting to have an RDD for each cell, which
> will generate all cells for all neede timeseries.
> It's more or less what DStream does, but here it won't help due what was
> stated above.
>
> That's how I came to a raw sketch of what could be named ContinuousRDD
> (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of simplicit=
y
> I've stuck with the definition of a DStream to think about it. Okay, let'=
s
> go ^^.
>
>
> Looking at the DStream contract, here is something that could be drafted
> around CRDD.
> A *CRDD* would be a generalized concept that relies on:
> * a reference space/continuum (to which data can be bound)
> * a binning function that can breaks the continuum into splits.
> Since *Space* is a continuum we could define it as:
> * a *SpacePoint* (the origin)
> * a SpacePoint=3D>SpacePoint (the continuous function)
> * a Ordering[SpacePoint]
>
> DStream uses a *JobGenerator* along with a DStreamGraph, which are using
> timer and clock to do their work, in the case of a CRDD we'll have to
> define also a point generator, as a more generic but also adaptable
> concept.
>
>
> So far (so good?), these definition should work quite fine for *ordered*
> space
> for which:
> * points are coming/fetched in order
> * the space is fully filled (no gaps)
> For these cases, the JobGenerator (f.i.) could be defined with two extra
> functions:
> * one is responsible to chop the batches even if the upper bound of the
> batch hasn't been seen yet
> * the other is responsible to handle outliers (and could wrap them into y=
et
> another CRDD ?)
>
>
> I created a gist here wrapping up the types and thus the skeleton of this
> idea, you can find it here:
> https://gist.github.com/andypetrella/12228eb24eea6b3e1389
>
> WDYT?
> *The answer can be: you're a fool!*
> Actually, I already I am, but also I like to know why.... so some
> explanations will help me :-D.
>
> Thanks to read 'till this point.
>
> Greetz,
>
>
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>

--bcaec52c637d680bb104fe4300d0--

From dev-return-8383-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 15 22:34:18 2014
Return-Path: <dev-return-8383-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6DFF11F07
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 15 Jul 2014 22:34:18 +0000 (UTC)
Received: (qmail 8609 invoked by uid 500); 15 Jul 2014 22:34:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8479 invoked by uid 500); 15 Jul 2014 22:34:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7973 invoked by uid 99); 15 Jul 2014 22:34:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 22:34:17 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 15 Jul 2014 22:34:14 +0000
Received: by mail-vc0-f178.google.com with SMTP id la4so126795vcb.37
        for <dev@spark.incubator.apache.org>; Tue, 15 Jul 2014 15:33:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=uxT3EqeMxguarFeLnL1x7kwKjJVfAOjNEMkkLfXmHjc=;
        b=UT9/yxHwWbPCpCx69+jjW9FA32+x2MP6oIG7icGMwGbfQNR+JmqhST/FuS42PEThPF
         AO7lU6BHYm4XrAwb6aU3+Bn3tXkwXDomTANgfhw19w3iU16PB6iE74wMkJxA2HM4LobM
         lLMukeHFsyoN6ZVONqS2Cqea2NcARLVVSTi325PMBj0xSTqsAAlK3taX4OlIbucGflTq
         /M+sTl0cfxqIkEHTxAPQZSfv1KvBu/7ZE7uJRKpRxkbyAwYXWC6mn1zASnBo95a/LuO0
         5hI8/opiTZ9pB1rlmEwLJ4STq+Eys19U0YK7ANGn9/T4V/QjgAKpr4Fyz4UMryxQTNr+
         jRrQ==
X-Received: by 10.52.135.7 with SMTP id po7mr3072786vdb.50.1405463629923; Tue,
 15 Jul 2014 15:33:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.131.39 with HTTP; Tue, 15 Jul 2014 15:33:19 -0700 (PDT)
In-Reply-To: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Tue, 15 Jul 2014 15:33:19 -0700
Message-ID: <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=bcaec52c637d680bb104fe4300d0
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec52c637d680bb104fe4300d0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Very interesting ideas Andy!

Conceptually i think it makes sense. In fact, it is true that dealing with
time series data, windowing over application time, windowing over number of
events, are things that DStream does not natively support. The real
challenge is actually mapping the conceptual windows with the underlying
RDD model. On aspect you correctly observed in the ordering of events
within the RDDs of the DStream. Another fundamental aspect is the fact that
RDDs as parallel collections, with no well-defined ordering in the records
in the RDDs. If you want to process the records in an RDD as a ordered
stream of events, you kind of have to process the stream sequentially,
which means you have to process each RDD partition one-by-one, and
therefore lose the parallelism. So implementing all these functionality may
mean adding functionality at the cost of performance. Whether that is okay
for Spark Streaming to have these OR this tradeoff is not-intuitive for
end-users and therefore should not come out-of-the-box with Spark Streaming
-- that is a definitely a question worth debating upon.

That said, for some limited usecases, like windowing over N events, can be
implemented using custom RDDs like SlidingRDD
<https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apach=
e/spark/mllib/rdd/SlidingRDD.scala>
without
losing parallelism. For things like app time based windows, and
random-application-event based windows, its much harder.

Interesting ideas nonetheless. I am curious to see how far we can push
using the RDD model underneath, without losing parallelism and performance.

TD



On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Dear Sparkers,
>
> *[sorry for the lengthy email... =3D> head to the gist
> <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a preview
> :-p**]*
>
> I would like to share some thinking I had due to a use case I faced.
> Basically, as the subject announced it, it's a generalization of the
> DStream currently available in the streaming project.
> First of all, I'd like to say that it's only a result of some personal
> thinking, alone in the dark with a use case, the spark code, a sheet of
> paper and a poor pen.
>
>
> DStream is a very great concept to deal with micro-batching use cases, an=
d
> it does it very well too!
> Also, it hardly relies on the elapsing time to create its internal
> micro-batches.
> However, there are similar use cases where we need micro-batches where th=
is
> constraint on the time doesn't hold, here are two of them:
> * a micro-batch has to be created every *n* events received
> * a micro-batch has to be generate based on the values of the items pushe=
d
> by the source (which might even not be a stream!).
>
> An example of use case (mine ^^) would be
> * the creation of timeseries from a cold source containing timestamped
> events (like S3).
> * one these timeseries have cells being the mean (sum, count, ...) of one
> of the fields of the event
> * the mean has to be computed over a window depending on a field
> *timestamp*.
>
> * a timeserie is created for each type of event (the number of types is
> high)
> So, in this case, it'd be interesting to have an RDD for each cell, which
> will generate all cells for all neede timeseries.
> It's more or less what DStream does, but here it won't help due what was
> stated above.
>
> That's how I came to a raw sketch of what could be named ContinuousRDD
> (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of simplicit=
y
> I've stuck with the definition of a DStream to think about it. Okay, let'=
s
> go ^^.
>
>
> Looking at the DStream contract, here is something that could be drafted
> around CRDD.
> A *CRDD* would be a generalized concept that relies on:
> * a reference space/continuum (to which data can be bound)
> * a binning function that can breaks the continuum into splits.
> Since *Space* is a continuum we could define it as:
> * a *SpacePoint* (the origin)
> * a SpacePoint=3D>SpacePoint (the continuous function)
> * a Ordering[SpacePoint]
>
> DStream uses a *JobGenerator* along with a DStreamGraph, which are using
> timer and clock to do their work, in the case of a CRDD we'll have to
> define also a point generator, as a more generic but also adaptable
> concept.
>
>
> So far (so good?), these definition should work quite fine for *ordered*
> space
> for which:
> * points are coming/fetched in order
> * the space is fully filled (no gaps)
> For these cases, the JobGenerator (f.i.) could be defined with two extra
> functions:
> * one is responsible to chop the batches even if the upper bound of the
> batch hasn't been seen yet
> * the other is responsible to handle outliers (and could wrap them into y=
et
> another CRDD ?)
>
>
> I created a gist here wrapping up the types and thus the skeleton of this
> idea, you can find it here:
> https://gist.github.com/andypetrella/12228eb24eea6b3e1389
>
> WDYT?
> *The answer can be: you're a fool!*
> Actually, I already I am, but also I like to know why.... so some
> explanations will help me :-D.
>
> Thanks to read 'till this point.
>
> Greetz,
>
>
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>

--bcaec52c637d680bb104fe4300d0--

From dev-return-8384-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 02:30:30 2014
Return-Path: <dev-return-8384-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 77872115BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 02:30:30 +0000 (UTC)
Received: (qmail 67218 invoked by uid 500); 16 Jul 2014 02:30:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67161 invoked by uid 500); 16 Jul 2014 02:30:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67150 invoked by uid 99); 16 Jul 2014 02:30:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:30:29 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of pelickzhang@qq.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:30:27 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <pelickzhang@qq.com>)
	id 1X7Eyp-0004go-30
	for dev@spark.incubator.apache.org; Tue, 15 Jul 2014 19:30:03 -0700
Date: Tue, 15 Jul 2014 19:30:03 -0700 (PDT)
From: Baofeng Zhang <pelickzhang@qq.com>
To: dev@spark.incubator.apache.org
Message-ID: <1405477803082-7358.post@n3.nabble.com>
In-Reply-To: <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com>
References: <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com> <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com>
Subject: Re: Catalyst dependency on Spark Core
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Is Matei following this?

Catalyst uses the Utils to get the ClassLoader which loaded Spark.

Can Catalyst directly do "getClass.getClassLoader" to avoid the dependency
on core?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Catalyst-dependency-on-Spark-Core-tp7303p7358.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8385-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 02:32:26 2014
Return-Path: <dev-return-8385-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 353F9115C1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 02:32:26 +0000 (UTC)
Received: (qmail 68713 invoked by uid 500); 16 Jul 2014 02:32:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68652 invoked by uid 500); 16 Jul 2014 02:32:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68640 invoked by uid 99); 16 Jul 2014 02:32:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:32:25 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.169 as permitted sender)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:32:20 +0000
Received: by mail-pd0-f169.google.com with SMTP id y10so372480pdj.14
        for <dev@spark.apache.org>; Tue, 15 Jul 2014 19:32:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=bWnZSI6n6n6m9yAw3kErdm9JUqrqlbjiKOLS1C69A8U=;
        b=RLq4B9eMnQMZGxXUcCfHDbCUslbRbYVLBJCMEvqqyKC1VLs8lj1DHIyY0+NlQi/ymR
         jDSyr5AhJzUGvcb5drZ+v0yrFatDCEZk3QoZEYMVXOIqSPdUxrM6dgv1HyCw4cIncdqA
         +v7vNrj4TijoswL6m//G0X6lu93RTLoC8527xT1NNoEAD5PGEilBiVwewLY/eyV35Gky
         vTsx3j5iFSkjqHPGN5f816tE3As9QEWZlkxeQFpmdmb+8j3hHySOo2IpLFMSoHSqvb8a
         2ehEcUdr5iD78HgoD+UmRQjT4WjyrTGcj6qpCVfxUKMbYfWjS6yIWdDt58UBxnPXlFJT
         s0Dw==
X-Received: by 10.70.51.138 with SMTP id k10mr17749824pdo.23.1405477920366;
        Tue, 15 Jul 2014 19:32:00 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id kt2sm15317199pbc.83.2014.07.15.19.31.58
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 15 Jul 2014 19:31:58 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Catalyst dependency on Spark Core
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <1405477803082-7358.post@n3.nabble.com>
Date: Tue, 15 Jul 2014 19:31:56 -0700
Cc: dev@spark.incubator.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <163620AF-914B-4125-8B0E-BAA27E4CD0D4@gmail.com>
References: <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com> <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com> <1405477803082-7358.post@n3.nabble.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, that seems like something we can inline :).

On Jul 15, 2014, at 7:30 PM, Baofeng Zhang <pelickzhang@qq.com> wrote:

> Is Matei following this?
>=20
> Catalyst uses the Utils to get the ClassLoader which loaded Spark.
>=20
> Can Catalyst directly do "getClass.getClassLoader" to avoid the =
dependency
> on core?
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Catalyst-depe=
ndency-on-Spark-Core-tp7303p7358.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.


From dev-return-8386-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 02:32:28 2014
Return-Path: <dev-return-8386-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 38C68115C2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 02:32:28 +0000 (UTC)
Received: (qmail 69647 invoked by uid 500); 16 Jul 2014 02:32:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69587 invoked by uid 500); 16 Jul 2014 02:32:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 69570 invoked by uid 99); 16 Jul 2014 02:32:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:32:27 +0000
X-ASF-Spam-Status: No, hits=1.3 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.169 as permitted sender)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:32:25 +0000
Received: by mail-pd0-f169.google.com with SMTP id y10so373280pdj.28
        for <dev@spark.incubator.apache.org>; Tue, 15 Jul 2014 19:32:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=bWnZSI6n6n6m9yAw3kErdm9JUqrqlbjiKOLS1C69A8U=;
        b=RLq4B9eMnQMZGxXUcCfHDbCUslbRbYVLBJCMEvqqyKC1VLs8lj1DHIyY0+NlQi/ymR
         jDSyr5AhJzUGvcb5drZ+v0yrFatDCEZk3QoZEYMVXOIqSPdUxrM6dgv1HyCw4cIncdqA
         +v7vNrj4TijoswL6m//G0X6lu93RTLoC8527xT1NNoEAD5PGEilBiVwewLY/eyV35Gky
         vTsx3j5iFSkjqHPGN5f816tE3As9QEWZlkxeQFpmdmb+8j3hHySOo2IpLFMSoHSqvb8a
         2ehEcUdr5iD78HgoD+UmRQjT4WjyrTGcj6qpCVfxUKMbYfWjS6yIWdDt58UBxnPXlFJT
         s0Dw==
X-Received: by 10.70.51.138 with SMTP id k10mr17749824pdo.23.1405477920366;
        Tue, 15 Jul 2014 19:32:00 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id kt2sm15317199pbc.83.2014.07.15.19.31.58
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 15 Jul 2014 19:31:58 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Catalyst dependency on Spark Core
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <1405477803082-7358.post@n3.nabble.com>
Date: Tue, 15 Jul 2014 19:31:56 -0700
Cc: dev@spark.incubator.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <163620AF-914B-4125-8B0E-BAA27E4CD0D4@gmail.com>
References: <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com> <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com> <1405477803082-7358.post@n3.nabble.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, that seems like something we can inline :).

On Jul 15, 2014, at 7:30 PM, Baofeng Zhang <pelickzhang@qq.com> wrote:

> Is Matei following this?
>=20
> Catalyst uses the Utils to get the ClassLoader which loaded Spark.
>=20
> Can Catalyst directly do "getClass.getClassLoader" to avoid the =
dependency
> on core?
>=20
>=20
>=20
> --
> View this message in context: =
http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Catalyst-depe=
ndency-on-Spark-Core-tp7303p7358.html
> Sent from the Apache Spark Developers List mailing list archive at =
Nabble.com.


From dev-return-8387-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 02:57:43 2014
Return-Path: <dev-return-8387-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DD78E1163B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 02:57:42 +0000 (UTC)
Received: (qmail 4787 invoked by uid 500); 16 Jul 2014 02:57:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4714 invoked by uid 500); 16 Jul 2014 02:57:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4703 invoked by uid 99); 16 Jul 2014 02:57:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:57:42 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of pelickzhang@qq.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 02:57:37 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <pelickzhang@qq.com>)
	id 1X7FPB-0005hl-8H
	for dev@spark.incubator.apache.org; Tue, 15 Jul 2014 19:57:17 -0700
Date: Tue, 15 Jul 2014 19:57:17 -0700 (PDT)
From: Baofeng Zhang <pelickzhang@qq.com>
To: dev@spark.incubator.apache.org
Message-ID: <1405479437249-7360.post@n3.nabble.com>
In-Reply-To: <163620AF-914B-4125-8B0E-BAA27E4CD0D4@gmail.com>
References: <CAAswR-6gnUA5dYUcgw9q2gGD090oa_RGza-SxXCzVFDxP=4YnQ@mail.gmail.com> <55292A11-1AFF-4C9A-A196-9C0E133D1C97@gmail.com> <1405477803082-7358.post@n3.nabble.com> <163620AF-914B-4125-8B0E-BAA27E4CD0D4@gmail.com>
Subject: Re: Catalyst dependency on Spark Core
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I see. 

So how about let me do this simple work to make my contribution :)

It is cooool.



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Catalyst-dependency-on-Spark-Core-tp7303p7360.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8388-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 07:06:49 2014
Return-Path: <dev-return-8388-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0F94A11CA3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 07:06:49 +0000 (UTC)
Received: (qmail 24038 invoked by uid 500); 16 Jul 2014 07:06:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23977 invoked by uid 500); 16 Jul 2014 07:06:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23958 invoked by uid 99); 16 Jul 2014 07:06:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 07:06:47 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.174 as permitted sender)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 07:06:44 +0000
Received: by mail-we0-f174.google.com with SMTP id x48so429784wes.33
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 00:06:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=QRPqaVv8tZB05hcoH7ES8bHXL3T1BS51eFj2gsBjloM=;
        b=l9M6BfjjKcfpf1m6xyG6m9tbBEVrXFxdWcW/VeEp1liGqCcsNU3e6Tm42JPMNFU3TO
         rtPZJtXMVB6K5l/jyDIUYWWGMZlDo021RyamWu4fd7f3aKJJnws4MoK6FxUTjGbSiaiT
         CfilqnWDviapCBnSH76ZKsQdHvaePFDfarawj7EKEjxq7sXZZeALtuye8hjD4pKDXGBj
         bqZ2BHsr3+ki9RaW7OsuCkRQj7p18dJxvPfP6X2ytJDAJ4rbCG4sMuVnYZvYiOvMs8Pl
         uLYJWzykJqrq/x3/h+/hZy4T31Lp3pGg2VObEb8ZOeriomBPujwkKUdDzEDYmmFJqfqN
         XSpg==
MIME-Version: 1.0
X-Received: by 10.180.94.5 with SMTP id cy5mr10908693wib.11.1405494382853;
 Wed, 16 Jul 2014 00:06:22 -0700 (PDT)
Received: by 10.194.6.74 with HTTP; Wed, 16 Jul 2014 00:06:22 -0700 (PDT)
In-Reply-To: <CABDsqqYP8QG+FOs3k4Jr9RDaStV8jxgGZ2K7=Q1HJqpZXruu8w@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
	<CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
	<CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
	<CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
	<CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com>
	<CABDsqqarh4OO2fdLEB49yXjroBoCHh7_oVuRdsGZx-jNFdb1Lw@mail.gmail.com>
	<CAG2iju36syXK7yJUDgwOS4U9PTrR+M=DYPcV9AaBP=baskZgvw@mail.gmail.com>
	<CABDsqqYP8QG+FOs3k4Jr9RDaStV8jxgGZ2K7=Q1HJqpZXruu8w@mail.gmail.com>
Date: Wed, 16 Jul 2014 15:06:22 +0800
Message-ID: <CABDsqqb6HZr8P3N2aVDX7kd8Oiiz4dBCfVkwpfy9LoWwmEUbNg@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d044403566c84fa04fe4a295b
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044403566c84fa04fe4a295b
Content-Type: text/plain; charset=UTF-8

let's me describe my scene:
----------------------
i have 8 machines (24 core , 16G memory, per machine) of spark cluster and
tachyon cluster.  On tachyon,  I create one table which contains 800M data,
when i run query sql on shark,   it will cost 2.43s,  but when i create the
same table on spark memory , i run  the same sql , it will cost 1.56s.
 data on tachyon cost more time than data on spark memory.   they all have
150 map process,  and per node 16-20 map process.
I think the reason is that when data is on tachyon, shark will let spark
slave load data from tachyon salve which is on the same node with tachyon
slave,
i have tried to set some configuration to tune shark and tachyon, but still
can not make the former more fast than 2.43s.
do anyone have some ideas ?

By the way ,  my tachyon block size is 1GB now,  i want to reset block size
,  will it work by setting tachyon.user.default.block.size.byte=8M ?  if
not,  what does tachyon.user.default.block.size.byte mean?


2014-07-14 13:13 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:

> Shark,  thanks for replying.
> Let's me clear my question again.
> ----------------------------------------------
> i create a table using " create table xxx1
> tblproperties("shark.cache"="tachyon") as select * from xxx2"
> when excuting some sql (for example , select * from xxx1) using shark,
>  shark will read data into shark's memory  from tachyon's memory.
> I think if each time we execute sql, shark always load data from tachyon,
> it is less effient.
> could we use some cache policy (such as,  CacheAllPolicy FIFOCachePolicy
> LRUCachePolicy ) to cache data to invoid reading data from tachyon for
> each sql query?
> ----------------------------------------------
>
>
>
> 2014-07-14 2:47 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
>
> Qingyang,
>>
>> Are you asking Spark or Shark (The first email was "Shark", the last email
>> was "Spark".)?
>>
>> Best,
>>
>> Haoyuan
>>
>>
>> On Wed, Jul 9, 2014 at 7:40 PM, qingyang li <liqingyang1985@gmail.com>
>> wrote:
>>
>> > could i set some cache policy to let spark load data from tachyon only
>> one
>> > time for all sql query?  for example by using CacheAllPolicy
>> > FIFOCachePolicy LRUCachePolicy.  But I have tried that three policy,
>> they
>> > are not useful.
>> > I think , if spark always load data for each sql query,  it will impact
>> the
>> > query speed , it will take more time than the case that data are
>> managed by
>> > spark itself.
>> >
>> >
>> >
>> >
>> > 2014-07-09 1:19 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
>> >
>> > > Yes. For Shark, two modes, "shark.cache=tachyon" and
>> > "shark.cache=memory",
>> > > have the same ser/de overhead. Shark loads data from outsize of the
>> > process
>> > > in Tachyon mode with the following benefits:
>> > >
>> > >
>> > >    - In-memory data sharing across multiple Shark instances (i.e.
>> > stronger
>> > >    isolation)
>> > >    - Instant recovery of in-memory tables
>> > >    - Reduce heap size => faster GC in shark
>> > >    - If the table is larger than the memory size, only the hot columns
>> > will
>> > >    be cached in memory
>> > >
>> > > from http://tachyon-project.org/master/Running-Shark-on-Tachyon.html
>> and
>> > > https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon
>> > >
>> > > Haoyuan
>> > >
>> > >
>> > > On Tue, Jul 8, 2014 at 9:58 AM, Aaron Davidson <ilikerps@gmail.com>
>> > wrote:
>> > >
>> > > > Shark's in-memory format is already serialized (it's compressed and
>> > > > column-based).
>> > > >
>> > > >
>> > > > On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <
>> mridul@gmail.com>
>> > > > wrote:
>> > > >
>> > > > > You are ignoring serde costs :-)
>> > > > >
>> > > > > - Mridul
>> > > > >
>> > > > > On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <
>> ilikerps@gmail.com>
>> > > > wrote:
>> > > > > > Tachyon should only be marginally less performant than
>> memory_only,
>> > > > > because
>> > > > > > we mmap the data from Tachyon's ramdisk. We do not have to, say,
>> > > > transfer
>> > > > > > the data over a pipe from Tachyon; we can directly read from the
>> > > > buffers
>> > > > > in
>> > > > > > the same way that Shark reads from its in-memory columnar
>> format.
>> > > > > >
>> > > > > >
>> > > > > >
>> > > > > > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <
>> > > liqingyang1985@gmail.com>
>> > > > > > wrote:
>> > > > > >
>> > > > > >> hi, when i create a table, i can point the cache strategy using
>> > > > > >> shark.cache,
>> > > > > >> i think "shark.cache=memory_only"  means data are managed by
>> > spark,
>> > > > and
>> > > > > >> data are in the same jvm with excutor;   while
>> > >  "shark.cache=tachyon"
>> > > > > >>  means  data are managed by tachyon which is off heap, and data
>> > are
>> > > > not
>> > > > > in
>> > > > > >> the same jvm with excutor,  so spark will load data from
>> tachyon
>> > for
>> > > > > each
>> > > > > >> query sql , so,  is  tachyon less efficient than memory_only
>> cache
>> > > > > strategy
>> > > > > >>  ?
>> > > > > >> if yes, can we let spark load all data once from tachyon  for
>> all
>> > > sql
>> > > > > query
>> > > > > >>  if i want to use tachyon cache strategy since tachyon is more
>> HA
>> > > than
>> > > > > >> memory_only ?
>> > > > > >>
>> > > > >
>> > > >
>> > >
>> > >
>> > >
>> > > --
>> > > Haoyuan Li
>> > > AMPLab, EECS, UC Berkeley
>> > > http://www.cs.berkeley.edu/~haoyuan/
>> > >
>> >
>>
>>
>>
>> --
>> Haoyuan Li
>> AMPLab, EECS, UC Berkeley
>> http://www.cs.berkeley.edu/~haoyuan/
>>
>
>

--f46d044403566c84fa04fe4a295b--

From dev-return-8389-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 09:38:40 2014
Return-Path: <dev-return-8389-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 298C111120
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 09:38:40 +0000 (UTC)
Received: (qmail 45899 invoked by uid 500); 16 Jul 2014 09:38:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45838 invoked by uid 500); 16 Jul 2014 09:38:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45822 invoked by uid 99); 16 Jul 2014 09:38:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 09:38:38 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andy.petrella@gmail.com designates 209.85.217.173 as permitted sender)
Received: from [209.85.217.173] (HELO mail-lb0-f173.google.com) (209.85.217.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 09:38:34 +0000
Received: by mail-lb0-f173.google.com with SMTP id n15so453132lbi.4
        for <dev@spark.incubator.apache.org>; Wed, 16 Jul 2014 02:38:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ZSiZlN+8ZX9lc90h6br+W84WFryIkf4E/l8+7tjaok0=;
        b=HndloorDYmGG3Lrp/VWzodh25SN5kOIzH3/TTfHsa347nt5kRLU2VVS1QtR8ciW4Qj
         GZx8TeHGDWiP4lYPyWHSxT8GryFpDQDoThuobCo6X+8aWwAoek+EuyvC/lfPMPq8Uz+H
         goxyAec1pBGFeoMI55ufIGxMPL2Mtiqvdr/AWXP/XqO3uPmk2KtaGECiIhc97Ms/cf5B
         IfJ6BHkowoE9vXTbMjRk7KScj+afAsJlqAtzA0S0Id1/w9KgVhXWAEt7PU58SwFPp9gP
         Sp0R5bUDhUjE6vVAtNLTMobWT1cAoErFL8biEN7RqUpAd1p+9lG0d7Vf1nb5DRDr9pmf
         XD/A==
X-Received: by 10.152.87.164 with SMTP id az4mr2274596lab.74.1405503492608;
 Wed, 16 Jul 2014 02:38:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.162.5 with HTTP; Wed, 16 Jul 2014 02:37:52 -0700 (PDT)
In-Reply-To: <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Wed, 16 Jul 2014 11:37:52 +0200
Message-ID: <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c354c268605804fe4c485d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c354c268605804fe4c485d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Heya TD,

Thanks for the detailed answer! Much appreciated.

Regarding order among elements within an RDD, you're definitively right,
it'd kill the //ism and would require synchronization which is completely
avoided in distributed env.

That's why, I won't push this constraint to the RDDs themselves actually,
only the Space is something that *defines* ordered elements, and thus there
are two functions that will break the RDDs based on a given (extensible,
plugable) heuristic f.i.
Since the Space is rather decoupled from the data, thus the source and the
partitions, it's the responsibility of the CRRD implementation to dictate
how (if necessary) the elements should be sorted in the RDDs... which will
require some shuffles :-s -- Or the couple (source, space) is something
intrinsically ordered (like it is for DStream).

To be more concrete an RDD would be composed of un-ordered iterator of
millions of events for which all timestamps land into the same time
interval.

WDYT, would that makes sense?

thanks again for the answer!

greetz

 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>


On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <tathagata.das1565@gmail.co=
m
> wrote:

> Very interesting ideas Andy!
>
> Conceptually i think it makes sense. In fact, it is true that dealing wit=
h
> time series data, windowing over application time, windowing over number =
of
> events, are things that DStream does not natively support. The real
> challenge is actually mapping the conceptual windows with the underlying
> RDD model. On aspect you correctly observed in the ordering of events
> within the RDDs of the DStream. Another fundamental aspect is the fact th=
at
> RDDs as parallel collections, with no well-defined ordering in the record=
s
> in the RDDs. If you want to process the records in an RDD as a ordered
> stream of events, you kind of have to process the stream sequentially,
> which means you have to process each RDD partition one-by-one, and
> therefore lose the parallelism. So implementing all these functionality m=
ay
> mean adding functionality at the cost of performance. Whether that is oka=
y
> for Spark Streaming to have these OR this tradeoff is not-intuitive for
> end-users and therefore should not come out-of-the-box with Spark Streami=
ng
> -- that is a definitely a question worth debating upon.
>
> That said, for some limited usecases, like windowing over N events, can b=
e
> implemented using custom RDDs like SlidingRDD
> <
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> >
> without
> losing parallelism. For things like app time based windows, and
> random-application-event based windows, its much harder.
>
> Interesting ideas nonetheless. I am curious to see how far we can push
> using the RDD model underneath, without losing parallelism and performanc=
e.
>
> TD
>
>
>
> On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <andy.petrella@gmail.com>
> wrote:
>
> > Dear Sparkers,
> >
> > *[sorry for the lengthy email... =3D> head to the gist
> > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
> preview
> > :-p**]*
> >
> > I would like to share some thinking I had due to a use case I faced.
> > Basically, as the subject announced it, it's a generalization of the
> > DStream currently available in the streaming project.
> > First of all, I'd like to say that it's only a result of some personal
> > thinking, alone in the dark with a use case, the spark code, a sheet of
> > paper and a poor pen.
> >
> >
> > DStream is a very great concept to deal with micro-batching use cases,
> and
> > it does it very well too!
> > Also, it hardly relies on the elapsing time to create its internal
> > micro-batches.
> > However, there are similar use cases where we need micro-batches where
> this
> > constraint on the time doesn't hold, here are two of them:
> > * a micro-batch has to be created every *n* events received
> > * a micro-batch has to be generate based on the values of the items
> pushed
> > by the source (which might even not be a stream!).
> >
> > An example of use case (mine ^^) would be
> > * the creation of timeseries from a cold source containing timestamped
> > events (like S3).
> > * one these timeseries have cells being the mean (sum, count, ...) of o=
ne
> > of the fields of the event
> > * the mean has to be computed over a window depending on a field
> > *timestamp*.
> >
> > * a timeserie is created for each type of event (the number of types is
> > high)
> > So, in this case, it'd be interesting to have an RDD for each cell, whi=
ch
> > will generate all cells for all neede timeseries.
> > It's more or less what DStream does, but here it won't help due what wa=
s
> > stated above.
> >
> > That's how I came to a raw sketch of what could be named ContinuousRDD
> > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> simplicity
> > I've stuck with the definition of a DStream to think about it. Okay,
> let's
> > go ^^.
> >
> >
> > Looking at the DStream contract, here is something that could be drafte=
d
> > around CRDD.
> > A *CRDD* would be a generalized concept that relies on:
> > * a reference space/continuum (to which data can be bound)
> > * a binning function that can breaks the continuum into splits.
> > Since *Space* is a continuum we could define it as:
> > * a *SpacePoint* (the origin)
> > * a SpacePoint=3D>SpacePoint (the continuous function)
> > * a Ordering[SpacePoint]
> >
> > DStream uses a *JobGenerator* along with a DStreamGraph, which are usin=
g
> > timer and clock to do their work, in the case of a CRDD we'll have to
> > define also a point generator, as a more generic but also adaptable
> > concept.
> >
> >
> > So far (so good?), these definition should work quite fine for *ordered=
*
> > space
> > for which:
> > * points are coming/fetched in order
> > * the space is fully filled (no gaps)
> > For these cases, the JobGenerator (f.i.) could be defined with two extr=
a
> > functions:
> > * one is responsible to chop the batches even if the upper bound of the
> > batch hasn't been seen yet
> > * the other is responsible to handle outliers (and could wrap them into
> yet
> > another CRDD ?)
> >
> >
> > I created a gist here wrapping up the types and thus the skeleton of th=
is
> > idea, you can find it here:
> > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> >
> > WDYT?
> > *The answer can be: you're a fool!*
> > Actually, I already I am, but also I like to know why.... so some
> > explanations will help me :-D.
> >
> > Thanks to read 'till this point.
> >
> > Greetz,
> >
> >
> >
> >  a=E2=84=95dy =E2=84=99etrella
> > about.me/noootsab
> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >
> > <http://about.me/noootsab>
> >
>

--001a11c354c268605804fe4c485d--

From dev-return-8390-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 09:38:40 2014
Return-Path: <dev-return-8390-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 439B411122
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 09:38:40 +0000 (UTC)
Received: (qmail 46867 invoked by uid 500); 16 Jul 2014 09:38:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46811 invoked by uid 500); 16 Jul 2014 09:38:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46771 invoked by uid 99); 16 Jul 2014 09:38:38 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 09:38:38 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andy.petrella@gmail.com designates 209.85.215.49 as permitted sender)
Received: from [209.85.215.49] (HELO mail-la0-f49.google.com) (209.85.215.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 09:38:37 +0000
Received: by mail-la0-f49.google.com with SMTP id hz20so447307lab.8
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 02:38:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=ZSiZlN+8ZX9lc90h6br+W84WFryIkf4E/l8+7tjaok0=;
        b=HndloorDYmGG3Lrp/VWzodh25SN5kOIzH3/TTfHsa347nt5kRLU2VVS1QtR8ciW4Qj
         GZx8TeHGDWiP4lYPyWHSxT8GryFpDQDoThuobCo6X+8aWwAoek+EuyvC/lfPMPq8Uz+H
         goxyAec1pBGFeoMI55ufIGxMPL2Mtiqvdr/AWXP/XqO3uPmk2KtaGECiIhc97Ms/cf5B
         IfJ6BHkowoE9vXTbMjRk7KScj+afAsJlqAtzA0S0Id1/w9KgVhXWAEt7PU58SwFPp9gP
         Sp0R5bUDhUjE6vVAtNLTMobWT1cAoErFL8biEN7RqUpAd1p+9lG0d7Vf1nb5DRDr9pmf
         XD/A==
X-Received: by 10.152.87.164 with SMTP id az4mr2274596lab.74.1405503492608;
 Wed, 16 Jul 2014 02:38:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.162.5 with HTTP; Wed, 16 Jul 2014 02:37:52 -0700 (PDT)
In-Reply-To: <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Wed, 16 Jul 2014 11:37:52 +0200
Message-ID: <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a11c354c268605804fe4c485d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c354c268605804fe4c485d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Heya TD,

Thanks for the detailed answer! Much appreciated.

Regarding order among elements within an RDD, you're definitively right,
it'd kill the //ism and would require synchronization which is completely
avoided in distributed env.

That's why, I won't push this constraint to the RDDs themselves actually,
only the Space is something that *defines* ordered elements, and thus there
are two functions that will break the RDDs based on a given (extensible,
plugable) heuristic f.i.
Since the Space is rather decoupled from the data, thus the source and the
partitions, it's the responsibility of the CRRD implementation to dictate
how (if necessary) the elements should be sorted in the RDDs... which will
require some shuffles :-s -- Or the couple (source, space) is something
intrinsically ordered (like it is for DStream).

To be more concrete an RDD would be composed of un-ordered iterator of
millions of events for which all timestamps land into the same time
interval.

WDYT, would that makes sense?

thanks again for the answer!

greetz

 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>


On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <tathagata.das1565@gmail.co=
m
> wrote:

> Very interesting ideas Andy!
>
> Conceptually i think it makes sense. In fact, it is true that dealing wit=
h
> time series data, windowing over application time, windowing over number =
of
> events, are things that DStream does not natively support. The real
> challenge is actually mapping the conceptual windows with the underlying
> RDD model. On aspect you correctly observed in the ordering of events
> within the RDDs of the DStream. Another fundamental aspect is the fact th=
at
> RDDs as parallel collections, with no well-defined ordering in the record=
s
> in the RDDs. If you want to process the records in an RDD as a ordered
> stream of events, you kind of have to process the stream sequentially,
> which means you have to process each RDD partition one-by-one, and
> therefore lose the parallelism. So implementing all these functionality m=
ay
> mean adding functionality at the cost of performance. Whether that is oka=
y
> for Spark Streaming to have these OR this tradeoff is not-intuitive for
> end-users and therefore should not come out-of-the-box with Spark Streami=
ng
> -- that is a definitely a question worth debating upon.
>
> That said, for some limited usecases, like windowing over N events, can b=
e
> implemented using custom RDDs like SlidingRDD
> <
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> >
> without
> losing parallelism. For things like app time based windows, and
> random-application-event based windows, its much harder.
>
> Interesting ideas nonetheless. I am curious to see how far we can push
> using the RDD model underneath, without losing parallelism and performanc=
e.
>
> TD
>
>
>
> On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <andy.petrella@gmail.com>
> wrote:
>
> > Dear Sparkers,
> >
> > *[sorry for the lengthy email... =3D> head to the gist
> > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
> preview
> > :-p**]*
> >
> > I would like to share some thinking I had due to a use case I faced.
> > Basically, as the subject announced it, it's a generalization of the
> > DStream currently available in the streaming project.
> > First of all, I'd like to say that it's only a result of some personal
> > thinking, alone in the dark with a use case, the spark code, a sheet of
> > paper and a poor pen.
> >
> >
> > DStream is a very great concept to deal with micro-batching use cases,
> and
> > it does it very well too!
> > Also, it hardly relies on the elapsing time to create its internal
> > micro-batches.
> > However, there are similar use cases where we need micro-batches where
> this
> > constraint on the time doesn't hold, here are two of them:
> > * a micro-batch has to be created every *n* events received
> > * a micro-batch has to be generate based on the values of the items
> pushed
> > by the source (which might even not be a stream!).
> >
> > An example of use case (mine ^^) would be
> > * the creation of timeseries from a cold source containing timestamped
> > events (like S3).
> > * one these timeseries have cells being the mean (sum, count, ...) of o=
ne
> > of the fields of the event
> > * the mean has to be computed over a window depending on a field
> > *timestamp*.
> >
> > * a timeserie is created for each type of event (the number of types is
> > high)
> > So, in this case, it'd be interesting to have an RDD for each cell, whi=
ch
> > will generate all cells for all neede timeseries.
> > It's more or less what DStream does, but here it won't help due what wa=
s
> > stated above.
> >
> > That's how I came to a raw sketch of what could be named ContinuousRDD
> > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> simplicity
> > I've stuck with the definition of a DStream to think about it. Okay,
> let's
> > go ^^.
> >
> >
> > Looking at the DStream contract, here is something that could be drafte=
d
> > around CRDD.
> > A *CRDD* would be a generalized concept that relies on:
> > * a reference space/continuum (to which data can be bound)
> > * a binning function that can breaks the continuum into splits.
> > Since *Space* is a continuum we could define it as:
> > * a *SpacePoint* (the origin)
> > * a SpacePoint=3D>SpacePoint (the continuous function)
> > * a Ordering[SpacePoint]
> >
> > DStream uses a *JobGenerator* along with a DStreamGraph, which are usin=
g
> > timer and clock to do their work, in the case of a CRDD we'll have to
> > define also a point generator, as a more generic but also adaptable
> > concept.
> >
> >
> > So far (so good?), these definition should work quite fine for *ordered=
*
> > space
> > for which:
> > * points are coming/fetched in order
> > * the space is fully filled (no gaps)
> > For these cases, the JobGenerator (f.i.) could be defined with two extr=
a
> > functions:
> > * one is responsible to chop the batches even if the upper bound of the
> > batch hasn't been seen yet
> > * the other is responsible to handle outliers (and could wrap them into
> yet
> > another CRDD ?)
> >
> >
> > I created a gist here wrapping up the types and thus the skeleton of th=
is
> > idea, you can find it here:
> > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> >
> > WDYT?
> > *The answer can be: you're a fool!*
> > Actually, I already I am, but also I like to know why.... so some
> > explanations will help me :-D.
> >
> > Thanks to read 'till this point.
> >
> > Greetz,
> >
> >
> >
> >  a=E2=84=95dy =E2=84=99etrella
> > about.me/noootsab
> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >
> > <http://about.me/noootsab>
> >
>

--001a11c354c268605804fe4c485d--

From dev-return-8391-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 16:00:56 2014
Return-Path: <dev-return-8391-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C558611D61
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 16:00:56 +0000 (UTC)
Received: (qmail 23150 invoked by uid 500); 16 Jul 2014 16:00:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23095 invoked by uid 500); 16 Jul 2014 16:00:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 1960 invoked by uid 99); 16 Jul 2014 15:52:28 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kartheek.mbms@gmail.com designates 209.85.212.172 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=IUu0qljtqcF1ajjqMXvENnIcrGzLGC4SHvnI9QCud50=;
        b=z1TBOvixTDg2ktIFGLgNNaYCswWeE/VkCuoOCFBwSED1lOv9jEs/5zNxMJpX61dkvo
         XgVPcFNGrH447EIfXNEqSvwowXcNtpdt57RVeimJIGxQVDbInFHd5nG5uXeb4TlKqDin
         iMjB6WYd7f6vDYkp9tdL/1eKH6slSf5zYSGSe+XH+UG8sIwFl4Fda8a1YJ1lZhHsAci9
         qsMm+qhuxYGOdkra9mdyflJIx4e3DDIPP68jabRb+wEVPlpk3Pgks4TE9bKoX0fqBVxk
         b4jqC8jM/YSQQndTpgBNOA9zFYAcy91lqf9i6pyRsbvPfWHlM2AgeMTXjQEoNWD5FpSW
         xMPA==
MIME-Version: 1.0
X-Received: by 10.180.37.230 with SMTP id b6mr14778373wik.47.1405525921751;
 Wed, 16 Jul 2014 08:52:01 -0700 (PDT)
Date: Wed, 16 Jul 2014 21:22:01 +0530
Message-ID: <CAAbaoBA-EE4m9yR5KLYK+VN2heSdHin_1-i+zAM8KJ-6r=8-tQ@mail.gmail.com>
Subject: Resource allocations
From: rapelly kartheek <kartheek.mbms@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f64701d49eaa204fe518145
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f64701d49eaa204fe518145
Content-Type: text/plain; charset=UTF-8

Hi,

I am trying to understand how the resource allocation happens in spark. I
understand the resourceOffer method in taskScheduler. This method takes
care of locality factor while allocating the resources. This resourceOffer
method gets invoked by the corresponding cluster manager.

I am working on stand-alone spark cluster. But I am not able to locate
starting point of the resource allocation. I want to understand from end to
end how exactly resource allocations happens, given a new Application to
the Spark. Also, apart from mesos, there is a folder called Local in the
Scheduler. Can someone tell me which all files should I look into to
understand the resource allocation in stand-alone spark cluster and what is
this "Local" for?

Thanks in advance!!
Karthik.

--e89a8f64701d49eaa204fe518145--

From dev-return-8392-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 17:30:48 2014
Return-Path: <dev-return-8392-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 843C8101D9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 17:30:48 +0000 (UTC)
Received: (qmail 59947 invoked by uid 500); 16 Jul 2014 17:30:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59896 invoked by uid 500); 16 Jul 2014 17:30:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59885 invoked by uid 99); 16 Jul 2014 17:30:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 17:30:47 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.144 as permitted sender)
Received: from [169.229.218.144] (HELO cm03fe.IST.Berkeley.EDU) (169.229.218.144)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 17:30:40 +0000
Received: from mail-yh0-f45.google.com ([209.85.213.45])
	by cm03fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1X7T23-0006NH-AH
	for dev@spark.apache.org; Wed, 16 Jul 2014 10:30:20 -0700
Received: by mail-yh0-f45.google.com with SMTP id 29so608070yhl.4
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 10:30:18 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.36.45 with SMTP id v33mr28418108yha.129.1405531818269;
 Wed, 16 Jul 2014 10:30:18 -0700 (PDT)
Received: by 10.170.141.212 with HTTP; Wed, 16 Jul 2014 10:30:18 -0700 (PDT)
In-Reply-To: <CAAbaoBA-EE4m9yR5KLYK+VN2heSdHin_1-i+zAM8KJ-6r=8-tQ@mail.gmail.com>
References: <CAAbaoBA-EE4m9yR5KLYK+VN2heSdHin_1-i+zAM8KJ-6r=8-tQ@mail.gmail.com>
Date: Wed, 16 Jul 2014 10:30:18 -0700
Message-ID: <CAKJXNjH4Q765BwFoFuBRrxs9sVERKabZoFN=o+pVnkheFp3r-g@mail.gmail.com>
Subject: Re: Resource allocations
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0160bc68bfa33304fe52e08b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160bc68bfa33304fe52e08b
Content-Type: text/plain; charset=UTF-8

Hi Karthik,

The resourceOffer() method is invoked from a class implementing the
SchedulerBackend interface; in the case of a standalone cluster, it's
invoked from a CoarseGrainedSchedulerBackend (in the makeOffers() method).
 If you look in TaskSchedulerImpl.submitTasks(), it calls
backend.reviveOffers() at the end -- which signals to the SchedulerBackend
that it should offer some resources.  This somewhat confusing interface
exists for historical reasons: Spark originally used only Mesos for
scheduling, which relies on resource offers for scheduling, and so
standalone mode was done in a similar way to maximize code re-use.

The scheduler/local folder contains code specific to running in local mode.
 With local mode, all code runs in a single JVM (as opposed to having one
JVM for the driver, one for the cluster master, and one for each worker).
 This mode is typically used for testing.

-Kay

On Wed, Jul 16, 2014 at 8:52 AM, rapelly kartheek <kartheek.mbms@gmail.com>
wrote:

> Hi,
>
> I am trying to understand how the resource allocation happens in spark. I
> understand the resourceOffer method in taskScheduler. This method takes
> care of locality factor while allocating the resources. This resourceOffer
> method gets invoked by the corresponding cluster manager.
>
> I am working on stand-alone spark cluster. But I am not able to locate
> starting point of the resource allocation. I want to understand from end to
> end how exactly resource allocations happens, given a new Application to
> the Spark. Also, apart from mesos, there is a folder called Local in the
> Scheduler. Can someone tell me which all files should I look into to
> understand the resource allocation in stand-alone spark cluster and what is
> this "Local" for?
>
> Thanks in advance!!
> Karthik.
>

--089e0160bc68bfa33304fe52e08b--

From dev-return-8393-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 22:32:04 2014
Return-Path: <dev-return-8393-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 31784114D9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 22:32:04 +0000 (UTC)
Received: (qmail 10882 invoked by uid 500); 16 Jul 2014 22:32:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10822 invoked by uid 500); 16 Jul 2014 22:32:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10799 invoked by uid 99); 16 Jul 2014 22:32:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 22:32:03 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 22:31:59 +0000
Received: by mail-vc0-f181.google.com with SMTP id lf12so2940169vcb.26
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 15:31:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=jqpXEXhtkO73qvZdc2N6ZONxb7CG8h4kUNgwVxjYxB8=;
        b=nTZNTGBnijhLp8pOWYoYHYJXa4NmRee+zNWJHjTYox8iEuUOUjMChFRsypK4yK+ri6
         lJ3yba6hhke1sIVMg+lJb5d7ai08mXSoua81E+ZvrAl8NTlk7mm8sEjpXVQbzCpIPNaV
         bZ5JHS3L8IRsO2T0LKvblRhikns0xW8gXuNCwXMJ8QVeTX038xHmLn+if2/WpN4yjQb7
         flv/cnGCNMRrJ4O/HVyo6pmO8qpld93X4+1tsWrRvssJtGxtFG8nLE6rIKydjMGHrse7
         HPOsEImxmH8PzTFORWsXLoQuVS/vOg7v+J/FGSVKF4vzP19poU9/p0TbXz+MttXUJ98p
         DSIA==
X-Received: by 10.221.63.195 with SMTP id xf3mr12420711vcb.36.1405549898244;
 Wed, 16 Jul 2014 15:31:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.131.39 with HTTP; Wed, 16 Jul 2014 15:31:08 -0700 (PDT)
In-Reply-To: <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com> <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Wed, 16 Jul 2014 15:31:08 -0700
Message-ID: <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1133482c662a1a04fe571694
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133482c662a1a04fe571694
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I think it makes sense, though without a concrete implementation its hard
to be sure. Applying sorting on the RDD according to the RDDs makes sense,
but I can think of two kinds of fundamental problems.

1. How do you deal with ordering across RDD boundaries. Say two consecutive
RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2, 3,
4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function through
all these records in the timestamp order. I am curious to find how this
problem can be solved without sacrificing efficiency (e.g. I can imagine
doing multiple pass magic)

2. An even more fundamental question is how do you ensure ordering with
delayed records. If you want to process in order of application time, and
records are delayed how do you deal with them.

Any ideas? ;)

TD



On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Heya TD,
>
> Thanks for the detailed answer! Much appreciated.
>
> Regarding order among elements within an RDD, you're definitively right,
> it'd kill the //ism and would require synchronization which is completely
> avoided in distributed env.
>
> That's why, I won't push this constraint to the RDDs themselves actually,
> only the Space is something that *defines* ordered elements, and thus the=
re
> are two functions that will break the RDDs based on a given (extensible,
> plugable) heuristic f.i.
> Since the Space is rather decoupled from the data, thus the source and th=
e
> partitions, it's the responsibility of the CRRD implementation to dictate
> how (if necessary) the elements should be sorted in the RDDs... which wil=
l
> require some shuffles :-s -- Or the couple (source, space) is something
> intrinsically ordered (like it is for DStream).
>
> To be more concrete an RDD would be composed of un-ordered iterator of
> millions of events for which all timestamps land into the same time
> interval.
>
> WDYT, would that makes sense?
>
> thanks again for the answer!
>
> greetz
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>
>
> On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
> tathagata.das1565@gmail.com
> > wrote:
>
> > Very interesting ideas Andy!
> >
> > Conceptually i think it makes sense. In fact, it is true that dealing
> with
> > time series data, windowing over application time, windowing over numbe=
r
> of
> > events, are things that DStream does not natively support. The real
> > challenge is actually mapping the conceptual windows with the underlyin=
g
> > RDD model. On aspect you correctly observed in the ordering of events
> > within the RDDs of the DStream. Another fundamental aspect is the fact
> that
> > RDDs as parallel collections, with no well-defined ordering in the
> records
> > in the RDDs. If you want to process the records in an RDD as a ordered
> > stream of events, you kind of have to process the stream sequentially,
> > which means you have to process each RDD partition one-by-one, and
> > therefore lose the parallelism. So implementing all these functionality
> may
> > mean adding functionality at the cost of performance. Whether that is
> okay
> > for Spark Streaming to have these OR this tradeoff is not-intuitive for
> > end-users and therefore should not come out-of-the-box with Spark
> Streaming
> > -- that is a definitely a question worth debating upon.
> >
> > That said, for some limited usecases, like windowing over N events, can
> be
> > implemented using custom RDDs like SlidingRDD
> > <
> >
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> > >
> > without
> > losing parallelism. For things like app time based windows, and
> > random-application-event based windows, its much harder.
> >
> > Interesting ideas nonetheless. I am curious to see how far we can push
> > using the RDD model underneath, without losing parallelism and
> performance.
> >
> > TD
> >
> >
> >
> > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <andy.petrella@gmail.co=
m
> >
> > wrote:
> >
> > > Dear Sparkers,
> > >
> > > *[sorry for the lengthy email... =3D> head to the gist
> > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
> > preview
> > > :-p**]*
> > >
> > > I would like to share some thinking I had due to a use case I faced.
> > > Basically, as the subject announced it, it's a generalization of the
> > > DStream currently available in the streaming project.
> > > First of all, I'd like to say that it's only a result of some persona=
l
> > > thinking, alone in the dark with a use case, the spark code, a sheet =
of
> > > paper and a poor pen.
> > >
> > >
> > > DStream is a very great concept to deal with micro-batching use cases=
,
> > and
> > > it does it very well too!
> > > Also, it hardly relies on the elapsing time to create its internal
> > > micro-batches.
> > > However, there are similar use cases where we need micro-batches wher=
e
> > this
> > > constraint on the time doesn't hold, here are two of them:
> > > * a micro-batch has to be created every *n* events received
> > > * a micro-batch has to be generate based on the values of the items
> > pushed
> > > by the source (which might even not be a stream!).
> > >
> > > An example of use case (mine ^^) would be
> > > * the creation of timeseries from a cold source containing timestampe=
d
> > > events (like S3).
> > > * one these timeseries have cells being the mean (sum, count, ...) of
> one
> > > of the fields of the event
> > > * the mean has to be computed over a window depending on a field
> > > *timestamp*.
> > >
> > > * a timeserie is created for each type of event (the number of types =
is
> > > high)
> > > So, in this case, it'd be interesting to have an RDD for each cell,
> which
> > > will generate all cells for all neede timeseries.
> > > It's more or less what DStream does, but here it won't help due what
> was
> > > stated above.
> > >
> > > That's how I came to a raw sketch of what could be named ContinuousRD=
D
> > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> > simplicity
> > > I've stuck with the definition of a DStream to think about it. Okay,
> > let's
> > > go ^^.
> > >
> > >
> > > Looking at the DStream contract, here is something that could be
> drafted
> > > around CRDD.
> > > A *CRDD* would be a generalized concept that relies on:
> > > * a reference space/continuum (to which data can be bound)
> > > * a binning function that can breaks the continuum into splits.
> > > Since *Space* is a continuum we could define it as:
> > > * a *SpacePoint* (the origin)
> > > * a SpacePoint=3D>SpacePoint (the continuous function)
> > > * a Ordering[SpacePoint]
> > >
> > > DStream uses a *JobGenerator* along with a DStreamGraph, which are
> using
> > > timer and clock to do their work, in the case of a CRDD we'll have to
> > > define also a point generator, as a more generic but also adaptable
> > > concept.
> > >
> > >
> > > So far (so good?), these definition should work quite fine for
> *ordered*
> > > space
> > > for which:
> > > * points are coming/fetched in order
> > > * the space is fully filled (no gaps)
> > > For these cases, the JobGenerator (f.i.) could be defined with two
> extra
> > > functions:
> > > * one is responsible to chop the batches even if the upper bound of t=
he
> > > batch hasn't been seen yet
> > > * the other is responsible to handle outliers (and could wrap them in=
to
> > yet
> > > another CRDD ?)
> > >
> > >
> > > I created a gist here wrapping up the types and thus the skeleton of
> this
> > > idea, you can find it here:
> > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> > >
> > > WDYT?
> > > *The answer can be: you're a fool!*
> > > Actually, I already I am, but also I like to know why.... so some
> > > explanations will help me :-D.
> > >
> > > Thanks to read 'till this point.
> > >
> > > Greetz,
> > >
> > >
> > >
> > >  a=E2=84=95dy =E2=84=99etrella
> > > about.me/noootsab
> > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> > >
> > > <http://about.me/noootsab>
> > >
> >
>

--001a1133482c662a1a04fe571694--

From dev-return-8394-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 22:32:05 2014
Return-Path: <dev-return-8394-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 102F2114DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 22:32:05 +0000 (UTC)
Received: (qmail 11037 invoked by uid 500); 16 Jul 2014 22:32:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10824 invoked by uid 500); 16 Jul 2014 22:32:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10798 invoked by uid 99); 16 Jul 2014 22:32:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 22:32:03 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.169 as permitted sender)
Received: from [209.85.220.169] (HELO mail-vc0-f169.google.com) (209.85.220.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 22:31:59 +0000
Received: by mail-vc0-f169.google.com with SMTP id hu12so3087243vcb.28
        for <dev@spark.incubator.apache.org>; Wed, 16 Jul 2014 15:31:38 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=jqpXEXhtkO73qvZdc2N6ZONxb7CG8h4kUNgwVxjYxB8=;
        b=nTZNTGBnijhLp8pOWYoYHYJXa4NmRee+zNWJHjTYox8iEuUOUjMChFRsypK4yK+ri6
         lJ3yba6hhke1sIVMg+lJb5d7ai08mXSoua81E+ZvrAl8NTlk7mm8sEjpXVQbzCpIPNaV
         bZ5JHS3L8IRsO2T0LKvblRhikns0xW8gXuNCwXMJ8QVeTX038xHmLn+if2/WpN4yjQb7
         flv/cnGCNMRrJ4O/HVyo6pmO8qpld93X4+1tsWrRvssJtGxtFG8nLE6rIKydjMGHrse7
         HPOsEImxmH8PzTFORWsXLoQuVS/vOg7v+J/FGSVKF4vzP19poU9/p0TbXz+MttXUJ98p
         DSIA==
X-Received: by 10.221.63.195 with SMTP id xf3mr12420711vcb.36.1405549898244;
 Wed, 16 Jul 2014 15:31:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.131.39 with HTTP; Wed, 16 Jul 2014 15:31:08 -0700 (PDT)
In-Reply-To: <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com> <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Wed, 16 Jul 2014 15:31:08 -0700
Message-ID: <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1133482c662a1a04fe571694
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133482c662a1a04fe571694
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I think it makes sense, though without a concrete implementation its hard
to be sure. Applying sorting on the RDD according to the RDDs makes sense,
but I can think of two kinds of fundamental problems.

1. How do you deal with ordering across RDD boundaries. Say two consecutive
RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2, 3,
4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function through
all these records in the timestamp order. I am curious to find how this
problem can be solved without sacrificing efficiency (e.g. I can imagine
doing multiple pass magic)

2. An even more fundamental question is how do you ensure ordering with
delayed records. If you want to process in order of application time, and
records are delayed how do you deal with them.

Any ideas? ;)

TD



On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.com>
wrote:

> Heya TD,
>
> Thanks for the detailed answer! Much appreciated.
>
> Regarding order among elements within an RDD, you're definitively right,
> it'd kill the //ism and would require synchronization which is completely
> avoided in distributed env.
>
> That's why, I won't push this constraint to the RDDs themselves actually,
> only the Space is something that *defines* ordered elements, and thus the=
re
> are two functions that will break the RDDs based on a given (extensible,
> plugable) heuristic f.i.
> Since the Space is rather decoupled from the data, thus the source and th=
e
> partitions, it's the responsibility of the CRRD implementation to dictate
> how (if necessary) the elements should be sorted in the RDDs... which wil=
l
> require some shuffles :-s -- Or the couple (source, space) is something
> intrinsically ordered (like it is for DStream).
>
> To be more concrete an RDD would be composed of un-ordered iterator of
> millions of events for which all timestamps land into the same time
> interval.
>
> WDYT, would that makes sense?
>
> thanks again for the answer!
>
> greetz
>
>  a=E2=84=95dy =E2=84=99etrella
> about.me/noootsab
> [image: a=E2=84=95dy =E2=84=99etrella on about.me]
>
> <http://about.me/noootsab>
>
>
> On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
> tathagata.das1565@gmail.com
> > wrote:
>
> > Very interesting ideas Andy!
> >
> > Conceptually i think it makes sense. In fact, it is true that dealing
> with
> > time series data, windowing over application time, windowing over numbe=
r
> of
> > events, are things that DStream does not natively support. The real
> > challenge is actually mapping the conceptual windows with the underlyin=
g
> > RDD model. On aspect you correctly observed in the ordering of events
> > within the RDDs of the DStream. Another fundamental aspect is the fact
> that
> > RDDs as parallel collections, with no well-defined ordering in the
> records
> > in the RDDs. If you want to process the records in an RDD as a ordered
> > stream of events, you kind of have to process the stream sequentially,
> > which means you have to process each RDD partition one-by-one, and
> > therefore lose the parallelism. So implementing all these functionality
> may
> > mean adding functionality at the cost of performance. Whether that is
> okay
> > for Spark Streaming to have these OR this tradeoff is not-intuitive for
> > end-users and therefore should not come out-of-the-box with Spark
> Streaming
> > -- that is a definitely a question worth debating upon.
> >
> > That said, for some limited usecases, like windowing over N events, can
> be
> > implemented using custom RDDs like SlidingRDD
> > <
> >
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> > >
> > without
> > losing parallelism. For things like app time based windows, and
> > random-application-event based windows, its much harder.
> >
> > Interesting ideas nonetheless. I am curious to see how far we can push
> > using the RDD model underneath, without losing parallelism and
> performance.
> >
> > TD
> >
> >
> >
> > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <andy.petrella@gmail.co=
m
> >
> > wrote:
> >
> > > Dear Sparkers,
> > >
> > > *[sorry for the lengthy email... =3D> head to the gist
> > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
> > preview
> > > :-p**]*
> > >
> > > I would like to share some thinking I had due to a use case I faced.
> > > Basically, as the subject announced it, it's a generalization of the
> > > DStream currently available in the streaming project.
> > > First of all, I'd like to say that it's only a result of some persona=
l
> > > thinking, alone in the dark with a use case, the spark code, a sheet =
of
> > > paper and a poor pen.
> > >
> > >
> > > DStream is a very great concept to deal with micro-batching use cases=
,
> > and
> > > it does it very well too!
> > > Also, it hardly relies on the elapsing time to create its internal
> > > micro-batches.
> > > However, there are similar use cases where we need micro-batches wher=
e
> > this
> > > constraint on the time doesn't hold, here are two of them:
> > > * a micro-batch has to be created every *n* events received
> > > * a micro-batch has to be generate based on the values of the items
> > pushed
> > > by the source (which might even not be a stream!).
> > >
> > > An example of use case (mine ^^) would be
> > > * the creation of timeseries from a cold source containing timestampe=
d
> > > events (like S3).
> > > * one these timeseries have cells being the mean (sum, count, ...) of
> one
> > > of the fields of the event
> > > * the mean has to be computed over a window depending on a field
> > > *timestamp*.
> > >
> > > * a timeserie is created for each type of event (the number of types =
is
> > > high)
> > > So, in this case, it'd be interesting to have an RDD for each cell,
> which
> > > will generate all cells for all neede timeseries.
> > > It's more or less what DStream does, but here it won't help due what
> was
> > > stated above.
> > >
> > > That's how I came to a raw sketch of what could be named ContinuousRD=
D
> > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> > simplicity
> > > I've stuck with the definition of a DStream to think about it. Okay,
> > let's
> > > go ^^.
> > >
> > >
> > > Looking at the DStream contract, here is something that could be
> drafted
> > > around CRDD.
> > > A *CRDD* would be a generalized concept that relies on:
> > > * a reference space/continuum (to which data can be bound)
> > > * a binning function that can breaks the continuum into splits.
> > > Since *Space* is a continuum we could define it as:
> > > * a *SpacePoint* (the origin)
> > > * a SpacePoint=3D>SpacePoint (the continuous function)
> > > * a Ordering[SpacePoint]
> > >
> > > DStream uses a *JobGenerator* along with a DStreamGraph, which are
> using
> > > timer and clock to do their work, in the case of a CRDD we'll have to
> > > define also a point generator, as a more generic but also adaptable
> > > concept.
> > >
> > >
> > > So far (so good?), these definition should work quite fine for
> *ordered*
> > > space
> > > for which:
> > > * points are coming/fetched in order
> > > * the space is fully filled (no gaps)
> > > For these cases, the JobGenerator (f.i.) could be defined with two
> extra
> > > functions:
> > > * one is responsible to chop the batches even if the upper bound of t=
he
> > > batch hasn't been seen yet
> > > * the other is responsible to handle outliers (and could wrap them in=
to
> > yet
> > > another CRDD ?)
> > >
> > >
> > > I created a gist here wrapping up the types and thus the skeleton of
> this
> > > idea, you can find it here:
> > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> > >
> > > WDYT?
> > > *The answer can be: you're a fool!*
> > > Actually, I already I am, but also I like to know why.... so some
> > > explanations will help me :-D.
> > >
> > > Thanks to read 'till this point.
> > >
> > > Greetz,
> > >
> > >
> > >
> > >  a=E2=84=95dy =E2=84=99etrella
> > > about.me/noootsab
> > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> > >
> > > <http://about.me/noootsab>
> > >
> >
>

--001a1133482c662a1a04fe571694--

From dev-return-8395-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 23:12:06 2014
Return-Path: <dev-return-8395-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8642A1165B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 23:12:06 +0000 (UTC)
Received: (qmail 13115 invoked by uid 500); 16 Jul 2014 23:12:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13069 invoked by uid 500); 16 Jul 2014 23:12:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13039 invoked by uid 99); 16 Jul 2014 23:12:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 23:12:05 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of andy.petrella@gmail.com designates 209.85.215.52 as permitted sender)
Received: from [209.85.215.52] (HELO mail-la0-f52.google.com) (209.85.215.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 23:12:01 +0000
Received: by mail-la0-f52.google.com with SMTP id e16so1133965lan.39
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 16:11:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=c1bLI+Gdv3YIIRiZJhQU8Sti7AjNeXHPoIGO5KvdxwU=;
        b=A1NFBLLo8mr5DG13U40x1i/krIkPVB3TWVFIL1TDxj98kiIKDesrKkk/pU+CTxVxLm
         UOQVLTMJdiquoFtygo8m77ELTTwp6kBRlR09UKyy2O/pSm3ACLq5vUM3YjUlp7d7nuIm
         ptnkUOww/n7TGSZryK/jx5TamNYXqwbjRg4GqRDE9QOg75fpsFxyIdE4YHqHkI3Veff2
         bg4zyjsoqgseO4a1mh4SYg6UhQb9Blz511ZWkvU32Vzz2t83Yu0rjLn//XSI8JYny73l
         EVbSLxWycyDZYasxTx5i1w+XQBUevQ9GrFmLeehrsgVnjbPpYHWc7bvhn4R5KAAqbo11
         j0+Q==
X-Received: by 10.112.130.196 with SMTP id og4mr27403911lbb.38.1405552300036;
 Wed, 16 Jul 2014 16:11:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.162.5 with HTTP; Wed, 16 Jul 2014 16:11:19 -0700 (PDT)
In-Reply-To: <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
 <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com> <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Thu, 17 Jul 2014 01:11:19 +0200
Message-ID: <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a84a48e987a04fe57a552
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a84a48e987a04fe57a552
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Indeed, these two cases are tightly coupled (the first one is a special
case of the second).

Actually, these "outliers" could be handled by a dedicated function what I
named outliersManager -- I was not so much inspired ^^, but we could name
these outliers, "outlaws" and thus the function would be "sheriff".
The purpose of this "sheriff" function would be to create yet another
distributed collection (RDD, CRDD, ...?) with only the --outliers-- outlaws
in it.

Because these problems have a nature which will be as different as the use
case will be, it's hard to find a generic way to tackle them. So, you
know... that's why... I put temporarily them in jail and wait for the judge
to show them the right path! (.... okay it's late in Belgium -- 1AM).

All in all, it's more or less what we would do in DStream as well actually.
Let me expand a bit this reasoning, let's assume that some data points can
come along with the time, but aren't in sync with it -- f.i., a device that
wakes up and send all it's data at once.
The DStream will package them into RDDs mixed-up with true current data
points, however, the logic of the job will have to use a 'Y' road :
* to integrate them into a database at the right place
* to simply drop them out because they're won't be part of a shown chart
* etc

In this case, the 'Y' road would be of the contract ;-), and so left at the
appreciation of the dev.

Another way, to do it would be to ignore but log them, but that would be
very crappy, non professional and useful (and of course I'm just kidding).

my0.002=C2=A2



 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>


On Thu, Jul 17, 2014 at 12:31 AM, Tathagata Das <tathagata.das1565@gmail.co=
m
> wrote:

> I think it makes sense, though without a concrete implementation its hard
> to be sure. Applying sorting on the RDD according to the RDDs makes sense=
,
> but I can think of two kinds of fundamental problems.
>
> 1. How do you deal with ordering across RDD boundaries. Say two consecuti=
ve
> RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2, =
3,
> 4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function throug=
h
> all these records in the timestamp order. I am curious to find how this
> problem can be solved without sacrificing efficiency (e.g. I can imagine
> doing multiple pass magic)
>
> 2. An even more fundamental question is how do you ensure ordering with
> delayed records. If you want to process in order of application time, and
> records are delayed how do you deal with them.
>
> Any ideas? ;)
>
> TD
>
>
>
> On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.com>
> wrote:
>
> > Heya TD,
> >
> > Thanks for the detailed answer! Much appreciated.
> >
> > Regarding order among elements within an RDD, you're definitively right=
,
> > it'd kill the //ism and would require synchronization which is complete=
ly
> > avoided in distributed env.
> >
> > That's why, I won't push this constraint to the RDDs themselves actuall=
y,
> > only the Space is something that *defines* ordered elements, and thus
> there
> > are two functions that will break the RDDs based on a given (extensible=
,
> > plugable) heuristic f.i.
> > Since the Space is rather decoupled from the data, thus the source and
> the
> > partitions, it's the responsibility of the CRRD implementation to dicta=
te
> > how (if necessary) the elements should be sorted in the RDDs... which
> will
> > require some shuffles :-s -- Or the couple (source, space) is something
> > intrinsically ordered (like it is for DStream).
> >
> > To be more concrete an RDD would be composed of un-ordered iterator of
> > millions of events for which all timestamps land into the same time
> > interval.
> >
> > WDYT, would that makes sense?
> >
> > thanks again for the answer!
> >
> > greetz
> >
> >  a=E2=84=95dy =E2=84=99etrella
> > about.me/noootsab
> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >
> > <http://about.me/noootsab>
> >
> >
> > On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
> > tathagata.das1565@gmail.com
> > > wrote:
> >
> > > Very interesting ideas Andy!
> > >
> > > Conceptually i think it makes sense. In fact, it is true that dealing
> > with
> > > time series data, windowing over application time, windowing over
> number
> > of
> > > events, are things that DStream does not natively support. The real
> > > challenge is actually mapping the conceptual windows with the
> underlying
> > > RDD model. On aspect you correctly observed in the ordering of events
> > > within the RDDs of the DStream. Another fundamental aspect is the fac=
t
> > that
> > > RDDs as parallel collections, with no well-defined ordering in the
> > records
> > > in the RDDs. If you want to process the records in an RDD as a ordere=
d
> > > stream of events, you kind of have to process the stream sequentially=
,
> > > which means you have to process each RDD partition one-by-one, and
> > > therefore lose the parallelism. So implementing all these functionali=
ty
> > may
> > > mean adding functionality at the cost of performance. Whether that is
> > okay
> > > for Spark Streaming to have these OR this tradeoff is not-intuitive f=
or
> > > end-users and therefore should not come out-of-the-box with Spark
> > Streaming
> > > -- that is a definitely a question worth debating upon.
> > >
> > > That said, for some limited usecases, like windowing over N events, c=
an
> > be
> > > implemented using custom RDDs like SlidingRDD
> > > <
> > >
> >
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> > > >
> > > without
> > > losing parallelism. For things like app time based windows, and
> > > random-application-event based windows, its much harder.
> > >
> > > Interesting ideas nonetheless. I am curious to see how far we can pus=
h
> > > using the RDD model underneath, without losing parallelism and
> > performance.
> > >
> > > TD
> > >
> > >
> > >
> > > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <
> andy.petrella@gmail.com
> > >
> > > wrote:
> > >
> > > > Dear Sparkers,
> > > >
> > > > *[sorry for the lengthy email... =3D> head to the gist
> > > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
> > > preview
> > > > :-p**]*
> > > >
> > > > I would like to share some thinking I had due to a use case I faced=
.
> > > > Basically, as the subject announced it, it's a generalization of th=
e
> > > > DStream currently available in the streaming project.
> > > > First of all, I'd like to say that it's only a result of some
> personal
> > > > thinking, alone in the dark with a use case, the spark code, a shee=
t
> of
> > > > paper and a poor pen.
> > > >
> > > >
> > > > DStream is a very great concept to deal with micro-batching use
> cases,
> > > and
> > > > it does it very well too!
> > > > Also, it hardly relies on the elapsing time to create its internal
> > > > micro-batches.
> > > > However, there are similar use cases where we need micro-batches
> where
> > > this
> > > > constraint on the time doesn't hold, here are two of them:
> > > > * a micro-batch has to be created every *n* events received
> > > > * a micro-batch has to be generate based on the values of the items
> > > pushed
> > > > by the source (which might even not be a stream!).
> > > >
> > > > An example of use case (mine ^^) would be
> > > > * the creation of timeseries from a cold source containing
> timestamped
> > > > events (like S3).
> > > > * one these timeseries have cells being the mean (sum, count, ...) =
of
> > one
> > > > of the fields of the event
> > > > * the mean has to be computed over a window depending on a field
> > > > *timestamp*.
> > > >
> > > > * a timeserie is created for each type of event (the number of type=
s
> is
> > > > high)
> > > > So, in this case, it'd be interesting to have an RDD for each cell,
> > which
> > > > will generate all cells for all neede timeseries.
> > > > It's more or less what DStream does, but here it won't help due wha=
t
> > was
> > > > stated above.
> > > >
> > > > That's how I came to a raw sketch of what could be named
> ContinuousRDD
> > > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> > > simplicity
> > > > I've stuck with the definition of a DStream to think about it. Okay=
,
> > > let's
> > > > go ^^.
> > > >
> > > >
> > > > Looking at the DStream contract, here is something that could be
> > drafted
> > > > around CRDD.
> > > > A *CRDD* would be a generalized concept that relies on:
> > > > * a reference space/continuum (to which data can be bound)
> > > > * a binning function that can breaks the continuum into splits.
> > > > Since *Space* is a continuum we could define it as:
> > > > * a *SpacePoint* (the origin)
> > > > * a SpacePoint=3D>SpacePoint (the continuous function)
> > > > * a Ordering[SpacePoint]
> > > >
> > > > DStream uses a *JobGenerator* along with a DStreamGraph, which are
> > using
> > > > timer and clock to do their work, in the case of a CRDD we'll have =
to
> > > > define also a point generator, as a more generic but also adaptable
> > > > concept.
> > > >
> > > >
> > > > So far (so good?), these definition should work quite fine for
> > *ordered*
> > > > space
> > > > for which:
> > > > * points are coming/fetched in order
> > > > * the space is fully filled (no gaps)
> > > > For these cases, the JobGenerator (f.i.) could be defined with two
> > extra
> > > > functions:
> > > > * one is responsible to chop the batches even if the upper bound of
> the
> > > > batch hasn't been seen yet
> > > > * the other is responsible to handle outliers (and could wrap them
> into
> > > yet
> > > > another CRDD ?)
> > > >
> > > >
> > > > I created a gist here wrapping up the types and thus the skeleton o=
f
> > this
> > > > idea, you can find it here:
> > > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> > > >
> > > > WDYT?
> > > > *The answer can be: you're a fool!*
> > > > Actually, I already I am, but also I like to know why.... so some
> > > > explanations will help me :-D.
> > > >
> > > > Thanks to read 'till this point.
> > > >
> > > > Greetz,
> > > >
> > > >
> > > >
> > > >  a=E2=84=95dy =E2=84=99etrella
> > > > about.me/noootsab
> > > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> > > >
> > > > <http://about.me/noootsab>
> > > >
> > >
> >
>

--047d7b3a84a48e987a04fe57a552--

From dev-return-8396-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 16 23:12:08 2014
Return-Path: <dev-return-8396-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 21A441165C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 16 Jul 2014 23:12:08 +0000 (UTC)
Received: (qmail 14157 invoked by uid 500); 16 Jul 2014 23:12:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14106 invoked by uid 500); 16 Jul 2014 23:12:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14074 invoked by uid 99); 16 Jul 2014 23:12:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 23:12:06 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andy.petrella@gmail.com designates 209.85.215.46 as permitted sender)
Received: from [209.85.215.46] (HELO mail-la0-f46.google.com) (209.85.215.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 16 Jul 2014 23:12:04 +0000
Received: by mail-la0-f46.google.com with SMTP id b8so1159290lan.33
        for <dev@spark.incubator.apache.org>; Wed, 16 Jul 2014 16:11:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=c1bLI+Gdv3YIIRiZJhQU8Sti7AjNeXHPoIGO5KvdxwU=;
        b=A1NFBLLo8mr5DG13U40x1i/krIkPVB3TWVFIL1TDxj98kiIKDesrKkk/pU+CTxVxLm
         UOQVLTMJdiquoFtygo8m77ELTTwp6kBRlR09UKyy2O/pSm3ACLq5vUM3YjUlp7d7nuIm
         ptnkUOww/n7TGSZryK/jx5TamNYXqwbjRg4GqRDE9QOg75fpsFxyIdE4YHqHkI3Veff2
         bg4zyjsoqgseO4a1mh4SYg6UhQb9Blz511ZWkvU32Vzz2t83Yu0rjLn//XSI8JYny73l
         EVbSLxWycyDZYasxTx5i1w+XQBUevQ9GrFmLeehrsgVnjbPpYHWc7bvhn4R5KAAqbo11
         j0+Q==
X-Received: by 10.112.130.196 with SMTP id og4mr27403911lbb.38.1405552300036;
 Wed, 16 Jul 2014 16:11:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.112.162.5 with HTTP; Wed, 16 Jul 2014 16:11:19 -0700 (PDT)
In-Reply-To: <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
References: <CAKn3j0v=tbBU-AGbKmEtd08MEKNhDwqrJWPto=3RN42420CTkQ@mail.gmail.com>
 <CAMwrk0mYe06zZOY5Dai8jxEWJgW0YPO6vO3Z+4Lr2pNauj6dbQ@mail.gmail.com>
 <CAKn3j0uyQjAd0Kve0pVAR8nu8W15J6jKe7SuJt9dR+fSg-1g0w@mail.gmail.com> <CAMwrk0nUPKKVcOmUSua7c6QTv=p6s2UgOUQ_Q7LTxZ9XTyexew@mail.gmail.com>
From: andy petrella <andy.petrella@gmail.com>
Date: Thu, 17 Jul 2014 01:11:19 +0200
Message-ID: <CAKn3j0sjwbJNstnAjZmQpmJKk55wekx6O+3FiukSaEp2ERS4Ng@mail.gmail.com>
Subject: Re: [brainsotrming] Generalization of DStream, a ContinuousRDD ?
To: dev@spark.apache.org
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a84a48e987a04fe57a552
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a84a48e987a04fe57a552
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Indeed, these two cases are tightly coupled (the first one is a special
case of the second).

Actually, these "outliers" could be handled by a dedicated function what I
named outliersManager -- I was not so much inspired ^^, but we could name
these outliers, "outlaws" and thus the function would be "sheriff".
The purpose of this "sheriff" function would be to create yet another
distributed collection (RDD, CRDD, ...?) with only the --outliers-- outlaws
in it.

Because these problems have a nature which will be as different as the use
case will be, it's hard to find a generic way to tackle them. So, you
know... that's why... I put temporarily them in jail and wait for the judge
to show them the right path! (.... okay it's late in Belgium -- 1AM).

All in all, it's more or less what we would do in DStream as well actually.
Let me expand a bit this reasoning, let's assume that some data points can
come along with the time, but aren't in sync with it -- f.i., a device that
wakes up and send all it's data at once.
The DStream will package them into RDDs mixed-up with true current data
points, however, the logic of the job will have to use a 'Y' road :
* to integrate them into a database at the right place
* to simply drop them out because they're won't be part of a shown chart
* etc

In this case, the 'Y' road would be of the contract ;-), and so left at the
appreciation of the dev.

Another way, to do it would be to ignore but log them, but that would be
very crappy, non professional and useful (and of course I'm just kidding).

my0.002=C2=A2



 a=E2=84=95dy =E2=84=99etrella
about.me/noootsab
[image: a=E2=84=95dy =E2=84=99etrella on about.me]

<http://about.me/noootsab>


On Thu, Jul 17, 2014 at 12:31 AM, Tathagata Das <tathagata.das1565@gmail.co=
m
> wrote:

> I think it makes sense, though without a concrete implementation its hard
> to be sure. Applying sorting on the RDD according to the RDDs makes sense=
,
> but I can think of two kinds of fundamental problems.
>
> 1. How do you deal with ordering across RDD boundaries. Say two consecuti=
ve
> RDDs in the DStream has the following record timestamps    RDD1: [ 1, 2, =
3,
> 4, 6, 7 ]   RDD 2: [ 5, 8, 9, 10] . And you want to run a function throug=
h
> all these records in the timestamp order. I am curious to find how this
> problem can be solved without sacrificing efficiency (e.g. I can imagine
> doing multiple pass magic)
>
> 2. An even more fundamental question is how do you ensure ordering with
> delayed records. If you want to process in order of application time, and
> records are delayed how do you deal with them.
>
> Any ideas? ;)
>
> TD
>
>
>
> On Wed, Jul 16, 2014 at 2:37 AM, andy petrella <andy.petrella@gmail.com>
> wrote:
>
> > Heya TD,
> >
> > Thanks for the detailed answer! Much appreciated.
> >
> > Regarding order among elements within an RDD, you're definitively right=
,
> > it'd kill the //ism and would require synchronization which is complete=
ly
> > avoided in distributed env.
> >
> > That's why, I won't push this constraint to the RDDs themselves actuall=
y,
> > only the Space is something that *defines* ordered elements, and thus
> there
> > are two functions that will break the RDDs based on a given (extensible=
,
> > plugable) heuristic f.i.
> > Since the Space is rather decoupled from the data, thus the source and
> the
> > partitions, it's the responsibility of the CRRD implementation to dicta=
te
> > how (if necessary) the elements should be sorted in the RDDs... which
> will
> > require some shuffles :-s -- Or the couple (source, space) is something
> > intrinsically ordered (like it is for DStream).
> >
> > To be more concrete an RDD would be composed of un-ordered iterator of
> > millions of events for which all timestamps land into the same time
> > interval.
> >
> > WDYT, would that makes sense?
> >
> > thanks again for the answer!
> >
> > greetz
> >
> >  a=E2=84=95dy =E2=84=99etrella
> > about.me/noootsab
> > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> >
> > <http://about.me/noootsab>
> >
> >
> > On Wed, Jul 16, 2014 at 12:33 AM, Tathagata Das <
> > tathagata.das1565@gmail.com
> > > wrote:
> >
> > > Very interesting ideas Andy!
> > >
> > > Conceptually i think it makes sense. In fact, it is true that dealing
> > with
> > > time series data, windowing over application time, windowing over
> number
> > of
> > > events, are things that DStream does not natively support. The real
> > > challenge is actually mapping the conceptual windows with the
> underlying
> > > RDD model. On aspect you correctly observed in the ordering of events
> > > within the RDDs of the DStream. Another fundamental aspect is the fac=
t
> > that
> > > RDDs as parallel collections, with no well-defined ordering in the
> > records
> > > in the RDDs. If you want to process the records in an RDD as a ordere=
d
> > > stream of events, you kind of have to process the stream sequentially=
,
> > > which means you have to process each RDD partition one-by-one, and
> > > therefore lose the parallelism. So implementing all these functionali=
ty
> > may
> > > mean adding functionality at the cost of performance. Whether that is
> > okay
> > > for Spark Streaming to have these OR this tradeoff is not-intuitive f=
or
> > > end-users and therefore should not come out-of-the-box with Spark
> > Streaming
> > > -- that is a definitely a question worth debating upon.
> > >
> > > That said, for some limited usecases, like windowing over N events, c=
an
> > be
> > > implemented using custom RDDs like SlidingRDD
> > > <
> > >
> >
> https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apac=
he/spark/mllib/rdd/SlidingRDD.scala
> > > >
> > > without
> > > losing parallelism. For things like app time based windows, and
> > > random-application-event based windows, its much harder.
> > >
> > > Interesting ideas nonetheless. I am curious to see how far we can pus=
h
> > > using the RDD model underneath, without losing parallelism and
> > performance.
> > >
> > > TD
> > >
> > >
> > >
> > > On Tue, Jul 15, 2014 at 10:11 AM, andy petrella <
> andy.petrella@gmail.com
> > >
> > > wrote:
> > >
> > > > Dear Sparkers,
> > > >
> > > > *[sorry for the lengthy email... =3D> head to the gist
> > > > <https://gist.github.com/andypetrella/12228eb24eea6b3e1389> for a
> > > preview
> > > > :-p**]*
> > > >
> > > > I would like to share some thinking I had due to a use case I faced=
.
> > > > Basically, as the subject announced it, it's a generalization of th=
e
> > > > DStream currently available in the streaming project.
> > > > First of all, I'd like to say that it's only a result of some
> personal
> > > > thinking, alone in the dark with a use case, the spark code, a shee=
t
> of
> > > > paper and a poor pen.
> > > >
> > > >
> > > > DStream is a very great concept to deal with micro-batching use
> cases,
> > > and
> > > > it does it very well too!
> > > > Also, it hardly relies on the elapsing time to create its internal
> > > > micro-batches.
> > > > However, there are similar use cases where we need micro-batches
> where
> > > this
> > > > constraint on the time doesn't hold, here are two of them:
> > > > * a micro-batch has to be created every *n* events received
> > > > * a micro-batch has to be generate based on the values of the items
> > > pushed
> > > > by the source (which might even not be a stream!).
> > > >
> > > > An example of use case (mine ^^) would be
> > > > * the creation of timeseries from a cold source containing
> timestamped
> > > > events (like S3).
> > > > * one these timeseries have cells being the mean (sum, count, ...) =
of
> > one
> > > > of the fields of the event
> > > > * the mean has to be computed over a window depending on a field
> > > > *timestamp*.
> > > >
> > > > * a timeserie is created for each type of event (the number of type=
s
> is
> > > > high)
> > > > So, in this case, it'd be interesting to have an RDD for each cell,
> > which
> > > > will generate all cells for all neede timeseries.
> > > > It's more or less what DStream does, but here it won't help due wha=
t
> > was
> > > > stated above.
> > > >
> > > > That's how I came to a raw sketch of what could be named
> ContinuousRDD
> > > > (CRDD) which is basically and RDD[RDD[_]]. And, for the sake of
> > > simplicity
> > > > I've stuck with the definition of a DStream to think about it. Okay=
,
> > > let's
> > > > go ^^.
> > > >
> > > >
> > > > Looking at the DStream contract, here is something that could be
> > drafted
> > > > around CRDD.
> > > > A *CRDD* would be a generalized concept that relies on:
> > > > * a reference space/continuum (to which data can be bound)
> > > > * a binning function that can breaks the continuum into splits.
> > > > Since *Space* is a continuum we could define it as:
> > > > * a *SpacePoint* (the origin)
> > > > * a SpacePoint=3D>SpacePoint (the continuous function)
> > > > * a Ordering[SpacePoint]
> > > >
> > > > DStream uses a *JobGenerator* along with a DStreamGraph, which are
> > using
> > > > timer and clock to do their work, in the case of a CRDD we'll have =
to
> > > > define also a point generator, as a more generic but also adaptable
> > > > concept.
> > > >
> > > >
> > > > So far (so good?), these definition should work quite fine for
> > *ordered*
> > > > space
> > > > for which:
> > > > * points are coming/fetched in order
> > > > * the space is fully filled (no gaps)
> > > > For these cases, the JobGenerator (f.i.) could be defined with two
> > extra
> > > > functions:
> > > > * one is responsible to chop the batches even if the upper bound of
> the
> > > > batch hasn't been seen yet
> > > > * the other is responsible to handle outliers (and could wrap them
> into
> > > yet
> > > > another CRDD ?)
> > > >
> > > >
> > > > I created a gist here wrapping up the types and thus the skeleton o=
f
> > this
> > > > idea, you can find it here:
> > > > https://gist.github.com/andypetrella/12228eb24eea6b3e1389
> > > >
> > > > WDYT?
> > > > *The answer can be: you're a fool!*
> > > > Actually, I already I am, but also I like to know why.... so some
> > > > explanations will help me :-D.
> > > >
> > > > Thanks to read 'till this point.
> > > >
> > > > Greetz,
> > > >
> > > >
> > > >
> > > >  a=E2=84=95dy =E2=84=99etrella
> > > > about.me/noootsab
> > > > [image: a=E2=84=95dy =E2=84=99etrella on about.me]
> > > >
> > > > <http://about.me/noootsab>
> > > >
> > >
> >
>

--047d7b3a84a48e987a04fe57a552--

From dev-return-8397-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 00:18:53 2014
Return-Path: <dev-return-8397-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 49A6A11892
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 00:18:53 +0000 (UTC)
Received: (qmail 70894 invoked by uid 500); 17 Jul 2014 00:18:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70824 invoked by uid 500); 17 Jul 2014 00:18:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70811 invoked by uid 99); 17 Jul 2014 00:18:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 00:18:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 74.125.82.175 as permitted sender)
Received: from [74.125.82.175] (HELO mail-we0-f175.google.com) (74.125.82.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 00:18:47 +0000
Received: by mail-we0-f175.google.com with SMTP id t60so1685318wes.34
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 17:18:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=0nvAuzZmvjmQk/+8XELCai6qbqQH4HnbfmexK7O1+Gs=;
        b=DfEreFvm/c+dsCzWEifuXT7O+bQLFG7w4rMSmX9bCjepGBaoIPHK5c5ItdARCEMFSD
         xQNWHHacdcA8xLBfomJypjeFN1J3fft4ZTTyWWpI1QJkmzGSK7QvGoSb5WRKpWsmTRbn
         T5INH2NAI6u83cDd64p6NFt9CINMI0D+B6Owc3xj2wDwqoxVvaHA2rgIXDDol1P4U2c/
         w+rSCMe/AZb4gMCn427b/91MPNXyjXSp1r3ksjempbbhy4S76TlXeUeFItZhKcU+JWc5
         dEttmI1D0j93aX5bh6HGMfSZL0UmqoYLq14CPEFuUmjQshdJydBD/SPYLF+bllcjtli7
         7Z9w==
X-Gm-Message-State: ALoCoQmSv+yxEKe3ZZFh3lq4xfcMBvhxMO0Oq/fmf+LrexK1L0Nqlp6NZaFd1dNDf1GJC+V3KUH4
MIME-Version: 1.0
X-Received: by 10.180.8.10 with SMTP id n10mr17848335wia.41.1405556306398;
 Wed, 16 Jul 2014 17:18:26 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Wed, 16 Jul 2014 17:18:26 -0700 (PDT)
In-Reply-To: <CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
	<CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
	<CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
Date: Wed, 16 Jul 2014 17:18:26 -0700
Message-ID: <CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=14dae9cc97c85aea3604fe58940e
X-Virus-Checked: Checked by ClamAV on apache.org

--14dae9cc97c85aea3604fe58940e
Content-Type: text/plain; charset=UTF-8

Hi, Sandy

    We do have some issue with this. The difference is in Yarn-Alpha and
Yarn Stable ( I noticed that in the latest build, the module name has
changed,
     yarn-alpha --> yarn
     yarn --> yarn-stable
)

For example:  MRJobConfig.class
the field:
"DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"


In Yarn-Alpha : the field returns   java.lang.String[]

  java.lang.String[] DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;

while in Yarn-Stable, it returns a String

  java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;

So in ClientBaseSuite.scala

The following code:

    val knownDefMRAppCP: Seq[String] =
      getFieldValue[*String*, Seq[String]](classOf[MRJobConfig],

 "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
                                         Seq[String]())(a => *a.split(",")*)


works for yarn-stable, but doesn't work for yarn-alpha.

This is the only failure for the SNAPSHOT I downloaded 2 weeks ago.  I
believe this can be refactored to yarn-alpha module and make different
tests according different API signatures.

 I just update the master branch and build doesn't even compile for
Yarn-Alpha (yarn) model. Yarn-Stable compile with no error and test passed.


Does the Spark Jenkins job run against yarn-alpha ?





Here is output from yarn-alpha compilation:

I got the 40 compilation errors.

sbt/sbt clean yarn/test:compile

Using /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home as
default JAVA_HOME.

Note, this will be overridden by -java-home if it is set.

[info] Loading project definition from
/Users/chester/projects/spark/project/project

[info] Loading project definition from
/Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project

[warn] Multiple resolvers having different access mechanism configured with
same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).

[info] Loading project definition from /Users/chester/projects/spark/project

NOTE: SPARK_HADOOP_VERSION is deprecated, please use
-Dhadoop.version=2.0.5-alpha

NOTE: SPARK_YARN is deprecated, please use -Pyarn flag.

[info] Set current project to spark-parent (in build
file:/Users/chester/projects/spark/)

[success] Total time: 0 s, completed Jul 16, 2014 5:13:06 PM

[info] Updating {file:/Users/chester/projects/spark/}core...

[info] Resolving org.fusesource.jansi#jansi;1.4 ...

[info] Done updating.

[info] Updating {file:/Users/chester/projects/spark/}yarn...

[info] Updating {file:/Users/chester/projects/spark/}yarn-stable...

[info] Resolving org.fusesource.jansi#jansi;1.4 ...

[info] Done updating.

[info] Resolving commons-net#commons-net;3.1 ...

[info] Compiling 358 Scala sources and 34 Java sources to
/Users/chester/projects/spark/core/target/scala-2.10/classes...

[info] Resolving org.fusesource.jansi#jansi;1.4 ...

[info] Done updating.

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.scala:43:
constructor TaskAttemptID in class TaskAttemptID is deprecated: see
corresponding Javadoc for more information.

[warn]     new TaskAttemptID(jtIdentifier, jobId, isMap, taskId, attemptId)

[warn]     ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:501:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewHadoopJob(hadoopConfiguration)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:634:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewHadoopJob(conf)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:167:
constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
for more information.

[warn]         new TaskAttemptID(new TaskID(jID.value, true, splitID),
attemptID))

[warn]                           ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:188:
method makeQualified in class Path is deprecated: see corresponding Javadoc
for more information.

[warn]     outputPath.makeQualified(fs)

[warn]                ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:84:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]     if (!fs.getFileStatus(path).isDir) {

[warn]                                 ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:118:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]       val logDirs = if (logStatus != null)
logStatus.filter(_.isDir).toSeq else Seq[FileStatus]()

[warn]                                                               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:56:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]       if (file.isDir) 0L else file.getLen

[warn]                ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:110:
method getDefaultReplication in class FileSystem is deprecated: see
corresponding Javadoc for more information.

[warn]       fs.create(tempOutputPath, false, bufferSize,
fs.getDefaultReplication, blockSize)

[warn]                                                       ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:267:
constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
for more information.

[warn]     val taId = new TaskAttemptID(new TaskID(jobID, true, splitId),
attemptId)

[warn]                                  ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:767:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewAPIHadoopJob(hadoopConf)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:830:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new NewAPIHadoopJob(hadoopConf)

[warn]               ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:185:
method isDir in class FileStatus is deprecated: see corresponding Javadoc
for more information.

[warn]           fileStatuses.filter(!_.isDir).map(_.getPath).toSeq

[warn]                                  ^

[warn]
/Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106:
constructor Job in class Job is deprecated: see corresponding Javadoc for
more information.

[warn]     val job = new Job(conf)

[warn]               ^

[warn] 14 warnings found

[warn] Note:
/Users/chester/projects/spark/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
uses unchecked or unsafe operations.

[warn] Note: Recompile with -Xlint:unchecked for details.

[info] Compiling 15 Scala sources to
/Users/chester/projects/spark/yarn/stable/target/scala-2.10/classes...

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:26:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.YarnClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:40:
not found: value YarnClient

[error]   val yarnClient = YarnClient.createYarnClient

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:32:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:33:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:36:
object util is not a member of package org.apache.hadoop.yarn.webapp

[error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:63:
value RM_AM_MAX_ATTEMPTS is not a member of object
org.apache.hadoop.yarn.conf.YarnConfiguration

[error]     YarnConfiguration.RM_AM_MAX_ATTEMPTS,
YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS)

[error]                       ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:65:
not found: type AMRMClient

[error]   private var amClient: AMRMClient[ContainerRequest] = _

[error]                         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:91:
not found: value AMRMClient

[error]     amClient = AMRMClient.createAMRMClient()

[error]                ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:136:
not found: value WebAppUtils

[error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:40:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:618:
not found: type AMRMClient

[error]       amClient: AMRMClient[ContainerRequest],

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:596:
not found: type AMRMClient

[error]       amClient: AMRMClient[ContainerRequest],

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:577:
not found: type AMRMClient

[error]       amClient: AMRMClient[ContainerRequest],

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:452:
value CONTAINER_ID is not a member of object
org.apache.hadoop.yarn.api.ApplicationConstants.Environment

[error]     val containerIdString = System.getenv(
ApplicationConstants.Environment.CONTAINER_ID.name())

[error]
        ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:128:
value setTokens is not a member of
org.apache.hadoop.yarn.api.records.ContainerLaunchContext

[error]     amContainer.setTokens(ByteBuffer.wrap(dob.getData()))

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:36:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:37:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:39:
object util is not a member of package org.apache.hadoop.yarn.webapp

[error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:62:
not found: type AMRMClient

[error]   private var amClient: AMRMClient[ContainerRequest] = _

[error]                         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:99:
not found: value AMRMClient

[error]     amClient = AMRMClient.createAMRMClient()

[error]                ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:158:
not found: value WebAppUtils

[error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:31:
object ProtoUtils is not a member of package
org.apache.hadoop.yarn.api.records.impl.pb

[error] import org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils

[error]        ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:33:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.NMClient

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:53:
not found: type NMClient

[error]   var nmClient: NMClient = _

[error]                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:59:
not found: value NMClient

[error]     nmClient = NMClient.createNMClient()

[error]                ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:79:
value setTokens is not a member of
org.apache.hadoop.yarn.api.records.ContainerLaunchContext

[error]     ctx.setTokens(ByteBuffer.wrap(dob.getData()))

[error]         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:35:
object ApplicationMasterProtocol is not a member of package
org.apache.hadoop.yarn.api

[error] import org.apache.hadoop.yarn.api.ApplicationMasterProtocol

[error]        ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:41:
object api is not a member of package org.apache.hadoop.yarn.client

[error] import org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest

[error]                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:65:
not found: type AMRMClient

[error]     val amClient: AMRMClient[ContainerRequest],

[error]                   ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:389:
not found: type ContainerRequest

[error]     ): ArrayBuffer[ContainerRequest] = {

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:388:
not found: type ContainerRequest

[error]       hostContainers: ArrayBuffer[ContainerRequest]

[error]                                   ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:405:
not found: type ContainerRequest

[error]     val requestedContainers = new
ArrayBuffer[ContainerRequest](rackToCounts.size)

[error]                                               ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:434:
not found: type ContainerRequest

[error]     val containerRequests: List[ContainerRequest] =

[error]                                 ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:508:
not found: type ContainerRequest

[error]     ): ArrayBuffer[ContainerRequest] = {

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:446:
not found: type ContainerRequest

[error]         val hostContainerRequests = new
ArrayBuffer[ContainerRequest](preferredHostToCount.size)

[error]                                                     ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:458:
not found: type ContainerRequest

[error]         val rackContainerRequests: List[ContainerRequest] =
createRackResourceRequests(

[error]                                         ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:467:
not found: type ContainerRequest

[error]         val containerRequestBuffer = new
ArrayBuffer[ContainerRequest](

[error]                                                      ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:542:
not found: type ContainerRequest

[error]     ): ArrayBuffer[ContainerRequest] = {

[error]                    ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:545:
value newInstance is not a member of object
org.apache.hadoop.yarn.api.records.Resource

[error]     val resource = Resource.newInstance(memoryRequest,
executorCores)

[error]                             ^

[error]
/Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:550:
not found: type ContainerRequest

[error]     val requests = new ArrayBuffer[ContainerRequest]()

[error]                                    ^

[error] 40 errors found

[error] (yarn-stable/compile:compile) Compilation failed

[error] Total time: 98 s, completed Jul 16, 2014 5:14:44 PM













On Wed, Jul 16, 2014 at 4:19 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> Hi Ron,
>
> I just checked and this bug is fixed in recent releases of Spark.
>
> -Sandy
>
>
> On Sun, Jul 13, 2014 at 8:15 PM, Chester Chen <chester@alpinenow.com>
> wrote:
>
>> Ron,
>>     Which distribution and Version of Hadoop are you using ?
>>
>>      I just looked at CDH5 (  hadoop-mapreduce-client-core-
>> 2.3.0-cdh5.0.0),
>>
>> MRJobConfig does have the field :
>>
>> java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>
>> Chester
>>
>>
>>
>> On Sun, Jul 13, 2014 at 6:49 PM, Ron Gonzalez <zlgonzalez@yahoo.com>
>> wrote:
>>
>>> Hi,
>>>   I was doing programmatic submission of Spark yarn jobs and I saw code
>>> in ClientBase.getDefaultYarnApplicationClasspath():
>>>
>>> val field =
>>> classOf[MRJobConfig].getField("DEFAULT_YARN_APPLICATION_CLASSPATH)
>>> MRJobConfig doesn't have this field so the created launch env is
>>> incomplete. Workaround is to set yarn.application.classpath with the value
>>> from YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH.
>>>
>>> This results in having the spark job hang if the submission config is
>>> different from the default config. For example, if my resource manager port
>>> is 8050 instead of 8030, then the spark app is not able to register itself
>>> and stays in ACCEPTED state.
>>>
>>> I can easily fix this by changing this to YarnConfiguration instead of
>>> MRJobConfig but was wondering what the steps are for submitting a fix.
>>>
>>> Thanks,
>>> Ron
>>>
>>> Sent from my iPhone
>>
>>
>>
>

--14dae9cc97c85aea3604fe58940e--

From dev-return-8398-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 00:38:54 2014
Return-Path: <dev-return-8398-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 78D751193C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 00:38:54 +0000 (UTC)
Received: (qmail 15591 invoked by uid 500); 17 Jul 2014 00:38:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15544 invoked by uid 500); 17 Jul 2014 00:38:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15526 invoked by uid 99); 17 Jul 2014 00:38:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 00:38:53 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yanfang724@gmail.com designates 209.85.216.48 as permitted sender)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 00:38:50 +0000
Received: by mail-qa0-f48.google.com with SMTP id m5so1319199qaj.7
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 17:38:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=4e9ErZH86B05jW7ZcpEm8rGRVuuzhNyM4Xc7W9UmlFQ=;
        b=Tn6mJzQXNCU8yB31WGJmgPgCW8N1VHc5uZmUimYuRLOiemO/9aVG4nEiwQFW1zNrSJ
         WeOB8s8xbjHckVPWnGEfuCWs93Mh0ndkZs1yfy/Pf0vRiS2XGubzy6H06ygKba3wugYG
         jHELCtP4Ss/KpuRMO2cCUlW06WW/YwrfLLjOI+zMxIRL+jXrX93SxCZ7LCuS/LFwhqus
         TCldQoGNipwArC33fUjqkq621nlznrTUkr2RUyfXIojVtw/zXDHsGHXOGx06RsauIZvK
         hMjEKL2b15u8cApeN+BBNfGUjSl8s7IOPO3RqmPUriQLJXnX1W9gJBEz6xiGGf0eAfgS
         wZbA==
X-Received: by 10.140.21.9 with SMTP id 9mr48442313qgk.39.1405557506255; Wed,
 16 Jul 2014 17:38:26 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.22.239 with HTTP; Wed, 16 Jul 2014 17:38:06 -0700 (PDT)
From: Yan Fang <yanfang724@gmail.com>
Date: Wed, 16 Jul 2014 17:38:06 -0700
Message-ID: <CAOErhNTsPtVq7wKR_6zrsLujjMrDwGzrSw_Gyr0ZJSw25V=wtQ@mail.gmail.com>
Subject: Does RDD checkpointing store the entire state in HDFS?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1566adf338604fe58dbc8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1566adf338604fe58dbc8
Content-Type: text/plain; charset=UTF-8

Hi guys,

am wondering how the RDD checkpointing
<https://spark.apache.org/docs/latest/streaming-programming-guide.html#RDD
Checkpointing> works in Spark Streaming. When I use updateStateByKey, does
the Spark store the entire state (at one time point) into the HDFS or only
put the transformation into the HDFS? Thank you.

Best,

Fang, Yan
yanfang724@gmail.com
+1 (206) 849-4108

--001a11c1566adf338604fe58dbc8--

From dev-return-8399-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 00:41:49 2014
Return-Path: <dev-return-8399-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7C9F61194E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 00:41:49 +0000 (UTC)
Received: (qmail 25088 invoked by uid 500); 17 Jul 2014 00:41:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25020 invoked by uid 500); 17 Jul 2014 00:41:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25009 invoked by uid 99); 17 Jul 2014 00:41:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 00:41:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of chester@alpinenow.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 00:41:44 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so1670241wgg.0
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 17:41:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=JsqPG4c6XHA1F0bgPmoWMsPnyRKnuCAwzzB/FSzD61Y=;
        b=G9xe9F5R56k1Vy4dwGNXgQtPyqDfyoxlEr4ZuFiH5yDKs+Ut8wgtxmf+2yLEs1txqf
         gcPD8bRA03V2UsU6LZGzuZDvuAKunMPn8kXrEGa4qX+RdP6t/Pj6ihktgy4xF8aoLeqK
         uVq8Uts2VAOeQvxd63WSiEXVfGkMhV1sgKt17FqcTi7lf2GtD26FkNstQPDccR1vtJue
         8V3mXLftUYDFlcOGzdvOrKSXwOveg2KjF9PJJVsZ2GcuxUu0eLUCLK5jXxtLsx5IVu+9
         5q7nZ71suo7Fm2bTxQXwBZPVD4SijIkHl70+LfzBDCoKpXZVdjNCUcHm40m4gaFyaetC
         Y5iw==
X-Gm-Message-State: ALoCoQmsHElllFXserta/pTYRulSwAYBVskg4ynP+qOJArxGiWSSuVz8CSF4xFTpT5NnZuekpXs/
MIME-Version: 1.0
X-Received: by 10.180.211.101 with SMTP id nb5mr17869257wic.53.1405557680142;
 Wed, 16 Jul 2014 17:41:20 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Wed, 16 Jul 2014 17:41:20 -0700 (PDT)
In-Reply-To: <CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
	<CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
	<CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
	<CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
Date: Wed, 16 Jul 2014 17:41:20 -0700
Message-ID: <CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c37cc63c974504fe58e6d0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37cc63c974504fe58e6d0
Content-Type: text/plain; charset=UTF-8

Hmm
looks like a Build script issue:

I run the command with :

sbt/sbt clean *yarn/*test:compile

but errors came from

[error] 40 errors found

[error] (*yarn-stable*/compile:compile) Compilation failed


Chester


On Wed, Jul 16, 2014 at 5:18 PM, Chester Chen <chester@alpinenow.com> wrote:

> Hi, Sandy
>
>     We do have some issue with this. The difference is in Yarn-Alpha and
> Yarn Stable ( I noticed that in the latest build, the module name has
> changed,
>      yarn-alpha --> yarn
>      yarn --> yarn-stable
> )
>
> For example:  MRJobConfig.class
> the field:
> "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"
>
>
> In Yarn-Alpha : the field returns   java.lang.String[]
>
>   java.lang.String[] DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>
> while in Yarn-Stable, it returns a String
>
>   java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>
> So in ClientBaseSuite.scala
>
> The following code:
>
>     val knownDefMRAppCP: Seq[String] =
>       getFieldValue[*String*, Seq[String]](classOf[MRJobConfig],
>
>  "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
>                                          Seq[String]())(a =>
> *a.split(",")*)
>
>
> works for yarn-stable, but doesn't work for yarn-alpha.
>
> This is the only failure for the SNAPSHOT I downloaded 2 weeks ago.  I
> believe this can be refactored to yarn-alpha module and make different
> tests according different API signatures.
>
>  I just update the master branch and build doesn't even compile for
> Yarn-Alpha (yarn) model. Yarn-Stable compile with no error and test passed.
>
>
> Does the Spark Jenkins job run against yarn-alpha ?
>
>
>
>
>
> Here is output from yarn-alpha compilation:
>
> I got the 40 compilation errors.
>
> sbt/sbt clean yarn/test:compile
>
> Using /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home as
> default JAVA_HOME.
>
> Note, this will be overridden by -java-home if it is set.
>
> [info] Loading project definition from
> /Users/chester/projects/spark/project/project
>
> [info] Loading project definition from
> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
>
> [warn] Multiple resolvers having different access mechanism configured
> with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
> project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).
>
> [info] Loading project definition from
> /Users/chester/projects/spark/project
>
> NOTE: SPARK_HADOOP_VERSION is deprecated, please use
> -Dhadoop.version=2.0.5-alpha
>
> NOTE: SPARK_YARN is deprecated, please use -Pyarn flag.
>
> [info] Set current project to spark-parent (in build
> file:/Users/chester/projects/spark/)
>
> [success] Total time: 0 s, completed Jul 16, 2014 5:13:06 PM
>
> [info] Updating {file:/Users/chester/projects/spark/}core...
>
> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>
> [info] Done updating.
>
> [info] Updating {file:/Users/chester/projects/spark/}yarn...
>
> [info] Updating {file:/Users/chester/projects/spark/}yarn-stable...
>
> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>
> [info] Done updating.
>
> [info] Resolving commons-net#commons-net;3.1 ...
>
> [info] Compiling 358 Scala sources and 34 Java sources to
> /Users/chester/projects/spark/core/target/scala-2.10/classes...
>
> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>
> [info] Done updating.
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.scala:43:
> constructor TaskAttemptID in class TaskAttemptID is deprecated: see
> corresponding Javadoc for more information.
>
> [warn]     new TaskAttemptID(jtIdentifier, jobId, isMap, taskId,
> attemptId)
>
> [warn]     ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:501:
> constructor Job in class Job is deprecated: see corresponding Javadoc for
> more information.
>
> [warn]     val job = new NewHadoopJob(hadoopConfiguration)
>
> [warn]               ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:634:
> constructor Job in class Job is deprecated: see corresponding Javadoc for
> more information.
>
> [warn]     val job = new NewHadoopJob(conf)
>
> [warn]               ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:167:
> constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
> for more information.
>
> [warn]         new TaskAttemptID(new TaskID(jID.value, true, splitID),
> attemptID))
>
> [warn]                           ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:188:
> method makeQualified in class Path is deprecated: see corresponding Javadoc
> for more information.
>
> [warn]     outputPath.makeQualified(fs)
>
> [warn]                ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:84:
> method isDir in class FileStatus is deprecated: see corresponding Javadoc
> for more information.
>
> [warn]     if (!fs.getFileStatus(path).isDir) {
>
> [warn]                                 ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:118:
> method isDir in class FileStatus is deprecated: see corresponding Javadoc
> for more information.
>
> [warn]       val logDirs = if (logStatus != null)
> logStatus.filter(_.isDir).toSeq else Seq[FileStatus]()
>
> [warn]                                                               ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:56:
> method isDir in class FileStatus is deprecated: see corresponding Javadoc
> for more information.
>
> [warn]       if (file.isDir) 0L else file.getLen
>
> [warn]                ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:110:
> method getDefaultReplication in class FileSystem is deprecated: see
> corresponding Javadoc for more information.
>
> [warn]       fs.create(tempOutputPath, false, bufferSize,
> fs.getDefaultReplication, blockSize)
>
> [warn]                                                       ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:267:
> constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
> for more information.
>
> [warn]     val taId = new TaskAttemptID(new TaskID(jobID, true, splitId),
> attemptId)
>
> [warn]                                  ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:767:
> constructor Job in class Job is deprecated: see corresponding Javadoc for
> more information.
>
> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
>
> [warn]               ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:830:
> constructor Job in class Job is deprecated: see corresponding Javadoc for
> more information.
>
> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
>
> [warn]               ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:185:
> method isDir in class FileStatus is deprecated: see corresponding Javadoc
> for more information.
>
> [warn]           fileStatuses.filter(!_.isDir).map(_.getPath).toSeq
>
> [warn]                                  ^
>
> [warn]
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106:
> constructor Job in class Job is deprecated: see corresponding Javadoc for
> more information.
>
> [warn]     val job = new Job(conf)
>
> [warn]               ^
>
> [warn] 14 warnings found
>
> [warn] Note:
> /Users/chester/projects/spark/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
> uses unchecked or unsafe operations.
>
> [warn] Note: Recompile with -Xlint:unchecked for details.
>
> [info] Compiling 15 Scala sources to
> /Users/chester/projects/spark/yarn/stable/target/scala-2.10/classes...
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:26:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import org.apache.hadoop.yarn.client.api.YarnClient
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:40:
> not found: value YarnClient
>
> [error]   val yarnClient = YarnClient.createYarnClient
>
> [error]                    ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:32:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:33:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import
> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:36:
> object util is not a member of package org.apache.hadoop.yarn.webapp
>
> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:63:
> value RM_AM_MAX_ATTEMPTS is not a member of object
> org.apache.hadoop.yarn.conf.YarnConfiguration
>
> [error]     YarnConfiguration.RM_AM_MAX_ATTEMPTS,
> YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS)
>
> [error]                       ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:65:
> not found: type AMRMClient
>
> [error]   private var amClient: AMRMClient[ContainerRequest] = _
>
> [error]                         ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:91:
> not found: value AMRMClient
>
> [error]     amClient = AMRMClient.createAMRMClient()
>
> [error]                ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:136:
> not found: value WebAppUtils
>
> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
>
> [error]                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:40:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:618:
> not found: type AMRMClient
>
> [error]       amClient: AMRMClient[ContainerRequest],
>
> [error]                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:596:
> not found: type AMRMClient
>
> [error]       amClient: AMRMClient[ContainerRequest],
>
> [error]                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:577:
> not found: type AMRMClient
>
> [error]       amClient: AMRMClient[ContainerRequest],
>
> [error]                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:452:
> value CONTAINER_ID is not a member of object
> org.apache.hadoop.yarn.api.ApplicationConstants.Environment
>
> [error]     val containerIdString = System.getenv(
> ApplicationConstants.Environment.CONTAINER_ID.name())
>
> [error]
>           ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:128:
> value setTokens is not a member of
> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
>
> [error]     amContainer.setTokens(ByteBuffer.wrap(dob.getData()))
>
> [error]                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:36:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:37:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import
> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:39:
> object util is not a member of package org.apache.hadoop.yarn.webapp
>
> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:62:
> not found: type AMRMClient
>
> [error]   private var amClient: AMRMClient[ContainerRequest] = _
>
> [error]                         ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:99:
> not found: value AMRMClient
>
> [error]     amClient = AMRMClient.createAMRMClient()
>
> [error]                ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:158:
> not found: value WebAppUtils
>
> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
>
> [error]                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:31:
> object ProtoUtils is not a member of package
> org.apache.hadoop.yarn.api.records.impl.pb
>
> [error] import org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils
>
> [error]        ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:33:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import org.apache.hadoop.yarn.client.api.NMClient
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:53:
> not found: type NMClient
>
> [error]   var nmClient: NMClient = _
>
> [error]                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:59:
> not found: value NMClient
>
> [error]     nmClient = NMClient.createNMClient()
>
> [error]                ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:79:
> value setTokens is not a member of
> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
>
> [error]     ctx.setTokens(ByteBuffer.wrap(dob.getData()))
>
> [error]         ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:35:
> object ApplicationMasterProtocol is not a member of package
> org.apache.hadoop.yarn.api
>
> [error] import org.apache.hadoop.yarn.api.ApplicationMasterProtocol
>
> [error]        ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:41:
> object api is not a member of package org.apache.hadoop.yarn.client
>
> [error] import
> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>
> [error]                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:65:
> not found: type AMRMClient
>
> [error]     val amClient: AMRMClient[ContainerRequest],
>
> [error]                   ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:389:
> not found: type ContainerRequest
>
> [error]     ): ArrayBuffer[ContainerRequest] = {
>
> [error]                    ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:388:
> not found: type ContainerRequest
>
> [error]       hostContainers: ArrayBuffer[ContainerRequest]
>
> [error]                                   ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:405:
> not found: type ContainerRequest
>
> [error]     val requestedContainers = new
> ArrayBuffer[ContainerRequest](rackToCounts.size)
>
> [error]                                               ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:434:
> not found: type ContainerRequest
>
> [error]     val containerRequests: List[ContainerRequest] =
>
> [error]                                 ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:508:
> not found: type ContainerRequest
>
> [error]     ): ArrayBuffer[ContainerRequest] = {
>
> [error]                    ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:446:
> not found: type ContainerRequest
>
> [error]         val hostContainerRequests = new
> ArrayBuffer[ContainerRequest](preferredHostToCount.size)
>
> [error]                                                     ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:458:
> not found: type ContainerRequest
>
> [error]         val rackContainerRequests: List[ContainerRequest] =
> createRackResourceRequests(
>
> [error]                                         ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:467:
> not found: type ContainerRequest
>
> [error]         val containerRequestBuffer = new
> ArrayBuffer[ContainerRequest](
>
> [error]                                                      ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:542:
> not found: type ContainerRequest
>
> [error]     ): ArrayBuffer[ContainerRequest] = {
>
> [error]                    ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:545:
> value newInstance is not a member of object
> org.apache.hadoop.yarn.api.records.Resource
>
> [error]     val resource = Resource.newInstance(memoryRequest,
> executorCores)
>
> [error]                             ^
>
> [error]
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:550:
> not found: type ContainerRequest
>
> [error]     val requests = new ArrayBuffer[ContainerRequest]()
>
> [error]                                    ^
>
> [error] 40 errors found
>
> [error] (yarn-stable/compile:compile) Compilation failed
>
> [error] Total time: 98 s, completed Jul 16, 2014 5:14:44 PM
>
>
>
>
>
>
>
>
>
>
>
>
>
> On Wed, Jul 16, 2014 at 4:19 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
>
>> Hi Ron,
>>
>> I just checked and this bug is fixed in recent releases of Spark.
>>
>> -Sandy
>>
>>
>> On Sun, Jul 13, 2014 at 8:15 PM, Chester Chen <chester@alpinenow.com>
>> wrote:
>>
>>> Ron,
>>>     Which distribution and Version of Hadoop are you using ?
>>>
>>>      I just looked at CDH5 (  hadoop-mapreduce-client-core-
>>> 2.3.0-cdh5.0.0),
>>>
>>> MRJobConfig does have the field :
>>>
>>> java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>>
>>> Chester
>>>
>>>
>>>
>>> On Sun, Jul 13, 2014 at 6:49 PM, Ron Gonzalez <zlgonzalez@yahoo.com>
>>> wrote:
>>>
>>>> Hi,
>>>>   I was doing programmatic submission of Spark yarn jobs and I saw code
>>>> in ClientBase.getDefaultYarnApplicationClasspath():
>>>>
>>>> val field =
>>>> classOf[MRJobConfig].getField("DEFAULT_YARN_APPLICATION_CLASSPATH)
>>>> MRJobConfig doesn't have this field so the created launch env is
>>>> incomplete. Workaround is to set yarn.application.classpath with the value
>>>> from YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH.
>>>>
>>>> This results in having the spark job hang if the submission config is
>>>> different from the default config. For example, if my resource manager port
>>>> is 8050 instead of 8030, then the spark app is not able to register itself
>>>> and stays in ACCEPTED state.
>>>>
>>>> I can easily fix this by changing this to YarnConfiguration instead of
>>>> MRJobConfig but was wondering what the steps are for submitting a fix.
>>>>
>>>> Thanks,
>>>> Ron
>>>>
>>>> Sent from my iPhone
>>>
>>>
>>>
>>
>

--001a11c37cc63c974504fe58e6d0--

From dev-return-8400-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 01:54:38 2014
Return-Path: <dev-return-8400-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E83DD11B57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 01:54:37 +0000 (UTC)
Received: (qmail 67025 invoked by uid 500); 17 Jul 2014 01:54:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66964 invoked by uid 500); 17 Jul 2014 01:54:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66952 invoked by uid 99); 17 Jul 2014 01:54:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 01:54:36 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 01:54:31 +0000
Received: by mail-vc0-f174.google.com with SMTP id la4so3349021vcb.19
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 18:54:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=5murTQ3Z+GaRYXoDkWc6yajTbDIhFxxwYk331khIftU=;
        b=ytMAakVCdEDui6H2l1+pz/+TUQh/lxcpVwkbLbqhYfQpQkqzh0WpOhTBkJczZJYOL3
         M8WNGLSZ3DJVOkxQ92CII7JCnRBXUrCSIMaGeVrqXel35xlyXQxuoRlw1Rim5TpK4bnI
         9R2AKO8O9THRtzKYZ+3GNaUfhZDZyIHKZdvyOc98oXQHUnN2oA85ZjCW1E3WHzrUzRPt
         0mJIMLBHYryMYx0kxEpyVnP3XnRwjkBQyfzqOFflgBJTDvauEESYuz3mFC3Mr0Y+iydl
         EZArzzGynFXZTz0AoJ2Q6qClxEWL5uMbd9wsFCnGqrk7TVMqQy6r6SCVuuxM3Hs0mH2d
         Kdfg==
X-Received: by 10.52.64.140 with SMTP id o12mr11021733vds.70.1405562046570;
 Wed, 16 Jul 2014 18:54:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.131.39 with HTTP; Wed, 16 Jul 2014 18:53:36 -0700 (PDT)
In-Reply-To: <CAOErhNTsPtVq7wKR_6zrsLujjMrDwGzrSw_Gyr0ZJSw25V=wtQ@mail.gmail.com>
References: <CAOErhNTsPtVq7wKR_6zrsLujjMrDwGzrSw_Gyr0ZJSw25V=wtQ@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Wed, 16 Jul 2014 18:53:36 -0700
Message-ID: <CAMwrk0mncUa5ne4CR8tx1pubuVCAyTyusuzCLA5aZgmv_wSfTg@mail.gmail.com>
Subject: Re: Does RDD checkpointing store the entire state in HDFS?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=20cf3079bcc07ee74004fe59ea31
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3079bcc07ee74004fe59ea31
Content-Type: text/plain; charset=UTF-8

After every checkpointing interval, the latest state RDD is stored to HDFS
in its entirety. Along with that, the series of DStream transformations
that was setup with the streaming context is also stored into HDFS (the
whole DAG of DStream objects is serialized and saved).

TD


On Wed, Jul 16, 2014 at 5:38 PM, Yan Fang <yanfang724@gmail.com> wrote:

> Hi guys,
>
> am wondering how the RDD checkpointing
> <https://spark.apache.org/docs/latest/streaming-programming-guide.html#RDD
> Checkpointing> works in Spark Streaming. When I use updateStateByKey, does
> the Spark store the entire state (at one time point) into the HDFS or only
> put the transformation into the HDFS? Thank you.
>
> Best,
>
> Fang, Yan
> yanfang724@gmail.com
> +1 (206) 849-4108
>

--20cf3079bcc07ee74004fe59ea31--

From dev-return-8401-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 04:39:35 2014
Return-Path: <dev-return-8401-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C6ACC11F49
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 04:39:35 +0000 (UTC)
Received: (qmail 11687 invoked by uid 500); 17 Jul 2014 04:39:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11624 invoked by uid 500); 17 Jul 2014 04:39:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11596 invoked by uid 99); 17 Jul 2014 04:39:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 04:39:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of chester@alpinenow.com designates 74.125.82.179 as permitted sender)
Received: from [74.125.82.179] (HELO mail-we0-f179.google.com) (74.125.82.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 04:39:29 +0000
Received: by mail-we0-f179.google.com with SMTP id u57so1747940wes.10
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 21:39:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=v2AWdOhb3ISvUJGBxMCfIHSR7x77Lrrzonz2NIcjNhQ=;
        b=NO6ed0G15BlL+3kQUxjK+L2dg9gm+ZdhjpFc99gVtDhs0aXapCYGZIDgitS/pIppHF
         H7dNGMw2rzVvrnHtLthOg+tnj2/qvp4AqkmGIW1Z0l7CM+v49rXgstE2a2H7Os9Zj79t
         wIPhWdEEUv2/MaYdXljzkcaBgBbtUq/pFz621rR4G5IfJux1envUZidLCrjMrIYefI9O
         fLxlxsWHbHdEiJ2TPKAucMzR1CGKj0XVmw2OZHtoRTUYfnKHHESbeEXukN/Dz0k1pAT2
         BP2imVlPcMOJpp91tFky3zlWcVaKB4cOwM5RJLGKTw0rFW45U7ZMPxRCansuFZF+6cy3
         /3Xg==
X-Gm-Message-State: ALoCoQnZGmZvxAHKLtzfpZSw2SGaX2jKb8xRa8m5lGf1gKFTzjMUxUJVCqcvR6nIwnBRTu9NZVZ/
MIME-Version: 1.0
X-Received: by 10.180.89.34 with SMTP id bl2mr18117420wib.41.1405571945618;
 Wed, 16 Jul 2014 21:39:05 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Wed, 16 Jul 2014 21:39:05 -0700 (PDT)
In-Reply-To: <CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
	<CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
	<CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
	<CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
	<CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com>
Date: Wed, 16 Jul 2014 21:39:05 -0700
Message-ID: <CAPYnQ0W3ZpR46eGSiwBwXuoKDbkd6JU3iq3+trj-s5K2AsRVpw@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d04447e7b86721104fe5c381d
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04447e7b86721104fe5c381d
Content-Type: text/plain; charset=UTF-8

Looking further, the yarn and yarn-stable are both for the stable version
of Yarn, that explains the compilation errors when using 2.0.5-alpha
version of hadoop.

the module yarn-alpha ( although is still on SparkBuild.scala), is no
longer there in sbt console.


> projects

[info] In file:/Users/chester/projects/spark/

[info]    assembly

[info]    bagel

[info]    catalyst

[info]    core

[info]    examples

[info]    graphx

[info]    hive

[info]    mllib

[info]    oldDeps

[info]    repl

[info]    spark

[info]    sql

[info]    streaming

[info]    streaming-flume

[info]    streaming-kafka

[info]    streaming-mqtt

[info]    streaming-twitter

[info]    streaming-zeromq

[info]    tools

[info]    yarn

[info]  * yarn-stable


On Wed, Jul 16, 2014 at 5:41 PM, Chester Chen <chester@alpinenow.com> wrote:

> Hmm
> looks like a Build script issue:
>
> I run the command with :
>
> sbt/sbt clean *yarn/*test:compile
>
> but errors came from
>
> [error] 40 errors found
>
> [error] (*yarn-stable*/compile:compile) Compilation failed
>
>
> Chester
>
>
> On Wed, Jul 16, 2014 at 5:18 PM, Chester Chen <chester@alpinenow.com>
> wrote:
>
>> Hi, Sandy
>>
>>     We do have some issue with this. The difference is in Yarn-Alpha and
>> Yarn Stable ( I noticed that in the latest build, the module name has
>> changed,
>>      yarn-alpha --> yarn
>>      yarn --> yarn-stable
>> )
>>
>> For example:  MRJobConfig.class
>> the field:
>> "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"
>>
>>
>> In Yarn-Alpha : the field returns   java.lang.String[]
>>
>>   java.lang.String[] DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>
>> while in Yarn-Stable, it returns a String
>>
>>   java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>
>> So in ClientBaseSuite.scala
>>
>> The following code:
>>
>>     val knownDefMRAppCP: Seq[String] =
>>       getFieldValue[*String*, Seq[String]](classOf[MRJobConfig],
>>
>>  "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
>>                                          Seq[String]())(a =>
>> *a.split(",")*)
>>
>>
>> works for yarn-stable, but doesn't work for yarn-alpha.
>>
>> This is the only failure for the SNAPSHOT I downloaded 2 weeks ago.  I
>> believe this can be refactored to yarn-alpha module and make different
>> tests according different API signatures.
>>
>>  I just update the master branch and build doesn't even compile for
>> Yarn-Alpha (yarn) model. Yarn-Stable compile with no error and test passed.
>>
>>
>> Does the Spark Jenkins job run against yarn-alpha ?
>>
>>
>>
>>
>>
>> Here is output from yarn-alpha compilation:
>>
>> I got the 40 compilation errors.
>>
>> sbt/sbt clean yarn/test:compile
>>
>> Using /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home as
>> default JAVA_HOME.
>>
>> Note, this will be overridden by -java-home if it is set.
>>
>> [info] Loading project definition from
>> /Users/chester/projects/spark/project/project
>>
>> [info] Loading project definition from
>> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
>>
>> [warn] Multiple resolvers having different access mechanism configured
>> with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
>> project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).
>>
>> [info] Loading project definition from
>> /Users/chester/projects/spark/project
>>
>> NOTE: SPARK_HADOOP_VERSION is deprecated, please use
>> -Dhadoop.version=2.0.5-alpha
>>
>> NOTE: SPARK_YARN is deprecated, please use -Pyarn flag.
>>
>> [info] Set current project to spark-parent (in build
>> file:/Users/chester/projects/spark/)
>>
>> [success] Total time: 0 s, completed Jul 16, 2014 5:13:06 PM
>>
>> [info] Updating {file:/Users/chester/projects/spark/}core...
>>
>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>>
>> [info] Done updating.
>>
>> [info] Updating {file:/Users/chester/projects/spark/}yarn...
>>
>> [info] Updating {file:/Users/chester/projects/spark/}yarn-stable...
>>
>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>>
>> [info] Done updating.
>>
>> [info] Resolving commons-net#commons-net;3.1 ...
>>
>> [info] Compiling 358 Scala sources and 34 Java sources to
>> /Users/chester/projects/spark/core/target/scala-2.10/classes...
>>
>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>>
>> [info] Done updating.
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.scala:43:
>> constructor TaskAttemptID in class TaskAttemptID is deprecated: see
>> corresponding Javadoc for more information.
>>
>> [warn]     new TaskAttemptID(jtIdentifier, jobId, isMap, taskId,
>> attemptId)
>>
>> [warn]     ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:501:
>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>> more information.
>>
>> [warn]     val job = new NewHadoopJob(hadoopConfiguration)
>>
>> [warn]               ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:634:
>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>> more information.
>>
>> [warn]     val job = new NewHadoopJob(conf)
>>
>> [warn]               ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:167:
>> constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
>> for more information.
>>
>> [warn]         new TaskAttemptID(new TaskID(jID.value, true, splitID),
>> attemptID))
>>
>> [warn]                           ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:188:
>> method makeQualified in class Path is deprecated: see corresponding Javadoc
>> for more information.
>>
>> [warn]     outputPath.makeQualified(fs)
>>
>> [warn]                ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:84:
>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>> for more information.
>>
>> [warn]     if (!fs.getFileStatus(path).isDir) {
>>
>> [warn]                                 ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:118:
>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>> for more information.
>>
>> [warn]       val logDirs = if (logStatus != null)
>> logStatus.filter(_.isDir).toSeq else Seq[FileStatus]()
>>
>> [warn]                                                               ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:56:
>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>> for more information.
>>
>> [warn]       if (file.isDir) 0L else file.getLen
>>
>> [warn]                ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:110:
>> method getDefaultReplication in class FileSystem is deprecated: see
>> corresponding Javadoc for more information.
>>
>> [warn]       fs.create(tempOutputPath, false, bufferSize,
>> fs.getDefaultReplication, blockSize)
>>
>> [warn]                                                       ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:267:
>> constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
>> for more information.
>>
>> [warn]     val taId = new TaskAttemptID(new TaskID(jobID, true,
>> splitId), attemptId)
>>
>> [warn]                                  ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:767:
>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>> more information.
>>
>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
>>
>> [warn]               ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:830:
>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>> more information.
>>
>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
>>
>> [warn]               ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:185:
>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>> for more information.
>>
>> [warn]           fileStatuses.filter(!_.isDir).map(_.getPath).toSeq
>>
>> [warn]                                  ^
>>
>> [warn]
>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106:
>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>> more information.
>>
>> [warn]     val job = new Job(conf)
>>
>> [warn]               ^
>>
>> [warn] 14 warnings found
>>
>> [warn] Note:
>> /Users/chester/projects/spark/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
>> uses unchecked or unsafe operations.
>>
>> [warn] Note: Recompile with -Xlint:unchecked for details.
>>
>> [info] Compiling 15 Scala sources to
>> /Users/chester/projects/spark/yarn/stable/target/scala-2.10/classes...
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:26:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import org.apache.hadoop.yarn.client.api.YarnClient
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:40:
>> not found: value YarnClient
>>
>> [error]   val yarnClient = YarnClient.createYarnClient
>>
>> [error]                    ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:32:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:33:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import
>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:36:
>> object util is not a member of package org.apache.hadoop.yarn.webapp
>>
>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:63:
>> value RM_AM_MAX_ATTEMPTS is not a member of object
>> org.apache.hadoop.yarn.conf.YarnConfiguration
>>
>> [error]     YarnConfiguration.RM_AM_MAX_ATTEMPTS,
>> YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS)
>>
>> [error]                       ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:65:
>> not found: type AMRMClient
>>
>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
>>
>> [error]                         ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:91:
>> not found: value AMRMClient
>>
>> [error]     amClient = AMRMClient.createAMRMClient()
>>
>> [error]                ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:136:
>> not found: value WebAppUtils
>>
>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
>>
>> [error]                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:40:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:618:
>> not found: type AMRMClient
>>
>> [error]       amClient: AMRMClient[ContainerRequest],
>>
>> [error]                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:596:
>> not found: type AMRMClient
>>
>> [error]       amClient: AMRMClient[ContainerRequest],
>>
>> [error]                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:577:
>> not found: type AMRMClient
>>
>> [error]       amClient: AMRMClient[ContainerRequest],
>>
>> [error]                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:452:
>> value CONTAINER_ID is not a member of object
>> org.apache.hadoop.yarn.api.ApplicationConstants.Environment
>>
>> [error]     val containerIdString = System.getenv(
>> ApplicationConstants.Environment.CONTAINER_ID.name())
>>
>> [error]
>>           ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:128:
>> value setTokens is not a member of
>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
>>
>> [error]     amContainer.setTokens(ByteBuffer.wrap(dob.getData()))
>>
>> [error]                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:36:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:37:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import
>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:39:
>> object util is not a member of package org.apache.hadoop.yarn.webapp
>>
>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:62:
>> not found: type AMRMClient
>>
>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
>>
>> [error]                         ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:99:
>> not found: value AMRMClient
>>
>> [error]     amClient = AMRMClient.createAMRMClient()
>>
>> [error]                ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:158:
>> not found: value WebAppUtils
>>
>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
>>
>> [error]                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:31:
>> object ProtoUtils is not a member of package
>> org.apache.hadoop.yarn.api.records.impl.pb
>>
>> [error] import org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils
>>
>> [error]        ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:33:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import org.apache.hadoop.yarn.client.api.NMClient
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:53:
>> not found: type NMClient
>>
>> [error]   var nmClient: NMClient = _
>>
>> [error]                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:59:
>> not found: value NMClient
>>
>> [error]     nmClient = NMClient.createNMClient()
>>
>> [error]                ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:79:
>> value setTokens is not a member of
>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
>>
>> [error]     ctx.setTokens(ByteBuffer.wrap(dob.getData()))
>>
>> [error]         ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:35:
>> object ApplicationMasterProtocol is not a member of package
>> org.apache.hadoop.yarn.api
>>
>> [error] import org.apache.hadoop.yarn.api.ApplicationMasterProtocol
>>
>> [error]        ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:41:
>> object api is not a member of package org.apache.hadoop.yarn.client
>>
>> [error] import
>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>>
>> [error]                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:65:
>> not found: type AMRMClient
>>
>> [error]     val amClient: AMRMClient[ContainerRequest],
>>
>> [error]                   ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:389:
>> not found: type ContainerRequest
>>
>> [error]     ): ArrayBuffer[ContainerRequest] = {
>>
>> [error]                    ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:388:
>> not found: type ContainerRequest
>>
>> [error]       hostContainers: ArrayBuffer[ContainerRequest]
>>
>> [error]                                   ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:405:
>> not found: type ContainerRequest
>>
>> [error]     val requestedContainers = new
>> ArrayBuffer[ContainerRequest](rackToCounts.size)
>>
>> [error]                                               ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:434:
>> not found: type ContainerRequest
>>
>> [error]     val containerRequests: List[ContainerRequest] =
>>
>> [error]                                 ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:508:
>> not found: type ContainerRequest
>>
>> [error]     ): ArrayBuffer[ContainerRequest] = {
>>
>> [error]                    ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:446:
>> not found: type ContainerRequest
>>
>> [error]         val hostContainerRequests = new
>> ArrayBuffer[ContainerRequest](preferredHostToCount.size)
>>
>> [error]                                                     ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:458:
>> not found: type ContainerRequest
>>
>> [error]         val rackContainerRequests: List[ContainerRequest] =
>> createRackResourceRequests(
>>
>> [error]                                         ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:467:
>> not found: type ContainerRequest
>>
>> [error]         val containerRequestBuffer = new
>> ArrayBuffer[ContainerRequest](
>>
>> [error]                                                      ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:542:
>> not found: type ContainerRequest
>>
>> [error]     ): ArrayBuffer[ContainerRequest] = {
>>
>> [error]                    ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:545:
>> value newInstance is not a member of object
>> org.apache.hadoop.yarn.api.records.Resource
>>
>> [error]     val resource = Resource.newInstance(memoryRequest,
>> executorCores)
>>
>> [error]                             ^
>>
>> [error]
>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:550:
>> not found: type ContainerRequest
>>
>> [error]     val requests = new ArrayBuffer[ContainerRequest]()
>>
>> [error]                                    ^
>>
>> [error] 40 errors found
>>
>> [error] (yarn-stable/compile:compile) Compilation failed
>>
>> [error] Total time: 98 s, completed Jul 16, 2014 5:14:44 PM
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>>
>> On Wed, Jul 16, 2014 at 4:19 PM, Sandy Ryza <sandy.ryza@cloudera.com>
>> wrote:
>>
>>> Hi Ron,
>>>
>>> I just checked and this bug is fixed in recent releases of Spark.
>>>
>>> -Sandy
>>>
>>>
>>> On Sun, Jul 13, 2014 at 8:15 PM, Chester Chen <chester@alpinenow.com>
>>> wrote:
>>>
>>>> Ron,
>>>>     Which distribution and Version of Hadoop are you using ?
>>>>
>>>>      I just looked at CDH5 (  hadoop-mapreduce-client-core-
>>>> 2.3.0-cdh5.0.0),
>>>>
>>>> MRJobConfig does have the field :
>>>>
>>>> java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>>>
>>>> Chester
>>>>
>>>>
>>>>
>>>> On Sun, Jul 13, 2014 at 6:49 PM, Ron Gonzalez <zlgonzalez@yahoo.com>
>>>> wrote:
>>>>
>>>>> Hi,
>>>>>   I was doing programmatic submission of Spark yarn jobs and I saw
>>>>> code in ClientBase.getDefaultYarnApplicationClasspath():
>>>>>
>>>>> val field =
>>>>> classOf[MRJobConfig].getField("DEFAULT_YARN_APPLICATION_CLASSPATH)
>>>>> MRJobConfig doesn't have this field so the created launch env is
>>>>> incomplete. Workaround is to set yarn.application.classpath with the value
>>>>> from YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH.
>>>>>
>>>>> This results in having the spark job hang if the submission config is
>>>>> different from the default config. For example, if my resource manager port
>>>>> is 8050 instead of 8030, then the spark app is not able to register itself
>>>>> and stays in ACCEPTED state.
>>>>>
>>>>> I can easily fix this by changing this to YarnConfiguration instead of
>>>>> MRJobConfig but was wondering what the steps are for submitting a fix.
>>>>>
>>>>> Thanks,
>>>>> Ron
>>>>>
>>>>> Sent from my iPhone
>>>>
>>>>
>>>>
>>>
>>
>

--f46d04447e7b86721104fe5c381d--

From dev-return-8402-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 05:06:55 2014
Return-Path: <dev-return-8402-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 397EC11FCE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 05:06:55 +0000 (UTC)
Received: (qmail 57371 invoked by uid 500); 17 Jul 2014 05:06:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57311 invoked by uid 500); 17 Jul 2014 05:06:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57296 invoked by uid 99); 17 Jul 2014 05:06:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:06:54 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:06:49 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so1596353qgd.23
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 22:06:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=mvVfk51GGuqh01YGeoa0BqaIB0NK1qkhH4R24XcaQUo=;
        b=BIeOM7QG3tYeC/DNPcKZ+jBaScEB60cEFi6d4N1eRhIT0zQQJqWoLXChklEUwRHwCH
         NVm85Q80CzuGOpgKZekPVg1C/gdeBHQc1BIiLXmhCbKWO13MA07anpwdSUaM2gDBmSHZ
         4ERDSRRxQO821Tf+XdRQWmH5LXXVY91lHpn4F6slhA6bTdZ+JzjI3DKnGFfzi7EHnGpD
         0KvTfnYWehisLgbxVSyjxr/SCJ8C20tqf4v+rTAJ7y9695EB0CYSVKbsmcLLe5KvZL80
         MIY4ZxO6VNRhUo93/d7oID8ibluof6IyC/OYaE5RMXlKHKHDwYndllXsQK002ISTHx92
         LbQA==
X-Gm-Message-State: ALoCoQl4i/JKhNNoRWHrn8o/JgRo6s6AwBISdPd/AHwDm2E2HweofT5pNPlKgAwxOhh9TUAFzZmT
X-Received: by 10.229.211.74 with SMTP id gn10mr1036774qcb.31.1405573588755;
 Wed, 16 Jul 2014 22:06:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 16 Jul 2014 22:06:08 -0700 (PDT)
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 16 Jul 2014 22:06:08 -0700
Message-ID: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
Subject: small (yet major) change going in: broadcasting RDD to reduce task size
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132ec8a76b91604fe5c9a14
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ec8a76b91604fe5c9a14
Content-Type: text/plain; charset=UTF-8

Hi Spark devs,

Want to give you guys a heads up that I'm working on a small (but major)
change with respect to how task dispatching works. Currently (as of Spark
1.0.1), Spark sends RDD object and closures using Akka along with the task
itself to the executors. This is however inefficient because all tasks in
the same stage use the same RDDs and closures, but we have to send these
closures and RDDs multiple times to the executors. This is especially bad
when some closure references some variable that is very large. The current
design led to users having to explicitly broadcast large variables.

The patch uses broadcast to send RDD objects and the closures to executors,
and use Akka to only send a reference to the broadcast RDD/closure along
with the partition specific information for the task. For those of you who
know more about the internals, Spark already relies on broadcast to send
the Hadoop JobConf every time it uses the Hadoop input, because the JobConf
is large.

The user-facing impact of the change include:

1. Users won't need to decide what to broadcast anymore
2. Task size will get smaller, resulting in faster scheduling and higher
task dispatch throughput.

In addition, the change will simplify some internals of Spark, removing the
need to maintain task caches and the complex logic to broadcast JobConf
(which also led to a deadlock recently).


Pull request attached: https://github.com/apache/spark/pull/1450

--001a1132ec8a76b91604fe5c9a14--

From dev-return-8403-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 05:07:58 2014
Return-Path: <dev-return-8403-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8061A11FD2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 05:07:58 +0000 (UTC)
Received: (qmail 58846 invoked by uid 500); 17 Jul 2014 05:07:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58789 invoked by uid 500); 17 Jul 2014 05:07:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58778 invoked by uid 99); 17 Jul 2014 05:07:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:07:56 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:07:54 +0000
Received: by mail-qc0-f181.google.com with SMTP id w7so1669103qcr.12
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 22:07:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=p8l9SzAidQZs+vwvlVgvY54SqTMLnIGzUeEYZ3S7IX8=;
        b=f+truMRCxt1Cj214VSYWE/Thk6MJJUm8Aeo/55Owrb8WeDHo1KTgYzzTLxmBwXX9Z6
         lyXYX+TJBB0pG3wMAzpdNYWdFnOLjzdQHID3UKtbYmGjxfc/YQQcIT1hFCiFxuiVvjNY
         zOTjhljizR/csgBd9RuhfQI4kv2/8EE+VVWB8OS+F7rWfo0swUopdR8D+BGU0qOKAJEc
         R28AYOAgVZl6taCXl9q1EQLNK2XEXa5aRWL28Z9L+VY3ocVpOSqg0SkU0/jK7tR4ISDJ
         E0IKZry1JWGBV8xFp37CNzIno6zdlwKQkyEys7XitSFpMYhO5oslG04J7nzAG3n/UosZ
         QCBw==
X-Gm-Message-State: ALoCoQmdH73B/NofwpkSzD25LsQC+4vGT6ko79iQgbfsxrVog7oRk9LMRC+Zgxnf3EDzXqBSYpvw
X-Received: by 10.140.51.37 with SMTP id t34mr51450684qga.50.1405573648524;
 Wed, 16 Jul 2014 22:07:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 16 Jul 2014 22:07:08 -0700 (PDT)
In-Reply-To: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
References: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 16 Jul 2014 22:07:08 -0700
Message-ID: <CAPh_B=bXL2CAGx7XphAE9HN+61YgvZusrZk6VJNhWuAqBDXTFg@mail.gmail.com>
Subject: Re: small (yet major) change going in: broadcasting RDD to reduce
 task size
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11351a5a06b50404fe5c9eb7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11351a5a06b50404fe5c9eb7
Content-Type: text/plain; charset=UTF-8

Oops - the pull request should be https://github.com/apache/spark/pull/1452


On Wed, Jul 16, 2014 at 10:06 PM, Reynold Xin <rxin@databricks.com> wrote:

> Hi Spark devs,
>
> Want to give you guys a heads up that I'm working on a small (but major)
> change with respect to how task dispatching works. Currently (as of Spark
> 1.0.1), Spark sends RDD object and closures using Akka along with the task
> itself to the executors. This is however inefficient because all tasks in
> the same stage use the same RDDs and closures, but we have to send these
> closures and RDDs multiple times to the executors. This is especially bad
> when some closure references some variable that is very large. The current
> design led to users having to explicitly broadcast large variables.
>
> The patch uses broadcast to send RDD objects and the closures to
> executors, and use Akka to only send a reference to the broadcast
> RDD/closure along with the partition specific information for the task. For
> those of you who know more about the internals, Spark already relies on
> broadcast to send the Hadoop JobConf every time it uses the Hadoop input,
> because the JobConf is large.
>
> The user-facing impact of the change include:
>
> 1. Users won't need to decide what to broadcast anymore
> 2. Task size will get smaller, resulting in faster scheduling and higher
> task dispatch throughput.
>
> In addition, the change will simplify some internals of Spark, removing
> the need to maintain task caches and the complex logic to broadcast JobConf
> (which also led to a deadlock recently).
>
>
> Pull request attached: https://github.com/apache/spark/pull/1450
>
>
>
>
>

--001a11351a5a06b50404fe5c9eb7--

From dev-return-8404-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 05:12:59 2014
Return-Path: <dev-return-8404-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3E53511FF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 05:12:59 +0000 (UTC)
Received: (qmail 64329 invoked by uid 500); 17 Jul 2014 05:12:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64284 invoked by uid 500); 17 Jul 2014 05:12:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64272 invoked by uid 99); 17 Jul 2014 05:12:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:12:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.53 as permitted sender)
Received: from [209.85.220.53] (HELO mail-pa0-f53.google.com) (209.85.220.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:12:51 +0000
Received: by mail-pa0-f53.google.com with SMTP id kq14so2583155pab.26
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 22:12:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=hLjfwsSJ2jdnt2BjyvZ1Cn0Xyqx8gYoYbDi4nfNbJuQ=;
        b=FJCbukH8SFADk7LwNTEG3vvAAXKlBsA44bO59IcN3WFwABjgatB7fpi0YfbtWfXW0l
         got81AIH2Wv7FDaEpqppgKmSHrgDdpR9aUns52dIH/QHH4yyYq5Wqtw3HW+3GG0EvBwu
         hOu8V88DOrRWcna9MQzQQPZes+ejMMLhgkYY+yGdA/jD77WasP7Nd3qhbFYm5N+k4q/5
         Q5pqsXDU5TjsSdivYKybRb59ttttO/GM7afNcIVB/pjsJoQNbYUo6wblWRcrbohaINxU
         t+BJBLz6bcWcId3azTEoOe6Xw4ZeuSuwYQxSm6dJPdtIhufaWif73giMyPiGjXPuGJPQ
         +HAQ==
X-Received: by 10.66.182.69 with SMTP id ec5mr15105683pac.125.1405573950824;
        Wed, 16 Jul 2014 22:12:30 -0700 (PDT)
Received: from [192.168.1.106] (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id cm7sm1579720pdb.74.2014.07.16.22.12.26
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 16 Jul 2014 22:12:27 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: small (yet major) change going in: broadcasting RDD to reduce task size
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAPh_B=bXL2CAGx7XphAE9HN+61YgvZusrZk6VJNhWuAqBDXTFg@mail.gmail.com>
Date: Wed, 16 Jul 2014 22:12:25 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <BC70F42F-5C1C-4857-88B9-43028C74CCAA@gmail.com>
References: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com> <CAPh_B=bXL2CAGx7XphAE9HN+61YgvZusrZk6VJNhWuAqBDXTFg@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Reynold, just to clarify, users will still have to manually =
broadcast objects that they want to use *across* operations (e.g. in =
multiple iterations of an algorithm, or multiple map functions, or stuff =
like that). But they won't have to broadcast something they only use =
once.

Matei

On Jul 16, 2014, at 10:07 PM, Reynold Xin <rxin@databricks.com> wrote:

> Oops - the pull request should be =
https://github.com/apache/spark/pull/1452
>=20
>=20
> On Wed, Jul 16, 2014 at 10:06 PM, Reynold Xin <rxin@databricks.com> =
wrote:
>=20
>> Hi Spark devs,
>>=20
>> Want to give you guys a heads up that I'm working on a small (but =
major)
>> change with respect to how task dispatching works. Currently (as of =
Spark
>> 1.0.1), Spark sends RDD object and closures using Akka along with the =
task
>> itself to the executors. This is however inefficient because all =
tasks in
>> the same stage use the same RDDs and closures, but we have to send =
these
>> closures and RDDs multiple times to the executors. This is especially =
bad
>> when some closure references some variable that is very large. The =
current
>> design led to users having to explicitly broadcast large variables.
>>=20
>> The patch uses broadcast to send RDD objects and the closures to
>> executors, and use Akka to only send a reference to the broadcast
>> RDD/closure along with the partition specific information for the =
task. For
>> those of you who know more about the internals, Spark already relies =
on
>> broadcast to send the Hadoop JobConf every time it uses the Hadoop =
input,
>> because the JobConf is large.
>>=20
>> The user-facing impact of the change include:
>>=20
>> 1. Users won't need to decide what to broadcast anymore
>> 2. Task size will get smaller, resulting in faster scheduling and =
higher
>> task dispatch throughput.
>>=20
>> In addition, the change will simplify some internals of Spark, =
removing
>> the need to maintain task caches and the complex logic to broadcast =
JobConf
>> (which also led to a deadlock recently).
>>=20
>>=20
>> Pull request attached: https://github.com/apache/spark/pull/1450
>>=20
>>=20
>>=20
>>=20
>>=20


From dev-return-8405-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 05:15:08 2014
Return-Path: <dev-return-8405-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 863DE11FFB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 05:15:08 +0000 (UTC)
Received: (qmail 67831 invoked by uid 500); 17 Jul 2014 05:15:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67765 invoked by uid 500); 17 Jul 2014 05:15:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67752 invoked by uid 99); 17 Jul 2014 05:15:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:15:07 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:15:03 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so1507786qaq.10
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 22:14:42 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=XqJFb/LpDReh1N1pP2Av3xBDVWVn+c7dHEKW5mvlHyg=;
        b=nFxqfiIGMcJoPeJk8LqWUNjHxICFRODHGhoWj3bVicONKgVeSUf5AKlq81YdqZWmcx
         rhbj5QzdcTlXyh/A1MXBp9IiaWm33hMr1xx/+aZLn5mBpr/tsfSARbKF/Whr4tEjG/D6
         +qeRNW9ivRYQSX4wJnftyYOLshbfI6H6oi6jCAnigIPBcxLngmhYKUuahr1//q51lnuE
         +LAxGoZV0Ic/z1k8xepuRScjPRHgI/TPQeQd9mpwSCMiYQlhQbT9w0d05vVf9od8KUmV
         mIesHBRDpVxKW6tEvteXn2YZnYr7s5IKd8Y2r8+5cOZS9VqArHYEjJco7WtCJASwcg3U
         styg==
X-Gm-Message-State: ALoCoQmzZAnK0A92xI8TsABl+IAe6AhwuMoMcHGSFI5u5klNFe57nFVRx7XFC1JEJMDevurCHwzl
X-Received: by 10.224.20.200 with SMTP id g8mr3537002qab.88.1405574082788;
 Wed, 16 Jul 2014 22:14:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Wed, 16 Jul 2014 22:14:22 -0700 (PDT)
In-Reply-To: <BC70F42F-5C1C-4857-88B9-43028C74CCAA@gmail.com>
References: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
 <CAPh_B=bXL2CAGx7XphAE9HN+61YgvZusrZk6VJNhWuAqBDXTFg@mail.gmail.com> <BC70F42F-5C1C-4857-88B9-43028C74CCAA@gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Wed, 16 Jul 2014 22:14:22 -0700
Message-ID: <CAPh_B=bky72egEg069MAFTqRpxfzvdh5rJiMbQTLDi7ehtuwTQ@mail.gmail.com>
Subject: Re: small (yet major) change going in: broadcasting RDD to reduce
 task size
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2a3b0e9129004fe5cb7e3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a3b0e9129004fe5cb7e3
Content-Type: text/plain; charset=UTF-8

Yup - that is correct.  Thanks for clarifying.


On Wed, Jul 16, 2014 at 10:12 PM, Matei Zaharia <matei.zaharia@gmail.com>
wrote:

> Hey Reynold, just to clarify, users will still have to manually broadcast
> objects that they want to use *across* operations (e.g. in multiple
> iterations of an algorithm, or multiple map functions, or stuff like that).
> But they won't have to broadcast something they only use once.
>
> Matei
>
> On Jul 16, 2014, at 10:07 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Oops - the pull request should be
> https://github.com/apache/spark/pull/1452
> >
> >
> > On Wed, Jul 16, 2014 at 10:06 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> >> Hi Spark devs,
> >>
> >> Want to give you guys a heads up that I'm working on a small (but major)
> >> change with respect to how task dispatching works. Currently (as of
> Spark
> >> 1.0.1), Spark sends RDD object and closures using Akka along with the
> task
> >> itself to the executors. This is however inefficient because all tasks
> in
> >> the same stage use the same RDDs and closures, but we have to send these
> >> closures and RDDs multiple times to the executors. This is especially
> bad
> >> when some closure references some variable that is very large. The
> current
> >> design led to users having to explicitly broadcast large variables.
> >>
> >> The patch uses broadcast to send RDD objects and the closures to
> >> executors, and use Akka to only send a reference to the broadcast
> >> RDD/closure along with the partition specific information for the task.
> For
> >> those of you who know more about the internals, Spark already relies on
> >> broadcast to send the Hadoop JobConf every time it uses the Hadoop
> input,
> >> because the JobConf is large.
> >>
> >> The user-facing impact of the change include:
> >>
> >> 1. Users won't need to decide what to broadcast anymore
> >> 2. Task size will get smaller, resulting in faster scheduling and higher
> >> task dispatch throughput.
> >>
> >> In addition, the change will simplify some internals of Spark, removing
> >> the need to maintain task caches and the complex logic to broadcast
> JobConf
> >> (which also led to a deadlock recently).
> >>
> >>
> >> Pull request attached: https://github.com/apache/spark/pull/1450
> >>
> >>
> >>
> >>
> >>
>
>

--001a11c2a3b0e9129004fe5cb7e3--

From dev-return-8406-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 05:24:04 2014
Return-Path: <dev-return-8406-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0177E11018
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 05:24:04 +0000 (UTC)
Received: (qmail 81104 invoked by uid 500); 17 Jul 2014 05:24:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81044 invoked by uid 500); 17 Jul 2014 05:24:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81028 invoked by uid 99); 17 Jul 2014 05:24:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:24:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of stephen.haberman@gmail.com designates 209.85.213.54 as permitted sender)
Received: from [209.85.213.54] (HELO mail-yh0-f54.google.com) (209.85.213.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:23:59 +0000
Received: by mail-yh0-f54.google.com with SMTP id v1so1075491yhn.27
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 22:23:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:subject:message-id:in-reply-to:references
         :mime-version:content-type:content-transfer-encoding;
        bh=/IxF9HUmsi8Mpkr/zEbevg+KujXhswCEWBueO/7gcIM=;
        b=mVTeq9HsFRfAoVeAbA/0PUvJVVnWA0YSGCcUuB+Odg+/mQR4HRaYoNtga9XmhbkQLw
         Q0/U1GgMP+jYq20J2AwfEGxyPAhiLXTL0mPK0ekwvHfVGhs+2crPlE4Kl1/XOweK38Mb
         16HhNo5u6bCva97918DelpHdSD1MYBYAUa9o3jfXBkSR47mcNaiCcLBKwNMU21+MAMZX
         hXlWRQ3a1d/SdffnsHOEKzU6Mc+hbUejNZ+ImFUwMcH2xdoBu7zv5yxheqbpKJptkxfm
         ZQnJZT7X1AtfuCeQQty61n1iyCvrkpxLx+X5bmliA9hkM7VWuTtoVMy/+Q3uc8aM0MpD
         IpSQ==
X-Received: by 10.236.222.138 with SMTP id t10mr56753551yhp.10.1405574614332;
        Wed, 16 Jul 2014 22:23:34 -0700 (PDT)
Received: from sh9 (wsip-184-187-11-226.om.om.cox.net. [184.187.11.226])
        by mx.google.com with ESMTPSA id g30sm3149249yhn.46.2014.07.16.22.23.33
        for <multiple recipients>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Wed, 16 Jul 2014 22:23:33 -0700 (PDT)
Date: Thu, 17 Jul 2014 00:23:32 -0500
From: Stephen Haberman <stephen.haberman@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: small (yet major) change going in: broadcasting RDD to reduce
 task size
Message-ID: <20140717002332.1b9dc18f@sh9>
In-Reply-To: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
References: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
X-Mailer: Claws Mail 3.10.1 (GTK+ 2.24.23; x86_64-pc-linux-gnu)
MIME-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org


Wow. Great writeup.

I keep tabs on several open source projects that we use heavily, and
I'd be ecstatic if more major changes were this well/succinctly
explained instead of the usual "just read the commit message/diff".

- Stephen


From dev-return-8407-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 05:25:01 2014
Return-Path: <dev-return-8407-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2573311026
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 05:25:01 +0000 (UTC)
Received: (qmail 85847 invoked by uid 500); 17 Jul 2014 05:25:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85795 invoked by uid 500); 17 Jul 2014 05:25:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85781 invoked by uid 99); 17 Jul 2014 05:25:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:25:00 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 05:24:55 +0000
Received: by mail-vc0-f180.google.com with SMTP id ij19so3626522vcb.11
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 22:24:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=taasQlqtgtEI5zuBQgK1+IyeLg1kZyvqHT12HrsvcSg=;
        b=bIE6rdlf/kzAWDoJs1D942vzkP9h1goFN1hcj9cmnDQye8BXWuBj+CUPaabPxf7jwa
         UeS1tIDmAPIR77CGialGVxvRV/sCIqGI4ViJ3dfGEszUaOWoOlZWGgnfh8WOucySSpnE
         aAsTjWmT5G/46eMNnI6Utgpa7pDaJBLy71KdxxFcX6VzsGdE+tSVUM6TyIi+xxcdQwza
         atNHDfJPiIr8MSKCmiBqG/4eGDpk7bGf55pATpLipvE3XtPWosiIRnls6k7HjLNdQxKU
         gnU6XpGzOSKg4ALywb9HgOcTBWjl3xTZoztMeiPcUwY74yaVE/QJpbOyL0CA3TWYvHYL
         uRYg==
X-Gm-Message-State: ALoCoQkD/iriNwtp7gKgbF7a22zTyTKBYO/p2VT1ncPQ/XCfA+y6S+7KACmQZUiQaMF2SJPiZjb+
X-Received: by 10.220.103.141 with SMTP id k13mr22412218vco.25.1405574672443;
        Wed, 16 Jul 2014 22:24:32 -0700 (PDT)
Received: from mail-vc0-f182.google.com (mail-vc0-f182.google.com [209.85.220.182])
        by mx.google.com with ESMTPSA id hk5sm1115977veb.4.2014.07.16.22.24.31
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 16 Jul 2014 22:24:31 -0700 (PDT)
Received: by mail-vc0-f182.google.com with SMTP id hy4so3538634vcb.41
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 22:24:30 -0700 (PDT)
X-Received: by 10.220.44.20 with SMTP id y20mr14333730vce.60.1405574670962;
 Wed, 16 Jul 2014 22:24:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Wed, 16 Jul 2014 22:24:10 -0700 (PDT)
In-Reply-To: <CABPQxsuDbTA95hjNW9b+AwuivA3GxcW5KLheHfdJNcs2UqHZMA@mail.gmail.com>
References: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
 <CA+FETE+MMOzGTU5brjLJt8wKvAMuzq2UVnAZCT=+9sdPaZyndg@mail.gmail.com>
 <CA+-p3AEKYARqtBgj+aKYWN7gx8Fxm37nvpEx=goRqh2QcQJHxw@mail.gmail.com> <CABPQxsuDbTA95hjNW9b+AwuivA3GxcW5KLheHfdJNcs2UqHZMA@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 17 Jul 2014 01:24:10 -0400
Message-ID: <CA+-p3AHTfsQThic-MGkV-6c6HuPer+vXFt52_L9Kwmw282VAow@mail.gmail.com>
Subject: Re: Hadoop's Configuration object isn't threadsafe
To: dev@spark.apache.org
Cc: Reynold Xin <rxin@databricks.com>
Content-Type: multipart/alternative; boundary=047d7b3a98d0f7d96504fe5cdaa8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a98d0f7d96504fe5cdaa8
Content-Type: text/plain; charset=UTF-8

Hi Patrick, thanks for taking a look.  I filed as
https://issues.apache.org/jira/browse/SPARK-2546

Would you recommend I pursue the cloned Configuration object approach now
and send in a PR?

Reynold's recent announcement of the broadcast RDD object patch may also
have implications of the right path forward here.  I'm not sure I fully
understand the implications though:
https://github.com/apache/spark/pull/1452

"Once this is committed, we can also remove the JobConf broadcast in
HadoopRDD."

Thanks!
Andrew


On Tue, Jul 15, 2014 at 5:20 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Andrew,
>
> Cloning the conf this might be a good/simple fix for this particular
> problem. It's definitely worth looking into.
>
> There are a few things we can probably do in Spark to deal with
> non-thread-safety inside of the Hadoop FileSystem and Configuration
> classes. One thing we can do in general is to add barriers around the
> locations where we knowingly access Hadoop FileSystem and
> Configuration state from multiple threads (e.g. during our own calls
> to getRecordReader in this case). But this will only deal with "writer
> writer" conflicts where we had multiple calls mutating the same object
> at the same time. It won't deal with "reader writer" conflicts where
> some of our initialization code touches state that is needed during
> normal execution of other tasks.
>
> - Patrick
>
> On Tue, Jul 15, 2014 at 12:56 PM, Andrew Ash <andrew@andrewash.com> wrote:
> > Hi Shengzhe,
> >
> > Even if we did make Configuration threadsafe, it'd take quite some time
> for
> > that to trickle down to a Hadoop release that we could actually rely on
> > Spark users having installed.  I agree we should consider whether making
> > Configuration threadsafe is something that Hadoop should do, but for the
> > short term I think Spark needs to be able to handle the common scenario
> of
> > Configuration being single-threaded.
> >
> > Thanks!
> > Andrew
> >
> >
> > On Tue, Jul 15, 2014 at 2:43 PM, yao <yaoshengzhe@gmail.com> wrote:
> >
> >> Good catch Andrew. In addition to your proposed solution, is that
> possible
> >> to fix Configuration class and make it thread-safe ? I think the fix
> should
> >> be trivial, just use a ConcurrentHashMap, but I am not sure if we can
> push
> >> this change upstream (will hadoop guys accept this change ? for them, it
> >> seems they never expect Configuration object being accessed by multiple
> >> threads).
> >>
> >> -Shengzhe
> >>
> >>
> >> On Mon, Jul 14, 2014 at 10:22 PM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >>
> >> > Hi Spark devs,
> >> >
> >> > We discovered a very interesting bug in Spark at work last week in
> Spark
> >> > 0.9.1 -- that the way Spark uses the Hadoop Configuration object is
> prone
> >> to
> >> > thread safety issues.  I believe it still applies in Spark 1.0.1 as
> well.
> >> >  Let me explain:
> >> >
> >> >
> >> > *Observations*
> >> >
> >> >    - Was running a relatively simple job (read from Avro files, do a
> map,
> >> >    do another map, write back to Avro files)
> >> >    - 412 of 413 tasks completed, but the last task was hung in RUNNING
> >> >    state
> >> >    - The 412 successful tasks completed in median time 3.4s
> >> >    - The last hung task didn't finish even in 20 hours
> >> >    - The executor with the hung task was responsible for 100% of one
> core
> >> >    of CPU usage
> >> >    - Jstack of the executor attached (relevant thread pasted below)
> >> >
> >> >
> >> > *Diagnosis*
> >> >
> >> > After doing some code spelunking, we determined the issue was
> concurrent
> >> > use of a Configuration object for each task on an executor.  In Hadoop
> >> each
> >> > task runs in its own JVM, but in Spark multiple tasks can run in the
> same
> >> > JVM, so the single-threaded access assumptions of the Configuration
> >> object
> >> > no longer hold in Spark.
> >> >
> >> > The specific issue is that the AvroRecordReader actually _modifies_
> the
> >> > JobConf it's given when it's instantiated!  It adds a key for the RPC
> >> > protocol engine in the process of connecting to the Hadoop FileSystem.
> >> >  When many tasks start at the same time (like at the start of a job),
> >> many
> >> > tasks are adding this configuration item to the one Configuration
> object
> >> at
> >> > once.  Internally Configuration uses a java.lang.HashMap, which isn't
> >> > threadsafe... The below post is an excellent explanation of what
> happens in
> >> > the situation where multiple threads insert into a HashMap at the same
> >> time.
> >> >
> >> > http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html
> >> >
> >> > The gist is that you have a thread following a cycle of linked list
> nodes
> >> > indefinitely.  This exactly matches our observations of the 100% CPU
> core
> >> > and also the final location in the stack trace.
> >> >
> >> > So it seems the way Spark shares a Configuration object between task
> >> > threads in an executor is incorrect.  We need some way to prevent
> >> > concurrent access to a single Configuration object.
> >> >
> >> >
> >> > *Proposed fix*
> >> >
> >> > We can clone the JobConf object in HadoopRDD.getJobConf() so each task
> >> > gets its own JobConf object (and thus Configuration object).  The
> >> > optimization of broadcasting the Configuration object across the
> cluster
> >> > can remain, but on the other side I think it needs to be cloned for
> each
> >> > task to allow for concurrent access.  I'm not sure the performance
> >> > implications, but the comments suggest that the Configuration object
> is
> >> > ~10KB so I would expect a clone on the object to be relatively speedy.
> >> >
> >> > Has this been observed before?  Does my suggested fix make sense?
>  I'd be
> >> > happy to file a Jira ticket and continue discussion there for the
> right
> >> way
> >> > to fix.
> >> >
> >> >
> >> > Thanks!
> >> > Andrew
> >> >
> >> >
> >> > P.S.  For others seeing this issue, our temporary workaround is to
> enable
> >> > spark.speculation, which retries failed (or hung) tasks on other
> >> machines.
> >> >
> >> >
> >> >
> >> > "Executor task launch worker-6" daemon prio=10 tid=0x00007f91f01fe000
> >> > nid=0x54b1 runnable [0x00007f92d74f1000]
> >> >    java.lang.Thread.State: RUNNABLE
> >> >     at java.util.HashMap.transfer(HashMap.java:601)
> >> >     at java.util.HashMap.resize(HashMap.java:581)
> >> >     at java.util.HashMap.addEntry(HashMap.java:879)
> >> >     at java.util.HashMap.put(HashMap.java:505)
> >> >     at
> org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
> >> >     at
> org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
> >> >     at
> >> > org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
> >> >     at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
> >> >     at
> >> >
> >>
> org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
> >> >     at
> >> >
> >>
> org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
> >> >     at
> >> >
> >>
> org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
> >> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
> >> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
> >> >     at
> >> >
> >>
> org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
> >> >     at
> >> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
> >> >     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
> >> >     at
> >> >
> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
> >> >     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
> >> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
> >> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
> >> >     at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
> >> >     at
> >> >
> org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
> >> >     at
> >> >
> >>
> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
> >> >     at
> org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
> >> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
> >> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >> >     at
> >> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
> >> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
> >> >     at
> >> >
> >>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
> >> >     at
> >> >
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> >> >     at
> >> >
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> >> >     at java.security.AccessController.doPrivileged(Native Method)
> >> >     at javax.security.auth.Subject.doAs(Subject.java:415)
> >> >     at
> >> >
> >>
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
> >> >     at
> >> >
> >>
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> >> >     at
> >> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >> >     at
> >> >
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >> >     at
> >> >
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >> >     at java.lang.Thread.run(Thread.java:745)
> >> >
> >> >
> >>
>

--047d7b3a98d0f7d96504fe5cdaa8--

From dev-return-8408-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 06:14:47 2014
Return-Path: <dev-return-8408-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63D6F11114
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 06:14:47 +0000 (UTC)
Received: (qmail 68743 invoked by uid 500); 17 Jul 2014 06:14:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68692 invoked by uid 500); 17 Jul 2014 06:14:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68657 invoked by uid 99); 17 Jul 2014 06:14:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 06:14:46 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.223.178 as permitted sender)
Received: from [209.85.223.178] (HELO mail-ie0-f178.google.com) (209.85.223.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 06:14:42 +0000
Received: by mail-ie0-f178.google.com with SMTP id tp5so2134426ieb.23
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 23:14:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=JdgYz+7AgRPAlXGYN2ZeBMXlzGt6RQKKSEaOlQryuqQ=;
        b=I3bIYQfyLkBWbyHZXXYq2cPOy6eoHqQ23D5vFwwguO2q3/XKF8DU7xIqMtm9CJe50E
         ZiyOlKEdVGZP2HJ1MzIReXgUmZmRSxAe2UtfRvWQ/Ib7ZOxZXEMy88EXsWCKLoEtea4B
         78jW0q4qsrfFUP1tSYqnQe4PK94PClGurgi1DT2omQg5mTTknPB86oPEs9UY3H7tReRg
         YNP0wGtD88jOPvr9wXgxQhE+4TyzoPlDN24WrxJYtO0X5qKIE9kD9GC7nKFVlUUieZqp
         h3lyyQhZrUEnSZw+Ye62fJ5icikt/+V+Z/FHgDM/9YKle+ywtfjqphbJvhaMiDK4Vxnh
         5Dzw==
MIME-Version: 1.0
X-Received: by 10.60.57.166 with SMTP id j6mr870749oeq.77.1405577661852; Wed,
 16 Jul 2014 23:14:21 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Wed, 16 Jul 2014 23:14:21 -0700 (PDT)
In-Reply-To: <CA+-p3AHTfsQThic-MGkV-6c6HuPer+vXFt52_L9Kwmw282VAow@mail.gmail.com>
References: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
	<CA+FETE+MMOzGTU5brjLJt8wKvAMuzq2UVnAZCT=+9sdPaZyndg@mail.gmail.com>
	<CA+-p3AEKYARqtBgj+aKYWN7gx8Fxm37nvpEx=goRqh2QcQJHxw@mail.gmail.com>
	<CABPQxsuDbTA95hjNW9b+AwuivA3GxcW5KLheHfdJNcs2UqHZMA@mail.gmail.com>
	<CA+-p3AHTfsQThic-MGkV-6c6HuPer+vXFt52_L9Kwmw282VAow@mail.gmail.com>
Date: Wed, 16 Jul 2014 23:14:21 -0700
Message-ID: <CABPQxsve+N7ywicAMiTkCt22iE_isV33qa7dzOEDyUyvqKfz3Q@mail.gmail.com>
Subject: Re: Hadoop's Configuration object isn't threadsafe
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Reynold Xin <rxin@databricks.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Andrew,

I think you are correct and a follow up to SPARK-2521 will end up
fixing this. The desing of SPARK-2521 automatically broadcasts RDD
data in tasks and the approach creates a new copy of the RDD and
associated data for each task. A natural follow-up to that patch is to
stop handling the jobConf separately (since we will now broadcast all
referents of the RDD itself) and just have it broadcasted with the
RDD. I'm not sure if Reynold plans to include this in SPARK-2521 or
afterwards, but it's likely we'd do that soon.

- Patrick

On Wed, Jul 16, 2014 at 10:24 PM, Andrew Ash <andrew@andrewash.com> wrote:
> Hi Patrick, thanks for taking a look.  I filed as
> https://issues.apache.org/jira/browse/SPARK-2546
>
> Would you recommend I pursue the cloned Configuration object approach now
> and send in a PR?
>
> Reynold's recent announcement of the broadcast RDD object patch may also
> have implications of the right path forward here.  I'm not sure I fully
> understand the implications though:
> https://github.com/apache/spark/pull/1452
>
> "Once this is committed, we can also remove the JobConf broadcast in
> HadoopRDD."
>
> Thanks!
> Andrew
>
>
> On Tue, Jul 15, 2014 at 5:20 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Hey Andrew,
>>
>> Cloning the conf this might be a good/simple fix for this particular
>> problem. It's definitely worth looking into.
>>
>> There are a few things we can probably do in Spark to deal with
>> non-thread-safety inside of the Hadoop FileSystem and Configuration
>> classes. One thing we can do in general is to add barriers around the
>> locations where we knowingly access Hadoop FileSystem and
>> Configuration state from multiple threads (e.g. during our own calls
>> to getRecordReader in this case). But this will only deal with "writer
>> writer" conflicts where we had multiple calls mutating the same object
>> at the same time. It won't deal with "reader writer" conflicts where
>> some of our initialization code touches state that is needed during
>> normal execution of other tasks.
>>
>> - Patrick
>>
>> On Tue, Jul 15, 2014 at 12:56 PM, Andrew Ash <andrew@andrewash.com> wrote:
>> > Hi Shengzhe,
>> >
>> > Even if we did make Configuration threadsafe, it'd take quite some time
>> for
>> > that to trickle down to a Hadoop release that we could actually rely on
>> > Spark users having installed.  I agree we should consider whether making
>> > Configuration threadsafe is something that Hadoop should do, but for the
>> > short term I think Spark needs to be able to handle the common scenario
>> of
>> > Configuration being single-threaded.
>> >
>> > Thanks!
>> > Andrew
>> >
>> >
>> > On Tue, Jul 15, 2014 at 2:43 PM, yao <yaoshengzhe@gmail.com> wrote:
>> >
>> >> Good catch Andrew. In addition to your proposed solution, is that
>> possible
>> >> to fix Configuration class and make it thread-safe ? I think the fix
>> should
>> >> be trivial, just use a ConcurrentHashMap, but I am not sure if we can
>> push
>> >> this change upstream (will hadoop guys accept this change ? for them, it
>> >> seems they never expect Configuration object being accessed by multiple
>> >> threads).
>> >>
>> >> -Shengzhe
>> >>
>> >>
>> >> On Mon, Jul 14, 2014 at 10:22 PM, Andrew Ash <andrew@andrewash.com>
>> wrote:
>> >>
>> >> > Hi Spark devs,
>> >> >
>> >> > We discovered a very interesting bug in Spark at work last week in
>> Spark
>> >> > 0.9.1 -- that the way Spark uses the Hadoop Configuration object is
>> prone
>> >> to
>> >> > thread safety issues.  I believe it still applies in Spark 1.0.1 as
>> well.
>> >> >  Let me explain:
>> >> >
>> >> >
>> >> > *Observations*
>> >> >
>> >> >    - Was running a relatively simple job (read from Avro files, do a
>> map,
>> >> >    do another map, write back to Avro files)
>> >> >    - 412 of 413 tasks completed, but the last task was hung in RUNNING
>> >> >    state
>> >> >    - The 412 successful tasks completed in median time 3.4s
>> >> >    - The last hung task didn't finish even in 20 hours
>> >> >    - The executor with the hung task was responsible for 100% of one
>> core
>> >> >    of CPU usage
>> >> >    - Jstack of the executor attached (relevant thread pasted below)
>> >> >
>> >> >
>> >> > *Diagnosis*
>> >> >
>> >> > After doing some code spelunking, we determined the issue was
>> concurrent
>> >> > use of a Configuration object for each task on an executor.  In Hadoop
>> >> each
>> >> > task runs in its own JVM, but in Spark multiple tasks can run in the
>> same
>> >> > JVM, so the single-threaded access assumptions of the Configuration
>> >> object
>> >> > no longer hold in Spark.
>> >> >
>> >> > The specific issue is that the AvroRecordReader actually _modifies_
>> the
>> >> > JobConf it's given when it's instantiated!  It adds a key for the RPC
>> >> > protocol engine in the process of connecting to the Hadoop FileSystem.
>> >> >  When many tasks start at the same time (like at the start of a job),
>> >> many
>> >> > tasks are adding this configuration item to the one Configuration
>> object
>> >> at
>> >> > once.  Internally Configuration uses a java.lang.HashMap, which isn't
>> >> > threadsafe... The below post is an excellent explanation of what
>> happens in
>> >> > the situation where multiple threads insert into a HashMap at the same
>> >> time.
>> >> >
>> >> > http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html
>> >> >
>> >> > The gist is that you have a thread following a cycle of linked list
>> nodes
>> >> > indefinitely.  This exactly matches our observations of the 100% CPU
>> core
>> >> > and also the final location in the stack trace.
>> >> >
>> >> > So it seems the way Spark shares a Configuration object between task
>> >> > threads in an executor is incorrect.  We need some way to prevent
>> >> > concurrent access to a single Configuration object.
>> >> >
>> >> >
>> >> > *Proposed fix*
>> >> >
>> >> > We can clone the JobConf object in HadoopRDD.getJobConf() so each task
>> >> > gets its own JobConf object (and thus Configuration object).  The
>> >> > optimization of broadcasting the Configuration object across the
>> cluster
>> >> > can remain, but on the other side I think it needs to be cloned for
>> each
>> >> > task to allow for concurrent access.  I'm not sure the performance
>> >> > implications, but the comments suggest that the Configuration object
>> is
>> >> > ~10KB so I would expect a clone on the object to be relatively speedy.
>> >> >
>> >> > Has this been observed before?  Does my suggested fix make sense?
>>  I'd be
>> >> > happy to file a Jira ticket and continue discussion there for the
>> right
>> >> way
>> >> > to fix.
>> >> >
>> >> >
>> >> > Thanks!
>> >> > Andrew
>> >> >
>> >> >
>> >> > P.S.  For others seeing this issue, our temporary workaround is to
>> enable
>> >> > spark.speculation, which retries failed (or hung) tasks on other
>> >> machines.
>> >> >
>> >> >
>> >> >
>> >> > "Executor task launch worker-6" daemon prio=10 tid=0x00007f91f01fe000
>> >> > nid=0x54b1 runnable [0x00007f92d74f1000]
>> >> >    java.lang.Thread.State: RUNNABLE
>> >> >     at java.util.HashMap.transfer(HashMap.java:601)
>> >> >     at java.util.HashMap.resize(HashMap.java:581)
>> >> >     at java.util.HashMap.addEntry(HashMap.java:879)
>> >> >     at java.util.HashMap.put(HashMap.java:505)
>> >> >     at
>> org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
>> >> >     at
>> org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
>> >> >     at
>> >> > org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
>> >> >     at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
>> >> >     at
>> >> >
>> >>
>> org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
>> >> >     at
>> >> >
>> >>
>> org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
>> >> >     at
>> >> >
>> >>
>> org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
>> >> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
>> >> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
>> >> >     at
>> >> >
>> >>
>> org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
>> >> >     at
>> >> > org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
>> >> >     at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
>> >> >     at
>> >> >
>> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
>> >> >     at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
>> >> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
>> >> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
>> >> >     at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
>> >> >     at
>> >> >
>> org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
>> >> >     at
>> >> >
>> >>
>> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
>> >> >     at
>> org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
>> >> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
>> >> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
>> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
>> >> >     at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
>> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
>> >> >     at
>> >> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
>> >> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
>> >> >     at java.security.AccessController.doPrivileged(Native Method)
>> >> >     at javax.security.auth.Subject.doAs(Subject.java:415)
>> >> >     at
>> >> >
>> >>
>> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
>> >> >     at
>> >> >
>> >>
>> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
>> >> >     at
>> >> > org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
>> >> >     at
>> >> >
>> >>
>> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>> >> >     at
>> >> >
>> >>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>> >> >     at java.lang.Thread.run(Thread.java:745)
>> >> >
>> >> >
>> >>
>>

From dev-return-8409-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 06:25:50 2014
Return-Path: <dev-return-8409-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4C2ED11185
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 06:25:50 +0000 (UTC)
Received: (qmail 87195 invoked by uid 500); 17 Jul 2014 06:25:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87142 invoked by uid 500); 17 Jul 2014 06:25:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87123 invoked by uid 99); 17 Jul 2014 06:25:49 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 06:25:49 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 06:25:46 +0000
Received: by mail-vc0-f177.google.com with SMTP id hy4so3679676vcb.36
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 23:25:21 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=WpPu5m7DtqYZm61hixAeRrjWTt3mpWAnd5jaBlaGMUI=;
        b=HkK82qcVsD63KAIsnUEcFNDNsoLxTqUS1Edcwp7hlxOLZYYCAiV9PVrsP7uBZ0lvoS
         1+b8yYKufoAwJDH+Zh7vzYPhYDr3V1P4qarcn1gV1z8v/MZOTcFoJr0QQjoZtGiSs7ph
         c6WaqAPgLRYu5dxaB7wDljUGNzXbyK00eYnIHlhbI5AU/VdeyJE2WkxSW+9/TQXc7S8O
         aYEXL1DW9a64b0PW8vPMTm/8bNhZmblTidbrbaLZtWHVP/mg/Oi5l77Z0OZ9BVzCDr0S
         ma/k7CR9QBQ5II1aVGmSdZ+l01z5nYuVcwhXiNZrsaBbe8QxNSaU2nkxw2tLYTaHlo8+
         FVSg==
X-Gm-Message-State: ALoCoQmEtODqwHGcWLSjb+mkJ+ZSRbe426xj4iUy945Ha4YxicEvXnO7+rjfhw/EMPWJTr+VfAmW
X-Received: by 10.221.24.7 with SMTP id rc7mr13817177vcb.54.1405578321287;
        Wed, 16 Jul 2014 23:25:21 -0700 (PDT)
Received: from mail-vc0-f177.google.com (mail-vc0-f177.google.com [209.85.220.177])
        by mx.google.com with ESMTPSA id tp8sm3059737vdc.20.2014.07.16.23.25.20
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 16 Jul 2014 23:25:20 -0700 (PDT)
Received: by mail-vc0-f177.google.com with SMTP id hy4so3679628vcb.36
        for <dev@spark.apache.org>; Wed, 16 Jul 2014 23:25:19 -0700 (PDT)
X-Received: by 10.52.7.163 with SMTP id k3mr12346125vda.58.1405578319711; Wed,
 16 Jul 2014 23:25:19 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Wed, 16 Jul 2014 23:24:59 -0700 (PDT)
In-Reply-To: <CABPQxsve+N7ywicAMiTkCt22iE_isV33qa7dzOEDyUyvqKfz3Q@mail.gmail.com>
References: <CA+-p3AHjQCZYQfNbVxOCu6p3+gpxj7QCSSR=V0i-YzS_a7rBpQ@mail.gmail.com>
 <CA+FETE+MMOzGTU5brjLJt8wKvAMuzq2UVnAZCT=+9sdPaZyndg@mail.gmail.com>
 <CA+-p3AEKYARqtBgj+aKYWN7gx8Fxm37nvpEx=goRqh2QcQJHxw@mail.gmail.com>
 <CABPQxsuDbTA95hjNW9b+AwuivA3GxcW5KLheHfdJNcs2UqHZMA@mail.gmail.com>
 <CA+-p3AHTfsQThic-MGkV-6c6HuPer+vXFt52_L9Kwmw282VAow@mail.gmail.com> <CABPQxsve+N7ywicAMiTkCt22iE_isV33qa7dzOEDyUyvqKfz3Q@mail.gmail.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 17 Jul 2014 02:24:59 -0400
Message-ID: <CA+-p3AHrbNrgRameeLm7qd+cPmx7=VcJO2ycHA2qffoDdjW-KQ@mail.gmail.com>
Subject: Re: Hadoop's Configuration object isn't threadsafe
To: dev@spark.apache.org
Cc: Reynold Xin <rxin@databricks.com>
Content-Type: multipart/alternative; boundary=bcaec50164af73552304fe5db481
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec50164af73552304fe5db481
Content-Type: text/plain; charset=UTF-8

Sounds good -- I added comments to the ticket.

Since SPARK-2521 is scheduled for a 1.1.0 release and we can work around
with spark.speculation, I don't personally see a need for a 1.0.2 backport.

Thanks looking through this issue!


On Thu, Jul 17, 2014 at 2:14 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Hey Andrew,
>
> I think you are correct and a follow up to SPARK-2521 will end up
> fixing this. The desing of SPARK-2521 automatically broadcasts RDD
> data in tasks and the approach creates a new copy of the RDD and
> associated data for each task. A natural follow-up to that patch is to
> stop handling the jobConf separately (since we will now broadcast all
> referents of the RDD itself) and just have it broadcasted with the
> RDD. I'm not sure if Reynold plans to include this in SPARK-2521 or
> afterwards, but it's likely we'd do that soon.
>
> - Patrick
>
> On Wed, Jul 16, 2014 at 10:24 PM, Andrew Ash <andrew@andrewash.com> wrote:
> > Hi Patrick, thanks for taking a look.  I filed as
> > https://issues.apache.org/jira/browse/SPARK-2546
> >
> > Would you recommend I pursue the cloned Configuration object approach now
> > and send in a PR?
> >
> > Reynold's recent announcement of the broadcast RDD object patch may also
> > have implications of the right path forward here.  I'm not sure I fully
> > understand the implications though:
> > https://github.com/apache/spark/pull/1452
> >
> > "Once this is committed, we can also remove the JobConf broadcast in
> > HadoopRDD."
> >
> > Thanks!
> > Andrew
> >
> >
> > On Tue, Jul 15, 2014 at 5:20 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> Hey Andrew,
> >>
> >> Cloning the conf this might be a good/simple fix for this particular
> >> problem. It's definitely worth looking into.
> >>
> >> There are a few things we can probably do in Spark to deal with
> >> non-thread-safety inside of the Hadoop FileSystem and Configuration
> >> classes. One thing we can do in general is to add barriers around the
> >> locations where we knowingly access Hadoop FileSystem and
> >> Configuration state from multiple threads (e.g. during our own calls
> >> to getRecordReader in this case). But this will only deal with "writer
> >> writer" conflicts where we had multiple calls mutating the same object
> >> at the same time. It won't deal with "reader writer" conflicts where
> >> some of our initialization code touches state that is needed during
> >> normal execution of other tasks.
> >>
> >> - Patrick
> >>
> >> On Tue, Jul 15, 2014 at 12:56 PM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >> > Hi Shengzhe,
> >> >
> >> > Even if we did make Configuration threadsafe, it'd take quite some
> time
> >> for
> >> > that to trickle down to a Hadoop release that we could actually rely
> on
> >> > Spark users having installed.  I agree we should consider whether
> making
> >> > Configuration threadsafe is something that Hadoop should do, but for
> the
> >> > short term I think Spark needs to be able to handle the common
> scenario
> >> of
> >> > Configuration being single-threaded.
> >> >
> >> > Thanks!
> >> > Andrew
> >> >
> >> >
> >> > On Tue, Jul 15, 2014 at 2:43 PM, yao <yaoshengzhe@gmail.com> wrote:
> >> >
> >> >> Good catch Andrew. In addition to your proposed solution, is that
> >> possible
> >> >> to fix Configuration class and make it thread-safe ? I think the fix
> >> should
> >> >> be trivial, just use a ConcurrentHashMap, but I am not sure if we can
> >> push
> >> >> this change upstream (will hadoop guys accept this change ? for
> them, it
> >> >> seems they never expect Configuration object being accessed by
> multiple
> >> >> threads).
> >> >>
> >> >> -Shengzhe
> >> >>
> >> >>
> >> >> On Mon, Jul 14, 2014 at 10:22 PM, Andrew Ash <andrew@andrewash.com>
> >> wrote:
> >> >>
> >> >> > Hi Spark devs,
> >> >> >
> >> >> > We discovered a very interesting bug in Spark at work last week in
> >> Spark
> >> >> > 0.9.1 -- that the way Spark uses the Hadoop Configuration object is
> >> prone
> >> >> to
> >> >> > thread safety issues.  I believe it still applies in Spark 1.0.1 as
> >> well.
> >> >> >  Let me explain:
> >> >> >
> >> >> >
> >> >> > *Observations*
> >> >> >
> >> >> >    - Was running a relatively simple job (read from Avro files, do
> a
> >> map,
> >> >> >    do another map, write back to Avro files)
> >> >> >    - 412 of 413 tasks completed, but the last task was hung in
> RUNNING
> >> >> >    state
> >> >> >    - The 412 successful tasks completed in median time 3.4s
> >> >> >    - The last hung task didn't finish even in 20 hours
> >> >> >    - The executor with the hung task was responsible for 100% of
> one
> >> core
> >> >> >    of CPU usage
> >> >> >    - Jstack of the executor attached (relevant thread pasted below)
> >> >> >
> >> >> >
> >> >> > *Diagnosis*
> >> >> >
> >> >> > After doing some code spelunking, we determined the issue was
> >> concurrent
> >> >> > use of a Configuration object for each task on an executor.  In
> Hadoop
> >> >> each
> >> >> > task runs in its own JVM, but in Spark multiple tasks can run in
> the
> >> same
> >> >> > JVM, so the single-threaded access assumptions of the Configuration
> >> >> object
> >> >> > no longer hold in Spark.
> >> >> >
> >> >> > The specific issue is that the AvroRecordReader actually _modifies_
> >> the
> >> >> > JobConf it's given when it's instantiated!  It adds a key for the
> RPC
> >> >> > protocol engine in the process of connecting to the Hadoop
> FileSystem.
> >> >> >  When many tasks start at the same time (like at the start of a
> job),
> >> >> many
> >> >> > tasks are adding this configuration item to the one Configuration
> >> object
> >> >> at
> >> >> > once.  Internally Configuration uses a java.lang.HashMap, which
> isn't
> >> >> > threadsafe... The below post is an excellent explanation of what
> >> happens in
> >> >> > the situation where multiple threads insert into a HashMap at the
> same
> >> >> time.
> >> >> >
> >> >> >
> http://mailinator.blogspot.com/2009/06/beautiful-race-condition.html
> >> >> >
> >> >> > The gist is that you have a thread following a cycle of linked list
> >> nodes
> >> >> > indefinitely.  This exactly matches our observations of the 100%
> CPU
> >> core
> >> >> > and also the final location in the stack trace.
> >> >> >
> >> >> > So it seems the way Spark shares a Configuration object between
> task
> >> >> > threads in an executor is incorrect.  We need some way to prevent
> >> >> > concurrent access to a single Configuration object.
> >> >> >
> >> >> >
> >> >> > *Proposed fix*
> >> >> >
> >> >> > We can clone the JobConf object in HadoopRDD.getJobConf() so each
> task
> >> >> > gets its own JobConf object (and thus Configuration object).  The
> >> >> > optimization of broadcasting the Configuration object across the
> >> cluster
> >> >> > can remain, but on the other side I think it needs to be cloned for
> >> each
> >> >> > task to allow for concurrent access.  I'm not sure the performance
> >> >> > implications, but the comments suggest that the Configuration
> object
> >> is
> >> >> > ~10KB so I would expect a clone on the object to be relatively
> speedy.
> >> >> >
> >> >> > Has this been observed before?  Does my suggested fix make sense?
> >>  I'd be
> >> >> > happy to file a Jira ticket and continue discussion there for the
> >> right
> >> >> way
> >> >> > to fix.
> >> >> >
> >> >> >
> >> >> > Thanks!
> >> >> > Andrew
> >> >> >
> >> >> >
> >> >> > P.S.  For others seeing this issue, our temporary workaround is to
> >> enable
> >> >> > spark.speculation, which retries failed (or hung) tasks on other
> >> >> machines.
> >> >> >
> >> >> >
> >> >> >
> >> >> > "Executor task launch worker-6" daemon prio=10
> tid=0x00007f91f01fe000
> >> >> > nid=0x54b1 runnable [0x00007f92d74f1000]
> >> >> >    java.lang.Thread.State: RUNNABLE
> >> >> >     at java.util.HashMap.transfer(HashMap.java:601)
> >> >> >     at java.util.HashMap.resize(HashMap.java:581)
> >> >> >     at java.util.HashMap.addEntry(HashMap.java:879)
> >> >> >     at java.util.HashMap.put(HashMap.java:505)
> >> >> >     at
> >> org.apache.hadoop.conf.Configuration.set(Configuration.java:803)
> >> >> >     at
> >> org.apache.hadoop.conf.Configuration.set(Configuration.java:783)
> >> >> >     at
> >> >> >
> org.apache.hadoop.conf.Configuration.setClass(Configuration.java:1662)
> >> >> >     at org.apache.hadoop.ipc.RPC.setProtocolEngine(RPC.java:193)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.hadoop.hdfs.NameNodeProxies.createNNProxyWithClientProtocol(NameNodeProxies.java:343)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.hadoop.hdfs.NameNodeProxies.createNonHAProxy(NameNodeProxies.java:168)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.hadoop.hdfs.NameNodeProxies.createProxy(NameNodeProxies.java:129)
> >> >> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:436)
> >> >> >     at org.apache.hadoop.hdfs.DFSClient.<init>(DFSClient.java:403)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.hadoop.hdfs.DistributedFileSystem.initialize(DistributedFileSystem.java:125)
> >> >> >     at
> >> >> >
> org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2262)
> >> >> >     at
> org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:86)
> >> >> >     at
> >> >> >
> >> org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2296)
> >> >> >     at
> org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2278)
> >> >> >     at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:316)
> >> >> >     at org.apache.hadoop.fs.Path.getFileSystem(Path.java:194)
> >> >> >     at org.apache.avro.mapred.FsInput.<init>(FsInput.java:37)
> >> >> >     at
> >> >> >
> >> org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)
> >> >> >     at
> >> org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:156)
> >> >> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:149)
> >> >> >     at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:64)
> >> >> >     at
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >> >> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> >> >> >     at
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >> >> >     at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
> >> >> >     at
> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:241)
> >> >> >     at org.apache.spark.rdd.RDD.iterator(RDD.scala:232)
> >> >> >     at
> >> >> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:109)
> >> >> >     at org.apache.spark.scheduler.Task.run(Task.scala:53)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.spark.executor.Executor$TaskRunner$$anonfun$run$1.apply$mcV$sp(Executor.scala:211)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:42)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:41)
> >> >> >     at java.security.AccessController.doPrivileged(Native Method)
> >> >> >     at javax.security.auth.Subject.doAs(Subject.java:415)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1408)
> >> >> >     at
> >> >> >
> >> >>
> >>
> org.apache.spark.deploy.SparkHadoopUtil.runAsUser(SparkHadoopUtil.scala:41)
> >> >> >     at
> >> >> >
> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:176)
> >> >> >     at
> >> >> >
> >> >>
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >> >> >     at
> >> >> >
> >> >>
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >> >> >     at java.lang.Thread.run(Thread.java:745)
> >> >> >
> >> >> >
> >> >>
> >>
>

--bcaec50164af73552304fe5db481--

From dev-return-8410-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 07:40:53 2014
Return-Path: <dev-return-8410-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2AB691133A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 07:40:53 +0000 (UTC)
Received: (qmail 43197 invoked by uid 500); 17 Jul 2014 07:40:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43134 invoked by uid 500); 17 Jul 2014 07:40:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43123 invoked by uid 99); 17 Jul 2014 07:40:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 07:40:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.170 as permitted sender)
Received: from [209.85.220.170] (HELO mail-vc0-f170.google.com) (209.85.220.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 07:40:46 +0000
Received: by mail-vc0-f170.google.com with SMTP id lf12so3771226vcb.29
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 00:40:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=A8Xye+TTOhj3FzmtpbH6kNyWR+cVvqA973WhsPRDYnA=;
        b=ge2Ejr9wK1+3hjVu+mlY7aPgoVkjzEeVplRN7cnc0PJg7DRFqzvu+iCydOgFw1pswD
         s2hJO8Jvxi31BnTxnpt7AC4TXALwVdPwx+GPKFInMSlkrVV1J7JAE3CbGjNaj68J5IoB
         FkHizRq5LMP35B+UukiyOF/5TiSRCp7vG+GNaoHlgb0VBo47oWNptBuVyQ+cBDEjP3eK
         ge+DXwfE74/sRZL0XLzwGQXQSc0xAh46l+mopK7AyEFxDDmRuSDKYfx7SsY0q0+kfeaA
         xscZPce0yCLsKJZuLRwdHCJ9wLMjbJrUjANEclYfVTcbPq6m62mtJrnb0+oDigpZJT1s
         /9zg==
X-Gm-Message-State: ALoCoQn/So/aiTbBbjgElRta7VEo9sZLhPvGGY6ISs51hEUmrKPXCWu2skqc0mpe2C7GtNLT9C6v
X-Received: by 10.52.120.83 with SMTP id la19mr12633582vdb.68.1405582825605;
 Thu, 17 Jul 2014 00:40:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Thu, 17 Jul 2014 00:40:05 -0700 (PDT)
In-Reply-To: <CAPYnQ0W3ZpR46eGSiwBwXuoKDbkd6JU3iq3+trj-s5K2AsRVpw@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
 <CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
 <CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
 <CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
 <CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com> <CAPYnQ0W3ZpR46eGSiwBwXuoKDbkd6JU3iq3+trj-s5K2AsRVpw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 17 Jul 2014 08:40:05 +0100
Message-ID: <CAMAsSdJh_o7tBtGmCioxpMQMoWqCheVY0eA5zBo27_8i_wz6iQ@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Are you setting -Pyarn-alpha? ./sbt/sbt -Pyarn-alpha, followed by
"projects", shows it as a module. You should only build yarn-stable
*or* yarn-alpha at any given time.

I don't remember the modules changing in a while. 'yarn-alpha' is for
YARN before it stabilized, circa early Hadoop 2.0.x. 'yarn-stable' is
for beta and stable YARN, circa late Hadoop 2.0.x and onwards. 'yarn'
is code common to both, so should compile with yarn-alpha.

What's the compile error, and are you setting yarn.version? the
default is to use hadoop.version, but that defaults to 1.0.4 and there
is no such YARN.

Unless I missed it, I only see compile errors in yarn-stable, and you
are trying to compile vs YARN alpha versions no?

On Thu, Jul 17, 2014 at 5:39 AM, Chester Chen <chester@alpinenow.com> wrote:
> Looking further, the yarn and yarn-stable are both for the stable version
> of Yarn, that explains the compilation errors when using 2.0.5-alpha
> version of hadoop.
>
> the module yarn-alpha ( although is still on SparkBuild.scala), is no
> longer there in sbt console.
>
>
>> projects
>
> [info] In file:/Users/chester/projects/spark/
>
> [info]    assembly
>
> [info]    bagel
>
> [info]    catalyst
>
> [info]    core
>
> [info]    examples
>
> [info]    graphx
>
> [info]    hive
>
> [info]    mllib
>
> [info]    oldDeps
>
> [info]    repl
>
> [info]    spark
>
> [info]    sql
>
> [info]    streaming
>
> [info]    streaming-flume
>
> [info]    streaming-kafka
>
> [info]    streaming-mqtt
>
> [info]    streaming-twitter
>
> [info]    streaming-zeromq
>
> [info]    tools
>
> [info]    yarn
>
> [info]  * yarn-stable
>
>
> On Wed, Jul 16, 2014 at 5:41 PM, Chester Chen <chester@alpinenow.com> wrote:
>
>> Hmm
>> looks like a Build script issue:
>>
>> I run the command with :
>>
>> sbt/sbt clean *yarn/*test:compile
>>
>> but errors came from
>>
>> [error] 40 errors found
>>
>> [error] (*yarn-stable*/compile:compile) Compilation failed
>>
>>
>> Chester
>>
>>
>> On Wed, Jul 16, 2014 at 5:18 PM, Chester Chen <chester@alpinenow.com>
>> wrote:
>>
>>> Hi, Sandy
>>>
>>>     We do have some issue with this. The difference is in Yarn-Alpha and
>>> Yarn Stable ( I noticed that in the latest build, the module name has
>>> changed,
>>>      yarn-alpha --> yarn
>>>      yarn --> yarn-stable
>>> )
>>>
>>> For example:  MRJobConfig.class
>>> the field:
>>> "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"
>>>
>>>
>>> In Yarn-Alpha : the field returns   java.lang.String[]
>>>
>>>   java.lang.String[] DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>>
>>> while in Yarn-Stable, it returns a String
>>>
>>>   java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>>
>>> So in ClientBaseSuite.scala
>>>
>>> The following code:
>>>
>>>     val knownDefMRAppCP: Seq[String] =
>>>       getFieldValue[*String*, Seq[String]](classOf[MRJobConfig],
>>>
>>>  "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
>>>                                          Seq[String]())(a =>
>>> *a.split(",")*)
>>>
>>>
>>> works for yarn-stable, but doesn't work for yarn-alpha.
>>>
>>> This is the only failure for the SNAPSHOT I downloaded 2 weeks ago.  I
>>> believe this can be refactored to yarn-alpha module and make different
>>> tests according different API signatures.
>>>
>>>  I just update the master branch and build doesn't even compile for
>>> Yarn-Alpha (yarn) model. Yarn-Stable compile with no error and test passed.
>>>
>>>
>>> Does the Spark Jenkins job run against yarn-alpha ?
>>>
>>>
>>>
>>>
>>>
>>> Here is output from yarn-alpha compilation:
>>>
>>> I got the 40 compilation errors.
>>>
>>> sbt/sbt clean yarn/test:compile
>>>
>>> Using /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home as
>>> default JAVA_HOME.
>>>
>>> Note, this will be overridden by -java-home if it is set.
>>>
>>> [info] Loading project definition from
>>> /Users/chester/projects/spark/project/project
>>>
>>> [info] Loading project definition from
>>> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
>>>
>>> [warn] Multiple resolvers having different access mechanism configured
>>> with same name 'sbt-plugin-releases'. To avoid conflict, Remove duplicate
>>> project resolvers (`resolvers`) or rename publishing resolver (`publishTo`).
>>>
>>> [info] Loading project definition from
>>> /Users/chester/projects/spark/project
>>>
>>> NOTE: SPARK_HADOOP_VERSION is deprecated, please use
>>> -Dhadoop.version=2.0.5-alpha
>>>
>>> NOTE: SPARK_YARN is deprecated, please use -Pyarn flag.
>>>
>>> [info] Set current project to spark-parent (in build
>>> file:/Users/chester/projects/spark/)
>>>
>>> [success] Total time: 0 s, completed Jul 16, 2014 5:13:06 PM
>>>
>>> [info] Updating {file:/Users/chester/projects/spark/}core...
>>>
>>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>>>
>>> [info] Done updating.
>>>
>>> [info] Updating {file:/Users/chester/projects/spark/}yarn...
>>>
>>> [info] Updating {file:/Users/chester/projects/spark/}yarn-stable...
>>>
>>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>>>
>>> [info] Done updating.
>>>
>>> [info] Resolving commons-net#commons-net;3.1 ...
>>>
>>> [info] Compiling 358 Scala sources and 34 Java sources to
>>> /Users/chester/projects/spark/core/target/scala-2.10/classes...
>>>
>>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
>>>
>>> [info] Done updating.
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.scala:43:
>>> constructor TaskAttemptID in class TaskAttemptID is deprecated: see
>>> corresponding Javadoc for more information.
>>>
>>> [warn]     new TaskAttemptID(jtIdentifier, jobId, isMap, taskId,
>>> attemptId)
>>>
>>> [warn]     ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:501:
>>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>>> more information.
>>>
>>> [warn]     val job = new NewHadoopJob(hadoopConfiguration)
>>>
>>> [warn]               ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:634:
>>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>>> more information.
>>>
>>> [warn]     val job = new NewHadoopJob(conf)
>>>
>>> [warn]               ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:167:
>>> constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
>>> for more information.
>>>
>>> [warn]         new TaskAttemptID(new TaskID(jID.value, true, splitID),
>>> attemptID))
>>>
>>> [warn]                           ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:188:
>>> method makeQualified in class Path is deprecated: see corresponding Javadoc
>>> for more information.
>>>
>>> [warn]     outputPath.makeQualified(fs)
>>>
>>> [warn]                ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:84:
>>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>>> for more information.
>>>
>>> [warn]     if (!fs.getFileStatus(path).isDir) {
>>>
>>> [warn]                                 ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:118:
>>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>>> for more information.
>>>
>>> [warn]       val logDirs = if (logStatus != null)
>>> logStatus.filter(_.isDir).toSeq else Seq[FileStatus]()
>>>
>>> [warn]                                                               ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:56:
>>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>>> for more information.
>>>
>>> [warn]       if (file.isDir) 0L else file.getLen
>>>
>>> [warn]                ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:110:
>>> method getDefaultReplication in class FileSystem is deprecated: see
>>> corresponding Javadoc for more information.
>>>
>>> [warn]       fs.create(tempOutputPath, false, bufferSize,
>>> fs.getDefaultReplication, blockSize)
>>>
>>> [warn]                                                       ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:267:
>>> constructor TaskID in class TaskID is deprecated: see corresponding Javadoc
>>> for more information.
>>>
>>> [warn]     val taId = new TaskAttemptID(new TaskID(jobID, true,
>>> splitId), attemptId)
>>>
>>> [warn]                                  ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:767:
>>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>>> more information.
>>>
>>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
>>>
>>> [warn]               ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:830:
>>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>>> more information.
>>>
>>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
>>>
>>> [warn]               ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:185:
>>> method isDir in class FileStatus is deprecated: see corresponding Javadoc
>>> for more information.
>>>
>>> [warn]           fileStatuses.filter(!_.isDir).map(_.getPath).toSeq
>>>
>>> [warn]                                  ^
>>>
>>> [warn]
>>> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106:
>>> constructor Job in class Job is deprecated: see corresponding Javadoc for
>>> more information.
>>>
>>> [warn]     val job = new Job(conf)
>>>
>>> [warn]               ^
>>>
>>> [warn] 14 warnings found
>>>
>>> [warn] Note:
>>> /Users/chester/projects/spark/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
>>> uses unchecked or unsafe operations.
>>>
>>> [warn] Note: Recompile with -Xlint:unchecked for details.
>>>
>>> [info] Compiling 15 Scala sources to
>>> /Users/chester/projects/spark/yarn/stable/target/scala-2.10/classes...
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:26:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import org.apache.hadoop.yarn.client.api.YarnClient
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:40:
>>> not found: value YarnClient
>>>
>>> [error]   val yarnClient = YarnClient.createYarnClient
>>>
>>> [error]                    ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:32:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:33:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import
>>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:36:
>>> object util is not a member of package org.apache.hadoop.yarn.webapp
>>>
>>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:63:
>>> value RM_AM_MAX_ATTEMPTS is not a member of object
>>> org.apache.hadoop.yarn.conf.YarnConfiguration
>>>
>>> [error]     YarnConfiguration.RM_AM_MAX_ATTEMPTS,
>>> YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS)
>>>
>>> [error]                       ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:65:
>>> not found: type AMRMClient
>>>
>>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
>>>
>>> [error]                         ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:91:
>>> not found: value AMRMClient
>>>
>>> [error]     amClient = AMRMClient.createAMRMClient()
>>>
>>> [error]                ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:136:
>>> not found: value WebAppUtils
>>>
>>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
>>>
>>> [error]                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:40:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:618:
>>> not found: type AMRMClient
>>>
>>> [error]       amClient: AMRMClient[ContainerRequest],
>>>
>>> [error]                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:596:
>>> not found: type AMRMClient
>>>
>>> [error]       amClient: AMRMClient[ContainerRequest],
>>>
>>> [error]                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:577:
>>> not found: type AMRMClient
>>>
>>> [error]       amClient: AMRMClient[ContainerRequest],
>>>
>>> [error]                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:452:
>>> value CONTAINER_ID is not a member of object
>>> org.apache.hadoop.yarn.api.ApplicationConstants.Environment
>>>
>>> [error]     val containerIdString = System.getenv(
>>> ApplicationConstants.Environment.CONTAINER_ID.name())
>>>
>>> [error]
>>>           ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:128:
>>> value setTokens is not a member of
>>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
>>>
>>> [error]     amContainer.setTokens(ByteBuffer.wrap(dob.getData()))
>>>
>>> [error]                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:36:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:37:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import
>>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:39:
>>> object util is not a member of package org.apache.hadoop.yarn.webapp
>>>
>>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:62:
>>> not found: type AMRMClient
>>>
>>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
>>>
>>> [error]                         ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:99:
>>> not found: value AMRMClient
>>>
>>> [error]     amClient = AMRMClient.createAMRMClient()
>>>
>>> [error]                ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:158:
>>> not found: value WebAppUtils
>>>
>>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
>>>
>>> [error]                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:31:
>>> object ProtoUtils is not a member of package
>>> org.apache.hadoop.yarn.api.records.impl.pb
>>>
>>> [error] import org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils
>>>
>>> [error]        ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:33:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import org.apache.hadoop.yarn.client.api.NMClient
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:53:
>>> not found: type NMClient
>>>
>>> [error]   var nmClient: NMClient = _
>>>
>>> [error]                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:59:
>>> not found: value NMClient
>>>
>>> [error]     nmClient = NMClient.createNMClient()
>>>
>>> [error]                ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:79:
>>> value setTokens is not a member of
>>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
>>>
>>> [error]     ctx.setTokens(ByteBuffer.wrap(dob.getData()))
>>>
>>> [error]         ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:35:
>>> object ApplicationMasterProtocol is not a member of package
>>> org.apache.hadoop.yarn.api
>>>
>>> [error] import org.apache.hadoop.yarn.api.ApplicationMasterProtocol
>>>
>>> [error]        ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:41:
>>> object api is not a member of package org.apache.hadoop.yarn.client
>>>
>>> [error] import
>>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
>>>
>>> [error]                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:65:
>>> not found: type AMRMClient
>>>
>>> [error]     val amClient: AMRMClient[ContainerRequest],
>>>
>>> [error]                   ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:389:
>>> not found: type ContainerRequest
>>>
>>> [error]     ): ArrayBuffer[ContainerRequest] = {
>>>
>>> [error]                    ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:388:
>>> not found: type ContainerRequest
>>>
>>> [error]       hostContainers: ArrayBuffer[ContainerRequest]
>>>
>>> [error]                                   ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:405:
>>> not found: type ContainerRequest
>>>
>>> [error]     val requestedContainers = new
>>> ArrayBuffer[ContainerRequest](rackToCounts.size)
>>>
>>> [error]                                               ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:434:
>>> not found: type ContainerRequest
>>>
>>> [error]     val containerRequests: List[ContainerRequest] =
>>>
>>> [error]                                 ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:508:
>>> not found: type ContainerRequest
>>>
>>> [error]     ): ArrayBuffer[ContainerRequest] = {
>>>
>>> [error]                    ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:446:
>>> not found: type ContainerRequest
>>>
>>> [error]         val hostContainerRequests = new
>>> ArrayBuffer[ContainerRequest](preferredHostToCount.size)
>>>
>>> [error]                                                     ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:458:
>>> not found: type ContainerRequest
>>>
>>> [error]         val rackContainerRequests: List[ContainerRequest] =
>>> createRackResourceRequests(
>>>
>>> [error]                                         ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:467:
>>> not found: type ContainerRequest
>>>
>>> [error]         val containerRequestBuffer = new
>>> ArrayBuffer[ContainerRequest](
>>>
>>> [error]                                                      ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:542:
>>> not found: type ContainerRequest
>>>
>>> [error]     ): ArrayBuffer[ContainerRequest] = {
>>>
>>> [error]                    ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:545:
>>> value newInstance is not a member of object
>>> org.apache.hadoop.yarn.api.records.Resource
>>>
>>> [error]     val resource = Resource.newInstance(memoryRequest,
>>> executorCores)
>>>
>>> [error]                             ^
>>>
>>> [error]
>>> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:550:
>>> not found: type ContainerRequest
>>>
>>> [error]     val requests = new ArrayBuffer[ContainerRequest]()
>>>
>>> [error]                                    ^
>>>
>>> [error] 40 errors found
>>>
>>> [error] (yarn-stable/compile:compile) Compilation failed
>>>
>>> [error] Total time: 98 s, completed Jul 16, 2014 5:14:44 PM
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>>
>>> On Wed, Jul 16, 2014 at 4:19 PM, Sandy Ryza <sandy.ryza@cloudera.com>
>>> wrote:
>>>
>>>> Hi Ron,
>>>>
>>>> I just checked and this bug is fixed in recent releases of Spark.
>>>>
>>>> -Sandy
>>>>
>>>>
>>>> On Sun, Jul 13, 2014 at 8:15 PM, Chester Chen <chester@alpinenow.com>
>>>> wrote:
>>>>
>>>>> Ron,
>>>>>     Which distribution and Version of Hadoop are you using ?
>>>>>
>>>>>      I just looked at CDH5 (  hadoop-mapreduce-client-core-
>>>>> 2.3.0-cdh5.0.0),
>>>>>
>>>>> MRJobConfig does have the field :
>>>>>
>>>>> java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
>>>>>
>>>>> Chester
>>>>>
>>>>>
>>>>>
>>>>> On Sun, Jul 13, 2014 at 6:49 PM, Ron Gonzalez <zlgonzalez@yahoo.com>
>>>>> wrote:
>>>>>
>>>>>> Hi,
>>>>>>   I was doing programmatic submission of Spark yarn jobs and I saw
>>>>>> code in ClientBase.getDefaultYarnApplicationClasspath():
>>>>>>
>>>>>> val field =
>>>>>> classOf[MRJobConfig].getField("DEFAULT_YARN_APPLICATION_CLASSPATH)
>>>>>> MRJobConfig doesn't have this field so the created launch env is
>>>>>> incomplete. Workaround is to set yarn.application.classpath with the value
>>>>>> from YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH.
>>>>>>
>>>>>> This results in having the spark job hang if the submission config is
>>>>>> different from the default config. For example, if my resource manager port
>>>>>> is 8050 instead of 8030, then the spark app is not able to register itself
>>>>>> and stays in ACCEPTED state.
>>>>>>
>>>>>> I can easily fix this by changing this to YarnConfiguration instead of
>>>>>> MRJobConfig but was wondering what the steps are for submitting a fix.
>>>>>>
>>>>>> Thanks,
>>>>>> Ron
>>>>>>
>>>>>> Sent from my iPhone
>>>>>
>>>>>
>>>>>
>>>>
>>>
>>

From dev-return-8411-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 07:45:26 2014
Return-Path: <dev-return-8411-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0663B11358
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 07:45:26 +0000 (UTC)
Received: (qmail 52923 invoked by uid 500); 17 Jul 2014 07:45:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52867 invoked by uid 500); 17 Jul 2014 07:45:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52855 invoked by uid 99); 17 Jul 2014 07:45:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 07:45:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 07:45:20 +0000
Received: by mail-qa0-f46.google.com with SMTP id v10so1582532qac.5
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 00:45:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=zv/x8P4S+hO/GLsIm6rkPkp2IMNtFl0z+9vOC5mYon4=;
        b=RHTztMRj1fv5Lg5qGI5frWHfb+x5AfdlBJCz+fSmC1ScUICxJhlO4ZvNBxAhlzbg7N
         6WQM+2sxXyj+NbJYqmMZIJFTfBhfG9ZfUOxCZ/oTo7xvBQkwov8G+WOObr4efqCC3JYc
         YgFi8JKn3VnEuRbztfRwZ8TWNMYqobWjxi6aEIqLIOvGWnqvc1V63q97Gcb8v0zhcvh+
         8X/KVeHnbk29YsEx9otCDCQW8m5s89wWF+C8gUqE/Lef/+cqgowU99ixqY1jK+mKK14e
         0oNRRf4mxPk0WAxB7b0aE9oOqmRNsv+f/X+pszoW0SkHiY6bLmofXIshEGL5L/vm9UJ8
         CWyw==
X-Gm-Message-State: ALoCoQnnsy4Q8lbapNYsP7TqfQ5UZfJta/IffLQFQBE4nS03FOelRmwfbX/QppcFoW1plCMRb2G7
MIME-Version: 1.0
X-Received: by 10.224.97.65 with SMTP id k1mr53054366qan.28.1405583099916;
 Thu, 17 Jul 2014 00:44:59 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Thu, 17 Jul 2014 00:44:59 -0700 (PDT)
In-Reply-To: <CAMAsSdJh_o7tBtGmCioxpMQMoWqCheVY0eA5zBo27_8i_wz6iQ@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
	<CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
	<CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
	<CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
	<CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com>
	<CAPYnQ0W3ZpR46eGSiwBwXuoKDbkd6JU3iq3+trj-s5K2AsRVpw@mail.gmail.com>
	<CAMAsSdJh_o7tBtGmCioxpMQMoWqCheVY0eA5zBo27_8i_wz6iQ@mail.gmail.com>
Date: Thu, 17 Jul 2014 00:44:59 -0700
Message-ID: <CACBYxKKwGR6-WgKguPH11Q4Z4uOe9F1wwFUpw7TUm3VbcCQ3_Q@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c3f6045f8ba504fe5ed177
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3f6045f8ba504fe5ed177
Content-Type: text/plain; charset=UTF-8

To add, we've made some effort to yarn-alpha to work with the 2.0.x line,
but this was a time when YARN went through wild API changes.  The only line
that the yarn-alpha profile is guaranteed to work against is the 0.23 line.


On Thu, Jul 17, 2014 at 12:40 AM, Sean Owen <sowen@cloudera.com> wrote:

> Are you setting -Pyarn-alpha? ./sbt/sbt -Pyarn-alpha, followed by
> "projects", shows it as a module. You should only build yarn-stable
> *or* yarn-alpha at any given time.
>
> I don't remember the modules changing in a while. 'yarn-alpha' is for
> YARN before it stabilized, circa early Hadoop 2.0.x. 'yarn-stable' is
> for beta and stable YARN, circa late Hadoop 2.0.x and onwards. 'yarn'
> is code common to both, so should compile with yarn-alpha.
>
> What's the compile error, and are you setting yarn.version? the
> default is to use hadoop.version, but that defaults to 1.0.4 and there
> is no such YARN.
>
> Unless I missed it, I only see compile errors in yarn-stable, and you
> are trying to compile vs YARN alpha versions no?
>
> On Thu, Jul 17, 2014 at 5:39 AM, Chester Chen <chester@alpinenow.com>
> wrote:
> > Looking further, the yarn and yarn-stable are both for the stable version
> > of Yarn, that explains the compilation errors when using 2.0.5-alpha
> > version of hadoop.
> >
> > the module yarn-alpha ( although is still on SparkBuild.scala), is no
> > longer there in sbt console.
> >
> >
> >> projects
> >
> > [info] In file:/Users/chester/projects/spark/
> >
> > [info]    assembly
> >
> > [info]    bagel
> >
> > [info]    catalyst
> >
> > [info]    core
> >
> > [info]    examples
> >
> > [info]    graphx
> >
> > [info]    hive
> >
> > [info]    mllib
> >
> > [info]    oldDeps
> >
> > [info]    repl
> >
> > [info]    spark
> >
> > [info]    sql
> >
> > [info]    streaming
> >
> > [info]    streaming-flume
> >
> > [info]    streaming-kafka
> >
> > [info]    streaming-mqtt
> >
> > [info]    streaming-twitter
> >
> > [info]    streaming-zeromq
> >
> > [info]    tools
> >
> > [info]    yarn
> >
> > [info]  * yarn-stable
> >
> >
> > On Wed, Jul 16, 2014 at 5:41 PM, Chester Chen <chester@alpinenow.com>
> wrote:
> >
> >> Hmm
> >> looks like a Build script issue:
> >>
> >> I run the command with :
> >>
> >> sbt/sbt clean *yarn/*test:compile
> >>
> >> but errors came from
> >>
> >> [error] 40 errors found
> >>
> >> [error] (*yarn-stable*/compile:compile) Compilation failed
> >>
> >>
> >> Chester
> >>
> >>
> >> On Wed, Jul 16, 2014 at 5:18 PM, Chester Chen <chester@alpinenow.com>
> >> wrote:
> >>
> >>> Hi, Sandy
> >>>
> >>>     We do have some issue with this. The difference is in Yarn-Alpha
> and
> >>> Yarn Stable ( I noticed that in the latest build, the module name has
> >>> changed,
> >>>      yarn-alpha --> yarn
> >>>      yarn --> yarn-stable
> >>> )
> >>>
> >>> For example:  MRJobConfig.class
> >>> the field:
> >>> "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"
> >>>
> >>>
> >>> In Yarn-Alpha : the field returns   java.lang.String[]
> >>>
> >>>   java.lang.String[] DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
> >>>
> >>> while in Yarn-Stable, it returns a String
> >>>
> >>>   java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
> >>>
> >>> So in ClientBaseSuite.scala
> >>>
> >>> The following code:
> >>>
> >>>     val knownDefMRAppCP: Seq[String] =
> >>>       getFieldValue[*String*, Seq[String]](classOf[MRJobConfig],
> >>>
> >>>  "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
> >>>                                          Seq[String]())(a =>
> >>> *a.split(",")*)
> >>>
> >>>
> >>> works for yarn-stable, but doesn't work for yarn-alpha.
> >>>
> >>> This is the only failure for the SNAPSHOT I downloaded 2 weeks ago.  I
> >>> believe this can be refactored to yarn-alpha module and make different
> >>> tests according different API signatures.
> >>>
> >>>  I just update the master branch and build doesn't even compile for
> >>> Yarn-Alpha (yarn) model. Yarn-Stable compile with no error and test
> passed.
> >>>
> >>>
> >>> Does the Spark Jenkins job run against yarn-alpha ?
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> Here is output from yarn-alpha compilation:
> >>>
> >>> I got the 40 compilation errors.
> >>>
> >>> sbt/sbt clean yarn/test:compile
> >>>
> >>> Using /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home
> as
> >>> default JAVA_HOME.
> >>>
> >>> Note, this will be overridden by -java-home if it is set.
> >>>
> >>> [info] Loading project definition from
> >>> /Users/chester/projects/spark/project/project
> >>>
> >>> [info] Loading project definition from
> >>>
> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
> >>>
> >>> [warn] Multiple resolvers having different access mechanism configured
> >>> with same name 'sbt-plugin-releases'. To avoid conflict, Remove
> duplicate
> >>> project resolvers (`resolvers`) or rename publishing resolver
> (`publishTo`).
> >>>
> >>> [info] Loading project definition from
> >>> /Users/chester/projects/spark/project
> >>>
> >>> NOTE: SPARK_HADOOP_VERSION is deprecated, please use
> >>> -Dhadoop.version=2.0.5-alpha
> >>>
> >>> NOTE: SPARK_YARN is deprecated, please use -Pyarn flag.
> >>>
> >>> [info] Set current project to spark-parent (in build
> >>> file:/Users/chester/projects/spark/)
> >>>
> >>> [success] Total time: 0 s, completed Jul 16, 2014 5:13:06 PM
> >>>
> >>> [info] Updating {file:/Users/chester/projects/spark/}core...
> >>>
> >>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> >>>
> >>> [info] Done updating.
> >>>
> >>> [info] Updating {file:/Users/chester/projects/spark/}yarn...
> >>>
> >>> [info] Updating {file:/Users/chester/projects/spark/}yarn-stable...
> >>>
> >>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> >>>
> >>> [info] Done updating.
> >>>
> >>> [info] Resolving commons-net#commons-net;3.1 ...
> >>>
> >>> [info] Compiling 358 Scala sources and 34 Java sources to
> >>> /Users/chester/projects/spark/core/target/scala-2.10/classes...
> >>>
> >>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> >>>
> >>> [info] Done updating.
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.scala:43:
> >>> constructor TaskAttemptID in class TaskAttemptID is deprecated: see
> >>> corresponding Javadoc for more information.
> >>>
> >>> [warn]     new TaskAttemptID(jtIdentifier, jobId, isMap, taskId,
> >>> attemptId)
> >>>
> >>> [warn]     ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:501:
> >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> for
> >>> more information.
> >>>
> >>> [warn]     val job = new NewHadoopJob(hadoopConfiguration)
> >>>
> >>> [warn]               ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:634:
> >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> for
> >>> more information.
> >>>
> >>> [warn]     val job = new NewHadoopJob(conf)
> >>>
> >>> [warn]               ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:167:
> >>> constructor TaskID in class TaskID is deprecated: see corresponding
> Javadoc
> >>> for more information.
> >>>
> >>> [warn]         new TaskAttemptID(new TaskID(jID.value, true, splitID),
> >>> attemptID))
> >>>
> >>> [warn]                           ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:188:
> >>> method makeQualified in class Path is deprecated: see corresponding
> Javadoc
> >>> for more information.
> >>>
> >>> [warn]     outputPath.makeQualified(fs)
> >>>
> >>> [warn]                ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:84:
> >>> method isDir in class FileStatus is deprecated: see corresponding
> Javadoc
> >>> for more information.
> >>>
> >>> [warn]     if (!fs.getFileStatus(path).isDir) {
> >>>
> >>> [warn]                                 ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:118:
> >>> method isDir in class FileStatus is deprecated: see corresponding
> Javadoc
> >>> for more information.
> >>>
> >>> [warn]       val logDirs = if (logStatus != null)
> >>> logStatus.filter(_.isDir).toSeq else Seq[FileStatus]()
> >>>
> >>> [warn]                                                               ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:56:
> >>> method isDir in class FileStatus is deprecated: see corresponding
> Javadoc
> >>> for more information.
> >>>
> >>> [warn]       if (file.isDir) 0L else file.getLen
> >>>
> >>> [warn]                ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:110:
> >>> method getDefaultReplication in class FileSystem is deprecated: see
> >>> corresponding Javadoc for more information.
> >>>
> >>> [warn]       fs.create(tempOutputPath, false, bufferSize,
> >>> fs.getDefaultReplication, blockSize)
> >>>
> >>> [warn]                                                       ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:267:
> >>> constructor TaskID in class TaskID is deprecated: see corresponding
> Javadoc
> >>> for more information.
> >>>
> >>> [warn]     val taId = new TaskAttemptID(new TaskID(jobID, true,
> >>> splitId), attemptId)
> >>>
> >>> [warn]                                  ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:767:
> >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> for
> >>> more information.
> >>>
> >>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
> >>>
> >>> [warn]               ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:830:
> >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> for
> >>> more information.
> >>>
> >>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
> >>>
> >>> [warn]               ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:185:
> >>> method isDir in class FileStatus is deprecated: see corresponding
> Javadoc
> >>> for more information.
> >>>
> >>> [warn]           fileStatuses.filter(!_.isDir).map(_.getPath).toSeq
> >>>
> >>> [warn]                                  ^
> >>>
> >>> [warn]
> >>>
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106:
> >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> for
> >>> more information.
> >>>
> >>> [warn]     val job = new Job(conf)
> >>>
> >>> [warn]               ^
> >>>
> >>> [warn] 14 warnings found
> >>>
> >>> [warn] Note:
> >>>
> /Users/chester/projects/spark/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
> >>> uses unchecked or unsafe operations.
> >>>
> >>> [warn] Note: Recompile with -Xlint:unchecked for details.
> >>>
> >>> [info] Compiling 15 Scala sources to
> >>> /Users/chester/projects/spark/yarn/stable/target/scala-2.10/classes...
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:26:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import org.apache.hadoop.yarn.client.api.YarnClient
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:40:
> >>> not found: value YarnClient
> >>>
> >>> [error]   val yarnClient = YarnClient.createYarnClient
> >>>
> >>> [error]                    ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:32:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:33:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import
> >>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:36:
> >>> object util is not a member of package org.apache.hadoop.yarn.webapp
> >>>
> >>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:63:
> >>> value RM_AM_MAX_ATTEMPTS is not a member of object
> >>> org.apache.hadoop.yarn.conf.YarnConfiguration
> >>>
> >>> [error]     YarnConfiguration.RM_AM_MAX_ATTEMPTS,
> >>> YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS)
> >>>
> >>> [error]                       ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:65:
> >>> not found: type AMRMClient
> >>>
> >>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
> >>>
> >>> [error]                         ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:91:
> >>> not found: value AMRMClient
> >>>
> >>> [error]     amClient = AMRMClient.createAMRMClient()
> >>>
> >>> [error]                ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:136:
> >>> not found: value WebAppUtils
> >>>
> >>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
> >>>
> >>> [error]                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:40:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:618:
> >>> not found: type AMRMClient
> >>>
> >>> [error]       amClient: AMRMClient[ContainerRequest],
> >>>
> >>> [error]                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:596:
> >>> not found: type AMRMClient
> >>>
> >>> [error]       amClient: AMRMClient[ContainerRequest],
> >>>
> >>> [error]                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:577:
> >>> not found: type AMRMClient
> >>>
> >>> [error]       amClient: AMRMClient[ContainerRequest],
> >>>
> >>> [error]                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:452:
> >>> value CONTAINER_ID is not a member of object
> >>> org.apache.hadoop.yarn.api.ApplicationConstants.Environment
> >>>
> >>> [error]     val containerIdString = System.getenv(
> >>> ApplicationConstants.Environment.CONTAINER_ID.name())
> >>>
> >>> [error]
> >>>           ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:128:
> >>> value setTokens is not a member of
> >>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
> >>>
> >>> [error]     amContainer.setTokens(ByteBuffer.wrap(dob.getData()))
> >>>
> >>> [error]                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:36:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:37:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import
> >>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:39:
> >>> object util is not a member of package org.apache.hadoop.yarn.webapp
> >>>
> >>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:62:
> >>> not found: type AMRMClient
> >>>
> >>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
> >>>
> >>> [error]                         ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:99:
> >>> not found: value AMRMClient
> >>>
> >>> [error]     amClient = AMRMClient.createAMRMClient()
> >>>
> >>> [error]                ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:158:
> >>> not found: value WebAppUtils
> >>>
> >>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
> >>>
> >>> [error]                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:31:
> >>> object ProtoUtils is not a member of package
> >>> org.apache.hadoop.yarn.api.records.impl.pb
> >>>
> >>> [error] import org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils
> >>>
> >>> [error]        ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:33:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import org.apache.hadoop.yarn.client.api.NMClient
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:53:
> >>> not found: type NMClient
> >>>
> >>> [error]   var nmClient: NMClient = _
> >>>
> >>> [error]                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:59:
> >>> not found: value NMClient
> >>>
> >>> [error]     nmClient = NMClient.createNMClient()
> >>>
> >>> [error]                ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:79:
> >>> value setTokens is not a member of
> >>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
> >>>
> >>> [error]     ctx.setTokens(ByteBuffer.wrap(dob.getData()))
> >>>
> >>> [error]         ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:35:
> >>> object ApplicationMasterProtocol is not a member of package
> >>> org.apache.hadoop.yarn.api
> >>>
> >>> [error] import org.apache.hadoop.yarn.api.ApplicationMasterProtocol
> >>>
> >>> [error]        ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:41:
> >>> object api is not a member of package org.apache.hadoop.yarn.client
> >>>
> >>> [error] import
> >>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
> >>>
> >>> [error]                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:65:
> >>> not found: type AMRMClient
> >>>
> >>> [error]     val amClient: AMRMClient[ContainerRequest],
> >>>
> >>> [error]                   ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:389:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]     ): ArrayBuffer[ContainerRequest] = {
> >>>
> >>> [error]                    ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:388:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]       hostContainers: ArrayBuffer[ContainerRequest]
> >>>
> >>> [error]                                   ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:405:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]     val requestedContainers = new
> >>> ArrayBuffer[ContainerRequest](rackToCounts.size)
> >>>
> >>> [error]                                               ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:434:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]     val containerRequests: List[ContainerRequest] =
> >>>
> >>> [error]                                 ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:508:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]     ): ArrayBuffer[ContainerRequest] = {
> >>>
> >>> [error]                    ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:446:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]         val hostContainerRequests = new
> >>> ArrayBuffer[ContainerRequest](preferredHostToCount.size)
> >>>
> >>> [error]                                                     ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:458:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]         val rackContainerRequests: List[ContainerRequest] =
> >>> createRackResourceRequests(
> >>>
> >>> [error]                                         ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:467:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]         val containerRequestBuffer = new
> >>> ArrayBuffer[ContainerRequest](
> >>>
> >>> [error]                                                      ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:542:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]     ): ArrayBuffer[ContainerRequest] = {
> >>>
> >>> [error]                    ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:545:
> >>> value newInstance is not a member of object
> >>> org.apache.hadoop.yarn.api.records.Resource
> >>>
> >>> [error]     val resource = Resource.newInstance(memoryRequest,
> >>> executorCores)
> >>>
> >>> [error]                             ^
> >>>
> >>> [error]
> >>>
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:550:
> >>> not found: type ContainerRequest
> >>>
> >>> [error]     val requests = new ArrayBuffer[ContainerRequest]()
> >>>
> >>> [error]                                    ^
> >>>
> >>> [error] 40 errors found
> >>>
> >>> [error] (yarn-stable/compile:compile) Compilation failed
> >>>
> >>> [error] Total time: 98 s, completed Jul 16, 2014 5:14:44 PM
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>>
> >>> On Wed, Jul 16, 2014 at 4:19 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> >>> wrote:
> >>>
> >>>> Hi Ron,
> >>>>
> >>>> I just checked and this bug is fixed in recent releases of Spark.
> >>>>
> >>>> -Sandy
> >>>>
> >>>>
> >>>> On Sun, Jul 13, 2014 at 8:15 PM, Chester Chen <chester@alpinenow.com>
> >>>> wrote:
> >>>>
> >>>>> Ron,
> >>>>>     Which distribution and Version of Hadoop are you using ?
> >>>>>
> >>>>>      I just looked at CDH5 (  hadoop-mapreduce-client-core-
> >>>>> 2.3.0-cdh5.0.0),
> >>>>>
> >>>>> MRJobConfig does have the field :
> >>>>>
> >>>>> java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
> >>>>>
> >>>>> Chester
> >>>>>
> >>>>>
> >>>>>
> >>>>> On Sun, Jul 13, 2014 at 6:49 PM, Ron Gonzalez <zlgonzalez@yahoo.com>
> >>>>> wrote:
> >>>>>
> >>>>>> Hi,
> >>>>>>   I was doing programmatic submission of Spark yarn jobs and I saw
> >>>>>> code in ClientBase.getDefaultYarnApplicationClasspath():
> >>>>>>
> >>>>>> val field =
> >>>>>> classOf[MRJobConfig].getField("DEFAULT_YARN_APPLICATION_CLASSPATH)
> >>>>>> MRJobConfig doesn't have this field so the created launch env is
> >>>>>> incomplete. Workaround is to set yarn.application.classpath with
> the value
> >>>>>> from YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH.
> >>>>>>
> >>>>>> This results in having the spark job hang if the submission config
> is
> >>>>>> different from the default config. For example, if my resource
> manager port
> >>>>>> is 8050 instead of 8030, then the spark app is not able to register
> itself
> >>>>>> and stays in ACCEPTED state.
> >>>>>>
> >>>>>> I can easily fix this by changing this to YarnConfiguration instead
> of
> >>>>>> MRJobConfig but was wondering what the steps are for submitting a
> fix.
> >>>>>>
> >>>>>> Thanks,
> >>>>>> Ron
> >>>>>>
> >>>>>> Sent from my iPhone
> >>>>>
> >>>>>
> >>>>>
> >>>>
> >>>
> >>
>

--001a11c3f6045f8ba504fe5ed177--

From dev-return-8412-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 10:43:01 2014
Return-Path: <dev-return-8412-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 16066118D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 10:43:01 +0000 (UTC)
Received: (qmail 22128 invoked by uid 500); 17 Jul 2014 10:43:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22071 invoked by uid 500); 17 Jul 2014 10:43:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22059 invoked by uid 99); 17 Jul 2014 10:43:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 10:43:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.173 as permitted sender)
Received: from [74.125.82.173] (HELO mail-we0-f173.google.com) (74.125.82.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 10:42:57 +0000
Received: by mail-we0-f173.google.com with SMTP id q58so2338277wes.18
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 03:42:33 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=AdDlljGe6tobE87DMtgvNk4oehbUvpO46hPvAzTTnCk=;
        b=tO6yhROBu7Whv6dWD+KCzYlozHFtqtKUPBIafvsq1QFd4m+38QwJ4Svw6RTo06hRd6
         aqG53rm9Z4GUWp4vhrYkkCqoQAePYPKWY8m7xvwf27DVxvShOjHefaimMb9i3M4sPsEd
         h7Kh5K7NwUVGgDpz0ynufPNcem93VUgJVC/KORNGcyAQLU9Q+dyfZt+LKZiT2CMym1P9
         JVq42IsShgewulVYIBNXkndPl5ViAto9WcmTu6G2ZSwPZm9yEpBqwEqw98ScoIlikwlN
         KtisilQLcxxVTlx6J07LRVRCB2jBDa0dQVZ/alZA45OLEv2DivlN+YAlRdkHowaC3JUz
         iYMQ==
MIME-Version: 1.0
X-Received: by 10.194.6.134 with SMTP id b6mr44714712wja.64.1405592201596;
 Thu, 17 Jul 2014 03:16:41 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Thu, 17 Jul 2014 03:16:41 -0700 (PDT)
Date: Thu, 17 Jul 2014 03:16:41 -0700
Message-ID: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 0.9.2!

The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~meng/spark-0.9.2-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/meng.asc

The staging repository for this release can be found at:
https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/

The documentation corresponding to this release can be found at:
http://people.apache.org/~meng/spark-0.9.2-rc1-docs/

Please vote on releasing this package as Apache Spark 0.9.2!

The vote is open until Sunday, July 20, at 11:10 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 0.9.2
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

=== About this release ===
This release fixes a few high-priority bugs in 0.9.1 and has a variety
of smaller fixes. The full list is here: http://s.apache.org/d0t. Some
of the more visible patches are:

SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size
SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
SPARK-1676: HDFS FileSystems continually pile up in the FS cache
SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
SPARK-1870: Secondary jars are not added to executor classpath for YARN

This is the second maintenance release on the 0.9 line. We plan to make
additional maintenance releases as new fixes come in.

Best,
Xiangrui

From dev-return-8413-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 14:24:41 2014
Return-Path: <dev-return-8413-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9010411F93
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 14:24:41 +0000 (UTC)
Received: (qmail 28529 invoked by uid 500); 17 Jul 2014 14:24:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28464 invoked by uid 500); 17 Jul 2014 14:24:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28451 invoked by uid 99); 17 Jul 2014 14:24:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:24:40 +0000
X-ASF-Spam-Status: No, hits=2.6 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,TRACKER_ID
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of chester@alpinenow.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:24:33 +0000
Received: by mail-wg0-f43.google.com with SMTP id l18so2110163wgh.26
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 07:24:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=6x/6eHrsaKBSmuqOuvA9OK4cmvvCDp1nko7K9MovpNQ=;
        b=kyrvNOtWl9Aq2XuLO4vCHmdL42f2YdaLHhHpUiR/SRWyYEtSnAtVMaGYWyqjpVsluT
         sXG63Gee7S2h1MWhrlRT3mUn+L638VYPYFhCCow8Xswz04pkv+PwAczVIkDdU8EbUX7Q
         y9hyH8Vj1R5m0QSYTZuRgm37n0Yh3tLhrFaxHdcfBGn1pHu3EHkPJcKyozpo+Jl9MYux
         P9qHR+Gu1yc1c65ajoD0cnxnpmWr1+aoDQo1rEF5/k46oPv2eSNmpWQz8h0F3Ag40txn
         jm7Wh8A7FWrCDlY+yROMcsnY03/2MEIh17mOTcU+1LlpO1BGuNM0tutiju+02nAnisye
         ml6g==
X-Gm-Message-State: ALoCoQkcEz5ag21tFAwnO9sH+9uwBS1P/v/LVST1jpUY6YagJQCylUMJ5bPXaDUgs24M3jSWIjpS
MIME-Version: 1.0
X-Received: by 10.180.8.10 with SMTP id n10mr22966373wia.41.1405607047389;
 Thu, 17 Jul 2014 07:24:07 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Thu, 17 Jul 2014 07:24:07 -0700 (PDT)
In-Reply-To: <CACBYxKKwGR6-WgKguPH11Q4Z4uOe9F1wwFUpw7TUm3VbcCQ3_Q@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
	<CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
	<CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
	<CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
	<CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com>
	<CAPYnQ0W3ZpR46eGSiwBwXuoKDbkd6JU3iq3+trj-s5K2AsRVpw@mail.gmail.com>
	<CAMAsSdJh_o7tBtGmCioxpMQMoWqCheVY0eA5zBo27_8i_wz6iQ@mail.gmail.com>
	<CACBYxKKwGR6-WgKguPH11Q4Z4uOe9F1wwFUpw7TUm3VbcCQ3_Q@mail.gmail.com>
Date: Thu, 17 Jul 2014 07:24:07 -0700
Message-ID: <CAPYnQ0Wo2sR7RS-ct6PZ99kwWS_SG3MgekX9Pz2mNde8mVD-7w@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=14dae9cc97c8c0fda504fe646464
X-Virus-Checked: Checked by ClamAV on apache.org

--14dae9cc97c8c0fda504fe646464
Content-Type: text/plain; charset=UTF-8

@Sean and @Sandy

   Thanks for the reply. I used to be able to see yarn-alpha and yarn
directories which corresponding to the modules.

   I guess due to the recent SparkBuild.scala changes, I did not see
yarn-alpha (by default) and I thought yarn-alpha is renamed to "yarn" and
"yarn-stable" is the old yarn. So I compiled "yarn" against the
hadoop.version = 2.0.5-alpha.  My mistake.



I tried
export SPARK_HADOOP_VERSION=2.0.5-alpha
sbt/sbt -Pyarn-alpha  yarn-alpha/test

the compilation errors are all gone.

sbt/sbt -Pyarn-alpha projects

does show the yarn-alpha project, I did not realize this is dynamically
enabled based on yarn flag. Thanks Sean for pointing that out.

To Sandy's point, I am not trying to use alpha version of Yarn. I am
experimenting some changes in Yarn Client and refactoring code and just
want to make sure I am passing tests for both yarn-alpha and yarn-stable.


The yarn-alpha tests actually failing due to the yarn API changes in
MRJobConfig class.

as I mentioned in earlier email

The field
DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH

returns String in yarn-stable, but returns String array in yarn-alpha API.

So the method in ClientBaseSuite.scala


    val knownDefMRAppCP: Seq[String] =
      getFieldValue[String, Seq[String]](classOf[MRJobConfig],

 "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
                                         Seq[String]())(a => a.split(","))

will fail for yarn-alpha.

sbt/sbt -Pyarn-alpha -Dhadoop.version=2.0.5-alpha yarn-alpha/test

...

4/07/17 07:07:16 INFO ClientBase: Using Spark's default log4j profile:
org/apache/spark/log4j-defaults.properties

[info] - default Yarn application classpath *** FAILED ***

[info]   java.lang.ClassCastException: [Ljava.lang.String; cannot be cast
to java.lang.String

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$$anonfun$12.apply(ClientBaseSuite.scala:152)

[info]   at scala.Option.map(Option.scala:145)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.getFieldValue(ClientBaseSuite.scala:180)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$.<init>(ClientBaseSuite.scala:152)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures$lzycompute(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$1.apply$mcV$sp(ClientBaseSuite.scala:47)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$1.apply(ClientBaseSuite.scala:47)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$1.apply(ClientBaseSuite.scala:47)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   ...

[info] - default MR application classpath *** FAILED ***

[info]   java.lang.ClassCastException: [Ljava.lang.String; cannot be cast
to java.lang.String

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$$anonfun$12.apply(ClientBaseSuite.scala:152)

[info]   at scala.Option.map(Option.scala:145)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.getFieldValue(ClientBaseSuite.scala:180)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$.<init>(ClientBaseSuite.scala:152)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures$lzycompute(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$2.apply$mcV$sp(ClientBaseSuite.scala:51)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$2.apply(ClientBaseSuite.scala:51)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$2.apply(ClientBaseSuite.scala:51)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   ...

[info] - resultant classpath for an application that defines a classpath
for YARN *** FAILED ***

[info]   java.lang.ClassCastException: [Ljava.lang.String; cannot be cast
to java.lang.String

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$$anonfun$12.apply(ClientBaseSuite.scala:152)

[info]   at scala.Option.map(Option.scala:145)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.getFieldValue(ClientBaseSuite.scala:180)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$.<init>(ClientBaseSuite.scala:152)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures$lzycompute(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$3.apply$mcV$sp(ClientBaseSuite.scala:55)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$3.apply(ClientBaseSuite.scala:55)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$3.apply(ClientBaseSuite.scala:55)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   ...

[info] - resultant classpath for an application that defines a classpath
for MR *** FAILED ***

[info]   java.lang.ClassCastException: [Ljava.lang.String; cannot be cast
to java.lang.String

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$$anonfun$12.apply(ClientBaseSuite.scala:152)

[info]   at scala.Option.map(Option.scala:145)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.getFieldValue(ClientBaseSuite.scala:180)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$.<init>(ClientBaseSuite.scala:152)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures$lzycompute(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$4.apply$mcV$sp(ClientBaseSuite.scala:64)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$4.apply(ClientBaseSuite.scala:64)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$4.apply(ClientBaseSuite.scala:64)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   ...

[info] - resultant classpath for an application that defines both
classpaths, YARN and MR *** FAILED ***

[info]   java.lang.ClassCastException: [Ljava.lang.String; cannot be cast
to java.lang.String

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$$anonfun$12.apply(ClientBaseSuite.scala:152)

[info]   at scala.Option.map(Option.scala:145)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.getFieldValue(ClientBaseSuite.scala:180)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$Fixtures$.<init>(ClientBaseSuite.scala:152)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures$lzycompute(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite.Fixtures(ClientBaseSuite.scala:141)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$5.apply$mcV$sp(ClientBaseSuite.scala:73)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$5.apply(ClientBaseSuite.scala:73)

[info]   at
org.apache.spark.deploy.yarn.ClientBaseSuite$$anonfun$5.apply(ClientBaseSuite.scala:73)

[info]   at
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)

[info]   ...

[info] - Local jar URIs















On Thu, Jul 17, 2014 at 12:44 AM, Sandy Ryza <sandy.ryza@cloudera.com>
wrote:

> To add, we've made some effort to yarn-alpha to work with the 2.0.x line,
> but this was a time when YARN went through wild API changes.  The only line
> that the yarn-alpha profile is guaranteed to work against is the 0.23 line.
>
>
> On Thu, Jul 17, 2014 at 12:40 AM, Sean Owen <sowen@cloudera.com> wrote:
>
> > Are you setting -Pyarn-alpha? ./sbt/sbt -Pyarn-alpha, followed by
> > "projects", shows it as a module. You should only build yarn-stable
> > *or* yarn-alpha at any given time.
> >
> > I don't remember the modules changing in a while. 'yarn-alpha' is for
> > YARN before it stabilized, circa early Hadoop 2.0.x. 'yarn-stable' is
> > for beta and stable YARN, circa late Hadoop 2.0.x and onwards. 'yarn'
> > is code common to both, so should compile with yarn-alpha.
> >
> > What's the compile error, and are you setting yarn.version? the
> > default is to use hadoop.version, but that defaults to 1.0.4 and there
> > is no such YARN.
> >
> > Unless I missed it, I only see compile errors in yarn-stable, and you
> > are trying to compile vs YARN alpha versions no?
> >
> > On Thu, Jul 17, 2014 at 5:39 AM, Chester Chen <chester@alpinenow.com>
> > wrote:
> > > Looking further, the yarn and yarn-stable are both for the stable
> version
> > > of Yarn, that explains the compilation errors when using 2.0.5-alpha
> > > version of hadoop.
> > >
> > > the module yarn-alpha ( although is still on SparkBuild.scala), is no
> > > longer there in sbt console.
> > >
> > >
> > >> projects
> > >
> > > [info] In file:/Users/chester/projects/spark/
> > >
> > > [info]    assembly
> > >
> > > [info]    bagel
> > >
> > > [info]    catalyst
> > >
> > > [info]    core
> > >
> > > [info]    examples
> > >
> > > [info]    graphx
> > >
> > > [info]    hive
> > >
> > > [info]    mllib
> > >
> > > [info]    oldDeps
> > >
> > > [info]    repl
> > >
> > > [info]    spark
> > >
> > > [info]    sql
> > >
> > > [info]    streaming
> > >
> > > [info]    streaming-flume
> > >
> > > [info]    streaming-kafka
> > >
> > > [info]    streaming-mqtt
> > >
> > > [info]    streaming-twitter
> > >
> > > [info]    streaming-zeromq
> > >
> > > [info]    tools
> > >
> > > [info]    yarn
> > >
> > > [info]  * yarn-stable
> > >
> > >
> > > On Wed, Jul 16, 2014 at 5:41 PM, Chester Chen <chester@alpinenow.com>
> > wrote:
> > >
> > >> Hmm
> > >> looks like a Build script issue:
> > >>
> > >> I run the command with :
> > >>
> > >> sbt/sbt clean *yarn/*test:compile
> > >>
> > >> but errors came from
> > >>
> > >> [error] 40 errors found
> > >>
> > >> [error] (*yarn-stable*/compile:compile) Compilation failed
> > >>
> > >>
> > >> Chester
> > >>
> > >>
> > >> On Wed, Jul 16, 2014 at 5:18 PM, Chester Chen <chester@alpinenow.com>
> > >> wrote:
> > >>
> > >>> Hi, Sandy
> > >>>
> > >>>     We do have some issue with this. The difference is in Yarn-Alpha
> > and
> > >>> Yarn Stable ( I noticed that in the latest build, the module name has
> > >>> changed,
> > >>>      yarn-alpha --> yarn
> > >>>      yarn --> yarn-stable
> > >>> )
> > >>>
> > >>> For example:  MRJobConfig.class
> > >>> the field:
> > >>> "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH"
> > >>>
> > >>>
> > >>> In Yarn-Alpha : the field returns   java.lang.String[]
> > >>>
> > >>>   java.lang.String[] DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
> > >>>
> > >>> while in Yarn-Stable, it returns a String
> > >>>
> > >>>   java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
> > >>>
> > >>> So in ClientBaseSuite.scala
> > >>>
> > >>> The following code:
> > >>>
> > >>>     val knownDefMRAppCP: Seq[String] =
> > >>>       getFieldValue[*String*, Seq[String]](classOf[MRJobConfig],
> > >>>
> > >>>  "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
> > >>>                                          Seq[String]())(a =>
> > >>> *a.split(",")*)
> > >>>
> > >>>
> > >>> works for yarn-stable, but doesn't work for yarn-alpha.
> > >>>
> > >>> This is the only failure for the SNAPSHOT I downloaded 2 weeks ago.
>  I
> > >>> believe this can be refactored to yarn-alpha module and make
> different
> > >>> tests according different API signatures.
> > >>>
> > >>>  I just update the master branch and build doesn't even compile for
> > >>> Yarn-Alpha (yarn) model. Yarn-Stable compile with no error and test
> > passed.
> > >>>
> > >>>
> > >>> Does the Spark Jenkins job run against yarn-alpha ?
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>> Here is output from yarn-alpha compilation:
> > >>>
> > >>> I got the 40 compilation errors.
> > >>>
> > >>> sbt/sbt clean yarn/test:compile
> > >>>
> > >>> Using /Library/Java/JavaVirtualMachines/jdk1.7.0_51.jdk/Contents/Home
> > as
> > >>> default JAVA_HOME.
> > >>>
> > >>> Note, this will be overridden by -java-home if it is set.
> > >>>
> > >>> [info] Loading project definition from
> > >>> /Users/chester/projects/spark/project/project
> > >>>
> > >>> [info] Loading project definition from
> > >>>
> >
> /Users/chester/.sbt/0.13/staging/ec3aa8f39111944cc5f2/sbt-pom-reader/project
> > >>>
> > >>> [warn] Multiple resolvers having different access mechanism
> configured
> > >>> with same name 'sbt-plugin-releases'. To avoid conflict, Remove
> > duplicate
> > >>> project resolvers (`resolvers`) or rename publishing resolver
> > (`publishTo`).
> > >>>
> > >>> [info] Loading project definition from
> > >>> /Users/chester/projects/spark/project
> > >>>
> > >>> NOTE: SPARK_HADOOP_VERSION is deprecated, please use
> > >>> -Dhadoop.version=2.0.5-alpha
> > >>>
> > >>> NOTE: SPARK_YARN is deprecated, please use -Pyarn flag.
> > >>>
> > >>> [info] Set current project to spark-parent (in build
> > >>> file:/Users/chester/projects/spark/)
> > >>>
> > >>> [success] Total time: 0 s, completed Jul 16, 2014 5:13:06 PM
> > >>>
> > >>> [info] Updating {file:/Users/chester/projects/spark/}core...
> > >>>
> > >>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> > >>>
> > >>> [info] Done updating.
> > >>>
> > >>> [info] Updating {file:/Users/chester/projects/spark/}yarn...
> > >>>
> > >>> [info] Updating {file:/Users/chester/projects/spark/}yarn-stable...
> > >>>
> > >>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> > >>>
> > >>> [info] Done updating.
> > >>>
> > >>> [info] Resolving commons-net#commons-net;3.1 ...
> > >>>
> > >>> [info] Compiling 358 Scala sources and 34 Java sources to
> > >>> /Users/chester/projects/spark/core/target/scala-2.10/classes...
> > >>>
> > >>> [info] Resolving org.fusesource.jansi#jansi;1.4 ...
> > >>>
> > >>> [info] Done updating.
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/hadoop/mapred/SparkHadoopMapRedUtil.scala:43:
> > >>> constructor TaskAttemptID in class TaskAttemptID is deprecated: see
> > >>> corresponding Javadoc for more information.
> > >>>
> > >>> [warn]     new TaskAttemptID(jtIdentifier, jobId, isMap, taskId,
> > >>> attemptId)
> > >>>
> > >>> [warn]     ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:501:
> > >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> > for
> > >>> more information.
> > >>>
> > >>> [warn]     val job = new NewHadoopJob(hadoopConfiguration)
> > >>>
> > >>> [warn]               ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkContext.scala:634:
> > >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> > for
> > >>> more information.
> > >>>
> > >>> [warn]     val job = new NewHadoopJob(conf)
> > >>>
> > >>> [warn]               ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:167:
> > >>> constructor TaskID in class TaskID is deprecated: see corresponding
> > Javadoc
> > >>> for more information.
> > >>>
> > >>> [warn]         new TaskAttemptID(new TaskID(jID.value, true,
> splitID),
> > >>> attemptID))
> > >>>
> > >>> [warn]                           ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/SparkHadoopWriter.scala:188:
> > >>> method makeQualified in class Path is deprecated: see corresponding
> > Javadoc
> > >>> for more information.
> > >>>
> > >>> [warn]     outputPath.makeQualified(fs)
> > >>>
> > >>> [warn]                ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:84:
> > >>> method isDir in class FileStatus is deprecated: see corresponding
> > Javadoc
> > >>> for more information.
> > >>>
> > >>> [warn]     if (!fs.getFileStatus(path).isDir) {
> > >>>
> > >>> [warn]                                 ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala:118:
> > >>> method isDir in class FileStatus is deprecated: see corresponding
> > Javadoc
> > >>> for more information.
> > >>>
> > >>> [warn]       val logDirs = if (logStatus != null)
> > >>> logStatus.filter(_.isDir).toSeq else Seq[FileStatus]()
> > >>>
> > >>> [warn]
> ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/input/WholeTextFileInputFormat.scala:56:
> > >>> method isDir in class FileStatus is deprecated: see corresponding
> > Javadoc
> > >>> for more information.
> > >>>
> > >>> [warn]       if (file.isDir) 0L else file.getLen
> > >>>
> > >>> [warn]                ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/CheckpointRDD.scala:110:
> > >>> method getDefaultReplication in class FileSystem is deprecated: see
> > >>> corresponding Javadoc for more information.
> > >>>
> > >>> [warn]       fs.create(tempOutputPath, false, bufferSize,
> > >>> fs.getDefaultReplication, blockSize)
> > >>>
> > >>> [warn]                                                       ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/HadoopRDD.scala:267:
> > >>> constructor TaskID in class TaskID is deprecated: see corresponding
> > Javadoc
> > >>> for more information.
> > >>>
> > >>> [warn]     val taId = new TaskAttemptID(new TaskID(jobID, true,
> > >>> splitId), attemptId)
> > >>>
> > >>> [warn]                                  ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:767:
> > >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> > for
> > >>> more information.
> > >>>
> > >>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
> > >>>
> > >>> [warn]               ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala:830:
> > >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> > for
> > >>> more information.
> > >>>
> > >>> [warn]     val job = new NewAPIHadoopJob(hadoopConf)
> > >>>
> > >>> [warn]               ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala:185:
> > >>> method isDir in class FileStatus is deprecated: see corresponding
> > Javadoc
> > >>> for more information.
> > >>>
> > >>> [warn]           fileStatuses.filter(!_.isDir).map(_.getPath).toSeq
> > >>>
> > >>> [warn]                                  ^
> > >>>
> > >>> [warn]
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/scala/org/apache/spark/scheduler/InputFormatInfo.scala:106:
> > >>> constructor Job in class Job is deprecated: see corresponding Javadoc
> > for
> > >>> more information.
> > >>>
> > >>> [warn]     val job = new Job(conf)
> > >>>
> > >>> [warn]               ^
> > >>>
> > >>> [warn] 14 warnings found
> > >>>
> > >>> [warn] Note:
> > >>>
> >
> /Users/chester/projects/spark/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java
> > >>> uses unchecked or unsafe operations.
> > >>>
> > >>> [warn] Note: Recompile with -Xlint:unchecked for details.
> > >>>
> > >>> [info] Compiling 15 Scala sources to
> > >>>
> /Users/chester/projects/spark/yarn/stable/target/scala-2.10/classes...
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:26:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.client.api.YarnClient
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:40:
> > >>> not found: value YarnClient
> > >>>
> > >>> [error]   val yarnClient = YarnClient.createYarnClient
> > >>>
> > >>> [error]                    ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:32:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:33:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import
> > >>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:36:
> > >>> object util is not a member of package org.apache.hadoop.yarn.webapp
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:63:
> > >>> value RM_AM_MAX_ATTEMPTS is not a member of object
> > >>> org.apache.hadoop.yarn.conf.YarnConfiguration
> > >>>
> > >>> [error]     YarnConfiguration.RM_AM_MAX_ATTEMPTS,
> > >>> YarnConfiguration.DEFAULT_RM_AM_MAX_ATTEMPTS)
> > >>>
> > >>> [error]                       ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:65:
> > >>> not found: type AMRMClient
> > >>>
> > >>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
> > >>>
> > >>> [error]                         ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:91:
> > >>> not found: value AMRMClient
> > >>>
> > >>> [error]     amClient = AMRMClient.createAMRMClient()
> > >>>
> > >>> [error]                ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:136:
> > >>> not found: value WebAppUtils
> > >>>
> > >>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
> > >>>
> > >>> [error]                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:40:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:618:
> > >>> not found: type AMRMClient
> > >>>
> > >>> [error]       amClient: AMRMClient[ContainerRequest],
> > >>>
> > >>> [error]                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:596:
> > >>> not found: type AMRMClient
> > >>>
> > >>> [error]       amClient: AMRMClient[ContainerRequest],
> > >>>
> > >>> [error]                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:577:
> > >>> not found: type AMRMClient
> > >>>
> > >>> [error]       amClient: AMRMClient[ContainerRequest],
> > >>>
> > >>> [error]                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala:452:
> > >>> value CONTAINER_ID is not a member of object
> > >>> org.apache.hadoop.yarn.api.ApplicationConstants.Environment
> > >>>
> > >>> [error]     val containerIdString = System.getenv(
> > >>> ApplicationConstants.Environment.CONTAINER_ID.name())
> > >>>
> > >>> [error]
> > >>>           ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/Client.scala:128:
> > >>> value setTokens is not a member of
> > >>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
> > >>>
> > >>> [error]     amContainer.setTokens(ByteBuffer.wrap(dob.getData()))
> > >>>
> > >>> [error]                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:36:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.client.api.AMRMClient
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:37:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import
> > >>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:39:
> > >>> object util is not a member of package org.apache.hadoop.yarn.webapp
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.webapp.util.WebAppUtils
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:62:
> > >>> not found: type AMRMClient
> > >>>
> > >>> [error]   private var amClient: AMRMClient[ContainerRequest] = _
> > >>>
> > >>> [error]                         ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:99:
> > >>> not found: value AMRMClient
> > >>>
> > >>> [error]     amClient = AMRMClient.createAMRMClient()
> > >>>
> > >>> [error]                ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorLauncher.scala:158:
> > >>> not found: value WebAppUtils
> > >>>
> > >>> [error]     val proxy = WebAppUtils.getProxyHostAndPort(conf)
> > >>>
> > >>> [error]                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:31:
> > >>> object ProtoUtils is not a member of package
> > >>> org.apache.hadoop.yarn.api.records.impl.pb
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.api.records.impl.pb.ProtoUtils
> > >>>
> > >>> [error]        ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:33:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.client.api.NMClient
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:53:
> > >>> not found: type NMClient
> > >>>
> > >>> [error]   var nmClient: NMClient = _
> > >>>
> > >>> [error]                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:59:
> > >>> not found: value NMClient
> > >>>
> > >>> [error]     nmClient = NMClient.createNMClient()
> > >>>
> > >>> [error]                ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/ExecutorRunnable.scala:79:
> > >>> value setTokens is not a member of
> > >>> org.apache.hadoop.yarn.api.records.ContainerLaunchContext
> > >>>
> > >>> [error]     ctx.setTokens(ByteBuffer.wrap(dob.getData()))
> > >>>
> > >>> [error]         ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:35:
> > >>> object ApplicationMasterProtocol is not a member of package
> > >>> org.apache.hadoop.yarn.api
> > >>>
> > >>> [error] import org.apache.hadoop.yarn.api.ApplicationMasterProtocol
> > >>>
> > >>> [error]        ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:41:
> > >>> object api is not a member of package org.apache.hadoop.yarn.client
> > >>>
> > >>> [error] import
> > >>> org.apache.hadoop.yarn.client.api.AMRMClient.ContainerRequest
> > >>>
> > >>> [error]                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:65:
> > >>> not found: type AMRMClient
> > >>>
> > >>> [error]     val amClient: AMRMClient[ContainerRequest],
> > >>>
> > >>> [error]                   ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:389:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]     ): ArrayBuffer[ContainerRequest] = {
> > >>>
> > >>> [error]                    ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:388:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]       hostContainers: ArrayBuffer[ContainerRequest]
> > >>>
> > >>> [error]                                   ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:405:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]     val requestedContainers = new
> > >>> ArrayBuffer[ContainerRequest](rackToCounts.size)
> > >>>
> > >>> [error]                                               ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:434:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]     val containerRequests: List[ContainerRequest] =
> > >>>
> > >>> [error]                                 ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:508:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]     ): ArrayBuffer[ContainerRequest] = {
> > >>>
> > >>> [error]                    ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:446:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]         val hostContainerRequests = new
> > >>> ArrayBuffer[ContainerRequest](preferredHostToCount.size)
> > >>>
> > >>> [error]                                                     ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:458:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]         val rackContainerRequests: List[ContainerRequest] =
> > >>> createRackResourceRequests(
> > >>>
> > >>> [error]                                         ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:467:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]         val containerRequestBuffer = new
> > >>> ArrayBuffer[ContainerRequest](
> > >>>
> > >>> [error]                                                      ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:542:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]     ): ArrayBuffer[ContainerRequest] = {
> > >>>
> > >>> [error]                    ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:545:
> > >>> value newInstance is not a member of object
> > >>> org.apache.hadoop.yarn.api.records.Resource
> > >>>
> > >>> [error]     val resource = Resource.newInstance(memoryRequest,
> > >>> executorCores)
> > >>>
> > >>> [error]                             ^
> > >>>
> > >>> [error]
> > >>>
> >
> /Users/chester/projects/spark/yarn/stable/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocationHandler.scala:550:
> > >>> not found: type ContainerRequest
> > >>>
> > >>> [error]     val requests = new ArrayBuffer[ContainerRequest]()
> > >>>
> > >>> [error]                                    ^
> > >>>
> > >>> [error] 40 errors found
> > >>>
> > >>> [error] (yarn-stable/compile:compile) Compilation failed
> > >>>
> > >>> [error] Total time: 98 s, completed Jul 16, 2014 5:14:44 PM
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>>
> > >>> On Wed, Jul 16, 2014 at 4:19 PM, Sandy Ryza <sandy.ryza@cloudera.com
> >
> > >>> wrote:
> > >>>
> > >>>> Hi Ron,
> > >>>>
> > >>>> I just checked and this bug is fixed in recent releases of Spark.
> > >>>>
> > >>>> -Sandy
> > >>>>
> > >>>>
> > >>>> On Sun, Jul 13, 2014 at 8:15 PM, Chester Chen <
> chester@alpinenow.com>
> > >>>> wrote:
> > >>>>
> > >>>>> Ron,
> > >>>>>     Which distribution and Version of Hadoop are you using ?
> > >>>>>
> > >>>>>      I just looked at CDH5 (  hadoop-mapreduce-client-core-
> > >>>>> 2.3.0-cdh5.0.0),
> > >>>>>
> > >>>>> MRJobConfig does have the field :
> > >>>>>
> > >>>>> java.lang.String DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH;
> > >>>>>
> > >>>>> Chester
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>> On Sun, Jul 13, 2014 at 6:49 PM, Ron Gonzalez <
> zlgonzalez@yahoo.com>
> > >>>>> wrote:
> > >>>>>
> > >>>>>> Hi,
> > >>>>>>   I was doing programmatic submission of Spark yarn jobs and I saw
> > >>>>>> code in ClientBase.getDefaultYarnApplicationClasspath():
> > >>>>>>
> > >>>>>> val field =
> > >>>>>> classOf[MRJobConfig].getField("DEFAULT_YARN_APPLICATION_CLASSPATH)
> > >>>>>> MRJobConfig doesn't have this field so the created launch env is
> > >>>>>> incomplete. Workaround is to set yarn.application.classpath with
> > the value
> > >>>>>> from YarnConfiguration.DEFAULT_YARN_APPLICATION_CLASSPATH.
> > >>>>>>
> > >>>>>> This results in having the spark job hang if the submission config
> > is
> > >>>>>> different from the default config. For example, if my resource
> > manager port
> > >>>>>> is 8050 instead of 8030, then the spark app is not able to
> register
> > itself
> > >>>>>> and stays in ACCEPTED state.
> > >>>>>>
> > >>>>>> I can easily fix this by changing this to YarnConfiguration
> instead
> > of
> > >>>>>> MRJobConfig but was wondering what the steps are for submitting a
> > fix.
> > >>>>>>
> > >>>>>> Thanks,
> > >>>>>> Ron
> > >>>>>>
> > >>>>>> Sent from my iPhone
> > >>>>>
> > >>>>>
> > >>>>>
> > >>>>
> > >>>
> > >>
> >
>

--14dae9cc97c8c0fda504fe646464--

From dev-return-8414-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 14:43:41 2014
Return-Path: <dev-return-8414-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E2EB51100D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 14:43:40 +0000 (UTC)
Received: (qmail 60220 invoked by uid 500); 17 Jul 2014 14:43:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60155 invoked by uid 500); 17 Jul 2014 14:43:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60143 invoked by uid 99); 17 Jul 2014 14:43:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:43:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.216.54 as permitted sender)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:43:37 +0000
Received: by mail-qa0-f54.google.com with SMTP id k15so1908700qaq.41
        for <dev@spark.incubator.apache.org>; Thu, 17 Jul 2014 07:43:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=rGARiFKze987xiDwel5HOsp2KsHGo8ToEN0RYtjQesM=;
        b=Xwho03bHuJEoVDAEomQm4baej+iZ/hSqn0GU2tKmQaLl+eHdVMSi9F8L+VGOqglRRX
         NS1E5zehr8zjAUdOvgKoZh7Bg2v+cVFMPT06ML+HIuUbB3D/PPsO5YA44b5xgpYmdGgd
         TfdwfgR+8FmXyamZ28jzUezfopZrWVOIlqFj4=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=rGARiFKze987xiDwel5HOsp2KsHGo8ToEN0RYtjQesM=;
        b=Y6HfCJBQxxY94GJaX02Jyse0LRJgkfqXfQr9886SOEeWpmB6EReQ3j6sG44bVof/Q9
         j0my0xdIxaMEi+j6/Lfd1taqP933lw3Pw0XJKbus1+4jWpCZVmVjyDPoqfXqH44et9Vi
         U4vICc5ryFly/gKGkbs5MQVkpEgeUH477/zUQPINLP2+b7Fgem+T2OVWuL+c4EVW690g
         jCGSQdaWitZkyKPZzZ6FmwazslKRhqGqOdv8ma1gXl8GTNCuHhI94adESk1pE3jmSljL
         6eDq9B0as3EG15odkGZwxxNDD3bmmiNjb8Jlxn5mV4bQMT56ZeEPqUUiqrzYsWqxcn7S
         2RwQ==
X-Gm-Message-State: ALoCoQnAOsSarrP84iJuCDDwsXlNJS4nvPz8+cZXTDta84Thb+FnJaU6wFFwfK9pNquCNp3Mq5KW
MIME-Version: 1.0
X-Received: by 10.140.106.74 with SMTP id d68mr21380703qgf.103.1405608192373;
 Thu, 17 Jul 2014 07:43:12 -0700 (PDT)
Received: by 10.96.20.8 with HTTP; Thu, 17 Jul 2014 07:43:12 -0700 (PDT)
Date: Thu, 17 Jul 2014 10:43:12 -0400
Message-ID: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
Subject: Compile error when compiling for cloudera
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a113b4ecc000feb04fe64a96c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b4ecc000feb04fe64a96c
Content-Type: text/plain; charset=UTF-8

I'm trying to compile the latest code, with the hadoop-version set for
2.0.0-mr1-cdh4.6.0.

I'm getting the following error, which I don't get when I don't set the
hadoop version:

[error]
/data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
overloaded method constructor NioServerSocketChannelFactory with
alternatives:
[error]   (x$1: java.util.concurrent.Executor,x$2:
java.util.concurrent.Executor,x$3:
Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory <and>
[error]   (x$1: java.util.concurrent.Executor,x$2:
java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
[error]  cannot be applied to ()
[error]       val channelFactory = new NioServerSocketChannelFactory
[error]                            ^
[error] one error found


I don't know flume from a hole in the wall - does anyone know what I can do
to fix this?


Thanks,
         -Nathan


-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--001a113b4ecc000feb04fe64a96c--

From dev-return-8415-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 14:56:52 2014
Return-Path: <dev-return-8415-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 906ED11094
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 14:56:52 +0000 (UTC)
Received: (qmail 87728 invoked by uid 500); 17 Jul 2014 14:56:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 87677 invoked by uid 500); 17 Jul 2014 14:56:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87662 invoked by uid 99); 17 Jul 2014 14:56:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:56:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:56:46 +0000
Received: by mail-vc0-f178.google.com with SMTP id la4so4821780vcb.9
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 07:56:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=SSpfD+4FoAV7eXr1RrVwfMW6cV1nvJJ1tW/MbLiLdOc=;
        b=CS1xdA3fPf3hiaZsr1BJIYpgAsMXcYAWabzqBRJE2YKwk2VDN3IJPMOFypbVlEjSVr
         +w1Epu8EoL2eEqBsLl21fWQQDeyBhyFr/UiAXHE7w2ybCkiJwJU8Rb52zb2oF9a2mvNH
         +Q0CvMu4CkA1lg6khqImICzRtebCTTTkhLxQpBMwrQgr63YC+oIh25lgNhQzVUt3vanv
         9z44yY/pacZDmc0/F+Y92IWdBkka5TM2zCyta+1VXfQekgH5njnKkCgwJsNW9x5Ah7/0
         tW2wUvTIKImzhsMuIjzvwMkj+X7iBj6jNo8Sxh1CEy97UWamyqzfmyp0i+LqyxzDaPxj
         wUAA==
X-Gm-Message-State: ALoCoQlZNFtDU4vVjGmR3AC/xx6FrU/teZkayp4stciobyl8mAObTY3LrbUDbC7SFhSLqUzakFjk
X-Received: by 10.52.77.97 with SMTP id r1mr22975498vdw.31.1405608985501; Thu,
 17 Jul 2014 07:56:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Thu, 17 Jul 2014 07:56:05 -0700 (PDT)
In-Reply-To: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
References: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 17 Jul 2014 15:56:05 +0100
Message-ID: <CAMAsSdKknzWUApb246coB_LSXsZs_UOF+BDADg4aE+rXtWmQWQ@mail.gmail.com>
Subject: Re: Compile error when compiling for cloudera
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

This looks like a Jetty version problem actually. Are you bringing in
something that might be changing the version of Jetty used by Spark?
It depends a lot on how you are building things.

Good to specify exactly how your'e building here.

On Thu, Jul 17, 2014 at 3:43 PM, Nathan Kronenfeld
<nkronenfeld@oculusinfo.com> wrote:
> I'm trying to compile the latest code, with the hadoop-version set for
> 2.0.0-mr1-cdh4.6.0.
>
> I'm getting the following error, which I don't get when I don't set the
> hadoop version:
>
> [error]
> /data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
> overloaded method constructor NioServerSocketChannelFactory with
> alternatives:
> [error]   (x$1: java.util.concurrent.Executor,x$2:
> java.util.concurrent.Executor,x$3:
> Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory <and>
> [error]   (x$1: java.util.concurrent.Executor,x$2:
> java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
> [error]  cannot be applied to ()
> [error]       val channelFactory = new NioServerSocketChannelFactory
> [error]                            ^
> [error] one error found
>
>
> I don't know flume from a hole in the wall - does anyone know what I can do
> to fix this?
>
>
> Thanks,
>          -Nathan
>
>
> --
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com

From dev-return-8416-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 14:59:06 2014
Return-Path: <dev-return-8416-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D427110A5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 14:59:06 +0000 (UTC)
Received: (qmail 96335 invoked by uid 500); 17 Jul 2014 14:59:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96271 invoked by uid 500); 17 Jul 2014 14:59:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96248 invoked by uid 99); 17 Jul 2014 14:59:05 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:59:05 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.216.177 as permitted sender)
Received: from [209.85.216.177] (HELO mail-qc0-f177.google.com) (209.85.216.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:59:02 +0000
Received: by mail-qc0-f177.google.com with SMTP id o8so2192007qcw.8
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 07:58:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RlK4aCErJY2hqwGe0p/Iwe4Y+9WXlqN7lcNxzq6GA50=;
        b=DVz7pTMU5KEUMjOG1i8Ifmtqe6xNJKDkc4csQ4QgrigvrcC5F9/Gx5BqeQnjNp9+sz
         O6KKXV3Gxp/oGKmmu3V47tqnyWVgGPmdhhLXgVHGG3Ma0BCwOD5V/nXUn4bx5+mXQuLF
         GEAErAojYZbpEcdDDDGRMK3OzdT1sKhZ3w590=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=RlK4aCErJY2hqwGe0p/Iwe4Y+9WXlqN7lcNxzq6GA50=;
        b=O6ZYLpL61yTIJ/XqQn/oqaDFbS+W8oLiiBYbIKXLkRExxuWEdJZMCNyBT1I5ECi+0T
         ySD0XGVcEJ6JY499TMyeb3PNQ3NgzZUuVAiGUBnydq3VTa6BCUNOtm3/De9G6wfrkKtD
         sAcFfSAq2rNLGgxmcenpcPEkOZQhnNFL9wqV+nj9CKobI9+no9ttDDuwm7SDKSchaNjl
         OvuUznIV87oSpuc7725UAIXksAYWakFidxw9lum9VQMTln83I4n0kz7axMTXewavWNEa
         T5QbmtSZgW4QpLf5h9ohL6nHp9Y+NC3/dLrMOssP86ws1XftQ6njWQDmbQSNBgOpz95M
         BHXA==
X-Gm-Message-State: ALoCoQkoQ7eOq3KIj4EwFefPHSfM3NetnvwagzzmbvBqaFFEtzS9iBY2E4QEXug7HwhP3llxFOPN
MIME-Version: 1.0
X-Received: by 10.224.65.73 with SMTP id h9mr57281607qai.83.1405609117881;
 Thu, 17 Jul 2014 07:58:37 -0700 (PDT)
Received: by 10.96.20.8 with HTTP; Thu, 17 Jul 2014 07:58:37 -0700 (PDT)
In-Reply-To: <CAMAsSdKknzWUApb246coB_LSXsZs_UOF+BDADg4aE+rXtWmQWQ@mail.gmail.com>
References: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
	<CAMAsSdKknzWUApb246coB_LSXsZs_UOF+BDADg4aE+rXtWmQWQ@mail.gmail.com>
Date: Thu, 17 Jul 2014 10:58:37 -0400
Message-ID: <CAEpWh49S0E96v=526GA8=xB9yGCzY0p6zmdYp4xzprpZ-jox8w@mail.gmail.com>
Subject: Re: Compile error when compiling for cloudera
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ba522a396304fe64e044
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ba522a396304fe64e044
Content-Type: text/plain; charset=UTF-8

My full build command is:
./sbt/sbt -Dhadoop.version=2.0.0-mr1-cdh4.6.0 clean assembly


I've changed one line in RDD.scala, nothing else.



On Thu, Jul 17, 2014 at 10:56 AM, Sean Owen <sowen@cloudera.com> wrote:

> This looks like a Jetty version problem actually. Are you bringing in
> something that might be changing the version of Jetty used by Spark?
> It depends a lot on how you are building things.
>
> Good to specify exactly how your'e building here.
>
> On Thu, Jul 17, 2014 at 3:43 PM, Nathan Kronenfeld
> <nkronenfeld@oculusinfo.com> wrote:
> > I'm trying to compile the latest code, with the hadoop-version set for
> > 2.0.0-mr1-cdh4.6.0.
> >
> > I'm getting the following error, which I don't get when I don't set the
> > hadoop version:
> >
> > [error]
> >
> /data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
> > overloaded method constructor NioServerSocketChannelFactory with
> > alternatives:
> > [error]   (x$1: java.util.concurrent.Executor,x$2:
> > java.util.concurrent.Executor,x$3:
> > Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
> <and>
> > [error]   (x$1: java.util.concurrent.Executor,x$2:
> >
> java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
> > [error]  cannot be applied to ()
> > [error]       val channelFactory = new NioServerSocketChannelFactory
> > [error]                            ^
> > [error] one error found
> >
> >
> > I don't know flume from a hole in the wall - does anyone know what I can
> do
> > to fix this?
> >
> >
> > Thanks,
> >          -Nathan
> >
> >
> > --
> > Nathan Kronenfeld
> > Senior Visualization Developer
> > Oculus Info Inc
> > 2 Berkeley Street, Suite 600,
> > Toronto, Ontario M5A 4J5
> > Phone:  +1-416-203-3003 x 238
> > Email:  nkronenfeld@oculusinfo.com
>



-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--001a11c2ba522a396304fe64e044--

From dev-return-8417-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 14:59:07 2014
Return-Path: <dev-return-8417-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 55F72110A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 14:59:07 +0000 (UTC)
Received: (qmail 97236 invoked by uid 500); 17 Jul 2014 14:59:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97147 invoked by uid 500); 17 Jul 2014 14:59:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96261 invoked by uid 99); 17 Jul 2014 14:59:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:59:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 14:59:01 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so4885321vcb.13
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 07:58:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=qm5Sy0NMIqx9rFPaXjStiwBaRzxcdHUTPQGoPcWKRz8=;
        b=lGYTOFMJ+SiEKYODuNaNAqv5T7gyCF0iz9oILUX9IxQYGSfdNuPowwmJMjKvsP7W3p
         bstQqI7tiC8iZEwapdBoE0T+RdPXNlGw1Z54COezY3ZTcFmI5clEAUzogMnhHMqqqFXA
         HCZ+Swl327wXznKVVd8YRoAfphs4ZfXcQFZuEwlcDgvm7TiiAYuR3crYCdf8kZ+JpSIs
         PqsvWiPUwMUKHaI4LUKm2PzzMQWUirJynRQvtuhZ7OJrEjlGzlYBlTLbVfdfA/ySRCjd
         bXhOriadGGli7J7qCzQt0iucQhcDkSGf1lrtPXC7yXC7NOiJKjHnDvbChlD4ElyZEVQ3
         qXAg==
X-Gm-Message-State: ALoCoQlBvGkp5VjXdyTayVuD5zXZ06dGAvslOZ/OMgo6mgh4HPDBlevXLQOkAir3DfjcIOMJ7WyG
X-Received: by 10.52.120.83 with SMTP id la19mr15741335vdb.68.1405609120315;
 Thu, 17 Jul 2014 07:58:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Thu, 17 Jul 2014 07:58:20 -0700 (PDT)
In-Reply-To: <CAPYnQ0Wo2sR7RS-ct6PZ99kwWS_SG3MgekX9Pz2mNde8mVD-7w@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
 <CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
 <CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
 <CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
 <CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com>
 <CAPYnQ0W3ZpR46eGSiwBwXuoKDbkd6JU3iq3+trj-s5K2AsRVpw@mail.gmail.com>
 <CAMAsSdJh_o7tBtGmCioxpMQMoWqCheVY0eA5zBo27_8i_wz6iQ@mail.gmail.com>
 <CACBYxKKwGR6-WgKguPH11Q4Z4uOe9F1wwFUpw7TUm3VbcCQ3_Q@mail.gmail.com> <CAPYnQ0Wo2sR7RS-ct6PZ99kwWS_SG3MgekX9Pz2mNde8mVD-7w@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 17 Jul 2014 15:58:20 +0100
Message-ID: <CAMAsSd+GSX4pwZhRjHZw5y_gmBeCW2NJwRcLt058YuWS1u2w1Q@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Looks like a real problem. I see it too. I think the same workaround
found in ClientBase.scala needs to be used here. There, the fact that
this field can be a String or String[] is handled explicitly. In fact
I think you can just call to ClientBase for this? PR it, I say.

On Thu, Jul 17, 2014 at 3:24 PM, Chester Chen <chester@alpinenow.com> wrote:
>     val knownDefMRAppCP: Seq[String] =
>       getFieldValue[String, Seq[String]](classOf[MRJobConfig],
>
>  "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
>                                          Seq[String]())(a => a.split(","))
>
> will fail for yarn-alpha.
>
> sbt/sbt -Pyarn-alpha -Dhadoop.version=2.0.5-alpha yarn-alpha/test
>

From dev-return-8418-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 15:00:09 2014
Return-Path: <dev-return-8418-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 883BB110AD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 15:00:09 +0000 (UTC)
Received: (qmail 99163 invoked by uid 500); 17 Jul 2014 15:00:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99108 invoked by uid 500); 17 Jul 2014 15:00:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99096 invoked by uid 99); 17 Jul 2014 15:00:08 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 15:00:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.216.46 as permitted sender)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 15:00:05 +0000
Received: by mail-qa0-f46.google.com with SMTP id v10so1912607qac.19
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 07:59:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/GKoab5h5rMX16jQeMRsj4pBTDq74nYU8GYmbFj1k2s=;
        b=lzOWQCo29AczMfKGPPYLkflz7uGUejqqW+wxtjvjq5RcVDjy1E4XG+TSdDRTFv7oMg
         vo2+erRowWOphoJTi3i4UkKScLLrzqN4MwDGX/Q3qP+8Q9HV0twe/YNYmRPKgUeE7CQv
         9cXk6ImwrDVmOiSfRIZNQorHdk8dasCRy5zqU=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=/GKoab5h5rMX16jQeMRsj4pBTDq74nYU8GYmbFj1k2s=;
        b=cHMAbQBmuqS9CvD4Eheg3e2l6j1yh2equbCbp++pvtpwBQkjtpRGo6Ru+AN2QVZHj/
         Zt7C19g8F5DWpiwTQFo2Kc1c/NrQDjWTW9jluzPqiGN6NRFMBlR8DgaFB6Q8lQxlpdRS
         7fgA3ZGsb/N8VRmn2jS9NX8d8l1ohELrNKVV6b3gqi/tysD5O85MaTpNZwo9znz1vYGO
         gHugaakO0gseJEJ2A9DH1HJbZwkuUSMIA1Szil2wSvueg4/N+RuB/9vI8U91fhwzWs5W
         RS5rZN77i+ztic3B9SKDPm869EfShIH98WgbLKvDKc0zuXA4N4ZqLuWAs7W+qNcEi913
         oF8A==
X-Gm-Message-State: ALoCoQkxd1xlQvOkmeKtpD+0mGD6bzmgs7jjvpqBfh9Egj+599F8D+s+R87gfe9T5NY2sNVjcTBn
MIME-Version: 1.0
X-Received: by 10.140.80.19 with SMTP id b19mr56220025qgd.102.1405609181111;
 Thu, 17 Jul 2014 07:59:41 -0700 (PDT)
Received: by 10.96.20.8 with HTTP; Thu, 17 Jul 2014 07:59:41 -0700 (PDT)
In-Reply-To: <CAEpWh49S0E96v=526GA8=xB9yGCzY0p6zmdYp4xzprpZ-jox8w@mail.gmail.com>
References: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
	<CAMAsSdKknzWUApb246coB_LSXsZs_UOF+BDADg4aE+rXtWmQWQ@mail.gmail.com>
	<CAEpWh49S0E96v=526GA8=xB9yGCzY0p6zmdYp4xzprpZ-jox8w@mail.gmail.com>
Date: Thu, 17 Jul 2014 10:59:41 -0400
Message-ID: <CAEpWh4-sjkj_tYaztZDNa5KzV0r+Yr89ONG6YUu0Qr+ZbZkwWA@mail.gmail.com>
Subject: Re: Compile error when compiling for cloudera
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c12ba4ef03c504fe64e319
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12ba4ef03c504fe64e319
Content-Type: text/plain; charset=UTF-8

er, that line being in toDebugString, where it really shouldn't affect
anything (no signature changes or the like)


On Thu, Jul 17, 2014 at 10:58 AM, Nathan Kronenfeld <
nkronenfeld@oculusinfo.com> wrote:

> My full build command is:
> ./sbt/sbt -Dhadoop.version=2.0.0-mr1-cdh4.6.0 clean assembly
>
>
> I've changed one line in RDD.scala, nothing else.
>
>
>
> On Thu, Jul 17, 2014 at 10:56 AM, Sean Owen <sowen@cloudera.com> wrote:
>
>> This looks like a Jetty version problem actually. Are you bringing in
>> something that might be changing the version of Jetty used by Spark?
>> It depends a lot on how you are building things.
>>
>> Good to specify exactly how your'e building here.
>>
>> On Thu, Jul 17, 2014 at 3:43 PM, Nathan Kronenfeld
>> <nkronenfeld@oculusinfo.com> wrote:
>> > I'm trying to compile the latest code, with the hadoop-version set for
>> > 2.0.0-mr1-cdh4.6.0.
>> >
>> > I'm getting the following error, which I don't get when I don't set the
>> > hadoop version:
>> >
>> > [error]
>> >
>> /data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
>> > overloaded method constructor NioServerSocketChannelFactory with
>> > alternatives:
>> > [error]   (x$1: java.util.concurrent.Executor,x$2:
>> > java.util.concurrent.Executor,x$3:
>> > Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
>> <and>
>> > [error]   (x$1: java.util.concurrent.Executor,x$2:
>> >
>> java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
>> > [error]  cannot be applied to ()
>> > [error]       val channelFactory = new NioServerSocketChannelFactory
>> > [error]                            ^
>> > [error] one error found
>> >
>> >
>> > I don't know flume from a hole in the wall - does anyone know what I
>> can do
>> > to fix this?
>> >
>> >
>> > Thanks,
>> >          -Nathan
>> >
>> >
>> > --
>> > Nathan Kronenfeld
>> > Senior Visualization Developer
>> > Oculus Info Inc
>> > 2 Berkeley Street, Suite 600,
>> > Toronto, Ontario M5A 4J5
>> > Phone:  +1-416-203-3003 x 238
>> > Email:  nkronenfeld@oculusinfo.com
>>
>
>
>
> --
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com
>



-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--001a11c12ba4ef03c504fe64e319--

From dev-return-8419-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 16:02:37 2014
Return-Path: <dev-return-8419-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7246411357
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 16:02:37 +0000 (UTC)
Received: (qmail 90934 invoked by uid 500); 17 Jul 2014 16:02:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90878 invoked by uid 500); 17 Jul 2014 16:02:37 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90866 invoked by uid 99); 17 Jul 2014 16:02:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 16:02:36 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_FONT_FACE_BAD,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yanfang724@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 16:02:31 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so2211810qgd.9
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 09:02:10 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=VqGl47CqRXbbeygGeVask8HI2QrcbE3BejeoyfGwBwE=;
        b=utr+ToygFfUx6vX0GJgKuoEnTDiXkhmB1onXKmMMTMaPzxaB7rmgd85PUU9NLEYlYb
         nitl9kp4rSGXhhkurqS+ceKhZ73DTBrgfML+BcVzYWYSHal3qy2JVZO1v+tLiBCINQKT
         06OW82mma3M2ndSTb9lh+2rbAz8q8UUbIZvUhZ0ep0+JtuikTEhj3AF9F4mAsTyndGDm
         5iJG7G3gXsfajJVmZs9RuK7xxF5W8eQZoxl3UfpUHjtW1Vo0zNI/4S88TPhe/OisZNBD
         N44kNu7eGLYTye+aY+Dn14qFYn9FPV6fBv4haUK1Hw/ORlyehL6jolkwQ2zrUx38bYys
         NoxA==
X-Received: by 10.224.2.70 with SMTP id 6mr61215140qai.18.1405612930354; Thu,
 17 Jul 2014 09:02:10 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.22.239 with HTTP; Thu, 17 Jul 2014 09:01:50 -0700 (PDT)
In-Reply-To: <CAMwrk0mncUa5ne4CR8tx1pubuVCAyTyusuzCLA5aZgmv_wSfTg@mail.gmail.com>
References: <CAOErhNTsPtVq7wKR_6zrsLujjMrDwGzrSw_Gyr0ZJSw25V=wtQ@mail.gmail.com>
 <CAMwrk0mncUa5ne4CR8tx1pubuVCAyTyusuzCLA5aZgmv_wSfTg@mail.gmail.com>
From: Yan Fang <yanfang724@gmail.com>
Date: Thu, 17 Jul 2014 09:01:50 -0700
Message-ID: <CAOErhNTsq+m0GjRGsKXVNkj-gjyB3BniUbywQ6wyiz3CB0xyfA@mail.gmail.com>
Subject: Re: Does RDD checkpointing store the entire state in HDFS?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3ca8467d61304fe65c305
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3ca8467d61304fe65c305
Content-Type: text/plain; charset=UTF-8

Thank you, TD !

Fang, Yan
yanfang724@gmail.com
+1 (206) 849-4108


On Wed, Jul 16, 2014 at 6:53 PM, Tathagata Das <tathagata.das1565@gmail.com>
wrote:

> After every checkpointing interval, the latest state RDD is stored to HDFS
> in its entirety. Along with that, the series of DStream transformations
> that was setup with the streaming context is also stored into HDFS (the
> whole DAG of DStream objects is serialized and saved).
>
> TD
>
>
> On Wed, Jul 16, 2014 at 5:38 PM, Yan Fang <yanfang724@gmail.com> wrote:
>
> > Hi guys,
> >
> > am wondering how the RDD checkpointing
> > <
> https://spark.apache.org/docs/latest/streaming-programming-guide.html#RDD
> > Checkpointing> works in Spark Streaming. When I use updateStateByKey,
> does
> > the Spark store the entire state (at one time point) into the HDFS or
> only
> > put the transformation into the HDFS? Thank you.
> >
> > Best,
> >
> > Fang, Yan
> > yanfang724@gmail.com
> > +1 (206) 849-4108
> >
>

--001a11c3ca8467d61304fe65c305--

From dev-return-8420-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 16:25:52 2014
Return-Path: <dev-return-8420-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 797DC11456
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 16:25:52 +0000 (UTC)
Received: (qmail 62974 invoked by uid 500); 17 Jul 2014 16:25:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62920 invoked by uid 500); 17 Jul 2014 16:25:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62908 invoked by uid 99); 17 Jul 2014 16:25:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 16:25:51 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 16:25:50 +0000
Received: by mail-vc0-f178.google.com with SMTP id la4so5015639vcb.23
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 09:25:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=pE3ThAcfdsJvgAOpiXNiYCpN32bPgIafnjzO228lDAI=;
        b=CswXcPbAs2XTDxEpk5AtHAl5vHl6ngOUGC1q8HD3bru7Sx2IsWAsIs5gqPUgeUqKZH
         sqh9FUypIT1GFejO3iMCfV/4X9+KgjHnb9EDouaVEXDaifLVFdWcrfcQ590ZgeyFlaxq
         BIWv+skShZhTRkEEKGGyW/k10mS24frVCdWKfCyfUaMFKzHd/UqbCHmL69SIMtsh/Mpz
         a+5qqA5ctuIAYig0tuYstKHdTpI4TlibKb5xsdkVSXI4VIDc/ZwROR2KQiEhGrqu41Oh
         N8yPGRG29c9+WUSmuO/WskkhaL+FxQ1enA2RrNa2GbMio/0IinceGv8XMtifbhfq0Tap
         hUyA==
X-Gm-Message-State: ALoCoQn9IWX/tvggELq359esrLrpzc1w/PQBZzlOKNe41Ce4CIjmRMtydIKcAdBsjHhfAPmciBjY
X-Received: by 10.221.24.135 with SMTP id re7mr19575419vcb.53.1405614323673;
 Thu, 17 Jul 2014 09:25:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Thu, 17 Jul 2014 09:25:03 -0700 (PDT)
In-Reply-To: <CAEpWh49S0E96v=526GA8=xB9yGCzY0p6zmdYp4xzprpZ-jox8w@mail.gmail.com>
References: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
 <CAMAsSdKknzWUApb246coB_LSXsZs_UOF+BDADg4aE+rXtWmQWQ@mail.gmail.com> <CAEpWh49S0E96v=526GA8=xB9yGCzY0p6zmdYp4xzprpZ-jox8w@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 17 Jul 2014 17:25:03 +0100
Message-ID: <CAMAsSdLL+7nfq4NcsqAzXW8iVDmX8Kg8BZSD29tMN3U1zDPH8g@mail.gmail.com>
Subject: Re: Compile error when compiling for cloudera
To: "dev@spark.apache.org" <dev@spark.apache.org>, Ted Malaska <ted.malaska@cloudera.com>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

CC tmalaska since he touched the line in question. This is a fun one.
So, here's the line of code added last week:

val channelFactory = new NioServerSocketChannelFactory
  (Executors.newCachedThreadPool(), Executors.newCachedThreadPool());

Scala parses this as two statements, one invoking a no-arg constructor
and one making a tuple for fun. Put it on one line and it's fine.

It works with newer Netty since there is a no-arg constructor. It
fails with older Netty, which is what you get with older Hadoop.

The fix is obvious. I'm away and if nobody beats me to a PR in the
meantime, I'll propose one as an addendum to the recent JIRA.

Sean

*

On Thu, Jul 17, 2014 at 3:58 PM, Nathan Kronenfeld
<nkronenfeld@oculusinfo.com> wrote:
> My full build command is:
> ./sbt/sbt -Dhadoop.version=2.0.0-mr1-cdh4.6.0 clean assembly
>
>
> I've changed one line in RDD.scala, nothing else.
>
>
>
> On Thu, Jul 17, 2014 at 10:56 AM, Sean Owen <sowen@cloudera.com> wrote:
>
>> This looks like a Jetty version problem actually. Are you bringing in
>> something that might be changing the version of Jetty used by Spark?
>> It depends a lot on how you are building things.
>>
>> Good to specify exactly how your'e building here.
>>
>> On Thu, Jul 17, 2014 at 3:43 PM, Nathan Kronenfeld
>> <nkronenfeld@oculusinfo.com> wrote:
>> > I'm trying to compile the latest code, with the hadoop-version set for
>> > 2.0.0-mr1-cdh4.6.0.
>> >
>> > I'm getting the following error, which I don't get when I don't set the
>> > hadoop version:
>> >
>> > [error]
>> >
>> /data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
>> > overloaded method constructor NioServerSocketChannelFactory with
>> > alternatives:
>> > [error]   (x$1: java.util.concurrent.Executor,x$2:
>> > java.util.concurrent.Executor,x$3:
>> > Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
>> <and>
>> > [error]   (x$1: java.util.concurrent.Executor,x$2:
>> >
>> java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
>> > [error]  cannot be applied to ()
>> > [error]       val channelFactory = new NioServerSocketChannelFactory
>> > [error]                            ^
>> > [error] one error found
>> >
>> >
>> > I don't know flume from a hole in the wall - does anyone know what I can
>> do
>> > to fix this?
>> >
>> >
>> > Thanks,
>> >          -Nathan
>> >
>> >
>> > --
>> > Nathan Kronenfeld
>> > Senior Visualization Developer
>> > Oculus Info Inc
>> > 2 Berkeley Street, Suite 600,
>> > Toronto, Ontario M5A 4J5
>> > Phone:  +1-416-203-3003 x 238
>> > Email:  nkronenfeld@oculusinfo.com
>>
>
>
>
> --
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com

From dev-return-8421-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 16:37:30 2014
Return-Path: <dev-return-8421-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4918B114C0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 16:37:30 +0000 (UTC)
Received: (qmail 566 invoked by uid 500); 17 Jul 2014 16:37:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 501 invoked by uid 500); 17 Jul 2014 16:37:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 92266 invoked by uid 99); 17 Jul 2014 16:33:21 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ted.malaska@cloudera.com designates 209.85.223.170 as permitted sender)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=yPlUNHOIIR440aDNW/D/7bj6ktI5oLo1m2KZR5ZpTsg=;
        b=VsXsevhyyBdXxKjS3+9MFAn8s8ojlcpzZwP5kiUUY+VPTMmYks+aQbL8X6YfUo6R+R
         0/pzUdTWNlERmezOZ+CFR6rOw9F+aOJ08MYPhyXdF6OeQeDqtLHIjNfQPHlHeAZ6c5AC
         14wKyJZBtnwMtvh1h541Up+xylfNA95ClJOPowAQbajXxs+jEQPT7ybLftVIluSLltdd
         J5UuXr0aVcNkVGN22OnlyRRRSvsMsdlWX4AtQZ31CcBLnFIswJNoeqCJBB8OIzt5WQLg
         c945PNti9RZik79554CdLWWrQaoHP7MmWhCRALqkV5EALcFqdZPU+CZbQohSJ9o2gUZ7
         3z6Q==
X-Gm-Message-State: ALoCoQlOFFitFmVRwbWWXB1edbzYDVBf3OXSyrVyVroOqG1r0UJkTPoJTBt3W3oavZJlx5rzARoF
MIME-Version: 1.0
X-Received: by 10.182.200.198 with SMTP id ju6mr36339913obc.1.1405614776930;
 Thu, 17 Jul 2014 09:32:56 -0700 (PDT)
In-Reply-To: <CAMAsSdLL+7nfq4NcsqAzXW8iVDmX8Kg8BZSD29tMN3U1zDPH8g@mail.gmail.com>
References: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
	<CAMAsSdKknzWUApb246coB_LSXsZs_UOF+BDADg4aE+rXtWmQWQ@mail.gmail.com>
	<CAEpWh49S0E96v=526GA8=xB9yGCzY0p6zmdYp4xzprpZ-jox8w@mail.gmail.com>
	<CAMAsSdLL+7nfq4NcsqAzXW8iVDmX8Kg8BZSD29tMN3U1zDPH8g@mail.gmail.com>
Date: Thu, 17 Jul 2014 12:32:56 -0400
Message-ID: <CANc1aFNcSXA26RnSbo2CGOuKZJCzvy4_h1-ZbjJjNf-nB5gpbQ@mail.gmail.com>
Subject: Re: Compile error when compiling for cloudera
From: Ted Malaska <ted.malaska@cloudera.com>
To: Sean Owen <sowen@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2c2327a00f704fe663151
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c2327a00f704fe663151
Content-Type: text/plain; charset=UTF-8

Don't make this change yet.  I have a 1642 that needs to get through around
the same code.

I can make this change after 1642 is through.


On Thu, Jul 17, 2014 at 12:25 PM, Sean Owen <sowen@cloudera.com> wrote:

> CC tmalaska since he touched the line in question. This is a fun one.
> So, here's the line of code added last week:
>
> val channelFactory = new NioServerSocketChannelFactory
>   (Executors.newCachedThreadPool(), Executors.newCachedThreadPool());
>
> Scala parses this as two statements, one invoking a no-arg constructor
> and one making a tuple for fun. Put it on one line and it's fine.
>
> It works with newer Netty since there is a no-arg constructor. It
> fails with older Netty, which is what you get with older Hadoop.
>
> The fix is obvious. I'm away and if nobody beats me to a PR in the
> meantime, I'll propose one as an addendum to the recent JIRA.
>
> Sean
>
> *
>
> On Thu, Jul 17, 2014 at 3:58 PM, Nathan Kronenfeld
> <nkronenfeld@oculusinfo.com> wrote:
> > My full build command is:
> > ./sbt/sbt -Dhadoop.version=2.0.0-mr1-cdh4.6.0 clean assembly
> >
> >
> > I've changed one line in RDD.scala, nothing else.
> >
> >
> >
> > On Thu, Jul 17, 2014 at 10:56 AM, Sean Owen <sowen@cloudera.com> wrote:
> >
> >> This looks like a Jetty version problem actually. Are you bringing in
> >> something that might be changing the version of Jetty used by Spark?
> >> It depends a lot on how you are building things.
> >>
> >> Good to specify exactly how your'e building here.
> >>
> >> On Thu, Jul 17, 2014 at 3:43 PM, Nathan Kronenfeld
> >> <nkronenfeld@oculusinfo.com> wrote:
> >> > I'm trying to compile the latest code, with the hadoop-version set for
> >> > 2.0.0-mr1-cdh4.6.0.
> >> >
> >> > I'm getting the following error, which I don't get when I don't set
> the
> >> > hadoop version:
> >> >
> >> > [error]
> >> >
> >>
> /data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
> >> > overloaded method constructor NioServerSocketChannelFactory with
> >> > alternatives:
> >> > [error]   (x$1: java.util.concurrent.Executor,x$2:
> >> > java.util.concurrent.Executor,x$3:
> >> > Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
> >> <and>
> >> > [error]   (x$1: java.util.concurrent.Executor,x$2:
> >> >
> >>
> java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
> >> > [error]  cannot be applied to ()
> >> > [error]       val channelFactory = new NioServerSocketChannelFactory
> >> > [error]                            ^
> >> > [error] one error found
> >> >
> >> >
> >> > I don't know flume from a hole in the wall - does anyone know what I
> can
> >> do
> >> > to fix this?
> >> >
> >> >
> >> > Thanks,
> >> >          -Nathan
> >> >
> >> >
> >> > --
> >> > Nathan Kronenfeld
> >> > Senior Visualization Developer
> >> > Oculus Info Inc
> >> > 2 Berkeley Street, Suite 600,
> >> > Toronto, Ontario M5A 4J5
> >> > Phone:  +1-416-203-3003 x 238
> >> > Email:  nkronenfeld@oculusinfo.com
> >>
> >
> >
> >
> > --
> > Nathan Kronenfeld
> > Senior Visualization Developer
> > Oculus Info Inc
> > 2 Berkeley Street, Suite 600,
> > Toronto, Ontario M5A 4J5
> > Phone:  +1-416-203-3003 x 238
> > Email:  nkronenfeld@oculusinfo.com
>

--001a11c2c2327a00f704fe663151--

From dev-return-8422-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 17:04:21 2014
Return-Path: <dev-return-8422-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D5DCD115E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 17:04:21 +0000 (UTC)
Received: (qmail 94512 invoked by uid 500); 17 Jul 2014 17:04:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94448 invoked by uid 500); 17 Jul 2014 17:04:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94413 invoked by uid 99); 17 Jul 2014 17:04:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 17:04:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of chester@alpinenow.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 17:04:19 +0000
Received: by mail-wg0-f50.google.com with SMTP id n12so2323452wgh.33
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 10:03:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=W4H6oDlbX4Pc3T/ChUJpaAx9uK9fAegCnT46/aC5Cvk=;
        b=QarBQ2SO7HPj62fdAcMUjdd8SUv8vtgsBv0sFrHpal9CNWm/Wha8TWXI/+DScK8Wno
         htwbipaqb4J3RxkfCMwq2CiYkxQLplii6oAkODQmt66F6+4vD7BjPHX2cJfWMlhveKeE
         pDqov05Y868/aMuupnbRHy4FEEKD4WYQDfyj24joRCkOKcCnXMyBghHBOao0qWiaeBT9
         ZOXo/rLwpTALh1QgHVxTvE2SQY/lKy8W+CvncwWW2JtCj9m4MWgBbJFovkTpdcq/i9M6
         kegExyO4AW02YefXJYXD2NhZEujvm9D0sUGSMtAhmU+HjyuxC/UwWquCT2XK574N6IsR
         NGDw==
X-Gm-Message-State: ALoCoQlRZ+lELN01Dy5zdy0KSqcjRXPnYe2fIQSeXh2V1vX6fYl8ti8vDPcX25AeGw1IDabUc3kv
MIME-Version: 1.0
X-Received: by 10.180.85.162 with SMTP id i2mr24224241wiz.53.1405616634392;
 Thu, 17 Jul 2014 10:03:54 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Thu, 17 Jul 2014 10:03:54 -0700 (PDT)
In-Reply-To: <CAMAsSd+GSX4pwZhRjHZw5y_gmBeCW2NJwRcLt058YuWS1u2w1Q@mail.gmail.com>
References: <71654A18-B5D3-4DE5-8F47-2353150CE6C1@yahoo.com>
	<CAPYnQ0W1m47=ebxBqF8fzNPR6RfoWpuhtx24UJfkQdsBsAAizw@mail.gmail.com>
	<CACBYxKLBU4YK0xzQFPTZ9M6xyCOUNbfmBzUmx5VSCNRhf-YQMQ@mail.gmail.com>
	<CAPYnQ0XQ_bOCj9s67fB9igZtY_J3v-cR0qNTXCfoxVFOg__e6Q@mail.gmail.com>
	<CAPYnQ0UQ3zrC4p6MJbDuZzcXovm3rC_iBtxAwmJfGVkSRbbwng@mail.gmail.com>
	<CAPYnQ0W3ZpR46eGSiwBwXuoKDbkd6JU3iq3+trj-s5K2AsRVpw@mail.gmail.com>
	<CAMAsSdJh_o7tBtGmCioxpMQMoWqCheVY0eA5zBo27_8i_wz6iQ@mail.gmail.com>
	<CACBYxKKwGR6-WgKguPH11Q4Z4uOe9F1wwFUpw7TUm3VbcCQ3_Q@mail.gmail.com>
	<CAPYnQ0Wo2sR7RS-ct6PZ99kwWS_SG3MgekX9Pz2mNde8mVD-7w@mail.gmail.com>
	<CAMAsSd+GSX4pwZhRjHZw5y_gmBeCW2NJwRcLt058YuWS1u2w1Q@mail.gmail.com>
Date: Thu, 17 Jul 2014 10:03:54 -0700
Message-ID: <CAPYnQ0XxiGZMsfydR=CN_=FCkVm66O6WvH175JAkGqsCsW-goQ@mail.gmail.com>
Subject: Re: Possible bug in ClientBase.scala?
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d0444e9cb2f0eae04fe66a0e2
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0444e9cb2f0eae04fe66a0e2
Content-Type: text/plain; charset=UTF-8

OK   I will create PR.

thanks



On Thu, Jul 17, 2014 at 7:58 AM, Sean Owen <sowen@cloudera.com> wrote:

> Looks like a real problem. I see it too. I think the same workaround
> found in ClientBase.scala needs to be used here. There, the fact that
> this field can be a String or String[] is handled explicitly. In fact
> I think you can just call to ClientBase for this? PR it, I say.
>
> On Thu, Jul 17, 2014 at 3:24 PM, Chester Chen <chester@alpinenow.com>
> wrote:
> >     val knownDefMRAppCP: Seq[String] =
> >       getFieldValue[String, Seq[String]](classOf[MRJobConfig],
> >
> >  "DEFAULT_MAPREDUCE_APPLICATION_CLASSPATH",
> >                                          Seq[String]())(a =>
> a.split(","))
> >
> > will fail for yarn-alpha.
> >
> > sbt/sbt -Pyarn-alpha -Dhadoop.version=2.0.5-alpha yarn-alpha/test
> >
>

--f46d0444e9cb2f0eae04fe66a0e2--

From dev-return-8423-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 17:26:29 2014
Return-Path: <dev-return-8423-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6509116CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 17:26:29 +0000 (UTC)
Received: (qmail 65286 invoked by uid 500); 17 Jul 2014 17:26:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65235 invoked by uid 500); 17 Jul 2014 17:26:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65224 invoked by uid 99); 17 Jul 2014 17:26:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 17:26:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 17:26:27 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so5369470vcb.27
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 10:26:02 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=COLtTgt73qBfTrUe9Uq0Zafs12MmWHfoOgqz1CxVF2o=;
        b=U2JBvo/UtCyYrcIya5+a4UUFYGPsqSoEeFpEFPfEa+6YZ0gqEu9zsAJ/sN6doZ3mTt
         k/nT0xZub+PRdc7LK3mPijN071F+vpH0Xy1iEuVDFYdoA1j+V8UGSpU649QoGj0F+hwp
         nBqiFcOf4sWbmwxs2/4dDtb9Od5pdCTe+Zc7VyxYE3F5F4Mes9H1KC5sPtM4NxAPSECP
         zK1WBLq8TMM2D2Y+exXeXYnfbqJHsokAhF4WYPs+krnlPYrEJIEN2FtnOr4lvn8YD+hG
         dpN4XGP7+MD7QKAfOfxGwLbsHQiAcOIUj/7JW3FNG+JUWygxU9eO3eFyOptbpdCjgU5m
         DgOg==
X-Gm-Message-State: ALoCoQntuEdy8viBrU45KXfhCpOk8xec3JSEjQrA5koF5gM/3lHs/9gafh4TFIIFRu5Vp8twByhR
X-Received: by 10.221.59.194 with SMTP id wp2mr20280448vcb.59.1405617962333;
 Thu, 17 Jul 2014 10:26:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Thu, 17 Jul 2014 10:25:42 -0700 (PDT)
In-Reply-To: <CANc1aFNcSXA26RnSbo2CGOuKZJCzvy4_h1-ZbjJjNf-nB5gpbQ@mail.gmail.com>
References: <CAEpWh4_ySGRa676eCV2m1HBsNc3=bW5pLb4p=g7PggBNVLFEPw@mail.gmail.com>
 <CAMAsSdKknzWUApb246coB_LSXsZs_UOF+BDADg4aE+rXtWmQWQ@mail.gmail.com>
 <CAEpWh49S0E96v=526GA8=xB9yGCzY0p6zmdYp4xzprpZ-jox8w@mail.gmail.com>
 <CAMAsSdLL+7nfq4NcsqAzXW8iVDmX8Kg8BZSD29tMN3U1zDPH8g@mail.gmail.com> <CANc1aFNcSXA26RnSbo2CGOuKZJCzvy4_h1-ZbjJjNf-nB5gpbQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Thu, 17 Jul 2014 18:25:42 +0100
Message-ID: <CAMAsSd+gNpouOo=GYSjFcx5yFnxubh=JkHxipnaNAV_656o1kg@mail.gmail.com>
Subject: Re: Compile error when compiling for cloudera
To: Ted Malaska <ted.malaska@cloudera.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Should be an easy rebase for your PR, so I went ahead just to get this fixed up:

https://github.com/apache/spark/pull/1466

On Thu, Jul 17, 2014 at 5:32 PM, Ted Malaska <ted.malaska@cloudera.com> wrote:
> Don't make this change yet.  I have a 1642 that needs to get through around
> the same code.
>
> I can make this change after 1642 is through.
>
>
> On Thu, Jul 17, 2014 at 12:25 PM, Sean Owen <sowen@cloudera.com> wrote:
>>
>> CC tmalaska since he touched the line in question. This is a fun one.
>> So, here's the line of code added last week:
>>
>> val channelFactory = new NioServerSocketChannelFactory
>>   (Executors.newCachedThreadPool(), Executors.newCachedThreadPool());
>>
>> Scala parses this as two statements, one invoking a no-arg constructor
>> and one making a tuple for fun. Put it on one line and it's fine.
>>
>> It works with newer Netty since there is a no-arg constructor. It
>> fails with older Netty, which is what you get with older Hadoop.
>>
>> The fix is obvious. I'm away and if nobody beats me to a PR in the
>> meantime, I'll propose one as an addendum to the recent JIRA.
>>
>> Sean
>>
>> *
>>
>> On Thu, Jul 17, 2014 at 3:58 PM, Nathan Kronenfeld
>> <nkronenfeld@oculusinfo.com> wrote:
>> > My full build command is:
>> > ./sbt/sbt -Dhadoop.version=2.0.0-mr1-cdh4.6.0 clean assembly
>> >
>> >
>> > I've changed one line in RDD.scala, nothing else.
>> >
>> >
>> >
>> > On Thu, Jul 17, 2014 at 10:56 AM, Sean Owen <sowen@cloudera.com> wrote:
>> >
>> >> This looks like a Jetty version problem actually. Are you bringing in
>> >> something that might be changing the version of Jetty used by Spark?
>> >> It depends a lot on how you are building things.
>> >>
>> >> Good to specify exactly how your'e building here.
>> >>
>> >> On Thu, Jul 17, 2014 at 3:43 PM, Nathan Kronenfeld
>> >> <nkronenfeld@oculusinfo.com> wrote:
>> >> > I'm trying to compile the latest code, with the hadoop-version set
>> >> > for
>> >> > 2.0.0-mr1-cdh4.6.0.
>> >> >
>> >> > I'm getting the following error, which I don't get when I don't set
>> >> > the
>> >> > hadoop version:
>> >> >
>> >> > [error]
>> >> >
>> >>
>> >> /data/hdfs/1/home/nkronenfeld/git/spark-ndk/external/flume/src/main/scala/org/apache/spark/streaming/flume/FlumeInputDStream.scala:156:
>> >> > overloaded method constructor NioServerSocketChannelFactory with
>> >> > alternatives:
>> >> > [error]   (x$1: java.util.concurrent.Executor,x$2:
>> >> > java.util.concurrent.Executor,x$3:
>> >> > Int)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
>> >> <and>
>> >> > [error]   (x$1: java.util.concurrent.Executor,x$2:
>> >> >
>> >>
>> >> java.util.concurrent.Executor)org.jboss.netty.channel.socket.nio.NioServerSocketChannelFactory
>> >> > [error]  cannot be applied to ()
>> >> > [error]       val channelFactory = new NioServerSocketChannelFactory
>> >> > [error]                            ^
>> >> > [error] one error found
>> >> >
>> >> >
>> >> > I don't know flume from a hole in the wall - does anyone know what I
>> >> > can
>> >> do
>> >> > to fix this?
>> >> >
>> >> >
>> >> > Thanks,
>> >> >          -Nathan
>> >> >
>> >> >
>> >> > --
>> >> > Nathan Kronenfeld
>> >> > Senior Visualization Developer
>> >> > Oculus Info Inc
>> >> > 2 Berkeley Street, Suite 600,
>> >> > Toronto, Ontario M5A 4J5
>> >> > Phone:  +1-416-203-3003 x 238
>> >> > Email:  nkronenfeld@oculusinfo.com
>> >>
>> >
>> >
>> >
>> > --
>> > Nathan Kronenfeld
>> > Senior Visualization Developer
>> > Oculus Info Inc
>> > 2 Berkeley Street, Suite 600,
>> > Toronto, Ontario M5A 4J5
>> > Phone:  +1-416-203-3003 x 238
>> > Email:  nkronenfeld@oculusinfo.com
>
>

From dev-return-8424-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 18:10:27 2014
Return-Path: <dev-return-8424-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 33B0F118B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 18:10:27 +0000 (UTC)
Received: (qmail 26101 invoked by uid 500); 17 Jul 2014 18:10:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26044 invoked by uid 500); 17 Jul 2014 18:10:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26031 invoked by uid 99); 17 Jul 2014 18:10:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 18:10:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 18:10:20 +0000
Received: by mail-wi0-f169.google.com with SMTP id n3so11794wiv.4
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 11:09:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=JDwMuNQomaIHF8uQWJeN3JJFseVVGCME2y/vceg0q38=;
        b=oSNBBxKsmTYkMQjAYm/jgOCwvUUIAk8+TooSKgXWTs/7ft0azs1McDSjK1osEqSFlG
         MQCe/2JRqcmsZtMQzlpRqDySkE8taw0W0Jc3o2lbisVgRnTTSJraR6Dyvq5zwBWsT+5S
         gFQ41iAMQGBgOLzowuBO8aYFxRRofs1QqnT+UHH7VTixt54mgrc1vu/1ztAMajlqefbg
         5iEkd4NExqfNGUItHhzu2gHsFeaKO7SbpyM97iTb7xJWaz/CWu6dvDpIohIDw3B1fSPF
         wEL99KXZn+yTpBpgAx7mo/tPl2C8WOpeSCBFL9LbtAUhxx9hPno0o0AD8kh2wCaopIfb
         diFA==
X-Received: by 10.194.76.99 with SMTP id j3mr48437512wjw.85.1405620598756;
 Thu, 17 Jul 2014 11:09:58 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.24.168 with HTTP; Thu, 17 Jul 2014 11:09:18 -0700 (PDT)
In-Reply-To: <20140717002332.1b9dc18f@sh9>
References: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
 <20140717002332.1b9dc18f@sh9>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Thu, 17 Jul 2014 14:09:18 -0400
Message-ID: <CAOhmDzetC1QYD=4X_TmDyxU2MaHshy=8KHyf1B=W-NjMHHsG_w@mail.gmail.com>
Subject: Re: small (yet major) change going in: broadcasting RDD to reduce
 task size
To: dev <dev@spark.apache.org>
Cc: Reynold Xin <rxin@databricks.com>
Content-Type: multipart/alternative; boundary=047d7bb04cf67a599e04fe678c65
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb04cf67a599e04fe678c65
Content-Type: text/plain; charset=UTF-8

On Thu, Jul 17, 2014 at 1:23 AM, Stephen Haberman <
stephen.haberman@gmail.com> wrote:

> I'd be ecstatic if more major changes were this well/succinctly
> explained
>

Ditto on that. The summary of user impact was very nice. It would be good
to repeat that on the user list or release notes when this change goes out.

Nick

--047d7bb04cf67a599e04fe678c65--

From dev-return-8425-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 18:12:58 2014
Return-Path: <dev-return-8425-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 23B09118CB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 18:12:58 +0000 (UTC)
Received: (qmail 33135 invoked by uid 500); 17 Jul 2014 18:12:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33078 invoked by uid 500); 17 Jul 2014 18:12:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 33066 invoked by uid 99); 17 Jul 2014 18:12:57 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 18:12:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 74.125.82.47 as permitted sender)
Received: from [74.125.82.47] (HELO mail-wg0-f47.google.com) (74.125.82.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 18:12:51 +0000
Received: by mail-wg0-f47.google.com with SMTP id b13so2439742wgh.30
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 11:12:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=lBt+LhxozX8dAiZrVm6slFsGqWx0wpT/Cs6e5nc6Bvo=;
        b=cCT1ysaKJlB1ypSSnCTaBBakYFN+YlrRASYXhkg0pE6lY7HBHHtHG6iTul1TqryTfz
         a1IgNX+1mLmGZORBox43pOY5UKiiBvTEVPtkq6BZAAVEAXxgtu+bdyPMyBhuo2lORQee
         /D0wt+R+DWu18YHVW8ZB//02Do9g3sbX4VN98wf8JYtFdydEgwUWFyV878RshExNj2pt
         bnXUQzMvV4RBsOLQvK40mP1LBztqnnx2CRXRgvagMe35dBOFK1RntvhgbSanECGMS2UV
         TOHyLFeJF+uNqeo7VydsBWkBODh+gYzvnWsJWLz3dPr4U9kPVs2LTiZPcvzk7CdiQQVU
         L3vQ==
MIME-Version: 1.0
X-Received: by 10.194.48.103 with SMTP id k7mr46833537wjn.68.1405620750383;
 Thu, 17 Jul 2014 11:12:30 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Thu, 17 Jul 2014 11:12:30 -0700 (PDT)
In-Reply-To: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
Date: Thu, 17 Jul 2014 11:12:30 -0700
Message-ID: <CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I start the voting with a +1.

Ran tests on the release candidates and some basic operations in
spark-shell and pyspark (local and standalone).

-Xiangrui

On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 0.9.2!
>
> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
>
> The release files, including signatures, digests, etc. can be found at:
> http://people.apache.org/~meng/spark-0.9.2-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/meng.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 0.9.2!
>
> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
>
> [ ] +1 Release this package as Apache Spark 0.9.2
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>
> === About this release ===
> This release fixes a few high-priority bugs in 0.9.1 and has a variety
> of smaller fixes. The full list is here: http://s.apache.org/d0t. Some
> of the more visible patches are:
>
> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size
> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
> SPARK-1870: Secondary jars are not added to executor classpath for YARN
>
> This is the second maintenance release on the 0.9 line. We plan to make
> additional maintenance releases as new fixes come in.
>
> Best,
> Xiangrui

From dev-return-8426-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 21:27:52 2014
Return-Path: <dev-return-8426-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5C2C71112B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 21:27:52 +0000 (UTC)
Received: (qmail 1405 invoked by uid 500); 17 Jul 2014 21:27:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1340 invoked by uid 500); 17 Jul 2014 21:27:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1325 invoked by uid 99); 17 Jul 2014 21:27:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 21:27:51 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [136.142.11.142] (HELO mb2i1.ns.pitt.edu) (136.142.11.142)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 21:27:47 +0000
Received: from mail-pd0-f181.google.com
 (mail-pd0-f181.google.com [209.85.192.181]) by pitt.edu (PMDF V6.5-x7 #31901)
 with ESMTPSA id <0N8V0000TKXLJ0@mb2i1.ns.pitt.edu> for dev@spark.apache.org;
 Thu, 17 Jul 2014 17:27:21 -0400 (EDT)
Received: by mail-pd0-f181.google.com with SMTP id g10so2349870pdj.40
 for <dev@spark.apache.org>; Thu, 17 Jul 2014 14:27:20 -0700 (PDT)
Received: by 10.70.9.131 with HTTP; Thu, 17 Jul 2014 14:27:20 -0700 (PDT)
X-Received: by 10.66.102.73 with SMTP id fm9mr87650pab.72.1405632440698; Thu,
 17 Jul 2014 14:27:20 -0700 (PDT)
Date: Thu, 17 Jul 2014 14:27:20 -0700
From: "Nick R. Katsipoulakis" <katsip@cs.pitt.edu>
Subject: InputSplit and RecordReader control on HadoopRDD
To: dev@spark.apache.org
Message-id: <CAJ2UJ5fzrk+tqu13yDO3p=wQJZk0b2n4d5F4-wy7NrfpcMKopg@mail.gmail.com>
MIME-version: 1.0
Content-type: multipart/alternative; boundary=047d7bd9098c500fba04fe6a4ee6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd9098c500fba04fe6a4ee6
Content-Type: text/plain; charset=UTF-8

Hello,

I am currently trying to extend some custom InputSplit and RecordReader
classes to provide to SparkContext's hadoopRDD() function.

My question is the following:

Does the value returned by InpuSplit.getLenght() and/or
RecordReader.getProgress() affect the execution of a map() function in the
Spark runtime?

I am asking because I have used these two custom classes on Hadoop and they
do not cause any problems. However, in Spark, I see that new InputSplit
objects are generated during runtime. To be more precise:

In the beginning, I see in my log file that an InputSplit object is
generated and the RecordReader object associated to it is fetching records.
At some point, the job that is handling the previous InputSplit stops, and
a new one is spawned with a new InputSplit. I do not understand why this is
happening?

Any help?

Thank you,
Nick

P.S.-1 : I am sorry for posting my question on the Developer Mailing List,
but I could not find anything similar in the User's list. Also, I really
need to understand the runtime of Spark and I believe that in the
developer's list my question will be read by contributors of Spark.

P.S.-2: I can provide more technical details if they are needed.

--047d7bd9098c500fba04fe6a4ee6--

From dev-return-8427-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 17 23:00:55 2014
Return-Path: <dev-return-8427-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 065291153B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 17 Jul 2014 23:00:55 +0000 (UTC)
Received: (qmail 25675 invoked by uid 500); 17 Jul 2014 23:00:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25612 invoked by uid 500); 17 Jul 2014 23:00:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25600 invoked by uid 99); 17 Jul 2014 23:00:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 23:00:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.220.179 as permitted sender)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 17 Jul 2014 23:00:51 +0000
Received: by mail-vc0-f179.google.com with SMTP id hq11so4501594vcb.10
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 16:00:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=UJawNq7xekTnfUZu8JOiv7YYdB6JavZywY7Gbq6eQkM=;
        b=OPfSWxD2Nf7WU+uaCbjdyTijeKKHw4VXt8V2gIkge6HvBkHeAuhdS050iDFXE7wcTq
         mlCAUd1CxyFymaTgcIu8q5GWZJj6WlmoKEOIB7UCDRrbzKJ7QvUlP1faSEg+YynmQBL5
         g9llpX5+dl+hiAAFgtbQWSh0IbyylYkFs+yOhW+CW1+vk08Fe9tmeuWyqpXBnqi3l81B
         X7xppb5k0oYQmB1QxvEXIlhsN+ZTeMPBrawytFJumE0iy/IqaLCv+2arjJGUbfYmeCW1
         1kFcoG6JGaSf9wqGGhaPn39t5vRqF5SlfljkvoNEhTr+L2Y/HkhmqiQNm/K2tHUhwjmh
         QNqA==
MIME-Version: 1.0
X-Received: by 10.221.59.194 with SMTP id wp2mr307636vcb.59.1405638026253;
 Thu, 17 Jul 2014 16:00:26 -0700 (PDT)
Received: by 10.58.234.41 with HTTP; Thu, 17 Jul 2014 16:00:26 -0700 (PDT)
Date: Thu, 17 Jul 2014 16:00:26 -0700
Message-ID: <CACkSZy3jeGykv1b493ja2ZiUvJJ+xC0HnzvmF44OQS651xp4DQ@mail.gmail.com>
Subject: Current way to include hive in a build
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113354723cdd1f04fe6b9ba6
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113354723cdd1f04fe6b9ba6
Content-Type: text/plain; charset=UTF-8

Having looked at trunk make-distribution.sh the --with-hive and --with-yarn
are now deprecated.

Here is the way I have built it:

Added to pom.xml:

   <profile>
      <id>cdh5</id>
      <activation>
        <activeByDefault>false</activeByDefault>
      </activation>
      <properties>
        <hadoop.version>2.3.0-cdh5.0.0</hadoop.version>
        <yarn.version>2.3.0-cdh5.0.0</yarn.version>
        <hbase.version>0.96.1.1-cdh5.0.0</hbase.version>
        <zookeeper.version>3.4.5-cdh5.0.0</zookeeper.version>
      </properties>
    </profile>

*mvn -Pyarn -Pcdh5 -Phive -Dhadoop.version=2.3.0-cdh5.0.1
-Dyarn.version=2.3.0-cdh5.0.0 -DskipTests clean package*


[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM .......................... SUCCESS [3.165s]
[INFO] Spark Project Core ................................ SUCCESS
[2:39.504s]
[INFO] Spark Project Bagel ............................... SUCCESS [7.596s]
[INFO] Spark Project GraphX .............................. SUCCESS [22.027s]
[INFO] Spark Project ML Library .......................... SUCCESS [36.284s]
[INFO] Spark Project Streaming ........................... SUCCESS [24.309s]
[INFO] Spark Project Tools ............................... SUCCESS [3.147s]
[INFO] Spark Project Catalyst ............................ SUCCESS [20.148s]
[INFO] Spark Project SQL ................................. SUCCESS [18.560s]
*[INFO] Spark Project Hive ................................ FAILURE
[33.962s]*

[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
(copy-dependencies) on project spark-hive_2.10: Execution copy-dependencies
of goal
org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
failed: Plugin org.apache.maven.plugins:maven-dependency-plugin:2.4 or one
of its dependencies could not be resolved: Could not find artifact
commons-logging:commons-logging:jar:1.0.4 -> [Help 1]

Anyone who is presently building with -Phive and has a suggestion for this?

--001a113354723cdd1f04fe6b9ba6--

From dev-return-8428-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 00:31:32 2014
Return-Path: <dev-return-8428-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4E77B11A2C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 00:31:32 +0000 (UTC)
Received: (qmail 35838 invoked by uid 500); 18 Jul 2014 00:31:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35776 invoked by uid 500); 18 Jul 2014 00:31:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35765 invoked by uid 99); 18 Jul 2014 00:31:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 00:31:31 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of freeman.jeremy@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 00:31:30 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <freeman.jeremy@gmail.com>)
	id 1X7w4m-0000d3-QT
	for dev@spark.incubator.apache.org; Thu, 17 Jul 2014 17:31:04 -0700
Date: Thu, 17 Jul 2014 17:31:04 -0700 (PDT)
From: Jeremy Freeman <freeman.jeremy@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1405643464811-7398.post@n3.nabble.com>
In-Reply-To: <1405003674571.192c3983@Nodemailer>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com> <CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com> <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com> <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com> <CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com> <CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com> <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com> <1404909567855.c2a8fd87@Nodemailer> <CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com> <1405003674571.192c3983@Nodemailer>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all, 

Cool discussion! I agree that a more standardized API for clustering, and
easy access to underlying routines, would be useful (we've also been
discussing this when trying to develop streaming clustering algorithms,
similar to https://github.com/apache/spark/pull/1361) 

For divisive, hierarchical clustering I implemented something awhile back,
here's a gist. 

https://gist.github.com/freeman-lab/5947e7c53b368fe90371

It does bisecting k-means clustering (with k=2), with a recursive class for
keeping track of the tree. I also found this much better than agglomerative
methods (for the reasons Hector points out).

This needs to be cleaned up, and can surely be optimized (esp. by replacing
the core KMeans step with existing MLLib code), but I can say I was running
it successfully on quite large data sets. 

RJ, depending on where you are in your progress, I'd be happy to help work
on this piece and / or have you use this as a jumping off point, if useful. 

-- Jeremy 



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8429-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 01:36:42 2014
Return-Path: <dev-return-8429-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48FF711BAC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 01:36:42 +0000 (UTC)
Received: (qmail 62774 invoked by uid 500); 18 Jul 2014 01:36:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62716 invoked by uid 500); 18 Jul 2014 01:36:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62697 invoked by uid 99); 18 Jul 2014 01:36:41 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 01:36:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.169 as permitted sender)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 01:36:37 +0000
Received: by mail-pd0-f169.google.com with SMTP id y10so4084450pdj.14
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 18:36:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=EIjGYfyUDlpaWATVi5xtHhOZ08r6Rp6SFkDBDTdiNmk=;
        b=cEVH5UZ/5sFM83wvVohCeguvjjmG9WUBHgW1+67/WxMZAR+n01okGb1O4Ru4WqueOw
         GHVRgPC/CVnfzC4wNN/KhSo2JfHxi871XFVl8/DvRtqEdOYt5am28tDxmPzcBhohe7H3
         CAKBmsQo67vK7Rb5CRqXlUkjoe6OAIvkducz9O53rS+Op8QTqA9oJZ0IGFQYwlpUjAqu
         glQeYhfa9BHcKG7dg+fUu6QpWJ+TyXNb43xC/caY04gLagCWiPCTovNSClFTA/2vOYFR
         QJfy1Gru6TqH51dVa5vM61QW+eQfLmoI1ZrQsnLumMDf1gL6swChrcyFqY/2UWauh6OQ
         oTKA==
X-Received: by 10.68.164.226 with SMTP id yt2mr1012438pbb.89.1405647371909;
        Thu, 17 Jul 2014 18:36:11 -0700 (PDT)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id r3sm5180253pdd.8.2014.07.17.18.36.08
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 17 Jul 2014 18:36:09 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
Date: Thu, 17 Jul 2014 18:36:07 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com> <CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested on Mac, verified CHANGES.txt is good, verified several of the bug =
fixes.

Matei

On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com> wrote:

> I start the voting with a +1.
>=20
> Ran tests on the release candidates and some basic operations in
> spark-shell and pyspark (local and standalone).
>=20
> -Xiangrui
>=20
> On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com> =
wrote:
>> Please vote on releasing the following candidate as Apache Spark =
version 0.9.2!
>>=20
>> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D4322c=
0ba7f411cf9a2483895091440011742246b
>>=20
>> The release files, including signatures, digests, etc. can be found =
at:
>> http://people.apache.org/~meng/spark-0.9.2-rc1/
>>=20
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/meng.asc
>>=20
>> The staging repository for this release can be found at:
>> =
https://repository.apache.org/service/local/repositories/orgapachespark-10=
23/content/
>>=20
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
>>=20
>> Please vote on releasing this package as Apache Spark 0.9.2!
>>=20
>> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>>=20
>> [ ] +1 Release this package as Apache Spark 0.9.2
>> [ ] -1 Do not release this package because ...
>>=20
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>>=20
>> =3D=3D=3D About this release =3D=3D=3D
>> This release fixes a few high-priority bugs in 0.9.1 and has a =
variety
>> of smaller fixes. The full list is here: http://s.apache.org/d0t. =
Some
>> of the more visible patches are:
>>=20
>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame =
size
>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
>> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
>> SPARK-1870: Secondary jars are not added to executor classpath for =
YARN
>>=20
>> This is the second maintenance release on the 0.9 line. We plan to =
make
>> additional maintenance releases as new fixes come in.
>>=20
>> Best,
>> Xiangrui


From dev-return-8430-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 01:53:24 2014
Return-Path: <dev-return-8430-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 60BA811C01
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 01:53:24 +0000 (UTC)
Received: (qmail 7299 invoked by uid 500); 18 Jul 2014 01:53:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7240 invoked by uid 500); 18 Jul 2014 01:53:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 7219 invoked by uid 99); 18 Jul 2014 01:53:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 01:53:22 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.175] (HELO mail-vc0-f175.google.com) (209.85.220.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 01:53:00 +0000
Received: by mail-vc0-f175.google.com with SMTP id hu12so6097268vcb.6
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 18:52:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=LRAT5gBAlSrQBMv8YBd+KybCmypXaph0ID5NG7P6sNI=;
        b=kJffUwdSd+N/wRdc5k2lDx18oB0IQ9htDfc5u4onnOz/Udfc6fbzJE8IYmanypw5r7
         mMvns5DYmnalnD6SOVn55h8FALZrmiYprJJ94C7mNj7Ky3BLoetUys8If1oriePHfWMc
         bDRnrHutQLWYcMsBnqEyfBocmsfzbnwe3vxSWeUoAeXJd6ObV/eCfCVl1GHGiK/NuZK8
         me+Kp+td5iAuqxZk9UKB3EeVF12uMp85PqXIsuwi7CppdN5kd+gm2Ka6Sp5yE2RG3XNC
         cMBikmyNI6YZuvijFUXY97P7WkLRio87spulhVKvxi2Ja3Kc6J+5mnCDmeObeQdlVhzG
         6gPw==
X-Gm-Message-State: ALoCoQmUYzQXGqykfXUOYDAGjvyCDTl0pWB3f/iIclI97sG5SzkHYmCJHYQak10fQMbRk6zyA0T7
MIME-Version: 1.0
X-Received: by 10.220.237.201 with SMTP id kp9mr1207904vcb.58.1405648355601;
 Thu, 17 Jul 2014 18:52:35 -0700 (PDT)
Received: by 10.220.1.200 with HTTP; Thu, 17 Jul 2014 18:52:35 -0700 (PDT)
In-Reply-To: <6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
	<CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
	<6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
Date: Thu, 17 Jul 2014 18:52:35 -0700
Message-ID: <CAEYYnxZb+Uhba0LY1FP_-6xZ2LpJX_8W4GiKpQbjDRjRgd=r1Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested with my Ubuntu Linux.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Thu, Jul 17, 2014 at 6:36 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> +1
>
> Tested on Mac, verified CHANGES.txt is good, verified several of the bug fixes.
>
> Matei
>
> On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>
>> I start the voting with a +1.
>>
>> Ran tests on the release candidates and some basic operations in
>> spark-shell and pyspark (local and standalone).
>>
>> -Xiangrui
>>
>> On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>> Please vote on releasing the following candidate as Apache Spark version 0.9.2!
>>>
>>> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
>>>
>>> The release files, including signatures, digests, etc. can be found at:
>>> http://people.apache.org/~meng/spark-0.9.2-rc1/
>>>
>>> Release artifacts are signed with the following key:
>>> https://people.apache.org/keys/committer/meng.asc
>>>
>>> The staging repository for this release can be found at:
>>> https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/
>>>
>>> The documentation corresponding to this release can be found at:
>>> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
>>>
>>> Please vote on releasing this package as Apache Spark 0.9.2!
>>>
>>> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
>>> a majority of at least 3 +1 PMC votes are cast.
>>>
>>> [ ] +1 Release this package as Apache Spark 0.9.2
>>> [ ] -1 Do not release this package because ...
>>>
>>> To learn more about Apache Spark, please see
>>> http://spark.apache.org/
>>>
>>> === About this release ===
>>> This release fixes a few high-priority bugs in 0.9.1 and has a variety
>>> of smaller fixes. The full list is here: http://s.apache.org/d0t. Some
>>> of the more visible patches are:
>>>
>>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame size
>>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>>> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
>>> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
>>> SPARK-1870: Secondary jars are not added to executor classpath for YARN
>>>
>>> This is the second maintenance release on the 0.9 line. We plan to make
>>> additional maintenance releases as new fixes come in.
>>>
>>> Best,
>>> Xiangrui
>

From dev-return-8431-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 02:51:35 2014
Return-Path: <dev-return-8431-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D35A011E98
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 02:51:35 +0000 (UTC)
Received: (qmail 41184 invoked by uid 500); 18 Jul 2014 02:51:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41126 invoked by uid 500); 18 Jul 2014 02:51:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41109 invoked by uid 99); 18 Jul 2014 02:51:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 02:51:35 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of wibenton@redhat.com designates 209.132.183.25 as permitted sender)
Received: from [209.132.183.25] (HELO mx4-phx2.redhat.com) (209.132.183.25)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 02:51:32 +0000
Received: from zmail14.collab.prod.int.phx2.redhat.com (zmail14.collab.prod.int.phx2.redhat.com [10.5.83.16])
	by mx4-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s6I2p4VR003128
	for <dev@spark.apache.org>; Thu, 17 Jul 2014 22:51:04 -0400
Date: Thu, 17 Jul 2014 22:51:04 -0400 (EDT)
From: Will Benton <willb@redhat.com>
To: dev@spark.apache.org
Message-ID: <1409585.21155055.1405651864653.JavaMail.zimbra@redhat.com>
In-Reply-To: <1307149058.20979409.1405628176619.JavaMail.zimbra@redhat.com>
Subject: preferred Hive/Hadoop environment for generating golden test
 outputs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - FF30 (Mac)/8.0.6_GA_5922)
Thread-Topic: preferred Hive/Hadoop environment for generating golden test outputs
Thread-Index: nXxZXBmdcM+iYr6Oh41RqofI6Hbhyw==
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

What's the preferred environment for generating golden test outputs for new Hive tests?  In particular:

* what Hadoop version and Hive version should I be using,
* are there particular distributions people have run successfully, and 
* are there any system properties or environment variables (beyond HADOOP_HOME, HIVE_HOME, and HIVE_DEV_HOME) I need to set before running the suite?

I ask because I'm getting some errors while trying to add new tests and would like to eliminate any possible problems caused by differences between what my environment offers and what Spark expects.  (I'm currently running with the Fedora packages for Hadoop 2.2.0 and a locally-built Hive 0.12.0.)  Since I'll only be using this for generating test outputs, something as simple to set up as possible would be great.

(Once I get something working, I'll be happy to write it up and contribute it as developer docs.)


thanks,
wb

From dev-return-8432-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 02:53:04 2014
Return-Path: <dev-return-8432-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1637611EA1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 02:53:04 +0000 (UTC)
Received: (qmail 43858 invoked by uid 500); 18 Jul 2014 02:53:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43803 invoked by uid 500); 18 Jul 2014 02:53:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 43783 invoked by uid 99); 18 Jul 2014 02:53:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 02:53:03 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 02:52:57 +0000
Received: by mail-qg0-f42.google.com with SMTP id j5so2841015qga.1
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 19:52:37 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=XjoKVsb+0Rdce1uOAueepTyAnphqEbPaNnm5cG+Elyw=;
        b=JC9cMdjEjKxXEH3kSZmMpkidL/ZfkYU2llKMdqnzv86uEDNG6LA+O2/MR+K5w9Fr+P
         f18h11FxnS51EEE1cKW/njPbNFhikM43HCDAPEHReqOIPTXL2B84RzFu0w0RpWL1yePq
         XlyTiPt/KMjcCMsQcrCMznOYNLXaFzgpRLjMRhYKcWJFLYawdxuMsLh17wC0VFOohQjc
         sy3XuVyTktHwyO4u5oOvuVW2iyFQXDh/q5c1qKzORj/Y5WgCA21Eg/79Ew4X43tiBAwh
         j7ucF0AR58X5bmuF2oguxyE8EF//VTDNB3YITVqO+e7iJvSBek17Et4ijXmFQXVzsIAf
         U/gg==
X-Gm-Message-State: ALoCoQkNvmp6B/9a5tz5xiYEhYRU692m/t3rpIbdTVLTuBXZPmlZRO/Dbcu56IEDrXEOm7OP5wkF
MIME-Version: 1.0
X-Received: by 10.140.35.232 with SMTP id n95mr2467520qgn.82.1405651956870;
 Thu, 17 Jul 2014 19:52:36 -0700 (PDT)
Received: by 10.96.98.100 with HTTP; Thu, 17 Jul 2014 19:52:36 -0700 (PDT)
In-Reply-To: <6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
	<CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
	<6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
Date: Thu, 17 Jul 2014 19:52:36 -0700
Message-ID: <CAPh_B=a3FooEaDYz5ZKANh7zSuQs1kfX2QQ8+cuz8JNExGx5BQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Reynold Xin <rxin@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c03f3491479e04fe6ed977
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c03f3491479e04fe6ed977
Content-Type: text/plain; charset=UTF-8

+1

On Thursday, July 17, 2014, Matei Zaharia <matei.zaharia@gmail.com> wrote:

> +1
>
> Tested on Mac, verified CHANGES.txt is good, verified several of the bug
> fixes.
>
> Matei
>
> On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com
> <javascript:;>> wrote:
>
> > I start the voting with a +1.
> >
> > Ran tests on the release candidates and some basic operations in
> > spark-shell and pyspark (local and standalone).
> >
> > -Xiangrui
> >
> > On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com
> <javascript:;>> wrote:
> >> Please vote on releasing the following candidate as Apache Spark
> version 0.9.2!
> >>
> >> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
> >>
> >> The release files, including signatures, digests, etc. can be found at:
> >> http://people.apache.org/~meng/spark-0.9.2-rc1/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/meng.asc
> >>
> >> The staging repository for this release can be found at:
> >>
> https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 0.9.2!
> >>
> >> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >>
> >> [ ] +1 Release this package as Apache Spark 0.9.2
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >>
> >> === About this release ===
> >> This release fixes a few high-priority bugs in 0.9.1 and has a variety
> >> of smaller fixes. The full list is here: http://s.apache.org/d0t. Some
> >> of the more visible patches are:
> >>
> >> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
> size
> >> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> >> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
> >> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
> >> SPARK-1870: Secondary jars are not added to executor classpath for YARN
> >>
> >> This is the second maintenance release on the 0.9 line. We plan to make
> >> additional maintenance releases as new fixes come in.
> >>
> >> Best,
> >> Xiangrui
>
>

--001a11c03f3491479e04fe6ed977--

From dev-return-8433-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 03:04:43 2014
Return-Path: <dev-return-8433-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E2E5C11F04
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 03:04:42 +0000 (UTC)
Received: (qmail 67220 invoked by uid 500); 18 Jul 2014 03:04:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67162 invoked by uid 500); 18 Jul 2014 03:04:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67148 invoked by uid 99); 18 Jul 2014 03:04:40 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 03:04:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of concretevitamin@gmail.com designates 74.125.82.178 as permitted sender)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 03:04:38 +0000
Received: by mail-we0-f178.google.com with SMTP id w61so3904115wes.37
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 20:04:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type:content-transfer-encoding;
        bh=bXJ2ivL51OWSt6tdcu+AddhizCmKXEZrbkndRJ4CjgU=;
        b=mXErLDdDRdZ8xy1qAWSusEsngysEF+1DofgGa1271uOLsk9GmkCdtUMIHUhGrDSMe+
         hDeQhHjVvyrxwcfHeuIO3/J6NosP7J0lQcgTHbtKtLI/+E3MdqEBALt49xeqyCO5TeRs
         hk6UytqyeGBROtBizRoxmb8Muaosce5Kp9pbB/+v5dEmFUvjOklaW5U0+geUbheqceOs
         vXqtThE+z1AEp5PoPb8QM+0gCUp25Ginkq118KuuF8RbUPZwI24VCvA6oWuQN8Kbt++e
         30wSoD7bJjD5Q5nrRjkcDCJXzzcfVYp87Ay/hz/6kvSAVPRCP5WBU18YO1AtA0El7nu5
         DoUA==
MIME-Version: 1.0
X-Received: by 10.194.90.51 with SMTP id bt19mr1829746wjb.105.1405652654291;
 Thu, 17 Jul 2014 20:04:14 -0700 (PDT)
Sender: concretevitamin@gmail.com
Received: by 10.217.92.66 with HTTP; Thu, 17 Jul 2014 20:04:14 -0700 (PDT)
In-Reply-To: <1409585.21155055.1405651864653.JavaMail.zimbra@redhat.com>
References: <1307149058.20979409.1405628176619.JavaMail.zimbra@redhat.com>
	<1409585.21155055.1405651864653.JavaMail.zimbra@redhat.com>
Date: Thu, 17 Jul 2014 20:04:14 -0700
X-Google-Sender-Auth: fQSSWJcdK41LU1ZPQIUj0_iv7Uo
Message-ID: <CAG2+eog1LhJEEw2NsH4Tr9o1042_bW49RNs1MWor1H5gkTEjpQ@mail.gmail.com>
Subject: Re: preferred Hive/Hadoop environment for generating golden test outputs
From: Zongheng Yang <zongheng.y@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Will,

These three environment variables are needed [1].

I have had success with Hive 0.12 and Hadoop 1.0.4. For Hive, getting
the source distribution seems to be required. Docs contribution will
be much appreciated!

[1] https://github.com/apache/spark/tree/master/sql#other-dependencies-for-=
developers

Zongheng

On Thu, Jul 17, 2014 at 7:51 PM, Will Benton <willb@redhat.com> wrote:
> Hi all,
>
> What's the preferred environment for generating golden test outputs for n=
ew Hive tests?  In particular:
>
> * what Hadoop version and Hive version should I be using,
> * are there particular distributions people have run successfully, and
> * are there any system properties or environment variables (beyond HADOOP=
_HOME, HIVE_HOME, and HIVE_DEV_HOME) I need to set before running the suite=
?
>
> I ask because I'm getting some errors while trying to add new tests and w=
ould like to eliminate any possible problems caused by differences between =
what my environment offers and what Spark expects.  (I'm currently running =
with the Fedora packages for Hadoop 2.2.0 and a locally-built Hive 0.12.0.)=
  Since I'll only be using this for generating test outputs, something as s=
imple to set up as possible would be great.
>
> (Once I get something working, I'll be happy to write it up and contribut=
e it as developer docs.)
>
>
> thanks,
> wb

From dev-return-8434-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 05:58:23 2014
Return-Path: <dev-return-8434-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F121A11310
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 05:58:23 +0000 (UTC)
Received: (qmail 32326 invoked by uid 500); 18 Jul 2014 05:58:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32268 invoked by uid 500); 18 Jul 2014 05:58:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32256 invoked by uid 99); 18 Jul 2014 05:58:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 05:58:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.53 as permitted sender)
Received: from [209.85.219.53] (HELO mail-oa0-f53.google.com) (209.85.219.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 05:58:17 +0000
Received: by mail-oa0-f53.google.com with SMTP id j17so2296891oag.26
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 22:57:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=+TYBhlXpvH7mtqiNyJFf0Nzwz0lK96rOJuQ3mCCK04A=;
        b=yGZUklVdInnzn/elPDQV1ZrlJAMrwtv6L71S0lpHYsSBdK/wXxMMFH2E22bhLMTaVM
         CG26tUVwqZ1c1s4/yth2U9gub/p4aupHZtqE0vpfLvDkLzLpA/yNkbF0sS2QWMXqVhkS
         OEocP5tSUJPhBoJbxL81VrJx/MObjnPczCHyOzA1HYhP1rJDHO4Ludh64XminE4MQSa1
         921glcZHmgPW2tD66jved1VOz06yMgJRIOUNN039UtEjxrCHN8T6wIhGM26SV7qS2leQ
         B/bnCTtF7VKNkFtRzHa4Ie1lfBaDDLqhodRLZcguYBcwMIk9y5a1avOnAzIo7oYqCM+d
         wz4w==
MIME-Version: 1.0
X-Received: by 10.60.158.41 with SMTP id wr9mr3010248oeb.46.1405663076436;
 Thu, 17 Jul 2014 22:57:56 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Thu, 17 Jul 2014 22:57:56 -0700 (PDT)
In-Reply-To: <CACkSZy3jeGykv1b493ja2ZiUvJJ+xC0HnzvmF44OQS651xp4DQ@mail.gmail.com>
References: <CACkSZy3jeGykv1b493ja2ZiUvJJ+xC0HnzvmF44OQS651xp4DQ@mail.gmail.com>
Date: Thu, 17 Jul 2014 22:57:56 -0700
Message-ID: <CABPQxsuk5xXk2uUw0Hfjd3sW27Yq-LuX3NXgjoebq8Qrjtr+DA@mail.gmail.com>
Subject: Re: Current way to include hive in a build
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Stephen,

The only change the build was that we ask users to run -Phive and
-Pyarn of --with-hive and --with-yarn (which internally just set
-Phive and -Pyarn). I don't think this should affect the dependency
graph.

Just to test this, what happens if you run *without* the CDH profile
and build with hadoop version 2.3.0? Does that work?

- Patrick

On Thu, Jul 17, 2014 at 4:00 PM, Stephen Boesch <javadba@gmail.com> wrote:
> Having looked at trunk make-distribution.sh the --with-hive and --with-yarn
> are now deprecated.
>
> Here is the way I have built it:
>
> Added to pom.xml:
>
>    <profile>
>       <id>cdh5</id>
>       <activation>
>         <activeByDefault>false</activeByDefault>
>       </activation>
>       <properties>
>         <hadoop.version>2.3.0-cdh5.0.0</hadoop.version>
>         <yarn.version>2.3.0-cdh5.0.0</yarn.version>
>         <hbase.version>0.96.1.1-cdh5.0.0</hbase.version>
>         <zookeeper.version>3.4.5-cdh5.0.0</zookeeper.version>
>       </properties>
>     </profile>
>
> *mvn -Pyarn -Pcdh5 -Phive -Dhadoop.version=2.3.0-cdh5.0.1
> -Dyarn.version=2.3.0-cdh5.0.0 -DskipTests clean package*
>
>
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project Parent POM .......................... SUCCESS [3.165s]
> [INFO] Spark Project Core ................................ SUCCESS
> [2:39.504s]
> [INFO] Spark Project Bagel ............................... SUCCESS [7.596s]
> [INFO] Spark Project GraphX .............................. SUCCESS [22.027s]
> [INFO] Spark Project ML Library .......................... SUCCESS [36.284s]
> [INFO] Spark Project Streaming ........................... SUCCESS [24.309s]
> [INFO] Spark Project Tools ............................... SUCCESS [3.147s]
> [INFO] Spark Project Catalyst ............................ SUCCESS [20.148s]
> [INFO] Spark Project SQL ................................. SUCCESS [18.560s]
> *[INFO] Spark Project Hive ................................ FAILURE
> [33.962s]*
>
> [ERROR] Failed to execute goal
> org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
> (copy-dependencies) on project spark-hive_2.10: Execution copy-dependencies
> of goal
> org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
> failed: Plugin org.apache.maven.plugins:maven-dependency-plugin:2.4 or one
> of its dependencies could not be resolved: Could not find artifact
> commons-logging:commons-logging:jar:1.0.4 -> [Help 1]
>
> Anyone who is presently building with -Phive and has a suggestion for this?

From dev-return-8435-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 06:13:37 2014
Return-Path: <dev-return-8435-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2C5E3113A9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 06:13:37 +0000 (UTC)
Received: (qmail 70589 invoked by uid 500); 18 Jul 2014 06:13:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70525 invoked by uid 500); 18 Jul 2014 06:13:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70501 invoked by uid 99); 18 Jul 2014 06:13:35 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 06:13:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 06:13:33 +0000
Received: by mail-wg0-f51.google.com with SMTP id b13so2992284wgh.10
        for <dev@spark.apache.org>; Thu, 17 Jul 2014 23:13:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TyptdjcMCVJg4wpfmIncid4N4PvoxvL8GqK0qHO69vA=;
        b=xFWEzn0HAz7UbMNcgP2zMYpXo5lXXgz30/KUl2k11mIkZ37T1I8KwsZjIGqGQFLzv7
         v+gDCeDkngDZ+KTZQ4olbF/+dYDUqbjAZQMhonJWqITK0dQaYRvozUB8yDMR9UPIPFiD
         sGpmWJKFVJ1+M/4fqzleAp48O6IzhgY0K9UNzbMQ7l1bfIruUHpOclVjQNt+uk9I5az7
         j8T9vwklT/nfgHWFep7XLFA743j7fdsgVJ/aq3NV94GhBiep+bQZsDg6ZD/4vlkpacuI
         z25o/+aRg0UxUBH1TBz3iymDLwic7Ag5wX/3edpg59yYB01zcolv5GGXRINTsAlq6umz
         aJbw==
MIME-Version: 1.0
X-Received: by 10.180.205.138 with SMTP id lg10mr4093802wic.49.1405663989752;
 Thu, 17 Jul 2014 23:13:09 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Thu, 17 Jul 2014 23:13:09 -0700 (PDT)
In-Reply-To: <CAPh_B=a3FooEaDYz5ZKANh7zSuQs1kfX2QQ8+cuz8JNExGx5BQ@mail.gmail.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
	<CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
	<6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
	<CAPh_B=a3FooEaDYz5ZKANh7zSuQs1kfX2QQ8+cuz8JNExGx5BQ@mail.gmail.com>
Date: Thu, 17 Jul 2014 23:13:09 -0700
Message-ID: <CAJgQjQ8mhng3d9==CfVqjEc1gUfkruO7xBeUzWM0rnAUHG0VYA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

UPDATE:

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1023/

The previous repo contains exactly the same content but mutable.
Thanks Patrick for pointing it out!

-Xiangrui

On Thu, Jul 17, 2014 at 7:52 PM, Reynold Xin <rxin@databricks.com> wrote:
> +1
>
> On Thursday, July 17, 2014, Matei Zaharia <matei.zaharia@gmail.com> wrote:
>
>> +1
>>
>> Tested on Mac, verified CHANGES.txt is good, verified several of the bug
>> fixes.
>>
>> Matei
>>
>> On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com
>> <javascript:;>> wrote:
>>
>> > I start the voting with a +1.
>> >
>> > Ran tests on the release candidates and some basic operations in
>> > spark-shell and pyspark (local and standalone).
>> >
>> > -Xiangrui
>> >
>> > On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com
>> <javascript:;>> wrote:
>> >> Please vote on releasing the following candidate as Apache Spark
>> version 0.9.2!
>> >>
>> >> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
>> >>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
>> >>
>> >> The release files, including signatures, digests, etc. can be found at:
>> >> http://people.apache.org/~meng/spark-0.9.2-rc1/
>> >>
>> >> Release artifacts are signed with the following key:
>> >> https://people.apache.org/keys/committer/meng.asc
>> >>
>> >> The staging repository for this release can be found at:
>> >>
>> https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/
>> >>
>> >> The documentation corresponding to this release can be found at:
>> >> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
>> >>
>> >> Please vote on releasing this package as Apache Spark 0.9.2!
>> >>
>> >> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
>> >> a majority of at least 3 +1 PMC votes are cast.
>> >>
>> >> [ ] +1 Release this package as Apache Spark 0.9.2
>> >> [ ] -1 Do not release this package because ...
>> >>
>> >> To learn more about Apache Spark, please see
>> >> http://spark.apache.org/
>> >>
>> >> === About this release ===
>> >> This release fixes a few high-priority bugs in 0.9.1 and has a variety
>> >> of smaller fixes. The full list is here: http://s.apache.org/d0t. Some
>> >> of the more visible patches are:
>> >>
>> >> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>> size
>> >> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>> >> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
>> >> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
>> >> SPARK-1870: Secondary jars are not added to executor classpath for YARN
>> >>
>> >> This is the second maintenance release on the 0.9 line. We plan to make
>> >> additional maintenance releases as new fixes come in.
>> >>
>> >> Best,
>> >> Xiangrui
>>
>>

From dev-return-8436-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 07:04:20 2014
Return-Path: <dev-return-8436-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1564A11500
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 07:04:20 +0000 (UTC)
Received: (qmail 72619 invoked by uid 500); 18 Jul 2014 07:04:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72559 invoked by uid 500); 18 Jul 2014 07:04:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72547 invoked by uid 99); 18 Jul 2014 07:04:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 07:04:18 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.52 as permitted sender)
Received: from [209.85.219.52] (HELO mail-oa0-f52.google.com) (209.85.219.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 07:04:13 +0000
Received: by mail-oa0-f52.google.com with SMTP id o6so2515352oag.11
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 00:03:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=dNY0B4P94R2xW0b9rUztBz+9OClsFsZU/FZa6XHEqu8=;
        b=vv4TsT8RDBgVPTOQfmSI5tvzHHByF2r0KnewFqSYmW5N9Fllr/gsLuiAvsIHwZP0/J
         nT8hYsj6XcclM+h9aasm221acCIonqq1PcwHJ/4ANYq7F170+Rnh5qP3YTncoNXfe9Fy
         d/WZSYk9adz2XdGYTaI2lAp/TnwWYHCXilmiJnSWLLPYqTnMTrH2hqZj1c0GyshPPRgJ
         toVFZPgUEjdSt8LyoIhRz6P8q9VUcpDRi8WZXsCjA3jaqxmSIY2aHjRsPg2j+76KDPiF
         bgTWANQvboqiN/d7WUVNb7ImuYl0J2E8YKT1Z8Lq8NRLG+wgHzIOwQegOOZiAKHjqXKL
         Mylw==
MIME-Version: 1.0
X-Received: by 10.182.108.228 with SMTP id hn4mr3341260obb.73.1405667033258;
 Fri, 18 Jul 2014 00:03:53 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Fri, 18 Jul 2014 00:03:53 -0700 (PDT)
In-Reply-To: <CAJgQjQ8mhng3d9==CfVqjEc1gUfkruO7xBeUzWM0rnAUHG0VYA@mail.gmail.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
	<CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
	<6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
	<CAPh_B=a3FooEaDYz5ZKANh7zSuQs1kfX2QQ8+cuz8JNExGx5BQ@mail.gmail.com>
	<CAJgQjQ8mhng3d9==CfVqjEc1gUfkruO7xBeUzWM0rnAUHG0VYA@mail.gmail.com>
Date: Fri, 18 Jul 2014 00:03:53 -0700
Message-ID: <CABPQxsvpHhXUyAaNEyL+Jx5Q9DJTSL76+q9sUAHcNFGusLvzGw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

+1

- Looked through the release commits
- Looked through JIRA issues
- Ran the audit tests (one issue with the maven app test, but was also
an issue with 0.9.1 so I think it's my environment)
- Checked sigs/sums

On Thu, Jul 17, 2014 at 11:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> UPDATE:
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1023/
>
> The previous repo contains exactly the same content but mutable.
> Thanks Patrick for pointing it out!
>
> -Xiangrui
>
> On Thu, Jul 17, 2014 at 7:52 PM, Reynold Xin <rxin@databricks.com> wrote:
>> +1
>>
>> On Thursday, July 17, 2014, Matei Zaharia <matei.zaharia@gmail.com> wrote:
>>
>>> +1
>>>
>>> Tested on Mac, verified CHANGES.txt is good, verified several of the bug
>>> fixes.
>>>
>>> Matei
>>>
>>> On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com
>>> <javascript:;>> wrote:
>>>
>>> > I start the voting with a +1.
>>> >
>>> > Ran tests on the release candidates and some basic operations in
>>> > spark-shell and pyspark (local and standalone).
>>> >
>>> > -Xiangrui
>>> >
>>> > On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com
>>> <javascript:;>> wrote:
>>> >> Please vote on releasing the following candidate as Apache Spark
>>> version 0.9.2!
>>> >>
>>> >> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
>>> >>
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
>>> >>
>>> >> The release files, including signatures, digests, etc. can be found at:
>>> >> http://people.apache.org/~meng/spark-0.9.2-rc1/
>>> >>
>>> >> Release artifacts are signed with the following key:
>>> >> https://people.apache.org/keys/committer/meng.asc
>>> >>
>>> >> The staging repository for this release can be found at:
>>> >>
>>> https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/
>>> >>
>>> >> The documentation corresponding to this release can be found at:
>>> >> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
>>> >>
>>> >> Please vote on releasing this package as Apache Spark 0.9.2!
>>> >>
>>> >> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
>>> >> a majority of at least 3 +1 PMC votes are cast.
>>> >>
>>> >> [ ] +1 Release this package as Apache Spark 0.9.2
>>> >> [ ] -1 Do not release this package because ...
>>> >>
>>> >> To learn more about Apache Spark, please see
>>> >> http://spark.apache.org/
>>> >>
>>> >> === About this release ===
>>> >> This release fixes a few high-priority bugs in 0.9.1 and has a variety
>>> >> of smaller fixes. The full list is here: http://s.apache.org/d0t. Some
>>> >> of the more visible patches are:
>>> >>
>>> >> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka frame
>>> size
>>> >> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>>> >> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
>>> >> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
>>> >> SPARK-1870: Secondary jars are not added to executor classpath for YARN
>>> >>
>>> >> This is the second maintenance release on the 0.9 line. We plan to make
>>> >> additional maintenance releases as new fixes come in.
>>> >>
>>> >> Best,
>>> >> Xiangrui
>>>
>>>

From dev-return-8437-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 08:08:20 2014
Return-Path: <dev-return-8437-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 71A4A117D5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 08:08:20 +0000 (UTC)
Received: (qmail 90079 invoked by uid 500); 18 Jul 2014 08:08:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90023 invoked by uid 500); 18 Jul 2014 08:08:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90010 invoked by uid 99); 18 Jul 2014 08:08:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 08:08:19 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.49] (HELO mail-pa0-f49.google.com) (209.85.220.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 08:08:17 +0000
Received: by mail-pa0-f49.google.com with SMTP id hz1so4894371pad.8
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 01:07:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=lmeUtkNZzSKn9wIh76hXePW6zfkV5UJGHCJ5IHNG0B0=;
        b=kLNYObcPp48177uxg3xwm2CXTI6Sh1DbhwstA28QdsMRF5Sy8XWGM/H0FMdYktYEC1
         FpXWtRT3mRBX9pkESHzUPzPV1m1h8iGl+4DlIfAxbeOYJ9fNc8rRM/jFSjVv+oZRgmKu
         1GgmaGODXr+JHUbxk9TtQIYv13caXapIlOW61MUZOsi3fylYiy5yTsUXs5S+C7i2UcJQ
         3CQmznqVuliMfZX1H0YKW8Tm+qCuc2Up1owDnTSu8DkHws8Rx+CTdwx/LMf4IemSJiHC
         T0zU/0Mx92aYlReNpkT29Tx/AT9NTuFtbT5QvGIVwaxmmbg9UyCdIbF0lFPYp9Yw7TW/
         BYTQ==
X-Gm-Message-State: ALoCoQl9cjW9DW0qgwLmcfOwuQD+IOXe3mHbQ65Y/OanXaP/wBMFnM7s72PwzD+QYSwV6gWiq+TF
MIME-Version: 1.0
X-Received: by 10.66.163.65 with SMTP id yg1mr2866425pab.33.1405670871393;
 Fri, 18 Jul 2014 01:07:51 -0700 (PDT)
Received: by 10.70.5.227 with HTTP; Fri, 18 Jul 2014 01:07:51 -0700 (PDT)
In-Reply-To: <CABPQxsvpHhXUyAaNEyL+Jx5Q9DJTSL76+q9sUAHcNFGusLvzGw@mail.gmail.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
	<CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
	<6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
	<CAPh_B=a3FooEaDYz5ZKANh7zSuQs1kfX2QQ8+cuz8JNExGx5BQ@mail.gmail.com>
	<CAJgQjQ8mhng3d9==CfVqjEc1gUfkruO7xBeUzWM0rnAUHG0VYA@mail.gmail.com>
	<CABPQxsvpHhXUyAaNEyL+Jx5Q9DJTSL76+q9sUAHcNFGusLvzGw@mail.gmail.com>
Date: Fri, 18 Jul 2014 01:07:51 -0700
Message-ID: <CAMJOb8kDsBv7NL0GFPfBdg9MkCMWgFNusa2c+5gL1u4cQ-Tnfg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b6d7c66f5ee1404fe734079
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6d7c66f5ee1404fe734079
Content-Type: text/plain; charset=UTF-8

+1, tested on standalone cluster and ran spark shell, pyspark and SparkPi


2014-07-18 0:03 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

> +1
>
> - Looked through the release commits
> - Looked through JIRA issues
> - Ran the audit tests (one issue with the maven app test, but was also
> an issue with 0.9.1 so I think it's my environment)
> - Checked sigs/sums
>
> On Thu, Jul 17, 2014 at 11:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
> > UPDATE:
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1023/
> >
> > The previous repo contains exactly the same content but mutable.
> > Thanks Patrick for pointing it out!
> >
> > -Xiangrui
> >
> > On Thu, Jul 17, 2014 at 7:52 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >> +1
> >>
> >> On Thursday, July 17, 2014, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> >>
> >>> +1
> >>>
> >>> Tested on Mac, verified CHANGES.txt is good, verified several of the
> bug
> >>> fixes.
> >>>
> >>> Matei
> >>>
> >>> On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com
> >>> <javascript:;>> wrote:
> >>>
> >>> > I start the voting with a +1.
> >>> >
> >>> > Ran tests on the release candidates and some basic operations in
> >>> > spark-shell and pyspark (local and standalone).
> >>> >
> >>> > -Xiangrui
> >>> >
> >>> > On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com
> >>> <javascript:;>> wrote:
> >>> >> Please vote on releasing the following candidate as Apache Spark
> >>> version 0.9.2!
> >>> >>
> >>> >> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
> >>> >>
> >>>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
> >>> >>
> >>> >> The release files, including signatures, digests, etc. can be found
> at:
> >>> >> http://people.apache.org/~meng/spark-0.9.2-rc1/
> >>> >>
> >>> >> Release artifacts are signed with the following key:
> >>> >> https://people.apache.org/keys/committer/meng.asc
> >>> >>
> >>> >> The staging repository for this release can be found at:
> >>> >>
> >>>
> https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/
> >>> >>
> >>> >> The documentation corresponding to this release can be found at:
> >>> >> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
> >>> >>
> >>> >> Please vote on releasing this package as Apache Spark 0.9.2!
> >>> >>
> >>> >> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
> >>> >> a majority of at least 3 +1 PMC votes are cast.
> >>> >>
> >>> >> [ ] +1 Release this package as Apache Spark 0.9.2
> >>> >> [ ] -1 Do not release this package because ...
> >>> >>
> >>> >> To learn more about Apache Spark, please see
> >>> >> http://spark.apache.org/
> >>> >>
> >>> >> === About this release ===
> >>> >> This release fixes a few high-priority bugs in 0.9.1 and has a
> variety
> >>> >> of smaller fixes. The full list is here: http://s.apache.org/d0t.
> Some
> >>> >> of the more visible patches are:
> >>> >>
> >>> >> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka
> frame
> >>> size
> >>> >> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
> >>> >> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
> >>> >> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
> >>> >> SPARK-1870: Secondary jars are not added to executor classpath for
> YARN
> >>> >>
> >>> >> This is the second maintenance release on the 0.9 line. We plan to
> make
> >>> >> additional maintenance releases as new fixes come in.
> >>> >>
> >>> >> Best,
> >>> >> Xiangrui
> >>>
> >>>
>

--047d7b6d7c66f5ee1404fe734079--

From dev-return-8438-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 10:16:53 2014
Return-Path: <dev-return-8438-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4C1F11BC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 10:16:53 +0000 (UTC)
Received: (qmail 43067 invoked by uid 500); 18 Jul 2014 10:16:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 43000 invoked by uid 500); 18 Jul 2014 10:16:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42985 invoked by uid 99); 18 Jul 2014 10:16:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 10:16:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.171 as permitted sender)
Received: from [209.85.220.171] (HELO mail-vc0-f171.google.com) (209.85.220.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 10:16:47 +0000
Received: by mail-vc0-f171.google.com with SMTP id hq11so5423273vcb.16
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 03:16:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=1u71/v9YxW5isIFK3VCaiHBExWgHFpiZttu9mCkADXk=;
        b=mZD6HUCgpOYu5RwlncpWZmAYHCAvw8V84N9hMQY2cmsppS+K3Ei4fWx3UQAmyop6xa
         Ixv7LnJl07ziLaIe6mMCvgLcWlqNGXC3L9EEfANoqlUyzC32esG2iaPwwCUuUU9bwfm8
         d1KDWsztIA/Rp0N+i0jrPulVy9idGz0nPzUJcUNULoKjBdHTKwdf3S2i4Aa5x7hwc5NV
         sOBqFqj12VD3qhmMV88imYBYlini5UZugjuOsAZebw3ydc5IMiN7Ve2UGGJgcCzrdaYt
         8tBqdBw2YVV8miYZ+0baouKYVXf03SuRYInKZjpHRrfXkfh73mlZFj21n39sVvZOyhDG
         iL4w==
X-Gm-Message-State: ALoCoQmHIwtaAOf9pAHhFbuPk1aiE6QBVGjXcNNyLkgt8AO43PZYGCPhoXXNSwiJnhHtZuwGvUDb
X-Received: by 10.53.13.200 with SMTP id fa8mr3280490vdd.57.1405678583164;
 Fri, 18 Jul 2014 03:16:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.247.97 with HTTP; Fri, 18 Jul 2014 03:16:03 -0700 (PDT)
In-Reply-To: <CACkSZy3jeGykv1b493ja2ZiUvJJ+xC0HnzvmF44OQS651xp4DQ@mail.gmail.com>
References: <CACkSZy3jeGykv1b493ja2ZiUvJJ+xC0HnzvmF44OQS651xp4DQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Fri, 18 Jul 2014 11:16:03 +0100
Message-ID: <CAMAsSdL_u8f-TE66KBu3t4k8YOS61=hoVW6ZQsFkTVqWYzg6XQ@mail.gmail.com>
Subject: Re: Current way to include hive in a build
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

This build invocation works just as you have it, for me. (At least, it
gets through Hive; Examples fails for a different unrelated reason.)

commons-logging 1.0.4 exists in Maven for sure. Maybe there is some
temporary problem accessing Maven's repo?

On Fri, Jul 18, 2014 at 12:00 AM, Stephen Boesch <javadba@gmail.com> wrote:
> Added to pom.xml:
>
>    <profile>
>       <id>cdh5</id>
>       <activation>
>         <activeByDefault>false</activeByDefault>
>       </activation>
>       <properties>
>         <hadoop.version>2.3.0-cdh5.0.0</hadoop.version>
>         <yarn.version>2.3.0-cdh5.0.0</yarn.version>
>         <hbase.version>0.96.1.1-cdh5.0.0</hbase.version>
>         <zookeeper.version>3.4.5-cdh5.0.0</zookeeper.version>
>       </properties>
>     </profile>
>
> *mvn -Pyarn -Pcdh5 -Phive -Dhadoop.version=2.3.0-cdh5.0.1
> -Dyarn.version=2.3.0-cdh5.0.0 -DskipTests clean package*
>
>
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project Parent POM .......................... SUCCESS [3.165s]
> [INFO] Spark Project Core ................................ SUCCESS
> [2:39.504s]
> [INFO] Spark Project Bagel ............................... SUCCESS [7.596s]
> [INFO] Spark Project GraphX .............................. SUCCESS [22.027s]
> [INFO] Spark Project ML Library .......................... SUCCESS [36.284s]
> [INFO] Spark Project Streaming ........................... SUCCESS [24.309s]
> [INFO] Spark Project Tools ............................... SUCCESS [3.147s]
> [INFO] Spark Project Catalyst ............................ SUCCESS [20.148s]
> [INFO] Spark Project SQL ................................. SUCCESS [18.560s]
> *[INFO] Spark Project Hive ................................ FAILURE
> [33.962s]*
>
> [ERROR] Failed to execute goal
> org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
> (copy-dependencies) on project spark-hive_2.10: Execution copy-dependencies
> of goal
> org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
> failed: Plugin org.apache.maven.plugins:maven-dependency-plugin:2.4 or one
> of its dependencies could not be resolved: Could not find artifact
> commons-logging:commons-logging:jar:1.0.4 -> [Help 1]
>
> Anyone who is presently building with -Phive and has a suggestion for this?

From dev-return-8439-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 11:44:14 2014
Return-Path: <dev-return-8439-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98A4511DB6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 11:44:14 +0000 (UTC)
Received: (qmail 73984 invoked by uid 500); 18 Jul 2014 11:44:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73944 invoked by uid 500); 18 Jul 2014 11:44:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73932 invoked by uid 99); 18 Jul 2014 11:44:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 11:44:13 +0000
X-ASF-Spam-Status: No, hits=-2.3 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy)
Received: from [15.201.208.53] (HELO g4t3425.houston.hp.com) (15.201.208.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 11:44:10 +0000
Received: from G9W0364.americas.hpqcorp.net (g9w0364.houston.hp.com [16.216.193.45])
	(using TLSv1 with cipher AES128-SHA (128/128 bits))
	(No client certificate requested)
	by g4t3425.houston.hp.com (Postfix) with ESMTPS id DA1761C0
	for <dev@spark.apache.org>; Fri, 18 Jul 2014 11:43:44 +0000 (UTC)
Received: from G4W6302.americas.hpqcorp.net (16.210.26.227) by
 G9W0364.americas.hpqcorp.net (16.216.193.45) with Microsoft SMTP Server (TLS)
 id 14.3.169.1; Fri, 18 Jul 2014 11:42:30 +0000
Received: from G4W3292.americas.hpqcorp.net ([169.254.1.241]) by
 G4W6302.americas.hpqcorp.net ([16.210.26.227]) with mapi id 14.03.0169.001;
 Fri, 18 Jul 2014 11:42:30 +0000
From: "Ulanov, Alexander" <alexander.ulanov@hp.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: Feature selection interface
Thread-Topic: Feature selection interface
Thread-Index: Ac+cYtczmLagap9pRNythu+fMYQelwGGcMvw
Date: Fri, 18 Jul 2014 11:42:29 +0000
Message-ID: <9D5B00849D2CDA4386BDA89E83F69E6C0FCB33E6@G4W3292.americas.hpqcorp.net>
References: <9D5B00849D2CDA4386BDA89E83F69E6C0FCADD05@G4W3292.americas.hpqcorp.net>
In-Reply-To: <9D5B00849D2CDA4386BDA89E83F69E6C0FCADD05@G4W3292.americas.hpqcorp.net>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [15.201.58.28]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

FYI This is my first take on feature selection, filtering and chi-squared:
https://github.com/apache/spark/pull/1484


-----Original Message-----
From: Ulanov, Alexander=20
Sent: Thursday, July 10, 2014 9:39 PM
To: dev@spark.apache.org
Subject: Feature selection interface

Hi,

I've implemented a class that does Chi-squared feature selection for RDD[La=
beledPoint]. It also computes basic class/feature occurrence statistics and=
 other methods like mutual information or information gain can be easily im=
plemented. I would like to make a pull request. However, MLlib master branc=
h doesn't have any feature selection methods implemented. So, I need to cre=
ate a proper interface that my class will extend or mix. It should be easy =
to use from developers and users prospective.

I was thinking that there should be FeatureEvaluator that for each feature =
from RDD[LabeledPoint] returns RDD[((featureIndex: Int, label: Double), val=
ue: Double)].
Then there should be FeatureSelector that selects top N features or top N f=
eatures group by class etc.
And the simplest one, FeatureFilter that filters the data based on set of f=
eature indices.

Additionally, there should be the interface for FeatureEvaluators that don'=
t use class labels, i.e. for RDD[Vector].

I am concerned that such design looks rather "disconnected" because there a=
re 3 disconnected objects.

As a result of use, I would like to see something like "val filteredData =
=3D Filter(data, ChiSquared(data).selectTop(100))".

Any ideas or suggestions?

Best regards, Alexander

From dev-return-8440-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 12:06:22 2014
Return-Path: <dev-return-8440-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C558A11E47
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 12:06:22 +0000 (UTC)
Received: (qmail 15956 invoked by uid 500); 18 Jul 2014 12:06:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15919 invoked by uid 500); 18 Jul 2014 12:06:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15904 invoked by uid 99); 18 Jul 2014 12:06:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 12:06:21 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 12:06:17 +0000
Received: by mail-wi0-f176.google.com with SMTP id bs8so742061wib.9
        for <dev@spark.incubator.apache.org>; Fri, 18 Jul 2014 05:05:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=zUaZ7mHnkblbDhiXMcsRMqleSKKVEUi3p3PKNK80IG0=;
        b=PGs2EV8VyNCQ/lDe68wZRsCkVKb8kZ/W5OXKTwAbtL59LLoxDGuZ+LmASGr2ejLGXF
         KyYIcA3ujYC91uNKNpAqAnre9Jd2/J0F9Q9laC2lb/3gzqYY2vLLnGljU2ugX8K1FYuS
         phX978tdHzWO7j6nK4P4OtTtgBjeL/S1D6I49BRpK15NFCKWUNHzbUWWzPKMeKi+zGK2
         k2ErfVUHacdOunaObCIPlqJQkixC66ycYYc04vc2/Yg9jSASdvn2rXvyxljPX/3wEyZF
         3ZvnGBDJMBkGRx7eJaCGx2eBuLy0SsrUc1RK5bZMdOynouaeQefTApxaf/hz+4eIsnbl
         Ev0A==
MIME-Version: 1.0
X-Received: by 10.180.74.11 with SMTP id p11mr31167293wiv.68.1405685155124;
 Fri, 18 Jul 2014 05:05:55 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Fri, 18 Jul 2014 05:05:55 -0700 (PDT)
In-Reply-To: <1405643464811-7398.post@n3.nabble.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
	<CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
	<CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
	<CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
	<CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
	<CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
	<1404909567855.c2a8fd87@Nodemailer>
	<CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
	<1405003674571.192c3983@Nodemailer>
	<1405643464811-7398.post@n3.nabble.com>
Date: Fri, 18 Jul 2014 08:05:55 -0400
Message-ID: <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Nice to meet you, Jeremy!

This is great!  Hierarchical clustering was next on my list --
currently trying to get my PR for MiniBatch KMeans accepted.

If it's cool with you, I'll try converting your code to fit in with
the existing MLLib code as you suggest. I also need to review the
Decision Tree code (as suggested above) to see how much of that can be
reused.

Maybe I can ask you to do a code review for me when I'm done?





On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
<freeman.jeremy@gmail.com> wrote:
> Hi all,
>
> Cool discussion! I agree that a more standardized API for clustering, and
> easy access to underlying routines, would be useful (we've also been
> discussing this when trying to develop streaming clustering algorithms,
> similar to https://github.com/apache/spark/pull/1361)
>
> For divisive, hierarchical clustering I implemented something awhile back,
> here's a gist.
>
> https://gist.github.com/freeman-lab/5947e7c53b368fe90371
>
> It does bisecting k-means clustering (with k=2), with a recursive class for
> keeping track of the tree. I also found this much better than agglomerative
> methods (for the reasons Hector points out).
>
> This needs to be cleaned up, and can surely be optimized (esp. by replacing
> the core KMeans step with existing MLLib code), but I can say I was running
> it successfully on quite large data sets.
>
> RJ, depending on where you are in your progress, I'd be happy to help work
> on this piece and / or have you use this as a jumping off point, if useful.
>
> -- Jeremy
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.



-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8441-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 12:06:23 2014
Return-Path: <dev-return-8441-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8225C11E48
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 12:06:23 +0000 (UTC)
Received: (qmail 16951 invoked by uid 500); 18 Jul 2014 12:06:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16900 invoked by uid 500); 18 Jul 2014 12:06:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16420 invoked by uid 99); 18 Jul 2014 12:06:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 12:06:22 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rnowling@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 12:06:20 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so720075wiv.1
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 05:05:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=zUaZ7mHnkblbDhiXMcsRMqleSKKVEUi3p3PKNK80IG0=;
        b=PGs2EV8VyNCQ/lDe68wZRsCkVKb8kZ/W5OXKTwAbtL59LLoxDGuZ+LmASGr2ejLGXF
         KyYIcA3ujYC91uNKNpAqAnre9Jd2/J0F9Q9laC2lb/3gzqYY2vLLnGljU2ugX8K1FYuS
         phX978tdHzWO7j6nK4P4OtTtgBjeL/S1D6I49BRpK15NFCKWUNHzbUWWzPKMeKi+zGK2
         k2ErfVUHacdOunaObCIPlqJQkixC66ycYYc04vc2/Yg9jSASdvn2rXvyxljPX/3wEyZF
         3ZvnGBDJMBkGRx7eJaCGx2eBuLy0SsrUc1RK5bZMdOynouaeQefTApxaf/hz+4eIsnbl
         Ev0A==
MIME-Version: 1.0
X-Received: by 10.180.74.11 with SMTP id p11mr31167293wiv.68.1405685155124;
 Fri, 18 Jul 2014 05:05:55 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Fri, 18 Jul 2014 05:05:55 -0700 (PDT)
In-Reply-To: <1405643464811-7398.post@n3.nabble.com>
References: <CAPi87hcYBHckVkQr3+K1N7BH5f1sEUqGFpak+wuGk5Wi1agSYw@mail.gmail.com>
	<CAPud8ToohXQyYWqyL0d4k-i00fhbbjdQj2AHXJWqseigg7j2XQ@mail.gmail.com>
	<CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com>
	<CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com>
	<CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com>
	<CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com>
	<CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com>
	<1404909567855.c2a8fd87@Nodemailer>
	<CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com>
	<1405003674571.192c3983@Nodemailer>
	<1405643464811-7398.post@n3.nabble.com>
Date: Fri, 18 Jul 2014 08:05:55 -0400
Message-ID: <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: dev@spark.incubator.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Nice to meet you, Jeremy!

This is great!  Hierarchical clustering was next on my list --
currently trying to get my PR for MiniBatch KMeans accepted.

If it's cool with you, I'll try converting your code to fit in with
the existing MLLib code as you suggest. I also need to review the
Decision Tree code (as suggested above) to see how much of that can be
reused.

Maybe I can ask you to do a code review for me when I'm done?





On Thu, Jul 17, 2014 at 8:31 PM, Jeremy Freeman
<freeman.jeremy@gmail.com> wrote:
> Hi all,
>
> Cool discussion! I agree that a more standardized API for clustering, and
> easy access to underlying routines, would be useful (we've also been
> discussing this when trying to develop streaming clustering algorithms,
> similar to https://github.com/apache/spark/pull/1361)
>
> For divisive, hierarchical clustering I implemented something awhile back,
> here's a gist.
>
> https://gist.github.com/freeman-lab/5947e7c53b368fe90371
>
> It does bisecting k-means clustering (with k=2), with a recursive class for
> keeping track of the tree. I also found this much better than agglomerative
> methods (for the reasons Hector points out).
>
> This needs to be cleaned up, and can surely be optimized (esp. by replacing
> the core KMeans step with existing MLLib code), but I can say I was running
> it successfully on quite large data sets.
>
> RJ, depending on where you are in your progress, I'd be happy to help work
> on this piece and / or have you use this as a jumping off point, if useful.
>
> -- Jeremy
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7398.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.



-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8442-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 16:36:53 2014
Return-Path: <dev-return-8442-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 582BF115D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 16:36:53 +0000 (UTC)
Received: (qmail 54019 invoked by uid 500); 18 Jul 2014 16:36:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53959 invoked by uid 500); 18 Jul 2014 16:36:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53948 invoked by uid 99); 18 Jul 2014 16:36:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 16:36:52 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Sean.McNamara@webtrends.com designates 216.64.169.23 as permitted sender)
Received: from [216.64.169.23] (HELO pdxmta02.webtrends.com) (216.64.169.23)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 16:36:48 +0000
Received: from pdxex2.webtrends.corp (Not Verified[10.61.2.221]) by pdxmta02.webtrends.com with MailMarshal (v7,2,2,6606) (using TLS: SSLv23)
	id <B53c94cd90000>; Fri, 18 Jul 2014 16:35:37 +0000
Received: from PDXEX1.WebTrends.corp ([172.27.5.220]) by pdxex2.webtrends.corp
 ([172.27.3.221]) with mapi id 14.03.0181.006; Fri, 18 Jul 2014 16:36:22 +0000
From: Sean McNamara <Sean.McNamara@Webtrends.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
Thread-Topic: [VOTE] Release Apache Spark 0.9.2 (RC1)
Thread-Index: AQHPoavkwPy8esZ6yUWl3PslyJQnupukkgAAgAB78oCAABVfAIAAOAiAgAAOLYCAABHfgIAAjhYA
Date: Fri, 18 Jul 2014 16:36:22 +0000
Message-ID: <EBA7027F-7C92-44CE-858F-754A3E872F07@webtrends.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
 <CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
 <6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
 <CAPh_B=a3FooEaDYz5ZKANh7zSuQs1kfX2QQ8+cuz8JNExGx5BQ@mail.gmail.com>
 <CAJgQjQ8mhng3d9==CfVqjEc1gUfkruO7xBeUzWM0rnAUHG0VYA@mail.gmail.com>
 <CABPQxsvpHhXUyAaNEyL+Jx5Q9DJTSL76+q9sUAHcNFGusLvzGw@mail.gmail.com>
 <CAMJOb8kDsBv7NL0GFPfBdg9MkCMWgFNusa2c+5gL1u4cQ-Tnfg@mail.gmail.com>
In-Reply-To: <CAMJOb8kDsBv7NL0GFPfBdg9MkCMWgFNusa2c+5gL1u4cQ-Tnfg@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.61.2.4]
Content-Type: text/plain; charset="us-ascii"
Content-ID: <5417C07902022445A23C9D0572F53185@WebTrends.com>
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

+1


On Jul 18, 2014, at 2:08 AM, Andrew Or <andrew@databricks.com> wrote:

> +1, tested on standalone cluster and ran spark shell, pyspark and SparkPi
>=20
>=20
> 2014-07-18 0:03 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:
>=20
>> +1
>>=20
>> - Looked through the release commits
>> - Looked through JIRA issues
>> - Ran the audit tests (one issue with the maven app test, but was also
>> an issue with 0.9.1 so I think it's my environment)
>> - Checked sigs/sums
>>=20
>> On Thu, Jul 17, 2014 at 11:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote=
:
>>> UPDATE:
>>>=20
>>> The staging repository for this release can be found at:
>>> https://repository.apache.org/content/repositories/orgapachespark-1023/
>>>=20
>>> The previous repo contains exactly the same content but mutable.
>>> Thanks Patrick for pointing it out!
>>>=20
>>> -Xiangrui
>>>=20
>>> On Thu, Jul 17, 2014 at 7:52 PM, Reynold Xin <rxin@databricks.com>
>> wrote:
>>>> +1
>>>>=20
>>>> On Thursday, July 17, 2014, Matei Zaharia <matei.zaharia@gmail.com>
>> wrote:
>>>>=20
>>>>> +1
>>>>>=20
>>>>> Tested on Mac, verified CHANGES.txt is good, verified several of the
>> bug
>>>>> fixes.
>>>>>=20
>>>>> Matei
>>>>>=20
>>>>> On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com
>>>>> <javascript:;>> wrote:
>>>>>=20
>>>>>> I start the voting with a +1.
>>>>>>=20
>>>>>> Ran tests on the release candidates and some basic operations in
>>>>>> spark-shell and pyspark (local and standalone).
>>>>>>=20
>>>>>> -Xiangrui
>>>>>>=20
>>>>>> On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com
>>>>> <javascript:;>> wrote:
>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>> version 0.9.2!
>>>>>>>=20
>>>>>>> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
>>>>>>>=20
>>>>>=20
>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D432=
2c0ba7f411cf9a2483895091440011742246b
>>>>>>>=20
>>>>>>> The release files, including signatures, digests, etc. can be found
>> at:
>>>>>>> http://people.apache.org/~meng/spark-0.9.2-rc1/
>>>>>>>=20
>>>>>>> Release artifacts are signed with the following key:
>>>>>>> https://people.apache.org/keys/committer/meng.asc
>>>>>>>=20
>>>>>>> The staging repository for this release can be found at:
>>>>>>>=20
>>>>>=20
>> https://repository.apache.org/service/local/repositories/orgapachespark-=
1023/content/
>>>>>>>=20
>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
>>>>>>>=20
>>>>>>> Please vote on releasing this package as Apache Spark 0.9.2!
>>>>>>>=20
>>>>>>> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>=20
>>>>>>> [ ] +1 Release this package as Apache Spark 0.9.2
>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>=20
>>>>>>> To learn more about Apache Spark, please see
>>>>>>> http://spark.apache.org/
>>>>>>>=20
>>>>>>> =3D=3D=3D About this release =3D=3D=3D
>>>>>>> This release fixes a few high-priority bugs in 0.9.1 and has a
>> variety
>>>>>>> of smaller fixes. The full list is here: http://s.apache.org/d0t.
>> Some
>>>>>>> of the more visible patches are:
>>>>>>>=20
>>>>>>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka
>> frame
>>>>> size
>>>>>>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>>>>>>> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
>>>>>>> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
>>>>>>> SPARK-1870: Secondary jars are not added to executor classpath for
>> YARN
>>>>>>>=20
>>>>>>> This is the second maintenance release on the 0.9 line. We plan to
>> make
>>>>>>> additional maintenance releases as new fixes come in.
>>>>>>>=20
>>>>>>> Best,
>>>>>>> Xiangrui
>>>>>=20
>>>>>=20
>>=20


From dev-return-8443-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 19:51:24 2014
Return-Path: <dev-return-8443-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 558A211E52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 19:51:24 +0000 (UTC)
Received: (qmail 56008 invoked by uid 500); 18 Jul 2014 19:51:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55950 invoked by uid 500); 18 Jul 2014 19:51:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55839 invoked by uid 99); 18 Jul 2014 19:51:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 19:51:22 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of meisam.fathi@gmail.com designates 209.85.213.48 as permitted sender)
Received: from [209.85.213.48] (HELO mail-yh0-f48.google.com) (209.85.213.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 19:51:21 +0000
Received: by mail-yh0-f48.google.com with SMTP id i57so2540710yha.7
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 12:50:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9mrxnXfyQf1eyVdsZgJmQZuT4epNuJH9Vl1YP8QMeO0=;
        b=eeE5Oz6naMwnA7/XfoUZXqCBFjNVkV8F1BOpm15CDpjLKBcUqmkbSUQtFTTjIpD8VU
         CUHyvsbzhAssl5M/MffdF1SQvFKZCrNHFi7ETwXDERNfvdDd5lBGz7KVFg1tHQ4kSjoQ
         mEHMnP3twCUuJUR9eXxj/VXtUebvK3xehd+RuJFV4aRs+pxyAHG/aiPO9tkr9ZROYgQS
         vuVmfoMCM6ZIczSi9FC99mKLIFMzaGQ+z8Qu3TGiY4ibqLJzAzT75mBPdW2CXCCXcbsx
         xrnhsuY/h6UGXZ1Coq4SlxgikiZHwuIHvUi+NYx/BVCy7EaOVASeoepCjXCXYTwe1pgE
         rGOQ==
MIME-Version: 1.0
X-Received: by 10.236.201.226 with SMTP id b62mr10429184yho.56.1405713056507;
 Fri, 18 Jul 2014 12:50:56 -0700 (PDT)
Received: by 10.170.65.133 with HTTP; Fri, 18 Jul 2014 12:50:56 -0700 (PDT)
In-Reply-To: <258A3B11-C4E8-409B-A525-C8FF6387AED4@gmail.com>
References: <CAByMnGu5rHP+rUXFGqHvXQ9=4_YFZggcQR5MQhrhn3bvLPvm1g@mail.gmail.com>
	<258A3B11-C4E8-409B-A525-C8FF6387AED4@gmail.com>
Date: Fri, 18 Jul 2014 15:50:56 -0400
Message-ID: <CAByMnGukWty2OgC7XZNwZsSWKCO9DJV79MAmHAH4EWRPecb5Dg@mail.gmail.com>
Subject: Re: Building Spark against Scala 2.10.1 virtualized
From: Meisam Fathi <meisam.fathi@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry for resurrecting this thread but project/SparkBuild.scala is
completely rewritten recently (after this commit
https://github.com/apache/spark/tree/628932b). Should library
dependencies be defined in pox.xml files after this commit?

Thanks
Meisam

On Thu, Jun 5, 2014 at 4:51 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> You can modify project/SparkBuild.scala and build Spark with sbt instead of Maven.
>
>
> On Jun 5, 2014, at 12:36 PM, Meisam Fathi <meisam.fathi@gmail.com> wrote:
>
>> Hi community,
>>
>> How should I change sbt to compile spark core with a different version
>> of Scala? I see maven pom files define dependencies to scala 2.10.4. I
>> need to override/ignore the maven dependencies and use Scala
>> virtualized, which needs these lines in a build.sbt file:
>>
>> scalaOrganization := "org.scala-lang.virtualized"
>>
>> scalaVersion := "2.10.1"
>>
>> libraryDependencies += "EPFL" %% "lms" % "0.3-SNAPSHOT"
>>
>> scalacOptions += "-Yvirtualize"
>>
>>
>> Thanks,
>> Meisam
>

From dev-return-8444-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 20:38:34 2014
Return-Path: <dev-return-8444-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BD07311228
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 20:38:34 +0000 (UTC)
Received: (qmail 9365 invoked by uid 500); 18 Jul 2014 20:38:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9318 invoked by uid 500); 18 Jul 2014 20:38:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9306 invoked by uid 99); 18 Jul 2014 20:38:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 20:38:33 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 20:38:31 +0000
Received: by mail-qc0-f181.google.com with SMTP id w7so3817430qcr.12
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 13:38:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=7QGz4cNSTBaOzZllfSsX2izBMw2yb4CcFsJXyKQXtBI=;
        b=Fnn7GpbO1IlsmllS8KfNCj0ikhRRgiREGbDF3ytWUsKtS1AwHkmfwkZcdzAQA49r/I
         bXbNpa6rFvIHVF6k7h72izKYh1lAI9ZguwYviMjtMag4gKqDD5/oVOJBYbagNWZkoAHL
         NSmRqOKeRkfMqD0jao/8bCiDYjTRaOf4AhuyYKbdlU7jaUjmVEgLY49tjaXXIpq9yyVK
         jZvCucrIdc3VCGaQD4YpWXTTIz9//qObSm053j4a2FsSisSi5dXPnEVmnc12Y7nOHfwz
         NL+RPjLnej4XTktqZfulcJHRKfJzpL4Ka5SNlz96YK65PCbBejAHnZxuMJ8t9Ygru3H7
         UXUA==
MIME-Version: 1.0
X-Received: by 10.140.92.235 with SMTP id b98mr11224086qge.97.1405715887077;
 Fri, 18 Jul 2014 13:38:07 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Fri, 18 Jul 2014 13:38:07 -0700 (PDT)
Date: Fri, 18 Jul 2014 13:38:07 -0700
Message-ID: <CA+B-+fz88+KnypqBED_T-Oxg-PgqvXHUScAmGHCeuQBq+NR97g@mail.gmail.com>
Subject: OWLQN
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a51b41ac2fb04fe7dbc6d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a51b41ac2fb04fe7dbc6d
Content-Type: text/plain; charset=UTF-8

Hi,

I thought OWLQN is already merged to mllib optimization but I don't see it
in the master yet...

Are there any issues in merging it in ? I see there are some merge
conflicts right now...

https://github.com/apache/spark/pull/840/

Thanks.
Deb

--001a113a51b41ac2fb04fe7dbc6d--

From dev-return-8445-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 20:40:47 2014
Return-Path: <dev-return-8445-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8256B11239
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 20:40:47 +0000 (UTC)
Received: (qmail 14654 invoked by uid 500); 18 Jul 2014 20:40:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14601 invoked by uid 500); 18 Jul 2014 20:40:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14589 invoked by uid 99); 18 Jul 2014 20:40:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 20:40:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.179 as permitted sender)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 20:40:41 +0000
Received: by mail-vc0-f179.google.com with SMTP id hq11so6893022vcb.10
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 13:40:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=oVRjFdbpV9x21SbCFhUw1KUhr5BganJN28TKPk1E6IE=;
        b=cgztIHTCITZ0MvFtEIGFG0vaAvsHRJpu3NaUBu7jViMVC2TDo9tpSLfNyHwD1w5FFY
         tNqyZXhvF8CC+HwTVOKrXJeiUJvkYEwmDFNSjibwlN06d5/W7ITgU2cGm28jdXNAJVZm
         R25028ir68LXXbCkUHy7wCr7M/1cmDbJnIZTSFIAHtfa91YAt1jxTAksGFZBJqaC/yMv
         xdaQe5G3SNlxR/gp6OiX1yKlpfJtSprO6KTmIdC2OqYa0hmYdS6dnVCoy7LFP8j2Jeri
         XLABTcshw1M5WZyVhli3P/ccylzEehlhLSLcb9OgFGnZ56FWYQVj2xDbpkdP0T7u9xch
         P+4w==
MIME-Version: 1.0
X-Received: by 10.52.106.162 with SMTP id gv2mr7867789vdb.47.1405716021047;
 Fri, 18 Jul 2014 13:40:21 -0700 (PDT)
Received: by 10.58.234.41 with HTTP; Fri, 18 Jul 2014 13:40:20 -0700 (PDT)
In-Reply-To: <CAMAsSdL_u8f-TE66KBu3t4k8YOS61=hoVW6ZQsFkTVqWYzg6XQ@mail.gmail.com>
References: <CACkSZy3jeGykv1b493ja2ZiUvJJ+xC0HnzvmF44OQS651xp4DQ@mail.gmail.com>
	<CAMAsSdL_u8f-TE66KBu3t4k8YOS61=hoVW6ZQsFkTVqWYzg6XQ@mail.gmail.com>
Date: Fri, 18 Jul 2014 13:40:20 -0700
Message-ID: <CACkSZy3dEC6hM=VsOxFLt34R_W90JxO93s=YPvbizn_s8pDHAg@mail.gmail.com>
Subject: Re: Current way to include hive in a build
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec548a9bd1745cd04fe7dc46d
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec548a9bd1745cd04fe7dc46d
Content-Type: text/plain; charset=UTF-8

Thanks v much Patrick and Sean.  I have the build working now as follows:

 mvn -Pyarn -Pcdh5 -Phive -DskipTests clean package

in Addition, I am in the midst of running some tests and so far so good.


The pom.xml changes:

Added to main/parent directory pom.xml:

    <profile>
      <id>cdh5</id>
      <properties>
        <hadoop.version>2.3.0-cdh5.0.0</hadoop.version>
        <yarn.version>2.3.0-cdh5.0.0</yarn.version>
        <zookeeper.version>3.4.5-cdh5.0.0</zookeeper.version>
        <protobuf.version>2.5.0</protobuf.version>
        <jets3t.version>0.9.0</jets3t.version>
        <hbase.version>0.96.1.1-cdh5.0.0</hbase.version>
      </properties>
    </profile>

Added four dependencies into  *examples/*pom.xml:

One each for :  (hbase-common, hbase-client, hbase-protocol, hbase-server).
  Here is the one for hbase-common:

    <dependency>
      <groupId>org.apache.hbase</groupId>
      <artifactId>hbase-common</artifactId>
      <version>${hbase.version}</version>
      <exclusions>
        <exclusion>
          <groupId>asm</groupId>
          <artifactId>asm</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.jboss.netty</groupId>
          <artifactId>netty</artifactId>
        </exclusion>
        <exclusion>
          <groupId>io.netty</groupId>
          <artifactId>netty</artifactId>
        </exclusion>
        <exclusion>
          <groupId>commons-logging</groupId>
          <artifactId>commons-logging</artifactId>
        </exclusion>
        <exclusion>
          <groupId>org.jruby</groupId>
          <artifactId>jruby-complete</artifactId>
        </exclusion>
      </exclusions>
    </dependency>

Duplicate the above for:

      <artifactId>hbase-client</artifactId>
..
      <artifactId>hbase-protocol</artifactId>
..
      <artifactId>hbase-server</artifactId>
..



2014-07-18 3:16 GMT-07:00 Sean Owen <sowen@cloudera.com>:

> This build invocation works just as you have it, for me. (At least, it
> gets through Hive; Examples fails for a different unrelated reason.)
>
> commons-logging 1.0.4 exists in Maven for sure. Maybe there is some
> temporary problem accessing Maven's repo?
>
> On Fri, Jul 18, 2014 at 12:00 AM, Stephen Boesch <javadba@gmail.com>
> wrote:
> > Added to pom.xml:
> >
> >    <profile>
> >       <id>cdh5</id>
> >       <activation>
> >         <activeByDefault>false</activeByDefault>
> >       </activation>
> >       <properties>
> >         <hadoop.version>2.3.0-cdh5.0.0</hadoop.version>
> >         <yarn.version>2.3.0-cdh5.0.0</yarn.version>
> >         <hbase.version>0.96.1.1-cdh5.0.0</hbase.version>
> >         <zookeeper.version>3.4.5-cdh5.0.0</zookeeper.version>
> >       </properties>
> >     </profile>
> >
> > *mvn -Pyarn -Pcdh5 -Phive -Dhadoop.version=2.3.0-cdh5.0.1
> > -Dyarn.version=2.3.0-cdh5.0.0 -DskipTests clean package*
> >
> >
> > [INFO]
> > ------------------------------------------------------------------------
> > [INFO] Reactor Summary:
> > [INFO]
> > [INFO] Spark Project Parent POM .......................... SUCCESS
> [3.165s]
> > [INFO] Spark Project Core ................................ SUCCESS
> > [2:39.504s]
> > [INFO] Spark Project Bagel ............................... SUCCESS
> [7.596s]
> > [INFO] Spark Project GraphX .............................. SUCCESS
> [22.027s]
> > [INFO] Spark Project ML Library .......................... SUCCESS
> [36.284s]
> > [INFO] Spark Project Streaming ........................... SUCCESS
> [24.309s]
> > [INFO] Spark Project Tools ............................... SUCCESS
> [3.147s]
> > [INFO] Spark Project Catalyst ............................ SUCCESS
> [20.148s]
> > [INFO] Spark Project SQL ................................. SUCCESS
> [18.560s]
> > *[INFO] Spark Project Hive ................................ FAILURE
> > [33.962s]*
> >
> > [ERROR] Failed to execute goal
> > org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
> > (copy-dependencies) on project spark-hive_2.10: Execution
> copy-dependencies
> > of goal
> > org.apache.maven.plugins:maven-dependency-plugin:2.4:copy-dependencies
> > failed: Plugin org.apache.maven.plugins:maven-dependency-plugin:2.4 or
> one
> > of its dependencies could not be resolved: Could not find artifact
> > commons-logging:commons-logging:jar:1.0.4 -> [Help 1]
> >
> > Anyone who is presently building with -Phive and has a suggestion for
> this?
>

--bcaec548a9bd1745cd04fe7dc46d--

From dev-return-8446-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 20:56:01 2014
Return-Path: <dev-return-8446-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 158DF112BF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 20:56:01 +0000 (UTC)
Received: (qmail 62454 invoked by uid 500); 18 Jul 2014 20:56:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62399 invoked by uid 500); 18 Jul 2014 20:56:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62383 invoked by uid 99); 18 Jul 2014 20:56:00 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 20:56:00 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 20:55:57 +0000
Received: by mail-qa0-f41.google.com with SMTP id j7so3463088qaq.0
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 13:55:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=85zd2xkyDFvMco5ztGxRvJWUPp0wadWKKmA6MdU7LYs=;
        b=EKc1fWH1LRNKTd4eWeYtCAaUh7BAXWr6hkkAvdaARsXqmVwzHRIjEv1mr4JzMjINGO
         3Diq7NFrw5mdtQo2UbR0shyT0He3ZdgJ4GlFHNu0U30vs2PV2B4egE/0CJrvA6B+oajI
         2uD+v/RiiweBb1WUgc87i6+T3G3r4aRT45yhEfsN8UTmTUdVYw7qFfH1y3FY90tPc5U/
         /WRs5tTXo3ZyAwawcbTiQrxN+uDZ8BSzaTxhabINeCUjzOwCAFYoqmz411/JTt9knczd
         IiQY6sbyZTv+FGrX3jr2CGcbPv4EiBpc1W7+XfSTsQmKPjy3pReLZp50eMa3yTFaizGd
         xMAw==
X-Gm-Message-State: ALoCoQna3SffWsZFsjOyE9l0iUKyPOEXdCRqvny85IXqGjKleU/Lfv3aWxJP4leVHUP2Z1l1Ozw4
X-Received: by 10.224.161.129 with SMTP id r1mr11486193qax.86.1405716932862;
 Fri, 18 Jul 2014 13:55:32 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Fri, 18 Jul 2014 13:55:11 -0700 (PDT)
In-Reply-To: <CAByMnGukWty2OgC7XZNwZsSWKCO9DJV79MAmHAH4EWRPecb5Dg@mail.gmail.com>
References: <CAByMnGu5rHP+rUXFGqHvXQ9=4_YFZggcQR5MQhrhn3bvLPvm1g@mail.gmail.com>
 <258A3B11-C4E8-409B-A525-C8FF6387AED4@gmail.com> <CAByMnGukWty2OgC7XZNwZsSWKCO9DJV79MAmHAH4EWRPecb5Dg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 18 Jul 2014 13:55:11 -0700
Message-ID: <CAPh_B=ZE1iSSrkFYvAu5XRL9v-ww2NB=ar0CqBaPz0G5SmNqnQ@mail.gmail.com>
Subject: Re: Building Spark against Scala 2.10.1 virtualized
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01538694703f8504fe7dfa8e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01538694703f8504fe7dfa8e
Content-Type: text/plain; charset=UTF-8

Yes.


On Fri, Jul 18, 2014 at 12:50 PM, Meisam Fathi <meisam.fathi@gmail.com>
wrote:

> Sorry for resurrecting this thread but project/SparkBuild.scala is
> completely rewritten recently (after this commit
> https://github.com/apache/spark/tree/628932b). Should library
> dependencies be defined in pox.xml files after this commit?
>
> Thanks
> Meisam
>
> On Thu, Jun 5, 2014 at 4:51 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > You can modify project/SparkBuild.scala and build Spark with sbt instead
> of Maven.
> >
> >
> > On Jun 5, 2014, at 12:36 PM, Meisam Fathi <meisam.fathi@gmail.com>
> wrote:
> >
> >> Hi community,
> >>
> >> How should I change sbt to compile spark core with a different version
> >> of Scala? I see maven pom files define dependencies to scala 2.10.4. I
> >> need to override/ignore the maven dependencies and use Scala
> >> virtualized, which needs these lines in a build.sbt file:
> >>
> >> scalaOrganization := "org.scala-lang.virtualized"
> >>
> >> scalaVersion := "2.10.1"
> >>
> >> libraryDependencies += "EPFL" %% "lms" % "0.3-SNAPSHOT"
> >>
> >> scalacOptions += "-Yvirtualize"
> >>
> >>
> >> Thanks,
> >> Meisam
> >
>

--089e01538694703f8504fe7dfa8e--

From dev-return-8447-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 21:01:44 2014
Return-Path: <dev-return-8447-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4A25112FE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 21:01:44 +0000 (UTC)
Received: (qmail 86473 invoked by uid 500); 18 Jul 2014 21:01:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86421 invoked by uid 500); 18 Jul 2014 21:01:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86405 invoked by uid 99); 18 Jul 2014 21:01:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 21:01:43 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 21:01:42 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so8540974vcb.13
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 14:01:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=qjAFatJn8LHfKZ0XwIEuexIFjPcAXxRil8L8Vj6v504=;
        b=MlsRa8UW7DwHqUQ9MR6OxsmEV3M6/QWNsHJ9RILEbFKWE3rdPphg9n2l3+Z9hMTLjw
         mdDSK59Zo/KiIRqzCCv67+TB/zZ2nCbijzDmKfNGR2yr8oGUY+Wr4mgfvtQtbYk1NbHp
         Qi+J7ckD7BUUWaO5qdPKTl7ED6jdXFo9lIL3fsouQbxexAmMWI4oavlJUQGs+RwOq6QS
         6LHztcOqhwWYccDnWLY1shdMQOySwG0LOTfGuJPvpl6y70mztudcK/kWgj09VyeWBhoK
         zf4Ge9y+MzYr9z4WG8v2R7mInd1XBF39vx3A1K7RdTVXFFcMCJvpthuX0dAuPCgx50x8
         hhrA==
X-Gm-Message-State: ALoCoQl1S15meSyw+yIV5U10NyWF03Eg3NPwSe3Dy++ztGd7aQd6UaO0KycS00uT/AehUiCgJQGO
MIME-Version: 1.0
X-Received: by 10.221.47.9 with SMTP id uq9mr9644915vcb.48.1405717277178; Fri,
 18 Jul 2014 14:01:17 -0700 (PDT)
Received: by 10.220.1.200 with HTTP; Fri, 18 Jul 2014 14:01:17 -0700 (PDT)
In-Reply-To: <CA+B-+fz88+KnypqBED_T-Oxg-PgqvXHUScAmGHCeuQBq+NR97g@mail.gmail.com>
References: <CA+B-+fz88+KnypqBED_T-Oxg-PgqvXHUScAmGHCeuQBq+NR97g@mail.gmail.com>
Date: Fri, 18 Jul 2014 14:01:17 -0700
Message-ID: <CAEYYnxZnbviJoNRPLnuws18jtNeMONVt4VOFsSSk_y1ZTKcxUw@mail.gmail.com>
Subject: Re: OWLQN
From: DB Tsai <dbtsai@dbtsai.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'm working on it with weighted regularization. The problem is that
OWLQN doesn't work nicely with Updater now since all the L1 logic
should be in OWLQN instead of L1Updater.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai


On Fri, Jul 18, 2014 at 1:38 PM, Debasish Das <debasish.das83@gmail.com> wrote:
> Hi,
>
> I thought OWLQN is already merged to mllib optimization but I don't see it
> in the master yet...
>
> Are there any issues in merging it in ? I see there are some merge
> conflicts right now...
>
> https://github.com/apache/spark/pull/840/
>
> Thanks.
> Deb

From dev-return-8448-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 18 21:42:19 2014
Return-Path: <dev-return-8448-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 120561155F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 18 Jul 2014 21:42:19 +0000 (UTC)
Received: (qmail 6659 invoked by uid 500); 18 Jul 2014 21:42:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6604 invoked by uid 500); 18 Jul 2014 21:42:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6581 invoked by uid 99); 18 Jul 2014 21:42:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 21:42:18 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of nitinpanj@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 18 Jul 2014 21:42:13 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <nitinpanj@gmail.com>)
	id 1X8Fub-0005aR-9P
	for dev@spark.incubator.apache.org; Fri, 18 Jul 2014 14:41:53 -0700
Date: Fri, 18 Jul 2014 14:41:53 -0700 (PDT)
From: npanj <nitinpanj@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1405719713250-7417.post@n3.nabble.com>
Subject: How to set Java options -Xmn
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,
I am trying to set -Xmn  to control GC in spark.executor.extraJavaOptions
(as recommended by tuning guide), but I am getting error that
"spark.executor.extraJavaOptions is not allowed to alter memory settings".
It seems that extraJavaOptions takes just one number, not list of java
options. 

How can I set -Xmn ?



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-set-Java-options-Xmn-tp7417.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8449-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 19 06:48:14 2014
Return-Path: <dev-return-8449-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B116110727
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 19 Jul 2014 06:48:14 +0000 (UTC)
Received: (qmail 2237 invoked by uid 500); 19 Jul 2014 06:48:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2165 invoked by uid 500); 19 Jul 2014 06:48:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2113 invoked by uid 99); 19 Jul 2014 06:48:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 06:48:13 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of freeman.jeremy@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 06:48:12 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <freeman.jeremy@gmail.com>)
	id 1X8OQs-0005cw-V6
	for dev@spark.incubator.apache.org; Fri, 18 Jul 2014 23:47:46 -0700
Date: Fri, 18 Jul 2014 23:47:46 -0700 (PDT)
From: Jeremy Freeman <freeman.jeremy@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1405752466952-7418.post@n3.nabble.com>
In-Reply-To: <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
References: <CADtDQQKdJRPAxS8EJVgJbZOGa97_25GVrJ3tQpKwaOZY+q+HBw@mail.gmail.com> <CAPi87hcaAEg3aBmm4pQNxRvsx2XFooBwmyHM0REHN2xkSzdJMw@mail.gmail.com> <CABjXkq7qRvTGS8Fj5nLDUsaoepvFKa=-0xt=98PjhzzRfPv_xg@mail.gmail.com> <CAPi87hcWapBEFBcZ5q+0eqJmRWgg3wgirAr3=f8--zpjSQCQuA@mail.gmail.com> <CADtDQQLM2jf0p8RZ8WZ7sRXpmFjik1GAs5E1YUQzQuqu_YjHqw@mail.gmail.com> <1404909567855.c2a8fd87@Nodemailer> <CADtDQQK-k53ivmKW3wPK=ZbBnUsG9XMDD3RoeoSV36+OQBMnhg@mail.gmail.com> <1405003674571.192c3983@Nodemailer> <1405643464811-7398.post@n3.nabble.com> <CADtDQQLkkrnOd9JOBcGYv092=qCR3TkKcE+-8Mp36SvdkZ_GKQ@mail.gmail.com>
Subject: Re: Contributing to MLlib: Proposal for Clustering Algorithms
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi RJ, that sounds like a great idea. I'd be happy to look over what you put
together.

-- Jeremy



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Contributing-to-MLlib-Proposal-for-Clustering-Algorithms-tp7212p7418.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8450-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 19 06:59:33 2014
Return-Path: <dev-return-8450-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1B5D910762
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 19 Jul 2014 06:59:33 +0000 (UTC)
Received: (qmail 13798 invoked by uid 500); 19 Jul 2014 06:59:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13739 invoked by uid 500); 19 Jul 2014 06:59:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 13728 invoked by uid 99); 19 Jul 2014 06:59:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 06:59:32 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.175] (HELO mail-qc0-f175.google.com) (209.85.216.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 06:59:26 +0000
Received: by mail-qc0-f175.google.com with SMTP id w7so4173433qcr.34
        for <dev@spark.apache.org>; Fri, 18 Jul 2014 23:59:05 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=PVO74DOnsBUshohgwViDYX1HQLJa6Rwv4I9hHsKKGvI=;
        b=fVoITT7KW+MFZtYIWQi7tjciuxdDq5OtKOToW6rfUha5Cev2QtF1ONKHsZNV8Ygndq
         RN7uvUSVA01oAbRsQJVsPuE5w/DL1TsrEBJOd6lFovahQL2dK19xWIs4swiGpK0oddgd
         YFmFMLsbF/FLC5mtuNl3I1CxkD6b36ttVciUgI1PXwcPROSvFDF4XS9S4UUR6GgybMUE
         ACmhKE7ZvWtPxMxJLGwW3CLXFZCWsPyoMZ7ZMRtq9O46O7LYnIRwhgm+CFLsHIBCY/o2
         nWMmdlgp+lmbh52FdXLKNU4KucK8HpKi9ynYMw5l31f1+uxausxflrVlQeSkpC1MleTi
         pdgA==
X-Gm-Message-State: ALoCoQnUv78hXatu9/S7FXZ3bJy/F1WShzIwgr2klIPHYzqlRY+ajvT0StgvpnmLBc1NG86XWv1f
X-Received: by 10.140.92.235 with SMTP id b98mr14603590qge.97.1405753145306;
 Fri, 18 Jul 2014 23:59:05 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Fri, 18 Jul 2014 23:58:45 -0700 (PDT)
In-Reply-To: <CAOhmDzetC1QYD=4X_TmDyxU2MaHshy=8KHyf1B=W-NjMHHsG_w@mail.gmail.com>
References: <CAPh_B=aKZZ5XuMOJ0V0cSRpPS92k0X5R+DO0fS2J_synFb=Cpg@mail.gmail.com>
 <20140717002332.1b9dc18f@sh9> <CAOhmDzetC1QYD=4X_TmDyxU2MaHshy=8KHyf1B=W-NjMHHsG_w@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 18 Jul 2014 23:58:45 -0700
Message-ID: <CAPh_B=ZE=JT4e=TwD5UuRbCRmPjnQZ6SfA2bnTjX_gVCsFdkEA@mail.gmail.com>
Subject: Re: small (yet major) change going in: broadcasting RDD to reduce
 task size
To: Nicholas Chammas <nicholas.chammas@gmail.com>
Cc: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a51b4de535704fe8668ce
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a51b4de535704fe8668ce
Content-Type: text/plain; charset=UTF-8

Thanks :)

FYI the pull request has been merged and will be part of Spark 1.1.0.



On Thu, Jul 17, 2014 at 11:09 AM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> On Thu, Jul 17, 2014 at 1:23 AM, Stephen Haberman <
> stephen.haberman@gmail.com> wrote:
>
>> I'd be ecstatic if more major changes were this well/succinctly
>> explained
>>
>
> Ditto on that. The summary of user impact was very nice. It would be good
> to repeat that on the user list or release notes when this change goes out.
>
> Nick
>

--001a113a51b4de535704fe8668ce--

From dev-return-8451-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 19 18:02:47 2014
Return-Path: <dev-return-8451-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C3ED911511
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 19 Jul 2014 18:02:47 +0000 (UTC)
Received: (qmail 55237 invoked by uid 500); 19 Jul 2014 18:02:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55185 invoked by uid 500); 19 Jul 2014 18:02:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55173 invoked by uid 99); 19 Jul 2014 18:02:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 18:02:46 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of debasish.das83@gmail.com designates 209.85.192.54 as permitted sender)
Received: from [209.85.192.54] (HELO mail-qg0-f54.google.com) (209.85.192.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 18:02:43 +0000
Received: by mail-qg0-f54.google.com with SMTP id z60so4210171qgd.27
        for <dev@spark.apache.org>; Sat, 19 Jul 2014 11:02:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=e+SdqukvI/9OQv1IKdRcNWKqhSc9lfFjYuV5NhskRZQ=;
        b=G7MOXUd1j1atALDFcV+qgAY140dilVAAAchEfbkX2vqqBX9s+fk41HiOjBjO2KxHxp
         ipzbfbu56Cs2EboAVIuDzbaOzcApgSeoHZUq2mzEPtYMooHVbI5tOPaMZkXujVUIgGae
         D0zbDJIEunJU5x3q69UmikmBWsucYbm4DD4N7w+FlZ69nadOP9SS9NHQIAwzfU5PItg4
         0IlRzNy20lfQhiKWqCzJ/wbdjCOvkPNji89mRDcZoo3BuGeKS5Y0weOqQYwyMkBHSIrd
         el8aZvKWPbsAamYPYWLRk0jFI3u24Fcwz0ole1bOmpH/19AXJ7aMhvK33Fjg/rnhVmPu
         hj9g==
MIME-Version: 1.0
X-Received: by 10.224.79.139 with SMTP id p11mr21725228qak.93.1405792938263;
 Sat, 19 Jul 2014 11:02:18 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sat, 19 Jul 2014 11:02:18 -0700 (PDT)
Date: Sat, 19 Jul 2014 11:02:18 -0700
Message-ID: <CA+B-+fy-ZqhU5bdC5tNW+PU3tdk2gn8Em=pz9C1ZFfWEtde3LQ@mail.gmail.com>
Subject: Master compilation with sbt
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bdc801ab689ba04fe8faccb
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc801ab689ba04fe8faccb
Content-Type: text/plain; charset=UTF-8

Hi,

Is sbt still used for master compilation ? I could compile for
2.3.0-cdh5.0.2 using maven following the instructions from the website:

http://spark.apache.org/docs/latest/building-with-maven.html

But when I am trying to use sbt for local testing and then I am getting
some weird errors...Is sbt still used by developers ? I am using JDK7...

org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element
type "settings" must be followed by either attribute specifications, ">" or
"/>".

at
com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)

at
com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)

at
com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)

at
com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)

at
com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.seekCloseOfStartTag(XMLDocumentFragmentScannerImpl.java:1394)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1327)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$ContentDriver.scanRootElementHook(XMLDocumentScannerImpl.java:1292)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3122)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:880)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)

at
com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)

at
com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)

at
com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)

at
com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)

at
com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)

at
com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)

at
com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333)

at scala.xml.factory.XMLLoader$class.loadXML(XMLLoader.scala:40)

at scala.xml.XML$.loadXML(XML.scala:57)

at scala.xml.factory.XMLLoader$class.load(XMLLoader.scala:52)

at scala.xml.XML$.load(XML.scala:57)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:225)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:224)

at sbt.Using.apply(Using.scala:25)

at com.typesafe.sbt.pom.MavenHelper$.settingsXml(MavenHelper.scala:224)

at
com.typesafe.sbt.pom.MavenHelper$.settingsXmlServers(MavenHelper.scala:245)

at
com.typesafe.sbt.pom.MavenHelper$.createSbtCredentialsFromSettingsXml(MavenHelper.scala:291)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)

at
com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)

at
sbt.Scoped$RichInitialize$$anonfun$map$1$$anonfun$apply$3.apply(Structure.scala:177)

at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)

at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)

at sbt.std.Transform$$anon$4.work(System.scala:64)

at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)

at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)

at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)

at sbt.Execute.work(Execute.scala:244)

at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)

at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)

at
sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)

at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)

at java.util.concurrent.FutureTask.run(FutureTask.java:262)

at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)

at java.util.concurrent.FutureTask.run(FutureTask.java:262)

at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

at java.lang.Thread.run(Thread.java:745)

Thanks.

Deb

--047d7bdc801ab689ba04fe8faccb--

From dev-return-8452-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 19 18:10:50 2014
Return-Path: <dev-return-8452-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E43211520
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 19 Jul 2014 18:10:50 +0000 (UTC)
Received: (qmail 60466 invoked by uid 500); 19 Jul 2014 18:10:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60405 invoked by uid 500); 19 Jul 2014 18:10:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60392 invoked by uid 99); 19 Jul 2014 18:10:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 18:10:48 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.53 as permitted sender)
Received: from [209.85.216.53] (HELO mail-qa0-f53.google.com) (209.85.216.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 18:10:43 +0000
Received: by mail-qa0-f53.google.com with SMTP id v10so3937795qac.26
        for <dev@spark.apache.org>; Sat, 19 Jul 2014 11:10:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=j8163bBcqwzEat8N2LgR9K0FjCHniu9DCSh+2N3Sl30=;
        b=mE0nCz38qGc1G/Zi3YNdpcSGjpBLd+o4lYMZ9D5c3I6d/KKLcX1Q09qE+5poDUX+A+
         RubBc/xL9VV5qvALkUK5xweOp0Atb4ACAKu7xIKVaFphbVVLI2TjABXzaluWABdmtksJ
         CnLdHkw/TTg1KreiQEfX6Qyfo4t+a2m012Ovz+v2dqDo7gsmsZDLc8R9DyQjcNKamQQv
         GNolRmBVd3IH3B+dbkL+KeDgyK9jActp/iERr4qhTdjn61qvGPVTtAUWOzY9D+6whaK0
         IyZn/KQR4qnEhvDwXrV0Z1qnBnlFBZwhJ+/UzVrLbSSMtt+FHhlUur5VJoRMlHudAG14
         smkA==
MIME-Version: 1.0
X-Received: by 10.224.223.135 with SMTP id ik7mr22964073qab.26.1405793422468;
 Sat, 19 Jul 2014 11:10:22 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sat, 19 Jul 2014 11:10:22 -0700 (PDT)
In-Reply-To: <CA+B-+fy-ZqhU5bdC5tNW+PU3tdk2gn8Em=pz9C1ZFfWEtde3LQ@mail.gmail.com>
References: <CA+B-+fy-ZqhU5bdC5tNW+PU3tdk2gn8Em=pz9C1ZFfWEtde3LQ@mail.gmail.com>
Date: Sat, 19 Jul 2014 11:10:22 -0700
Message-ID: <CA+B-+fz9njC91b-0N924WRMyV7JcAfbRLKq5W+Rt_bgiBZumzg@mail.gmail.com>
Subject: Re: Master compilation with sbt
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c22d8692f03f04fe8fc9e1
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c22d8692f03f04fe8fc9e1
Content-Type: text/plain; charset=UTF-8

I am at the reservoir sampling commit:

commit 586e716e47305cd7c2c3ff35c0e828b63ef2f6a8
Author: Reynold Xin <rxin@apache.org>
Date:   Fri Jul 18 12:41:50 2014 -0700

sbt/sbt -Dhttp.nonProxyHosts=132.197.10.21

> project mllib

[info] Set current project to spark-mllib (in build
file:/Users/v606014/spark-master/)

> compile

[trace] Stack trace suppressed: run last mllib/*:credentials for the full
output.

[trace] Stack trace suppressed: run last core/*:credentials for the full
output.

[error] (mllib/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4;
columnNumber: 57; Element type "settings" must be followed by either
attribute specifications, ">" or "/>".

[error] (core/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4;
columnNumber: 57; Element type "settings" must be followed by either
attribute specifications, ">" or "/>".

[error] Total time: 0 s, completed Jul 19, 2014 6:09:24 PM
On Sat, Jul 19, 2014 at 11:02 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> Hi,
>
> Is sbt still used for master compilation ? I could compile for
> 2.3.0-cdh5.0.2 using maven following the instructions from the website:
>
> http://spark.apache.org/docs/latest/building-with-maven.html
>
> But when I am trying to use sbt for local testing and then I am getting
> some weird errors...Is sbt still used by developers ? I am using JDK7...
>
> org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element
> type "settings" must be followed by either attribute specifications, ">" or
> "/>".
>
> at
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
>
> at
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.seekCloseOfStartTag(XMLDocumentFragmentScannerImpl.java:1394)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1327)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$ContentDriver.scanRootElementHook(XMLDocumentScannerImpl.java:1292)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3122)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:880)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
>
> at
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
>
> at
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
>
> at
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
>
> at
> com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
>
> at
> com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)
>
> at
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)
>
> at
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333)
>
> at scala.xml.factory.XMLLoader$class.loadXML(XMLLoader.scala:40)
>
> at scala.xml.XML$.loadXML(XML.scala:57)
>
> at scala.xml.factory.XMLLoader$class.load(XMLLoader.scala:52)
>
> at scala.xml.XML$.load(XML.scala:57)
>
> at
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:225)
>
> at
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:224)
>
> at sbt.Using.apply(Using.scala:25)
>
> at com.typesafe.sbt.pom.MavenHelper$.settingsXml(MavenHelper.scala:224)
>
> at
> com.typesafe.sbt.pom.MavenHelper$.settingsXmlServers(MavenHelper.scala:245)
>
> at
> com.typesafe.sbt.pom.MavenHelper$.createSbtCredentialsFromSettingsXml(MavenHelper.scala:291)
>
> at
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
>
> at
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
>
> at
> sbt.Scoped$RichInitialize$$anonfun$map$1$$anonfun$apply$3.apply(Structure.scala:177)
>
> at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
>
> at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
>
> at sbt.std.Transform$$anon$4.work(System.scala:64)
>
> at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
>
> at sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
>
> at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
>
> at sbt.Execute.work(Execute.scala:244)
>
> at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
>
> at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
>
> at
> sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)
>
> at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)
>
> at java.util.concurrent.FutureTask.run(FutureTask.java:262)
>
> at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
>
> at java.util.concurrent.FutureTask.run(FutureTask.java:262)
>
> at
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>
> at
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>
> at java.lang.Thread.run(Thread.java:745)
>
> Thanks.
>
> Deb
>

--001a11c22d8692f03f04fe8fc9e1--

From dev-return-8453-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 19 19:50:30 2014
Return-Path: <dev-return-8453-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4418F11673
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 19 Jul 2014 19:50:30 +0000 (UTC)
Received: (qmail 61899 invoked by uid 500); 19 Jul 2014 19:50:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61834 invoked by uid 500); 19 Jul 2014 19:50:29 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61635 invoked by uid 99); 19 Jul 2014 19:50:29 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 19:50:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 19:50:23 +0000
Received: by mail-wi0-f176.google.com with SMTP id bs8so2342080wib.9
        for <dev@spark.apache.org>; Sat, 19 Jul 2014 12:50:01 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ixzPWOwedhJiFvX6sEz9yrhIyGzkShEhSR027x8ACfQ=;
        b=PEXJB9xaMThJxV2SPPouPZcvleRmUqqD0IalYh4CTqfKaDbiVZ4LxWao2tLRSAf9/j
         qzEa3Qg/5Gn/ZNjXFxoYy6a9yuL3nIdB6peKR3iV0VoS5nCPeSf978B4ydDcHAXFRsdA
         Ck4UeAjhGbuxzjKp0nKyWBG9CI/1vRjoUwlaWGOc5z4YRxg2Oqki0MH/72eh8AI3qTs5
         Cy87beqoTlJxxIJHuIeKOxj1QwncUWAdEzkWFKTHz5bm+5tuelF8lTxhwtHPhfksgppF
         r5DWB16SqgcBMGEYRioXcYxrEUQYTYCiKfNjQBXg5Susm/URutzNbp+rayPyQx53Rh49
         pH2g==
X-Gm-Message-State: ALoCoQnJKW/BPP8uej+Suky/Tb0mN6JtJId4DT4HXzM43ZinvmABc+x3rBEUROLl6zY6XzpihRj3
MIME-Version: 1.0
X-Received: by 10.180.211.101 with SMTP id nb5mr43927821wic.53.1405799401468;
 Sat, 19 Jul 2014 12:50:01 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Sat, 19 Jul 2014 12:50:01 -0700 (PDT)
In-Reply-To: <CA+B-+fz9njC91b-0N924WRMyV7JcAfbRLKq5W+Rt_bgiBZumzg@mail.gmail.com>
References: <CA+B-+fy-ZqhU5bdC5tNW+PU3tdk2gn8Em=pz9C1ZFfWEtde3LQ@mail.gmail.com>
	<CA+B-+fz9njC91b-0N924WRMyV7JcAfbRLKq5W+Rt_bgiBZumzg@mail.gmail.com>
Date: Sat, 19 Jul 2014 12:50:01 -0700
Message-ID: <CAAsvFPm4BTDe85JwuvVuTkJNrAYk+K9PnXGXUApZmDyJ8k4+WA@mail.gmail.com>
Subject: Re: Master compilation with sbt
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c37cc6f3505b04fe912d6f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c37cc6f3505b04fe912d6f
Content-Type: text/plain; charset=UTF-8

> project mllib
.
.
.
> clean
.
.
.
> compile
.
.
.
>test

...all works fine for me @2a732110d46712c535b75dd4f5a73761b6463aa8


On Sat, Jul 19, 2014 at 11:10 AM, Debasish Das <debasish.das83@gmail.com>
wrote:

> I am at the reservoir sampling commit:
>
> commit 586e716e47305cd7c2c3ff35c0e828b63ef2f6a8
> Author: Reynold Xin <rxin@apache.org>
> Date:   Fri Jul 18 12:41:50 2014 -0700
>
> sbt/sbt -Dhttp.nonProxyHosts=132.197.10.21
>
> > project mllib
>
> [info] Set current project to spark-mllib (in build
> file:/Users/v606014/spark-master/)
>
> > compile
>
> [trace] Stack trace suppressed: run last mllib/*:credentials for the full
> output.
>
> [trace] Stack trace suppressed: run last core/*:credentials for the full
> output.
>
> [error] (mllib/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4;
> columnNumber: 57; Element type "settings" must be followed by either
> attribute specifications, ">" or "/>".
>
> [error] (core/*:credentials) org.xml.sax.SAXParseException; lineNumber: 4;
> columnNumber: 57; Element type "settings" must be followed by either
> attribute specifications, ">" or "/>".
>
> [error] Total time: 0 s, completed Jul 19, 2014 6:09:24 PM
> On Sat, Jul 19, 2014 at 11:02 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > Hi,
> >
> > Is sbt still used for master compilation ? I could compile for
> > 2.3.0-cdh5.0.2 using maven following the instructions from the website:
> >
> > http://spark.apache.org/docs/latest/building-with-maven.html
> >
> > But when I am trying to use sbt for local testing and then I am getting
> > some weird errors...Is sbt still used by developers ? I am using JDK7...
> >
> > org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element
> > type "settings" must be followed by either attribute specifications, ">"
> or
> > "/>".
> >
> > at
> >
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.seekCloseOfStartTag(XMLDocumentFragmentScannerImpl.java:1394)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1327)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$ContentDriver.scanRootElementHook(XMLDocumentScannerImpl.java:1292)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3122)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:880)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)
> >
> > at
> >
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333)
> >
> > at scala.xml.factory.XMLLoader$class.loadXML(XMLLoader.scala:40)
> >
> > at scala.xml.XML$.loadXML(XML.scala:57)
> >
> > at scala.xml.factory.XMLLoader$class.load(XMLLoader.scala:52)
> >
> > at scala.xml.XML$.load(XML.scala:57)
> >
> > at
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:225)
> >
> > at
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:224)
> >
> > at sbt.Using.apply(Using.scala:25)
> >
> > at com.typesafe.sbt.pom.MavenHelper$.settingsXml(MavenHelper.scala:224)
> >
> > at
> >
> com.typesafe.sbt.pom.MavenHelper$.settingsXmlServers(MavenHelper.scala:245)
> >
> > at
> >
> com.typesafe.sbt.pom.MavenHelper$.createSbtCredentialsFromSettingsXml(MavenHelper.scala:291)
> >
> > at
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
> >
> > at
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
> >
> > at
> >
> sbt.Scoped$RichInitialize$$anonfun$map$1$$anonfun$apply$3.apply(Structure.scala:177)
> >
> > at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
> >
> > at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
> >
> > at sbt.std.Transform$$anon$4.work(System.scala:64)
> >
> > at
> sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
> >
> > at
> sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
> >
> > at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
> >
> > at sbt.Execute.work(Execute.scala:244)
> >
> > at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
> >
> > at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
> >
> > at
> >
> sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)
> >
> > at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)
> >
> > at java.util.concurrent.FutureTask.run(FutureTask.java:262)
> >
> > at
> java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
> >
> > at java.util.concurrent.FutureTask.run(FutureTask.java:262)
> >
> > at
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >
> > at
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >
> > at java.lang.Thread.run(Thread.java:745)
> >
> > Thanks.
> >
> > Deb
> >
>

--001a11c37cc6f3505b04fe912d6f--

From dev-return-8454-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 19 20:24:57 2014
Return-Path: <dev-return-8454-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D4ED2116FA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 19 Jul 2014 20:24:57 +0000 (UTC)
Received: (qmail 97575 invoked by uid 500); 19 Jul 2014 20:24:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 97516 invoked by uid 500); 19 Jul 2014 20:24:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 97502 invoked by uid 99); 19 Jul 2014 20:24:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 20:24:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of chester@alpinenow.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 19 Jul 2014 20:24:52 +0000
Received: by mail-wi0-f175.google.com with SMTP id ho1so2345061wib.2
        for <dev@spark.apache.org>; Sat, 19 Jul 2014 13:24:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=cCqs8+cpHfcWoSseOEe3ILJIhXdd4YHu5aQfwD1RvuU=;
        b=fQ29Urd47tcb//rdwHnJQj0kniYeRIf4pHTY47kT5aLPpOzLjd5fobfJFAwNKIquVE
         bNzS5JD1IYTM5NU8ZJdnwYRscevTh7GzATXET64rQoY777R1aq68IjD0z7z8FIrcIrr0
         I2AiCz4zgdRqU2ty9ayD3iAZA4ZTTz11m8bsbWiwGkwxLXxSDkSFikLm1QAFuyPVEbuR
         h5dMB6Kfs14OvJl1wR7kDnhMl1kk3VqzKfvinSv1BSIWZTMBg7Yq7YYm542quB5ZtDOO
         Fddt3fuCbqhAGZcCCtQiiiYvbYdiRMgGZ80zYUkITFnU05+lUaBK8/6sgQ6tZ7sVVNw9
         /J6g==
X-Gm-Message-State: ALoCoQltRSEdZMrZq0MMWUXnB/y2tq2oIZJdvTo8VLV/vcWEJ4zKKFmFeF3P/Ll9j7RKE+gngIze
MIME-Version: 1.0
X-Received: by 10.194.71.132 with SMTP id v4mr7766962wju.102.1405801471287;
 Sat, 19 Jul 2014 13:24:31 -0700 (PDT)
Received: by 10.194.14.34 with HTTP; Sat, 19 Jul 2014 13:24:31 -0700 (PDT)
In-Reply-To: <CAAsvFPm4BTDe85JwuvVuTkJNrAYk+K9PnXGXUApZmDyJ8k4+WA@mail.gmail.com>
References: <CA+B-+fy-ZqhU5bdC5tNW+PU3tdk2gn8Em=pz9C1ZFfWEtde3LQ@mail.gmail.com>
	<CA+B-+fz9njC91b-0N924WRMyV7JcAfbRLKq5W+Rt_bgiBZumzg@mail.gmail.com>
	<CAAsvFPm4BTDe85JwuvVuTkJNrAYk+K9PnXGXUApZmDyJ8k4+WA@mail.gmail.com>
Date: Sat, 19 Jul 2014 13:24:31 -0700
Message-ID: <CAPYnQ0VDM_-bMHdr5K=aO6BXfNkpFTvMOFPGw_0+yD4FT6hcwQ@mail.gmail.com>
Subject: Re: Master compilation with sbt
From: Chester Chen <chester@alpinenow.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bfd0d4252486704fe91a945
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0d4252486704fe91a945
Content-Type: text/plain; charset=UTF-8

Works for me as well:


git branch

  branch-0.9

  branch-1.0

* master

Chesters-MacBook-Pro:spark chester$ git pull --rebase

remote: Counting objects: 578, done.

remote: Compressing objects: 100% (369/369), done.

remote: Total 578 (delta 122), reused 418 (delta 71)

Receiving objects: 100% (578/578), 432.42 KiB | 354.00 KiB/s, done.

Resolving deltas: 100% (122/122), done.

>From https://github.com/apache/spark

   9c24974..2a73211  master     -> origin/master

   8e5604b..c93f4a0  branch-0.9 -> origin/branch-0.9

   0b0b895..7611840  branch-1.0 -> origin/branch-1.0

>From https://github.com/apache/spark

 * [new tag]         v0.9.2-rc1 -> v0.9.2-rc1

First, rewinding head to replay your work on top of it...

Fast-forwarded master to 2a732110d46712c535b75dd4f5a73761b6463aa8.


Chesters-MacBook-Pro:spark chester$ sbt/sbt package

....

[info] Done packaging.

[success] Total time: 146 s, completed Jul 19, 2014 1:08:52 PM





On Sat, Jul 19, 2014 at 12:50 PM, Mark Hamstra <mark@clearstorydata.com>
wrote:

> > project mllib
> .
> .
> .
> > clean
> .
> .
> .
> > compile
> .
> .
> .
> >test
>
> ...all works fine for me @2a732110d46712c535b75dd4f5a73761b6463aa8
>
>
> On Sat, Jul 19, 2014 at 11:10 AM, Debasish Das <debasish.das83@gmail.com>
> wrote:
>
> > I am at the reservoir sampling commit:
> >
> > commit 586e716e47305cd7c2c3ff35c0e828b63ef2f6a8
> > Author: Reynold Xin <rxin@apache.org>
> > Date:   Fri Jul 18 12:41:50 2014 -0700
> >
> > sbt/sbt -Dhttp.nonProxyHosts=132.197.10.21
> >
> > > project mllib
> >
> > [info] Set current project to spark-mllib (in build
> > file:/Users/v606014/spark-master/)
> >
> > > compile
> >
> > [trace] Stack trace suppressed: run last mllib/*:credentials for the full
> > output.
> >
> > [trace] Stack trace suppressed: run last core/*:credentials for the full
> > output.
> >
> > [error] (mllib/*:credentials) org.xml.sax.SAXParseException; lineNumber:
> 4;
> > columnNumber: 57; Element type "settings" must be followed by either
> > attribute specifications, ">" or "/>".
> >
> > [error] (core/*:credentials) org.xml.sax.SAXParseException; lineNumber:
> 4;
> > columnNumber: 57; Element type "settings" must be followed by either
> > attribute specifications, ">" or "/>".
> >
> > [error] Total time: 0 s, completed Jul 19, 2014 6:09:24 PM
> > On Sat, Jul 19, 2014 at 11:02 AM, Debasish Das <debasish.das83@gmail.com
> >
> > wrote:
> >
> > > Hi,
> > >
> > > Is sbt still used for master compilation ? I could compile for
> > > 2.3.0-cdh5.0.2 using maven following the instructions from the website:
> > >
> > > http://spark.apache.org/docs/latest/building-with-maven.html
> > >
> > > But when I am trying to use sbt for local testing and then I am getting
> > > some weird errors...Is sbt still used by developers ? I am using
> JDK7...
> > >
> > > org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57; Element
> > > type "settings" must be followed by either attribute specifications,
> ">"
> > or
> > > "/>".
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.seekCloseOfStartTag(XMLDocumentFragmentScannerImpl.java:1394)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1327)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$ContentDriver.scanRootElementHook(XMLDocumentScannerImpl.java:1292)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3122)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:880)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)
> > >
> > > at
> > >
> >
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333)
> > >
> > > at scala.xml.factory.XMLLoader$class.loadXML(XMLLoader.scala:40)
> > >
> > > at scala.xml.XML$.loadXML(XML.scala:57)
> > >
> > > at scala.xml.factory.XMLLoader$class.load(XMLLoader.scala:52)
> > >
> > > at scala.xml.XML$.load(XML.scala:57)
> > >
> > > at
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:225)
> > >
> > > at
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:224)
> > >
> > > at sbt.Using.apply(Using.scala:25)
> > >
> > > at com.typesafe.sbt.pom.MavenHelper$.settingsXml(MavenHelper.scala:224)
> > >
> > > at
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$.settingsXmlServers(MavenHelper.scala:245)
> > >
> > > at
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$.createSbtCredentialsFromSettingsXml(MavenHelper.scala:291)
> > >
> > > at
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
> > >
> > > at
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
> > >
> > > at
> > >
> >
> sbt.Scoped$RichInitialize$$anonfun$map$1$$anonfun$apply$3.apply(Structure.scala:177)
> > >
> > > at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
> > >
> > > at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
> > >
> > > at sbt.std.Transform$$anon$4.work(System.scala:64)
> > >
> > > at
> > sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
> > >
> > > at
> > sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
> > >
> > > at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
> > >
> > > at sbt.Execute.work(Execute.scala:244)
> > >
> > > at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
> > >
> > > at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
> > >
> > > at
> > >
> >
> sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)
> > >
> > > at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)
> > >
> > > at java.util.concurrent.FutureTask.run(FutureTask.java:262)
> > >
> > > at
> > java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
> > >
> > > at java.util.concurrent.FutureTask.run(FutureTask.java:262)
> > >
> > > at
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > >
> > > at
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > >
> > > at java.lang.Thread.run(Thread.java:745)
> > >
> > > Thanks.
> > >
> > > Deb
> > >
> >
>

--047d7bfd0d4252486704fe91a945--

From dev-return-8455-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 20 03:10:33 2014
Return-Path: <dev-return-8455-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A0D5611BB3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Jul 2014 03:10:33 +0000 (UTC)
Received: (qmail 51312 invoked by uid 500); 20 Jul 2014 03:10:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51248 invoked by uid 500); 20 Jul 2014 03:10:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51236 invoked by uid 99); 20 Jul 2014 03:10:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 03:10:32 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.49 as permitted sender)
Received: from [209.85.218.49] (HELO mail-oi0-f49.google.com) (209.85.218.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 03:10:26 +0000
Received: by mail-oi0-f49.google.com with SMTP id u20so2572881oif.8
        for <dev@spark.apache.org>; Sat, 19 Jul 2014 20:10:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=AKWfe8qkMDz/FklWNdm176Ukiq36SLyeronjrvYL8e8=;
        b=nSFdrM6fK+Ar8Qcr0XAq2T67elSYvdfwK0ubhNCEvn2FYjS/4LneH5YAuVbnFf25x8
         aQcFWcvAfSJoBKK1PCG4BqJu0bjCaH4g1IWFxuGnFuq4THpqoOsufCFzmAflEJVoIVVT
         yQN5UF54XOzYXWOILm3di+rEjKdLeAKScm11Adonp/gy70lXqrEhr7E4YJ1ADo3/uQyf
         PBHt55+oYLPK5niR6QKHsuIfFyRd2rltjs0+ZF6qlqiUij4NcuTVURGBWkQxRyOmKu8e
         RBZd72fquUw8a9sAnGbIJpC92LHafIoapx22c51tf+f8Zq5BRI+syfBK3Rt+0uaS9cgA
         JrOA==
MIME-Version: 1.0
X-Received: by 10.182.112.134 with SMTP id iq6mr22212821obb.34.1405825806374;
 Sat, 19 Jul 2014 20:10:06 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sat, 19 Jul 2014 20:10:06 -0700 (PDT)
Date: Sat, 19 Jul 2014 20:10:06 -0700
Message-ID: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
Subject: Pull requests will be automatically linked to JIRA when submitted
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Just a small note, today I committed a tool that will automatically
mirror pull requests to JIRA issues, so contributors will no longer
have to manually post a pull request on the JIRA when they make one.

It will create a "link" on the JIRA and also make a comment to trigger
an e-mail to people watching.

This should make some things easier, such as avoiding accidental
duplicate effort on the same JIRA.

- Patrick

From dev-return-8456-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 20 07:11:06 2014
Return-Path: <dev-return-8456-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 250A511EAE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Jul 2014 07:11:06 +0000 (UTC)
Received: (qmail 21933 invoked by uid 500); 20 Jul 2014 07:11:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21869 invoked by uid 500); 20 Jul 2014 07:11:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21849 invoked by uid 99); 20 Jul 2014 07:11:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 07:11:04 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of debasish.das83@gmail.com designates 209.85.216.181 as permitted sender)
Received: from [209.85.216.181] (HELO mail-qc0-f181.google.com) (209.85.216.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 07:11:00 +0000
Received: by mail-qc0-f181.google.com with SMTP id w7so4695328qcr.12
        for <dev@spark.apache.org>; Sun, 20 Jul 2014 00:10:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=+58gnJFr+3WLpwYi5Ky0Mkkca+vW4fpu6nWq3c0Is3o=;
        b=zENSu+fM5EDwtGt4GOiApfBr1TnsgRaf/6TLSKk7j/XewIs0jdXl79v+M7OAdX5f0J
         UGOd3+nRqx5QbWKBdDQHFLIMx/DMOXJCN1XhvWiktSbMPYAk4OQeK6+VIQgb/Ck/9UBB
         iIyIR4MPNpaFJ+mKzqWkUzT4lFW2ze9plXcZtF6L7vaNtxtjoI4tOWXLjEPSQ4qFuPNe
         OKc9KWM4LwyOErRQKHMdcBzupZ9iJsIxgdMLcV4hYgvFLs2LjYKmrK9p4lRHWPoNMNPo
         ZPdPMvgY2O0cCwEUL1BHSL1YWEeFTmfd3aWJfHTitm4aFXwsNDqEORnl5Leom83qwFSA
         IYNw==
MIME-Version: 1.0
X-Received: by 10.140.93.161 with SMTP id d30mr24534006qge.53.1405840239472;
 Sun, 20 Jul 2014 00:10:39 -0700 (PDT)
Received: by 10.140.85.149 with HTTP; Sun, 20 Jul 2014 00:10:39 -0700 (PDT)
In-Reply-To: <CAPYnQ0VDM_-bMHdr5K=aO6BXfNkpFTvMOFPGw_0+yD4FT6hcwQ@mail.gmail.com>
References: <CA+B-+fy-ZqhU5bdC5tNW+PU3tdk2gn8Em=pz9C1ZFfWEtde3LQ@mail.gmail.com>
	<CA+B-+fz9njC91b-0N924WRMyV7JcAfbRLKq5W+Rt_bgiBZumzg@mail.gmail.com>
	<CAAsvFPm4BTDe85JwuvVuTkJNrAYk+K9PnXGXUApZmDyJ8k4+WA@mail.gmail.com>
	<CAPYnQ0VDM_-bMHdr5K=aO6BXfNkpFTvMOFPGw_0+yD4FT6hcwQ@mail.gmail.com>
Date: Sun, 20 Jul 2014 00:10:39 -0700
Message-ID: <CA+B-+fw-ao-HAGN93tSahoTNQuAbBT6XXDXGEM6buS=L+c6Jtw@mail.gmail.com>
Subject: Re: Master compilation with sbt
From: Debasish Das <debasish.das83@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113b977615b71004fe9ab002
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113b977615b71004fe9ab002
Content-Type: text/plain; charset=UTF-8

I figured out the cause...brew is updated to scala 2.11 and I got the
latest scala version...

Once I reverted back to 2.10.4, HEAD and 1.0.1 tag compiles fine...


On Sat, Jul 19, 2014 at 1:24 PM, Chester Chen <chester@alpinenow.com> wrote:

> Works for me as well:
>
>
> git branch
>
>   branch-0.9
>
>   branch-1.0
>
> * master
>
> Chesters-MacBook-Pro:spark chester$ git pull --rebase
>
> remote: Counting objects: 578, done.
>
> remote: Compressing objects: 100% (369/369), done.
>
> remote: Total 578 (delta 122), reused 418 (delta 71)
>
> Receiving objects: 100% (578/578), 432.42 KiB | 354.00 KiB/s, done.
>
> Resolving deltas: 100% (122/122), done.
>
> From https://github.com/apache/spark
>
>    9c24974..2a73211  master     -> origin/master
>
>    8e5604b..c93f4a0  branch-0.9 -> origin/branch-0.9
>
>    0b0b895..7611840  branch-1.0 -> origin/branch-1.0
>
> From https://github.com/apache/spark
>
>  * [new tag]         v0.9.2-rc1 -> v0.9.2-rc1
>
> First, rewinding head to replay your work on top of it...
>
> Fast-forwarded master to 2a732110d46712c535b75dd4f5a73761b6463aa8.
>
>
> Chesters-MacBook-Pro:spark chester$ sbt/sbt package
>
> ....
>
> [info] Done packaging.
>
> [success] Total time: 146 s, completed Jul 19, 2014 1:08:52 PM
>
>
>
>
>
> On Sat, Jul 19, 2014 at 12:50 PM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
>
> > > project mllib
> > .
> > .
> > .
> > > clean
> > .
> > .
> > .
> > > compile
> > .
> > .
> > .
> > >test
> >
> > ...all works fine for me @2a732110d46712c535b75dd4f5a73761b6463aa8
> >
> >
> > On Sat, Jul 19, 2014 at 11:10 AM, Debasish Das <debasish.das83@gmail.com
> >
> > wrote:
> >
> > > I am at the reservoir sampling commit:
> > >
> > > commit 586e716e47305cd7c2c3ff35c0e828b63ef2f6a8
> > > Author: Reynold Xin <rxin@apache.org>
> > > Date:   Fri Jul 18 12:41:50 2014 -0700
> > >
> > > sbt/sbt -Dhttp.nonProxyHosts=132.197.10.21
> > >
> > > > project mllib
> > >
> > > [info] Set current project to spark-mllib (in build
> > > file:/Users/v606014/spark-master/)
> > >
> > > > compile
> > >
> > > [trace] Stack trace suppressed: run last mllib/*:credentials for the
> full
> > > output.
> > >
> > > [trace] Stack trace suppressed: run last core/*:credentials for the
> full
> > > output.
> > >
> > > [error] (mllib/*:credentials) org.xml.sax.SAXParseException;
> lineNumber:
> > 4;
> > > columnNumber: 57; Element type "settings" must be followed by either
> > > attribute specifications, ">" or "/>".
> > >
> > > [error] (core/*:credentials) org.xml.sax.SAXParseException; lineNumber:
> > 4;
> > > columnNumber: 57; Element type "settings" must be followed by either
> > > attribute specifications, ">" or "/>".
> > >
> > > [error] Total time: 0 s, completed Jul 19, 2014 6:09:24 PM
> > > On Sat, Jul 19, 2014 at 11:02 AM, Debasish Das <
> debasish.das83@gmail.com
> > >
> > > wrote:
> > >
> > > > Hi,
> > > >
> > > > Is sbt still used for master compilation ? I could compile for
> > > > 2.3.0-cdh5.0.2 using maven following the instructions from the
> website:
> > > >
> > > > http://spark.apache.org/docs/latest/building-with-maven.html
> > > >
> > > > But when I am trying to use sbt for local testing and then I am
> getting
> > > > some weird errors...Is sbt still used by developers ? I am using
> > JDK7...
> > > >
> > > > org.xml.sax.SAXParseException; lineNumber: 4; columnNumber: 57;
> Element
> > > > type "settings" must be followed by either attribute specifications,
> > ">"
> > > or
> > > > "/>".
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.createSAXParseException(ErrorHandlerWrapper.java:198)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.util.ErrorHandlerWrapper.fatalError(ErrorHandlerWrapper.java:177)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:441)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLErrorReporter.reportError(XMLErrorReporter.java:368)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLScanner.reportFatalError(XMLScanner.java:1436)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.seekCloseOfStartTag(XMLDocumentFragmentScannerImpl.java:1394)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanStartElement(XMLDocumentFragmentScannerImpl.java:1327)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$ContentDriver.scanRootElementHook(XMLDocumentScannerImpl.java:1292)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl$FragmentContentDriver.next(XMLDocumentFragmentScannerImpl.java:3122)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl$PrologDriver.next(XMLDocumentScannerImpl.java:880)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentScannerImpl.next(XMLDocumentScannerImpl.java:606)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.impl.XMLDocumentFragmentScannerImpl.scanDocument(XMLDocumentFragmentScannerImpl.java:510)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:848)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.XML11Configuration.parse(XML11Configuration.java:777)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.XMLParser.parse(XMLParser.java:141)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.parsers.AbstractSAXParser.parse(AbstractSAXParser.java:1213)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl$JAXPSAXParser.parse(SAXParserImpl.java:649)
> > > >
> > > > at
> > > >
> > >
> >
> com.sun.org.apache.xerces.internal.jaxp.SAXParserImpl.parse(SAXParserImpl.java:333)
> > > >
> > > > at scala.xml.factory.XMLLoader$class.loadXML(XMLLoader.scala:40)
> > > >
> > > > at scala.xml.XML$.loadXML(XML.scala:57)
> > > >
> > > > at scala.xml.factory.XMLLoader$class.load(XMLLoader.scala:52)
> > > >
> > > > at scala.xml.XML$.load(XML.scala:57)
> > > >
> > > > at
> > > >
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:225)
> > > >
> > > > at
> > > >
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$settingsXml$1.apply(MavenHelper.scala:224)
> > > >
> > > > at sbt.Using.apply(Using.scala:25)
> > > >
> > > > at
> com.typesafe.sbt.pom.MavenHelper$.settingsXml(MavenHelper.scala:224)
> > > >
> > > > at
> > > >
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$.settingsXmlServers(MavenHelper.scala:245)
> > > >
> > > > at
> > > >
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$.createSbtCredentialsFromSettingsXml(MavenHelper.scala:291)
> > > >
> > > > at
> > > >
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
> > > >
> > > > at
> > > >
> > >
> >
> com.typesafe.sbt.pom.MavenHelper$$anonfun$pullSettingsFromPom$10.apply(MavenHelper.scala:83)
> > > >
> > > > at
> > > >
> > >
> >
> sbt.Scoped$RichInitialize$$anonfun$map$1$$anonfun$apply$3.apply(Structure.scala:177)
> > > >
> > > > at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
> > > >
> > > > at sbt.std.Transform$$anon$3$$anonfun$apply$2.apply(System.scala:45)
> > > >
> > > > at sbt.std.Transform$$anon$4.work(System.scala:64)
> > > >
> > > > at
> > > sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
> > > >
> > > > at
> > > sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:237)
> > > >
> > > > at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
> > > >
> > > > at sbt.Execute.work(Execute.scala:244)
> > > >
> > > > at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
> > > >
> > > > at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:237)
> > > >
> > > > at
> > > >
> > >
> >
> sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)
> > > >
> > > > at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)
> > > >
> > > > at java.util.concurrent.FutureTask.run(FutureTask.java:262)
> > > >
> > > > at
> > > java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
> > > >
> > > > at java.util.concurrent.FutureTask.run(FutureTask.java:262)
> > > >
> > > > at
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> > > >
> > > > at
> > > >
> > >
> >
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> > > >
> > > > at java.lang.Thread.run(Thread.java:745)
> > > >
> > > > Thanks.
> > > >
> > > > Deb
> > > >
> > >
> >
>

--001a113b977615b71004fe9ab002--

From dev-return-8457-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 20 11:14:33 2014
Return-Path: <dev-return-8457-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 187461140C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Jul 2014 11:14:33 +0000 (UTC)
Received: (qmail 39162 invoked by uid 500); 20 Jul 2014 11:14:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39101 invoked by uid 500); 20 Jul 2014 11:14:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39088 invoked by uid 99); 20 Jul 2014 11:14:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 11:14:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.216.41 as permitted sender)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 11:14:21 +0000
Received: by mail-qa0-f41.google.com with SMTP id j7so4395058qaq.28
        for <dev@spark.apache.org>; Sun, 20 Jul 2014 04:14:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=5zrp7nqsZINzKE0epliIaMZnmsDEqjvRDlYnhkuaZ+I=;
        b=FiHQn92aSHbDNmke+K5dOsanbybKVKWhqZlTamemEaZSUVp4X6rOcz6bsA2S60FIwz
         bWQKBjGLQf9Lvni0Ssr+BbgZBmFw/r5uX7GXvlgQ6UHVI+pvzcbX7NY/vA/X/uGQ187B
         wuNb3y9QFlU46+UKp/kjuxMeGvbcFvhB0OLVXvU3UXiCkWKDyVfJ1p6IgMKnBnQzMj1/
         bBQiX5nJbYMbrp8rgmogqoX94CjJ5aRc9jM/oJ7NGgAtQPrvZX/fCoCEXzIR9nQwFmk3
         v4eTvlJNAm23OVPclrVTu9ZaxsxFOd8PvnBJUbhZTEUIhcgistMg6Q9FdQcr+AXVPn/o
         xlvQ==
MIME-Version: 1.0
X-Received: by 10.229.219.193 with SMTP id hv1mr19090662qcb.4.1405854840647;
 Sun, 20 Jul 2014 04:14:00 -0700 (PDT)
Received: by 10.140.42.107 with HTTP; Sun, 20 Jul 2014 04:14:00 -0700 (PDT)
In-Reply-To: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
Date: Sun, 20 Jul 2014 07:14:00 -0400
Message-ID: <CAMtqZedqHZ=XeLGLk4d720s_fbW8ux3-jn3gKtarKZzJ2cz_4Q@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
From: Nan Zhu <zhunanmcgill@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1134495461f58504fe9e1641
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134495461f58504fe9e1641
Content-Type: text/plain; charset=UTF-8

Awesome!

On Saturday, July 19, 2014, Patrick Wendell <pwendell@gmail.com> wrote:

> Just a small note, today I committed a tool that will automatically
> mirror pull requests to JIRA issues, so contributors will no longer
> have to manually post a pull request on the JIRA when they make one.
>
> It will create a "link" on the JIRA and also make a comment to trigger
> an e-mail to people watching.
>
> This should make some things easier, such as avoiding accidental
> duplicate effort on the same JIRA.
>
> - Patrick
>

--001a1134495461f58504fe9e1641--

From dev-return-8458-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 20 15:07:15 2014
Return-Path: <dev-return-8458-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 92D4111723
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Jul 2014 15:07:15 +0000 (UTC)
Received: (qmail 41288 invoked by uid 500); 20 Jul 2014 15:07:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41224 invoked by uid 500); 20 Jul 2014 15:07:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41211 invoked by uid 99); 20 Jul 2014 15:07:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 15:07:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.174 as permitted sender)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 15:07:11 +0000
Received: by mail-we0-f174.google.com with SMTP id x48so6645867wes.33
        for <dev@spark.apache.org>; Sun, 20 Jul 2014 08:06:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=twRWbRL5vx3yMoZhaFh2TJxW0WVtIUleNXtE+vqumRo=;
        b=p/MtLNBLFaBTDna28bjrEIwBZUUOXgzgqvJVdBoLzZZTCcBu18wONnnOTUQlCIZPbE
         usEjSWOD2thIkvPSSKOioNn+TGdU4Fhfklw0CkVGzgdAOljHtQ/0q4umSEPqOnrDcI08
         wfKSVEz5hufJ/tnfyXJ5MDuqxdpMm5/s3XoF6fSyVtm0CQOqLzPvxIRX4oD9l51LQoom
         QeowGQW+afBsjtu76EonSqyK/QuhS2irAhZHtsAtodpCuQhqPx9evYPGkZJpqqfCS6Ea
         irw/3CvQmAQ7ghH1SGAuFghWoepirmc7NryC/R1BzyEW6IM+ImCA7yMgYgd7Th5k48Il
         mIPg==
X-Received: by 10.180.94.166 with SMTP id dd6mr24353941wib.33.1405868807259;
 Sun, 20 Jul 2014 08:06:47 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Sun, 20 Jul 2014 08:06:07 -0700 (PDT)
In-Reply-To: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 20 Jul 2014 11:06:07 -0400
Message-ID: <CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d044480fbdb8feb04fea1561a
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d044480fbdb8feb04fea1561a
Content-Type: text/plain; charset=UTF-8

That's pretty neat.

How does it work? Do we just need to put the issue ID (e.g. SPARK-1234)
anywhere in the pull request?

Nick


On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Just a small note, today I committed a tool that will automatically
> mirror pull requests to JIRA issues, so contributors will no longer
> have to manually post a pull request on the JIRA when they make one.
>
> It will create a "link" on the JIRA and also make a comment to trigger
> an e-mail to people watching.
>
> This should make some things easier, such as avoiding accidental
> duplicate effort on the same JIRA.
>
> - Patrick
>

--f46d044480fbdb8feb04fea1561a--

From dev-return-8459-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 20 16:51:22 2014
Return-Path: <dev-return-8459-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 593A811884
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Jul 2014 16:51:22 +0000 (UTC)
Received: (qmail 38213 invoked by uid 500); 20 Jul 2014 16:51:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38151 invoked by uid 500); 20 Jul 2014 16:51:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38139 invoked by uid 99); 20 Jul 2014 16:51:21 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 16:51:21 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 16:51:19 +0000
Received: by mail-oa0-f49.google.com with SMTP id eb12so6121484oac.36
        for <dev@spark.apache.org>; Sun, 20 Jul 2014 09:50:55 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bMJs8TRoaf0JUkMuJOGplZDfZclmsSJ3YacRNvW3B+U=;
        b=WyoUg+/wBJZRvtgIlw7GEAII8s3QUbmJcYRX+tV5dqOpsEBpWzZMJ3qTtujvXnJrU/
         p42GvjmUKqZaPDjB+WlVYZAk+ZCBzFSyOuTa0/98PYPoc5HJC5a3Y/CWlDcWmMj8anBL
         TJlmTDdPpu2WhdwyLo2tzjjl29SEd2F5+8RbF/CPwxFrorfJYxXdxRZxQbE2YECZBxx4
         XUgHEvokUMkvbyOFUyrOQIjS0gKLDst0/+TvMMVMFi7c1J3xWM+4nwsV4AYkw//KMjGn
         JNtDDDWjOu/t2kSfJSjl2QOlMD4N4io99N/3tCcqj4NrkCIODN4U0k4dsQ++RGdrbPBU
         d6oA==
MIME-Version: 1.0
X-Received: by 10.60.120.98 with SMTP id lb2mr28711378oeb.52.1405875054884;
 Sun, 20 Jul 2014 09:50:54 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Sun, 20 Jul 2014 09:50:54 -0700 (PDT)
In-Reply-To: <CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
	<CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com>
Date: Sun, 20 Jul 2014 09:50:54 -0700
Message-ID: <CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah it needs to have SPARK-XXX in the title (this is the format we
request already). It just works with small synchronization script I
wrote that we run every five minutes on Jeknins that uses the Github
and Jenkins API:

https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd139232929

- Patrick

On Sun, Jul 20, 2014 at 8:06 AM, Nicholas Chammas
<nicholas.chammas@gmail.com> wrote:
> That's pretty neat.
>
> How does it work? Do we just need to put the issue ID (e.g. SPARK-1234)
> anywhere in the pull request?
>
> Nick
>
>
> On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Just a small note, today I committed a tool that will automatically
>> mirror pull requests to JIRA issues, so contributors will no longer
>> have to manually post a pull request on the JIRA when they make one.
>>
>> It will create a "link" on the JIRA and also make a comment to trigger
>> an e-mail to people watching.
>>
>> This should make some things easier, such as avoiding accidental
>> duplicate effort on the same JIRA.
>>
>> - Patrick
>>

From dev-return-8460-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 20 20:57:24 2014
Return-Path: <dev-return-8460-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5166811C74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 20 Jul 2014 20:57:24 +0000 (UTC)
Received: (qmail 2311 invoked by uid 500); 20 Jul 2014 20:57:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 2246 invoked by uid 500); 20 Jul 2014 20:57:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 2232 invoked by uid 99); 20 Jul 2014 20:57:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 20:57:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_05_10,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 20 Jul 2014 20:57:18 +0000
Received: by mail-wg0-f52.google.com with SMTP id a1so5593384wgh.23
        for <dev@spark.apache.org>; Sun, 20 Jul 2014 13:56:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=c4HYTJFPuul0NsBTjzXrrXnVWQbO5c2BgLZLQ/1Cp7U=;
        b=FxP7sXYVdNMLELipU+020orVARXiOOEcXJUTU2eSiOEFc23ecttBGSw34Tc8Los6nn
         0Zq+Y//u4mclLEsKGHEWoIhoVA5Ti+pbmDglwZIia58aCjIaSIbu7bbMSGYtifIkN20V
         gOnCZnh7oEcjjZb7XL4j3y0aoyDS9Rp/z1cACzSpaL1lLT94ZS4qSFtsKau+K80cfpaC
         mT+cMlqd9az4aoyEywweFVxaF80mIrqkCRve02Phnx19qpapyzlR62errzymDjjBny0O
         FRbqK9gXQ90YLsiUHoLxYzn0uBXh2vu8R992BHVR19L9chvcWOYsV8G3WF8y5S48ibsz
         lFNA==
X-Received: by 10.194.57.132 with SMTP id i4mr15905597wjq.6.1405889816564;
 Sun, 20 Jul 2014 13:56:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Sun, 20 Jul 2014 13:56:16 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Sun, 20 Jul 2014 16:56:16 -0400
Message-ID: <CAOhmDzf2uU7RzRn9za7ibbXCQaYMtnx0Ygh7_ydcHWi61VDcLw@mail.gmail.com>
Subject: sbt/sbt test steals window focus on OS X
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7ba9782e1c172504fea63b59
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7ba9782e1c172504fea63b59
Content-Type: text/plain; charset=UTF-8

I just created SPARK-2602 <https://issues.apache.org/jira/browse/SPARK-2602> to
track this issue.

Are there others who can confirm this is an issue? Also, does this issue
extend to other OSes?

Nick

--047d7ba9782e1c172504fea63b59--

From dev-return-8461-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 03:28:40 2014
Return-Path: <dev-return-8461-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F4BC11151
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 03:28:40 +0000 (UTC)
Received: (qmail 92646 invoked by uid 500); 21 Jul 2014 03:28:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92587 invoked by uid 500); 21 Jul 2014 03:28:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92576 invoked by uid 99); 21 Jul 2014 03:28:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 03:28:39 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hshreedharan@cloudera.com designates 209.85.220.177 as permitted sender)
Received: from [209.85.220.177] (HELO mail-vc0-f177.google.com) (209.85.220.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 03:28:33 +0000
Received: by mail-vc0-f177.google.com with SMTP id hy4so10882671vcb.22
        for <dev@spark.apache.org>; Sun, 20 Jul 2014 20:28:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=WK8gi2ir4u84QQOpL5cIjnn7AQVtKWcCGl5a8Ea0Sqs=;
        b=RG1RFZBm3Qs8CmAK4k83XHhQzfYkZ22X/S2Z+NBb//H3c7xpIV8R1N7Gv32wFfX/tY
         cT+GnZg4Z3IAzZK/b2F6G9FIZ8JPHYW75Xoea86APKFASWjmS4u7xzmmsLjaad5QRsOY
         yXiMHj8424y0PdTUFyl3HIzPVIPT6o1tA2VuXaxkOfuI3JCBC081Poa4yl7kRILlmpbL
         PDx0g1oZmOsoKQOXwUSqMpzwoQGZbitnXYaUyuRcn5Da0h5uou9+Myau4D1LkH+OKNaJ
         Cmc3oPRKCJQXc4Z1COEwSPYz0uqrePnNxqg6qrwTWNTn41x0EKtzibOowo4DMgUJz+Qm
         oaHA==
X-Gm-Message-State: ALoCoQmJdoDwkx2uWDEyW4vLi2bYfSLn2GN2meUR31PskTe6F7WwfgZ1DspKIIK1iPi+pSut7A0M
X-Received: by 10.220.110.77 with SMTP id m13mr418037vcp.73.1405913292942;
 Sun, 20 Jul 2014 20:28:12 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.221.48.4 with HTTP; Sun, 20 Jul 2014 20:27:52 -0700 (PDT)
In-Reply-To: <CAOhmDzf2uU7RzRn9za7ibbXCQaYMtnx0Ygh7_ydcHWi61VDcLw@mail.gmail.com>
References: <CAOhmDzf2uU7RzRn9za7ibbXCQaYMtnx0Ygh7_ydcHWi61VDcLw@mail.gmail.com>
From: Hari Shreedharan <hshreedharan@cloudera.com>
Date: Sun, 20 Jul 2014 20:27:52 -0700
Message-ID: <CAHbPYVbYdXAsjSQB=pKUi90riO1S5iDWj4dEmnNxjbCgxWYUfg@mail.gmail.com>
Subject: Re: sbt/sbt test steals window focus on OS X
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b3a9060695b0c04feabb2d3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a9060695b0c04feabb2d3
Content-Type: text/plain; charset=UTF-8

Add this to your .bash_profile (or .bashrc) - that will fix it.

export _JAVA_OPTIONS=-Djava.awt.headless=true


Hari


On Sun, Jul 20, 2014 at 1:56 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> I just created SPARK-2602 <
> https://issues.apache.org/jira/browse/SPARK-2602> to
> track this issue.
>
> Are there others who can confirm this is an issue? Also, does this issue
> extend to other OSes?
>
> Nick
>

--047d7b3a9060695b0c04feabb2d3--

From dev-return-8462-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 08:30:31 2014
Return-Path: <dev-return-8462-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4C507116C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 08:30:31 +0000 (UTC)
Received: (qmail 28196 invoked by uid 500); 21 Jul 2014 08:30:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28140 invoked by uid 500); 21 Jul 2014 08:30:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28128 invoked by uid 99); 21 Jul 2014 08:30:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 08:30:30 +0000
X-ASF-Spam-Status: No, hits=3.1 required=10.0
	tests=HK_RANDOM_ENVFROM,HK_RANDOM_FROM,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of haoyuan.li@gmail.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 08:30:26 +0000
Received: by mail-we0-f181.google.com with SMTP id k48so6030297wev.40
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 01:30:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ESq055hh8KnWzTFow30woR4S8hI3c9p/N3fG1T9knvE=;
        b=v1iWrOMNHQSoSZhdTmPpan9LYmFAvUClqx6AdfysF9EIy03xpMRHmoinGzD5iu3TJ0
         YP7mdRuu6Fd94gspVQAA9dH5Pa6LfOzC8rdYOIaUhyMFRNic6O8zN66QZtOweVZiWGpd
         oqL4l+JCXyA2mwbPMXRma/Z6DsOkDvjkg3TfdSSO1K/hCifdVpr7I4rdx3WfMz79YBR6
         BPAFy8vrQCpWvjxinJXoxmKgMK5o/PBqVzypMwLTnr3nyV6IlfNcBY+NRY4yS5uEnqRq
         KpAhfJ2CCph+m6jocZiTKHBydIyl43EOtsaEa4zxQ25C2fhpkWI2PNkWczHU9Q79hb4L
         6+5g==
X-Received: by 10.180.19.40 with SMTP id b8mr1901702wie.77.1405931402262; Mon,
 21 Jul 2014 01:30:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.216.204.6 with HTTP; Mon, 21 Jul 2014 01:29:42 -0700 (PDT)
In-Reply-To: <CABDsqqb6HZr8P3N2aVDX7kd8Oiiz4dBCfVkwpfy9LoWwmEUbNg@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
 <CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
 <CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
 <CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
 <CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com>
 <CABDsqqarh4OO2fdLEB49yXjroBoCHh7_oVuRdsGZx-jNFdb1Lw@mail.gmail.com>
 <CAG2iju36syXK7yJUDgwOS4U9PTrR+M=DYPcV9AaBP=baskZgvw@mail.gmail.com>
 <CABDsqqYP8QG+FOs3k4Jr9RDaStV8jxgGZ2K7=Q1HJqpZXruu8w@mail.gmail.com> <CABDsqqb6HZr8P3N2aVDX7kd8Oiiz4dBCfVkwpfy9LoWwmEUbNg@mail.gmail.com>
From: Haoyuan Li <haoyuan.li@gmail.com>
Date: Mon, 21 Jul 2014 01:29:42 -0700
Message-ID: <CAG2iju0g8ge+AqPsCxVf5GEHJvOqc97sKNR6KTpwZ2OeUd3O7g@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec53d5df5cf819c04feafe948
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec53d5df5cf819c04feafe948
Content-Type: text/plain; charset=UTF-8

Qingyang,

Aha. Got it.

800MB data is pretty small. Loading from Tachyon does have a bit of extra
overhead. But it will have more benefit when the data size is larger. Also,
if you store the table in Tachyon, you can have different shark servers to
query the data at the same time. For more trade-off, please refer to this
page: http://tachyon-project.org/Running-Shark-on-Tachyon.html

Best,

Haoyuan


On Wed, Jul 16, 2014 at 12:06 AM, qingyang li <liqingyang1985@gmail.com>
wrote:

> let's me describe my scene:
> ----------------------
> i have 8 machines (24 core , 16G memory, per machine) of spark cluster and
> tachyon cluster.  On tachyon,  I create one table which contains 800M data,
> when i run query sql on shark,   it will cost 2.43s,  but when i create the
> same table on spark memory , i run  the same sql , it will cost 1.56s.
>  data on tachyon cost more time than data on spark memory.   they all have
> 150 map process,  and per node 16-20 map process.
> I think the reason is that when data is on tachyon, shark will let spark
> slave load data from tachyon salve which is on the same node with tachyon
> slave,
> i have tried to set some configuration to tune shark and tachyon, but still
> can not make the former more fast than 2.43s.
> do anyone have some ideas ?
>
> By the way ,  my tachyon block size is 1GB now,  i want to reset block size
> ,  will it work by setting tachyon.user.default.block.size.byte=8M ?  if
> not,  what does tachyon.user.default.block.size.byte mean?
>
>
> 2014-07-14 13:13 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:
>
> > Shark,  thanks for replying.
> > Let's me clear my question again.
> > ----------------------------------------------
> > i create a table using " create table xxx1
> > tblproperties("shark.cache"="tachyon") as select * from xxx2"
> > when excuting some sql (for example , select * from xxx1) using shark,
> >  shark will read data into shark's memory  from tachyon's memory.
> > I think if each time we execute sql, shark always load data from tachyon,
> > it is less effient.
> > could we use some cache policy (such as,  CacheAllPolicy FIFOCachePolicy
> > LRUCachePolicy ) to cache data to invoid reading data from tachyon for
> > each sql query?
> > ----------------------------------------------
> >
> >
> >
> > 2014-07-14 2:47 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
> >
> > Qingyang,
> >>
> >> Are you asking Spark or Shark (The first email was "Shark", the last
> email
> >> was "Spark".)?
> >>
> >> Best,
> >>
> >> Haoyuan
> >>
> >>
> >> On Wed, Jul 9, 2014 at 7:40 PM, qingyang li <liqingyang1985@gmail.com>
> >> wrote:
> >>
> >> > could i set some cache policy to let spark load data from tachyon only
> >> one
> >> > time for all sql query?  for example by using CacheAllPolicy
> >> > FIFOCachePolicy LRUCachePolicy.  But I have tried that three policy,
> >> they
> >> > are not useful.
> >> > I think , if spark always load data for each sql query,  it will
> impact
> >> the
> >> > query speed , it will take more time than the case that data are
> >> managed by
> >> > spark itself.
> >> >
> >> >
> >> >
> >> >
> >> > 2014-07-09 1:19 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
> >> >
> >> > > Yes. For Shark, two modes, "shark.cache=tachyon" and
> >> > "shark.cache=memory",
> >> > > have the same ser/de overhead. Shark loads data from outsize of the
> >> > process
> >> > > in Tachyon mode with the following benefits:
> >> > >
> >> > >
> >> > >    - In-memory data sharing across multiple Shark instances (i.e.
> >> > stronger
> >> > >    isolation)
> >> > >    - Instant recovery of in-memory tables
> >> > >    - Reduce heap size => faster GC in shark
> >> > >    - If the table is larger than the memory size, only the hot
> columns
> >> > will
> >> > >    be cached in memory
> >> > >
> >> > > from
> http://tachyon-project.org/master/Running-Shark-on-Tachyon.html
> >> and
> >> > > https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon
> >> > >
> >> > > Haoyuan
> >> > >
> >> > >
> >> > > On Tue, Jul 8, 2014 at 9:58 AM, Aaron Davidson <ilikerps@gmail.com>
> >> > wrote:
> >> > >
> >> > > > Shark's in-memory format is already serialized (it's compressed
> and
> >> > > > column-based).
> >> > > >
> >> > > >
> >> > > > On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <
> >> mridul@gmail.com>
> >> > > > wrote:
> >> > > >
> >> > > > > You are ignoring serde costs :-)
> >> > > > >
> >> > > > > - Mridul
> >> > > > >
> >> > > > > On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <
> >> ilikerps@gmail.com>
> >> > > > wrote:
> >> > > > > > Tachyon should only be marginally less performant than
> >> memory_only,
> >> > > > > because
> >> > > > > > we mmap the data from Tachyon's ramdisk. We do not have to,
> say,
> >> > > > transfer
> >> > > > > > the data over a pipe from Tachyon; we can directly read from
> the
> >> > > > buffers
> >> > > > > in
> >> > > > > > the same way that Shark reads from its in-memory columnar
> >> format.
> >> > > > > >
> >> > > > > >
> >> > > > > >
> >> > > > > > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <
> >> > > liqingyang1985@gmail.com>
> >> > > > > > wrote:
> >> > > > > >
> >> > > > > >> hi, when i create a table, i can point the cache strategy
> using
> >> > > > > >> shark.cache,
> >> > > > > >> i think "shark.cache=memory_only"  means data are managed by
> >> > spark,
> >> > > > and
> >> > > > > >> data are in the same jvm with excutor;   while
> >> > >  "shark.cache=tachyon"
> >> > > > > >>  means  data are managed by tachyon which is off heap, and
> data
> >> > are
> >> > > > not
> >> > > > > in
> >> > > > > >> the same jvm with excutor,  so spark will load data from
> >> tachyon
> >> > for
> >> > > > > each
> >> > > > > >> query sql , so,  is  tachyon less efficient than memory_only
> >> cache
> >> > > > > strategy
> >> > > > > >>  ?
> >> > > > > >> if yes, can we let spark load all data once from tachyon  for
> >> all
> >> > > sql
> >> > > > > query
> >> > > > > >>  if i want to use tachyon cache strategy since tachyon is
> more
> >> HA
> >> > > than
> >> > > > > >> memory_only ?
> >> > > > > >>
> >> > > > >
> >> > > >
> >> > >
> >> > >
> >> > >
> >> > > --
> >> > > Haoyuan Li
> >> > > AMPLab, EECS, UC Berkeley
> >> > > http://www.cs.berkeley.edu/~haoyuan/
> >> > >
> >> >
> >>
> >>
> >>
> >> --
> >> Haoyuan Li
> >> AMPLab, EECS, UC Berkeley
> >> http://www.cs.berkeley.edu/~haoyuan/
> >>
> >
> >
>



-- 
Haoyuan Li
AMPLab, EECS, UC Berkeley
http://www.cs.berkeley.edu/~haoyuan/

--bcaec53d5df5cf819c04feafe948--

From dev-return-8463-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 14:47:23 2014
Return-Path: <dev-return-8463-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 73A191119A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 14:47:23 +0000 (UTC)
Received: (qmail 92263 invoked by uid 500); 21 Jul 2014 14:47:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92217 invoked by uid 500); 21 Jul 2014 14:47:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92206 invoked by uid 99); 21 Jul 2014 14:47:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 14:47:22 +0000
X-ASF-Spam-Status: No, hits=2.0 required=10.0
	tests=SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 14:47:21 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <aaron@placeiq.com>)
	id 1X9Erg-0002vW-3F
	for dev@spark.incubator.apache.org; Mon, 21 Jul 2014 07:46:56 -0700
Date: Mon, 21 Jul 2014 07:46:56 -0700 (PDT)
From: aaronjosephs <aaron@placeiq.com>
To: dev@spark.incubator.apache.org
Message-ID: <1405954016057-7432.post@n3.nabble.com>
Subject: Pull request for PLAT-911
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

I found the spark Jira ticket,
https://issues.apache.org/jira/browse/SPARK-911, and noticed that no one had
done it. It seemed like a useful and interesting feature. I implemented a
pull request here ( https://github.com/apache/spark/pull/1381) and was
looking for some input on it( go easy on me first time contributor).



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Pull-request-for-PLAT-911-tp7432.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8464-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 15:24:37 2014
Return-Path: <dev-return-8464-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 13EF01132E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 15:24:37 +0000 (UTC)
Received: (qmail 20123 invoked by uid 500); 21 Jul 2014 15:24:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20068 invoked by uid 500); 21 Jul 2014 15:24:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20056 invoked by uid 99); 21 Jul 2014 15:24:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:24:35 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eerlands@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:24:31 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s6LFOBZY016732
	for <dev@spark.apache.org>; Mon, 21 Jul 2014 11:24:11 -0400
Date: Mon, 21 Jul 2014 11:24:11 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: dev@spark.apache.org
Message-ID: <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
In-Reply-To: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
Subject: RFC:  Supporting the Scala drop Method for Spark RDDs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC35 (Linux)/8.0.6_GA_5922)
Thread-Topic: Supporting the Scala drop Method for Spark RDDs
Thread-Index: czYoDBASkLze8Ye9zbTfdPgQS5mwKA==
X-Virus-Checked: Checked by ClamAV on apache.org

A few weeks ago I submitted a PR for supporting rdd.drop(n), under SPARK-2315:
https://issues.apache.org/jira/browse/SPARK-2315

Supporting the drop method would make some operations convenient, however it forces computation of >= 1 partition of the parent RDD, and so it would behave like a "partial action" that returns an RDD as the result.

I wrote up a discussion of these trade-offs here:
http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/

From dev-return-8465-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 15:28:02 2014
Return-Path: <dev-return-8465-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 244871134D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 15:28:02 +0000 (UTC)
Received: (qmail 32376 invoked by uid 500); 21 Jul 2014 15:28:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32310 invoked by uid 500); 21 Jul 2014 15:28:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32294 invoked by uid 99); 21 Jul 2014 15:28:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:28:01 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:27:57 +0000
Received: by mail-vc0-f181.google.com with SMTP id lf12so12348086vcb.40
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 08:27:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=W+0K+A/vrUexbvvWpWOLrU6unI5VMFqujaUx6eCo2/4=;
        b=FPoidXr+pUww21qlpqg34i61B3LdhjybtPp6hGxTvxgHCwCJmE5jhXU2jtE31pDm1t
         CZVtgslmuJfOa8ltPOkFbeKkVhtKW41QUh0rCrd9gRhBCkvMj9tm0pVc4jBMA4ex3VqE
         YWEKepdyvqWCxmZiKWt85Ty0j9b0kB9K05M3jFArjX7cDV4Gv7jnYVOA8AvC2umntZSu
         yH0+F7EHIzqK1GEbXFtHaLfiqP926/YXc3l18ym0OPlNHGk0vPIgTonFwAIyElt2TqAf
         Y+kRVzSedEHTVTRauj7GGPSW3TAh8zMP++q/O9StzbeKJW3jHYDSCbWya3YpdiRvrWpb
         DJ9g==
X-Gm-Message-State: ALoCoQnC/88xenOv8lOxBNHbPS2vAM+1hgIlJwUVD43AWkLopbb3KY/NiKAtbANvPyus/+irHwOp
X-Received: by 10.220.94.135 with SMTP id z7mr18301143vcm.46.1405956452439;
        Mon, 21 Jul 2014 08:27:32 -0700 (PDT)
Received: from mail-vc0-f169.google.com (mail-vc0-f169.google.com [209.85.220.169])
        by mx.google.com with ESMTPSA id sx10sm22720621vdb.16.2014.07.21.08.27.31
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Mon, 21 Jul 2014 08:27:31 -0700 (PDT)
Received: by mail-vc0-f169.google.com with SMTP id hu12so12628645vcb.28
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 08:27:31 -0700 (PDT)
X-Received: by 10.221.55.70 with SMTP id vx6mr30213026vcb.23.1405956451091;
 Mon, 21 Jul 2014 08:27:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Mon, 21 Jul 2014 08:27:10 -0700 (PDT)
In-Reply-To: <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com> <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
From: Andrew Ash <andrew@andrewash.com>
Date: Mon, 21 Jul 2014 08:27:10 -0700
Message-ID: <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
To: dev@spark.apache.org, Erik Erlandson <eje@redhat.com>
Content-Type: multipart/alternative; boundary=001a113389d2d64e2504feb5be0c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113389d2d64e2504feb5be0c
Content-Type: text/plain; charset=UTF-8

Personally I'd find the method useful -- I've often had a .csv file with a
header row that I want to drop so filter it out, which touches all
partitions anyway.  I don't have any comments on the implementation quite
yet though.


On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com> wrote:

> A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> SPARK-2315:
> https://issues.apache.org/jira/browse/SPARK-2315
>
> Supporting the drop method would make some operations convenient, however
> it forces computation of >= 1 partition of the parent RDD, and so it would
> behave like a "partial action" that returns an RDD as the result.
>
> I wrote up a discussion of these trade-offs here:
>
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
>

--001a113389d2d64e2504feb5be0c--

From dev-return-8466-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 15:36:54 2014
Return-Path: <dev-return-8466-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CC420113AF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 15:36:54 +0000 (UTC)
Received: (qmail 60921 invoked by uid 500); 21 Jul 2014 15:36:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60862 invoked by uid 500); 21 Jul 2014 15:36:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60850 invoked by uid 99); 21 Jul 2014 15:36:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:36:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 74.125.82.181 as permitted sender)
Received: from [74.125.82.181] (HELO mail-we0-f181.google.com) (74.125.82.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:36:48 +0000
Received: by mail-we0-f181.google.com with SMTP id k48so6667550wev.12
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 08:36:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=AO7UzPYDdoPckHwpxyo8sYaqBUor8qsCDgfpQjjzA04=;
        b=pzrzTXkDJByKsZqsGanVrWLDK14u7qR6VvVUshbJ6Z2sOACEVqGGA2ywp21xyeFS8b
         0TVfvSXVLR3iz0cL64CdgFeyY0ZlzFhpNzTLtujxArSrYqj4I4V6uUhXlxhEth5ZC0Xy
         PJaLa9UMLBcHjYjphG927DN4CD3MGvhVN26Xo8NZRSCheBdjRDUQFZCkSpi8gi6/ZU78
         EyGvREBLZxpDauwD+UbCiX8QgOEk7ZbqV5749wql9wqjpSnl1girsfslp1phN6zlBtrb
         Dpm/8usbXVBvsjjM1uFeapBxWuxDmK3jecYCM53b+7X28imlMZ0VRKrUKnxIPrTAL2hk
         NouQ==
MIME-Version: 1.0
X-Received: by 10.180.109.168 with SMTP id ht8mr5418981wib.68.1405956986524;
 Mon, 21 Jul 2014 08:36:26 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Mon, 21 Jul 2014 08:36:26 -0700 (PDT)
Date: Mon, 21 Jul 2014 11:36:26 -0400
Message-ID: <CADtDQQLFwCQD9WnaL0s1YuG6D+MK1X0xFwQiOPRiyQ0Mg89BAw@mail.gmail.com>
Subject: Examples have SparkContext improperly labeled?
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

The examples listed here

https://spark.apache.org/examples.html

refer to the spark context as "spark" but when running Spark Shell
uses "sc" for the SparkContext.

Am I missing something?

Thanks!

RJ


-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8467-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 15:47:23 2014
Return-Path: <dev-return-8467-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9592D11432
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 15:47:23 +0000 (UTC)
Received: (qmail 94751 invoked by uid 500); 21 Jul 2014 15:47:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94691 invoked by uid 500); 21 Jul 2014 15:47:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94678 invoked by uid 99); 21 Jul 2014 15:47:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:47:22 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of aniket.bhatnagar@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:47:18 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <aniket.bhatnagar@gmail.com>)
	id 1X9Fni-00070r-54
	for dev@spark.incubator.apache.org; Mon, 21 Jul 2014 08:46:54 -0700
Date: Mon, 21 Jul 2014 08:46:54 -0700 (PDT)
From: Aniket <aniket.bhatnagar@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAJOb8buadAjzhvi4ujpW_nPxRt7ncjpGC8R86mFbeJOjfLiE2w@mail.gmail.com>
In-Reply-To: <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
References: <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com> <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_254888_22681217.1405957614147"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_254888_22681217.1405957614147
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

I too would like this feature. Erik's post makes sense. However, shouldn't
the RDD also repartition itself after drop to effectively make use of
cluster resources?
On Jul 21, 2014 8:58 PM, "Andrew Ash [via Apache Spark Developers List]" <
ml-node+s1001551n7434h99@n3.nabble.com> wrote:

> Personally I'd find the method useful -- I've often had a .csv file with a
> header row that I want to drop so filter it out, which touches all
> partitions anyway.  I don't have any comments on the implementation quite
> yet though.
>
>
> On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <[hidden email]
> <http://user/SendEmail.jtp?type=node&node=7434&i=0>> wrote:
>
> > A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> > SPARK-2315:
> > https://issues.apache.org/jira/browse/SPARK-2315
> >
> > Supporting the drop method would make some operations convenient,
> however
> > it forces computation of >= 1 partition of the parent RDD, and so it
> would
> > behave like a "partial action" that returns an RDD as the result.
> >
> > I wrote up a discussion of these trade-offs here:
> >
> >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> >
>
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/RFC-Supporting-the-Scala-drop-Method-for-Spark-RDDs-tp7433p7434.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h76@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YW5pa2V0LmJoYXRuYWdhckBnbWFpbC5jb218MXwxMzE3NTAzMzQz>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/RFC-Supporting-the-Scala-drop-Method-for-Spark-RDDs-tp7433p7436.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_254888_22681217.1405957614147--

From dev-return-8468-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 15:53:41 2014
Return-Path: <dev-return-8468-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 568EB11481
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 15:53:41 +0000 (UTC)
Received: (qmail 17478 invoked by uid 500); 21 Jul 2014 15:53:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17416 invoked by uid 500); 21 Jul 2014 15:53:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17405 invoked by uid 99); 21 Jul 2014 15:53:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:53:40 +0000
X-ASF-Spam-Status: No, hits=-3.7 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eerlands@redhat.com designates 209.132.183.24 as permitted sender)
Received: from [209.132.183.24] (HELO mx3-phx2.redhat.com) (209.132.183.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:53:36 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx3-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s6LFrEkY016340;
	Mon, 21 Jul 2014 11:53:14 -0400
Date: Mon, 21 Jul 2014 11:53:14 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Message-ID: <1629640389.11059237.1405957994035.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAJOb8buadAjzhvi4ujpW_nPxRt7ncjpGC8R86mFbeJOjfLiE2w@mail.gmail.com>
References: <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com> <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com> <CAJOb8buadAjzhvi4ujpW_nPxRt7ncjpGC8R86mFbeJOjfLiE2w@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC35 (Linux)/8.0.6_GA_5922)
Thread-Topic: Supporting the Scala drop Method for Spark RDDs
Thread-Index: qWFhpVu4JTioZVlaOrvD6+BI4yfOsg==
X-Virus-Checked: Checked by ClamAV on apache.org



----- Original Message -----
> I too would like this feature. Erik's post makes sense. However, shouldn't
> the RDD also repartition itself after drop to effectively make use of
> cluster resources?


My thinking is that in most use cases(*), one is dropping a small number of rows, and they are in only the 1st partition, and so repartitioning would not be worth the cost.  The first partition would be passed mostly intact, and the remainder would be completely unchanged.

(*) or at least most use cases that I've considered.


> On Jul 21, 2014 8:58 PM, "Andrew Ash [via Apache Spark Developers List]" <
> ml-node+s1001551n7434h99@n3.nabble.com> wrote:
> 
> > Personally I'd find the method useful -- I've often had a .csv file with a
> > header row that I want to drop so filter it out, which touches all
> > partitions anyway.  I don't have any comments on the implementation quite
> > yet though.
> >
> >
> > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <[hidden email]
> > <http://user/SendEmail.jtp?type=node&node=7434&i=0>> wrote:
> >
> > > A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> > > SPARK-2315:
> > > https://issues.apache.org/jira/browse/SPARK-2315
> > >
> > > Supporting the drop method would make some operations convenient,
> > however
> > > it forces computation of >= 1 partition of the parent RDD, and so it
> > would
> > > behave like a "partial action" that returns an RDD as the result.
> > >
> > > I wrote up a discussion of these trade-offs here:
> > >
> > >
> > http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > >
> >
> >
> > ------------------------------
> >  If you reply to this email, your message will be added to the discussion
> > below:
> >
> > http://apache-spark-developers-list.1001551.n3.nabble.com/RFC-Supporting-the-Scala-drop-Method-for-Spark-RDDs-tp7433p7434.html
> >  To start a new topic under Apache Spark Developers List, email
> > ml-node+s1001551n1h76@n3.nabble.com
> > To unsubscribe from Apache Spark Developers List, click here
> > <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=YW5pa2V0LmJoYXRuYWdhckBnbWFpbC5jb218MXwxMzE3NTAzMzQz>
> > .
> > NAML
> > <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
> >
> 
> 
> 
> 
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/RFC-Supporting-the-Scala-drop-Method-for-Spark-RDDs-tp7433p7436.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.

From dev-return-8469-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 15:53:45 2014
Return-Path: <dev-return-8469-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B84031148A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 15:53:45 +0000 (UTC)
Received: (qmail 18536 invoked by uid 500); 21 Jul 2014 15:53:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18480 invoked by uid 500); 21 Jul 2014 15:53:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18454 invoked by uid 99); 21 Jul 2014 15:53:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:53:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 15:53:42 +0000
Received: by mail-wi0-f174.google.com with SMTP id d1so4386483wiv.13
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 08:53:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=x+jJPnAb1u9SqIC9MrxB0C5yWU2+7B6jivi3ieiqVAI=;
        b=iJImKrLUgJTIaaEAXAj2ACa8ChSbr5zP3LN0LtqwDWyOJiblh6B7M+0ZnrTQejv6ke
         fDpCgVXVVuz3mA7dqL2qpIGyxw9/dBcPS6vmL8T1EsBUhN2IZRSv3nKS5YEU1gnIHxSi
         rQ+YyA1qbvptBhjQcqiiGa6N4leEXLZyALAm28mQiu65KSJGo1/qdrqH+hu4+N2j2kTq
         TDsOxtlHGhHeQxN58S5JKDyNlyTgYr66xVC7+/dlj/bY5p38/ca41WpJQE0DYevvRz8m
         q/aiV7VIH6ktKx7O736cxnlLE5638fkc7a/4VyWv6GuPkKusvPwJuVB12n+OZN2uthUx
         4IMA==
X-Gm-Message-State: ALoCoQlk+HqPzDAvwkorDnNYIlrQB+Lg87b1LIR5y4bBlNDvitVYukhaP3l5xuiwjXpZlrUFQtlQ
MIME-Version: 1.0
X-Received: by 10.180.105.202 with SMTP id go10mr5765714wib.66.1405957995348;
 Mon, 21 Jul 2014 08:53:15 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Mon, 21 Jul 2014 08:53:15 -0700 (PDT)
In-Reply-To: <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
	<702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
	<CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
Date: Mon, 21 Jul 2014 08:53:15 -0700
Message-ID: <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d0418271ce1e40304feb61adc
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d0418271ce1e40304feb61adc
Content-Type: text/plain; charset=UTF-8

Sure, drop() would be useful, but breaking the "transformations are lazy;
only actions launch jobs" model is abhorrent -- which is not to say that we
haven't already broken that model for useful operations (cf.
RangePartitioner, which is used for sorted RDDs), but rather that each such
exception to the model is a significant source of pain that can be hard to
work with or work around.

I really wouldn't like to see another such model-breaking transformation
added to the API.  On the other hand, being able to write transformations
with dependencies on these kind of "internal" jobs is sometimes very
useful, so a significant reworking of Spark's Dependency model that would
allow for lazily running such internal jobs and making the results
available to subsequent stages may be something worth pursuing.


On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <andrew@andrewash.com> wrote:

> Personally I'd find the method useful -- I've often had a .csv file with a
> header row that I want to drop so filter it out, which touches all
> partitions anyway.  I don't have any comments on the implementation quite
> yet though.
>
>
> On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com> wrote:
>
> > A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> > SPARK-2315:
> > https://issues.apache.org/jira/browse/SPARK-2315
> >
> > Supporting the drop method would make some operations convenient, however
> > it forces computation of >= 1 partition of the parent RDD, and so it
> would
> > behave like a "partial action" that returns an RDD as the result.
> >
> > I wrote up a discussion of these trade-offs here:
> >
> >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> >
>

--f46d0418271ce1e40304feb61adc--

From dev-return-8470-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 16:52:44 2014
Return-Path: <dev-return-8470-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7962711806
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 16:52:44 +0000 (UTC)
Received: (qmail 1534 invoked by uid 500); 21 Jul 2014 16:52:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1477 invoked by uid 500); 21 Jul 2014 16:52:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1456 invoked by uid 99); 21 Jul 2014 16:52:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 16:52:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.216.42 as permitted sender)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 16:52:40 +0000
Received: by mail-qa0-f42.google.com with SMTP id j15so5483492qaq.29
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 09:52:15 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=0WF4fheANDCb82dnGjIyKAK8VurDUwZjtvUHK548bko=;
        b=QIuHFXL+spIbszWQwOQ9E2nJdJvsMo6kaSqDFwoY7e8x0DpDLAeOpV5RYM5vDE8ijG
         0hNDJBxVJSTVErbVxB3wWF69pTRk/j3nIkeQsyuJ9t+Hz80FmLiIHDP959Laf6D60mE2
         oxIFLC7sJhDlnCTVNzWETSJCwD7YjWCPjmExmZnFMGVZufQ1k8UATiee99C901cgNDwu
         K7IAFSecQB2M+oUBRpHVrQwWeGwAg4qVoz3dteYA1VS9lS07n8cbu77XP2rZeJ5qQbVV
         trNjdgdgwMPqHDw6LO9qGaCD28Kp3rUXy/fPhnKByYUVVchexvdQQOye7SwhJE0ek708
         AvQQ==
X-Gm-Message-State: ALoCoQlHpTRcDh2K2C+qGBNP/7n0GasCCh0XHGzc/WK1RCfChHbnXbAOejoqRAVT++c+aVgBJl1A
MIME-Version: 1.0
X-Received: by 10.140.26.149 with SMTP id 21mr40474315qgv.51.1405961535643;
 Mon, 21 Jul 2014 09:52:15 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Mon, 21 Jul 2014 09:52:15 -0700 (PDT)
In-Reply-To: <CADtDQQLFwCQD9WnaL0s1YuG6D+MK1X0xFwQiOPRiyQ0Mg89BAw@mail.gmail.com>
References: <CADtDQQLFwCQD9WnaL0s1YuG6D+MK1X0xFwQiOPRiyQ0Mg89BAw@mail.gmail.com>
Date: Mon, 21 Jul 2014 09:52:15 -0700
Message-ID: <CACBYxKL86hVo8ivCS=b4dWS8+bA4xJVdtOdoKj4oJN9s8VR+ew@mail.gmail.com>
Subject: Re: Examples have SparkContext improperly labeled?
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c032c0e6712104feb6ed13
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c032c0e6712104feb6ed13
Content-Type: text/plain; charset=UTF-8

Hi RJ,

Spark Shell instantiates a SparkContext for you named "sc".  In other apps,
the user instantiates it themself and can give the variable whatever name
they want, e.g. "spark".

-Sandy


On Mon, Jul 21, 2014 at 8:36 AM, RJ Nowling <rnowling@gmail.com> wrote:

> Hi all,
>
> The examples listed here
>
> https://spark.apache.org/examples.html
>
> refer to the spark context as "spark" but when running Spark Shell
> uses "sc" for the SparkContext.
>
> Am I missing something?
>
> Thanks!
>
> RJ
>
>
> --
> em rnowling@gmail.com
> c 954.496.2314
>

--001a11c032c0e6712104feb6ed13--

From dev-return-8471-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 17:24:41 2014
Return-Path: <dev-return-8471-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 12F0F119B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 17:24:41 +0000 (UTC)
Received: (qmail 27704 invoked by uid 500); 21 Jul 2014 17:24:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27487 invoked by uid 500); 21 Jul 2014 17:24:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26815 invoked by uid 99); 21 Jul 2014 17:24:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 17:24:39 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eerlands@redhat.com designates 209.132.183.25 as permitted sender)
Received: from [209.132.183.25] (HELO mx4-phx2.redhat.com) (209.132.183.25)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 17:24:35 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx4-phx2.redhat.com (8.13.8/8.13.8) with ESMTP id s6LHOEUj004449
	for <dev@spark.apache.org>; Mon, 21 Jul 2014 13:24:14 -0400
Date: Mon, 21 Jul 2014 13:24:14 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: dev@spark.apache.org
Message-ID: <80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com> <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com> <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com> <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC35 (Linux)/8.0.6_GA_5922)
Thread-Topic: Supporting the Scala drop Method for Spark RDDs
Thread-Index: J2TlbeIHDta2Uue4iBJ0AVk4ntfOcA==
X-Virus-Checked: Checked by ClamAV on apache.org



----- Original Message -----
> Sure, drop() would be useful, but breaking the "transformations are lazy;
> only actions launch jobs" model is abhorrent -- which is not to say that we
> haven't already broken that model for useful operations (cf.
> RangePartitioner, which is used for sorted RDDs), but rather that each such
> exception to the model is a significant source of pain that can be hard to
> work with or work around.

A thought that comes to my mind here is that there are in fact already two categories of transform: ones that are truly lazy, and ones that are not.  A possible option is to embrace that, and commit to documenting the two categories as such, with an obvious bias towards favoring lazy transforms (to paraphrase Churchill, we're down to haggling over the price).
 

> 
> I really wouldn't like to see another such model-breaking transformation
> added to the API.  On the other hand, being able to write transformations
> with dependencies on these kind of "internal" jobs is sometimes very
> useful, so a significant reworking of Spark's Dependency model that would
> allow for lazily running such internal jobs and making the results
> available to subsequent stages may be something worth pursuing.


This seems like a very interesting angle.   I don't have much feel for what a solution would look like, but it sounds as if it would involve caching all operations embodied by RDD transform method code for provisional execution.  I believe that these levels of invocation are currently executed in the master, not executor nodes.


> 
> 
> On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <andrew@andrewash.com> wrote:
> 
> > Personally I'd find the method useful -- I've often had a .csv file with a
> > header row that I want to drop so filter it out, which touches all
> > partitions anyway.  I don't have any comments on the implementation quite
> > yet though.
> >
> >
> > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com> wrote:
> >
> > > A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> > > SPARK-2315:
> > > https://issues.apache.org/jira/browse/SPARK-2315
> > >
> > > Supporting the drop method would make some operations convenient, however
> > > it forces computation of >= 1 partition of the parent RDD, and so it
> > would
> > > behave like a "partial action" that returns an RDD as the result.
> > >
> > > I wrote up a discussion of these trade-offs here:
> > >
> > >
> > http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > >
> >
> 

From dev-return-8472-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 17:29:03 2014
Return-Path: <dev-return-8472-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8E1AA119F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 17:29:03 +0000 (UTC)
Received: (qmail 52782 invoked by uid 500); 21 Jul 2014 17:29:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 52724 invoked by uid 500); 21 Jul 2014 17:29:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 52712 invoked by uid 99); 21 Jul 2014 17:29:02 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 17:29:02 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rnowling@gmail.com designates 209.85.212.181 as permitted sender)
Received: from [209.85.212.181] (HELO mail-wi0-f181.google.com) (209.85.212.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 17:28:58 +0000
Received: by mail-wi0-f181.google.com with SMTP id bs8so4500591wib.8
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 10:28:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ZkOuf6+DN9MBAorQrFTG7GNr3pzmLQZQqr8TCjQz5RQ=;
        b=hi3VVFHMLZ0rBvVShjixnn/H583ZHv8sRgnM+VApD9u+PzbsHlFnrbF6gbVeIvuBQ0
         YbvwKv027fe0SavgSHb7X7qkA7nUqw50oFElr70FFNP1jLDGcshApvSihnx/eCSZyFUp
         f+dG/iXIx21/xPO97IKnZc40ue3oXGzQO370FuWzeG+LAWxSoxXmv4FNE+adkwgKhyCo
         K22LvVFq/O2H/KbjZIcMGahjUIX7vtdbqAcPaLXv+RgUVJbPGV2xI4WJfxePSmy4hc+1
         jby07DbUVKjmqIBs+q9zT2KWM/C5zdwned0JlNlxvCIOPiiVvpLMPE/XVmD9EknpZG1J
         ljiQ==
MIME-Version: 1.0
X-Received: by 10.180.109.168 with SMTP id ht8mr6307429wib.68.1405963716878;
 Mon, 21 Jul 2014 10:28:36 -0700 (PDT)
Received: by 10.194.108.134 with HTTP; Mon, 21 Jul 2014 10:28:36 -0700 (PDT)
In-Reply-To: <CACBYxKL86hVo8ivCS=b4dWS8+bA4xJVdtOdoKj4oJN9s8VR+ew@mail.gmail.com>
References: <CADtDQQLFwCQD9WnaL0s1YuG6D+MK1X0xFwQiOPRiyQ0Mg89BAw@mail.gmail.com>
	<CACBYxKL86hVo8ivCS=b4dWS8+bA4xJVdtOdoKj4oJN9s8VR+ew@mail.gmail.com>
Date: Mon, 21 Jul 2014 13:28:36 -0400
Message-ID: <CADtDQQKeers6M8ek0ZtVCkBwFSRXFiHA+GzYv4hxuHpeCF1r1g@mail.gmail.com>
Subject: Re: Examples have SparkContext improperly labeled?
From: RJ Nowling <rnowling@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for the clarification, Sandy.


On Mon, Jul 21, 2014 at 12:52 PM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:
> Hi RJ,
>
> Spark Shell instantiates a SparkContext for you named "sc".  In other apps,
> the user instantiates it themself and can give the variable whatever name
> they want, e.g. "spark".
>
> -Sandy
>
>
> On Mon, Jul 21, 2014 at 8:36 AM, RJ Nowling <rnowling@gmail.com> wrote:
>
>> Hi all,
>>
>> The examples listed here
>>
>> https://spark.apache.org/examples.html
>>
>> refer to the spark context as "spark" but when running Spark Shell
>> uses "sc" for the SparkContext.
>>
>> Am I missing something?
>>
>> Thanks!
>>
>> RJ
>>
>>
>> --
>> em rnowling@gmail.com
>> c 954.496.2314
>>



-- 
em rnowling@gmail.com
c 954.496.2314

From dev-return-8473-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 18:07:04 2014
Return-Path: <dev-return-8473-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7A54A11C21
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 18:07:04 +0000 (UTC)
Received: (qmail 5467 invoked by uid 500); 21 Jul 2014 18:07:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5410 invoked by uid 500); 21 Jul 2014 18:07:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5396 invoked by uid 99); 21 Jul 2014 18:07:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 18:07:03 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 18:07:00 +0000
Received: by mail-we0-f180.google.com with SMTP id w61so7788655wes.11
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 11:06:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=zXb1bD7x320pB/xXQX4xFHBYd7Y76td3+dSOwr8C6Kc=;
        b=Hgi/QEFaoQaCLe72gJ/snJh9oUYa1+XuLxrWZ9l9VQTwvZg2XjzYHnoiNhNDumyvcs
         EWZQcUzmCCo3OYmGi6/EHgJujo7ef7iyqHhSjCCLYiZWjO0zOXHFaU0IAKWsZwWN7d5Z
         y/RdzzLcRl1F9RRLCJosrP3pXUB8x1FpZzVwfYGg9qXhHOZpNHRR+7X7PVDWRHcSL139
         35D0Tw8Mowo/csi8LMXUL6b9IXwz3XoR/QSryt5fcOf0hpQwRw+S7PM8lpnzV7PbRnro
         HwS8PC//aXPbBZRCurXtAAIJUWWrJRZNCw+zYVNohn7F0jMEa1rZGNwISDejmWV39JR3
         fM4A==
X-Gm-Message-State: ALoCoQlbsCcKMLLYPdyh5i9z458U2q06oEfJkKbCtx+7QXQTe/5H+SK4RidbJCHrOHxpOX5Iow3g
MIME-Version: 1.0
X-Received: by 10.194.71.132 with SMTP id v4mr25794153wju.102.1405965994291;
 Mon, 21 Jul 2014 11:06:34 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Mon, 21 Jul 2014 11:06:34 -0700 (PDT)
In-Reply-To: <80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
	<702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
	<CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
	<CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
	<80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
Date: Mon, 21 Jul 2014 11:06:34 -0700
Message-ID: <CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org, Erik Erlandson <eje@redhat.com>
Content-Type: multipart/alternative; boundary=047d7bfd0d42a808ac04feb7f70c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0d42a808ac04feb7f70c
Content-Type: text/plain; charset=UTF-8

Rather than embrace non-lazy transformations and add more of them, I'd
rather we 1) try to fully characterize the needs that are driving their
creation/usage; and 2) design and implement new Spark abstractions that
will allow us to meet those needs and eliminate existing non-lazy
transformation.  They really mess up things like creation of asynchronous
FutureActions, job cancellation and accounting of job resource usage, etc.,
so I'd rather we seek a way out of the existing hole rather than make it
deeper.


On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com> wrote:

>
>
> ----- Original Message -----
> > Sure, drop() would be useful, but breaking the "transformations are lazy;
> > only actions launch jobs" model is abhorrent -- which is not to say that
> we
> > haven't already broken that model for useful operations (cf.
> > RangePartitioner, which is used for sorted RDDs), but rather that each
> such
> > exception to the model is a significant source of pain that can be hard
> to
> > work with or work around.
>
> A thought that comes to my mind here is that there are in fact already two
> categories of transform: ones that are truly lazy, and ones that are not.
>  A possible option is to embrace that, and commit to documenting the two
> categories as such, with an obvious bias towards favoring lazy transforms
> (to paraphrase Churchill, we're down to haggling over the price).
>
>
> >
> > I really wouldn't like to see another such model-breaking transformation
> > added to the API.  On the other hand, being able to write transformations
> > with dependencies on these kind of "internal" jobs is sometimes very
> > useful, so a significant reworking of Spark's Dependency model that would
> > allow for lazily running such internal jobs and making the results
> > available to subsequent stages may be something worth pursuing.
>
>
> This seems like a very interesting angle.   I don't have much feel for
> what a solution would look like, but it sounds as if it would involve
> caching all operations embodied by RDD transform method code for
> provisional execution.  I believe that these levels of invocation are
> currently executed in the master, not executor nodes.
>
>
> >
> >
> > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <andrew@andrewash.com>
> wrote:
> >
> > > Personally I'd find the method useful -- I've often had a .csv file
> with a
> > > header row that I want to drop so filter it out, which touches all
> > > partitions anyway.  I don't have any comments on the implementation
> quite
> > > yet though.
> > >
> > >
> > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com>
> wrote:
> > >
> > > > A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> > > > SPARK-2315:
> > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > >
> > > > Supporting the drop method would make some operations convenient,
> however
> > > > it forces computation of >= 1 partition of the parent RDD, and so it
> > > would
> > > > behave like a "partial action" that returns an RDD as the result.
> > > >
> > > > I wrote up a discussion of these trade-offs here:
> > > >
> > > >
> > >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > >
> > >
> >
>

--047d7bfd0d42a808ac04feb7f70c--

From dev-return-8474-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 18:36:00 2014
Return-Path: <dev-return-8474-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB2B511DE3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 18:35:59 +0000 (UTC)
Received: (qmail 41209 invoked by uid 500); 21 Jul 2014 18:35:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41148 invoked by uid 500); 21 Jul 2014 18:35:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 41131 invoked by uid 99); 21 Jul 2014 18:35:59 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 18:35:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.218.43 as permitted sender)
Received: from [209.85.218.43] (HELO mail-oi0-f43.google.com) (209.85.218.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 18:35:55 +0000
Received: by mail-oi0-f43.google.com with SMTP id u20so3677733oif.16
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 11:35:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:subject:mime-version:content-type;
        bh=asqklFxm2LiLWyeJbMkUW7ZEdrZmm2Xa/Y0Kz4PzlJg=;
        b=oIDrIVbsdFskyqZmoF5UJP6y7wOPA+LrXYqO7nZjl/mmDyCl1mHfIjwhQAmAgGP9G2
         V3cyCWIB3lKxx6IjQW+NOgvZ4fZn99zc+RIlxpLdPlg7I/WSBWZ26dE18/RAcz1yVXIV
         F6dFBPzeASnJMBfjiRfW/+DIQgmp22nGJhvuCVNWhS3NU7vG9hCvaDyw7OmSOs40QmIt
         tvHcW4TEBPDqXkLqWqMR7xxOkooiPcWf8t7Z6CjP5LbVTkqDghX2MBbvS6lYfgSQl6AZ
         0RPL9ruGu5YR/lpUIH8yhgkqWF/wJ6Bi5RQhK4Fh/b3hV91afVLVKzPsEMJQhibv43hI
         Sthw==
X-Received: by 10.60.158.41 with SMTP id wr9mr40261717oeb.46.1405967727941;
        Mon, 21 Jul 2014 11:35:27 -0700 (PDT)
Received: from [192.168.2.10] (MTRLPQ02-1177746539.sdsl.bell.ca. [70.50.252.107])
        by mx.google.com with ESMTPSA id gp4sm29564016obb.17.2014.07.21.11.35.27
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 21 Jul 2014 11:35:27 -0700 (PDT)
Date: Mon, 21 Jul 2014 14:46:36 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <CC593AAFF6274F70A4428DCC86D5CC5B@gmail.com>
Subject: spark.executor.memory is not applicable when running unit test
 in Jenkins?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53cd600c_a0382c5_1ec"
X-Virus-Checked: Checked by ClamAV on apache.org

--53cd600c_a0382c5_1ec
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

Hi, all =20

I=E2=80=99m running some unit tests for my Spark applications in Jenkins

it seems that even I set spark.executor.memory to 5g, the value I got wit=
h Runtime.getRuntime.maxMemory is still around 1G

Is it saying that Jenkins limit the process to use no more than 1G (by de=
fault)=3F how to change that=3F

Thanks,


-- =20
Nan Zhu


--53cd600c_a0382c5_1ec--


From dev-return-8475-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 19:00:07 2014
Return-Path: <dev-return-8475-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4A1311EE5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 19:00:07 +0000 (UTC)
Received: (qmail 17058 invoked by uid 500); 21 Jul 2014 19:00:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16993 invoked by uid 500); 21 Jul 2014 19:00:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 16978 invoked by uid 99); 21 Jul 2014 19:00:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 19:00:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 19:00:04 +0000
Received: by mail-oi0-f52.google.com with SMTP id h136so3640212oig.25
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 11:59:39 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=8z9DhnbMjCat7gOXPNJ/P+ctIuXXHu1KR3KZG+LCVBk=;
        b=FOaeqtB76xe1PkpMHcjrLq7+IiWIqMWB3Co7YywnSa/FHpQeBI+tloy/BPEo5PkZ0L
         3vVH0q8VUq8fOokpv8XFj97/dMudwQATO/lxCWBIhyM0YWKnUshcNDkppPYvZB0PwCDv
         CWd8SusxXZ2OmU+w2Piyu7ltPXh4OA1q01z+TNxlb9hipZCLn6QrnH2tQstLyFf9xPnN
         e25LhHGfBPo+OGadiK6JWNTIIItMsI0A9AiCazJSt9OdTaPS9oiGcUZKZKwylA61BMe7
         S0pljOomA9iHzufPi0KonHZXBKXIvaf7cZSrTgshpCLr9vN1gFEk72zuEGeeV3JuvfEI
         JVzg==
MIME-Version: 1.0
X-Received: by 10.182.27.225 with SMTP id w1mr40785562obg.64.1405969179276;
 Mon, 21 Jul 2014 11:59:39 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 21 Jul 2014 11:59:39 -0700 (PDT)
Date: Mon, 21 Jul 2014 11:59:39 -0700
Message-ID: <CABPQxsvBUXG52c799ZWwaFLgp8=OdiY0mh3vODN5RUqsnfChMQ@mail.gmail.com>
Subject: SPARK-1199 has been reverted in branch-1.0
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Just a note - there was a fix in branch-1.0 (and Spark 1.0.1) that
introduced a new bug worse than the original one.

https://issues.apache.org/jira/browse/SPARK-1199

The original bug was an issue with case classes defined in the repl.
The fix caused a separate bug which broke most compound statements in
the repl (val x = 1; val y = x + 1).

I've reverted the original SPARK-1199 fix in the 1.0 branch. Since
repl changes are some of the riskier ones in Spark, I'm planning to
leave this fix out of 1.0.X entirely. The final, correct fix for this
will appear in Spark 1.1.

- Patrick

From dev-return-8476-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 20:26:04 2014
Return-Path: <dev-return-8476-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C2D0112BB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 20:26:04 +0000 (UTC)
Received: (qmail 99794 invoked by uid 500); 21 Jul 2014 20:26:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99738 invoked by uid 500); 21 Jul 2014 20:26:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 99725 invoked by uid 99); 21 Jul 2014 20:26:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:26:03 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eerlands@redhat.com designates 209.132.183.37 as permitted sender)
Received: from [209.132.183.37] (HELO mx5-phx2.redhat.com) (209.132.183.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:25:59 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx5-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s6LKPc3C015689
	for <dev@spark.apache.org>; Mon, 21 Jul 2014 16:25:38 -0400
Date: Mon, 21 Jul 2014 16:25:37 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: dev@spark.apache.org
Message-ID: <1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com> <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com> <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com> <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com> <80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com> <CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.11]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC35 (Linux)/8.0.6_GA_5922)
Thread-Topic: Supporting the Scala drop Method for Spark RDDs
Thread-Index: U1c2fxm5zXjHdroVOczKe8a3u9yo/g==
X-Virus-Checked: Checked by ClamAV on apache.org



----- Original Message -----
> Rather than embrace non-lazy transformations and add more of them, I'd
> rather we 1) try to fully characterize the needs that are driving their
> creation/usage; and 2) design and implement new Spark abstractions that
> will allow us to meet those needs and eliminate existing non-lazy
> transformation.  


In the case of drop, obtaining the index of the boundary partition can be viewed as the action forcing compute -- one that happens to be invoked inside of a transform.  The concept of a "lazy action", that is only triggered if the result rdd has compute invoked on it, might be sufficient to restore laziness to the drop transform.   For that matter, I might find some way to make use of Scala lazy values directly and achieve the same goal for drop.



> They really mess up things like creation of asynchronous
> FutureActions, job cancellation and accounting of job resource usage, etc.,
> so I'd rather we seek a way out of the existing hole rather than make it
> deeper.
> 
> 
> On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com> wrote:
> 
> >
> >
> > ----- Original Message -----
> > > Sure, drop() would be useful, but breaking the "transformations are lazy;
> > > only actions launch jobs" model is abhorrent -- which is not to say that
> > we
> > > haven't already broken that model for useful operations (cf.
> > > RangePartitioner, which is used for sorted RDDs), but rather that each
> > such
> > > exception to the model is a significant source of pain that can be hard
> > to
> > > work with or work around.
> >
> > A thought that comes to my mind here is that there are in fact already two
> > categories of transform: ones that are truly lazy, and ones that are not.
> >  A possible option is to embrace that, and commit to documenting the two
> > categories as such, with an obvious bias towards favoring lazy transforms
> > (to paraphrase Churchill, we're down to haggling over the price).
> >
> >
> > >
> > > I really wouldn't like to see another such model-breaking transformation
> > > added to the API.  On the other hand, being able to write transformations
> > > with dependencies on these kind of "internal" jobs is sometimes very
> > > useful, so a significant reworking of Spark's Dependency model that would
> > > allow for lazily running such internal jobs and making the results
> > > available to subsequent stages may be something worth pursuing.
> >
> >
> > This seems like a very interesting angle.   I don't have much feel for
> > what a solution would look like, but it sounds as if it would involve
> > caching all operations embodied by RDD transform method code for
> > provisional execution.  I believe that these levels of invocation are
> > currently executed in the master, not executor nodes.
> >
> >
> > >
> > >
> > > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <andrew@andrewash.com>
> > wrote:
> > >
> > > > Personally I'd find the method useful -- I've often had a .csv file
> > with a
> > > > header row that I want to drop so filter it out, which touches all
> > > > partitions anyway.  I don't have any comments on the implementation
> > quite
> > > > yet though.
> > > >
> > > >
> > > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com>
> > wrote:
> > > >
> > > > > A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> > > > > SPARK-2315:
> > > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > > >
> > > > > Supporting the drop method would make some operations convenient,
> > however
> > > > > it forces computation of >= 1 partition of the parent RDD, and so it
> > > > would
> > > > > behave like a "partial action" that returns an RDD as the result.
> > > > >
> > > > > I wrote up a discussion of these trade-offs here:
> > > > >
> > > > >
> > > >
> > http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > > >
> > > >
> > >
> >
> 

From dev-return-8477-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 20:45:25 2014
Return-Path: <dev-return-8477-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 304DE11356
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 20:45:25 +0000 (UTC)
Received: (qmail 47208 invoked by uid 500); 21 Jul 2014 20:45:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47153 invoked by uid 500); 21 Jul 2014 20:45:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47140 invoked by uid 99); 21 Jul 2014 20:45:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:45:23 +0000
X-ASF-Spam-Status: No, hits=0.9 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_MED,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of keo@eecs.berkeley.edu does not designate 169.229.218.145 as permitted sender)
Received: from [169.229.218.145] (HELO cm04fe.IST.Berkeley.EDU) (169.229.218.145)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:45:17 +0000
Received: from mail-yk0-f179.google.com ([209.85.160.179])
	by cm04fe.ist.berkeley.edu with esmtpsa (TLSv1:RC4-SHA:128)
	(Exim 4.76)
	(auth plain:keo@eecs.berkeley.edu)
	(envelope-from <keo@eecs.berkeley.edu>)
	id 1X9KS7-0002Iu-Fk
	for dev@spark.apache.org; Mon, 21 Jul 2014 13:44:57 -0700
Received: by mail-yk0-f179.google.com with SMTP id 142so4294938ykq.10
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 13:44:55 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.236.53.69 with SMTP id f45mr43332515yhc.53.1405975495046;
 Mon, 21 Jul 2014 13:44:55 -0700 (PDT)
Received: by 10.170.166.212 with HTTP; Mon, 21 Jul 2014 13:44:54 -0700 (PDT)
Date: Mon, 21 Jul 2014 13:44:54 -0700
Message-ID: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
Subject: -1s on pull requests?
From: Kay Ousterhout <keo@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0122971cf1fb6804feba2d00
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122971cf1fb6804feba2d00
Content-Type: text/plain; charset=UTF-8

Hi all,

As the number of committers / contributors on Spark has increased, there
are cases where pull requests get merged before all the review comments
have been addressed. This happens say when one committer points out a
problem with the pull request, and another committer doesn't see the
earlier comment and merges the PR before the comment has been addressed.
 This is especially tricky for pull requests with a large number of
comments, because it can be difficult to notice early comments describing
blocking issues.

This also happens when something accidentally gets merged after the tests
have started but before tests have passed.

Do folks have ideas on how we can handle this issue? Are there other
projects that have good ways of handling this? It looks like for Hadoop,
people can -1 / +1 on the JIRA.

-Kay

--089e0122971cf1fb6804feba2d00--

From dev-return-8478-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 20:45:45 2014
Return-Path: <dev-return-8478-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2411511359
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 20:45:45 +0000 (UTC)
Received: (qmail 48427 invoked by uid 500); 21 Jul 2014 20:45:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48363 invoked by uid 500); 21 Jul 2014 20:45:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48341 invoked by uid 99); 21 Jul 2014 20:45:44 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:45:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:45:42 +0000
Received: by mail-wi0-f172.google.com with SMTP id n3so4841721wiv.5
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 13:45:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=cFNn2l2hDZPhSqL+3HL7O+H74Rb6QZcb23frTQuuzQ4=;
        b=JzeLHm8uwIaJmX/aOe2iEfM2Gvlqg3TpKrm0HX6G6CDBITZCgnZ8vTnBwTo06T1pJA
         1dLfyoDDZF72h0i5MpPdxYauOFT56ZApwj8CzciyKKZvID8rSzHqmQKhm0zDhi6VDlpb
         L2iCSgLGBUkEHJmF2X2MLbWLLBc2OvrHw47eOUbgIONERAOCFxfQ5TRVoWJxPgPmZxHB
         XWqcff9krtTCQa8eIyauH3rKxCob5IWV52VRa4ECR6yegne1DDqhPhHpB9rF0ZyVdnVz
         AuUim5xv8iUeLf1AwA6So7JEav/2H+JbV9BgxDfzWODOdrZE5wGdyxJZDPoaLH2eRRue
         5+Ng==
X-Gm-Message-State: ALoCoQmUSP2nTVMg2ShQlTKwkBs4684To2I8APhPk3U+dPq5UsNPUnVGVq9RZ0BcozESgkQaHs2H
MIME-Version: 1.0
X-Received: by 10.194.7.67 with SMTP id h3mr26123028wja.111.1405975517479;
 Mon, 21 Jul 2014 13:45:17 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Mon, 21 Jul 2014 13:45:17 -0700 (PDT)
In-Reply-To: <1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
	<702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
	<CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
	<CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
	<80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
	<CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
	<1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com>
Date: Mon, 21 Jul 2014 13:45:17 -0700
Message-ID: <CAAsvFP=XOK8mZ0065H=SExLwc_4UJg6uvhDxQHoE4731g3h9=A@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org, Erik Erlandson <eje@redhat.com>
Content-Type: multipart/alternative; boundary=047d7b450aa248604404feba2fb4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b450aa248604404feba2fb4
Content-Type: text/plain; charset=UTF-8

You can find some of the prior, related discussion here:
https://issues.apache.org/jira/browse/SPARK-1021


On Mon, Jul 21, 2014 at 1:25 PM, Erik Erlandson <eje@redhat.com> wrote:

>
>
> ----- Original Message -----
> > Rather than embrace non-lazy transformations and add more of them, I'd
> > rather we 1) try to fully characterize the needs that are driving their
> > creation/usage; and 2) design and implement new Spark abstractions that
> > will allow us to meet those needs and eliminate existing non-lazy
> > transformation.
>
>
> In the case of drop, obtaining the index of the boundary partition can be
> viewed as the action forcing compute -- one that happens to be invoked
> inside of a transform.  The concept of a "lazy action", that is only
> triggered if the result rdd has compute invoked on it, might be sufficient
> to restore laziness to the drop transform.   For that matter, I might find
> some way to make use of Scala lazy values directly and achieve the same
> goal for drop.
>
>
>
> > They really mess up things like creation of asynchronous
> > FutureActions, job cancellation and accounting of job resource usage,
> etc.,
> > so I'd rather we seek a way out of the existing hole rather than make it
> > deeper.
> >
> >
> > On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com> wrote:
> >
> > >
> > >
> > > ----- Original Message -----
> > > > Sure, drop() would be useful, but breaking the "transformations are
> lazy;
> > > > only actions launch jobs" model is abhorrent -- which is not to say
> that
> > > we
> > > > haven't already broken that model for useful operations (cf.
> > > > RangePartitioner, which is used for sorted RDDs), but rather that
> each
> > > such
> > > > exception to the model is a significant source of pain that can be
> hard
> > > to
> > > > work with or work around.
> > >
> > > A thought that comes to my mind here is that there are in fact already
> two
> > > categories of transform: ones that are truly lazy, and ones that are
> not.
> > >  A possible option is to embrace that, and commit to documenting the
> two
> > > categories as such, with an obvious bias towards favoring lazy
> transforms
> > > (to paraphrase Churchill, we're down to haggling over the price).
> > >
> > >
> > > >
> > > > I really wouldn't like to see another such model-breaking
> transformation
> > > > added to the API.  On the other hand, being able to write
> transformations
> > > > with dependencies on these kind of "internal" jobs is sometimes very
> > > > useful, so a significant reworking of Spark's Dependency model that
> would
> > > > allow for lazily running such internal jobs and making the results
> > > > available to subsequent stages may be something worth pursuing.
> > >
> > >
> > > This seems like a very interesting angle.   I don't have much feel for
> > > what a solution would look like, but it sounds as if it would involve
> > > caching all operations embodied by RDD transform method code for
> > > provisional execution.  I believe that these levels of invocation are
> > > currently executed in the master, not executor nodes.
> > >
> > >
> > > >
> > > >
> > > > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <andrew@andrewash.com>
> > > wrote:
> > > >
> > > > > Personally I'd find the method useful -- I've often had a .csv file
> > > with a
> > > > > header row that I want to drop so filter it out, which touches all
> > > > > partitions anyway.  I don't have any comments on the implementation
> > > quite
> > > > > yet though.
> > > > >
> > > > >
> > > > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com>
> > > wrote:
> > > > >
> > > > > > A few weeks ago I submitted a PR for supporting rdd.drop(n),
> under
> > > > > > SPARK-2315:
> > > > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > > > >
> > > > > > Supporting the drop method would make some operations convenient,
> > > however
> > > > > > it forces computation of >= 1 partition of the parent RDD, and
> so it
> > > > > would
> > > > > > behave like a "partial action" that returns an RDD as the result.
> > > > > >
> > > > > > I wrote up a discussion of these trade-offs here:
> > > > > >
> > > > > >
> > > > >
> > >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > > > >
> > > > >
> > > >
> > >
> >
>

--047d7b450aa248604404feba2fb4--

From dev-return-8479-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 20:57:02 2014
Return-Path: <dev-return-8479-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B8BE7113E7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 20:57:02 +0000 (UTC)
Received: (qmail 83192 invoked by uid 500); 21 Jul 2014 20:57:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83126 invoked by uid 500); 21 Jul 2014 20:57:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83110 invoked by uid 99); 21 Jul 2014 20:57:01 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:57:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 74.125.82.46 as permitted sender)
Received: from [74.125.82.46] (HELO mail-wg0-f46.google.com) (74.125.82.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:57:00 +0000
Received: by mail-wg0-f46.google.com with SMTP id m15so6916112wgh.17
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 13:56:35 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=xgMvg+YYiZJ70RcqV8Rk03dhVA5Dv32CbO2j2wEm6RI=;
        b=gPEy1xAz9tb5xLF/3MRCrXDDgxKOLsSLwAmRvyKP56D/UOmC4mbOJSuo9f3j5SP7oS
         Q3OEgnPVsqJxuxb9R0DjkWWYLm36+SInG4mYst4pMyvbJ/ZI0r/JtAg4igcNPOxMl3o8
         HddGu9qfGcCaRVQX3zmX3QTSvdXEsrXI04PdUJfsCOTLmJyBtLkD3RIF+zdlCUjaK2L4
         DH51yDM4emO2FhQuEA3BzJD8xF9QmewVzUxD70v06rarNAAZvV9Y4adE/nhFLLxUz412
         q3UrBNMcj/x/9E6PtLw/8wIvHROXR6XYNqx/iXixOdo46hyEQa4T54inEo1ArLRdpwJW
         l6rg==
X-Gm-Message-State: ALoCoQnBWofpx7uMYO2SIGbShdkZXPfgaylYn0YZzjhnAvCeTqFr3oorVi5NIVe4WdyM8ygI7J74
MIME-Version: 1.0
X-Received: by 10.180.210.167 with SMTP id mv7mr8203252wic.72.1405976195224;
 Mon, 21 Jul 2014 13:56:35 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.217.48.72 with HTTP; Mon, 21 Jul 2014 13:56:35 -0700 (PDT)
In-Reply-To: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
Date: Mon, 21 Jul 2014 13:56:35 -0700
Message-ID: <CAKx7Bf_+dYK_b1zzEfV0MJ4O-n0uizbcCNQGnjkGfvds4H=19Q@mail.gmail.com>
Subject: Re: -1s on pull requests?
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2691eade55e04feba57b8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2691eade55e04feba57b8
Content-Type: text/plain; charset=UTF-8

One way to do this would be to have a Github hook that parses -1s or +1s
and posts a commit status [1] (like say Travis [2]) right next to the PR.
Does anybody know of an existing tool that does this ?

Shivaram

[1] https://github.com/blog/1227-commit-status-api
[2]
http://blog.travis-ci.com/2012-09-04-pull-requests-just-got-even-more-awesome/


On Mon, Jul 21, 2014 at 1:44 PM, Kay Ousterhout <keo@eecs.berkeley.edu>
wrote:

> Hi all,
>
> As the number of committers / contributors on Spark has increased, there
> are cases where pull requests get merged before all the review comments
> have been addressed. This happens say when one committer points out a
> problem with the pull request, and another committer doesn't see the
> earlier comment and merges the PR before the comment has been addressed.
>  This is especially tricky for pull requests with a large number of
> comments, because it can be difficult to notice early comments describing
> blocking issues.
>
> This also happens when something accidentally gets merged after the tests
> have started but before tests have passed.
>
> Do folks have ideas on how we can handle this issue? Are there other
> projects that have good ways of handling this? It looks like for Hadoop,
> people can -1 / +1 on the JIRA.
>
> -Kay
>

--001a11c2691eade55e04feba57b8--

From dev-return-8480-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 20:59:56 2014
Return-Path: <dev-return-8480-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63E61113FC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 20:59:56 +0000 (UTC)
Received: (qmail 89122 invoked by uid 500); 21 Jul 2014 20:59:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89058 invoked by uid 500); 21 Jul 2014 20:59:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89043 invoked by uid 99); 21 Jul 2014 20:59:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:59:55 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 20:59:54 +0000
Received: by mail-oa0-f41.google.com with SMTP id j17so8407515oag.0
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 13:59:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=+VN4vhw9TuCmF6TQ9afPJO7P0YmZ8dFxq0qpQK8cbqo=;
        b=SXoeaCbrqTF0K8r7jwaZXtB4S4cTiPd4WMuojtv6MqshoKqaTDJSohgCQkWFK+tivO
         NoCo9WlRXgmpTVMM0nD3ZLyMHtZ4JyuU0CBwopRLUxyiBj4MO69bSN8SgRcIh6nQcqKZ
         w64bWnx0jHrYJ4hGxzhTN82tqrw8l0SlsdxUwmj7hSMbHWcDiZ+y0Bx/5Q3zOlSJQkLz
         exikYlbH8z32DwKIpGs1NRyXmh+4QnDJhHZk0DqWJ8XNM3j+0VncEHtaLXDgY6J4mc+x
         Iz3g+37hXyq9RLvxfwpS1oOFxtj44Bn5+CcAmSzKlnBMSUCWLRC118+RYh0EmfdEvonb
         Jw8A==
MIME-Version: 1.0
X-Received: by 10.60.158.41 with SMTP id wr9mr41526809oeb.46.1405976368962;
 Mon, 21 Jul 2014 13:59:28 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Mon, 21 Jul 2014 13:59:28 -0700 (PDT)
In-Reply-To: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
Date: Mon, 21 Jul 2014 13:59:28 -0700
Message-ID: <CABPQxsuGAJU0rrkOk8vkSi7k_7FXW+Oogd5wNcV_MOseoD0Zpg@mail.gmail.com>
Subject: Re: -1s on pull requests?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I've always operated under the assumption that if a commiter makes a
comment on a PR, and that's not addressed, that should block the PR
from being merged (even without a specific "-1"). I don't know of any
cases where this has intentionally been violated, but I do think this
happens accidentally some times.

Unfortunately, we are not allowed to use those github hooks because of
the way the ASF github integration works.

I've lately been using a custom-made tool to help review pull
requests. One thing I could do is add a feature here saying which
committers have said LGTM on a PR (vs the ones that have commented).
We could also indicate the latest test status as Green/Yellow/Red
based on the Jenkins comments:

http://pwendell.github.io/spark-github-shim/

As a warning to potential users, my tool might crash your browser.

- Patrick

On Mon, Jul 21, 2014 at 1:44 PM, Kay Ousterhout <keo@eecs.berkeley.edu> wrote:
> Hi all,
>
> As the number of committers / contributors on Spark has increased, there
> are cases where pull requests get merged before all the review comments
> have been addressed. This happens say when one committer points out a
> problem with the pull request, and another committer doesn't see the
> earlier comment and merges the PR before the comment has been addressed.
>  This is especially tricky for pull requests with a large number of
> comments, because it can be difficult to notice early comments describing
> blocking issues.
>
> This also happens when something accidentally gets merged after the tests
> have started but before tests have passed.
>
> Do folks have ideas on how we can handle this issue? Are there other
> projects that have good ways of handling this? It looks like for Hadoop,
> people can -1 / +1 on the JIRA.
>
> -Kay

From dev-return-8481-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 21:10:53 2014
Return-Path: <dev-return-8481-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4FE861148E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 21:10:53 +0000 (UTC)
Received: (qmail 27537 invoked by uid 500); 21 Jul 2014 21:10:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27467 invoked by uid 500); 21 Jul 2014 21:10:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27455 invoked by uid 99); 21 Jul 2014 21:10:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 21:10:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nferguson@gmail.com designates 209.85.192.42 as permitted sender)
Received: from [209.85.192.42] (HELO mail-qg0-f42.google.com) (209.85.192.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 21:10:49 +0000
Received: by mail-qg0-f42.google.com with SMTP id j5so6109326qga.15
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 14:10:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=JEPIidFrFNfuXsEy3EUDURIRpdIQ4nTqzruVJpc6Zs0=;
        b=Pa42zEbpbRXQUidCKIS6nfpTMb2RmOufkLErq+ydtL4eUxNU83BTpgPnOPB4LzX/Es
         0RkB+5slTom+qXauF/IL3BOeypFMWQgPSkk3LEtKOsFFK1hwty8EUFHYzzs/B4MhKUrx
         UCXOJ5/t/e8IhUUwCM/vojIIjaOVYODQihMpvjURRC/ltwoJXuGNxpPErxWI4K5q60dA
         ViL84hFCaPVE7nqOmrDtiAlwGe38wMZgFLQDeLjREh+PribtcuTug+YN+D43iHd2Lf5L
         /Ke/Jtr2ZmQtKEtT0NGGtdiMd5P8xl2yHIqe+AkguDx/CXvs4xM7AjAiEvmOzXzdNhsL
         qhuQ==
MIME-Version: 1.0
X-Received: by 10.224.137.135 with SMTP id w7mr32132670qat.52.1405977025062;
 Mon, 21 Jul 2014 14:10:25 -0700 (PDT)
Received: by 10.140.48.35 with HTTP; Mon, 21 Jul 2014 14:10:25 -0700 (PDT)
Date: Mon, 21 Jul 2014 22:10:25 +0100
Message-ID: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
Subject: "Dynamic variables" in Spark
From: Neil Ferguson <nferguson@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2eb3424301204feba8912
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2eb3424301204feba8912
Content-Type: text/plain; charset=UTF-8

Hi all

I have been adding some metrics to the ADAM project
https://github.com/bigdatagenomics/adam, which runs on Spark, and have a
proposal for an enhancement to Spark that would make this work cleaner and
easier.

I need to pass some Accumulators around, which will aggregate metrics
(timing stats and other metrics) across the cluster. However, it is
cumbersome to have to explicitly pass some "context" containing these
accumulators around everywhere that might need them. I can use Scala
implicits, which help slightly, but I'd still need to modify every method
in the call stack to take an implicit variable.

So, I'd like to propose that we add the ability to have "dynamic variables"
(basically thread-local variables) to Spark. This would avoid having to
pass the Accumulators around explicitly.

My proposed approach is to add a method to the SparkContext class as
follows:

/**
 * Sets the value of a "dynamic variable". This value is made available to
jobs
 * without having to be passed around explicitly. During execution of a
Spark job
 * this value can be obtained from the [[SparkDynamic]] object.
 */
def setDynamicVariableValue(value: Any)

Then, when a job is executing the SparkDynamic can be accessed to obtain
the value of the dynamic variable. The implementation of this object is as
follows:

object SparkDynamic {
  private val dynamicVariable = new DynamicVariable[Any]()
  /**
   * Gets the value of the "dynamic variable" that has been set in the
[[SparkContext]]
   */
  def getValue: Option[Any] = {
    Option(dynamicVariable.value)
  }
  private[spark] def withValue[S](threadValue: Option[Any])(thunk: => S): S
= {
    dynamicVariable.withValue(threadValue.orNull)(thunk)
  }
}

The change involves modifying the Task object to serialize the value of the
dynamic variable, and modifying the TaskRunner class to deserialize the
value and make it available in the thread that is running the task (using
the SparkDynamic.withValue method).

I have done a quick prototype of this in this commit:
https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
and it seems to work fine in my (limited) testing. It needs more testing,
tidy-up and documentation though.

One drawback is that the dynamic variable will be serialized for every Task
whether it needs it or not. For my use case this might not be too much of a
problem, as serializing and deserializing Accumulators looks fairly
lightweight -- however we should certainly warn users against setting a
dynamic variable containing lots of data. I thought about using broadcast
tables here, but I don't think it's possible to put Accumulators in a
broadcast table (as I understand it, they're intended for purely read-only
data).

What do people think about this proposal? My use case aside, it seems like
it would be a generally useful enhancment to be able to pass certain data
around without having to explicitly pass it everywhere.

Neil

--001a11c2eb3424301204feba8912--

From dev-return-8482-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 21:20:04 2014
Return-Path: <dev-return-8482-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB24F1152E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 21:20:04 +0000 (UTC)
Received: (qmail 62252 invoked by uid 500); 21 Jul 2014 21:20:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62191 invoked by uid 500); 21 Jul 2014 21:20:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62179 invoked by uid 99); 21 Jul 2014 21:20:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 21:20:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.50 as permitted sender)
Received: from [74.125.82.50] (HELO mail-wg0-f50.google.com) (74.125.82.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 21:20:01 +0000
Received: by mail-wg0-f50.google.com with SMTP id n12so6949638wgh.21
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 14:19:37 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=3CFTj9N+YLpQx78OK457iWz0nAMEQstKjnMj1kOiTDo=;
        b=fW10D6xnUf1op4LoyfBXRVuLncOgwIyAkeCOGifD70dCHTD4TrzrCyUhAH3xWMkW6P
         r/hKuNzZgIKZAAWNulRV2NZdkD789poU7JOkshvKI9KMMxanLFwzEijRo+7HZyZs8DaH
         x7xr02UDpBvrWbR4jmEv6YYeviHffisOMjZYr3HAW5VVVkMUTd9fk+e90uQhcGu+pQ9f
         qQFdDr3duDCrebXMbni3Fq+7rdBfhiPwTDYaY/vi31A0trsNzL7hDupiERNRv/hDP+s5
         evoaArbCc8K+v9Zk++9NgQ4x2YSdawUG3ajQKadDpnNjyNKASVCrCi9v0ipqqcH4rdZg
         B3KQ==
MIME-Version: 1.0
X-Received: by 10.194.184.237 with SMTP id ex13mr2264946wjc.82.1405977577317;
 Mon, 21 Jul 2014 14:19:37 -0700 (PDT)
Received: by 10.216.130.7 with HTTP; Mon, 21 Jul 2014 14:19:37 -0700 (PDT)
In-Reply-To: <CABPQxsuGAJU0rrkOk8vkSi7k_7FXW+Oogd5wNcV_MOseoD0Zpg@mail.gmail.com>
References: <CAKJXNjHBk3Cj8dr0w4sJjRN7e=iPmUWm2zfnLs7k8UOFSMxuvA@mail.gmail.com>
	<CABPQxsuGAJU0rrkOk8vkSi7k_7FXW+Oogd5wNcV_MOseoD0Zpg@mail.gmail.com>
Date: Mon, 21 Jul 2014 14:19:37 -0700
Message-ID: <CALuGr6bLFTqJ8sW6xMX3aGO8tF4ORvdK2Lj5Ki++6QvFp_x0eA@mail.gmail.com>
Subject: Re: -1s on pull requests?
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

There is ASF guidelines about Voting, including code review for
patches: http://www.apache.org/foundation/voting.html

Some ASF project do three +1 votes are required (to the issues like
JIRA or Github PR in this case) for a patch unless it is tagged with
lazy consensus [1] of like 48 hours.
For patches that are not critical, waiting for a while to let some
time for additional committers to review would be the best way to go.

Another thing is that all contributors need to be patience once their
patches have been submitted and pending reviewed. This is part of
being in open community.

Hope this helps.


- Henry

[1] http://www.apache.org/foundation/glossary.html#LazyConsensus

On Mon, Jul 21, 2014 at 1:59 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> I've always operated under the assumption that if a commiter makes a
> comment on a PR, and that's not addressed, that should block the PR
> from being merged (even without a specific "-1"). I don't know of any
> cases where this has intentionally been violated, but I do think this
> happens accidentally some times.
>
> Unfortunately, we are not allowed to use those github hooks because of
> the way the ASF github integration works.
>
> I've lately been using a custom-made tool to help review pull
> requests. One thing I could do is add a feature here saying which
> committers have said LGTM on a PR (vs the ones that have commented).
> We could also indicate the latest test status as Green/Yellow/Red
> based on the Jenkins comments:
>
> http://pwendell.github.io/spark-github-shim/
>
> As a warning to potential users, my tool might crash your browser.
>
> - Patrick
>
> On Mon, Jul 21, 2014 at 1:44 PM, Kay Ousterhout <keo@eecs.berkeley.edu> wrote:
>> Hi all,
>>
>> As the number of committers / contributors on Spark has increased, there
>> are cases where pull requests get merged before all the review comments
>> have been addressed. This happens say when one committer points out a
>> problem with the pull request, and another committer doesn't see the
>> earlier comment and merges the PR before the comment has been addressed.
>>  This is especially tricky for pull requests with a large number of
>> comments, because it can be difficult to notice early comments describing
>> blocking issues.
>>
>> This also happens when something accidentally gets merged after the tests
>> have started but before tests have passed.
>>
>> Do folks have ideas on how we can handle this issue? Are there other
>> projects that have good ways of handling this? It looks like for Hadoop,
>> people can -1 / +1 on the JIRA.
>>
>> -Kay

From dev-return-8483-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 22:43:39 2014
Return-Path: <dev-return-8483-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3D5E31199E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 22:43:39 +0000 (UTC)
Received: (qmail 75085 invoked by uid 500); 21 Jul 2014 22:43:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75020 invoked by uid 500); 21 Jul 2014 22:43:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75001 invoked by uid 99); 21 Jul 2014 22:43:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 22:43:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 22:43:32 +0000
Received: by mail-wg0-f44.google.com with SMTP id m15so7097708wgh.27
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 15:43:11 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=lMj+B173MFqwOjyNF6OXAfkGR/BL+No+U2JTm+lfonI=;
        b=cZPGFIhan9Qj2zbLvcsTdsmRc+iBRlV526nNVry4bB8hWF/uJS0XHJaoREz9XwB+ev
         icJXA4kjtKnnpQp+Z2lyfIh1G5tzAqwycc+QRuP+z3sacgsrXx8Nwm6236qbqV2GpJRI
         8rIra1Fu260EHjttDqiy1FjZ2pEGwEkZdBcOz6mVJU4OyTnSRg3pWNy/OJjVjZOMQO0R
         sATee61uNB/8YgVNrrXnA9RmWS8T4Rzkyw63h0BFDvtuYo5lyCsCS8+Ig9YaUTI704Lk
         K3ucHO0xfU8SFKrF21/z7P/IdbyQKbI/ETBGXjTKZGluk8ZhhJx3TAEVG1ZaQH4kloZx
         UoXw==
X-Received: by 10.180.208.13 with SMTP id ma13mr8616672wic.45.1405982591772;
 Mon, 21 Jul 2014 15:43:11 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 21 Jul 2014 15:42:31 -0700 (PDT)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 21 Jul 2014 18:42:31 -0400
Message-ID: <CAOhmDzeSt8NVox0PU+24eBKno+NCQ=g61OGZQEjHXR1rdMpNHw@mail.gmail.com>
Subject: Contributing to Spark needs PySpark build/test instructions
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c38d8af1703904febbd48d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c38d8af1703904febbd48d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Contributing to Spark
<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
needs a line or two about building and testing PySpark. A call out of
run-tests, for example, would be helpful for new contributors to PySpark.

Nick
=E2=80=8B

--001a11c38d8af1703904febbd48d--

From dev-return-8484-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 21 22:55:11 2014
Return-Path: <dev-return-8484-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B854C119F4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 21 Jul 2014 22:55:11 +0000 (UTC)
Received: (qmail 10789 invoked by uid 500); 21 Jul 2014 22:55:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10734 invoked by uid 500); 21 Jul 2014 22:55:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 10715 invoked by uid 99); 21 Jul 2014 22:55:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 22:55:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 21 Jul 2014 22:55:06 +0000
Received: by mail-we0-f170.google.com with SMTP id w62so8163582wes.1
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 15:54:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=Cr27PQTloYf8jsq0G7mBQCEDAhx4VkQN0/DPER5mNW0=;
        b=RIp+TXTieQmHJ9p7sby6ZVeXnWMNKLpHlpC5YqWZ0YMFDCykAdi7qCKmuyJrLbEE+J
         H4552HzrwU8TOH4cVS5pH0Wim5PfZsIGKSNDYEYlt/dtcnMW/MrlmG7DSXXe7sqXYA5d
         6vpIYnQSBCMeZYsXqIOJGl1G2c/1CZDF/rm+ZHTJTyVqz2B5E5trjDOwrDEX5782fKj0
         i+kvIJ9HlkP176OotxQMgsBXevEUHzbYFd1HKKQTFTFXvcAEavXAoZseEjyUhyr9+Jmr
         MEIo1DIW8PfmDs2LWUBGKZRRFqUnNoHc4r52kOd7X+EgPQyC1DlHMjRuGILYpfcnS6bL
         m0AQ==
X-Received: by 10.180.21.235 with SMTP id y11mr8450916wie.75.1405983285412;
 Mon, 21 Jul 2014 15:54:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 21 Jul 2014 15:54:05 -0700 (PDT)
In-Reply-To: <CAOhmDzeSt8NVox0PU+24eBKno+NCQ=g61OGZQEjHXR1rdMpNHw@mail.gmail.com>
References: <CAOhmDzeSt8NVox0PU+24eBKno+NCQ=g61OGZQEjHXR1rdMpNHw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 21 Jul 2014 18:54:05 -0400
Message-ID: <CAOhmDzdi6uXa+dh6eTDLAgiF+z44aaArXZZ7OYe-9Bjz_2dd8g@mail.gmail.com>
Subject: Re: Contributing to Spark needs PySpark build/test instructions
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b874e0449892b04febbfe87
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b874e0449892b04febbfe87
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

For the record, the triggering discussion is here
<https://github.com/apache/spark/pull/1505#issuecomment-49671550>. I
assumed that sbt/sbt test covers all the tests required before submitting a
patch, and it appears that it doesn=E2=80=99t.
=E2=80=8B


On Mon, Jul 21, 2014 at 6:42 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Contributing to Spark
> <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark>
> needs a line or two about building and testing PySpark. A call out of
> run-tests, for example, would be helpful for new contributors to PySpark.
>
> Nick
> =E2=80=8B
>

--047d7b874e0449892b04febbfe87--

From dev-return-8485-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 00:54:57 2014
Return-Path: <dev-return-8485-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8CDC111D9C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 00:54:57 +0000 (UTC)
Received: (qmail 69047 invoked by uid 500); 22 Jul 2014 00:54:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68992 invoked by uid 500); 22 Jul 2014 00:54:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68980 invoked by uid 99); 22 Jul 2014 00:54:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 00:54:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ctn@adatao.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 00:54:52 +0000
Received: by mail-ig0-f169.google.com with SMTP id r2so3680937igi.4
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 17:54:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=nfYPIJZd7KZXwlVMpfafNk2/eJSnbKVfveQwYJ6cRu4=;
        b=P2QZCfHRq33v3qQpT6XXyQlvk9Jr50NRSOHereCppRFFisxj+K8nWKFd486mpzKp6u
         lbDECezEVUeEnW182iTaKnvwEIpMZpK9z/Omo286XHOBQaTnIuVLvwrlaviLCroPtWS2
         T49mVvyRxAjYkF3H07SCx3i8d6RB7TDMDZkPw=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=nfYPIJZd7KZXwlVMpfafNk2/eJSnbKVfveQwYJ6cRu4=;
        b=TE6ECq8OdYXY81i7xHZjX+LqNz7D424xZQ/FnYn+/MrZNUnj4uHIpsz1uTkVBuEMCh
         +Ow/+aPrSwAY5uRpMYq3R2mvOI7OTo3Wfm9rUN6Vt6HcUxxoO/nx+kHJh9uKxAqrgU28
         /YBcdYDebRY5KSPN+0fXGU6KMM0BDufDAgIvCrEWtT03e7dISJgX4E2jUYAH1bfJPnPq
         domgkWcq/t75OI2Wr30CJ143nxHExN+mG+F9S8wthyDUK+7fmo5qTxNh0DtLSaS4bXD6
         apCPovsf0GEfcMhm+Zu8Feewx43HQIBWpm9PCqvZV5bYKl4DSyy8O5O5QEhGSAx+m7h6
         JlxQ==
X-Gm-Message-State: ALoCoQmOSWgrRAd0ZBbCNVAC3wXwRrBQDfPf44oxcJEWX70Y0joTLEqIWajd7uIt3vpfnd/bQWg0
X-Received: by 10.42.100.6 with SMTP id y6mr17746853icn.28.1405990470350; Mon,
 21 Jul 2014 17:54:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.71.130 with HTTP; Mon, 21 Jul 2014 17:54:09 -0700 (PDT)
X-Originating-IP: [162.235.126.227]
In-Reply-To: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
References: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
From: Christopher Nguyen <ctn@adatao.com>
Date: Mon, 21 Jul 2014 17:54:09 -0700
Message-ID: <CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf30223ca58b383904febdaa21
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30223ca58b383904febdaa21
Content-Type: text/plain; charset=UTF-8

Hi Neil, first off, I'm generally a sympathetic advocate for making changes
to Spark internals to make it easier/better/faster/more awesome.

In this case, I'm (a) not clear about what you're trying to accomplish, and
(b) a bit worried about the proposed solution.

On (a): it is stated that you want to pass some Accumulators around. Yet
the proposed solution is for some "shared" variable that may be set and
"mapped out" and possibly "reduced back", but without any accompanying
accumulation semantics. And yet it doesn't seem like you only want just the
broadcast property. Can you clarify the problem statement with some
before/after client code examples?

On (b): you're right that adding variables to SparkContext should be done
with caution, as it may have unintended consequences beyond just serdes
payload size. For example, there is a stated intention of supporting
multiple SparkContexts in the future, and this proposed solution can make
it a bigger challenge to do so. Indeed, we had a gut-wrenching call to make
a while back on a subject related to this (see
https://github.com/mesos/spark/pull/779). Furthermore, even in a single
SparkContext application, there may be multiple "clients" (of that
application) whose intent to use the proposed "SparkDynamic" would not
necessarily be coordinated.

So, considering a ratio of a/b (benefit/cost), it's not clear to me that
the benefits are significant enough to warrant the costs. Do I
misunderstand that the benefit is to save one explicit parameter (the
"context") in the signature/closure code?

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen



On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <nferguson@gmail.com> wrote:

> Hi all
>
> I have been adding some metrics to the ADAM project
> https://github.com/bigdatagenomics/adam, which runs on Spark, and have a
> proposal for an enhancement to Spark that would make this work cleaner and
> easier.
>
> I need to pass some Accumulators around, which will aggregate metrics
> (timing stats and other metrics) across the cluster. However, it is
> cumbersome to have to explicitly pass some "context" containing these
> accumulators around everywhere that might need them. I can use Scala
> implicits, which help slightly, but I'd still need to modify every method
> in the call stack to take an implicit variable.
>
> So, I'd like to propose that we add the ability to have "dynamic variables"
> (basically thread-local variables) to Spark. This would avoid having to
> pass the Accumulators around explicitly.
>
> My proposed approach is to add a method to the SparkContext class as
> follows:
>
> /**
>  * Sets the value of a "dynamic variable". This value is made available to
> jobs
>  * without having to be passed around explicitly. During execution of a
> Spark job
>  * this value can be obtained from the [[SparkDynamic]] object.
>  */
> def setDynamicVariableValue(value: Any)
>
> Then, when a job is executing the SparkDynamic can be accessed to obtain
> the value of the dynamic variable. The implementation of this object is as
> follows:
>
> object SparkDynamic {
>   private val dynamicVariable = new DynamicVariable[Any]()
>   /**
>    * Gets the value of the "dynamic variable" that has been set in the
> [[SparkContext]]
>    */
>   def getValue: Option[Any] = {
>     Option(dynamicVariable.value)
>   }
>   private[spark] def withValue[S](threadValue: Option[Any])(thunk: => S): S
> = {
>     dynamicVariable.withValue(threadValue.orNull)(thunk)
>   }
> }
>
> The change involves modifying the Task object to serialize the value of the
> dynamic variable, and modifying the TaskRunner class to deserialize the
> value and make it available in the thread that is running the task (using
> the SparkDynamic.withValue method).
>
> I have done a quick prototype of this in this commit:
>
> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
> and it seems to work fine in my (limited) testing. It needs more testing,
> tidy-up and documentation though.
>
> One drawback is that the dynamic variable will be serialized for every Task
> whether it needs it or not. For my use case this might not be too much of a
> problem, as serializing and deserializing Accumulators looks fairly
> lightweight -- however we should certainly warn users against setting a
> dynamic variable containing lots of data. I thought about using broadcast
> tables here, but I don't think it's possible to put Accumulators in a
> broadcast table (as I understand it, they're intended for purely read-only
> data).
>
> What do people think about this proposal? My use case aside, it seems like
> it would be a generally useful enhancment to be able to pass certain data
> around without having to explicitly pass it everywhere.
>
> Neil
>

--20cf30223ca58b383904febdaa21--

From dev-return-8486-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 01:16:41 2014
Return-Path: <dev-return-8486-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 03F1A11E4A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 01:16:41 +0000 (UTC)
Received: (qmail 18467 invoked by uid 500); 22 Jul 2014 01:16:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18399 invoked by uid 500); 22 Jul 2014 01:16:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18315 invoked by uid 99); 22 Jul 2014 01:16:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 01:16:39 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE
X-Spam-Check-By: apache.org
Received-SPF: unknown (athena.apache.org: error in processing during lookup of taeyun.kim@innowireless.co.kr)
Received: from [59.12.193.45] (HELO MAIL1.innowireless.co.kr) (59.12.193.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 01:16:34 +0000
Received: from INNOC358 (218.154.28.162) by MAIL1.innowireless.co.kr
 (59.12.193.45) with Microsoft SMTP Server id 14.3.195.1; Tue, 22 Jul 2014
 10:14:51 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
Subject: Suggestion for SPARK-1825
Date: Tue, 22 Jul 2014 10:16:07 +0900
Message-ID: <00ae01cfa54a$83a33d30$8ae9b790$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_000_00AF_01CFA595.F38B8170"
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac+lSMMSbHNRcCHBTfCup91Q0pcZOA==
Content-Language: ko
X-Originating-IP: [218.154.28.162]
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_000_00AF_01CFA595.F38B8170
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit

Hi,

 

A couple of month ago, I made a pull request to fix
https://issues.apache.org/jira/browse/SPARK-1825.

My pull request is here: https://github.com/apache/spark/pull/899 

 

But that pull request has problems:

l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.

l  The related Hadoop API is marked as '@Unstable'.

 

Here is an idea to remedy the problems: a new Spark configuration variable.

Maybe it can be named as "spark.yarn.submit.crossplatform".

If it is set to "true"(default is false), the related Spark code can use the
hard-coded strings that is the same as the Hadoop API provides, thus
avoiding compile error on the Hadoop versions below 2.4.0.

 

Can someone implement this feature, if this idea is acceptable?

Currently my knowledge on Spark source code and Scala is limited to
implement it myself.

To the right person, the modification should be trivial.

You can refer to the source code changes of my pull request.

 

Thanks.

 


------=_NextPart_000_00AF_01CFA595.F38B8170--

From dev-return-8487-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 03:43:59 2014
Return-Path: <dev-return-8487-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9C06211180
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 03:43:59 +0000 (UTC)
Received: (qmail 68339 invoked by uid 500); 22 Jul 2014 03:43:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68277 invoked by uid 500); 22 Jul 2014 03:43:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68264 invoked by uid 99); 22 Jul 2014 03:43:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 03:43:58 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 03:43:56 +0000
Received: by mail-qa0-f42.google.com with SMTP id j15so6126677qaq.29
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 20:43:31 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=0goSoj6y9XK6ht2t+nDIy34kVzR6SftKyFL+YrX88GA=;
        b=USQcM658LjLXzB0mIgY8qSf6zzzfgg273haY3lXjpa9pOj73ANropwFgr/LWRTG0J+
         DaqPVKZ3x7G/8sj6IVKWOxiywkJ+u8mVRckE8FnSJdiEayHENRsFRdGhoi0Jc7FamVI+
         /4+BrDVpAF2NA2ryRKQBRZQzN13B/B4bDz0sJYCl+rdf4pTsWq2lAW+oa/4H3bizDs2d
         VYUnIX7W5FH4hLmOAsw0TPseTavsTcGTkbe4jerZ3BqV8ooK4+81XXlqM8nqfwjgdB5F
         oUxpZtedRF/iRKPA+bncg5p7jBvJMoNokYY1FQABRFfFhS+2Rh5MNeF9GwxB6JM0XPa+
         RZFw==
X-Gm-Message-State: ALoCoQlBNDthkAD8Ncr9HCJ4EU31+WGGcK4TpARkRZaiLcUNkdtwVUMSFg1kUjJ/ovQQD1i8Q2lm
X-Received: by 10.224.20.200 with SMTP id g8mr50530757qab.88.1406000611694;
 Mon, 21 Jul 2014 20:43:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 21 Jul 2014 20:43:11 -0700 (PDT)
In-Reply-To: <CAOhmDzdi6uXa+dh6eTDLAgiF+z44aaArXZZ7OYe-9Bjz_2dd8g@mail.gmail.com>
References: <CAOhmDzeSt8NVox0PU+24eBKno+NCQ=g61OGZQEjHXR1rdMpNHw@mail.gmail.com>
 <CAOhmDzdi6uXa+dh6eTDLAgiF+z44aaArXZZ7OYe-9Bjz_2dd8g@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 21 Jul 2014 20:43:11 -0700
Message-ID: <CAPh_B=aPS5EXD3E2VTNFASVEMiVR1cBoBp0NucK6506XK4Q2vA@mail.gmail.com>
Subject: Re: Contributing to Spark needs PySpark build/test instructions
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2a3b003ac8c04fec0074f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a3b003ac8c04fec0074f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I added an automated testing section:
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#Con=
tributingtoSpark-AutomatedTesting

Can you take a look to see if it is what you had in mind?



On Mon, Jul 21, 2014 at 3:54 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> For the record, the triggering discussion is here
> <https://github.com/apache/spark/pull/1505#issuecomment-49671550>. I
> assumed that sbt/sbt test covers all the tests required before submitting=
 a
> patch, and it appears that it doesn=E2=80=99t.
> =E2=80=8B
>
>
> On Mon, Jul 21, 2014 at 6:42 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> > Contributing to Spark
> > <https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spar=
k
> >
> > needs a line or two about building and testing PySpark. A call out of
> > run-tests, for example, would be helpful for new contributors to PySpar=
k.
> >
> > Nick
> > =E2=80=8B
> >
>

--001a11c2a3b003ac8c04fec0074f--

From dev-return-8488-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 04:21:17 2014
Return-Path: <dev-return-8488-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE9BD1124D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 04:21:17 +0000 (UTC)
Received: (qmail 24708 invoked by uid 500); 22 Jul 2014 04:21:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24635 invoked by uid 500); 22 Jul 2014 04:21:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24623 invoked by uid 99); 22 Jul 2014 04:21:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:21:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:21:15 +0000
Received: by mail-wg0-f52.google.com with SMTP id a1so7332639wgh.23
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 21:20:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=PuwVEImzciqbNH/UJcddNEO5FuNKe8MmyJAkTbUl4Y0=;
        b=mC6uEHF9x4ZdFnFGmerbV9i5NX3HD+yxwwAhyGDsySkuNLW7AOwsxU5FrSKCY3PSVt
         3/5WCESQFVzUTQza7nA0ndX/514HSHXLdvDGr93naZVqms4rHzLVqziI6PsrDOFXpzLE
         1islJObjfQZ8+H9LY4eDU3iXkOcH/GaKtqyJVcP+i6SnoL59Bc67c51hSv9qm58t1L0D
         W0SDqPK8IDUYK1I9STETYZ7xpr1tBJMnHMVULOYqL6u5r4YJwbFhEK1GVGKpktp9hgeX
         iyD9YNbnDdCVR57G6sB+UvGSKy88HO0wdvlEIAQua56eZp3OFAVFx0qWy/4hmJimeeql
         hEUA==
X-Received: by 10.180.24.66 with SMTP id s2mr10377777wif.33.1406002850733;
 Mon, 21 Jul 2014 21:20:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 21 Jul 2014 21:20:10 -0700 (PDT)
In-Reply-To: <CAPh_B=aPS5EXD3E2VTNFASVEMiVR1cBoBp0NucK6506XK4Q2vA@mail.gmail.com>
References: <CAOhmDzeSt8NVox0PU+24eBKno+NCQ=g61OGZQEjHXR1rdMpNHw@mail.gmail.com>
 <CAOhmDzdi6uXa+dh6eTDLAgiF+z44aaArXZZ7OYe-9Bjz_2dd8g@mail.gmail.com> <CAPh_B=aPS5EXD3E2VTNFASVEMiVR1cBoBp0NucK6506XK4Q2vA@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 22 Jul 2014 00:20:10 -0400
Message-ID: <CAOhmDzeMnNQuQ8LfOY_Xm4uK-=rBra6bWv_5SF+PivtzJhXXkw@mail.gmail.com>
Subject: Re: Contributing to Spark needs PySpark build/test instructions
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043be1d078a40a04fec08c7f
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043be1d078a40a04fec08c7f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Looks good. Does sbt/sbt test cover the same tests as /dev/run-tests?

I=E2=80=99m looking at step 5 under =E2=80=9CContributing Code=E2=80=9D. So=
meone contributing to
PySpark will want to be directed to run something in addition to (or
instead of) sbt/sbt test, I believe.

Nick
=E2=80=8B


On Mon, Jul 21, 2014 at 11:43 PM, Reynold Xin <rxin@databricks.com> wrote:

> I added an automated testing section:
>
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#C=
ontributingtoSpark-AutomatedTesting
>
> Can you take a look to see if it is what you had in mind?
>
>
>
> On Mon, Jul 21, 2014 at 3:54 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> > For the record, the triggering discussion is here
> > <https://github.com/apache/spark/pull/1505#issuecomment-49671550>. I
> > assumed that sbt/sbt test covers all the tests required before
> submitting a
> > patch, and it appears that it doesn=E2=80=99t.
> > =E2=80=8B
> >
> >
> > On Mon, Jul 21, 2014 at 6:42 PM, Nicholas Chammas <
> > nicholas.chammas@gmail.com> wrote:
> >
> > > Contributing to Spark
> > > <
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> > >
> > > needs a line or two about building and testing PySpark. A call out of
> > > run-tests, for example, would be helpful for new contributors to
> PySpark.
> > >
> > > Nick
> > > =E2=80=8B
> > >
> >
>

--f46d043be1d078a40a04fec08c7f--

From dev-return-8489-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 04:29:19 2014
Return-Path: <dev-return-8489-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2F9FE11272
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 04:29:19 +0000 (UTC)
Received: (qmail 32907 invoked by uid 500); 22 Jul 2014 04:29:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32857 invoked by uid 500); 22 Jul 2014 04:29:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32846 invoked by uid 99); 22 Jul 2014 04:29:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:29:17 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:29:14 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so6393467qgf.6
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 21:28:52 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=FqOvIUqv90/4cOlXlHKuOIC4KNjpzP8ppZmuCR3iYmQ=;
        b=WJVR5mP6Hv27N8O6YYnuXDOry9n4+/iheroggJ2dgzsmBAegIGpOnpjvIifTC34/pn
         ztJoNtPjPb8krzBiG2IlELJf9+e44WJK6UEZy+xRMKaGgSVcmbOAUMKfl2NoN2mXcMPX
         Q0CcP48k+YBrS4WyY3+NqBhwQVgTM5B9xi+6zDODnYYiHGIkqhlb0WnQnwBclLxWyDbP
         YDqH1fJl/On7pAvRFRM4xnh2GDpVG+gTtRlUeLJKBLR4GH/hPM/rD/0J7kzpawd/Org5
         LxciSecZ3gifnj2oLULX+DwWP0xUaaFK70vSeR6twyMLapFguBYIrMOgfdQAAmQkvRlv
         LF4g==
X-Gm-Message-State: ALoCoQnx7QrjQ0/7sUgfILS1mqhhDy9+o/4le0aCHvlUnBztjoH11F5v83DzJ02j+qbj8RWfdqTx
X-Received: by 10.140.25.11 with SMTP id 11mr35395241qgs.9.1406003332384; Mon,
 21 Jul 2014 21:28:52 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 21 Jul 2014 21:28:32 -0700 (PDT)
In-Reply-To: <CAOhmDzeMnNQuQ8LfOY_Xm4uK-=rBra6bWv_5SF+PivtzJhXXkw@mail.gmail.com>
References: <CAOhmDzeSt8NVox0PU+24eBKno+NCQ=g61OGZQEjHXR1rdMpNHw@mail.gmail.com>
 <CAOhmDzdi6uXa+dh6eTDLAgiF+z44aaArXZZ7OYe-9Bjz_2dd8g@mail.gmail.com>
 <CAPh_B=aPS5EXD3E2VTNFASVEMiVR1cBoBp0NucK6506XK4Q2vA@mail.gmail.com> <CAOhmDzeMnNQuQ8LfOY_Xm4uK-=rBra6bWv_5SF+PivtzJhXXkw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 21 Jul 2014 21:28:32 -0700
Message-ID: <CAPh_B=aGEqdhnUtVCPnn+pYNgrUov_O4epqyNfbSaZ7fGtpmHg@mail.gmail.com>
Subject: Re: Contributing to Spark needs PySpark build/test instructions
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c02a8e2e226604fec0a9a7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c02a8e2e226604fec0a9a7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I missed that bullet point. I removed that and just pointed it towards the
instruction.


On Mon, Jul 21, 2014 at 9:20 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> Looks good. Does sbt/sbt test cover the same tests as /dev/run-tests?
>
> I=E2=80=99m looking at step 5 under =E2=80=9CContributing Code=E2=80=9D. =
Someone contributing to
> PySpark will want to be directed to run something in addition to (or
> instead of) sbt/sbt test, I believe.
>
> Nick
> =E2=80=8B
>
>
> On Mon, Jul 21, 2014 at 11:43 PM, Reynold Xin <rxin@databricks.com> wrote=
:
>
> > I added an automated testing section:
> >
> >
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#C=
ontributingtoSpark-AutomatedTesting
> >
> > Can you take a look to see if it is what you had in mind?
> >
> >
> >
> > On Mon, Jul 21, 2014 at 3:54 PM, Nicholas Chammas <
> > nicholas.chammas@gmail.com> wrote:
> >
> > > For the record, the triggering discussion is here
> > > <https://github.com/apache/spark/pull/1505#issuecomment-49671550>. I
> > > assumed that sbt/sbt test covers all the tests required before
> > submitting a
> > > patch, and it appears that it doesn=E2=80=99t.
> > > =E2=80=8B
> > >
> > >
> > > On Mon, Jul 21, 2014 at 6:42 PM, Nicholas Chammas <
> > > nicholas.chammas@gmail.com> wrote:
> > >
> > > > Contributing to Spark
> > > > <
> > https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> > > >
> > > > needs a line or two about building and testing PySpark. A call out =
of
> > > > run-tests, for example, would be helpful for new contributors to
> > PySpark.
> > > >
> > > > Nick
> > > > =E2=80=8B
> > > >
> > >
> >
>

--001a11c02a8e2e226604fec0a9a7--

From dev-return-8490-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 04:39:49 2014
Return-Path: <dev-return-8490-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B4806112A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 04:39:49 +0000 (UTC)
Received: (qmail 53655 invoked by uid 500); 22 Jul 2014 04:39:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53596 invoked by uid 500); 22 Jul 2014 04:39:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 53585 invoked by uid 99); 22 Jul 2014 04:39:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:39:48 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:39:45 +0000
Received: by mail-qg0-f49.google.com with SMTP id j107so6271090qga.8
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 21:39:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=tpOnra+I6wSWJd9vZ771mHYJJ22FuHJ6XrIxBN13YlY=;
        b=OaJGznTSJpVI8mol6numnW4zjw8p9N0ZEiei5hIc0ofGx7YbadWNrXZpHtiQl1lEOp
         5D6sRhSfFquYw6l98T2fNwidt9FJmqdGgKtnyVvXppZcAT8WGDNxS5Xe3UwgV0mVBExB
         5xN4Lob1yuS/4lIDQHpAcfHADg2eWhQnGG/o3CPSUWUnzEqJVSY1P8akmYvrc51GKw9I
         V+EkGP68QMD2Nf06PhjCy4oY369p1YLvf74s/dgfBND202lMAp+ZCLLg8fTyYXi7SmB7
         hSDEvXYcxM7uSZkG+aMSzHX5AyP5MA1QOFxwDeNfwvY6jG7Y2Mid4BqCJV1tZM2MpKWN
         QVnQ==
X-Gm-Message-State: ALoCoQlfm0c4H5IVAF4uSzC7Vv2cF3hQ9YoWKvsTQ1HYm1tuPD6Kq7AlwwrwsYQVg2Q3dPczZ0+Y
X-Received: by 10.229.211.74 with SMTP id gn10mr2281498qcb.31.1406003964165;
 Mon, 21 Jul 2014 21:39:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 21 Jul 2014 21:39:04 -0700 (PDT)
In-Reply-To: <CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
References: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
 <CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 21 Jul 2014 21:39:04 -0700
Message-ID: <CAPh_B=bERSLwz47h0UbWFw3T53hwoBvMCWDa2Pp1qXJ4BdZNDQ@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a1132ec8ad657ea04fec0ce71
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1132ec8ad657ea04fec0ce71
Content-Type: text/plain; charset=UTF-8

Thanks for the thoughtful email, Neil and Christopher.

If I understand this correctly, it seems like the dynamic variable is just
a variant of the accumulator (a static one since it is a global object).
Accumulators are already implemented using thread-local variables under the
hood. Am I misunderstanding something?



On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.com> wrote:

> Hi Neil, first off, I'm generally a sympathetic advocate for making changes
> to Spark internals to make it easier/better/faster/more awesome.
>
> In this case, I'm (a) not clear about what you're trying to accomplish, and
> (b) a bit worried about the proposed solution.
>
> On (a): it is stated that you want to pass some Accumulators around. Yet
> the proposed solution is for some "shared" variable that may be set and
> "mapped out" and possibly "reduced back", but without any accompanying
> accumulation semantics. And yet it doesn't seem like you only want just the
> broadcast property. Can you clarify the problem statement with some
> before/after client code examples?
>
> On (b): you're right that adding variables to SparkContext should be done
> with caution, as it may have unintended consequences beyond just serdes
> payload size. For example, there is a stated intention of supporting
> multiple SparkContexts in the future, and this proposed solution can make
> it a bigger challenge to do so. Indeed, we had a gut-wrenching call to make
> a while back on a subject related to this (see
> https://github.com/mesos/spark/pull/779). Furthermore, even in a single
> SparkContext application, there may be multiple "clients" (of that
> application) whose intent to use the proposed "SparkDynamic" would not
> necessarily be coordinated.
>
> So, considering a ratio of a/b (benefit/cost), it's not clear to me that
> the benefits are significant enough to warrant the costs. Do I
> misunderstand that the benefit is to save one explicit parameter (the
> "context") in the signature/closure code?
>
> --
> Christopher T. Nguyen
> Co-founder & CEO, Adatao <http://adatao.com>
> linkedin.com/in/ctnguyen
>
>
>
> On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <nferguson@gmail.com>
> wrote:
>
> > Hi all
> >
> > I have been adding some metrics to the ADAM project
> > https://github.com/bigdatagenomics/adam, which runs on Spark, and have a
> > proposal for an enhancement to Spark that would make this work cleaner
> and
> > easier.
> >
> > I need to pass some Accumulators around, which will aggregate metrics
> > (timing stats and other metrics) across the cluster. However, it is
> > cumbersome to have to explicitly pass some "context" containing these
> > accumulators around everywhere that might need them. I can use Scala
> > implicits, which help slightly, but I'd still need to modify every method
> > in the call stack to take an implicit variable.
> >
> > So, I'd like to propose that we add the ability to have "dynamic
> variables"
> > (basically thread-local variables) to Spark. This would avoid having to
> > pass the Accumulators around explicitly.
> >
> > My proposed approach is to add a method to the SparkContext class as
> > follows:
> >
> > /**
> >  * Sets the value of a "dynamic variable". This value is made available
> to
> > jobs
> >  * without having to be passed around explicitly. During execution of a
> > Spark job
> >  * this value can be obtained from the [[SparkDynamic]] object.
> >  */
> > def setDynamicVariableValue(value: Any)
> >
> > Then, when a job is executing the SparkDynamic can be accessed to obtain
> > the value of the dynamic variable. The implementation of this object is
> as
> > follows:
> >
> > object SparkDynamic {
> >   private val dynamicVariable = new DynamicVariable[Any]()
> >   /**
> >    * Gets the value of the "dynamic variable" that has been set in the
> > [[SparkContext]]
> >    */
> >   def getValue: Option[Any] = {
> >     Option(dynamicVariable.value)
> >   }
> >   private[spark] def withValue[S](threadValue: Option[Any])(thunk: =>
> S): S
> > = {
> >     dynamicVariable.withValue(threadValue.orNull)(thunk)
> >   }
> > }
> >
> > The change involves modifying the Task object to serialize the value of
> the
> > dynamic variable, and modifying the TaskRunner class to deserialize the
> > value and make it available in the thread that is running the task (using
> > the SparkDynamic.withValue method).
> >
> > I have done a quick prototype of this in this commit:
> >
> >
> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
> > and it seems to work fine in my (limited) testing. It needs more testing,
> > tidy-up and documentation though.
> >
> > One drawback is that the dynamic variable will be serialized for every
> Task
> > whether it needs it or not. For my use case this might not be too much
> of a
> > problem, as serializing and deserializing Accumulators looks fairly
> > lightweight -- however we should certainly warn users against setting a
> > dynamic variable containing lots of data. I thought about using broadcast
> > tables here, but I don't think it's possible to put Accumulators in a
> > broadcast table (as I understand it, they're intended for purely
> read-only
> > data).
> >
> > What do people think about this proposal? My use case aside, it seems
> like
> > it would be a generally useful enhancment to be able to pass certain data
> > around without having to explicitly pass it everywhere.
> >
> > Neil
> >
>

--001a1132ec8ad657ea04fec0ce71--

From dev-return-8491-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 04:45:53 2014
Return-Path: <dev-return-8491-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6CF9B112B1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 04:45:53 +0000 (UTC)
Received: (qmail 60284 invoked by uid 500); 22 Jul 2014 04:45:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60228 invoked by uid 500); 22 Jul 2014 04:45:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60215 invoked by uid 99); 22 Jul 2014 04:45:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:45:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.174 as permitted sender)
Received: from [74.125.82.174] (HELO mail-we0-f174.google.com) (74.125.82.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 04:45:48 +0000
Received: by mail-we0-f174.google.com with SMTP id x48so8534203wes.5
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 21:45:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=3CeoRiR74SnnX1SabumsV/keMGU5I6679JN2hfkZbJ0=;
        b=lgmJMvem6UQpLL+UGXIH/HgukZMy+FIbF+JYYuB1y7XsJ4Ia/6wW5UP2ZZ7Ou62ooI
         M9rA3WDbCsi0mJ78SFGSRCH4Zaqlq+i8vZZctj5X1VNZWm7m9EgVD+h10r09Y4fjQD1e
         hHcOvP3a7ThI0awNkX3TYNxA5CtODjm1zf2YtenGm6qDbLs1KLmuGhTwJmUzu5vunyIV
         FNCAJNcFx+ii/ROuC04WgpPnjTCbAojvrmYvsXmDMz9k3tjT81z3ZUAoWNE4guN3uB8M
         ygmcQqyy0NYK3oqoFkbBlD1ryhs6CAKxqEm47+qMQdnXsIBTBoTkDMIYGSCc5+EeHwkM
         LfRg==
X-Received: by 10.194.6.134 with SMTP id b6mr30477490wja.64.1406004327354;
 Mon, 21 Jul 2014 21:45:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 21 Jul 2014 21:44:47 -0700 (PDT)
In-Reply-To: <CAPh_B=aGEqdhnUtVCPnn+pYNgrUov_O4epqyNfbSaZ7fGtpmHg@mail.gmail.com>
References: <CAOhmDzeSt8NVox0PU+24eBKno+NCQ=g61OGZQEjHXR1rdMpNHw@mail.gmail.com>
 <CAOhmDzdi6uXa+dh6eTDLAgiF+z44aaArXZZ7OYe-9Bjz_2dd8g@mail.gmail.com>
 <CAPh_B=aPS5EXD3E2VTNFASVEMiVR1cBoBp0NucK6506XK4Q2vA@mail.gmail.com>
 <CAOhmDzeMnNQuQ8LfOY_Xm4uK-=rBra6bWv_5SF+PivtzJhXXkw@mail.gmail.com> <CAPh_B=aGEqdhnUtVCPnn+pYNgrUov_O4epqyNfbSaZ7fGtpmHg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 22 Jul 2014 00:44:47 -0400
Message-ID: <CAOhmDzfeSqxWRWBRbkHs4NT=aWbEVxFrm4R5QuPM1TK8XMNhkQ@mail.gmail.com>
Subject: Re: Contributing to Spark needs PySpark build/test instructions
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b4501167c163b04fec0e4d5
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b4501167c163b04fec0e4d5
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

That works! Thank you.


On Tue, Jul 22, 2014 at 12:28 AM, Reynold Xin <rxin@databricks.com> wrote:

> I missed that bullet point. I removed that and just pointed it towards th=
e
> instruction.
>
>
> On Mon, Jul 21, 2014 at 9:20 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
> > Looks good. Does sbt/sbt test cover the same tests as /dev/run-tests?
> >
> > I=E2=80=99m looking at step 5 under =E2=80=9CContributing Code=E2=80=9D=
. Someone contributing to
> > PySpark will want to be directed to run something in addition to (or
> > instead of) sbt/sbt test, I believe.
> >
> > Nick
> > =E2=80=8B
> >
> >
> > On Mon, Jul 21, 2014 at 11:43 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> > > I added an automated testing section:
> > >
> > >
> >
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#C=
ontributingtoSpark-AutomatedTesting
> > >
> > > Can you take a look to see if it is what you had in mind?
> > >
> > >
> > >
> > > On Mon, Jul 21, 2014 at 3:54 PM, Nicholas Chammas <
> > > nicholas.chammas@gmail.com> wrote:
> > >
> > > > For the record, the triggering discussion is here
> > > > <https://github.com/apache/spark/pull/1505#issuecomment-49671550>. =
I
> > > > assumed that sbt/sbt test covers all the tests required before
> > > submitting a
> > > > patch, and it appears that it doesn=E2=80=99t.
> > > > =E2=80=8B
> > > >
> > > >
> > > > On Mon, Jul 21, 2014 at 6:42 PM, Nicholas Chammas <
> > > > nicholas.chammas@gmail.com> wrote:
> > > >
> > > > > Contributing to Spark
> > > > > <
> > >
> https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
> > > > >
> > > > > needs a line or two about building and testing PySpark. A call ou=
t
> of
> > > > > run-tests, for example, would be helpful for new contributors to
> > > PySpark.
> > > > >
> > > > > Nick
> > > > > =E2=80=8B
> > > > >
> > > >
> > >
> >
>

--047d7b4501167c163b04fec0e4d5--

From dev-return-8492-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 05:37:52 2014
Return-Path: <dev-return-8492-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B99EB113DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 05:37:52 +0000 (UTC)
Received: (qmail 60480 invoked by uid 500); 22 Jul 2014 05:37:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60425 invoked by uid 500); 22 Jul 2014 05:37:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60413 invoked by uid 99); 22 Jul 2014 05:37:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 05:37:51 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.182] (HELO mail-qc0-f182.google.com) (209.85.216.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 05:37:50 +0000
Received: by mail-qc0-f182.google.com with SMTP id r5so5949985qcx.27
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 22:37:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=1YI92ajHf/SiTE0ybC5vNVgFjKkp83fSjPjLpWSzleY=;
        b=jkRm9a81VJMW3Hxvpb3pFMA7ezfXJ7EfPJOrB6cQU9DlfTkzlaroet6eO2jeNREA40
         i4uc2wnnpEs4hB8dIJ9KPn+5G/j4U8jvyJlUF2Ts6bVkjNQxSv5+48Z+oayhtA6nW3E6
         e/JxscOeaR/bni8vHE6/5H6iUg9byt44P16dhv3zdd7KKFEwsZZZnJ1mno7BbWy1h1Rd
         p3nXzwEtT3lpcV3eg86E0P8m+9GS/gF2BqGpt7ZdXtS9uo5QEegMSE922ekBlzCiZsuD
         a+gX/dZd7Zu4hwmDimaJGWnYLPB7v6lzfTDMq8j22sJUqMQyCW8ud4ZuznQ006icSRxp
         enjg==
X-Gm-Message-State: ALoCoQlcKyhA3x4XzGb1fMoWMzN3jDh3t4VRkPG4DArezmpMjraMpqo7XnztA5j3mgn7yoEJKzY9
X-Received: by 10.224.20.200 with SMTP id g8mr51363645qab.88.1406007445038;
 Mon, 21 Jul 2014 22:37:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 21 Jul 2014 22:37:04 -0700 (PDT)
In-Reply-To: <CAAsvFP=XOK8mZ0065H=SExLwc_4UJg6uvhDxQHoE4731g3h9=A@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
 <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
 <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
 <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
 <80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
 <CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
 <1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com> <CAAsvFP=XOK8mZ0065H=SExLwc_4UJg6uvhDxQHoE4731g3h9=A@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 21 Jul 2014 22:37:04 -0700
Message-ID: <CAPh_B=aq1yiU57T+g_DSx-8H5HNVq2ZOthetATqM0e6WMz5OuA@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Erik Erlandson <eje@redhat.com>
Content-Type: multipart/alternative; boundary=001a11c2a3b0503c3604fec19e6c
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a3b0503c3604fec19e6c
Content-Type: text/plain; charset=UTF-8

If the purpose is for dropping csv headers, perhaps we don't really need a
common drop and only one that drops the first line in a file? I'd really
try hard to avoid a common drop/dropWhile because they can be expensive to
do.

Note that I think we will be adding this functionality (ignoring headers)
to the CsvRDD functionality in Spark SQL.
 https://github.com/apache/spark/pull/1351


On Mon, Jul 21, 2014 at 1:45 PM, Mark Hamstra <mark@clearstorydata.com>
wrote:

> You can find some of the prior, related discussion here:
> https://issues.apache.org/jira/browse/SPARK-1021
>
>
> On Mon, Jul 21, 2014 at 1:25 PM, Erik Erlandson <eje@redhat.com> wrote:
>
> >
> >
> > ----- Original Message -----
> > > Rather than embrace non-lazy transformations and add more of them, I'd
> > > rather we 1) try to fully characterize the needs that are driving their
> > > creation/usage; and 2) design and implement new Spark abstractions that
> > > will allow us to meet those needs and eliminate existing non-lazy
> > > transformation.
> >
> >
> > In the case of drop, obtaining the index of the boundary partition can be
> > viewed as the action forcing compute -- one that happens to be invoked
> > inside of a transform.  The concept of a "lazy action", that is only
> > triggered if the result rdd has compute invoked on it, might be
> sufficient
> > to restore laziness to the drop transform.   For that matter, I might
> find
> > some way to make use of Scala lazy values directly and achieve the same
> > goal for drop.
> >
> >
> >
> > > They really mess up things like creation of asynchronous
> > > FutureActions, job cancellation and accounting of job resource usage,
> > etc.,
> > > so I'd rather we seek a way out of the existing hole rather than make
> it
> > > deeper.
> > >
> > >
> > > On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com>
> wrote:
> > >
> > > >
> > > >
> > > > ----- Original Message -----
> > > > > Sure, drop() would be useful, but breaking the "transformations are
> > lazy;
> > > > > only actions launch jobs" model is abhorrent -- which is not to say
> > that
> > > > we
> > > > > haven't already broken that model for useful operations (cf.
> > > > > RangePartitioner, which is used for sorted RDDs), but rather that
> > each
> > > > such
> > > > > exception to the model is a significant source of pain that can be
> > hard
> > > > to
> > > > > work with or work around.
> > > >
> > > > A thought that comes to my mind here is that there are in fact
> already
> > two
> > > > categories of transform: ones that are truly lazy, and ones that are
> > not.
> > > >  A possible option is to embrace that, and commit to documenting the
> > two
> > > > categories as such, with an obvious bias towards favoring lazy
> > transforms
> > > > (to paraphrase Churchill, we're down to haggling over the price).
> > > >
> > > >
> > > > >
> > > > > I really wouldn't like to see another such model-breaking
> > transformation
> > > > > added to the API.  On the other hand, being able to write
> > transformations
> > > > > with dependencies on these kind of "internal" jobs is sometimes
> very
> > > > > useful, so a significant reworking of Spark's Dependency model that
> > would
> > > > > allow for lazily running such internal jobs and making the results
> > > > > available to subsequent stages may be something worth pursuing.
> > > >
> > > >
> > > > This seems like a very interesting angle.   I don't have much feel
> for
> > > > what a solution would look like, but it sounds as if it would involve
> > > > caching all operations embodied by RDD transform method code for
> > > > provisional execution.  I believe that these levels of invocation are
> > > > currently executed in the master, not executor nodes.
> > > >
> > > >
> > > > >
> > > > >
> > > > > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <andrew@andrewash.com>
> > > > wrote:
> > > > >
> > > > > > Personally I'd find the method useful -- I've often had a .csv
> file
> > > > with a
> > > > > > header row that I want to drop so filter it out, which touches
> all
> > > > > > partitions anyway.  I don't have any comments on the
> implementation
> > > > quite
> > > > > > yet though.
> > > > > >
> > > > > >
> > > > > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com>
> > > > wrote:
> > > > > >
> > > > > > > A few weeks ago I submitted a PR for supporting rdd.drop(n),
> > under
> > > > > > > SPARK-2315:
> > > > > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > > > > >
> > > > > > > Supporting the drop method would make some operations
> convenient,
> > > > however
> > > > > > > it forces computation of >= 1 partition of the parent RDD, and
> > so it
> > > > > > would
> > > > > > > behave like a "partial action" that returns an RDD as the
> result.
> > > > > > >
> > > > > > > I wrote up a discussion of these trade-offs here:
> > > > > > >
> > > > > > >
> > > > > >
> > > >
> >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
>

--001a11c2a3b0503c3604fec19e6c--

From dev-return-8493-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 05:50:50 2014
Return-Path: <dev-return-8493-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 02A2C11443
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 05:50:50 +0000 (UTC)
Received: (qmail 86092 invoked by uid 500); 22 Jul 2014 05:50:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86038 invoked by uid 500); 22 Jul 2014 05:50:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86025 invoked by uid 99); 22 Jul 2014 05:50:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 05:50:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 05:50:45 +0000
Received: by mail-qg0-f45.google.com with SMTP id f51so6378078qge.18
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 22:50:24 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=ehkaMWjCf6Zmey2gkq5nHiqTqQCNmjHuJuk+cjkw1VI=;
        b=UoOwXhw1JuddtUuhaeCapQLBjkWGHp4mPa9jSeKiMLnzEA8VMfWOi0tXE3Gi4bcg0C
         rSWjZDgydsOVysJueaN+NbdG3GifqKb7aoySja3xwiXXh+wGZ39MkDBUI9fKbzaYrO7v
         S9H3wLpXVwqKmduvpBlcXzK1FQvQiQb5G1e8BF2/XILMxDMccxiVvP6zuv2NCCpYlZT/
         qEMUMr67ZrMiG0lH9XjHTSsPliLVfY7rz/uAxQMfo0+EskdsJSykGQxxh7hVgU5ONcrc
         lyyvq87ucDq3FoQrr3khHXu/eANcoorTecfOGqhuhTzlqPD7bovLKttyu3iEpIzUBp+e
         L7Yg==
X-Gm-Message-State: ALoCoQl8dncRRNomsRBaVlDF8gxBFRrm1AbldU+7uGABMntrc7q8gLv+N5z6gA4+S/ImNBBl7TLb
MIME-Version: 1.0
X-Received: by 10.224.43.196 with SMTP id x4mr25223342qae.63.1406008224545;
 Mon, 21 Jul 2014 22:50:24 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Mon, 21 Jul 2014 22:50:24 -0700 (PDT)
In-Reply-To: <CAPh_B=aq1yiU57T+g_DSx-8H5HNVq2ZOthetATqM0e6WMz5OuA@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
	<702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
	<CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
	<CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
	<80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
	<CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
	<1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com>
	<CAAsvFP=XOK8mZ0065H=SExLwc_4UJg6uvhDxQHoE4731g3h9=A@mail.gmail.com>
	<CAPh_B=aq1yiU57T+g_DSx-8H5HNVq2ZOthetATqM0e6WMz5OuA@mail.gmail.com>
Date: Mon, 21 Jul 2014 22:50:24 -0700
Message-ID: <CACBYxKKLq98wB0-612QEu1zP+NU53pUrkv-rH9KG03zxoFPPVw@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Erik Erlandson <eje@redhat.com>
Content-Type: multipart/alternative; boundary=047d7bdc875ec6934e04fec1cc1b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc875ec6934e04fec1cc1b
Content-Type: text/plain; charset=UTF-8

It could make sense to add a skipHeader argument to SparkContext.textFile?


On Mon, Jul 21, 2014 at 10:37 PM, Reynold Xin <rxin@databricks.com> wrote:

> If the purpose is for dropping csv headers, perhaps we don't really need a
> common drop and only one that drops the first line in a file? I'd really
> try hard to avoid a common drop/dropWhile because they can be expensive to
> do.
>
> Note that I think we will be adding this functionality (ignoring headers)
> to the CsvRDD functionality in Spark SQL.
>  https://github.com/apache/spark/pull/1351
>
>
> On Mon, Jul 21, 2014 at 1:45 PM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
>
> > You can find some of the prior, related discussion here:
> > https://issues.apache.org/jira/browse/SPARK-1021
> >
> >
> > On Mon, Jul 21, 2014 at 1:25 PM, Erik Erlandson <eje@redhat.com> wrote:
> >
> > >
> > >
> > > ----- Original Message -----
> > > > Rather than embrace non-lazy transformations and add more of them,
> I'd
> > > > rather we 1) try to fully characterize the needs that are driving
> their
> > > > creation/usage; and 2) design and implement new Spark abstractions
> that
> > > > will allow us to meet those needs and eliminate existing non-lazy
> > > > transformation.
> > >
> > >
> > > In the case of drop, obtaining the index of the boundary partition can
> be
> > > viewed as the action forcing compute -- one that happens to be invoked
> > > inside of a transform.  The concept of a "lazy action", that is only
> > > triggered if the result rdd has compute invoked on it, might be
> > sufficient
> > > to restore laziness to the drop transform.   For that matter, I might
> > find
> > > some way to make use of Scala lazy values directly and achieve the same
> > > goal for drop.
> > >
> > >
> > >
> > > > They really mess up things like creation of asynchronous
> > > > FutureActions, job cancellation and accounting of job resource usage,
> > > etc.,
> > > > so I'd rather we seek a way out of the existing hole rather than make
> > it
> > > > deeper.
> > > >
> > > >
> > > > On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com>
> > wrote:
> > > >
> > > > >
> > > > >
> > > > > ----- Original Message -----
> > > > > > Sure, drop() would be useful, but breaking the "transformations
> are
> > > lazy;
> > > > > > only actions launch jobs" model is abhorrent -- which is not to
> say
> > > that
> > > > > we
> > > > > > haven't already broken that model for useful operations (cf.
> > > > > > RangePartitioner, which is used for sorted RDDs), but rather that
> > > each
> > > > > such
> > > > > > exception to the model is a significant source of pain that can
> be
> > > hard
> > > > > to
> > > > > > work with or work around.
> > > > >
> > > > > A thought that comes to my mind here is that there are in fact
> > already
> > > two
> > > > > categories of transform: ones that are truly lazy, and ones that
> are
> > > not.
> > > > >  A possible option is to embrace that, and commit to documenting
> the
> > > two
> > > > > categories as such, with an obvious bias towards favoring lazy
> > > transforms
> > > > > (to paraphrase Churchill, we're down to haggling over the price).
> > > > >
> > > > >
> > > > > >
> > > > > > I really wouldn't like to see another such model-breaking
> > > transformation
> > > > > > added to the API.  On the other hand, being able to write
> > > transformations
> > > > > > with dependencies on these kind of "internal" jobs is sometimes
> > very
> > > > > > useful, so a significant reworking of Spark's Dependency model
> that
> > > would
> > > > > > allow for lazily running such internal jobs and making the
> results
> > > > > > available to subsequent stages may be something worth pursuing.
> > > > >
> > > > >
> > > > > This seems like a very interesting angle.   I don't have much feel
> > for
> > > > > what a solution would look like, but it sounds as if it would
> involve
> > > > > caching all operations embodied by RDD transform method code for
> > > > > provisional execution.  I believe that these levels of invocation
> are
> > > > > currently executed in the master, not executor nodes.
> > > > >
> > > > >
> > > > > >
> > > > > >
> > > > > > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <
> andrew@andrewash.com>
> > > > > wrote:
> > > > > >
> > > > > > > Personally I'd find the method useful -- I've often had a .csv
> > file
> > > > > with a
> > > > > > > header row that I want to drop so filter it out, which touches
> > all
> > > > > > > partitions anyway.  I don't have any comments on the
> > implementation
> > > > > quite
> > > > > > > yet though.
> > > > > > >
> > > > > > >
> > > > > > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <
> eje@redhat.com>
> > > > > wrote:
> > > > > > >
> > > > > > > > A few weeks ago I submitted a PR for supporting rdd.drop(n),
> > > under
> > > > > > > > SPARK-2315:
> > > > > > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > > > > > >
> > > > > > > > Supporting the drop method would make some operations
> > convenient,
> > > > > however
> > > > > > > > it forces computation of >= 1 partition of the parent RDD,
> and
> > > so it
> > > > > > > would
> > > > > > > > behave like a "partial action" that returns an RDD as the
> > result.
> > > > > > > >
> > > > > > > > I wrote up a discussion of these trade-offs here:
> > > > > > > >
> > > > > > > >
> > > > > > >
> > > > >
> > >
> >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > > > > > >
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
>

--047d7bdc875ec6934e04fec1cc1b--

From dev-return-8494-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 05:55:53 2014
Return-Path: <dev-return-8494-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 444791145C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 05:55:53 +0000 (UTC)
Received: (qmail 94873 invoked by uid 500); 22 Jul 2014 05:55:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94810 invoked by uid 500); 22 Jul 2014 05:55:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94796 invoked by uid 99); 22 Jul 2014 05:55:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 05:55:52 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 05:55:48 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so5882855qaq.38
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 22:55:27 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=tOfj59iB450f4R7lyHplyHChvnce7UDSRiW5CqSiaVI=;
        b=c1BEum9soHvrqGsaKq5KJ8b5aqExPQ/r4L/yNTwQ4lPkxWLu/FXNoxdk0DEG7k3GLh
         Jb0gkl+TXdVeQL8FYEZVpxZS9gbCs/by2c4Lx9V7h970vsXaq6x0uBOIx80oc+96MTRl
         lJ9YwrWojyb8jWhEo9lhjRqdo5dkFgUzAsc43X0VFTq+eYfvwfWpNEv44votTxKV85js
         VeAJhgAzh/a4N6HBbT4nM0lLgNO0ZudtGp0qtzsgr9W3Fn5Xf3cHEHySTqL2wAKnfNkn
         PxbGcOAPBiJZRLMj2+qabCKS0eoqI2nO2AvRzectRs9BsvlbSjxTSKJkD0TSTYV/U5Gf
         GAQA==
X-Gm-Message-State: ALoCoQkXjp/aZQ5K2jqI2EXA+soRxucsJFiwPD4ECv04ZkV20YhuIHmlCU13rnN++P2ErxcIucWW
X-Received: by 10.140.92.235 with SMTP id b98mr45488364qge.97.1406008527075;
 Mon, 21 Jul 2014 22:55:27 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 21 Jul 2014 22:55:05 -0700 (PDT)
In-Reply-To: <CACBYxKKLq98wB0-612QEu1zP+NU53pUrkv-rH9KG03zxoFPPVw@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
 <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
 <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
 <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
 <80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
 <CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
 <1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com>
 <CAAsvFP=XOK8mZ0065H=SExLwc_4UJg6uvhDxQHoE4731g3h9=A@mail.gmail.com>
 <CAPh_B=aq1yiU57T+g_DSx-8H5HNVq2ZOthetATqM0e6WMz5OuA@mail.gmail.com> <CACBYxKKLq98wB0-612QEu1zP+NU53pUrkv-rH9KG03zxoFPPVw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 21 Jul 2014 22:55:05 -0700
Message-ID: <CAPh_B=Zf3fxspJX7SNtV0nGAOwvupi_02qHT=efagq5UUwHrsQ@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Erik Erlandson <eje@redhat.com>
Content-Type: multipart/alternative; boundary=001a113a51b4ced5e704fec1deac
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a51b4ced5e704fec1deac
Content-Type: text/plain; charset=UTF-8

Yes, that could work. But it is not as simple as just a binary flag.

We might want to skip the first row for every file, or the header only for
the first file. The former is not really supported out of the box by the
input format I think?


On Mon, Jul 21, 2014 at 10:50 PM, Sandy Ryza <sandy.ryza@cloudera.com>
wrote:

> It could make sense to add a skipHeader argument to SparkContext.textFile?
>
>
> On Mon, Jul 21, 2014 at 10:37 PM, Reynold Xin <rxin@databricks.com> wrote:
>
> > If the purpose is for dropping csv headers, perhaps we don't really need
> a
> > common drop and only one that drops the first line in a file? I'd really
> > try hard to avoid a common drop/dropWhile because they can be expensive
> to
> > do.
> >
> > Note that I think we will be adding this functionality (ignoring headers)
> > to the CsvRDD functionality in Spark SQL.
> >  https://github.com/apache/spark/pull/1351
> >
> >
> > On Mon, Jul 21, 2014 at 1:45 PM, Mark Hamstra <mark@clearstorydata.com>
> > wrote:
> >
> > > You can find some of the prior, related discussion here:
> > > https://issues.apache.org/jira/browse/SPARK-1021
> > >
> > >
> > > On Mon, Jul 21, 2014 at 1:25 PM, Erik Erlandson <eje@redhat.com>
> wrote:
> > >
> > > >
> > > >
> > > > ----- Original Message -----
> > > > > Rather than embrace non-lazy transformations and add more of them,
> > I'd
> > > > > rather we 1) try to fully characterize the needs that are driving
> > their
> > > > > creation/usage; and 2) design and implement new Spark abstractions
> > that
> > > > > will allow us to meet those needs and eliminate existing non-lazy
> > > > > transformation.
> > > >
> > > >
> > > > In the case of drop, obtaining the index of the boundary partition
> can
> > be
> > > > viewed as the action forcing compute -- one that happens to be
> invoked
> > > > inside of a transform.  The concept of a "lazy action", that is only
> > > > triggered if the result rdd has compute invoked on it, might be
> > > sufficient
> > > > to restore laziness to the drop transform.   For that matter, I might
> > > find
> > > > some way to make use of Scala lazy values directly and achieve the
> same
> > > > goal for drop.
> > > >
> > > >
> > > >
> > > > > They really mess up things like creation of asynchronous
> > > > > FutureActions, job cancellation and accounting of job resource
> usage,
> > > > etc.,
> > > > > so I'd rather we seek a way out of the existing hole rather than
> make
> > > it
> > > > > deeper.
> > > > >
> > > > >
> > > > > On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com>
> > > wrote:
> > > > >
> > > > > >
> > > > > >
> > > > > > ----- Original Message -----
> > > > > > > Sure, drop() would be useful, but breaking the "transformations
> > are
> > > > lazy;
> > > > > > > only actions launch jobs" model is abhorrent -- which is not to
> > say
> > > > that
> > > > > > we
> > > > > > > haven't already broken that model for useful operations (cf.
> > > > > > > RangePartitioner, which is used for sorted RDDs), but rather
> that
> > > > each
> > > > > > such
> > > > > > > exception to the model is a significant source of pain that can
> > be
> > > > hard
> > > > > > to
> > > > > > > work with or work around.
> > > > > >
> > > > > > A thought that comes to my mind here is that there are in fact
> > > already
> > > > two
> > > > > > categories of transform: ones that are truly lazy, and ones that
> > are
> > > > not.
> > > > > >  A possible option is to embrace that, and commit to documenting
> > the
> > > > two
> > > > > > categories as such, with an obvious bias towards favoring lazy
> > > > transforms
> > > > > > (to paraphrase Churchill, we're down to haggling over the price).
> > > > > >
> > > > > >
> > > > > > >
> > > > > > > I really wouldn't like to see another such model-breaking
> > > > transformation
> > > > > > > added to the API.  On the other hand, being able to write
> > > > transformations
> > > > > > > with dependencies on these kind of "internal" jobs is sometimes
> > > very
> > > > > > > useful, so a significant reworking of Spark's Dependency model
> > that
> > > > would
> > > > > > > allow for lazily running such internal jobs and making the
> > results
> > > > > > > available to subsequent stages may be something worth pursuing.
> > > > > >
> > > > > >
> > > > > > This seems like a very interesting angle.   I don't have much
> feel
> > > for
> > > > > > what a solution would look like, but it sounds as if it would
> > involve
> > > > > > caching all operations embodied by RDD transform method code for
> > > > > > provisional execution.  I believe that these levels of invocation
> > are
> > > > > > currently executed in the master, not executor nodes.
> > > > > >
> > > > > >
> > > > > > >
> > > > > > >
> > > > > > > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <
> > andrew@andrewash.com>
> > > > > > wrote:
> > > > > > >
> > > > > > > > Personally I'd find the method useful -- I've often had a
> .csv
> > > file
> > > > > > with a
> > > > > > > > header row that I want to drop so filter it out, which
> touches
> > > all
> > > > > > > > partitions anyway.  I don't have any comments on the
> > > implementation
> > > > > > quite
> > > > > > > > yet though.
> > > > > > > >
> > > > > > > >
> > > > > > > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <
> > eje@redhat.com>
> > > > > > wrote:
> > > > > > > >
> > > > > > > > > A few weeks ago I submitted a PR for supporting
> rdd.drop(n),
> > > > under
> > > > > > > > > SPARK-2315:
> > > > > > > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > > > > > > >
> > > > > > > > > Supporting the drop method would make some operations
> > > convenient,
> > > > > > however
> > > > > > > > > it forces computation of >= 1 partition of the parent RDD,
> > and
> > > > so it
> > > > > > > > would
> > > > > > > > > behave like a "partial action" that returns an RDD as the
> > > result.
> > > > > > > > >
> > > > > > > > > I wrote up a discussion of these trade-offs here:
> > > > > > > > >
> > > > > > > > >
> > > > > > > >
> > > > > >
> > > >
> > >
> >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > > > > > > >
> > > > > > > >
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
>

--001a113a51b4ced5e704fec1deac--

From dev-return-8495-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 06:00:27 2014
Return-Path: <dev-return-8495-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A6AF11490
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 06:00:27 +0000 (UTC)
Received: (qmail 6765 invoked by uid 500); 22 Jul 2014 06:00:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6702 invoked by uid 500); 22 Jul 2014 06:00:26 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6691 invoked by uid 99); 22 Jul 2014 06:00:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 06:00:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 06:00:24 +0000
Received: by mail-qg0-f45.google.com with SMTP id f51so6372860qge.4
        for <dev@spark.apache.org>; Mon, 21 Jul 2014 22:59:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=wYBawHOtV5oQbwQQeJCalqp91DZW87oNOe1P8Set50g=;
        b=Z/rL7jjaYADXK0VRSWQ5LgGzitTrRSKaDrnvE9W0vLEva9IDOSc7q1kl9vhpTmZgNr
         3k5ssLNablgI0eWc73VwM7a5Prx/e5BYAEVhC7J5Rv1uR3/KXp//ZdIq0uQN+QLGnW51
         QQjZFA2daIyztsvSmafoXd+2q+KTFR6HaJO+vZCFw4oNutEQZaS16NNzzkAZbgHj/YQ5
         hQinUjqSUIc05vh06wRWlFTqbHswnd5aUoeuv8eZCV/Brsp5T6HPh17FDBnS5inIL+Mv
         fZW4NaE7nlJQKs5zzOROtHGieZCuHUMCyEchqA0c9yg5511hS0niWfMfhwwHlIbBAu/G
         vMgw==
X-Gm-Message-State: ALoCoQnu5CmeAMUbvZ0xpWEvNkyy5qPr7MFjrjVwaGLTcHoLNJuGqFNvJtuVHcET8S1koPfTka6a
MIME-Version: 1.0
X-Received: by 10.229.53.133 with SMTP id m5mr2882784qcg.19.1406008799219;
 Mon, 21 Jul 2014 22:59:59 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Mon, 21 Jul 2014 22:59:59 -0700 (PDT)
In-Reply-To: <CAPh_B=Zf3fxspJX7SNtV0nGAOwvupi_02qHT=efagq5UUwHrsQ@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com>
	<702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com>
	<CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com>
	<CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
	<80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com>
	<CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com>
	<1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com>
	<CAAsvFP=XOK8mZ0065H=SExLwc_4UJg6uvhDxQHoE4731g3h9=A@mail.gmail.com>
	<CAPh_B=aq1yiU57T+g_DSx-8H5HNVq2ZOthetATqM0e6WMz5OuA@mail.gmail.com>
	<CACBYxKKLq98wB0-612QEu1zP+NU53pUrkv-rH9KG03zxoFPPVw@mail.gmail.com>
	<CAPh_B=Zf3fxspJX7SNtV0nGAOwvupi_02qHT=efagq5UUwHrsQ@mail.gmail.com>
Date: Mon, 21 Jul 2014 22:59:59 -0700
Message-ID: <CACBYxK+1zHqHJ6wLb++WF4xJCvNKHKh+jAMQf-A4xyJKvweJLg@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Erik Erlandson <eje@redhat.com>
Content-Type: multipart/alternative; boundary=001a1133da3207672e04fec1ef76
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133da3207672e04fec1ef76
Content-Type: text/plain; charset=UTF-8

Yeah, the input format doesn't support this behavior.  But it does tell you
the byte position of each record in the file.


On Mon, Jul 21, 2014 at 10:55 PM, Reynold Xin <rxin@databricks.com> wrote:

> Yes, that could work. But it is not as simple as just a binary flag.
>
> We might want to skip the first row for every file, or the header only for
> the first file. The former is not really supported out of the box by the
> input format I think?
>
>
> On Mon, Jul 21, 2014 at 10:50 PM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
>
> > It could make sense to add a skipHeader argument to
> SparkContext.textFile?
> >
> >
> > On Mon, Jul 21, 2014 at 10:37 PM, Reynold Xin <rxin@databricks.com>
> wrote:
> >
> > > If the purpose is for dropping csv headers, perhaps we don't really
> need
> > a
> > > common drop and only one that drops the first line in a file? I'd
> really
> > > try hard to avoid a common drop/dropWhile because they can be expensive
> > to
> > > do.
> > >
> > > Note that I think we will be adding this functionality (ignoring
> headers)
> > > to the CsvRDD functionality in Spark SQL.
> > >  https://github.com/apache/spark/pull/1351
> > >
> > >
> > > On Mon, Jul 21, 2014 at 1:45 PM, Mark Hamstra <mark@clearstorydata.com
> >
> > > wrote:
> > >
> > > > You can find some of the prior, related discussion here:
> > > > https://issues.apache.org/jira/browse/SPARK-1021
> > > >
> > > >
> > > > On Mon, Jul 21, 2014 at 1:25 PM, Erik Erlandson <eje@redhat.com>
> > wrote:
> > > >
> > > > >
> > > > >
> > > > > ----- Original Message -----
> > > > > > Rather than embrace non-lazy transformations and add more of
> them,
> > > I'd
> > > > > > rather we 1) try to fully characterize the needs that are driving
> > > their
> > > > > > creation/usage; and 2) design and implement new Spark
> abstractions
> > > that
> > > > > > will allow us to meet those needs and eliminate existing non-lazy
> > > > > > transformation.
> > > > >
> > > > >
> > > > > In the case of drop, obtaining the index of the boundary partition
> > can
> > > be
> > > > > viewed as the action forcing compute -- one that happens to be
> > invoked
> > > > > inside of a transform.  The concept of a "lazy action", that is
> only
> > > > > triggered if the result rdd has compute invoked on it, might be
> > > > sufficient
> > > > > to restore laziness to the drop transform.   For that matter, I
> might
> > > > find
> > > > > some way to make use of Scala lazy values directly and achieve the
> > same
> > > > > goal for drop.
> > > > >
> > > > >
> > > > >
> > > > > > They really mess up things like creation of asynchronous
> > > > > > FutureActions, job cancellation and accounting of job resource
> > usage,
> > > > > etc.,
> > > > > > so I'd rather we seek a way out of the existing hole rather than
> > make
> > > > it
> > > > > > deeper.
> > > > > >
> > > > > >
> > > > > > On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com
> >
> > > > wrote:
> > > > > >
> > > > > > >
> > > > > > >
> > > > > > > ----- Original Message -----
> > > > > > > > Sure, drop() would be useful, but breaking the
> "transformations
> > > are
> > > > > lazy;
> > > > > > > > only actions launch jobs" model is abhorrent -- which is not
> to
> > > say
> > > > > that
> > > > > > > we
> > > > > > > > haven't already broken that model for useful operations (cf.
> > > > > > > > RangePartitioner, which is used for sorted RDDs), but rather
> > that
> > > > > each
> > > > > > > such
> > > > > > > > exception to the model is a significant source of pain that
> can
> > > be
> > > > > hard
> > > > > > > to
> > > > > > > > work with or work around.
> > > > > > >
> > > > > > > A thought that comes to my mind here is that there are in fact
> > > > already
> > > > > two
> > > > > > > categories of transform: ones that are truly lazy, and ones
> that
> > > are
> > > > > not.
> > > > > > >  A possible option is to embrace that, and commit to
> documenting
> > > the
> > > > > two
> > > > > > > categories as such, with an obvious bias towards favoring lazy
> > > > > transforms
> > > > > > > (to paraphrase Churchill, we're down to haggling over the
> price).
> > > > > > >
> > > > > > >
> > > > > > > >
> > > > > > > > I really wouldn't like to see another such model-breaking
> > > > > transformation
> > > > > > > > added to the API.  On the other hand, being able to write
> > > > > transformations
> > > > > > > > with dependencies on these kind of "internal" jobs is
> sometimes
> > > > very
> > > > > > > > useful, so a significant reworking of Spark's Dependency
> model
> > > that
> > > > > would
> > > > > > > > allow for lazily running such internal jobs and making the
> > > results
> > > > > > > > available to subsequent stages may be something worth
> pursuing.
> > > > > > >
> > > > > > >
> > > > > > > This seems like a very interesting angle.   I don't have much
> > feel
> > > > for
> > > > > > > what a solution would look like, but it sounds as if it would
> > > involve
> > > > > > > caching all operations embodied by RDD transform method code
> for
> > > > > > > provisional execution.  I believe that these levels of
> invocation
> > > are
> > > > > > > currently executed in the master, not executor nodes.
> > > > > > >
> > > > > > >
> > > > > > > >
> > > > > > > >
> > > > > > > > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <
> > > andrew@andrewash.com>
> > > > > > > wrote:
> > > > > > > >
> > > > > > > > > Personally I'd find the method useful -- I've often had a
> > .csv
> > > > file
> > > > > > > with a
> > > > > > > > > header row that I want to drop so filter it out, which
> > touches
> > > > all
> > > > > > > > > partitions anyway.  I don't have any comments on the
> > > > implementation
> > > > > > > quite
> > > > > > > > > yet though.
> > > > > > > > >
> > > > > > > > >
> > > > > > > > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <
> > > eje@redhat.com>
> > > > > > > wrote:
> > > > > > > > >
> > > > > > > > > > A few weeks ago I submitted a PR for supporting
> > rdd.drop(n),
> > > > > under
> > > > > > > > > > SPARK-2315:
> > > > > > > > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > > > > > > > >
> > > > > > > > > > Supporting the drop method would make some operations
> > > > convenient,
> > > > > > > however
> > > > > > > > > > it forces computation of >= 1 partition of the parent
> RDD,
> > > and
> > > > > so it
> > > > > > > > > would
> > > > > > > > > > behave like a "partial action" that returns an RDD as the
> > > > result.
> > > > > > > > > >
> > > > > > > > > > I wrote up a discussion of these trade-offs here:
> > > > > > > > > >
> > > > > > > > > >
> > > > > > > > >
> > > > > > >
> > > > >
> > > >
> > >
> >
> http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > > > > > > > >
> > > > > > > > >
> > > > > > > >
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
>

--001a1133da3207672e04fec1ef76--

From dev-return-8496-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 07:47:48 2014
Return-Path: <dev-return-8496-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BEF2E11841
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 07:47:48 +0000 (UTC)
Received: (qmail 64099 invoked by uid 500); 22 Jul 2014 07:47:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64045 invoked by uid 500); 22 Jul 2014 07:47:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64032 invoked by uid 99); 22 Jul 2014 07:47:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 07:47:47 +0000
X-ASF-Spam-Status: No, hits=3.2 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of taeyun.kim@innowireless.co.kr does not designate 59.12.193.45 as permitted sender)
Received: from [59.12.193.45] (HELO MAIL1.innowireless.co.kr) (59.12.193.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 07:47:42 +0000
Received: from INNOC358 (218.154.28.162) by MAIL1.innowireless.co.kr
 (59.12.193.45) with Microsoft SMTP Server id 14.3.195.1; Tue, 22 Jul 2014
 16:46:04 +0900
From: innowireless TaeYun Kim <taeyun.kim@innowireless.co.kr>
To: <dev@spark.apache.org>
References:
In-Reply-To:
Subject: Suggestion for SPARK-1825
Date: Tue, 22 Jul 2014 16:47:20 +0900
Message-ID: <000301cfa581$2b119310$8134b930$@innowireless.co.kr>
MIME-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_000_0004_01CFA5CC.9AF9D750"
X-Mailer: Microsoft Outlook 14.0
Thread-Index: Ac+lSMMSbHNRcCHBTfCup91Q0pcZOAAN57bA
Content-Language: ko
X-Originating-IP: [218.154.28.162]
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_000_0004_01CFA5CC.9AF9D750
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: 7bit

(I'm resending this mail since it seems that it was not sent. Sorry if this
was already sent.)

Hi,

 

A couple of month ago, I made a pull request to fix
https://issues.apache.org/jira/browse/SPARK-1825.

My pull request is here: https://github.com/apache/spark/pull/899 

 

But that pull request has problems:

l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.

l  The related Hadoop API is marked as '@Unstable'.

 

Here is an idea to remedy the problems: a new Spark configuration variable.

Maybe it can be named as "spark.yarn.submit.crossplatform".

If it is set to "true"(default is false), the related Spark code can use the
hard-coded strings that is the same as the Hadoop API provides, thus
avoiding compile error on the Hadoop versions below 2.4.0.

 

Can someone implement this feature, if this idea is acceptable?

Currently my knowledge on Spark source code and Scala is limited to
implement it myself.

To the right person, the modification should be trivial.

You can refer to the source code changes of my pull request.

 

Thanks.

 


------=_NextPart_000_0004_01CFA5CC.9AF9D750--

From dev-return-8497-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 13:55:15 2014
Return-Path: <dev-return-8497-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2780E11490
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 13:55:15 +0000 (UTC)
Received: (qmail 5256 invoked by uid 500); 22 Jul 2014 13:55:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5198 invoked by uid 500); 22 Jul 2014 13:55:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5186 invoked by uid 99); 22 Jul 2014 13:55:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 13:55:14 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_NONE,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eerlands@redhat.com designates 209.132.183.39 as permitted sender)
Received: from [209.132.183.39] (HELO mx6-phx2.redhat.com) (209.132.183.39)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 13:55:10 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx6-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s6MDsnB6019391
	for <dev@spark.apache.org>; Tue, 22 Jul 2014 09:54:49 -0400
Date: Tue, 22 Jul 2014 09:54:48 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: dev@spark.apache.org
Message-ID: <229470582.11588056.1406037288384.JavaMail.zimbra@redhat.com>
In-Reply-To: <CACBYxKKLq98wB0-612QEu1zP+NU53pUrkv-rH9KG03zxoFPPVw@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com> <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com> <80269506.11187237.1405963454183.JavaMail.zimbra@redhat.com> <CAAsvFPn7Y3fDu6nferS5TDb9t7_DQaT0HF+PXvdQabC7NajpLg@mail.gmail.com> <1569095909.11352664.1405974337410.JavaMail.zimbra@redhat.com> <CAAsvFP=XOK8mZ0065H=SExLwc_4UJg6uvhDxQHoE4731g3h9=A@mail.gmail.com> <CAPh_B=aq1yiU57T+g_DSx-8H5HNVq2ZOthetATqM0e6WMz5OuA@mail.gmail.com> <CACBYxKKLq98wB0-612QEu1zP+NU53pUrkv-rH9KG03zxoFPPVw@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.12]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC35 (Linux)/8.0.6_GA_5922)
Thread-Topic: Supporting the Scala drop Method for Spark RDDs
Thread-Index: numligWcWpm5c2Q6vkkO1qTZqT6ozQ==
X-Virus-Checked: Checked by ClamAV on apache.org



----- Original Message -----
> It could make sense to add a skipHeader argument to SparkContext.textFile?

I also looked into this.   I don't think it's feasible given the limits of the InputFormat and RecordReader interfaces.  RecordReader can't (I think) *ever* know which split it's attached to, and the getSplits() method has no concept of RecordReader, so it can't know how many records reside in its splits.   At least in RDD it's possible to do, if not attractive.



> 
> 
> On Mon, Jul 21, 2014 at 10:37 PM, Reynold Xin <rxin@databricks.com> wrote:
> 
> > If the purpose is for dropping csv headers, perhaps we don't really need a
> > common drop and only one that drops the first line in a file? I'd really
> > try hard to avoid a common drop/dropWhile because they can be expensive to
> > do.
> >
> > Note that I think we will be adding this functionality (ignoring headers)
> > to the CsvRDD functionality in Spark SQL.
> >  https://github.com/apache/spark/pull/1351
> >
> >
> > On Mon, Jul 21, 2014 at 1:45 PM, Mark Hamstra <mark@clearstorydata.com>
> > wrote:
> >
> > > You can find some of the prior, related discussion here:
> > > https://issues.apache.org/jira/browse/SPARK-1021
> > >
> > >
> > > On Mon, Jul 21, 2014 at 1:25 PM, Erik Erlandson <eje@redhat.com> wrote:
> > >
> > > >
> > > >
> > > > ----- Original Message -----
> > > > > Rather than embrace non-lazy transformations and add more of them,
> > I'd
> > > > > rather we 1) try to fully characterize the needs that are driving
> > their
> > > > > creation/usage; and 2) design and implement new Spark abstractions
> > that
> > > > > will allow us to meet those needs and eliminate existing non-lazy
> > > > > transformation.
> > > >
> > > >
> > > > In the case of drop, obtaining the index of the boundary partition can
> > be
> > > > viewed as the action forcing compute -- one that happens to be invoked
> > > > inside of a transform.  The concept of a "lazy action", that is only
> > > > triggered if the result rdd has compute invoked on it, might be
> > > sufficient
> > > > to restore laziness to the drop transform.   For that matter, I might
> > > find
> > > > some way to make use of Scala lazy values directly and achieve the same
> > > > goal for drop.
> > > >
> > > >
> > > >
> > > > > They really mess up things like creation of asynchronous
> > > > > FutureActions, job cancellation and accounting of job resource usage,
> > > > etc.,
> > > > > so I'd rather we seek a way out of the existing hole rather than make
> > > it
> > > > > deeper.
> > > > >
> > > > >
> > > > > On Mon, Jul 21, 2014 at 10:24 AM, Erik Erlandson <eje@redhat.com>
> > > wrote:
> > > > >
> > > > > >
> > > > > >
> > > > > > ----- Original Message -----
> > > > > > > Sure, drop() would be useful, but breaking the "transformations
> > are
> > > > lazy;
> > > > > > > only actions launch jobs" model is abhorrent -- which is not to
> > say
> > > > that
> > > > > > we
> > > > > > > haven't already broken that model for useful operations (cf.
> > > > > > > RangePartitioner, which is used for sorted RDDs), but rather that
> > > > each
> > > > > > such
> > > > > > > exception to the model is a significant source of pain that can
> > be
> > > > hard
> > > > > > to
> > > > > > > work with or work around.
> > > > > >
> > > > > > A thought that comes to my mind here is that there are in fact
> > > already
> > > > two
> > > > > > categories of transform: ones that are truly lazy, and ones that
> > are
> > > > not.
> > > > > >  A possible option is to embrace that, and commit to documenting
> > the
> > > > two
> > > > > > categories as such, with an obvious bias towards favoring lazy
> > > > transforms
> > > > > > (to paraphrase Churchill, we're down to haggling over the price).
> > > > > >
> > > > > >
> > > > > > >
> > > > > > > I really wouldn't like to see another such model-breaking
> > > > transformation
> > > > > > > added to the API.  On the other hand, being able to write
> > > > transformations
> > > > > > > with dependencies on these kind of "internal" jobs is sometimes
> > > very
> > > > > > > useful, so a significant reworking of Spark's Dependency model
> > that
> > > > would
> > > > > > > allow for lazily running such internal jobs and making the
> > results
> > > > > > > available to subsequent stages may be something worth pursuing.
> > > > > >
> > > > > >
> > > > > > This seems like a very interesting angle.   I don't have much feel
> > > for
> > > > > > what a solution would look like, but it sounds as if it would
> > involve
> > > > > > caching all operations embodied by RDD transform method code for
> > > > > > provisional execution.  I believe that these levels of invocation
> > are
> > > > > > currently executed in the master, not executor nodes.
> > > > > >
> > > > > >
> > > > > > >
> > > > > > >
> > > > > > > On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <
> > andrew@andrewash.com>
> > > > > > wrote:
> > > > > > >
> > > > > > > > Personally I'd find the method useful -- I've often had a .csv
> > > file
> > > > > > with a
> > > > > > > > header row that I want to drop so filter it out, which touches
> > > all
> > > > > > > > partitions anyway.  I don't have any comments on the
> > > implementation
> > > > > > quite
> > > > > > > > yet though.
> > > > > > > >
> > > > > > > >
> > > > > > > > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <
> > eje@redhat.com>
> > > > > > wrote:
> > > > > > > >
> > > > > > > > > A few weeks ago I submitted a PR for supporting rdd.drop(n),
> > > > under
> > > > > > > > > SPARK-2315:
> > > > > > > > > https://issues.apache.org/jira/browse/SPARK-2315
> > > > > > > > >
> > > > > > > > > Supporting the drop method would make some operations
> > > convenient,
> > > > > > however
> > > > > > > > > it forces computation of >= 1 partition of the parent RDD,
> > and
> > > > so it
> > > > > > > > would
> > > > > > > > > behave like a "partial action" that returns an RDD as the
> > > result.
> > > > > > > > >
> > > > > > > > > I wrote up a discussion of these trade-offs here:
> > > > > > > > >
> > > > > > > > >
> > > > > > > >
> > > > > >
> > > >
> > >
> > http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > > > > > > > >
> > > > > > > >
> > > > > > >
> > > > > >
> > > > >
> > > >
> > >
> >
> 

From dev-return-8498-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 14:21:37 2014
Return-Path: <dev-return-8498-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 256F21159F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 14:21:37 +0000 (UTC)
Received: (qmail 86149 invoked by uid 500); 22 Jul 2014 14:21:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86092 invoked by uid 500); 22 Jul 2014 14:21:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84998 invoked by uid 99); 22 Jul 2014 14:21:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 14:21:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gerard.maas@gmail.com designates 74.125.82.178 as permitted sender)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 14:21:30 +0000
Received: by mail-we0-f178.google.com with SMTP id w61so9287375wes.9
        for <multiple recipients>; Tue, 22 Jul 2014 07:21:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=lnGGmTdzOHqjNP9z0Z2gTf8N7V2Tjr49zL0yiwttPAQ=;
        b=bp/uNNrXKbLRQ4znDwzUWW2uvCUOyaztsrUuP7qAmUAZMhwGzigAZXa8PnNP5i4CjG
         PQAhvg5NFO4vE4t/6hST71yb5Ze6jdZRNMNHp1vLzJV18s4zwT7vNSiAIyICAfa4FgRq
         gS+5qitWkhJ+btyFQ0iiNS5sgtyzgUApl9qGIZK1ZX8GipGqLvqOuOrbg9p2Jlaph0f+
         Q1YRP42ujQvfOFci6xT4id5EM3lXJVzNZZ/Zum4fWZrU1QzaMzcBpddPUiAgAjS2OAyC
         NXw++weHCA8HQu6XAIBqsr+al2nYKQgnXdlLLhIQGXEOCfvXM4kQciTyMrTL5BtRxz5r
         67jg==
X-Received: by 10.180.39.139 with SMTP id p11mr15548913wik.50.1406038867801;
 Tue, 22 Jul 2014 07:21:07 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.123.66 with HTTP; Tue, 22 Jul 2014 07:20:37 -0700 (PDT)
From: Gerard Maas <gerard.maas@gmail.com>
Date: Tue, 22 Jul 2014 16:20:37 +0200
Message-ID: <CAMc-71k53FMTkaUuwRP5b7ovZf0w+wRMNXY__c21LZZQ9WSDeg@mail.gmail.com>
Subject: Using case classes as keys does not seem to work.
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c354a4417f6904fec8ef94
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c354a4417f6904fec8ef94
Content-Type: text/plain; charset=UTF-8

Using a case class as a key doesn't seem to work properly. [Spark 1.0.0]

A minimal example:

case class P(name:String)
val ps = Array(P("alice"), P("bob"), P("charly"), P("bob"))
sc.parallelize(ps).map(x=> (x,1)).reduceByKey((x,y) => x+y).collect
[Spark shell local mode] res : Array[(P, Int)] = Array((P(bob),1),
(P(bob),1), (P(abe),1), (P(charly),1))

In contrast to the expected behavior, that should be equivalent to:
sc.parallelize(ps).map(x=> (x.name,1)).reduceByKey((x,y) => x+y).collect
Array[(String, Int)] = Array((charly,1), (abe,1), (bob,2))

Any ideas why this doesn't work?

-kr, Gerard.

--001a11c354a4417f6904fec8ef94--

From dev-return-8499-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 14:31:44 2014
Return-Path: <dev-return-8499-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BF58D115DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 14:31:44 +0000 (UTC)
Received: (qmail 19478 invoked by uid 500); 22 Jul 2014 14:31:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19396 invoked by uid 500); 22 Jul 2014 14:31:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18342 invoked by uid 99); 22 Jul 2014 14:31:42 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 14:31:42 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.176 as permitted sender)
Received: from [209.85.212.176] (HELO mail-wi0-f176.google.com) (209.85.212.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 14:31:39 +0000
Received: by mail-wi0-f176.google.com with SMTP id bs8so6103963wib.15
        for <multiple recipients>; Tue, 22 Jul 2014 07:31:14 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=PImuFzUDRPSnqbJIlYNMJgUSBlQHeMacPNjgQ5tnNfI=;
        b=iIc9OcC3e3g4nIdKCvwwO5myJos/7fmcM4+lc+KxK+odkhUF6RDGeyR/qBpeP6P8R6
         /ZVQsccPNA8ga1gybeAtwMG3Nt40RMK/Tg80KdvsgdrH2hiHUXqITGw5uDpPm8kFiBhl
         QByjP8zOPtRZzM/612je4q6ufIpi16h7bFLfXDHK47swp1HeWFij6haU9NaJQoQv98Tc
         yAjak/q+Mq7xbGpyg65n7Pg15eweOxK9H4yFPbhzfI/3d9pO6jTvKq/YmzadPs/Owtdr
         rHBLNw/Sw6D7tsJYFov/LHQQGoAqAc4jbzG0w1wIeWq3Za4SoXr8W56JO+HP2ruhK4TR
         1Cjw==
X-Received: by 10.194.71.132 with SMTP id v4mr34974523wju.102.1406039473702;
 Tue, 22 Jul 2014 07:31:13 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.123.66 with HTTP; Tue, 22 Jul 2014 07:30:43 -0700 (PDT)
In-Reply-To: <CAMc-71k53FMTkaUuwRP5b7ovZf0w+wRMNXY__c21LZZQ9WSDeg@mail.gmail.com>
References: <CAMc-71k53FMTkaUuwRP5b7ovZf0w+wRMNXY__c21LZZQ9WSDeg@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Tue, 22 Jul 2014 16:30:43 +0200
Message-ID: <CAMc-71nKdKExZFZ8CnFeeN81ZWA5ncoLJNtdxb3Nt3BTzux7kw@mail.gmail.com>
Subject: Re: Using case classes as keys does not seem to work.
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bfd0d425ecb4004fec913a8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfd0d425ecb4004fec913a8
Content-Type: text/plain; charset=UTF-8

Just to narrow down the issue, it looks like the issue is in 'reduceByKey'
and derivates like 'distinct'.

groupByKey() seems to work

sc.parallelize(ps).map(x=> (x.name,1)).groupByKey().collect
res: Array[(String, Iterable[Int])] = Array((charly,ArrayBuffer(1)),
(abe,ArrayBuffer(1)), (bob,ArrayBuffer(1, 1)))



On Tue, Jul 22, 2014 at 4:20 PM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Using a case class as a key doesn't seem to work properly. [Spark 1.0.0]
>
> A minimal example:
>
> case class P(name:String)
> val ps = Array(P("alice"), P("bob"), P("charly"), P("bob"))
> sc.parallelize(ps).map(x=> (x,1)).reduceByKey((x,y) => x+y).collect
> [Spark shell local mode] res : Array[(P, Int)] = Array((P(bob),1),
> (P(bob),1), (P(abe),1), (P(charly),1))
>
> In contrast to the expected behavior, that should be equivalent to:
> sc.parallelize(ps).map(x=> (x.name,1)).reduceByKey((x,y) => x+y).collect
> Array[(String, Int)] = Array((charly,1), (abe,1), (bob,2))
>
> Any ideas why this doesn't work?
>
> -kr, Gerard.
>

--047d7bfd0d425ecb4004fec913a8--

From dev-return-8500-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 15:26:37 2014
Return-Path: <dev-return-8500-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4839B1185B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 15:26:37 +0000 (UTC)
Received: (qmail 39992 invoked by uid 500); 22 Jul 2014 15:26:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39938 invoked by uid 500); 22 Jul 2014 15:26:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39927 invoked by uid 99); 22 Jul 2014 15:26:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 15:26:36 +0000
X-ASF-Spam-Status: No, hits=-1.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of Muthu.X.Sundaram.ctr@sabre.com does not designate 151.193.220.17 as permitted sender)
Received: from [151.193.220.17] (HELO sgtulmg01-out.sabre.com) (151.193.220.17)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 15:26:28 +0000
X-ExtLoop1: From 10.12.97.30
X-IronPort-AV: E=Sophos;i="5.01,710,1400043600"; 
   d="scan'208,217";a="634424966"
Received: from unknown (HELO SGTULMHP001.Global.ad.sabre.com) ([10.12.97.30])
  by sgtulmg01-out.sabre.com with ESMTP/TLS/AES128-SHA; 22 Jul 2014 10:24:12 -0500
Received: from SGTULMMP003.Global.ad.sabre.com ([::1]) by
 SGTULMHP001.Global.ad.sabre.com ([::1]) with mapi; Tue, 22 Jul 2014 10:24:12
 -0500
From: "Sundaram, Muthu X." <Muthu.X.Sundaram.ctr@sabre.com>
To: "user@spark.apache.org" <user@spark.apache.org>,
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Date: Tue, 22 Jul 2014 10:24:11 -0500
Subject: Tranforming flume events using Spark transformation functions
Thread-Topic: Tranforming flume events using Spark transformation functions
Thread-Index: Ac+lwPuhlSOI3B38R+mRTGfQNzenqQ==
Message-ID: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950@SGTULMMP003.Global.ad.sabre.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950SGTULMMP003Gl_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950SGTULMMP003Gl_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi All,
  I am getting events from flume using following line.

  JavaDStream<SparkFlumeEvent> flumeStream =3D FlumeUtils.createStream(ssc,=
 host, port);

Each event is a delimited record. I like to use some of the transformation =
functions like map and reduce on this. Do I need to convert the JavaDStream=
<SparkFlumeEvent> to JavaDStream<String> or can I apply these function dire=
ctly on this?

I need to do following kind of operations

XXXX                     AA
YYYYY                    Delta
TTTTT                    AA
CCCC                     Southwest
XXXX                     AA

Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket, Southwest=
 - 1 ticket.

I have to do transformations like this. Right now I am able to receives rec=
ords. But I am struggling to transform them using spark transformation func=
tions since they are not of type JavaRDD<String>.

Can I create new JavaRDD<String>? How do I create new JavaRDD?

I loop through  the events like below

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws =
Exception {
                     String logRecord =3D null;
                     List<SparkFlumeEvent> events =3D eventsData.collect();
                     Iterator<SparkFlumeEvent> batchedEvents =3D events.ite=
rator();
                     long t1 =3D System.currentTimeMillis();
                     AvroFlumeEvent avroEvent =3D null;
                     ByteBuffer bytePayload =3D null;
                     // All the user level data is carried as payload in Fl=
ume Event
                     while(batchedEvents.hasNext()) {
                            SparkFlumeEvent flumeEvent =3D batchedEvents.ne=
xt();
                            avroEvent =3D flumeEvent.event();
                            bytePayload =3D avroEvent.getBody();
                            logRecord =3D new String(bytePayload.array());

                            System.out.println(">>>>>>>>LOG RECORD =3D " + =
logRecord);
}

Where do I create new JavaRDD<String>? DO I do it before this loop? How do =
I create this JavaRDD<String>?
In the loop I am able to get every record and I am able to print them.

I appreciate any help here.

Thanks,
Muthu



--_000_3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950SGTULMMP003Gl_--

From dev-return-8501-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 16:12:52 2014
Return-Path: <dev-return-8501-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1F0E511A18
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 16:12:52 +0000 (UTC)
Received: (qmail 13933 invoked by uid 500); 22 Jul 2014 16:12:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13864 invoked by uid 500); 22 Jul 2014 16:12:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12803 invoked by uid 99); 22 Jul 2014 16:12:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 16:12:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.174 as permitted sender)
Received: from [209.85.212.174] (HELO mail-wi0-f174.google.com) (209.85.212.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 16:12:48 +0000
Received: by mail-wi0-f174.google.com with SMTP id d1so6331214wiv.7
        for <multiple recipients>; Tue, 22 Jul 2014 09:12:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=OK8Rtp7Owy2tdVE7qUUK597Pi8OmZlIhD2cFghBeFI4=;
        b=ceJDOF/Hn2oNMqLz8TnenRoMlROukkFa6kOqlYPg3n+UlUUQUCSuzLZn6NYIIynUMI
         As/yLSUj8AxteqqLixL57+Kt9d0AkKjUW0+H2bvaOwRMmEMUuZBykgKuEqeNFLnlGpuI
         tk6K3X1HkzQa2Wh5bBlTivc+NpwloZwsnsX7xLuV2zQJUHQokXsUjCCaOWMG1W8yn6p4
         zoe2NCtqb97VqNXpdTdmMk1HDk8o+JeOCd2BJcbVmeE2RAt2uPnErCWWhNFebN1sLwIS
         nxcvQpR9PrFhOS+czOiL47MfbCtZXuzuFRwvBMIBeFxUUhfTDykMGVU/rrvJBLUyB7Uf
         /SWg==
X-Received: by 10.180.198.173 with SMTP id jd13mr16588252wic.9.1406045544360;
 Tue, 22 Jul 2014 09:12:24 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.194.123.66 with HTTP; Tue, 22 Jul 2014 09:11:54 -0700 (PDT)
In-Reply-To: <CAMc-71nzd9VSdMYBtXZMnVQwVOimgZ6OdrdMxddrJWPv-UrDhQ@mail.gmail.com>
References: <CAMc-71k53FMTkaUuwRP5b7ovZf0w+wRMNXY__c21LZZQ9WSDeg@mail.gmail.com>
 <CAMc-71nKdKExZFZ8CnFeeN81ZWA5ncoLJNtdxb3Nt3BTzux7kw@mail.gmail.com>
 <CAMseSVNHg5--y7_wMfRa2YZAkRsGQL=Gt7o=zFYhObwf1MsWOQ@mail.gmail.com> <CAMc-71nzd9VSdMYBtXZMnVQwVOimgZ6OdrdMxddrJWPv-UrDhQ@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Tue, 22 Jul 2014 18:11:54 +0200
Message-ID: <CAMc-71=xdefBmvcq9qnU+pKa+HbhB1nq8tqwZufes+cUKP-FOw@mail.gmail.com>
Subject: Re: Using case classes as keys does not seem to work.
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7b62498a35b37004feca7dcc
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b62498a35b37004feca7dcc
Content-Type: text/plain; charset=UTF-8

I created https://issues.apache.org/jira/browse/SPARK-2620 to track this.

Maybe useful to know, this is a regression on Spark 1.0.0. I tested the
same sample code on 0.9.1 and it worked (we have several jobs using case
classes as key aggregators, so it better does)

-kr, Gerard.


On Tue, Jul 22, 2014 at 5:37 PM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Yes, right. 'sc.parallelize(ps).map(x=> (**x.name**,1)).groupByKey().
> collect'
> An oversight from my side.
>
> Thanks!,  Gerard.
>
>
> On Tue, Jul 22, 2014 at 5:24 PM, Daniel Siegmann <daniel.siegmann@velos.io
> > wrote:
>
>> I can confirm this bug. The behavior for groupByKey is the same as
>> reduceByKey - your example is actually grouping on just the name. Try
>> this:
>>
>> sc.parallelize(ps).map(x=> (x,1)).groupByKey().collect
>> res1: Array[(P, Iterable[Int])] = Array((P(bob),ArrayBuffer(1)),
>> (P(bob),ArrayBuffer(1)), (P(alice),ArrayBuffer(1)),
>> (P(charly),ArrayBuffer(1)))
>>
>>
>> On Tue, Jul 22, 2014 at 10:30 AM, Gerard Maas <gerard.maas@gmail.com>
>> wrote:
>>
>>> Just to narrow down the issue, it looks like the issue is in
>>> 'reduceByKey' and derivates like 'distinct'.
>>>
>>> groupByKey() seems to work
>>>
>>> sc.parallelize(ps).map(x=> (x.name,1)).groupByKey().collect
>>> res: Array[(String, Iterable[Int])] = Array((charly,ArrayBuffer(1)),
>>> (abe,ArrayBuffer(1)), (bob,ArrayBuffer(1, 1)))
>>>
>>>
>>>
>>> On Tue, Jul 22, 2014 at 4:20 PM, Gerard Maas <gerard.maas@gmail.com>
>>> wrote:
>>>
>>>> Using a case class as a key doesn't seem to work properly. [Spark 1.0.0]
>>>>
>>>> A minimal example:
>>>>
>>>> case class P(name:String)
>>>> val ps = Array(P("alice"), P("bob"), P("charly"), P("bob"))
>>>> sc.parallelize(ps).map(x=> (x,1)).reduceByKey((x,y) => x+y).collect
>>>> [Spark shell local mode] res : Array[(P, Int)] = Array((P(bob),1),
>>>> (P(bob),1), (P(abe),1), (P(charly),1))
>>>>
>>>> In contrast to the expected behavior, that should be equivalent to:
>>>> sc.parallelize(ps).map(x=> (x.name,1)).reduceByKey((x,y) =>
>>>> x+y).collect
>>>> Array[(String, Int)] = Array((charly,1), (abe,1), (bob,2))
>>>>
>>>> Any ideas why this doesn't work?
>>>>
>>>> -kr, Gerard.
>>>>
>>>
>>>
>>
>>
>> --
>> Daniel Siegmann, Software Developer
>> Velos
>> Accelerating Machine Learning
>>
>> 440 NINTH AVENUE, 11TH FLOOR, NEW YORK, NY 10001
>> E: daniel.siegmann@velos.io W: www.velos.io
>>
>
>

--047d7b62498a35b37004feca7dcc--

From dev-return-8502-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 20:43:32 2014
Return-Path: <dev-return-8502-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 936CF117DE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 20:43:32 +0000 (UTC)
Received: (qmail 75164 invoked by uid 500); 22 Jul 2014 20:43:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75110 invoked by uid 500); 22 Jul 2014 20:43:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75095 invoked by uid 99); 22 Jul 2014 20:43:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 20:43:31 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nferguson@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 20:43:29 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so248095qap.23
        for <dev@spark.apache.org>; Tue, 22 Jul 2014 13:43:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=WAfp2PfNJiLhDxM5Dj0cHIK/igcjrO8/IpJF16eTOmc=;
        b=bocXXwm9zSy8RDqmMnlqUVQcin2Ec+fcfFf4E1Kq0EdsmZrUBZQezYt6hxXEuSZqs6
         RaO2kLnKW5moXr0MQ8M+5HUZBRUbK2vLWjIw3AIW3x0Cwmr4EetVncADS3/YTiHCfhyL
         j5g+vRBr+i/X2zWZhN4jN36iC8PUt9ZXkdO/c8qqHWICq83ygg6cJ48jMfUP4TSTBEB7
         M3p/+0Bw/WZR9iYeqM6md2DedJGhQ9gelZn31995lEKEdmp0ef014QM0bF6OofSQ/kNv
         wJfzSc2wPwU4YGamCXffXaT4TM5iQV50W9g8IKdo0YSMBRtSWs9vf6pOKqu5m9gGDQOP
         HRuA==
MIME-Version: 1.0
X-Received: by 10.224.80.67 with SMTP id s3mr59445315qak.92.1406061784306;
 Tue, 22 Jul 2014 13:43:04 -0700 (PDT)
Received: by 10.140.48.35 with HTTP; Tue, 22 Jul 2014 13:43:04 -0700 (PDT)
In-Reply-To: <CAPh_B=bERSLwz47h0UbWFw3T53hwoBvMCWDa2Pp1qXJ4BdZNDQ@mail.gmail.com>
References: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
	<CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
	<CAPh_B=bERSLwz47h0UbWFw3T53hwoBvMCWDa2Pp1qXJ4BdZNDQ@mail.gmail.com>
Date: Tue, 22 Jul 2014 21:43:04 +0100
Message-ID: <CAKqT-W3eJuSfABAfJ5PYzeusBA27BGHVoRz4OEHDto=Yb86EnQ@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
From: Neil Ferguson <nferguson@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2d7ec2f9a2504fece454e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d7ec2f9a2504fece454e
Content-Type: text/plain; charset=UTF-8

Hi Reynold

Thanks for your reply.

Accumulators are, of course, stored in the Accumulators object as
thread-local variables. However, the Accumulators object isn't public, so
when a Task is executing there's no way to get the set of accumulators for
the current thread -- accumulators still have to be passed to every method
that needs them.

Additionally, unless an accumulator is explicitly referenced it won't be
serialized as part of a Task, and won't make it into the Accumulators
object in the first place.

I should also note that what I'm proposing is not specific to Accumulators
-- I am proposing that any data can be stored in a thread-local variable. I
think there are probably many other use cases other than my one.

Neil


On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com> wrote:

> Thanks for the thoughtful email, Neil and Christopher.
>
> If I understand this correctly, it seems like the dynamic variable is just
> a variant of the accumulator (a static one since it is a global object).
> Accumulators are already implemented using thread-local variables under the
> hood. Am I misunderstanding something?
>
>
>
> On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.com>
> wrote:
>
> > Hi Neil, first off, I'm generally a sympathetic advocate for making
> changes
> > to Spark internals to make it easier/better/faster/more awesome.
> >
> > In this case, I'm (a) not clear about what you're trying to accomplish,
> and
> > (b) a bit worried about the proposed solution.
> >
> > On (a): it is stated that you want to pass some Accumulators around. Yet
> > the proposed solution is for some "shared" variable that may be set and
> > "mapped out" and possibly "reduced back", but without any accompanying
> > accumulation semantics. And yet it doesn't seem like you only want just
> the
> > broadcast property. Can you clarify the problem statement with some
> > before/after client code examples?
> >
> > On (b): you're right that adding variables to SparkContext should be done
> > with caution, as it may have unintended consequences beyond just serdes
> > payload size. For example, there is a stated intention of supporting
> > multiple SparkContexts in the future, and this proposed solution can make
> > it a bigger challenge to do so. Indeed, we had a gut-wrenching call to
> make
> > a while back on a subject related to this (see
> > https://github.com/mesos/spark/pull/779). Furthermore, even in a single
> > SparkContext application, there may be multiple "clients" (of that
> > application) whose intent to use the proposed "SparkDynamic" would not
> > necessarily be coordinated.
> >
> > So, considering a ratio of a/b (benefit/cost), it's not clear to me that
> > the benefits are significant enough to warrant the costs. Do I
> > misunderstand that the benefit is to save one explicit parameter (the
> > "context") in the signature/closure code?
> >
> > --
> > Christopher T. Nguyen
> > Co-founder & CEO, Adatao <http://adatao.com>
> > linkedin.com/in/ctnguyen
> >
> >
> >
> > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <nferguson@gmail.com>
> > wrote:
> >
> > > Hi all
> > >
> > > I have been adding some metrics to the ADAM project
> > > https://github.com/bigdatagenomics/adam, which runs on Spark, and
> have a
> > > proposal for an enhancement to Spark that would make this work cleaner
> > and
> > > easier.
> > >
> > > I need to pass some Accumulators around, which will aggregate metrics
> > > (timing stats and other metrics) across the cluster. However, it is
> > > cumbersome to have to explicitly pass some "context" containing these
> > > accumulators around everywhere that might need them. I can use Scala
> > > implicits, which help slightly, but I'd still need to modify every
> method
> > > in the call stack to take an implicit variable.
> > >
> > > So, I'd like to propose that we add the ability to have "dynamic
> > variables"
> > > (basically thread-local variables) to Spark. This would avoid having to
> > > pass the Accumulators around explicitly.
> > >
> > > My proposed approach is to add a method to the SparkContext class as
> > > follows:
> > >
> > > /**
> > >  * Sets the value of a "dynamic variable". This value is made available
> > to
> > > jobs
> > >  * without having to be passed around explicitly. During execution of a
> > > Spark job
> > >  * this value can be obtained from the [[SparkDynamic]] object.
> > >  */
> > > def setDynamicVariableValue(value: Any)
> > >
> > > Then, when a job is executing the SparkDynamic can be accessed to
> obtain
> > > the value of the dynamic variable. The implementation of this object is
> > as
> > > follows:
> > >
> > > object SparkDynamic {
> > >   private val dynamicVariable = new DynamicVariable[Any]()
> > >   /**
> > >    * Gets the value of the "dynamic variable" that has been set in the
> > > [[SparkContext]]
> > >    */
> > >   def getValue: Option[Any] = {
> > >     Option(dynamicVariable.value)
> > >   }
> > >   private[spark] def withValue[S](threadValue: Option[Any])(thunk: =>
> > S): S
> > > = {
> > >     dynamicVariable.withValue(threadValue.orNull)(thunk)
> > >   }
> > > }
> > >
> > > The change involves modifying the Task object to serialize the value of
> > the
> > > dynamic variable, and modifying the TaskRunner class to deserialize the
> > > value and make it available in the thread that is running the task
> (using
> > > the SparkDynamic.withValue method).
> > >
> > > I have done a quick prototype of this in this commit:
> > >
> > >
> >
> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
> > > and it seems to work fine in my (limited) testing. It needs more
> testing,
> > > tidy-up and documentation though.
> > >
> > > One drawback is that the dynamic variable will be serialized for every
> > Task
> > > whether it needs it or not. For my use case this might not be too much
> > of a
> > > problem, as serializing and deserializing Accumulators looks fairly
> > > lightweight -- however we should certainly warn users against setting a
> > > dynamic variable containing lots of data. I thought about using
> broadcast
> > > tables here, but I don't think it's possible to put Accumulators in a
> > > broadcast table (as I understand it, they're intended for purely
> > read-only
> > > data).
> > >
> > > What do people think about this proposal? My use case aside, it seems
> > like
> > > it would be a generally useful enhancment to be able to pass certain
> data
> > > around without having to explicitly pass it everywhere.
> > >
> > > Neil
> > >
> >
>

--001a11c2d7ec2f9a2504fece454e--

From dev-return-8503-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 20:55:54 2014
Return-Path: <dev-return-8503-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00FD811872
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 20:55:54 +0000 (UTC)
Received: (qmail 25455 invoked by uid 500); 22 Jul 2014 20:55:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25337 invoked by uid 500); 22 Jul 2014 20:55:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24443 invoked by uid 99); 22 Jul 2014 20:55:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 20:55:51 +0000
X-ASF-Spam-Status: No, hits=-4.0 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of Muthu.X.Sundaram.ctr@sabre.com does not designate 151.193.220.19 as permitted sender)
Received: from [151.193.220.19] (HELO sgtulmg02-out.sabre.com) (151.193.220.19)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 20:55:46 +0000
X-ExtLoop1: From 10.12.97.30
X-IronPort-AV: E=Sophos;i="5.01,712,1400043600"; 
   d="scan'208";a="612241755"
Received: from unknown (HELO SGTULMHP001.Global.ad.sabre.com) ([10.12.97.30])
  by sgtulmg02-out.sabre.com with ESMTP/TLS/AES128-SHA; 22 Jul 2014 15:55:25 -0500
Received: from SGTULMMP003.Global.ad.sabre.com ([::1]) by
 SGTULMHP001.Global.ad.sabre.com ([::1]) with mapi; Tue, 22 Jul 2014 15:55:24
 -0500
From: "Sundaram, Muthu X." <Muthu.X.Sundaram.ctr@sabre.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org"
	<user@spark.apache.org>, "dev@spark.incubator.apache.org"
	<dev@spark.incubator.apache.org>, "tathagata.das1565@gmail.com"
	<tathagata.das1565@gmail.com>
Date: Tue, 22 Jul 2014 15:55:23 -0500
Subject: RE: Tranforming flume events using Spark transformation functions
Thread-Topic: Tranforming flume events using Spark transformation functions
Thread-Index: Ac+lwPuhlSOI3B38R+mRTGfQNzenqQALddTA
Message-ID: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7ABB@SGTULMMP003.Global.ad.sabre.com>
References: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950@SGTULMMP003.Global.ad.sabre.com>
In-Reply-To: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950@SGTULMMP003.Global.ad.sabre.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
acceptlanguage: en-US
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

I tried to map SparkFlumeEvents to String of RDDs like below. But that map =
and call are not at all executed. I might be doing this in a wrong way. Any=
 help would be appreciated.

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws =
Exception {
                                System.out.println("<<<<<<Inside for each..=
.call>>>>");

                                JavaRDD<String> records =3D eventsData.map(
            new Function<SparkFlumeEvent, String>() {
                                @Override
                public String call(SparkFlumeEvent flume) throws Exception =
{
                    String logRecord =3D null;
                AvroFlumeEvent avroEvent =3D null;
      ByteBuffer bytePayload =3D null;

                                                                           =
     System.out.println("<<<<<<Inside Map..call>>>>");
                    /* List<SparkFlumeEvent> events =3D flume.collect();
                     Iterator<SparkFlumeEvent> batchedEvents =3D events.ite=
rator();=20
                                                                           =
    =20
                            SparkFlumeEvent flumeEvent =3D batchedEvents.ne=
xt();*/
                            avroEvent =3D flume.event();
                            bytePayload =3D avroEvent.getBody();
                            logRecord =3D new String(bytePayload.array()); =
                              =20
                                                                           =
                                     System.out.println("<<<<Record is" + l=
ogRecord);
                                                                           =
    =20
                    return logRecord;
                }
            });                                              =20
                                return null;
}

-----Original Message-----
From: Sundaram, Muthu X. [mailto:Muthu.X.Sundaram.ctr@sabre.com]=20
Sent: Tuesday, July 22, 2014 10:24 AM
To: user@spark.apache.org; dev@spark.incubator.apache.org
Subject: Tranforming flume events using Spark transformation functions

Hi All,
  I am getting events from flume using following line.

  JavaDStream<SparkFlumeEvent> flumeStream =3D FlumeUtils.createStream(ssc,=
 host, port);

Each event is a delimited record. I like to use some of the transformation =
functions like map and reduce on this. Do I need to convert the JavaDStream=
<SparkFlumeEvent> to JavaDStream<String> or can I apply these function dire=
ctly on this?

I need to do following kind of operations

XXXX                     AA
YYYYY                    Delta
TTTTT                    AA
CCCC                     Southwest
XXXX                     AA

Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket, Southwest=
 - 1 ticket.

I have to do transformations like this. Right now I am able to receives rec=
ords. But I am struggling to transform them using spark transformation func=
tions since they are not of type JavaRDD<String>.

Can I create new JavaRDD<String>? How do I create new JavaRDD?

I loop through  the events like below

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws =
Exception {
                     String logRecord =3D null;
                     List<SparkFlumeEvent> events =3D eventsData.collect();
                     Iterator<SparkFlumeEvent> batchedEvents =3D events.ite=
rator();
                     long t1 =3D System.currentTimeMillis();
                     AvroFlumeEvent avroEvent =3D null;
                     ByteBuffer bytePayload =3D null;
                     // All the user level data is carried as payload in Fl=
ume Event
                     while(batchedEvents.hasNext()) {
                            SparkFlumeEvent flumeEvent =3D batchedEvents.ne=
xt();
                            avroEvent =3D flumeEvent.event();
                            bytePayload =3D avroEvent.getBody();
                            logRecord =3D new String(bytePayload.array());

                            System.out.println(">>>>>>>>LOG RECORD =3D " + =
logRecord); }

Where do I create new JavaRDD<String>? DO I do it before this loop? How do =
I create this JavaRDD<String>?
In the loop I am able to get every record and I am able to print them.

I appreciate any help here.

Thanks,
Muthu



From dev-return-8504-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 20:55:59 2014
Return-Path: <dev-return-8504-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B3B0111873
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 20:55:59 +0000 (UTC)
Received: (qmail 27238 invoked by uid 500); 22 Jul 2014 20:55:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27181 invoked by uid 500); 22 Jul 2014 20:55:59 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27162 invoked by uid 99); 22 Jul 2014 20:55:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 20:55:58 +0000
X-ASF-Spam-Status: No, hits=-4.0 required=10.0
	tests=RCVD_IN_DNSWL_HI,SPF_SOFTFAIL
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of Muthu.X.Sundaram.ctr@sabre.com does not designate 151.193.220.19 as permitted sender)
Received: from [151.193.220.19] (HELO sgtulmg02-out.sabre.com) (151.193.220.19)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 20:55:50 +0000
X-ExtLoop1: From 10.12.97.30
X-IronPort-AV: E=Sophos;i="5.01,712,1400043600"; 
   d="scan'208";a="612241755"
Received: from unknown (HELO SGTULMHP001.Global.ad.sabre.com) ([10.12.97.30])
  by sgtulmg02-out.sabre.com with ESMTP/TLS/AES128-SHA; 22 Jul 2014 15:55:25 -0500
Received: from SGTULMMP003.Global.ad.sabre.com ([::1]) by
 SGTULMHP001.Global.ad.sabre.com ([::1]) with mapi; Tue, 22 Jul 2014 15:55:24
 -0500
From: "Sundaram, Muthu X." <Muthu.X.Sundaram.ctr@sabre.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org"
	<user@spark.apache.org>, "dev@spark.incubator.apache.org"
	<dev@spark.incubator.apache.org>, "tathagata.das1565@gmail.com"
	<tathagata.das1565@gmail.com>
Date: Tue, 22 Jul 2014 15:55:23 -0500
Subject: RE: Tranforming flume events using Spark transformation functions
Thread-Topic: Tranforming flume events using Spark transformation functions
Thread-Index: Ac+lwPuhlSOI3B38R+mRTGfQNzenqQALddTA
Message-ID: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7ABB@SGTULMMP003.Global.ad.sabre.com>
References: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950@SGTULMMP003.Global.ad.sabre.com>
In-Reply-To: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950@SGTULMMP003.Global.ad.sabre.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
acceptlanguage: en-US
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

I tried to map SparkFlumeEvents to String of RDDs like below. But that map =
and call are not at all executed. I might be doing this in a wrong way. Any=
 help would be appreciated.

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws =
Exception {
                                System.out.println("<<<<<<Inside for each..=
.call>>>>");

                                JavaRDD<String> records =3D eventsData.map(
            new Function<SparkFlumeEvent, String>() {
                                @Override
                public String call(SparkFlumeEvent flume) throws Exception =
{
                    String logRecord =3D null;
                AvroFlumeEvent avroEvent =3D null;
      ByteBuffer bytePayload =3D null;

                                                                           =
     System.out.println("<<<<<<Inside Map..call>>>>");
                    /* List<SparkFlumeEvent> events =3D flume.collect();
                     Iterator<SparkFlumeEvent> batchedEvents =3D events.ite=
rator();=20
                                                                           =
    =20
                            SparkFlumeEvent flumeEvent =3D batchedEvents.ne=
xt();*/
                            avroEvent =3D flume.event();
                            bytePayload =3D avroEvent.getBody();
                            logRecord =3D new String(bytePayload.array()); =
                              =20
                                                                           =
                                     System.out.println("<<<<Record is" + l=
ogRecord);
                                                                           =
    =20
                    return logRecord;
                }
            });                                              =20
                                return null;
}

-----Original Message-----
From: Sundaram, Muthu X. [mailto:Muthu.X.Sundaram.ctr@sabre.com]=20
Sent: Tuesday, July 22, 2014 10:24 AM
To: user@spark.apache.org; dev@spark.incubator.apache.org
Subject: Tranforming flume events using Spark transformation functions

Hi All,
  I am getting events from flume using following line.

  JavaDStream<SparkFlumeEvent> flumeStream =3D FlumeUtils.createStream(ssc,=
 host, port);

Each event is a delimited record. I like to use some of the transformation =
functions like map and reduce on this. Do I need to convert the JavaDStream=
<SparkFlumeEvent> to JavaDStream<String> or can I apply these function dire=
ctly on this?

I need to do following kind of operations

XXXX                     AA
YYYYY                    Delta
TTTTT                    AA
CCCC                     Southwest
XXXX                     AA

Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket, Southwest=
 - 1 ticket.

I have to do transformations like this. Right now I am able to receives rec=
ords. But I am struggling to transform them using spark transformation func=
tions since they are not of type JavaRDD<String>.

Can I create new JavaRDD<String>? How do I create new JavaRDD?

I loop through  the events like below

flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
              @Override
              public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws =
Exception {
                     String logRecord =3D null;
                     List<SparkFlumeEvent> events =3D eventsData.collect();
                     Iterator<SparkFlumeEvent> batchedEvents =3D events.ite=
rator();
                     long t1 =3D System.currentTimeMillis();
                     AvroFlumeEvent avroEvent =3D null;
                     ByteBuffer bytePayload =3D null;
                     // All the user level data is carried as payload in Fl=
ume Event
                     while(batchedEvents.hasNext()) {
                            SparkFlumeEvent flumeEvent =3D batchedEvents.ne=
xt();
                            avroEvent =3D flumeEvent.event();
                            bytePayload =3D avroEvent.getBody();
                            logRecord =3D new String(bytePayload.array());

                            System.out.println(">>>>>>>>LOG RECORD =3D " + =
logRecord); }

Where do I create new JavaRDD<String>? DO I do it before this loop? How do =
I create this JavaRDD<String>?
In the loop I am able to get every record and I am able to print them.

I appreciate any help here.

Thanks,
Muthu



From dev-return-8505-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 21:09:58 2014
Return-Path: <dev-return-8505-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 28E38118FB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 21:09:58 +0000 (UTC)
Received: (qmail 78531 invoked by uid 500); 22 Jul 2014 21:09:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78477 invoked by uid 500); 22 Jul 2014 21:09:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78461 invoked by uid 99); 22 Jul 2014 21:09:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 21:09:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of shivaram@berkeley.edu designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 21:09:54 +0000
Received: by mail-wi0-f182.google.com with SMTP id d1so1159778wiv.9
        for <dev@spark.apache.org>; Tue, 22 Jul 2014 14:09:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:reply-to:in-reply-to:references
         :date:message-id:subject:from:to:content-type;
        bh=iN9gUv4KBH16AZieqNYhOolZsCe51yzvIuCbzU33e7E=;
        b=iDT1EpFXBPNOt4JXtI72+rw87Spqu5ekVXcoXKdvBnzB6iS39HoNn2pXLG+LhXEh7J
         AdxCcR74EP6cp4KnL555oC63BiEansQIA3jBqToyGOJTf9aJMArz5tejp/SThQ7BHeUV
         f+sUoi59hlY6vQFVaoLAT4mkDvGJG9DEtqgwfhLYHWU+Fjd07/rU3t5e7seT/gzBgyag
         JQCzZ4Mj1xxjfF93gXZV5mxSWexB7UtsCmdEPMNXs1WnrJjPrY2LAKx3JEqMxGt2dfmm
         mONpfCMCdbYCO+wO6d0Z/6xDGTYt9r7tLy26xlDWZsOcLCusQlIvo4CuNhBtzT37CBby
         11Aw==
X-Gm-Message-State: ALoCoQkFM0h/RsIOj7PJZyQzBURG/u0MA4xy59zeqPTtiSRGsy6h82S+cpG3ogLwlv7emnLdzp2D
MIME-Version: 1.0
X-Received: by 10.194.104.97 with SMTP id gd1mr36704866wjb.77.1406063370018;
 Tue, 22 Jul 2014 14:09:30 -0700 (PDT)
Reply-To: shivaram@eecs.berkeley.edu
Received: by 10.217.48.72 with HTTP; Tue, 22 Jul 2014 14:09:29 -0700 (PDT)
In-Reply-To: <CAKqT-W3eJuSfABAfJ5PYzeusBA27BGHVoRz4OEHDto=Yb86EnQ@mail.gmail.com>
References: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
	<CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
	<CAPh_B=bERSLwz47h0UbWFw3T53hwoBvMCWDa2Pp1qXJ4BdZNDQ@mail.gmail.com>
	<CAKqT-W3eJuSfABAfJ5PYzeusBA27BGHVoRz4OEHDto=Yb86EnQ@mail.gmail.com>
Date: Tue, 22 Jul 2014 14:09:29 -0700
Message-ID: <CAKx7Bf_Da4q=XgEtE1RMryA7koaqOZm2tmod24Y7bH21ut0xmg@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
From: Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0102f332b3a83504fecea3a3
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0102f332b3a83504fecea3a3
Content-Type: text/plain; charset=UTF-8

>From reading Neil's first e-mail, I think the motivation is to get some
metrics in ADAM ? --  I've run into a similar use-case with having
user-defined metrics in long-running tasks and I think a nice way to solve
this would be to have user-defined TaskMetrics.

To state my problem more clearly, lets say you have two functions you use
in a map call and want to measure how much time each of them takes. For
example, if you have a code block like the one below and you want to
measure how much time f1 takes as a fraction of the task.

a.map { l =>
   val f = f1(l)
   ... some work here ...
}

It would be really cool if we could do something like

a.map { l =>
   val start = System.nanoTime
   val f = f1(l)
   TaskMetrics.get("f1-time").add(System.nanoTime - start)
}

These task metrics have a different purpose from Accumulators in the sense
that we don't need to track lineage, perform commutative operations etc.
 Further we also have a bunch of code in place to aggregate task metrics
across a stage etc. So it would be great if we could also populate these in
the UI and show median/max etc.
I think counters [1] in Hadoop served a similar purpose.

Thanks
Shivaram

[1]
https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/counters



On Tue, Jul 22, 2014 at 1:43 PM, Neil Ferguson <nferguson@gmail.com> wrote:

> Hi Reynold
>
> Thanks for your reply.
>
> Accumulators are, of course, stored in the Accumulators object as
> thread-local variables. However, the Accumulators object isn't public, so
> when a Task is executing there's no way to get the set of accumulators for
> the current thread -- accumulators still have to be passed to every method
> that needs them.
>
> Additionally, unless an accumulator is explicitly referenced it won't be
> serialized as part of a Task, and won't make it into the Accumulators
> object in the first place.
>
> I should also note that what I'm proposing is not specific to Accumulators
> -- I am proposing that any data can be stored in a thread-local variable. I
> think there are probably many other use cases other than my one.
>
> Neil
>
>
> On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> > Thanks for the thoughtful email, Neil and Christopher.
> >
> > If I understand this correctly, it seems like the dynamic variable is
> just
> > a variant of the accumulator (a static one since it is a global object).
> > Accumulators are already implemented using thread-local variables under
> the
> > hood. Am I misunderstanding something?
> >
> >
> >
> > On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.com>
> > wrote:
> >
> > > Hi Neil, first off, I'm generally a sympathetic advocate for making
> > changes
> > > to Spark internals to make it easier/better/faster/more awesome.
> > >
> > > In this case, I'm (a) not clear about what you're trying to accomplish,
> > and
> > > (b) a bit worried about the proposed solution.
> > >
> > > On (a): it is stated that you want to pass some Accumulators around.
> Yet
> > > the proposed solution is for some "shared" variable that may be set and
> > > "mapped out" and possibly "reduced back", but without any accompanying
> > > accumulation semantics. And yet it doesn't seem like you only want just
> > the
> > > broadcast property. Can you clarify the problem statement with some
> > > before/after client code examples?
> > >
> > > On (b): you're right that adding variables to SparkContext should be
> done
> > > with caution, as it may have unintended consequences beyond just serdes
> > > payload size. For example, there is a stated intention of supporting
> > > multiple SparkContexts in the future, and this proposed solution can
> make
> > > it a bigger challenge to do so. Indeed, we had a gut-wrenching call to
> > make
> > > a while back on a subject related to this (see
> > > https://github.com/mesos/spark/pull/779). Furthermore, even in a
> single
> > > SparkContext application, there may be multiple "clients" (of that
> > > application) whose intent to use the proposed "SparkDynamic" would not
> > > necessarily be coordinated.
> > >
> > > So, considering a ratio of a/b (benefit/cost), it's not clear to me
> that
> > > the benefits are significant enough to warrant the costs. Do I
> > > misunderstand that the benefit is to save one explicit parameter (the
> > > "context") in the signature/closure code?
> > >
> > > --
> > > Christopher T. Nguyen
> > > Co-founder & CEO, Adatao <http://adatao.com>
> > > linkedin.com/in/ctnguyen
> > >
> > >
> > >
> > > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <nferguson@gmail.com>
> > > wrote:
> > >
> > > > Hi all
> > > >
> > > > I have been adding some metrics to the ADAM project
> > > > https://github.com/bigdatagenomics/adam, which runs on Spark, and
> > have a
> > > > proposal for an enhancement to Spark that would make this work
> cleaner
> > > and
> > > > easier.
> > > >
> > > > I need to pass some Accumulators around, which will aggregate metrics
> > > > (timing stats and other metrics) across the cluster. However, it is
> > > > cumbersome to have to explicitly pass some "context" containing these
> > > > accumulators around everywhere that might need them. I can use Scala
> > > > implicits, which help slightly, but I'd still need to modify every
> > method
> > > > in the call stack to take an implicit variable.
> > > >
> > > > So, I'd like to propose that we add the ability to have "dynamic
> > > variables"
> > > > (basically thread-local variables) to Spark. This would avoid having
> to
> > > > pass the Accumulators around explicitly.
> > > >
> > > > My proposed approach is to add a method to the SparkContext class as
> > > > follows:
> > > >
> > > > /**
> > > >  * Sets the value of a "dynamic variable". This value is made
> available
> > > to
> > > > jobs
> > > >  * without having to be passed around explicitly. During execution
> of a
> > > > Spark job
> > > >  * this value can be obtained from the [[SparkDynamic]] object.
> > > >  */
> > > > def setDynamicVariableValue(value: Any)
> > > >
> > > > Then, when a job is executing the SparkDynamic can be accessed to
> > obtain
> > > > the value of the dynamic variable. The implementation of this object
> is
> > > as
> > > > follows:
> > > >
> > > > object SparkDynamic {
> > > >   private val dynamicVariable = new DynamicVariable[Any]()
> > > >   /**
> > > >    * Gets the value of the "dynamic variable" that has been set in
> the
> > > > [[SparkContext]]
> > > >    */
> > > >   def getValue: Option[Any] = {
> > > >     Option(dynamicVariable.value)
> > > >   }
> > > >   private[spark] def withValue[S](threadValue: Option[Any])(thunk: =>
> > > S): S
> > > > = {
> > > >     dynamicVariable.withValue(threadValue.orNull)(thunk)
> > > >   }
> > > > }
> > > >
> > > > The change involves modifying the Task object to serialize the value
> of
> > > the
> > > > dynamic variable, and modifying the TaskRunner class to deserialize
> the
> > > > value and make it available in the thread that is running the task
> > (using
> > > > the SparkDynamic.withValue method).
> > > >
> > > > I have done a quick prototype of this in this commit:
> > > >
> > > >
> > >
> >
> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
> > > > and it seems to work fine in my (limited) testing. It needs more
> > testing,
> > > > tidy-up and documentation though.
> > > >
> > > > One drawback is that the dynamic variable will be serialized for
> every
> > > Task
> > > > whether it needs it or not. For my use case this might not be too
> much
> > > of a
> > > > problem, as serializing and deserializing Accumulators looks fairly
> > > > lightweight -- however we should certainly warn users against
> setting a
> > > > dynamic variable containing lots of data. I thought about using
> > broadcast
> > > > tables here, but I don't think it's possible to put Accumulators in a
> > > > broadcast table (as I understand it, they're intended for purely
> > > read-only
> > > > data).
> > > >
> > > > What do people think about this proposal? My use case aside, it seems
> > > like
> > > > it would be a generally useful enhancment to be able to pass certain
> > data
> > > > around without having to explicitly pass it everywhere.
> > > >
> > > > Neil
> > > >
> > >
> >
>

--089e0102f332b3a83504fecea3a3--

From dev-return-8506-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 21:17:55 2014
Return-Path: <dev-return-8506-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E96DF11950
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 21:17:55 +0000 (UTC)
Received: (qmail 6765 invoked by uid 500); 22 Jul 2014 21:17:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6701 invoked by uid 500); 22 Jul 2014 21:17:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6686 invoked by uid 99); 22 Jul 2014 21:17:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 21:17:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nferguson@gmail.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 21:17:52 +0000
Received: by mail-qa0-f47.google.com with SMTP id i13so287183qae.20
        for <dev@spark.apache.org>; Tue, 22 Jul 2014 14:17:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=gcZq7exqpAcDfUmGXn27qmLACHAmyhiVdic4uGNNZ1g=;
        b=gVUmLQwpl+2xsB61GXKge5zgYZNmXRCh93zGpnZfZiLzLLOWy+7yxlZJhp3vafhLg8
         NVfx3ThZBTR4MOXNE5WDMgcLx/3SWdBQZT4VEf1dn+2ar3upP6dchNLYBmEB3IM/MwKn
         QevnPEGNkKQJZlBPRYeVJKyJTE5noiMx5eIURnsgeQfltLaP1jf7aLg0+98Xk0oJ8krg
         u2jlCCrx8oj9nRRgy46M24vBRmLdlOyPqLPEfUUjM3afof9ZV+oN2lUxifH4M2iv8bhu
         2U9QDLsrP4jkvyv6DT+h4HF61z2oyYDer7SJXfLW/y57bdyX4zbQUxBsKXikQ/MGgaKe
         O/HA==
MIME-Version: 1.0
X-Received: by 10.140.104.176 with SMTP id a45mr54588790qgf.75.1406063847801;
 Tue, 22 Jul 2014 14:17:27 -0700 (PDT)
Received: by 10.140.48.35 with HTTP; Tue, 22 Jul 2014 14:17:27 -0700 (PDT)
In-Reply-To: <CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
References: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
	<CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
Date: Tue, 22 Jul 2014 22:17:27 +0100
Message-ID: <CAKqT-W1OG2qF+b-0Dd1XXv1ntacX1En1K0nE5xEDM9VOuMtwFg@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
From: Neil Ferguson <nferguson@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134f5802e0d5604fecec062
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134f5802e0d5604fecec062
Content-Type: text/plain; charset=UTF-8

Hi Christopher

Thanks for your reply. I'll try and address your points -- please let me
know if I missed anything.

Regarding clarifying the problem statement, let me try and do that with a
real-world example. I have a method that I want to measure the performance
of, which has the following signature at the moment:

def merge(target: IndelRealignmentTarget): IndelRealignmentTarget = { //
impl }

Let's assume I have a Timers class, which contains various timers (that are
internally implemented using Accumulators). Each timer lets me instrument a
function call using its "time" method. Let's imagine that I add that as an
implicit parameter to the above method, as follows:

def merge(target: IndelRealignmentTarget)(implicit timers: Timers):
IndelRealignmentTarget = timers.mergeTarget.time { // impl }

This is not a big problem -- I've had to add an extra parameter to the
method, but it's not a big deal. However, the call stack of this method
looks something like the following:

IndelRealignmentTarget.merge
|-RealignmentTargetFinder.joinTargets
  |-RealignmentTargetFinder.findTargets
    |-RealignmentTargetFinder$.apply
      |-RealignIndels.realignIndels
        |-RealignIndels$.apply
          |-ADAMRecordRDDFunctions.adamRealignIndels
            |-Transform.run

So, I'd have to change every one of these methods to take the extra
parameter, which is pretty cumbersome. More importantly, when developers
want to add additional metrics to the code they'll have to think about how
to get an instance of Timers to the code they're developing.

So I'd really like the Timers object to be available in a thread-local
variable when I need it, without having to pass it around.

Regarding the implications of adding additional variables to SparkContext
-- I'm not sure I understand why this change would make it more difficult
to have multiple SparkContexts in the future. Could you clarify please?
Bear in mind that I'm not proposing adding any thread-local data to
SparkContext. The SparkContext merely holds the data, which is added to a
thread-local variable at task execution time.

Regarding having multiple clients of the SparkContext -- are you talking
about having multiple applications all sharing the same SparkContext? It
seems like there's data in SparkContext that is specific to a particular
application at the moment, like the JAR files for example, so this doesn't
seem inconsistent with that. Perhaps I'm misunderstanding here.

Neil


On Tue, Jul 22, 2014 at 1:54 AM, Christopher Nguyen <ctn@adatao.com> wrote:

> Hi Neil, first off, I'm generally a sympathetic advocate for making changes
> to Spark internals to make it easier/better/faster/more awesome.
>
> In this case, I'm (a) not clear about what you're trying to accomplish, and
> (b) a bit worried about the proposed solution.
>
> On (a): it is stated that you want to pass some Accumulators around. Yet
> the proposed solution is for some "shared" variable that may be set and
> "mapped out" and possibly "reduced back", but without any accompanying
> accumulation semantics. And yet it doesn't seem like you only want just the
> broadcast property. Can you clarify the problem statement with some
> before/after client code examples?
>
> On (b): you're right that adding variables to SparkContext should be done
> with caution, as it may have unintended consequences beyond just serdes
> payload size. For example, there is a stated intention of supporting
> multiple SparkContexts in the future, and this proposed solution can make
> it a bigger challenge to do so. Indeed, we had a gut-wrenching call to make
> a while back on a subject related to this (see
> https://github.com/mesos/spark/pull/779). Furthermore, even in a single
> SparkContext application, there may be multiple "clients" (of that
> application) whose intent to use the proposed "SparkDynamic" would not
> necessarily be coordinated.
>
> So, considering a ratio of a/b (benefit/cost), it's not clear to me that
> the benefits are significant enough to warrant the costs. Do I
> misunderstand that the benefit is to save one explicit parameter (the
> "context") in the signature/closure code?
>
> --
> Christopher T. Nguyen
> Co-founder & CEO, Adatao <http://adatao.com>
> linkedin.com/in/ctnguyen
>
>
>
> On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <nferguson@gmail.com>
> wrote:
>
> > Hi all
> >
> > I have been adding some metrics to the ADAM project
> > https://github.com/bigdatagenomics/adam, which runs on Spark, and have a
> > proposal for an enhancement to Spark that would make this work cleaner
> and
> > easier.
> >
> > I need to pass some Accumulators around, which will aggregate metrics
> > (timing stats and other metrics) across the cluster. However, it is
> > cumbersome to have to explicitly pass some "context" containing these
> > accumulators around everywhere that might need them. I can use Scala
> > implicits, which help slightly, but I'd still need to modify every method
> > in the call stack to take an implicit variable.
> >
> > So, I'd like to propose that we add the ability to have "dynamic
> variables"
> > (basically thread-local variables) to Spark. This would avoid having to
> > pass the Accumulators around explicitly.
> >
> > My proposed approach is to add a method to the SparkContext class as
> > follows:
> >
> > /**
> >  * Sets the value of a "dynamic variable". This value is made available
> to
> > jobs
> >  * without having to be passed around explicitly. During execution of a
> > Spark job
> >  * this value can be obtained from the [[SparkDynamic]] object.
> >  */
> > def setDynamicVariableValue(value: Any)
> >
> > Then, when a job is executing the SparkDynamic can be accessed to obtain
> > the value of the dynamic variable. The implementation of this object is
> as
> > follows:
> >
> > object SparkDynamic {
> >   private val dynamicVariable = new DynamicVariable[Any]()
> >   /**
> >    * Gets the value of the "dynamic variable" that has been set in the
> > [[SparkContext]]
> >    */
> >   def getValue: Option[Any] = {
> >     Option(dynamicVariable.value)
> >   }
> >   private[spark] def withValue[S](threadValue: Option[Any])(thunk: =>
> S): S
> > = {
> >     dynamicVariable.withValue(threadValue.orNull)(thunk)
> >   }
> > }
> >
> > The change involves modifying the Task object to serialize the value of
> the
> > dynamic variable, and modifying the TaskRunner class to deserialize the
> > value and make it available in the thread that is running the task (using
> > the SparkDynamic.withValue method).
> >
> > I have done a quick prototype of this in this commit:
> >
> >
> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
> > and it seems to work fine in my (limited) testing. It needs more testing,
> > tidy-up and documentation though.
> >
> > One drawback is that the dynamic variable will be serialized for every
> Task
> > whether it needs it or not. For my use case this might not be too much
> of a
> > problem, as serializing and deserializing Accumulators looks fairly
> > lightweight -- however we should certainly warn users against setting a
> > dynamic variable containing lots of data. I thought about using broadcast
> > tables here, but I don't think it's possible to put Accumulators in a
> > broadcast table (as I understand it, they're intended for purely
> read-only
> > data).
> >
> > What do people think about this proposal? My use case aside, it seems
> like
> > it would be a generally useful enhancment to be able to pass certain data
> > around without having to explicitly pass it everywhere.
> >
> > Neil
> >
>

--001a1134f5802e0d5604fecec062--

From dev-return-8507-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 22 21:21:34 2014
Return-Path: <dev-return-8507-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 91D1911988
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 22 Jul 2014 21:21:34 +0000 (UTC)
Received: (qmail 22019 invoked by uid 500); 22 Jul 2014 21:21:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21959 invoked by uid 500); 22 Jul 2014 21:21:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21947 invoked by uid 99); 22 Jul 2014 21:21:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 21:21:32 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 22 Jul 2014 21:21:28 +0000
Received: by mail-oa0-f50.google.com with SMTP id g18so403074oah.9
        for <dev@spark.apache.org>; Tue, 22 Jul 2014 14:21:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=lqzFwGmzaC/pp8QEz46P1LaFjPOynB7BqdQmEjjW+NM=;
        b=QNVfAHr9CLjWQcn/oEeeJjKGfeEZ82o2v/fJMcF1XcIDEHHh+s2zhpXr1jQMWsbgD4
         KbVHthfaTAY5qnZ+UqHibqAM7sDlhW6ZKNT77JiHsQiUQJY/Dxq6cog6DTZ1dXA8npNv
         +mVWXed1FZYKpuyyN9DcF/smRfTI7pQBJMsisLFyCm29h2gSXBU5wcLx1EJhEw7QOw2K
         /P1RrT2C9493baDk2G7jSMfp63BQmMqoSQy7nI7j1GDGBoj0A0sfbWxxPmQstP8o5G1V
         sevjksbqxvLHYX2TmQXGu1687Sm6Q/aWo9ITEZC1q4bnLi3myqPsduJqFxRynXsjgZRS
         BBPw==
MIME-Version: 1.0
X-Received: by 10.60.79.104 with SMTP id i8mr52216346oex.67.1406064067447;
 Tue, 22 Jul 2014 14:21:07 -0700 (PDT)
Received: by 10.202.50.194 with HTTP; Tue, 22 Jul 2014 14:21:07 -0700 (PDT)
In-Reply-To: <CAKx7Bf_Da4q=XgEtE1RMryA7koaqOZm2tmod24Y7bH21ut0xmg@mail.gmail.com>
References: <CAKqT-W3QZBmU96DnjsAcg3wfKe=ooLZaB35wFK3HJoHbH5L3Bg@mail.gmail.com>
	<CAGh_TuNv+iXa3dfA3Rzh91zVa16Raey29+y28sD04-bXqdgTxw@mail.gmail.com>
	<CAPh_B=bERSLwz47h0UbWFw3T53hwoBvMCWDa2Pp1qXJ4BdZNDQ@mail.gmail.com>
	<CAKqT-W3eJuSfABAfJ5PYzeusBA27BGHVoRz4OEHDto=Yb86EnQ@mail.gmail.com>
	<CAKx7Bf_Da4q=XgEtE1RMryA7koaqOZm2tmod24Y7bH21ut0xmg@mail.gmail.com>
Date: Tue, 22 Jul 2014 14:21:07 -0700
Message-ID: <CABPQxstaTsp9XbNJgwVHTfG459G4S-ACKMwTgCh0CpawhCFKSg@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Shivaram,

You should take a look at this patch which adds support for naming
accumulators - this is likely to get merged in soon. I actually
started this patch by supporting named TaskMetrics similar to what you
have there, but then I realized there is too much semantic overlap
with accumulators, so I just went that route.

For instance, it would be nice if any user-defined metrics are
accessible at the driver program.

https://github.com/apache/spark/pull/1309

In your example, you could just define an accumulator here on the RDD
and you'd see the incremental update in the web UI automatically.

- Patrick

On Tue, Jul 22, 2014 at 2:09 PM, Shivaram Venkataraman
<shivaram@eecs.berkeley.edu> wrote:
> From reading Neil's first e-mail, I think the motivation is to get some
> metrics in ADAM ? --  I've run into a similar use-case with having
> user-defined metrics in long-running tasks and I think a nice way to solve
> this would be to have user-defined TaskMetrics.
>
> To state my problem more clearly, lets say you have two functions you use
> in a map call and want to measure how much time each of them takes. For
> example, if you have a code block like the one below and you want to
> measure how much time f1 takes as a fraction of the task.
>
> a.map { l =>
>    val f = f1(l)
>    ... some work here ...
> }
>
> It would be really cool if we could do something like
>
> a.map { l =>
>    val start = System.nanoTime
>    val f = f1(l)
>    TaskMetrics.get("f1-time").add(System.nanoTime - start)
> }
>
> These task metrics have a different purpose from Accumulators in the sense
> that we don't need to track lineage, perform commutative operations etc.
>  Further we also have a bunch of code in place to aggregate task metrics
> across a stage etc. So it would be great if we could also populate these in
> the UI and show median/max etc.
> I think counters [1] in Hadoop served a similar purpose.
>
> Thanks
> Shivaram
>
> [1]
> https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/counters
>
>
>
> On Tue, Jul 22, 2014 at 1:43 PM, Neil Ferguson <nferguson@gmail.com> wrote:
>
>> Hi Reynold
>>
>> Thanks for your reply.
>>
>> Accumulators are, of course, stored in the Accumulators object as
>> thread-local variables. However, the Accumulators object isn't public, so
>> when a Task is executing there's no way to get the set of accumulators for
>> the current thread -- accumulators still have to be passed to every method
>> that needs them.
>>
>> Additionally, unless an accumulator is explicitly referenced it won't be
>> serialized as part of a Task, and won't make it into the Accumulators
>> object in the first place.
>>
>> I should also note that what I'm proposing is not specific to Accumulators
>> -- I am proposing that any data can be stored in a thread-local variable. I
>> think there are probably many other use cases other than my one.
>>
>> Neil
>>
>>
>> On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com> wrote:
>>
>> > Thanks for the thoughtful email, Neil and Christopher.
>> >
>> > If I understand this correctly, it seems like the dynamic variable is
>> just
>> > a variant of the accumulator (a static one since it is a global object).
>> > Accumulators are already implemented using thread-local variables under
>> the
>> > hood. Am I misunderstanding something?
>> >
>> >
>> >
>> > On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.com>
>> > wrote:
>> >
>> > > Hi Neil, first off, I'm generally a sympathetic advocate for making
>> > changes
>> > > to Spark internals to make it easier/better/faster/more awesome.
>> > >
>> > > In this case, I'm (a) not clear about what you're trying to accomplish,
>> > and
>> > > (b) a bit worried about the proposed solution.
>> > >
>> > > On (a): it is stated that you want to pass some Accumulators around.
>> Yet
>> > > the proposed solution is for some "shared" variable that may be set and
>> > > "mapped out" and possibly "reduced back", but without any accompanying
>> > > accumulation semantics. And yet it doesn't seem like you only want just
>> > the
>> > > broadcast property. Can you clarify the problem statement with some
>> > > before/after client code examples?
>> > >
>> > > On (b): you're right that adding variables to SparkContext should be
>> done
>> > > with caution, as it may have unintended consequences beyond just serdes
>> > > payload size. For example, there is a stated intention of supporting
>> > > multiple SparkContexts in the future, and this proposed solution can
>> make
>> > > it a bigger challenge to do so. Indeed, we had a gut-wrenching call to
>> > make
>> > > a while back on a subject related to this (see
>> > > https://github.com/mesos/spark/pull/779). Furthermore, even in a
>> single
>> > > SparkContext application, there may be multiple "clients" (of that
>> > > application) whose intent to use the proposed "SparkDynamic" would not
>> > > necessarily be coordinated.
>> > >
>> > > So, considering a ratio of a/b (benefit/cost), it's not clear to me
>> that
>> > > the benefits are significant enough to warrant the costs. Do I
>> > > misunderstand that the benefit is to save one explicit parameter (the
>> > > "context") in the signature/closure code?
>> > >
>> > > --
>> > > Christopher T. Nguyen
>> > > Co-founder & CEO, Adatao <http://adatao.com>
>> > > linkedin.com/in/ctnguyen
>> > >
>> > >
>> > >
>> > > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <nferguson@gmail.com>
>> > > wrote:
>> > >
>> > > > Hi all
>> > > >
>> > > > I have been adding some metrics to the ADAM project
>> > > > https://github.com/bigdatagenomics/adam, which runs on Spark, and
>> > have a
>> > > > proposal for an enhancement to Spark that would make this work
>> cleaner
>> > > and
>> > > > easier.
>> > > >
>> > > > I need to pass some Accumulators around, which will aggregate metrics
>> > > > (timing stats and other metrics) across the cluster. However, it is
>> > > > cumbersome to have to explicitly pass some "context" containing these
>> > > > accumulators around everywhere that might need them. I can use Scala
>> > > > implicits, which help slightly, but I'd still need to modify every
>> > method
>> > > > in the call stack to take an implicit variable.
>> > > >
>> > > > So, I'd like to propose that we add the ability to have "dynamic
>> > > variables"
>> > > > (basically thread-local variables) to Spark. This would avoid having
>> to
>> > > > pass the Accumulators around explicitly.
>> > > >
>> > > > My proposed approach is to add a method to the SparkContext class as
>> > > > follows:
>> > > >
>> > > > /**
>> > > >  * Sets the value of a "dynamic variable". This value is made
>> available
>> > > to
>> > > > jobs
>> > > >  * without having to be passed around explicitly. During execution
>> of a
>> > > > Spark job
>> > > >  * this value can be obtained from the [[SparkDynamic]] object.
>> > > >  */
>> > > > def setDynamicVariableValue(value: Any)
>> > > >
>> > > > Then, when a job is executing the SparkDynamic can be accessed to
>> > obtain
>> > > > the value of the dynamic variable. The implementation of this object
>> is
>> > > as
>> > > > follows:
>> > > >
>> > > > object SparkDynamic {
>> > > >   private val dynamicVariable = new DynamicVariable[Any]()
>> > > >   /**
>> > > >    * Gets the value of the "dynamic variable" that has been set in
>> the
>> > > > [[SparkContext]]
>> > > >    */
>> > > >   def getValue: Option[Any] = {
>> > > >     Option(dynamicVariable.value)
>> > > >   }
>> > > >   private[spark] def withValue[S](threadValue: Option[Any])(thunk: =>
>> > > S): S
>> > > > = {
>> > > >     dynamicVariable.withValue(threadValue.orNull)(thunk)
>> > > >   }
>> > > > }
>> > > >
>> > > > The change involves modifying the Task object to serialize the value
>> of
>> > > the
>> > > > dynamic variable, and modifying the TaskRunner class to deserialize
>> the
>> > > > value and make it available in the thread that is running the task
>> > (using
>> > > > the SparkDynamic.withValue method).
>> > > >
>> > > > I have done a quick prototype of this in this commit:
>> > > >
>> > > >
>> > >
>> >
>> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
>> > > > and it seems to work fine in my (limited) testing. It needs more
>> > testing,
>> > > > tidy-up and documentation though.
>> > > >
>> > > > One drawback is that the dynamic variable will be serialized for
>> every
>> > > Task
>> > > > whether it needs it or not. For my use case this might not be too
>> much
>> > > of a
>> > > > problem, as serializing and deserializing Accumulators looks fairly
>> > > > lightweight -- however we should certainly warn users against
>> setting a
>> > > > dynamic variable containing lots of data. I thought about using
>> > broadcast
>> > > > tables here, but I don't think it's possible to put Accumulators in a
>> > > > broadcast table (as I understand it, they're intended for purely
>> > > read-only
>> > > > data).
>> > > >
>> > > > What do people think about this proposal? My use case aside, it seems
>> > > like
>> > > > it would be a generally useful enhancment to be able to pass certain
>> > data
>> > > > around without having to explicitly pass it everywhere.
>> > > >
>> > > > Neil
>> > > >
>> > >
>> >
>>

From dev-return-8508-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 23 02:51:01 2014
Return-Path: <dev-return-8508-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3DB941140D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Jul 2014 02:51:01 +0000 (UTC)
Received: (qmail 92135 invoked by uid 500); 23 Jul 2014 02:51:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92064 invoked by uid 500); 23 Jul 2014 02:51:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92052 invoked by uid 99); 23 Jul 2014 02:51:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 02:51:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mengxr@gmail.com designates 209.85.212.180 as permitted sender)
Received: from [209.85.212.180] (HELO mail-wi0-f180.google.com) (209.85.212.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 02:50:56 +0000
Received: by mail-wi0-f180.google.com with SMTP id n3so1443192wiv.1
        for <dev@spark.apache.org>; Tue, 22 Jul 2014 19:50:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=GAlegXfnyKHwN1hNHwMHV+U1hZ3dY3/AJW5ZJkAQDuI=;
        b=HPzOLkortreYXE0KJP2J96DWiN/thbVta/6vsQlEPq6ehRYExGd8Um3jiZGT/gm/US
         /iUufHv1wOkRajqHaFWFMlmjGXPxbvCwf2jrmxikE5jP0hBZsCa5TGhZgpqgxmbqwHUW
         hF6dSqbqdh9hEDQGIL2yGQLjGQRjnluxBcYiB4HBb+yhBjsStJdYuO6wvqUjs9a/iQdm
         lVYPhDucsHNDqvVyAW1y4IGV0k/zdX14W3Tv0vWTlxAZM4hQWaGbbWKNO5563p24mqE6
         A10G5TNqU3GYK3DKq+b8foBghERlNki/U66Z9QT6n2rWQ4IO6/MBegFUqtn/f9RHjQM/
         Oc7w==
MIME-Version: 1.0
X-Received: by 10.180.74.11 with SMTP id p11mr20163417wiv.68.1406083835144;
 Tue, 22 Jul 2014 19:50:35 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Tue, 22 Jul 2014 19:50:35 -0700 (PDT)
In-Reply-To: <EBA7027F-7C92-44CE-858F-754A3E872F07@webtrends.com>
References: <CAJgQjQ9KO_vCo=rk5kMZwzwiecGPJ7gKhxbkb27X32tY9hYPAQ@mail.gmail.com>
	<CAJgQjQ-u-t0P+oJGJzv3zHM4u2Dx-_4Dj2_v_rMV7TREA_SVUg@mail.gmail.com>
	<6C2FD5A2-0153-4630-8685-9BB109A6185F@gmail.com>
	<CAPh_B=a3FooEaDYz5ZKANh7zSuQs1kfX2QQ8+cuz8JNExGx5BQ@mail.gmail.com>
	<CAJgQjQ8mhng3d9==CfVqjEc1gUfkruO7xBeUzWM0rnAUHG0VYA@mail.gmail.com>
	<CABPQxsvpHhXUyAaNEyL+Jx5Q9DJTSL76+q9sUAHcNFGusLvzGw@mail.gmail.com>
	<CAMJOb8kDsBv7NL0GFPfBdg9MkCMWgFNusa2c+5gL1u4cQ-Tnfg@mail.gmail.com>
	<EBA7027F-7C92-44CE-858F-754A3E872F07@webtrends.com>
Date: Tue, 22 Jul 2014 19:50:35 -0700
Message-ID: <CAJgQjQ88CECo8Cpdd6zATpU-7xh-qFPGtn6JnSaMCg2n5Mm9Kg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 0.9.2 (RC1)
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

The vote has passed with 7 "+1" votes (4 binding) and 0 "-1" vote:

+1:

Xiangrui Meng*
Matei Zaharia*
DB Tsai
Reynold Xin*
Patrick Wendell*
Andrew Or
Sean McNamara

I'm closing this vote and going to package v0.9.2 today. Thanks
everyone for voting!

Best,
Xiangrui

On Fri, Jul 18, 2014 at 9:36 AM, Sean McNamara
<Sean.McNamara@webtrends.com> wrote:
> +1
>
>
> On Jul 18, 2014, at 2:08 AM, Andrew Or <andrew@databricks.com> wrote:
>
>> +1, tested on standalone cluster and ran spark shell, pyspark and SparkPi
>>
>>
>> 2014-07-18 0:03 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:
>>
>>> +1
>>>
>>> - Looked through the release commits
>>> - Looked through JIRA issues
>>> - Ran the audit tests (one issue with the maven app test, but was also
>>> an issue with 0.9.1 so I think it's my environment)
>>> - Checked sigs/sums
>>>
>>> On Thu, Jul 17, 2014 at 11:13 PM, Xiangrui Meng <mengxr@gmail.com> wrote:
>>>> UPDATE:
>>>>
>>>> The staging repository for this release can be found at:
>>>> https://repository.apache.org/content/repositories/orgapachespark-1023/
>>>>
>>>> The previous repo contains exactly the same content but mutable.
>>>> Thanks Patrick for pointing it out!
>>>>
>>>> -Xiangrui
>>>>
>>>> On Thu, Jul 17, 2014 at 7:52 PM, Reynold Xin <rxin@databricks.com>
>>> wrote:
>>>>> +1
>>>>>
>>>>> On Thursday, July 17, 2014, Matei Zaharia <matei.zaharia@gmail.com>
>>> wrote:
>>>>>
>>>>>> +1
>>>>>>
>>>>>> Tested on Mac, verified CHANGES.txt is good, verified several of the
>>> bug
>>>>>> fixes.
>>>>>>
>>>>>> Matei
>>>>>>
>>>>>> On Jul 17, 2014, at 11:12 AM, Xiangrui Meng <mengxr@gmail.com
>>>>>> <javascript:;>> wrote:
>>>>>>
>>>>>>> I start the voting with a +1.
>>>>>>>
>>>>>>> Ran tests on the release candidates and some basic operations in
>>>>>>> spark-shell and pyspark (local and standalone).
>>>>>>>
>>>>>>> -Xiangrui
>>>>>>>
>>>>>>> On Thu, Jul 17, 2014 at 3:16 AM, Xiangrui Meng <mengxr@gmail.com
>>>>>> <javascript:;>> wrote:
>>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>>> version 0.9.2!
>>>>>>>>
>>>>>>>> The tag to be voted on is v0.9.2-rc1 (commit 4322c0ba):
>>>>>>>>
>>>>>>
>>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=4322c0ba7f411cf9a2483895091440011742246b
>>>>>>>>
>>>>>>>> The release files, including signatures, digests, etc. can be found
>>> at:
>>>>>>>> http://people.apache.org/~meng/spark-0.9.2-rc1/
>>>>>>>>
>>>>>>>> Release artifacts are signed with the following key:
>>>>>>>> https://people.apache.org/keys/committer/meng.asc
>>>>>>>>
>>>>>>>> The staging repository for this release can be found at:
>>>>>>>>
>>>>>>
>>> https://repository.apache.org/service/local/repositories/orgapachespark-1023/content/
>>>>>>>>
>>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>>> http://people.apache.org/~meng/spark-0.9.2-rc1-docs/
>>>>>>>>
>>>>>>>> Please vote on releasing this package as Apache Spark 0.9.2!
>>>>>>>>
>>>>>>>> The vote is open until Sunday, July 20, at 11:10 UTC and passes if
>>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>>
>>>>>>>> [ ] +1 Release this package as Apache Spark 0.9.2
>>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>>
>>>>>>>> To learn more about Apache Spark, please see
>>>>>>>> http://spark.apache.org/
>>>>>>>>
>>>>>>>> === About this release ===
>>>>>>>> This release fixes a few high-priority bugs in 0.9.1 and has a
>>> variety
>>>>>>>> of smaller fixes. The full list is here: http://s.apache.org/d0t.
>>> Some
>>>>>>>> of the more visible patches are:
>>>>>>>>
>>>>>>>> SPARK-2156 and SPARK-1112: Issues with jobs hanging due to akka
>>> frame
>>>>>> size
>>>>>>>> SPARK-2043: ExternalAppendOnlyMap doesn't always find matching keys
>>>>>>>> SPARK-1676: HDFS FileSystems continually pile up in the FS cache
>>>>>>>> SPARK-1775: Unneeded lock in ShuffleMapTask.deserializeInfo
>>>>>>>> SPARK-1870: Secondary jars are not added to executor classpath for
>>> YARN
>>>>>>>>
>>>>>>>> This is the second maintenance release on the 0.9 line. We plan to
>>> make
>>>>>>>> additional maintenance releases as new fixes come in.
>>>>>>>>
>>>>>>>> Best,
>>>>>>>> Xiangrui
>>>>>>
>>>>>>
>>>
>

From dev-return-8509-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 23 05:21:15 2014
Return-Path: <dev-return-8509-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E74161175E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Jul 2014 05:21:15 +0000 (UTC)
Received: (qmail 31061 invoked by uid 500); 23 Jul 2014 05:21:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30802 invoked by uid 500); 23 Jul 2014 05:21:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30137 invoked by uid 99); 23 Jul 2014 05:21:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 05:21:13 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 05:21:10 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so1245192vcb.27
        for <dev@spark.incubator.apache.org>; Tue, 22 Jul 2014 22:20:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=JsqPhIuCYAMY6ok7/lmuTqrDGfbaqVKihZmOVW9rT6Q=;
        b=AtBYtoPunzf1D2HTfVkiPQ3IiYBQU5fRfUcmZTOapsofmi2ymzkfTrNQz2reu9j9sn
         ECs42Q15Y7xHBs1eFo2JM5EkeGgDlpMJelYXwVCNA3ub2+bYJzz8BabBuilEmUjn0gqQ
         fRDNcXFa3abhNW6LiNw0rwMH6gzeHg4FDxhGD7VZSdelp/htgnkjdoqkFuN0rMEqR+DF
         pSbd4n/QchSyJtYhM7LhGFVyw6jGVJfxfF/JHPMdV3ebTpZDzHXj3Ti+rLjN/2I+iOWJ
         2j0TUjRpxD8PnQQXPs8OqAzTJBhOuqS9xv9veW7yoQAhnkSp8qW6+Eq5kl7Ay8mfYHJQ
         Y24w==
X-Received: by 10.220.137.145 with SMTP id w17mr33976942vct.47.1406092845929;
 Tue, 22 Jul 2014 22:20:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.131.39 with HTTP; Tue, 22 Jul 2014 22:20:15 -0700 (PDT)
In-Reply-To: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7ABB@SGTULMMP003.Global.ad.sabre.com>
References: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950@SGTULMMP003.Global.ad.sabre.com>
 <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7ABB@SGTULMMP003.Global.ad.sabre.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Tue, 22 Jul 2014 22:20:15 -0700
Message-ID: <CAMwrk0=5XV-WwCp_qAPFC9oFXkyTgiAEpSrb1iFfjjatzmu8Jg@mail.gmail.com>
Subject: Re: Tranforming flume events using Spark transformation functions
To: "Sundaram, Muthu X." <Muthu.X.Sundaram.ctr@sabre.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3433949a5a7e04fed5800a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3433949a5a7e04fed5800a
Content-Type: text/plain; charset=UTF-8

This is because of the RDD's lazy evaluation! Unless you force a
transformed (mapped/filtered/etc.) RDD to give you back some data (like
RDD.count) or output the data (like RDD.saveAsTextFile()), Spark will not
do anything.

So after the eventData.map(...), if you do take(10) and then print the
result, you should seem 10 items from each batch be printed.

Also you can do the same map operation on the Dstream as well. FYI.

inputDStream.map(...).foreachRDD(...)     is equivalent to
 inputDStream.foreachRDD(     // call rdd.map(...) )

Either way you have to call some RDD "action" (count, collect, take,
saveAsHadoopFile, etc.)  that asks the system to something concrete with
the data.

TD




On Tue, Jul 22, 2014 at 1:55 PM, Sundaram, Muthu X. <
Muthu.X.Sundaram.ctr@sabre.com> wrote:

> I tried to map SparkFlumeEvents to String of RDDs like below. But that map
> and call are not at all executed. I might be doing this in a wrong way. Any
> help would be appreciated.
>
> flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
>               @Override
>               public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws
> Exception {
>                                 System.out.println("<<<<<<Inside for
> each...call>>>>");
>
>                                 JavaRDD<String> records = eventsData.map(
>             new Function<SparkFlumeEvent, String>() {
>                                 @Override
>                 public String call(SparkFlumeEvent flume) throws Exception
> {
>                     String logRecord = null;
>                 AvroFlumeEvent avroEvent = null;
>       ByteBuffer bytePayload = null;
>
>
>       System.out.println("<<<<<<Inside Map..call>>>>");
>                     /* List<SparkFlumeEvent> events = flume.collect();
>                      Iterator<SparkFlumeEvent> batchedEvents =
> events.iterator();
>
>                             SparkFlumeEvent flumeEvent =
> batchedEvents.next();*/
>                             avroEvent = flume.event();
>                             bytePayload = avroEvent.getBody();
>                             logRecord = new String(bytePayload.array());
>
>                                       System.out.println("<<<<Record is" +
> logRecord);
>
>                     return logRecord;
>                 }
>             });
>                                 return null;
> }
>
> -----Original Message-----
> From: Sundaram, Muthu X. [mailto:Muthu.X.Sundaram.ctr@sabre.com]
> Sent: Tuesday, July 22, 2014 10:24 AM
> To: user@spark.apache.org; dev@spark.incubator.apache.org
> Subject: Tranforming flume events using Spark transformation functions
>
> Hi All,
>   I am getting events from flume using following line.
>
>   JavaDStream<SparkFlumeEvent> flumeStream = FlumeUtils.createStream(ssc,
> host, port);
>
> Each event is a delimited record. I like to use some of the transformation
> functions like map and reduce on this. Do I need to convert the
> JavaDStream<SparkFlumeEvent> to JavaDStream<String> or can I apply these
> function directly on this?
>
> I need to do following kind of operations
>
> XXXX                     AA
> YYYYY                    Delta
> TTTTT                    AA
> CCCC                     Southwest
> XXXX                     AA
>
> Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
> Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
> AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket,
> Southwest - 1 ticket.
>
> I have to do transformations like this. Right now I am able to receives
> records. But I am struggling to transform them using spark transformation
> functions since they are not of type JavaRDD<String>.
>
> Can I create new JavaRDD<String>? How do I create new JavaRDD?
>
> I loop through  the events like below
>
> flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
>               @Override
>               public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws
> Exception {
>                      String logRecord = null;
>                      List<SparkFlumeEvent> events = eventsData.collect();
>                      Iterator<SparkFlumeEvent> batchedEvents =
> events.iterator();
>                      long t1 = System.currentTimeMillis();
>                      AvroFlumeEvent avroEvent = null;
>                      ByteBuffer bytePayload = null;
>                      // All the user level data is carried as payload in
> Flume Event
>                      while(batchedEvents.hasNext()) {
>                             SparkFlumeEvent flumeEvent =
> batchedEvents.next();
>                             avroEvent = flumeEvent.event();
>                             bytePayload = avroEvent.getBody();
>                             logRecord = new String(bytePayload.array());
>
>                             System.out.println(">>>>>>>>LOG RECORD = " +
> logRecord); }
>
> Where do I create new JavaRDD<String>? DO I do it before this loop? How do
> I create this JavaRDD<String>?
> In the loop I am able to get every record and I am able to print them.
>
> I appreciate any help here.
>
> Thanks,
> Muthu
>
>
>

--047d7b3433949a5a7e04fed5800a--

From dev-return-8510-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 23 05:21:21 2014
Return-Path: <dev-return-8510-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8612F1175F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Jul 2014 05:21:21 +0000 (UTC)
Received: (qmail 31201 invoked by uid 500); 23 Jul 2014 05:21:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31096 invoked by uid 500); 23 Jul 2014 05:21:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30149 invoked by uid 99); 23 Jul 2014 05:21:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 05:21:13 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 05:21:11 +0000
Received: by mail-vc0-f174.google.com with SMTP id la4so1223225vcb.19
        for <multiple recipients>; Tue, 22 Jul 2014 22:20:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=JsqPhIuCYAMY6ok7/lmuTqrDGfbaqVKihZmOVW9rT6Q=;
        b=AtBYtoPunzf1D2HTfVkiPQ3IiYBQU5fRfUcmZTOapsofmi2ymzkfTrNQz2reu9j9sn
         ECs42Q15Y7xHBs1eFo2JM5EkeGgDlpMJelYXwVCNA3ub2+bYJzz8BabBuilEmUjn0gqQ
         fRDNcXFa3abhNW6LiNw0rwMH6gzeHg4FDxhGD7VZSdelp/htgnkjdoqkFuN0rMEqR+DF
         pSbd4n/QchSyJtYhM7LhGFVyw6jGVJfxfF/JHPMdV3ebTpZDzHXj3Ti+rLjN/2I+iOWJ
         2j0TUjRpxD8PnQQXPs8OqAzTJBhOuqS9xv9veW7yoQAhnkSp8qW6+Eq5kl7Ay8mfYHJQ
         Y24w==
X-Received: by 10.220.137.145 with SMTP id w17mr33976942vct.47.1406092845929;
 Tue, 22 Jul 2014 22:20:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.131.39 with HTTP; Tue, 22 Jul 2014 22:20:15 -0700 (PDT)
In-Reply-To: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7ABB@SGTULMMP003.Global.ad.sabre.com>
References: <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7950@SGTULMMP003.Global.ad.sabre.com>
 <3D464D9FDD998B4AB6CE5B16E4A9A41822D7BD7ABB@SGTULMMP003.Global.ad.sabre.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Tue, 22 Jul 2014 22:20:15 -0700
Message-ID: <CAMwrk0=5XV-WwCp_qAPFC9oFXkyTgiAEpSrb1iFfjjatzmu8Jg@mail.gmail.com>
Subject: Re: Tranforming flume events using Spark transformation functions
To: "Sundaram, Muthu X." <Muthu.X.Sundaram.ctr@sabre.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>, "user@spark.apache.org" <user@spark.apache.org>, 
	"dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3433949a5a7e04fed5800a
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3433949a5a7e04fed5800a
Content-Type: text/plain; charset=UTF-8

This is because of the RDD's lazy evaluation! Unless you force a
transformed (mapped/filtered/etc.) RDD to give you back some data (like
RDD.count) or output the data (like RDD.saveAsTextFile()), Spark will not
do anything.

So after the eventData.map(...), if you do take(10) and then print the
result, you should seem 10 items from each batch be printed.

Also you can do the same map operation on the Dstream as well. FYI.

inputDStream.map(...).foreachRDD(...)     is equivalent to
 inputDStream.foreachRDD(     // call rdd.map(...) )

Either way you have to call some RDD "action" (count, collect, take,
saveAsHadoopFile, etc.)  that asks the system to something concrete with
the data.

TD




On Tue, Jul 22, 2014 at 1:55 PM, Sundaram, Muthu X. <
Muthu.X.Sundaram.ctr@sabre.com> wrote:

> I tried to map SparkFlumeEvents to String of RDDs like below. But that map
> and call are not at all executed. I might be doing this in a wrong way. Any
> help would be appreciated.
>
> flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
>               @Override
>               public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws
> Exception {
>                                 System.out.println("<<<<<<Inside for
> each...call>>>>");
>
>                                 JavaRDD<String> records = eventsData.map(
>             new Function<SparkFlumeEvent, String>() {
>                                 @Override
>                 public String call(SparkFlumeEvent flume) throws Exception
> {
>                     String logRecord = null;
>                 AvroFlumeEvent avroEvent = null;
>       ByteBuffer bytePayload = null;
>
>
>       System.out.println("<<<<<<Inside Map..call>>>>");
>                     /* List<SparkFlumeEvent> events = flume.collect();
>                      Iterator<SparkFlumeEvent> batchedEvents =
> events.iterator();
>
>                             SparkFlumeEvent flumeEvent =
> batchedEvents.next();*/
>                             avroEvent = flume.event();
>                             bytePayload = avroEvent.getBody();
>                             logRecord = new String(bytePayload.array());
>
>                                       System.out.println("<<<<Record is" +
> logRecord);
>
>                     return logRecord;
>                 }
>             });
>                                 return null;
> }
>
> -----Original Message-----
> From: Sundaram, Muthu X. [mailto:Muthu.X.Sundaram.ctr@sabre.com]
> Sent: Tuesday, July 22, 2014 10:24 AM
> To: user@spark.apache.org; dev@spark.incubator.apache.org
> Subject: Tranforming flume events using Spark transformation functions
>
> Hi All,
>   I am getting events from flume using following line.
>
>   JavaDStream<SparkFlumeEvent> flumeStream = FlumeUtils.createStream(ssc,
> host, port);
>
> Each event is a delimited record. I like to use some of the transformation
> functions like map and reduce on this. Do I need to convert the
> JavaDStream<SparkFlumeEvent> to JavaDStream<String> or can I apply these
> function directly on this?
>
> I need to do following kind of operations
>
> XXXX                     AA
> YYYYY                    Delta
> TTTTT                    AA
> CCCC                     Southwest
> XXXX                     AA
>
> Unique tickets are XXXX , YYYYY, TTTT, CCCC, XXXX.
> Count is XXXX 2, YYYY 1, TTTTT 1 and so on...
> AA - 2 tickets(Should not count the duplicate), Delta - 1 ticket,
> Southwest - 1 ticket.
>
> I have to do transformations like this. Right now I am able to receives
> records. But I am struggling to transform them using spark transformation
> functions since they are not of type JavaRDD<String>.
>
> Can I create new JavaRDD<String>? How do I create new JavaRDD?
>
> I loop through  the events like below
>
> flumeStream.foreach(new Function<JavaRDD<SparkFlumeEvent>,Void> () {
>               @Override
>               public Void call(JavaRDD<SparkFlumeEvent> eventsData) throws
> Exception {
>                      String logRecord = null;
>                      List<SparkFlumeEvent> events = eventsData.collect();
>                      Iterator<SparkFlumeEvent> batchedEvents =
> events.iterator();
>                      long t1 = System.currentTimeMillis();
>                      AvroFlumeEvent avroEvent = null;
>                      ByteBuffer bytePayload = null;
>                      // All the user level data is carried as payload in
> Flume Event
>                      while(batchedEvents.hasNext()) {
>                             SparkFlumeEvent flumeEvent =
> batchedEvents.next();
>                             avroEvent = flumeEvent.event();
>                             bytePayload = avroEvent.getBody();
>                             logRecord = new String(bytePayload.array());
>
>                             System.out.println(">>>>>>>>LOG RECORD = " +
> logRecord); }
>
> Where do I create new JavaRDD<String>? DO I do it before this loop? How do
> I create this JavaRDD<String>?
> In the loop I am able to get every record and I am able to print them.
>
> I appreciate any help here.
>
> Thanks,
> Muthu
>
>
>

--047d7b3433949a5a7e04fed5800a--

From dev-return-8511-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 23 07:31:25 2014
Return-Path: <dev-return-8511-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A93ED11A59
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Jul 2014 07:31:25 +0000 (UTC)
Received: (qmail 70738 invoked by uid 500); 23 Jul 2014 07:31:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70679 invoked by uid 500); 23 Jul 2014 07:31:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 70667 invoked by uid 99); 23 Jul 2014 07:31:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 07:31:24 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nferguson@gmail.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 07:31:21 +0000
Received: by mail-qg0-f52.google.com with SMTP id f51so889976qge.25
        for <dev@spark.apache.org>; Wed, 23 Jul 2014 00:30:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=uO3woFstHEySYxe8SqyatoSz/ZzuiKSXgH2QZi32Uk8=;
        b=LVdrCJga2LD05iGR0WKKxjedXW6LG96GppvdkuBXCgwk3jQXtxznrUi1xV0PbyiTwQ
         o/iVmOXGG78ThQIu4cdHdHjcM1WXVPMGiBWGZqwFCXwj292b9zHUQXj4trzVl9GSRdSS
         CHFnRKIyKuam+k6Poet2Fz2ZLtxHTgGl7gBK4A+uKGB0mH9ovZ6OV9+nH2dlfifEl9eD
         SVoHUM0aJ9PfYZJcfNAvlOEc8sGbPIuOD3Pn3lV98YPn4AsTI1g040UBcGvrSrZ4dfFH
         cs+6GEBuFlQjG3Nobjn1v60UluTuB+Pr1ueF+8Iqbqbx/7hxcxAPYr+7ceoLzrd2RPLq
         gLgA==
X-Received: by 10.224.114.138 with SMTP id e10mr66568564qaq.71.1406100656020;
        Wed, 23 Jul 2014 00:30:56 -0700 (PDT)
Received: from hedwig-14.prd.orcali.com (ec2-54-85-253-60.compute-1.amazonaws.com. [54.85.253.60])
        by mx.google.com with ESMTPSA id 98sm2216458qgh.5.2014.07.23.00.30.55
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 23 Jul 2014 00:30:55 -0700 (PDT)
Date: Wed, 23 Jul 2014 00:30:55 -0700 (PDT)
X-Google-Original-Date: Wed, 23 Jul 2014 07:20:54 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1406100054997.e9c355d6@Nodemailer>
In-Reply-To: <CABPQxstaTsp9XbNJgwVHTfG459G4S-ACKMwTgCh0CpawhCFKSg@mail.gmail.com>
References: <CABPQxstaTsp9XbNJgwVHTfG459G4S-ACKMwTgCh0CpawhCFKSg@mail.gmail.com>
X-Orchestra-Oid: 9D351E0C-3127-4713-B772-7FBBC771B400
X-Orchestra-Sig: eb6453798a73943988123a03ccfbf36336fac315
X-Orchestra-Thrid: TFBC64990-E6A9-40EC-BA1D-CD152380485C_1474273764458606556
X-Orchestra-Thrid-Sig: 22cd8238446a1491b2505311a5bd2080ecf7d28c
X-Orchestra-Account: 219b0e7ad8cb2a243bd5d6e9fd3aa36106643289
From: "Neil Ferguson" <nferguson@gmail.com>
To: dev@spark.apache.org
Cc: "Shivaram Venkataraman" <shivaram@eecs.berkeley.edu>,
 dev@spark.apache.org
Subject: Re: "Dynamic variables" in Spark
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1406100655217"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1406100655217
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Hi Patrick.




That looks very useful. The thing that seems to be missing from Shivaram's =
example is the ability to access TaskMetrics statically (this is the same =
problem that I am trying to solve with dynamic variables).






You mention defining an accumulator on the RDD. Perhaps I am missing =
something here, but my understanding was that accumulators are defined in =
SparkContext and are not part of the RDD. Is that correct=3F




Neil


On Tue, Jul 22, 2014 at 22:21, Patrick Wendell <pwendell@gmail.=
com=3D=22mailto:pwendell@gmail.com=22>> wrote:
Shivaram,


You should take a look at this patch which adds support for naming

accumulators - this is likely to get merged in soon. I actually

started this patch by supporting named TaskMetrics similar to what you

have there, but then I realized there is too much semantic overlap

with accumulators, so I just went that route.


For instance, it would be nice if any user-defined metrics are

accessible at the driver program.


https://github.com/apache/spark/pull/1309


In your example, you could just define an accumulator here on the RDD

and you'd see the incremental update in the web UI automatically.


- Patrick


On Tue, Jul 22, 2014 at 2:09 PM, Shivaram Venkataraman

<shivaram@eecs.berkeley.edu> wrote:

> From reading Neil's first e-mail, I think the motivation is to get some

> metrics in ADAM =3F --  I've run into a similar use-case with having

> user-defined metrics in long-running tasks and I think a nice way to =
solve

> this would be to have user-defined TaskMetrics.

>

> To state my problem more clearly, lets say you have two functions you =
use

> in a map call and want to measure how much time each of them takes. For

> example, if you have a code block like the one below and you want to

> measure how much time f1 takes as a fraction of the task.

>

> a.map { l =3D>

>    val f =3D f1(l)

>    ... some work here ...

> }

>

> It would be really cool if we could do something like

>

> a.map { l =3D>

>    val start =3D System.nanoTime

>    val f =3D f1(l)

>    TaskMetrics.get(=22f1-time=22).add(System.nanoTime - start)

> }

>

> These task metrics have a different purpose from Accumulators in the =
sense

> that we don't need to track lineage, perform commutative operations etc.

>  Further we also have a bunch of code in place to aggregate task metrics

> across a stage etc. So it would be great if we could also populate these =
in

> the UI and show median/max etc.

> I think counters [1] in Hadoop served a similar purpose.

>

> Thanks

> Shivaram

>

> [1]

> https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapte=
r-8/counters

>

>

>

> On Tue, Jul 22, 2014 at 1:43 PM, Neil Ferguson <nferguson@gmail.com> =
wrote:

>

>> Hi Reynold

>>

>> Thanks for your reply.

>>

>> Accumulators are, of course, stored in the Accumulators object as

>> thread-local variables. However, the Accumulators object isn't public, =
so

>> when a Task is executing there's no way to get the set of accumulators =
for

>> the current thread -- accumulators still have to be passed to every =
method

>> that needs them.

>>

>> Additionally, unless an accumulator is explicitly referenced it won't =
be

>> serialized as part of a Task, and won't make it into the Accumulators

>> object in the first place.

>>

>> I should also note that what I'm proposing is not specific to =
Accumulators

>> -- I am proposing that any data can be stored in a thread-local variable=
. I

>> think there are probably many other use cases other than my one.

>>

>> Neil

>>

>>

>> On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com> =
wrote:

>>

>> > Thanks for the thoughtful email, Neil and Christopher.

>> >

>> > If I understand this correctly, it seems like the dynamic variable is

>> just

>> > a variant of the accumulator (a static one since it is a global =
object).

>> > Accumulators are already implemented using thread-local variables =
under

>> the

>> > hood. Am I misunderstanding something=3F

>> >

>> >

>> >

>> > On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.com>

>> > wrote:

>> >

>> > > Hi Neil, first off, I'm generally a sympathetic advocate for making

>> > changes

>> > > to Spark internals to make it easier/better/faster/more awesome.

>> > >

>> > > In this case, I'm (a) not clear about what you're trying to =
accomplish,

>> > and

>> > > (b) a bit worried about the proposed solution.

>> > >

>> > > On (a): it is stated that you want to pass some Accumulators around.=


>> Yet

>> > > the proposed solution is for some =22shared=22 variable that may be =
set and

>> > > =22mapped out=22 and possibly =22reduced back=22, but without any =
accompanying

>> > > accumulation semantics. And yet it doesn't seem like you only want =
just

>> > the

>> > > broadcast property. Can you clarify the problem statement with some

>> > > before/after client code examples=3F

>> > >

>> > > On (b): you're right that adding variables to SparkContext should =
be

>> done

>> > > with caution, as it may have unintended consequences beyond just =
serdes

>> > > payload size. For example, there is a stated intention of =
supporting

>> > > multiple SparkContexts in the future, and this proposed solution =
can

>> make

>> > > it a bigger challenge to do so. Indeed, we had a gut-wrenching call =
to

>> > make

>> > > a while back on a subject related to this (see

>> > > https://github.com/mesos/spark/pull/779). Furthermore, even in a

>> single

>> > > SparkContext application, there may be multiple =22clients=22 (of =
that

>> > > application) whose intent to use the proposed =22SparkDynamic=22 =
would not

>> > > necessarily be coordinated.

>> > >

>> > > So, considering a ratio of a/b (benefit/cost), it's not clear to me

>> that

>> > > the benefits are significant enough to warrant the costs. Do I

>> > > misunderstand that the benefit is to save one explicit parameter =
(the

>> > > =22context=22) in the signature/closure code=3F

>> > >

>> > > --

>> > > Christopher T. Nguyen

>> > > Co-founder & CEO, Adatao <http://adatao.com>

>> > > linkedin.com/in/ctnguyen

>> > >

>> > >

>> > >

>> > > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <nferguson@gmail.=
com>

>> > > wrote:

>> > >

>> > > > Hi all

>> > > >

>> > > > I have been adding some metrics to the ADAM project

>> > > > https://github.com/bigdatagenomics/adam, which runs on Spark, and

>> > have a

>> > > > proposal for an enhancement to Spark that would make this work

>> cleaner

>> > > and

>> > > > easier.

>> > > >

>> > > > I need to pass some Accumulators around, which will aggregate =
metrics

>> > > > (timing stats and other metrics) across the cluster. However, it =
is

>> > > > cumbersome to have to explicitly pass some =22context=22 =
containing these

>> > > > accumulators around everywhere that might need them. I can use =
Scala

>> > > > implicits, which help slightly, but I'd still need to modify =
every

>> > method

>> > > > in the call stack to take an implicit variable.

>> > > >

>> > > > So, I'd like to propose that we add the ability to have =
=22dynamic

>> > > variables=22

>> > > > (basically thread-local variables) to Spark. This would avoid =
having

>> to

>> > > > pass the Accumulators around explicitly.

>> > > >

>> > > > My proposed approach is to add a method to the SparkContext class =
as

>> > > > follows:

>> > > >

>> > > > /**

>> > > >  * Sets the value of a =22dynamic variable=22. This value is made

>> available

>> > > to

>> > > > jobs

>> > > >  * without having to be passed around explicitly. During =
execution

>> of a

>> > > > Spark job

>> > > >  * this value can be obtained from the [[SparkDynamic]] object.

>> > > >  */

>> > > > def setDynamicVariableValue(value: Any)

>> > > >

>> > > > Then, when a job is executing the SparkDynamic can be accessed to

>> > obtain

>> > > > the value of the dynamic variable. The implementation of this =
object

>> is

>> > > as

>> > > > follows:

>> > > >

>> > > > object SparkDynamic {

>> > > >   private val dynamicVariable =3D new DynamicVariable[Any]()

>> > > >   /**

>> > > >    * Gets the value of the =22dynamic variable=22 that has been =
set in

>> the

>> > > > [[SparkContext]]

>> > > >    */

>> > > >   def getValue: Option[Any] =3D {

>> > > >     Option(dynamicVariable.value)

>> > > >   }

>> > > >   private[spark] def withValue[S](threadValue: Option[Any])(thunk:=
 =3D>

>> > > S): S

>> > > > =3D {

>> > > >     dynamicVariable.withValue(threadValue.orNull)(thunk)

>> > > >   }

>> > > > }

>> > > >

>> > > > The change involves modifying the Task object to serialize the =
value

>> of

>> > > the

>> > > > dynamic variable, and modifying the TaskRunner class to =
deserialize

>> the

>> > > > value and make it available in the thread that is running the =
task

>> > (using

>> > > > the SparkDynamic.withValue method).

>> > > >

>> > > > I have done a quick prototype of this in this commit:

>> > > >

>> > > >

>> > >

>> >

>> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d2=
73dcea6

>> > > > and it seems to work fine in my (limited) testing. It needs more

>> > testing,

>> > > > tidy-up and documentation though.

>> > > >

>> > > > One drawback is that the dynamic variable will be serialized for

>> every

>> > > Task

>> > > > whether it needs it or not. For my use case this might not be too

>> much

>> > > of a

>> > > > problem, as serializing and deserializing Accumulators looks =
fairly

>> > > > lightweight -- however we should certainly warn users against

>> setting a

>> > > > dynamic variable containing lots of data. I thought about using

>> > broadcast

>> > > > tables here, but I don't think it's possible to put Accumulators =
in a

>> > > > broadcast table (as I understand it, they're intended for purely

>> > > read-only

>> > > > data).

>> > > >

>> > > > What do people think about this proposal=3F My use case aside, it =
seems

>> > > like

>> > > > it would be a generally useful enhancment to be able to pass =
certain

>> > data

>> > > > around without having to explicitly pass it everywhere.

>> > > >

>> > > > Neil

>> > > >

>> > >

>> >

>>
------Nodemailer-0.5.0-?=_1-1406100655217--

From dev-return-8512-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 23 16:53:49 2014
Return-Path: <dev-return-8512-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90F8D10CED
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Jul 2014 16:53:49 +0000 (UTC)
Received: (qmail 76233 invoked by uid 500); 23 Jul 2014 16:53:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76172 invoked by uid 500); 23 Jul 2014 16:53:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76155 invoked by uid 99); 23 Jul 2014 16:53:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 16:53:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 16:53:45 +0000
Received: by mail-wg0-f44.google.com with SMTP id m15so1463912wgh.3
        for <dev@spark.apache.org>; Wed, 23 Jul 2014 09:53:21 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=TUQtBRgXWoitrv/l4X0Mwb+ekDbHAtVaUm//S8Uo4wA=;
        b=P7B7E4RFPxnv6bv4Agglh1qEAu2E54T9ERoRoHWgLNAAYeQTSRKJHFUXSPIOO5HASp
         uOLGEYE8FqeCkeazJiHQDnbD8sZ5+u8i5htpJp7Pgcki7E2S4oAKEdo0N16rkA/K52A6
         fhqyaojIbOKNlBNHejQQlQMcP5eelN/sw1xN8dPo2V8ywQP0yOCx+PdrStfJyVFlYS3v
         HxXVMnJeML/MCk7TdUsMtioklEM/WC13+P8ys3hz3FtcL0H/BeBo3LxyWm0dDMBzsPoJ
         rrRhukoSvbXkgzm0MVzBL8puqNECElbntRuVKHP62v31qMtKnhaGjKxnaL9UhI/P6QgM
         iq7A==
X-Received: by 10.194.6.134 with SMTP id b6mr3866505wja.64.1406134401507; Wed,
 23 Jul 2014 09:53:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Wed, 23 Jul 2014 09:52:41 -0700 (PDT)
In-Reply-To: <CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com>
References: <CABPQxsuSKvwdsBqCDP5fAMqttvknCBmn4FcYj=jxG0WfDgL1Bg@mail.gmail.com>
 <CAOhmDzeW_4TKxGoi=ZHzYF9yK-4H6WRZ3z23KjWpVx_fp2rYUA@mail.gmail.com> <CABPQxsudyaW7YchoFL3TfpRkBaa83vEaUrwvKyrKWkqkmgnvOw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 23 Jul 2014 12:52:41 -0400
Message-ID: <CAOhmDzfJn6311eiH21WUYcvo-u1DsHJvqX1YfEiJdBuK+kY0FQ@mail.gmail.com>
Subject: Re: Pull requests will be automatically linked to JIRA when submitted
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b45011682287404fedf2dd8
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b45011682287404fedf2dd8
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

By the way, it looks like there=E2=80=99s a JIRA plugin that integrates it =
with
GitHub:

   -
   https://marketplace.atlassian.com/plugins/com.atlassian.jira.plugins.jir=
a-bitbucket-connector-plugin
   -
   https://confluence.atlassian.com/display/BITBUCKET/Linking+Bitbucket+and=
+GitHub+accounts+to+JIRA

It does the automatic linking and shows some additional information
<https://marketplace-cdn.atlassian.com/files/images/com.atlassian.jira.plug=
ins.jira-bitbucket-connector-plugin/86ff1a21-44fb-4227-aa4f-44c77aec2c97.pn=
g>
that might be nice to have for heavy JIRA users.

Nick
=E2=80=8B


On Sun, Jul 20, 2014 at 12:50 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Yeah it needs to have SPARK-XXX in the title (this is the format we
> request already). It just works with small synchronization script I
> wrote that we run every five minutes on Jeknins that uses the Github
> and Jenkins API:
>
>
> https://github.com/apache/spark/commit/49e472744951d875627d78b0d6e93cd139=
232929
>
> - Patrick
>
> On Sun, Jul 20, 2014 at 8:06 AM, Nicholas Chammas
> <nicholas.chammas@gmail.com> wrote:
> > That's pretty neat.
> >
> > How does it work? Do we just need to put the issue ID (e.g. SPARK-1234)
> > anywhere in the pull request?
> >
> > Nick
> >
> >
> > On Sat, Jul 19, 2014 at 11:10 PM, Patrick Wendell <pwendell@gmail.com>
> > wrote:
> >
> >> Just a small note, today I committed a tool that will automatically
> >> mirror pull requests to JIRA issues, so contributors will no longer
> >> have to manually post a pull request on the JIRA when they make one.
> >>
> >> It will create a "link" on the JIRA and also make a comment to trigger
> >> an e-mail to people watching.
> >>
> >> This should make some things easier, such as avoiding accidental
> >> duplicate effort on the same JIRA.
> >>
> >> - Patrick
> >>
>

--047d7b45011682287404fedf2dd8--

From dev-return-8513-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 23 22:16:33 2014
Return-Path: <dev-return-8513-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 901EE11AAB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 23 Jul 2014 22:16:33 +0000 (UTC)
Received: (qmail 23167 invoked by uid 500); 23 Jul 2014 22:16:31 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23116 invoked by uid 500); 23 Jul 2014 22:16:31 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22013 invoked by uid 99); 23 Jul 2014 22:16:31 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 22:16:31 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 23 Jul 2014 22:16:28 +0000
Received: by mail-wi0-f175.google.com with SMTP id ho1so8529418wib.2
        for <multiple recipients>; Wed, 23 Jul 2014 15:16:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=6wOl1wph7q0ElobNru1GCPoycU636DC7Ry+UDdxt0Og=;
        b=yOh+iscyFWVV5ceskGkDLH8wppIFK2YfuXm6vBWoVO+0AmnWtdq3N8FcQ9WtUq5ooy
         IgFCWWeaSIXSE7IPCrEao26sm2WYRKXzhm2w7veTmaExIPjKeJ+zziiUwPY7K8mSZH1E
         nKo5ABlAe2fQvYKk5LmLEXkE7DNs/7hKZgGdkNjQflSDFkzDUX3K78LLBggNb03f6j7/
         RfzQ4iI4NpBqRaTgZhl/LY8igyHM3ideZDueBzquxw5+18ShdmCKqMHYnjxLi/U5wWvy
         6bk1jJi9ZCoNk6+QRPapDwYbsmx/czbcjPjad6T8INVeWt7YY4U6sj65tWekRhkFLjiz
         aIgA==
MIME-Version: 1.0
X-Received: by 10.194.6.134 with SMTP id b6mr6335845wja.64.1406153763967; Wed,
 23 Jul 2014 15:16:03 -0700 (PDT)
Received: by 10.194.169.234 with HTTP; Wed, 23 Jul 2014 15:16:03 -0700 (PDT)
Date: Wed, 23 Jul 2014 15:16:03 -0700
Message-ID: <CAJgQjQ8i4VB07xdboB_9TwKUekZQYfLm_1KrC6CkYx5JJ_7RNA@mail.gmail.com>
Subject: Announcing Spark 0.9.2
From: Xiangrui Meng <mengxr@gmail.com>
To: "user@spark.apache.org" <user@spark.apache.org>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

I'm happy to announce the availability of Spark 0.9.2! Spark 0.9.2 is
a maintenance release with bug fixes across several areas of Spark,
including Spark Core, PySpark, MLlib, Streaming, and GraphX. We
recommend all 0.9.x users to upgrade to this stable release.
Contributions to this release came from 28 developers.

Visit the release notes[1] to read about the release and download[2]
the release today.

[1] http://spark.apache.org/releases/spark-release-0-9-2.html
[2] http://spark.apache.org/downloads.html

Best,
Xiangrui

From dev-return-8514-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 05:12:51 2014
Return-Path: <dev-return-8514-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6239311553
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 05:12:51 +0000 (UTC)
Received: (qmail 46807 invoked by uid 500); 24 Jul 2014 05:12:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46745 invoked by uid 500); 24 Jul 2014 05:12:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46730 invoked by uid 99); 24 Jul 2014 05:12:50 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 05:12:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nravi@cloudera.com designates 209.85.218.52 as permitted sender)
Received: from [209.85.218.52] (HELO mail-oi0-f52.google.com) (209.85.218.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 05:12:48 +0000
Received: by mail-oi0-f52.google.com with SMTP id h136so1667701oig.11
        for <dev@spark.apache.org>; Wed, 23 Jul 2014 22:12:23 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=RtTRzrfxQ4YMSEs+N2OkZLssyd0lFy6XmN+SmEyo8W4=;
        b=EGCuABebiMXDZjVxdqsiTsR1nCcGzG5Shu4vTcrs1SPWWB8mSlDgmbzC2bP9iUO+qu
         JztV0+KMjwnSPRdsu3unu3nX9tIZbiCOlVxgWMFxCDqKVMj/Ee7yviVvoBgXEaS6d/cT
         Cyznh6DGcm6jBI6DvMWuY/QP5PZZpkIw69mC5V19dcKDcOytPzULI0Hu3c7IBsemDJp8
         7PUBAdFqCieYVotetVm7kOGB2W4vpYdK4ue1A6z3oH60cqRdPbkahUtO0kFDUN73lycI
         HE1SdMvDnwjmTqyuYESyuA2427w+MsWxg3a3+da2rhpy9siUuWhYRLvkWJcB4k/Wwe+n
         /vOA==
X-Gm-Message-State: ALoCoQni29stL5uvoz8JDnwBIAq4oKwxz+Fh+EJEivRJKP5i7nHG7fY4A8hw5/1U/+uepTUU3S0+
X-Received: by 10.60.125.103 with SMTP id mp7mr8863712oeb.53.1406178743354;
 Wed, 23 Jul 2014 22:12:23 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.182.129.194 with HTTP; Wed, 23 Jul 2014 22:12:03 -0700 (PDT)
In-Reply-To: <CAPbPHg=hG9CNnahtDz7sRMyMsMhAWgTcpN0Va065O7ASTA1qqw@mail.gmail.com>
References: <CAPbPHg=8pUkk0cNvFy-GYSTj-njpcsFKfuQGdgrPx8zBNPtS1Q@mail.gmail.com>
 <CA+-p3AF3rSpzMewXGfCoG3N6VyYMqjmgjV0sVr0d5Uzhd8p=Gg@mail.gmail.com> <CAPbPHg=hG9CNnahtDz7sRMyMsMhAWgTcpN0Va065O7ASTA1qqw@mail.gmail.com>
From: Nishkam Ravi <nravi@cloudera.com>
Date: Wed, 23 Jul 2014 22:12:03 -0700
Message-ID: <CACfA1zWkFZRBMvJ1S+oWGRsCcrE+V0sxZMpxd9S-_nSF8xyi3g@mail.gmail.com>
Subject: Re: Configuring Spark Memory
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/related; boundary=047d7b3442fc7d256504fee980a4
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3442fc7d256504fee980a4
Content-Type: multipart/alternative; boundary=047d7b3442fc7d256104fee980a3

--047d7b3442fc7d256104fee980a3
Content-Type: text/plain; charset=UTF-8

See if this helps:

https://github.com/nishkamravi2/SparkAutoConfig/

It's a very simple tool for auto-configuring default parameters in Spark.
Takes as input high-level parameters (like number of nodes, cores per node,
memory per node, etc) and spits out default configuration, user advice and
command line. Compile (javac SparkConfigure.java) and run (java
SparkConfigure).

Also cc'ing dev in case others are interested in helping evolve this over
time (by refining the heuristics and adding more parameters).


On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <martin@skimlinks.com>
wrote:

> Thanks Andrew,
>
> So if there is only one SparkContext there is only one executor per
> machine? This seems to contradict Aaron's message from the link above:
>
> "If each machine has 16 GB of RAM and 4 cores, for example, you might set
> spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by Spark.)"
>
> Am I reading this incorrectly?
>
> Anyway our configuration is 21 machines (one master and 20 slaves) each
> with 60Gb. We would like to use 4 cores per machine. This is pyspark so we
> want to leave say 16Gb on each machine for python processes.
>
> Thanks again for the advice!
>
>
>
> --
> Martin Goodson  |  VP Data Science
> (0)20 3397 1240
> [image: Inline image 1]
>
>
> On Wed, Jul 23, 2014 at 4:19 PM, Andrew Ash <andrew@andrewash.com> wrote:
>
>> Hi Martin,
>>
>> In standalone mode, each SparkContext you initialize gets its own set of
>> executors across the cluster.  So for example if you have two shells open,
>> they'll each get two JVMs on each worker machine in the cluster.
>>
>> As far as the other docs, you can configure the total number of cores
>> requested for the SparkContext, the amount of memory for the executor JVM
>> on each machine, the amount of memory for the Master/Worker daemons (little
>> needed since work is done in executors), and several other settings.
>>
>> Which of those are you interested in?  What spec hardware do you have and
>> how do you want to configure it?
>>
>> Andrew
>>
>>
>> On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <martin@skimlinks.com>
>> wrote:
>>
>>> We are having difficulties configuring Spark, partly because we still
>>> don't understand some key concepts. For instance, how many executors are
>>> there per machine in standalone mode? This is after having closely read
>>> the documentation several times:
>>>
>>> *http://spark.apache.org/docs/latest/configuration.html
>>> <http://spark.apache.org/docs/latest/configuration.html>*
>>> *http://spark.apache.org/docs/latest/spark-standalone.html
>>> <http://spark.apache.org/docs/latest/spark-standalone.html>*
>>> *http://spark.apache.org/docs/latest/tuning.html
>>> <http://spark.apache.org/docs/latest/tuning.html>*
>>> *http://spark.apache.org/docs/latest/cluster-overview.html
>>> <http://spark.apache.org/docs/latest/cluster-overview.html>*
>>>
>>> The cluster overview has some information here about executors but is
>>> ambiguous about whether there are single executors or multiple executors on
>>> each machine.
>>>
>>>  This message from Aaron Davidson implies that the executor memory
>>> should be set to total available memory on the machine divided by the
>>> number of cores:
>>> *http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E
>>> <http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E>*
>>>
>>> But other messages imply that the executor memory should be set to the
>>> *total* available memory of each machine.
>>>
>>> We would very much appreciate some clarity on this and the myriad of
>>> other memory settings available (daemon memory, worker memory etc). Perhaps
>>> a worked example could be added to the docs? I would be happy to provide
>>> some text as soon as someone can enlighten me on the technicalities!
>>>
>>> Thank you
>>>
>>> --
>>> Martin Goodson  |  VP Data Science
>>> (0)20 3397 1240
>>> [image: Inline image 1]
>>>
>>
>>
>

--047d7b3442fc7d256104fee980a3
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">See if this helps:<div><br></div><div><a href=3D"https://g=
ithub.com/nishkamravi2/SparkAutoConfig/" target=3D"_blank">https://github.c=
om/nishkamravi2/SparkAutoConfig/</a><br></div><div><br></div><div>It&#39;s =
a very simple tool for auto-configuring default parameters in Spark. Takes =
as input high-level parameters (like number of nodes, cores per node, memor=
y per node, etc) and spits out default configuration, user advice and comma=
nd line. Compile (javac SparkConfigure.java) and run (java SparkConfigure).=
</div>


<div><br></div><div>Also cc&#39;ing dev in case others are interested in he=
lping evolve this over time (by refining the heuristics and adding more par=
ameters).=C2=A0</div><div class=3D"gmail_extra"><br><br><div class=3D"gmail=
_quote">

On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>Thanks Andrew,</div><d=
iv><br></div><div>So if there is only one SparkContext there is only one ex=
ecutor per machine? This seems to contradict Aaron&#39;s message from the l=
ink above:</div>


<div><br></div>

&quot;If each machine has 16 GB of RAM and 4 cores, for example, you might =
set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by Spar=
k.)&quot;<div><br></div><div>Am I reading this incorrectly?</div><div>



<br>
</div><div>Anyway our configuration is 21 machines (one master and 20 slave=
s) each with 60Gb. We would like to use 4 cores per machine. This is pyspar=
k so we want to leave say 16Gb on each machine for python processes.</div>




<div><br></div><div>Thanks again for the advice!</div><div><br></div></div>=
<div class=3D"gmail_extra"><div><br clear=3D"all"><div><div dir=3D"ltr"><br=
 style=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,136,136)">--=
=C2=A0</span><br style=3D"color:rgb(136,136,136)">




<div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span style=3D"backgrou=
nd-color:rgb(255,255,204)">Martin Goodson</span><span style=3D"color:rgb(10=
2,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span></div><div style=3D"co=
lor:rgb(34,34,34);font-size:12.7273px">




<span style=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span st=
yle=3D"color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(3=
4,34,34);font-size:12.7273px"><img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"I=
nline image 1"><span style=3D"color:rgb(102,102,102)"><span></span><span></=
span><br>




</span></div></div></div></div>
<br><br></div><div class=3D"gmail_quote"><div>On Wed, Jul 23, 2014 at 4:19 =
PM, Andrew Ash <span dir=3D"ltr">&lt;<a href=3D"mailto:andrew@andrewash.com=
" target=3D"_blank">andrew@andrewash.com</a>&gt;</span> wrote:<br></div>
<div><div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bord=
er-left:1px #ccc solid;padding-left:1ex">

<div dir=3D"ltr">Hi Martin,<div><br></div><div>In standalone mode, each Spa=
rkContext you initialize gets its own set of executors across the cluster. =
=C2=A0So for example if you have two shells open, they&#39;ll each get two =
JVMs on each worker machine in the cluster.</div>






<div><br></div><div>As far as the other docs, you can configure the total n=
umber of cores requested for the SparkContext, the amount of memory for the=
 executor JVM on each machine, the amount of memory for the Master/Worker d=
aemons (little needed since work is done in executors), and several other s=
ettings.</div>






<div><br></div><div>Which of those are you interested in? =C2=A0What spec h=
ardware do you have and how do you want to configure it?</div><span><font c=
olor=3D"#888888"><div><br></div><div>Andrew</div></font></span></div>

<div><div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">






<div dir=3D"ltr"><font face=3D"Arial">We are having difficulties configurin=
g Spark, partly because we still don&#39;t understand some key concepts. Fo=
r instance, how many executors are there per machine in standalone mode</fo=
nt><font face=3D"Arial">?=C2=A0</font><font face=3D"Arial">This is after ha=
ving closely read the documentation several times:</font><br>









<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/configuration.html" target=3D"_=
blank">http://spark.apache.org/docs/latest/configuration.html</a></u></span=
><br>







<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/spark-standalone.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a></=
u></span><br>







<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/tuning.html" target=3D"_blank">=
http://spark.apache.org/docs/latest/tuning.html</a></u></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/cluster-overview.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a></=
u></span><br>







<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">The cluster overview has s=
ome information here about executors but is ambiguous about whether there a=
re single executors or multiple executors on each machine.</span><div><font=
 face=3D"Arial"><br>








</font>
<span style=3D"font-size:13px;font-family:Arial">This message from=C2=A0Aar=
on Davidson implies that the executor memory should be set to total availab=
le memory on the machine divided by the number of cores:</span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C=
CANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E" ta=
rget=3D"_blank">http://mail-archives.apache.org/mod_mbox/spark-user/201312.=
mbox/%3CCANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.co=
m%3E</a></u></span><br>









<span style=3D"font-size:13px;font-family:Arial"></span><br><span style=3D"=
font-size:13px;font-family:Arial">But other messages imply that the executo=
r memory should be set to the <i>total</i>=C2=A0available memory of each ma=
chine.</span><br>









<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">We would very much appreci=
ate some clarity on this and the myriad of other memory settings available =
(daemon memory, worker memory etc). Perhaps a worked example could be added=
 to the docs? I would be happy to provide some text as soon as someone can =
enlighten me on the technicalities!</span><div>








<div><div dir=3D"ltr"><br></div><div dir=3D"ltr">Thank you</div><span><font=
 color=3D"#888888"><div dir=3D"ltr"><br><span style=3D"color:rgb(136,136,13=
6)">--=C2=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"ltr"><d=
iv style=3D"font-size:12.7273px">






<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>

</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">








<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></font></span></div>
</div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div></div></div><br></div>
</blockquote></div><br></div></div>

--047d7b3442fc7d256104fee980a3--
--047d7b3442fc7d256504fee980a4--

From dev-return-8515-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 06:59:57 2014
Return-Path: <dev-return-8515-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC18311893
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 06:59:56 +0000 (UTC)
Received: (qmail 57577 invoked by uid 500); 24 Jul 2014 06:59:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57524 invoked by uid 500); 24 Jul 2014 06:59:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57511 invoked by uid 99); 24 Jul 2014 06:59:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 06:59:55 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xiaodi@sjtu.edu.cn designates 202.112.26.52 as permitted sender)
Received: from [202.112.26.52] (HELO proxy01.sjtu.edu.cn) (202.112.26.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 06:59:48 +0000
Received: from proxy03.sjtu.edu.cn (unknown [202.121.179.33])
	by proxy01.sjtu.edu.cn (Postfix) with ESMTP id 9603E26003D
	for <dev@spark.apache.org>; Thu, 24 Jul 2014 14:59:26 +0800 (CST)
Received: from localhost (localhost [127.0.0.1])
	by proxy03.sjtu.edu.cn (Postfix) with ESMTP id 8C490260B3E
	for <dev@spark.apache.org>; Thu, 24 Jul 2014 14:59:26 +0800 (GMT-8)
X-Virus-Scanned: amavisd-new at 
Received: from proxy03.sjtu.edu.cn ([127.0.0.1])
	by localhost (proxy03.sjtu.edu.cn [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id 3cXQgrhlrbk7 for <dev@spark.apache.org>;
	Thu, 24 Jul 2014 14:59:26 +0800 (GMT-8)
Received: from loca.ipads-lab.se.sjtu.edu.cn (unknown [202.120.40.82])
	(Authenticated sender: xiaodi)
	by proxy03.sjtu.edu.cn (Postfix) with ESMTPSA id 7023D260A78
	for <dev@spark.apache.org>; Thu, 24 Jul 2014 14:59:26 +0800 (GMT-8)
Message-ID: <53D0AECE.2070608@sjtu.edu.cn>
Date: Thu, 24 Jul 2014 14:59:26 +0800
From: Larry Xiao <xiaodi@sjtu.edu.cn>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: GraphX graph partitioning strategy
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 8bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I'm implementing graph partitioning strategy for GraphX, learning from 
researches on graph computing.

I have two questions:

- a specific implement question:
In current design, only vertex ID of src and dst are provided 
(PartitionStrategy.scala).
And some strategies require knowledge about the graph (like degrees) and 
can consist more than one passes to finally produce the partition ID.
So I'm changing the PartitionStrategy.getPartition API to provide more 
info, but I don't want to make it complex. (the current one looks very 
clean)

- an open question:
What advice would you give considering partitioning, considering the 
procedure Spark adopt on graph processing?

Any advice is much appreciated.

Best Regards,
Larry Xiao

Reference

Bipartite-oriented Distributed Graph Partitioning for Big Learning.
PowerLyra: Differentiated Graph Computation and Partitioning on Skewed 
Graphs

From dev-return-8516-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 15:41:26 2014
Return-Path: <dev-return-8516-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4D1FC116F3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 15:41:26 +0000 (UTC)
Received: (qmail 45926 invoked by uid 500); 24 Jul 2014 15:41:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45876 invoked by uid 500); 24 Jul 2014 15:41:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45858 invoked by uid 99); 24 Jul 2014 15:41:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 15:41:25 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of echeipesh@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 15:41:23 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <echeipesh@gmail.com>)
	id 1XAL8c-0002Rc-CS
	for dev@spark.incubator.apache.org; Thu, 24 Jul 2014 08:40:58 -0700
Date: Thu, 24 Jul 2014 08:40:58 -0700 (PDT)
From: Eugene Cheipesh <echeipesh@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1406216458354-7484.post@n3.nabble.com>
Subject: pre-filtered hadoop RDD use case
MIME-Version: 1.0
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Hello,

I have an interesting use case for a pre-filtered RDD. I have two solutions
that I am not entirly happy with and would like to get some feedback and
thoughts. Perhaps it is a use case that could be more explicitly supported
in Spark.

My data has well defined semantics for they key values that I can use to
pre-filter an RDD to exclude those partitions and records that I will not
need from being loaded at all. In most cases this is significant savings.

Essentially the dataset is geographic image tiles, as you would see on
google maps. The entire dataset could be huge, covering an entire continent
at high resolution. But if I want to work with a subset, lets say a single
city, it makes no sense for me to load all the partitions into memory just
so I can filter them as a first step.

First attempt was to extent NewHadoopRDD as follows:

abstract class PreFilteredHadoopRDD[K, V](
    sc : SparkContext,
    inputFormatClass: Class[_ <: InputFormat[K, V]],
    keyClass: Class[K],
    valueClass: Class[V],
    @transient conf: Configuration)
  extends NewHadoopRDD(sc, inputFormatClass, keyClass, valueClass, conf)
{
  /** returns true if specific partition has relevant keys */
  def includePartition(p: Partition): Boolean

  /** returns true if the specific key in the partition passes the filter *=
/
  def includeKey(key: K): Boolean

  override def getPartitions: Array[Partition] =3D {
    val partitions =3D super.getPartitions
    partitions.filter(includePartition)
  }

  override def compute(theSplit: Partition, context: TaskContext) =3D {
    val ii =3D super.compute(theSplit, context)
    new InterruptibleIterator(ii.context, ii.delegate.filter{case (k,v) =3D=
>
includeKey(k)})
  }
}=20

NewHadoopRDD for reference:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/=
spark/rdd/NewHadoopRDD.scala

This is nice and handles the partition portion of the issue well enough, bu=
t
by the time the iterator is created by super.compute there is no way avoid
reading the values from records that do not pass my filter.=20

Since I am actually using =E2=80=98SequenceFileInputFormat=E2=80=99 as my I=
nputFormat I can
do better, and avoid deserializing the values if I could get my hands on th=
e
reader and re-implement compute(). But this does not seem possible to do
through extension because both the NewHadooprRDD.confBroadcast and
NewHadoopPartition are private. There  does not seem to be a choice but to
copy/paste extend the NewHadoopRDD.

The two solutions that are apparent are:
1. remove those private modifiers
2. factor out reader creation to a method that can be used to reimplement
compute() in a sub-class

I would be curious to hear if anybody had/has similar problem and any
thoughts on the issue. If you think there is PR in this I=E2=80=99d be happ=
y to code
it up and submit it.


Thank you
--
Eugene Cheipesh



--
View this message in context: http://apache-spark-developers-list.1001551.n=
3.nabble.com/pre-filtered-hadoop-RDD-use-case-tp7484.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.c=
om.

From dev-return-8517-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 17:10:15 2014
Return-Path: <dev-return-8517-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A1E9411A38
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 17:10:15 +0000 (UTC)
Received: (qmail 86002 invoked by uid 500); 24 Jul 2014 17:10:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85898 invoked by uid 500); 24 Jul 2014 17:10:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 84918 invoked by uid 99); 24 Jul 2014 17:10:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 17:10:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.216.42 as permitted sender)
Received: from [209.85.216.42] (HELO mail-qa0-f42.google.com) (209.85.216.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 17:10:09 +0000
Received: by mail-qa0-f42.google.com with SMTP id j15so3289241qaq.1
        for <multiple recipients>; Thu, 24 Jul 2014 10:09:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=YB1hYnGZx8HnX6+r4Hoxu4GOOSXsMACO9Br+yJz0+HQ=;
        b=IooyvM3Lc/znG3pSgmSiWLhrbv4S7UwQ4z2efWQqR+TxRV2eOUwQ1rxvkNLNDbfIqr
         lLcaEcx8cMh6Qf3FUshWjWJfueI7r5hSwRSUrYuxDKv7g00HO731yCH2se4V8qC7GqJc
         CNP8U1Wj5IFYqEPnJim+BiNeLdx47c8wA5yrlBC61d9xOFOUDlyH2z4FfmiOGunHZ2dZ
         BlZzOeMJu3lm4NTGgqaj02/rPklR9S89GfGDCDym0KT4z2SrNjJEpf7ygAioznH51xt+
         VdO5tM7QH5sgJH/y7bSMlczTuhf0gJe1F0wgUspsv5enroNp+e722YsO8tmGuAJhONBI
         MvpA==
X-Received: by 10.140.87.75 with SMTP id q69mr16572609qgd.94.1406221783972;
 Thu, 24 Jul 2014 10:09:43 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Thu, 24 Jul 2014 10:09:23 -0700 (PDT)
In-Reply-To: <CAPbPHgmZ0uDsH420-1cT_3ng0NTnFibrscrZKqn5nZ8fUeSdog@mail.gmail.com>
References: <CAPbPHg=8pUkk0cNvFy-GYSTj-njpcsFKfuQGdgrPx8zBNPtS1Q@mail.gmail.com>
 <CA+-p3AF3rSpzMewXGfCoG3N6VyYMqjmgjV0sVr0d5Uzhd8p=Gg@mail.gmail.com>
 <CAPbPHg=hG9CNnahtDz7sRMyMsMhAWgTcpN0Va065O7ASTA1qqw@mail.gmail.com>
 <CACfA1zWkFZRBMvJ1S+oWGRsCcrE+V0sxZMpxd9S-_nSF8xyi3g@mail.gmail.com> <CAPbPHgmZ0uDsH420-1cT_3ng0NTnFibrscrZKqn5nZ8fUeSdog@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Thu, 24 Jul 2014 10:09:23 -0700
Message-ID: <CANGvG8odA4ZTBmdM0EaNbAnY8dERx34zLvMgdJocWo8WXTqmJw@mail.gmail.com>
Subject: Re: Configuring Spark Memory
To: user@spark.apache.org
Cc: dev@spark.apache.org
Content-Type: multipart/related; boundary=001a113a5afce8dada04fef385f2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a5afce8dada04fef385f2
Content-Type: multipart/alternative; boundary=001a113a5afce8dad504fef385f1

--001a113a5afce8dad504fef385f1
Content-Type: text/plain; charset=UTF-8

Whoops, I was mistaken in my original post last year. By default, there is
one executor per node per Spark Context, as you said.
"spark.executor.memory" is the amount of memory that the application
requests for each of its executors. SPARK_WORKER_MEMORY is the amount of
memory a Spark Worker is willing to allocate in executors.

So if you were to set SPARK_WORKER_MEMORY to 8g everywhere on your cluster,
and spark.executor.memory to 4g, you would be able to run 2 simultaneous
Spark Contexts who get 4g per node. Similarly, if spark.executor.memory
were 8g, you could only run 1 Spark Context at a time on the cluster, but
it would get all the cluster's memory.


On Thu, Jul 24, 2014 at 7:25 AM, Martin Goodson <martin@skimlinks.com>
wrote:

> Thank you Nishkam,
> I have read your code. So, for the sake of my understanding, it seems that
> for each spark context there is one executor per node? Can anyone confirm
> this?
>
>
> --
> Martin Goodson  |  VP Data Science
> (0)20 3397 1240
> [image: Inline image 1]
>
>
> On Thu, Jul 24, 2014 at 6:12 AM, Nishkam Ravi <nravi@cloudera.com> wrote:
>
>> See if this helps:
>>
>> https://github.com/nishkamravi2/SparkAutoConfig/
>>
>> It's a very simple tool for auto-configuring default parameters in Spark.
>> Takes as input high-level parameters (like number of nodes, cores per node,
>> memory per node, etc) and spits out default configuration, user advice and
>> command line. Compile (javac SparkConfigure.java) and run (java
>> SparkConfigure).
>>
>> Also cc'ing dev in case others are interested in helping evolve this over
>> time (by refining the heuristics and adding more parameters).
>>
>>
>>  On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <martin@skimlinks.com>
>> wrote:
>>
>>> Thanks Andrew,
>>>
>>> So if there is only one SparkContext there is only one executor per
>>> machine? This seems to contradict Aaron's message from the link above:
>>>
>>> "If each machine has 16 GB of RAM and 4 cores, for example, you might
>>> set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by
>>> Spark.)"
>>>
>>> Am I reading this incorrectly?
>>>
>>> Anyway our configuration is 21 machines (one master and 20 slaves) each
>>> with 60Gb. We would like to use 4 cores per machine. This is pyspark so we
>>> want to leave say 16Gb on each machine for python processes.
>>>
>>> Thanks again for the advice!
>>>
>>>
>>>
>>> --
>>> Martin Goodson  |  VP Data Science
>>> (0)20 3397 1240
>>> [image: Inline image 1]
>>>
>>>
>>> On Wed, Jul 23, 2014 at 4:19 PM, Andrew Ash <andrew@andrewash.com>
>>> wrote:
>>>
>>>> Hi Martin,
>>>>
>>>> In standalone mode, each SparkContext you initialize gets its own set
>>>> of executors across the cluster.  So for example if you have two shells
>>>> open, they'll each get two JVMs on each worker machine in the cluster.
>>>>
>>>> As far as the other docs, you can configure the total number of cores
>>>> requested for the SparkContext, the amount of memory for the executor JVM
>>>> on each machine, the amount of memory for the Master/Worker daemons (little
>>>> needed since work is done in executors), and several other settings.
>>>>
>>>> Which of those are you interested in?  What spec hardware do you have
>>>> and how do you want to configure it?
>>>>
>>>> Andrew
>>>>
>>>>
>>>> On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <martin@skimlinks.com>
>>>> wrote:
>>>>
>>>>> We are having difficulties configuring Spark, partly because we still
>>>>> don't understand some key concepts. For instance, how many executors are
>>>>> there per machine in standalone mode? This is after having closely
>>>>> read the documentation several times:
>>>>>
>>>>> *http://spark.apache.org/docs/latest/configuration.html
>>>>> <http://spark.apache.org/docs/latest/configuration.html>*
>>>>> *http://spark.apache.org/docs/latest/spark-standalone.html
>>>>> <http://spark.apache.org/docs/latest/spark-standalone.html>*
>>>>> *http://spark.apache.org/docs/latest/tuning.html
>>>>> <http://spark.apache.org/docs/latest/tuning.html>*
>>>>> *http://spark.apache.org/docs/latest/cluster-overview.html
>>>>> <http://spark.apache.org/docs/latest/cluster-overview.html>*
>>>>>
>>>>> The cluster overview has some information here about executors but is
>>>>> ambiguous about whether there are single executors or multiple executors on
>>>>> each machine.
>>>>>
>>>>>  This message from Aaron Davidson implies that the executor memory
>>>>> should be set to total available memory on the machine divided by the
>>>>> number of cores:
>>>>> *http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E
>>>>> <http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E>*
>>>>>
>>>>> But other messages imply that the executor memory should be set to the
>>>>> *total* available memory of each machine.
>>>>>
>>>>> We would very much appreciate some clarity on this and the myriad of
>>>>> other memory settings available (daemon memory, worker memory etc). Perhaps
>>>>> a worked example could be added to the docs? I would be happy to provide
>>>>> some text as soon as someone can enlighten me on the technicalities!
>>>>>
>>>>> Thank you
>>>>>
>>>>> --
>>>>> Martin Goodson  |  VP Data Science
>>>>> (0)20 3397 1240
>>>>> [image: Inline image 1]
>>>>>
>>>>
>>>>
>>>
>>
>

--001a113a5afce8dad504fef385f1
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Whoops, I was mistaken in my original post last year. By d=
efault, there is one executor per node per Spark Context, as you said. &quo=
t;spark.executor.memory&quot; is the amount of memory that the application =
requests for each of its executors. SPARK_WORKER_MEMORY is the amount of me=
mory a Spark Worker is willing to allocate in executors.<div>

<br></div><div>So if you were to set SPARK_WORKER_MEMORY to 8g everywhere o=
n your cluster, and spark.executor.memory to 4g, you would be able to run 2=
 simultaneous Spark Contexts who get 4g per node. Similarly, if spark.execu=
tor.memory were 8g, you could only run 1 Spark Context at a time on the clu=
ster, but it would get all the cluster&#39;s memory.</div>

</div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">On Thu,=
 Jul 24, 2014 at 7:25 AM, Martin Goodson <span dir=3D"ltr">&lt;<a href=3D"m=
ailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</a>&gt;<=
/span> wrote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">Thank you=C2=A0Nishkam,<div=
>I have read your code. So, for the sake of my understanding, it seems that=
 for each spark context there is one executor per node? Can anyone confirm =
this?</div>

</div><div class=3D"gmail_extra"><div class=3D"">

<br clear=3D"all"><div><div dir=3D"ltr"><br style=3D"color:rgb(136,136,136)=
"><span style=3D"color:rgb(136,136,136)">--=C2=A0</span><br style=3D"color:=
rgb(136,136,136)"><div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span=
 style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><span sty=
le=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>



</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">



<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></div>
<br><br></div><div><div class=3D"h5"><div class=3D"gmail_quote">On Thu, Jul=
 24, 2014 at 6:12 AM, Nishkam Ravi <span dir=3D"ltr">&lt;<a href=3D"mailto:=
nravi@cloudera.com" target=3D"_blank">nravi@cloudera.com</a>&gt;</span> wro=
te:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">

<div dir=3D"ltr">See if this helps:<div><br></div><div><a href=3D"https://g=
ithub.com/nishkamravi2/SparkAutoConfig/" target=3D"_blank">https://github.c=
om/nishkamravi2/SparkAutoConfig/</a><br></div><div><br></div><div>It&#39;s =
a very simple tool for auto-configuring default parameters in Spark. Takes =
as input high-level parameters (like number of nodes, cores per node, memor=
y per node, etc) and spits out default configuration, user advice and comma=
nd line. Compile (javac SparkConfigure.java) and run (java SparkConfigure).=
</div>






<div><br></div><div>Also cc&#39;ing dev in case others are interested in he=
lping evolve this over time (by refining the heuristics and adding more par=
ameters).=C2=A0</div><div><div><div class=3D"gmail_extra"><br><br>

<div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>Thanks Andrew,</div><d=
iv><br></div><div>So if there is only one SparkContext there is only one ex=
ecutor per machine? This seems to contradict Aaron&#39;s message from the l=
ink above:</div>






<div><br></div>

&quot;If each machine has 16 GB of RAM and 4 cores, for example, you might =
set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by Spar=
k.)&quot;<div><br></div><div>Am I reading this incorrectly?</div><div>







<br>
</div><div>Anyway our configuration is 21 machines (one master and 20 slave=
s) each with 60Gb. We would like to use 4 cores per machine. This is pyspar=
k so we want to leave say 16Gb on each machine for python processes.</div>








<div><br></div><div>Thanks again for the advice!</div><div><br></div></div>=
<div class=3D"gmail_extra"><div><br clear=3D"all"><div><div dir=3D"ltr"><br=
 style=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,136,136)">--=
=C2=A0</span><br style=3D"color:rgb(136,136,136)">








<div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span style=3D"backgrou=
nd-color:rgb(255,255,204)">Martin Goodson</span><span style=3D"color:rgb(10=
2,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span></div><div style=3D"co=
lor:rgb(34,34,34);font-size:12.7273px">








<span style=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span st=
yle=3D"color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(3=
4,34,34);font-size:12.7273px"><img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"I=
nline image 1"><span style=3D"color:rgb(102,102,102)"><span></span><span></=
span><br>








</span></div></div></div></div>
<br><br></div><div class=3D"gmail_quote"><div>On Wed, Jul 23, 2014 at 4:19 =
PM, Andrew Ash <span dir=3D"ltr">&lt;<a href=3D"mailto:andrew@andrewash.com=
" target=3D"_blank">andrew@andrewash.com</a>&gt;</span> wrote:<br></div>
<div><div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bord=
er-left:1px #ccc solid;padding-left:1ex">

<div dir=3D"ltr">Hi Martin,<div><br></div><div>In standalone mode, each Spa=
rkContext you initialize gets its own set of executors across the cluster. =
=C2=A0So for example if you have two shells open, they&#39;ll each get two =
JVMs on each worker machine in the cluster.</div>










<div><br></div><div>As far as the other docs, you can configure the total n=
umber of cores requested for the SparkContext, the amount of memory for the=
 executor JVM on each machine, the amount of memory for the Master/Worker d=
aemons (little needed since work is done in executors), and several other s=
ettings.</div>










<div><br></div><div>Which of those are you interested in? =C2=A0What spec h=
ardware do you have and how do you want to configure it?</div><span><font c=
olor=3D"#888888"><div><br></div><div>Andrew</div></font></span></div>

<div><div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">










<div dir=3D"ltr"><font face=3D"Arial">We are having difficulties configurin=
g Spark, partly because we still don&#39;t understand some key concepts. Fo=
r instance, how many executors are there per machine in standalone mode</fo=
nt><font face=3D"Arial">?=C2=A0</font><font face=3D"Arial">This is after ha=
ving closely read the documentation several times:</font><br>













<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/configuration.html" target=3D"_=
blank">http://spark.apache.org/docs/latest/configuration.html</a></u></span=
><br>











<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/spark-standalone.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a></=
u></span><br>











<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/tuning.html" target=3D"_blank">=
http://spark.apache.org/docs/latest/tuning.html</a></u></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/cluster-overview.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a></=
u></span><br>











<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">The cluster overview has s=
ome information here about executors but is ambiguous about whether there a=
re single executors or multiple executors on each machine.</span><div><font=
 face=3D"Arial"><br>












</font>
<span style=3D"font-size:13px;font-family:Arial">This message from=C2=A0Aar=
on Davidson implies that the executor memory should be set to total availab=
le memory on the machine divided by the number of cores:</span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C=
CANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E" ta=
rget=3D"_blank">http://mail-archives.apache.org/mod_mbox/spark-user/201312.=
mbox/%3CCANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.co=
m%3E</a></u></span><br>













<span style=3D"font-size:13px;font-family:Arial"></span><br><span style=3D"=
font-size:13px;font-family:Arial">But other messages imply that the executo=
r memory should be set to the <i>total</i>=C2=A0available memory of each ma=
chine.</span><br>













<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">We would very much appreci=
ate some clarity on this and the myriad of other memory settings available =
(daemon memory, worker memory etc). Perhaps a worked example could be added=
 to the docs? I would be happy to provide some text as soon as someone can =
enlighten me on the technicalities!</span><div>












<div><div dir=3D"ltr"><br></div><div dir=3D"ltr">Thank you</div><span><font=
 color=3D"#888888"><div dir=3D"ltr"><br><span style=3D"color:rgb(136,136,13=
6)">--=C2=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"ltr"><d=
iv style=3D"font-size:12.7273px">










<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>

</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">












<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></font></span></div>
</div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div></div></div><br></div>
</blockquote></div><br></div></div></div></div>
</blockquote></div><br></div></div></div>
</blockquote></div><br></div>

--001a113a5afce8dad504fef385f1--
--001a113a5afce8dada04fef385f2--

From dev-return-8518-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 17:16:21 2014
Return-Path: <dev-return-8518-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E694711A70
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 17:16:20 +0000 (UTC)
Received: (qmail 10700 invoked by uid 500); 24 Jul 2014 17:16:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10646 invoked by uid 500); 24 Jul 2014 17:16:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 6949 invoked by uid 99); 24 Jul 2014 17:15:07 -0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of martin@skimlinks.com designates 209.85.213.180 as permitted sender)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=YD4XG9t0L+CpMvZM5Iani4sWYDpexr6nY5uPy6bgDm4=;
        b=UTyJktBH2UzmlP7YyYB7O0EeSXSpii5QGsPBHscthuUmczvGoJ1lw6STmzB6zQ5cLY
         Nbk01Qnfj+5aXXxKFjPlatYUC5Dk/FAyCH1UuWHGdwyOK/zi6m0jqQZu9Af4+C/hqFVv
         lBF6Xr7aOaUGJ5l4Qj4er2OdBufaujYkjNTvl6tU+FiT9Yxa7GdAXcNtUvFfUWJ2hhoV
         wGHPQ5ewSkxb7CcUVmZvsNByxRK/1nsphV0zL1zZCYZtB2Ak4kys8gAFsMl9vxK9BSbP
         Fd7b76FeCmC6GHacZkQW1J1RVHfdXv95oZyJqt2hBTg01IGDfIk0VmHElmmukEA1o7x0
         rDRQ==
X-Gm-Message-State: ALoCoQn+tSSBjlo2rLwu8U7kvsdBpY+sHu0yOEBiWGRKxZSp7XJdji8rPFsQNjlSJ9c8cgT5Ai2o
X-Received: by 10.42.82.6 with SMTP id b6mr11230822icl.51.1406222079368; Thu,
 24 Jul 2014 10:14:39 -0700 (PDT)
MIME-Version: 1.0
In-Reply-To: <CANGvG8odA4ZTBmdM0EaNbAnY8dERx34zLvMgdJocWo8WXTqmJw@mail.gmail.com>
References: <CAPbPHg=8pUkk0cNvFy-GYSTj-njpcsFKfuQGdgrPx8zBNPtS1Q@mail.gmail.com>
 <CA+-p3AF3rSpzMewXGfCoG3N6VyYMqjmgjV0sVr0d5Uzhd8p=Gg@mail.gmail.com>
 <CAPbPHg=hG9CNnahtDz7sRMyMsMhAWgTcpN0Va065O7ASTA1qqw@mail.gmail.com>
 <CACfA1zWkFZRBMvJ1S+oWGRsCcrE+V0sxZMpxd9S-_nSF8xyi3g@mail.gmail.com>
 <CAPbPHgmZ0uDsH420-1cT_3ng0NTnFibrscrZKqn5nZ8fUeSdog@mail.gmail.com> <CANGvG8odA4ZTBmdM0EaNbAnY8dERx34zLvMgdJocWo8WXTqmJw@mail.gmail.com>
From: Martin Goodson <martin@skimlinks.com>
Date: Thu, 24 Jul 2014 18:14:19 +0100
Message-ID: <CAPbPHgm2KiaadLHMKXU2vD5u9nQQnLEbi4z0_W8gbA_i6NDREQ@mail.gmail.com>
Subject: Re: Configuring Spark Memory
To: user@spark.apache.org
Cc: dev@spark.apache.org
Content-Type: multipart/related; boundary=485b397dd701844b2404fef39761
X-Virus-Checked: Checked by ClamAV on apache.org

--485b397dd701844b2404fef39761
Content-Type: multipart/alternative; boundary=485b397dd701844b2104fef39760

--485b397dd701844b2104fef39760
Content-Type: text/plain; charset=UTF-8

Great - thanks for the clarification Aaron. The offer stands for me to
write some documentation and an example that covers this without leaving
*any* room for ambiguity.




-- 
Martin Goodson  |  VP Data Science
(0)20 3397 1240
[image: Inline image 1]


On Thu, Jul 24, 2014 at 6:09 PM, Aaron Davidson <ilikerps@gmail.com> wrote:

> Whoops, I was mistaken in my original post last year. By default, there is
> one executor per node per Spark Context, as you said.
> "spark.executor.memory" is the amount of memory that the application
> requests for each of its executors. SPARK_WORKER_MEMORY is the amount of
> memory a Spark Worker is willing to allocate in executors.
>
> So if you were to set SPARK_WORKER_MEMORY to 8g everywhere on your
> cluster, and spark.executor.memory to 4g, you would be able to run 2
> simultaneous Spark Contexts who get 4g per node. Similarly, if
> spark.executor.memory were 8g, you could only run 1 Spark Context at a time
> on the cluster, but it would get all the cluster's memory.
>
>
> On Thu, Jul 24, 2014 at 7:25 AM, Martin Goodson <martin@skimlinks.com>
> wrote:
>
>> Thank you Nishkam,
>> I have read your code. So, for the sake of my understanding, it seems
>> that for each spark context there is one executor per node? Can anyone
>> confirm this?
>>
>>
>> --
>> Martin Goodson  |  VP Data Science
>> (0)20 3397 1240
>> [image: Inline image 1]
>>
>>
>> On Thu, Jul 24, 2014 at 6:12 AM, Nishkam Ravi <nravi@cloudera.com> wrote:
>>
>>> See if this helps:
>>>
>>> https://github.com/nishkamravi2/SparkAutoConfig/
>>>
>>> It's a very simple tool for auto-configuring default parameters in
>>> Spark. Takes as input high-level parameters (like number of nodes, cores
>>> per node, memory per node, etc) and spits out default configuration, user
>>> advice and command line. Compile (javac SparkConfigure.java) and run (java
>>> SparkConfigure).
>>>
>>> Also cc'ing dev in case others are interested in helping evolve this
>>> over time (by refining the heuristics and adding more parameters).
>>>
>>>
>>>  On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <martin@skimlinks.com>
>>> wrote:
>>>
>>>> Thanks Andrew,
>>>>
>>>> So if there is only one SparkContext there is only one executor per
>>>> machine? This seems to contradict Aaron's message from the link above:
>>>>
>>>> "If each machine has 16 GB of RAM and 4 cores, for example, you might
>>>> set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by
>>>> Spark.)"
>>>>
>>>> Am I reading this incorrectly?
>>>>
>>>> Anyway our configuration is 21 machines (one master and 20 slaves) each
>>>> with 60Gb. We would like to use 4 cores per machine. This is pyspark so we
>>>> want to leave say 16Gb on each machine for python processes.
>>>>
>>>> Thanks again for the advice!
>>>>
>>>>
>>>>
>>>> --
>>>> Martin Goodson  |  VP Data Science
>>>> (0)20 3397 1240
>>>> [image: Inline image 1]
>>>>
>>>>
>>>> On Wed, Jul 23, 2014 at 4:19 PM, Andrew Ash <andrew@andrewash.com>
>>>> wrote:
>>>>
>>>>> Hi Martin,
>>>>>
>>>>> In standalone mode, each SparkContext you initialize gets its own set
>>>>> of executors across the cluster.  So for example if you have two shells
>>>>> open, they'll each get two JVMs on each worker machine in the cluster.
>>>>>
>>>>> As far as the other docs, you can configure the total number of cores
>>>>> requested for the SparkContext, the amount of memory for the executor JVM
>>>>> on each machine, the amount of memory for the Master/Worker daemons (little
>>>>> needed since work is done in executors), and several other settings.
>>>>>
>>>>> Which of those are you interested in?  What spec hardware do you have
>>>>> and how do you want to configure it?
>>>>>
>>>>> Andrew
>>>>>
>>>>>
>>>>> On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <martin@skimlinks.com>
>>>>> wrote:
>>>>>
>>>>>> We are having difficulties configuring Spark, partly because we still
>>>>>> don't understand some key concepts. For instance, how many executors are
>>>>>> there per machine in standalone mode? This is after having closely
>>>>>> read the documentation several times:
>>>>>>
>>>>>> *http://spark.apache.org/docs/latest/configuration.html
>>>>>> <http://spark.apache.org/docs/latest/configuration.html>*
>>>>>> *http://spark.apache.org/docs/latest/spark-standalone.html
>>>>>> <http://spark.apache.org/docs/latest/spark-standalone.html>*
>>>>>> *http://spark.apache.org/docs/latest/tuning.html
>>>>>> <http://spark.apache.org/docs/latest/tuning.html>*
>>>>>> *http://spark.apache.org/docs/latest/cluster-overview.html
>>>>>> <http://spark.apache.org/docs/latest/cluster-overview.html>*
>>>>>>
>>>>>> The cluster overview has some information here about executors but is
>>>>>> ambiguous about whether there are single executors or multiple executors on
>>>>>> each machine.
>>>>>>
>>>>>>  This message from Aaron Davidson implies that the executor memory
>>>>>> should be set to total available memory on the machine divided by the
>>>>>> number of cores:
>>>>>> *http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E
>>>>>> <http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E>*
>>>>>>
>>>>>> But other messages imply that the executor memory should be set to
>>>>>> the *total* available memory of each machine.
>>>>>>
>>>>>> We would very much appreciate some clarity on this and the myriad of
>>>>>> other memory settings available (daemon memory, worker memory etc). Perhaps
>>>>>> a worked example could be added to the docs? I would be happy to provide
>>>>>> some text as soon as someone can enlighten me on the technicalities!
>>>>>>
>>>>>> Thank you
>>>>>>
>>>>>> --
>>>>>> Martin Goodson  |  VP Data Science
>>>>>> (0)20 3397 1240
>>>>>> [image: Inline image 1]
>>>>>>
>>>>>
>>>>>
>>>>
>>>
>>
>

--485b397dd701844b2104fef39760
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">Great - thanks for the clarification Aaron. The offer stan=
ds for me to write some documentation and an example that covers this witho=
ut leaving <i>any</i> room for ambiguity.<div><br><div><br></div></div></di=
v>

<div class=3D"gmail_extra"><br clear=3D"all"><div><div dir=3D"ltr"><br styl=
e=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,136,136)">--=C2=
=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"ltr"><div style=
=3D"font-size:12.7273px">

<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span=
></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span style=
=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"colo=
r:rgb(102,102,102)">=C2=A0</span></div>

<div style=3D"color:rgb(34,34,34);font-size:12.7273px"><img src=3D"cid:ii_1=
3d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"color:rgb(102,102,10=
2)"><span></span><span></span><br></span></div></div></div></div>
<br><br><div class=3D"gmail_quote">On Thu, Jul 24, 2014 at 6:09 PM, Aaron D=
avidson <span dir=3D"ltr">&lt;<a href=3D"mailto:ilikerps@gmail.com" target=
=3D"_blank">ilikerps@gmail.com</a>&gt;</span> wrote:<br><blockquote class=
=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1px #ccc solid;padd=
ing-left:1ex">

<div dir=3D"ltr">Whoops, I was mistaken in my original post last year. By d=
efault, there is one executor per node per Spark Context, as you said. &quo=
t;spark.executor.memory&quot; is the amount of memory that the application =
requests for each of its executors. SPARK_WORKER_MEMORY is the amount of me=
mory a Spark Worker is willing to allocate in executors.<div>



<br></div><div>So if you were to set SPARK_WORKER_MEMORY to 8g everywhere o=
n your cluster, and spark.executor.memory to 4g, you would be able to run 2=
 simultaneous Spark Contexts who get 4g per node. Similarly, if spark.execu=
tor.memory were 8g, you could only run 1 Spark Context at a time on the clu=
ster, but it would get all the cluster&#39;s memory.</div>



</div><div class=3D"HOEnZb"><div class=3D"h5"><div class=3D"gmail_extra"><b=
r><br><div class=3D"gmail_quote">On Thu, Jul 24, 2014 at 7:25 AM, Martin Go=
odson <span dir=3D"ltr">&lt;<a href=3D"mailto:martin@skimlinks.com" target=
=3D"_blank">martin@skimlinks.com</a>&gt;</span> wrote:<br>



<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">Thank you=C2=A0Nishkam,<div=
>I have read your code. So, for the sake of my understanding, it seems that=
 for each spark context there is one executor per node? Can anyone confirm =
this?</div>



</div><div class=3D"gmail_extra"><div>

<br clear=3D"all"><div><div dir=3D"ltr"><br style=3D"color:rgb(136,136,136)=
"><span style=3D"color:rgb(136,136,136)">--=C2=A0</span><br style=3D"color:=
rgb(136,136,136)"><div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span=
 style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><span sty=
le=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>





</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">





<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></div>
<br><br></div><div><div><div class=3D"gmail_quote">On Thu, Jul 24, 2014 at =
6:12 AM, Nishkam Ravi <span dir=3D"ltr">&lt;<a href=3D"mailto:nravi@clouder=
a.com" target=3D"_blank">nravi@cloudera.com</a>&gt;</span> wrote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">

<div dir=3D"ltr">See if this helps:<div><br></div><div><a href=3D"https://g=
ithub.com/nishkamravi2/SparkAutoConfig/" target=3D"_blank">https://github.c=
om/nishkamravi2/SparkAutoConfig/</a><br></div><div><br></div><div>It&#39;s =
a very simple tool for auto-configuring default parameters in Spark. Takes =
as input high-level parameters (like number of nodes, cores per node, memor=
y per node, etc) and spits out default configuration, user advice and comma=
nd line. Compile (javac SparkConfigure.java) and run (java SparkConfigure).=
</div>








<div><br></div><div>Also cc&#39;ing dev in case others are interested in he=
lping evolve this over time (by refining the heuristics and adding more par=
ameters).=C2=A0</div><div><div><div class=3D"gmail_extra"><br><br>

<div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>Thanks Andrew,</div><d=
iv><br></div><div>So if there is only one SparkContext there is only one ex=
ecutor per machine? This seems to contradict Aaron&#39;s message from the l=
ink above:</div>








<div><br></div>

&quot;If each machine has 16 GB of RAM and 4 cores, for example, you might =
set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by Spar=
k.)&quot;<div><br></div><div>Am I reading this incorrectly?</div><div>









<br>
</div><div>Anyway our configuration is 21 machines (one master and 20 slave=
s) each with 60Gb. We would like to use 4 cores per machine. This is pyspar=
k so we want to leave say 16Gb on each machine for python processes.</div>










<div><br></div><div>Thanks again for the advice!</div><div><br></div></div>=
<div class=3D"gmail_extra"><div><br clear=3D"all"><div><div dir=3D"ltr"><br=
 style=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,136,136)">--=
=C2=A0</span><br style=3D"color:rgb(136,136,136)">










<div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span style=3D"backgrou=
nd-color:rgb(255,255,204)">Martin Goodson</span><span style=3D"color:rgb(10=
2,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span></div><div style=3D"co=
lor:rgb(34,34,34);font-size:12.7273px">










<span style=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span st=
yle=3D"color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(3=
4,34,34);font-size:12.7273px"><img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"I=
nline image 1"><span style=3D"color:rgb(102,102,102)"><span></span><span></=
span><br>










</span></div></div></div></div>
<br><br></div><div class=3D"gmail_quote"><div>On Wed, Jul 23, 2014 at 4:19 =
PM, Andrew Ash <span dir=3D"ltr">&lt;<a href=3D"mailto:andrew@andrewash.com=
" target=3D"_blank">andrew@andrewash.com</a>&gt;</span> wrote:<br></div>
<div><div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bord=
er-left:1px #ccc solid;padding-left:1ex">

<div dir=3D"ltr">Hi Martin,<div><br></div><div>In standalone mode, each Spa=
rkContext you initialize gets its own set of executors across the cluster. =
=C2=A0So for example if you have two shells open, they&#39;ll each get two =
JVMs on each worker machine in the cluster.</div>












<div><br></div><div>As far as the other docs, you can configure the total n=
umber of cores requested for the SparkContext, the amount of memory for the=
 executor JVM on each machine, the amount of memory for the Master/Worker d=
aemons (little needed since work is done in executors), and several other s=
ettings.</div>












<div><br></div><div>Which of those are you interested in? =C2=A0What spec h=
ardware do you have and how do you want to configure it?</div><span><font c=
olor=3D"#888888"><div><br></div><div>Andrew</div></font></span></div>

<div><div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">












<div dir=3D"ltr"><font face=3D"Arial">We are having difficulties configurin=
g Spark, partly because we still don&#39;t understand some key concepts. Fo=
r instance, how many executors are there per machine in standalone mode</fo=
nt><font face=3D"Arial">?=C2=A0</font><font face=3D"Arial">This is after ha=
ving closely read the documentation several times:</font><br>















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/configuration.html" target=3D"_=
blank">http://spark.apache.org/docs/latest/configuration.html</a></u></span=
><br>













<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/spark-standalone.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a></=
u></span><br>













<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/tuning.html" target=3D"_blank">=
http://spark.apache.org/docs/latest/tuning.html</a></u></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/cluster-overview.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a></=
u></span><br>













<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">The cluster overview has s=
ome information here about executors but is ambiguous about whether there a=
re single executors or multiple executors on each machine.</span><div><font=
 face=3D"Arial"><br>














</font>
<span style=3D"font-size:13px;font-family:Arial">This message from=C2=A0Aar=
on Davidson implies that the executor memory should be set to total availab=
le memory on the machine divided by the number of cores:</span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C=
CANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E" ta=
rget=3D"_blank">http://mail-archives.apache.org/mod_mbox/spark-user/201312.=
mbox/%3CCANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.co=
m%3E</a></u></span><br>















<span style=3D"font-size:13px;font-family:Arial"></span><br><span style=3D"=
font-size:13px;font-family:Arial">But other messages imply that the executo=
r memory should be set to the <i>total</i>=C2=A0available memory of each ma=
chine.</span><br>















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">We would very much appreci=
ate some clarity on this and the myriad of other memory settings available =
(daemon memory, worker memory etc). Perhaps a worked example could be added=
 to the docs? I would be happy to provide some text as soon as someone can =
enlighten me on the technicalities!</span><div>














<div><div dir=3D"ltr"><br></div><div dir=3D"ltr">Thank you</div><span><font=
 color=3D"#888888"><div dir=3D"ltr"><br><span style=3D"color:rgb(136,136,13=
6)">--=C2=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"ltr"><d=
iv style=3D"font-size:12.7273px">












<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>

</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">














<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></font></span></div>
</div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div></div></div><br></div>
</blockquote></div><br></div></div></div></div>
</blockquote></div><br></div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div><br></div>

--485b397dd701844b2104fef39760--
--485b397dd701844b2404fef39761--

From dev-return-8519-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 17:33:17 2014
Return-Path: <dev-return-8519-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9796E11AFC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 17:33:17 +0000 (UTC)
Received: (qmail 72620 invoked by uid 500); 24 Jul 2014 17:33:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72558 invoked by uid 500); 24 Jul 2014 17:33:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72544 invoked by uid 99); 24 Jul 2014 17:33:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 17:33:16 +0000
X-ASF-Spam-Status: No, hits=2.7 required=10.0
	tests=HTML_MESSAGE,MISSING_HEADERS,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ilikerps@gmail.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 17:33:14 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so3637300qgf.7
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 10:32:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:cc
         :content-type;
        bh=lL1uEvpIbcjKRPHAiRJL4PJ03FdwRKyosCg20egBqx0=;
        b=knHxLbI9uX5Jak0rO3amaVDp7hMNzIovpxKQveOD7SiusZBHL6O5Ch12VOEWMNjNEO
         E/y6GTv9Rmbg+BMsL4GhRP5F3R6BuhdKB2lGlgj0umEW/6v+s83kXIzg5weZNsCi1Ngf
         POExNnf/gUUbWYmHrSJGYYb924OORDJcPabZWBg6J+eRyBM0ANbdlIQPC6jFXDqj8wNk
         0GLpNOADj90EUoROPO6lc/u4O2foR6hQrDA+cboewXzgku6OEWckotqqV7BJI6fGVUeP
         GiwvUTq9lX4Uehenu02dZl3ngh+mGqjjCZgHbaA0xkvWZBLGjU/1FRz6KjNArrSt+6jD
         eOjg==
X-Received: by 10.140.39.164 with SMTP id v33mr2228968qgv.39.1406223169089;
 Thu, 24 Jul 2014 10:32:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.91.139 with HTTP; Thu, 24 Jul 2014 10:32:28 -0700 (PDT)
In-Reply-To: <CAPbPHgm2KiaadLHMKXU2vD5u9nQQnLEbi4z0_W8gbA_i6NDREQ@mail.gmail.com>
References: <CAPbPHg=8pUkk0cNvFy-GYSTj-njpcsFKfuQGdgrPx8zBNPtS1Q@mail.gmail.com>
 <CA+-p3AF3rSpzMewXGfCoG3N6VyYMqjmgjV0sVr0d5Uzhd8p=Gg@mail.gmail.com>
 <CAPbPHg=hG9CNnahtDz7sRMyMsMhAWgTcpN0Va065O7ASTA1qqw@mail.gmail.com>
 <CACfA1zWkFZRBMvJ1S+oWGRsCcrE+V0sxZMpxd9S-_nSF8xyi3g@mail.gmail.com>
 <CAPbPHgmZ0uDsH420-1cT_3ng0NTnFibrscrZKqn5nZ8fUeSdog@mail.gmail.com>
 <CANGvG8odA4ZTBmdM0EaNbAnY8dERx34zLvMgdJocWo8WXTqmJw@mail.gmail.com> <CAPbPHgm2KiaadLHMKXU2vD5u9nQQnLEbi4z0_W8gbA_i6NDREQ@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Thu, 24 Jul 2014 10:32:28 -0700
Message-ID: <CANGvG8qhOVrM15YBnvHfm=XBjek09aT+RppzVnjWwNZ7Z9DkVg@mail.gmail.com>
Subject: Re: Configuring Spark Memory
Cc: dev@spark.apache.org
Content-Type: multipart/related; boundary=001a11c14eae78796504fef3d8d8
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c14eae78796504fef3d8d8
Content-Type: multipart/alternative; boundary=001a11c14eae78796204fef3d8d7

--001a11c14eae78796204fef3d8d7
Content-Type: text/plain; charset=UTF-8

More documentation on this would be undoubtedly useful. Many of the
properties changed or were deprecated in Spark 1.0, and I'm not sure our
current set of documentation via userlists is up to par, since many of the
previous suggestions are deprecated.


On Thu, Jul 24, 2014 at 10:14 AM, Martin Goodson <martin@skimlinks.com>
wrote:

> Great - thanks for the clarification Aaron. The offer stands for me to
> write some documentation and an example that covers this without leaving
> *any* room for ambiguity.
>
>
>
>
> --
> Martin Goodson  |  VP Data Science
> (0)20 3397 1240
> [image: Inline image 1]
>
>
> On Thu, Jul 24, 2014 at 6:09 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
>
>> Whoops, I was mistaken in my original post last year. By default, there
>> is one executor per node per Spark Context, as you said.
>> "spark.executor.memory" is the amount of memory that the application
>> requests for each of its executors. SPARK_WORKER_MEMORY is the amount of
>> memory a Spark Worker is willing to allocate in executors.
>>
>> So if you were to set SPARK_WORKER_MEMORY to 8g everywhere on your
>> cluster, and spark.executor.memory to 4g, you would be able to run 2
>> simultaneous Spark Contexts who get 4g per node. Similarly, if
>> spark.executor.memory were 8g, you could only run 1 Spark Context at a time
>> on the cluster, but it would get all the cluster's memory.
>>
>>
>> On Thu, Jul 24, 2014 at 7:25 AM, Martin Goodson <martin@skimlinks.com>
>> wrote:
>>
>>> Thank you Nishkam,
>>> I have read your code. So, for the sake of my understanding, it seems
>>> that for each spark context there is one executor per node? Can anyone
>>> confirm this?
>>>
>>>
>>> --
>>> Martin Goodson  |  VP Data Science
>>> (0)20 3397 1240
>>> [image: Inline image 1]
>>>
>>>
>>> On Thu, Jul 24, 2014 at 6:12 AM, Nishkam Ravi <nravi@cloudera.com>
>>> wrote:
>>>
>>>> See if this helps:
>>>>
>>>> https://github.com/nishkamravi2/SparkAutoConfig/
>>>>
>>>> It's a very simple tool for auto-configuring default parameters in
>>>> Spark. Takes as input high-level parameters (like number of nodes, cores
>>>> per node, memory per node, etc) and spits out default configuration, user
>>>> advice and command line. Compile (javac SparkConfigure.java) and run (java
>>>> SparkConfigure).
>>>>
>>>> Also cc'ing dev in case others are interested in helping evolve this
>>>> over time (by refining the heuristics and adding more parameters).
>>>>
>>>>
>>>>  On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <martin@skimlinks.com>
>>>> wrote:
>>>>
>>>>> Thanks Andrew,
>>>>>
>>>>> So if there is only one SparkContext there is only one executor per
>>>>> machine? This seems to contradict Aaron's message from the link above:
>>>>>
>>>>> "If each machine has 16 GB of RAM and 4 cores, for example, you might
>>>>> set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by
>>>>> Spark.)"
>>>>>
>>>>> Am I reading this incorrectly?
>>>>>
>>>>> Anyway our configuration is 21 machines (one master and 20 slaves)
>>>>> each with 60Gb. We would like to use 4 cores per machine. This is pyspark
>>>>> so we want to leave say 16Gb on each machine for python processes.
>>>>>
>>>>> Thanks again for the advice!
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Martin Goodson  |  VP Data Science
>>>>> (0)20 3397 1240
>>>>> [image: Inline image 1]
>>>>>
>>>>>
>>>>> On Wed, Jul 23, 2014 at 4:19 PM, Andrew Ash <andrew@andrewash.com>
>>>>> wrote:
>>>>>
>>>>>> Hi Martin,
>>>>>>
>>>>>> In standalone mode, each SparkContext you initialize gets its own set
>>>>>> of executors across the cluster.  So for example if you have two shells
>>>>>> open, they'll each get two JVMs on each worker machine in the cluster.
>>>>>>
>>>>>> As far as the other docs, you can configure the total number of cores
>>>>>> requested for the SparkContext, the amount of memory for the executor JVM
>>>>>> on each machine, the amount of memory for the Master/Worker daemons (little
>>>>>> needed since work is done in executors), and several other settings.
>>>>>>
>>>>>> Which of those are you interested in?  What spec hardware do you have
>>>>>> and how do you want to configure it?
>>>>>>
>>>>>> Andrew
>>>>>>
>>>>>>
>>>>>> On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <martin@skimlinks.com
>>>>>> > wrote:
>>>>>>
>>>>>>> We are having difficulties configuring Spark, partly because we
>>>>>>> still don't understand some key concepts. For instance, how many executors
>>>>>>> are there per machine in standalone mode? This is after having
>>>>>>> closely read the documentation several times:
>>>>>>>
>>>>>>> *http://spark.apache.org/docs/latest/configuration.html
>>>>>>> <http://spark.apache.org/docs/latest/configuration.html>*
>>>>>>> *http://spark.apache.org/docs/latest/spark-standalone.html
>>>>>>> <http://spark.apache.org/docs/latest/spark-standalone.html>*
>>>>>>> *http://spark.apache.org/docs/latest/tuning.html
>>>>>>> <http://spark.apache.org/docs/latest/tuning.html>*
>>>>>>> *http://spark.apache.org/docs/latest/cluster-overview.html
>>>>>>> <http://spark.apache.org/docs/latest/cluster-overview.html>*
>>>>>>>
>>>>>>> The cluster overview has some information here about executors but
>>>>>>> is ambiguous about whether there are single executors or multiple executors
>>>>>>> on each machine.
>>>>>>>
>>>>>>>  This message from Aaron Davidson implies that the executor memory
>>>>>>> should be set to total available memory on the machine divided by the
>>>>>>> number of cores:
>>>>>>> *http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E
>>>>>>> <http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E>*
>>>>>>>
>>>>>>> But other messages imply that the executor memory should be set to
>>>>>>> the *total* available memory of each machine.
>>>>>>>
>>>>>>> We would very much appreciate some clarity on this and the myriad of
>>>>>>> other memory settings available (daemon memory, worker memory etc). Perhaps
>>>>>>> a worked example could be added to the docs? I would be happy to provide
>>>>>>> some text as soon as someone can enlighten me on the technicalities!
>>>>>>>
>>>>>>> Thank you
>>>>>>>
>>>>>>> --
>>>>>>> Martin Goodson  |  VP Data Science
>>>>>>> (0)20 3397 1240
>>>>>>> [image: Inline image 1]
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--001a11c14eae78796204fef3d8d7
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">More documentation on this would be undoubtedly useful. Ma=
ny of the properties changed or were deprecated in Spark 1.0, and I&#39;m n=
ot sure our current set of documentation via userlists is up to par, since =
many of the previous suggestions are deprecated.</div>

<div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">On Thu, Jul 2=
4, 2014 at 10:14 AM, Martin Goodson <span dir=3D"ltr">&lt;<a href=3D"mailto=
:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</a>&gt;</span=
> wrote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">Great - thanks for the clar=
ification Aaron. The offer stands for me to write some documentation and an=
 example that covers this without leaving <i>any</i> room for ambiguity.<di=
v>

<br><div><br></div></div></div>

<div class=3D"gmail_extra"><div class=3D""><br clear=3D"all"><div><div dir=
=3D"ltr"><br style=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,=
136,136)">--=C2=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"l=
tr"><div style=3D"font-size:12.7273px">



<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span=
></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span style=
=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"colo=
r:rgb(102,102,102)">=C2=A0</span></div>



<div style=3D"color:rgb(34,34,34);font-size:12.7273px"><img src=3D"cid:ii_1=
3d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"color:rgb(102,102,10=
2)"><span></span><span></span><br></span></div></div></div></div>
<br><br></div><div><div class=3D"h5"><div class=3D"gmail_quote">On Thu, Jul=
 24, 2014 at 6:09 PM, Aaron Davidson <span dir=3D"ltr">&lt;<a href=3D"mailt=
o:ilikerps@gmail.com" target=3D"_blank">ilikerps@gmail.com</a>&gt;</span> w=
rote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">

<div dir=3D"ltr">Whoops, I was mistaken in my original post last year. By d=
efault, there is one executor per node per Spark Context, as you said. &quo=
t;spark.executor.memory&quot; is the amount of memory that the application =
requests for each of its executors. SPARK_WORKER_MEMORY is the amount of me=
mory a Spark Worker is willing to allocate in executors.<div>





<br></div><div>So if you were to set SPARK_WORKER_MEMORY to 8g everywhere o=
n your cluster, and spark.executor.memory to 4g, you would be able to run 2=
 simultaneous Spark Contexts who get 4g per node. Similarly, if spark.execu=
tor.memory were 8g, you could only run 1 Spark Context at a time on the clu=
ster, but it would get all the cluster&#39;s memory.</div>





</div><div><div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quot=
e">On Thu, Jul 24, 2014 at 7:25 AM, Martin Goodson <span dir=3D"ltr">&lt;<a=
 href=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.co=
m</a>&gt;</span> wrote:<br>





<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">Thank you=C2=A0Nishkam,<div=
>I have read your code. So, for the sake of my understanding, it seems that=
 for each spark context there is one executor per node? Can anyone confirm =
this?</div>





</div><div class=3D"gmail_extra"><div>

<br clear=3D"all"><div><div dir=3D"ltr"><br style=3D"color:rgb(136,136,136)=
"><span style=3D"color:rgb(136,136,136)">--=C2=A0</span><br style=3D"color:=
rgb(136,136,136)"><div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span=
 style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><span sty=
le=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>







</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">







<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></div>
<br><br></div><div><div><div class=3D"gmail_quote">On Thu, Jul 24, 2014 at =
6:12 AM, Nishkam Ravi <span dir=3D"ltr">&lt;<a href=3D"mailto:nravi@clouder=
a.com" target=3D"_blank">nravi@cloudera.com</a>&gt;</span> wrote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">

<div dir=3D"ltr">See if this helps:<div><br></div><div><a href=3D"https://g=
ithub.com/nishkamravi2/SparkAutoConfig/" target=3D"_blank">https://github.c=
om/nishkamravi2/SparkAutoConfig/</a><br></div><div><br></div><div>It&#39;s =
a very simple tool for auto-configuring default parameters in Spark. Takes =
as input high-level parameters (like number of nodes, cores per node, memor=
y per node, etc) and spits out default configuration, user advice and comma=
nd line. Compile (javac SparkConfigure.java) and run (java SparkConfigure).=
</div>










<div><br></div><div>Also cc&#39;ing dev in case others are interested in he=
lping evolve this over time (by refining the heuristics and adding more par=
ameters).=C2=A0</div><div><div><div class=3D"gmail_extra"><br><br>

<div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>Thanks Andrew,</div><d=
iv><br></div><div>So if there is only one SparkContext there is only one ex=
ecutor per machine? This seems to contradict Aaron&#39;s message from the l=
ink above:</div>










<div><br></div>

&quot;If each machine has 16 GB of RAM and 4 cores, for example, you might =
set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by Spar=
k.)&quot;<div><br></div><div>Am I reading this incorrectly?</div><div>











<br>
</div><div>Anyway our configuration is 21 machines (one master and 20 slave=
s) each with 60Gb. We would like to use 4 cores per machine. This is pyspar=
k so we want to leave say 16Gb on each machine for python processes.</div>












<div><br></div><div>Thanks again for the advice!</div><div><br></div></div>=
<div class=3D"gmail_extra"><div><br clear=3D"all"><div><div dir=3D"ltr"><br=
 style=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,136,136)">--=
=C2=A0</span><br style=3D"color:rgb(136,136,136)">












<div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span style=3D"backgrou=
nd-color:rgb(255,255,204)">Martin Goodson</span><span style=3D"color:rgb(10=
2,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span></div><div style=3D"co=
lor:rgb(34,34,34);font-size:12.7273px">












<span style=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span st=
yle=3D"color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(3=
4,34,34);font-size:12.7273px"><img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"I=
nline image 1"><span style=3D"color:rgb(102,102,102)"><span></span><span></=
span><br>












</span></div></div></div></div>
<br><br></div><div class=3D"gmail_quote"><div>On Wed, Jul 23, 2014 at 4:19 =
PM, Andrew Ash <span dir=3D"ltr">&lt;<a href=3D"mailto:andrew@andrewash.com=
" target=3D"_blank">andrew@andrewash.com</a>&gt;</span> wrote:<br></div>
<div><div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bord=
er-left:1px #ccc solid;padding-left:1ex">

<div dir=3D"ltr">Hi Martin,<div><br></div><div>In standalone mode, each Spa=
rkContext you initialize gets its own set of executors across the cluster. =
=C2=A0So for example if you have two shells open, they&#39;ll each get two =
JVMs on each worker machine in the cluster.</div>














<div><br></div><div>As far as the other docs, you can configure the total n=
umber of cores requested for the SparkContext, the amount of memory for the=
 executor JVM on each machine, the amount of memory for the Master/Worker d=
aemons (little needed since work is done in executors), and several other s=
ettings.</div>














<div><br></div><div>Which of those are you interested in? =C2=A0What spec h=
ardware do you have and how do you want to configure it?</div><span><font c=
olor=3D"#888888"><div><br></div><div>Andrew</div></font></span></div>

<div><div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">














<div dir=3D"ltr"><font face=3D"Arial">We are having difficulties configurin=
g Spark, partly because we still don&#39;t understand some key concepts. Fo=
r instance, how many executors are there per machine in standalone mode</fo=
nt><font face=3D"Arial">?=C2=A0</font><font face=3D"Arial">This is after ha=
ving closely read the documentation several times:</font><br>

















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/configuration.html" target=3D"_=
blank">http://spark.apache.org/docs/latest/configuration.html</a></u></span=
><br>















<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/spark-standalone.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a></=
u></span><br>















<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/tuning.html" target=3D"_blank">=
http://spark.apache.org/docs/latest/tuning.html</a></u></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/cluster-overview.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a></=
u></span><br>















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">The cluster overview has s=
ome information here about executors but is ambiguous about whether there a=
re single executors or multiple executors on each machine.</span><div><font=
 face=3D"Arial"><br>
















</font>
<span style=3D"font-size:13px;font-family:Arial">This message from=C2=A0Aar=
on Davidson implies that the executor memory should be set to total availab=
le memory on the machine divided by the number of cores:</span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C=
CANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E" ta=
rget=3D"_blank">http://mail-archives.apache.org/mod_mbox/spark-user/201312.=
mbox/%3CCANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.co=
m%3E</a></u></span><br>

















<span style=3D"font-size:13px;font-family:Arial"></span><br><span style=3D"=
font-size:13px;font-family:Arial">But other messages imply that the executo=
r memory should be set to the <i>total</i>=C2=A0available memory of each ma=
chine.</span><br>

















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">We would very much appreci=
ate some clarity on this and the myriad of other memory settings available =
(daemon memory, worker memory etc). Perhaps a worked example could be added=
 to the docs? I would be happy to provide some text as soon as someone can =
enlighten me on the technicalities!</span><div>
















<div><div dir=3D"ltr"><br></div><div dir=3D"ltr">Thank you</div><span><font=
 color=3D"#888888"><div dir=3D"ltr"><br><span style=3D"color:rgb(136,136,13=
6)">--=C2=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"ltr"><d=
iv style=3D"font-size:12.7273px">














<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>

</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">
















<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></font></span></div>
</div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div></div></div><br></div>
</blockquote></div><br></div></div></div></div>
</blockquote></div><br></div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div><br></div></div></div>
</blockquote></div><br></div>

--001a11c14eae78796204fef3d8d7--
--001a11c14eae78796504fef3d8d8--

From dev-return-8520-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 18:09:31 2014
Return-Path: <dev-return-8520-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DF09211C86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 18:09:30 +0000 (UTC)
Received: (qmail 74347 invoked by uid 500); 24 Jul 2014 18:09:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74287 invoked by uid 500); 24 Jul 2014 18:09:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74272 invoked by uid 99); 24 Jul 2014 18:09:30 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 18:09:30 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of foundart@gmail.com designates 209.85.192.169 as permitted sender)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 18:09:24 +0000
Received: by mail-pd0-f169.google.com with SMTP id y10so4150625pdj.0
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 11:09:04 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=vMwg42+Vx27rXO3l2FYe2CjcAEyiGWPPfchA0tJAKpQ=;
        b=bg7oEQ9Rh2VCYt5Vq/Hij+JerM6jhicZk/p7JVRnXf1t453fh+CE7IDLiWym98Bmge
         y023PHXxg1dcur2cuEJSSYoDRmV+9o6l7HsrhGMjlNvvhoPcq2ZluhD65IvnWaV/lRSz
         CcmKoFkTiTHwccV7PXQp51C8V4Z1moKijwD++eDMoR1Cqvj6T6YYY1ghkmp9xywa0d98
         TT134MNBvsZ2KxPb0euP5Na5/GfyvLAcPLN+09wLFZ+f5bLyabVbe6NKOMSfY2lyPkU4
         qHu9uzFLW/shzPE2h47nBFJflJ1MTKTIcvp1Cyax6n5HGqhVBziEKS+O/vlo6eE+8VXs
         +PRA==
MIME-Version: 1.0
X-Received: by 10.70.34.39 with SMTP id w7mr12689649pdi.19.1406225344273; Thu,
 24 Jul 2014 11:09:04 -0700 (PDT)
Received: by 10.66.26.69 with HTTP; Thu, 24 Jul 2014 11:09:04 -0700 (PDT)
Date: Thu, 24 Jul 2014 11:09:04 -0700
Message-ID: <CAASS4f7kAiQVn+BkPi_YJLEt9LtKJs8FHLScXNQGj+WN0Zs3dw@mail.gmail.com>
Subject: continuing processing when errors occur
From: Art Peel <foundart@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=047d7bd908d41ea5c604fef45a21
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd908d41ea5c604fef45a21
Content-Type: text/plain; charset=UTF-8

Our system works with RDDs generated from Hadoop files. It processes each
record in a Hadoop file and for a subset of those records generates output
that is written to an external system via RDD.foreach. There are no
dependencies between the records that are processed.

If writing to the external system fails (due to a detail of what is being
written) and throws an exception, I see the following behavior:

1. Spark retries the entire partition (thus wasting time and effort),
reaches the problem record and fails again.
2. It repeats step 1 until the configured number of retries is reached and
then gives up. As a result, the rest of records from that Hadoop file are
not processed.
3. The executor where the final attempt occurred is marked as failed and
told to shut down and thus I lose a core for processing the remaining
Hadoop files, thus slowing down the entire process.

For this particular problem, I know how to prevent the underlying
exception, but I'd still like to get a handle on error handling for future
situations that I haven't yet encountered.

My goal is this:
Retry the problem record only (rather than starting over at the beginning
of the partition) up to N times, then give up and move on to process the
rest of the partition.

As far as I can tell, I need to supply my own retry behavior and if I want
to process records after the problem record I have to swallow exceptions
inside the foreach block.

My 2 questions are:
1. Is there anything I can do to prevent the executor where the final retry
occurred from being shut down when a failure occurs?

2. Are there ways Spark can help me get closer to my goal of retrying only
the problem record without writing my own re-try code and swallowing
exceptions?

Regards,
Art

--047d7bd908d41ea5c604fef45a21--

From dev-return-8521-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 18:12:15 2014
Return-Path: <dev-return-8521-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06F1511CA2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 18:12:15 +0000 (UTC)
Received: (qmail 80435 invoked by uid 500); 24 Jul 2014 18:12:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80372 invoked by uid 500); 24 Jul 2014 18:12:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80359 invoked by uid 99); 24 Jul 2014 18:12:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 18:12:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of foundart@gmail.com designates 209.85.192.169 as permitted sender)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 18:12:07 +0000
Received: by mail-pd0-f169.google.com with SMTP id y10so4138124pdj.28
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 11:11:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/BtI7vZAQOZ8W583A94iZdtV59tTtXC5nfD543X98FE=;
        b=voEsHAFwUaX7fkOop/0pbTdkOnrQiui2McmNRHOvhPi67JQFJPk5QSpq8eoniziLrS
         s+5vFGCBy262FcTHiK4IQIPa/wcJ6ecCAJg7woBNJhRwRvMVb2Yo6fTbQEvn8h0P4agU
         K6fD9sNYTWdijVKrFByUmQAmY6fG0fuJNgkjEDkCsOQepn15BAcJpvdpRVLhQFkN6Ghx
         5mtbs+enYzjXxI7dqqmFp66ggEv9lC+bxLTdecJyWYr5xtKBzGdJajc74A2KSp0WgtE8
         YgiJv0swy+2BKb897YrSTpTOSs2IJzjISTuEVklauIGsyZtAdRxIttUQIbwbcPscBpbs
         mqbQ==
MIME-Version: 1.0
X-Received: by 10.68.133.40 with SMTP id oz8mr12876134pbb.132.1406225506863;
 Thu, 24 Jul 2014 11:11:46 -0700 (PDT)
Received: by 10.66.26.69 with HTTP; Thu, 24 Jul 2014 11:11:46 -0700 (PDT)
In-Reply-To: <CAASS4f7kAiQVn+BkPi_YJLEt9LtKJs8FHLScXNQGj+WN0Zs3dw@mail.gmail.com>
References: <CAASS4f7kAiQVn+BkPi_YJLEt9LtKJs8FHLScXNQGj+WN0Zs3dw@mail.gmail.com>
Date: Thu, 24 Jul 2014 11:11:46 -0700
Message-ID: <CAASS4f57GwWXiMoy24PZXQA0-Wt-cGEqmb+v2D08XtXM=GkTCw@mail.gmail.com>
Subject: Re: continuing processing when errors occur
From: Art Peel <foundart@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8ffbabd3cf92fe04fef463d2
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ffbabd3cf92fe04fef463d2
Content-Type: text/plain; charset=UTF-8

Sorry, I sent this to the dev list instead of user.  Please ignore.  I'll
re-post to the correct list.

Regards,
Art



On Thu, Jul 24, 2014 at 11:09 AM, Art Peel <foundart@gmail.com> wrote:

> Our system works with RDDs generated from Hadoop files. It processes each
> record in a Hadoop file and for a subset of those records generates output
> that is written to an external system via RDD.foreach. There are no
> dependencies between the records that are processed.
>
> If writing to the external system fails (due to a detail of what is being
> written) and throws an exception, I see the following behavior:
>
> 1. Spark retries the entire partition (thus wasting time and effort),
> reaches the problem record and fails again.
> 2. It repeats step 1 until the configured number of retries is reached and
> then gives up. As a result, the rest of records from that Hadoop file are
> not processed.
> 3. The executor where the final attempt occurred is marked as failed and
> told to shut down and thus I lose a core for processing the remaining
> Hadoop files, thus slowing down the entire process.
>
> For this particular problem, I know how to prevent the underlying
> exception, but I'd still like to get a handle on error handling for future
> situations that I haven't yet encountered.
>
> My goal is this:
> Retry the problem record only (rather than starting over at the beginning
> of the partition) up to N times, then give up and move on to process the
> rest of the partition.
>
> As far as I can tell, I need to supply my own retry behavior and if I want
> to process records after the problem record I have to swallow exceptions
> inside the foreach block.
>
> My 2 questions are:
> 1. Is there anything I can do to prevent the executor where the final
> retry occurred from being shut down when a failure occurs?
>
> 2. Are there ways Spark can help me get closer to my goal of retrying only
> the problem record without writing my own re-try code and swallowing
> exceptions?
>
> Regards,
> Art
>
>

--e89a8ffbabd3cf92fe04fef463d2--

From dev-return-8522-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 19:05:25 2014
Return-Path: <dev-return-8522-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 077C311E74
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 19:05:25 +0000 (UTC)
Received: (qmail 38875 invoked by uid 500); 24 Jul 2014 19:05:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38807 invoked by uid 500); 24 Jul 2014 19:05:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 38783 invoked by uid 99); 24 Jul 2014 19:05:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 19:05:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.176 as permitted sender)
Received: from [209.85.220.176] (HELO mail-vc0-f176.google.com) (209.85.220.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 19:05:19 +0000
Received: by mail-vc0-f176.google.com with SMTP id id10so5655269vcb.35
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 12:04:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=OGMeaofAZNFnAzzf1rYkRxKwImC7DZqJN8sEctMJ/SU=;
        b=bS/FwWAVx/QgYC9zkgo/er3EWKSDzU4okgB9noNR6UvRH/DxZzXNgVcESL96WYnpfm
         Y7U8CIighLzJfqGlu9g3unIsbX16c2yd7NT7y4yhMEUhv3feQm6JxqJvOBoP1ZBSdwRu
         Jg3juZxa4TwEo0IG6o0nBLcmXy6a81qM8HhaWp8JkDS47416Eb+TGHcRVZqfpCPvD+27
         3sZAaQUhxWvmhXHQAuDmomr7obMJRoYPXVCQadXQHnsYOwS0OWFCgrAr0vlv/+Fi/5Lv
         nLFo8Gu7BL3DfTWru6JFHDS2PcAVecW7saVzhMjrsCV9OBrpQwFWCB0Id77wlwzRlW+3
         6vcw==
MIME-Version: 1.0
X-Received: by 10.52.53.135 with SMTP id b7mr12753487vdp.67.1406228695760;
 Thu, 24 Jul 2014 12:04:55 -0700 (PDT)
Received: by 10.58.3.134 with HTTP; Thu, 24 Jul 2014 12:04:55 -0700 (PDT)
Date: Thu, 24 Jul 2014 12:04:55 -0700
Message-ID: <CACkSZy36xUKKZ8zD-StKcJE93Zzpz2Ec0V48ar+jii4AD7o-Fg@mail.gmail.com>
Subject: SQLQuerySuite error
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1136b63ce2458c04fef521a4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1136b63ce2458c04fef521a4
Content-Type: text/plain; charset=UTF-8

Are other developers seeing the following error for the recently added
substr() method?  If not, any ideas why the following invocation of tests
would be failing for me - i.e. how the given invocation would need to be
tweaked?

mvn -Pyarn -Pcdh5 test  -pl sql/core
-DwildcardSuites=org.apache.spark.sql.SQLQuerySuite

(note cdh5 is a custom profile for cdh5.0.0 but should not be affecting
these results)

Only the test("SPARK-2407 Added Parser of SQL SUBSTR()") fails: all of the
other 33 tests pass.

SQLQuerySuite:
- SPARK-2041 column name equals tablename
- SPARK-2407 Added Parser of SQL SUBSTR() *** FAILED ***
  Exception thrown while executing query:
  == Logical Plan ==
  java.lang.UnsupportedOperationException
  == Optimized Logical Plan ==
  java.lang.UnsupportedOperationException
  == Physical Plan ==
  java.lang.UnsupportedOperationException
  == Exception ==
  java.lang.UnsupportedOperationException
  java.lang.UnsupportedOperationException
  at
org.apache.spark.sql.catalyst.analysis.EmptyFunctionRegistry$.lookupFunction(FunctionRegistry.scala:33)
  at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:131)
  at
org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:129)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
  at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at
scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at
scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at
scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at
scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:212)
  at
org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:168)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org
$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:52)
  at
org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:66)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.immutable.List.foreach(List.scala:318)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
  at scala.collection.AbstractTraversable.map(Traversable.scala:105)
  at

--001a1136b63ce2458c04fef521a4--

From dev-return-8523-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 20:10:20 2014
Return-Path: <dev-return-8523-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C4C761107B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 20:10:20 +0000 (UTC)
Received: (qmail 95539 invoked by uid 500); 24 Jul 2014 20:10:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95501 invoked by uid 500); 24 Jul 2014 20:10:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95483 invoked by uid 99); 24 Jul 2014 20:10:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 20:10:19 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 20:10:15 +0000
Received: by mail-vc0-f180.google.com with SMTP id ij19so5671802vcb.25
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 13:09:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=4TpnI/1b76kMvf8wjHPbx+mTR4zZwvX+2KzDcedeUdU=;
        b=M1ncpqlWdO4PU7ic4SiqyHf4cQrJsLOTxgdj/HAHt6JvML3CaRQLIMbd4mQRWxui74
         SDnAvafsvO3dTaBfSRBjhFMYVeudRajbWuEnp8zvRvJFRgMM+bKty/FYQ2F91I8FHtin
         o9kakSiyS5Vx0s1oZ897cCGBp6DjDOMb/AkmcQuEckDqb2YcxGRFL2sQWPggxOvbKS5J
         5v9sitJmajJlsAAyxXDLEqaceHUHqDFdMRL6Xs4cYCBVFWitVSUS+ZxwtknUfmqlczG1
         CRYoYFhIiWYr0hH1MlPe+DaLrbBRg2hxMWN2PGSgdNuXWSZBzz0/rF3LGOkRGL9heYxJ
         fz3Q==
MIME-Version: 1.0
X-Received: by 10.52.106.162 with SMTP id gv2mr13387209vdb.47.1406232593479;
 Thu, 24 Jul 2014 13:09:53 -0700 (PDT)
Received: by 10.58.3.134 with HTTP; Thu, 24 Jul 2014 13:09:53 -0700 (PDT)
In-Reply-To: <CACkSZy36xUKKZ8zD-StKcJE93Zzpz2Ec0V48ar+jii4AD7o-Fg@mail.gmail.com>
References: <CACkSZy36xUKKZ8zD-StKcJE93Zzpz2Ec0V48ar+jii4AD7o-Fg@mail.gmail.com>
Date: Thu, 24 Jul 2014 13:09:53 -0700
Message-ID: <CACkSZy0zbTwo15HcL=7W0GPcsYH2-mwZi+F7cvT33FbheCnmSA@mail.gmail.com>
Subject: Re: SQLQuerySuite error
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec548a9bd34c06e04fef60ab7
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec548a9bd34c06e04fef60ab7
Content-Type: text/plain; charset=UTF-8

OK I did find my error.  The missing step:

  mvn install

I should have republished (mvn install) all of the other modules .

The mvn -pl  will rely on the modules locally published and so the latest
code that I had git pull'ed was not being used (except  the sql/core module
code).

The tests are passing after having properly performed the mvn install
before  running with the mvn -pl sql/core.




2014-07-24 12:04 GMT-07:00 Stephen Boesch <javadba@gmail.com>:

>
> Are other developers seeing the following error for the recently added
> substr() method?  If not, any ideas why the following invocation of tests
> would be failing for me - i.e. how the given invocation would need to be
> tweaked?
>
> mvn -Pyarn -Pcdh5 test  -pl sql/core
> -DwildcardSuites=org.apache.spark.sql.SQLQuerySuite
>
> (note cdh5 is a custom profile for cdh5.0.0 but should not be affecting
> these results)
>
> Only the test("SPARK-2407 Added Parser of SQL SUBSTR()") fails: all of the
> other 33 tests pass.
>
> SQLQuerySuite:
> - SPARK-2041 column name equals tablename
> - SPARK-2407 Added Parser of SQL SUBSTR() *** FAILED ***
>   Exception thrown while executing query:
>   == Logical Plan ==
>   java.lang.UnsupportedOperationException
>   == Optimized Logical Plan ==
>   java.lang.UnsupportedOperationException
>   == Physical Plan ==
>   java.lang.UnsupportedOperationException
>   == Exception ==
>   java.lang.UnsupportedOperationException
>   java.lang.UnsupportedOperationException
>   at
> org.apache.spark.sql.catalyst.analysis.EmptyFunctionRegistry$.lookupFunction(FunctionRegistry.scala:33)
>   at
> org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:131)
>   at
> org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:129)
>   at
> org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
>   at
> org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183)
>   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
>   at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>   at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>   at
> scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
>   at
> scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
>   at
> scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
>   at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
>   at scala.collection.AbstractIterator.to(Iterator.scala:1157)
>   at
> scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
>   at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
>   at
> scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
>   at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
>   at
> org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:212)
>   at
> org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:168)
>   at org.apache.spark.sql.catalyst.plans.QueryPlan.org
> $apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:52)
>   at
> org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:66)
>   at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
>   at
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
>   at scala.collection.immutable.List.foreach(List.scala:318)
>   at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
>   at scala.collection.AbstractTraversable.map(Traversable.scala:105)
>   at
>

--bcaec548a9bd34c06e04fef60ab7--

From dev-return-8524-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 20:52:15 2014
Return-Path: <dev-return-8524-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 35923111DB
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 20:52:15 +0000 (UTC)
Received: (qmail 95996 invoked by uid 500); 24 Jul 2014 20:52:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95938 invoked by uid 500); 24 Jul 2014 20:52:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95924 invoked by uid 99); 24 Jul 2014 20:52:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 20:52:14 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nferguson@gmail.com designates 209.85.192.44 as permitted sender)
Received: from [209.85.192.44] (HELO mail-qg0-f44.google.com) (209.85.192.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 20:52:12 +0000
Received: by mail-qg0-f44.google.com with SMTP id e89so4011951qgf.17
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 13:51:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=uiQgc/eig+67VJ9/r4iyyDpeL1dUDaWcxiHHOz4guQ8=;
        b=fJaqgcSRFz93FChpQyHkei9rcubA0GGI46jtfOqha94B/sIhquJSC0wsW2XIW/iNak
         PbjEhPGJRGe7ZVDZBm1OGHnel2XHIIAnMH4GlSfyg+gYDEXsxD5kb7Lb5MaLP8C0NU64
         kSWDwiyh/bTWfgYtuv9iVmFR2ZYeHp7dK5B8mWlO9+/583SEjAn1BpXi5U/aHe/+KZwu
         ipARLu+jkqEC96y1Xiy/QUnZR0f60Grpo2E+umadM1GCWK4l5ITscd94S0IwShre6sF8
         UQElrlXvhoV4hUeKiFTgPlicENvt5bvkhxW1XpxzqK/j1r7q/OG8hsXZSOKOuemv30zB
         +T7g==
MIME-Version: 1.0
X-Received: by 10.140.80.170 with SMTP id c39mr18324784qgd.79.1406235106946;
 Thu, 24 Jul 2014 13:51:46 -0700 (PDT)
Received: by 10.140.48.35 with HTTP; Thu, 24 Jul 2014 13:51:46 -0700 (PDT)
In-Reply-To: <1406100054997.e9c355d6@Nodemailer>
References: <CABPQxstaTsp9XbNJgwVHTfG459G4S-ACKMwTgCh0CpawhCFKSg@mail.gmail.com>
	<1406100054997.e9c355d6@Nodemailer>
Date: Thu, 24 Jul 2014 21:51:46 +0100
Message-ID: <CAKqT-W2051aukTGRiinCwU-rUxHTDmjMGQ3h5P+TAzT7CjVJTQ@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
From: Neil Ferguson <nferguson@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c1268c05360e04fef6a072
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1268c05360e04fef6a072
Content-Type: text/plain; charset=UTF-8

I realised that my last reply wasn't very clear -- let me try and clarify.

The patch for named accumulators looks very useful, however in Shivaram's
example he was able to retrieve the named task metrics (statically) from a
TaskMetrics object, as follows:

TaskMetrics.get("f1-time")

However, I don't think this would be possible with the named accumulators
-- I believe they'd need to be passed to every function that needs them,
which I think would be cumbersome in any application of reasonable
complexity.

This is what I was trying to solve with my proposal for dynamic variables
in Spark. However, the ability to retrieve named accumulators from a
thread-local would work just as well for my use case. I'd be happy to
implement either solution if there's interest.

Alternatively, if I'm missing some other way to accomplish this please let
me know.

On a (slight) aside, I now think it would be possible to implement dynamic
variables by broadcasting them. I was looking at Reynold's PR [1] to
broadcast the RDD object, and I think it would be possible to take a
similar approach -- that is, broadcast the serialized form, and deserialize
when executing each task.

[1] https://github.com/apache/spark/pull/1498







On Wed, Jul 23, 2014 at 8:30 AM, Neil Ferguson <nferguson@gmail.com> wrote:

>  Hi Patrick.
>
> That looks very useful. The thing that seems to be missing from Shivaram's
> example is the ability to access TaskMetrics statically (this is the same
> problem that I am trying to solve with dynamic variables).
>
>
>
> You mention defining an accumulator on the RDD. Perhaps I am missing
> something here, but my understanding was that accumulators are defined in
> SparkContext and are not part of the RDD. Is that correct?
>
> Neil
>
> On Tue, Jul 22, 2014 at 22:21, Patrick Wendell <pwendell@gmail.com
> ="mailto:pwendell@gmail.com">> wrote:
>
>> Shivaram,
>>
>> You should take a look at this patch which adds support for naming
>> accumulators - this is likely to get merged in soon. I actually
>> started this patch by supporting named TaskMetrics similar to what you
>> have there, but then I realized there is too much semantic overlap
>> with accumulators, so I just went that route.
>>
>> For instance, it would be nice if any user-defined metrics are
>> accessible at the driver program.
>>
>> https://github.com/apache/spark/pull/1309
>>
>> In your example, you could just define an accumulator here on the RDD
>> and you'd see the incremental update in the web UI automatically.
>>
>> - Patrick
>>
>> On Tue, Jul 22, 2014 at 2:09 PM, Shivaram Venkataraman
>> <shivaram@eecs.berkeley.edu> wrote:
>> > From reading Neil's first e-mail, I think the motivation is to get some
>> > metrics in ADAM ? -- I've run into a similar use-case with having
>> > user-defined metrics in long-running tasks and I think a nice way to
>> solve
>> > this would be to have user-defined TaskMetrics.
>> >
>> > To state my problem more clearly, lets say you have two functions you
>> use
>> > in a map call and want to measure how much time each of them takes. For
>> > example, if you have a code block like the one below and you want to
>> > measure how much time f1 takes as a fraction of the task.
>> >
>> > a.map { l =>
>> > val f = f1(l)
>> > ... some work here ...
>> > }
>> >
>> > It would be really cool if we could do something like
>> >
>> > a.map { l =>
>> > val start = System.nanoTime
>> > val f = f1(l)
>> > TaskMetrics.get("f1-time").add(System.nanoTime - start)
>> > }
>> >
>> > These task metrics have a different purpose from Accumulators in the
>> sense
>> > that we don't need to track lineage, perform commutative operations
>> etc.
>> > Further we also have a bunch of code in place to aggregate task metrics
>> > across a stage etc. So it would be great if we could also populate
>> these in
>> > the UI and show median/max etc.
>> > I think counters [1] in Hadoop served a similar purpose.
>> >
>> > Thanks
>> > Shivaram
>> >
>> > [1]
>> >
>> https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/counters
>> >
>> >
>> >
>> > On Tue, Jul 22, 2014 at 1:43 PM, Neil Ferguson <nferguson@gmail.com>
>> wrote:
>> >
>> >> Hi Reynold
>> >>
>> >> Thanks for your reply.
>> >>
>> >> Accumulators are, of course, stored in the Accumulators object as
>> >> thread-local variables. However, the Accumulators object isn't public,
>> so
>> >> when a Task is executing there's no way to get the set of accumulators
>> for
>> >> the current thread -- accumulators still have to be passed to every
>> method
>> >> that needs them.
>> >>
>> >> Additionally, unless an accumulator is explicitly referenced it won't
>> be
>> >> serialized as part of a Task, and won't make it into the Accumulators
>> >> object in the first place.
>> >>
>> >> I should also note that what I'm proposing is not specific to
>> Accumulators
>> >> -- I am proposing that any data can be stored in a thread-local
>> variable. I
>> >> think there are probably many other use cases other than my one.
>> >>
>> >> Neil
>> >>
>> >>
>> >> On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com>
>> wrote:
>> >>
>> >> > Thanks for the thoughtful email, Neil and Christopher.
>> >> >
>> >> > If I understand this correctly, it seems like the dynamic variable
>> is
>> >> just
>> >> > a variant of the accumulator (a static one since it is a global
>> object).
>> >> > Accumulators are already implemented using thread-local variables
>> under
>> >> the
>> >> > hood. Am I misunderstanding something?
>> >> >
>> >> >
>> >> >
>> >> > On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.com>
>>
>> >> > wrote:
>> >> >
>> >> > > Hi Neil, first off, I'm generally a sympathetic advocate for
>> making
>> >> > changes
>> >> > > to Spark internals to make it easier/better/faster/more awesome.
>> >> > >
>> >> > > In this case, I'm (a) not clear about what you're trying to
>> accomplish,
>> >> > and
>> >> > > (b) a bit worried about the proposed solution.
>> >> > >
>> >> > > On (a): it is stated that you want to pass some Accumulators
>> around.
>> >> Yet
>> >> > > the proposed solution is for some "shared" variable that may be
>> set and
>> >> > > "mapped out" and possibly "reduced back", but without any
>> accompanying
>> >> > > accumulation semantics. And yet it doesn't seem like you only want
>> just
>> >> > the
>> >> > > broadcast property. Can you clarify the problem statement with
>> some
>> >> > > before/after client code examples?
>> >> > >
>> >> > > On (b): you're right that adding variables to SparkContext should
>> be
>> >> done
>> >> > > with caution, as it may have unintended consequences beyond just
>> serdes
>> >> > > payload size. For example, there is a stated intention of
>> supporting
>> >> > > multiple SparkContexts in the future, and this proposed solution
>> can
>> >> make
>> >> > > it a bigger challenge to do so. Indeed, we had a gut-wrenching
>> call to
>> >> > make
>> >> > > a while back on a subject related to this (see
>> >> > > https://github.com/mesos/spark/pull/779). Furthermore, even in a
>> >> single
>> >> > > SparkContext application, there may be multiple "clients" (of that
>> >> > > application) whose intent to use the proposed "SparkDynamic" would
>> not
>> >> > > necessarily be coordinated.
>> >> > >
>> >> > > So, considering a ratio of a/b (benefit/cost), it's not clear to
>> me
>> >> that
>> >> > > the benefits are significant enough to warrant the costs. Do I
>> >> > > misunderstand that the benefit is to save one explicit parameter
>> (the
>> >> > > "context") in the signature/closure code?
>> >> > >
>> >> > > --
>> >> > > Christopher T. Nguyen
>> >> > > Co-founder & CEO, Adatao <http://adatao.com>
>> >> > > linkedin.com/in/ctnguyen
>> >> > >
>> >> > >
>> >> > >
>> >> > > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <
>> nferguson@gmail.com>
>> >> > > wrote:
>> >> > >
>> >> > > > Hi all
>> >> > > >
>> >> > > > I have been adding some metrics to the ADAM project
>> >> > > > https://github.com/bigdatagenomics/adam, which runs on Spark,
>> and
>> >> > have a
>> >> > > > proposal for an enhancement to Spark that would make this work
>> >> cleaner
>> >> > > and
>> >> > > > easier.
>> >> > > >
>> >> > > > I need to pass some Accumulators around, which will aggregate
>> metrics
>> >> > > > (timing stats and other metrics) across the cluster. However, it
>> is
>> >> > > > cumbersome to have to explicitly pass some "context" containing
>> these
>> >> > > > accumulators around everywhere that might need them. I can use
>> Scala
>> >> > > > implicits, which help slightly, but I'd still need to modify
>> every
>> >> > method
>> >> > > > in the call stack to take an implicit variable.
>> >> > > >
>> >> > > > So, I'd like to propose that we add the ability to have "dynamic
>> >> > > variables"
>> >> > > > (basically thread-local variables) to Spark. This would avoid
>> having
>> >> to
>> >> > > > pass the Accumulators around explicitly.
>> >> > > >
>> >> > > > My proposed approach is to add a method to the SparkContext
>> class as
>> >> > > > follows:
>> >> > > >
>> >> > > > /**
>> >> > > > * Sets the value of a "dynamic variable". This value is made
>> >> available
>> >> > > to
>> >> > > > jobs
>> >> > > > * without having to be passed around explicitly. During
>> execution
>> >> of a
>> >> > > > Spark job
>> >> > > > * this value can be obtained from the [[SparkDynamic]] object.
>> >> > > > */
>> >> > > > def setDynamicVariableValue(value: Any)
>> >> > > >
>> >> > > > Then, when a job is executing the SparkDynamic can be accessed
>> to
>> >> > obtain
>> >> > > > the value of the dynamic variable. The implementation of this
>> object
>> >> is
>> >> > > as
>> >> > > > follows:
>> >> > > >
>> >> > > > object SparkDynamic {
>> >> > > > private val dynamicVariable = new DynamicVariable[Any]()
>> >> > > > /**
>> >> > > > * Gets the value of the "dynamic variable" that has been set in
>> >> the
>> >> > > > [[SparkContext]]
>> >> > > > */
>> >> > > > def getValue: Option[Any] = {
>> >> > > > Option(dynamicVariable.value)
>> >> > > > }
>> >> > > > private[spark] def withValue[S](threadValue: Option[Any])(thunk:
>> =>
>> >> > > S): S
>> >> > > > = {
>> >> > > > dynamicVariable.withValue(threadValue.orNull)(thunk)
>> >> > > > }
>> >> > > > }
>> >> > > >
>> >> > > > The change involves modifying the Task object to serialize the
>> value
>> >> of
>> >> > > the
>> >> > > > dynamic variable, and modifying the TaskRunner class to
>> deserialize
>> >> the
>> >> > > > value and make it available in the thread that is running the
>> task
>> >> > (using
>> >> > > > the SparkDynamic.withValue method).
>> >> > > >
>> >> > > > I have done a quick prototype of this in this commit:
>> >> > > >
>> >> > > >
>> >> > >
>> >> >
>> >>
>> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
>> >> > > > and it seems to work fine in my (limited) testing. It needs more
>> >> > testing,
>> >> > > > tidy-up and documentation though.
>> >> > > >
>> >> > > > One drawback is that the dynamic variable will be serialized for
>> >> every
>> >> > > Task
>> >> > > > whether it needs it or not. For my use case this might not be
>> too
>> >> much
>> >> > > of a
>> >> > > > problem, as serializing and deserializing Accumulators looks
>> fairly
>> >> > > > lightweight -- however we should certainly warn users against
>> >> setting a
>> >> > > > dynamic variable containing lots of data. I thought about using
>> >> > broadcast
>> >> > > > tables here, but I don't think it's possible to put Accumulators
>> in a
>> >> > > > broadcast table (as I understand it, they're intended for purely
>> >> > > read-only
>> >> > > > data).
>> >> > > >
>> >> > > > What do people think about this proposal? My use case aside, it
>> seems
>> >> > > like
>> >> > > > it would be a generally useful enhancment to be able to pass
>> certain
>> >> > data
>> >> > > > around without having to explicitly pass it everywhere.
>> >> > > >
>> >> > > > Neil
>> >> > > >
>> >> > >
>> >> >
>> >>
>>
>

--001a11c1268c05360e04fef6a072--

From dev-return-8525-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 21:17:35 2014
Return-Path: <dev-return-8525-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0821C112CD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 21:17:35 +0000 (UTC)
Received: (qmail 75986 invoked by uid 500); 24 Jul 2014 21:17:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75930 invoked by uid 500); 24 Jul 2014 21:17:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 75888 invoked by uid 99); 24 Jul 2014 21:17:33 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 21:17:33 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 21:17:30 +0000
Received: by mail-oi0-f53.google.com with SMTP id e131so2505410oig.26
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 14:17:09 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=kn/uaLSaZ6frxS3vO4QAPgDGyCSzMTlsBPPYcQc4LwA=;
        b=VRcOLqrWIlAp62gmywnjL/o1hod5HVnWEtByIK+0lJ7w8k5A8dqU10R4dmsgvNPXdF
         rj/Qg0At4p5iBolyzBrjwnVRS9KiAm2Ivxks5HfXxA6Q0p/HxhWCTOPRWA5WJBj9+iR9
         kIgztZTw5mKa0bG01gezR+HDhD7UQFFUZUfAc/ug8nmtelAWb4z/A3LQ8utzUWYNIdOt
         Jyfy8okV/JAPqeDn6sd/sqkx3KoncpgBwxDxj+xHmUWg2KFYMNK7zMF6ea4iTfnKyAPL
         4cFbZQWB2jNAolO1N5wHtViwhDVYscOd3m9frsEhiv5+RglTVPUu63GYpwNAAJUglaD6
         3z+g==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr17057546obc.75.1406236629555;
 Thu, 24 Jul 2014 14:17:09 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Thu, 24 Jul 2014 14:17:09 -0700 (PDT)
In-Reply-To: <CAKqT-W2051aukTGRiinCwU-rUxHTDmjMGQ3h5P+TAzT7CjVJTQ@mail.gmail.com>
References: <CABPQxstaTsp9XbNJgwVHTfG459G4S-ACKMwTgCh0CpawhCFKSg@mail.gmail.com>
	<1406100054997.e9c355d6@Nodemailer>
	<CAKqT-W2051aukTGRiinCwU-rUxHTDmjMGQ3h5P+TAzT7CjVJTQ@mail.gmail.com>
Date: Thu, 24 Jul 2014 14:17:09 -0700
Message-ID: <CABPQxsth1osuj5A-mwV8wSSOfi8xv5hyKkNBBE1cPjazxiMZDw@mail.gmail.com>
Subject: Re: "Dynamic variables" in Spark
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

What if we have a registry for accumulators, where you can access them
statically by name?

- Patrick

On Thu, Jul 24, 2014 at 1:51 PM, Neil Ferguson <nferguson@gmail.com> wrote:
> I realised that my last reply wasn't very clear -- let me try and clarify.
>
> The patch for named accumulators looks very useful, however in Shivaram's
> example he was able to retrieve the named task metrics (statically) from a
> TaskMetrics object, as follows:
>
> TaskMetrics.get("f1-time")
>
> However, I don't think this would be possible with the named accumulators
> -- I believe they'd need to be passed to every function that needs them,
> which I think would be cumbersome in any application of reasonable
> complexity.
>
> This is what I was trying to solve with my proposal for dynamic variables
> in Spark. However, the ability to retrieve named accumulators from a
> thread-local would work just as well for my use case. I'd be happy to
> implement either solution if there's interest.
>
> Alternatively, if I'm missing some other way to accomplish this please let
> me know.
>
> On a (slight) aside, I now think it would be possible to implement dynamic
> variables by broadcasting them. I was looking at Reynold's PR [1] to
> broadcast the RDD object, and I think it would be possible to take a
> similar approach -- that is, broadcast the serialized form, and deserialize
> when executing each task.
>
> [1] https://github.com/apache/spark/pull/1498
>
>
>
>
>
>
>
> On Wed, Jul 23, 2014 at 8:30 AM, Neil Ferguson <nferguson@gmail.com> wrote:
>
>>  Hi Patrick.
>>
>> That looks very useful. The thing that seems to be missing from Shivaram's
>> example is the ability to access TaskMetrics statically (this is the same
>> problem that I am trying to solve with dynamic variables).
>>
>>
>>
>> You mention defining an accumulator on the RDD. Perhaps I am missing
>> something here, but my understanding was that accumulators are defined in
>> SparkContext and are not part of the RDD. Is that correct?
>>
>> Neil
>>
>> On Tue, Jul 22, 2014 at 22:21, Patrick Wendell <pwendell@gmail.com
>> ="mailto:pwendell@gmail.com">> wrote:
>>
>>> Shivaram,
>>>
>>> You should take a look at this patch which adds support for naming
>>> accumulators - this is likely to get merged in soon. I actually
>>> started this patch by supporting named TaskMetrics similar to what you
>>> have there, but then I realized there is too much semantic overlap
>>> with accumulators, so I just went that route.
>>>
>>> For instance, it would be nice if any user-defined metrics are
>>> accessible at the driver program.
>>>
>>> https://github.com/apache/spark/pull/1309
>>>
>>> In your example, you could just define an accumulator here on the RDD
>>> and you'd see the incremental update in the web UI automatically.
>>>
>>> - Patrick
>>>
>>> On Tue, Jul 22, 2014 at 2:09 PM, Shivaram Venkataraman
>>> <shivaram@eecs.berkeley.edu> wrote:
>>> > From reading Neil's first e-mail, I think the motivation is to get some
>>> > metrics in ADAM ? -- I've run into a similar use-case with having
>>> > user-defined metrics in long-running tasks and I think a nice way to
>>> solve
>>> > this would be to have user-defined TaskMetrics.
>>> >
>>> > To state my problem more clearly, lets say you have two functions you
>>> use
>>> > in a map call and want to measure how much time each of them takes. For
>>> > example, if you have a code block like the one below and you want to
>>> > measure how much time f1 takes as a fraction of the task.
>>> >
>>> > a.map { l =>
>>> > val f = f1(l)
>>> > ... some work here ...
>>> > }
>>> >
>>> > It would be really cool if we could do something like
>>> >
>>> > a.map { l =>
>>> > val start = System.nanoTime
>>> > val f = f1(l)
>>> > TaskMetrics.get("f1-time").add(System.nanoTime - start)
>>> > }
>>> >
>>> > These task metrics have a different purpose from Accumulators in the
>>> sense
>>> > that we don't need to track lineage, perform commutative operations
>>> etc.
>>> > Further we also have a bunch of code in place to aggregate task metrics
>>> > across a stage etc. So it would be great if we could also populate
>>> these in
>>> > the UI and show median/max etc.
>>> > I think counters [1] in Hadoop served a similar purpose.
>>> >
>>> > Thanks
>>> > Shivaram
>>> >
>>> > [1]
>>> >
>>> https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/chapter-8/counters
>>> >
>>> >
>>> >
>>> > On Tue, Jul 22, 2014 at 1:43 PM, Neil Ferguson <nferguson@gmail.com>
>>> wrote:
>>> >
>>> >> Hi Reynold
>>> >>
>>> >> Thanks for your reply.
>>> >>
>>> >> Accumulators are, of course, stored in the Accumulators object as
>>> >> thread-local variables. However, the Accumulators object isn't public,
>>> so
>>> >> when a Task is executing there's no way to get the set of accumulators
>>> for
>>> >> the current thread -- accumulators still have to be passed to every
>>> method
>>> >> that needs them.
>>> >>
>>> >> Additionally, unless an accumulator is explicitly referenced it won't
>>> be
>>> >> serialized as part of a Task, and won't make it into the Accumulators
>>> >> object in the first place.
>>> >>
>>> >> I should also note that what I'm proposing is not specific to
>>> Accumulators
>>> >> -- I am proposing that any data can be stored in a thread-local
>>> variable. I
>>> >> think there are probably many other use cases other than my one.
>>> >>
>>> >> Neil
>>> >>
>>> >>
>>> >> On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com>
>>> wrote:
>>> >>
>>> >> > Thanks for the thoughtful email, Neil and Christopher.
>>> >> >
>>> >> > If I understand this correctly, it seems like the dynamic variable
>>> is
>>> >> just
>>> >> > a variant of the accumulator (a static one since it is a global
>>> object).
>>> >> > Accumulators are already implemented using thread-local variables
>>> under
>>> >> the
>>> >> > hood. Am I misunderstanding something?
>>> >> >
>>> >> >
>>> >> >
>>> >> > On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.com>
>>>
>>> >> > wrote:
>>> >> >
>>> >> > > Hi Neil, first off, I'm generally a sympathetic advocate for
>>> making
>>> >> > changes
>>> >> > > to Spark internals to make it easier/better/faster/more awesome.
>>> >> > >
>>> >> > > In this case, I'm (a) not clear about what you're trying to
>>> accomplish,
>>> >> > and
>>> >> > > (b) a bit worried about the proposed solution.
>>> >> > >
>>> >> > > On (a): it is stated that you want to pass some Accumulators
>>> around.
>>> >> Yet
>>> >> > > the proposed solution is for some "shared" variable that may be
>>> set and
>>> >> > > "mapped out" and possibly "reduced back", but without any
>>> accompanying
>>> >> > > accumulation semantics. And yet it doesn't seem like you only want
>>> just
>>> >> > the
>>> >> > > broadcast property. Can you clarify the problem statement with
>>> some
>>> >> > > before/after client code examples?
>>> >> > >
>>> >> > > On (b): you're right that adding variables to SparkContext should
>>> be
>>> >> done
>>> >> > > with caution, as it may have unintended consequences beyond just
>>> serdes
>>> >> > > payload size. For example, there is a stated intention of
>>> supporting
>>> >> > > multiple SparkContexts in the future, and this proposed solution
>>> can
>>> >> make
>>> >> > > it a bigger challenge to do so. Indeed, we had a gut-wrenching
>>> call to
>>> >> > make
>>> >> > > a while back on a subject related to this (see
>>> >> > > https://github.com/mesos/spark/pull/779). Furthermore, even in a
>>> >> single
>>> >> > > SparkContext application, there may be multiple "clients" (of that
>>> >> > > application) whose intent to use the proposed "SparkDynamic" would
>>> not
>>> >> > > necessarily be coordinated.
>>> >> > >
>>> >> > > So, considering a ratio of a/b (benefit/cost), it's not clear to
>>> me
>>> >> that
>>> >> > > the benefits are significant enough to warrant the costs. Do I
>>> >> > > misunderstand that the benefit is to save one explicit parameter
>>> (the
>>> >> > > "context") in the signature/closure code?
>>> >> > >
>>> >> > > --
>>> >> > > Christopher T. Nguyen
>>> >> > > Co-founder & CEO, Adatao <http://adatao.com>
>>> >> > > linkedin.com/in/ctnguyen
>>> >> > >
>>> >> > >
>>> >> > >
>>> >> > > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <
>>> nferguson@gmail.com>
>>> >> > > wrote:
>>> >> > >
>>> >> > > > Hi all
>>> >> > > >
>>> >> > > > I have been adding some metrics to the ADAM project
>>> >> > > > https://github.com/bigdatagenomics/adam, which runs on Spark,
>>> and
>>> >> > have a
>>> >> > > > proposal for an enhancement to Spark that would make this work
>>> >> cleaner
>>> >> > > and
>>> >> > > > easier.
>>> >> > > >
>>> >> > > > I need to pass some Accumulators around, which will aggregate
>>> metrics
>>> >> > > > (timing stats and other metrics) across the cluster. However, it
>>> is
>>> >> > > > cumbersome to have to explicitly pass some "context" containing
>>> these
>>> >> > > > accumulators around everywhere that might need them. I can use
>>> Scala
>>> >> > > > implicits, which help slightly, but I'd still need to modify
>>> every
>>> >> > method
>>> >> > > > in the call stack to take an implicit variable.
>>> >> > > >
>>> >> > > > So, I'd like to propose that we add the ability to have "dynamic
>>> >> > > variables"
>>> >> > > > (basically thread-local variables) to Spark. This would avoid
>>> having
>>> >> to
>>> >> > > > pass the Accumulators around explicitly.
>>> >> > > >
>>> >> > > > My proposed approach is to add a method to the SparkContext
>>> class as
>>> >> > > > follows:
>>> >> > > >
>>> >> > > > /**
>>> >> > > > * Sets the value of a "dynamic variable". This value is made
>>> >> available
>>> >> > > to
>>> >> > > > jobs
>>> >> > > > * without having to be passed around explicitly. During
>>> execution
>>> >> of a
>>> >> > > > Spark job
>>> >> > > > * this value can be obtained from the [[SparkDynamic]] object.
>>> >> > > > */
>>> >> > > > def setDynamicVariableValue(value: Any)
>>> >> > > >
>>> >> > > > Then, when a job is executing the SparkDynamic can be accessed
>>> to
>>> >> > obtain
>>> >> > > > the value of the dynamic variable. The implementation of this
>>> object
>>> >> is
>>> >> > > as
>>> >> > > > follows:
>>> >> > > >
>>> >> > > > object SparkDynamic {
>>> >> > > > private val dynamicVariable = new DynamicVariable[Any]()
>>> >> > > > /**
>>> >> > > > * Gets the value of the "dynamic variable" that has been set in
>>> >> the
>>> >> > > > [[SparkContext]]
>>> >> > > > */
>>> >> > > > def getValue: Option[Any] = {
>>> >> > > > Option(dynamicVariable.value)
>>> >> > > > }
>>> >> > > > private[spark] def withValue[S](threadValue: Option[Any])(thunk:
>>> =>
>>> >> > > S): S
>>> >> > > > = {
>>> >> > > > dynamicVariable.withValue(threadValue.orNull)(thunk)
>>> >> > > > }
>>> >> > > > }
>>> >> > > >
>>> >> > > > The change involves modifying the Task object to serialize the
>>> value
>>> >> of
>>> >> > > the
>>> >> > > > dynamic variable, and modifying the TaskRunner class to
>>> deserialize
>>> >> the
>>> >> > > > value and make it available in the thread that is running the
>>> task
>>> >> > (using
>>> >> > > > the SparkDynamic.withValue method).
>>> >> > > >
>>> >> > > > I have done a quick prototype of this in this commit:
>>> >> > > >
>>> >> > > >
>>> >> > >
>>> >> >
>>> >>
>>> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7d273dcea6
>>> >> > > > and it seems to work fine in my (limited) testing. It needs more
>>> >> > testing,
>>> >> > > > tidy-up and documentation though.
>>> >> > > >
>>> >> > > > One drawback is that the dynamic variable will be serialized for
>>> >> every
>>> >> > > Task
>>> >> > > > whether it needs it or not. For my use case this might not be
>>> too
>>> >> much
>>> >> > > of a
>>> >> > > > problem, as serializing and deserializing Accumulators looks
>>> fairly
>>> >> > > > lightweight -- however we should certainly warn users against
>>> >> setting a
>>> >> > > > dynamic variable containing lots of data. I thought about using
>>> >> > broadcast
>>> >> > > > tables here, but I don't think it's possible to put Accumulators
>>> in a
>>> >> > > > broadcast table (as I understand it, they're intended for purely
>>> >> > > read-only
>>> >> > > > data).
>>> >> > > >
>>> >> > > > What do people think about this proposal? My use case aside, it
>>> seems
>>> >> > > like
>>> >> > > > it would be a generally useful enhancment to be able to pass
>>> certain
>>> >> > data
>>> >> > > > around without having to explicitly pass it everywhere.
>>> >> > > >
>>> >> > > > Neil
>>> >> > > >
>>> >> > >
>>> >> >
>>> >>
>>>
>>

From dev-return-8526-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 24 21:31:08 2014
Return-Path: <dev-return-8526-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 94CE011328
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 24 Jul 2014 21:31:08 +0000 (UTC)
Received: (qmail 11913 invoked by uid 500); 24 Jul 2014 21:31:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11873 invoked by uid 500); 24 Jul 2014 21:31:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11860 invoked by uid 99); 24 Jul 2014 21:31:07 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 21:31:07 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nferguson@gmail.com designates 209.85.216.50 as permitted sender)
Received: from [209.85.216.50] (HELO mail-qa0-f50.google.com) (209.85.216.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 24 Jul 2014 21:31:05 +0000
Received: by mail-qa0-f50.google.com with SMTP id s7so3603351qap.23
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 14:30:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:mime-version:message-id:in-reply-to:references:from:to:cc
         :subject:content-type;
        bh=Rh3MDVOcqXM7lVI08wEyHWQM6iW8wEtsmLeIGL4Wm+E=;
        b=t0lV72oRHpaMiZNuYIOrpZN5w9Ue9kabmUcZ9IXgHJFG4ntxvD1/EYbYYrP8tz6a70
         2loMkHIjhTWPDRg2PS5dOLIzUDI7tu44ei5GfUm2ypw1IUvjtNqeBBLz58lvMGALxBib
         7vpsmbH2nyUbe5B94FO9RVeE3pJBbNb9C9DQ5rSZ8LNebPsu202Wv/YEziQoAlTwvpmn
         djoPfXWZftAIUve0UnpCaaQnvwMMhsVICdIRqig7fC04kPgmratfmFuRzrcXMD0TrMSH
         BpEZrdKFyPaSwJmoWMwHz3mKYlmRqWGrrJML6e0NjpnPeovV1Vr+54HgXvQ7ST3I59yK
         nWnw==
X-Received: by 10.140.92.235 with SMTP id b98mr18776782qge.97.1406237439982;
        Thu, 24 Jul 2014 14:30:39 -0700 (PDT)
Received: from hedwig-14.prd.orcali.com (ec2-54-85-253-60.compute-1.amazonaws.com. [54.85.253.60])
        by mx.google.com with ESMTPSA id a44sm8522262qgf.9.2014.07.24.14.30.39
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 24 Jul 2014 14:30:39 -0700 (PDT)
Date: Thu, 24 Jul 2014 14:30:39 -0700 (PDT)
X-Google-Original-Date: Thu, 24 Jul 2014 21:30:35 GMT
MIME-Version: 1.0
X-Mailer: Nodemailer (0.5.0; +http://www.nodemailer.com/)
Message-Id: <1406237435217.8dcdb73e@Nodemailer>
In-Reply-To: <CABPQxsth1osuj5A-mwV8wSSOfi8xv5hyKkNBBE1cPjazxiMZDw@mail.gmail.com>
References: <CABPQxsth1osuj5A-mwV8wSSOfi8xv5hyKkNBBE1cPjazxiMZDw@mail.gmail.com>
X-Orchestra-Oid: 091D7F99-3BCA-4625-8788-1AA0B14B2BD2
X-Orchestra-Sig: b15a187593f095fb150d8a1529e991117e9a5b8a
X-Orchestra-Thrid: TFBC64990-E6A9-40EC-BA1D-CD152380485C_1474273764458606556
X-Orchestra-Thrid-Sig: 22cd8238446a1491b2505311a5bd2080ecf7d28c
X-Orchestra-Account: 472d6ce904307128f39cc44b081c2f6ea2edc767
From: "Neil Ferguson" <nferguson@gmail.com>
To: dev@spark.apache.org
Cc: dev@spark.apache.org
Subject: Re: "Dynamic variables" in Spark
Content-Type: multipart/alternative;
 boundary="----Nodemailer-0.5.0-?=_1-1406237439368"
X-Virus-Checked: Checked by ClamAV on apache.org

------Nodemailer-0.5.0-?=_1-1406237439368
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

That would work well for me! Do you think it would be necessary to specify =
which accumulators should be available in the registry, or would we just =
broadcast all named accumulators registered in SparkContext and make them =
available in the registry=3F




Anyway, I'm happy to make the necessary changes (unless someone else wants =
to).

On Thu, Jul 24, 2014 at 10:17 PM, Patrick Wendell <pwendell@gmail.com>
wrote:

> What if we have a registry for accumulators, where you can access them
> statically by name=3F
> - Patrick
> On Thu, Jul 24, 2014 at 1:51 PM, Neil Ferguson <nferguson@gmail.com> =
wrote:
>> I realised that my last reply wasn't very clear -- let me try and =
clarify.
>>
>> The patch for named accumulators looks very useful, however in =
Shivaram's
>> example he was able to retrieve the named task metrics (statically) from=
 a
>> TaskMetrics object, as follows:
>>
>> TaskMetrics.get(=22f1-time=22)
>>
>> However, I don't think this would be possible with the named =
accumulators
>> -- I believe they'd need to be passed to every function that needs them,=

>> which I think would be cumbersome in any application of reasonable
>> complexity.
>>
>> This is what I was trying to solve with my proposal for dynamic =
variables
>> in Spark. However, the ability to retrieve named accumulators from a
>> thread-local would work just as well for my use case. I'd be happy to
>> implement either solution if there's interest.
>>
>> Alternatively, if I'm missing some other way to accomplish this please =
let
>> me know.
>>
>> On a (slight) aside, I now think it would be possible to implement =
dynamic
>> variables by broadcasting them. I was looking at Reynold's PR [1] to
>> broadcast the RDD object, and I think it would be possible to take a
>> similar approach -- that is, broadcast the serialized form, and =
deserialize
>> when executing each task.
>>
>> [1] https://github.com/apache/spark/pull/1498
>>
>>
>>
>>
>>
>>
>>
>> On Wed, Jul 23, 2014 at 8:30 AM, Neil Ferguson <nferguson@gmail.com> =
wrote:
>>
>>>  Hi Patrick.
>>>
>>> That looks very useful. The thing that seems to be missing from =
Shivaram's
>>> example is the ability to access TaskMetrics statically (this is the =
same
>>> problem that I am trying to solve with dynamic variables).
>>>
>>>
>>>
>>> You mention defining an accumulator on the RDD. Perhaps I am missing
>>> something here, but my understanding was that accumulators are defined =
in
>>> SparkContext and are not part of the RDD. Is that correct=3F
>>>
>>> Neil
>>>
>>> On Tue, Jul 22, 2014 at 22:21, Patrick Wendell <pwendell@gmail.com
>>> =3D=22mailto:pwendell@gmail.com=22>> wrote:
>>>
>>>> Shivaram,
>>>>
>>>> You should take a look at this patch which adds support for naming
>>>> accumulators - this is likely to get merged in soon. I actually
>>>> started this patch by supporting named TaskMetrics similar to what =
you
>>>> have there, but then I realized there is too much semantic overlap
>>>> with accumulators, so I just went that route.
>>>>
>>>> For instance, it would be nice if any user-defined metrics are
>>>> accessible at the driver program.
>>>>
>>>> https://github.com/apache/spark/pull/1309
>>>>
>>>> In your example, you could just define an accumulator here on the RDD
>>>> and you'd see the incremental update in the web UI automatically.
>>>>
>>>> - Patrick
>>>>
>>>> On Tue, Jul 22, 2014 at 2:09 PM, Shivaram Venkataraman
>>>> <shivaram@eecs.berkeley.edu> wrote:
>>>> > From reading Neil's first e-mail, I think the motivation is to get =
some
>>>> > metrics in ADAM =3F -- I've run into a similar use-case with having
>>>> > user-defined metrics in long-running tasks and I think a nice way =
to
>>>> solve
>>>> > this would be to have user-defined TaskMetrics.
>>>> >
>>>> > To state my problem more clearly, lets say you have two functions =
you
>>>> use
>>>> > in a map call and want to measure how much time each of them takes. =
For
>>>> > example, if you have a code block like the one below and you want =
to
>>>> > measure how much time f1 takes as a fraction of the task.
>>>> >
>>>> > a.map { l =3D>
>>>> > val f =3D f1(l)
>>>> > ... some work here ...
>>>> > }
>>>> >
>>>> > It would be really cool if we could do something like
>>>> >
>>>> > a.map { l =3D>
>>>> > val start =3D System.nanoTime
>>>> > val f =3D f1(l)
>>>> > TaskMetrics.get(=22f1-time=22).add(System.nanoTime - start)
>>>> > }
>>>> >
>>>> > These task metrics have a different purpose from Accumulators in =
the
>>>> sense
>>>> > that we don't need to track lineage, perform commutative operations
>>>> etc.
>>>> > Further we also have a bunch of code in place to aggregate task =
metrics
>>>> > across a stage etc. So it would be great if we could also populate
>>>> these in
>>>> > the UI and show median/max etc.
>>>> > I think counters [1] in Hadoop served a similar purpose.
>>>> >
>>>> > Thanks
>>>> > Shivaram
>>>> >
>>>> > [1]
>>>> >
>>>> https://www.inkling.com/read/hadoop-definitive-guide-tom-white-3rd/cha=
pter-8/counters
>>>> >
>>>> >
>>>> >
>>>> > On Tue, Jul 22, 2014 at 1:43 PM, Neil Ferguson <nferguson@gmail.=
com>
>>>> wrote:
>>>> >
>>>> >> Hi Reynold
>>>> >>
>>>> >> Thanks for your reply.
>>>> >>
>>>> >> Accumulators are, of course, stored in the Accumulators object as
>>>> >> thread-local variables. However, the Accumulators object isn't =
public,
>>>> so
>>>> >> when a Task is executing there's no way to get the set of =
accumulators
>>>> for
>>>> >> the current thread -- accumulators still have to be passed to =
every
>>>> method
>>>> >> that needs them.
>>>> >>
>>>> >> Additionally, unless an accumulator is explicitly referenced it =
won't
>>>> be
>>>> >> serialized as part of a Task, and won't make it into the =
Accumulators
>>>> >> object in the first place.
>>>> >>
>>>> >> I should also note that what I'm proposing is not specific to
>>>> Accumulators
>>>> >> -- I am proposing that any data can be stored in a thread-local
>>>> variable. I
>>>> >> think there are probably many other use cases other than my one.
>>>> >>
>>>> >> Neil
>>>> >>
>>>> >>
>>>> >> On Tue, Jul 22, 2014 at 5:39 AM, Reynold Xin <rxin@databricks.com>
>>>> wrote:
>>>> >>
>>>> >> > Thanks for the thoughtful email, Neil and Christopher.
>>>> >> >
>>>> >> > If I understand this correctly, it seems like the dynamic =
variable
>>>> is
>>>> >> just
>>>> >> > a variant of the accumulator (a static one since it is a global
>>>> object).
>>>> >> > Accumulators are already implemented using thread-local =
variables
>>>> under
>>>> >> the
>>>> >> > hood. Am I misunderstanding something=3F
>>>> >> >
>>>> >> >
>>>> >> >
>>>> >> > On Mon, Jul 21, 2014 at 5:54 PM, Christopher Nguyen <ctn@adatao.=
com>
>>>>
>>>> >> > wrote:
>>>> >> >
>>>> >> > > Hi Neil, first off, I'm generally a sympathetic advocate for
>>>> making
>>>> >> > changes
>>>> >> > > to Spark internals to make it easier/better/faster/more awesome=
.
>>>> >> > >
>>>> >> > > In this case, I'm (a) not clear about what you're trying to
>>>> accomplish,
>>>> >> > and
>>>> >> > > (b) a bit worried about the proposed solution.
>>>> >> > >
>>>> >> > > On (a): it is stated that you want to pass some Accumulators
>>>> around.
>>>> >> Yet
>>>> >> > > the proposed solution is for some =22shared=22 variable that =
may be
>>>> set and
>>>> >> > > =22mapped out=22 and possibly =22reduced back=22, but without =
any
>>>> accompanying
>>>> >> > > accumulation semantics. And yet it doesn't seem like you only =
want
>>>> just
>>>> >> > the
>>>> >> > > broadcast property. Can you clarify the problem statement with
>>>> some
>>>> >> > > before/after client code examples=3F
>>>> >> > >
>>>> >> > > On (b): you're right that adding variables to SparkContext =
should
>>>> be
>>>> >> done
>>>> >> > > with caution, as it may have unintended consequences beyond =
just
>>>> serdes
>>>> >> > > payload size. For example, there is a stated intention of
>>>> supporting
>>>> >> > > multiple SparkContexts in the future, and this proposed =
solution
>>>> can
>>>> >> make
>>>> >> > > it a bigger challenge to do so. Indeed, we had a gut-wrenching
>>>> call to
>>>> >> > make
>>>> >> > > a while back on a subject related to this (see
>>>> >> > > https://github.com/mesos/spark/pull/779). Furthermore, even in =
a
>>>> >> single
>>>> >> > > SparkContext application, there may be multiple =22clients=22 =
(of that
>>>> >> > > application) whose intent to use the proposed =
=22SparkDynamic=22 would
>>>> not
>>>> >> > > necessarily be coordinated.
>>>> >> > >
>>>> >> > > So, considering a ratio of a/b (benefit/cost), it's not clear =
to
>>>> me
>>>> >> that
>>>> >> > > the benefits are significant enough to warrant the costs. Do I
>>>> >> > > misunderstand that the benefit is to save one explicit =
parameter
>>>> (the
>>>> >> > > =22context=22) in the signature/closure code=3F
>>>> >> > >
>>>> >> > > --
>>>> >> > > Christopher T. Nguyen
>>>> >> > > Co-founder & CEO, Adatao <http://adatao.com>
>>>> >> > > linkedin.com/in/ctnguyen
>>>> >> > >
>>>> >> > >
>>>> >> > >
>>>> >> > > On Mon, Jul 21, 2014 at 2:10 PM, Neil Ferguson <
>>>> nferguson@gmail.com>
>>>> >> > > wrote:
>>>> >> > >
>>>> >> > > > Hi all
>>>> >> > > >
>>>> >> > > > I have been adding some metrics to the ADAM project
>>>> >> > > > https://github.com/bigdatagenomics/adam, which runs on Spark,=

>>>> and
>>>> >> > have a
>>>> >> > > > proposal for an enhancement to Spark that would make this =
work
>>>> >> cleaner
>>>> >> > > and
>>>> >> > > > easier.
>>>> >> > > >
>>>> >> > > > I need to pass some Accumulators around, which will =
aggregate
>>>> metrics
>>>> >> > > > (timing stats and other metrics) across the cluster. However,=
 it
>>>> is
>>>> >> > > > cumbersome to have to explicitly pass some =22context=22 =
containing
>>>> these
>>>> >> > > > accumulators around everywhere that might need them. I can =
use
>>>> Scala
>>>> >> > > > implicits, which help slightly, but I'd still need to modify
>>>> every
>>>> >> > method
>>>> >> > > > in the call stack to take an implicit variable.
>>>> >> > > >
>>>> >> > > > So, I'd like to propose that we add the ability to have =
=22dynamic
>>>> >> > > variables=22
>>>> >> > > > (basically thread-local variables) to Spark. This would =
avoid
>>>> having
>>>> >> to
>>>> >> > > > pass the Accumulators around explicitly.
>>>> >> > > >
>>>> >> > > > My proposed approach is to add a method to the SparkContext
>>>> class as
>>>> >> > > > follows:
>>>> >> > > >
>>>> >> > > > /**
>>>> >> > > > * Sets the value of a =22dynamic variable=22. This value is =
made
>>>> >> available
>>>> >> > > to
>>>> >> > > > jobs
>>>> >> > > > * without having to be passed around explicitly. During
>>>> execution
>>>> >> of a
>>>> >> > > > Spark job
>>>> >> > > > * this value can be obtained from the [[SparkDynamic]] object=
.
>>>> >> > > > */
>>>> >> > > > def setDynamicVariableValue(value: Any)
>>>> >> > > >
>>>> >> > > > Then, when a job is executing the SparkDynamic can be =
accessed
>>>> to
>>>> >> > obtain
>>>> >> > > > the value of the dynamic variable. The implementation of =
this
>>>> object
>>>> >> is
>>>> >> > > as
>>>> >> > > > follows:
>>>> >> > > >
>>>> >> > > > object SparkDynamic {
>>>> >> > > > private val dynamicVariable =3D new DynamicVariable[Any]()
>>>> >> > > > /**
>>>> >> > > > * Gets the value of the =22dynamic variable=22 that has been =
set in
>>>> >> the
>>>> >> > > > [[SparkContext]]
>>>> >> > > > */
>>>> >> > > > def getValue: Option[Any] =3D {
>>>> >> > > > Option(dynamicVariable.value)
>>>> >> > > > }
>>>> >> > > > private[spark] def withValue[S](threadValue: =
Option[Any])(thunk:
>>>> =3D>
>>>> >> > > S): S
>>>> >> > > > =3D {
>>>> >> > > > dynamicVariable.withValue(threadValue.orNull)(thunk)
>>>> >> > > > }
>>>> >> > > > }
>>>> >> > > >
>>>> >> > > > The change involves modifying the Task object to serialize =
the
>>>> value
>>>> >> of
>>>> >> > > the
>>>> >> > > > dynamic variable, and modifying the TaskRunner class to
>>>> deserialize
>>>> >> the
>>>> >> > > > value and make it available in the thread that is running =
the
>>>> task
>>>> >> > (using
>>>> >> > > > the SparkDynamic.withValue method).
>>>> >> > > >
>>>> >> > > > I have done a quick prototype of this in this commit:
>>>> >> > > >
>>>> >> > > >
>>>> >> > >
>>>> >> >
>>>> >>
>>>> https://github.com/nfergu/spark/commit/8be28d878f43ad6c49f892764011ae7=
d273dcea6
>>>> >> > > > and it seems to work fine in my (limited) testing. It needs =
more
>>>> >> > testing,
>>>> >> > > > tidy-up and documentation though.
>>>> >> > > >
>>>> >> > > > One drawback is that the dynamic variable will be serialized =
for
>>>> >> every
>>>> >> > > Task
>>>> >> > > > whether it needs it or not. For my use case this might not =
be
>>>> too
>>>> >> much
>>>> >> > > of a
>>>> >> > > > problem, as serializing and deserializing Accumulators looks
>>>> fairly
>>>> >> > > > lightweight -- however we should certainly warn users =
against
>>>> >> setting a
>>>> >> > > > dynamic variable containing lots of data. I thought about =
using
>>>> >> > broadcast
>>>> >> > > > tables here, but I don't think it's possible to put =
Accumulators
>>>> in a
>>>> >> > > > broadcast table (as I understand it, they're intended for =
purely
>>>> >> > > read-only
>>>> >> > > > data).
>>>> >> > > >
>>>> >> > > > What do people think about this proposal=3F My use case aside=
, it
>>>> seems
>>>> >> > > like
>>>> >> > > > it would be a generally useful enhancment to be able to pass
>>>> certain
>>>> >> > data
>>>> >> > > > around without having to explicitly pass it everywhere.
>>>> >> > > >
>>>> >> > > > Neil
>>>> >> > > >
>>>> >> > >
>>>> >> >
>>>> >>
>>>>
>>>
------Nodemailer-0.5.0-?=_1-1406237439368--

From dev-return-8527-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 01:28:58 2014
Return-Path: <dev-return-8527-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD9DB11AAD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 01:28:58 +0000 (UTC)
Received: (qmail 77489 invoked by uid 500); 25 Jul 2014 01:28:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77429 invoked by uid 500); 25 Jul 2014 01:28:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 77418 invoked by uid 99); 25 Jul 2014 01:28:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 01:28:57 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.54] (HELO mail-qa0-f54.google.com) (209.85.216.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 01:28:55 +0000
Received: by mail-qa0-f54.google.com with SMTP id k15so3923099qaq.27
        for <dev@spark.apache.org>; Thu, 24 Jul 2014 18:28:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Dpmpd7oeSjjK+4xKt6HBOBGQTHFUrYm4NrH/pwazNFU=;
        b=JwoL8Mb0wGaxYEYN0f6wcYsw0I9D0tGhWt+LIJ6+wZDm9C60B8icoGCZrbQT53aOWy
         NNQ4X7oIsooJQt978N7uZlr/uS/rOHBR9AAkZgANlLbq4+LHuBeQv0iA4jLY2/HrjS79
         UzrhKLrGuaiw563vy9ekHoBBiv2ll91I5nqDvZBpiGoxnW7EE+VdvXfwvYHZ0h7/3MjQ
         L05meu3FXw3ukZWDk3HTJlQkPHknlD0ic/GO4uZrZqR6FFKFO6ir/z/k9NSgMY4xjHNP
         o12GAzudNfOpI3dpPzZyYXu0imQAQ2nemtSBbBo2UVLbt9e1GdqsP5+osmktm7lmGmH6
         NwqQ==
X-Gm-Message-State: ALoCoQlWUY32NDJ8Fie+U82x6acaabymcyH91tmtJmYp8cPhgxiStXTe97GR+muN7oRkOUlWbV72
X-Received: by 10.224.163.83 with SMTP id z19mr21916313qax.68.1406251710490;
 Thu, 24 Jul 2014 18:28:30 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.206.9 with HTTP; Thu, 24 Jul 2014 18:28:10 -0700 (PDT)
In-Reply-To: <CACkSZy0zbTwo15HcL=7W0GPcsYH2-mwZi+F7cvT33FbheCnmSA@mail.gmail.com>
References: <CACkSZy36xUKKZ8zD-StKcJE93Zzpz2Ec0V48ar+jii4AD7o-Fg@mail.gmail.com>
 <CACkSZy0zbTwo15HcL=7W0GPcsYH2-mwZi+F7cvT33FbheCnmSA@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Thu, 24 Jul 2014 18:28:10 -0700
Message-ID: <CAAswR-6qc=ar1tbJ+SLuzF7V51erTnspVxwFpzsWVaKscCOeJg@mail.gmail.com>
Subject: Re: SQLQuerySuite error
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01294f66ab497204fefa7d38
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01294f66ab497204fefa7d38
Content-Type: text/plain; charset=UTF-8

Thanks for reporting back.  I was pretty confused trying to reproduce the
error :)


On Thu, Jul 24, 2014 at 1:09 PM, Stephen Boesch <javadba@gmail.com> wrote:

> OK I did find my error.  The missing step:
>
>   mvn install
>
> I should have republished (mvn install) all of the other modules .
>
> The mvn -pl  will rely on the modules locally published and so the latest
> code that I had git pull'ed was not being used (except  the sql/core module
> code).
>
> The tests are passing after having properly performed the mvn install
> before  running with the mvn -pl sql/core.
>
>
>
>
> 2014-07-24 12:04 GMT-07:00 Stephen Boesch <javadba@gmail.com>:
>
> >
> > Are other developers seeing the following error for the recently added
> > substr() method?  If not, any ideas why the following invocation of tests
> > would be failing for me - i.e. how the given invocation would need to be
> > tweaked?
> >
> > mvn -Pyarn -Pcdh5 test  -pl sql/core
> > -DwildcardSuites=org.apache.spark.sql.SQLQuerySuite
> >
> > (note cdh5 is a custom profile for cdh5.0.0 but should not be affecting
> > these results)
> >
> > Only the test("SPARK-2407 Added Parser of SQL SUBSTR()") fails: all of
> the
> > other 33 tests pass.
> >
> > SQLQuerySuite:
> > - SPARK-2041 column name equals tablename
> > - SPARK-2407 Added Parser of SQL SUBSTR() *** FAILED ***
> >   Exception thrown while executing query:
> >   == Logical Plan ==
> >   java.lang.UnsupportedOperationException
> >   == Optimized Logical Plan ==
> >   java.lang.UnsupportedOperationException
> >   == Physical Plan ==
> >   java.lang.UnsupportedOperationException
> >   == Exception ==
> >   java.lang.UnsupportedOperationException
> >   java.lang.UnsupportedOperationException
> >   at
> >
> org.apache.spark.sql.catalyst.analysis.EmptyFunctionRegistry$.lookupFunction(FunctionRegistry.scala:33)
> >   at
> >
> org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:131)
> >   at
> >
> org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$5$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:129)
> >   at
> >
> org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:165)
> >   at
> >
> org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:183)
> >   at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
> >   at scala.collection.Iterator$class.foreach(Iterator.scala:727)
> >   at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> >   at
> > scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
> >   at
> > scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
> >   at
> > scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
> >   at scala.collection.TraversableOnce$class.to
> (TraversableOnce.scala:273)
> >   at scala.collection.AbstractIterator.to(Iterator.scala:1157)
> >   at
> >
> scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
> >   at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
> >   at
> > scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
> >   at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
> >   at
> >
> org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:212)
> >   at
> >
> org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:168)
> >   at org.apache.spark.sql.catalyst.plans.QueryPlan.org
> >
> $apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:52)
> >   at
> >
> org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:66)
> >   at
> >
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
> >   at
> >
> scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
> >   at scala.collection.immutable.List.foreach(List.scala:318)
> >   at
> scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
> >   at scala.collection.AbstractTraversable.map(Traversable.scala:105)
> >   at
> >
>

--089e01294f66ab497204fefa7d38--

From dev-return-8528-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 14:36:55 2014
Return-Path: <dev-return-8528-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8C6AF11D27
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 14:36:55 +0000 (UTC)
Received: (qmail 3492 invoked by uid 500); 25 Jul 2014 14:36:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3439 invoked by uid 500); 25 Jul 2014 14:36:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 31545 invoked by uid 99); 24 Jul 2014 20:27:52 -0000
X-ASF-Spam-Status: No, hits=1.9 required=10.0
	tests=HTML_MESSAGE,NO_DNS_FOR_FROM,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of john@omernik.com designates 209.85.220.182 as permitted sender)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=nnLhahlC/QVAT0ZX5tchA4NLloAhQogBqvDg7kHEsR8=;
        b=I3sCsdLEXx+S4/cF2gSQGuN0gzurR1RFQ6Hgh9a6eL1TaC5gpUGp9NZB2HiqlqbxyG
         Nex5PuNvwF5qwoPI91LahSj4f75vymfmbnQjnEcb/lzllkCkFLfg8P7AwHwbLK3mMVWL
         D5xfM8Fy0w5d7TmFy4XE2Y3pT+GxY4uBvRBuLdPousXT/vU9f/XC6t4CvKqwA57HT3G3
         2zuWa5mim2FM7fEW3pS1POHbsYQqGHSIOZ5bv2k3uq8eoYrDNIkWNuz6XoHLeaK1Iavq
         c/mPQb0dHnYkeTEw8PfGNE+GTRhfJZu8Btzwc1LyxovpgnsijCkgCZLpho7J/0oyeC5e
         sBUg==
X-Gm-Message-State: ALoCoQkAupeMs0k8drmSMhn4TdC09M3gPao2VzTpq1+RaDZlONtt6w64bjaSe223gtYF2aNUG67o
X-Received: by 10.220.144.10 with SMTP id x10mr16238187vcu.42.1406233647484;
 Thu, 24 Jul 2014 13:27:27 -0700 (PDT)
MIME-Version: 1.0
In-Reply-To: <CAPbPHgm2KiaadLHMKXU2vD5u9nQQnLEbi4z0_W8gbA_i6NDREQ@mail.gmail.com>
References: <CAPbPHg=8pUkk0cNvFy-GYSTj-njpcsFKfuQGdgrPx8zBNPtS1Q@mail.gmail.com>
 <CA+-p3AF3rSpzMewXGfCoG3N6VyYMqjmgjV0sVr0d5Uzhd8p=Gg@mail.gmail.com>
 <CAPbPHg=hG9CNnahtDz7sRMyMsMhAWgTcpN0Va065O7ASTA1qqw@mail.gmail.com>
 <CACfA1zWkFZRBMvJ1S+oWGRsCcrE+V0sxZMpxd9S-_nSF8xyi3g@mail.gmail.com>
 <CAPbPHgmZ0uDsH420-1cT_3ng0NTnFibrscrZKqn5nZ8fUeSdog@mail.gmail.com>
 <CANGvG8odA4ZTBmdM0EaNbAnY8dERx34zLvMgdJocWo8WXTqmJw@mail.gmail.com> <CAPbPHgm2KiaadLHMKXU2vD5u9nQQnLEbi4z0_W8gbA_i6NDREQ@mail.gmail.com>
From: John Omernik <john@omernik.com>
Date: Thu, 24 Jul 2014 15:27:06 -0500
Message-ID: <CAKOFcwo6cMFa36zkPCvtawSvuEzqK9u3fcDRDk5jW_z4xf2shw@mail.gmail.com>
Subject: Re: Configuring Spark Memory
To: user@spark.apache.org
Cc: dev@spark.apache.org
Content-Type: multipart/related; boundary=047d7b34368207ba9104fef64965
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b34368207ba9104fef64965
Content-Type: multipart/alternative; boundary=047d7b34368207ba8d04fef64964

--047d7b34368207ba8d04fef64964
Content-Type: text/plain; charset=UTF-8

SO this is good information for standalone, but how is memory distributed
within Mesos?  There's coarse grain mode where the execute stays active, or
theres fine grained mode where it appears each task is it's only process in
mesos, how to memory allocations work in these cases? Thanks!



On Thu, Jul 24, 2014 at 12:14 PM, Martin Goodson <martin@skimlinks.com>
wrote:

> Great - thanks for the clarification Aaron. The offer stands for me to
> write some documentation and an example that covers this without leaving
> *any* room for ambiguity.
>
>
>
>
> --
> Martin Goodson  |  VP Data Science
> (0)20 3397 1240
> [image: Inline image 1]
>
>
> On Thu, Jul 24, 2014 at 6:09 PM, Aaron Davidson <ilikerps@gmail.com>
> wrote:
>
>> Whoops, I was mistaken in my original post last year. By default, there
>> is one executor per node per Spark Context, as you said.
>> "spark.executor.memory" is the amount of memory that the application
>> requests for each of its executors. SPARK_WORKER_MEMORY is the amount of
>> memory a Spark Worker is willing to allocate in executors.
>>
>> So if you were to set SPARK_WORKER_MEMORY to 8g everywhere on your
>> cluster, and spark.executor.memory to 4g, you would be able to run 2
>> simultaneous Spark Contexts who get 4g per node. Similarly, if
>> spark.executor.memory were 8g, you could only run 1 Spark Context at a time
>> on the cluster, but it would get all the cluster's memory.
>>
>>
>> On Thu, Jul 24, 2014 at 7:25 AM, Martin Goodson <martin@skimlinks.com>
>> wrote:
>>
>>> Thank you Nishkam,
>>> I have read your code. So, for the sake of my understanding, it seems
>>> that for each spark context there is one executor per node? Can anyone
>>> confirm this?
>>>
>>>
>>> --
>>> Martin Goodson  |  VP Data Science
>>> (0)20 3397 1240
>>> [image: Inline image 1]
>>>
>>>
>>> On Thu, Jul 24, 2014 at 6:12 AM, Nishkam Ravi <nravi@cloudera.com>
>>> wrote:
>>>
>>>> See if this helps:
>>>>
>>>> https://github.com/nishkamravi2/SparkAutoConfig/
>>>>
>>>> It's a very simple tool for auto-configuring default parameters in
>>>> Spark. Takes as input high-level parameters (like number of nodes, cores
>>>> per node, memory per node, etc) and spits out default configuration, user
>>>> advice and command line. Compile (javac SparkConfigure.java) and run (java
>>>> SparkConfigure).
>>>>
>>>> Also cc'ing dev in case others are interested in helping evolve this
>>>> over time (by refining the heuristics and adding more parameters).
>>>>
>>>>
>>>>  On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <martin@skimlinks.com>
>>>> wrote:
>>>>
>>>>> Thanks Andrew,
>>>>>
>>>>> So if there is only one SparkContext there is only one executor per
>>>>> machine? This seems to contradict Aaron's message from the link above:
>>>>>
>>>>> "If each machine has 16 GB of RAM and 4 cores, for example, you might
>>>>> set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by
>>>>> Spark.)"
>>>>>
>>>>> Am I reading this incorrectly?
>>>>>
>>>>> Anyway our configuration is 21 machines (one master and 20 slaves)
>>>>> each with 60Gb. We would like to use 4 cores per machine. This is pyspark
>>>>> so we want to leave say 16Gb on each machine for python processes.
>>>>>
>>>>> Thanks again for the advice!
>>>>>
>>>>>
>>>>>
>>>>> --
>>>>> Martin Goodson  |  VP Data Science
>>>>> (0)20 3397 1240
>>>>> [image: Inline image 1]
>>>>>
>>>>>
>>>>> On Wed, Jul 23, 2014 at 4:19 PM, Andrew Ash <andrew@andrewash.com>
>>>>> wrote:
>>>>>
>>>>>> Hi Martin,
>>>>>>
>>>>>> In standalone mode, each SparkContext you initialize gets its own set
>>>>>> of executors across the cluster.  So for example if you have two shells
>>>>>> open, they'll each get two JVMs on each worker machine in the cluster.
>>>>>>
>>>>>> As far as the other docs, you can configure the total number of cores
>>>>>> requested for the SparkContext, the amount of memory for the executor JVM
>>>>>> on each machine, the amount of memory for the Master/Worker daemons (little
>>>>>> needed since work is done in executors), and several other settings.
>>>>>>
>>>>>> Which of those are you interested in?  What spec hardware do you have
>>>>>> and how do you want to configure it?
>>>>>>
>>>>>> Andrew
>>>>>>
>>>>>>
>>>>>> On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <martin@skimlinks.com
>>>>>> > wrote:
>>>>>>
>>>>>>> We are having difficulties configuring Spark, partly because we
>>>>>>> still don't understand some key concepts. For instance, how many executors
>>>>>>> are there per machine in standalone mode? This is after having
>>>>>>> closely read the documentation several times:
>>>>>>>
>>>>>>> *http://spark.apache.org/docs/latest/configuration.html
>>>>>>> <http://spark.apache.org/docs/latest/configuration.html>*
>>>>>>> *http://spark.apache.org/docs/latest/spark-standalone.html
>>>>>>> <http://spark.apache.org/docs/latest/spark-standalone.html>*
>>>>>>> *http://spark.apache.org/docs/latest/tuning.html
>>>>>>> <http://spark.apache.org/docs/latest/tuning.html>*
>>>>>>> *http://spark.apache.org/docs/latest/cluster-overview.html
>>>>>>> <http://spark.apache.org/docs/latest/cluster-overview.html>*
>>>>>>>
>>>>>>> The cluster overview has some information here about executors but
>>>>>>> is ambiguous about whether there are single executors or multiple executors
>>>>>>> on each machine.
>>>>>>>
>>>>>>>  This message from Aaron Davidson implies that the executor memory
>>>>>>> should be set to total available memory on the machine divided by the
>>>>>>> number of cores:
>>>>>>> *http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E
>>>>>>> <http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3CCANGvG8o5K1SxgnFMT_9DK=vJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E>*
>>>>>>>
>>>>>>> But other messages imply that the executor memory should be set to
>>>>>>> the *total* available memory of each machine.
>>>>>>>
>>>>>>> We would very much appreciate some clarity on this and the myriad of
>>>>>>> other memory settings available (daemon memory, worker memory etc). Perhaps
>>>>>>> a worked example could be added to the docs? I would be happy to provide
>>>>>>> some text as soon as someone can enlighten me on the technicalities!
>>>>>>>
>>>>>>> Thank you
>>>>>>>
>>>>>>> --
>>>>>>> Martin Goodson  |  VP Data Science
>>>>>>> (0)20 3397 1240
>>>>>>> [image: Inline image 1]
>>>>>>>
>>>>>>
>>>>>>
>>>>>
>>>>
>>>
>>
>

--047d7b34368207ba8d04fef64964
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr">SO this is good information for standalone, but how is mem=
ory distributed within Mesos? =C2=A0There&#39;s coarse grain mode where the=
 execute stays active, or theres fine grained mode where it appears each ta=
sk is it&#39;s only process in mesos, how to memory allocations work in the=
se cases? Thanks!<div>

<br></div></div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quot=
e">On Thu, Jul 24, 2014 at 12:14 PM, Martin Goodson <span dir=3D"ltr">&lt;<=
a href=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.c=
om</a>&gt;</span> wrote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">Great - thanks for the clar=
ification Aaron. The offer stands for me to write some documentation and an=
 example that covers this without leaving <i>any</i> room for ambiguity.<di=
v>

<br><div><br></div></div></div>

<div class=3D"gmail_extra"><div class=3D""><br clear=3D"all"><div><div dir=
=3D"ltr"><br style=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,=
136,136)">--=C2=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"l=
tr"><div style=3D"font-size:12.7273px">



<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span=
></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span style=
=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"colo=
r:rgb(102,102,102)">=C2=A0</span></div>



<div style=3D"color:rgb(34,34,34);font-size:12.7273px"><img src=3D"cid:ii_1=
3d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"color:rgb(102,102,10=
2)"><span></span><span></span><br></span></div></div></div></div>
<br><br></div><div><div class=3D"h5"><div class=3D"gmail_quote">On Thu, Jul=
 24, 2014 at 6:09 PM, Aaron Davidson <span dir=3D"ltr">&lt;<a href=3D"mailt=
o:ilikerps@gmail.com" target=3D"_blank">ilikerps@gmail.com</a>&gt;</span> w=
rote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">

<div dir=3D"ltr">Whoops, I was mistaken in my original post last year. By d=
efault, there is one executor per node per Spark Context, as you said. &quo=
t;spark.executor.memory&quot; is the amount of memory that the application =
requests for each of its executors. SPARK_WORKER_MEMORY is the amount of me=
mory a Spark Worker is willing to allocate in executors.<div>





<br></div><div>So if you were to set SPARK_WORKER_MEMORY to 8g everywhere o=
n your cluster, and spark.executor.memory to 4g, you would be able to run 2=
 simultaneous Spark Contexts who get 4g per node. Similarly, if spark.execu=
tor.memory were 8g, you could only run 1 Spark Context at a time on the clu=
ster, but it would get all the cluster&#39;s memory.</div>





</div><div><div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quot=
e">On Thu, Jul 24, 2014 at 7:25 AM, Martin Goodson <span dir=3D"ltr">&lt;<a=
 href=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.co=
m</a>&gt;</span> wrote:<br>





<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr">Thank you=C2=A0Nishkam,<div=
>I have read your code. So, for the sake of my understanding, it seems that=
 for each spark context there is one executor per node? Can anyone confirm =
this?</div>





</div><div class=3D"gmail_extra"><div>

<br clear=3D"all"><div><div dir=3D"ltr"><br style=3D"color:rgb(136,136,136)=
"><span style=3D"color:rgb(136,136,136)">--=C2=A0</span><br style=3D"color:=
rgb(136,136,136)"><div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span=
 style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><span sty=
le=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>







</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">







<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></div>
<br><br></div><div><div><div class=3D"gmail_quote">On Thu, Jul 24, 2014 at =
6:12 AM, Nishkam Ravi <span dir=3D"ltr">&lt;<a href=3D"mailto:nravi@clouder=
a.com" target=3D"_blank">nravi@cloudera.com</a>&gt;</span> wrote:<br>

<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex">

<div dir=3D"ltr">See if this helps:<div><br></div><div><a href=3D"https://g=
ithub.com/nishkamravi2/SparkAutoConfig/" target=3D"_blank">https://github.c=
om/nishkamravi2/SparkAutoConfig/</a><br></div><div><br></div><div>It&#39;s =
a very simple tool for auto-configuring default parameters in Spark. Takes =
as input high-level parameters (like number of nodes, cores per node, memor=
y per node, etc) and spits out default configuration, user advice and comma=
nd line. Compile (javac SparkConfigure.java) and run (java SparkConfigure).=
</div>










<div><br></div><div>Also cc&#39;ing dev in case others are interested in he=
lping evolve this over time (by refining the heuristics and adding more par=
ameters).=C2=A0</div><div><div><div class=3D"gmail_extra"><br><br>

<div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 8:31 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br>
<blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;border-left:1p=
x #ccc solid;padding-left:1ex"><div dir=3D"ltr"><div>Thanks Andrew,</div><d=
iv><br></div><div>So if there is only one SparkContext there is only one ex=
ecutor per machine? This seems to contradict Aaron&#39;s message from the l=
ink above:</div>










<div><br></div>

&quot;If each machine has 16 GB of RAM and 4 cores, for example, you might =
set spark.executor.memory between 2 and 3 GB, totaling 8-12 GB used by Spar=
k.)&quot;<div><br></div><div>Am I reading this incorrectly?</div><div>











<br>
</div><div>Anyway our configuration is 21 machines (one master and 20 slave=
s) each with 60Gb. We would like to use 4 cores per machine. This is pyspar=
k so we want to leave say 16Gb on each machine for python processes.</div>












<div><br></div><div>Thanks again for the advice!</div><div><br></div></div>=
<div class=3D"gmail_extra"><div><br clear=3D"all"><div><div dir=3D"ltr"><br=
 style=3D"color:rgb(136,136,136)"><span style=3D"color:rgb(136,136,136)">--=
=C2=A0</span><br style=3D"color:rgb(136,136,136)">












<div dir=3D"ltr"><div style=3D"font-size:12.7273px"><span style=3D"backgrou=
nd-color:rgb(255,255,204)">Martin Goodson</span><span style=3D"color:rgb(10=
2,102,102)">=C2=A0 | =C2=A0VP Data Science<br></span></div><div style=3D"co=
lor:rgb(34,34,34);font-size:12.7273px">












<span style=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span st=
yle=3D"color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(3=
4,34,34);font-size:12.7273px"><img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"I=
nline image 1"><span style=3D"color:rgb(102,102,102)"><span></span><span></=
span><br>












</span></div></div></div></div>
<br><br></div><div class=3D"gmail_quote"><div>On Wed, Jul 23, 2014 at 4:19 =
PM, Andrew Ash <span dir=3D"ltr">&lt;<a href=3D"mailto:andrew@andrewash.com=
" target=3D"_blank">andrew@andrewash.com</a>&gt;</span> wrote:<br></div>
<div><div><blockquote class=3D"gmail_quote" style=3D"margin:0 0 0 .8ex;bord=
er-left:1px #ccc solid;padding-left:1ex">

<div dir=3D"ltr">Hi Martin,<div><br></div><div>In standalone mode, each Spa=
rkContext you initialize gets its own set of executors across the cluster. =
=C2=A0So for example if you have two shells open, they&#39;ll each get two =
JVMs on each worker machine in the cluster.</div>














<div><br></div><div>As far as the other docs, you can configure the total n=
umber of cores requested for the SparkContext, the amount of memory for the=
 executor JVM on each machine, the amount of memory for the Master/Worker d=
aemons (little needed since work is done in executors), and several other s=
ettings.</div>














<div><br></div><div>Which of those are you interested in? =C2=A0What spec h=
ardware do you have and how do you want to configure it?</div><span><font c=
olor=3D"#888888"><div><br></div><div>Andrew</div></font></span></div>

<div><div><div class=3D"gmail_extra"><br><br><div class=3D"gmail_quote">

On Wed, Jul 23, 2014 at 6:10 AM, Martin Goodson <span dir=3D"ltr">&lt;<a hr=
ef=3D"mailto:martin@skimlinks.com" target=3D"_blank">martin@skimlinks.com</=
a>&gt;</span> wrote:<br><blockquote class=3D"gmail_quote" style=3D"margin:0=
 0 0 .8ex;border-left:1px #ccc solid;padding-left:1ex">














<div dir=3D"ltr"><font face=3D"Arial">We are having difficulties configurin=
g Spark, partly because we still don&#39;t understand some key concepts. Fo=
r instance, how many executors are there per machine in standalone mode</fo=
nt><font face=3D"Arial">?=C2=A0</font><font face=3D"Arial">This is after ha=
ving closely read the documentation several times:</font><br>

















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/configuration.html" target=3D"_=
blank">http://spark.apache.org/docs/latest/configuration.html</a></u></span=
><br>















<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/spark-standalone.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/spark-standalone.html</a></=
u></span><br>















<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/tuning.html" target=3D"_blank">=
http://spark.apache.org/docs/latest/tuning.html</a></u></span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://spark.apache.org/docs/latest/cluster-overview.html" target=
=3D"_blank">http://spark.apache.org/docs/latest/cluster-overview.html</a></=
u></span><br>















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">The cluster overview has s=
ome information here about executors but is ambiguous about whether there a=
re single executors or multiple executors on each machine.</span><div><font=
 face=3D"Arial"><br>
















</font>
<span style=3D"font-size:13px;font-family:Arial">This message from=C2=A0Aar=
on Davidson implies that the executor memory should be set to total availab=
le memory on the machine divided by the number of cores:</span><br>
<span style=3D"font-size:13px;font-family:Arial;color:rgb(4,46,238)"><u><a =
href=3D"http://mail-archives.apache.org/mod_mbox/spark-user/201312.mbox/%3C=
CANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.com%3E" ta=
rget=3D"_blank">http://mail-archives.apache.org/mod_mbox/spark-user/201312.=
mbox/%3CCANGvG8o5K1SxgnFMT_9DK=3DvJ_pLBVe6zH_DN5sjwPznPbcpATA@mail.gmail.co=
m%3E</a></u></span><br>

















<span style=3D"font-size:13px;font-family:Arial"></span><br><span style=3D"=
font-size:13px;font-family:Arial">But other messages imply that the executo=
r memory should be set to the <i>total</i>=C2=A0available memory of each ma=
chine.</span><br>

















<span style=3D"font-size:13px;font-family:Arial"></span><br>
<span style=3D"font-size:13px;font-family:Arial">We would very much appreci=
ate some clarity on this and the myriad of other memory settings available =
(daemon memory, worker memory etc). Perhaps a worked example could be added=
 to the docs? I would be happy to provide some text as soon as someone can =
enlighten me on the technicalities!</span><div>
















<div><div dir=3D"ltr"><br></div><div dir=3D"ltr">Thank you</div><span><font=
 color=3D"#888888"><div dir=3D"ltr"><br><span style=3D"color:rgb(136,136,13=
6)">--=C2=A0</span><br style=3D"color:rgb(136,136,136)"><div dir=3D"ltr"><d=
iv style=3D"font-size:12.7273px">














<span style=3D"background-color:rgb(255,255,204)">Martin Goodson</span><spa=
n style=3D"color:rgb(102,102,102)">=C2=A0 | =C2=A0VP Data Science<br>

</span></div><div style=3D"color:rgb(34,34,34);font-size:12.7273px"><span s=
tyle=3D"color:rgb(102,102,102)">(0)20 3397 1240=C2=A0</span><span style=3D"=
color:rgb(102,102,102)">=C2=A0</span></div><div style=3D"color:rgb(34,34,34=
);font-size:12.7273px">
















<img src=3D"cid:ii_13d3cc6e9d9fe84c" alt=3D"Inline image 1"><span style=3D"=
color:rgb(102,102,102)"><span></span><span></span><br></span></div></div></=
div></font></span></div>
</div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div></div></div><br></div>
</blockquote></div><br></div></div></div></div>
</blockquote></div><br></div></div></div>
</blockquote></div><br></div>
</div></div></blockquote></div><br></div></div></div>
</blockquote></div><br></div>

--047d7b34368207ba8d04fef64964--
--047d7b34368207ba9104fef64965--

From dev-return-8529-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 18:24:01 2014
Return-Path: <dev-return-8529-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 98BEF10856
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 18:24:01 +0000 (UTC)
Received: (qmail 28932 invoked by uid 500); 25 Jul 2014 18:24:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28879 invoked by uid 500); 25 Jul 2014 18:24:01 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28867 invoked by uid 99); 25 Jul 2014 18:24:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 18:24:00 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rarecactus@gmail.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 18:23:56 +0000
Received: by mail-we0-f180.google.com with SMTP id w61so4688420wes.25
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 11:23:35 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=UIxu5aBsS9XP25bOoM09KmH2l4YUciyr+znJhGUnvGg=;
        b=IBi+BMRFLl4te6UAmTYnHzvUQxP1eaGGGxy81BPn6s4ANuq9bsFjM4vTXpVd/i1UZn
         3mD8bvVmZeUT4Z9I11eS1zA5FmYNJ3yNr97j/d4V9ro/0UlYtVbzM/GaUFfK4T11JICK
         LNGqbLquDVskV6/zY8OOlhV3mMHXZILZ43nLUBvWXoA+O0zgmVX8AWg3tHFcU7c6FRQQ
         tCwcN3JfcPmPJNiCVo2kTDXLkBg2to1+prZKHUa2ohudm+ASgzK1CXgauPNr4XlM+dBy
         kCVmHZVRscgiuRj787iLnFVN9NLJYJrtnmQOKgyRbqi2dR4PhWba7Z9+V33MMNtJaMpP
         4mmw==
MIME-Version: 1.0
X-Received: by 10.194.110.10 with SMTP id hw10mr24229413wjb.81.1406312615520;
 Fri, 25 Jul 2014 11:23:35 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.6.5 with HTTP; Fri, 25 Jul 2014 11:23:35 -0700 (PDT)
In-Reply-To: <000301cfa581$2b119310$8134b930$@innowireless.co.kr>
References: <000301cfa581$2b119310$8134b930$@innowireless.co.kr>
Date: Fri, 25 Jul 2014 11:23:35 -0700
X-Google-Sender-Auth: 85lpxdyUDtN18lsfyKjjmJQOxiI
Message-ID: <CA+qbEUPc8ggp0PGz14inYP2yi=OQbQxGTewxdP_OJTwe4An2Zg@mail.gmail.com>
Subject: Re: Suggestion for SPARK-1825
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e010d870ce42ee104ff08ab46
X-Virus-Checked: Checked by ClamAV on apache.org

--089e010d870ce42ee104ff08ab46
Content-Type: text/plain; charset=UTF-8

I have a similar issue with SPARK-1767.  There are basically three ways to
resolve the issue:

1. Use reflection to access classes newer than 0.21 (or whatever the oldest
version of Hadoop is that Spark supports)
2. Add a build variant (in Maven this would be a profile) that deals with
this.
3. Auto-detect which classes are available and use those.

#1 is the easiest for end-users, but it can lead to some ugly code.

#2 makes the code look nicer, but requires some effort on the part of
people building spark.  This can also lead to headaches for IDEs, if people
don't remember to select the new profile.  (For example, in IntelliJ, you
can't see any of the yarn classes when you import the project from Maven
without the YARN profile selected.)

#3 is something that... I don't know how to do in sbt or Maven.  I've been
told that an antrun task might work here, but it seems like it could get
really tricky.

Overall, I'd lean more towards #2 here.

best,
Colin


On Tue, Jul 22, 2014 at 12:47 AM, innowireless TaeYun Kim <
taeyun.kim@innowireless.co.kr> wrote:

> (I'm resending this mail since it seems that it was not sent. Sorry if this
> was already sent.)
>
> Hi,
>
>
>
> A couple of month ago, I made a pull request to fix
> https://issues.apache.org/jira/browse/SPARK-1825.
>
> My pull request is here: https://github.com/apache/spark/pull/899
>
>
>
> But that pull request has problems:
>
> l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.
>
> l  The related Hadoop API is marked as '@Unstable'.
>
>
>
> Here is an idea to remedy the problems: a new Spark configuration variable.
>
> Maybe it can be named as "spark.yarn.submit.crossplatform".
>
> If it is set to "true"(default is false), the related Spark code can use
> the
> hard-coded strings that is the same as the Hadoop API provides, thus
> avoiding compile error on the Hadoop versions below 2.4.0.
>
>
>
> Can someone implement this feature, if this idea is acceptable?
>
> Currently my knowledge on Spark source code and Scala is limited to
> implement it myself.
>
> To the right person, the modification should be trivial.
>
> You can refer to the source code changes of my pull request.
>
>
>
> Thanks.
>
>
>
>

--089e010d870ce42ee104ff08ab46--

From dev-return-8530-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 18:28:26 2014
Return-Path: <dev-return-8530-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E94C610881
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 18:28:25 +0000 (UTC)
Received: (qmail 40378 invoked by uid 500); 25 Jul 2014 18:28:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40286 invoked by uid 500); 25 Jul 2014 18:28:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39377 invoked by uid 99); 25 Jul 2014 18:28:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 18:28:24 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 18:28:21 +0000
Received: by mail-qg0-f41.google.com with SMTP id q107so5543582qgd.28
        for <multiple recipients>; Fri, 25 Jul 2014 11:27:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=PehcqcG/OsAhBS4KuOT/UyumuuM8znbtcP0Pw5gHmrc=;
        b=gVGzmgCwQMxB4mP9ZKm8ENB9M7OIN6aNOU+MblmE5izIu2OuGr89OokdnyZ6XK+8hF
         Uj1vNaG4qi2glo/lG2U67X657YHmnkOYtEPSM+T4UiEqD7LYaP9myxY9dHpQRYcKDjtD
         AHNTLWtHP64x49kwnHXZHoQSnHUrJA4eZkxPdNmGcR+HnIgtP9Ittb+dMrBPbTb6OXIE
         EtlS7hkMeVciEENBuXtxm4gOfeNfrN9+XmJUF7bZWIl7maDA0IZIqbdFbfQ7Hm95T57v
         hLoEpZ5SSjPP1Yef7E122Sfe9xnZd3KGlonDfqI6enka3GgmR1h7RDSbZvYfqPiTtI11
         1q2w==
MIME-Version: 1.0
X-Received: by 10.224.3.201 with SMTP id 9mr29168179qao.73.1406312876766; Fri,
 25 Jul 2014 11:27:56 -0700 (PDT)
Received: by 10.140.39.239 with HTTP; Fri, 25 Jul 2014 11:27:56 -0700 (PDT)
Date: Fri, 25 Jul 2014 14:27:56 -0400
Message-ID: <CAGOvqip4_UQzXX4xxK7bB=Gv=mgjGZqHm_y1E19GnVvh1PAw0Q@mail.gmail.com>
Subject: Kryo Issue on Spark 1.0.1, Mesos 0.18.2
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c24cfc7678e904ff08bb74
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24cfc7678e904ff08bb74
Content-Type: text/plain; charset=UTF-8

After upgrading to Spark 1.0.1 from 0.9.1 everything seemed to be going
well.  Looking at the Mesos slave logs, I noticed:

ERROR KryoSerializer: Failed to run spark.kryo.registrator
java.lang.ClassNotFoundException:
com/mediacrossing/verrazano/kryo/MxDataRegistrator

My spark-env.sh has the following when I run the Spark Shell:

export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so

export MASTER=mesos://zk://n-01:2181,n-02:2181,n-03:2181/masters

export ADD_JARS=/opt/spark/mx-lib/verrazano-assembly.jar


# -XX:+UseCompressedOops must be disabled to use more than 32GB RAM

SPARK_JAVA_OPTS="-Xss2m -XX:+UseCompressedOops
-Dspark.local.dir=/opt/mesos-tmp -Dspark.executor.memory=4g
 -Dspark.serializer=org.apache.spark.serializer.KryoSerializer
-Dspark.kryo.registrator=com.mediacrossing.verrazano.kryo.MxDataRegistrator
-Dspark.kryoserializer.buffer.mb=16 -Dspark.akka.askTimeout=30"


I was able to verify that our custom jar was being copied to each worker,
but for some reason it is not finding my registrator class.  Is anyone else
struggling with Kryo on 1.0.x branch?

--001a11c24cfc7678e904ff08bb74--

From dev-return-8531-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 19:50:29 2014
Return-Path: <dev-return-8531-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 32C5310B6F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 19:50:29 +0000 (UTC)
Received: (qmail 42223 invoked by uid 500); 25 Jul 2014 19:50:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42159 invoked by uid 500); 25 Jul 2014 19:50:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 42142 invoked by uid 99); 25 Jul 2014 19:50:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 19:50:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of ankurdave@gmail.com designates 209.85.216.47 as permitted sender)
Received: from [209.85.216.47] (HELO mail-qa0-f47.google.com) (209.85.216.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 19:50:26 +0000
Received: by mail-qa0-f47.google.com with SMTP id i13so5036584qae.6
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 12:50:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=smZ5ShoyqViJSoI/Y47ydcfYCqV1AkRZkZ15zx1ifGo=;
        b=kWyouDoO+yCO8tcbBTiU5/tRK3URZg+jkWSixX/Hi8aAFkcgJ5HQmB0yhS33KJX+nc
         rn2LVPbnqtY/WPGPZU726aZ8gMZ5Jmk2zZWgch2BoTsjLBglmAWJLaXmnk0+R47VKbUK
         gaaSojEGCLXESc1b9SGPQ0ihlb8G+abToa5BEOaR3ccnVbFolQ85Ubrfloz4jWyzgqZz
         JoeicA1HlJ32skCqr2WuhRZDZaYJQxUvhiBud3vOCRg2ioQuLd0r8tvASy4U1/xJUbIX
         dRlheFItC467d3VLojSVxy/DZG17O2h5AUhhD34gqQgLo2sCBf52407N22S/0RlQVYXH
         bdDA==
X-Received: by 10.140.85.101 with SMTP id m92mr31049142qgd.26.1406317801257;
 Fri, 25 Jul 2014 12:50:01 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.37.132 with HTTP; Fri, 25 Jul 2014 12:49:41 -0700 (PDT)
In-Reply-To: <53D0AECE.2070608@sjtu.edu.cn>
References: <53D0AECE.2070608@sjtu.edu.cn>
From: Ankur Dave <ankurdave@gmail.com>
Date: Fri, 25 Jul 2014 12:49:41 -0700
Message-ID: <CAK1A71wP__Gvr+2sLDHgRXWUzVpB63BTvKHJZGqSBbPkiL6FFA@mail.gmail.com>
Subject: Re: GraphX graph partitioning strategy
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c155eafcdab104ff09e0fa
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c155eafcdab104ff09e0fa
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Larry,

GraphX's graph constructor leaves the edges in their original partitions by
default. To support arbitrary multipass graph partitioning, one idea is to
take advantage of that by partitioning the graph externally to GraphX
(though possibly using information from GraphX such as the degrees), then
pass the partitioned edges to GraphX.

For example, if you had an edge partitioning function that needed the full
triplet to assign a partition, you could do this as follows:

val unpartitionedGraph: Graph[Int, Int] =3D ...val numPartitions: Int =3D 1=
28
def getTripletPartition(e: EdgeTriplet[Int, Int]): PartitionID =3D ...
// Get the triplets using GraphX, then use Spark to repartition
themval partitionedEdges =3D unpartitionedGraph.triplets
  .map(e =3D> (getTripletPartition(e), e))
  .partitionBy(new HashPartitioner(numPartitions))
val partitionedGraph =3D Graph(unpartitionedGraph.vertices, partitionedEdge=
s)


A multipass partitioning algorithm could store its results in the edge
attribute, and then you could use the code above to do the partitioning.

Ankur <http://www.ankurdave.com/>


On Wed, Jul 23, 2014 at 11:59 PM, Larry Xiao <xiaodi@sjtu.edu.cn> wrote:

> Hi all,
>
> I'm implementing graph partitioning strategy for GraphX, learning from
> researches on graph computing.
>
> I have two questions:
>
> - a specific implement question:
> In current design, only vertex ID of src and dst are provided
> (PartitionStrategy.scala).
> And some strategies require knowledge about the graph (like degrees) and
> can consist more than one passes to finally produce the partition ID.
> So I'm changing the PartitionStrategy.getPartition API to provide more
> info, but I don't want to make it complex. (the current one looks very
> clean)
>
> - an open question:
> What advice would you give considering partitioning, considering the
> procedure Spark adopt on graph processing?
>
> Any advice is much appreciated.
>
> Best Regards,
> Larry Xiao
>
> Reference
>
> Bipartite-oriented Distributed Graph Partitioning for Big Learning.
> PowerLyra=E2=80=AF: Differentiated Graph Computation and Partitioning on =
Skewed
> Graphs
>

--001a11c155eafcdab104ff09e0fa--

From dev-return-8532-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 20:03:55 2014
Return-Path: <dev-return-8532-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 75FC310C1E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 20:03:55 +0000 (UTC)
Received: (qmail 74207 invoked by uid 500); 25 Jul 2014 20:03:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74140 invoked by uid 500); 25 Jul 2014 20:03:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 74126 invoked by uid 99); 25 Jul 2014 20:03:54 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 20:03:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ankurdave@gmail.com designates 209.85.192.46 as permitted sender)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 20:03:50 +0000
Received: by mail-qg0-f46.google.com with SMTP id z60so5670302qgd.19
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 13:03:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=opYkTEBa/oIEkfwmjPd4ZunxLl/CKBYipC+8KVFOE/Q=;
        b=krl5jepXtatzheTwqeX/hSD43zMJKYXceD6B6leDtzYPax3b8x/dd5eqwPIleeQblD
         GArdL8eIftKoadpa9/3om6qkkGT+yb1Lv0BrA0M6sb4fD/whOhj2ZixF0nYTmT5zmeXg
         Fw1sQYpIda7vgl/SmPbBYVMTxvKOJbIX8euAmqN7HLQx7wWngBpXg9vKHmVY6BAga/Z3
         moA4L6jABKD4JMU4RPq5C2EBvYOvb9r1OvaqaFmeyKe+czlyFvu563QdUHwAjRtWpn7z
         O//DjVwYfmQKUBp8uQkYivNDzQWmbsU+RGbbZoDkS9lj9HkygqEUszKRzc+YZf5TGJEq
         8xHA==
X-Received: by 10.224.55.202 with SMTP id v10mr31749866qag.10.1406318609466;
 Fri, 25 Jul 2014 13:03:29 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.37.132 with HTTP; Fri, 25 Jul 2014 13:03:09 -0700 (PDT)
In-Reply-To: <CAK1A71wP__Gvr+2sLDHgRXWUzVpB63BTvKHJZGqSBbPkiL6FFA@mail.gmail.com>
References: <53D0AECE.2070608@sjtu.edu.cn> <CAK1A71wP__Gvr+2sLDHgRXWUzVpB63BTvKHJZGqSBbPkiL6FFA@mail.gmail.com>
From: Ankur Dave <ankurdave@gmail.com>
Date: Fri, 25 Jul 2014 13:03:09 -0700
Message-ID: <CAK1A71wZGN-vSfRLpxpB7-NjchBqVhYMB+OKSyhVWAs+=ATr7g@mail.gmail.com>
Subject: Re: GraphX graph partitioning strategy
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3029a288dde04ff0a1187
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3029a288dde04ff0a1187
Content-Type: text/plain; charset=UTF-8

Oops, the code should be:

val unpartitionedGraph: Graph[Int, Int] = ...val numPartitions: Int = 128
def getTripletPartition(e: EdgeTriplet[Int, Int]): PartitionID = ...
// Get the triplets using GraphX, then use Spark to repartition
themval partitionedEdges = unpartitionedGraph.triplets
  .map(e => (getTripletPartition(e), e))
  .partitionBy(new HashPartitioner(numPartitions))
  *.map(pair => Edge(pair._2.srcId, pair._2.dstId, pair._2.attr))*
val partitionedGraph = Graph(unpartitionedGraph.vertices, partitionedEdges)


Ankur <http://www.ankurdave.com/>

--001a11c3029a288dde04ff0a1187--

From dev-return-8533-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 22:10:21 2014
Return-Path: <dev-return-8533-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0A4EC110DA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 22:10:21 +0000 (UTC)
Received: (qmail 89785 invoked by uid 500); 25 Jul 2014 22:10:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89721 invoked by uid 500); 25 Jul 2014 22:10:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89701 invoked by uid 99); 25 Jul 2014 22:10:20 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 22:10:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rarecactus@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 22:10:17 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so4831189wgg.12
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 15:09:53 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=F1LRhjnEuXqr0YpejE7EZZegRs1CMf6YLK/p9T+DFOc=;
        b=RlksE7ygkb6GOpe1mz1zbUUmTqWPK+66Hgipgvu2jFFXfhlNN6jpLURintVyReOimO
         4tqFZsO+5joZ17h9t8m8Ivga56bmIywb1l7IFG1t5fkpf+MibhC64JbhmbGmsSU7XVrV
         AG+xobFlNJW8tciQim0iL5qf/fWJ8cblWki+b6uGDcSUsQJAgQ8J45iI9GruocS0cLKD
         k1w7r6QKoX6KN0HtijXzS9Awttn9oOJpCufl7dIjX2PQLvw28E0wPuhh0ySGgaB4Dm4Y
         G0V1Dv1LWZWaiiXQdxEzQoAP25SpLDbn1lBMQDpTZvYN2GqXC+Sxbriv7xUo8aPaSvhZ
         fkIA==
MIME-Version: 1.0
X-Received: by 10.180.35.134 with SMTP id h6mr8671463wij.0.1406326193328; Fri,
 25 Jul 2014 15:09:53 -0700 (PDT)
Sender: rarecactus@gmail.com
Received: by 10.194.6.5 with HTTP; Fri, 25 Jul 2014 15:09:53 -0700 (PDT)
In-Reply-To: <CA+qbEUPc8ggp0PGz14inYP2yi=OQbQxGTewxdP_OJTwe4An2Zg@mail.gmail.com>
References: <000301cfa581$2b119310$8134b930$@innowireless.co.kr>
	<CA+qbEUPc8ggp0PGz14inYP2yi=OQbQxGTewxdP_OJTwe4An2Zg@mail.gmail.com>
Date: Fri, 25 Jul 2014 15:09:53 -0700
X-Google-Sender-Auth: F7DT0cP7zM9vYuSUuE4wOU7MjTI
Message-ID: <CA+qbEUPYJ6+4uuq01OJPwnkBFrNff=fHmukQ=WOtfmgP-KwAVw@mail.gmail.com>
Subject: Re: Suggestion for SPARK-1825
From: Colin McCabe <cmccabe@alumni.cmu.edu>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8f83a44331180d04ff0bd53e
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8f83a44331180d04ff0bd53e
Content-Type: text/plain; charset=UTF-8

So, I'm leaning more towards using reflection for this.  Maven profiles
could work, but it's tough since we have new stuff coming in in 2.4, 2.5,
etc.  and the number of profiles will multiply quickly if we have to do it
that way.  Reflection is the approach HBase took in a similar situation.

best,
Colin


On Fri, Jul 25, 2014 at 11:23 AM, Colin McCabe <cmccabe@alumni.cmu.edu>
wrote:

> I have a similar issue with SPARK-1767.  There are basically three ways to
> resolve the issue:
>
> 1. Use reflection to access classes newer than 0.21 (or whatever the
> oldest version of Hadoop is that Spark supports)
> 2. Add a build variant (in Maven this would be a profile) that deals with
> this.
> 3. Auto-detect which classes are available and use those.
>
> #1 is the easiest for end-users, but it can lead to some ugly code.
>
> #2 makes the code look nicer, but requires some effort on the part of
> people building spark.  This can also lead to headaches for IDEs, if people
> don't remember to select the new profile.  (For example, in IntelliJ, you
> can't see any of the yarn classes when you import the project from Maven
> without the YARN profile selected.)
>
> #3 is something that... I don't know how to do in sbt or Maven.  I've been
> told that an antrun task might work here, but it seems like it could get
> really tricky.
>
> Overall, I'd lean more towards #2 here.
>
> best,
> Colin
>
>
> On Tue, Jul 22, 2014 at 12:47 AM, innowireless TaeYun Kim <
> taeyun.kim@innowireless.co.kr> wrote:
>
>> (I'm resending this mail since it seems that it was not sent. Sorry if
>> this
>> was already sent.)
>>
>> Hi,
>>
>>
>>
>> A couple of month ago, I made a pull request to fix
>> https://issues.apache.org/jira/browse/SPARK-1825.
>>
>> My pull request is here: https://github.com/apache/spark/pull/899
>>
>>
>>
>> But that pull request has problems:
>>
>> l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.
>>
>> l  The related Hadoop API is marked as '@Unstable'.
>>
>>
>>
>> Here is an idea to remedy the problems: a new Spark configuration
>> variable.
>>
>> Maybe it can be named as "spark.yarn.submit.crossplatform".
>>
>> If it is set to "true"(default is false), the related Spark code can use
>> the
>> hard-coded strings that is the same as the Hadoop API provides, thus
>> avoiding compile error on the Hadoop versions below 2.4.0.
>>
>>
>>
>> Can someone implement this feature, if this idea is acceptable?
>>
>> Currently my knowledge on Spark source code and Scala is limited to
>> implement it myself.
>>
>> To the right person, the modification should be trivial.
>>
>> You can refer to the source code changes of my pull request.
>>
>>
>>
>> Thanks.
>>
>>
>>
>>
>

--e89a8f83a44331180d04ff0bd53e--

From dev-return-8534-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 22:36:04 2014
Return-Path: <dev-return-8534-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 59EEF111EC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 22:36:04 +0000 (UTC)
Received: (qmail 61150 invoked by uid 500); 25 Jul 2014 22:36:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61093 invoked by uid 500); 25 Jul 2014 22:36:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61082 invoked by uid 99); 25 Jul 2014 22:36:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 22:36:03 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 22:35:59 +0000
Received: by mail-qa0-f41.google.com with SMTP id j7so5196432qaq.0
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 15:35:38 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=iQmjgSHujhukNbwlgtWLM8+Lr28Jqt/Ej4xZ8reIcac=;
        b=kHi7dTdeuiLJXDHidIzNvsGNVM/xbP+Pc5JkN8Ns2KBkU83s21fpXzPookcdtAgiMj
         4BAnjTkM4DJdXYi7yosDpT1II4/Dy2dZhX8+voXBBdY7WbvWPL3Nlfom0C1jdWBZuQBP
         2BAaBO14BA1s7Xj+J8bD4WPXo8Y+SG9oNPKkWWFUy2JPSy9yP2kZkjJ3xFe2aodAvY9s
         ND2UABLXPGORKt65EKtQTFis4xM2SIRGIbIwQyjEUAes43pkAjQLUn6LhPYlaXH9xPDr
         J28CP/8QB/GxilkK8Vyp83ate+FLFLWzAYazgdrOdMSHRGfRAmuEVuutrQf0bf6YFqcI
         W0SQ==
X-Gm-Message-State: ALoCoQmkWa6GwtLICETmHDBM8tK0FvA2sdcq0bXTv96AITJ5VKU78qBZBgp0qkWnr2IavMeXgJCP
X-Received: by 10.224.43.196 with SMTP id x4mr29786816qae.63.1406327738798;
 Fri, 25 Jul 2014 15:35:38 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Fri, 25 Jul 2014 15:35:18 -0700 (PDT)
In-Reply-To: <CA+qbEUPYJ6+4uuq01OJPwnkBFrNff=fHmukQ=WOtfmgP-KwAVw@mail.gmail.com>
References: <000301cfa581$2b119310$8134b930$@innowireless.co.kr>
 <CA+qbEUPc8ggp0PGz14inYP2yi=OQbQxGTewxdP_OJTwe4An2Zg@mail.gmail.com> <CA+qbEUPYJ6+4uuq01OJPwnkBFrNff=fHmukQ=WOtfmgP-KwAVw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Fri, 25 Jul 2014 15:35:18 -0700
Message-ID: <CAPh_B=ZooXKHM8tP+4k193wtsuvbin-e74wyMmXkT3+2PjYjvw@mail.gmail.com>
Subject: Re: Suggestion for SPARK-1825
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7bdc875e4f28e504ff0c31f3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc875e4f28e504ff0c31f3
Content-Type: text/plain; charset=UTF-8

Actually reflection is probably a better, lighter weight process for this.
An extra project brings more overhead for something simple.





On Fri, Jul 25, 2014 at 3:09 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
wrote:

> So, I'm leaning more towards using reflection for this.  Maven profiles
> could work, but it's tough since we have new stuff coming in in 2.4, 2.5,
> etc.  and the number of profiles will multiply quickly if we have to do it
> that way.  Reflection is the approach HBase took in a similar situation.
>
> best,
> Colin
>
>
> On Fri, Jul 25, 2014 at 11:23 AM, Colin McCabe <cmccabe@alumni.cmu.edu>
> wrote:
>
> > I have a similar issue with SPARK-1767.  There are basically three ways
> to
> > resolve the issue:
> >
> > 1. Use reflection to access classes newer than 0.21 (or whatever the
> > oldest version of Hadoop is that Spark supports)
> > 2. Add a build variant (in Maven this would be a profile) that deals with
> > this.
> > 3. Auto-detect which classes are available and use those.
> >
> > #1 is the easiest for end-users, but it can lead to some ugly code.
> >
> > #2 makes the code look nicer, but requires some effort on the part of
> > people building spark.  This can also lead to headaches for IDEs, if
> people
> > don't remember to select the new profile.  (For example, in IntelliJ, you
> > can't see any of the yarn classes when you import the project from Maven
> > without the YARN profile selected.)
> >
> > #3 is something that... I don't know how to do in sbt or Maven.  I've
> been
> > told that an antrun task might work here, but it seems like it could get
> > really tricky.
> >
> > Overall, I'd lean more towards #2 here.
> >
> > best,
> > Colin
> >
> >
> > On Tue, Jul 22, 2014 at 12:47 AM, innowireless TaeYun Kim <
> > taeyun.kim@innowireless.co.kr> wrote:
> >
> >> (I'm resending this mail since it seems that it was not sent. Sorry if
> >> this
> >> was already sent.)
> >>
> >> Hi,
> >>
> >>
> >>
> >> A couple of month ago, I made a pull request to fix
> >> https://issues.apache.org/jira/browse/SPARK-1825.
> >>
> >> My pull request is here: https://github.com/apache/spark/pull/899
> >>
> >>
> >>
> >> But that pull request has problems:
> >>
> >> l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.
> >>
> >> l  The related Hadoop API is marked as '@Unstable'.
> >>
> >>
> >>
> >> Here is an idea to remedy the problems: a new Spark configuration
> >> variable.
> >>
> >> Maybe it can be named as "spark.yarn.submit.crossplatform".
> >>
> >> If it is set to "true"(default is false), the related Spark code can use
> >> the
> >> hard-coded strings that is the same as the Hadoop API provides, thus
> >> avoiding compile error on the Hadoop versions below 2.4.0.
> >>
> >>
> >>
> >> Can someone implement this feature, if this idea is acceptable?
> >>
> >> Currently my knowledge on Spark source code and Scala is limited to
> >> implement it myself.
> >>
> >> To the right person, the modification should be trivial.
> >>
> >> You can refer to the source code changes of my pull request.
> >>
> >>
> >>
> >> Thanks.
> >>
> >>
> >>
> >>
> >
>

--047d7bdc875e4f28e504ff0c31f3--

From dev-return-8535-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 23:09:49 2014
Return-Path: <dev-return-8535-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A2C82112D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 23:09:49 +0000 (UTC)
Received: (qmail 48791 invoked by uid 500); 25 Jul 2014 23:09:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48731 invoked by uid 500); 25 Jul 2014 23:09:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48705 invoked by uid 99); 25 Jul 2014 23:09:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 23:09:48 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 23:09:45 +0000
Received: by mail-ig0-f180.google.com with SMTP id l13so1468123iga.7
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 16:09:20 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:from:date:message-id:subject:to:content-type;
        bh=JFh7subic2eNp7IX1ebuSe7VWm87ExGB3RqDw6flMaw=;
        b=MnTb+GdnoFtkouqP33RcngW+7mAXOXlCLgQo67c7a2kGQYL6QQk+Ylzv4QIYHyc5zQ
         xmjbfG7QeuPGLLT7JvyzbgymJG5Fz/7E2G0vGwT/4Ni51ehXIXwI1gCCs8usLxIgycZe
         Tr/6q+Nv1uYz6rNcaAZB/3eXIxjM1Bje6tKUhHQaGSO46EuA5YYJZK0q1X4DMMIp+AYt
         /fDVwcQCJArsa1UgYOhqA1MqY0/10YRtXdd4nhJA2Vq1up051pBgOKH0Xy/H7rI4s8ek
         b7/5uj1kN6G9Al/pn26+LFmAhp2WDJyoqfFjubH6rlK4IhlWbD+XPXb8g6fhgiohAMFQ
         6jbQ==
X-Received: by 10.42.15.19 with SMTP id j19mr23674538ica.59.1406329760354;
 Fri, 25 Jul 2014 16:09:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.129.10 with HTTP; Fri, 25 Jul 2014 16:08:50 -0700 (PDT)
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Fri, 25 Jul 2014 16:08:50 -0700
Message-ID: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
Subject: [VOTE] Release Apache Spark 1.0.2 (RC1)
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Please vote on releasing the following candidate as Apache Spark version 1.0.2.

This release fixes a number of bugs in Spark 1.0.1.
Some of the notable ones are
- SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
SPARK-1199. The fix was reverted for 1.0.2.
- SPARK-2576: NoClassDefFoundError when executing Spark QL query on
HDFS CSV file.
The full list is at http://s.apache.org/9NJ

The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f

The release files, including signatures, digests, etc can be found at:
http://people.apache.org/~tdas/spark-1.0.2-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/tdas.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1024/

The documentation corresponding to this release can be found at:
http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/

Please vote on releasing this package as Apache Spark 1.0.2!

The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
a majority of at least 3 +1 PMC votes are cast.
[ ] +1 Release this package as Apache Spark 1.0.2
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see
http://spark.apache.org/

From dev-return-8536-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Fri Jul 25 23:24:16 2014
Return-Path: <dev-return-8536-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2BD15112EF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 25 Jul 2014 23:24:16 +0000 (UTC)
Received: (qmail 65632 invoked by uid 500); 25 Jul 2014 23:24:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65582 invoked by uid 500); 25 Jul 2014 23:24:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65570 invoked by uid 99); 25 Jul 2014 23:24:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 23:24:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.173 as permitted sender)
Received: from [74.125.82.173] (HELO mail-we0-f173.google.com) (74.125.82.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 25 Jul 2014 23:24:11 +0000
Received: by mail-we0-f173.google.com with SMTP id q58so4991535wes.32
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 16:23:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=sjZrRViKjgOw8ZHDU9lEgj6WDQCYOQPhDSNwsPkDXJg=;
        b=QpeetqoLdLv+M1nQk2QQTZGTAzbungNV3hZHcYSrt6a7oUjmPTnDb01GYVbY6GVJr5
         PCtWqDjM6RbNKlNQCkpL9PcqXxVV+ULSotPWusKtIDo1u4pYaKNd5gy1ianq2pMxSOsx
         9AawH0n7yyO6kLlpvkMzrHn8W0e6+EDFqAfvair93VYogMmeY2ZVECJ6OnfKFdzxQL2L
         wpHjnbNWeh+BMAT15wmSS3+Rk6hKe+BWgeqXk0pq06BmjFiBFyt71SdZ5iXIJx0nPniS
         uybhVKsJh28UpyvWgusZy7fd8CgdYwDtpYaxkj3YVFkC1/XaQStMYM25Crc3MJxckti/
         YDbQ==
X-Received: by 10.180.24.97 with SMTP id t1mr9336365wif.45.1406330627228; Fri,
 25 Jul 2014 16:23:47 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Fri, 25 Jul 2014 16:23:07 -0700 (PDT)
In-Reply-To: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Fri, 25 Jul 2014 19:23:07 -0400
Message-ID: <CAOhmDzd7HP0V4LB7owB1yt1gu+QokNC3=kFyZ49eRyQjw4+B=Q@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043c8062790ab804ff0cdd94
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c8062790ab804ff0cdd94
Content-Type: text/plain; charset=UTF-8

TD, there are a couple of unresolved issues slated for 1.0.2
<https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC>.
Should they be edited somehow?


On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <tathagata.das1565@gmail.com>
wrote:

> Please vote on releasing the following candidate as Apache Spark version
> 1.0.2.
>
> This release fixes a number of bugs in Spark 1.0.1.
> Some of the notable ones are
> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> SPARK-1199. The fix was reverted for 1.0.2.
> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> HDFS CSV file.
> The full list is at http://s.apache.org/9NJ
>
> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>
> The release files, including signatures, digests, etc can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1024/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.2!
>
> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> [ ] +1 Release this package as Apache Spark 1.0.2
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/
>

--f46d043c8062790ab804ff0cdd94--

From dev-return-8537-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 00:36:11 2014
Return-Path: <dev-return-8537-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CD1F811444
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 00:36:11 +0000 (UTC)
Received: (qmail 61799 invoked by uid 500); 26 Jul 2014 00:36:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61751 invoked by uid 500); 26 Jul 2014 00:36:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61740 invoked by uid 99); 26 Jul 2014 00:36:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 00:36:10 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 00:36:05 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so5257492qaq.24
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 17:35:44 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=ezUCvYBn5UmvymMq7XkHsJE+xaFDErXKCovdMyGK+0o=;
        b=kl4W0Wj5nQ1R09cPVad7j/egVhyTfY4PeYbm7tLLJMBjnCJ107fxRBITkSM2xeboi1
         BNKNzp/sHIefjBd/esxwroljCjhowMe5wv0QRhs0e/DwLTjTauGlw7rYmz2x4Z7/MUm1
         JIcxwYKSIWaUTkmdhAB0bY5UitHk2o1jNZ0aR+ExQFynAYJsbcH+7nqo1PNOSsinAYZX
         GxZfIpPU9ZGckzDeVK3acwE2VDsA+TlGj0uTaH/13QUjlDYuYlJxHpY3+hsayCbQeEfv
         ie9pKfr9+k83GZcwXbeU1Otc5smuek1F5o/LWk9Lkh98gD5Gn9uwD2Z0l5fJt2UEEyFU
         zDVQ==
X-Gm-Message-State: ALoCoQnPMER+R1pfkuZZ6HGimBzAirI0soeQnJKu2se1n9Q7Gv34+ByWwxYb83+H5EOvQDjyXkl1
X-Received: by 10.224.80.129 with SMTP id t1mr33227304qak.102.1406334944380;
 Fri, 25 Jul 2014 17:35:44 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.206.9 with HTTP; Fri, 25 Jul 2014 17:35:24 -0700 (PDT)
In-Reply-To: <CAOhmDzd7HP0V4LB7owB1yt1gu+QokNC3=kFyZ49eRyQjw4+B=Q@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
 <CAOhmDzd7HP0V4LB7owB1yt1gu+QokNC3=kFyZ49eRyQjw4+B=Q@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Fri, 25 Jul 2014 17:35:24 -0700
Message-ID: <CAAswR-5swpxuukMZOH6wjwRWiteH7oWbrztAbzb6ma8g9HOsaA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c3c334cbebeb04ff0dde0d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3c334cbebeb04ff0dde0d
Content-Type: text/plain; charset=UTF-8

That query is looking at "Fix Version" not "Target Version".  The fact that
the first one is still open is only because the bug is not resolved in
master.  It is fixed in 1.0.2.  The second one is partially fixed in 1.0.2,
but is not worth blocking the release for.


On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> TD, there are a couple of unresolved issues slated for 1.0.2
> <
> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
> >.
> Should they be edited somehow?
>
>
> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
> tathagata.das1565@gmail.com>
> wrote:
>
> > Please vote on releasing the following candidate as Apache Spark version
> > 1.0.2.
> >
> > This release fixes a number of bugs in Spark 1.0.1.
> > Some of the notable ones are
> > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> > SPARK-1199. The fix was reverted for 1.0.2.
> > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> > HDFS CSV file.
> > The full list is at http://s.apache.org/9NJ
> >
> > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> >
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
> >
> > The release files, including signatures, digests, etc can be found at:
> > http://people.apache.org/~tdas/spark-1.0.2-rc1/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/tdas.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1024/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.2!
> >
> > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> > a majority of at least 3 +1 PMC votes are cast.
> > [ ] +1 Release this package as Apache Spark 1.0.2
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
> >
>

--001a11c3c334cbebeb04ff0dde0d--

From dev-return-8538-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 02:00:42 2014
Return-Path: <dev-return-8538-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 119C911606
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 02:00:42 +0000 (UTC)
Received: (qmail 80847 invoked by uid 500); 26 Jul 2014 02:00:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80782 invoked by uid 500); 26 Jul 2014 02:00:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 80738 invoked by uid 99); 26 Jul 2014 02:00:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 02:00:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 02:00:36 +0000
Received: by mail-oa0-f45.google.com with SMTP id i7so6486548oag.18
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 19:00:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TieFHPWPpvposNKO6S3eguXVy4Q4BMK3zDLdLBWIy4c=;
        b=Depj9zduQuidRZIt4b3UfL7undoA2gY73kFzRlpM67GXQyRPkOptoqRGzC7ABjFqsq
         5KXM7OA5McZicZ4/ZMpZ1/Ig8LrbgpnDVXyCrQ7q9tAhxeIKngRftlSNmIgwlbyx1sfL
         2ZdYNMhXIz+FTeenTQ2lRyPAau+ReJlLpU81gGT3u+qXcFcho/amIMjC4MsI/tO38M74
         HOTRSPLL9KfQnY06nCn/oh5w4hNPnv/94d6dlW4uCqWVofhDQj47NVfRUu/JkXu1VVbe
         4o6tqoHiB9Xk6wPRUL33nKvSjApRXN3rN8Sf71PL4XUnWmLU/VMQiTvxmz8KQ4P26KU9
         Ffjw==
MIME-Version: 1.0
X-Received: by 10.60.62.197 with SMTP id a5mr14326956oes.78.1406340016105;
 Fri, 25 Jul 2014 19:00:16 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 25 Jul 2014 19:00:16 -0700 (PDT)
In-Reply-To: <CAAswR-5swpxuukMZOH6wjwRWiteH7oWbrztAbzb6ma8g9HOsaA@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<CAOhmDzd7HP0V4LB7owB1yt1gu+QokNC3=kFyZ49eRyQjw4+B=Q@mail.gmail.com>
	<CAAswR-5swpxuukMZOH6wjwRWiteH7oWbrztAbzb6ma8g9HOsaA@mail.gmail.com>
Date: Fri, 25 Jul 2014 19:00:16 -0700
Message-ID: <CABPQxsusN8ygHHPKNsOVxmg=DG7SnqO5eJ-szwmFMMxEq6kfgQ@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

The most important issue in this release is actually an ammendment to
an earlier fix. The original fix caused a deadlock which was a
regression from 1.0.0->1.0.1:

Issue:
https://issues.apache.org/jira/browse/SPARK-1097

1.0.1 Fix:
https://github.com/apache/spark/pull/1273/files (had a deadlock)

1.0.2 Fix:
https://github.com/apache/spark/pull/1409/files

I failed to correctly label this on JIRA, but I've updated it!

On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
<michael@databricks.com> wrote:
> That query is looking at "Fix Version" not "Target Version".  The fact that
> the first one is still open is only because the bug is not resolved in
> master.  It is fixed in 1.0.2.  The second one is partially fixed in 1.0.2,
> but is not worth blocking the release for.
>
>
> On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com> wrote:
>
>> TD, there are a couple of unresolved issues slated for 1.0.2
>> <
>> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
>> >.
>> Should they be edited somehow?
>>
>>
>> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
>> tathagata.das1565@gmail.com>
>> wrote:
>>
>> > Please vote on releasing the following candidate as Apache Spark version
>> > 1.0.2.
>> >
>> > This release fixes a number of bugs in Spark 1.0.1.
>> > Some of the notable ones are
>> > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
>> > SPARK-1199. The fix was reverted for 1.0.2.
>> > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
>> > HDFS CSV file.
>> > The full list is at http://s.apache.org/9NJ
>> >
>> > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>> >
>> >
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>> >
>> > The release files, including signatures, digests, etc can be found at:
>> > http://people.apache.org/~tdas/spark-1.0.2-rc1/
>> >
>> > Release artifacts are signed with the following key:
>> > https://people.apache.org/keys/committer/tdas.asc
>> >
>> > The staging repository for this release can be found at:
>> > https://repository.apache.org/content/repositories/orgapachespark-1024/
>> >
>> > The documentation corresponding to this release can be found at:
>> > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>> >
>> > Please vote on releasing this package as Apache Spark 1.0.2!
>> >
>> > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
>> > a majority of at least 3 +1 PMC votes are cast.
>> > [ ] +1 Release this package as Apache Spark 1.0.2
>> > [ ] -1 Do not release this package because ...
>> >
>> > To learn more about Apache Spark, please see
>> > http://spark.apache.org/
>> >
>>

From dev-return-8539-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 02:02:26 2014
Return-Path: <dev-return-8539-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DEE111611
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 02:02:26 +0000 (UTC)
Received: (qmail 83600 invoked by uid 500); 26 Jul 2014 02:02:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83524 invoked by uid 500); 26 Jul 2014 02:02:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83512 invoked by uid 99); 26 Jul 2014 02:02:25 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 02:02:25 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 02:02:21 +0000
Received: by mail-wg0-f45.google.com with SMTP id x12so5064033wgg.16
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 19:02:00 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Qupfz3Wx+ijnVN0FkrYmrzVtt2qfnK7r73LsHGKnq3Q=;
        b=a2yQyYG29Qql2srf5yB/mrPIiEfjX3wgrSbvmH5yBoyI2op6851RyzwiIWW35UIhYj
         OkM1AmPZs43U4J6zIZnLxmJC/J0UbEX9BoOIDp2MHWLaTjyKXEsAQcsjE2QU2B9FGbt6
         P5IgMIULVvM4mPVaUpPCmFWKnSufosHIO0TsLKtB/jwAHOclOLkm+/di8h7JtXxHLo7V
         15WBRJK2z1OQ+Q8xb81ahli78L3nRDWVV9oiIy+zTlVexffFcRSDh4vpa7y2eVGLUhlJ
         wBejCH+Nt14ryT6k9Mmh67RlCs7kPHhUqWqBpSL5oR7lHqOHC0pwd5sh0aHBQnn+n4Wc
         jPkQ==
MIME-Version: 1.0
X-Received: by 10.194.120.129 with SMTP id lc1mr26891458wjb.16.1406340119868;
 Fri, 25 Jul 2014 19:01:59 -0700 (PDT)
Received: by 10.180.92.232 with HTTP; Fri, 25 Jul 2014 19:01:59 -0700 (PDT)
In-Reply-To: <CAAswR-5swpxuukMZOH6wjwRWiteH7oWbrztAbzb6ma8g9HOsaA@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<CAOhmDzd7HP0V4LB7owB1yt1gu+QokNC3=kFyZ49eRyQjw4+B=Q@mail.gmail.com>
	<CAAswR-5swpxuukMZOH6wjwRWiteH7oWbrztAbzb6ma8g9HOsaA@mail.gmail.com>
Date: Fri, 25 Jul 2014 22:01:59 -0400
Message-ID: <CAOhmDzc=gH62OZrTX+DDCw5QHJQUYZyA7az15-2Jns-Z-x+Mdg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Nicholas Chammas <nicholas.chammas@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e01227b12473dc504ff0f13ab
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01227b12473dc504ff0f13ab
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

OK, thanks for the clarification.

2014=EB=85=84 7=EC=9B=94 25=EC=9D=BC =EA=B8=88=EC=9A=94=EC=9D=BC, Michael A=
rmbrust<michael@databricks.com>=EB=8B=98=EC=9D=B4 =EC=9E=91=EC=84=B1=ED=95=
=9C =EB=A9=94=EC=8B=9C=EC=A7=80:

> That query is looking at "Fix Version" not "Target Version".  The fact th=
at
> the first one is still open is only because the bug is not resolved in
> master.  It is fixed in 1.0.2.  The second one is partially fixed in 1.0.=
2,
> but is not worth blocking the release for.
>
>
> On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
> nicholas.chammas@gmail.com <javascript:;>> wrote:
>
> > TD, there are a couple of unresolved issues slated for 1.0.2
> > <
> >
> https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%=
20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20=
BY%20priority%20DESC
> > >.
> > Should they be edited somehow?
> >
> >
> > On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
> > tathagata.das1565@gmail.com <javascript:;>>
> > wrote:
> >
> > > Please vote on releasing the following candidate as Apache Spark
> version
> > > 1.0.2.
> > >
> > > This release fixes a number of bugs in Spark 1.0.1.
> > > Some of the notable ones are
> > > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> > > SPARK-1199. The fix was reverted for 1.0.2.
> > > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> > > HDFS CSV file.
> > > The full list is at http://s.apache.org/9NJ
> > >
> > > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> > >
> > >
> >
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D8fb6=
f00e195fb258f3f70f04756e07c259a2351f
> > >
> > > The release files, including signatures, digests, etc can be found at=
:
> > > http://people.apache.org/~tdas/spark-1.0.2-rc1/
> > >
> > > Release artifacts are signed with the following key:
> > > https://people.apache.org/keys/committer/tdas.asc
> > >
> > > The staging repository for this release can be found at:
> > >
> https://repository.apache.org/content/repositories/orgapachespark-1024/
> > >
> > > The documentation corresponding to this release can be found at:
> > > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> > >
> > > Please vote on releasing this package as Apache Spark 1.0.2!
> > >
> > > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> > > a majority of at least 3 +1 PMC votes are cast.
> > > [ ] +1 Release this package as Apache Spark 1.0.2
> > > [ ] -1 Do not release this package because ...
> > >
> > > To learn more about Apache Spark, please see
> > > http://spark.apache.org/
> > >
> >
>

--089e01227b12473dc504ff0f13ab--

From dev-return-8540-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 02:04:12 2014
Return-Path: <dev-return-8540-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DF1F1161F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 02:04:12 +0000 (UTC)
Received: (qmail 88443 invoked by uid 500); 26 Jul 2014 02:04:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88399 invoked by uid 500); 26 Jul 2014 02:04:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88386 invoked by uid 99); 26 Jul 2014 02:04:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 02:04:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 02:04:07 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so6405902oag.21
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 19:03:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=SGcZruimmaZCyuvDL5J9Hj7ksgM8OkgW1kUtztKCn2k=;
        b=sCDfmTifzxUq+VoAGQzS9L2/EcGOxcOWbnLmV6QH6nZBP4Lu8/ME3OrrhGjIcIdbFH
         hMOkkXWD4VGWBx1lAo385dWibY/GuWClwIaSZX6XvmLuLftM2vVXwFP/Wp+emGaGpbIx
         D1v7Wjk6j781T2STQXABnrj9g08LHvhE7NDPaylMtYk8FmIQS3VEOmYFE5vFRF1KwmOj
         /n7CsnPRDXWfhIbzr21A/zD74kvW/FFbXnB003PFhOenPDROYGBgqiH+v1WvzQcX+pbL
         dc1F+0XQFabI27mm7gvKlZQdhpCppYsKMs4agJOdzzoGpHG6dNXjQMYqNcvLWq3j2+8T
         +jxA==
MIME-Version: 1.0
X-Received: by 10.182.98.194 with SMTP id ek2mr27360566obb.5.1406340227328;
 Fri, 25 Jul 2014 19:03:47 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Fri, 25 Jul 2014 19:03:47 -0700 (PDT)
In-Reply-To: <CAPh_B=ZooXKHM8tP+4k193wtsuvbin-e74wyMmXkT3+2PjYjvw@mail.gmail.com>
References: <000301cfa581$2b119310$8134b930$@innowireless.co.kr>
	<CA+qbEUPc8ggp0PGz14inYP2yi=OQbQxGTewxdP_OJTwe4An2Zg@mail.gmail.com>
	<CA+qbEUPYJ6+4uuq01OJPwnkBFrNff=fHmukQ=WOtfmgP-KwAVw@mail.gmail.com>
	<CAPh_B=ZooXKHM8tP+4k193wtsuvbin-e74wyMmXkT3+2PjYjvw@mail.gmail.com>
Date: Fri, 25 Jul 2014 19:03:47 -0700
Message-ID: <CABPQxsvwehZaMF0S3htHSEXzkH=cPa9_2aozwxSFmxsNtMreUg@mail.gmail.com>
Subject: Re: Suggestion for SPARK-1825
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah I agree reflection is the best solution. Whenever we do
reflection we should clearly document in the code which YARN API
version corresponds to which code path. I'm guessing since YARN is
adding new features... we'll just have to do this over time.

- Patrick

On Fri, Jul 25, 2014 at 3:35 PM, Reynold Xin <rxin@databricks.com> wrote:
> Actually reflection is probably a better, lighter weight process for this.
> An extra project brings more overhead for something simple.
>
>
>
>
>
> On Fri, Jul 25, 2014 at 3:09 PM, Colin McCabe <cmccabe@alumni.cmu.edu>
> wrote:
>
>> So, I'm leaning more towards using reflection for this.  Maven profiles
>> could work, but it's tough since we have new stuff coming in in 2.4, 2.5,
>> etc.  and the number of profiles will multiply quickly if we have to do it
>> that way.  Reflection is the approach HBase took in a similar situation.
>>
>> best,
>> Colin
>>
>>
>> On Fri, Jul 25, 2014 at 11:23 AM, Colin McCabe <cmccabe@alumni.cmu.edu>
>> wrote:
>>
>> > I have a similar issue with SPARK-1767.  There are basically three ways
>> to
>> > resolve the issue:
>> >
>> > 1. Use reflection to access classes newer than 0.21 (or whatever the
>> > oldest version of Hadoop is that Spark supports)
>> > 2. Add a build variant (in Maven this would be a profile) that deals with
>> > this.
>> > 3. Auto-detect which classes are available and use those.
>> >
>> > #1 is the easiest for end-users, but it can lead to some ugly code.
>> >
>> > #2 makes the code look nicer, but requires some effort on the part of
>> > people building spark.  This can also lead to headaches for IDEs, if
>> people
>> > don't remember to select the new profile.  (For example, in IntelliJ, you
>> > can't see any of the yarn classes when you import the project from Maven
>> > without the YARN profile selected.)
>> >
>> > #3 is something that... I don't know how to do in sbt or Maven.  I've
>> been
>> > told that an antrun task might work here, but it seems like it could get
>> > really tricky.
>> >
>> > Overall, I'd lean more towards #2 here.
>> >
>> > best,
>> > Colin
>> >
>> >
>> > On Tue, Jul 22, 2014 at 12:47 AM, innowireless TaeYun Kim <
>> > taeyun.kim@innowireless.co.kr> wrote:
>> >
>> >> (I'm resending this mail since it seems that it was not sent. Sorry if
>> >> this
>> >> was already sent.)
>> >>
>> >> Hi,
>> >>
>> >>
>> >>
>> >> A couple of month ago, I made a pull request to fix
>> >> https://issues.apache.org/jira/browse/SPARK-1825.
>> >>
>> >> My pull request is here: https://github.com/apache/spark/pull/899
>> >>
>> >>
>> >>
>> >> But that pull request has problems:
>> >>
>> >> l  It is Hadoop 2.4.0+ only. It won't compile on the versions below it.
>> >>
>> >> l  The related Hadoop API is marked as '@Unstable'.
>> >>
>> >>
>> >>
>> >> Here is an idea to remedy the problems: a new Spark configuration
>> >> variable.
>> >>
>> >> Maybe it can be named as "spark.yarn.submit.crossplatform".
>> >>
>> >> If it is set to "true"(default is false), the related Spark code can use
>> >> the
>> >> hard-coded strings that is the same as the Hadoop API provides, thus
>> >> avoiding compile error on the Hadoop versions below 2.4.0.
>> >>
>> >>
>> >>
>> >> Can someone implement this feature, if this idea is acceptable?
>> >>
>> >> Currently my knowledge on Spark source code and Scala is limited to
>> >> implement it myself.
>> >>
>> >> To the right person, the modification should be trivial.
>> >>
>> >> You can refer to the source code changes of my pull request.
>> >>
>> >>
>> >>
>> >> Thanks.
>> >>
>> >>
>> >>
>> >>
>> >
>>

From dev-return-8541-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 03:12:19 2014
Return-Path: <dev-return-8541-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E75C711712
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 03:12:18 +0000 (UTC)
Received: (qmail 46860 invoked by uid 500); 26 Jul 2014 03:12:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45958 invoked by uid 500); 26 Jul 2014 03:12:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45707 invoked by uid 99); 26 Jul 2014 03:12:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 03:12:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of malouf.gary@gmail.com designates 209.85.192.45 as permitted sender)
Received: from [209.85.192.45] (HELO mail-qg0-f45.google.com) (209.85.192.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 03:12:13 +0000
Received: by mail-qg0-f45.google.com with SMTP id f51so5996656qge.32
        for <multiple recipients>; Fri, 25 Jul 2014 20:11:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=jdaNqf5bKwe9kadORdoD1R2daZT9Ba3bDuV7QvAcM2M=;
        b=JLPdBfo2B9r4S3/90ZAtEy0w64hayF2Tpo9FGIJGIKInxGn81uanRAr6Tpa7XynQkU
         0v0O+fcUsK60I9VUDi4cdfyfW9d02NvOme4A7VXRgEZyKZ7NplWICKnUoC4dBoDlR+dA
         CCS4nyyEhbhozkntRmlq98m0pXM8ib3b15wYog8PlqHgyg6GpoiOTBpG0X4YmLwHi5ZK
         Eso9dloDvpQlePiCeC042ZVbjVNeQsaHjpSTMQs/O+eGnD/NDcrf4V/0z5gi+1Y8nciF
         sY5El6/EELDvFIOhVe6kTYRdNA1UQTF/sRU22QVO+zkQgRIzyLghLW0ORy8mby41okvJ
         3r7w==
MIME-Version: 1.0
X-Received: by 10.140.51.37 with SMTP id t34mr33237805qga.50.1406344308648;
 Fri, 25 Jul 2014 20:11:48 -0700 (PDT)
Received: by 10.140.39.239 with HTTP; Fri, 25 Jul 2014 20:11:48 -0700 (PDT)
In-Reply-To: <CAGOvqip4_UQzXX4xxK7bB=Gv=mgjGZqHm_y1E19GnVvh1PAw0Q@mail.gmail.com>
References: <CAGOvqip4_UQzXX4xxK7bB=Gv=mgjGZqHm_y1E19GnVvh1PAw0Q@mail.gmail.com>
Date: Fri, 25 Jul 2014 23:11:48 -0400
Message-ID: <CAGOvqirX77dQ-rXRtPVdNgow0qsRmST5i5z9cFW=h+JzkBuhjA@mail.gmail.com>
Subject: Re: Kryo Issue on Spark 1.0.1, Mesos 0.18.2
From: Gary Malouf <malouf.gary@gmail.com>
To: dev@spark.apache.org, user@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11351a5af2f44e04ff100c72
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11351a5af2f44e04ff100c72
Content-Type: text/plain; charset=UTF-8

Maybe this is me misunderstanding the Spark system property behavior, but
I'm not clear why the class being loaded ends up having '/' rather than '.'
in it's fully qualified name.  When I tested this out locally, the '/' were
preventing the class from being loaded.


On Fri, Jul 25, 2014 at 2:27 PM, Gary Malouf <malouf.gary@gmail.com> wrote:

> After upgrading to Spark 1.0.1 from 0.9.1 everything seemed to be going
> well.  Looking at the Mesos slave logs, I noticed:
>
> ERROR KryoSerializer: Failed to run spark.kryo.registrator
> java.lang.ClassNotFoundException:
> com/mediacrossing/verrazano/kryo/MxDataRegistrator
>
> My spark-env.sh has the following when I run the Spark Shell:
>
> export MESOS_NATIVE_LIBRARY=/usr/local/lib/libmesos.so
>
> export MASTER=mesos://zk://n-01:2181,n-02:2181,n-03:2181/masters
>
> export ADD_JARS=/opt/spark/mx-lib/verrazano-assembly.jar
>
>
> # -XX:+UseCompressedOops must be disabled to use more than 32GB RAM
>
> SPARK_JAVA_OPTS="-Xss2m -XX:+UseCompressedOops
> -Dspark.local.dir=/opt/mesos-tmp -Dspark.executor.memory=4g
>  -Dspark.serializer=org.apache.spark.serializer.KryoSerializer
> -Dspark.kryo.registrator=com.mediacrossing.verrazano.kryo.MxDataRegistrator
> -Dspark.kryoserializer.buffer.mb=16 -Dspark.akka.askTimeout=30"
>
>
> I was able to verify that our custom jar was being copied to each worker,
> but for some reason it is not finding my registrator class.  Is anyone else
> struggling with Kryo on 1.0.x branch?
>

--001a11351a5af2f44e04ff100c72--

From dev-return-8542-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 03:41:42 2014
Return-Path: <dev-return-8542-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BFE1D11750
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 03:41:42 +0000 (UTC)
Received: (qmail 62555 invoked by uid 500); 26 Jul 2014 03:41:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62366 invoked by uid 500); 26 Jul 2014 03:41:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62353 invoked by uid 99); 26 Jul 2014 03:41:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 03:41:41 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xiaodi@sjtu.edu.cn designates 202.112.26.52 as permitted sender)
Received: from [202.112.26.52] (HELO proxy01.sjtu.edu.cn) (202.112.26.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 03:41:36 +0000
Received: from proxy03.sjtu.edu.cn (unknown [202.121.179.33])
	by proxy01.sjtu.edu.cn (Postfix) with ESMTP id 0F02E260036
	for <dev@spark.apache.org>; Sat, 26 Jul 2014 11:41:14 +0800 (CST)
Received: from localhost (localhost [127.0.0.1])
	by proxy03.sjtu.edu.cn (Postfix) with ESMTP id 06CA8260C08
	for <dev@spark.apache.org>; Sat, 26 Jul 2014 11:41:14 +0800 (GMT-8)
X-Virus-Scanned: amavisd-new at 
Received: from proxy03.sjtu.edu.cn ([127.0.0.1])
	by localhost (proxy03.sjtu.edu.cn [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id ibbZFxmgng6Y for <dev@spark.apache.org>;
	Sat, 26 Jul 2014 11:41:13 +0800 (GMT-8)
Received: from Loca.local (unknown [59.78.3.8])
	(Authenticated sender: xiaodi)
	by proxy03.sjtu.edu.cn (Postfix) with ESMTPSA id E21BD260BED
	for <dev@spark.apache.org>; Sat, 26 Jul 2014 11:41:13 +0800 (GMT-8)
Message-ID: <53D32359.7020302@sjtu.edu.cn>
Date: Sat, 26 Jul 2014 11:41:13 +0800
From: Larry Xiao <xiaodi@sjtu.edu.cn>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org
Subject: Re: GraphX graph partitioning strategy
References: <53D0AECE.2070608@sjtu.edu.cn> <CAK1A71wP__Gvr+2sLDHgRXWUzVpB63BTvKHJZGqSBbPkiL6FFA@mail.gmail.com> <CAK1A71wZGN-vSfRLpxpB7-NjchBqVhYMB+OKSyhVWAs+=ATr7g@mail.gmail.com>
In-Reply-To: <CAK1A71wZGN-vSfRLpxpB7-NjchBqVhYMB+OKSyhVWAs+=ATr7g@mail.gmail.com>
Content-Type: text/plain; charset=UTF-8; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

On 7/26/14, 4:03 AM, Ankur Dave wrote:
> Oops, the code should be:
>
> val unpartitionedGraph: Graph[Int, Int] = ...val numPartitions: Int = 128
> def getTripletPartition(e: EdgeTriplet[Int, Int]): PartitionID = ...
> // Get the triplets using GraphX, then use Spark to repartition
> themval partitionedEdges = unpartitionedGraph.triplets
>    .map(e => (getTripletPartition(e), e))
>    .partitionBy(new HashPartitioner(numPartitions))
>    *.map(pair => Edge(pair._2.srcId, pair._2.dstId, pair._2.attr))*
> val partitionedGraph = Graph(unpartitionedGraph.vertices, partitionedEdges)
>
>
> Ankur <http://www.ankurdave.com/>
>
Hi Ankur,

Thanks for clear advice!

I tested the 4 partitioning algorithm in GraphX, and implemented two others.
And I find EdgePartition2D performs the best.
(the two other algorithms performs only tiny bit better on graphs that 
are highly skewed or bipartite)

Larry

From dev-return-8543-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 05:14:28 2014
Return-Path: <dev-return-8543-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DC8D21185E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 05:14:28 +0000 (UTC)
Received: (qmail 24990 invoked by uid 500); 26 Jul 2014 05:14:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24919 invoked by uid 500); 26 Jul 2014 05:14:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 24903 invoked by uid 99); 26 Jul 2014 05:14:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 05:14:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.47 as permitted sender)
Received: from [209.85.213.47] (HELO mail-yh0-f47.google.com) (209.85.213.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 05:14:22 +0000
Received: by mail-yh0-f47.google.com with SMTP id f10so3513674yha.20
        for <dev@spark.apache.org>; Fri, 25 Jul 2014 22:14:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=BsINTdnOI3Vh3C1M+qALdu598WJgPztyPH7BNP8OPbk=;
        b=J/nxeZ218ZQVEg4qghNSVXyH1m35X7Lr6/Jj9IIcKr//ErUgOjzcZPu9+oa3830aVG
         gVKle329qzfbzHGMqUuvJZ8VkZ2QWIWU0k3MD6QZ3cwOLpiuLtec7DBx9M5XRx/xYhTq
         6ra+uQbmuNBI1a4WQuzb2YFut+NMnbmXBhgx7OqR9GzeV0Q8T837wz3Q7DkFh2LDsVfP
         0Pkq6Kg5zgoXvlaY9EHitdxGt6wz9UbHpKE9bfqlZH8EOvuYendLFbbjd0gfe4dvd2lh
         lxvUQe0doUfKdSP38BA4FAWAvmQAEG9MYyUob34gLzyt+ET7OzzwxzHZL5zROtWhMnM1
         9yDw==
MIME-Version: 1.0
X-Received: by 10.236.125.72 with SMTP id y48mr29785600yhh.111.1406351641967;
 Fri, 25 Jul 2014 22:14:01 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Fri, 25 Jul 2014 22:14:01 -0700 (PDT)
In-Reply-To: <CABPQxsusN8ygHHPKNsOVxmg=DG7SnqO5eJ-szwmFMMxEq6kfgQ@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<CAOhmDzd7HP0V4LB7owB1yt1gu+QokNC3=kFyZ49eRyQjw4+B=Q@mail.gmail.com>
	<CAAswR-5swpxuukMZOH6wjwRWiteH7oWbrztAbzb6ma8g9HOsaA@mail.gmail.com>
	<CABPQxsusN8ygHHPKNsOVxmg=DG7SnqO5eJ-szwmFMMxEq6kfgQ@mail.gmail.com>
Date: Fri, 25 Jul 2014 21:14:01 -0800
Message-ID: <CALte62zZGv2V-KSVz4zWccPsHUkxrhLSP0h8Vx5yJ3PyxBSCkw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303b3c970c847e04ff11c23c
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303b3c970c847e04ff11c23c
Content-Type: text/plain; charset=UTF-8

HADOOP-10456 is fixed in hadoop 2.4.1

Does this mean that synchronization
on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for hadoop
2.4.1 ?

Cheers


On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> The most important issue in this release is actually an ammendment to
> an earlier fix. The original fix caused a deadlock which was a
> regression from 1.0.0->1.0.1:
>
> Issue:
> https://issues.apache.org/jira/browse/SPARK-1097
>
> 1.0.1 Fix:
> https://github.com/apache/spark/pull/1273/files (had a deadlock)
>
> 1.0.2 Fix:
> https://github.com/apache/spark/pull/1409/files
>
> I failed to correctly label this on JIRA, but I've updated it!
>
> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
> <michael@databricks.com> wrote:
> > That query is looking at "Fix Version" not "Target Version".  The fact
> that
> > the first one is still open is only because the bug is not resolved in
> > master.  It is fixed in 1.0.2.  The second one is partially fixed in
> 1.0.2,
> > but is not worth blocking the release for.
> >
> >
> > On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
> > nicholas.chammas@gmail.com> wrote:
> >
> >> TD, there are a couple of unresolved issues slated for 1.0.2
> >> <
> >>
> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
> >> >.
> >> Should they be edited somehow?
> >>
> >>
> >> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
> >> tathagata.das1565@gmail.com>
> >> wrote:
> >>
> >> > Please vote on releasing the following candidate as Apache Spark
> version
> >> > 1.0.2.
> >> >
> >> > This release fixes a number of bugs in Spark 1.0.1.
> >> > Some of the notable ones are
> >> > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> >> > SPARK-1199. The fix was reverted for 1.0.2.
> >> > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> >> > HDFS CSV file.
> >> > The full list is at http://s.apache.org/9NJ
> >> >
> >> > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> >> >
> >> >
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
> >> >
> >> > The release files, including signatures, digests, etc can be found at:
> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1/
> >> >
> >> > Release artifacts are signed with the following key:
> >> > https://people.apache.org/keys/committer/tdas.asc
> >> >
> >> > The staging repository for this release can be found at:
> >> >
> https://repository.apache.org/content/repositories/orgapachespark-1024/
> >> >
> >> > The documentation corresponding to this release can be found at:
> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> >> >
> >> > Please vote on releasing this package as Apache Spark 1.0.2!
> >> >
> >> > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> >> > a majority of at least 3 +1 PMC votes are cast.
> >> > [ ] +1 Release this package as Apache Spark 1.0.2
> >> > [ ] -1 Do not release this package because ...
> >> >
> >> > To learn more about Apache Spark, please see
> >> > http://spark.apache.org/
> >> >
> >>
>

--20cf303b3c970c847e04ff11c23c--

From dev-return-8544-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 14:42:44 2014
Return-Path: <dev-return-8544-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EB22111F7E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 14:42:43 +0000 (UTC)
Received: (qmail 28253 invoked by uid 500); 26 Jul 2014 14:42:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28188 invoked by uid 500); 26 Jul 2014 14:42:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28175 invoked by uid 99); 26 Jul 2014 14:42:43 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 14:42:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.181 as permitted sender)
Received: from [209.85.160.181] (HELO mail-yk0-f181.google.com) (209.85.160.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 14:42:40 +0000
Received: by mail-yk0-f181.google.com with SMTP id q200so3538069ykb.26
        for <dev@spark.apache.org>; Sat, 26 Jul 2014 07:42:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=7QACfededFKmcJEHus+RfksdGc4J2YMOOmW7UoT78MQ=;
        b=T4F3Kzwg84yEfNrYu7qTyohGRSlHABwDVqZMdRqPtMbJbuVXB33At859u+gvsKxoqN
         SkTly56dF/vYm8xX4bPGRjYhu27dnfpYxwESOIWWhu37TxtfdF4RwgaZRBr5hsl4dMrr
         vcfYws5Ku1nS9p1s//2GBMCRK5ZYGhBOsnmmbDmG522AIWV/EqAfDK3nVRAh5gd42QbP
         G46F6ikbvnFEdppIePTs5UTodq4AmAJNs2XaYcsFH+WcjNN8wGPPkSxkO/gd7oEU5ZrD
         LRNSynP1K7Z1VPKTZD5WKGFWSIKyXFG764Ss6TN74qCQ5aRmL0NSXzfqvzmZLtre1fTy
         LI0w==
MIME-Version: 1.0
X-Received: by 10.236.119.146 with SMTP id n18mr33846945yhh.23.1406385735400;
 Sat, 26 Jul 2014 07:42:15 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Sat, 26 Jul 2014 07:42:15 -0700 (PDT)
Date: Sat, 26 Jul 2014 06:42:15 -0800
Message-ID: <CALte62xnoWryUuhOmu_Q91Lpi=6z8H6=u87_=yK-TokG+9JqXg@mail.gmail.com>
Subject: setting inputMetrics in HadoopRDD#compute()
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf30050cd42e1bad04ff19b28a
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30050cd42e1bad04ff19b28a
Content-Type: text/plain; charset=UTF-8

Hi,
Starting at line 203:
      try {
        /* bytesRead may not exactly equal the bytes read by a task: split
boundaries aren't
         * always at record boundaries, so tasks may need to read into
other splits to complete
         * a record. */
        inputMetrics.bytesRead = split.inputSplit.value.getLength()
      } catch {
        case e: java.io.IOException =>
          logWarning("Unable to get input size to set InputMetrics for
task", e)
      }
      context.taskMetrics.inputMetrics = Some(inputMetrics)

If there is IOException, context.taskMetrics.inputMetrics is set by
wrapping inputMetrics - as if there wasn't any error.

I wonder if the above code should distinguish the error condition.

Cheers

--20cf30050cd42e1bad04ff19b28a--

From dev-return-8545-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 17:48:21 2014
Return-Path: <dev-return-8545-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DEBD9112D2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 17:48:20 +0000 (UTC)
Received: (qmail 3673 invoked by uid 500); 26 Jul 2014 17:48:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3627 invoked by uid 500); 26 Jul 2014 17:48:20 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 3607 invoked by uid 99); 26 Jul 2014 17:48:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 17:48:19 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 17:48:16 +0000
Received: by mail-qa0-f43.google.com with SMTP id w8so6000638qac.30
        for <dev@spark.apache.org>; Sat, 26 Jul 2014 10:47:50 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=iiy+lJ17ArrDSFmac80Unk2+tXLIroRC0MRgokLR6ZQ=;
        b=bEXdsjIY69acCTxpbvaAQoAo68ey0BYEhUWQT6gJduP5Ig6b3ZNT+76aoBhsxRBupD
         vp1sUc2hOe2Ii3/666aTaOr3KW27bHUVAckgpJt+orz6DPcCAvHmfH6wK+MnETsrHArQ
         96dfXD/5vJUKaim0LGCVlGTfrbfT8HK21krO94YtMkK+r58zYyDk7v0yEoNAnKO+u2Az
         zuSqFuWZv/vgXAlApqzjvl+lVhLXEAFD/wsHZGkqjMG14mrX36OEhjz1zuqNFln05hYA
         EpsftToOKyy/K5jymQNKzoLIyN9TjZhfwxzMeeVw6eeYkUXS4wcH2VC3ccWXGnH8Q2sl
         nVGQ==
X-Gm-Message-State: ALoCoQk4gSuubyTyCn8ZbSSlcosatXQ8kprb1Y37HPNlG6IpYZYt5hoR28P87WomHyzxodHOMfem
X-Received: by 10.140.25.11 with SMTP id 11mr41669992qgs.9.1406396870414; Sat,
 26 Jul 2014 10:47:50 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sat, 26 Jul 2014 10:47:30 -0700 (PDT)
In-Reply-To: <CALte62xnoWryUuhOmu_Q91Lpi=6z8H6=u87_=yK-TokG+9JqXg@mail.gmail.com>
References: <CALte62xnoWryUuhOmu_Q91Lpi=6z8H6=u87_=yK-TokG+9JqXg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sat, 26 Jul 2014 10:47:30 -0700
Message-ID: <CAPh_B=Z9giZ0Ty1BJQU2BR=ByMdc9o1oJ0_w5AsONcgP8ErgLA@mail.gmail.com>
Subject: Re: setting inputMetrics in HadoopRDD#compute()
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Kay Ousterhout <kayousterhout@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c02a8edff42304ff1c4973
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c02a8edff42304ff1c4973
Content-Type: text/plain; charset=UTF-8

There is one piece of information that'd be useful to know, which is the
source of the input. Even in the presence of an IOException, the input
metrics still specifies the task is reading from Hadoop.

However, I'm slightly confused by this -- I think usually we'd want to
report the number of bytes read, rather than the total input size. For
example, if there is a limit (only read the first 5 records), the actual
number of bytes read is much smaller than the total split size.

Kay, am I mis-interpreting this?



On Sat, Jul 26, 2014 at 7:42 AM, Ted Yu <yuzhihong@gmail.com> wrote:

> Hi,
> Starting at line 203:
>       try {
>         /* bytesRead may not exactly equal the bytes read by a task: split
> boundaries aren't
>          * always at record boundaries, so tasks may need to read into
> other splits to complete
>          * a record. */
>         inputMetrics.bytesRead = split.inputSplit.value.getLength()
>       } catch {
>         case e: java.io.IOException =>
>           logWarning("Unable to get input size to set InputMetrics for
> task", e)
>       }
>       context.taskMetrics.inputMetrics = Some(inputMetrics)
>
> If there is IOException, context.taskMetrics.inputMetrics is set by
> wrapping inputMetrics - as if there wasn't any error.
>
> I wonder if the above code should distinguish the error condition.
>
> Cheers
>

--001a11c02a8edff42304ff1c4973--

From dev-return-8546-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 18:47:28 2014
Return-Path: <dev-return-8546-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6F24113C4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 18:47:28 +0000 (UTC)
Received: (qmail 62904 invoked by uid 500); 26 Jul 2014 18:47:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62857 invoked by uid 500); 26 Jul 2014 18:47:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62845 invoked by uid 99); 26 Jul 2014 18:47:26 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 18:47:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.48 as permitted sender)
Received: from [209.85.192.48] (HELO mail-qg0-f48.google.com) (209.85.192.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 18:47:25 +0000
Received: by mail-qg0-f48.google.com with SMTP id i50so6669835qgf.35
        for <dev@spark.apache.org>; Sat, 26 Jul 2014 11:47:00 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=v0X+1K+Pnos15+zo7POPrI8GUXSnIYEpQFELuT8Ndy4=;
        b=Gj/CuxiV+5+Nz5kNr9ReqFuWu2Ahw6UvR3K3r7M+Agg/LD0PMu0nkuWXXKiHQWGkc3
         cQKTZwqkSgGsJURP4SuQBJ4OWL88iD0C+vxJ2VHuroL53r+plSWiyrsj3XazL5vT+BRi
         vUFi4CZbqfZ5Pb7XCOqIWIZPRk3UXNUDld/1ASIv79441M9xXYhXhqlm1CZkoTLM4Hbv
         PsUzgQOq0IJbhvODAdD20bK72EXFKsA8BOcVLdZBEP4oGZqCo/gjOnIwLy9Y71j5YjoS
         oUgHawnLfZyEr4beqdm1hjWrQzT4rmHkAPvPD2Bk4Wp8QsMsjVzqfE/5PqQ0sFNkq6lf
         G3bA==
X-Gm-Message-State: ALoCoQnxFpDkxyJRwaY61o6/TBexpNdo62Rj9x40cyQCNM9CbhgRCUrwmaGM74yQs+S8voE/bs8C
MIME-Version: 1.0
X-Received: by 10.140.39.164 with SMTP id v33mr25804570qgv.39.1406400419982;
 Sat, 26 Jul 2014 11:46:59 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Sat, 26 Jul 2014 11:46:59 -0700 (PDT)
In-Reply-To: <CAPh_B=Z9giZ0Ty1BJQU2BR=ByMdc9o1oJ0_w5AsONcgP8ErgLA@mail.gmail.com>
References: <CALte62xnoWryUuhOmu_Q91Lpi=6z8H6=u87_=yK-TokG+9JqXg@mail.gmail.com>
	<CAPh_B=Z9giZ0Ty1BJQU2BR=ByMdc9o1oJ0_w5AsONcgP8ErgLA@mail.gmail.com>
Date: Sat, 26 Jul 2014 11:46:59 -0700
Message-ID: <CACBYxKLY10G5B0HH=pRaMO2pQVZn4yRgbO74F-Sm5gqRiVHfbw@mail.gmail.com>
Subject: Re: setting inputMetrics in HadoopRDD#compute()
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Kay Ousterhout <kayousterhout@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c14eae720c3b04ff1d1d56
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c14eae720c3b04ff1d1d56
Content-Type: text/plain; charset=UTF-8

I'm working on a patch that switches this stuff out with the Hadoop
FileSystem StatisticsData, which will both give an accurate count and allow
us to get metrics while the task is in progress.  A hitch is that it relies
on https://issues.apache.org/jira/browse/HADOOP-10688, so we still might
want a fallback for versions of Hadoop that don't have this API.


On Sat, Jul 26, 2014 at 10:47 AM, Reynold Xin <rxin@databricks.com> wrote:

> There is one piece of information that'd be useful to know, which is the
> source of the input. Even in the presence of an IOException, the input
> metrics still specifies the task is reading from Hadoop.
>
> However, I'm slightly confused by this -- I think usually we'd want to
> report the number of bytes read, rather than the total input size. For
> example, if there is a limit (only read the first 5 records), the actual
> number of bytes read is much smaller than the total split size.
>
> Kay, am I mis-interpreting this?
>
>
>
> On Sat, Jul 26, 2014 at 7:42 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>
> > Hi,
> > Starting at line 203:
> >       try {
> >         /* bytesRead may not exactly equal the bytes read by a task:
> split
> > boundaries aren't
> >          * always at record boundaries, so tasks may need to read into
> > other splits to complete
> >          * a record. */
> >         inputMetrics.bytesRead = split.inputSplit.value.getLength()
> >       } catch {
> >         case e: java.io.IOException =>
> >           logWarning("Unable to get input size to set InputMetrics for
> > task", e)
> >       }
> >       context.taskMetrics.inputMetrics = Some(inputMetrics)
> >
> > If there is IOException, context.taskMetrics.inputMetrics is set by
> > wrapping inputMetrics - as if there wasn't any error.
> >
> > I wonder if the above code should distinguish the error condition.
> >
> > Cheers
> >
>

--001a11c14eae720c3b04ff1d1d56--

From dev-return-8547-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 19:13:20 2014
Return-Path: <dev-return-8547-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 614B51148A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 19:13:20 +0000 (UTC)
Received: (qmail 85984 invoked by uid 500); 26 Jul 2014 19:13:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85926 invoked by uid 500); 26 Jul 2014 19:13:19 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85915 invoked by uid 99); 26 Jul 2014 19:13:19 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 19:13:19 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 19:13:15 +0000
Received: by mail-qg0-f53.google.com with SMTP id q107so6581959qgd.40
        for <dev@spark.apache.org>; Sat, 26 Jul 2014 12:12:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=HtWXokaxwSUd9PoRjLYCnOKS1FOPrC2GM8uS9cMj+lw=;
        b=PbstZjYEYHCB5iVvFMp3eMMLhu54a3Huy1tKbGcDGFcfunpTsInfD5PJoGIBDcgiYx
         QCoJVeGlT0QHiIy2Ctq1pjI+whkbCjw+k8h4bVFCHjv9mIExv5fCZOueDRsPdVmked/s
         QLwd/T3vjo1AVZGhtUE2xeYvDcrrR/aXkyap0GWjWjqoPEhbTH1om1hClIrQbWtMEnZC
         92oogkatZrN8iefumEuiux0LcTh+Q35y1B+0OMj1kyz+Ye70lTiDI+FPXEI6IqQYEkLL
         S1U/nuoCRW2iN3ZDlsD80uFXnh7kl0wegIuzuUVHB9oP42+STQGISPGUSZ61/oX6NL3t
         foBw==
X-Gm-Message-State: ALoCoQnjpU4YNdQgqUWjyQJ5yDwn5q0kAXg5G8/Ou3JKXzJKe3CIKdMQx5yJxEr16uVhgMruBaSD
X-Received: by 10.224.120.138 with SMTP id d10mr41574873qar.9.1406401974301;
 Sat, 26 Jul 2014 12:12:54 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sat, 26 Jul 2014 12:12:33 -0700 (PDT)
In-Reply-To: <CACBYxKLY10G5B0HH=pRaMO2pQVZn4yRgbO74F-Sm5gqRiVHfbw@mail.gmail.com>
References: <CALte62xnoWryUuhOmu_Q91Lpi=6z8H6=u87_=yK-TokG+9JqXg@mail.gmail.com>
 <CAPh_B=Z9giZ0Ty1BJQU2BR=ByMdc9o1oJ0_w5AsONcgP8ErgLA@mail.gmail.com> <CACBYxKLY10G5B0HH=pRaMO2pQVZn4yRgbO74F-Sm5gqRiVHfbw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sat, 26 Jul 2014 12:12:33 -0700
Message-ID: <CAPh_B=Y+Bqk7TkJxgys4AKsEOtFTrM4mRAGNLh18OiP=QEDHFw@mail.gmail.com>
Subject: Re: setting inputMetrics in HadoopRDD#compute()
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Kay Ousterhout <kayousterhout@gmail.com>
Content-Type: multipart/alternative; boundary=001a11c2eea016fcfe04ff1d7a01
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2eea016fcfe04ff1d7a01
Content-Type: text/plain; charset=UTF-8

That makes sense, Sandy.

When you add the patch, can you make sure you comment inline on why the
fallback is needed?



On Sat, Jul 26, 2014 at 11:46 AM, Sandy Ryza <sandy.ryza@cloudera.com>
wrote:

> I'm working on a patch that switches this stuff out with the Hadoop
> FileSystem StatisticsData, which will both give an accurate count and allow
> us to get metrics while the task is in progress.  A hitch is that it relies
> on https://issues.apache.org/jira/browse/HADOOP-10688, so we still might
> want a fallback for versions of Hadoop that don't have this API.
>
>
> On Sat, Jul 26, 2014 at 10:47 AM, Reynold Xin <rxin@databricks.com> wrote:
>
> > There is one piece of information that'd be useful to know, which is the
> > source of the input. Even in the presence of an IOException, the input
> > metrics still specifies the task is reading from Hadoop.
> >
> > However, I'm slightly confused by this -- I think usually we'd want to
> > report the number of bytes read, rather than the total input size. For
> > example, if there is a limit (only read the first 5 records), the actual
> > number of bytes read is much smaller than the total split size.
> >
> > Kay, am I mis-interpreting this?
> >
> >
> >
> > On Sat, Jul 26, 2014 at 7:42 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> >
> > > Hi,
> > > Starting at line 203:
> > >       try {
> > >         /* bytesRead may not exactly equal the bytes read by a task:
> > split
> > > boundaries aren't
> > >          * always at record boundaries, so tasks may need to read into
> > > other splits to complete
> > >          * a record. */
> > >         inputMetrics.bytesRead = split.inputSplit.value.getLength()
> > >       } catch {
> > >         case e: java.io.IOException =>
> > >           logWarning("Unable to get input size to set InputMetrics for
> > > task", e)
> > >       }
> > >       context.taskMetrics.inputMetrics = Some(inputMetrics)
> > >
> > > If there is IOException, context.taskMetrics.inputMetrics is set by
> > > wrapping inputMetrics - as if there wasn't any error.
> > >
> > > I wonder if the above code should distinguish the error condition.
> > >
> > > Cheers
> > >
> >
>

--001a11c2eea016fcfe04ff1d7a01--

From dev-return-8548-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sat Jul 26 19:15:18 2014
Return-Path: <dev-return-8548-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 79E6411498
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 26 Jul 2014 19:15:18 +0000 (UTC)
Received: (qmail 88874 invoked by uid 500); 26 Jul 2014 19:15:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88814 invoked by uid 500); 26 Jul 2014 19:15:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88801 invoked by uid 99); 26 Jul 2014 19:15:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 19:15:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kayousterhout@gmail.com designates 209.85.213.48 as permitted sender)
Received: from [209.85.213.48] (HELO mail-yh0-f48.google.com) (209.85.213.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 26 Jul 2014 19:15:15 +0000
Received: by mail-yh0-f48.google.com with SMTP id i57so3802217yha.35
        for <dev@spark.apache.org>; Sat, 26 Jul 2014 12:14:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=xjMbxaFiKCcn702l9c/ySVhpspGrC+d/7Nd3LGJcLgU=;
        b=nvRlXHySQykUfeT9ymBIoNh08Zy4s/jIYfgoiB/A+2KjyYrZ0+iQRZ/h4YWKCupQXh
         nDduX+bVC/TcTs3sA0PmDXVpw0qpmtMYR8FT9oGMo6hYEFh5hAhuNOdQqcmQllv6boj1
         VWAQmEX7zUo2go+4JUxXLXWaM/ZhZXRybCoa7jdHiFZCcBkKv6A1WKSMnoc027hhhA9U
         sf824KnNsOlA176xjGIHXXNxAcykju9XdgxQK0izWqt9OablIgvuBDwdOEAHns8OIi9f
         K6vMrwjNb+FN4Bg6bE5DeTJY3L+1tyialbibVPuEppAXUWiBXJ1RAdw1xNdzkf2ldpcM
         5mOA==
MIME-Version: 1.0
X-Received: by 10.236.231.178 with SMTP id l48mr35586366yhq.143.1406402090442;
 Sat, 26 Jul 2014 12:14:50 -0700 (PDT)
Received: by 10.170.166.212 with HTTP; Sat, 26 Jul 2014 12:14:50 -0700 (PDT)
In-Reply-To: <CAPh_B=Y+Bqk7TkJxgys4AKsEOtFTrM4mRAGNLh18OiP=QEDHFw@mail.gmail.com>
References: <CALte62xnoWryUuhOmu_Q91Lpi=6z8H6=u87_=yK-TokG+9JqXg@mail.gmail.com>
	<CAPh_B=Z9giZ0Ty1BJQU2BR=ByMdc9o1oJ0_w5AsONcgP8ErgLA@mail.gmail.com>
	<CACBYxKLY10G5B0HH=pRaMO2pQVZn4yRgbO74F-Sm5gqRiVHfbw@mail.gmail.com>
	<CAPh_B=Y+Bqk7TkJxgys4AKsEOtFTrM4mRAGNLh18OiP=QEDHFw@mail.gmail.com>
Date: Sat, 26 Jul 2014 12:14:50 -0700
Message-ID: <CAKJXNjHewyerXHmEgsiMVfOZ0gyEhObvwKQUpPMY4VHJvmpQ-Q@mail.gmail.com>
Subject: Re: setting inputMetrics in HadoopRDD#compute()
From: Kay Ousterhout <kayousterhout@gmail.com>
To: Reynold Xin <rxin@databricks.com>
Cc: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c1bdd403257104ff1d81ae
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1bdd403257104ff1d81ae
Content-Type: text/plain; charset=UTF-8

Reynold you're totally right, as discussed offline -- I didn't think about
the limit use case when I wrote this.  Sandy, is it easy to fix this as
part of your patch to use StatisticsData?  If not, I can fix it in a
separate patch.


On Sat, Jul 26, 2014 at 12:12 PM, Reynold Xin <rxin@databricks.com> wrote:

> That makes sense, Sandy.
>
> When you add the patch, can you make sure you comment inline on why the
> fallback is needed?
>
>
>
> On Sat, Jul 26, 2014 at 11:46 AM, Sandy Ryza <sandy.ryza@cloudera.com>
> wrote:
>
>> I'm working on a patch that switches this stuff out with the Hadoop
>> FileSystem StatisticsData, which will both give an accurate count and
>> allow
>> us to get metrics while the task is in progress.  A hitch is that it
>> relies
>> on https://issues.apache.org/jira/browse/HADOOP-10688, so we still might
>> want a fallback for versions of Hadoop that don't have this API.
>>
>>
>> On Sat, Jul 26, 2014 at 10:47 AM, Reynold Xin <rxin@databricks.com>
>> wrote:
>>
>> > There is one piece of information that'd be useful to know, which is the
>> > source of the input. Even in the presence of an IOException, the input
>> > metrics still specifies the task is reading from Hadoop.
>> >
>> > However, I'm slightly confused by this -- I think usually we'd want to
>> > report the number of bytes read, rather than the total input size. For
>> > example, if there is a limit (only read the first 5 records), the actual
>> > number of bytes read is much smaller than the total split size.
>> >
>> > Kay, am I mis-interpreting this?
>> >
>> >
>> >
>> > On Sat, Jul 26, 2014 at 7:42 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>> >
>> > > Hi,
>> > > Starting at line 203:
>> > >       try {
>> > >         /* bytesRead may not exactly equal the bytes read by a task:
>> > split
>> > > boundaries aren't
>> > >          * always at record boundaries, so tasks may need to read into
>> > > other splits to complete
>> > >          * a record. */
>> > >         inputMetrics.bytesRead = split.inputSplit.value.getLength()
>> > >       } catch {
>> > >         case e: java.io.IOException =>
>> > >           logWarning("Unable to get input size to set InputMetrics for
>> > > task", e)
>> > >       }
>> > >       context.taskMetrics.inputMetrics = Some(inputMetrics)
>> > >
>> > > If there is IOException, context.taskMetrics.inputMetrics is set by
>> > > wrapping inputMetrics - as if there wasn't any error.
>> > >
>> > > I wonder if the above code should distinguish the error condition.
>> > >
>> > > Cheers
>> > >
>> >
>>
>
>

--001a11c1bdd403257104ff1d81ae--

From dev-return-8549-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 02:31:12 2014
Return-Path: <dev-return-8549-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 820721123A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 02:31:12 +0000 (UTC)
Received: (qmail 45761 invoked by uid 500); 27 Jul 2014 02:31:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45719 invoked by uid 500); 27 Jul 2014 02:31:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44584 invoked by uid 99); 27 Jul 2014 02:31:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 02:31:09 +0000
X-ASF-Spam-Status: No, hits=3.5 required=5.0
	tests=HTML_MESSAGE,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of anand.avati@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 02:31:07 +0000
Received: by mail-oi0-f53.google.com with SMTP id e131so4611632oig.26
        for <multiple recipients>; Sat, 26 Jul 2014 19:30:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:date:message-id:subject
         :from:to:content-type;
        bh=HFkLWpMzOeCq2c/bpzhwTvTovoHkwMLRxKJeRShNRu0=;
        b=nNJmFcHMQjcpy8hmdhom2ZQqnoWjLxTAcBpwm8nV3RI9JfhBmR7+cY4PTwijL/1ezp
         j300khZGansiZfgsna2T1qbM9HbVirlPunUY1ES/Lb88TxdV1CJhLY0Cn0s9enQ9Bh4C
         ejj1HZkIUCIpHkf/J/w2mZNhZ7HKcOYseeLKZamuyBz9EvSXQ4j35AcS5wv2nrjY9zsS
         JYuxvePzDCuFKR231lpF5IFufoyHSNKq6XqnbTAbjCEUtnRw8snxZe2naRg/I09fNyuh
         nehkK/22XLw3SMm84iVLHx+mnxC94Piyh5o14uoxQNWyu0XvYPOXT9RiIiMnVKlaELDA
         Ymkg==
MIME-Version: 1.0
X-Received: by 10.60.57.3 with SMTP id e3mr35840281oeq.33.1406428242562; Sat,
 26 Jul 2014 19:30:42 -0700 (PDT)
Sender: anand.avati@gmail.com
Received: by 10.202.226.147 with HTTP; Sat, 26 Jul 2014 19:30:42 -0700 (PDT)
In-Reply-To: <1401456800744-6582.post@n3.nabble.com>
References: <1392811914173-1753.post@n3.nabble.com>
	<1393431287764-2080.post@n3.nabble.com>
	<1400140061067-5739.post@n3.nabble.com>
	<1401456800744-6582.post@n3.nabble.com>
Date: Sat, 26 Jul 2014 19:30:42 -0700
X-Google-Sender-Auth: qSPjYygTRf5ET25d7TeL8xU5fGo
Message-ID: <CAFboF2xRAOf-KDUyrEEM4NEv6_R6+LVB-LMfgZyWa0GWzmmQNg@mail.gmail.com>
Subject: Re: SparkContext startup time out
From: Anand Avati <avati@gluster.org>
To: user@spark.apache.org, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0149495cccd59604ff239701
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149495cccd59604ff239701
Content-Type: text/plain; charset=UTF-8

I am bumping into this problem as well. I am trying to move to akka 2.3.x
from 2.2.x in order to port to Scala 2.11 - only akka 2.3.x is available in
Scala 2.11. All 2.2.x akka works fine, and all 2.3.x akka give the
following exception in "new SparkContext". Still investigating why..

  java.util.concurrent.TimeoutException: Futures timed out after
[10000 milliseconds]
  at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)
  at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
  at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)
  at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
  at scala.concurrent.Await$.result(package.scala:107)
  at akka.remote.Remoting.start(Remoting.scala:180)
  at akka.remote.RemoteActorRefProvider.init(RemoteActorRefProvider.scala:184)
  at akka.actor.ActorSystemImpl.liftedTree2$1(ActorSystem.scala:618)
  at akka.actor.ActorSystemImpl._start$lzycompute(ActorSystem.scala:615)
  at akka.actor.ActorSystemImpl._start(ActorSystem.scala:615)




On Fri, May 30, 2014 at 6:33 AM, Pierre B <
pierre.borckmans@realimpactanalytics.com> wrote:

> I was annoyed by this as well.
> It appears that just permuting the order of decencies inclusion solves this
> problem:
>
> first spark, than your cdh hadoop distro.
>
> HTH,
>
> Pierre
>
>
>
> --
> View this message in context:
> http://apache-spark-user-list.1001560.n3.nabble.com/SparkContext-startup-time-out-tp1753p6582.html
> Sent from the Apache Spark User List mailing list archive at Nabble.com.
>

--089e0149495cccd59604ff239701--

From dev-return-8550-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 16:53:07 2014
Return-Path: <dev-return-8550-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E69E611AE4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 16:53:06 +0000 (UTC)
Received: (qmail 46735 invoked by uid 500); 27 Jul 2014 16:53:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46672 invoked by uid 500); 27 Jul 2014 16:53:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46659 invoked by uid 99); 27 Jul 2014 16:53:04 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 16:53:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 16:53:00 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so7611853oag.21
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 09:52:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ewX75dLSu4g5bYnfpGod1n+p0KLDlGOGKK5x2vk5eBw=;
        b=0QGa+aYK1WCYzVbMQbCgchAJX8D13SdNOzqUXLRwRdTov2F+iqEhxeh0hSR8A8yK/o
         NO7hK41nHKTLQ6jqoGFE3z0+1poTuow3QgHUQws4un32VDzbETVYrXZNCTOulKAWa5Bl
         1RKO4RlJAJ8T5P57uyUYGEAv1dSviHfDiyEy2jSU2CCEh36Kpvq4TiqH1pBgVLoh3c6s
         I7nALcxEML9YVzQOlogahKlYMwaXHHkLs/CWYy1mGp3p1199CoIF3VcU+ba8wKT3gpMg
         41BnePxF9twDn76kKg7SfjNNYmyK1JcLWaRHH/k7VYNlrAbjOm3V2gbCeOemX0O8t8H3
         Ah4Q==
MIME-Version: 1.0
X-Received: by 10.182.98.194 with SMTP id ek2mr40751153obb.5.1406479960225;
 Sun, 27 Jul 2014 09:52:40 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 09:52:40 -0700 (PDT)
In-Reply-To: <CALte62zZGv2V-KSVz4zWccPsHUkxrhLSP0h8Vx5yJ3PyxBSCkw@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<CAOhmDzd7HP0V4LB7owB1yt1gu+QokNC3=kFyZ49eRyQjw4+B=Q@mail.gmail.com>
	<CAAswR-5swpxuukMZOH6wjwRWiteH7oWbrztAbzb6ma8g9HOsaA@mail.gmail.com>
	<CABPQxsusN8ygHHPKNsOVxmg=DG7SnqO5eJ-szwmFMMxEq6kfgQ@mail.gmail.com>
	<CALte62zZGv2V-KSVz4zWccPsHUkxrhLSP0h8Vx5yJ3PyxBSCkw@mail.gmail.com>
Date: Sun, 27 Jul 2014 09:52:40 -0700
Message-ID: <CABPQxsvzZRugYZri2Ssb-bVtG8qR8SWWtAEGn3hifTRt+t3iZg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Ted - technically I think you are correct, although I wouldn't
recommend disabling this lock. This lock is not expensive (acquired
once per task, as are many other locks already). Also, we've seen some
cases where Hadoop concurrency bugs ended up requiring multiple fixes
- concurrency of client access is not well tested in the Hadoop
codebase since most of the Hadoop tools to not use concurrent access.
So in general it's good to be conservative in what we expect of the
Hadoop client libraries.

If you'd like to discuss this further, please fork a new thread, since
this is a vote thread. Thanks!

On Fri, Jul 25, 2014 at 10:14 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> HADOOP-10456 is fixed in hadoop 2.4.1
>
> Does this mean that synchronization
> on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for hadoop
> 2.4.1 ?
>
> Cheers
>
>
> On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> The most important issue in this release is actually an ammendment to
>> an earlier fix. The original fix caused a deadlock which was a
>> regression from 1.0.0->1.0.1:
>>
>> Issue:
>> https://issues.apache.org/jira/browse/SPARK-1097
>>
>> 1.0.1 Fix:
>> https://github.com/apache/spark/pull/1273/files (had a deadlock)
>>
>> 1.0.2 Fix:
>> https://github.com/apache/spark/pull/1409/files
>>
>> I failed to correctly label this on JIRA, but I've updated it!
>>
>> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
>> <michael@databricks.com> wrote:
>> > That query is looking at "Fix Version" not "Target Version".  The fact
>> that
>> > the first one is still open is only because the bug is not resolved in
>> > master.  It is fixed in 1.0.2.  The second one is partially fixed in
>> 1.0.2,
>> > but is not worth blocking the release for.
>> >
>> >
>> > On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
>> > nicholas.chammas@gmail.com> wrote:
>> >
>> >> TD, there are a couple of unresolved issues slated for 1.0.2
>> >> <
>> >>
>> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
>> >> >.
>> >> Should they be edited somehow?
>> >>
>> >>
>> >> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
>> >> tathagata.das1565@gmail.com>
>> >> wrote:
>> >>
>> >> > Please vote on releasing the following candidate as Apache Spark
>> version
>> >> > 1.0.2.
>> >> >
>> >> > This release fixes a number of bugs in Spark 1.0.1.
>> >> > Some of the notable ones are
>> >> > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
>> >> > SPARK-1199. The fix was reverted for 1.0.2.
>> >> > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
>> >> > HDFS CSV file.
>> >> > The full list is at http://s.apache.org/9NJ
>> >> >
>> >> > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>> >> >
>> >> >
>> >>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>> >> >
>> >> > The release files, including signatures, digests, etc can be found at:
>> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1/
>> >> >
>> >> > Release artifacts are signed with the following key:
>> >> > https://people.apache.org/keys/committer/tdas.asc
>> >> >
>> >> > The staging repository for this release can be found at:
>> >> >
>> https://repository.apache.org/content/repositories/orgapachespark-1024/
>> >> >
>> >> > The documentation corresponding to this release can be found at:
>> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>> >> >
>> >> > Please vote on releasing this package as Apache Spark 1.0.2!
>> >> >
>> >> > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
>> >> > a majority of at least 3 +1 PMC votes are cast.
>> >> > [ ] +1 Release this package as Apache Spark 1.0.2
>> >> > [ ] -1 Do not release this package because ...
>> >> >
>> >> > To learn more about Apache Spark, please see
>> >> > http://spark.apache.org/
>> >> >
>> >>
>>

From dev-return-8551-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 17:43:41 2014
Return-Path: <dev-return-8551-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7B2BD11B7B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 17:43:41 +0000 (UTC)
Received: (qmail 92027 invoked by uid 500); 27 Jul 2014 17:43:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91983 invoked by uid 500); 27 Jul 2014 17:43:40 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91960 invoked by uid 99); 27 Jul 2014 17:43:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 17:43:39 +0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of witgo@qq.com designates 184.105.206.84 as permitted sender)
Received: from [184.105.206.84] (HELO smtpproxy19.qq.com) (184.105.206.84)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 17:43:36 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1406482985; bh=CXiWsOYrh/Rt7BPlZPqOih9MCN4UqPVF9mcxbmxJWXI=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=xEU4z1LDs8c5Kcw4Cf58F3Ps6/sj+O29t5W65aZcYwIBAjdyhbzI1hkO4Vswweq3n
	 bCavl7sVjtm40uoCdu/3nTX1xi1soI1gU7NGigB2yKqGvliZvzfeIaSPbYAb7iZkSB
	 nEVQEb60Icj61LMz/LBh3b+BqrO0jZ8XecgyfscE=
X-QQ-FEAT: toZ1iIzNt07xwWD2ZCSmOprdyzxX1Vf+bni+z3nGkRDxbmENfZPp5ibHuf171
	38Uf33mVE2Cf7AYAhF61PK8ChISGFFh1lRsWlDbiWb5eGmVPBa+sN+zJTGLeR8zw6+UJPA+
	gtaABCu7/nHmP754itqxGvlQD6gtx63LSWWug3xilAsielWTlewDGVrHHqn8
X-QQ-SSF: 000000000000002000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 111.161.216.129
In-Reply-To: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1406482984t968253
From: "=?utf-8?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?utf-8?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re:[VOTE] Release Apache Spark 1.0.2 (RC1)
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_53D53A28_09323540_73530E9B"
Content-Transfer-Encoding: 8Bit
Date: Mon, 28 Jul 2014 01:43:04 +0800
X-Priority: 3
Message-ID: <tencent_06D97CD376CA48C605301ACC@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 4288765555
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_53D53A28_09323540_73530E9B
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64
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------=_NextPart_53D53A28_09323540_73530E9B--




From dev-return-8552-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 17:48:23 2014
Return-Path: <dev-return-8552-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8D87011B99
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 17:48:23 +0000 (UTC)
Received: (qmail 94786 invoked by uid 500); 27 Jul 2014 17:48:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94723 invoked by uid 500); 27 Jul 2014 17:48:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94711 invoked by uid 99); 27 Jul 2014 17:48:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 17:48:22 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.41] (HELO mail-qa0-f41.google.com) (209.85.216.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 17:48:20 +0000
Received: by mail-qa0-f41.google.com with SMTP id j7so6872558qaq.28
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 10:47:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=RAYrWg9oQFQ5/68AB61qVLsZuy/fpVR7W3qFOb+Z5fA=;
        b=TcoJzXndbzFsTgGlFOErPw/qJjCna3bDXiSAJ2dQDzR6ggd6Ye+TU8nyYQrxPgWrZu
         783mtm9RIdRziwSqXzgzvy+U02PJmqp7n0LTYY+fpLktsmYyUyOZQUKI6PRr8bEW0E0J
         NEHdb/2GB6a69uCJJb8u07veJYUQE2oYE9bD9gB3sx3WZw+pdXw+ASBV0j2aDv14o0Ut
         4OcJ5J81A6WQA41YTaKbFMzxWFe6YntUTbhtd151WU77fbCNv6uMvUocp9bTiJCL7evb
         M4WY3+ADzdhAYehbdF0+tRdpakGQz59qfgR1Jx6I1vMkzfQnSFk0lGIawU0ONKdXMt9C
         8dsQ==
X-Gm-Message-State: ALoCoQltaOK2rM3Ay7Yhh622LO2Hm9LG4o8V/Wtk+BMBdPTRADH3wWlWXe2bqXTSxkpfll0vZJe0
X-Received: by 10.224.15.72 with SMTP id j8mr52170527qaa.8.1406483274713;
        Sun, 27 Jul 2014 10:47:54 -0700 (PDT)
Received: from mail-qg0-f53.google.com (mail-qg0-f53.google.com [209.85.192.53])
        by mx.google.com with ESMTPSA id s6sm19371526qge.45.2014.07.27.10.47.52
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 10:47:52 -0700 (PDT)
Received: by mail-qg0-f53.google.com with SMTP id q107so7470096qgd.12
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 10:47:52 -0700 (PDT)
MIME-Version: 1.0
X-Received: by 10.140.106.225 with SMTP id e88mr50785481qgf.20.1406483272134;
 Sun, 27 Jul 2014 10:47:52 -0700 (PDT)
Received: by 10.140.90.75 with HTTP; Sun, 27 Jul 2014 10:47:52 -0700 (PDT)
Received: by 10.140.90.75 with HTTP; Sun, 27 Jul 2014 10:47:52 -0700 (PDT)
In-Reply-To: <tencent_06D97CD376CA48C605301ACC@qq.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<tencent_06D97CD376CA48C605301ACC@qq.com>
Date: Sun, 27 Jul 2014 10:47:52 -0700
Message-ID: <CA+-p3AFxqDBor+4kMtX7WqX-c7jJAM0C1E4xVPkmSpSA=pms_w@mail.gmail.com>
Subject: Re:[VOTE] Release Apache Spark 1.0.2 (RC1)
From: Andrew Ash <andrew@andrewash.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a113a8626d17ae704ff3067b3
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a8626d17ae704ff3067b3
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Is that a regression since 1.0.0?
On Jul 27, 2014 10:43 AM, "witgo" <witgo@qq.com> wrote:

> -1
> The following bug should be fixed:
> https://issues.apache.org/jira/browse/SPARK-2677=E2=80=8D
>
>
>
>
>
> ------------------ Original ------------------
> From:  "Tathagata Das";<tathagata.das1565@gmail.com>;
> Date:  Sat, Jul 26, 2014 07:08 AM
> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>
> Subject:  [VOTE] Release Apache Spark 1.0.2 (RC1)
>
>
>
> Please vote on releasing the following candidate as Apache Spark version
> 1.0.2.
>
> This release fixes a number of bugs in Spark 1.0.1.
> Some of the notable ones are
> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> SPARK-1199. The fix was reverted for 1.0.2.
> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> HDFS CSV file.
> The full list is at http://s.apache.org/9NJ
>
> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D8fb6=
f00e195fb258f3f70f04756e07c259a2351f
>
> The release files, including signatures, digests, etc can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1024/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.2!
>
> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> [ ] +1 Release this package as Apache Spark 1.0.2
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/

--001a113a8626d17ae704ff3067b3--

From dev-return-8553-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 18:00:12 2014
Return-Path: <dev-return-8553-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 73C8011BC3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 18:00:12 +0000 (UTC)
Received: (qmail 5013 invoked by uid 500); 27 Jul 2014 18:00:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 4949 invoked by uid 500); 27 Jul 2014 18:00:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 4937 invoked by uid 99); 27 Jul 2014 18:00:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:00:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.44 as permitted sender)
Received: from [209.85.219.44] (HELO mail-oa0-f44.google.com) (209.85.219.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:00:09 +0000
Received: by mail-oa0-f44.google.com with SMTP id eb12so7730248oac.17
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 10:59:44 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2CQzShbQeELxggA1SoC+obywr8a5vHyVLBKsqVmbu9s=;
        b=keA3/3/L7zKkhDi3uIGHa/lqwS9bqDcu91aiiY5ia3JvtaDGrnlRtSNQeKWafxlV3r
         zATh4FuHSytGzKeOJLw2aaHIC7wWAGqejhsupUiEw+daHD/vnunXPv3oBTbllM5+/jjG
         AJgAI9E6d1s6rq2WEkeK286n9FTPgwFwu3+Td5fWXQ9lHIANQD3cvrBFpNEMbXdNy72v
         5ptt6NRDLFPyo8PzWYUpXock6MvFe14L2gef3iT3azMqoCJcHdhuTq0Du2/qXiXZDbL9
         3jOx6NorKBM+eZjupGlLz/9wonxalrFm6RQGBTqXt6PgpeT+4N10OXn3uBadkdLghzen
         Bzeg==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr42142175obc.75.1406483984869;
 Sun, 27 Jul 2014 10:59:44 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 10:59:44 -0700 (PDT)
In-Reply-To: <CA+-p3AFxqDBor+4kMtX7WqX-c7jJAM0C1E4xVPkmSpSA=pms_w@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<tencent_06D97CD376CA48C605301ACC@qq.com>
	<CA+-p3AFxqDBor+4kMtX7WqX-c7jJAM0C1E4xVPkmSpSA=pms_w@mail.gmail.com>
Date: Sun, 27 Jul 2014 10:59:44 -0700
Message-ID: <CABPQxsswgWcrJ9byR0oDyB0r+PbL1d6KW7YG5=cC1mMWfezUAA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

SPARK-2677 is a long standing issue and not a regression. Also, as far
as I can see there is no patch for it or clear understanding of the
cause. This type of bug does not warrant holding a release. If we fix
SPARK-2677 we can just make another release with the fix.

On Sun, Jul 27, 2014 at 10:47 AM, Andrew Ash <andrew@andrewash.com> wrote:
> Is that a regression since 1.0.0?
> On Jul 27, 2014 10:43 AM, "witgo" <witgo@qq.com> wrote:
>
>> -1
>> The following bug should be fixed:
>> https://issues.apache.org/jira/browse/SPARK-2677
>>
>>
>>
>>
>>
>> ------------------ Original ------------------
>> From:  "Tathagata Das";<tathagata.das1565@gmail.com>;
>> Date:  Sat, Jul 26, 2014 07:08 AM
>> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>>
>> Subject:  [VOTE] Release Apache Spark 1.0.2 (RC1)
>>
>>
>>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.0.2.
>>
>> This release fixes a number of bugs in Spark 1.0.1.
>> Some of the notable ones are
>> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
>> SPARK-1199. The fix was reverted for 1.0.2.
>> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
>> HDFS CSV file.
>> The full list is at http://s.apache.org/9NJ
>>
>> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>>
>> The release files, including signatures, digests, etc can be found at:
>> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/tdas.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1024/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.2!
>>
>> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>> [ ] +1 Release this package as Apache Spark 1.0.2
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/

From dev-return-8554-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 18:05:00 2014
Return-Path: <dev-return-8554-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D3F0011BF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 18:05:00 +0000 (UTC)
Received: (qmail 12721 invoked by uid 500); 27 Jul 2014 18:05:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12656 invoked by uid 500); 27 Jul 2014 18:05:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12639 invoked by uid 99); 27 Jul 2014 18:04:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:04:59 +0000
X-ASF-Spam-Status: No, hits=5.0 required=10.0
	tests=FROM_EXCESS_BASE64,HTML_MESSAGE,RCVD_IN_PSBL,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of witgo@qq.com designates 54.238.162.12 as permitted sender)
Received: from [54.238.162.12] (HELO smtpbgjp2.qq.com) (54.238.162.12)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:04:55 +0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=qq.com; s=s201307;
	t=1406484270; bh=h3OkIbhvzNMbdNlLKBueWQ4itH2AhJojPP3YkZcu7Oc=;
	h=X-QQ-FEAT:X-QQ-SSF:X-HAS-ATTACH:X-QQ-BUSINESS-ORIGIN:
	 X-Originating-IP:In-Reply-To:References:X-QQ-STYLE:X-QQ-mid:From:To:Subject:Mime-Version:Content-Type:Content-Transfer-Encoding:Date:
	 X-Priority:Message-ID:X-QQ-MIME:X-Mailer:X-QQ-Mailer:
	 X-QQ-ReplyHash:X-QQ-SENDSIZE;
	b=fMF5y67xKcQDKvTFYqphyD3CkRXiAA62aJwy1viyX8pGysFa8SJh+xuWmqfw1WIF8
	 vDsYi0ihue1fixfyHwkUrayf7qUL1qu1Vaoy/kWgQD6edil3sqstgBbTJ25wfdnbN1
	 9GP2SKRqYs9FK6Sc1AdWk3tHfKPlu33/W1BBKEwo=
X-QQ-FEAT: D8MthjT6Z6qUcsn8BTMpk3c9KQ8UzCfXYDh9UrRQ3X18mhOen1Onk0g5msI39
	/UrOdgKCwqYHEEfNc/6UF1DSAOPrT9XT7QIZ91564CeS7gvjabbXtlsy9MZ3JVZDtgv+f32
	kbAThm+oxa5dEBmLOoIBNnKzLrf935AbG5RlVZVyIrwWx3CNwFOf1tcd8W82
X-QQ-SSF: 0000000A0000002000000000000000M
X-HAS-ATTACH: no
X-QQ-BUSINESS-ORIGIN: 2
X-Originating-IP: 111.161.216.129
In-Reply-To: <CA+-p3AFxqDBor+4kMtX7WqX-c7jJAM0C1E4xVPkmSpSA=pms_w@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<tencent_06D97CD376CA48C605301ACC@qq.com>
	<CA+-p3AFxqDBor+4kMtX7WqX-c7jJAM0C1E4xVPkmSpSA=pms_w@mail.gmail.com>
X-QQ-STYLE: 
X-QQ-mid: webmail421t1406484270t5414831
From: "=?utf-8?B?d2l0Z28=?=" <witgo@qq.com>
To: "=?utf-8?B?ZGV2?=" <dev@spark.apache.org>
Subject: Re:[VOTE] Release Apache Spark 1.0.2 (RC1)
Mime-Version: 1.0
Content-Type: multipart/alternative;
	boundary="----=_NextPart_53D53F2E_09414498_4D4FA95A"
Content-Transfer-Encoding: 8Bit
Date: Mon, 28 Jul 2014 02:04:30 +0800
X-Priority: 3
Message-ID: <tencent_5AD0698D1BAC11203D539597@qq.com>
X-QQ-MIME: TCMime 1.0 by Tencent
X-Mailer: QQMail 2.x
X-QQ-Mailer: QQMail 2.x
X-QQ-ReplyHash: 316905199
X-QQ-SENDSIZE: 520
X-QQ-Bgrelay: 1
X-Virus-Checked: Checked by ClamAV on apache.org

------=_NextPart_53D53F2E_09414498_4D4FA95A
Content-Type: text/plain;
	charset="utf-8"
Content-Transfer-Encoding: base64
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------=_NextPart_53D53F2E_09414498_4D4FA95A--




From dev-return-8555-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 18:10:06 2014
Return-Path: <dev-return-8555-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74AAB11C0D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 18:10:06 +0000 (UTC)
Received: (qmail 26071 invoked by uid 500); 27 Jul 2014 18:10:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26018 invoked by uid 500); 27 Jul 2014 18:10:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26006 invoked by uid 99); 27 Jul 2014 18:10:05 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:10:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:10:01 +0000
Received: by mail-oa0-f41.google.com with SMTP id j17so7796331oag.28
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 11:09:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/uKkuubLM+UxHzPegI1WFhTbLCU7QetGBtd31Y2Hq1A=;
        b=z9HoESwfuF01tcsKd5MfUQC9umYSefM1roPIfvX/vWyVgLRAvcag0Gz36udonCBvis
         JxLsqGqDoJvE1nSXY29Wr93peLuN3T+vwhjwLfdfCEt0vQAP0cjPbBXQHoXQHyNG7CNq
         YeWrW+bX0ruQlsqkXSIlHPzF2Fc9qnpBqBybmRwXjau+ZZSd+uKNMxalyQg9JPW2Buiv
         WnR5LeJdF7q5TWEThkZiZ2pv6Dz0ESqGsox2LWh+KYe0t1G93gtmvLgFZ5QMZTO1LNct
         JRsZbnvV1NOs5tRdfXw7eMsIZFgDIhkRvW5NsNrumssTaaKI7n/yT5fvCA/SqGhV8z/K
         yebQ==
MIME-Version: 1.0
X-Received: by 10.182.112.134 with SMTP id iq6mr41428254obb.34.1406484580721;
 Sun, 27 Jul 2014 11:09:40 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 11:09:40 -0700 (PDT)
In-Reply-To: <tencent_5AD0698D1BAC11203D539597@qq.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<tencent_06D97CD376CA48C605301ACC@qq.com>
	<CA+-p3AFxqDBor+4kMtX7WqX-c7jJAM0C1E4xVPkmSpSA=pms_w@mail.gmail.com>
	<tencent_5AD0698D1BAC11203D539597@qq.com>
Date: Sun, 27 Jul 2014 11:09:40 -0700
Message-ID: <CABPQxsuCZSgNJj6+Y2_8ARqAP+3m2imzf7Y=s+xQkWr4Gn83hA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I just made a note on the JIRA - I realize it might not be clear
whether it was a regression, but in fact, this issue has been observed
in earlier versions as well.

On Sun, Jul 27, 2014 at 11:04 AM, witgo <witgo@qq.com> wrote:
> It's not sure. I only tested 1.0.1, 1.0.2 (RC) version.
> In addition, my vote is not binding.
>
>
>
>
> ------------------ Original ------------------
> From:  "Andrew Ash";<andrew@andrewash.com>;
> Date:  Mon, Jul 28, 2014 01:47 AM
> To:  "dev"<dev@spark.apache.org>;
>
> Subject:  Re:[VOTE] Release Apache Spark 1.0.2 (RC1)
>
>
>
> Is that a regression since 1.0.0?
> On Jul 27, 2014 10:43 AM, "witgo" <witgo@qq.com> wrote:
>
>> -1
>> The following bug should be fixed:
>> https://issues.apache.org/jira/browse/SPARK-2677
>>
>>
>>
>>
>>
>> ------------------ Original ------------------
>> From:  "Tathagata Das";<tathagata.das1565@gmail.com>;
>> Date:  Sat, Jul 26, 2014 07:08 AM
>> To:  "dev@spark.apache.org"<dev@spark.apache.org>;
>>
>> Subject:  [VOTE] Release Apache Spark 1.0.2 (RC1)
>>
>>
>>
>> Please vote on releasing the following candidate as Apache Spark version
>> 1.0.2.
>>
>> This release fixes a number of bugs in Spark 1.0.1.
>> Some of the notable ones are
>> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
>> SPARK-1199. The fix was reverted for 1.0.2.
>> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
>> HDFS CSV file.
>> The full list is at http://s.apache.org/9NJ
>>
>> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>>
>> The release files, including signatures, digests, etc can be found at:
>> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/tdas.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1024/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.2!
>>
>> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>> [ ] +1 Release this package as Apache Spark 1.0.2
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/

From dev-return-8556-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 18:31:49 2014
Return-Path: <dev-return-8556-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7270611C52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 18:31:49 +0000 (UTC)
Received: (qmail 57680 invoked by uid 500); 27 Jul 2014 18:31:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 57621 invoked by uid 500); 27 Jul 2014 18:31:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 57609 invoked by uid 99); 27 Jul 2014 18:31:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:31:47 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.41 as permitted sender)
Received: from [209.85.219.41] (HELO mail-oa0-f41.google.com) (209.85.219.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:31:42 +0000
Received: by mail-oa0-f41.google.com with SMTP id j17so7807986oag.28
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 11:31:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=D97bkpPJJ8jacCklpxtg5zMRVV2+zsJ4fRrw5H/ggrA=;
        b=o8PLyfH920F2ha7wLdu1BqU4rI9nKW6+0cbZR1NpVFnHy/g3qku91W7HgkCOWKi5kP
         QF9t3ysQxFnryohNkC0T3zT15Kz5JDEv4q5Hj0ybHFrQHzXswZvnZB80sVaaEP02okTH
         ykuFR0ca9SYVSynRHFQQcEEIXag8wUkKASKHqJSoHJBrofXcrbLgYo8M4e1/2iqzwS6q
         NpZx9a75hLSB2v7goKvK7VcGyAIh1aWXUjfFugoUw92hK5+BOeqnrNiW0Nsu8eqs9uNb
         5xCn7XH+xAXB61lPaRy1zCd/3BWnxUz0Aw8kj5cvigaNUHqeMZCVxfqTHvohu4skxJsU
         d8yg==
MIME-Version: 1.0
X-Received: by 10.60.120.98 with SMTP id lb2mr42262693oeb.52.1406485877593;
 Sun, 27 Jul 2014 11:31:17 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 11:31:17 -0700 (PDT)
Date: Sun, 27 Jul 2014 11:31:17 -0700
Message-ID: <CABPQxsvRL0QZKchHGyYnNms41SR-1RJNpQC7+u8bggcRyAEsHQ@mail.gmail.com>
Subject: branch-1.1 will be cut on Friday
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey All,

Just a heads up, we'll cut branch-1.1 on this Friday, August 1st. Once
the release branch is cut we'll start community QA and go into the
normal triage process for merging patches into that branch.

For Spark core, we'll be conservative in merging things past the
freeze date (e.g. high priority fixes) to ensure a healthy amount of
time for testing. A key focus of this release in core is improving
overall stability and resilience of Spark core.

As always, I'll encourage of committers/contributors to help review
patches this week to so we can get as many things in as possible.
People have been quite active recently, which is great!

Good luck!
- Patrick

From dev-return-8557-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 18:44:14 2014
Return-Path: <dev-return-8557-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF7EC11C72
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 18:44:14 +0000 (UTC)
Received: (qmail 76073 invoked by uid 500); 27 Jul 2014 18:44:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 76035 invoked by uid 500); 27 Jul 2014 18:44:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 76016 invoked by uid 99); 27 Jul 2014 18:44:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:44:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 18:44:11 +0000
Received: by mail-ie0-f180.google.com with SMTP id at20so5548760iec.25
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 11:43:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=7Eahfvp1ErvvxwhlRxArG12mvK033CAANz3Fcd1iZ5U=;
        b=upzBLc4hN0M/KCMWBNQk1akIXUraFnprzlvDadX+E26Nl3Reg5u1WPboGV+DOMYW4a
         m9mKE2kG4BOyf4x5XKmhCGMMBcuAQEupKw6qymQkKMlDjMU/2gubGza4dZaYc8e4FrTX
         id23esgHkC4SdERVg2xARmnwk/cWUoiFW7i4h8UhxqSZeTKqfopZ4ql1IjKWN9ChDr55
         Rzl1G9Eh5qMsr7oiA3PwTtZ42PTo+9GaBEThhJLzkSLh7JURQhcz3AbtxKzirmi2G9Er
         9vfzJXJfAyMbpo7N0tQSjiGZAZ9NAxD71GT2uVHGIfu297c5suCZn7HYy5DQ0zt+rVnX
         nTjQ==
X-Received: by 10.50.117.106 with SMTP id kd10mr25012226igb.5.1406486626104;
        Sun, 27 Jul 2014 11:43:46 -0700 (PDT)
Received: from [192.168.2.13] ([69.157.143.63])
        by mx.google.com with ESMTPSA id y11sm18701693igp.14.2014.07.27.11.43.45
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 11:43:45 -0700 (PDT)
Date: Sun, 27 Jul 2014 14:55:11 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <60C69C1B130846D8B46E6DBBF6AB03B4@gmail.com>
In-Reply-To: <CABPQxsvRL0QZKchHGyYnNms41SR-1RJNpQC7+u8bggcRyAEsHQ@mail.gmail.com>
References: <CABPQxsvRL0QZKchHGyYnNms41SR-1RJNpQC7+u8bggcRyAEsHQ@mail.gmail.com>
Subject: Re: branch-1.1 will be cut on Friday
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53d54b0f_1a27709e_1b8"
X-Virus-Checked: Checked by ClamAV on apache.org

--53d54b0f_1a27709e_1b8
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Good news, we will see the official version containing JDBC in very soon! 

Also, I have several pending PRs, can anyone continue the review process in this week?

Avoid overwriting already-set SPARK_HOME in spark-submit: https://github.com/apache/spark/pull/1331

fix locality inversion bug in TaskSetManager: https://github.com/apache/spark/pull/1313 (Matei and Mridulm are working on it)

Allow multiple executor per worker in Standalone mode: https://github.com/apache/spark/pull/731 

Ensure actor is self-contained  in DAGScheduler: https://github.com/apache/spark/pull/637

Best, 

-- 
Nan Zhu


On Sunday, July 27, 2014 at 2:31 PM, Patrick Wendell wrote:

> Hey All,
> 
> Just a heads up, we'll cut branch-1.1 on this Friday, August 1st. Once
> the release branch is cut we'll start community QA and go into the
> normal triage process for merging patches into that branch.
> 
> For Spark core, we'll be conservative in merging things past the
> freeze date (e.g. high priority fixes) to ensure a healthy amount of
> time for testing. A key focus of this release in core is improving
> overall stability and resilience of Spark core.
> 
> As always, I'll encourage of committers/contributors to help review
> patches this week to so we can get as many things in as possible.
> People have been quite active recently, which is great!
> 
> Good luck!
> - Patrick
> 
> 



--53d54b0f_1a27709e_1b8--


From dev-return-8558-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 19:02:25 2014
Return-Path: <dev-return-8558-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E2FCB11D15
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 19:02:24 +0000 (UTC)
Received: (qmail 95195 invoked by uid 500); 27 Jul 2014 19:02:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95149 invoked by uid 500); 27 Jul 2014 19:02:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 94039 invoked by uid 99); 27 Jul 2014 19:02:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 19:02:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.41 as permitted sender)
Received: from [209.85.213.41] (HELO mail-yh0-f41.google.com) (209.85.213.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 19:02:18 +0000
Received: by mail-yh0-f41.google.com with SMTP id b6so4347802yha.0
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 12:01:57 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=R0jvrUg14W38k9aNAFTsEfFAc/wL5wxT7WG8nXfToTc=;
        b=fqvHtB5lkeVh4CdbfDZGYesU7aDaYPYAQDl7442QcYuO78DHQERg4pl1EDzN5uCccz
         lT7g3MhBbYQsbZvew24t1JTTq6iD0Djkl6fsuEGzMwB6tSeA3mODzR2o0o6RW5V5BhRu
         zYMd320cuBmnJ0UE6iRkptbNpGrjsbdbqLLuQvz5cDfmYNSFkZZ/EPQznhN1hmC0rKVL
         XqjU6ci1FwUUrtlnGJu0Bbcv7wHZDB+nAs+/qQf2zwBB5sSQ2Z4KW8Gmumti801zMogM
         W3JIdC/D1lgTD77NNzLIMYp3I71eW80ymT5XKyl1SdMlnYKSO87+8E/HyCejlezQuvz2
         br0w==
MIME-Version: 1.0
X-Received: by 10.236.148.209 with SMTP id v57mr5079562yhj.140.1406487717523;
 Sun, 27 Jul 2014 12:01:57 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Sun, 27 Jul 2014 12:01:57 -0700 (PDT)
Date: Sun, 27 Jul 2014 11:01:57 -0800
Message-ID: <CALte62zBjT6hNdNiw0SjzKQvPS_ROgeS-NERFeqmKSMjAqs93w@mail.gmail.com>
Subject: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303a32f3c9763a04ff317024
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303a32f3c9763a04ff317024
Content-Type: text/plain; charset=UTF-8

Thanks for replying, Patrick.

The intention of my first email was for utilizing newer hadoop releases for
their bug fixes. I am still looking for clean way of passing hadoop release
version number to individual classes.
Using newer hadoop releases would encourage pushing bug fixes / new
features upstream. Ultimately Spark code would become cleaner.

Cheers

On Sun, Jul 27, 2014 at 8:52 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> Ted - technically I think you are correct, although I wouldn't
> recommend disabling this lock. This lock is not expensive (acquired
> once per task, as are many other locks already). Also, we've seen some
> cases where Hadoop concurrency bugs ended up requiring multiple fixes
> - concurrency of client access is not well tested in the Hadoop
> codebase since most of the Hadoop tools to not use concurrent access.
> So in general it's good to be conservative in what we expect of the
> Hadoop client libraries.
>
> If you'd like to discuss this further, please fork a new thread, since
> this is a vote thread. Thanks!
>
> On Fri, Jul 25, 2014 at 10:14 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> > HADOOP-10456 is fixed in hadoop 2.4.1
> >
> > Does this mean that synchronization
> > on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for hadoop
> > 2.4.1 ?
> >
> > Cheers
> >
> >
> > On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> The most important issue in this release is actually an ammendment to
> >> an earlier fix. The original fix caused a deadlock which was a
> >> regression from 1.0.0->1.0.1:
> >>
> >> Issue:
> >> https://issues.apache.org/jira/browse/SPARK-1097
> >>
> >> 1.0.1 Fix:
> >> https://github.com/apache/spark/pull/1273/files (had a deadlock)
> >>
> >> 1.0.2 Fix:
> >> https://github.com/apache/spark/pull/1409/files
> >>
> >> I failed to correctly label this on JIRA, but I've updated it!
> >>
> >> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
> >> <michael@databricks.com> wrote:
> >> > That query is looking at "Fix Version" not "Target Version".  The fact
> >> that
> >> > the first one is still open is only because the bug is not resolved in
> >> > master.  It is fixed in 1.0.2.  The second one is partially fixed in
> >> 1.0.2,
> >> > but is not worth blocking the release for.
> >> >
> >> >
> >> > On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
> >> > nicholas.chammas@gmail.com> wrote:
> >> >
> >> >> TD, there are a couple of unresolved issues slated for 1.0.2
> >> >> <
> >> >>
> >>
> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
> >> >> >.
> >> >> Should they be edited somehow?
> >> >>
> >> >>
> >> >> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
> >> >> tathagata.das1565@gmail.com>
> >> >> wrote:
> >> >>
> >> >> > Please vote on releasing the following candidate as Apache Spark
> >> version
> >> >> > 1.0.2.
> >> >> >
> >> >> > This release fixes a number of bugs in Spark 1.0.1.
> >> >> > Some of the notable ones are
> >> >> > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix
> for
> >> >> > SPARK-1199. The fix was reverted for 1.0.2.
> >> >> > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> >> >> > HDFS CSV file.
> >> >> > The full list is at http://s.apache.org/9NJ
> >> >> >
> >> >> > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> >> >> >
> >> >> >
> >> >>
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
> >> >> >
> >> >> > The release files, including signatures, digests, etc can be found
> at:
> >> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1/
> >> >> >
> >> >> > Release artifacts are signed with the following key:
> >> >> > https://people.apache.org/keys/committer/tdas.asc
> >> >> >
> >> >> > The staging repository for this release can be found at:
> >> >> >
> >> https://repository.apache.org/content/repositories/orgapachespark-1024/
> >> >> >
> >> >> > The documentation corresponding to this release can be found at:
> >> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> >> >> >
> >> >> > Please vote on releasing this package as Apache Spark 1.0.2!
> >> >> >
> >> >> > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> >> >> > a majority of at least 3 +1 PMC votes are cast.
> >> >> > [ ] +1 Release this package as Apache Spark 1.0.2
> >> >> > [ ] -1 Do not release this package because ...
> >> >> >
> >> >> > To learn more about Apache Spark, please see
> >> >> > http://spark.apache.org/
> >> >> >
> >> >>
> >>
>

--20cf303a32f3c9763a04ff317024--

From dev-return-8559-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 19:20:04 2014
Return-Path: <dev-return-8559-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A97BA11D50
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 19:20:04 +0000 (UTC)
Received: (qmail 19200 invoked by uid 500); 27 Jul 2014 19:20:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19140 invoked by uid 500); 27 Jul 2014 19:20:03 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19118 invoked by uid 99); 27 Jul 2014 19:20:03 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 19:20:03 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.53 as permitted sender)
Received: from [209.85.218.53] (HELO mail-oi0-f53.google.com) (209.85.218.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 19:20:01 +0000
Received: by mail-oi0-f53.google.com with SMTP id e131so5058100oig.26
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 12:19:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=WSIRc/PmLlqa+ZV0f9RVi4XDwPgiXVrGbscIi4eSzz4=;
        b=fOasnIEW4DzN4HV731qyyxczE9KA2u3c7xpNbHUKo9U2/+cLvb+h9Iz2fuH4XGrNVh
         azyb9p2B8GnF49N6iRXwcRx5qNcK3dqlkbeVGzL8u+HC//BYUeEc7rcoW1CxPGeC53+E
         HigEY6UwxlueJWKAaYalJvMVlJRiT3vVOC+kCFX/bQ57U6kwL4G2dO/e9WKpvbfIxfqU
         Hv+mwRRgE3aRq8rNk3qamQwKnJMQ5O0ocTKOZAu+72LMPMuPK2Kt6NImIMIvz7tnwu7U
         7oyFAcYfMFVtq5XtItP1/8FDLeHUV4QH+HeU1FaeO/3OTu2deqNqy91aI2rjlcFS18sI
         OPWA==
MIME-Version: 1.0
X-Received: by 10.182.24.38 with SMTP id r6mr43241906obf.10.1406488776528;
 Sun, 27 Jul 2014 12:19:36 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 12:19:36 -0700 (PDT)
In-Reply-To: <CALte62zBjT6hNdNiw0SjzKQvPS_ROgeS-NERFeqmKSMjAqs93w@mail.gmail.com>
References: <CALte62zBjT6hNdNiw0SjzKQvPS_ROgeS-NERFeqmKSMjAqs93w@mail.gmail.com>
Date: Sun, 27 Jul 2014 12:19:36 -0700
Message-ID: <CABPQxsthk-1JFgU9uzJz0KCv5pdKKbpN+hbs+LfLHkeCWV8CkQ@mail.gmail.com>
Subject: Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark
 1.0.2 (RC1)
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Ted,

We always intend Spark to work with the newer Hadoop versions and
encourage Spark users to use the newest Hadoop versions for best
performance.

We do try to be liberal in terms of supporting older versions as well.
This is because many people run older HDFS versions and we want Spark
to read and write data from them. So far we've been willing to do this
despite some maintenance cost.

The reason is that for many users it's very expensive to do a
whole-sale upgrade of HDFS, but trying out new versions of Spark is
much easier. For instance, some of the largest scale Spark users run
fairly old or forked HDFS versions.

- Patrick

On Sun, Jul 27, 2014 at 12:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> Thanks for replying, Patrick.
>
> The intention of my first email was for utilizing newer hadoop releases for
> their bug fixes. I am still looking for clean way of passing hadoop release
> version number to individual classes.
> Using newer hadoop releases would encourage pushing bug fixes / new
> features upstream. Ultimately Spark code would become cleaner.
>
> Cheers
>
> On Sun, Jul 27, 2014 at 8:52 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Ted - technically I think you are correct, although I wouldn't
>> recommend disabling this lock. This lock is not expensive (acquired
>> once per task, as are many other locks already). Also, we've seen some
>> cases where Hadoop concurrency bugs ended up requiring multiple fixes
>> - concurrency of client access is not well tested in the Hadoop
>> codebase since most of the Hadoop tools to not use concurrent access.
>> So in general it's good to be conservative in what we expect of the
>> Hadoop client libraries.
>>
>> If you'd like to discuss this further, please fork a new thread, since
>> this is a vote thread. Thanks!
>>
>> On Fri, Jul 25, 2014 at 10:14 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> > HADOOP-10456 is fixed in hadoop 2.4.1
>> >
>> > Does this mean that synchronization
>> > on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for hadoop
>> > 2.4.1 ?
>> >
>> > Cheers
>> >
>> >
>> > On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>> >
>> >> The most important issue in this release is actually an ammendment to
>> >> an earlier fix. The original fix caused a deadlock which was a
>> >> regression from 1.0.0->1.0.1:
>> >>
>> >> Issue:
>> >> https://issues.apache.org/jira/browse/SPARK-1097
>> >>
>> >> 1.0.1 Fix:
>> >> https://github.com/apache/spark/pull/1273/files (had a deadlock)
>> >>
>> >> 1.0.2 Fix:
>> >> https://github.com/apache/spark/pull/1409/files
>> >>
>> >> I failed to correctly label this on JIRA, but I've updated it!
>> >>
>> >> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
>> >> <michael@databricks.com> wrote:
>> >> > That query is looking at "Fix Version" not "Target Version".  The fact
>> >> that
>> >> > the first one is still open is only because the bug is not resolved in
>> >> > master.  It is fixed in 1.0.2.  The second one is partially fixed in
>> >> 1.0.2,
>> >> > but is not worth blocking the release for.
>> >> >
>> >> >
>> >> > On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
>> >> > nicholas.chammas@gmail.com> wrote:
>> >> >
>> >> >> TD, there are a couple of unresolved issues slated for 1.0.2
>> >> >> <
>> >> >>
>> >>
>> https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20BY%20priority%20DESC
>> >> >> >.
>> >> >> Should they be edited somehow?
>> >> >>
>> >> >>
>> >> >> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
>> >> >> tathagata.das1565@gmail.com>
>> >> >> wrote:
>> >> >>
>> >> >> > Please vote on releasing the following candidate as Apache Spark
>> >> version
>> >> >> > 1.0.2.
>> >> >> >
>> >> >> > This release fixes a number of bugs in Spark 1.0.1.
>> >> >> > Some of the notable ones are
>> >> >> > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix
>> for
>> >> >> > SPARK-1199. The fix was reverted for 1.0.2.
>> >> >> > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
>> >> >> > HDFS CSV file.
>> >> >> > The full list is at http://s.apache.org/9NJ
>> >> >> >
>> >> >> > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>> >> >> >
>> >> >> >
>> >> >>
>> >>
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>> >> >> >
>> >> >> > The release files, including signatures, digests, etc can be found
>> at:
>> >> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1/
>> >> >> >
>> >> >> > Release artifacts are signed with the following key:
>> >> >> > https://people.apache.org/keys/committer/tdas.asc
>> >> >> >
>> >> >> > The staging repository for this release can be found at:
>> >> >> >
>> >> https://repository.apache.org/content/repositories/orgapachespark-1024/
>> >> >> >
>> >> >> > The documentation corresponding to this release can be found at:
>> >> >> > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>> >> >> >
>> >> >> > Please vote on releasing this package as Apache Spark 1.0.2!
>> >> >> >
>> >> >> > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
>> >> >> > a majority of at least 3 +1 PMC votes are cast.
>> >> >> > [ ] +1 Release this package as Apache Spark 1.0.2
>> >> >> > [ ] -1 Do not release this package because ...
>> >> >> >
>> >> >> > To learn more about Apache Spark, please see
>> >> >> > http://spark.apache.org/
>> >> >> >
>> >> >>
>> >>
>>

From dev-return-8560-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 20:51:05 2014
Return-Path: <dev-return-8560-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6D74111EB2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 20:51:05 +0000 (UTC)
Received: (qmail 12278 invoked by uid 500); 27 Jul 2014 20:51:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12210 invoked by uid 500); 27 Jul 2014 20:51:04 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12195 invoked by uid 99); 27 Jul 2014 20:51:04 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 20:51:04 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.181 as permitted sender)
Received: from [209.85.192.181] (HELO mail-pd0-f181.google.com) (209.85.192.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 20:50:56 +0000
Received: by mail-pd0-f181.google.com with SMTP id g10so8677728pdj.12
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 13:50:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=ZZKbdVS8hMYhZqwn6aC9mS0S9Z+oADUTA+1JLDYfPkA=;
        b=GIODU/bZ97VItau+NkjAJOS2iVtv7RGXCafQkAGBN4tkfq+Spsh5kOgQhD+ZDReEj+
         oIjetmTAPZRTPAaLStiSsdnzXkFPLTb9VPcDsN9C94GGrbcmhJQUl7GDZq0Q/0fiHlEt
         3+dVFr21nPXQ1tIBNJS2LJY8oC7GwbE6F+RPOdrN0OHWAV7HofOYJjo1ZK823inQYRiL
         YSdWMcfTdZGvZHQW6TQ+8zyRCFQXGhL79JoTf62bPeNYqZSnznxuc9Jl5TUJ5xnLVWEj
         PLAe1z7aMOxKilS5smNdOGzJ6Jpy8YcRvucJGKlQokyss7djsZc89jrZi9gWbrwk6j2S
         WAfw==
X-Received: by 10.68.113.133 with SMTP id iy5mr4215360pbb.135.1406494231462;
        Sun, 27 Jul 2014 13:50:31 -0700 (PDT)
Received: from [192.168.1.106] (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id t5sm15419631pbs.4.2014.07.27.13.50.30
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 13:50:30 -0700 (PDT)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CABPQxsthk-1JFgU9uzJz0KCv5pdKKbpN+hbs+LfLHkeCWV8CkQ@mail.gmail.com>
Date: Sun, 27 Jul 2014 13:50:29 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <B01A3EB3-9CE8-49C4-80DC-F68845189E1C@gmail.com>
References: <CALte62zBjT6hNdNiw0SjzKQvPS_ROgeS-NERFeqmKSMjAqs93w@mail.gmail.com> <CABPQxsthk-1JFgU9uzJz0KCv5pdKKbpN+hbs+LfLHkeCWV8CkQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

For this particular issue, it would be good to know if Hadoop provides =
an API to determine the Hadoop version. If not, maybe that can be added =
to Hadoop in its next release, and we can check for it with reflection. =
We recently added a SparkContext.version() method in Spark to let you =
tell the version.

Matei

On Jul 27, 2014, at 12:19 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:

> Hey Ted,
>=20
> We always intend Spark to work with the newer Hadoop versions and
> encourage Spark users to use the newest Hadoop versions for best
> performance.
>=20
> We do try to be liberal in terms of supporting older versions as well.
> This is because many people run older HDFS versions and we want Spark
> to read and write data from them. So far we've been willing to do this
> despite some maintenance cost.
>=20
> The reason is that for many users it's very expensive to do a
> whole-sale upgrade of HDFS, but trying out new versions of Spark is
> much easier. For instance, some of the largest scale Spark users run
> fairly old or forked HDFS versions.
>=20
> - Patrick
>=20
> On Sun, Jul 27, 2014 at 12:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> Thanks for replying, Patrick.
>>=20
>> The intention of my first email was for utilizing newer hadoop =
releases for
>> their bug fixes. I am still looking for clean way of passing hadoop =
release
>> version number to individual classes.
>> Using newer hadoop releases would encourage pushing bug fixes / new
>> features upstream. Ultimately Spark code would become cleaner.
>>=20
>> Cheers
>>=20
>> On Sun, Jul 27, 2014 at 8:52 AM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>>=20
>>> Ted - technically I think you are correct, although I wouldn't
>>> recommend disabling this lock. This lock is not expensive (acquired
>>> once per task, as are many other locks already). Also, we've seen =
some
>>> cases where Hadoop concurrency bugs ended up requiring multiple =
fixes
>>> - concurrency of client access is not well tested in the Hadoop
>>> codebase since most of the Hadoop tools to not use concurrent =
access.
>>> So in general it's good to be conservative in what we expect of the
>>> Hadoop client libraries.
>>>=20
>>> If you'd like to discuss this further, please fork a new thread, =
since
>>> this is a vote thread. Thanks!
>>>=20
>>> On Fri, Jul 25, 2014 at 10:14 PM, Ted Yu <yuzhihong@gmail.com> =
wrote:
>>>> HADOOP-10456 is fixed in hadoop 2.4.1
>>>>=20
>>>> Does this mean that synchronization
>>>> on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for =
hadoop
>>>> 2.4.1 ?
>>>>=20
>>>> Cheers
>>>>=20
>>>>=20
>>>> On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell =
<pwendell@gmail.com>
>>> wrote:
>>>>=20
>>>>> The most important issue in this release is actually an ammendment =
to
>>>>> an earlier fix. The original fix caused a deadlock which was a
>>>>> regression from 1.0.0->1.0.1:
>>>>>=20
>>>>> Issue:
>>>>> https://issues.apache.org/jira/browse/SPARK-1097
>>>>>=20
>>>>> 1.0.1 Fix:
>>>>> https://github.com/apache/spark/pull/1273/files (had a deadlock)
>>>>>=20
>>>>> 1.0.2 Fix:
>>>>> https://github.com/apache/spark/pull/1409/files
>>>>>=20
>>>>> I failed to correctly label this on JIRA, but I've updated it!
>>>>>=20
>>>>> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
>>>>> <michael@databricks.com> wrote:
>>>>>> That query is looking at "Fix Version" not "Target Version".  The =
fact
>>>>> that
>>>>>> the first one is still open is only because the bug is not =
resolved in
>>>>>> master.  It is fixed in 1.0.2.  The second one is partially fixed =
in
>>>>> 1.0.2,
>>>>>> but is not worth blocking the release for.
>>>>>>=20
>>>>>>=20
>>>>>> On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
>>>>>> nicholas.chammas@gmail.com> wrote:
>>>>>>=20
>>>>>>> TD, there are a couple of unresolved issues slated for 1.0.2
>>>>>>> <
>>>>>>>=20
>>>>>=20
>>> =
https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20=
BY%20priority%20DESC
>>>>>>>> .
>>>>>>> Should they be edited somehow?
>>>>>>>=20
>>>>>>>=20
>>>>>>> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
>>>>>>> tathagata.das1565@gmail.com>
>>>>>>> wrote:
>>>>>>>=20
>>>>>>>> Please vote on releasing the following candidate as Apache =
Spark
>>>>> version
>>>>>>>> 1.0.2.
>>>>>>>>=20
>>>>>>>> This release fixes a number of bugs in Spark 1.0.1.
>>>>>>>> Some of the notable ones are
>>>>>>>> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted =
fix
>>> for
>>>>>>>> SPARK-1199. The fix was reverted for 1.0.2.
>>>>>>>> - SPARK-2576: NoClassDefFoundError when executing Spark QL =
query on
>>>>>>>> HDFS CSV file.
>>>>>>>> The full list is at http://s.apache.org/9NJ
>>>>>>>>=20
>>>>>>>> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>>>>>>>>=20
>>>>>>>>=20
>>>>>>>=20
>>>>>=20
>>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D8fb6f=
00e195fb258f3f70f04756e07c259a2351f
>>>>>>>>=20
>>>>>>>> The release files, including signatures, digests, etc can be =
found
>>> at:
>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>>>>>>>>=20
>>>>>>>> Release artifacts are signed with the following key:
>>>>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>>>>=20
>>>>>>>> The staging repository for this release can be found at:
>>>>>>>>=20
>>>>> =
https://repository.apache.org/content/repositories/orgapachespark-1024/
>>>>>>>>=20
>>>>>>>> The documentation corresponding to this release can be found =
at:
>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>>>>>>>>=20
>>>>>>>> Please vote on releasing this package as Apache Spark 1.0.2!
>>>>>>>>=20
>>>>>>>> The vote is open until Tuesday, July 29, at 23:00 UTC and =
passes if
>>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.2
>>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>>=20
>>>>>>>> To learn more about Apache Spark, please see
>>>>>>>> http://spark.apache.org/
>>>>>>>>=20
>>>>>>>=20
>>>>>=20
>>>=20


From dev-return-8561-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 20:58:17 2014
Return-Path: <dev-return-8561-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3548111EC6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 20:58:17 +0000 (UTC)
Received: (qmail 17931 invoked by uid 500); 27 Jul 2014 20:58:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17869 invoked by uid 500); 27 Jul 2014 20:58:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17856 invoked by uid 99); 27 Jul 2014 20:58:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 20:58:16 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.181 as permitted sender)
Received: from [209.85.220.181] (HELO mail-vc0-f181.google.com) (209.85.220.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 20:58:13 +0000
Received: by mail-vc0-f181.google.com with SMTP id lf12so10074174vcb.26
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 13:57:48 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type:content-transfer-encoding;
        bh=qY9G9xidTZ4nSXLqLRFBlKc15VATmDOKInVs2qkXDnE=;
        b=YdQGJ0CDbalz8EpxID55ctp9SgvXlYFUVZqfUfryVRYf9Jn4ektaR3G/EzTTMynhVP
         xPysXsJVL2C+Qc3KO4BisTQjmK5jw8rs+ckGfpuNHIEx/iDZ87DcLdWQHnrRVBImXLtg
         5TWN0wilBSkOF7BtZinwv/W1r8uq3/9SHRXbQT23GY1PHHYNu0FGvVCiPOBvH0VGEmAJ
         VJQwLCIA69wWZqWYySmwwu6AIYq6BgdkWM/6k76v0QC7PXTzqq/J01VABSH7GMywGWPA
         lHOzqC/myHa2o4YkxkFq29oRSdpjYcoVSzdjm5rKUv646SvGdiwjDzSo5FXx1Uy6B0bA
         BNjQ==
X-Gm-Message-State: ALoCoQmuT+VyrnapUtiAIkas1epYpy2UeFp8kiZi25/5REQ37Z3LkI0I+zyBDhtK7LcDmWqin3ma
X-Received: by 10.52.249.9 with SMTP id yq9mr32203092vdc.6.1406494668565; Sun,
 27 Jul 2014 13:57:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Sun, 27 Jul 2014 13:57:28 -0700 (PDT)
In-Reply-To: <B01A3EB3-9CE8-49C4-80DC-F68845189E1C@gmail.com>
References: <CALte62zBjT6hNdNiw0SjzKQvPS_ROgeS-NERFeqmKSMjAqs93w@mail.gmail.com>
 <CABPQxsthk-1JFgU9uzJz0KCv5pdKKbpN+hbs+LfLHkeCWV8CkQ@mail.gmail.com> <B01A3EB3-9CE8-49C4-80DC-F68845189E1C@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Sun, 27 Jul 2014 21:57:28 +0100
Message-ID: <CAMAsSdLHmmprM_vWXLCd5o492+gWVNVn7dxEH05n7Tvn_uBDeQ@mail.gmail.com>
Subject: Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark
 1.0.2 (RC1)
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Good idea, although it gets difficult in the context of multiple
distributions. Say change X is not present in version A, but present
in version B. If you depend on X, what version can you look for to
detect it? The distribution will return "A" or "A+X" or somesuch, but
testing for "A" will give an incorrect answer, and the code can't be
expected to look for everyone's "A+X" versions. Actually inspecting
the code is more robust if a bit messier.

On Sun, Jul 27, 2014 at 9:50 PM, Matei Zaharia <matei.zaharia@gmail.com> wr=
ote:
> For this particular issue, it would be good to know if Hadoop provides an=
 API to determine the Hadoop version. If not, maybe that can be added to Ha=
doop in its next release, and we can check for it with reflection. We recen=
tly added a SparkContext.version() method in Spark to let you tell the vers=
ion.
>
> Matei
>
> On Jul 27, 2014, at 12:19 PM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> Hey Ted,
>>
>> We always intend Spark to work with the newer Hadoop versions and
>> encourage Spark users to use the newest Hadoop versions for best
>> performance.
>>
>> We do try to be liberal in terms of supporting older versions as well.
>> This is because many people run older HDFS versions and we want Spark
>> to read and write data from them. So far we've been willing to do this
>> despite some maintenance cost.
>>
>> The reason is that for many users it's very expensive to do a
>> whole-sale upgrade of HDFS, but trying out new versions of Spark is
>> much easier. For instance, some of the largest scale Spark users run
>> fairly old or forked HDFS versions.
>>
>> - Patrick
>>
>> On Sun, Jul 27, 2014 at 12:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>> Thanks for replying, Patrick.
>>>
>>> The intention of my first email was for utilizing newer hadoop releases=
 for
>>> their bug fixes. I am still looking for clean way of passing hadoop rel=
ease
>>> version number to individual classes.
>>> Using newer hadoop releases would encourage pushing bug fixes / new
>>> features upstream. Ultimately Spark code would become cleaner.
>>>
>>> Cheers
>>>
>>> On Sun, Jul 27, 2014 at 8:52 AM, Patrick Wendell <pwendell@gmail.com> w=
rote:
>>>
>>>> Ted - technically I think you are correct, although I wouldn't
>>>> recommend disabling this lock. This lock is not expensive (acquired
>>>> once per task, as are many other locks already). Also, we've seen some
>>>> cases where Hadoop concurrency bugs ended up requiring multiple fixes
>>>> - concurrency of client access is not well tested in the Hadoop
>>>> codebase since most of the Hadoop tools to not use concurrent access.
>>>> So in general it's good to be conservative in what we expect of the
>>>> Hadoop client libraries.
>>>>
>>>> If you'd like to discuss this further, please fork a new thread, since
>>>> this is a vote thread. Thanks!
>>>>
>>>> On Fri, Jul 25, 2014 at 10:14 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>>>> HADOOP-10456 is fixed in hadoop 2.4.1
>>>>>
>>>>> Does this mean that synchronization
>>>>> on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for had=
oop
>>>>> 2.4.1 ?
>>>>>
>>>>> Cheers
>>>>>
>>>>>
>>>>> On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.com>
>>>> wrote:
>>>>>
>>>>>> The most important issue in this release is actually an ammendment t=
o
>>>>>> an earlier fix. The original fix caused a deadlock which was a
>>>>>> regression from 1.0.0->1.0.1:
>>>>>>
>>>>>> Issue:
>>>>>> https://issues.apache.org/jira/browse/SPARK-1097
>>>>>>
>>>>>> 1.0.1 Fix:
>>>>>> https://github.com/apache/spark/pull/1273/files (had a deadlock)
>>>>>>
>>>>>> 1.0.2 Fix:
>>>>>> https://github.com/apache/spark/pull/1409/files
>>>>>>
>>>>>> I failed to correctly label this on JIRA, but I've updated it!
>>>>>>
>>>>>> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
>>>>>> <michael@databricks.com> wrote:
>>>>>>> That query is looking at "Fix Version" not "Target Version".  The f=
act
>>>>>> that
>>>>>>> the first one is still open is only because the bug is not resolved=
 in
>>>>>>> master.  It is fixed in 1.0.2.  The second one is partially fixed i=
n
>>>>>> 1.0.2,
>>>>>>> but is not worth blocking the release for.
>>>>>>>
>>>>>>>
>>>>>>> On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
>>>>>>> nicholas.chammas@gmail.com> wrote:
>>>>>>>
>>>>>>>> TD, there are a couple of unresolved issues slated for 1.0.2
>>>>>>>> <
>>>>>>>>
>>>>>>
>>>> https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20A=
ND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER=
%20BY%20priority%20DESC
>>>>>>>>> .
>>>>>>>> Should they be edited somehow?
>>>>>>>>
>>>>>>>>
>>>>>>>> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
>>>>>>>> tathagata.das1565@gmail.com>
>>>>>>>> wrote:
>>>>>>>>
>>>>>>>>> Please vote on releasing the following candidate as Apache Spark
>>>>>> version
>>>>>>>>> 1.0.2.
>>>>>>>>>
>>>>>>>>> This release fixes a number of bugs in Spark 1.0.1.
>>>>>>>>> Some of the notable ones are
>>>>>>>>> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix
>>>> for
>>>>>>>>> SPARK-1199. The fix was reverted for 1.0.2.
>>>>>>>>> - SPARK-2576: NoClassDefFoundError when executing Spark QL query =
on
>>>>>>>>> HDFS CSV file.
>>>>>>>>> The full list is at http://s.apache.org/9NJ
>>>>>>>>>
>>>>>>>>> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>>>>>>>>>
>>>>>>>>>
>>>>>>>>
>>>>>>
>>>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D8=
fb6f00e195fb258f3f70f04756e07c259a2351f
>>>>>>>>>
>>>>>>>>> The release files, including signatures, digests, etc can be foun=
d
>>>> at:
>>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>>>>>>>>>
>>>>>>>>> Release artifacts are signed with the following key:
>>>>>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>>>>>
>>>>>>>>> The staging repository for this release can be found at:
>>>>>>>>>
>>>>>> https://repository.apache.org/content/repositories/orgapachespark-10=
24/
>>>>>>>>>
>>>>>>>>> The documentation corresponding to this release can be found at:
>>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>>>>>>>>>
>>>>>>>>> Please vote on releasing this package as Apache Spark 1.0.2!
>>>>>>>>>
>>>>>>>>> The vote is open until Tuesday, July 29, at 23:00 UTC and passes =
if
>>>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.2
>>>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>>>
>>>>>>>>> To learn more about Apache Spark, please see
>>>>>>>>> http://spark.apache.org/
>>>>>>>>>
>>>>>>>>
>>>>>>
>>>>
>

From dev-return-8562-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 23:53:08 2014
Return-Path: <dev-return-8562-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A5693100FA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 23:53:08 +0000 (UTC)
Received: (qmail 28346 invoked by uid 500); 27 Jul 2014 23:53:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 28262 invoked by uid 500); 27 Jul 2014 23:53:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 28250 invoked by uid 99); 27 Jul 2014 23:53:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 23:53:06 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.50 as permitted sender)
Received: from [209.85.220.50] (HELO mail-pa0-f50.google.com) (209.85.220.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 23:53:00 +0000
Received: by mail-pa0-f50.google.com with SMTP id et14so9265210pad.23
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 16:52:40 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=AMu2iZ2QIgYCrlNc9/LLEJbXMvZYzmKst3R+JAW2tds=;
        b=yROZSBkS+Cey6S9ohLGh2TWo8trhrQGQfOYy/hueKK9kQcJ9vJhbAlfk1MdjLJYXvV
         5R+Om/OtFp8HbPYrYVaJnKSUKxZg5CPk9GxGY1Bt9nDev6cW4POpUu5rbAecfrNHXKUl
         2XYKCPSkqsCZ+ttr9e8Sma57M+CjzmjdmCxPZ4mp6E9UpQTAgAWMZYq5HMIHfM7AVV0i
         vLrShe1O2e0WFCr1GniQBaqKGmVAcEarNrmt6u+L6IWovWemXTT3TQZufCVzFX3uHi52
         fkikM6++o6Uyt6Rchlxfdd9ydkyCH9zM9VhaqvvDZVMONZa9MSMLp2KzI5BOVTp9svme
         Fbiw==
X-Received: by 10.70.41.110 with SMTP id e14mr34379035pdl.15.1406505159974;
        Sun, 27 Jul 2014 16:52:39 -0700 (PDT)
Received: from [192.168.1.106] (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id re8sm16994539pdb.61.2014.07.27.16.52.36
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 16:52:37 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
Date: Sun, 27 Jul 2014 16:52:35 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <71937D1E-0EC2-413E-A447-D60B49D16631@gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested this on Mac OS X.

Matei

On Jul 25, 2014, at 4:08 PM, Tathagata Das <tathagata.das1565@gmail.com> =
wrote:

> Please vote on releasing the following candidate as Apache Spark =
version 1.0.2.
>=20
> This release fixes a number of bugs in Spark 1.0.1.
> Some of the notable ones are
> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> SPARK-1199. The fix was reverted for 1.0.2.
> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> HDFS CSV file.
> The full list is at http://s.apache.org/9NJ
>=20
> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D8fb6f=
00e195fb258f3f70f04756e07c259a2351f
>=20
> The release files, including signatures, digests, etc can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>=20
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>=20
> The staging repository for this release can be found at:
> =
https://repository.apache.org/content/repositories/orgapachespark-1024/
>=20
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>=20
> Please vote on releasing this package as Apache Spark 1.0.2!
>=20
> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> [ ] +1 Release this package as Apache Spark 1.0.2
> [ ] -1 Do not release this package because ...
>=20
> To learn more about Apache Spark, please see
> http://spark.apache.org/


From dev-return-8563-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Sun Jul 27 23:54:51 2014
Return-Path: <dev-return-8563-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA99410101
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 27 Jul 2014 23:54:51 +0000 (UTC)
Received: (qmail 30839 invoked by uid 500); 27 Jul 2014 23:54:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30775 invoked by uid 500); 27 Jul 2014 23:54:51 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30763 invoked by uid 99); 27 Jul 2014 23:54:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 23:54:50 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.220.45 as permitted sender)
Received: from [209.85.220.45] (HELO mail-pa0-f45.google.com) (209.85.220.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 27 Jul 2014 23:54:46 +0000
Received: by mail-pa0-f45.google.com with SMTP id eu11so9344418pac.4
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 16:54:26 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=94fSd45C5BKHgk2ZDyf7qEyf5ajKB+3VIA7YO89iP3Q=;
        b=LunDHEZpLIC37mmMSbFT1IbqKWNYEOKN8Rn+5sT5jJ3cshzDv7PvWyU2I9abys/2LP
         3bes5kRGj6TnqkdoXwHZun+npW+DJf1mv505YAOc16FzELiByynoybCrYswMl9keHdDF
         fnduAYtVvlSAG2XLFF9xPIzYe4+JA1K7Ta5QZxWo1+0n6GD2kx50c3ZvPAJ8CgymP6ou
         F5HimAX/9W7iF5nUnEJxJKaxsCfmLisy8lzJXpneqOExkYbl37TRFiLPyeKzCemlpKZf
         q69x1xI5DNMwHFnRJF2g7TtJJV53CpaikXAQMVc1gNH4dqGz9+40HINeXamhETwd7xoK
         R98Q==
X-Received: by 10.70.15.37 with SMTP id u5mr34646217pdc.90.1406505266240;
        Sun, 27 Jul 2014 16:54:26 -0700 (PDT)
Received: from [192.168.1.106] (c-50-174-127-216.hsd1.ca.comcast.net. [50.174.127.216])
        by mx.google.com with ESMTPSA id av2sm15659548pbc.16.2014.07.27.16.54.22
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 16:54:23 -0700 (PDT)
Content-Type: text/plain; charset=us-ascii
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAMAsSdLHmmprM_vWXLCd5o492+gWVNVn7dxEH05n7Tvn_uBDeQ@mail.gmail.com>
Date: Sun, 27 Jul 2014 16:54:21 -0700
Content-Transfer-Encoding: quoted-printable
Message-Id: <6527FEF6-C3D5-4BB2-9153-BC1FBCCFA85B@gmail.com>
References: <CALte62zBjT6hNdNiw0SjzKQvPS_ROgeS-NERFeqmKSMjAqs93w@mail.gmail.com> <CABPQxsthk-1JFgU9uzJz0KCv5pdKKbpN+hbs+LfLHkeCWV8CkQ@mail.gmail.com> <B01A3EB3-9CE8-49C4-80DC-F68845189E1C@gmail.com> <CAMAsSdLHmmprM_vWXLCd5o492+gWVNVn7dxEH05n7Tvn_uBDeQ@mail.gmail.com>
To: dev@spark.apache.org
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

We could also do this, though it would be great if the Hadoop project =
provided this version number as at least a baseline. It's up to =
distributors to decide which version they report but I imagine they =
won't remove stuff that's in the reported version number.

Matei

On Jul 27, 2014, at 1:57 PM, Sean Owen <sowen@cloudera.com> wrote:

> Good idea, although it gets difficult in the context of multiple
> distributions. Say change X is not present in version A, but present
> in version B. If you depend on X, what version can you look for to
> detect it? The distribution will return "A" or "A+X" or somesuch, but
> testing for "A" will give an incorrect answer, and the code can't be
> expected to look for everyone's "A+X" versions. Actually inspecting
> the code is more robust if a bit messier.
>=20
> On Sun, Jul 27, 2014 at 9:50 PM, Matei Zaharia =
<matei.zaharia@gmail.com> wrote:
>> For this particular issue, it would be good to know if Hadoop =
provides an API to determine the Hadoop version. If not, maybe that can =
be added to Hadoop in its next release, and we can check for it with =
reflection. We recently added a SparkContext.version() method in Spark =
to let you tell the version.
>>=20
>> Matei
>>=20
>> On Jul 27, 2014, at 12:19 PM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>>=20
>>> Hey Ted,
>>>=20
>>> We always intend Spark to work with the newer Hadoop versions and
>>> encourage Spark users to use the newest Hadoop versions for best
>>> performance.
>>>=20
>>> We do try to be liberal in terms of supporting older versions as =
well.
>>> This is because many people run older HDFS versions and we want =
Spark
>>> to read and write data from them. So far we've been willing to do =
this
>>> despite some maintenance cost.
>>>=20
>>> The reason is that for many users it's very expensive to do a
>>> whole-sale upgrade of HDFS, but trying out new versions of Spark is
>>> much easier. For instance, some of the largest scale Spark users run
>>> fairly old or forked HDFS versions.
>>>=20
>>> - Patrick
>>>=20
>>> On Sun, Jul 27, 2014 at 12:01 PM, Ted Yu <yuzhihong@gmail.com> =
wrote:
>>>> Thanks for replying, Patrick.
>>>>=20
>>>> The intention of my first email was for utilizing newer hadoop =
releases for
>>>> their bug fixes. I am still looking for clean way of passing hadoop =
release
>>>> version number to individual classes.
>>>> Using newer hadoop releases would encourage pushing bug fixes / new
>>>> features upstream. Ultimately Spark code would become cleaner.
>>>>=20
>>>> Cheers
>>>>=20
>>>> On Sun, Jul 27, 2014 at 8:52 AM, Patrick Wendell =
<pwendell@gmail.com> wrote:
>>>>=20
>>>>> Ted - technically I think you are correct, although I wouldn't
>>>>> recommend disabling this lock. This lock is not expensive =
(acquired
>>>>> once per task, as are many other locks already). Also, we've seen =
some
>>>>> cases where Hadoop concurrency bugs ended up requiring multiple =
fixes
>>>>> - concurrency of client access is not well tested in the Hadoop
>>>>> codebase since most of the Hadoop tools to not use concurrent =
access.
>>>>> So in general it's good to be conservative in what we expect of =
the
>>>>> Hadoop client libraries.
>>>>>=20
>>>>> If you'd like to discuss this further, please fork a new thread, =
since
>>>>> this is a vote thread. Thanks!
>>>>>=20
>>>>> On Fri, Jul 25, 2014 at 10:14 PM, Ted Yu <yuzhihong@gmail.com> =
wrote:
>>>>>> HADOOP-10456 is fixed in hadoop 2.4.1
>>>>>>=20
>>>>>> Does this mean that synchronization
>>>>>> on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for =
hadoop
>>>>>> 2.4.1 ?
>>>>>>=20
>>>>>> Cheers
>>>>>>=20
>>>>>>=20
>>>>>> On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell =
<pwendell@gmail.com>
>>>>> wrote:
>>>>>>=20
>>>>>>> The most important issue in this release is actually an =
ammendment to
>>>>>>> an earlier fix. The original fix caused a deadlock which was a
>>>>>>> regression from 1.0.0->1.0.1:
>>>>>>>=20
>>>>>>> Issue:
>>>>>>> https://issues.apache.org/jira/browse/SPARK-1097
>>>>>>>=20
>>>>>>> 1.0.1 Fix:
>>>>>>> https://github.com/apache/spark/pull/1273/files (had a deadlock)
>>>>>>>=20
>>>>>>> 1.0.2 Fix:
>>>>>>> https://github.com/apache/spark/pull/1409/files
>>>>>>>=20
>>>>>>> I failed to correctly label this on JIRA, but I've updated it!
>>>>>>>=20
>>>>>>> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
>>>>>>> <michael@databricks.com> wrote:
>>>>>>>> That query is looking at "Fix Version" not "Target Version".  =
The fact
>>>>>>> that
>>>>>>>> the first one is still open is only because the bug is not =
resolved in
>>>>>>>> master.  It is fixed in 1.0.2.  The second one is partially =
fixed in
>>>>>>> 1.0.2,
>>>>>>>> but is not worth blocking the release for.
>>>>>>>>=20
>>>>>>>>=20
>>>>>>>> On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
>>>>>>>> nicholas.chammas@gmail.com> wrote:
>>>>>>>>=20
>>>>>>>>> TD, there are a couple of unresolved issues slated for 1.0.2
>>>>>>>>> <
>>>>>>>>>=20
>>>>>>>=20
>>>>> =
https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%20AND%2=
0fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20ORDER%20=
BY%20priority%20DESC
>>>>>>>>>> .
>>>>>>>>> Should they be edited somehow?
>>>>>>>>>=20
>>>>>>>>>=20
>>>>>>>>> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
>>>>>>>>> tathagata.das1565@gmail.com>
>>>>>>>>> wrote:
>>>>>>>>>=20
>>>>>>>>>> Please vote on releasing the following candidate as Apache =
Spark
>>>>>>> version
>>>>>>>>>> 1.0.2.
>>>>>>>>>>=20
>>>>>>>>>> This release fixes a number of bugs in Spark 1.0.1.
>>>>>>>>>> Some of the notable ones are
>>>>>>>>>> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted =
fix
>>>>> for
>>>>>>>>>> SPARK-1199. The fix was reverted for 1.0.2.
>>>>>>>>>> - SPARK-2576: NoClassDefFoundError when executing Spark QL =
query on
>>>>>>>>>> HDFS CSV file.
>>>>>>>>>> The full list is at http://s.apache.org/9NJ
>>>>>>>>>>=20
>>>>>>>>>> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>>>>>>>>>>=20
>>>>>>>>>>=20
>>>>>>>>>=20
>>>>>>>=20
>>>>> =
https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D8fb6f=
00e195fb258f3f70f04756e07c259a2351f
>>>>>>>>>>=20
>>>>>>>>>> The release files, including signatures, digests, etc can be =
found
>>>>> at:
>>>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>>>>>>>>>>=20
>>>>>>>>>> Release artifacts are signed with the following key:
>>>>>>>>>> https://people.apache.org/keys/committer/tdas.asc
>>>>>>>>>>=20
>>>>>>>>>> The staging repository for this release can be found at:
>>>>>>>>>>=20
>>>>>>> =
https://repository.apache.org/content/repositories/orgapachespark-1024/
>>>>>>>>>>=20
>>>>>>>>>> The documentation corresponding to this release can be found =
at:
>>>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>>>>>>>>>>=20
>>>>>>>>>> Please vote on releasing this package as Apache Spark 1.0.2!
>>>>>>>>>>=20
>>>>>>>>>> The vote is open until Tuesday, July 29, at 23:00 UTC and =
passes if
>>>>>>>>>> a majority of at least 3 +1 PMC votes are cast.
>>>>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.2
>>>>>>>>>> [ ] -1 Do not release this package because ...
>>>>>>>>>>=20
>>>>>>>>>> To learn more about Apache Spark, please see
>>>>>>>>>> http://spark.apache.org/
>>>>>>>>>>=20
>>>>>>>>>=20
>>>>>>>=20
>>>>>=20
>>=20


From dev-return-8564-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 00:15:13 2014
Return-Path: <dev-return-8564-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AD8751015A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 00:15:13 +0000 (UTC)
Received: (qmail 48759 invoked by uid 500); 28 Jul 2014 00:15:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48693 invoked by uid 500); 28 Jul 2014 00:15:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 48681 invoked by uid 99); 28 Jul 2014 00:15:12 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 00:15:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.213.177 as permitted sender)
Received: from [209.85.213.177] (HELO mail-ig0-f177.google.com) (209.85.213.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 00:15:07 +0000
Received: by mail-ig0-f177.google.com with SMTP id hn18so2954683igb.4
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 17:14:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:subject:mime-version:content-type;
        bh=5F22NaznPEEVk4Zd5L6DKsSBNsiznXoiNPqZgqefxh0=;
        b=iaZmrkjk0FyvGtzMsHMMfkrsqtnoG/qT+zN9MV0n10FhWNVTUFKXsovmDG4FnB94Sa
         beVzoaigrEoEijtg0LAfxOJN3Sk1ZX7YO3CjvEE6/KbVeV4p01KmP/ZfsLfUXPh4/isJ
         tU8ZEOn7q9nR79NAE7tm1wD2ENHWg4JAcDQufOv21mQ/G2yCnJtZiwvgK29UVMkD1vPm
         ARq2y88NXzS7GjObjxKyLjMj5DhmWN+ULxQjAzALMcxXIJ1lAZFCKJQdBTQFZm71fAJZ
         pmPVgWan18d/Uz/95x4XkaAx5TOlOlT+hfPAh0nb5CZX/myvwZ2I0GIK1aQSeuMCPzZM
         qAvw==
X-Received: by 10.42.90.74 with SMTP id j10mr38585081icm.29.1406506486891;
        Sun, 27 Jul 2014 17:14:46 -0700 (PDT)
Received: from [192.168.2.17] ([69.157.143.63])
        by mx.google.com with ESMTPSA id z12sm21144761igu.11.2014.07.27.17.14.46
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 17:14:46 -0700 (PDT)
Date: Sun, 27 Jul 2014 20:26:12 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Message-ID: <C3014F7E1AF2489A9E7B43F7D9B8335F@gmail.com>
Subject: new JDBC server test cases seems failed ?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53d598a4_1d545c4d_1b8"
X-Virus-Checked: Checked by ClamAV on apache.org

--53d598a4_1d545c4d_1b8
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

Hi, all

It seems that the JDBC test cases are failed unexpectedly in Jenkins?


[info] - test query execution against a Hive Thrift server *** FAILED *** [info] java.sql.SQLException: Could not open connection to jdbc:hive2://localhost:45518/: java.net.ConnectException: Connection refused [info] at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146) [info] at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info] at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:110) [info] at org.apache.spark.sql.hive.thri
ftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107) [info] ... [info] Cause: org.apache.thrift.transport.TTransportException: java.net.ConnectException: Connection refused [info] at org.apache.thrift.transport.TSocket.open(TSocket.java:185) [info] at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248) [info] at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [info] at org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144) [info] at org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info] at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at org.apache.spark.sql.hive.thriftserver.H
iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131) [info] at org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134) [info] ... [info] Cause: java.net.ConnectException: Connection refused [info] at java.net.PlainSocketImpl.socketConnect(Native Method) [info] at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339) [info] at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200) [info] at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182) [info] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [info] at java.net.Socket.connect(Socket.java:579) [info] at org.apache.thrift.transport.TSocket.open(TSocket.java:180) [info] at org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248) [info] at org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37) [info] at org.apache.hive.jdbc.HiveConn
ection.openTransport(HiveConnection.java:144) [info] ... [info] CliSuite: Executing: create table hive_test1(key int, val string);, expecting output: OK [warn] four warnings found [warn] Note: /home/jenkins/workspace/SparkPullRequestBuilder@4/core/src/test/java/org/apache/spark/JavaAPISuite.java uses or overrides a deprecated API. [warn] Note: Recompile with -Xlint:deprecation for details. [info] - simple commands *** FAILED *** [info] java.lang.AssertionError: assertion failed: Didn't find "OK" in the output: [info] at scala.Predef$.assert(Predef.scala:179) [info] at org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUtils.scala:70) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scala:25) [info] at org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUtils.scala:62) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scala:25) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite
$$anonfun$1.apply$mcV$sp(CliSuite.scala:53) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51) [info] at org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51) [info] at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22) [info] at org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22) [log4j:WARN No appenders could be found for logger (org.apache.hadoop.util.Shell). log4j:WARN Please initialize the log4j system properly. log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info. 14/07/27 17:06:43 INFO ClientBase: Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties info] ... [info] ScalaTest [info] Run completed in 41 seconds, 789 milliseconds. [info] Total number of tests run: 2 [info] Suites: completed 2, aborted 0 [info] Tests: succeeded 0, failed 2, canceled 0, ignored 0, pending 0 [info] *** 2 TESTS FAILED ***

Best, 

-- 
Nan Zhu
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)


--53d598a4_1d545c4d_1b8--


From dev-return-8565-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 00:54:29 2014
Return-Path: <dev-return-8565-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5EB181023F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 00:54:29 +0000 (UTC)
Received: (qmail 93052 invoked by uid 500); 28 Jul 2014 00:54:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92984 invoked by uid 500); 28 Jul 2014 00:54:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92973 invoked by uid 99); 28 Jul 2014 00:54:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 00:54:27 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.47] (HELO mail-qg0-f47.google.com) (209.85.192.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 00:54:25 +0000
Received: by mail-qg0-f47.google.com with SMTP id i50so7806971qgf.34
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 17:53:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=ZlSuwBIRzLag3v7YfGUcklELaaLxKBsSksM+oRQ6LJg=;
        b=GzApDyOVn+1WT3JYoxcd++U/W/bbHFqozOM7YP34a/C+l1i6B4o5kHoB3YmeOZvEQw
         +euJW68DdjLEccNdHP0UIr4g3dwYG7LAiFJwSSMVsGj1+2mPcdxpzIvh9EwoHJ2mZXVk
         jcu6vRslKAXQsdDfxmMeUwWKZN0kJvDx0c8mvjlQIBEuIVCEPT/idiF/HiK156bOfFaK
         6phTrpf1q1VrULy3N7z5na6/tcu8O6TGjIEdJPoiFVJH5x4t0+fq+tu77Deqvp/GLvs7
         W3SjGCZ6+Tbj5A0Nh9N9bz5yymQMlcMURfwJ0xA1281bL/0C2lG7kXlwuK+fdfWCD350
         yu3A==
X-Gm-Message-State: ALoCoQlEwh9CFldA6Z0Bjn8rt0kBlcS/TCEcZQLXD7KbxcJ3vY/zMFwWFYqooLjGRPTozeJ84Kxk
X-Received: by 10.224.163.83 with SMTP id z19mr53632609qax.68.1406508839561;
 Sun, 27 Jul 2014 17:53:59 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.206.9 with HTTP; Sun, 27 Jul 2014 17:53:39 -0700 (PDT)
In-Reply-To: <C3014F7E1AF2489A9E7B43F7D9B8335F@gmail.com>
References: <C3014F7E1AF2489A9E7B43F7D9B8335F@gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Sun, 27 Jul 2014 17:53:39 -0700
Message-ID: <CAAswR-5w_kLPNCKKaWgOtf1EumY9A7dkqP+xa89CBkctB05w=Q@mail.gmail.com>
Subject: Re: new JDBC server test cases seems failed ?
To: dev@spark.apache.org, Cheng Lian <lian@databricks.com>, 
	Patrick Wendell <patrick@databricks.com>
Content-Type: multipart/alternative; boundary=089e01294f66c1887704ff365bac
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01294f66c1887704ff365bac
Content-Type: text/plain; charset=UTF-8

How recent is this? We've already reverted this patch once due to failing
tests.  It would be helpful to include a link to the failed build.  If its
failing again we'll have to revert again.


On Sun, Jul 27, 2014 at 5:26 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:

> Hi, all
>
> It seems that the JDBC test cases are failed unexpectedly in Jenkins?
>
>
> [info] - test query execution against a Hive Thrift server *** FAILED ***
> [info] java.sql.SQLException: Could not open connection to
> jdbc:hive2://localhost:45518/: java.net.ConnectException: Connection
> refused [info] at
> org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146)
> [info] at
> org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info]
> at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at
> java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at
> java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at
> org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)
> [info] at
> org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134)
> [info] at
> org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:110)
> [info] at org.apache.spark.sql.hive.thri
> ftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107)
> [info] at
> org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107)
> [info] ... [info] Cause: org.apache.thrift.transport.TTransportException:
> java.net.ConnectException: Connection refused [info] at
> org.apache.thrift.transport.TSocket.open(TSocket.java:185) [info] at
> org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
> [info] at
> org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
> [info] at
> org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)
> [info] at
> org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info]
> at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at
> java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at
> java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at
> org.apache.spark.sql.hive.thriftserver.H
> iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)
> [info] at
> org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134)
> [info] ... [info] Cause: java.net.ConnectException: Connection refused
> [info] at java.net.PlainSocketImpl.socketConnect(Native Method) [info] at
> java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
> [info] at
> java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
> [info] at
> java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
> [info] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [info]
> at java.net.Socket.connect(Socket.java:579) [info] at
> org.apache.thrift.transport.TSocket.open(TSocket.java:180) [info] at
> org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
> [info] at
> org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
> [info] at org.apache.hive.jdbc.HiveConn
> ection.openTransport(HiveConnection.java:144) [info] ... [info] CliSuite:
> Executing: create table hive_test1(key int, val string);, expecting output:
> OK [warn] four warnings found [warn] Note:
> /home/jenkins/workspace/SparkPullRequestBuilder@4/core/src/test/java/org/apache/spark/JavaAPISuite.java
> uses or overrides a deprecated API. [warn] Note: Recompile with
> -Xlint:deprecation for details. [info] - simple commands *** FAILED ***
> [info] java.lang.AssertionError: assertion failed: Didn't find "OK" in the
> output: [info] at scala.Predef$.assert(Predef.scala:179) [info] at
> org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUtils.scala:70)
> [info] at
> org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scala:25)
> [info] at
> org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUtils.scala:62)
> [info] at
> org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scala:25)
> [info] at org.apache.spark.sql.hive.thriftserver.CliSuite
> $$anonfun$1.apply$mcV$sp(CliSuite.scala:53) [info] at
> org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51)
> [info] at
> org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51)
> [info] at
> org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
> [info] at
> org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
> [log4j:WARN No appenders could be found for logger
> (org.apache.hadoop.util.Shell). log4j:WARN Please initialize the log4j
> system properly. log4j:WARN See
> http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
> 14/07/27 17:06:43 INFO ClientBase: Using Spark's default log4j profile:
> org/apache/spark/log4j-defaults.properties info] ... [info] ScalaTest
> [info] Run completed in 41 seconds, 789 milliseconds. [info] Total number
> of tests run: 2 [info] Suites: completed 2, aborted 0 [info] Tests:
> succeeded 0, failed 2, canceled 0, ignored 0, pending 0 [info] *** 2 TESTS
> FAILED ***
>
> Best,
>
> --
> Nan Zhu
> Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
>
>

--089e01294f66c1887704ff365bac--

From dev-return-8566-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 00:55:53 2014
Return-Path: <dev-return-8566-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4672410246
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 00:55:53 +0000 (UTC)
Received: (qmail 96815 invoked by uid 500); 28 Jul 2014 00:55:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96754 invoked by uid 500); 28 Jul 2014 00:55:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96740 invoked by uid 99); 28 Jul 2014 00:55:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 00:55:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zhunanmcgill@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 00:55:49 +0000
Received: by mail-ie0-f181.google.com with SMTP id rp18so5937737iec.12
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 17:55:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:cc:message-id:in-reply-to:references:subject
         :mime-version:content-type;
        bh=OcfO+5DS1cIwySDx/x1rOrAyO7tNv84UGg9u5oDfwxA=;
        b=jCQcXI+L19f4yZYSDnBnNIV4lD5p5Y/umAPqN/uEVy8aHBuoGZERmw5bJ1+qkbLs2p
         NWeSohUjpiAxLoGuEXO1LwhqO6NlxophaAlE48IWvF16vMOojb0O0RtYVTQLkFxPxTeE
         54TTERj1l18gs6MBSZ7PFfuibog9+3gyXV/yIc/kTmQrmijkLT4fkxCVLNt3cPpxN+ld
         lAtP1Z3X4f1MERuWwozYA9ZuRgsk6rzghYIFAk1/K0jbOmzvkMc0/C0QwlPoPourEvle
         wyrhnvtTIH+IVA4uTstn+9BPhpoK87vH4BBJdhlBOFBUAw0yNwjP4Xh0qQ910/X79lhZ
         jFzA==
X-Received: by 10.50.61.148 with SMTP id p20mr19653797igr.44.1406508924023;
        Sun, 27 Jul 2014 17:55:24 -0700 (PDT)
Received: from [192.168.2.17] ([69.157.143.63])
        by mx.google.com with ESMTPSA id kw1sm21466042igb.2.2014.07.27.17.55.23
        for <multiple recipients>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 17:55:23 -0700 (PDT)
Date: Sun, 27 Jul 2014 21:06:48 -0400
From: Nan Zhu <zhunanmcgill@gmail.com>
To: dev@spark.apache.org
Cc: Cheng Lian <lian@databricks.com>, Patrick Wendell
 <patrick@databricks.com>
Message-ID: <F9B23E166F854EDABEBC9B6EA8FE2D14@gmail.com>
In-Reply-To: <CAAswR-5w_kLPNCKKaWgOtf1EumY9A7dkqP+xa89CBkctB05w=Q@mail.gmail.com>
References: <C3014F7E1AF2489A9E7B43F7D9B8335F@gmail.com>
 <CAAswR-5w_kLPNCKKaWgOtf1EumY9A7dkqP+xa89CBkctB05w=Q@mail.gmail.com>
Subject: Re: new JDBC server test cases seems failed ?
X-Mailer: sparrow 1.6.4 (build 1176)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53d5a228_2a155dbc_1b8"
X-Virus-Checked: Checked by ClamAV on apache.org

--53d5a228_2a155dbc_1b8
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: quoted-printable
Content-Disposition: inline

it=E2=80=99s 20 minutes ago =20

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17259/=
console=46ull =20

-- =20
Nan Zhu


On Sunday, July 27, 2014 at 8:53 PM, Michael Armbrust wrote:

> How recent is this=3F We've already reverted this patch once due to fai=
ling
> tests. It would be helpful to include a link to the failed build. If it=
s
> failing again we'll have to revert again.
> =20
> =20
> On Sun, Jul 27, 2014 at 5:26 PM, Nan Zhu <zhunanmcgill=40gmail.com (mai=
lto:zhunanmcgill=40gmail.com)> wrote:
> =20
> > Hi, all
> > =20
> > It seems that the JDBC test cases are failed unexpectedly in Jenkins=3F=

> > =20
> > =20
> > =5Binfo=5D - test query execution against a Hive Thrift server *** =46=
AILED ***
> > =5Binfo=5D java.sql.SQLException: Could not open connection to
> > jdbc:hive2://localhost:45518/: java.net.ConnectException: Connection
> > refused =5Binfo=5D at
> > org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java=
:146)
> > =5Binfo=5D at
> > org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) =5B=
info=5D
> > at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) =5Bin=
fo=5D at
> > java.sql.DriverManager.getConnection(DriverManager.java:571) =5Binfo=5D=
 at
> > java.sql.DriverManager.getConnection(DriverManager.java:215) =5Binfo=5D=
 at
> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConn=
ection(HiveThriftServer2Suite.scala:131)
> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createS=
tatement(HiveThriftServer2Suite.scala:134)
> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite=24=24an=
onfun=241.apply=24mcV=24sp(HiveThriftServer2Suite.scala:110)
> > =5Binfo=5D at org.apache.spark.sql.hive.thri
> > ftserver.HiveThriftServer2Suite=24=24anonfun=241.apply(HiveThriftServ=
er2Suite.scala:107)
> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite=24=24an=
onfun=241.apply(HiveThriftServer2Suite.scala:107)
> > =5Binfo=5D ... =5Binfo=5D Cause: org.apache.thrift.transport.TTranspo=
rtException:
> > java.net.ConnectException: Connection refused =5Binfo=5D at
> > org.apache.thrift.transport.TSocket.open(TSocket.java:185) =5Binfo=5D=
 at
> > org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:2=
48)
> > =5Binfo=5D at
> > org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTran=
sport.java:37)
> > =5Binfo=5D at
> > org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java=
:144)
> > =5Binfo=5D at
> > org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) =5B=
info=5D
> > at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) =5Bin=
fo=5D at
> > java.sql.DriverManager.getConnection(DriverManager.java:571) =5Binfo=5D=
 at
> > java.sql.DriverManager.getConnection(DriverManager.java:215) =5Binfo=5D=
 at
> > org.apache.spark.sql.hive.thriftserver.H
> > iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)=

> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createS=
tatement(HiveThriftServer2Suite.scala:134)
> > =5Binfo=5D ... =5Binfo=5D Cause: java.net.ConnectException: Connectio=
n refused
> > =5Binfo=5D at java.net.PlainSocketImpl.socketConnect(Native Method) =5B=
info=5D at
> > java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.ja=
va:339)
> > =5Binfo=5D at
> > java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocket=
Impl.java:200)
> > =5Binfo=5D at
> > java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java=
:182)
> > =5Binfo=5D at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:3=
92) =5Binfo=5D
> > at java.net.Socket.connect(Socket.java:579) =5Binfo=5D at
> > org.apache.thrift.transport.TSocket.open(TSocket.java:180) =5Binfo=5D=
 at
> > org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:2=
48)
> > =5Binfo=5D at
> > org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTran=
sport.java:37)
> > =5Binfo=5D at org.apache.hive.jdbc.HiveConn
> > ection.openTransport(HiveConnection.java:144) =5Binfo=5D ... =5Binfo=5D=
 CliSuite:
> > Executing: create table hive=5Ftest1(key int, val string);, expecting=
 output:
> > OK =5Bwarn=5D four warnings found =5Bwarn=5D Note:
> > /home/jenkins/workspace/SparkPullRequestBuilder=404/core/src/test/jav=
a/org/apache/spark/JavaAPISuite.java
> > uses or overrides a deprecated API. =5Bwarn=5D Note: Recompile with
> > -Xlint:deprecation for details. =5Binfo=5D - simple commands *** =46A=
ILED ***
> > =5Binfo=5D java.lang.AssertionError: assertion failed: Didn't find =22=
OK=22 in the
> > output: =5Binfo=5D at scala.Predef=24.assert(Predef.scala:179) =5Binf=
o=5D at
> > org.apache.spark.sql.hive.thriftserver.TestUtils=24class.wait=46orQue=
ry(TestUtils.scala:70)
> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.CliSuite.wait=46orQuery(CliSui=
te.scala:25)
> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.TestUtils=24class.executeQuery=
(TestUtils.scala:62)
> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite=
.scala:25)
> > =5Binfo=5D at org.apache.spark.sql.hive.thriftserver.CliSuite
> > =24=24anonfun=241.apply=24mcV=24sp(CliSuite.scala:53) =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.CliSuite=24=24anonfun=241.appl=
y(CliSuite.scala:51)
> > =5Binfo=5D at
> > org.apache.spark.sql.hive.thriftserver.CliSuite=24=24anonfun=241.appl=
y(CliSuite.scala:51)
> > =5Binfo=5D at
> > org.scalatest.Transformer=24=24anonfun=24apply=241.apply(Transformer.=
scala:22)
> > =5Binfo=5D at
> > org.scalatest.Transformer=24=24anonfun=24apply=241.apply(Transformer.=
scala:22)
> > =5Blog4j:WARN No appenders could be found for logger
> > (org.apache.hadoop.util.Shell). log4j:WARN Please initialize the log4=
j
> > system properly. log4j:WARN See
> > http://logging.apache.org/log4j/1.2/faq.html=23noconfig for more info=
.
> > 14/07/27 17:06:43 IN=46O ClientBase: Using Spark's default log4j prof=
ile:
> > org/apache/spark/log4j-defaults.properties info=5D ... =5Binfo=5D Sca=
laTest
> > =5Binfo=5D Run completed in 41 seconds, 789 milliseconds. =5Binfo=5D =
Total number
> > of tests run: 2 =5Binfo=5D Suites: completed 2, aborted 0 =5Binfo=5D =
Tests:
> > succeeded 0, failed 2, canceled 0, ignored 0, pending 0 =5Binfo=5D **=
* 2 TESTS
> > =46AILED ***
> > =20
> > Best,
> > =20
> > --
> > Nan Zhu
> > Sent with Sparrow (http://www.sparrowmailapp.com/=3Fsig)
> > =20
> =20
> =20
> =20



--53d5a228_2a155dbc_1b8--


From dev-return-8567-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 01:47:50 2014
Return-Path: <dev-return-8567-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3F6CB1030D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 01:47:50 +0000 (UTC)
Received: (qmail 47251 invoked by uid 500); 28 Jul 2014 01:47:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47184 invoked by uid 500); 28 Jul 2014 01:47:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 47172 invoked by uid 99); 28 Jul 2014 01:47:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 01:47:48 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 01:47:44 +0000
Received: by mail-oa0-f54.google.com with SMTP id n16so7941468oag.41
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 18:47:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=V/WqNR1Jg5rEFIo4RXb0ewhOFkCjdvF4FFNXxwAi/4w=;
        b=s79DDUEHgPlBgANWjVXxgC7oC2Bs2cQM8RBxGczak6EenMwBnv4aNsNyiIn5H1VT9V
         NY53UmTghitN8PsAp9ZcNSmXaukxCCleGwMqOcw04R2Oymqs4zuj15UPxAK69uR4SEto
         W3zj5bpdvdGBCei1mQTyO4VkCwymsBsX6WdR7ERuJBAKnO2GKkt8+4Xzn/WGrvu9F4Kr
         LkdLWA/i1t3XjoOIbMCL0V8CzfHXyo2Z8wNcyfVTfqjhMwBTUFAXcvoOZEhujsP3iCcL
         TDMUon1Xsr2iC/pRNg4wSD0dGUSI9NpWmPySa5q3WfA4EY45aMImlEA6yR+6KFE5swYK
         UReQ==
MIME-Version: 1.0
X-Received: by 10.60.120.98 with SMTP id lb2mr44485790oeb.52.1406512044146;
 Sun, 27 Jul 2014 18:47:24 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 18:47:24 -0700 (PDT)
In-Reply-To: <F9B23E166F854EDABEBC9B6EA8FE2D14@gmail.com>
References: <C3014F7E1AF2489A9E7B43F7D9B8335F@gmail.com>
	<CAAswR-5w_kLPNCKKaWgOtf1EumY9A7dkqP+xa89CBkctB05w=Q@mail.gmail.com>
	<F9B23E166F854EDABEBC9B6EA8FE2D14@gmail.com>
Date: Sun, 27 Jul 2014 18:47:24 -0700
Message-ID: <CABPQxsuyTj3WT1SN1Jf9pJ7GdJL1r8cVQGcSwy8-3BXciE69Vg@mail.gmail.com>
Subject: Re: new JDBC server test cases seems failed ?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: Cheng Lian <lian@databricks.com>, Patrick Wendell <patrick@databricks.com>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I'm going to revert it again - Cheng can you try to look into this? Thanks.

On Sun, Jul 27, 2014 at 6:06 PM, Nan Zhu <zhunanmcgill@gmail.com> wrote:
> it's 20 minutes ago
>
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17259/consoleFull
>
> --
> Nan Zhu
>
>
> On Sunday, July 27, 2014 at 8:53 PM, Michael Armbrust wrote:
>
>> How recent is this? We've already reverted this patch once due to failing
>> tests. It would be helpful to include a link to the failed build. If its
>> failing again we'll have to revert again.
>>
>>
>> On Sun, Jul 27, 2014 at 5:26 PM, Nan Zhu <zhunanmcgill@gmail.com (mailto:zhunanmcgill@gmail.com)> wrote:
>>
>> > Hi, all
>> >
>> > It seems that the JDBC test cases are failed unexpectedly in Jenkins?
>> >
>> >
>> > [info] - test query execution against a Hive Thrift server *** FAILED ***
>> > [info] java.sql.SQLException: Could not open connection to
>> > jdbc:hive2://localhost:45518/: java.net.ConnectException: Connection
>> > refused [info] at
>> > org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146)
>> > [info] at
>> > org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info]
>> > at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at
>> > java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at
>> > java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at
>> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply$mcV$sp(HiveThriftServer2Suite.scala:110)
>> > [info] at org.apache.spark.sql.hive.thri
>> > ftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.scala:107)
>> > [info] ... [info] Cause: org.apache.thrift.transport.TTransportException:
>> > java.net.ConnectException: Connection refused [info] at
>> > org.apache.thrift.transport.TSocket.open(TSocket.java:185) [info] at
>> > org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
>> > [info] at
>> > org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
>> > [info] at
>> > org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)
>> > [info] at
>> > org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) [info]
>> > at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) [info] at
>> > java.sql.DriverManager.getConnection(DriverManager.java:571) [info] at
>> > java.sql.DriverManager.getConnection(DriverManager.java:215) [info] at
>> > org.apache.spark.sql.hive.thriftserver.H
>> > iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatement(HiveThriftServer2Suite.scala:134)
>> > [info] ... [info] Cause: java.net.ConnectException: Connection refused
>> > [info] at java.net.PlainSocketImpl.socketConnect(Native Method) [info] at
>> > java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
>> > [info] at
>> > java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
>> > [info] at
>> > java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
>> > [info] at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [info]
>> > at java.net.Socket.connect(Socket.java:579) [info] at
>> > org.apache.thrift.transport.TSocket.open(TSocket.java:180) [info] at
>> > org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
>> > [info] at
>> > org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport.java:37)
>> > [info] at org.apache.hive.jdbc.HiveConn
>> > ection.openTransport(HiveConnection.java:144) [info] ... [info] CliSuite:
>> > Executing: create table hive_test1(key int, val string);, expecting output:
>> > OK [warn] four warnings found [warn] Note:
>> > /home/jenkins/workspace/SparkPullRequestBuilder@4/core/src/test/java/org/apache/spark/JavaAPISuite.java
>> > uses or overrides a deprecated API. [warn] Note: Recompile with
>> > -Xlint:deprecation for details. [info] - simple commands *** FAILED ***
>> > [info] java.lang.AssertionError: assertion failed: Didn't find "OK" in the
>> > output: [info] at scala.Predef$.assert(Predef.scala:179) [info] at
>> > org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUtils.scala:70)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scala:25)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUtils.scala:62)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scala:25)
>> > [info] at org.apache.spark.sql.hive.thriftserver.CliSuite
>> > $$anonfun$1.apply$mcV$sp(CliSuite.scala:53) [info] at
>> > org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51)
>> > [info] at
>> > org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.scala:51)
>> > [info] at
>> > org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>> > [info] at
>> > org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>> > [log4j:WARN No appenders could be found for logger
>> > (org.apache.hadoop.util.Shell). log4j:WARN Please initialize the log4j
>> > system properly. log4j:WARN See
>> > http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
>> > 14/07/27 17:06:43 INFO ClientBase: Using Spark's default log4j profile:
>> > org/apache/spark/log4j-defaults.properties info] ... [info] ScalaTest
>> > [info] Run completed in 41 seconds, 789 milliseconds. [info] Total number
>> > of tests run: 2 [info] Suites: completed 2, aborted 0 [info] Tests:
>> > succeeded 0, failed 2, canceled 0, ignored 0, pending 0 [info] *** 2 TESTS
>> > FAILED ***
>> >
>> > Best,
>> >
>> > --
>> > Nan Zhu
>> > Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
>> >
>>
>>
>>
>
>

From dev-return-8568-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 02:08:14 2014
Return-Path: <dev-return-8568-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 24EAC1035D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 02:08:14 +0000 (UTC)
Received: (qmail 62885 invoked by uid 500); 28 Jul 2014 02:08:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62810 invoked by uid 500); 28 Jul 2014 02:08:13 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62797 invoked by uid 99); 28 Jul 2014 02:08:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:08:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.223.177 as permitted sender)
Received: from [209.85.223.177] (HELO mail-ie0-f177.google.com) (209.85.223.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:08:07 +0000
Received: by mail-ie0-f177.google.com with SMTP id at20so5991276iec.36
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 19:07:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=FtHiEMEhIVYkP7uP3nz0Xz1AGcb6Icitr5Gk99/4DHE=;
        b=tDoCnrqWpTZwmPwEmowXeWUJD1Vm0W+99sx64Aof6bmW2Nk/lxLbhMyuqCGqxofLTS
         xmS9aMRQalQvWnP5fKoYFcG1ZnHwJvT9qApUNQ6+PnoHEdAuSLs1jQCMikU45vF7Qomx
         JIlNGXLVBi/vKmyp/nJYPjz+JOOyjiNwjlgr/ZupRV3K01BWvL15gjJjL+Mimr+3W1L0
         5RiqUTvBtVm1kgPz27qGsX15wM36sVuX84PfVC4J6dMKQh6JOyuDd4jFCF99Q3WgmmoU
         H2yTJwcR5OeUNTY9YgYl+R7pyxosR6wOE6f+ASAs0iJ/bsQ9oEC1L2oF8f5vJsgsL51h
         hmdw==
MIME-Version: 1.0
X-Received: by 10.50.152.40 with SMTP id uv8mr27850786igb.40.1406513267194;
 Sun, 27 Jul 2014 19:07:47 -0700 (PDT)
Received: by 10.107.134.203 with HTTP; Sun, 27 Jul 2014 19:07:47 -0700 (PDT)
Date: Sun, 27 Jul 2014 19:07:47 -0700
Message-ID: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
Subject: No such file or directory errors running tests
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01494832a9c2a204ff3763c9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01494832a9c2a204ff3763c9
Content-Type: text/plain; charset=UTF-8

I have pulled latest from github this afternoon.   There are many many
errors:

<source_home>/assembly/target/scala-2.10: No such file or directory

This causes many tests to fail.

Here is the command line I am running

    mvn -Pyarn -Phadoop-2.3 -Phive package test

--089e01494832a9c2a204ff3763c9--

From dev-return-8569-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 02:11:14 2014
Return-Path: <dev-return-8569-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 920131036A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 02:11:14 +0000 (UTC)
Received: (qmail 66705 invoked by uid 500); 28 Jul 2014 02:11:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66644 invoked by uid 500); 28 Jul 2014 02:11:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 66615 invoked by uid 99); 28 Jul 2014 02:11:13 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:11:13 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:11:11 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so7728962qgd.23
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 19:10:46 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=nEy8GK8Wqw82qBwMn6nimYAz9DKf3WSO386kO/bPxno=;
        b=ipXg0SlfBI/uCBT5LX2KNYBX0p8I80CIav9GsGp4SEYWYznittzXwTgkgI2sUPENeU
         yjcoigYnRr5EWRgJp8UfTeB0L5x1N2LqzYAJ593aUPIAaRJU6BRKDcM1UtjNgqxW0Bqs
         c50c9WH7mSIjuI9n7H5TYBuilswQX86NGVy4smxgJY0X4qDq2A1+Cx0JsZzuamKGLG64
         jonjXm23R5avLg4By/fu/qPsUqR+bQ/c1d8O8Bek0yjJDNYV9oAaaqMp11wHma8hhaj4
         BHBoljHvguAlR6FMR8o1dHUP4YkNAkz5XilhG8ryYeER+B7aGbTaXPcQEgTl4iN10od/
         QRMA==
X-Gm-Message-State: ALoCoQlFV5Gvf4TjnHauNueeoxLnY7Fr+jSRXCDI6aYnqDt9ZbMJlWvZ7vVb2HiDNjiwouGPGOjr
X-Received: by 10.140.26.149 with SMTP id 21mr53520700qgv.51.1406513446201;
 Sun, 27 Jul 2014 19:10:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sun, 27 Jul 2014 19:10:26 -0700 (PDT)
In-Reply-To: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
References: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 27 Jul 2014 19:10:26 -0700
Message-ID: <CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com>
Subject: Re: No such file or directory errors running tests
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c032c055415704ff376e6f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c032c055415704ff376e6f
Content-Type: text/plain; charset=UTF-8

To run through all the tests you'd need to create the assembly jar first.


I've seen this asked a few times. Maybe we should make it more obvious.



http://spark.apache.org/docs/latest/building-with-maven.html

Spark Tests in Maven

Tests are run by default via the ScalaTest Maven plugin
<http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin>.
Some of the require Spark to be packaged first, so always run mvn package
 with -DskipTests the first time. You can then run the tests with mvn
-Dhadoop.version=... test.

The ScalaTest plugin also supports running only a specific test suite as
follows:

mvn -Dhadoop.version=... -DwildcardSuites=org.apache.spark.repl.ReplSuite test





On Sun, Jul 27, 2014 at 7:07 PM, Stephen Boesch <javadba@gmail.com> wrote:

> I have pulled latest from github this afternoon.   There are many many
> errors:
>
> <source_home>/assembly/target/scala-2.10: No such file or directory
>
> This causes many tests to fail.
>
> Here is the command line I am running
>
>     mvn -Pyarn -Phadoop-2.3 -Phive package test
>

--001a11c032c055415704ff376e6f--

From dev-return-8570-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 02:35:57 2014
Return-Path: <dev-return-8570-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4CA9210423
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 02:35:57 +0000 (UTC)
Received: (qmail 85605 invoked by uid 500); 28 Jul 2014 02:35:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85538 invoked by uid 500); 28 Jul 2014 02:35:56 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85526 invoked by uid 99); 28 Jul 2014 02:35:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:35:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:35:51 +0000
Received: by mail-ig0-f182.google.com with SMTP id c1so3060829igq.9
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 19:35:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=4GzmnFw/Ncn3bXx4P/CmZMTI/pOrEyT6gfZKE7E3daE=;
        b=xsQKszL/1YqHpq3e7v6X8lOfxt5WHSHBZo4fhA54gEAFFNJg0C7xCtvzsN/5fYjrtX
         5VOAGZ7KWNn4PABqID9YsDPYRO0fY5qod9XjFxQn3SPK2bBTjOQNkaTHMre99RMbPrTn
         I26925AzAf1HNOfRrmtraeLkLGsLYlFX09D3Teh8AwQqpryJyQm2c5K5h1kLfqpbFzse
         1JuQVGhh2eUYsJ6WCVzhymkzew//8CXtcgTqNJ8h7lx8XcTsjoVS5FtsRaL60lqlV8/P
         AC87yBqdijts7RJqc0F3xe/xqAsiYhr/ZMaLSHQL5OT1GzJtijOQxWr5AHODuWX+nf60
         5fcA==
MIME-Version: 1.0
X-Received: by 10.50.88.37 with SMTP id bd5mr28334655igb.1.1406514931277; Sun,
 27 Jul 2014 19:35:31 -0700 (PDT)
Received: by 10.107.134.203 with HTTP; Sun, 27 Jul 2014 19:35:31 -0700 (PDT)
In-Reply-To: <CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com>
References: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
	<CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com>
Date: Sun, 27 Jul 2014 19:35:31 -0700
Message-ID: <CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
Subject: Re: No such file or directory errors running tests
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011777ffd9a57704ff37c6f4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011777ffd9a57704ff37c6f4
Content-Type: text/plain; charset=UTF-8

i Reynold,
  thanks for responding here. Yes I had looked at the building with maven
page in the past.  I have not noticed  that the "package" step must happen
*before *the test.  I had assumed it were a corequisite -as seen in my
command line.

So the following sequence appears to work fine (so far so good - well past
when the prior attempts failed):


 mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
mvn -Pyarn -Phadoop-2.3 -Phive test

AFA documentation,  yes adding another sentence to that same "Building with
Maven" page would likely be helpful to future generations.


2014-07-27 19:10 GMT-07:00 Reynold Xin <rxin@databricks.com>:

> To run through all the tests you'd need to create the assembly jar first.
>
>
> I've seen this asked a few times. Maybe we should make it more obvious.
>
>
>
> http://spark.apache.org/docs/latest/building-with-maven.html
>
> Spark Tests in Maven
>
> Tests are run by default via the ScalaTest Maven plugin
> <http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin>.
> Some of the require Spark to be packaged first, so always run mvn package
>  with -DskipTests the first time. You can then run the tests with mvn
> -Dhadoop.version=... test.
>
> The ScalaTest plugin also supports running only a specific test suite as
> follows:
>
> mvn -Dhadoop.version=... -DwildcardSuites=org.apache.spark.repl.ReplSuite
> test
>
>
>
>
>
> On Sun, Jul 27, 2014 at 7:07 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
> > I have pulled latest from github this afternoon.   There are many many
> > errors:
> >
> > <source_home>/assembly/target/scala-2.10: No such file or directory
> >
> > This causes many tests to fail.
> >
> > Here is the command line I am running
> >
> >     mvn -Pyarn -Phadoop-2.3 -Phive package test
> >
>

--089e011777ffd9a57704ff37c6f4--

From dev-return-8571-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 02:35:58 2014
Return-Path: <dev-return-8571-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 329E510424
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 02:35:58 +0000 (UTC)
Received: (qmail 86561 invoked by uid 500); 28 Jul 2014 02:35:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86517 invoked by uid 500); 28 Jul 2014 02:35:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 85266 invoked by uid 99); 28 Jul 2014 02:34:52 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 216.139.236.26 as permitted sender)
Date: Sun, 27 Jul 2014 19:34:27 -0700 (PDT)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1406514867558-7538.post@n3.nabble.com>
Subject: Can I translate the documentations of Spark in Japanese?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi all,

I'm Yu Ishikawa, a Japanese.
I would like to translate the documentations of Spark 1.0.x officially.
If I will translate them and send a pull request, then can you merge it ?
And where is the best directory to create the Japanese documentations ?

Best,
Yu



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8572-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 02:37:25 2014
Return-Path: <dev-return-8572-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0041D1042D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 02:37:25 +0000 (UTC)
Received: (qmail 88070 invoked by uid 500); 28 Jul 2014 02:37:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88005 invoked by uid 500); 28 Jul 2014 02:37:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 87994 invoked by uid 99); 28 Jul 2014 02:37:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:37:24 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.43] (HELO mail-qg0-f43.google.com) (209.85.192.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:37:22 +0000
Received: by mail-qg0-f43.google.com with SMTP id a108so7753874qge.2
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 19:36:57 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=+0xHDROjLVHlW6iHy2wZEbyAQeRrb3UNn97JNVLBJZA=;
        b=QtmFi4o2KraEKWT1uA7/OeYNLwTw5sACNX2/n0+A1QFJv/g6sDYnQIgsUMRi5cEAmL
         O6X3+Yq9A1KQq1MInT6EH1Jpke7FgPcxD+aTudJqUrBh7TOVZsuOzZIWqSYoXI9PsmTp
         it/AZsftTNnbq9jxfIoZ9IjqUt6eRACes8kuaAmmzd+1vaEh6cQK5tIxl73J1RmrfxeM
         gFR32sJy1qLV6SW7GH2hS9MBma8orncPM5hGugXwDQgMPsMHE0j2uyM+zUCcx0Xo8Y5Z
         NObToYtaYXzl5tJF+TjVEph8DbZToS8gt/9f2JVCGSu3qemCSYzdmeMWobhckbvX1gOf
         3uww==
X-Gm-Message-State: ALoCoQl3f+6rsqDzGl1XYzKr17XdDlunnDig6wyH9wLcE55anHG3v3P/bTudT8WoA722an6NuRcu
X-Received: by 10.140.25.11 with SMTP id 11mr55779140qgs.9.1406515017251; Sun,
 27 Jul 2014 19:36:57 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Sun, 27 Jul 2014 19:36:37 -0700 (PDT)
In-Reply-To: <CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
References: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
 <CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com> <CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Sun, 27 Jul 2014 19:36:37 -0700
Message-ID: <CAPh_B=bayes8d__FZ4N=cq1vHnNsJV_mCQTgvTTjGfvDX_D-2Q@mail.gmail.com>
Subject: Re: No such file or directory errors running tests
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c02a8ef9ae5404ff37cb94
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c02a8ef9ae5404ff37cb94
Content-Type: text/plain; charset=UTF-8

Would you like to submit a pull request? All doc source code are in the
docs folder. Cheers.



On Sun, Jul 27, 2014 at 7:35 PM, Stephen Boesch <javadba@gmail.com> wrote:

> i Reynold,
>   thanks for responding here. Yes I had looked at the building with maven
> page in the past.  I have not noticed  that the "package" step must happen
> *before *the test.  I had assumed it were a corequisite -as seen in my
> command line.
>
> So the following sequence appears to work fine (so far so good - well past
> when the prior attempts failed):
>
>
>  mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
> mvn -Pyarn -Phadoop-2.3 -Phive test
>
> AFA documentation,  yes adding another sentence to that same "Building with
> Maven" page would likely be helpful to future generations.
>
>
> 2014-07-27 19:10 GMT-07:00 Reynold Xin <rxin@databricks.com>:
>
> > To run through all the tests you'd need to create the assembly jar first.
> >
> >
> > I've seen this asked a few times. Maybe we should make it more obvious.
> >
> >
> >
> > http://spark.apache.org/docs/latest/building-with-maven.html
> >
> > Spark Tests in Maven
> >
> > Tests are run by default via the ScalaTest Maven plugin
> > <http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin>.
> > Some of the require Spark to be packaged first, so always run mvn package
> >  with -DskipTests the first time. You can then run the tests with mvn
> > -Dhadoop.version=... test.
> >
> > The ScalaTest plugin also supports running only a specific test suite as
> > follows:
> >
> > mvn -Dhadoop.version=... -DwildcardSuites=org.apache.spark.repl.ReplSuite
> > test
> >
> >
> >
> >
> >
> > On Sun, Jul 27, 2014 at 7:07 PM, Stephen Boesch <javadba@gmail.com>
> wrote:
> >
> > > I have pulled latest from github this afternoon.   There are many many
> > > errors:
> > >
> > > <source_home>/assembly/target/scala-2.10: No such file or directory
> > >
> > > This causes many tests to fail.
> > >
> > > Here is the command line I am running
> > >
> > >     mvn -Pyarn -Phadoop-2.3 -Phive package test
> > >
> >
>

--001a11c02a8ef9ae5404ff37cb94--

From dev-return-8573-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 02:39:50 2014
Return-Path: <dev-return-8573-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D390710432
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 02:39:50 +0000 (UTC)
Received: (qmail 90216 invoked by uid 500); 28 Jul 2014 02:39:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90155 invoked by uid 500); 28 Jul 2014 02:39:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 90142 invoked by uid 99); 28 Jul 2014 02:39:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:39:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.213.169 as permitted sender)
Received: from [209.85.213.169] (HELO mail-ig0-f169.google.com) (209.85.213.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 02:39:45 +0000
Received: by mail-ig0-f169.google.com with SMTP id r2so3085462igi.2
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 19:39:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RMlD/peGiPC2fBl6W9zuDGSZfsV/lKGqd+YaVqEe0j4=;
        b=OtQVCfne3fUMQd2E/MYTsMSa5wj1fel7VTkXugci7t38QmTigQa6IHCErVJGfGonWr
         a8OaahpOMQzTXwdwB6VqmVLCbzpIi4VsDGikLBw3C7VDd3+bYui/ChnrG32LKOXdFFFc
         xr2XoVM7Mkj05T8N7W8xHPyVmF0vP338Qt6IDhvWbK7KaeyMdXBac8PwVV0lY0oOm3dK
         epB7R6Wrbv2E7JpxgqRC6LmcO7obl4iRLWwKWMrmQ2NgNbI5Ayca9qObnssfqbMSXiw/
         UH0m4B/nEkXdvsUFdhLPVPNS7XJUZJh1dQwTZIivIXlO/Wzk2rB6G2ImUwPHQEidAjX8
         KBSw==
MIME-Version: 1.0
X-Received: by 10.50.176.202 with SMTP id ck10mr27963290igc.2.1406515165447;
 Sun, 27 Jul 2014 19:39:25 -0700 (PDT)
Received: by 10.107.134.203 with HTTP; Sun, 27 Jul 2014 19:39:25 -0700 (PDT)
In-Reply-To: <CAPh_B=bayes8d__FZ4N=cq1vHnNsJV_mCQTgvTTjGfvDX_D-2Q@mail.gmail.com>
References: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
	<CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com>
	<CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
	<CAPh_B=bayes8d__FZ4N=cq1vHnNsJV_mCQTgvTTjGfvDX_D-2Q@mail.gmail.com>
Date: Sun, 27 Jul 2014 19:39:25 -0700
Message-ID: <CACkSZy0WyK+gFx82-CEtsFnyLKvRuFD18KzbyK6RC52mh1L96w@mail.gmail.com>
Subject: Re: No such file or directory errors running tests
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0111d758cecbae04ff37d48c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0111d758cecbae04ff37d48c
Content-Type: text/plain; charset=UTF-8

 OK i'll do it after confirming all the tests run


2014-07-27 19:36 GMT-07:00 Reynold Xin <rxin@databricks.com>:

> Would you like to submit a pull request? All doc source code are in the
> docs folder. Cheers.
>
>
>
> On Sun, Jul 27, 2014 at 7:35 PM, Stephen Boesch <javadba@gmail.com> wrote:
>
> > i Reynold,
> >   thanks for responding here. Yes I had looked at the building with maven
> > page in the past.  I have not noticed  that the "package" step must
> happen
> > *before *the test.  I had assumed it were a corequisite -as seen in my
> > command line.
> >
> > So the following sequence appears to work fine (so far so good - well
> past
> > when the prior attempts failed):
> >
> >
> >  mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
> > mvn -Pyarn -Phadoop-2.3 -Phive test
> >
> > AFA documentation,  yes adding another sentence to that same "Building
> with
> > Maven" page would likely be helpful to future generations.
> >
> >
> > 2014-07-27 19:10 GMT-07:00 Reynold Xin <rxin@databricks.com>:
> >
> > > To run through all the tests you'd need to create the assembly jar
> first.
> > >
> > >
> > > I've seen this asked a few times. Maybe we should make it more obvious.
> > >
> > >
> > >
> > > http://spark.apache.org/docs/latest/building-with-maven.html
> > >
> > > Spark Tests in Maven
> > >
> > > Tests are run by default via the ScalaTest Maven plugin
> > > <http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin
> >.
> > > Some of the require Spark to be packaged first, so always run mvn
> package
> > >  with -DskipTests the first time. You can then run the tests with mvn
> > > -Dhadoop.version=... test.
> > >
> > > The ScalaTest plugin also supports running only a specific test suite
> as
> > > follows:
> > >
> > > mvn -Dhadoop.version=...
> -DwildcardSuites=org.apache.spark.repl.ReplSuite
> > > test
> > >
> > >
> > >
> > >
> > >
> > > On Sun, Jul 27, 2014 at 7:07 PM, Stephen Boesch <javadba@gmail.com>
> > wrote:
> > >
> > > > I have pulled latest from github this afternoon.   There are many
> many
> > > > errors:
> > > >
> > > > <source_home>/assembly/target/scala-2.10: No such file or directory
> > > >
> > > > This causes many tests to fail.
> > > >
> > > > Here is the command line I am running
> > > >
> > > >     mvn -Pyarn -Phadoop-2.3 -Phive package test
> > > >
> > >
> >
>

--089e0111d758cecbae04ff37d48c--

From dev-return-8574-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 03:02:54 2014
Return-Path: <dev-return-8574-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2B4E3104A8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 03:02:54 +0000 (UTC)
Received: (qmail 5720 invoked by uid 500); 28 Jul 2014 03:02:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5650 invoked by uid 500); 28 Jul 2014 03:02:53 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5634 invoked by uid 99); 28 Jul 2014 03:02:53 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 03:02:53 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of snunez@hortonworks.com designates 209.85.220.52 as permitted sender)
Received: from [209.85.220.52] (HELO mail-pa0-f52.google.com) (209.85.220.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 03:02:50 +0000
Received: by mail-pa0-f52.google.com with SMTP id bj1so9504852pad.25
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 20:02:25 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:message-id
         :thread-topic:references:in-reply-to:mime-version:content-type
         :content-transfer-encoding;
        bh=eddHbr5UeBTV4yvSZwLHBO7Rmrrcjd28ozoBeylRUEI=;
        b=ZuX96TJBKXRKjTk9fLiJsHfN1YeYkvuG2ziQVVPOKmnr+drbseQORsWhBZpVK6sROF
         6YZ3JUQjrssvjcdQ2N0YSOOWXve3N8tW7z2eEfBBd/o/raKYoGcd8uxhIm8MFPNs0mkQ
         UrSrIxsKmtjwaPSbQ8IVNbnOy50+BioZvR/tEWdxeEkLCiTvhfWAn2WXximy/j+cmiIY
         3vnZ5IjLEOMPQmYVq8+UU1lczk3jpsPsC2OBgOzp6vwd/k60PT9GnR30uqaDzXxy35sG
         uKXebMmUsLCRTrt4REbTUU2qYC7C0nnJdnDd4+czrR/qOOu/yRY0ofduz8uy1ZIEsTe6
         ZOUw==
X-Gm-Message-State: ALoCoQna54KzJthKgWHMc0TF12bv8K3UTUFcj1InOHVOnTig6IIJmU96nbNNRDV1mho9S1F+eIn2z9M52eGjFmpi9zJbgcZccKX2YtT9H2JgFM1Sn/h1Vik=
X-Received: by 10.66.123.36 with SMTP id lx4mr35246630pab.21.1406516545471;
        Sun, 27 Jul 2014 20:02:25 -0700 (PDT)
Received: from [10.0.0.6] ([24.130.63.105])
        by mx.google.com with ESMTPSA id i10sm61706868pat.36.2014.07.27.20.02.23
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Sun, 27 Jul 2014 20:02:24 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Sun, 27 Jul 2014 20:02:20 -0700
Subject: Re: No such file or directory errors running tests
From: Steve Nunez <snunez@hortonworks.com>
To: <dev@spark.apache.org>
Message-ID: <CFFB0AA5.2D26%snunez@hortonworks.com>
Thread-Topic: No such file or directory errors running tests
References: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
 <CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com>
 <CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
 <CAPh_B=bayes8d__FZ4N=cq1vHnNsJV_mCQTgvTTjGfvDX_D-2Q@mail.gmail.com>
 <CACkSZy0WyK+gFx82-CEtsFnyLKvRuFD18KzbyK6RC52mh1L96w@mail.gmail.com>
In-Reply-To: <CACkSZy0WyK+gFx82-CEtsFnyLKvRuFD18KzbyK6RC52mh1L96w@mail.gmail.com>
Mime-version: 1.0
Content-type: text/plain; charset=ISO-8859-1
Content-transfer-encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Whilst we=B9re on this topic, I=B9d be interested to see if you get hive
failures. I=B9m trying to build on a Mac using HDP and seem to be getting
failures related to Parquet. I=B9ll know for sure once I get in tomorrow an=
d
confirm with engineering, but this is likely because the version of Hive
is 0.12.0, and Parquet is only supported in Hive 0.13 (HDP is 0.13)

Any idea on what it would take to bump the Hive version up to the latest?

Regards,
	- SteveN



On 7/27/14, 19:39, "Stephen Boesch" <javadba@gmail.com> wrote:

> OK i'll do it after confirming all the tests run
>
>
>2014-07-27 19:36 GMT-07:00 Reynold Xin <rxin@databricks.com>:
>
>> Would you like to submit a pull request? All doc source code are in the
>> docs folder. Cheers.
>>
>>
>>
>> On Sun, Jul 27, 2014 at 7:35 PM, Stephen Boesch <javadba@gmail.com>
>>wrote:
>>
>> > i Reynold,
>> >   thanks for responding here. Yes I had looked at the building with
>>maven
>> > page in the past.  I have not noticed  that the "package" step must
>> happen
>> > *before *the test.  I had assumed it were a corequisite -as seen in my
>> > command line.
>> >
>> > So the following sequence appears to work fine (so far so good - well
>> past
>> > when the prior attempts failed):
>> >
>> >
>> >  mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
>> > mvn -Pyarn -Phadoop-2.3 -Phive test
>> >
>> > AFA documentation,  yes adding another sentence to that same "Building
>> with
>> > Maven" page would likely be helpful to future generations.
>> >
>> >
>> > 2014-07-27 19:10 GMT-07:00 Reynold Xin <rxin@databricks.com>:
>> >
>> > > To run through all the tests you'd need to create the assembly jar
>> first.
>> > >
>> > >
>> > > I've seen this asked a few times. Maybe we should make it more
>>obvious.
>> > >
>> > >
>> > >
>> > > http://spark.apache.org/docs/latest/building-with-maven.html
>> > >
>> > > Spark Tests in Maven
>> > >
>> > > Tests are run by default via the ScalaTest Maven plugin
>> > >=20
>><http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin
>> >.
>> > > Some of the require Spark to be packaged first, so always run mvn
>> package
>> > >  with -DskipTests the first time. You can then run the tests with
>>mvn
>> > > -Dhadoop.version=3D... test.
>> > >
>> > > The ScalaTest plugin also supports running only a specific test
>>suite
>> as
>> > > follows:
>> > >
>> > > mvn -Dhadoop.version=3D...
>> -DwildcardSuites=3Dorg.apache.spark.repl.ReplSuite
>> > > test
>> > >
>> > >
>> > >
>> > >
>> > >
>> > > On Sun, Jul 27, 2014 at 7:07 PM, Stephen Boesch <javadba@gmail.com>
>> > wrote:
>> > >
>> > > > I have pulled latest from github this afternoon.   There are many
>> many
>> > > > errors:
>> > > >
>> > > > <source_home>/assembly/target/scala-2.10: No such file or
>>directory
>> > > >
>> > > > This causes many tests to fail.
>> > > >
>> > > > Here is the command line I am running
>> > > >
>> > > >     mvn -Pyarn -Phadoop-2.3 -Phive package test
>> > > >
>> > >
>> >
>>



--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

From dev-return-8575-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 03:12:45 2014
Return-Path: <dev-return-8575-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 30785104EA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 03:12:45 +0000 (UTC)
Received: (qmail 17367 invoked by uid 500); 28 Jul 2014 03:12:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 17302 invoked by uid 500); 28 Jul 2014 03:12:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17290 invoked by uid 99); 28 Jul 2014 03:12:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 03:12:44 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of javadba@gmail.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 03:12:40 +0000
Received: by mail-ie0-f169.google.com with SMTP id rd18so6193543iec.28
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 20:12:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=/hEasx7OTf19zWOidcL6suRz281n8hidjLZ5V89sagE=;
        b=eiq5FZgJ7iImbVZUTv6+kQqITdS73oPROJz2/vKl89tB1pJfo/ZzY1W6BRe4Jh4sPd
         MUNuqqvvb93fHhIsQE7mRvTmn7JxiNNH4LXYIRURkye8eLnIQ8+GEiJOO+YwxZbhDXZL
         k90nf5/X1+824PLl14UQIfzdpeRRhn2vmLmW1bnIrtSsduGcNVw3/xlzYBjFv7cFoYWg
         PElp1ZxbE3OO321Zi4hP0d8cFtlK8bAbCfj4QkuDH/ixwELMYWaMKC8tddWCB24wDd0D
         lvt+SoHg4B3IEKpsZAmJrby1k0DmXD+wTNOgoF54l7KaSS+2xK5lAjXPMwX1c0InBvZx
         FOSA==
MIME-Version: 1.0
X-Received: by 10.50.176.202 with SMTP id ck10mr28117040igc.2.1406517139787;
 Sun, 27 Jul 2014 20:12:19 -0700 (PDT)
Received: by 10.107.134.203 with HTTP; Sun, 27 Jul 2014 20:12:19 -0700 (PDT)
In-Reply-To: <CFFB0AA5.2D26%snunez@hortonworks.com>
References: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
	<CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com>
	<CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
	<CAPh_B=bayes8d__FZ4N=cq1vHnNsJV_mCQTgvTTjGfvDX_D-2Q@mail.gmail.com>
	<CACkSZy0WyK+gFx82-CEtsFnyLKvRuFD18KzbyK6RC52mh1L96w@mail.gmail.com>
	<CFFB0AA5.2D26%snunez@hortonworks.com>
Date: Sun, 27 Jul 2014 20:12:19 -0700
Message-ID: <CACkSZy0wf4mCt-4+0cAmkwWWCJOXQeMyBjuD-K8aU_rNa=k-VA@mail.gmail.com>
Subject: Re: No such file or directory errors running tests
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0111d7587cd7a904ff384a96
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0111d7587cd7a904ff384a96
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Steve,
  I am running on the cdh5.0.0 VM (which is CentOS 6.5)   Given the
difference in O/S and Hadoop distro between us my results are not likely to
be of direct help to you. But in any case i will let you know (likely
offline).


2014-07-27 20:02 GMT-07:00 Steve Nunez <snunez@hortonworks.com>:

> Whilst we=C2=B9re on this topic, I=C2=B9d be interested to see if you get=
 hive
> failures. I=C2=B9m trying to build on a Mac using HDP and seem to be gett=
ing
> failures related to Parquet. I=C2=B9ll know for sure once I get in tomorr=
ow and
> confirm with engineering, but this is likely because the version of Hive
> is 0.12.0, and Parquet is only supported in Hive 0.13 (HDP is 0.13)
>
> Any idea on what it would take to bump the Hive version up to the latest?
>
> Regards,
>         - SteveN
>
>
>
> On 7/27/14, 19:39, "Stephen Boesch" <javadba@gmail.com> wrote:
>
> > OK i'll do it after confirming all the tests run
> >
> >
> >2014-07-27 19:36 GMT-07:00 Reynold Xin <rxin@databricks.com>:
> >
> >> Would you like to submit a pull request? All doc source code are in th=
e
> >> docs folder. Cheers.
> >>
> >>
> >>
> >> On Sun, Jul 27, 2014 at 7:35 PM, Stephen Boesch <javadba@gmail.com>
> >>wrote:
> >>
> >> > i Reynold,
> >> >   thanks for responding here. Yes I had looked at the building with
> >>maven
> >> > page in the past.  I have not noticed  that the "package" step must
> >> happen
> >> > *before *the test.  I had assumed it were a corequisite -as seen in =
my
> >> > command line.
> >> >
> >> > So the following sequence appears to work fine (so far so good - wel=
l
> >> past
> >> > when the prior attempts failed):
> >> >
> >> >
> >> >  mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
> >> > mvn -Pyarn -Phadoop-2.3 -Phive test
> >> >
> >> > AFA documentation,  yes adding another sentence to that same "Buildi=
ng
> >> with
> >> > Maven" page would likely be helpful to future generations.
> >> >
> >> >
> >> > 2014-07-27 19:10 GMT-07:00 Reynold Xin <rxin@databricks.com>:
> >> >
> >> > > To run through all the tests you'd need to create the assembly jar
> >> first.
> >> > >
> >> > >
> >> > > I've seen this asked a few times. Maybe we should make it more
> >>obvious.
> >> > >
> >> > >
> >> > >
> >> > > http://spark.apache.org/docs/latest/building-with-maven.html
> >> > >
> >> > > Spark Tests in Maven
> >> > >
> >> > > Tests are run by default via the ScalaTest Maven plugin
> >> > >
> >><http://www.scalatest.org/user_guide/using_the_scalatest_maven_plugin
> >> >.
> >> > > Some of the require Spark to be packaged first, so always run mvn
> >> package
> >> > >  with -DskipTests the first time. You can then run the tests with
> >>mvn
> >> > > -Dhadoop.version=3D... test.
> >> > >
> >> > > The ScalaTest plugin also supports running only a specific test
> >>suite
> >> as
> >> > > follows:
> >> > >
> >> > > mvn -Dhadoop.version=3D...
> >> -DwildcardSuites=3Dorg.apache.spark.repl.ReplSuite
> >> > > test
> >> > >
> >> > >
> >> > >
> >> > >
> >> > >
> >> > > On Sun, Jul 27, 2014 at 7:07 PM, Stephen Boesch <javadba@gmail.com=
>
> >> > wrote:
> >> > >
> >> > > > I have pulled latest from github this afternoon.   There are man=
y
> >> many
> >> > > > errors:
> >> > > >
> >> > > > <source_home>/assembly/target/scala-2.10: No such file or
> >>directory
> >> > > >
> >> > > > This causes many tests to fail.
> >> > > >
> >> > > > Here is the command line I am running
> >> > > >
> >> > > >     mvn -Pyarn -Phadoop-2.3 -Phive package test
> >> > > >
> >> > >
> >> >
> >>
>
>
>
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity =
to
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified th=
at
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediate=
ly
> and delete it from your system. Thank You.
>

--089e0111d7587cd7a904ff384a96--

From dev-return-8577-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 04:48:56 2014
Return-Path: <dev-return-8577-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5D2971065B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 04:48:56 +0000 (UTC)
Received: (qmail 99236 invoked by uid 500); 28 Jul 2014 04:48:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99092 invoked by uid 500); 28 Jul 2014 04:48:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98556 invoked by uid 99); 28 Jul 2014 04:48:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 04:48:54 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 04:48:52 +0000
Received: by mail-oa0-f54.google.com with SMTP id n16so8072806oag.41
        for <dev@spark.apache.org>; Sun, 27 Jul 2014 21:48:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=zVrJD3clGvxGs1dzIsKvXnO4D7wwM2IMbvfCCh9XkW4=;
        b=r7ob8x3H2ofTbR7foAuI6QSZFiqYtfBVQSO7s+a9IufBhFsH4jSmG5/6gIkF9gZTLu
         cEIFOMmpCmMxh8qMbpH54ggnqTwpSXvwXtTfrOhXK6Tf69k8lVjnULPuEbNdcHr+um8R
         chmvo3O6nuJB0EAeEQ2PGNiVxXHc0Wsfaj6HtHy3TPx/In/PCW+FBWmK0U/xjpISlz/u
         ibZxitzan7iLwNlpchRq+ZgZS3quDkLlf+FEjEHBkdlCDG5x6GGedVqpdgHgKKiE9oHI
         oyyOQjCuV/rmiNeW1BVNLcGZJV7HYCX2eI51JgZlo/gsZ3ST5pOrnMDowJhDbMQWyaei
         93dQ==
MIME-Version: 1.0
X-Received: by 10.60.50.65 with SMTP id a1mr45123793oeo.77.1406522907522; Sun,
 27 Jul 2014 21:48:27 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 21:48:27 -0700 (PDT)
In-Reply-To: <1406514867558-7538.post@n3.nabble.com>
References: <1406514867558-7538.post@n3.nabble.com>
Date: Sun, 27 Jul 2014 21:48:27 -0700
Message-ID: <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Yu,

I think we could definitely put a pointer to documentation in other
languages that is hosted somewhere welse, but since we are not in a
position to maintain this, I'm not sure we could merge it into the
mainline Spark codebase. I'd be interested to know what other projects
do about this situation!

- Patrick

On Sun, Jul 27, 2014 at 7:34 PM, Yu Ishikawa
<yuu.ishikawa+spark@gmail.com> wrote:
> Hi all,
>
> I'm Yu Ishikawa, a Japanese.
> I would like to translate the documentations of Spark 1.0.x officially.
> If I will translate them and send a pull request, then can you merge it ?
> And where is the best directory to create the Japanese documentations ?
>
> Best,
> Yu
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8576-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 04:48:56 2014
Return-Path: <dev-return-8576-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A53D61065E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 04:48:56 +0000 (UTC)
Received: (qmail 98631 invoked by uid 500); 28 Jul 2014 04:48:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98569 invoked by uid 500); 28 Jul 2014 04:48:54 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 98545 invoked by uid 99); 28 Jul 2014 04:48:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 04:48:54 +0000
X-ASF-Spam-Status: No, hits=0.6 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 04:48:52 +0000
Received: by mail-oa0-f50.google.com with SMTP id g18so8155030oah.23
        for <dev@spark.incubator.apache.org>; Sun, 27 Jul 2014 21:48:27 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=zVrJD3clGvxGs1dzIsKvXnO4D7wwM2IMbvfCCh9XkW4=;
        b=r7ob8x3H2ofTbR7foAuI6QSZFiqYtfBVQSO7s+a9IufBhFsH4jSmG5/6gIkF9gZTLu
         cEIFOMmpCmMxh8qMbpH54ggnqTwpSXvwXtTfrOhXK6Tf69k8lVjnULPuEbNdcHr+um8R
         chmvo3O6nuJB0EAeEQ2PGNiVxXHc0Wsfaj6HtHy3TPx/In/PCW+FBWmK0U/xjpISlz/u
         ibZxitzan7iLwNlpchRq+ZgZS3quDkLlf+FEjEHBkdlCDG5x6GGedVqpdgHgKKiE9oHI
         oyyOQjCuV/rmiNeW1BVNLcGZJV7HYCX2eI51JgZlo/gsZ3ST5pOrnMDowJhDbMQWyaei
         93dQ==
MIME-Version: 1.0
X-Received: by 10.60.50.65 with SMTP id a1mr45123793oeo.77.1406522907522; Sun,
 27 Jul 2014 21:48:27 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Sun, 27 Jul 2014 21:48:27 -0700 (PDT)
In-Reply-To: <1406514867558-7538.post@n3.nabble.com>
References: <1406514867558-7538.post@n3.nabble.com>
Date: Sun, 27 Jul 2014 21:48:27 -0700
Message-ID: <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Yu,

I think we could definitely put a pointer to documentation in other
languages that is hosted somewhere welse, but since we are not in a
position to maintain this, I'm not sure we could merge it into the
mainline Spark codebase. I'd be interested to know what other projects
do about this situation!

- Patrick

On Sun, Jul 27, 2014 at 7:34 PM, Yu Ishikawa
<yuu.ishikawa+spark@gmail.com> wrote:
> Hi all,
>
> I'm Yu Ishikawa, a Japanese.
> I would like to translate the documentations of Spark 1.0.x officially.
> If I will translate them and send a pull request, then can you merge it ?
> And where is the best directory to create the Japanese documentations ?
>
> Best,
> Yu
>
>
>
> --
> View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538.html
> Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8578-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 07:42:07 2014
Return-Path: <dev-return-8578-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 95C03109F7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 07:42:07 +0000 (UTC)
Received: (qmail 70235 invoked by uid 500); 28 Jul 2014 07:42:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70176 invoked by uid 500); 28 Jul 2014 07:42:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 47535 invoked by uid 99); 28 Jul 2014 07:32:16 -0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:content-type:mime-version:subject:from
         :in-reply-to:date:cc:content-transfer-encoding:message-id:references
         :to;
        bh=kTQy/fQSQ/b8kI0GGtTNsGLfew7Uy/llmA+EQAD6BlE=;
        b=fOd39Lx1bwafKY539q7yV9ZyBvJxG6tq7P/g0vYMkPymWneObgnpU6PqVcHWeGA47z
         nFqpYJcVmLwW5bLWd1/MHxxOtbCuDT7Wc2liGYx90EyDOpZ+zOn5P4MMM6OMnSFQcHE1
         VI+MpG2+AWvjouMyh0oNuoqxifUtrr4TNUshGa+VVNsyxfdRBIYKiHFBR2+2Poj55fc3
         TBSHvFERvyJ1IvnAOxLaL7kRPEoNJB0sLK+FAwrHU08OFToZcgqGV6e+xWpV9cNY8rMf
         YQb3sDa9Svr/5bRxCIqF/vEl/DIqLdhVJZhPjg9u8YsBdbQ+Wmpc9MA2JFOsst4LuP7i
         1Zcw==
X-Gm-Message-State: ALoCoQnHkKml4or3aJVuXP9r0aJLkOojnp2bGjnQtzE4btlO9208loAWWR9jYiVnpc0AB7+GX0l5
X-Received: by 10.70.103.106 with SMTP id fv10mr36543015pdb.63.1406532708821;
        Mon, 28 Jul 2014 00:31:48 -0700 (PDT)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.3 \(1878.6\))
Subject: Re: new JDBC server test cases seems failed ?
From: Cheng Lian <lian@databricks.com>
In-Reply-To: <5215C719-6E46-406A-89BB-AB8AE1E928CC@databricks.com>
Date: Mon, 28 Jul 2014 15:31:49 +0800
Cc: Patrick Wendell <patrick@databricks.com>
Content-Transfer-Encoding: quoted-printable
Message-Id: <4349FE98-05BD-4895-8634-CBD89C870DD0@databricks.com>
References: <C3014F7E1AF2489A9E7B43F7D9B8335F@gmail.com> <CAAswR-5w_kLPNCKKaWgOtf1EumY9A7dkqP+xa89CBkctB05w=Q@mail.gmail.com> <F9B23E166F854EDABEBC9B6EA8FE2D14@gmail.com> <CABPQxsuyTj3WT1SN1Jf9pJ7GdJL1r8cVQGcSwy8-3BXciE69Vg@mail.gmail.com> <5215C719-6E46-406A-89BB-AB8AE1E928CC@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
X-Mailer: Apple Mail (2.1878.6)
X-Virus-Checked: Checked by ClamAV on apache.org

Noticed that Nan=92s PR is not related to SQL, but the JDBC test suites =
got executed. Then I checked PRs of all those Jenkins builds that failed =
because of the JDBC suites, it turns out that none of them touched SQL =
code.  The JDBC code is only contained in the assembly file when the =
hive-thriftserver build profile is enabled. So it seems that the root =
cause is related to Maven build changes that makes the JDBC suites =
always get executed and fail because JDBC code isn't included in the =
assembly jar. This also explains why I can=92t reproduce it locally (I =
always enable hive-thriftserver profile), and why once the build fail, =
all JDBC suites fail together.

Working on a patch to fix this. Thanks to Patrick for helping debugging =
this!

On Jul 28, 2014, at 10:07 AM, Cheng Lian <lian@databricks.com> wrote:

> I=92m looking into this, will fix this ASAP, sorry for the =
inconvenience.
>=20
> On Jul 28, 2014, at 9:47 AM, Patrick Wendell <pwendell@gmail.com> =
wrote:
>=20
>> I'm going to revert it again - Cheng can you try to look into this? =
Thanks.
>>=20
>> On Sun, Jul 27, 2014 at 6:06 PM, Nan Zhu <zhunanmcgill@gmail.com> =
wrote:
>>> it's 20 minutes ago
>>>=20
>>> =
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17259/c=
onsoleFull
>>>=20
>>> --
>>> Nan Zhu
>>>=20
>>>=20
>>> On Sunday, July 27, 2014 at 8:53 PM, Michael Armbrust wrote:
>>>=20
>>>> How recent is this? We've already reverted this patch once due to =
failing
>>>> tests. It would be helpful to include a link to the failed build. =
If its
>>>> failing again we'll have to revert again.
>>>>=20
>>>>=20
>>>> On Sun, Jul 27, 2014 at 5:26 PM, Nan Zhu <zhunanmcgill@gmail.com =
(mailto:zhunanmcgill@gmail.com)> wrote:
>>>>=20
>>>>> Hi, all
>>>>>=20
>>>>> It seems that the JDBC test cases are failed unexpectedly in =
Jenkins?
>>>>>=20
>>>>>=20
>>>>> [info] - test query execution against a Hive Thrift server *** =
FAILED ***
>>>>> [info] java.sql.SQLException: Could not open connection to
>>>>> jdbc:hive2://localhost:45518/: java.net.ConnectException: =
Connection
>>>>> refused [info] at
>>>>> =
org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:146)=

>>>>> [info] at
>>>>> =
org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) =
[info]
>>>>> at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) =
[info] at
>>>>> java.sql.DriverManager.getConnection(DriverManager.java:571) =
[info] at
>>>>> java.sql.DriverManager.getConnection(DriverManager.java:215) =
[info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.getConnectio=
n(HiveThriftServer2Suite.scala:131)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatem=
ent(HiveThriftServer2Suite.scala:134)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.a=
pply$mcV$sp(HiveThriftServer2Suite.scala:110)
>>>>> [info] at org.apache.spark.sql.hive.thri
>>>>> =
ftserver.HiveThriftServer2Suite$$anonfun$1.apply(HiveThriftServer2Suite.sc=
ala:107)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite$$anonfun$1.a=
pply(HiveThriftServer2Suite.scala:107)
>>>>> [info] ... [info] Cause: =
org.apache.thrift.transport.TTransportException:
>>>>> java.net.ConnectException: Connection refused [info] at
>>>>> org.apache.thrift.transport.TSocket.open(TSocket.java:185) [info] =
at
>>>>> =
org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
>>>>> [info] at
>>>>> =
org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport=
.java:37)
>>>>> [info] at
>>>>> =
org.apache.hive.jdbc.HiveConnection.openTransport(HiveConnection.java:144)=

>>>>> [info] at
>>>>> =
org.apache.hive.jdbc.HiveConnection.<init>(HiveConnection.java:123) =
[info]
>>>>> at org.apache.hive.jdbc.HiveDriver.connect(HiveDriver.java:105) =
[info] at
>>>>> java.sql.DriverManager.getConnection(DriverManager.java:571) =
[info] at
>>>>> java.sql.DriverManager.getConnection(DriverManager.java:215) =
[info] at
>>>>> org.apache.spark.sql.hive.thriftserver.H
>>>>> =
iveThriftServer2Suite.getConnection(HiveThriftServer2Suite.scala:131)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.HiveThriftServer2Suite.createStatem=
ent(HiveThriftServer2Suite.scala:134)
>>>>> [info] ... [info] Cause: java.net.ConnectException: Connection =
refused
>>>>> [info] at java.net.PlainSocketImpl.socketConnect(Native Method) =
[info] at
>>>>> =
java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:33=
9)
>>>>> [info] at
>>>>> =
java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.=
java:200)
>>>>> [info] at
>>>>> =
java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)=

>>>>> [info] at =
java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392) [info]
>>>>> at java.net.Socket.connect(Socket.java:579) [info] at
>>>>> org.apache.thrift.transport.TSocket.open(TSocket.java:180) [info] =
at
>>>>> =
org.apache.thrift.transport.TSaslTransport.open(TSaslTransport.java:248)
>>>>> [info] at
>>>>> =
org.apache.thrift.transport.TSaslClientTransport.open(TSaslClientTransport=
.java:37)
>>>>> [info] at org.apache.hive.jdbc.HiveConn
>>>>> ection.openTransport(HiveConnection.java:144) [info] ... [info] =
CliSuite:
>>>>> Executing: create table hive_test1(key int, val string);, =
expecting output:
>>>>> OK [warn] four warnings found [warn] Note:
>>>>> =
/home/jenkins/workspace/SparkPullRequestBuilder@4/core/src/test/java/org/a=
pache/spark/JavaAPISuite.java
>>>>> uses or overrides a deprecated API. [warn] Note: Recompile with
>>>>> -Xlint:deprecation for details. [info] - simple commands *** =
FAILED ***
>>>>> [info] java.lang.AssertionError: assertion failed: Didn't find =
"OK" in the
>>>>> output: [info] at scala.Predef$.assert(Predef.scala:179) [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.TestUtils$class.waitForQuery(TestUt=
ils.scala:70)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.CliSuite.waitForQuery(CliSuite.scal=
a:25)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.TestUtils$class.executeQuery(TestUt=
ils.scala:62)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.CliSuite.executeQuery(CliSuite.scal=
a:25)
>>>>> [info] at org.apache.spark.sql.hive.thriftserver.CliSuite
>>>>> $$anonfun$1.apply$mcV$sp(CliSuite.scala:53) [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.=
scala:51)
>>>>> [info] at
>>>>> =
org.apache.spark.sql.hive.thriftserver.CliSuite$$anonfun$1.apply(CliSuite.=
scala:51)
>>>>> [info] at
>>>>> =
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>>>>> [info] at
>>>>> =
org.scalatest.Transformer$$anonfun$apply$1.apply(Transformer.scala:22)
>>>>> [log4j:WARN No appenders could be found for logger
>>>>> (org.apache.hadoop.util.Shell). log4j:WARN Please initialize the =
log4j
>>>>> system properly. log4j:WARN See
>>>>> http://logging.apache.org/log4j/1.2/faq.html#noconfig for more =
info.
>>>>> 14/07/27 17:06:43 INFO ClientBase: Using Spark's default log4j =
profile:
>>>>> org/apache/spark/log4j-defaults.properties info] ... [info] =
ScalaTest
>>>>> [info] Run completed in 41 seconds, 789 milliseconds. [info] Total =
number
>>>>> of tests run: 2 [info] Suites: completed 2, aborted 0 [info] =
Tests:
>>>>> succeeded 0, failed 2, canceled 0, ignored 0, pending 0 [info] *** =
2 TESTS
>>>>> FAILED ***
>>>>>=20
>>>>> Best,
>>>>>=20
>>>>> --
>>>>> Nan Zhu
>>>>> Sent with Sparrow (http://www.sparrowmailapp.com/?sig)
>>>>>=20
>>>>=20
>>>>=20
>>>>=20
>>>=20
>>>=20
>=20


From dev-return-8579-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 08:59:25 2014
Return-Path: <dev-return-8579-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1C82010B9C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 08:59:25 +0000 (UTC)
Received: (qmail 6988 invoked by uid 500); 28 Jul 2014 08:59:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 6934 invoked by uid 500); 28 Jul 2014 08:59:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 6918 invoked by uid 99); 28 Jul 2014 08:59:24 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 08:59:24 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.174 as permitted sender)
Received: from [209.85.220.174] (HELO mail-vc0-f174.google.com) (209.85.220.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 08:59:21 +0000
Received: by mail-vc0-f174.google.com with SMTP id la4so10695346vcb.33
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 01:58:56 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=7GVs2zdpXWdbKPUyDK1bOZ0FdCICeEKv6kCPv1B8GyY=;
        b=A/5qhedhAe5m7mN6CltG/wud8+Gyn5FHXuC2LvOwqqS8lSVgIejpP6NEKu4TSnZtAD
         +MPkuC7waA0MmVfiP8SVvF8n8BpGv/MMiGx4V63v+Rp6Ut8GKM0hwPmOM0R7YfiP8EVU
         0k7ZeDunKFv4m+5m3m5GmBiYZSw2snaJQ1KMxUajE+dW3miZLQ/z7F0FPdb/9iL/3CSd
         B9HeqI05WNZ8r5I/O0urS0apwHlk6b1ZS+ybzhavD4WUocBS0p1qnRt6dY3OJZMQlrt9
         E/bdCnY8raRaJjRCwWil7438ngbRFFCwap+dwbtgOy87KqEttV8eLq6gf3b3OSDw5Nfu
         XOHg==
X-Gm-Message-State: ALoCoQldmS5VZhhV3f0WecGNHRcEGPRyiWnMx9vl+ak4DyjDm0vfLNu6qvbk/CNuW6sH+Pm5MprQ
X-Received: by 10.52.9.35 with SMTP id w3mr11962074vda.12.1406537936670; Mon,
 28 Jul 2014 01:58:56 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Mon, 28 Jul 2014 01:58:36 -0700 (PDT)
In-Reply-To: <CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
References: <CACkSZy0gCA4o1z766wgNzVTcwJa26VV5fX-MV6jFsCZnoqO3pw@mail.gmail.com>
 <CAPh_B=Y8B2GgVxhnPOm2vhVTZ4RZ2Bn3_qzGBqRxofcWitiHNQ@mail.gmail.com> <CACkSZy2XyKvgBpCDW-TzWQ4XjgFty63ztwsB=HQKObjDyKremQ@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 28 Jul 2014 09:58:36 +0100
Message-ID: <CAMAsSdK=d_z3G6H5v3Ou-C=jy98p_4rw7SWE6n1o2ezQtgWaJg@mail.gmail.com>
Subject: Re: No such file or directory errors running tests
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

On Mon, Jul 28, 2014 at 3:35 AM, Stephen Boesch <javadba@gmail.com> wrote:
>  mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
> mvn -Pyarn -Phadoop-2.3 -Phive test

Yes, it's unintuitive for Maven, since package always happens after
test, which kind of makes sense in general. I suppose we could bind
the generation of needed assemblies to the test phase instead, or as
well?

From dev-return-8580-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 09:09:44 2014
Return-Path: <dev-return-8580-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB4E910BF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 09:09:44 +0000 (UTC)
Received: (qmail 25417 invoked by uid 500); 28 Jul 2014 09:09:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25373 invoked by uid 500); 28 Jul 2014 09:09:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25361 invoked by uid 99); 28 Jul 2014 09:09:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 09:09:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 09:09:37 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so10786224vcb.13
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 02:09:17 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type:content-transfer-encoding;
        bh=pjBlLop99rdY6+LYbqj9VayNgKHscZqnYKjMsTm9w2I=;
        b=XlZGk2AVeINv9ez3lat5CJITprdFulvhOsKHjQeNeb8OlRFdauM3gZhd5sRdomnBxf
         mg2Et52lXLEzRopNiJs7eJilzPH85o1qha2gd/ZCDRFzkHQ5fEkffBf8AZllw30itQ+R
         ebGYvJ4iXUbmy0Iu1/E0GIGOnQ44qRpDrgJCC5gi9bE8dtymKapdb0NQUnfIrCAvtZ4m
         8z4HabgmsMToIKZm6ZfQthb5nZnfXsEq9aDMHPWjeXSRZKnDyrEWLWUIf9XZuOX9uTu5
         gj6miOarw1AYNNJx9KMZPXh8sY7vuElDzB/sKYI5zu7MA0Ot/l5lxAEH4heO55JSXKJZ
         BhCQ==
X-Gm-Message-State: ALoCoQkyuz43b8L5zNJNo870+JeRoi8exK06oFLMdtIhB8+awMjxYqwntu3EXpNKoZ6311KBKZJA
X-Received: by 10.52.88.74 with SMTP id be10mr5008483vdb.54.1406538556092;
 Mon, 28 Jul 2014 02:09:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Mon, 28 Jul 2014 02:08:55 -0700 (PDT)
In-Reply-To: <6527FEF6-C3D5-4BB2-9153-BC1FBCCFA85B@gmail.com>
References: <CALte62zBjT6hNdNiw0SjzKQvPS_ROgeS-NERFeqmKSMjAqs93w@mail.gmail.com>
 <CABPQxsthk-1JFgU9uzJz0KCv5pdKKbpN+hbs+LfLHkeCWV8CkQ@mail.gmail.com>
 <B01A3EB3-9CE8-49C4-80DC-F68845189E1C@gmail.com> <CAMAsSdLHmmprM_vWXLCd5o492+gWVNVn7dxEH05n7Tvn_uBDeQ@mail.gmail.com>
 <6527FEF6-C3D5-4BB2-9153-BC1FBCCFA85B@gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 28 Jul 2014 10:08:55 +0100
Message-ID: <CAMAsSdKHRQnurbDmRrpWezXjtFuv4N9zutqznD7u26EkGSXRYA@mail.gmail.com>
Subject: Re: Utilize newer hadoop releases WAS: [VOTE] Release Apache Spark
 1.0.2 (RC1)
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Right, the scenario is, for example, that a class is added in release
2.5.0, but has been back-ported to a 2.4.1-based release. 2.4.1 isn't
missing anything from 2.4.1. But a version of "2.4.1" doesn't tell you
whether or not the class is there reliably.

By the way, I just found there is already such a class,
org.apache.hadoop.util.VersionInfo:

https://github.com/apache/hadoop-common/blob/release-2.4.1/hadoop-common-pr=
oject/hadoop-common/src/main/java/org/apache/hadoop/util/VersionInfo.java

It appears to have been around for a long time. Theoretical problems
aside, there may be cases where querying the version is a fine and
reliable solution.

On Jul 28, 2014 12:54 AM, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:
>
> We could also do this, though it would be great if the Hadoop project pro=
vided this version number as at least a baseline. It's up to distributors t=
o decide which version they report but I imagine they won't remove stuff th=
at's in the reported version number.
>
> Matei
>
> On Jul 27, 2014, at 1:57 PM, Sean Owen <sowen@cloudera.com> wrote:
>
> > Good idea, although it gets difficult in the context of multiple
> > distributions. Say change X is not present in version A, but present
> > in version B. If you depend on X, what version can you look for to
> > detect it? The distribution will return "A" or "A+X" or somesuch, but
> > testing for "A" will give an incorrect answer, and the code can't be
> > expected to look for everyone's "A+X" versions. Actually inspecting
> > the code is more robust if a bit messier.
> >
> > On Sun, Jul 27, 2014 at 9:50 PM, Matei Zaharia <matei.zaharia@gmail.com=
> wrote:
> >> For this particular issue, it would be good to know if Hadoop provides=
 an API to determine the Hadoop version. If not, maybe that can be added to=
 Hadoop in its next release, and we can check for it with reflection. We re=
cently added a SparkContext.version() method in Spark to let you tell the v=
ersion.
> >>
> >> Matei
> >>
> >> On Jul 27, 2014, at 12:19 PM, Patrick Wendell <pwendell@gmail.com> wro=
te:
> >>
> >>> Hey Ted,
> >>>
> >>> We always intend Spark to work with the newer Hadoop versions and
> >>> encourage Spark users to use the newest Hadoop versions for best
> >>> performance.
> >>>
> >>> We do try to be liberal in terms of supporting older versions as well=
.
> >>> This is because many people run older HDFS versions and we want Spark
> >>> to read and write data from them. So far we've been willing to do thi=
s
> >>> despite some maintenance cost.
> >>>
> >>> The reason is that for many users it's very expensive to do a
> >>> whole-sale upgrade of HDFS, but trying out new versions of Spark is
> >>> much easier. For instance, some of the largest scale Spark users run
> >>> fairly old or forked HDFS versions.
> >>>
> >>> - Patrick
> >>>
> >>> On Sun, Jul 27, 2014 at 12:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >>>> Thanks for replying, Patrick.
> >>>>
> >>>> The intention of my first email was for utilizing newer hadoop relea=
ses for
> >>>> their bug fixes. I am still looking for clean way of passing hadoop =
release
> >>>> version number to individual classes.
> >>>> Using newer hadoop releases would encourage pushing bug fixes / new
> >>>> features upstream. Ultimately Spark code would become cleaner.
> >>>>
> >>>> Cheers
> >>>>
> >>>> On Sun, Jul 27, 2014 at 8:52 AM, Patrick Wendell <pwendell@gmail.com=
> wrote:
> >>>>
> >>>>> Ted - technically I think you are correct, although I wouldn't
> >>>>> recommend disabling this lock. This lock is not expensive (acquired
> >>>>> once per task, as are many other locks already). Also, we've seen s=
ome
> >>>>> cases where Hadoop concurrency bugs ended up requiring multiple fix=
es
> >>>>> - concurrency of client access is not well tested in the Hadoop
> >>>>> codebase since most of the Hadoop tools to not use concurrent acces=
s.
> >>>>> So in general it's good to be conservative in what we expect of the
> >>>>> Hadoop client libraries.
> >>>>>
> >>>>> If you'd like to discuss this further, please fork a new thread, si=
nce
> >>>>> this is a vote thread. Thanks!
> >>>>>
> >>>>> On Fri, Jul 25, 2014 at 10:14 PM, Ted Yu <yuzhihong@gmail.com> wrot=
e:
> >>>>>> HADOOP-10456 is fixed in hadoop 2.4.1
> >>>>>>
> >>>>>> Does this mean that synchronization
> >>>>>> on HadoopRDD.CONFIGURATION_INSTANTIATION_LOCK can be bypassed for =
hadoop
> >>>>>> 2.4.1 ?
> >>>>>>
> >>>>>> Cheers
> >>>>>>
> >>>>>>
> >>>>>> On Fri, Jul 25, 2014 at 6:00 PM, Patrick Wendell <pwendell@gmail.c=
om>
> >>>>> wrote:
> >>>>>>
> >>>>>>> The most important issue in this release is actually an ammendmen=
t to
> >>>>>>> an earlier fix. The original fix caused a deadlock which was a
> >>>>>>> regression from 1.0.0->1.0.1:
> >>>>>>>
> >>>>>>> Issue:
> >>>>>>> https://issues.apache.org/jira/browse/SPARK-1097
> >>>>>>>
> >>>>>>> 1.0.1 Fix:
> >>>>>>> https://github.com/apache/spark/pull/1273/files (had a deadlock)
> >>>>>>>
> >>>>>>> 1.0.2 Fix:
> >>>>>>> https://github.com/apache/spark/pull/1409/files
> >>>>>>>
> >>>>>>> I failed to correctly label this on JIRA, but I've updated it!
> >>>>>>>
> >>>>>>> On Fri, Jul 25, 2014 at 5:35 PM, Michael Armbrust
> >>>>>>> <michael@databricks.com> wrote:
> >>>>>>>> That query is looking at "Fix Version" not "Target Version".  Th=
e fact
> >>>>>>> that
> >>>>>>>> the first one is still open is only because the bug is not resol=
ved in
> >>>>>>>> master.  It is fixed in 1.0.2.  The second one is partially fixe=
d in
> >>>>>>> 1.0.2,
> >>>>>>>> but is not worth blocking the release for.
> >>>>>>>>
> >>>>>>>>
> >>>>>>>> On Fri, Jul 25, 2014 at 4:23 PM, Nicholas Chammas <
> >>>>>>>> nicholas.chammas@gmail.com> wrote:
> >>>>>>>>
> >>>>>>>>> TD, there are a couple of unresolved issues slated for 1.0.2
> >>>>>>>>> <
> >>>>>>>>>
> >>>>>>>
> >>>>> https://issues.apache.org/jira/issues/?jql=3Dproject%20%3D%20SPARK%=
20AND%20fixVersion%20%3D%201.0.2%20AND%20resolution%20%3D%20Unresolved%20OR=
DER%20BY%20priority%20DESC
> >>>>>>>>>> .
> >>>>>>>>> Should they be edited somehow?
> >>>>>>>>>
> >>>>>>>>>
> >>>>>>>>> On Fri, Jul 25, 2014 at 7:08 PM, Tathagata Das <
> >>>>>>>>> tathagata.das1565@gmail.com>
> >>>>>>>>> wrote:
> >>>>>>>>>
> >>>>>>>>>> Please vote on releasing the following candidate as Apache Spa=
rk
> >>>>>>> version
> >>>>>>>>>> 1.0.2.
> >>>>>>>>>>
> >>>>>>>>>> This release fixes a number of bugs in Spark 1.0.1.
> >>>>>>>>>> Some of the notable ones are
> >>>>>>>>>> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted f=
ix
> >>>>> for
> >>>>>>>>>> SPARK-1199. The fix was reverted for 1.0.2.
> >>>>>>>>>> - SPARK-2576: NoClassDefFoundError when executing Spark QL que=
ry on
> >>>>>>>>>> HDFS CSV file.
> >>>>>>>>>> The full list is at http://s.apache.org/9NJ
> >>>>>>>>>>
> >>>>>>>>>> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> >>>>>>>>>>
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>
> >>>>> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=
=3D8fb6f00e195fb258f3f70f04756e07c259a2351f
> >>>>>>>>>>
> >>>>>>>>>> The release files, including signatures, digests, etc can be f=
ound
> >>>>> at:
> >>>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1/
> >>>>>>>>>>
> >>>>>>>>>> Release artifacts are signed with the following key:
> >>>>>>>>>> https://people.apache.org/keys/committer/tdas.asc
> >>>>>>>>>>
> >>>>>>>>>> The staging repository for this release can be found at:
> >>>>>>>>>>
> >>>>>>> https://repository.apache.org/content/repositories/orgapachespark=
-1024/
> >>>>>>>>>>
> >>>>>>>>>> The documentation corresponding to this release can be found a=
t:
> >>>>>>>>>> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> >>>>>>>>>>
> >>>>>>>>>> Please vote on releasing this package as Apache Spark 1.0.2!
> >>>>>>>>>>
> >>>>>>>>>> The vote is open until Tuesday, July 29, at 23:00 UTC and pass=
es if
> >>>>>>>>>> a majority of at least 3 +1 PMC votes are cast.
> >>>>>>>>>> [ ] +1 Release this package as Apache Spark 1.0.2
> >>>>>>>>>> [ ] -1 Do not release this package because ...
> >>>>>>>>>>
> >>>>>>>>>> To learn more about Apache Spark, please see
> >>>>>>>>>> http://spark.apache.org/
> >>>>>>>>>>
> >>>>>>>>>
> >>>>>>>
> >>>>>
> >>
>

From dev-return-8581-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 11:34:11 2014
Return-Path: <dev-return-8581-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8623911338
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 11:34:11 +0000 (UTC)
Received: (qmail 92653 invoked by uid 500); 28 Jul 2014 11:34:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92591 invoked by uid 500); 28 Jul 2014 11:34:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92579 invoked by uid 99); 28 Jul 2014 11:34:10 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 11:34:10 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of xiaodi@sjtu.edu.cn designates 202.112.26.52 as permitted sender)
Received: from [202.112.26.52] (HELO proxy01.sjtu.edu.cn) (202.112.26.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 11:34:03 +0000
Received: from proxy03.sjtu.edu.cn (unknown [202.121.179.33])
	by proxy01.sjtu.edu.cn (Postfix) with ESMTP id 5A8B4260393
	for <dev@spark.apache.org>; Mon, 28 Jul 2014 19:33:42 +0800 (CST)
Received: from localhost (localhost [127.0.0.1])
	by proxy03.sjtu.edu.cn (Postfix) with ESMTP id 5052E260B75;
	Mon, 28 Jul 2014 19:33:42 +0800 (GMT-8)
X-Virus-Scanned: amavisd-new at 
Received: from proxy03.sjtu.edu.cn ([127.0.0.1])
	by localhost (proxy03.sjtu.edu.cn [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id AJhxmQNRns1u; Mon, 28 Jul 2014 19:33:42 +0800 (GMT-8)
Received: from loca.ipads-lab.se.sjtu.edu.cn (unknown [202.120.40.83])
	(Authenticated sender: xiaodi)
	by proxy03.sjtu.edu.cn (Postfix) with ESMTPSA id 38E9E260A2A;
	Mon, 28 Jul 2014 19:33:42 +0800 (GMT-8)
Message-ID: <53D63515.8060102@sjtu.edu.cn>
Date: Mon, 28 Jul 2014 19:33:41 +0800
From: Larry Xiao <xiaodi@sjtu.edu.cn>
User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:24.0) Gecko/20100101 Thunderbird/24.6.0
MIME-Version: 1.0
To: dev@spark.apache.org
CC: xiaodi@sjtu.edu.cn
Subject: package/assemble with local spark
Content-Type: text/plain; charset=ISO-8859-1; format=flowed
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

How do you package an app with modified spark?

In seems sbt would resolve the dependencies, and use the official spark 
release.

Thank you!

Larry

From dev-return-8582-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 14:16:25 2014
Return-Path: <dev-return-8582-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 948C111878
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 14:16:25 +0000 (UTC)
Received: (qmail 62226 invoked by uid 500); 28 Jul 2014 14:16:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62171 invoked by uid 500); 28 Jul 2014 14:16:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62159 invoked by uid 99); 28 Jul 2014 14:16:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 14:16:22 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of jitendra.shelar410@gmail.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 14:16:16 +0000
Received: by mail-qg0-f53.google.com with SMTP id q107so8638741qgd.12
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 07:15:56 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=q4IZMlGe7JuWyA8VT/7tQu5lb2fpkbx9mSqZNQZ/cS0=;
        b=sq/zy5AOckPn64AFVjd0iaSYrIqSLdiP8DE3d7u8J2K+LHthp4INEwik76ufFcpUJH
         sfH1eO/loWQCeX0ycnR2fvxNkBcb3YS0XMz9lf1IYY1SrUzkMU3y6Fci5lTVzhKyMcOS
         hwo6Y8lm/I9jhkS9K2qV/5MAK4XGV0WS/kqY8TbYsXfm/11gkgRqTy4AEMSnpQ/NDITB
         w8VxPoc23m00sz9JoR0+EgU24l9Na5vZz+FsD5+seh4cq0V55M4T/pLYgV8+QUWM8jeH
         +L/aYazYbVQb0/OqdXReuXRTPH/Vsi3vWzF0XUKi1dBCgFYwcDAR3Nd+m9P2bL61EcV0
         oteA==
MIME-Version: 1.0
X-Received: by 10.140.97.131 with SMTP id m3mr18641522qge.80.1406556955874;
 Mon, 28 Jul 2014 07:15:55 -0700 (PDT)
Received: by 10.140.95.205 with HTTP; Mon, 28 Jul 2014 07:15:55 -0700 (PDT)
Date: Mon, 28 Jul 2014 19:45:55 +0530
Message-ID: <CADX=mBpCvx9C_uLN6k3ycWJ+pE376-mFYchs0e-NMTbwR0-Nyg@mail.gmail.com>
Subject: Fraud management system implementation
From: jitendra shelar <jitendra.shelar410@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi,

I am new to spark. I am learning spark and scala.

I had some queries.

1) Can somebody please tell me if it is possible to implement credit
card fraud management system using spark?
2) If yes, can somebody please guide me how to proceed.
3) Shall I prefer Scala or Java for this implementation?

4) Please suggest me some pointers related to Hidden Markonav Model
(HMM) and anomaly detection in data mining (using spark).

Thanks,
Jitendra

From dev-return-8583-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 14:44:45 2014
Return-Path: <dev-return-8583-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2DE3911939
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 14:44:45 +0000 (UTC)
Received: (qmail 15599 invoked by uid 500); 28 Jul 2014 14:44:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15530 invoked by uid 500); 28 Jul 2014 14:44:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 15514 invoked by uid 99); 28 Jul 2014 14:44:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 14:44:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of snunez@hortonworks.com designates 209.85.192.174 as permitted sender)
Received: from [209.85.192.174] (HELO mail-pd0-f174.google.com) (209.85.192.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 14:44:36 +0000
Received: by mail-pd0-f174.google.com with SMTP id fp1so9957126pdb.5
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 07:44:16 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:message-id
         :thread-topic:mime-version:content-type;
        bh=0j2xR/BJDHv8nr3Z1i/3v0LXUyP23jglBtxGXNH9SRw=;
        b=g25prcwg3btUsPo+OS7Zm83OkCR+/syhFfJALQsmcpRHhWatOaUacx84a9HB+rXi5t
         NYQiqMeWy2OZU6O5JK+NlTi1OA3tS8OtiRZO2p6AJ8d89GBqPIGJnjURIAu/Bvs7p4wL
         FNqz1B4DdsVXk+e6lvDI0yR9lNEGxIyVHPdmCQrT+SNk+JzqSwZSZBmrXdNtFPA9Aewr
         9J3XmI5qC89kXKccXZxOm4nzrfIovQJiBBTx0ewj+kV/6dguvb3WekFkhyDA3I2A4KS8
         U8KHbNx8iVz973i/tepWjbDRBoQyycPsjiZ5xxC3ASZyEeduxpaJwN8Lb4EESqHqMOPO
         eacA==
X-Gm-Message-State: ALoCoQlYSlVeQKEnqyT8QpjDgRiCJy/bW0EatbXB87SHu+zkB0K57BiIKu9l6gtHBhKZyazRlnQjFyI+tJphUYs1y18zJjozpNvgiIyU39Ljyj2u21VO1dw=
X-Received: by 10.66.66.135 with SMTP id f7mr39653148pat.32.1406558653981;
        Mon, 28 Jul 2014 07:44:13 -0700 (PDT)
Received: from [10.11.3.247] ([192.175.27.2])
        by mx.google.com with ESMTPSA id gy2sm17880283pbb.50.2014.07.28.07.44.12
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 28 Jul 2014 07:44:13 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Mon, 28 Jul 2014 07:44:08 -0700
Subject: Working Formula for Hive 0.13?
From: Steve Nunez <snunez@hortonworks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <CFFBAFC8.2D91%snunez@hortonworks.com>
Thread-Topic: Working Formula for Hive 0.13?
Mime-version: 1.0
Content-type: multipart/alternative;
	boundary="B_3489378252_104607"
X-Virus-Checked: Checked by ClamAV on apache.org

--B_3489378252_104607
Content-type: text/plain; charset=US-ASCII

I saw a note earlier, perhaps on the user list, that at least one person is
using Hive 0.13. Anyone got a working build configuration for this version
of Hive?

Regards,
- Steve



-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

--B_3489378252_104607--



From dev-return-8584-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 15:01:27 2014
Return-Path: <dev-return-8584-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 96FBC119A4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 15:01:27 +0000 (UTC)
Received: (qmail 59329 invoked by uid 500); 28 Jul 2014 15:01:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59277 invoked by uid 500); 28 Jul 2014 15:01:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59261 invoked by uid 99); 28 Jul 2014 15:01:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:01:26 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.179 as permitted sender)
Received: from [209.85.160.179] (HELO mail-yk0-f179.google.com) (209.85.160.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:01:22 +0000
Received: by mail-yk0-f179.google.com with SMTP id 142so4693477ykq.10
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 08:01:01 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=GEVAEHiCBJ7XFPmT3h2rCc/d45c6S9q1+deY/aZ1Vk4=;
        b=Lnre1QWy3zmT1dMjoX/LB61JcLnxvei34dhA/0ouS3zeg4v5tVMlfBiX+gZStHraMP
         Cni19M3sRgVVA9xS8i19FVWVNjC8Iue+MUbGdn8cUGFrAOQQLY79k/tgcj5BIuGdgYoK
         jVpqXDt9kt0pz3wWCDBDxK62AKsOCI7zCzDStcsbBvM+A/Q+tGsTBR3GMIrq0ZKoZmN/
         R5aiRfast2W5yBDn81WlEU2uzwrAFO+iSKEzwajAjRAbqMF+qnPQLaIFr1ybIVXBT7By
         Dj8AMp4JSz/XFJZ4p2uPq2ja8c3HjL0u9IkOWW5M4JdiombvXVcrJxXHOx2zpjemYJ0a
         RuGQ==
MIME-Version: 1.0
X-Received: by 10.236.125.72 with SMTP id y48mr12871675yhh.111.1406559661917;
 Mon, 28 Jul 2014 08:01:01 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Mon, 28 Jul 2014 08:01:01 -0700 (PDT)
In-Reply-To: <CFFBAFC8.2D91%snunez@hortonworks.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
Date: Mon, 28 Jul 2014 08:01:01 -0700
Message-ID: <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf303b3c97010c9604ff423177
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf303b3c97010c9604ff423177
Content-Type: text/plain; charset=UTF-8

I found 0.13.1 artifacts in maven:
http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar

However, Spark uses groupId of org.spark-project.hive, not org.apache.hive

Can someone tell me how it is supposed to work ?

Cheers


On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <snunez@hortonworks.com> wrote:

> I saw a note earlier, perhaps on the user list, that at least one person is
> using Hive 0.13. Anyone got a working build configuration for this version
> of Hive?
>
> Regards,
> - Steve
>
>
>
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity to
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified that
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediately
> and delete it from your system. Thank You.
>

--20cf303b3c97010c9604ff423177--

From dev-return-8585-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 15:17:02 2014
Return-Path: <dev-return-8585-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A7D6611A15
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 15:17:02 +0000 (UTC)
Received: (qmail 5235 invoked by uid 500); 28 Jul 2014 15:17:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5170 invoked by uid 500); 28 Jul 2014 15:17:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5158 invoked by uid 99); 28 Jul 2014 15:17:01 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:17:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:16:43 +0000
Received: by mail-wg0-f41.google.com with SMTP id z12so7487651wgg.12
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 08:16:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=6pZD13AHPis3AQGNW+NeSECixCKclQSoKTQMXIdCjEE=;
        b=yY7J8UxlGgvNIQ+Fgl7hE6FhugML++lbe6ZdYrGEq4vNPHkxuHJSoHCUvwK28TqCti
         XnxzfSUV5z1LQy4V58Xq7lPRyLliCDwibRGwxT47WHSpVJvBGarO4YhaQMup02807GN1
         vp0JVIgw8Cyrkv/fDRAo68ZhXQtzb+NVIa7WFyztM+fkaYRHjyUkf3SobgIqZBRc1WFV
         HbG1fsuT4icm9Ae7ycULqL0E4CTLL0QTYfd+ZkLZAK/b01MpfSzHuhEJRx4rA1PoLdcu
         q0dPjT7O7pTFhZqzuiVok2bXSJmKy/yWqOSbiMpIRnBg895uQkgYfEC736URqEoVY0IA
         D+0w==
X-Received: by 10.194.3.74 with SMTP id a10mr42836849wja.85.1406560576620;
 Mon, 28 Jul 2014 08:16:16 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 28 Jul 2014 08:15:36 -0700 (PDT)
In-Reply-To: <CADX=mBpCvx9C_uLN6k3ycWJ+pE376-mFYchs0e-NMTbwR0-Nyg@mail.gmail.com>
References: <CADX=mBpCvx9C_uLN6k3ycWJ+pE376-mFYchs0e-NMTbwR0-Nyg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 28 Jul 2014 11:15:36 -0400
Message-ID: <CAOhmDzfY6JOHq=-cy_Rou20-E46dhsxbrbH0P-E5_oruV6f2Mw@mail.gmail.com>
Subject: Re: Fraud management system implementation
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3a815c8650f804ff426746
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a815c8650f804ff426746
Content-Type: text/plain; charset=UTF-8

This sounds more like a user list <https://spark.apache.org/community.html>
question. This is the dev list, where people discuss things related to
contributing code and such to Spark.


On Mon, Jul 28, 2014 at 10:15 AM, jitendra shelar <
jitendra.shelar410@gmail.com> wrote:

> Hi,
>
> I am new to spark. I am learning spark and scala.
>
> I had some queries.
>
> 1) Can somebody please tell me if it is possible to implement credit
> card fraud management system using spark?
> 2) If yes, can somebody please guide me how to proceed.
> 3) Shall I prefer Scala or Java for this implementation?
>
> 4) Please suggest me some pointers related to Hidden Markonav Model
> (HMM) and anomaly detection in data mining (using spark).
>
> Thanks,
> Jitendra
>

--047d7b3a815c8650f804ff426746--

From dev-return-8586-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 15:27:31 2014
Return-Path: <dev-return-8586-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8212511A68
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 15:27:31 +0000 (UTC)
Received: (qmail 26030 invoked by uid 500); 28 Jul 2014 15:27:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25966 invoked by uid 500); 28 Jul 2014 15:27:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 25954 invoked by uid 99); 28 Jul 2014 15:27:30 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:27:30 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:27:14 +0000
Received: by mail-vc0-f178.google.com with SMTP id la4so11518478vcb.9
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 08:26:49 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=yLcFgRn5GH5dU8iULVCiC9aIHr1E6kjEE1tclGt66as=;
        b=YtzvOj3NxTJka6fOxl7zMfFqFbrCzZc7CuyzqTcchrVfsNUfPT5RjSJpdsNcBEu0Pm
         fPSMwI3Ut5lw6Hg2nahgLnxcZ+hjlPegEB/OhZMUNrlc7FHCCHnUnVF69kTzWBCRi4dW
         iNTBqdI89nAeq4tofyy8mDuPl9Q+LdXMktnymmwz+AbdIjnsqZG+O+J/rDceDqSkydm/
         L5aDRY1MKfxT3W13YkBb8AcD9To+99JfU/3Yss2oMi+8GCiCtOwGuXg3OIKyCEZxQLNP
         tVktdc/uP98m5pR+PQUAQEJvzkP6BNDWseFhpqpqoW/7g9Jysn2I1MFPpqIUYrkiNp+l
         zclQ==
X-Gm-Message-State: ALoCoQlcr25/bWvzE/NHBlnewm3OJfSkIZD8diE8GHLME5KcKx3CxPd0OWAkhUW9fZsOOUmKDA/3
X-Received: by 10.52.88.74 with SMTP id be10mr7575957vdb.54.1406561209228;
 Mon, 28 Jul 2014 08:26:49 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Mon, 28 Jul 2014 08:26:29 -0700 (PDT)
In-Reply-To: <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com> <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 28 Jul 2014 16:26:29 +0100
Message-ID: <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

The reason for org.spark-project.hive is that Spark relies on
hive-exec, but the Hive project does not publish this artifact by
itself, only with all its dependencies as an uber jar. Maybe that's
been improved. If so, you need to point at the new hive-exec and
perhaps sort out its dependencies manually in your build.

On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> I found 0.13.1 artifacts in maven:
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
>
> However, Spark uses groupId of org.spark-project.hive, not org.apache.hive
>
> Can someone tell me how it is supposed to work ?
>
> Cheers
>
>
> On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <snunez@hortonworks.com> wrote:
>
>> I saw a note earlier, perhaps on the user list, that at least one person is
>> using Hive 0.13. Anyone got a working build configuration for this version
>> of Hive?
>>
>> Regards,
>> - Steve
>>
>>
>>
>> --
>> CONFIDENTIALITY NOTICE
>> NOTICE: This message is intended for the use of the individual or entity to
>> which it is addressed and may contain information that is confidential,
>> privileged and exempt from disclosure under applicable law. If the reader
>> of this message is not the intended recipient, you are hereby notified that
>> any printing, copying, dissemination, distribution, disclosure or
>> forwarding of this communication is strictly prohibited. If you have
>> received this communication in error, please contact the sender immediately
>> and delete it from your system. Thank You.
>>

From dev-return-8587-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 15:48:23 2014
Return-Path: <dev-return-8587-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CE0FE11B34
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 15:48:23 +0000 (UTC)
Received: (qmail 79040 invoked by uid 500); 28 Jul 2014 15:48:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78984 invoked by uid 500); 28 Jul 2014 15:48:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78958 invoked by uid 99); 28 Jul 2014 15:48:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:48:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.170 as permitted sender)
Received: from [209.85.160.170] (HELO mail-yk0-f170.google.com) (209.85.160.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 15:48:19 +0000
Received: by mail-yk0-f170.google.com with SMTP id 9so4872616ykp.29
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 08:47:58 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=7edxAgETUWtnw1vQoUaQnZcwMGKKOqXq6/WwFuvRyMs=;
        b=L6zsKoFhyVMaEfj2m92HazSsJqDKvpMglw8pbYZG3M/z7pYQcWOMrni27/mL1EI7oh
         N7Ls0nCUGnbLhYdNJJvOpJhBH7+64qAqr9KlcSQDKssPrdxxztzJWg8n+PZW4KnnF0qy
         48nhz9zmQ+oD+ybbdxhHb0qbffsstwcgsI35I8zdESmUkl5RIeC65xq8u3J+OQxlO3rh
         5GjadqhQx/buDGDTKiyciVCeOaw0z23YzAYwlSC2iVWwdkYZ2qIKtOCixyOV4H2xtk8W
         VdmuYSrsHpMnwzHkh41wjKBgnf2vCUobuHz7CNaewavNBQnSK96j4YYSXfWGmr0QtDd7
         ANXg==
MIME-Version: 1.0
X-Received: by 10.236.145.4 with SMTP id o4mr2702747yhj.188.1406562478401;
 Mon, 28 Jul 2014 08:47:58 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Mon, 28 Jul 2014 08:47:58 -0700 (PDT)
In-Reply-To: <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
Date: Mon, 28 Jul 2014 08:47:58 -0700
Message-ID: <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf30426fc2e12cca04ff42d863
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30426fc2e12cca04ff42d863
Content-Type: text/plain; charset=UTF-8

hive-exec (as of 0.13.1) is published here:
http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar

Should a JIRA be opened so that dependency on hive-metastore can be
replaced by dependency on hive-exec ?

Cheers


On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com> wrote:

> The reason for org.spark-project.hive is that Spark relies on
> hive-exec, but the Hive project does not publish this artifact by
> itself, only with all its dependencies as an uber jar. Maybe that's
> been improved. If so, you need to point at the new hive-exec and
> perhaps sort out its dependencies manually in your build.
>
> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> > I found 0.13.1 artifacts in maven:
> >
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
> >
> > However, Spark uses groupId of org.spark-project.hive, not
> org.apache.hive
> >
> > Can someone tell me how it is supposed to work ?
> >
> > Cheers
> >
> >
> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <snunez@hortonworks.com>
> wrote:
> >
> >> I saw a note earlier, perhaps on the user list, that at least one
> person is
> >> using Hive 0.13. Anyone got a working build configuration for this
> version
> >> of Hive?
> >>
> >> Regards,
> >> - Steve
> >>
> >>
> >>
> >> --
> >> CONFIDENTIALITY NOTICE
> >> NOTICE: This message is intended for the use of the individual or
> entity to
> >> which it is addressed and may contain information that is confidential,
> >> privileged and exempt from disclosure under applicable law. If the
> reader
> >> of this message is not the intended recipient, you are hereby notified
> that
> >> any printing, copying, dissemination, distribution, disclosure or
> >> forwarding of this communication is strictly prohibited. If you have
> >> received this communication in error, please contact the sender
> immediately
> >> and delete it from your system. Thank You.
> >>
>

--20cf30426fc2e12cca04ff42d863--

From dev-return-8588-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 16:14:26 2014
Return-Path: <dev-return-8588-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 54C5811C37
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 16:14:26 +0000 (UTC)
Received: (qmail 67838 invoked by uid 500); 28 Jul 2014 16:14:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67784 invoked by uid 500); 28 Jul 2014 16:14:25 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67773 invoked by uid 99); 28 Jul 2014 16:14:25 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:14:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.53 as permitted sender)
Received: from [209.85.192.53] (HELO mail-qg0-f53.google.com) (209.85.192.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:14:05 +0000
Received: by mail-qg0-f53.google.com with SMTP id q107so8621093qgd.26
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 09:13:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=/vPWyujmYzd/uXPwSmmiu2kIMWrg/4WaGJ+9ojwJf7I=;
        b=cO9aPe/K4NxMxaUX+A1GU+6MDlweSipdWJ3cSGQuouPJtER+yOd+WDG3DyJoQmGauQ
         jNznq78v4euSjs7jLpRdWhp7+8c6xFBwbi9LkCi+137iNzwfMCL1LbmMWvUX2fcSiMEE
         gH9C4ynzGZCjE7y7Antd8t1yVEmRx4ZnSbOSCqyAyPbZ16LVJ/3aSu5cU4nX+jH5gs4o
         Xez1sTyRa1YUvIXJhHQwChj5Zf5WjpDCHeGuVuKghKaIc3eVuleewiCARe2WXFogy4Et
         KUTdG5dUmePgCp0HqhXRQY6xUYONQ52mW8g4+iUV0HoPDtiLbXo0oqTX8kwodlyq5woP
         qhcg==
X-Gm-Message-State: ALoCoQli9eTr8ARcVTW1oM+IsCa+LU5+cYsNiN6ptkLpMHVUBtVH3bTk1kDREED54iNZ4lZ/tFAB
MIME-Version: 1.0
X-Received: by 10.224.43.196 with SMTP id x4mr58586994qae.63.1406564020544;
 Mon, 28 Jul 2014 09:13:40 -0700 (PDT)
Received: by 10.140.92.56 with HTTP; Mon, 28 Jul 2014 09:13:40 -0700 (PDT)
In-Reply-To: <CADX=mBpCvx9C_uLN6k3ycWJ+pE376-mFYchs0e-NMTbwR0-Nyg@mail.gmail.com>
References: <CADX=mBpCvx9C_uLN6k3ycWJ+pE376-mFYchs0e-NMTbwR0-Nyg@mail.gmail.com>
Date: Mon, 28 Jul 2014 09:13:40 -0700
Message-ID: <CACBYxKJWsjcjCMHdmAJ4qHmV1rzdxDwhEoVQCOEtoad_50oiKw@mail.gmail.com>
Subject: Re: Fraud management system implementation
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: "user@spark.apache.org" <user@spark.apache.org>
Cc: jitendra.shelar410@gmail.com
Content-Type: multipart/alternative; boundary=047d7bdc875ecc9c3604ff433461
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bdc875ecc9c3604ff433461
Content-Type: text/plain; charset=UTF-8

+user list
bcc: dev list

It's definitely possible to implement credit fraud management using Spark.
 A good start would be using some of the supervised learning algorithms
that Spark provides in MLLib (logistic regression or linear SVMs).

Spark doesn't have any HMM implementation right now.  Sean Owen has a great
talk on performing anomaly detection with KMeans clustering in Spark -
https://www.youtube.com/watch?v=TC5cKYBZAeI

-Sandy


On Mon, Jul 28, 2014 at 7:15 AM, jitendra shelar <
jitendra.shelar410@gmail.com> wrote:

> Hi,
>
> I am new to spark. I am learning spark and scala.
>
> I had some queries.
>
> 1) Can somebody please tell me if it is possible to implement credit
> card fraud management system using spark?
> 2) If yes, can somebody please guide me how to proceed.
> 3) Shall I prefer Scala or Java for this implementation?
>
> 4) Please suggest me some pointers related to Hidden Markonav Model
> (HMM) and anomaly detection in data mining (using spark).
>
> Thanks,
> Jitendra
>

--047d7bdc875ecc9c3604ff433461--

From dev-return-8589-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 16:16:36 2014
Return-Path: <dev-return-8589-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 63B2911C50
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 16:16:36 +0000 (UTC)
Received: (qmail 73521 invoked by uid 500); 28 Jul 2014 16:16:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73465 invoked by uid 500); 28 Jul 2014 16:16:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 73446 invoked by uid 99); 28 Jul 2014 16:16:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:16:35 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of sowen@cloudera.com designates 209.85.220.173 as permitted sender)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:16:31 +0000
Received: by mail-vc0-f173.google.com with SMTP id hy10so11487716vcb.32
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 09:16:10 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=B6P5LOvoJFBvdugU+H0NkXtEOokHQltkU9Xc+kM+Zds=;
        b=cRVhNqs06D+H1JwVpyFTe/btrZ6sL1KowpVpq1ZH1qCMSJUImJW5H+UB2aWY3LteYh
         osbfMesq5QPifl0P2tU+Wccnu0DnRYF0srUnxnf0jMMEOU/IITJJ2hmVvMUnHEoAT+DD
         FO8+2uy/amTWe1Wjj47NwGZx2YokrxFFUIu9JLilcVptLjTIfb9gUTEHfCgQNpHoRodK
         q21TzoRlSfEyhhsIfoN+wTZ2dNNMPcRyr74iR5xPYCxpcDYMwN/8KhUPZuK+pWxcVLcr
         /Lfq6MyTHxzr1KjoaZQYR8QJjfMT3P2Q3lZHvtus91cfZdtp1I3nZ4szl7v6rTQZY1v8
         oTNg==
X-Gm-Message-State: ALoCoQmp4WnEuateWmpiX/jLZXM7sRXs0gmj64NnSLq63vSmCd5lvxhQ+OlvjoNMpRYzF3w8sL8R
X-Received: by 10.220.163.130 with SMTP id a2mr1747992vcy.52.1406564170814;
 Mon, 28 Jul 2014 09:16:10 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Mon, 28 Jul 2014 09:15:50 -0700 (PDT)
In-Reply-To: <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com> <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
 <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com> <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Mon, 28 Jul 2014 17:15:50 +0100
Message-ID: <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yes, it is published. As of previous versions, at least, hive-exec
included all of its dependencies *in its artifact*, making it unusable
as-is because it contained copies of dependencies that clash with
versions present in other artifacts, and can't be managed with Maven
mechanisms.

I am not sure why hive-exec was not published normally, with just its
own classes. That's why it was copied, into an artifact with just
hive-exec code.

You could do the same thing for hive-exec 0.13.1.
Or maybe someone knows that it's published more 'normally' now.
I don't think hive-metastore is related to this question?

I am no expert on the Hive artifacts, just remembering what the issue
was initially in case it helps you get to a similar solution.

On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> hive-exec (as of 0.13.1) is published here:
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
>
> Should a JIRA be opened so that dependency on hive-metastore can be
> replaced by dependency on hive-exec ?
>
> Cheers
>
>
> On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com> wrote:
>
>> The reason for org.spark-project.hive is that Spark relies on
>> hive-exec, but the Hive project does not publish this artifact by
>> itself, only with all its dependencies as an uber jar. Maybe that's
>> been improved. If so, you need to point at the new hive-exec and
>> perhaps sort out its dependencies manually in your build.
>>
>> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> > I found 0.13.1 artifacts in maven:
>> >
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
>> >
>> > However, Spark uses groupId of org.spark-project.hive, not
>> org.apache.hive
>> >
>> > Can someone tell me how it is supposed to work ?
>> >
>> > Cheers
>> >
>> >
>> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <snunez@hortonworks.com>
>> wrote:
>> >
>> >> I saw a note earlier, perhaps on the user list, that at least one
>> person is
>> >> using Hive 0.13. Anyone got a working build configuration for this
>> version
>> >> of Hive?
>> >>
>> >> Regards,
>> >> - Steve
>> >>
>> >>
>> >>
>> >> --
>> >> CONFIDENTIALITY NOTICE
>> >> NOTICE: This message is intended for the use of the individual or
>> entity to
>> >> which it is addressed and may contain information that is confidential,
>> >> privileged and exempt from disclosure under applicable law. If the
>> reader
>> >> of this message is not the intended recipient, you are hereby notified
>> that
>> >> any printing, copying, dissemination, distribution, disclosure or
>> >> forwarding of this communication is strictly prohibited. If you have
>> >> received this communication in error, please contact the sender
>> immediately
>> >> and delete it from your system. Thank You.
>> >>
>>

From dev-return-8590-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 16:34:44 2014
Return-Path: <dev-return-8590-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D84A111CE5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 16:34:44 +0000 (UTC)
Received: (qmail 19378 invoked by uid 500); 28 Jul 2014 16:34:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19328 invoked by uid 500); 28 Jul 2014 16:34:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19316 invoked by uid 99); 28 Jul 2014 16:34:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:34:43 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.178 as permitted sender)
Received: from [209.85.160.178] (HELO mail-yk0-f178.google.com) (209.85.160.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:34:39 +0000
Received: by mail-yk0-f178.google.com with SMTP id 142so4811545ykq.37
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 09:34:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=6dlm1IrwDneu4zJZ4Ofk4qW9WR/iml+yZMdLe/1iRFc=;
        b=XaWTXd0ja//OcVmGu/+dXyhiLJB9+VJBaEGLypjNBbmjt6I0L2NapeW0Kf1BdIxczj
         LDPovXkcXLQstgNZDHzBjTgfy1y55+jsT7+hocdZeaCB8741bkG7DwfHVFjWLqLcX81N
         jup1vG4I8YIT6HfsR3JzQXrAzHvkPZptMRqMPRPu9l8Mkpkq+yNwvlsAgxN4bnNanm6H
         ItFd+BsG8dwXdV+tre1BrnS5CPG31ma2uF+CSqcdTQoov5NoMDmj46gCRPfwOLlB90Rl
         sUf2czSIJCQF+pRbY1FTJVCHtlg/ma/dHTPKq2zzYa8dLttifP5qB3L6M4FY7A1F3HUa
         nZsA==
MIME-Version: 1.0
X-Received: by 10.236.14.34 with SMTP id c22mr14042921yhc.97.1406565259272;
 Mon, 28 Jul 2014 09:34:19 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Mon, 28 Jul 2014 09:34:19 -0700 (PDT)
In-Reply-To: <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
Date: Mon, 28 Jul 2014 09:34:19 -0700
Message-ID: <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013a29daa1eb5104ff437e4e
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a29daa1eb5104ff437e4e
Content-Type: text/plain; charset=UTF-8

Talked with Owen offline. He confirmed that as of 0.13, hive-exec is still
uber jar.

Right now I am facing the following error building against Hive 0.13.1 :

[ERROR] Failed to execute goal on project spark-hive_2.10: Could not
resolve dependencies for project
org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
artifacts could not be resolved:
org.spark-project.hive:hive-metastore:jar:0.13.1,
org.spark-project.hive:hive-exec:jar:0.13.1,
org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
org.spark-project.hive:hive-metastore:jar:0.13.1 in
http://repo.maven.apache.org/maven2 was cached in the local repository,
resolution will not be reattempted until the update interval of maven-repo
has elapsed or updates are forced -> [Help 1]

Some hint would be appreciated.

Cheers


On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com> wrote:

> Yes, it is published. As of previous versions, at least, hive-exec
> included all of its dependencies *in its artifact*, making it unusable
> as-is because it contained copies of dependencies that clash with
> versions present in other artifacts, and can't be managed with Maven
> mechanisms.
>
> I am not sure why hive-exec was not published normally, with just its
> own classes. That's why it was copied, into an artifact with just
> hive-exec code.
>
> You could do the same thing for hive-exec 0.13.1.
> Or maybe someone knows that it's published more 'normally' now.
> I don't think hive-metastore is related to this question?
>
> I am no expert on the Hive artifacts, just remembering what the issue
> was initially in case it helps you get to a similar solution.
>
> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> > hive-exec (as of 0.13.1) is published here:
> >
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
> >
> > Should a JIRA be opened so that dependency on hive-metastore can be
> > replaced by dependency on hive-exec ?
> >
> > Cheers
> >
> >
> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com> wrote:
> >
> >> The reason for org.spark-project.hive is that Spark relies on
> >> hive-exec, but the Hive project does not publish this artifact by
> >> itself, only with all its dependencies as an uber jar. Maybe that's
> >> been improved. If so, you need to point at the new hive-exec and
> >> perhaps sort out its dependencies manually in your build.
> >>
> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> > I found 0.13.1 artifacts in maven:
> >> >
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
> >> >
> >> > However, Spark uses groupId of org.spark-project.hive, not
> >> org.apache.hive
> >> >
> >> > Can someone tell me how it is supposed to work ?
> >> >
> >> > Cheers
> >> >
> >> >
> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <snunez@hortonworks.com>
> >> wrote:
> >> >
> >> >> I saw a note earlier, perhaps on the user list, that at least one
> >> person is
> >> >> using Hive 0.13. Anyone got a working build configuration for this
> >> version
> >> >> of Hive?
> >> >>
> >> >> Regards,
> >> >> - Steve
> >> >>
> >> >>
> >> >>
> >> >> --
> >> >> CONFIDENTIALITY NOTICE
> >> >> NOTICE: This message is intended for the use of the individual or
> >> entity to
> >> >> which it is addressed and may contain information that is
> confidential,
> >> >> privileged and exempt from disclosure under applicable law. If the
> >> reader
> >> >> of this message is not the intended recipient, you are hereby
> notified
> >> that
> >> >> any printing, copying, dissemination, distribution, disclosure or
> >> >> forwarding of this communication is strictly prohibited. If you have
> >> >> received this communication in error, please contact the sender
> >> immediately
> >> >> and delete it from your system. Thank You.
> >> >>
> >>
>

--089e013a29daa1eb5104ff437e4e--

From dev-return-8591-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 16:55:57 2014
Return-Path: <dev-return-8591-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6A8D11DAE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 16:55:57 +0000 (UTC)
Received: (qmail 92840 invoked by uid 500); 28 Jul 2014 16:55:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 92782 invoked by uid 500); 28 Jul 2014 16:55:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 92769 invoked by uid 99); 28 Jul 2014 16:55:56 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:55:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.218.44 as permitted sender)
Received: from [209.85.218.44] (HELO mail-oi0-f44.google.com) (209.85.218.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 16:55:40 +0000
Received: by mail-oi0-f44.google.com with SMTP id x69so6275827oia.17
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 09:55:15 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=2FOHK+480XK4WFMP3wmVEBH5OhD4ACoYJGZ84aJWipg=;
        b=Rri2dBgsXJ4u2HKtDwXiJSBUvuNs/Dpv5JTaqe0ew5k6CTxLAGEXXWJ9KYBD71Bg7v
         VQ5pWAPC6Rz7Iq/alcQmrLC8hFtVMb3lKUPY95dsTBeWgZuu3xMcTm1RX8xsvxDuWEQt
         +4Qj8BjvNBUKxz9nFKFB6rOIGxh8cPoZ7HzSzvRLNN1AUAxalPcK4iSNonyBhF8Ks4LN
         fiBq0IYh+81HqAq9o+o/7TGw/OcWjHcy04Rzhtl+rst8Li8kp4dNAEfaohXetUc6NQGT
         3y9yAPut8c5z6ZUUCY5kAUxYBWB1fcmY4YZYMOrMcxM+uZHITJBp0OrwKSAj0Zp8L3Xo
         v9pg==
MIME-Version: 1.0
X-Received: by 10.182.18.101 with SMTP id v5mr36490980obd.64.1406566515019;
 Mon, 28 Jul 2014 09:55:15 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 28 Jul 2014 09:55:14 -0700 (PDT)
In-Reply-To: <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
Date: Mon, 28 Jul 2014 09:55:14 -0700
Message-ID: <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

It would be great if the hive team can fix that issue. If not, we'll
have to continue forking our own version of Hive to change the way it
publishes artifacts.

- Patrick

On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> Talked with Owen offline. He confirmed that as of 0.13, hive-exec is still
> uber jar.
>
> Right now I am facing the following error building against Hive 0.13.1 :
>
> [ERROR] Failed to execute goal on project spark-hive_2.10: Could not
> resolve dependencies for project
> org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
> artifacts could not be resolved:
> org.spark-project.hive:hive-metastore:jar:0.13.1,
> org.spark-project.hive:hive-exec:jar:0.13.1,
> org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> org.spark-project.hive:hive-metastore:jar:0.13.1 in
> http://repo.maven.apache.org/maven2 was cached in the local repository,
> resolution will not be reattempted until the update interval of maven-repo
> has elapsed or updates are forced -> [Help 1]
>
> Some hint would be appreciated.
>
> Cheers
>
>
> On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com> wrote:
>
>> Yes, it is published. As of previous versions, at least, hive-exec
>> included all of its dependencies *in its artifact*, making it unusable
>> as-is because it contained copies of dependencies that clash with
>> versions present in other artifacts, and can't be managed with Maven
>> mechanisms.
>>
>> I am not sure why hive-exec was not published normally, with just its
>> own classes. That's why it was copied, into an artifact with just
>> hive-exec code.
>>
>> You could do the same thing for hive-exec 0.13.1.
>> Or maybe someone knows that it's published more 'normally' now.
>> I don't think hive-metastore is related to this question?
>>
>> I am no expert on the Hive artifacts, just remembering what the issue
>> was initially in case it helps you get to a similar solution.
>>
>> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> > hive-exec (as of 0.13.1) is published here:
>> >
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
>> >
>> > Should a JIRA be opened so that dependency on hive-metastore can be
>> > replaced by dependency on hive-exec ?
>> >
>> > Cheers
>> >
>> >
>> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com> wrote:
>> >
>> >> The reason for org.spark-project.hive is that Spark relies on
>> >> hive-exec, but the Hive project does not publish this artifact by
>> >> itself, only with all its dependencies as an uber jar. Maybe that's
>> >> been improved. If so, you need to point at the new hive-exec and
>> >> perhaps sort out its dependencies manually in your build.
>> >>
>> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> >> > I found 0.13.1 artifacts in maven:
>> >> >
>> >>
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
>> >> >
>> >> > However, Spark uses groupId of org.spark-project.hive, not
>> >> org.apache.hive
>> >> >
>> >> > Can someone tell me how it is supposed to work ?
>> >> >
>> >> > Cheers
>> >> >
>> >> >
>> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <snunez@hortonworks.com>
>> >> wrote:
>> >> >
>> >> >> I saw a note earlier, perhaps on the user list, that at least one
>> >> person is
>> >> >> using Hive 0.13. Anyone got a working build configuration for this
>> >> version
>> >> >> of Hive?
>> >> >>
>> >> >> Regards,
>> >> >> - Steve
>> >> >>
>> >> >>
>> >> >>
>> >> >> --
>> >> >> CONFIDENTIALITY NOTICE
>> >> >> NOTICE: This message is intended for the use of the individual or
>> >> entity to
>> >> >> which it is addressed and may contain information that is
>> confidential,
>> >> >> privileged and exempt from disclosure under applicable law. If the
>> >> reader
>> >> >> of this message is not the intended recipient, you are hereby
>> notified
>> >> that
>> >> >> any printing, copying, dissemination, distribution, disclosure or
>> >> >> forwarding of this communication is strictly prohibited. If you have
>> >> >> received this communication in error, please contact the sender
>> >> immediately
>> >> >> and delete it from your system. Thank You.
>> >> >>
>> >>
>>

From dev-return-8592-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 17:01:53 2014
Return-Path: <dev-return-8592-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7508A11DDE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 17:01:53 +0000 (UTC)
Received: (qmail 11093 invoked by uid 500); 28 Jul 2014 17:01:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11037 invoked by uid 500); 28 Jul 2014 17:01:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11025 invoked by uid 99); 28 Jul 2014 17:01:52 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:01:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.169 as permitted sender)
Received: from [209.85.160.169] (HELO mail-yk0-f169.google.com) (209.85.160.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:01:42 +0000
Received: by mail-yk0-f169.google.com with SMTP id 131so4884193ykp.0
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 10:01:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=T2772zGWGG/RJRNUYnJKqCAsFzh2nZIaJYoJ1ZSzhNA=;
        b=sLrTv5qOLMA9p1f2gE1ijGRDWY8stF8HtjZ6PvzJTZwfOvR4dhD6GJURpjDPsnANFp
         1kgomXD08ltcXRldfDPs2fnwHMwJTJ91my+aGkdNTjSXxSgHNcZOp+clbl9YLrovkM5t
         Ukvoq9Cgbnn5rlZEhTLMBDmI2oDD/OT4GD1NRZS7ViwxRSu/VX69yMLnp92Ai+qlCgJK
         l3U2Yefkpa9GLIpXA/SaXjMtOw1451uLda/oG7IxKqCsFNEoI2PaUpYhtPDpeaomqmQw
         oHOKQMd8dLShyQOHNPIkH5Lqw+G5sU75HKGy4Y15T3U3Y7h/Oi0cIZ9LYEirwv93FU6U
         qEag==
MIME-Version: 1.0
X-Received: by 10.236.127.81 with SMTP id c57mr6769016yhi.118.1406566877281;
 Mon, 28 Jul 2014 10:01:17 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Mon, 28 Jul 2014 10:01:17 -0700 (PDT)
In-Reply-To: <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
Date: Mon, 28 Jul 2014 10:01:17 -0700
Message-ID: <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf301af57f12c2f304ff43df18
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf301af57f12c2f304ff43df18
Content-Type: text/plain; charset=UTF-8

Owen helped me find this:
https://issues.apache.org/jira/browse/HIVE-7423

I guess this means that for Hive 0.14, Spark should be able to directly
pull in hive-exec-core.jar

Cheers


On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> It would be great if the hive team can fix that issue. If not, we'll
> have to continue forking our own version of Hive to change the way it
> publishes artifacts.
>
> - Patrick
>
> On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> > Talked with Owen offline. He confirmed that as of 0.13, hive-exec is
> still
> > uber jar.
> >
> > Right now I am facing the following error building against Hive 0.13.1 :
> >
> > [ERROR] Failed to execute goal on project spark-hive_2.10: Could not
> > resolve dependencies for project
> > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
> > artifacts could not be resolved:
> > org.spark-project.hive:hive-metastore:jar:0.13.1,
> > org.spark-project.hive:hive-exec:jar:0.13.1,
> > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> > http://repo.maven.apache.org/maven2 was cached in the local repository,
> > resolution will not be reattempted until the update interval of
> maven-repo
> > has elapsed or updates are forced -> [Help 1]
> >
> > Some hint would be appreciated.
> >
> > Cheers
> >
> >
> > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com> wrote:
> >
> >> Yes, it is published. As of previous versions, at least, hive-exec
> >> included all of its dependencies *in its artifact*, making it unusable
> >> as-is because it contained copies of dependencies that clash with
> >> versions present in other artifacts, and can't be managed with Maven
> >> mechanisms.
> >>
> >> I am not sure why hive-exec was not published normally, with just its
> >> own classes. That's why it was copied, into an artifact with just
> >> hive-exec code.
> >>
> >> You could do the same thing for hive-exec 0.13.1.
> >> Or maybe someone knows that it's published more 'normally' now.
> >> I don't think hive-metastore is related to this question?
> >>
> >> I am no expert on the Hive artifacts, just remembering what the issue
> >> was initially in case it helps you get to a similar solution.
> >>
> >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> > hive-exec (as of 0.13.1) is published here:
> >> >
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
> >> >
> >> > Should a JIRA be opened so that dependency on hive-metastore can be
> >> > replaced by dependency on hive-exec ?
> >> >
> >> > Cheers
> >> >
> >> >
> >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >> >
> >> >> The reason for org.spark-project.hive is that Spark relies on
> >> >> hive-exec, but the Hive project does not publish this artifact by
> >> >> itself, only with all its dependencies as an uber jar. Maybe that's
> >> >> been improved. If so, you need to point at the new hive-exec and
> >> >> perhaps sort out its dependencies manually in your build.
> >> >>
> >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> >> > I found 0.13.1 artifacts in maven:
> >> >> >
> >> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
> >> >> >
> >> >> > However, Spark uses groupId of org.spark-project.hive, not
> >> >> org.apache.hive
> >> >> >
> >> >> > Can someone tell me how it is supposed to work ?
> >> >> >
> >> >> > Cheers
> >> >> >
> >> >> >
> >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> snunez@hortonworks.com>
> >> >> wrote:
> >> >> >
> >> >> >> I saw a note earlier, perhaps on the user list, that at least one
> >> >> person is
> >> >> >> using Hive 0.13. Anyone got a working build configuration for this
> >> >> version
> >> >> >> of Hive?
> >> >> >>
> >> >> >> Regards,
> >> >> >> - Steve
> >> >> >>
> >> >> >>
> >> >> >>
> >> >> >> --
> >> >> >> CONFIDENTIALITY NOTICE
> >> >> >> NOTICE: This message is intended for the use of the individual or
> >> >> entity to
> >> >> >> which it is addressed and may contain information that is
> >> confidential,
> >> >> >> privileged and exempt from disclosure under applicable law. If the
> >> >> reader
> >> >> >> of this message is not the intended recipient, you are hereby
> >> notified
> >> >> that
> >> >> >> any printing, copying, dissemination, distribution, disclosure or
> >> >> >> forwarding of this communication is strictly prohibited. If you
> have
> >> >> >> received this communication in error, please contact the sender
> >> >> immediately
> >> >> >> and delete it from your system. Thank You.
> >> >> >>
> >> >>
> >>
>

--20cf301af57f12c2f304ff43df18--

From dev-return-8593-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 17:03:24 2014
Return-Path: <dev-return-8593-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7EED911DE3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 17:03:24 +0000 (UTC)
Received: (qmail 12922 invoked by uid 500); 28 Jul 2014 17:03:24 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 12867 invoked by uid 500); 28 Jul 2014 17:03:24 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 12856 invoked by uid 99); 28 Jul 2014 17:03:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:03:23 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:03:19 +0000
Received: by mail-wg0-f45.google.com with SMTP id x12so7770449wgg.16
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 10:02:54 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=TIdox4PjRkDIGDNh9JO6/PPmriiTpL89mAKA7xztm1o=;
        b=NCPTVkv7SKP2JiMqyyxsSL4s1J9R487oMyvm8yT1QipOm/F4Cmtt97aCEMGrhczQd9
         x/OPY+m7ez4HFYwdKvTsc1vCw9rbSkXSrzgISBPntFx7G0SCNj82KgOHzK7eWHQVjxrA
         atEeFmfwVU9b3HDveAxuea8c/I27lc0anIUF1ea5lzj38hwB38+Zv8cvbo3GT9YgqejU
         Z2tmIqd37UvhwsJZ9e7bD6Z6SrKP53hzmf2xNzZuoVA94BPcEp3GuS9XdJk4ItYURxED
         8/ZBKprmseKETF58a7oSod5ECeUkvKCs38siQRbFX+axxPOeei/sQnqXnJP3wZYYPIBo
         07dw==
X-Gm-Message-State: ALoCoQlNkWczd7r+GPl/RFeZ3j9Swfh8WdAwvY4wEi3V8mp5wzWz2KqvrvEzvf0gXwnBWf2epfRa
MIME-Version: 1.0
X-Received: by 10.180.89.34 with SMTP id bl2mr33431436wib.41.1406566973987;
 Mon, 28 Jul 2014 10:02:53 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Mon, 28 Jul 2014 10:02:53 -0700 (PDT)
In-Reply-To: <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
Date: Mon, 28 Jul 2014 10:02:53 -0700
Message-ID: <CAAsvFPm9R9Zy30GPmhsjQZQuzGffPpHfsa2PKHA6HfdwMe_Bzg@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04447e7bd67b8804ff43e431
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04447e7bd67b8804ff43e431
Content-Type: text/plain; charset=UTF-8

Where and how is that fork being maintained?  I'm not seeing an obviously
correct branch or tag in the main asf hive repo & github mirror.


On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> It would be great if the hive team can fix that issue. If not, we'll
> have to continue forking our own version of Hive to change the way it
> publishes artifacts.
>
> - Patrick
>
> On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> > Talked with Owen offline. He confirmed that as of 0.13, hive-exec is
> still
> > uber jar.
> >
> > Right now I am facing the following error building against Hive 0.13.1 :
> >
> > [ERROR] Failed to execute goal on project spark-hive_2.10: Could not
> > resolve dependencies for project
> > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
> > artifacts could not be resolved:
> > org.spark-project.hive:hive-metastore:jar:0.13.1,
> > org.spark-project.hive:hive-exec:jar:0.13.1,
> > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> > http://repo.maven.apache.org/maven2 was cached in the local repository,
> > resolution will not be reattempted until the update interval of
> maven-repo
> > has elapsed or updates are forced -> [Help 1]
> >
> > Some hint would be appreciated.
> >
> > Cheers
> >
> >
> > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com> wrote:
> >
> >> Yes, it is published. As of previous versions, at least, hive-exec
> >> included all of its dependencies *in its artifact*, making it unusable
> >> as-is because it contained copies of dependencies that clash with
> >> versions present in other artifacts, and can't be managed with Maven
> >> mechanisms.
> >>
> >> I am not sure why hive-exec was not published normally, with just its
> >> own classes. That's why it was copied, into an artifact with just
> >> hive-exec code.
> >>
> >> You could do the same thing for hive-exec 0.13.1.
> >> Or maybe someone knows that it's published more 'normally' now.
> >> I don't think hive-metastore is related to this question?
> >>
> >> I am no expert on the Hive artifacts, just remembering what the issue
> >> was initially in case it helps you get to a similar solution.
> >>
> >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> > hive-exec (as of 0.13.1) is published here:
> >> >
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
> >> >
> >> > Should a JIRA be opened so that dependency on hive-metastore can be
> >> > replaced by dependency on hive-exec ?
> >> >
> >> > Cheers
> >> >
> >> >
> >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >> >
> >> >> The reason for org.spark-project.hive is that Spark relies on
> >> >> hive-exec, but the Hive project does not publish this artifact by
> >> >> itself, only with all its dependencies as an uber jar. Maybe that's
> >> >> been improved. If so, you need to point at the new hive-exec and
> >> >> perhaps sort out its dependencies manually in your build.
> >> >>
> >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> >> > I found 0.13.1 artifacts in maven:
> >> >> >
> >> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
> >> >> >
> >> >> > However, Spark uses groupId of org.spark-project.hive, not
> >> >> org.apache.hive
> >> >> >
> >> >> > Can someone tell me how it is supposed to work ?
> >> >> >
> >> >> > Cheers
> >> >> >
> >> >> >
> >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> snunez@hortonworks.com>
> >> >> wrote:
> >> >> >
> >> >> >> I saw a note earlier, perhaps on the user list, that at least one
> >> >> person is
> >> >> >> using Hive 0.13. Anyone got a working build configuration for this
> >> >> version
> >> >> >> of Hive?
> >> >> >>
> >> >> >> Regards,
> >> >> >> - Steve
> >> >> >>
> >> >> >>
> >> >> >>
> >> >> >> --
> >> >> >> CONFIDENTIALITY NOTICE
> >> >> >> NOTICE: This message is intended for the use of the individual or
> >> >> entity to
> >> >> >> which it is addressed and may contain information that is
> >> confidential,
> >> >> >> privileged and exempt from disclosure under applicable law. If the
> >> >> reader
> >> >> >> of this message is not the intended recipient, you are hereby
> >> notified
> >> >> that
> >> >> >> any printing, copying, dissemination, distribution, disclosure or
> >> >> >> forwarding of this communication is strictly prohibited. If you
> have
> >> >> >> received this communication in error, please contact the sender
> >> >> immediately
> >> >> >> and delete it from your system. Thank You.
> >> >> >>
> >> >>
> >>
>

--f46d04447e7bd67b8804ff43e431--

From dev-return-8594-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 17:17:49 2014
Return-Path: <dev-return-8594-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B925811E72
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 17:17:49 +0000 (UTC)
Received: (qmail 58192 invoked by uid 500); 28 Jul 2014 17:17:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58134 invoked by uid 500); 28 Jul 2014 17:17:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 58122 invoked by uid 99); 28 Jul 2014 17:17:48 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:17:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.41 as permitted sender)
Received: from [209.85.192.41] (HELO mail-qg0-f41.google.com) (209.85.192.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:17:31 +0000
Received: by mail-qg0-f41.google.com with SMTP id q107so8803266qgd.0
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 10:17:06 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ja+BYtj19OlbSua8ra35oSiTAUzKzAN2lMCeFyZFdOQ=;
        b=C105CvwmVXPaIDvOxAofX022Cfl3YNbbfcbd23+eQw/D7+jalDQTHEQhdvqBSVbQjV
         K7/Bm3aZHV04TCqUQWHuB3ogPt2iEqF+b+UJud/LLRzb5RRh0TBosx/kRf2Sp1ZkDYqs
         E3zBtWnsMzeLT5FFADnNLQBBtKJnvOJpU0hiP6dXfBfAY//PLE8xeXbJUN97lH1FiJYj
         jTyeD4jFuDdj88APUAsD2cAyc9Fgwp+NiVuBjjOTMtl/jITlEctLslUrEAyDKbQdYtV2
         zXYHgwvJpTXWk+bH33ZUg6glvvElH92il3j4BIgPjRjIndjXNBUlxnVFPSJ02rt8bpHb
         EQAw==
X-Received: by 10.224.69.136 with SMTP id z8mr9561160qai.60.1406567826318;
 Mon, 28 Jul 2014 10:17:06 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.23.113 with HTTP; Mon, 28 Jul 2014 10:16:46 -0700 (PDT)
In-Reply-To: <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com> <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
 <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
 <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
 <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
 <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
 <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com> <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Tue, 29 Jul 2014 01:16:46 +0800
Message-ID: <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c28f22a3eb2204ff44177e
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c28f22a3eb2204ff44177e
Content-Type: text/plain; charset=UTF-8

AFAIK, according a recent talk, Hulu team in China has built Spark SQL
against Hive 0.13 (or 0.13.1?) successfully. Basically they also
re-packaged Hive 0.13 as what the Spark team did. The slides of the talk
hasn't been released yet though.


On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com> wrote:

> Owen helped me find this:
> https://issues.apache.org/jira/browse/HIVE-7423
>
> I guess this means that for Hive 0.14, Spark should be able to directly
> pull in hive-exec-core.jar
>
> Cheers
>
>
> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
> > It would be great if the hive team can fix that issue. If not, we'll
> > have to continue forking our own version of Hive to change the way it
> > publishes artifacts.
> >
> > - Patrick
> >
> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> > > Talked with Owen offline. He confirmed that as of 0.13, hive-exec is
> > still
> > > uber jar.
> > >
> > > Right now I am facing the following error building against Hive 0.13.1
> :
> > >
> > > [ERROR] Failed to execute goal on project spark-hive_2.10: Could not
> > > resolve dependencies for project
> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
> > > artifacts could not be resolved:
> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
> > > org.spark-project.hive:hive-exec:jar:0.13.1,
> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> > > http://repo.maven.apache.org/maven2 was cached in the local
> repository,
> > > resolution will not be reattempted until the update interval of
> > maven-repo
> > > has elapsed or updates are forced -> [Help 1]
> > >
> > > Some hint would be appreciated.
> > >
> > > Cheers
> > >
> > >
> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com> wrote:
> > >
> > >> Yes, it is published. As of previous versions, at least, hive-exec
> > >> included all of its dependencies *in its artifact*, making it unusable
> > >> as-is because it contained copies of dependencies that clash with
> > >> versions present in other artifacts, and can't be managed with Maven
> > >> mechanisms.
> > >>
> > >> I am not sure why hive-exec was not published normally, with just its
> > >> own classes. That's why it was copied, into an artifact with just
> > >> hive-exec code.
> > >>
> > >> You could do the same thing for hive-exec 0.13.1.
> > >> Or maybe someone knows that it's published more 'normally' now.
> > >> I don't think hive-metastore is related to this question?
> > >>
> > >> I am no expert on the Hive artifacts, just remembering what the issue
> > >> was initially in case it helps you get to a similar solution.
> > >>
> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> > >> > hive-exec (as of 0.13.1) is published here:
> > >> >
> > >>
> >
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
> > >> >
> > >> > Should a JIRA be opened so that dependency on hive-metastore can be
> > >> > replaced by dependency on hive-exec ?
> > >> >
> > >> > Cheers
> > >> >
> > >> >
> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com>
> > wrote:
> > >> >
> > >> >> The reason for org.spark-project.hive is that Spark relies on
> > >> >> hive-exec, but the Hive project does not publish this artifact by
> > >> >> itself, only with all its dependencies as an uber jar. Maybe that's
> > >> >> been improved. If so, you need to point at the new hive-exec and
> > >> >> perhaps sort out its dependencies manually in your build.
> > >> >>
> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com>
> wrote:
> > >> >> > I found 0.13.1 artifacts in maven:
> > >> >> >
> > >> >>
> > >>
> >
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
> > >> >> >
> > >> >> > However, Spark uses groupId of org.spark-project.hive, not
> > >> >> org.apache.hive
> > >> >> >
> > >> >> > Can someone tell me how it is supposed to work ?
> > >> >> >
> > >> >> > Cheers
> > >> >> >
> > >> >> >
> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> > snunez@hortonworks.com>
> > >> >> wrote:
> > >> >> >
> > >> >> >> I saw a note earlier, perhaps on the user list, that at least
> one
> > >> >> person is
> > >> >> >> using Hive 0.13. Anyone got a working build configuration for
> this
> > >> >> version
> > >> >> >> of Hive?
> > >> >> >>
> > >> >> >> Regards,
> > >> >> >> - Steve
> > >> >> >>
> > >> >> >>
> > >> >> >>
> > >> >> >> --
> > >> >> >> CONFIDENTIALITY NOTICE
> > >> >> >> NOTICE: This message is intended for the use of the individual
> or
> > >> >> entity to
> > >> >> >> which it is addressed and may contain information that is
> > >> confidential,
> > >> >> >> privileged and exempt from disclosure under applicable law. If
> the
> > >> >> reader
> > >> >> >> of this message is not the intended recipient, you are hereby
> > >> notified
> > >> >> that
> > >> >> >> any printing, copying, dissemination, distribution, disclosure
> or
> > >> >> >> forwarding of this communication is strictly prohibited. If you
> > have
> > >> >> >> received this communication in error, please contact the sender
> > >> >> immediately
> > >> >> >> and delete it from your system. Thank You.
> > >> >> >>
> > >> >>
> > >>
> >
>

--001a11c28f22a3eb2204ff44177e--

From dev-return-8595-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 17:17:59 2014
Return-Path: <dev-return-8595-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2786711E73
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 17:17:59 +0000 (UTC)
Received: (qmail 59327 invoked by uid 500); 28 Jul 2014 17:17:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59273 invoked by uid 500); 28 Jul 2014 17:17:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 59261 invoked by uid 99); 28 Jul 2014 17:17:58 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:17:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.47 as permitted sender)
Received: from [209.85.219.47] (HELO mail-oa0-f47.google.com) (209.85.219.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:17:41 +0000
Received: by mail-oa0-f47.google.com with SMTP id g18so9059487oah.34
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 10:17:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Ib2j+U9nIDnibnzBL5xEaxELPz4v/23//ctGVJB5k84=;
        b=S9OVo9CZt6cT55ZHxFvZUIlABV82R72N8WacOiu/7aB+Aq81vaboNHtL+oZ2W//F75
         QLe4oAmGE+b7AZcCQDUKDeSC7WfhJa9RIMLsFHQYyPW0ATJVAOc/9HgIM2dHIFkXhFoF
         w+lAyROPs3VE64Di9HGA+r3TXvU8TtNjNw9QKSp/iu6D1AO3tiFPjRRfPPjSVsop3Wez
         gAw23W/CJiS5jnW1HS2yPo8Jq8jsLxkKIejSDdRvXxAOpHeyov7U7kfJR5khgQa3su24
         mky+IXLXgj+jIbndlO3pVl5jEFGRHvZHCqZeDfdcMEI6pkDbYME1r7bOVtysaqioxNpf
         8R6w==
MIME-Version: 1.0
X-Received: by 10.182.112.134 with SMTP id iq6mr50687549obb.34.1406567836851;
 Mon, 28 Jul 2014 10:17:16 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 28 Jul 2014 10:17:16 -0700 (PDT)
In-Reply-To: <CAAsvFPm9R9Zy30GPmhsjQZQuzGffPpHfsa2PKHA6HfdwMe_Bzg@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
	<CAAsvFPm9R9Zy30GPmhsjQZQuzGffPpHfsa2PKHA6HfdwMe_Bzg@mail.gmail.com>
Date: Mon, 28 Jul 2014 10:17:16 -0700
Message-ID: <CABPQxstkLg=JJ8WY+vK=x57mh2aeM57iE+6xYJZtnLFniNb_Vg@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah so we need a model for this (Mark - do you have any ideas?). I
did this in a personal github repo. I just did it quickly because
dependency issues were blocking the 1.0 release:

https://github.com/pwendell/hive/tree/branch-0.12-shaded-protobuf

I think what we want is to have a semi official github repo with an
index to each of the shaded dependencies and what version is included
in which branch.

- Patrick

On Mon, Jul 28, 2014 at 10:02 AM, Mark Hamstra <mark@clearstorydata.com> wrote:
> Where and how is that fork being maintained?  I'm not seeing an obviously
> correct branch or tag in the main asf hive repo & github mirror.
>
>
> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com> wrote:
>
>> It would be great if the hive team can fix that issue. If not, we'll
>> have to continue forking our own version of Hive to change the way it
>> publishes artifacts.
>>
>> - Patrick
>>
>> On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>> > Talked with Owen offline. He confirmed that as of 0.13, hive-exec is
>> still
>> > uber jar.
>> >
>> > Right now I am facing the following error building against Hive 0.13.1 :
>> >
>> > [ERROR] Failed to execute goal on project spark-hive_2.10: Could not
>> > resolve dependencies for project
>> > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
>> > artifacts could not be resolved:
>> > org.spark-project.hive:hive-metastore:jar:0.13.1,
>> > org.spark-project.hive:hive-exec:jar:0.13.1,
>> > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
>> > org.spark-project.hive:hive-metastore:jar:0.13.1 in
>> > http://repo.maven.apache.org/maven2 was cached in the local repository,
>> > resolution will not be reattempted until the update interval of
>> maven-repo
>> > has elapsed or updates are forced -> [Help 1]
>> >
>> > Some hint would be appreciated.
>> >
>> > Cheers
>> >
>> >
>> > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com> wrote:
>> >
>> >> Yes, it is published. As of previous versions, at least, hive-exec
>> >> included all of its dependencies *in its artifact*, making it unusable
>> >> as-is because it contained copies of dependencies that clash with
>> >> versions present in other artifacts, and can't be managed with Maven
>> >> mechanisms.
>> >>
>> >> I am not sure why hive-exec was not published normally, with just its
>> >> own classes. That's why it was copied, into an artifact with just
>> >> hive-exec code.
>> >>
>> >> You could do the same thing for hive-exec 0.13.1.
>> >> Or maybe someone knows that it's published more 'normally' now.
>> >> I don't think hive-metastore is related to this question?
>> >>
>> >> I am no expert on the Hive artifacts, just remembering what the issue
>> >> was initially in case it helps you get to a similar solution.
>> >>
>> >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> >> > hive-exec (as of 0.13.1) is published here:
>> >> >
>> >>
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
>> >> >
>> >> > Should a JIRA be opened so that dependency on hive-metastore can be
>> >> > replaced by dependency on hive-exec ?
>> >> >
>> >> > Cheers
>> >> >
>> >> >
>> >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com>
>> wrote:
>> >> >
>> >> >> The reason for org.spark-project.hive is that Spark relies on
>> >> >> hive-exec, but the Hive project does not publish this artifact by
>> >> >> itself, only with all its dependencies as an uber jar. Maybe that's
>> >> >> been improved. If so, you need to point at the new hive-exec and
>> >> >> perhaps sort out its dependencies manually in your build.
>> >> >>
>> >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> >> >> > I found 0.13.1 artifacts in maven:
>> >> >> >
>> >> >>
>> >>
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
>> >> >> >
>> >> >> > However, Spark uses groupId of org.spark-project.hive, not
>> >> >> org.apache.hive
>> >> >> >
>> >> >> > Can someone tell me how it is supposed to work ?
>> >> >> >
>> >> >> > Cheers
>> >> >> >
>> >> >> >
>> >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>> snunez@hortonworks.com>
>> >> >> wrote:
>> >> >> >
>> >> >> >> I saw a note earlier, perhaps on the user list, that at least one
>> >> >> person is
>> >> >> >> using Hive 0.13. Anyone got a working build configuration for this
>> >> >> version
>> >> >> >> of Hive?
>> >> >> >>
>> >> >> >> Regards,
>> >> >> >> - Steve
>> >> >> >>
>> >> >> >>
>> >> >> >>
>> >> >> >> --
>> >> >> >> CONFIDENTIALITY NOTICE
>> >> >> >> NOTICE: This message is intended for the use of the individual or
>> >> >> entity to
>> >> >> >> which it is addressed and may contain information that is
>> >> confidential,
>> >> >> >> privileged and exempt from disclosure under applicable law. If the
>> >> >> reader
>> >> >> >> of this message is not the intended recipient, you are hereby
>> >> notified
>> >> >> that
>> >> >> >> any printing, copying, dissemination, distribution, disclosure or
>> >> >> >> forwarding of this communication is strictly prohibited. If you
>> have
>> >> >> >> received this communication in error, please contact the sender
>> >> >> immediately
>> >> >> >> and delete it from your system. Thank You.
>> >> >> >>
>> >> >>
>> >>
>>

From dev-return-8596-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 17:20:43 2014
Return-Path: <dev-return-8596-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2FF4111E86
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 17:20:43 +0000 (UTC)
Received: (qmail 66025 invoked by uid 500); 28 Jul 2014 17:20:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 65965 invoked by uid 500); 28 Jul 2014 17:20:42 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 65947 invoked by uid 99); 28 Jul 2014 17:20:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:20:41 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.218.47 as permitted sender)
Received: from [209.85.218.47] (HELO mail-oi0-f47.google.com) (209.85.218.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:20:38 +0000
Received: by mail-oi0-f47.google.com with SMTP id x69so6306336oia.20
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 10:20:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=27eEjUlDGNgdI0XCGBlYloEdAgxaQY20qhEPVnAAtVU=;
        b=SFnXCPlszQdLEcsdsyDQOCOWlt0OqxjxooLzFl6V6tTrYYuc+hFhdlM8az0cJaZZBE
         plhxd32q+jvUG1VH2rOXadL/DMReMlu3d17/FK7wCjXc4TzhArDtL5J1oFxvACNTIm3T
         71BfqJXbPGDIRwVdXXhD6HRfKMyHWF3dFPx5GyZZ62fOd3IfhcJrPF4eZHSirBhp+TbV
         2hkBrvUdaxTzZp/sD43ogLgE6NgLFPp5U/JO5NuAT8SARpm41UrcjdcB4HK2Lpv0vWtI
         Srn4TYkCNXIn/sFt5lhCMm5O//9xoNK3jrtQLHzXimzjuLRyWaIC3qQcKI2YfxBO5HTy
         njjA==
MIME-Version: 1.0
X-Received: by 10.182.199.5 with SMTP id jg5mr51770126obc.75.1406568017373;
 Mon, 28 Jul 2014 10:20:17 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 28 Jul 2014 10:20:17 -0700 (PDT)
In-Reply-To: <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
	<CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
	<CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
Date: Mon, 28 Jul 2014 10:20:17 -0700
Message-ID: <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

I've heard from Cloudera that there were hive internal changes between
0.12 and 0.13 that required code re-writing. Over time it might be
possible for us to integrate with hive using API's that are more
stable (this is the domain of Michael/Cheng/Yin more than me!). It
would be interesting to see what the Hulu folks did.

- Patrick

On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com> wrote:
> AFAIK, according a recent talk, Hulu team in China has built Spark SQL
> against Hive 0.13 (or 0.13.1?) successfully. Basically they also
> re-packaged Hive 0.13 as what the Spark team did. The slides of the talk
> hasn't been released yet though.
>
>
> On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>
>> Owen helped me find this:
>> https://issues.apache.org/jira/browse/HIVE-7423
>>
>> I guess this means that for Hive 0.14, Spark should be able to directly
>> pull in hive-exec-core.jar
>>
>> Cheers
>>
>>
>> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>> > It would be great if the hive team can fix that issue. If not, we'll
>> > have to continue forking our own version of Hive to change the way it
>> > publishes artifacts.
>> >
>> > - Patrick
>> >
>> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>> > > Talked with Owen offline. He confirmed that as of 0.13, hive-exec is
>> > still
>> > > uber jar.
>> > >
>> > > Right now I am facing the following error building against Hive 0.13.1
>> :
>> > >
>> > > [ERROR] Failed to execute goal on project spark-hive_2.10: Could not
>> > > resolve dependencies for project
>> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
>> > > artifacts could not be resolved:
>> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
>> > > org.spark-project.hive:hive-exec:jar:0.13.1,
>> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
>> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
>> > > http://repo.maven.apache.org/maven2 was cached in the local
>> repository,
>> > > resolution will not be reattempted until the update interval of
>> > maven-repo
>> > > has elapsed or updates are forced -> [Help 1]
>> > >
>> > > Some hint would be appreciated.
>> > >
>> > > Cheers
>> > >
>> > >
>> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com> wrote:
>> > >
>> > >> Yes, it is published. As of previous versions, at least, hive-exec
>> > >> included all of its dependencies *in its artifact*, making it unusable
>> > >> as-is because it contained copies of dependencies that clash with
>> > >> versions present in other artifacts, and can't be managed with Maven
>> > >> mechanisms.
>> > >>
>> > >> I am not sure why hive-exec was not published normally, with just its
>> > >> own classes. That's why it was copied, into an artifact with just
>> > >> hive-exec code.
>> > >>
>> > >> You could do the same thing for hive-exec 0.13.1.
>> > >> Or maybe someone knows that it's published more 'normally' now.
>> > >> I don't think hive-metastore is related to this question?
>> > >>
>> > >> I am no expert on the Hive artifacts, just remembering what the issue
>> > >> was initially in case it helps you get to a similar solution.
>> > >>
>> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> > >> > hive-exec (as of 0.13.1) is published here:
>> > >> >
>> > >>
>> >
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
>> > >> >
>> > >> > Should a JIRA be opened so that dependency on hive-metastore can be
>> > >> > replaced by dependency on hive-exec ?
>> > >> >
>> > >> > Cheers
>> > >> >
>> > >> >
>> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com>
>> > wrote:
>> > >> >
>> > >> >> The reason for org.spark-project.hive is that Spark relies on
>> > >> >> hive-exec, but the Hive project does not publish this artifact by
>> > >> >> itself, only with all its dependencies as an uber jar. Maybe that's
>> > >> >> been improved. If so, you need to point at the new hive-exec and
>> > >> >> perhaps sort out its dependencies manually in your build.
>> > >> >>
>> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com>
>> wrote:
>> > >> >> > I found 0.13.1 artifacts in maven:
>> > >> >> >
>> > >> >>
>> > >>
>> >
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
>> > >> >> >
>> > >> >> > However, Spark uses groupId of org.spark-project.hive, not
>> > >> >> org.apache.hive
>> > >> >> >
>> > >> >> > Can someone tell me how it is supposed to work ?
>> > >> >> >
>> > >> >> > Cheers
>> > >> >> >
>> > >> >> >
>> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>> > snunez@hortonworks.com>
>> > >> >> wrote:
>> > >> >> >
>> > >> >> >> I saw a note earlier, perhaps on the user list, that at least
>> one
>> > >> >> person is
>> > >> >> >> using Hive 0.13. Anyone got a working build configuration for
>> this
>> > >> >> version
>> > >> >> >> of Hive?
>> > >> >> >>
>> > >> >> >> Regards,
>> > >> >> >> - Steve
>> > >> >> >>
>> > >> >> >>
>> > >> >> >>
>> > >> >> >> --
>> > >> >> >> CONFIDENTIALITY NOTICE
>> > >> >> >> NOTICE: This message is intended for the use of the individual
>> or
>> > >> >> entity to
>> > >> >> >> which it is addressed and may contain information that is
>> > >> confidential,
>> > >> >> >> privileged and exempt from disclosure under applicable law. If
>> the
>> > >> >> reader
>> > >> >> >> of this message is not the intended recipient, you are hereby
>> > >> notified
>> > >> >> that
>> > >> >> >> any printing, copying, dissemination, distribution, disclosure
>> or
>> > >> >> >> forwarding of this communication is strictly prohibited. If you
>> > have
>> > >> >> >> received this communication in error, please contact the sender
>> > >> >> immediately
>> > >> >> >> and delete it from your system. Thank You.
>> > >> >> >>
>> > >> >>
>> > >>
>> >
>>

From dev-return-8597-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 17:25:08 2014
Return-Path: <dev-return-8597-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9721211ECD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 17:25:08 +0000 (UTC)
Received: (qmail 85624 invoked by uid 500); 28 Jul 2014 17:25:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85564 invoked by uid 500); 28 Jul 2014 17:25:07 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 85544 invoked by uid 99); 28 Jul 2014 17:25:07 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:25:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mark@clearstorydata.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:25:06 +0000
Received: by mail-wg0-f42.google.com with SMTP id l18so7538875wgh.13
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 10:24:39 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=MYxPjnyiQ5+Bac6FQBvHyfUiH+FW5s2iB40TddZmIm8=;
        b=BLbH5HGnCRsxt4Z56kXJ1Fb9vm1IP/xu9H7LBOoTZYqGFc5keJtBWOkSI3IeqXpi7v
         BknRORAGQrEjqXFLL+DbnUIwveCpZnkKzN/brJx119mfWVgjq+lBFlTxbZLNrX3/j4lj
         +dxxBkk5Wu1nb8r9022M7MNTFfylCZu/ZKJH/Qe8yU6/V+0Xpo8k2XgcXsNMaASgp6cx
         EO5b/fixwY2QcasicIfHg0OHBPo917kyoohF7mIz82irM6W4FAo7Sjpn/NLEt6Ay3Rfu
         iw6rZsaTTofqV2ZkrtnQ6YSdm6Kkc4E+VD9xDG1PEVGkyicETnsSUDgOzHSiHkQSdN3r
         gK0g==
X-Gm-Message-State: ALoCoQlXmbAaWnnae1d0XDlk/2ovJfn754GMWrY+xQfDpVvadataokb8mb9kvgCMi7CTbEOTEuJK
MIME-Version: 1.0
X-Received: by 10.194.134.228 with SMTP id pn4mr1316589wjb.111.1406568278577;
 Mon, 28 Jul 2014 10:24:38 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Mon, 28 Jul 2014 10:24:38 -0700 (PDT)
In-Reply-To: <CABPQxstkLg=JJ8WY+vK=x57mh2aeM57iE+6xYJZtnLFniNb_Vg@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
	<CAAsvFPm9R9Zy30GPmhsjQZQuzGffPpHfsa2PKHA6HfdwMe_Bzg@mail.gmail.com>
	<CABPQxstkLg=JJ8WY+vK=x57mh2aeM57iE+6xYJZtnLFniNb_Vg@mail.gmail.com>
Date: Mon, 28 Jul 2014 10:24:38 -0700
Message-ID: <CAAsvFPnJjrFHVsVp+tqRgfB3OFiytL3vf+NVbifJRkonJ7-eiw@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01228bb098f46004ff443213
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01228bb098f46004ff443213
Content-Type: text/plain; charset=UTF-8

Getting and maintaining our own branch in the main asf hive repo is a
non-starter or isn't workable?


On Mon, Jul 28, 2014 at 10:17 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> Yeah so we need a model for this (Mark - do you have any ideas?). I
> did this in a personal github repo. I just did it quickly because
> dependency issues were blocking the 1.0 release:
>
> https://github.com/pwendell/hive/tree/branch-0.12-shaded-protobuf
>
> I think what we want is to have a semi official github repo with an
> index to each of the shaded dependencies and what version is included
> in which branch.
>
> - Patrick
>
> On Mon, Jul 28, 2014 at 10:02 AM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
> > Where and how is that fork being maintained?  I'm not seeing an obviously
> > correct branch or tag in the main asf hive repo & github mirror.
> >
> >
> > On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> >
> >> It would be great if the hive team can fix that issue. If not, we'll
> >> have to continue forking our own version of Hive to change the way it
> >> publishes artifacts.
> >>
> >> - Patrick
> >>
> >> On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> > Talked with Owen offline. He confirmed that as of 0.13, hive-exec is
> >> still
> >> > uber jar.
> >> >
> >> > Right now I am facing the following error building against Hive
> 0.13.1 :
> >> >
> >> > [ERROR] Failed to execute goal on project spark-hive_2.10: Could not
> >> > resolve dependencies for project
> >> > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
> >> > artifacts could not be resolved:
> >> > org.spark-project.hive:hive-metastore:jar:0.13.1,
> >> > org.spark-project.hive:hive-exec:jar:0.13.1,
> >> > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> >> > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> >> > http://repo.maven.apache.org/maven2 was cached in the local
> repository,
> >> > resolution will not be reattempted until the update interval of
> >> maven-repo
> >> > has elapsed or updates are forced -> [Help 1]
> >> >
> >> > Some hint would be appreciated.
> >> >
> >> > Cheers
> >> >
> >> >
> >> > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >> >
> >> >> Yes, it is published. As of previous versions, at least, hive-exec
> >> >> included all of its dependencies *in its artifact*, making it
> unusable
> >> >> as-is because it contained copies of dependencies that clash with
> >> >> versions present in other artifacts, and can't be managed with Maven
> >> >> mechanisms.
> >> >>
> >> >> I am not sure why hive-exec was not published normally, with just its
> >> >> own classes. That's why it was copied, into an artifact with just
> >> >> hive-exec code.
> >> >>
> >> >> You could do the same thing for hive-exec 0.13.1.
> >> >> Or maybe someone knows that it's published more 'normally' now.
> >> >> I don't think hive-metastore is related to this question?
> >> >>
> >> >> I am no expert on the Hive artifacts, just remembering what the issue
> >> >> was initially in case it helps you get to a similar solution.
> >> >>
> >> >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> >> > hive-exec (as of 0.13.1) is published here:
> >> >> >
> >> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C0.13.1%7Cjar
> >> >> >
> >> >> > Should a JIRA be opened so that dependency on hive-metastore can be
> >> >> > replaced by dependency on hive-exec ?
> >> >> >
> >> >> > Cheers
> >> >> >
> >> >> >
> >> >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com>
> >> wrote:
> >> >> >
> >> >> >> The reason for org.spark-project.hive is that Spark relies on
> >> >> >> hive-exec, but the Hive project does not publish this artifact by
> >> >> >> itself, only with all its dependencies as an uber jar. Maybe
> that's
> >> >> >> been improved. If so, you need to point at the new hive-exec and
> >> >> >> perhaps sort out its dependencies manually in your build.
> >> >> >>
> >> >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com>
> wrote:
> >> >> >> > I found 0.13.1 artifacts in maven:
> >> >> >> >
> >> >> >>
> >> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metastore%7C0.13.1%7Cjar
> >> >> >> >
> >> >> >> > However, Spark uses groupId of org.spark-project.hive, not
> >> >> >> org.apache.hive
> >> >> >> >
> >> >> >> > Can someone tell me how it is supposed to work ?
> >> >> >> >
> >> >> >> > Cheers
> >> >> >> >
> >> >> >> >
> >> >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> >> snunez@hortonworks.com>
> >> >> >> wrote:
> >> >> >> >
> >> >> >> >> I saw a note earlier, perhaps on the user list, that at least
> one
> >> >> >> person is
> >> >> >> >> using Hive 0.13. Anyone got a working build configuration for
> this
> >> >> >> version
> >> >> >> >> of Hive?
> >> >> >> >>
> >> >> >> >> Regards,
> >> >> >> >> - Steve
> >> >> >> >>
> >> >> >> >>
> >> >> >> >>
> >> >> >> >> --
> >> >> >> >> CONFIDENTIALITY NOTICE
> >> >> >> >> NOTICE: This message is intended for the use of the individual
> or
> >> >> >> entity to
> >> >> >> >> which it is addressed and may contain information that is
> >> >> confidential,
> >> >> >> >> privileged and exempt from disclosure under applicable law. If
> the
> >> >> >> reader
> >> >> >> >> of this message is not the intended recipient, you are hereby
> >> >> notified
> >> >> >> that
> >> >> >> >> any printing, copying, dissemination, distribution, disclosure
> or
> >> >> >> >> forwarding of this communication is strictly prohibited. If you
> >> have
> >> >> >> >> received this communication in error, please contact the sender
> >> >> >> immediately
> >> >> >> >> and delete it from your system. Thank You.
> >> >> >> >>
> >> >> >>
> >> >>
> >>
>

--089e01228bb098f46004ff443213--

From dev-return-8598-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 17:51:28 2014
Return-Path: <dev-return-8598-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A971C11FBA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 17:51:28 +0000 (UTC)
Received: (qmail 67769 invoked by uid 500); 28 Jul 2014 17:51:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 67729 invoked by uid 500); 28 Jul 2014 17:51:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 67717 invoked by uid 99); 28 Jul 2014 17:51:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:51:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.216.48 as permitted sender)
Received: from [209.85.216.48] (HELO mail-qa0-f48.google.com) (209.85.216.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 17:51:23 +0000
Received: by mail-qa0-f48.google.com with SMTP id m5so8174650qaj.21
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 10:51:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=EEgnBoGH35SUi+BGpmy3uObTWNlaM+1SUtyDwqFbP7w=;
        b=FxqhrCYt17vdWlFJiN+MZmLsA4zCSslxGfCSJUTWsC8BxzAalyXhGjGS1/CJQwHo+s
         w4mtrUpdXf6cm10upxjPMMjh/ltLTHEsEmCaTNG5c+Sf0khAcNucgBnSrpDdo+728Hd5
         lJnSpQU/i6j+tB8kN2gZqKTN2urKc872jnACSSVWBvufxbNFqUr9AAeSv1WdQwR/j/7k
         yI8jeuS1ArDYVYw8Zj5JT3kxeoyyY/Ky+D/6ud8GMJVaYyld9z/cWigYO5672kXeIcR2
         itSW0yW4TelBgKeOxwTFrv2hdiI4VlzEVl2FpqDEVy3+1eokWZgW8P6ewgWIens0Ptzx
         WzPw==
X-Received: by 10.140.80.2 with SMTP id b2mr55256475qgd.75.1406569862299; Mon,
 28 Jul 2014 10:51:02 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.23.113 with HTTP; Mon, 28 Jul 2014 10:50:42 -0700 (PDT)
In-Reply-To: <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com> <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
 <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
 <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
 <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
 <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
 <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
 <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
 <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com> <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Tue, 29 Jul 2014 01:50:42 +0800
Message-ID: <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c12562fe874804ff44900f
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c12562fe874804ff44900f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Exactly, forgot to mention Hulu team also made changes to cope with those
incompatibility issues, but they said that=E2=80=99s relatively easy once t=
he
re-packaging work is done.


On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.com> wrote=
:

> I've heard from Cloudera that there were hive internal changes between
> 0.12 and 0.13 that required code re-writing. Over time it might be
> possible for us to integrate with hive using API's that are more
> stable (this is the domain of Michael/Cheng/Yin more than me!). It
> would be interesting to see what the Hulu folks did.
>
> - Patrick
>
> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com>
> wrote:
> > AFAIK, according a recent talk, Hulu team in China has built Spark SQL
> > against Hive 0.13 (or 0.13.1?) successfully. Basically they also
> > re-packaged Hive 0.13 as what the Spark team did. The slides of the tal=
k
> > hasn't been released yet though.
> >
> >
> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> >
> >> Owen helped me find this:
> >> https://issues.apache.org/jira/browse/HIVE-7423
> >>
> >> I guess this means that for Hive 0.14, Spark should be able to directl=
y
> >> pull in hive-exec-core.jar
> >>
> >> Cheers
> >>
> >>
> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com>
> >> wrote:
> >>
> >> > It would be great if the hive team can fix that issue. If not, we'll
> >> > have to continue forking our own version of Hive to change the way i=
t
> >> > publishes artifacts.
> >> >
> >> > - Patrick
> >> >
> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> > > Talked with Owen offline. He confirmed that as of 0.13, hive-exec =
is
> >> > still
> >> > > uber jar.
> >> > >
> >> > > Right now I am facing the following error building against Hive
> 0.13.1
> >> :
> >> > >
> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10: Could n=
ot
> >> > > resolve dependencies for project
> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The following
> >> > > artifacts could not be resolved:
> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> >> > > http://repo.maven.apache.org/maven2 was cached in the local
> >> repository,
> >> > > resolution will not be reattempted until the update interval of
> >> > maven-repo
> >> > > has elapsed or updates are forced -> [Help 1]
> >> > >
> >> > > Some hint would be appreciated.
> >> > >
> >> > > Cheers
> >> > >
> >> > >
> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com>
> wrote:
> >> > >
> >> > >> Yes, it is published. As of previous versions, at least, hive-exe=
c
> >> > >> included all of its dependencies *in its artifact*, making it
> unusable
> >> > >> as-is because it contained copies of dependencies that clash with
> >> > >> versions present in other artifacts, and can't be managed with
> Maven
> >> > >> mechanisms.
> >> > >>
> >> > >> I am not sure why hive-exec was not published normally, with just
> its
> >> > >> own classes. That's why it was copied, into an artifact with just
> >> > >> hive-exec code.
> >> > >>
> >> > >> You could do the same thing for hive-exec 0.13.1.
> >> > >> Or maybe someone knows that it's published more 'normally' now.
> >> > >> I don't think hive-metastore is related to this question?
> >> > >>
> >> > >> I am no expert on the Hive artifacts, just remembering what the
> issue
> >> > >> was initially in case it helps you get to a similar solution.
> >> > >>
> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com>
> wrote:
> >> > >> > hive-exec (as of 0.13.1) is published here:
> >> > >> >
> >> > >>
> >> >
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C=
0.13.1%7Cjar
> >> > >> >
> >> > >> > Should a JIRA be opened so that dependency on hive-metastore ca=
n
> be
> >> > >> > replaced by dependency on hive-exec ?
> >> > >> >
> >> > >> > Cheers
> >> > >> >
> >> > >> >
> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen <sowen@cloudera.com>
> >> > wrote:
> >> > >> >
> >> > >> >> The reason for org.spark-project.hive is that Spark relies on
> >> > >> >> hive-exec, but the Hive project does not publish this artifact
> by
> >> > >> >> itself, only with all its dependencies as an uber jar. Maybe
> that's
> >> > >> >> been improved. If so, you need to point at the new hive-exec a=
nd
> >> > >> >> perhaps sort out its dependencies manually in your build.
> >> > >> >>
> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com>
> >> wrote:
> >> > >> >> > I found 0.13.1 artifacts in maven:
> >> > >> >> >
> >> > >> >>
> >> > >>
> >> >
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metasto=
re%7C0.13.1%7Cjar
> >> > >> >> >
> >> > >> >> > However, Spark uses groupId of org.spark-project.hive, not
> >> > >> >> org.apache.hive
> >> > >> >> >
> >> > >> >> > Can someone tell me how it is supposed to work ?
> >> > >> >> >
> >> > >> >> > Cheers
> >> > >> >> >
> >> > >> >> >
> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> >> > snunez@hortonworks.com>
> >> > >> >> wrote:
> >> > >> >> >
> >> > >> >> >> I saw a note earlier, perhaps on the user list, that at lea=
st
> >> one
> >> > >> >> person is
> >> > >> >> >> using Hive 0.13. Anyone got a working build configuration f=
or
> >> this
> >> > >> >> version
> >> > >> >> >> of Hive?
> >> > >> >> >>
> >> > >> >> >> Regards,
> >> > >> >> >> - Steve
> >> > >> >> >>
> >> > >> >> >>
> >> > >> >> >>
> >> > >> >> >> --
> >> > >> >> >> CONFIDENTIALITY NOTICE
> >> > >> >> >> NOTICE: This message is intended for the use of the
> individual
> >> or
> >> > >> >> entity to
> >> > >> >> >> which it is addressed and may contain information that is
> >> > >> confidential,
> >> > >> >> >> privileged and exempt from disclosure under applicable law.
> If
> >> the
> >> > >> >> reader
> >> > >> >> >> of this message is not the intended recipient, you are here=
by
> >> > >> notified
> >> > >> >> that
> >> > >> >> >> any printing, copying, dissemination, distribution,
> disclosure
> >> or
> >> > >> >> >> forwarding of this communication is strictly prohibited. If
> you
> >> > have
> >> > >> >> >> received this communication in error, please contact the
> sender
> >> > >> >> immediately
> >> > >> >> >> and delete it from your system. Thank You.
> >> > >> >> >>
> >> > >> >>
> >> > >>
> >> >
> >>
>

--001a11c12562fe874804ff44900f--

From dev-return-8599-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 18:45:13 2014
Return-Path: <dev-return-8599-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 77751112B4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 18:45:13 +0000 (UTC)
Received: (qmail 18141 invoked by uid 500); 28 Jul 2014 18:45:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18043 invoked by uid 500); 28 Jul 2014 18:45:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 17150 invoked by uid 99); 28 Jul 2014 18:45:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 18:45:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ankurdave@gmail.com designates 209.85.192.52 as permitted sender)
Received: from [209.85.192.52] (HELO mail-qg0-f52.google.com) (209.85.192.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 18:45:07 +0000
Received: by mail-qg0-f52.google.com with SMTP id f51so8868611qge.25
        for <multiple recipients>; Mon, 28 Jul 2014 11:44:46 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=X70o+otCEQFNFj4e+vORYkqaZ/7RtBn9RGKxV+r1UnE=;
        b=FRo1g5+khxTiUR4BowSqzmCPIxcJWdU/lw2pKSfH9iZTmain/ZDzxwTZJ1hlMfJtJE
         XsUqug8VTRadzF8u0xcWKciaj1sWuB7/+dDJMiXSwxWMmg1VUaNe6kLrRZm9h5hYAdrn
         DZyifnnV92zKc4s+/RlqYe/Fbuciq9RnRRu5Jowp6tR1CD6E8FgQoVKA16bgl6DB0aO9
         3aRoMjsb394BfgOLJqnkPjPcwn0p4E1LIZVeMPFVCuMLfmaoNNNXTiYicaR2iQT87sO6
         TZUUP2KHZY3c5SEs8zESwS4lniGfbkUpN3wZikXwfyJ65y+TG3nWrbNdRb6JrIlmsI/c
         REpg==
X-Received: by 10.224.171.197 with SMTP id i5mr63550006qaz.55.1406573086315;
 Mon, 28 Jul 2014 11:44:46 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.37.132 with HTTP; Mon, 28 Jul 2014 11:44:26 -0700 (PDT)
In-Reply-To: <53D63414.6060202@sjtu.edu.cn>
References: <1406533297643-10763.post@n3.nabble.com> <53D63414.6060202@sjtu.edu.cn>
From: Ankur Dave <ankurdave@gmail.com>
Date: Mon, 28 Jul 2014 11:44:26 -0700
Message-ID: <CAK1A71zKxypUwWCYL2gi8aszXQp5MVyKxWJwZdEF8Y+pSRhDOA@mail.gmail.com>
Subject: Re: VertexPartition and ShippableVertexPartition
To: "user@spark.apache.org" <user@spark.apache.org>
Cc: shijiaxin.cn@gmail.com, dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2c83a291e6104ff455185
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c83a291e6104ff455185
Content-Type: text/plain; charset=UTF-8

On Mon, Jul 28, 2014 at 4:29 AM, Larry Xiao <xiaodi@sjtu.edu.cn> wrote:

> On 7/28/14, 3:41 PM, shijiaxin wrote:
>
>> There is a VertexPartition in the EdgePartition,which is created by
>>
>> EdgePartitionBuilder.toEdgePartition.
>>
>> and There is also a ShippableVertexPartition in the VertexRDD.
>>
>> These two Partitions have a lot of common things like index, data and
>>
>> Bitset, why is this necessary?
>>
>>

There is a VertexPartition in the EdgePartition,which is created by
>
Is the VertexPartition in the EdgePartition, the Mirror Cache part?


Yes, exactly. The primary copy of each vertex is stored in the VertexRDD
using the index, values, and mask data structures, which together form a
hash map. In addition, each partition of the VertexRDD stores the
corresponding partition of the routing table to facilitate joining with the
edges. The ShippableVertexPartition class encapsulates the vertex hash map
along with a RoutingTablePartition.

After joining the vertices with the edges, the edge partitions cache their
adjacent vertices in the mirror cache. They use the VertexPartition for
this, which provides only the hash map functionality and not the routing
table.

Ankur <http://www.ankurdave.com/>

--001a11c2c83a291e6104ff455185--

From dev-return-8600-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:07:46 2014
Return-Path: <dev-return-8600-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0559911803
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:07:46 +0000 (UTC)
Received: (qmail 84053 invoked by uid 500); 28 Jul 2014 20:07:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83999 invoked by uid 500); 28 Jul 2014 20:07:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83988 invoked by uid 99); 28 Jul 2014 20:07:45 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:07:45 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of snunez@hortonworks.com designates 209.85.192.182 as permitted sender)
Received: from [209.85.192.182] (HELO mail-pd0-f182.google.com) (209.85.192.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:07:26 +0000
Received: by mail-pd0-f182.google.com with SMTP id fp1so10480190pdb.13
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 13:06:59 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:message-id
         :thread-topic:mime-version:content-type;
        bh=KG9DoOx21LFeFYwzzyMdutalFV+ySGifAaIQCCFDky4=;
        b=YA9iECTXyYy4mq2dgtKui711kD+AMNiRCJimkYTTz6LMagBM1XqIdyWaZeViz4/Wjq
         0jLELROKaxSZ9DizIEiTNT10QqHWlktNp0THmqWfScIeaZwHs9lpQk2sSsVsIfBNNYHr
         IjMylfwAmyMPPLn+QvQ/O6hhbT1EOz9nF1yXcK3bEsTZo72jn0cy2ASZ1oOm0I5wx2sV
         dBR4mNd5rK/JgLjEX/GjUH3YE6dDHgw59YAgc4fIkVf+Edf9Qh469gW+2v23Llhrldy3
         5+WIqbCsS30NO2pn0+vdt2DbvXxrcojudwB561BDvC/nA7Mh2ofHhqS/cagv917uNQ/8
         IYvQ==
X-Gm-Message-State: ALoCoQkTf545MF/ex8oVLV6k4b4Xkoc+zaPhBCi/2joQlQIhhZ2zqf7NfAtf9i1fXW/J6Knr0ZTJVGMMhVGgB4CNokbEitSVz/M02Nm/QXUBfKyGoAtCHO8=
X-Received: by 10.68.57.144 with SMTP id i16mr41257987pbq.48.1406578019396;
        Mon, 28 Jul 2014 13:06:59 -0700 (PDT)
Received: from [10.11.3.247] ([192.175.27.2])
        by mx.google.com with ESMTPSA id ce13sm25576763pdb.76.2014.07.28.13.06.56
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 28 Jul 2014 13:06:58 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Mon, 28 Jul 2014 13:06:50 -0700
Subject: 'Proper' Build Tool
From: Steve Nunez <snunez@hortonworks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Message-ID: <CFFBFB6A.2EBD%snunez@hortonworks.com>
Thread-Topic: 'Proper' Build Tool
Mime-version: 1.0
Content-type: multipart/alternative;
	boundary="B_3489397617_1276759"
X-Virus-Checked: Checked by ClamAV on apache.org

--B_3489397617_1276759
Content-type: text/plain; charset=ISO-8859-1
Content-transfer-encoding: quoted-printable

Gents,

It seem that until recently, building via sbt was a documented process in
the 0.9 overview:

http://spark.apache.org/docs/0.9.0/

The section on building mentions using sbt/sbt assembly. However in the
latest overview:

http://spark.apache.org/docs/latest/index.html

There=B9s no mention of building with sbt.

What=B9s the recommended way to build? What are most people using in their
daily workflow?

Cheers,
- SteveN





--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

--B_3489397617_1276759--



From dev-return-8601-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:21:16 2014
Return-Path: <dev-return-8601-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BE3341188E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:21:16 +0000 (UTC)
Received: (qmail 9759 invoked by uid 500); 28 Jul 2014 20:21:15 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9715 invoked by uid 500); 28 Jul 2014 20:21:15 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 9703 invoked by uid 99); 28 Jul 2014 20:21:14 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:21:14 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of javadba@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:20:57 +0000
Received: by mail-ie0-f181.google.com with SMTP id rp18so7265384iec.12
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 13:20:32 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TRRI4Rwk827cmAM8QEymqnkB0ddGORYPBgZjHmJlKRg=;
        b=jssSTrjXacxCsS/DLNzoxsNnHn69vDpP2dS82sA99rajolAWmXPKZbR0VqgXyRlBRd
         cJsPjXWlf3Ck9u0zttLbQHKF0eAYVpfpihspJdgABslOnhyW3PJxsDODDkS/zM29tt0l
         rnZdY7GTHdKBdoXOacqzhO1999WnD/7Y+54EunPCA9TXQjIprmuBgXyMM5O+V/LkbdDB
         p/HVVSrBaDCByJjtW435kT5suJdr+ldYCFboCWDxIi+MhuoaRFBFQbLRV0N0fJ+PxFer
         uQztgqmtlmp5xuQorDCldTEnP2+jSR7R9jsi7j5kp0k5CuqnsvATlJ5iu8FZSxwQ+Oow
         uoEg==
MIME-Version: 1.0
X-Received: by 10.42.50.82 with SMTP id z18mr46943079icf.47.1406578832509;
 Mon, 28 Jul 2014 13:20:32 -0700 (PDT)
Received: by 10.107.134.203 with HTTP; Mon, 28 Jul 2014 13:20:32 -0700 (PDT)
In-Reply-To: <CFFBFB6A.2EBD%snunez@hortonworks.com>
References: <CFFBFB6A.2EBD%snunez@hortonworks.com>
Date: Mon, 28 Jul 2014 13:20:32 -0700
Message-ID: <CACkSZy0AEuU4uc8+dzRptf=+Uen__DD6Jee_QTFkV_zfFJWZ=w@mail.gmail.com>
Subject: Re: 'Proper' Build Tool
From: Stephen Boesch <javadba@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=90e6ba1efe80a9115904ff46a797
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba1efe80a9115904ff46a797
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Steve,
  I had the opportunity to ask this question at the Summit to Andrew Orr.
 He mentioned that with 1.0 the recommended build tool is with maven. sbt
is however still supported. You will notice that the dependencies are now
completely handled within the maven pom.xml:  the SparkBuild.scala /sbt
reads the dependencies from the pom.xml.

Andrew further suggested to look at the make-distribution.sh to see the
"recommended" way to create builds.  Using mvn on the command line is fine
- but the aforementioned script provides a framework /guideline to set
things up properly.




2014-07-28 13:06 GMT-07:00 Steve Nunez <snunez@hortonworks.com>:

> Gents,
>
> It seem that until recently, building via sbt was a documented process in
> the 0.9 overview:
>
> http://spark.apache.org/docs/0.9.0/
>
> The section on building mentions using sbt/sbt assembly. However in the
> latest overview:
>
> http://spark.apache.org/docs/latest/index.html
>
> There=C2=B9s no mention of building with sbt.
>
> What=C2=B9s the recommended way to build? What are most people using in t=
heir
> daily workflow?
>
> Cheers,
> - SteveN
>
>
>
>
>
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity =
to
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified th=
at
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediate=
ly
> and delete it from your system. Thank You.
>

--90e6ba1efe80a9115904ff46a797--

From dev-return-8602-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:23:46 2014
Return-Path: <dev-return-8602-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 19300118B2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:23:46 +0000 (UTC)
Received: (qmail 20125 invoked by uid 500); 28 Jul 2014 20:23:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20069 invoked by uid 500); 28 Jul 2014 20:23:45 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 20058 invoked by uid 99); 28 Jul 2014 20:23:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:23:45 +0000
X-ASF-Spam-Status: No, hits=4.5 required=10.0
	tests=HTML_MESSAGE,SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of ugw.gi.world@gmail.com does not designate 216.139.236.26 as permitted sender)
Received: from [216.139.236.26] (HELO sam.nabble.com) (216.139.236.26)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:23:41 +0000
Received: from ben.nabble.com ([192.168.236.152])
	by sam.nabble.com with esmtp (Exim 4.72)
	(envelope-from <ugw.gi.world@gmail.com>)
	id 1XBrS3-0002Eu-Ui
	for dev@spark.incubator.apache.org; Mon, 28 Jul 2014 13:23:20 -0700
Date: Mon, 28 Jul 2014 13:23:19 -0700 (PDT)
From: giwa <ugw.gi.world@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <CAKs7j9c_o+KxUnodEiS_HjO7=9OvmyBMAELZvkksP+OTFvWT3g@mail.gmail.com>
In-Reply-To: <1406534584408-7546.post@n3.nabble.com>
References: <1406514867558-7538.post@n3.nabble.com> <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com> <1406534584408-7546.post@n3.nabble.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_204817_8919269.1406578999943"
X-Virus-Checked: Checked by ClamAV on apache.org

------=_Part_204817_8919269.1406578999943
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit

Hi Yu,

I could help translating Spark documentation to Japanese. Please let me
know if you need.

Best,

Ken


On Mon, Jul 28, 2014 at 1:03 AM, Yu Ishikawa [via Apache Spark Developers
List] <ml-node+s1001551n7546h59@n3.nabble.com> wrote:

> Hello Patrick,
>
> Thank you for your replying.
> I checked some other projects in terms of i18n of documentations.
>
> For example, the documentations of the Apache HTTP server project are
> supported i18n natively.
> https://github.com/apache/httpd/blob/trunk/docs%2Fmanual%2Findex.html
>
> But it seems that the Chinese documentations of Apache HBase are only
> linked from the top page of HBase.
> From:http://hbase.apache.org/
> To: http://abloz.com/hbase/book.html
>
> I think that it is currently difficult to support i18n in Apache Spark
> documentations.
> I suggest that I will translate the documentations in Japanese in Github
> page unofficially.
> If possible, would you please link from translated documentations to
> Apache Spark documentation.
>
> Regards,
> Yu
>
> ------------------------------
>  If you reply to this email, your message will be added to the discussion
> below:
>
> http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538p7546.html
>  To start a new topic under Apache Spark Developers List, email
> ml-node+s1001551n1h64@n3.nabble.com
> To unsubscribe from Apache Spark Developers List, click here
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=unsubscribe_by_code&node=1&code=dWd3LmdpLndvcmxkQGdtYWlsLmNvbXwxfC0zMTQ3MDY5ODA=>
> .
> NAML
> <http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
>



-- 

Kenichi Takagiwa
-------------------------------------------------------------
Keio University
Graduate School of Science and Technology
Department of Open and Environmental Systems
Faculty of Computer Science
Hiroaki Nishi Laboratory
Email: ugw.gi.world@gmail.com
Phone: +81-50-3575-6586




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538p7570.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.
------=_Part_204817_8919269.1406578999943--

From dev-return-8603-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:26:34 2014
Return-Path: <dev-return-8603-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81E9B118E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:26:34 +0000 (UTC)
Received: (qmail 27166 invoked by uid 500); 28 Jul 2014 20:26:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27104 invoked by uid 500); 28 Jul 2014 20:26:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27090 invoked by uid 99); 28 Jul 2014 20:26:32 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:26:32 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.46 as permitted sender)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:26:14 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so9369955oag.33
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 13:25:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=b7aePxbtb0tPq13UAeQ498Qj2aC+gco6HM2aym+omso=;
        b=FuCeNWHznigafO8YjtmDHEGJC5Def77NrezusbM3OsUfZv07VZ5JYfod09VUWAm3Hf
         3ecddG5P3eyQyaa1rh2hJT1MnB6lqelc8IZGZIflRL7mf9m1ECn9j4Io/GT6ANvMBqEI
         dAWsFLBGgmP8Yl0BspT1MSyrtDJzYU5snDyFbyvPGLPd/tFGlhwDhWzBDK+I5AbYRDDg
         0RwMdUhlv9AQjlPpByH0Tr9Mb4R83d8U/lXpoVD7245wojtevKHnGgc+skmJhEzeMrul
         bSjRrQu9X7YKpQIbPuFRl5+IeW+gc58Sh5TqIrjedTXhN1oHWH7d+09lEJbac51VFZAw
         9yeg==
MIME-Version: 1.0
X-Received: by 10.182.98.194 with SMTP id ek2mr52120364obb.5.1406579149804;
 Mon, 28 Jul 2014 13:25:49 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 28 Jul 2014 13:25:49 -0700 (PDT)
In-Reply-To: <CACkSZy0AEuU4uc8+dzRptf=+Uen__DD6Jee_QTFkV_zfFJWZ=w@mail.gmail.com>
References: <CFFBFB6A.2EBD%snunez@hortonworks.com>
	<CACkSZy0AEuU4uc8+dzRptf=+Uen__DD6Jee_QTFkV_zfFJWZ=w@mail.gmail.com>
Date: Mon, 28 Jul 2014 13:25:49 -0700
Message-ID: <CABPQxsttVgS_=e2tG=qKfEeSP9cqtzpk6r9LduR3-tX6ns4r4A@mail.gmail.com>
Subject: Re: 'Proper' Build Tool
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah for packagers we officially recommend using maven. Spark's
dependency graph is very complicated and Maven and SBT use different
conflict resolution strategies, so we've opted to official support
Maven.

SBT is still around though and it's used more often by day-to-day developers.

- Patrick

From dev-return-8604-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:33:28 2014
Return-Path: <dev-return-8604-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 930D011984
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:33:28 +0000 (UTC)
Received: (qmail 50421 invoked by uid 500); 28 Jul 2014 20:33:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50368 invoked by uid 500); 28 Jul 2014 20:33:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 50357 invoked by uid 99); 28 Jul 2014 20:33:27 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:33:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of snunez@hortonworks.com designates 209.85.192.173 as permitted sender)
Received: from [209.85.192.173] (HELO mail-pd0-f173.google.com) (209.85.192.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:33:10 +0000
Received: by mail-pd0-f173.google.com with SMTP id w10so10291175pde.18
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 13:32:45 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:message-id
         :thread-topic:references:in-reply-to:mime-version:content-type
         :content-transfer-encoding;
        bh=9mzHAC5CiaPb/2oPA5vEqmxb5x88JtPuuw3xFCkrMRY=;
        b=WFZLaUx6vblmbcVrdaw2AZxOUuVKdPLorVCw6XNVoiE0QjRlUZE1RE0wrUrn8S403A
         1e1i3VkkLhuJZeW9lYkkOyf1j7u50g+sdTOuNlS8tR8ePGnU7midTym9dPznt/AO9dwB
         4D61pfdSN//+V2hC3J6YmPL2dzttz8dzDACm9i4k1xVKVZyYJxGXCJmTn4aTalo2mseE
         SpwRWoXT1WVcIiMIX+Bv9DjoQlbuuqBVkRp6dTBDNs714TDEuI9Ot7Ihj6+qCQNOZmCP
         9qgYnfuOcidVUaKENDMZANmcgW0dbIk9lseyyfhEnBnOj+4YmOMNVQNOZrWTWnrwPUsR
         czqQ==
X-Gm-Message-State: ALoCoQnwD73tHTk1LHEbr9o8S8GaZm7DffZj/jGxu9ccodIMlGWpLWNPfDb1BxSfrk+E7syPF19WEmq1AD3xbaJrJAWtRGVMio0g7NeejCuEShCeJsncstU=
X-Received: by 10.68.166.36 with SMTP id zd4mr41375653pbb.54.1406579565089;
        Mon, 28 Jul 2014 13:32:45 -0700 (PDT)
Received: from [10.11.3.247] ([192.175.27.2])
        by mx.google.com with ESMTPSA id st5sm18565815pbc.68.2014.07.28.13.32.41
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 28 Jul 2014 13:32:44 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Mon, 28 Jul 2014 13:32:38 -0700
Subject: Re: Working Formula for Hive 0.13?
From: Steve Nunez <snunez@hortonworks.com>
To: <dev@spark.apache.org>
Message-ID: <CFFC00D1.2ED4%snunez@hortonworks.com>
Thread-Topic: Working Formula for Hive 0.13?
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
 <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
 <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
 <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
 <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
 <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
 <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
 <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
 <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
 <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
 <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
In-Reply-To: <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
Mime-version: 1.0
Content-type: text/plain; charset=ISO-8859-1
Content-transfer-encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

So, do we have a short-term fix until Hive 0.14 comes out? Perhaps adding
the hive-exec jar to the spark-project repo? It doesn=B9t look like there=
=B9s
a release date schedule for 0.14.



On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:

>Exactly, forgot to mention Hulu team also made changes to cope with those
>incompatibility issues, but they said that=B9s relatively easy once the
>re-packaging work is done.
>
>
>On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.com>
>wrote:
>
>> I've heard from Cloudera that there were hive internal changes between
>> 0.12 and 0.13 that required code re-writing. Over time it might be
>> possible for us to integrate with hive using API's that are more
>> stable (this is the domain of Michael/Cheng/Yin more than me!). It
>> would be interesting to see what the Hulu folks did.
>>
>> - Patrick
>>
>> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com>
>> wrote:
>> > AFAIK, according a recent talk, Hulu team in China has built Spark SQL
>> > against Hive 0.13 (or 0.13.1?) successfully. Basically they also
>> > re-packaged Hive 0.13 as what the Spark team did. The slides of the
>>talk
>> > hasn't been released yet though.
>> >
>> >
>> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com> wrote:
>> >
>> >> Owen helped me find this:
>> >> https://issues.apache.org/jira/browse/HIVE-7423
>> >>
>> >> I guess this means that for Hive 0.14, Spark should be able to
>>directly
>> >> pull in hive-exec-core.jar
>> >>
>> >> Cheers
>> >>
>> >>
>> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.com>
>> >> wrote:
>> >>
>> >> > It would be great if the hive team can fix that issue. If not,
>>we'll
>> >> > have to continue forking our own version of Hive to change the way
>>it
>> >> > publishes artifacts.
>> >> >
>> >> > - Patrick
>> >> >
>> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com>
>>wrote:
>> >> > > Talked with Owen offline. He confirmed that as of 0.13,
>>hive-exec is
>> >> > still
>> >> > > uber jar.
>> >> > >
>> >> > > Right now I am facing the following error building against Hive
>> 0.13.1
>> >> :
>> >> > >
>> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10: Could
>>not
>> >> > > resolve dependencies for project
>> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
>>following
>> >> > > artifacts could not be resolved:
>> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
>> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
>> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
>> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
>> >> > > http://repo.maven.apache.org/maven2 was cached in the local
>> >> repository,
>> >> > > resolution will not be reattempted until the update interval of
>> >> > maven-repo
>> >> > > has elapsed or updates are forced -> [Help 1]
>> >> > >
>> >> > > Some hint would be appreciated.
>> >> > >
>> >> > > Cheers
>> >> > >
>> >> > >
>> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com>
>> wrote:
>> >> > >
>> >> > >> Yes, it is published. As of previous versions, at least,
>>hive-exec
>> >> > >> included all of its dependencies *in its artifact*, making it
>> unusable
>> >> > >> as-is because it contained copies of dependencies that clash
>>with
>> >> > >> versions present in other artifacts, and can't be managed with
>> Maven
>> >> > >> mechanisms.
>> >> > >>
>> >> > >> I am not sure why hive-exec was not published normally, with
>>just
>> its
>> >> > >> own classes. That's why it was copied, into an artifact with
>>just
>> >> > >> hive-exec code.
>> >> > >>
>> >> > >> You could do the same thing for hive-exec 0.13.1.
>> >> > >> Or maybe someone knows that it's published more 'normally' now.
>> >> > >> I don't think hive-metastore is related to this question?
>> >> > >>
>> >> > >> I am no expert on the Hive artifacts, just remembering what the
>> issue
>> >> > >> was initially in case it helps you get to a similar solution.
>> >> > >>
>> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com>
>> wrote:
>> >> > >> > hive-exec (as of 0.13.1) is published here:
>> >> > >> >
>> >> > >>
>> >> >
>> >>
>>=20
>>http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C
>>0.13.1%7Cjar
>> >> > >> >
>> >> > >> > Should a JIRA be opened so that dependency on hive-metastore
>>can
>> be
>> >> > >> > replaced by dependency on hive-exec ?
>> >> > >> >
>> >> > >> > Cheers
>> >> > >> >
>> >> > >> >
>> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
>><sowen@cloudera.com>
>> >> > wrote:
>> >> > >> >
>> >> > >> >> The reason for org.spark-project.hive is that Spark relies on
>> >> > >> >> hive-exec, but the Hive project does not publish this
>>artifact
>> by
>> >> > >> >> itself, only with all its dependencies as an uber jar. Maybe
>> that's
>> >> > >> >> been improved. If so, you need to point at the new hive-exec
>>and
>> >> > >> >> perhaps sort out its dependencies manually in your build.
>> >> > >> >>
>> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.com>
>> >> wrote:
>> >> > >> >> > I found 0.13.1 artifacts in maven:
>> >> > >> >> >
>> >> > >> >>
>> >> > >>
>> >> >
>> >>
>>=20
>>http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metasto
>>re%7C0.13.1%7Cjar
>> >> > >> >> >
>> >> > >> >> > However, Spark uses groupId of org.spark-project.hive, not
>> >> > >> >> org.apache.hive
>> >> > >> >> >
>> >> > >> >> > Can someone tell me how it is supposed to work ?
>> >> > >> >> >
>> >> > >> >> > Cheers
>> >> > >> >> >
>> >> > >> >> >
>> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>> >> > snunez@hortonworks.com>
>> >> > >> >> wrote:
>> >> > >> >> >
>> >> > >> >> >> I saw a note earlier, perhaps on the user list, that at
>>least
>> >> one
>> >> > >> >> person is
>> >> > >> >> >> using Hive 0.13. Anyone got a working build configuration
>>for
>> >> this
>> >> > >> >> version
>> >> > >> >> >> of Hive?
>> >> > >> >> >>
>> >> > >> >> >> Regards,
>> >> > >> >> >> - Steve
>> >> > >> >> >>
>> >> > >> >> >>
>> >> > >> >> >>
>> >> > >> >> >> --
>> >> > >> >> >> CONFIDENTIALITY NOTICE
>> >> > >> >> >> NOTICE: This message is intended for the use of the
>> individual
>> >> or
>> >> > >> >> entity to
>> >> > >> >> >> which it is addressed and may contain information that is
>> >> > >> confidential,
>> >> > >> >> >> privileged and exempt from disclosure under applicable
>>law.
>> If
>> >> the
>> >> > >> >> reader
>> >> > >> >> >> of this message is not the intended recipient, you are
>>hereby
>> >> > >> notified
>> >> > >> >> that
>> >> > >> >> >> any printing, copying, dissemination, distribution,
>> disclosure
>> >> or
>> >> > >> >> >> forwarding of this communication is strictly prohibited.
>>If
>> you
>> >> > have
>> >> > >> >> >> received this communication in error, please contact the
>> sender
>> >> > >> >> immediately
>> >> > >> >> >> and delete it from your system. Thank You.
>> >> > >> >> >>
>> >> > >> >>
>> >> > >>
>> >> >
>> >>
>>



--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

From dev-return-8605-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:33:57 2014
Return-Path: <dev-return-8605-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D0F52119A1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:33:57 +0000 (UTC)
Received: (qmail 55845 invoked by uid 500); 28 Jul 2014 20:33:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55787 invoked by uid 500); 28 Jul 2014 20:33:57 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55775 invoked by uid 99); 28 Jul 2014 20:33:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:33:56 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.179] (HELO mail-vc0-f179.google.com) (209.85.220.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:33:52 +0000
Received: by mail-vc0-f179.google.com with SMTP id hq11so12345737vcb.10
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 13:33:30 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=bIR6oDoTxrFuvVqkiB1tAEiv9jEz27z5HBx6dahlSE8=;
        b=QkVEeHRUtuZ4Uj1Zfzv8OScpwm9uwE/QnamrJ3RBP13to2CGWzljlA9cjccMwNLlOv
         zm3S3lLohVmoPa2aiafQQo8fQ22b7WE1WSHbMUTAfXs0XNLb7Cta1dy1EuYe+DJefR9a
         RXWiMXTMllDElaMlHhgwjgoO05ytdPL5hln5MAHuI1Gi3BdPg1RhYo/+qkjwlLMcpijt
         umIHKP0BS5mnUx4Nc4+VpMHNQnZH9fkIR43D4esM3/fVwVHqnBx9mrzs64sc7kbDmzo5
         8yE9ukegF61Hp0Lcdqlri4vNwpHu3iTIGL7a5jVC4KKLZM6uZ7pdSf58grfgA67RGZ3j
         SoUg==
X-Gm-Message-State: ALoCoQnmhidwWghactazX+m4LaWWgnbXP8Qu6v7rMdUXyLKL5cX81Zcc551L2LJxN2JR3RmSOFZw
MIME-Version: 1.0
X-Received: by 10.221.24.135 with SMTP id re7mr3369837vcb.53.1406579610581;
 Mon, 28 Jul 2014 13:33:30 -0700 (PDT)
Received: by 10.220.1.200 with HTTP; Mon, 28 Jul 2014 13:33:30 -0700 (PDT)
Date: Mon, 28 Jul 2014 13:33:30 -0700
Message-ID: <CAEYYnxbDHcT71P0DyaS66fA6LT-qitTnS36WoK0SOEgO=e7fRw@mail.gmail.com>
Subject: Jenkins Documentation Build
From: DB Tsai <dbtsai@dbtsai.com>
To: Patrick Wendell <pwendell@gmail.com>, dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

I started to work on the documentation about my work in spark. Since
it has lots of dependencies to get the document build setup locally,
it will be nice that people are able to verify/preview the document
build for each PR. Is it possible to build the doc in Jenkins, and
have a link pointing to the doc in github comment?

Also, it will be very handy to have the test report link in the github
comment like this one,
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/17283/testReport/
Then developers can easily find which tests fail the build.

(ps, seems that the build system is not merging the PR approved by
committer now.)

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai

From dev-return-8606-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:35:35 2014
Return-Path: <dev-return-8606-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E288119B6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:35:35 +0000 (UTC)
Received: (qmail 61845 invoked by uid 500); 28 Jul 2014 20:35:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61780 invoked by uid 500); 28 Jul 2014 20:35:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 61762 invoked by uid 99); 28 Jul 2014 20:35:34 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:35:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.175 as permitted sender)
Received: from [209.85.212.175] (HELO mail-wi0-f175.google.com) (209.85.212.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:35:16 +0000
Received: by mail-wi0-f175.google.com with SMTP id ho1so5072465wib.8
        for <dev@spark.incubator.apache.org>; Mon, 28 Jul 2014 13:34:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=+yTekxafFVxd/j5WabBwsf+nnaaXpvUoUEmJVcjaDGU=;
        b=uAPR8JmEIPGF7fE6bKx32XQ8IFy5J7JhDYjawoHs4dXw+d7aqya4ntdG2nBUrWm/up
         W1vp+DlMMVMJuEd8o6d0jqXBrUDo7mqdW+4vOoWLooNqgPQJfAWuXxfDS4q/Oz6w4hqN
         lqivQtfRP7X71Fzrkn+TaaJ4IcGU+T8ZvUiTtUGGmPUjBK4BeI8B9qkRT12A2iYXpohI
         I0HhHzbzakpDXCiz0KKPuj0tJ2C190ybDOVDBFWhQp07BfjwpqPPhrW1jlqwrTGQ8Vx8
         TCC1+hEYQjK44LXs9Ii+y5v/oDK5qqGHA74doz8Q/xpD6IuWv9LJ6mscuHbVYUw8CDM2
         Lg7w==
X-Received: by 10.180.21.235 with SMTP id y11mr33872181wie.75.1406579688065;
 Mon, 28 Jul 2014 13:34:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 28 Jul 2014 13:34:07 -0700 (PDT)
In-Reply-To: <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
References: <1406514867558-7538.post@n3.nabble.com> <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 28 Jul 2014 16:34:07 -0400
Message-ID: <CAOhmDzc36qMwnPC6937sewU8yAkQOWs2TDKuuScGz3U996+aqQ@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
To: dev <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b874e04a7d06204ff46da53
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b874e04a7d06204ff46da53
Content-Type: text/plain; charset=UTF-8

On Mon, Jul 28, 2014 at 12:48 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> I'd be interested to know what other projects
> do about this situation!
>

I know some projects get translations crowdsourced via one website or
other. Googling real quick, it appears there are a few sites that offer
homes for this kind of work, but I wouldn't know about any of them.

Nick

--047d7b874e04a7d06204ff46da53--

From dev-return-8607-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 20:35:36 2014
Return-Path: <dev-return-8607-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7039A119B7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 20:35:36 +0000 (UTC)
Received: (qmail 62862 invoked by uid 500); 28 Jul 2014 20:35:35 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62801 invoked by uid 500); 28 Jul 2014 20:35:35 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 62771 invoked by uid 99); 28 Jul 2014 20:35:35 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:35:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.178 as permitted sender)
Received: from [74.125.82.178] (HELO mail-we0-f178.google.com) (74.125.82.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 20:35:17 +0000
Received: by mail-we0-f178.google.com with SMTP id w61so7861858wes.23
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 13:34:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=+yTekxafFVxd/j5WabBwsf+nnaaXpvUoUEmJVcjaDGU=;
        b=uAPR8JmEIPGF7fE6bKx32XQ8IFy5J7JhDYjawoHs4dXw+d7aqya4ntdG2nBUrWm/up
         W1vp+DlMMVMJuEd8o6d0jqXBrUDo7mqdW+4vOoWLooNqgPQJfAWuXxfDS4q/Oz6w4hqN
         lqivQtfRP7X71Fzrkn+TaaJ4IcGU+T8ZvUiTtUGGmPUjBK4BeI8B9qkRT12A2iYXpohI
         I0HhHzbzakpDXCiz0KKPuj0tJ2C190ybDOVDBFWhQp07BfjwpqPPhrW1jlqwrTGQ8Vx8
         TCC1+hEYQjK44LXs9Ii+y5v/oDK5qqGHA74doz8Q/xpD6IuWv9LJ6mscuHbVYUw8CDM2
         Lg7w==
X-Received: by 10.180.21.235 with SMTP id y11mr33872181wie.75.1406579688065;
 Mon, 28 Jul 2014 13:34:48 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Mon, 28 Jul 2014 13:34:07 -0700 (PDT)
In-Reply-To: <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
References: <1406514867558-7538.post@n3.nabble.com> <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Mon, 28 Jul 2014 16:34:07 -0400
Message-ID: <CAOhmDzc36qMwnPC6937sewU8yAkQOWs2TDKuuScGz3U996+aqQ@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
To: dev <dev@spark.apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b874e04a7d06204ff46da53
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b874e04a7d06204ff46da53
Content-Type: text/plain; charset=UTF-8

On Mon, Jul 28, 2014 at 12:48 AM, Patrick Wendell <pwendell@gmail.com>
wrote:

> I'd be interested to know what other projects
> do about this situation!
>

I know some projects get translations crowdsourced via one website or
other. Googling real quick, it appears there are a few sites that offer
homes for this kind of work, but I wouldn't know about any of them.

Nick

--047d7b874e04a7d06204ff46da53--

From dev-return-8608-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 22:00:10 2014
Return-Path: <dev-return-8608-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 89CB811FFC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 22:00:10 +0000 (UTC)
Received: (qmail 31203 invoked by uid 500); 28 Jul 2014 22:00:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31145 invoked by uid 500); 28 Jul 2014 22:00:09 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31132 invoked by uid 99); 28 Jul 2014 22:00:09 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 22:00:09 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 22:00:04 +0000
Received: by mail-ie0-f169.google.com with SMTP id rd18so7439259iec.14
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 14:59:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=0QIvqifMxXwS348staVQrIwHjdLndJ87WfCUqtGwtrc=;
        b=teFgew/if4+w3G2goQIZVyeQEqdFqMX780p7Y8U3pGTagPn46nYoHr+pLXMqwB9pqk
         frmJOQPW6RRz0KMDjEHegPzsOQIl/kfI+Z9qCVEggGboGn59SK4Aw4YuH3w/UG0HRJ2n
         p9tEpMX83QMHqDgIrc45omnWYT2Jz4b1jSR25gVHHeghrj+ktpGHG4XWh8EMSlYRhGSO
         ueppkCkCFuG1/wtnFdf+ae37tbhdzYPnpIu/iqkpTb+f2LTXPgRAGDgbnUT54jUcDqvV
         5k0xYEaHFxhUfILOuN3xT+w/dCqyNfH3vB8REaXKWrdLzT2q9tvkgY70XFvMe3ePxUux
         8enQ==
X-Received: by 10.42.186.2 with SMTP id cq2mr1462icb.25.1406584782511; Mon, 28
 Jul 2014 14:59:42 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.129.10 with HTTP; Mon, 28 Jul 2014 14:59:12 -0700 (PDT)
In-Reply-To: <71937D1E-0EC2-413E-A447-D60B49D16631@gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
 <71937D1E-0EC2-413E-A447-D60B49D16631@gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Mon, 28 Jul 2014 14:59:12 -0700
Message-ID: <CAMwrk0m3saqjbX8MPtN_=fEdjiLv8ev+QR4jwFoEgXh_11Nqag@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Let me add my vote as well.
Did some basic tests by running simple projects with various Spark
modules. Tested checksums.

+1

On Sun, Jul 27, 2014 at 4:52 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> +1
>
> Tested this on Mac OS X.
>
> Matei
>
> On Jul 25, 2014, at 4:08 PM, Tathagata Das <tathagata.das1565@gmail.com> wrote:
>
>> Please vote on releasing the following candidate as Apache Spark version 1.0.2.
>>
>> This release fixes a number of bugs in Spark 1.0.1.
>> Some of the notable ones are
>> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
>> SPARK-1199. The fix was reverted for 1.0.2.
>> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
>> HDFS CSV file.
>> The full list is at http://s.apache.org/9NJ
>>
>> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>>
>> The release files, including signatures, digests, etc can be found at:
>> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>>
>> Release artifacts are signed with the following key:
>> https://people.apache.org/keys/committer/tdas.asc
>>
>> The staging repository for this release can be found at:
>> https://repository.apache.org/content/repositories/orgapachespark-1024/
>>
>> The documentation corresponding to this release can be found at:
>> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>>
>> Please vote on releasing this package as Apache Spark 1.0.2!
>>
>> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
>> a majority of at least 3 +1 PMC votes are cast.
>> [ ] +1 Release this package as Apache Spark 1.0.2
>> [ ] -1 Do not release this package because ...
>>
>> To learn more about Apache Spark, please see
>> http://spark.apache.org/
>

From dev-return-8609-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 22:41:46 2014
Return-Path: <dev-return-8609-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 97A10112F0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 22:41:46 +0000 (UTC)
Received: (qmail 19120 invoked by uid 500); 28 Jul 2014 22:41:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19063 invoked by uid 500); 28 Jul 2014 22:41:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 19049 invoked by uid 99); 28 Jul 2014 22:41:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 22:41:45 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.216.43] (HELO mail-qa0-f43.google.com) (209.85.216.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 22:41:41 +0000
Received: by mail-qa0-f43.google.com with SMTP id w8so8467883qac.2
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 15:41:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=9GdwDj2svloBdv1ScLvSK7FIWjEoh/CGtiYZgClxaTs=;
        b=kBKqWopIHc+vMAPwjUZtTzFtHxjJjGA7Ihulm0zwxsxEeSkU0EY4c6eob90bxCtwMk
         B/H8mX1HvDKY6WUBriZy5ODF39CKhjyCJw/m5XZYTtAsECWyMhZINAkbWbWZ4+JUYZ+O
         ujXiTH8GOFPX7LzmifsG5POrmwaUa1SQ+W5qDRUyMVwC0wGugwRcu3+s+BQ9QDxEjZM3
         OrukYBvQpAIN73voz8MTvn8Q9zNYa6hYXnaIWQpBSc668qvSJn3udy+3UyQBTwWu+K2C
         sZjtFzHe8OznJrLZrODwnvJrIpnPF8SvaPhhJSO8UUKxuL8EkkCYQGaI4h1KOc7/jhuF
         SRoA==
X-Gm-Message-State: ALoCoQmqszvmaCz3eFRsB8d7B30XWHyAnyjqfV48oDwptY5AMyNwIYGlVQdOICPNVS5Gpyx3Q5sU
X-Received: by 10.140.25.11 with SMTP id 11mr67586561qgs.9.1406587280075; Mon,
 28 Jul 2014 15:41:20 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 28 Jul 2014 15:41:00 -0700 (PDT)
In-Reply-To: <53D63515.8060102@sjtu.edu.cn>
References: <53D63515.8060102@sjtu.edu.cn>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 28 Jul 2014 15:41:00 -0700
Message-ID: <CAPh_B=YnRWGbm3vZg7Fm9SxJJGHoMyFL-N2=ingxLacQO5skWA@mail.gmail.com>
Subject: Re: package/assemble with local spark
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c02a8e2cc8a304ff489fed
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c02a8e2cc8a304ff489fed
Content-Type: text/plain; charset=UTF-8

You can use publish-local in sbt.

If you want to be more careful, you can give Spark a different version
number and use that version number in your app.



On Mon, Jul 28, 2014 at 4:33 AM, Larry Xiao <xiaodi@sjtu.edu.cn> wrote:

> Hi,
>
> How do you package an app with modified spark?
>
> In seems sbt would resolve the dependencies, and use the official spark
> release.
>
> Thank you!
>
> Larry
>

--001a11c02a8e2cc8a304ff489fed--

From dev-return-8610-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 22:42:31 2014
Return-Path: <dev-return-8610-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0C56D1131B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 22:42:31 +0000 (UTC)
Received: (qmail 26079 invoked by uid 500); 28 Jul 2014 22:42:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26027 invoked by uid 500); 28 Jul 2014 22:42:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 26014 invoked by uid 99); 28 Jul 2014 22:42:29 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 22:42:29 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.180 as permitted sender)
Received: from [209.85.160.180] (HELO mail-yk0-f180.google.com) (209.85.160.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 22:42:12 +0000
Received: by mail-yk0-f180.google.com with SMTP id 200so5077955ykr.39
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 15:41:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=qH4COf+Bp5yFeL2JYMcszV3LMWzzNbp04oDIvRHRd1w=;
        b=nHMZk9OPmvKZQJdWttZmX7g3J2mPjrm9Y5Va2HcEu29ZVg0YXeKXsb4w6l9b9DI+tg
         zoLX3Uq7wpM1csgISZu2l3AD+FOaDNj79jJGGhYNyiqkvYCe3yxOXgN2kX+iobQGvwJ9
         5t2fVj5dbWzDFqIUUzOEFqJ9WrryUqRuXzk+am19PVP7ugl/A21AQ5ONXoj7jRGLm9dX
         +azuOCEDGKwf3BeJCVyI3WwF9p7RtizdQ9WDFIqMfrTHbjNIeGGuztXU7S/F8KVZxIJL
         AN9IZ7Oxlf+VjcUWQ4U0EtSWyCc1VbbA+XXW1MMZ6uIWicHRZlYe0c2tU4mQrAJFoc1U
         sdNA==
MIME-Version: 1.0
X-Received: by 10.236.134.8 with SMTP id r8mr7868287yhi.153.1406587307323;
 Mon, 28 Jul 2014 15:41:47 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Mon, 28 Jul 2014 15:41:47 -0700 (PDT)
In-Reply-To: <CFFC00D1.2ED4%snunez@hortonworks.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
	<CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
	<CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
	<CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
	<CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
	<CFFC00D1.2ED4%snunez@hortonworks.com>
Date: Mon, 28 Jul 2014 15:41:47 -0700
Message-ID: <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=485b397dcf91cc754904ff48a0ae
X-Virus-Checked: Checked by ClamAV on apache.org

--485b397dcf91cc754904ff48a0ae
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

After manually copying hive 0.13.1 jars to local maven repo, I got the
following errors when building spark-hive_2.10 module :

[ERROR]
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveConte=
xt.scala:182:
type mismatch;
 found   : String
 required: Array[String]
[ERROR]       val proc: CommandProcessor =3D
CommandProcessorFactory.get(tokens(0), hiveconf)
[ERROR]
 ^
[ERROR]
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetas=
toreCatalog.scala:60:
value getAllPartitionsForPruner is not a member of org.apache.
 hadoop.hive.ql.metadata.Hive
[ERROR]         client.getAllPartitionsForPruner(table).toSeq
[ERROR]                ^
[ERROR]
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetas=
toreCatalog.scala:267:
overloaded method constructor TableDesc with alternatives:
  (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
Class[_],x$3: java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
<and>
  ()org.apache.hadoop.hive.ql.plan.TableDesc
 cannot be applied to (Class[org.apache.hadoop.hive.serde2.Deserializer],
Class[(some other)?0(in value tableDesc)(in value tableDesc)], Class[?0(in
value tableDesc)(in   value tableDesc)], java.util.Properties)
[ERROR]   val tableDesc =3D new TableDesc(
[ERROR]                   ^
[WARNING] Class org.antlr.runtime.tree.CommonTree not found - continuing
with a stub.
[WARNING] Class org.antlr.runtime.Token not found - continuing with a stub.
[WARNING] Class org.antlr.runtime.tree.Tree not found - continuing with a
stub.
[ERROR]
     while compiling:
/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.sc=
ala
        during phase: typer
     library version: version 2.10.4
    compiler version: version 2.10.4

The above shows incompatible changes between 0.12 and 0.13.1
e.g. the first error corresponds to the following method
in CommandProcessorFactory :
  public static CommandProcessor get(String[] cmd, HiveConf conf)

Cheers


On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com> wrote=
:

> So, do we have a short-term fix until Hive 0.14 comes out? Perhaps adding
> the hive-exec jar to the spark-project repo? It doesn=C2=B9t look like th=
ere=C2=B9s
> a release date schedule for 0.14.
>
>
>
> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
>
> >Exactly, forgot to mention Hulu team also made changes to cope with thos=
e
> >incompatibility issues, but they said that=C2=B9s relatively easy once t=
he
> >re-packaging work is done.
> >
> >
> >On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.com>
> >wrote:
> >
> >> I've heard from Cloudera that there were hive internal changes between
> >> 0.12 and 0.13 that required code re-writing. Over time it might be
> >> possible for us to integrate with hive using API's that are more
> >> stable (this is the domain of Michael/Cheng/Yin more than me!). It
> >> would be interesting to see what the Hulu folks did.
> >>
> >> - Patrick
> >>
> >> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com>
> >> wrote:
> >> > AFAIK, according a recent talk, Hulu team in China has built Spark S=
QL
> >> > against Hive 0.13 (or 0.13.1?) successfully. Basically they also
> >> > re-packaged Hive 0.13 as what the Spark team did. The slides of the
> >>talk
> >> > hasn't been released yet though.
> >> >
> >> >
> >> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com> wrote:
> >> >
> >> >> Owen helped me find this:
> >> >> https://issues.apache.org/jira/browse/HIVE-7423
> >> >>
> >> >> I guess this means that for Hive 0.14, Spark should be able to
> >>directly
> >> >> pull in hive-exec-core.jar
> >> >>
> >> >> Cheers
> >> >>
> >> >>
> >> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <pwendell@gmail.co=
m
> >
> >> >> wrote:
> >> >>
> >> >> > It would be great if the hive team can fix that issue. If not,
> >>we'll
> >> >> > have to continue forking our own version of Hive to change the wa=
y
> >>it
> >> >> > publishes artifacts.
> >> >> >
> >> >> > - Patrick
> >> >> >
> >> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com>
> >>wrote:
> >> >> > > Talked with Owen offline. He confirmed that as of 0.13,
> >>hive-exec is
> >> >> > still
> >> >> > > uber jar.
> >> >> > >
> >> >> > > Right now I am facing the following error building against Hive
> >> 0.13.1
> >> >> :
> >> >> > >
> >> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10: Coul=
d
> >>not
> >> >> > > resolve dependencies for project
> >> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
> >>following
> >> >> > > artifacts could not be resolved:
> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
> >> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
> >> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> >> >> > > http://repo.maven.apache.org/maven2 was cached in the local
> >> >> repository,
> >> >> > > resolution will not be reattempted until the update interval of
> >> >> > maven-repo
> >> >> > > has elapsed or updates are forced -> [Help 1]
> >> >> > >
> >> >> > > Some hint would be appreciated.
> >> >> > >
> >> >> > > Cheers
> >> >> > >
> >> >> > >
> >> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com>
> >> wrote:
> >> >> > >
> >> >> > >> Yes, it is published. As of previous versions, at least,
> >>hive-exec
> >> >> > >> included all of its dependencies *in its artifact*, making it
> >> unusable
> >> >> > >> as-is because it contained copies of dependencies that clash
> >>with
> >> >> > >> versions present in other artifacts, and can't be managed with
> >> Maven
> >> >> > >> mechanisms.
> >> >> > >>
> >> >> > >> I am not sure why hive-exec was not published normally, with
> >>just
> >> its
> >> >> > >> own classes. That's why it was copied, into an artifact with
> >>just
> >> >> > >> hive-exec code.
> >> >> > >>
> >> >> > >> You could do the same thing for hive-exec 0.13.1.
> >> >> > >> Or maybe someone knows that it's published more 'normally' now=
.
> >> >> > >> I don't think hive-metastore is related to this question?
> >> >> > >>
> >> >> > >> I am no expert on the Hive artifacts, just remembering what th=
e
> >> issue
> >> >> > >> was initially in case it helps you get to a similar solution.
> >> >> > >>
> >> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com>
> >> wrote:
> >> >> > >> > hive-exec (as of 0.13.1) is published here:
> >> >> > >> >
> >> >> > >>
> >> >> >
> >> >>
> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C
> >>0.13.1%7Cjar
> >> >> > >> >
> >> >> > >> > Should a JIRA be opened so that dependency on hive-metastore
> >>can
> >> be
> >> >> > >> > replaced by dependency on hive-exec ?
> >> >> > >> >
> >> >> > >> > Cheers
> >> >> > >> >
> >> >> > >> >
> >> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
> >><sowen@cloudera.com>
> >> >> > wrote:
> >> >> > >> >
> >> >> > >> >> The reason for org.spark-project.hive is that Spark relies =
on
> >> >> > >> >> hive-exec, but the Hive project does not publish this
> >>artifact
> >> by
> >> >> > >> >> itself, only with all its dependencies as an uber jar. Mayb=
e
> >> that's
> >> >> > >> >> been improved. If so, you need to point at the new hive-exe=
c
> >>and
> >> >> > >> >> perhaps sort out its dependencies manually in your build.
> >> >> > >> >>
> >> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <yuzhihong@gmail.co=
m
> >
> >> >> wrote:
> >> >> > >> >> > I found 0.13.1 artifacts in maven:
> >> >> > >> >> >
> >> >> > >> >>
> >> >> > >>
> >> >> >
> >> >>
> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metasto
> >>re%7C0.13.1%7Cjar
> >> >> > >> >> >
> >> >> > >> >> > However, Spark uses groupId of org.spark-project.hive, no=
t
> >> >> > >> >> org.apache.hive
> >> >> > >> >> >
> >> >> > >> >> > Can someone tell me how it is supposed to work ?
> >> >> > >> >> >
> >> >> > >> >> > Cheers
> >> >> > >> >> >
> >> >> > >> >> >
> >> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> >> >> > snunez@hortonworks.com>
> >> >> > >> >> wrote:
> >> >> > >> >> >
> >> >> > >> >> >> I saw a note earlier, perhaps on the user list, that at
> >>least
> >> >> one
> >> >> > >> >> person is
> >> >> > >> >> >> using Hive 0.13. Anyone got a working build configuratio=
n
> >>for
> >> >> this
> >> >> > >> >> version
> >> >> > >> >> >> of Hive?
> >> >> > >> >> >>
> >> >> > >> >> >> Regards,
> >> >> > >> >> >> - Steve
> >> >> > >> >> >>
> >> >> > >> >> >>
> >> >> > >> >> >>
> >> >> > >> >> >> --
> >> >> > >> >> >> CONFIDENTIALITY NOTICE
> >> >> > >> >> >> NOTICE: This message is intended for the use of the
> >> individual
> >> >> or
> >> >> > >> >> entity to
> >> >> > >> >> >> which it is addressed and may contain information that i=
s
> >> >> > >> confidential,
> >> >> > >> >> >> privileged and exempt from disclosure under applicable
> >>law.
> >> If
> >> >> the
> >> >> > >> >> reader
> >> >> > >> >> >> of this message is not the intended recipient, you are
> >>hereby
> >> >> > >> notified
> >> >> > >> >> that
> >> >> > >> >> >> any printing, copying, dissemination, distribution,
> >> disclosure
> >> >> or
> >> >> > >> >> >> forwarding of this communication is strictly prohibited.
> >>If
> >> you
> >> >> > have
> >> >> > >> >> >> received this communication in error, please contact the
> >> sender
> >> >> > >> >> immediately
> >> >> > >> >> >> and delete it from your system. Thank You.
> >> >> > >> >> >>
> >> >> > >> >>
> >> >> > >>
> >> >> >
> >> >>
> >>
>
>
>
> --
> CONFIDENTIALITY NOTICE
> NOTICE: This message is intended for the use of the individual or entity =
to
> which it is addressed and may contain information that is confidential,
> privileged and exempt from disclosure under applicable law. If the reader
> of this message is not the intended recipient, you are hereby notified th=
at
> any printing, copying, dissemination, distribution, disclosure or
> forwarding of this communication is strictly prohibited. If you have
> received this communication in error, please contact the sender immediate=
ly
> and delete it from your system. Thank You.
>

--485b397dcf91cc754904ff48a0ae--

From dev-return-8611-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Mon Jul 28 23:06:34 2014
Return-Path: <dev-return-8611-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0FF9B11483
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 28 Jul 2014 23:06:34 +0000 (UTC)
Received: (qmail 78278 invoked by uid 500); 28 Jul 2014 23:06:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 78212 invoked by uid 500); 28 Jul 2014 23:06:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 78200 invoked by uid 99); 28 Jul 2014 23:06:32 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 23:06:32 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.50 as permitted sender)
Received: from [209.85.213.50] (HELO mail-yh0-f50.google.com) (209.85.213.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 28 Jul 2014 23:06:14 +0000
Received: by mail-yh0-f50.google.com with SMTP id v1so5300957yhn.23
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 16:05:49 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bvtdEVfg//7k31zPZWmlrbx3ZELpiV4a4EB0o1KbUGQ=;
        b=dCE0E2zLZ2t8iV3WBb2mvh3OCzcapzHau3XSRGRaSjcdOlCUxXyawX643kQlZOyHVp
         CgJU0McgAyldnEK2vZWMZnjRoDhw+lEVSsklwjxrjxV3qRQH6SPduRgZkmH4dUv8mg4y
         zeGrB5H/DFj2eXUTJDAztqND5vT2JUomkyFZB+MSmdSFntAk8pRVrDgaTBnw/Ex39UWI
         XnLCsXix648b/TGyijunTjwoD7NM/Ng73cXsa/w6eEQ/kRtmn+rAu+oxDvOamxW/WTCO
         lu3mnEUbJdhkno3QE9IwHTgkrGX5yRYWAmv9zbfxATPROaMpxiFQnU8EWnUnOUu/2+4Y
         4FvA==
MIME-Version: 1.0
X-Received: by 10.236.142.227 with SMTP id i63mr33808926yhj.88.1406588749448;
 Mon, 28 Jul 2014 16:05:49 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Mon, 28 Jul 2014 16:05:49 -0700 (PDT)
In-Reply-To: <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
	<CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
	<CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
	<CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
	<CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
	<CFFC00D1.2ED4%snunez@hortonworks.com>
	<CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
Date: Mon, 28 Jul 2014 16:05:49 -0700
Message-ID: <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf300e5293c18d3904ff48f68a
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf300e5293c18d3904ff48f68a
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I was looking for a class where reflection-related code should reside.

I found this but don't think it is the proper class for bridging
differences between hive 0.12 and 0.13.1:
sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.s=
cala

Cheers


On Mon, Jul 28, 2014 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> After manually copying hive 0.13.1 jars to local maven repo, I got the
> following errors when building spark-hive_2.10 module :
>
> [ERROR]
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveCon=
text.scala:182:
> type mismatch;
>  found   : String
>  required: Array[String]
> [ERROR]       val proc: CommandProcessor =3D
> CommandProcessorFactory.get(tokens(0), hiveconf)
> [ERROR]
>    ^
> [ERROR]
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet=
astoreCatalog.scala:60:
> value getAllPartitionsForPruner is not a member of org.apache.
>  hadoop.hive.ql.metadata.Hive
> [ERROR]         client.getAllPartitionsForPruner(table).toSeq
> [ERROR]                ^
> [ERROR]
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet=
astoreCatalog.scala:267:
> overloaded method constructor TableDesc with alternatives:
>   (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
> Class[_],x$3: java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDe=
sc
> <and>
>   ()org.apache.hadoop.hive.ql.plan.TableDesc
>  cannot be applied to (Class[org.apache.hadoop.hive.serde2.Deserializer],
> Class[(some other)?0(in value tableDesc)(in value tableDesc)], Class[?0(i=
n
> value tableDesc)(in   value tableDesc)], java.util.Properties)
> [ERROR]   val tableDesc =3D new TableDesc(
> [ERROR]                   ^
> [WARNING] Class org.antlr.runtime.tree.CommonTree not found - continuing
> with a stub.
> [WARNING] Class org.antlr.runtime.Token not found - continuing with a stu=
b.
> [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing with a
> stub.
> [ERROR]
>      while compiling:
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.=
scala
>         during phase: typer
>      library version: version 2.10.4
>     compiler version: version 2.10.4
>
> The above shows incompatible changes between 0.12 and 0.13.1
> e.g. the first error corresponds to the following method
> in CommandProcessorFactory :
>   public static CommandProcessor get(String[] cmd, HiveConf conf)
>
> Cheers
>
>
> On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com>
> wrote:
>
>> So, do we have a short-term fix until Hive 0.14 comes out? Perhaps addin=
g
>> the hive-exec jar to the spark-project repo? It doesn=C2=B9t look like t=
here=C2=B9s
>> a release date schedule for 0.14.
>>
>>
>>
>> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
>>
>> >Exactly, forgot to mention Hulu team also made changes to cope with tho=
se
>> >incompatibility issues, but they said that=C2=B9s relatively easy once =
the
>> >re-packaging work is done.
>> >
>> >
>> >On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.com>
>>
>> >wrote:
>> >
>> >> I've heard from Cloudera that there were hive internal changes betwee=
n
>> >> 0.12 and 0.13 that required code re-writing. Over time it might be
>> >> possible for us to integrate with hive using API's that are more
>> >> stable (this is the domain of Michael/Cheng/Yin more than me!). It
>> >> would be interesting to see what the Hulu folks did.
>> >>
>> >> - Patrick
>> >>
>> >> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com>
>> >> wrote:
>> >> > AFAIK, according a recent talk, Hulu team in China has built Spark
>> SQL
>> >> > against Hive 0.13 (or 0.13.1?) successfully. Basically they also
>> >> > re-packaged Hive 0.13 as what the Spark team did. The slides of the
>> >>talk
>> >> > hasn't been released yet though.
>> >> >
>> >> >
>> >> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com> wrote=
:
>> >> >
>> >> >> Owen helped me find this:
>> >> >> https://issues.apache.org/jira/browse/HIVE-7423
>> >> >>
>> >> >> I guess this means that for Hive 0.14, Spark should be able to
>> >>directly
>> >> >> pull in hive-exec-core.jar
>> >> >>
>> >> >> Cheers
>> >> >>
>> >> >>
>> >> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <
>> pwendell@gmail.com>
>> >> >> wrote:
>> >> >>
>> >> >> > It would be great if the hive team can fix that issue. If not,
>> >>we'll
>> >> >> > have to continue forking our own version of Hive to change the w=
ay
>> >>it
>> >> >> > publishes artifacts.
>> >> >> >
>> >> >> > - Patrick
>> >> >> >
>> >> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com>
>> >>wrote:
>> >> >> > > Talked with Owen offline. He confirmed that as of 0.13,
>> >>hive-exec is
>> >> >> > still
>> >> >> > > uber jar.
>> >> >> > >
>> >> >> > > Right now I am facing the following error building against Hiv=
e
>> >> 0.13.1
>> >> >> :
>> >> >> > >
>> >> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10: Cou=
ld
>> >>not
>> >> >> > > resolve dependencies for project
>> >> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
>> >>following
>> >> >> > > artifacts could not be resolved:
>> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
>> >> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
>> >> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to find
>> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
>> >> >> > > http://repo.maven.apache.org/maven2 was cached in the local
>> >> >> repository,
>> >> >> > > resolution will not be reattempted until the update interval o=
f
>> >> >> > maven-repo
>> >> >> > > has elapsed or updates are forced -> [Help 1]
>> >> >> > >
>> >> >> > > Some hint would be appreciated.
>> >> >> > >
>> >> >> > > Cheers
>> >> >> > >
>> >> >> > >
>> >> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <sowen@cloudera.com=
>
>> >> wrote:
>> >> >> > >
>> >> >> > >> Yes, it is published. As of previous versions, at least,
>> >>hive-exec
>> >> >> > >> included all of its dependencies *in its artifact*, making it
>> >> unusable
>> >> >> > >> as-is because it contained copies of dependencies that clash
>> >>with
>> >> >> > >> versions present in other artifacts, and can't be managed wit=
h
>> >> Maven
>> >> >> > >> mechanisms.
>> >> >> > >>
>> >> >> > >> I am not sure why hive-exec was not published normally, with
>> >>just
>> >> its
>> >> >> > >> own classes. That's why it was copied, into an artifact with
>> >>just
>> >> >> > >> hive-exec code.
>> >> >> > >>
>> >> >> > >> You could do the same thing for hive-exec 0.13.1.
>> >> >> > >> Or maybe someone knows that it's published more 'normally' no=
w.
>> >> >> > >> I don't think hive-metastore is related to this question?
>> >> >> > >>
>> >> >> > >> I am no expert on the Hive artifacts, just remembering what t=
he
>> >> issue
>> >> >> > >> was initially in case it helps you get to a similar solution.
>> >> >> > >>
>> >> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.com>
>> >> wrote:
>> >> >> > >> > hive-exec (as of 0.13.1) is published here:
>> >> >> > >> >
>> >> >> > >>
>> >> >> >
>> >> >>
>> >>
>> >>
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7=
C
>> >>0.13.1%7Cjar
>> >> >> > >> >
>> >> >> > >> > Should a JIRA be opened so that dependency on hive-metastor=
e
>> >>can
>> >> be
>> >> >> > >> > replaced by dependency on hive-exec ?
>> >> >> > >> >
>> >> >> > >> > Cheers
>> >> >> > >> >
>> >> >> > >> >
>> >> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
>> >><sowen@cloudera.com>
>> >> >> > wrote:
>> >> >> > >> >
>> >> >> > >> >> The reason for org.spark-project.hive is that Spark relies
>> on
>> >> >> > >> >> hive-exec, but the Hive project does not publish this
>> >>artifact
>> >> by
>> >> >> > >> >> itself, only with all its dependencies as an uber jar. May=
be
>> >> that's
>> >> >> > >> >> been improved. If so, you need to point at the new hive-ex=
ec
>> >>and
>> >> >> > >> >> perhaps sort out its dependencies manually in your build.
>> >> >> > >> >>
>> >> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <
>> yuzhihong@gmail.com>
>> >> >> wrote:
>> >> >> > >> >> > I found 0.13.1 artifacts in maven:
>> >> >> > >> >> >
>> >> >> > >> >>
>> >> >> > >>
>> >> >> >
>> >> >>
>> >>
>> >>
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metast=
o
>> >>re%7C0.13.1%7Cjar
>> >> >> > >> >> >
>> >> >> > >> >> > However, Spark uses groupId of org.spark-project.hive, n=
ot
>> >> >> > >> >> org.apache.hive
>> >> >> > >> >> >
>> >> >> > >> >> > Can someone tell me how it is supposed to work ?
>> >> >> > >> >> >
>> >> >> > >> >> > Cheers
>> >> >> > >> >> >
>> >> >> > >> >> >
>> >> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>> >> >> > snunez@hortonworks.com>
>> >> >> > >> >> wrote:
>> >> >> > >> >> >
>> >> >> > >> >> >> I saw a note earlier, perhaps on the user list, that at
>> >>least
>> >> >> one
>> >> >> > >> >> person is
>> >> >> > >> >> >> using Hive 0.13. Anyone got a working build configurati=
on
>> >>for
>> >> >> this
>> >> >> > >> >> version
>> >> >> > >> >> >> of Hive?
>> >> >> > >> >> >>
>> >> >> > >> >> >> Regards,
>> >> >> > >> >> >> - Steve
>> >> >> > >> >> >>
>> >> >> > >> >> >>
>> >> >> > >> >> >>
>> >> >> > >> >> >> --
>> >> >> > >> >> >> CONFIDENTIALITY NOTICE
>> >> >> > >> >> >> NOTICE: This message is intended for the use of the
>> >> individual
>> >> >> or
>> >> >> > >> >> entity to
>> >> >> > >> >> >> which it is addressed and may contain information that =
is
>> >> >> > >> confidential,
>> >> >> > >> >> >> privileged and exempt from disclosure under applicable
>> >>law.
>> >> If
>> >> >> the
>> >> >> > >> >> reader
>> >> >> > >> >> >> of this message is not the intended recipient, you are
>> >>hereby
>> >> >> > >> notified
>> >> >> > >> >> that
>> >> >> > >> >> >> any printing, copying, dissemination, distribution,
>> >> disclosure
>> >> >> or
>> >> >> > >> >> >> forwarding of this communication is strictly prohibited=
.
>> >>If
>> >> you
>> >> >> > have
>> >> >> > >> >> >> received this communication in error, please contact th=
e
>> >> sender
>> >> >> > >> >> immediately
>> >> >> > >> >> >> and delete it from your system. Thank You.
>> >> >> > >> >> >>
>> >> >> > >> >>
>> >> >> > >>
>> >> >> >
>> >> >>
>> >>
>>
>>
>>
>> --
>> CONFIDENTIALITY NOTICE
>> NOTICE: This message is intended for the use of the individual or entity
>> to
>> which it is addressed and may contain information that is confidential,
>> privileged and exempt from disclosure under applicable law. If the reade=
r
>> of this message is not the intended recipient, you are hereby notified
>> that
>> any printing, copying, dissemination, distribution, disclosure or
>> forwarding of this communication is strictly prohibited. If you have
>> received this communication in error, please contact the sender
>> immediately
>> and delete it from your system. Thank You.
>>
>
>

--20cf300e5293c18d3904ff48f68a--

From dev-return-8612-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 00:23:24 2014
Return-Path: <dev-return-8612-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 496C3116E0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 00:23:24 +0000 (UTC)
Received: (qmail 27244 invoked by uid 500); 29 Jul 2014 00:23:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27181 invoked by uid 500); 29 Jul 2014 00:23:23 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 27170 invoked by uid 99); 29 Jul 2014 00:23:23 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 00:23:23 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 00:23:05 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so8878020qaq.10
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 17:22:40 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=ooQtDGdEW5+1T6kd77Dngb2gUDGT6zqcaj7SYcWICgY=;
        b=SeVEAh1cMoUAC3/VlmBpnwye2k8R6FX7ycKYKzU4rFL1Nf49lkBhtV2MQe5RipOdkf
         +3JD51e6WAjenL9WfRj2+0hhh3daPn4idYePEdeYHaE3OcdQD0+nbvstyRAm4L3ZBhrd
         waEaPfmfEJUHCZY0BVoQLqzAUr/xZvJyYXX0WTJIGk+FWDFQP9tFucs8z3dVEnc7Mo0d
         GwCh6eZhlfGwZ/2562A7PlzOllQ3cYKlwgLDbsrY8i87lDSn9XnX1T+xu+E9ylo1ymlg
         w79DB+doQCpUEhiRB/cp3bD+syLuFwkzbB+xKCkQAlVRQG3xxdIMna/i1LGjBsQP4b+Q
         CtWA==
X-Gm-Message-State: ALoCoQltkjBgg5X/HJrtP9v9sxqgvmwPTLoHBsj0zbH74VcsjPf809Ugw6vSqZ5TYl86AQrPjyEU
X-Received: by 10.229.226.135 with SMTP id iw7mr3329056qcb.13.1406593360672;
 Mon, 28 Jul 2014 17:22:40 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.224.206.9 with HTTP; Mon, 28 Jul 2014 17:22:20 -0700 (PDT)
In-Reply-To: <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com> <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
 <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
 <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
 <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
 <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
 <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
 <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
 <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
 <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
 <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
 <CFFC00D1.2ED4%snunez@hortonworks.com> <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
 <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
From: Michael Armbrust <michael@databricks.com>
Date: Mon, 28 Jul 2014 17:22:20 -0700
Message-ID: <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1134aa829b4f9e04ff4a09d7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1134aa829b4f9e04ff4a09d7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

A few things:
 - When we upgrade to Hive 0.13.0, Patrick will likely republish the
hive-exec jar just as we did for 0.12.0
 - Since we have to tie into some pretty low level APIs it is unsurprising
that the code doesn't just compile out of the box against 0.13.0
 - ScalaReflection is for determining Schema from Scala classes, not
reflection based bridge code.  Either way its unclear to if there is any
reason to use reflection to support multiple versions, instead of just
upgrading to Hive 0.13.0

One question I have is, What is the goal of upgrading to hive 0.13.0?  Is
it purely because you are having problems connecting to newer metastores?
 Are there some features you are hoping for?  This will help me prioritize
this effort.

Michael


On Mon, Jul 28, 2014 at 4:05 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> I was looking for a class where reflection-related code should reside.
>
> I found this but don't think it is the proper class for bridging
> differences between hive 0.12 and 0.13.1:
>
> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection=
.scala
>
> Cheers
>
>
> On Mon, Jul 28, 2014 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>
> > After manually copying hive 0.13.1 jars to local maven repo, I got the
> > following errors when building spark-hive_2.10 module :
> >
> > [ERROR]
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveCon=
text.scala:182:
> > type mismatch;
> >  found   : String
> >  required: Array[String]
> > [ERROR]       val proc: CommandProcessor =3D
> > CommandProcessorFactory.get(tokens(0), hiveconf)
> > [ERROR]
> >    ^
> > [ERROR]
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet=
astoreCatalog.scala:60:
> > value getAllPartitionsForPruner is not a member of org.apache.
> >  hadoop.hive.ql.metadata.Hive
> > [ERROR]         client.getAllPartitionsForPruner(table).toSeq
> > [ERROR]                ^
> > [ERROR]
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet=
astoreCatalog.scala:267:
> > overloaded method constructor TableDesc with alternatives:
> >   (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
> > Class[_],x$3:
> java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
> > <and>
> >   ()org.apache.hadoop.hive.ql.plan.TableDesc
> >  cannot be applied to (Class[org.apache.hadoop.hive.serde2.Deserializer=
],
> > Class[(some other)?0(in value tableDesc)(in value tableDesc)],
> Class[?0(in
> > value tableDesc)(in   value tableDesc)], java.util.Properties)
> > [ERROR]   val tableDesc =3D new TableDesc(
> > [ERROR]                   ^
> > [WARNING] Class org.antlr.runtime.tree.CommonTree not found - continuin=
g
> > with a stub.
> > [WARNING] Class org.antlr.runtime.Token not found - continuing with a
> stub.
> > [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing with=
 a
> > stub.
> > [ERROR]
> >      while compiling:
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.=
scala
> >         during phase: typer
> >      library version: version 2.10.4
> >     compiler version: version 2.10.4
> >
> > The above shows incompatible changes between 0.12 and 0.13.1
> > e.g. the first error corresponds to the following method
> > in CommandProcessorFactory :
> >   public static CommandProcessor get(String[] cmd, HiveConf conf)
> >
> > Cheers
> >
> >
> > On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com>
> > wrote:
> >
> >> So, do we have a short-term fix until Hive 0.14 comes out? Perhaps
> adding
> >> the hive-exec jar to the spark-project repo? It doesn=C2=B9t look like
> there=C2=B9s
> >> a release date schedule for 0.14.
> >>
> >>
> >>
> >> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
> >>
> >> >Exactly, forgot to mention Hulu team also made changes to cope with
> those
> >> >incompatibility issues, but they said that=C2=B9s relatively easy onc=
e the
> >> >re-packaging work is done.
> >> >
> >> >
> >> >On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.com>
> >>
> >> >wrote:
> >> >
> >> >> I've heard from Cloudera that there were hive internal changes
> between
> >> >> 0.12 and 0.13 that required code re-writing. Over time it might be
> >> >> possible for us to integrate with hive using API's that are more
> >> >> stable (this is the domain of Michael/Cheng/Yin more than me!). It
> >> >> would be interesting to see what the Hulu folks did.
> >> >>
> >> >> - Patrick
> >> >>
> >> >> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <lian.cs.zju@gmail.com=
>
> >> >> wrote:
> >> >> > AFAIK, according a recent talk, Hulu team in China has built Spar=
k
> >> SQL
> >> >> > against Hive 0.13 (or 0.13.1?) successfully. Basically they also
> >> >> > re-packaged Hive 0.13 as what the Spark team did. The slides of t=
he
> >> >>talk
> >> >> > hasn't been released yet though.
> >> >> >
> >> >> >
> >> >> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com>
> wrote:
> >> >> >
> >> >> >> Owen helped me find this:
> >> >> >> https://issues.apache.org/jira/browse/HIVE-7423
> >> >> >>
> >> >> >> I guess this means that for Hive 0.14, Spark should be able to
> >> >>directly
> >> >> >> pull in hive-exec-core.jar
> >> >> >>
> >> >> >> Cheers
> >> >> >>
> >> >> >>
> >> >> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <
> >> pwendell@gmail.com>
> >> >> >> wrote:
> >> >> >>
> >> >> >> > It would be great if the hive team can fix that issue. If not,
> >> >>we'll
> >> >> >> > have to continue forking our own version of Hive to change the
> way
> >> >>it
> >> >> >> > publishes artifacts.
> >> >> >> >
> >> >> >> > - Patrick
> >> >> >> >
> >> >> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com>
> >> >>wrote:
> >> >> >> > > Talked with Owen offline. He confirmed that as of 0.13,
> >> >>hive-exec is
> >> >> >> > still
> >> >> >> > > uber jar.
> >> >> >> > >
> >> >> >> > > Right now I am facing the following error building against
> Hive
> >> >> 0.13.1
> >> >> >> :
> >> >> >> > >
> >> >> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10:
> Could
> >> >>not
> >> >> >> > > resolve dependencies for project
> >> >> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
> >> >>following
> >> >> >> > > artifacts could not be resolved:
> >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
> >> >> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
> >> >> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to fin=
d
> >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> >> >> >> > > http://repo.maven.apache.org/maven2 was cached in the local
> >> >> >> repository,
> >> >> >> > > resolution will not be reattempted until the update interval
> of
> >> >> >> > maven-repo
> >> >> >> > > has elapsed or updates are forced -> [Help 1]
> >> >> >> > >
> >> >> >> > > Some hint would be appreciated.
> >> >> >> > >
> >> >> >> > > Cheers
> >> >> >> > >
> >> >> >> > >
> >> >> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <
> sowen@cloudera.com>
> >> >> wrote:
> >> >> >> > >
> >> >> >> > >> Yes, it is published. As of previous versions, at least,
> >> >>hive-exec
> >> >> >> > >> included all of its dependencies *in its artifact*, making =
it
> >> >> unusable
> >> >> >> > >> as-is because it contained copies of dependencies that clas=
h
> >> >>with
> >> >> >> > >> versions present in other artifacts, and can't be managed
> with
> >> >> Maven
> >> >> >> > >> mechanisms.
> >> >> >> > >>
> >> >> >> > >> I am not sure why hive-exec was not published normally, wit=
h
> >> >>just
> >> >> its
> >> >> >> > >> own classes. That's why it was copied, into an artifact wit=
h
> >> >>just
> >> >> >> > >> hive-exec code.
> >> >> >> > >>
> >> >> >> > >> You could do the same thing for hive-exec 0.13.1.
> >> >> >> > >> Or maybe someone knows that it's published more 'normally'
> now.
> >> >> >> > >> I don't think hive-metastore is related to this question?
> >> >> >> > >>
> >> >> >> > >> I am no expert on the Hive artifacts, just remembering what
> the
> >> >> issue
> >> >> >> > >> was initially in case it helps you get to a similar solutio=
n.
> >> >> >> > >>
> >> >> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <yuzhihong@gmail.co=
m
> >
> >> >> wrote:
> >> >> >> > >> > hive-exec (as of 0.13.1) is published here:
> >> >> >> > >> >
> >> >> >> > >>
> >> >> >> >
> >> >> >>
> >> >>
> >> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C
> >> >>0.13.1%7Cjar
> >> >> >> > >> >
> >> >> >> > >> > Should a JIRA be opened so that dependency on
> hive-metastore
> >> >>can
> >> >> be
> >> >> >> > >> > replaced by dependency on hive-exec ?
> >> >> >> > >> >
> >> >> >> > >> > Cheers
> >> >> >> > >> >
> >> >> >> > >> >
> >> >> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
> >> >><sowen@cloudera.com>
> >> >> >> > wrote:
> >> >> >> > >> >
> >> >> >> > >> >> The reason for org.spark-project.hive is that Spark reli=
es
> >> on
> >> >> >> > >> >> hive-exec, but the Hive project does not publish this
> >> >>artifact
> >> >> by
> >> >> >> > >> >> itself, only with all its dependencies as an uber jar.
> Maybe
> >> >> that's
> >> >> >> > >> >> been improved. If so, you need to point at the new
> hive-exec
> >> >>and
> >> >> >> > >> >> perhaps sort out its dependencies manually in your build=
.
> >> >> >> > >> >>
> >> >> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <
> >> yuzhihong@gmail.com>
> >> >> >> wrote:
> >> >> >> > >> >> > I found 0.13.1 artifacts in maven:
> >> >> >> > >> >> >
> >> >> >> > >> >>
> >> >> >> > >>
> >> >> >> >
> >> >> >>
> >> >>
> >> >>
> >>
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metasto
> >> >>re%7C0.13.1%7Cjar
> >> >> >> > >> >> >
> >> >> >> > >> >> > However, Spark uses groupId of org.spark-project.hive,
> not
> >> >> >> > >> >> org.apache.hive
> >> >> >> > >> >> >
> >> >> >> > >> >> > Can someone tell me how it is supposed to work ?
> >> >> >> > >> >> >
> >> >> >> > >> >> > Cheers
> >> >> >> > >> >> >
> >> >> >> > >> >> >
> >> >> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> >> >> >> > snunez@hortonworks.com>
> >> >> >> > >> >> wrote:
> >> >> >> > >> >> >
> >> >> >> > >> >> >> I saw a note earlier, perhaps on the user list, that =
at
> >> >>least
> >> >> >> one
> >> >> >> > >> >> person is
> >> >> >> > >> >> >> using Hive 0.13. Anyone got a working build
> configuration
> >> >>for
> >> >> >> this
> >> >> >> > >> >> version
> >> >> >> > >> >> >> of Hive?
> >> >> >> > >> >> >>
> >> >> >> > >> >> >> Regards,
> >> >> >> > >> >> >> - Steve
> >> >> >> > >> >> >>
> >> >> >> > >> >> >>
> >> >> >> > >> >> >>
> >> >> >> > >> >> >> --
> >> >> >> > >> >> >> CONFIDENTIALITY NOTICE
> >> >> >> > >> >> >> NOTICE: This message is intended for the use of the
> >> >> individual
> >> >> >> or
> >> >> >> > >> >> entity to
> >> >> >> > >> >> >> which it is addressed and may contain information tha=
t
> is
> >> >> >> > >> confidential,
> >> >> >> > >> >> >> privileged and exempt from disclosure under applicabl=
e
> >> >>law.
> >> >> If
> >> >> >> the
> >> >> >> > >> >> reader
> >> >> >> > >> >> >> of this message is not the intended recipient, you ar=
e
> >> >>hereby
> >> >> >> > >> notified
> >> >> >> > >> >> that
> >> >> >> > >> >> >> any printing, copying, dissemination, distribution,
> >> >> disclosure
> >> >> >> or
> >> >> >> > >> >> >> forwarding of this communication is strictly
> prohibited.
> >> >>If
> >> >> you
> >> >> >> > have
> >> >> >> > >> >> >> received this communication in error, please contact
> the
> >> >> sender
> >> >> >> > >> >> immediately
> >> >> >> > >> >> >> and delete it from your system. Thank You.
> >> >> >> > >> >> >>
> >> >> >> > >> >>
> >> >> >> > >>
> >> >> >> >
> >> >> >>
> >> >>
> >>
> >>
> >>
> >> --
> >> CONFIDENTIALITY NOTICE
> >> NOTICE: This message is intended for the use of the individual or enti=
ty
> >> to
> >> which it is addressed and may contain information that is confidential=
,
> >> privileged and exempt from disclosure under applicable law. If the
> reader
> >> of this message is not the intended recipient, you are hereby notified
> >> that
> >> any printing, copying, dissemination, distribution, disclosure or
> >> forwarding of this communication is strictly prohibited. If you have
> >> received this communication in error, please contact the sender
> >> immediately
> >> and delete it from your system. Thank You.
> >>
> >
> >
>

--001a1134aa829b4f9e04ff4a09d7--

From dev-return-8613-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 00:28:18 2014
Return-Path: <dev-return-8613-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8DE5011708
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 00:28:18 +0000 (UTC)
Received: (qmail 34335 invoked by uid 500); 29 Jul 2014 00:28:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34287 invoked by uid 500); 29 Jul 2014 00:28:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34276 invoked by uid 99); 29 Jul 2014 00:28:17 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 00:28:17 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of snunez@hortonworks.com designates 209.85.192.181 as permitted sender)
Received: from [209.85.192.181] (HELO mail-pd0-f181.google.com) (209.85.192.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 00:28:00 +0000
Received: by mail-pd0-f181.google.com with SMTP id g10so10767122pdj.12
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 17:27:34 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:user-agent:date:subject:from:to:message-id
         :thread-topic:references:in-reply-to:mime-version:content-type
         :content-transfer-encoding;
        bh=EulJZY6dQ08f9coGiLyE0e4/GRGAepnqsc2Ij2Atu+Q=;
        b=dUh2+6lEHuzxrk0Jm+pSE3S7uQsPz+tiyBXActhAUD2wkNmXECly4QwjQ8jcQpUxwx
         VyFq0l1I9bcp0QjOsE131VdsM6ugBIBm9Aha1D4COkR1qOwRAvjzZ5AstZiSYcKJLZfz
         KdgF9RSe3Czpw5ysaK0uLJeV2B329mSn9etbo+ZhoL4VmLdGV0Xl+nFNVv1Cns6Wxzr0
         LLAAGipzuJDZTYrdu7PPF4/pjWXk5qBFpzrSM+2aJDaUFpPYhtHbwSSokQbjLbVm5OZC
         of2pkcfycLYfDc7bKv4nhwtvfJJFLF1oWf9ZMGJc0jszkmukqYKCItrndiHqC01zGNCT
         DCEA==
X-Gm-Message-State: ALoCoQnImsC9ICPjKEOGQsWt9CQb9x7PzEcCq3v4nEeXnUPfIvM7cUJUIw0pjUb9AJGZc5r5gXh/IdZsvzmWwFn8TSlzUlNG92TwVhFa16VlbajDHvixeHk=
X-Received: by 10.68.57.165 with SMTP id j5mr6600166pbq.147.1406593654771;
        Mon, 28 Jul 2014 17:27:34 -0700 (PDT)
Received: from [10.0.0.6] ([24.130.63.105])
        by mx.google.com with ESMTPSA id qw8sm71648590pab.17.2014.07.28.17.27.31
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=RC4-SHA bits=128/128);
        Mon, 28 Jul 2014 17:27:34 -0700 (PDT)
User-Agent: Microsoft-MacOutlook/14.4.3.140616
Date: Mon, 28 Jul 2014 17:27:28 -0700
Subject: Re: Working Formula for Hive 0.13?
From: Steve Nunez <snunez@hortonworks.com>
To: <dev@spark.apache.org>
Message-ID: <CFFC3805.2F31%snunez@hortonworks.com>
Thread-Topic: Working Formula for Hive 0.13?
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
 <CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
 <CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
 <CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
 <CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
 <CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
 <CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
 <CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
 <CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
 <CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
 <CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
 <CFFC00D1.2ED4%snunez@hortonworks.com>
 <CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
 <CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
 <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
In-Reply-To: <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
Mime-version: 1.0
Content-type: text/plain; charset=EUC-KR
Content-transfer-encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

The larger goal is to get a clean compile & test in the environment I have
to use. As near as I can tell, tests fail in parquet because parquet was
only added in Hive 0.13. There could well be issues in later meta-stores,
but one thing at a time...

	- SteveN



On 7/28/14, 17:22, "Michael Armbrust" <michael@databricks.com> wrote:

>A few things:
> - When we upgrade to Hive 0.13.0, Patrick will likely republish the
>hive-exec jar just as we did for 0.12.0
> - Since we have to tie into some pretty low level APIs it is unsurprising
>that the code doesn't just compile out of the box against 0.13.0
> - ScalaReflection is for determining Schema from Scala classes, not
>reflection based bridge code.  Either way its unclear to if there is any
>reason to use reflection to support multiple versions, instead of just
>upgrading to Hive 0.13.0
>
>One question I have is, What is the goal of upgrading to hive 0.13.0?  Is
>it purely because you are having problems connecting to newer metastores?
> Are there some features you are hoping for?  This will help me prioritize
>this effort.
>
>Michael
>
>
>On Mon, Jul 28, 2014 at 4:05 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>
>> I was looking for a class where reflection-related code should reside.
>>
>> I found this but don't think it is the proper class for bridging
>> differences between hive 0.12 and 0.13.1:
>>
>>=20
>>sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection
>>.scala
>>
>> Cheers
>>
>>
>> On Mon, Jul 28, 2014 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>
>> > After manually copying hive 0.13.1 jars to local maven repo, I got the
>> > following errors when building spark-hive_2.10 module :
>> >
>> > [ERROR]
>> >
>>=20
>>/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveCon
>>text.scala:182:
>> > type mismatch;
>> >  found   : String
>> >  required: Array[String]
>> > [ERROR]       val proc: CommandProcessor =3D
>> > CommandProcessorFactory.get(tokens(0), hiveconf)
>> > [ERROR]
>> >    ^
>> > [ERROR]
>> >
>>=20
>>/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet
>>astoreCatalog.scala:60:
>> > value getAllPartitionsForPruner is not a member of org.apache.
>> >  hadoop.hive.ql.metadata.Hive
>> > [ERROR]         client.getAllPartitionsForPruner(table).toSeq
>> > [ERROR]                ^
>> > [ERROR]
>> >
>>=20
>>/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet
>>astoreCatalog.scala:267:
>> > overloaded method constructor TableDesc with alternatives:
>> >   (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
>> > Class[_],x$3:
>> java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
>> > <and>
>> >   ()org.apache.hadoop.hive.ql.plan.TableDesc
>> >  cannot be applied to
>>(Class[org.apache.hadoop.hive.serde2.Deserializer],
>> > Class[(some other)?0(in value tableDesc)(in value tableDesc)],
>> Class[?0(in
>> > value tableDesc)(in   value tableDesc)], java.util.Properties)
>> > [ERROR]   val tableDesc =3D new TableDesc(
>> > [ERROR]                   ^
>> > [WARNING] Class org.antlr.runtime.tree.CommonTree not found -
>>continuing
>> > with a stub.
>> > [WARNING] Class org.antlr.runtime.Token not found - continuing with a
>> stub.
>> > [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing
>>with a
>> > stub.
>> > [ERROR]
>> >      while compiling:
>> >
>>=20
>>/homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.
>>scala
>> >         during phase: typer
>> >      library version: version 2.10.4
>> >     compiler version: version 2.10.4
>> >
>> > The above shows incompatible changes between 0.12 and 0.13.1
>> > e.g. the first error corresponds to the following method
>> > in CommandProcessorFactory :
>> >   public static CommandProcessor get(String[] cmd, HiveConf conf)
>> >
>> > Cheers
>> >
>> >
>> > On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com>
>> > wrote:
>> >
>> >> So, do we have a short-term fix until Hive 0.14 comes out? Perhaps
>> adding
>> >> the hive-exec jar to the spark-project repo? It doesn=A9=F6t look lik=
e
>> there=A9=F6s
>> >> a release date schedule for 0.14.
>> >>
>> >>
>> >>
>> >> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
>> >>
>> >> >Exactly, forgot to mention Hulu team also made changes to cope with
>> those
>> >> >incompatibility issues, but they said that=A9=F6s relatively easy on=
ce
>>the
>> >> >re-packaging work is done.
>> >> >
>> >> >
>> >> >On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell
>><pwendell@gmail.com>
>> >>
>> >> >wrote:
>> >> >
>> >> >> I've heard from Cloudera that there were hive internal changes
>> between
>> >> >> 0.12 and 0.13 that required code re-writing. Over time it might be
>> >> >> possible for us to integrate with hive using API's that are more
>> >> >> stable (this is the domain of Michael/Cheng/Yin more than me!). It
>> >> >> would be interesting to see what the Hulu folks did.
>> >> >>
>> >> >> - Patrick
>> >> >>
>> >> >> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian
>><lian.cs.zju@gmail.com>
>> >> >> wrote:
>> >> >> > AFAIK, according a recent talk, Hulu team in China has built
>>Spark
>> >> SQL
>> >> >> > against Hive 0.13 (or 0.13.1?) successfully. Basically they also
>> >> >> > re-packaged Hive 0.13 as what the Spark team did. The slides of
>>the
>> >> >>talk
>> >> >> > hasn't been released yet though.
>> >> >> >
>> >> >> >
>> >> >> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com>
>> wrote:
>> >> >> >
>> >> >> >> Owen helped me find this:
>> >> >> >> https://issues.apache.org/jira/browse/HIVE-7423
>> >> >> >>
>> >> >> >> I guess this means that for Hive 0.14, Spark should be able to
>> >> >>directly
>> >> >> >> pull in hive-exec-core.jar
>> >> >> >>
>> >> >> >> Cheers
>> >> >> >>
>> >> >> >>
>> >> >> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <
>> >> pwendell@gmail.com>
>> >> >> >> wrote:
>> >> >> >>
>> >> >> >> > It would be great if the hive team can fix that issue. If
>>not,
>> >> >>we'll
>> >> >> >> > have to continue forking our own version of Hive to change
>>the
>> way
>> >> >>it
>> >> >> >> > publishes artifacts.
>> >> >> >> >
>> >> >> >> > - Patrick
>> >> >> >> >
>> >> >> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com>
>> >> >>wrote:
>> >> >> >> > > Talked with Owen offline. He confirmed that as of 0.13,
>> >> >>hive-exec is
>> >> >> >> > still
>> >> >> >> > > uber jar.
>> >> >> >> > >
>> >> >> >> > > Right now I am facing the following error building against
>> Hive
>> >> >> 0.13.1
>> >> >> >> :
>> >> >> >> > >
>> >> >> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10:
>> Could
>> >> >>not
>> >> >> >> > > resolve dependencies for project
>> >> >> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
>> >> >>following
>> >> >> >> > > artifacts could not be resolved:
>> >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
>> >> >> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
>> >> >> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to
>>find
>> >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
>> >> >> >> > > http://repo.maven.apache.org/maven2 was cached in the local
>> >> >> >> repository,
>> >> >> >> > > resolution will not be reattempted until the update
>>interval
>> of
>> >> >> >> > maven-repo
>> >> >> >> > > has elapsed or updates are forced -> [Help 1]
>> >> >> >> > >
>> >> >> >> > > Some hint would be appreciated.
>> >> >> >> > >
>> >> >> >> > > Cheers
>> >> >> >> > >
>> >> >> >> > >
>> >> >> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <
>> sowen@cloudera.com>
>> >> >> wrote:
>> >> >> >> > >
>> >> >> >> > >> Yes, it is published. As of previous versions, at least,
>> >> >>hive-exec
>> >> >> >> > >> included all of its dependencies *in its artifact*,
>>making it
>> >> >> unusable
>> >> >> >> > >> as-is because it contained copies of dependencies that
>>clash
>> >> >>with
>> >> >> >> > >> versions present in other artifacts, and can't be managed
>> with
>> >> >> Maven
>> >> >> >> > >> mechanisms.
>> >> >> >> > >>
>> >> >> >> > >> I am not sure why hive-exec was not published normally,
>>with
>> >> >>just
>> >> >> its
>> >> >> >> > >> own classes. That's why it was copied, into an artifact
>>with
>> >> >>just
>> >> >> >> > >> hive-exec code.
>> >> >> >> > >>
>> >> >> >> > >> You could do the same thing for hive-exec 0.13.1.
>> >> >> >> > >> Or maybe someone knows that it's published more 'normally'
>> now.
>> >> >> >> > >> I don't think hive-metastore is related to this question?
>> >> >> >> > >>
>> >> >> >> > >> I am no expert on the Hive artifacts, just remembering
>>what
>> the
>> >> >> issue
>> >> >> >> > >> was initially in case it helps you get to a similar
>>solution.
>> >> >> >> > >>
>> >> >> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu
>><yuzhihong@gmail.com
>> >
>> >> >> wrote:
>> >> >> >> > >> > hive-exec (as of 0.13.1) is published here:
>> >> >> >> > >> >
>> >> >> >> > >>
>> >> >> >> >
>> >> >> >>
>> >> >>
>> >> >>
>> >>
>>=20
>>http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C
>> >> >>0.13.1%7Cjar
>> >> >> >> > >> >
>> >> >> >> > >> > Should a JIRA be opened so that dependency on
>> hive-metastore
>> >> >>can
>> >> >> be
>> >> >> >> > >> > replaced by dependency on hive-exec ?
>> >> >> >> > >> >
>> >> >> >> > >> > Cheers
>> >> >> >> > >> >
>> >> >> >> > >> >
>> >> >> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
>> >> >><sowen@cloudera.com>
>> >> >> >> > wrote:
>> >> >> >> > >> >
>> >> >> >> > >> >> The reason for org.spark-project.hive is that Spark
>>relies
>> >> on
>> >> >> >> > >> >> hive-exec, but the Hive project does not publish this
>> >> >>artifact
>> >> >> by
>> >> >> >> > >> >> itself, only with all its dependencies as an uber jar.
>> Maybe
>> >> >> that's
>> >> >> >> > >> >> been improved. If so, you need to point at the new
>> hive-exec
>> >> >>and
>> >> >> >> > >> >> perhaps sort out its dependencies manually in your
>>build.
>> >> >> >> > >> >>
>> >> >> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <
>> >> yuzhihong@gmail.com>
>> >> >> >> wrote:
>> >> >> >> > >> >> > I found 0.13.1 artifacts in maven:
>> >> >> >> > >> >> >
>> >> >> >> > >> >>
>> >> >> >> > >>
>> >> >> >> >
>> >> >> >>
>> >> >>
>> >> >>
>> >>
>>=20
>>http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metasto
>> >> >>re%7C0.13.1%7Cjar
>> >> >> >> > >> >> >
>> >> >> >> > >> >> > However, Spark uses groupId of
>>org.spark-project.hive,
>> not
>> >> >> >> > >> >> org.apache.hive
>> >> >> >> > >> >> >
>> >> >> >> > >> >> > Can someone tell me how it is supposed to work ?
>> >> >> >> > >> >> >
>> >> >> >> > >> >> > Cheers
>> >> >> >> > >> >> >
>> >> >> >> > >> >> >
>> >> >> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>> >> >> >> > snunez@hortonworks.com>
>> >> >> >> > >> >> wrote:
>> >> >> >> > >> >> >
>> >> >> >> > >> >> >> I saw a note earlier, perhaps on the user list,
>>that at
>> >> >>least
>> >> >> >> one
>> >> >> >> > >> >> person is
>> >> >> >> > >> >> >> using Hive 0.13. Anyone got a working build
>> configuration
>> >> >>for
>> >> >> >> this
>> >> >> >> > >> >> version
>> >> >> >> > >> >> >> of Hive?
>> >> >> >> > >> >> >>
>> >> >> >> > >> >> >> Regards,
>> >> >> >> > >> >> >> - Steve
>> >> >> >> > >> >> >>
>> >> >> >> > >> >> >>
>> >> >> >> > >> >> >>
>> >> >> >> > >> >> >> --
>> >> >> >> > >> >> >> CONFIDENTIALITY NOTICE
>> >> >> >> > >> >> >> NOTICE: This message is intended for the use of the
>> >> >> individual
>> >> >> >> or
>> >> >> >> > >> >> entity to
>> >> >> >> > >> >> >> which it is addressed and may contain information
>>that
>> is
>> >> >> >> > >> confidential,
>> >> >> >> > >> >> >> privileged and exempt from disclosure under
>>applicable
>> >> >>law.
>> >> >> If
>> >> >> >> the
>> >> >> >> > >> >> reader
>> >> >> >> > >> >> >> of this message is not the intended recipient, you
>>are
>> >> >>hereby
>> >> >> >> > >> notified
>> >> >> >> > >> >> that
>> >> >> >> > >> >> >> any printing, copying, dissemination, distribution,
>> >> >> disclosure
>> >> >> >> or
>> >> >> >> > >> >> >> forwarding of this communication is strictly
>> prohibited.
>> >> >>If
>> >> >> you
>> >> >> >> > have
>> >> >> >> > >> >> >> received this communication in error, please contact
>> the
>> >> >> sender
>> >> >> >> > >> >> immediately
>> >> >> >> > >> >> >> and delete it from your system. Thank You.
>> >> >> >> > >> >> >>
>> >> >> >> > >> >>
>> >> >> >> > >>
>> >> >> >> >
>> >> >> >>
>> >> >>
>> >>
>> >>
>> >>
>> >> --
>> >> CONFIDENTIALITY NOTICE
>> >> NOTICE: This message is intended for the use of the individual or
>>entity
>> >> to
>> >> which it is addressed and may contain information that is
>>confidential,
>> >> privileged and exempt from disclosure under applicable law. If the
>> reader
>> >> of this message is not the intended recipient, you are hereby
>>notified
>> >> that
>> >> any printing, copying, dissemination, distribution, disclosure or
>> >> forwarding of this communication is strictly prohibited. If you have
>> >> received this communication in error, please contact the sender
>> >> immediately
>> >> and delete it from your system. Thank You.
>> >>
>> >
>> >
>>



--=20
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to=
=20
which it is addressed and may contain information that is confidential,=20
privileged and exempt from disclosure under applicable law. If the reader=
=20
of this message is not the intended recipient, you are hereby notified that=
=20
any printing, copying, dissemination, distribution, disclosure or=20
forwarding of this communication is strictly prohibited. If you have=20
received this communication in error, please contact the sender immediately=
=20
and delete it from your system. Thank You.

From dev-return-8614-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 00:42:01 2014
Return-Path: <dev-return-8614-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 68CCE1177D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 00:42:01 +0000 (UTC)
Received: (qmail 63791 invoked by uid 500); 29 Jul 2014 00:42:01 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63735 invoked by uid 500); 29 Jul 2014 00:42:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 63716 invoked by uid 99); 29 Jul 2014 00:42:00 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 00:42:00 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.45 as permitted sender)
Received: from [209.85.219.45] (HELO mail-oa0-f45.google.com) (209.85.219.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 00:41:42 +0000
Received: by mail-oa0-f45.google.com with SMTP id i7so9637601oag.18
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 17:41:17 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=U649JiQPzd72pDEmawdA7lWvjGbG2YO/hJomkIgo4eo=;
        b=wkfGOPJFvBo+fXqQCJnw2wjKh/6HnSX/GgK5I098pp5II4kEzuUTriw12+rmuzDXsP
         19u4mgFh4FO9C4fX6i3CS+TBg2IHo7WljXiQiE56B1rzLVPBDfPCoUuRjdqQhYNvhu/j
         f2d826hruV03e35dVREHDHcz6mBR7VEx6ZPMM2ijt24LeqsC5L1AEB55SjLupZsR/TIf
         mDgdFrO/OeWDW0jMJ8qOY2dnYqgsBBTQmvZVjolcBrVEei4oNPF0XZOUtfTU4I1h90AZ
         8FkGVEefITHngP7jXDmlwl/J6NN6v/lZi9tCbq3xUzwbZzhsVKAkiWWVvLJco5zMTZs2
         hGgQ==
MIME-Version: 1.0
X-Received: by 10.182.98.194 with SMTP id ek2mr53550118obb.5.1406594477737;
 Mon, 28 Jul 2014 17:41:17 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Mon, 28 Jul 2014 17:41:17 -0700 (PDT)
Date: Mon, 28 Jul 2014 17:41:17 -0700
Message-ID: <CABPQxsu1AdYU=P2p9GTUHz-jD4LepVmvsF5ZBtZV0hmYT6gMSg@mail.gmail.com>
Subject: Github mirroring is running behind
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

https://issues.apache.org/jira/browse/INFRA-8116

Just a heads up, the github mirroring is running behind. You can
follow that JIRA to keep up to date on the fix.

In the mean time you can use the Apache git itself:

https://git-wip-us.apache.org/repos/asf/spark.git

Some people have reported issues checking out Apache git as well, but
it might work.

- Patrick

From dev-return-8615-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 01:00:12 2014
Return-Path: <dev-return-8615-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E5E1A11815
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 01:00:12 +0000 (UTC)
Received: (qmail 6057 invoked by uid 500); 29 Jul 2014 01:00:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5999 invoked by uid 500); 29 Jul 2014 01:00:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5982 invoked by uid 99); 29 Jul 2014 01:00:11 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 01:00:11 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.179 as permitted sender)
Received: from [209.85.160.179] (HELO mail-yk0-f179.google.com) (209.85.160.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 00:59:54 +0000
Received: by mail-yk0-f179.google.com with SMTP id 142so5081998ykq.24
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 17:59:29 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=bA6h5yyCs4EtkgosmXwbfACHFIlnz5rSRizz7u/ceTU=;
        b=OSr+yFWIdO3wPnEI8S5Uanndbq/zIOcd7Dox5b25xHWnXTpWJr+XIMFH03tDDknTJ/
         /Doxv/9deYJ1ghwSQEv0vwnjMgVyqBcPdgDe6l4k66n4xxiI8NhaXzRaT2CnES29xQ2Z
         7hwhHDHiGOVcJLo1DLY1sUqA7eLDrQMKCzT6XgVECBS/j8Zrezrx8sE3AH2RD0rPsfrQ
         PUx4TtsaqqEtIX7LtrTjtV7SocRMgGvqh8AgphW1iy2WV6unJw+xsCbOFWUHbYuazDXV
         k+ROv3BHoyhcjxCH4IL8b9/O60L+r1Umpq265Qi4vTLmuNuSNo6SxUVXWZYplqcsOijz
         iHwg==
MIME-Version: 1.0
X-Received: by 10.236.93.2 with SMTP id k2mr105076yhf.184.1406595569255; Mon,
 28 Jul 2014 17:59:29 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Mon, 28 Jul 2014 17:59:29 -0700 (PDT)
In-Reply-To: <CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
	<CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
	<CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
	<CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
	<CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
	<CFFC00D1.2ED4%snunez@hortonworks.com>
	<CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
	<CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
	<CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
Date: Mon, 28 Jul 2014 17:59:29 -0700
Message-ID: <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf301b5eab3fa69704ff4a8d1d
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf301b5eab3fa69704ff4a8d1d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

bq. Either way its unclear to if there is any reason to use reflection to
support multiple versions, instead of just upgrading to Hive 0.13.0

Which Spark release would this Hive upgrade take place ?
I agree it is cleaner to upgrade Hive dependency vs. introducing reflection=
.

Cheers


On Mon, Jul 28, 2014 at 5:22 PM, Michael Armbrust <michael@databricks.com>
wrote:

> A few things:
>  - When we upgrade to Hive 0.13.0, Patrick will likely republish the
> hive-exec jar just as we did for 0.12.0
>  - Since we have to tie into some pretty low level APIs it is unsurprisin=
g
> that the code doesn't just compile out of the box against 0.13.0
>  - ScalaReflection is for determining Schema from Scala classes, not
> reflection based bridge code.  Either way its unclear to if there is any
> reason to use reflection to support multiple versions, instead of just
> upgrading to Hive 0.13.0
>
> One question I have is, What is the goal of upgrading to hive 0.13.0?  Is
> it purely because you are having problems connecting to newer metastores?
>  Are there some features you are hoping for?  This will help me prioritiz=
e
> this effort.
>
> Michael
>
>
> On Mon, Jul 28, 2014 at 4:05 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>
> > I was looking for a class where reflection-related code should reside.
> >
> > I found this but don't think it is the proper class for bridging
> > differences between hive 0.12 and 0.13.1:
> >
> >
> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection=
.scala
> >
> > Cheers
> >
> >
> > On Mon, Jul 28, 2014 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:
> >
> > > After manually copying hive 0.13.1 jars to local maven repo, I got th=
e
> > > following errors when building spark-hive_2.10 module :
> > >
> > > [ERROR]
> > >
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveCon=
text.scala:182:
> > > type mismatch;
> > >  found   : String
> > >  required: Array[String]
> > > [ERROR]       val proc: CommandProcessor =3D
> > > CommandProcessorFactory.get(tokens(0), hiveconf)
> > > [ERROR]
> > >    ^
> > > [ERROR]
> > >
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet=
astoreCatalog.scala:60:
> > > value getAllPartitionsForPruner is not a member of org.apache.
> > >  hadoop.hive.ql.metadata.Hive
> > > [ERROR]         client.getAllPartitionsForPruner(table).toSeq
> > > [ERROR]                ^
> > > [ERROR]
> > >
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMet=
astoreCatalog.scala:267:
> > > overloaded method constructor TableDesc with alternatives:
> > >   (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
> > > Class[_],x$3:
> > java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
> > > <and>
> > >   ()org.apache.hadoop.hive.ql.plan.TableDesc
> > >  cannot be applied to
> (Class[org.apache.hadoop.hive.serde2.Deserializer],
> > > Class[(some other)?0(in value tableDesc)(in value tableDesc)],
> > Class[?0(in
> > > value tableDesc)(in   value tableDesc)], java.util.Properties)
> > > [ERROR]   val tableDesc =3D new TableDesc(
> > > [ERROR]                   ^
> > > [WARNING] Class org.antlr.runtime.tree.CommonTree not found -
> continuing
> > > with a stub.
> > > [WARNING] Class org.antlr.runtime.Token not found - continuing with a
> > stub.
> > > [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing
> with a
> > > stub.
> > > [ERROR]
> > >      while compiling:
> > >
> >
> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.=
scala
> > >         during phase: typer
> > >      library version: version 2.10.4
> > >     compiler version: version 2.10.4
> > >
> > > The above shows incompatible changes between 0.12 and 0.13.1
> > > e.g. the first error corresponds to the following method
> > > in CommandProcessorFactory :
> > >   public static CommandProcessor get(String[] cmd, HiveConf conf)
> > >
> > > Cheers
> > >
> > >
> > > On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com>
> > > wrote:
> > >
> > >> So, do we have a short-term fix until Hive 0.14 comes out? Perhaps
> > adding
> > >> the hive-exec jar to the spark-project repo? It doesn=C2=B9t look li=
ke
> > there=C2=B9s
> > >> a release date schedule for 0.14.
> > >>
> > >>
> > >>
> > >> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
> > >>
> > >> >Exactly, forgot to mention Hulu team also made changes to cope with
> > those
> > >> >incompatibility issues, but they said that=C2=B9s relatively easy o=
nce the
> > >> >re-packaging work is done.
> > >> >
> > >> >
> > >> >On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <pwendell@gmail.co=
m
> >
> > >>
> > >> >wrote:
> > >> >
> > >> >> I've heard from Cloudera that there were hive internal changes
> > between
> > >> >> 0.12 and 0.13 that required code re-writing. Over time it might b=
e
> > >> >> possible for us to integrate with hive using API's that are more
> > >> >> stable (this is the domain of Michael/Cheng/Yin more than me!). I=
t
> > >> >> would be interesting to see what the Hulu folks did.
> > >> >>
> > >> >> - Patrick
> > >> >>
> > >> >> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <
> lian.cs.zju@gmail.com>
> > >> >> wrote:
> > >> >> > AFAIK, according a recent talk, Hulu team in China has built
> Spark
> > >> SQL
> > >> >> > against Hive 0.13 (or 0.13.1?) successfully. Basically they als=
o
> > >> >> > re-packaged Hive 0.13 as what the Spark team did. The slides of
> the
> > >> >>talk
> > >> >> > hasn't been released yet though.
> > >> >> >
> > >> >> >
> > >> >> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com>
> > wrote:
> > >> >> >
> > >> >> >> Owen helped me find this:
> > >> >> >> https://issues.apache.org/jira/browse/HIVE-7423
> > >> >> >>
> > >> >> >> I guess this means that for Hive 0.14, Spark should be able to
> > >> >>directly
> > >> >> >> pull in hive-exec-core.jar
> > >> >> >>
> > >> >> >> Cheers
> > >> >> >>
> > >> >> >>
> > >> >> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <
> > >> pwendell@gmail.com>
> > >> >> >> wrote:
> > >> >> >>
> > >> >> >> > It would be great if the hive team can fix that issue. If no=
t,
> > >> >>we'll
> > >> >> >> > have to continue forking our own version of Hive to change t=
he
> > way
> > >> >>it
> > >> >> >> > publishes artifacts.
> > >> >> >> >
> > >> >> >> > - Patrick
> > >> >> >> >
> > >> >> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.com=
>
> > >> >>wrote:
> > >> >> >> > > Talked with Owen offline. He confirmed that as of 0.13,
> > >> >>hive-exec is
> > >> >> >> > still
> > >> >> >> > > uber jar.
> > >> >> >> > >
> > >> >> >> > > Right now I am facing the following error building against
> > Hive
> > >> >> 0.13.1
> > >> >> >> :
> > >> >> >> > >
> > >> >> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10:
> > Could
> > >> >>not
> > >> >> >> > > resolve dependencies for project
> > >> >> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
> > >> >>following
> > >> >> >> > > artifacts could not be resolved:
> > >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
> > >> >> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
> > >> >> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to
> find
> > >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
> > >> >> >> > > http://repo.maven.apache.org/maven2 was cached in the loca=
l
> > >> >> >> repository,
> > >> >> >> > > resolution will not be reattempted until the update interv=
al
> > of
> > >> >> >> > maven-repo
> > >> >> >> > > has elapsed or updates are forced -> [Help 1]
> > >> >> >> > >
> > >> >> >> > > Some hint would be appreciated.
> > >> >> >> > >
> > >> >> >> > > Cheers
> > >> >> >> > >
> > >> >> >> > >
> > >> >> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <
> > sowen@cloudera.com>
> > >> >> wrote:
> > >> >> >> > >
> > >> >> >> > >> Yes, it is published. As of previous versions, at least,
> > >> >>hive-exec
> > >> >> >> > >> included all of its dependencies *in its artifact*, makin=
g
> it
> > >> >> unusable
> > >> >> >> > >> as-is because it contained copies of dependencies that
> clash
> > >> >>with
> > >> >> >> > >> versions present in other artifacts, and can't be managed
> > with
> > >> >> Maven
> > >> >> >> > >> mechanisms.
> > >> >> >> > >>
> > >> >> >> > >> I am not sure why hive-exec was not published normally,
> with
> > >> >>just
> > >> >> its
> > >> >> >> > >> own classes. That's why it was copied, into an artifact
> with
> > >> >>just
> > >> >> >> > >> hive-exec code.
> > >> >> >> > >>
> > >> >> >> > >> You could do the same thing for hive-exec 0.13.1.
> > >> >> >> > >> Or maybe someone knows that it's published more 'normally=
'
> > now.
> > >> >> >> > >> I don't think hive-metastore is related to this question?
> > >> >> >> > >>
> > >> >> >> > >> I am no expert on the Hive artifacts, just remembering wh=
at
> > the
> > >> >> issue
> > >> >> >> > >> was initially in case it helps you get to a similar
> solution.
> > >> >> >> > >>
> > >> >> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <
> yuzhihong@gmail.com
> > >
> > >> >> wrote:
> > >> >> >> > >> > hive-exec (as of 0.13.1) is published here:
> > >> >> >> > >> >
> > >> >> >> > >>
> > >> >> >> >
> > >> >> >>
> > >> >>
> > >> >>
> > >>
> >
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7C
> > >> >>0.13.1%7Cjar
> > >> >> >> > >> >
> > >> >> >> > >> > Should a JIRA be opened so that dependency on
> > hive-metastore
> > >> >>can
> > >> >> be
> > >> >> >> > >> > replaced by dependency on hive-exec ?
> > >> >> >> > >> >
> > >> >> >> > >> > Cheers
> > >> >> >> > >> >
> > >> >> >> > >> >
> > >> >> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
> > >> >><sowen@cloudera.com>
> > >> >> >> > wrote:
> > >> >> >> > >> >
> > >> >> >> > >> >> The reason for org.spark-project.hive is that Spark
> relies
> > >> on
> > >> >> >> > >> >> hive-exec, but the Hive project does not publish this
> > >> >>artifact
> > >> >> by
> > >> >> >> > >> >> itself, only with all its dependencies as an uber jar.
> > Maybe
> > >> >> that's
> > >> >> >> > >> >> been improved. If so, you need to point at the new
> > hive-exec
> > >> >>and
> > >> >> >> > >> >> perhaps sort out its dependencies manually in your
> build.
> > >> >> >> > >> >>
> > >> >> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <
> > >> yuzhihong@gmail.com>
> > >> >> >> wrote:
> > >> >> >> > >> >> > I found 0.13.1 artifacts in maven:
> > >> >> >> > >> >> >
> > >> >> >> > >> >>
> > >> >> >> > >>
> > >> >> >> >
> > >> >> >>
> > >> >>
> > >> >>
> > >>
> >
> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metasto
> > >> >>re%7C0.13.1%7Cjar
> > >> >> >> > >> >> >
> > >> >> >> > >> >> > However, Spark uses groupId of org.spark-project.hiv=
e,
> > not
> > >> >> >> > >> >> org.apache.hive
> > >> >> >> > >> >> >
> > >> >> >> > >> >> > Can someone tell me how it is supposed to work ?
> > >> >> >> > >> >> >
> > >> >> >> > >> >> > Cheers
> > >> >> >> > >> >> >
> > >> >> >> > >> >> >
> > >> >> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
> > >> >> >> > snunez@hortonworks.com>
> > >> >> >> > >> >> wrote:
> > >> >> >> > >> >> >
> > >> >> >> > >> >> >> I saw a note earlier, perhaps on the user list, tha=
t
> at
> > >> >>least
> > >> >> >> one
> > >> >> >> > >> >> person is
> > >> >> >> > >> >> >> using Hive 0.13. Anyone got a working build
> > configuration
> > >> >>for
> > >> >> >> this
> > >> >> >> > >> >> version
> > >> >> >> > >> >> >> of Hive?
> > >> >> >> > >> >> >>
> > >> >> >> > >> >> >> Regards,
> > >> >> >> > >> >> >> - Steve
> > >> >> >> > >> >> >>
> > >> >> >> > >> >> >>
> > >> >> >> > >> >> >>
> > >> >> >> > >> >> >> --
> > >> >> >> > >> >> >> CONFIDENTIALITY NOTICE
> > >> >> >> > >> >> >> NOTICE: This message is intended for the use of the
> > >> >> individual
> > >> >> >> or
> > >> >> >> > >> >> entity to
> > >> >> >> > >> >> >> which it is addressed and may contain information
> that
> > is
> > >> >> >> > >> confidential,
> > >> >> >> > >> >> >> privileged and exempt from disclosure under
> applicable
> > >> >>law.
> > >> >> If
> > >> >> >> the
> > >> >> >> > >> >> reader
> > >> >> >> > >> >> >> of this message is not the intended recipient, you
> are
> > >> >>hereby
> > >> >> >> > >> notified
> > >> >> >> > >> >> that
> > >> >> >> > >> >> >> any printing, copying, dissemination, distribution,
> > >> >> disclosure
> > >> >> >> or
> > >> >> >> > >> >> >> forwarding of this communication is strictly
> > prohibited.
> > >> >>If
> > >> >> you
> > >> >> >> > have
> > >> >> >> > >> >> >> received this communication in error, please contac=
t
> > the
> > >> >> sender
> > >> >> >> > >> >> immediately
> > >> >> >> > >> >> >> and delete it from your system. Thank You.
> > >> >> >> > >> >> >>
> > >> >> >> > >> >>
> > >> >> >> > >>
> > >> >> >> >
> > >> >> >>
> > >> >>
> > >>
> > >>
> > >>
> > >> --
> > >> CONFIDENTIALITY NOTICE
> > >> NOTICE: This message is intended for the use of the individual or
> entity
> > >> to
> > >> which it is addressed and may contain information that is
> confidential,
> > >> privileged and exempt from disclosure under applicable law. If the
> > reader
> > >> of this message is not the intended recipient, you are hereby notifi=
ed
> > >> that
> > >> any printing, copying, dissemination, distribution, disclosure or
> > >> forwarding of this communication is strictly prohibited. If you have
> > >> received this communication in error, please contact the sender
> > >> immediately
> > >> and delete it from your system. Thank You.
> > >>
> > >
> > >
> >
>

--20cf301b5eab3fa69704ff4a8d1d--

From dev-return-8616-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 01:52:41 2014
Return-Path: <dev-return-8616-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8B99D1196A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 01:52:41 +0000 (UTC)
Received: (qmail 89467 invoked by uid 500); 29 Jul 2014 01:52:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 89401 invoked by uid 500); 29 Jul 2014 01:52:39 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 89355 invoked by uid 99); 29 Jul 2014 01:52:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 01:52:39 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.220.41] (HELO mail-pa0-f41.google.com) (209.85.220.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 01:52:34 +0000
Received: by mail-pa0-f41.google.com with SMTP id rd3so11506856pab.14
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 18:52:13 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=VkJITq2u4n10ZSOM2L/tkpBnGxhAlawE904dvJ+6RsU=;
        b=UYR0akDnwCkgzBMlXhDJkwC4vGLUVgviK5T4u74S/7NpCj9vBNp/0Qv9W0N7G/tXd4
         JeCMXjq2waRGLxwWxgofqBhU5Zz81tvAO/rrdbF0OfO/fbf7RSzzpe9iB11wjo0qwBpl
         qguistTlWQyc6ZrnotWvclCoCCdsAXfoivkhvdiaX9JOGVweoUjQxOVlyP5tvNQYQdqi
         dSxnIgZIlGpat+N2G7w2w71up2NJtLVbjz57C4t6kgOMqhs9Qxee0I7KZrdNVGQ1oTVS
         GKrqErcqCGFGFAYrBJENJp/UvsnzDxtqFORGScwLLgOZD1L2fb6gKzQ3Zl9Ed1r52MkQ
         x5iw==
X-Gm-Message-State: ALoCoQlV4fLtJgEHbCHOpvZoi5jlTRe9orLjQx6JNQ7jlQxDycwppqLVJsDAtmQniYYwVLTbZYvi
MIME-Version: 1.0
X-Received: by 10.70.65.100 with SMTP id w4mr7894190pds.128.1406598733174;
 Mon, 28 Jul 2014 18:52:13 -0700 (PDT)
Received: by 10.70.4.133 with HTTP; Mon, 28 Jul 2014 18:52:13 -0700 (PDT)
In-Reply-To: <CAMwrk0m3saqjbX8MPtN_=fEdjiLv8ev+QR4jwFoEgXh_11Nqag@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<71937D1E-0EC2-413E-A447-D60B49D16631@gmail.com>
	<CAMwrk0m3saqjbX8MPtN_=fEdjiLv8ev+QR4jwFoEgXh_11Nqag@mail.gmail.com>
Date: Mon, 28 Jul 2014 18:52:13 -0700
Message-ID: <CAMJOb8mDdoOb9i5sBWUte6U=J8eyEYYM6i3g5z8yM7XtbobHGA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Andrew Or <andrew@databricks.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e0160d14cd532a704ff4b4913
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160d14cd532a704ff4b4913
Content-Type: text/plain; charset=UTF-8

+1 Tested on standalone and yarn clusters


2014-07-28 14:59 GMT-07:00 Tathagata Das <tathagata.das1565@gmail.com>:

> Let me add my vote as well.
> Did some basic tests by running simple projects with various Spark
> modules. Tested checksums.
>
> +1
>
> On Sun, Jul 27, 2014 at 4:52 PM, Matei Zaharia <matei.zaharia@gmail.com>
> wrote:
> > +1
> >
> > Tested this on Mac OS X.
> >
> > Matei
> >
> > On Jul 25, 2014, at 4:08 PM, Tathagata Das <tathagata.das1565@gmail.com>
> wrote:
> >
> >> Please vote on releasing the following candidate as Apache Spark
> version 1.0.2.
> >>
> >> This release fixes a number of bugs in Spark 1.0.1.
> >> Some of the notable ones are
> >> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> >> SPARK-1199. The fix was reverted for 1.0.2.
> >> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> >> HDFS CSV file.
> >> The full list is at http://s.apache.org/9NJ
> >>
> >> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> >>
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
> >>
> >> The release files, including signatures, digests, etc can be found at:
> >> http://people.apache.org/~tdas/spark-1.0.2-rc1/
> >>
> >> Release artifacts are signed with the following key:
> >> https://people.apache.org/keys/committer/tdas.asc
> >>
> >> The staging repository for this release can be found at:
> >> https://repository.apache.org/content/repositories/orgapachespark-1024/
> >>
> >> The documentation corresponding to this release can be found at:
> >> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> >>
> >> Please vote on releasing this package as Apache Spark 1.0.2!
> >>
> >> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> >> a majority of at least 3 +1 PMC votes are cast.
> >> [ ] +1 Release this package as Apache Spark 1.0.2
> >> [ ] -1 Do not release this package because ...
> >>
> >> To learn more about Apache Spark, please see
> >> http://spark.apache.org/
> >
>

--089e0160d14cd532a704ff4b4913--

From dev-return-8617-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 02:37:08 2014
Return-Path: <dev-return-8617-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E0E9E11A6D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 02:37:08 +0000 (UTC)
Received: (qmail 45644 invoked by uid 500); 29 Jul 2014 02:37:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 45591 invoked by uid 500); 29 Jul 2014 02:37:08 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 45575 invoked by uid 99); 29 Jul 2014 02:37:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 02:37:07 +0000
X-ASF-Spam-Status: No, hits=1.7 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of liqingyang1985@gmail.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 02:37:04 +0000
Received: by mail-we0-f170.google.com with SMTP id w62so8289747wes.1
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 19:36:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=1zRRx1lIR7cATRPdLUpNdc4huz/DGzJ1KFkGjzLPOvs=;
        b=wRu1k7P/miLJkBffjgoEkZ7RRmNrvMdJ7kr0IxisSs13c+aQ7GY70zoq5S6FAUyTsQ
         IgfBydNm5vEo6a8UREXaqy7QzzHTzqbf6cOoENe08auqyfPIpNus7XYiXFvxZy+QCQAC
         Nr03T0uFumUAadYWi3LGZcSH6nyAD+DXnIOHCMEnDvDYw9KGjbvV4Zo1hd6VVSEdlnt5
         wE8t9v7AxwzGsz734G5E42ACUy6lgyuA0LBtIGU6xB2f7A62eB2vN+1FYTXEO1ESfCco
         pVdQ5Up4kyqrJkaLZRdZ6aEuLVESIEoL+MQnmCoB5OQ2mRN4OWpRvBvqsAujvgR+bFlb
         2y7w==
MIME-Version: 1.0
X-Received: by 10.180.94.5 with SMTP id cy5mr1639474wib.11.1406601402664; Mon,
 28 Jul 2014 19:36:42 -0700 (PDT)
Received: by 10.194.173.98 with HTTP; Mon, 28 Jul 2014 19:36:42 -0700 (PDT)
In-Reply-To: <CAG2iju0g8ge+AqPsCxVf5GEHJvOqc97sKNR6KTpwZ2OeUd3O7g@mail.gmail.com>
References: <CABDsqqaLsq5HvqUNMDoenJ7CTaOTCaoriGBUZOC=euQNZRzQ6Q@mail.gmail.com>
	<CANGvG8rb-7LqLb4jiBMui-2rc2K9Bka531SgPaG+_nW4A66hkg@mail.gmail.com>
	<CAJiQeY+_ONnMiwSiyK=79UB9gTSDHr3fWVhwo3Xh0iQ1J_O0eQ@mail.gmail.com>
	<CANGvG8raanrLnc_c6JMNwVxZrOLXVSBjiKaGU3wp9=R7M-XQLQ@mail.gmail.com>
	<CAG2iju1jF0cMnLBFX=Rk1qNhfHhFsXAO22iUT4dcmjCB_-GyoA@mail.gmail.com>
	<CABDsqqarh4OO2fdLEB49yXjroBoCHh7_oVuRdsGZx-jNFdb1Lw@mail.gmail.com>
	<CAG2iju36syXK7yJUDgwOS4U9PTrR+M=DYPcV9AaBP=baskZgvw@mail.gmail.com>
	<CABDsqqYP8QG+FOs3k4Jr9RDaStV8jxgGZ2K7=Q1HJqpZXruu8w@mail.gmail.com>
	<CABDsqqb6HZr8P3N2aVDX7kd8Oiiz4dBCfVkwpfy9LoWwmEUbNg@mail.gmail.com>
	<CAG2iju0g8ge+AqPsCxVf5GEHJvOqc97sKNR6KTpwZ2OeUd3O7g@mail.gmail.com>
Date: Tue, 29 Jul 2014 10:36:42 +0800
Message-ID: <CABDsqqYnMsc9tzvSvQ=5gTBMNA=74KGFDVgjxKLyXxTaEX5PGQ@mail.gmail.com>
Subject: Re: on shark, is tachyon less efficient than memory_only cache
 strategy ?
From: qingyang li <liqingyang1985@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=f46d04440356f24d5f04ff4be8f9
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d04440356f24d5f04ff4be8f9
Content-Type: text/plain; charset=UTF-8

hi, haoyuan, thanks for replying.


2014-07-21 16:29 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:

> Qingyang,
>
> Aha. Got it.
>
> 800MB data is pretty small. Loading from Tachyon does have a bit of extra
> overhead. But it will have more benefit when the data size is larger. Also,
> if you store the table in Tachyon, you can have different shark servers to
> query the data at the same time. For more trade-off, please refer to this
> page: http://tachyon-project.org/Running-Shark-on-Tachyon.html
>
> Best,
>
> Haoyuan
>
>
> On Wed, Jul 16, 2014 at 12:06 AM, qingyang li <liqingyang1985@gmail.com>
> wrote:
>
> > let's me describe my scene:
> > ----------------------
> > i have 8 machines (24 core , 16G memory, per machine) of spark cluster
> and
> > tachyon cluster.  On tachyon,  I create one table which contains 800M
> data,
> > when i run query sql on shark,   it will cost 2.43s,  but when i create
> the
> > same table on spark memory , i run  the same sql , it will cost 1.56s.
> >  data on tachyon cost more time than data on spark memory.   they all
> have
> > 150 map process,  and per node 16-20 map process.
> > I think the reason is that when data is on tachyon, shark will let spark
> > slave load data from tachyon salve which is on the same node with tachyon
> > slave,
> > i have tried to set some configuration to tune shark and tachyon, but
> still
> > can not make the former more fast than 2.43s.
> > do anyone have some ideas ?
> >
> > By the way ,  my tachyon block size is 1GB now,  i want to reset block
> size
> > ,  will it work by setting tachyon.user.default.block.size.byte=8M ?  if
> > not,  what does tachyon.user.default.block.size.byte mean?
> >
> >
> > 2014-07-14 13:13 GMT+08:00 qingyang li <liqingyang1985@gmail.com>:
> >
> > > Shark,  thanks for replying.
> > > Let's me clear my question again.
> > > ----------------------------------------------
> > > i create a table using " create table xxx1
> > > tblproperties("shark.cache"="tachyon") as select * from xxx2"
> > > when excuting some sql (for example , select * from xxx1) using shark,
> > >  shark will read data into shark's memory  from tachyon's memory.
> > > I think if each time we execute sql, shark always load data from
> tachyon,
> > > it is less effient.
> > > could we use some cache policy (such as,  CacheAllPolicy
> FIFOCachePolicy
> > > LRUCachePolicy ) to cache data to invoid reading data from tachyon for
> > > each sql query?
> > > ----------------------------------------------
> > >
> > >
> > >
> > > 2014-07-14 2:47 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
> > >
> > > Qingyang,
> > >>
> > >> Are you asking Spark or Shark (The first email was "Shark", the last
> > email
> > >> was "Spark".)?
> > >>
> > >> Best,
> > >>
> > >> Haoyuan
> > >>
> > >>
> > >> On Wed, Jul 9, 2014 at 7:40 PM, qingyang li <liqingyang1985@gmail.com
> >
> > >> wrote:
> > >>
> > >> > could i set some cache policy to let spark load data from tachyon
> only
> > >> one
> > >> > time for all sql query?  for example by using CacheAllPolicy
> > >> > FIFOCachePolicy LRUCachePolicy.  But I have tried that three policy,
> > >> they
> > >> > are not useful.
> > >> > I think , if spark always load data for each sql query,  it will
> > impact
> > >> the
> > >> > query speed , it will take more time than the case that data are
> > >> managed by
> > >> > spark itself.
> > >> >
> > >> >
> > >> >
> > >> >
> > >> > 2014-07-09 1:19 GMT+08:00 Haoyuan Li <haoyuan.li@gmail.com>:
> > >> >
> > >> > > Yes. For Shark, two modes, "shark.cache=tachyon" and
> > >> > "shark.cache=memory",
> > >> > > have the same ser/de overhead. Shark loads data from outsize of
> the
> > >> > process
> > >> > > in Tachyon mode with the following benefits:
> > >> > >
> > >> > >
> > >> > >    - In-memory data sharing across multiple Shark instances (i.e.
> > >> > stronger
> > >> > >    isolation)
> > >> > >    - Instant recovery of in-memory tables
> > >> > >    - Reduce heap size => faster GC in shark
> > >> > >    - If the table is larger than the memory size, only the hot
> > columns
> > >> > will
> > >> > >    be cached in memory
> > >> > >
> > >> > > from
> > http://tachyon-project.org/master/Running-Shark-on-Tachyon.html
> > >> and
> > >> > > https://github.com/amplab/shark/wiki/Running-Shark-with-Tachyon
> > >> > >
> > >> > > Haoyuan
> > >> > >
> > >> > >
> > >> > > On Tue, Jul 8, 2014 at 9:58 AM, Aaron Davidson <
> ilikerps@gmail.com>
> > >> > wrote:
> > >> > >
> > >> > > > Shark's in-memory format is already serialized (it's compressed
> > and
> > >> > > > column-based).
> > >> > > >
> > >> > > >
> > >> > > > On Tue, Jul 8, 2014 at 9:50 AM, Mridul Muralidharan <
> > >> mridul@gmail.com>
> > >> > > > wrote:
> > >> > > >
> > >> > > > > You are ignoring serde costs :-)
> > >> > > > >
> > >> > > > > - Mridul
> > >> > > > >
> > >> > > > > On Tue, Jul 8, 2014 at 8:48 PM, Aaron Davidson <
> > >> ilikerps@gmail.com>
> > >> > > > wrote:
> > >> > > > > > Tachyon should only be marginally less performant than
> > >> memory_only,
> > >> > > > > because
> > >> > > > > > we mmap the data from Tachyon's ramdisk. We do not have to,
> > say,
> > >> > > > transfer
> > >> > > > > > the data over a pipe from Tachyon; we can directly read from
> > the
> > >> > > > buffers
> > >> > > > > in
> > >> > > > > > the same way that Shark reads from its in-memory columnar
> > >> format.
> > >> > > > > >
> > >> > > > > >
> > >> > > > > >
> > >> > > > > > On Tue, Jul 8, 2014 at 1:18 AM, qingyang li <
> > >> > > liqingyang1985@gmail.com>
> > >> > > > > > wrote:
> > >> > > > > >
> > >> > > > > >> hi, when i create a table, i can point the cache strategy
> > using
> > >> > > > > >> shark.cache,
> > >> > > > > >> i think "shark.cache=memory_only"  means data are managed
> by
> > >> > spark,
> > >> > > > and
> > >> > > > > >> data are in the same jvm with excutor;   while
> > >> > >  "shark.cache=tachyon"
> > >> > > > > >>  means  data are managed by tachyon which is off heap, and
> > data
> > >> > are
> > >> > > > not
> > >> > > > > in
> > >> > > > > >> the same jvm with excutor,  so spark will load data from
> > >> tachyon
> > >> > for
> > >> > > > > each
> > >> > > > > >> query sql , so,  is  tachyon less efficient than
> memory_only
> > >> cache
> > >> > > > > strategy
> > >> > > > > >>  ?
> > >> > > > > >> if yes, can we let spark load all data once from tachyon
>  for
> > >> all
> > >> > > sql
> > >> > > > > query
> > >> > > > > >>  if i want to use tachyon cache strategy since tachyon is
> > more
> > >> HA
> > >> > > than
> > >> > > > > >> memory_only ?
> > >> > > > > >>
> > >> > > > >
> > >> > > >
> > >> > >
> > >> > >
> > >> > >
> > >> > > --
> > >> > > Haoyuan Li
> > >> > > AMPLab, EECS, UC Berkeley
> > >> > > http://www.cs.berkeley.edu/~haoyuan/
> > >> > >
> > >> >
> > >>
> > >>
> > >>
> > >> --
> > >> Haoyuan Li
> > >> AMPLab, EECS, UC Berkeley
> > >> http://www.cs.berkeley.edu/~haoyuan/
> > >>
> > >
> > >
> >
>
>
>
> --
> Haoyuan Li
> AMPLab, EECS, UC Berkeley
> http://www.cs.berkeley.edu/~haoyuan/
>

--f46d04440356f24d5f04ff4be8f9--

From dev-return-8618-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 03:30:12 2014
Return-Path: <dev-return-8618-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4BFF11BE2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 03:30:12 +0000 (UTC)
Received: (qmail 11457 invoked by uid 500); 29 Jul 2014 03:30:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11173 invoked by uid 500); 29 Jul 2014 03:30:11 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 11161 invoked by uid 99); 29 Jul 2014 03:30:10 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 03:30:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of spark.devuser@gmail.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 03:29:53 +0000
Received: by mail-vc0-f180.google.com with SMTP id ij19so12635344vcb.39
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 20:29:28 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Cn3ZkHF38AQ/FEWeXq5W/fH8fJwGk129xgTNnLjrTgQ=;
        b=P4sbINRix928LnGsdKamizCPaklgR6H+Fea+8vgte7zIGOtKtU+Ww79AUYuKDPZJpK
         X0E++vduXu774VepwFEr5tfC6CNiJUTpEgOocyFCfvAliyT+jhRImWYsxoDgjm/2e8Bf
         A5HdFG9oJ7gcgJ5pxgX/QVKGtHJWWr7GZoxCfYqCZhDyuBTXfZ2SNuHHhpFdnepU0hCb
         EsUakqo2W8INvx3UOS43tqzv/XQJmjhZPmvMCjQ/7cPb4iVsvlPcrhGWSsDD7OgGwa9L
         Wwq6KaMgQb1uk1C6gggZhMs6f4q4Lhmvhkq07XqHlRdXfkfT/ZqL9tnJDBZvy6Yer3dr
         O/uA==
MIME-Version: 1.0
X-Received: by 10.221.59.194 with SMTP id wp2mr4884177vcb.59.1406604568315;
 Mon, 28 Jul 2014 20:29:28 -0700 (PDT)
Received: by 10.220.247.132 with HTTP; Mon, 28 Jul 2014 20:29:28 -0700 (PDT)
In-Reply-To: <CAMJOb8mDdoOb9i5sBWUte6U=J8eyEYYM6i3g5z8yM7XtbobHGA@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<71937D1E-0EC2-413E-A447-D60B49D16631@gmail.com>
	<CAMwrk0m3saqjbX8MPtN_=fEdjiLv8ev+QR4jwFoEgXh_11Nqag@mail.gmail.com>
	<CAMJOb8mDdoOb9i5sBWUte6U=J8eyEYYM6i3g5z8yM7XtbobHGA@mail.gmail.com>
Date: Mon, 28 Jul 2014 20:29:28 -0700
Message-ID: <CAPV_Hny8UjaS5rP-CY-R4Z+did=VHLp89J5w4oPWew2hDEDVnA@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Mubarak Seyed <spark.devuser@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11335472a24b0404ff4ca57a
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11335472a24b0404ff4ca57a
Content-Type: text/plain; charset=UTF-8

+1 (non-binding)

Tested this on Mac OS X.


On Mon, Jul 28, 2014 at 6:52 PM, Andrew Or <andrew@databricks.com> wrote:

> +1 Tested on standalone and yarn clusters
>
>
> 2014-07-28 14:59 GMT-07:00 Tathagata Das <tathagata.das1565@gmail.com>:
>
> > Let me add my vote as well.
> > Did some basic tests by running simple projects with various Spark
> > modules. Tested checksums.
> >
> > +1
> >
> > On Sun, Jul 27, 2014 at 4:52 PM, Matei Zaharia <matei.zaharia@gmail.com>
> > wrote:
> > > +1
> > >
> > > Tested this on Mac OS X.
> > >
> > > Matei
> > >
> > > On Jul 25, 2014, at 4:08 PM, Tathagata Das <
> tathagata.das1565@gmail.com>
> > wrote:
> > >
> > >> Please vote on releasing the following candidate as Apache Spark
> > version 1.0.2.
> > >>
> > >> This release fixes a number of bugs in Spark 1.0.1.
> > >> Some of the notable ones are
> > >> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> > >> SPARK-1199. The fix was reverted for 1.0.2.
> > >> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> > >> HDFS CSV file.
> > >> The full list is at http://s.apache.org/9NJ
> > >>
> > >> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> > >>
> >
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
> > >>
> > >> The release files, including signatures, digests, etc can be found at:
> > >> http://people.apache.org/~tdas/spark-1.0.2-rc1/
> > >>
> > >> Release artifacts are signed with the following key:
> > >> https://people.apache.org/keys/committer/tdas.asc
> > >>
> > >> The staging repository for this release can be found at:
> > >>
> https://repository.apache.org/content/repositories/orgapachespark-1024/
> > >>
> > >> The documentation corresponding to this release can be found at:
> > >> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> > >>
> > >> Please vote on releasing this package as Apache Spark 1.0.2!
> > >>
> > >> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> > >> a majority of at least 3 +1 PMC votes are cast.
> > >> [ ] +1 Release this package as Apache Spark 1.0.2
> > >> [ ] -1 Do not release this package because ...
> > >>
> > >> To learn more about Apache Spark, please see
> > >> http://spark.apache.org/
> > >
> >
>

--001a11335472a24b0404ff4ca57a--

From dev-return-8619-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 05:18:12 2014
Return-Path: <dev-return-8619-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B9B3B11DC7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 05:18:12 +0000 (UTC)
Received: (qmail 34502 invoked by uid 500); 29 Jul 2014 05:18:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34440 invoked by uid 500); 29 Jul 2014 05:18:12 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34426 invoked by uid 99); 29 Jul 2014 05:18:11 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 05:18:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mengxr@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 05:17:55 +0000
Received: by mail-ig0-f172.google.com with SMTP id h15so4926030igd.5
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 22:17:30 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=Ln5nyMNoMPeUFY8VSx7ZhRLkxVeOSAs5az55WZS3gDg=;
        b=EvXCybFAzLFe3PORj5lEFpq1sAdNaT5E/EL80DTTUrA7QUu7XgWTDfwBdBg/2KfOEh
         0q1XYVVuCVY0C1ahs/SkYvkZzmVptg/2lH4AkHO1Vmn5rcmVdmh1YsY+D2aV//TDWL+Z
         7Py5rmKnxIEqwYhgf0jxFIAJK1qJiMyZb0BCyskhtI+ocz8Vxu3rc8XYtz1rQ28BnwP5
         AkpMcTeKjLpHmCmKu8rjREAC6COTy9GN5YHiImY7unZEqTOApu8KJpnfsp/GOChOblH4
         JP9U0vpFsitsRtftvgcNTvEKrwWso6Rr1BhuNU4yHxvAEpox+n6XcYZXcrcd9ZbYZyLY
         3x5g==
MIME-Version: 1.0
X-Received: by 10.50.56.38 with SMTP id x6mr2629334igp.30.1406611050096; Mon,
 28 Jul 2014 22:17:30 -0700 (PDT)
Received: by 10.107.130.100 with HTTP; Mon, 28 Jul 2014 22:17:30 -0700 (PDT)
In-Reply-To: <CAPV_Hny8UjaS5rP-CY-R4Z+did=VHLp89J5w4oPWew2hDEDVnA@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
	<71937D1E-0EC2-413E-A447-D60B49D16631@gmail.com>
	<CAMwrk0m3saqjbX8MPtN_=fEdjiLv8ev+QR4jwFoEgXh_11Nqag@mail.gmail.com>
	<CAMJOb8mDdoOb9i5sBWUte6U=J8eyEYYM6i3g5z8yM7XtbobHGA@mail.gmail.com>
	<CAPV_Hny8UjaS5rP-CY-R4Z+did=VHLp89J5w4oPWew2hDEDVnA@mail.gmail.com>
Date: Mon, 28 Jul 2014 22:17:30 -0700
Message-ID: <CAJgQjQ-oG9_9zRRFoVm=Uk6gRXg7OBgUWKViHVUsozfLd5YVMg@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Xiangrui Meng <mengxr@gmail.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

+1

Tested basic spark-shell and pyspark operations and MLlib examples on a Mac.

On Mon, Jul 28, 2014 at 8:29 PM, Mubarak Seyed <spark.devuser@gmail.com> wrote:
> +1 (non-binding)
>
> Tested this on Mac OS X.
>
>
> On Mon, Jul 28, 2014 at 6:52 PM, Andrew Or <andrew@databricks.com> wrote:
>
>> +1 Tested on standalone and yarn clusters
>>
>>
>> 2014-07-28 14:59 GMT-07:00 Tathagata Das <tathagata.das1565@gmail.com>:
>>
>> > Let me add my vote as well.
>> > Did some basic tests by running simple projects with various Spark
>> > modules. Tested checksums.
>> >
>> > +1
>> >
>> > On Sun, Jul 27, 2014 at 4:52 PM, Matei Zaharia <matei.zaharia@gmail.com>
>> > wrote:
>> > > +1
>> > >
>> > > Tested this on Mac OS X.
>> > >
>> > > Matei
>> > >
>> > > On Jul 25, 2014, at 4:08 PM, Tathagata Das <
>> tathagata.das1565@gmail.com>
>> > wrote:
>> > >
>> > >> Please vote on releasing the following candidate as Apache Spark
>> > version 1.0.2.
>> > >>
>> > >> This release fixes a number of bugs in Spark 1.0.1.
>> > >> Some of the notable ones are
>> > >> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
>> > >> SPARK-1199. The fix was reverted for 1.0.2.
>> > >> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
>> > >> HDFS CSV file.
>> > >> The full list is at http://s.apache.org/9NJ
>> > >>
>> > >> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
>> > >>
>> >
>> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>> > >>
>> > >> The release files, including signatures, digests, etc can be found at:
>> > >> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>> > >>
>> > >> Release artifacts are signed with the following key:
>> > >> https://people.apache.org/keys/committer/tdas.asc
>> > >>
>> > >> The staging repository for this release can be found at:
>> > >>
>> https://repository.apache.org/content/repositories/orgapachespark-1024/
>> > >>
>> > >> The documentation corresponding to this release can be found at:
>> > >> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>> > >>
>> > >> Please vote on releasing this package as Apache Spark 1.0.2!
>> > >>
>> > >> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
>> > >> a majority of at least 3 +1 PMC votes are cast.
>> > >> [ ] +1 Release this package as Apache Spark 1.0.2
>> > >> [ ] -1 Do not release this package because ...
>> > >>
>> > >> To learn more about Apache Spark, please see
>> > >> http://spark.apache.org/
>> > >
>> >
>>

From dev-return-8620-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 05:47:06 2014
Return-Path: <dev-return-8620-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1511811E4C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 05:47:06 +0000 (UTC)
Received: (qmail 73078 invoked by uid 500); 29 Jul 2014 05:47:05 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 73013 invoked by uid 500); 29 Jul 2014 05:47:05 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 72998 invoked by uid 99); 29 Jul 2014 05:47:05 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 05:47:05 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 05:46:47 +0000
Received: by mail-wg0-f44.google.com with SMTP id m15so8320557wgh.27
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 22:46:23 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=ZLb1iHlfSILvHLUdP7s/0AxuNgDbhsGVNiArLaBM0CE=;
        b=00YWcMB4TgV6C2ZwsuvdS5PwVKM+uI+nIz+y/PzZwxDGwcoKxROlUX4nBKYqlo62bb
         mVJJgX8xtCzJrHMNy+xctkSf7clCzQCZJ41Ezo0cimhBwtgxRGMCbuAq49sCXtaK41hm
         MQh4wpKWAcYfL14R2JO2rvqaBwz+7292Hf7dIxjtK27A8NIi4A/yUrqiuqV5ok223ivA
         sBdrOhjA6/NMiq7p+wNhjTTTuelUPzSKjHwzY9ZvGD4mXjEMcUWPfm0jV6MPFOa15ArU
         hVTcB/QCFic05MUSGwErbt/vQPWSpEEzbDFnhn3SLWh9jfsp1FE5RlOFRP+8J6BXCxZP
         V08Q==
MIME-Version: 1.0
X-Received: by 10.180.73.109 with SMTP id k13mr37168735wiv.11.1406612783440;
 Mon, 28 Jul 2014 22:46:23 -0700 (PDT)
Received: by 10.216.130.7 with HTTP; Mon, 28 Jul 2014 22:46:23 -0700 (PDT)
In-Reply-To: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
Date: Mon, 28 Jul 2014 22:46:23 -0700
Message-ID: <CALuGr6ZwYZQ097PsubLpycQuQ9S0L78ZLv+8cPXkDMnKtufskw@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

NOTICE and LICENSE files look good
Hashes and sigs look good
No executable in the source distribution
Compile source and run standalone

+1

- Henry

On Fri, Jul 25, 2014 at 4:08 PM, Tathagata Das
<tathagata.das1565@gmail.com> wrote:
> Please vote on releasing the following candidate as Apache Spark version 1.0.2.
>
> This release fixes a number of bugs in Spark 1.0.1.
> Some of the notable ones are
> - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> SPARK-1199. The fix was reverted for 1.0.2.
> - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> HDFS CSV file.
> The full list is at http://s.apache.org/9NJ
>
> The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=8fb6f00e195fb258f3f70f04756e07c259a2351f
>
> The release files, including signatures, digests, etc can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1/
>
> Release artifacts are signed with the following key:
> https://people.apache.org/keys/committer/tdas.asc
>
> The staging repository for this release can be found at:
> https://repository.apache.org/content/repositories/orgapachespark-1024/
>
> The documentation corresponding to this release can be found at:
> http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
>
> Please vote on releasing this package as Apache Spark 1.0.2!
>
> The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> a majority of at least 3 +1 PMC votes are cast.
> [ ] +1 Release this package as Apache Spark 1.0.2
> [ ] -1 Do not release this package because ...
>
> To learn more about Apache Spark, please see
> http://spark.apache.org/

From dev-return-8621-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 05:58:39 2014
Return-Path: <dev-return-8621-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AEF2611E78
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 05:58:39 +0000 (UTC)
Received: (qmail 83649 invoked by uid 500); 29 Jul 2014 05:58:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 83592 invoked by uid 500); 29 Jul 2014 05:58:38 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 83579 invoked by uid 99); 29 Jul 2014 05:58:37 -0000
Received: from Unknown (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 05:58:37 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.46] (HELO mail-qa0-f46.google.com) (209.85.216.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 05:58:20 +0000
Received: by mail-qa0-f46.google.com with SMTP id v10so8980521qac.33
        for <dev@spark.apache.org>; Mon, 28 Jul 2014 22:57:53 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=/uHIebaOvU1sxLecGdJBlY80y3LUVsZ++B6DSCpOxDE=;
        b=YH5Yp64U1PnMTB27LGY720DZBJOd9sP71zxQ9jz1B/CBMO0A9AuDIxJnbJtPj62s5V
         RSn2cFWcibHfoOlodQVaAEvOUCMptNFOqwj3VzJCKkEMlsdNFbCJToCu3esZGyIgvw3p
         KCQldfhIbYG6zJNJn4wN4nv7nCb8H4xnbiqDoun7lM9bSqX29/EbKXTSXSLM5EKwQ4IW
         MdXTqrBVVDILUDvo6quA7VBVlIIHt0Es8Zss8kPJiB85LrG8C3wQc3lTWOMjeaVejNT6
         CZh55V2H1PkRPBO8qikiE0XVZm36yQxit4gf2pFM5UH+FsiBe66u2Fno2FdhIl3Gk5we
         EBpA==
X-Gm-Message-State: ALoCoQnZt+5C46vCDUbJ3b8eYYb/e7obcTkUGz+a1fTfmmZ/nQEnKZUlta5LE39cJt0jn4wCWw+x
X-Received: by 10.224.20.200 with SMTP id g8mr67664667qab.88.1406613473609;
 Mon, 28 Jul 2014 22:57:53 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Mon, 28 Jul 2014 22:57:32 -0700 (PDT)
In-Reply-To: <CABPQxsu1AdYU=P2p9GTUHz-jD4LepVmvsF5ZBtZV0hmYT6gMSg@mail.gmail.com>
References: <CABPQxsu1AdYU=P2p9GTUHz-jD4LepVmvsF5ZBtZV0hmYT6gMSg@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Mon, 28 Jul 2014 22:57:32 -0700
Message-ID: <CAPh_B=bTknSVd2EAurm=exih4jnsrw6yUzi_ZLq7e6-gSN4o6w@mail.gmail.com>
Subject: Re: Github mirroring is running behind
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c2a3b06e651804ff4eb8e4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a3b06e651804ff4eb8e4
Content-Type: text/plain; charset=UTF-8

Hi devs,

I don't know if this is going to help, but if you can watch & vote on the
ticket, it might help ASF INFRA prioritize and triage it faster:
https://issues.apache.org/jira/browse/INFRA-8116

Please do. Thanks!



On Mon, Jul 28, 2014 at 5:41 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> https://issues.apache.org/jira/browse/INFRA-8116
>
> Just a heads up, the github mirroring is running behind. You can
> follow that JIRA to keep up to date on the fix.
>
> In the mean time you can use the Apache git itself:
>
> https://git-wip-us.apache.org/repos/asf/spark.git
>
> Some people have reported issues checking out Apache git as well, but
> it might work.
>
> - Patrick
>

--001a11c2a3b06e651804ff4eb8e4--

From dev-return-8622-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 07:55:32 2014
Return-Path: <dev-return-8622-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F196F111FA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 07:55:31 +0000 (UTC)
Received: (qmail 91210 invoked by uid 500); 29 Jul 2014 07:55:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91148 invoked by uid 500); 29 Jul 2014 07:55:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 91136 invoked by uid 99); 29 Jul 2014 07:55:30 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 07:55:30 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 07:55:28 +0000
Received: by mail-qg0-f49.google.com with SMTP id j107so9728146qga.22
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 00:55:03 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=9kMnHV7Ui22Yt0kB9OaUjdHAP9I3iIA7q6hvhVis7oo=;
        b=KWDfxtOwTF3ANMXW6CXLKOElvQ1fwZkrWtXM9GwN8q/lC5nJMAjEBvXztuy6y8n+/H
         gZQl3dlCYtk/xjZwoHAdPzsfvIiKDt+5gCxf4du9c4EBDU3n4ax66vkpIn8kBSskS1Mi
         qGoCbUsXEEaGtvPm9dcyOHmwyi39x0cJ6vYYhxhHwnArC9grf2kiaOjMPgOhB7oLlZxi
         igaO59aLv4H95YaVJzPAPl6vriA5h7rW6TMJJjlucYCPi1FA9PcLaDS40CCIPUbsoQJX
         0A7hceO6rnzKiWQYX7Lt+62f7MEGayZ89pVoL8p2OXxIV7OtAx4chmR5OReyn68oi9gi
         +ocA==
X-Gm-Message-State: ALoCoQmTqFNfEZIB36wBwHgr4dOk46BjsKb6iiG9UIXRtUUVdUtopJU6ExIPBh8wAgc9BuBQH6h7
X-Received: by 10.140.42.10 with SMTP id b10mr652041qga.82.1406620503552; Tue,
 29 Jul 2014 00:55:03 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Tue, 29 Jul 2014 00:54:43 -0700 (PDT)
In-Reply-To: <1406216458354-7484.post@n3.nabble.com>
References: <1406216458354-7484.post@n3.nabble.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 29 Jul 2014 00:54:43 -0700
Message-ID: <CAPh_B=aBVCNZDqU1dk9n5puAZHQ+iwsMK5PV2Ptui4L1C-fr_w@mail.gmail.com>
Subject: Re: pre-filtered hadoop RDD use case
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a11c138dc72cf2f04ff505b7d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c138dc72cf2f04ff505b7d
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Would something like this help?

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/=
spark/rdd/PartitionPruningRDD.scala




On Thu, Jul 24, 2014 at 8:40 AM, Eugene Cheipesh <echeipesh@gmail.com>
wrote:

> Hello,
>
> I have an interesting use case for a pre-filtered RDD. I have two solutio=
ns
> that I am not entirly happy with and would like to get some feedback and
> thoughts. Perhaps it is a use case that could be more explicitly supporte=
d
> in Spark.
>
> My data has well defined semantics for they key values that I can use to
> pre-filter an RDD to exclude those partitions and records that I will not
> need from being loaded at all. In most cases this is significant savings.
>
> Essentially the dataset is geographic image tiles, as you would see on
> google maps. The entire dataset could be huge, covering an entire contine=
nt
> at high resolution. But if I want to work with a subset, lets say a singl=
e
> city, it makes no sense for me to load all the partitions into memory jus=
t
> so I can filter them as a first step.
>
> First attempt was to extent NewHadoopRDD as follows:
>
> abstract class PreFilteredHadoopRDD[K, V](
>     sc : SparkContext,
>     inputFormatClass: Class[_ <: InputFormat[K, V]],
>     keyClass: Class[K],
>     valueClass: Class[V],
>     @transient conf: Configuration)
>   extends NewHadoopRDD(sc, inputFormatClass, keyClass, valueClass, conf)
> {
>   /** returns true if specific partition has relevant keys */
>   def includePartition(p: Partition): Boolean
>
>   /** returns true if the specific key in the partition passes the filter
> */
>   def includeKey(key: K): Boolean
>
>   override def getPartitions: Array[Partition] =3D {
>     val partitions =3D super.getPartitions
>     partitions.filter(includePartition)
>   }
>
>   override def compute(theSplit: Partition, context: TaskContext) =3D {
>     val ii =3D super.compute(theSplit, context)
>     new InterruptibleIterator(ii.context, ii.delegate.filter{case (k,v) =
=3D>
> includeKey(k)})
>   }
> }
>
> NewHadoopRDD for reference:
>
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apach=
e/spark/rdd/NewHadoopRDD.scala
>
> This is nice and handles the partition portion of the issue well enough,
> but
> by the time the iterator is created by super.compute there is no way avoi=
d
> reading the values from records that do not pass my filter.
>
> Since I am actually using =E2=80=98SequenceFileInputFormat=E2=80=99 as my=
 InputFormat I can
> do better, and avoid deserializing the values if I could get my hands on
> the
> reader and re-implement compute(). But this does not seem possible to do
> through extension because both the NewHadooprRDD.confBroadcast and
> NewHadoopPartition are private. There  does not seem to be a choice but t=
o
> copy/paste extend the NewHadoopRDD.
>
> The two solutions that are apparent are:
> 1. remove those private modifiers
> 2. factor out reader creation to a method that can be used to reimplement
> compute() in a sub-class
>
> I would be curious to hear if anybody had/has similar problem and any
> thoughts on the issue. If you think there is PR in this I=E2=80=99d be ha=
ppy to
> code
> it up and submit it.
>
>
> Thank you
> --
> Eugene Cheipesh
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/pre-filtered-ha=
doop-RDD-use-case-tp7484.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c138dc72cf2f04ff505b7d--

From dev-return-8623-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 15:53:59 2014
Return-Path: <dev-return-8623-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 30AF111091
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 15:53:59 +0000 (UTC)
Received: (qmail 30873 invoked by uid 500); 29 Jul 2014 15:53:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30821 invoked by uid 500); 29 Jul 2014 15:53:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 30809 invoked by uid 99); 29 Jul 2014 15:53:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 15:53:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 74.125.82.51 as permitted sender)
Received: from [74.125.82.51] (HELO mail-wg0-f51.google.com) (74.125.82.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 15:53:52 +0000
Received: by mail-wg0-f51.google.com with SMTP id b13so9157224wgh.22
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 08:53:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=bWwC2Hb8mwsV26t59rH2payZ4ehdQQRt0g9z/WuEO6I=;
        b=ieX7IgihPOu94LvKysRdreYpW4ObXqAuYKF4Assb5TFgdAMEHslFaPpMXDZXguZMJa
         0fEdvO3g8Xad5TMIJx98Ob1zGLUm6V6DTf5NR0XC2FfhHz4QrrRhxZAqollOC9NkH/15
         I7qwkCuaEuJzoVR1atd1ySwwRqrD2dNX1grYQpvSK6yWK2Fq9usM5nzK8y3I75ozKKvg
         RddPFM5wPhp/sxjKfP8Ul1X1AKIUiPXWAVxs+IdUF3Fks+rw6wdpWy2ahpa2CzNlFL0/
         856zZuo7+vaKAEK3/HfH7QBWUabaXTOWnBLdILeTNMUUBBrvCpqteO0zF84DZdTHCcki
         5p4A==
X-Received: by 10.180.24.97 with SMTP id t1mr43071607wif.45.1406649211509;
 Tue, 29 Jul 2014 08:53:31 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Tue, 29 Jul 2014 08:52:50 -0700 (PDT)
In-Reply-To: <CALuGr6ZwYZQ097PsubLpycQuQ9S0L78ZLv+8cPXkDMnKtufskw@mail.gmail.com>
References: <CAMwrk0kCc7HBpvvgZkU2bf51HvQGVWdudQq+z+LKBPV1CJJPvg@mail.gmail.com>
 <CALuGr6ZwYZQ097PsubLpycQuQ9S0L78ZLv+8cPXkDMnKtufskw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 29 Jul 2014 11:52:50 -0400
Message-ID: <CAOhmDze+o4tRfZuzJHFyBBFMoOZC9gEJG1pgoa3HqNJQ2ZgY3g@mail.gmail.com>
Subject: Re: [VOTE] Release Apache Spark 1.0.2 (RC1)
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=f46d043c80629368bf04ff570a23
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043c80629368bf04ff570a23
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

   - spun up an EC2 cluster successfully using spark-ec2
   - tested S3 file access from that cluster successfully

+1
=E2=80=8B


On Tue, Jul 29, 2014 at 1:46 AM, Henry Saputra <henry.saputra@gmail.com>
wrote:

> NOTICE and LICENSE files look good
> Hashes and sigs look good
> No executable in the source distribution
> Compile source and run standalone
>
> +1
>
> - Henry
>
> On Fri, Jul 25, 2014 at 4:08 PM, Tathagata Das
> <tathagata.das1565@gmail.com> wrote:
> > Please vote on releasing the following candidate as Apache Spark versio=
n
> 1.0.2.
> >
> > This release fixes a number of bugs in Spark 1.0.1.
> > Some of the notable ones are
> > - SPARK-2452: Known issue is Spark 1.0.1 caused by attempted fix for
> > SPARK-1199. The fix was reverted for 1.0.2.
> > - SPARK-2576: NoClassDefFoundError when executing Spark QL query on
> > HDFS CSV file.
> > The full list is at http://s.apache.org/9NJ
> >
> > The tag to be voted on is v1.0.2-rc1 (commit 8fb6f00e):
> >
> https://git-wip-us.apache.org/repos/asf?p=3Dspark.git;a=3Dcommit;h=3D8fb6=
f00e195fb258f3f70f04756e07c259a2351f
> >
> > The release files, including signatures, digests, etc can be found at:
> > http://people.apache.org/~tdas/spark-1.0.2-rc1/
> >
> > Release artifacts are signed with the following key:
> > https://people.apache.org/keys/committer/tdas.asc
> >
> > The staging repository for this release can be found at:
> > https://repository.apache.org/content/repositories/orgapachespark-1024/
> >
> > The documentation corresponding to this release can be found at:
> > http://people.apache.org/~tdas/spark-1.0.2-rc1-docs/
> >
> > Please vote on releasing this package as Apache Spark 1.0.2!
> >
> > The vote is open until Tuesday, July 29, at 23:00 UTC and passes if
> > a majority of at least 3 +1 PMC votes are cast.
> > [ ] +1 Release this package as Apache Spark 1.0.2
> > [ ] -1 Do not release this package because ...
> >
> > To learn more about Apache Spark, please see
> > http://spark.apache.org/
>

--f46d043c80629368bf04ff570a23--

From dev-return-8624-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 18:35:49 2014
Return-Path: <dev-return-8624-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B6165116D3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 18:35:49 +0000 (UTC)
Received: (qmail 60340 invoked by uid 500); 29 Jul 2014 18:35:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60289 invoked by uid 500); 29 Jul 2014 18:35:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 60268 invoked by uid 99); 29 Jul 2014 18:35:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 18:35:48 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Yan.Zhou.sc@huawei.com designates 206.16.17.72 as permitted sender)
Received: from [206.16.17.72] (HELO dfwrgout.huawei.com) (206.16.17.72)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 18:35:46 +0000
Received: from 172.18.9.243 (EHLO dfweml704-chm.china.huawei.com) ([172.18.9.243])
	by dfwrg01-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id CEU47332;
	Tue, 29 Jul 2014 13:35:19 -0500 (CDT)
Received: from SJCEML701-CHM.china.huawei.com (10.212.94.47) by
 dfweml704-chm.china.huawei.com (10.193.5.141) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Tue, 29 Jul 2014 11:35:18 -0700
Received: from SJCEML702-CHM.china.huawei.com ([169.254.4.137]) by
 SJCEML701-CHM.china.huawei.com ([169.254.3.190]) with mapi id 14.03.0158.001;
 Tue, 29 Jul 2014 11:35:17 -0700
From: "Yan Zhou.sc" <Yan.Zhou.sc@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: pre-filtered hadoop RDD use case
Thread-Topic: pre-filtered hadoop RDD use case
Thread-Index: AQHPp1W/O/dW09aknEq3BH13Ug+AoZu3K2SAgAAZlEA=
Date: Tue, 29 Jul 2014 18:35:16 +0000
Message-ID: <C434A3773D08A842B26FED6A1BA2E6546D14DFFD@SJCEML702-CHM.china.huawei.com>
References: <1406216458354-7484.post@n3.nabble.com>
 <CAPh_B=aBVCNZDqU1dk9n5puAZHQ+iwsMK5PV2Ptui4L1C-fr_w@mail.gmail.com>
In-Reply-To: <CAPh_B=aBVCNZDqU1dk9n5puAZHQ+iwsMK5PV2Ptui4L1C-fr_w@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.36.100]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org

UGFydGl0aW9uUHJ1bmluZ1JERC5zY2FsYSBzdGlsbCBvbmx5IGhhbmRsZXMsIGFzIHNhaWQsIHRo
ZSBwYXJ0aXRpb24gcG9ydGlvbiBvZiB0aGUgaXNzdWUuIA0KDQpPbiB0aGUgInJlY29yZCBwcnVu
aW5nIiBwb3J0aW9uLCBhbHRob3VnaCBjaGVhcCBmaXhlcyBjb3VsZCBiZSBhdmFpbGFibGUgZm9y
IHRoaXMgaXNzdWUgYXMgcmVwb3J0ZWQsIGJ1dCBJIGJlbGlldmUgYQ0KZnVuZGFtZW50YWwgaXNz
dWUgaXMgbGFjayBvZiBhIG1lY2hhbmlzbSBvZiBwcm9jZXNzaW5nIG1lcmdpbmcvcHVzaGRvd24u
IEdpdmVuIHRoZSBwb3B1bGFyaXR5IG9mIGNvbHVtbmFyLCBhbmQgZXZlbiBtb3JlIGludGVsbGln
ZW50LCBkYXRhIHN0b3JlcywNCml0IG1ha2VzIHNlbnNlIHRvIHN1cHBvcnQgc29tZSAicG9zdCBw
cm9jZXNzaW5nIiBiZWZvcmUgYSBSREQgaXMgZm9ybWVkLiBUaGlzICJwb3N0IHByb2Nlc3Npbmci
IGNvdWxkIGJlIHBlcmZvcm1lZCBieSB0aGUgUkREIGl0c2VsZiBpbiBjb21wdXRlKCk7IG9yIGl0
IGNvdWxkIGJlIHBlcmZvcm1lZCBieSBzb21lIGRhdGEgc3RvcmUgd2hpY2ggc3VwcG9ydHMgc3Vj
aCBwdXNoZG93bnMuIEluIHRoZSBsYXRlciBjYXNlLCBzdWNoIHByb2Nlc3NpbmcgaW5mbyBzaG91
bGQgYmUgbWFkZSBhdmFpbGFibGUgdG8gdGhlIGRhdGEgc3RvcmUgUkREIHRvIHBhc3Mgb24gdG8g
dGhlIHN0b3Jlcy4gDQoNCkZvciBpbnN0YW5jZSwgRmlsdGVyZWRSREQgcmVxdWlyZXMgdGhlIHBh
cmVudCB0byBtYXRlcmlhbGl6ZSB0aGUgcmVjb3JkIGZ1bGx5IGJlZm9yZSBpdCBjYW4gc3RhcnQg
aXRzIG93biBwcm9jZXNzaW5nLiBJZiBpdCBjb3VsZCBiZSAibWVyZ2VkIiB3aXRoIGl0cyBwYXJl
bnQsIGEgbXVjaCBzbWFsbGVyIFJERCBmb290cHJpbnQgd291bGQgcmVzdWx0Lg0KDQoiUGlwZWxp
bmVkIGV4ZWN1dGlvbiIgaXMgbWVudGlvbmVkIGZvciBOYXJyb3dEZXBlbmRlbmN5IGluIGNvZGUs
IGJ1dCBubyBpbXBsZW1lbnRhdGlvbiBzZWVtcyB0byBiZSBpbiBwbGFjZSwgYW5kIG1vcmUgb3B0
aW1pemF0aW9uIGlzIGRlc2lyZWQgYmV5b25kIGp1c3QgcmVjb3JkLW9yaWVudGVkIGV4ZWN1dGlv
biBwaXBlbGluaW5nLg0KDQoNCg0KLS0tLS1PcmlnaW5hbCBNZXNzYWdlLS0tLS0NCkZyb206IFJl
eW5vbGQgWGluIFttYWlsdG86cnhpbkBkYXRhYnJpY2tzLmNvbV0gDQpTZW50OiBUdWVzZGF5LCBK
dWx5IDI5LCAyMDE0IDEyOjU1IEFNDQpUbzogZGV2QHNwYXJrLmFwYWNoZS5vcmcNClN1YmplY3Q6
IFJlOiBwcmUtZmlsdGVyZWQgaGFkb29wIFJERCB1c2UgY2FzZQ0KDQpXb3VsZCBzb21ldGhpbmcg
bGlrZSB0aGlzIGhlbHA/DQoNCmh0dHBzOi8vZ2l0aHViLmNvbS9hcGFjaGUvc3BhcmsvYmxvYi9t
YXN0ZXIvY29yZS9zcmMvbWFpbi9zY2FsYS9vcmcvYXBhY2hlL3NwYXJrL3JkZC9QYXJ0aXRpb25Q
cnVuaW5nUkRELnNjYWxhDQoNCg0KDQoNCk9uIFRodSwgSnVsIDI0LCAyMDE0IGF0IDg6NDAgQU0s
IEV1Z2VuZSBDaGVpcGVzaCA8ZWNoZWlwZXNoQGdtYWlsLmNvbT4NCndyb3RlOg0KDQo+IEhlbGxv
LA0KPg0KPiBJIGhhdmUgYW4gaW50ZXJlc3RpbmcgdXNlIGNhc2UgZm9yIGEgcHJlLWZpbHRlcmVk
IFJERC4gSSBoYXZlIHR3byANCj4gc29sdXRpb25zIHRoYXQgSSBhbSBub3QgZW50aXJseSBoYXBw
eSB3aXRoIGFuZCB3b3VsZCBsaWtlIHRvIGdldCBzb21lIA0KPiBmZWVkYmFjayBhbmQgdGhvdWdo
dHMuIFBlcmhhcHMgaXQgaXMgYSB1c2UgY2FzZSB0aGF0IGNvdWxkIGJlIG1vcmUgDQo+IGV4cGxp
Y2l0bHkgc3VwcG9ydGVkIGluIFNwYXJrLg0KPg0KPiBNeSBkYXRhIGhhcyB3ZWxsIGRlZmluZWQg
c2VtYW50aWNzIGZvciB0aGV5IGtleSB2YWx1ZXMgdGhhdCBJIGNhbiB1c2UgDQo+IHRvIHByZS1m
aWx0ZXIgYW4gUkREIHRvIGV4Y2x1ZGUgdGhvc2UgcGFydGl0aW9ucyBhbmQgcmVjb3JkcyB0aGF0
IEkgDQo+IHdpbGwgbm90IG5lZWQgZnJvbSBiZWluZyBsb2FkZWQgYXQgYWxsLiBJbiBtb3N0IGNh
c2VzIHRoaXMgaXMgc2lnbmlmaWNhbnQgc2F2aW5ncy4NCj4NCj4gRXNzZW50aWFsbHkgdGhlIGRh
dGFzZXQgaXMgZ2VvZ3JhcGhpYyBpbWFnZSB0aWxlcywgYXMgeW91IHdvdWxkIHNlZSBvbiANCj4g
Z29vZ2xlIG1hcHMuIFRoZSBlbnRpcmUgZGF0YXNldCBjb3VsZCBiZSBodWdlLCBjb3ZlcmluZyBh
biBlbnRpcmUgDQo+IGNvbnRpbmVudCBhdCBoaWdoIHJlc29sdXRpb24uIEJ1dCBpZiBJIHdhbnQg
dG8gd29yayB3aXRoIGEgc3Vic2V0LCANCj4gbGV0cyBzYXkgYSBzaW5nbGUgY2l0eSwgaXQgbWFr
ZXMgbm8gc2Vuc2UgZm9yIG1lIHRvIGxvYWQgYWxsIHRoZSANCj4gcGFydGl0aW9ucyBpbnRvIG1l
bW9yeSBqdXN0IHNvIEkgY2FuIGZpbHRlciB0aGVtIGFzIGEgZmlyc3Qgc3RlcC4NCj4NCj4gRmly
c3QgYXR0ZW1wdCB3YXMgdG8gZXh0ZW50IE5ld0hhZG9vcFJERCBhcyBmb2xsb3dzOg0KPg0KPiBh
YnN0cmFjdCBjbGFzcyBQcmVGaWx0ZXJlZEhhZG9vcFJERFtLLCBWXSgNCj4gICAgIHNjIDogU3Bh
cmtDb250ZXh0LA0KPiAgICAgaW5wdXRGb3JtYXRDbGFzczogQ2xhc3NbXyA8OiBJbnB1dEZvcm1h
dFtLLCBWXV0sDQo+ICAgICBrZXlDbGFzczogQ2xhc3NbS10sDQo+ICAgICB2YWx1ZUNsYXNzOiBD
bGFzc1tWXSwNCj4gICAgIEB0cmFuc2llbnQgY29uZjogQ29uZmlndXJhdGlvbikNCj4gICBleHRl
bmRzIE5ld0hhZG9vcFJERChzYywgaW5wdXRGb3JtYXRDbGFzcywga2V5Q2xhc3MsIHZhbHVlQ2xh
c3MsIA0KPiBjb25mKSB7DQo+ICAgLyoqIHJldHVybnMgdHJ1ZSBpZiBzcGVjaWZpYyBwYXJ0aXRp
b24gaGFzIHJlbGV2YW50IGtleXMgKi8NCj4gICBkZWYgaW5jbHVkZVBhcnRpdGlvbihwOiBQYXJ0
aXRpb24pOiBCb29sZWFuDQo+DQo+ICAgLyoqIHJldHVybnMgdHJ1ZSBpZiB0aGUgc3BlY2lmaWMg
a2V5IGluIHRoZSBwYXJ0aXRpb24gcGFzc2VzIHRoZSANCj4gZmlsdGVyICovDQo+ICAgZGVmIGlu
Y2x1ZGVLZXkoa2V5OiBLKTogQm9vbGVhbg0KPg0KPiAgIG92ZXJyaWRlIGRlZiBnZXRQYXJ0aXRp
b25zOiBBcnJheVtQYXJ0aXRpb25dID0gew0KPiAgICAgdmFsIHBhcnRpdGlvbnMgPSBzdXBlci5n
ZXRQYXJ0aXRpb25zDQo+ICAgICBwYXJ0aXRpb25zLmZpbHRlcihpbmNsdWRlUGFydGl0aW9uKQ0K
PiAgIH0NCj4NCj4gICBvdmVycmlkZSBkZWYgY29tcHV0ZSh0aGVTcGxpdDogUGFydGl0aW9uLCBj
b250ZXh0OiBUYXNrQ29udGV4dCkgPSB7DQo+ICAgICB2YWwgaWkgPSBzdXBlci5jb21wdXRlKHRo
ZVNwbGl0LCBjb250ZXh0KQ0KPiAgICAgbmV3IEludGVycnVwdGlibGVJdGVyYXRvcihpaS5jb250
ZXh0LCBpaS5kZWxlZ2F0ZS5maWx0ZXJ7Y2FzZSANCj4gKGssdikgPT4NCj4gaW5jbHVkZUtleShr
KX0pDQo+ICAgfQ0KPiB9DQo+DQo+IE5ld0hhZG9vcFJERCBmb3IgcmVmZXJlbmNlOg0KPg0KPiBo
dHRwczovL2dpdGh1Yi5jb20vYXBhY2hlL3NwYXJrL2Jsb2IvbWFzdGVyL2NvcmUvc3JjL21haW4v
c2NhbGEvb3JnL2FwDQo+IGFjaGUvc3BhcmsvcmRkL05ld0hhZG9vcFJERC5zY2FsYQ0KPg0KPiBU
aGlzIGlzIG5pY2UgYW5kIGhhbmRsZXMgdGhlIHBhcnRpdGlvbiBwb3J0aW9uIG9mIHRoZSBpc3N1
ZSB3ZWxsIA0KPiBlbm91Z2gsIGJ1dCBieSB0aGUgdGltZSB0aGUgaXRlcmF0b3IgaXMgY3JlYXRl
ZCBieSBzdXBlci5jb21wdXRlIHRoZXJlIA0KPiBpcyBubyB3YXkgYXZvaWQgcmVhZGluZyB0aGUg
dmFsdWVzIGZyb20gcmVjb3JkcyB0aGF0IGRvIG5vdCBwYXNzIG15IA0KPiBmaWx0ZXIuDQo+DQo+
IFNpbmNlIEkgYW0gYWN0dWFsbHkgdXNpbmcg4oCYU2VxdWVuY2VGaWxlSW5wdXRGb3JtYXTigJkg
YXMgbXkgSW5wdXRGb3JtYXQgDQo+IEkgY2FuIGRvIGJldHRlciwgYW5kIGF2b2lkIGRlc2VyaWFs
aXppbmcgdGhlIHZhbHVlcyBpZiBJIGNvdWxkIGdldCBteSANCj4gaGFuZHMgb24gdGhlIHJlYWRl
ciBhbmQgcmUtaW1wbGVtZW50IGNvbXB1dGUoKS4gQnV0IHRoaXMgZG9lcyBub3Qgc2VlbSANCj4g
cG9zc2libGUgdG8gZG8gdGhyb3VnaCBleHRlbnNpb24gYmVjYXVzZSBib3RoIHRoZSANCj4gTmV3
SGFkb29wclJERC5jb25mQnJvYWRjYXN0IGFuZCBOZXdIYWRvb3BQYXJ0aXRpb24gYXJlIHByaXZh
dGUuIFRoZXJlICANCj4gZG9lcyBub3Qgc2VlbSB0byBiZSBhIGNob2ljZSBidXQgdG8gY29weS9w
YXN0ZSBleHRlbmQgdGhlIA0KPiBOZXdIYWRvb3BSREQuDQo+DQo+IFRoZSB0d28gc29sdXRpb25z
IHRoYXQgYXJlIGFwcGFyZW50IGFyZToNCj4gMS4gcmVtb3ZlIHRob3NlIHByaXZhdGUgbW9kaWZp
ZXJzDQo+IDIuIGZhY3RvciBvdXQgcmVhZGVyIGNyZWF0aW9uIHRvIGEgbWV0aG9kIHRoYXQgY2Fu
IGJlIHVzZWQgdG8gDQo+IHJlaW1wbGVtZW50DQo+IGNvbXB1dGUoKSBpbiBhIHN1Yi1jbGFzcw0K
Pg0KPiBJIHdvdWxkIGJlIGN1cmlvdXMgdG8gaGVhciBpZiBhbnlib2R5IGhhZC9oYXMgc2ltaWxh
ciBwcm9ibGVtIGFuZCBhbnkgDQo+IHRob3VnaHRzIG9uIHRoZSBpc3N1ZS4gSWYgeW91IHRoaW5r
IHRoZXJlIGlzIFBSIGluIHRoaXMgSeKAmWQgYmUgaGFwcHkgDQo+IHRvIGNvZGUgaXQgdXAgYW5k
IHN1Ym1pdCBpdC4NCj4NCj4NCj4gVGhhbmsgeW91DQo+IC0tDQo+IEV1Z2VuZSBDaGVpcGVzaA0K
Pg0KPg0KPg0KPiAtLQ0KPiBWaWV3IHRoaXMgbWVzc2FnZSBpbiBjb250ZXh0Og0KPiBodHRwOi8v
YXBhY2hlLXNwYXJrLWRldmVsb3BlcnMtbGlzdC4xMDAxNTUxLm4zLm5hYmJsZS5jb20vcHJlLWZp
bHRlcmVkDQo+IC1oYWRvb3AtUkRELXVzZS1jYXNlLXRwNzQ4NC5odG1sIFNlbnQgZnJvbSB0aGUg
QXBhY2hlIFNwYXJrIERldmVsb3BlcnMgDQo+IExpc3QgbWFpbGluZyBsaXN0IGFyY2hpdmUgYXQg
TmFiYmxlLmNvbS4NCj4NCg==

From dev-return-8625-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 18:44:56 2014
Return-Path: <dev-return-8625-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8BACB11743
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 18:44:56 +0000 (UTC)
Received: (qmail 5422 invoked by uid 500); 29 Jul 2014 18:44:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5382 invoked by uid 500); 29 Jul 2014 18:44:55 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5348 invoked by uid 99); 29 Jul 2014 18:44:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 18:44:55 +0000
X-ASF-Spam-Status: No, hits=3.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 18:44:54 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so100578qaq.10
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 11:44:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=Od/3hrIaxQp9FYmeu31TlQHS5b0zMTyQjRFQZFxe9V8=;
        b=PshwAoCq+V7awCwyRBTSbzt0nHI0JaYkEcDkn2KBvmefy120YtRLpHNGgEOFshqsYA
         y1bKj45KgZcPdV8RZZZJq4c/5KziSHkjOooNIJMVOaoh8ieKgZBxWmp9h/fZXATsAebk
         czxEdqFFVMR4+fRhDRB3qbbyAkDMiXbHMUdXDNO6ZNMm+VSrVQeXFm4wtxu0QJHIsdpk
         zbzgsKfd4Bj+BWN7fs5r1PMO5ZxFl36cn7/UsgDqEfNuxR0gUz07oXKAUyrMyjx90JQv
         nkVfANg5in0JbTs4r3GFvoIub0B7qAVlUEalHJD9Uu+8EsoggzOqW6RF9AWgxLs0Fkuj
         hZJQ==
X-Gm-Message-State: ALoCoQn0kxho63R3gSRrAw2QMoA/uGmMaK4zjm8Q3bvSFclKahvZcm2hlWpTBGNOUmzkLTo1RNwq
X-Received: by 10.140.92.20 with SMTP id a20mr1834655qge.0.1406659468448; Tue,
 29 Jul 2014 11:44:28 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Tue, 29 Jul 2014 11:44:07 -0700 (PDT)
In-Reply-To: <C434A3773D08A842B26FED6A1BA2E6546D14DFFD@SJCEML702-CHM.china.huawei.com>
References: <1406216458354-7484.post@n3.nabble.com> <CAPh_B=aBVCNZDqU1dk9n5puAZHQ+iwsMK5PV2Ptui4L1C-fr_w@mail.gmail.com>
 <C434A3773D08A842B26FED6A1BA2E6546D14DFFD@SJCEML702-CHM.china.huawei.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 29 Jul 2014 11:44:07 -0700
Message-ID: <CAPh_B=ahdo4sXOOzHvi6+F7c=Tx+sqCpjE0ZmjpAo8-W8c5W1Q@mail.gmail.com>
Subject: Re: pre-filtered hadoop RDD use case
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113ab95af0307c04ff596dcd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113ab95af0307c04ff596dcd
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I am not sure if I agree that it lacks the mechanism to do pushdowns.

Hadoop InputFormat itself provides some basic mechanism to push down
predicates already. The HBase InputFormat already implements it. In Spark,
you can also run arbitrary user code, and you can decide what to do. You
can also just subclass RDD to deal with arbitrary input sources. In the
future, we will build a more standard API to interface with external stores
in SchemaRDD.

The topic of discussion here is whether Eugene can reuse as much of
HadoopRDD/NewHadoopRDD as possible. We can certainly make the HadoopRDD
interface more pluggable, but that'd require opening up the internals of
that class and stabilize the API. I am not sure if it is something we'd
want to do in a hurry, because there is a clear workaround right now
(subclass RDD) and it is very hard to change that once the project is
committed to that API.






On Tue, Jul 29, 2014 at 11:35 AM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com>
wrote:

> PartitionPruningRDD.scala still only handles, as said, the partition
> portion of the issue.
>
> On the "record pruning" portion, although cheap fixes could be available
> for this issue as reported, but I believe a
> fundamental issue is lack of a mechanism of processing merging/pushdown.
> Given the popularity of columnar, and even more intelligent, data stores,
> it makes sense to support some "post processing" before a RDD is formed.
> This "post processing" could be performed by the RDD itself in compute();
> or it could be performed by some data store which supports such pushdowns=
.
> In the later case, such processing info should be made available to the
> data store RDD to pass on to the stores.
>
> For instance, FilteredRDD requires the parent to materialize the record
> fully before it can start its own processing. If it could be "merged" wit=
h
> its parent, a much smaller RDD footprint would result.
>
> "Pipelined execution" is mentioned for NarrowDependency in code, but no
> implementation seems to be in place, and more optimization is desired
> beyond just record-oriented execution pipelining.
>
>
>
> -----Original Message-----
> From: Reynold Xin [mailto:rxin@databricks.com]
> Sent: Tuesday, July 29, 2014 12:55 AM
> To: dev@spark.apache.org
> Subject: Re: pre-filtered hadoop RDD use case
>
> Would something like this help?
>
>
> https://github.com/apache/spark/blob/master/core/src/main/scala/org/apach=
e/spark/rdd/PartitionPruningRDD.scala
>
>
>
>
> On Thu, Jul 24, 2014 at 8:40 AM, Eugene Cheipesh <echeipesh@gmail.com>
> wrote:
>
> > Hello,
> >
> > I have an interesting use case for a pre-filtered RDD. I have two
> > solutions that I am not entirly happy with and would like to get some
> > feedback and thoughts. Perhaps it is a use case that could be more
> > explicitly supported in Spark.
> >
> > My data has well defined semantics for they key values that I can use
> > to pre-filter an RDD to exclude those partitions and records that I
> > will not need from being loaded at all. In most cases this is
> significant savings.
> >
> > Essentially the dataset is geographic image tiles, as you would see on
> > google maps. The entire dataset could be huge, covering an entire
> > continent at high resolution. But if I want to work with a subset,
> > lets say a single city, it makes no sense for me to load all the
> > partitions into memory just so I can filter them as a first step.
> >
> > First attempt was to extent NewHadoopRDD as follows:
> >
> > abstract class PreFilteredHadoopRDD[K, V](
> >     sc : SparkContext,
> >     inputFormatClass: Class[_ <: InputFormat[K, V]],
> >     keyClass: Class[K],
> >     valueClass: Class[V],
> >     @transient conf: Configuration)
> >   extends NewHadoopRDD(sc, inputFormatClass, keyClass, valueClass,
> > conf) {
> >   /** returns true if specific partition has relevant keys */
> >   def includePartition(p: Partition): Boolean
> >
> >   /** returns true if the specific key in the partition passes the
> > filter */
> >   def includeKey(key: K): Boolean
> >
> >   override def getPartitions: Array[Partition] =3D {
> >     val partitions =3D super.getPartitions
> >     partitions.filter(includePartition)
> >   }
> >
> >   override def compute(theSplit: Partition, context: TaskContext) =3D {
> >     val ii =3D super.compute(theSplit, context)
> >     new InterruptibleIterator(ii.context, ii.delegate.filter{case
> > (k,v) =3D>
> > includeKey(k)})
> >   }
> > }
> >
> > NewHadoopRDD for reference:
> >
> > https://github.com/apache/spark/blob/master/core/src/main/scala/org/ap
> > ache/spark/rdd/NewHadoopRDD.scala
> >
> > This is nice and handles the partition portion of the issue well
> > enough, but by the time the iterator is created by super.compute there
> > is no way avoid reading the values from records that do not pass my
> > filter.
> >
> > Since I am actually using =E2=80=98SequenceFileInputFormat=E2=80=99 as =
my InputFormat
> > I can do better, and avoid deserializing the values if I could get my
> > hands on the reader and re-implement compute(). But this does not seem
> > possible to do through extension because both the
> > NewHadooprRDD.confBroadcast and NewHadoopPartition are private. There
> > does not seem to be a choice but to copy/paste extend the
> > NewHadoopRDD.
> >
> > The two solutions that are apparent are:
> > 1. remove those private modifiers
> > 2. factor out reader creation to a method that can be used to
> > reimplement
> > compute() in a sub-class
> >
> > I would be curious to hear if anybody had/has similar problem and any
> > thoughts on the issue. If you think there is PR in this I=E2=80=99d be =
happy
> > to code it up and submit it.
> >
> >
> > Thank you
> > --
> > Eugene Cheipesh
> >
> >
> >
> > --
> > View this message in context:
> > http://apache-spark-developers-list.1001551.n3.nabble.com/pre-filtered
> > -hadoop-RDD-use-case-tp7484.html Sent from the Apache Spark Developers
> > List mailing list archive at Nabble.com.
> >
>

--001a113ab95af0307c04ff596dcd--

From dev-return-8626-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 19:43:35 2014
Return-Path: <dev-return-8626-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 566E7119A9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 19:43:35 +0000 (UTC)
Received: (qmail 21829 invoked by uid 500); 29 Jul 2014 19:43:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21773 invoked by uid 500); 29 Jul 2014 19:43:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 21756 invoked by uid 99); 29 Jul 2014 19:43:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 19:43:34 +0000
X-ASF-Spam-Status: No, hits=-1.0 required=10.0
	tests=RCVD_IN_DNSWL_MED,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of Yan.Zhou.sc@huawei.com designates 206.16.17.72 as permitted sender)
Received: from [206.16.17.72] (HELO dfwrgout.huawei.com) (206.16.17.72)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 19:43:30 +0000
Received: from 172.18.9.243 (EHLO dfweml706-chm.china.huawei.com) ([172.18.9.243])
	by dfwrg02-dlp.huawei.com (MOS 4.3.7-GA FastPath queued)
	with ESMTP id AWF81771;
	Tue, 29 Jul 2014 14:43:09 -0500 (CDT)
Received: from SJCEML703-CHM.china.huawei.com (10.212.94.49) by
 dfweml706-chm.china.huawei.com (10.193.5.225) with Microsoft SMTP Server
 (TLS) id 14.3.158.1; Tue, 29 Jul 2014 12:43:08 -0700
Received: from SJCEML702-CHM.china.huawei.com ([169.254.4.137]) by
 SJCEML703-CHM.china.huawei.com ([169.254.5.229]) with mapi id 14.03.0158.001;
 Tue, 29 Jul 2014 12:43:04 -0700
From: "Yan Zhou.sc" <Yan.Zhou.sc@huawei.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Subject: RE: pre-filtered hadoop RDD use case
Thread-Topic: pre-filtered hadoop RDD use case
Thread-Index: AQHPp1W/O/dW09aknEq3BH13Ug+AoZu3K2SAgAAZlECAAJvcgP//jSVw
Date: Tue, 29 Jul 2014 19:43:04 +0000
Message-ID: <C434A3773D08A842B26FED6A1BA2E6546D14E0BE@SJCEML702-CHM.china.huawei.com>
References: <1406216458354-7484.post@n3.nabble.com>
 <CAPh_B=aBVCNZDqU1dk9n5puAZHQ+iwsMK5PV2Ptui4L1C-fr_w@mail.gmail.com>
 <C434A3773D08A842B26FED6A1BA2E6546D14DFFD@SJCEML702-CHM.china.huawei.com>
 <CAPh_B=ahdo4sXOOzHvi6+F7c=Tx+sqCpjE0ZmjpAo8-W8c5W1Q@mail.gmail.com>
In-Reply-To: <CAPh_B=ahdo4sXOOzHvi6+F7c=Tx+sqCpjE0ZmjpAo8-W8c5W1Q@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach: 
X-MS-TNEF-Correlator: 
x-originating-ip: [10.193.36.100]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-CFilter-Loop: Reflected
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-8627-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 23:35:22 2014
Return-Path: <dev-return-8627-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B0C3B1141D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 23:35:22 +0000 (UTC)
Received: (qmail 35967 invoked by uid 500); 29 Jul 2014 23:35:22 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 35910 invoked by uid 500); 29 Jul 2014 23:35:22 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 35897 invoked by uid 99); 29 Jul 2014 23:35:21 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:35:21 +0000
X-ASF-Spam-Status: No, hits=3.9 required=10.0
	tests=HTML_MESSAGE,HTML_OBFUSCATE_20_30,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mark@clearstorydata.com designates 74.125.82.45 as permitted sender)
Received: from [74.125.82.45] (HELO mail-wg0-f45.google.com) (74.125.82.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:35:16 +0000
Received: by mail-wg0-f45.google.com with SMTP id x12so368421wgg.4
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 16:34:55 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=E3E9dE8K/BeqZP4oTB8fAZVR6qJOfSH0/lvfLqKXU6c=;
        b=DLXmZW5cC+F/m2P5QNh1HSvnqhRDQeQiQzALcUB96VXrETpQSggWCvR9YXr/8R6hWW
         vzdUulfgTTCmFA4hSdcpuSVCtHS/8i3roknctGmJNlaO7nzqSlXjBkKkxQB11xd51kke
         8RU2wzcDR8Noa9uXXLIsMcxSRnv66IAGXQNmFFBJiAd+bnhjDjamEFt0HrTh6rck0iF+
         8F/EC+os2VYoaADODNuUFUF5FK2hbEHWt3T1B13qlaTi3v2Uk820zoxmV2Wk5mow27ST
         moa7mKj9HE8MXuzeDCLUJ+Vo8akbfQFPHg+4CkOeyhTABDQaqUdZvA8iPhfvDDjnc696
         Lh9A==
X-Gm-Message-State: ALoCoQkf5N2zZTqHO+1Mmqab6r1NDPgz3IxCwyOHdDSgeDvKHE/M2hybgJtsX6zimTn8TRKrZ2ts
MIME-Version: 1.0
X-Received: by 10.194.134.228 with SMTP id pn4mr297998wjb.111.1406676895028;
 Tue, 29 Jul 2014 16:34:55 -0700 (PDT)
Received: by 10.216.161.68 with HTTP; Tue, 29 Jul 2014 16:34:54 -0700 (PDT)
Date: Tue, 29 Jul 2014 16:34:54 -0700
Message-ID: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
Subject: JIRA content request
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01228bb0a47caf04ff5d7c02
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01228bb0a47caf04ff5d7c02
Content-Type: text/plain; charset=UTF-8

Of late, I've been coming across quite a few pull requests and associated
JIRA issues that contain nothing indicating their purpose beyond a pretty
minimal description of what the pull request does.  On the pull request
itself, a reference to the corresponding JIRA in the title combined with a
description that gives us a sketch of what the PR does is fine, but if
there is no description in at least the JIRA of *why* you think some change
to Spark would be good, then it often makes getting started on code reviews
a little harder for those of us doing the reviews.  So, I'm requesting that
if you are submitting a JIRA or pull request for something that isn't
obviously a bug or bug fix, you please include some sort of motivation in
at least the JIRA body so that the reviewers can more easily get through
the head-scratching phase of trying to figure out why Spark might be
improved by merging a pull request.

--089e01228bb0a47caf04ff5d7c02--

From dev-return-8628-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 23:36:49 2014
Return-Path: <dev-return-8628-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 16F7B1142D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 23:36:49 +0000 (UTC)
Received: (qmail 40166 invoked by uid 500); 29 Jul 2014 23:36:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 40110 invoked by uid 500); 29 Jul 2014 23:36:48 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 40097 invoked by uid 99); 29 Jul 2014 23:36:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:36:48 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.192.49] (HELO mail-qg0-f49.google.com) (209.85.192.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:36:43 +0000
Received: by mail-qg0-f49.google.com with SMTP id j107so543671qga.8
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 16:36:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=qwpLef24bVGR9T9B0dOmz+mZQWXQjtNofb/th8kTsHQ=;
        b=L7xerW7g8xkPHpvPLM+L3HOear9cvNmlYJRDHAc7wwB27PeOmYDtFvtGjju1K4Hgnl
         nBi2oAZm4X2viGp0q2hIojW7PrShJy/Lp2URABu/Jt9cY/okCCXmvTU7YqEv/rERsX6n
         x49Y91cYs+ZpukKDOTneIMPXdi+0dXYQj/0vVB1tifiRssGHs0aszvZqyEgBClMBV1TM
         eaRJ2TACBAXRUmxIl06HKvWVmNUiNGMLZmNnqsOEZNnPv2feSJdLCfZMIykjlIAjfJlg
         t6hEU4qfhyO6iGrd4R9bTnhHjuSEGo5Zw+EIvpqHJQDPil4DMObZssAR8u2fxtv64DDX
         y65w==
X-Gm-Message-State: ALoCoQmOFvs0sTAkJIuZD9uKuyllVQT+Y5immUWPQyyT2qDoN8CVH/0xXUCfOcMVUqCMXDqdlx57
X-Received: by 10.140.92.235 with SMTP id b98mr290145qge.97.1406676982667;
 Tue, 29 Jul 2014 16:36:22 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.96.98.100 with HTTP; Tue, 29 Jul 2014 16:36:02 -0700 (PDT)
In-Reply-To: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
References: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
From: Reynold Xin <rxin@databricks.com>
Date: Tue, 29 Jul 2014 16:36:02 -0700
Message-ID: <CAPh_B=bz=KEHj1N8=fcE6ZDfKMmdFh=npbkNCjApTyJ8j8VVbw@mail.gmail.com>
Subject: Re: JIRA content request
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113a51b4de6c1504ff5d813d
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113a51b4de6c1504ff5d813d
Content-Type: text/plain; charset=UTF-8

+1 on this.


On Tue, Jul 29, 2014 at 4:34 PM, Mark Hamstra <mark@clearstorydata.com>
wrote:

> Of late, I've been coming across quite a few pull requests and associated
> JIRA issues that contain nothing indicating their purpose beyond a pretty
> minimal description of what the pull request does.  On the pull request
> itself, a reference to the corresponding JIRA in the title combined with a
> description that gives us a sketch of what the PR does is fine, but if
> there is no description in at least the JIRA of *why* you think some change
> to Spark would be good, then it often makes getting started on code reviews
> a little harder for those of us doing the reviews.  So, I'm requesting that
> if you are submitting a JIRA or pull request for something that isn't
> obviously a bug or bug fix, you please include some sort of motivation in
> at least the JIRA body so that the reviewers can more easily get through
> the head-scratching phase of trying to figure out why Spark might be
> improved by merging a pull request.
>

--001a113a51b4de6c1504ff5d813d--

From dev-return-8629-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 23:49:38 2014
Return-Path: <dev-return-8629-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EDAA0114A7
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 23:49:37 +0000 (UTC)
Received: (qmail 86316 invoked by uid 500); 29 Jul 2014 23:49:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86256 invoked by uid 500); 29 Jul 2014 23:49:36 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 86240 invoked by uid 99); 29 Jul 2014 23:49:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:49:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sowen@cloudera.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:49:34 +0000
Received: by mail-vc0-f182.google.com with SMTP id hy4so678750vcb.41
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 16:49:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=SDclm9+yt81qeMsXZRrvGygfMImCUgeOTqRxuY7TQPc=;
        b=PM7tSSFJD3Xm6+N4YKbcLT52w5ELBIbwqJrn4VoYl1ZhKtyniKV0GsdGNncgcxZPY0
         9JEFrOc2jEOUKEopoCRvH0NADdl2NL9lu2pME0Ng78/0U8GHKH4dpGEHOILbjL77qW0s
         0UM9S/Ks83fAWyRk570WYK1wFuxRXOa9MHJodOkwkHFgKbZ27II5CX9795OuGImUg92k
         UjI8goxLPNfoBRDgQeoz5vkSNNm9nQV1DxlHY5vdZ0iZxVPM42P5QJrjKQjEAwccJ/zI
         oGu2/5hpSX2mN6dv53dYNIh0PVSeOYeT/MIvuOZKRxKGJ79oWNNf+ry2pbi4GQZPjgtz
         0FPg==
X-Gm-Message-State: ALoCoQljUdl2FjiQcEajlzu4ABPzvikTo+rtGIii5wJxONNOGDdoqCQOpw9Nx3UU471P90hoSXhQ
X-Received: by 10.221.47.9 with SMTP id uq9mr374586vcb.48.1406677748382; Tue,
 29 Jul 2014 16:49:08 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.197.196 with HTTP; Tue, 29 Jul 2014 16:48:48 -0700 (PDT)
In-Reply-To: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
References: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
From: Sean Owen <sowen@cloudera.com>
Date: Wed, 30 Jul 2014 00:48:48 +0100
Message-ID: <CAMAsSdKj6pHKwyeyKcqiYzwqk9DP8nAjwwZVVvLnwqCDeTw7Dw@mail.gmail.com>
Subject: Re: JIRA content request
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

How about using a JIRA status like "Documentation Required" to mean
"burden's on you to elaborate with a motivation and/or PR". This could
both prompt people to do so, and also let one see when a JIRA has been
waiting on the reporter for months, rather than simply never been
looked at, and should thus time out and be closed. Both of these would
probably help the JIRA backlog.

On Wed, Jul 30, 2014 at 12:34 AM, Mark Hamstra <mark@clearstorydata.com> wrote:
> Of late, I've been coming across quite a few pull requests and associated
> JIRA issues that contain nothing indicating their purpose beyond a pretty
> minimal description of what the pull request does.  On the pull request
> itself, a reference to the corresponding JIRA in the title combined with a
> description that gives us a sketch of what the PR does is fine, but if
> there is no description in at least the JIRA of *why* you think some change
> to Spark would be good, then it often makes getting started on code reviews
> a little harder for those of us doing the reviews.  So, I'm requesting that
> if you are submitting a JIRA or pull request for something that isn't
> obviously a bug or bug fix, you please include some sort of motivation in
> at least the JIRA body so that the reviewers can more easily get through
> the head-scratching phase of trying to figure out why Spark might be
> improved by merging a pull request.

From dev-return-8630-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Tue Jul 29 23:51:29 2014
Return-Path: <dev-return-8630-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4CDBD114CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 29 Jul 2014 23:51:29 +0000 (UTC)
Received: (qmail 95862 invoked by uid 500); 29 Jul 2014 23:51:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95798 invoked by uid 500); 29 Jul 2014 23:51:28 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 95740 invoked by uid 99); 29 Jul 2014 23:51:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:51:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_HELO_PASS,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of eerlands@redhat.com designates 209.132.183.39 as permitted sender)
Received: from [209.132.183.39] (HELO mx6-phx2.redhat.com) (209.132.183.39)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 29 Jul 2014 23:51:23 +0000
Received: from zmail12.collab.prod.int.phx2.redhat.com (zmail12.collab.prod.int.phx2.redhat.com [10.5.83.14])
	by mx6-phx2.redhat.com (8.14.4/8.14.4) with ESMTP id s6TNp28E020935
	for <dev@spark.apache.org>; Tue, 29 Jul 2014 19:51:02 -0400
Date: Tue, 29 Jul 2014 19:51:02 -0400 (EDT)
From: Erik Erlandson <eje@redhat.com>
Reply-To: Erik Erlandson <eje@redhat.com>
To: dev@spark.apache.org
Message-ID: <2115361831.15023040.1406677862264.JavaMail.zimbra@redhat.com>
In-Reply-To: <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
References: <447620292.11027248.1405955629875.JavaMail.zimbra@redhat.com> <702095265.11034458.1405956251006.JavaMail.zimbra@redhat.com> <CA+-p3AFBxHLENpEZ077CYPxHkqHQgR9k_5QneA+G8TzUCMDurg@mail.gmail.com> <CAAsvFP=oyiE2KW3uEvoP8M8ERASHfU1Nb0mjxg-DY-vCd_Hxjw@mail.gmail.com>
Subject: Re: RFC: Supporting the Scala drop Method for Spark RDDs
MIME-Version: 1.0
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit
X-Originating-IP: [10.5.82.6]
X-Mailer: Zimbra 8.0.6_GA_5922 (ZimbraWebClient - GC36 (Linux)/8.0.6_GA_5922)
Thread-Topic: Supporting the Scala drop Method for Spark RDDs
Thread-Index: RPYyT9sPe4gRPkMCc9yiBRATAHQMkg==
X-Virus-Checked: Checked by ClamAV on apache.org



----- Original Message -----
> Sure, drop() would be useful, but breaking the "transformations are lazy;
> only actions launch jobs" model is abhorrent -- which is not to say that we
> haven't already broken that model for useful operations (cf.
> RangePartitioner, which is used for sorted RDDs), but rather that each such
> exception to the model is a significant source of pain that can be hard to
> work with or work around.
> 
> I really wouldn't like to see another such model-breaking transformation
> added to the API.  On the other hand, being able to write transformations
> with dependencies on these kind of "internal" jobs is sometimes very
> useful, so a significant reworking of Spark's Dependency model that would
> allow for lazily running such internal jobs and making the results
> available to subsequent stages may be something worth pursuing.


It turns out that drop can be implemented as a proper lazy transform.  I discuss how that works here:
http://erikerlandson.github.io/blog/2014/07/29/deferring-spark-actions-to-lazy-transforms-with-the-promise-rdd/

I updated the PR with this lazy implementation.




> 
> 
> On Mon, Jul 21, 2014 at 8:27 AM, Andrew Ash <andrew@andrewash.com> wrote:
> 
> > Personally I'd find the method useful -- I've often had a .csv file with a
> > header row that I want to drop so filter it out, which touches all
> > partitions anyway.  I don't have any comments on the implementation quite
> > yet though.
> >
> >
> > On Mon, Jul 21, 2014 at 8:24 AM, Erik Erlandson <eje@redhat.com> wrote:
> >
> > > A few weeks ago I submitted a PR for supporting rdd.drop(n), under
> > > SPARK-2315:
> > > https://issues.apache.org/jira/browse/SPARK-2315
> > >
> > > Supporting the drop method would make some operations convenient, however
> > > it forces computation of >= 1 partition of the parent RDD, and so it
> > would
> > > behave like a "partial action" that returns an RDD as the result.
> > >
> > > I wrote up a discussion of these trade-offs here:
> > >
> > >
> > http://erikerlandson.github.io/blog/2014/07/20/some-implications-of-supporting-the-scala-drop-method-for-spark-rdds/
> > >
> >
> 

From dev-return-8631-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 00:12:00 2014
Return-Path: <dev-return-8631-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C352311590
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 00:12:00 +0000 (UTC)
Received: (qmail 32290 invoked by uid 500); 30 Jul 2014 00:12:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32234 invoked by uid 500); 30 Jul 2014 00:12:00 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 32211 invoked by uid 99); 30 Jul 2014 00:11:59 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 00:11:59 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.212.172 as permitted sender)
Received: from [209.85.212.172] (HELO mail-wi0-f172.google.com) (209.85.212.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 00:11:55 +0000
Received: by mail-wi0-f172.google.com with SMTP id n3so6566802wiv.11
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 17:11:34 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=pH6ZDGoNebD9TIn4dhJ8fMmMjXhRtm8zpuCzjjBEMu4=;
        b=YyvhKJBOmC9whI89RzBPYx7IaPRpHV5YLq+TAWhsml/QeM1VJH6NP8F77m6BktijLH
         N1MzEFxhc8rjA+uiKSzM03aM9QqElG41FOD6NuO5bCuOMBA6gcej56+auEXL9Z6bD2LD
         MmpflcCwqT+MaDbJ/pY9sxQcHUAeUSbmAxXLtL6lTax0k3bYi2FdxcQ4b2X4Af/f+rN7
         sMaFZRMKcIEqEBHQn887n8KiJJE0dIAcAUjxV6x1hOzjanEA28f8c6PzhcK94hsfSzmN
         E+t3BDXpLUoaKPlugA3CpuT/FQuWD2H0ylms1Y2ze09u9ZVwe76bkUzEVT2bQ9yJQ60V
         yKBA==
X-Received: by 10.194.6.134 with SMTP id b6mr809401wja.64.1406679094761; Tue,
 29 Jul 2014 17:11:34 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.180.92.232 with HTTP; Tue, 29 Jul 2014 17:10:54 -0700 (PDT)
In-Reply-To: <CAMAsSdKj6pHKwyeyKcqiYzwqk9DP8nAjwwZVVvLnwqCDeTw7Dw@mail.gmail.com>
References: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
 <CAMAsSdKj6pHKwyeyKcqiYzwqk9DP8nAjwwZVVvLnwqCDeTw7Dw@mail.gmail.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Tue, 29 Jul 2014 20:10:54 -0400
Message-ID: <CAOhmDzedFqMsV46dfeXaTTnHk-TO0aRvBkda7kdxZqmvO_wc_w@mail.gmail.com>
Subject: Re: JIRA content request
To: dev <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=047d7b450116c1a1f204ff5dff9f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b450116c1a1f204ff5dff9f
Content-Type: text/plain; charset=UTF-8

+1 on using JIRA workflows to manage the backlog, and +9000 on having
decent descriptions for all JIRA issues.


On Tue, Jul 29, 2014 at 7:48 PM, Sean Owen <sowen@cloudera.com> wrote:

> How about using a JIRA status like "Documentation Required" to mean
> "burden's on you to elaborate with a motivation and/or PR". This could
> both prompt people to do so, and also let one see when a JIRA has been
> waiting on the reporter for months, rather than simply never been
> looked at, and should thus time out and be closed. Both of these would
> probably help the JIRA backlog.
>
> On Wed, Jul 30, 2014 at 12:34 AM, Mark Hamstra <mark@clearstorydata.com>
> wrote:
> > Of late, I've been coming across quite a few pull requests and associated
> > JIRA issues that contain nothing indicating their purpose beyond a pretty
> > minimal description of what the pull request does.  On the pull request
> > itself, a reference to the corresponding JIRA in the title combined with
> a
> > description that gives us a sketch of what the PR does is fine, but if
> > there is no description in at least the JIRA of *why* you think some
> change
> > to Spark would be good, then it often makes getting started on code
> reviews
> > a little harder for those of us doing the reviews.  So, I'm requesting
> that
> > if you are submitting a JIRA or pull request for something that isn't
> > obviously a bug or bug fix, you please include some sort of motivation in
> > at least the JIRA body so that the reviewers can more easily get through
> > the head-scratching phase of trying to figure out why Spark might be
> > improved by merging a pull request.
>

--047d7b450116c1a1f204ff5dff9f--

From dev-return-8632-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 00:41:49 2014
Return-Path: <dev-return-8632-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BC50911697
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 00:41:49 +0000 (UTC)
Received: (qmail 23410 invoked by uid 500); 30 Jul 2014 00:41:49 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23351 invoked by uid 500); 30 Jul 2014 00:41:49 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23339 invoked by uid 99); 30 Jul 2014 00:41:48 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 00:41:48 +0000
X-ASF-Spam-Status: No, hits=2.5 required=10.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.177 as permitted sender)
Received: from [209.85.192.177] (HELO mail-pd0-f177.google.com) (209.85.192.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 00:41:44 +0000
Received: by mail-pd0-f177.google.com with SMTP id p10so490008pdj.22
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 17:41:24 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:message-id:in-reply-to:references:subject:mime-version
         :content-type;
        bh=Q8NF/T1RkcoKoIA25wgyK7pBF6ZDhqBCTyNVRlWF7t4=;
        b=x4vjWZJdQ146pCapLJRBUbW2TJVdAX/0uIEBHfNxkLIrowSTOwVI4q465v9jEVugFS
         L85a/gDv3RFqWY8fwZ0MNooTwkRfHM+JuEpjxu9PppO4irJ0DuSEt2MeSrIkfXt8l2wK
         fhXmEy2dSASE93BDTSU4nncjSEYpKTPJG8XEMSaJ8LRRUMVUyuJBizNkyjY3zoYLayIo
         mCbJ86VIcCgRHQf2tXCzRwtAWAD0gHlscOjj9HQZHeMgA7KahaRontb0aVI25Z480lT3
         og5EaU7Vzw7CUxvwx+DCGXBftCokP90PnYCH8Drhkf1Jpu1fcGZAHZxPhO8BegVAosRD
         BPcg==
X-Received: by 10.68.215.68 with SMTP id og4mr647833pbc.112.1406680883905;
        Tue, 29 Jul 2014 17:41:23 -0700 (PDT)
Received: from mbp-3 (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id pl10sm683345pdb.27.2014.07.29.17.41.22
        for <multiple recipients>
        (version=TLSv1.2 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 29 Jul 2014 17:41:23 -0700 (PDT)
Date: Tue, 29 Jul 2014 17:41:20 -0700
From: Matei Zaharia <matei.zaharia@gmail.com>
To: Nicholas Chammas <nicholas.chammas@gmail.com>, dev@spark.apache.org
Message-ID: <etPan.53d83f30.1f16e9e8.5ddc@mbp-3>
In-Reply-To: <CAOhmDzedFqMsV46dfeXaTTnHk-TO0aRvBkda7kdxZqmvO_wc_w@mail.gmail.com>
References: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
 <CAMAsSdKj6pHKwyeyKcqiYzwqk9DP8nAjwwZVVvLnwqCDeTw7Dw@mail.gmail.com>
 <CAOhmDzedFqMsV46dfeXaTTnHk-TO0aRvBkda7kdxZqmvO_wc_w@mail.gmail.com>
Subject: Re: JIRA content request
X-Mailer: Airmail (247)
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="53d83f30_1190cde7_5ddc"
X-Virus-Checked: Checked by ClamAV on apache.org

--53d83f30_1190cde7_5ddc
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: 7bit
Content-Disposition: inline

I agree as well. FWIW sometimes I've seen this happen due to language barriers, i.e. contributors whose primary language is not English, but we need more motivation for each change.

On July 29, 2014 at 5:12:01 PM, Nicholas Chammas (nicholas.chammas@gmail.com) wrote:

+1 on using JIRA workflows to manage the backlog, and +9000 on having 
decent descriptions for all JIRA issues. 


On Tue, Jul 29, 2014 at 7:48 PM, Sean Owen <sowen@cloudera.com> wrote: 

> How about using a JIRA status like "Documentation Required" to mean 
> "burden's on you to elaborate with a motivation and/or PR". This could 
> both prompt people to do so, and also let one see when a JIRA has been 
> waiting on the reporter for months, rather than simply never been 
> looked at, and should thus time out and be closed. Both of these would 
> probably help the JIRA backlog. 
> 
> On Wed, Jul 30, 2014 at 12:34 AM, Mark Hamstra <mark@clearstorydata.com> 
> wrote: 
> > Of late, I've been coming across quite a few pull requests and associated 
> > JIRA issues that contain nothing indicating their purpose beyond a pretty 
> > minimal description of what the pull request does. On the pull request 
> > itself, a reference to the corresponding JIRA in the title combined with 
> a 
> > description that gives us a sketch of what the PR does is fine, but if 
> > there is no description in at least the JIRA of *why* you think some 
> change 
> > to Spark would be good, then it often makes getting started on code 
> reviews 
> > a little harder for those of us doing the reviews. So, I'm requesting 
> that 
> > if you are submitting a JIRA or pull request for something that isn't 
> > obviously a bug or bug fix, you please include some sort of motivation in 
> > at least the JIRA body so that the reviewers can more easily get through 
> > the head-scratching phase of trying to figure out why Spark might be 
> > improved by merging a pull request. 
> 

--53d83f30_1190cde7_5ddc--


From dev-return-8633-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 00:49:33 2014
Return-Path: <dev-return-8633-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4575D116BC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 00:49:33 +0000 (UTC)
Received: (qmail 34790 invoked by uid 500); 30 Jul 2014 00:49:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34729 invoked by uid 500); 30 Jul 2014 00:49:32 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34715 invoked by uid 99); 30 Jul 2014 00:49:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 00:49:32 +0000
X-ASF-Spam-Status: No, hits=0.3 required=10.0
	tests=FREEMAIL_REPLY,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.44 as permitted sender)
Received: from [74.125.82.44] (HELO mail-wg0-f44.google.com) (74.125.82.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 00:49:28 +0000
Received: by mail-wg0-f44.google.com with SMTP id m15so409573wgh.15
        for <dev@spark.apache.org>; Tue, 29 Jul 2014 17:49:07 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=6SXoTcpftfHYyUvnH0oVNTQWRk1N4eAxMl0JnZ640dg=;
        b=A8aIV5pA0EIBIYW71z69NDR+7+fzqKkDthICASf+/V+iGrIeC5DKvfpYASS4TK47+3
         c1Xo1Kq4lTME//dkS8IGMEL4heU6ItxxW0Tk43JwU9EWDTXzF2bkmql8aauEf5F/L8SK
         o+07EufbLLtdVIFAogk5ZGCdbrkjMi51J4nyNXRYfSO7U3e+xwUx4CgXn4OTXAzovM3I
         +o6SrHaAnJ4pOPIpDVPlzBhlp89I8RZ+LIRggNAlKP/UwCRPKTKiITtTLsWK3OmR3fML
         rFeVzP1N/paN2HqVFrdekr12ipQi6zZ5RHEF+mxB/seAtC+MqxYjS9NLN963xHVakpCP
         Xw1w==
MIME-Version: 1.0
X-Received: by 10.194.158.101 with SMTP id wt5mr902626wjb.136.1406681347180;
 Tue, 29 Jul 2014 17:49:07 -0700 (PDT)
Received: by 10.216.130.7 with HTTP; Tue, 29 Jul 2014 17:49:07 -0700 (PDT)
In-Reply-To: <etPan.53d83f30.1f16e9e8.5ddc@mbp-3>
References: <CAAsvFPmJu1aGKZtt8Rx5PoxnLjokBGAQ60hxZQXT-s5urabDOw@mail.gmail.com>
	<CAMAsSdKj6pHKwyeyKcqiYzwqk9DP8nAjwwZVVvLnwqCDeTw7Dw@mail.gmail.com>
	<CAOhmDzedFqMsV46dfeXaTTnHk-TO0aRvBkda7kdxZqmvO_wc_w@mail.gmail.com>
	<etPan.53d83f30.1f16e9e8.5ddc@mbp-3>
Date: Tue, 29 Jul 2014 17:49:07 -0700
Message-ID: <CALuGr6auUh+ObyTW2sHOQLjCG1BjFEBPkXzEFwoOwSZ8eA-JKA@mail.gmail.com>
Subject: Re: JIRA content request
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Yup, I am seeing this in some other Apache projects as well and
usually if being asked to add more information more reporter gladly
comply as requested.

Have to diligently nudge some JIRA filers at the beginning but usually
people see that more description are better and the habit get pick up
by new contributors.

- Henry

On Tue, Jul 29, 2014 at 5:41 PM, Matei Zaharia <matei.zaharia@gmail.com> wrote:
> I agree as well. FWIW sometimes I've seen this happen due to language barriers, i.e. contributors whose primary language is not English, but we need more motivation for each change.
>
> On July 29, 2014 at 5:12:01 PM, Nicholas Chammas (nicholas.chammas@gmail.com) wrote:
>
> +1 on using JIRA workflows to manage the backlog, and +9000 on having
> decent descriptions for all JIRA issues.
>
>
> On Tue, Jul 29, 2014 at 7:48 PM, Sean Owen <sowen@cloudera.com> wrote:
>
>> How about using a JIRA status like "Documentation Required" to mean
>> "burden's on you to elaborate with a motivation and/or PR". This could
>> both prompt people to do so, and also let one see when a JIRA has been
>> waiting on the reporter for months, rather than simply never been
>> looked at, and should thus time out and be closed. Both of these would
>> probably help the JIRA backlog.
>>
>> On Wed, Jul 30, 2014 at 12:34 AM, Mark Hamstra <mark@clearstorydata.com>
>> wrote:
>> > Of late, I've been coming across quite a few pull requests and associated
>> > JIRA issues that contain nothing indicating their purpose beyond a pretty
>> > minimal description of what the pull request does. On the pull request
>> > itself, a reference to the corresponding JIRA in the title combined with
>> a
>> > description that gives us a sketch of what the PR does is fine, but if
>> > there is no description in at least the JIRA of *why* you think some
>> change
>> > to Spark would be good, then it often makes getting started on code
>> reviews
>> > a little harder for those of us doing the reviews. So, I'm requesting
>> that
>> > if you are submitting a JIRA or pull request for something that isn't
>> > obviously a bug or bug fix, you please include some sort of motivation in
>> > at least the JIRA body so that the reviewers can more easily get through
>> > the head-scratching phase of trying to figure out why Spark might be
>> > improved by merging a pull request.
>>

From dev-return-8634-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 20:13:03 2014
Return-Path: <dev-return-8634-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 16C2611A60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 20:13:03 +0000 (UTC)
Received: (qmail 31929 invoked by uid 500); 30 Jul 2014 20:13:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31868 invoked by uid 500); 30 Jul 2014 20:13:02 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 31851 invoked by uid 99); 30 Jul 2014 20:13:02 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 20:13:01 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.219.46] (HELO mail-oa0-f46.google.com) (209.85.219.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 20:12:58 +0000
Received: by mail-oa0-f46.google.com with SMTP id m1so1342408oag.19
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 13:12:32 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=vs3CGYesSi0NPbOPBe/seeScvzaiPEHQjI9VO/UXYRc=;
        b=SBmoYKK+KGIloPltnS5QlFRvvXsygblJYwQAViRwD9uPhmxeZFmXHeUIdPWOJtb1GV
         W3Yk+tstDO7D2pIIu48cJB/zJ2PAWYX3kyj4AGWdGp5BBc5sjMuI86NlJwlZ0sF+fjaC
         VjpbxOU27hNMcDwCYePpVEmuCd1/ei5gRyY+ihKsBedEwz7IuyX57LGdHjSHwUdJmEL/
         MF50iXE1T1sUiGNdbVgpT6Y2wT82h25d8rIB6HXsBvojfeyF44ONVsy8LNaBbgZpsdGy
         9Xlycl2qxJFlscpMD2IQ2e/qVcELFRnCC8pjCbYIpkW+GNEKVs6XRVn7wme7hJZticUM
         9EPw==
X-Gm-Message-State: ALoCoQk5sA+R9cWmXS7JtqXw6U15fD5/u5oDa7+9egVZfG5/3cvIXuCuZScuZeMNBKIl/ya5pVHC
MIME-Version: 1.0
X-Received: by 10.182.144.131 with SMTP id sm3mr9141122obb.3.1406751152681;
 Wed, 30 Jul 2014 13:12:32 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Wed, 30 Jul 2014 13:12:32 -0700 (PDT)
Date: Wed, 30 Jul 2014 15:12:32 -0500
Message-ID: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
Subject: replacement for SPARK_JAVA_OPTS
From: Cody Koeninger <cody@koeninger.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0158aba2be49b804ff6ec690
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158aba2be49b804ff6ec690
Content-Type: text/plain; charset=UTF-8

We were previously using SPARK_JAVA_OPTS to set java system properties via
-D.

This was used for properties that varied on a per-deployment-environment
basis, but needed to be available in the spark shell and workers.

On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been deprecated, and
replaced by spark-defaults.conf and command line arguments to spark-submit
or spark-shell.

However, setting spark.driver.extraJavaOptions and
spark.executor.extraJavaOptions in spark-defaults.conf is not a replacement
for SPARK_JAVA_OPTS:


$ cat conf/spark-defaults.conf
spark.driver.extraJavaOptions=-Dfoo.bar.baz=23

$ ./bin/spark-shell

scala> System.getProperty("foo.bar.baz")
res0: String = null


$ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"

scala> System.getProperty("foo.bar.baz")
res0: String = 23


Looking through the shell scripts for spark-submit and spark-class, I can
see why this is; parsing spark-defaults.conf from bash could be brittle.

But from an ergonomic point of view, it's a step back to go from a
set-it-and-forget-it configuration in spark-env.sh, to requiring command
line arguments.

I can solve this with an ad-hoc script to wrap spark-shell with the
appropriate arguments, but I wanted to bring the issue up to see if anyone
else had run into it,
or had any direction for a general solution (beyond parsing java properties
files from bash).

--089e0158aba2be49b804ff6ec690--

From dev-return-8635-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 20:32:53 2014
Return-Path: <dev-return-8635-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F3B4611B6F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 20:32:52 +0000 (UTC)
Received: (qmail 18605 invoked by uid 500); 30 Jul 2014 20:32:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18539 invoked by uid 500); 30 Jul 2014 20:32:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 18528 invoked by uid 99); 30 Jul 2014 20:32:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 20:32:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of vanzin@cloudera.com designates 209.85.216.51 as permitted sender)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 20:32:47 +0000
Received: by mail-qa0-f51.google.com with SMTP id k15so1519606qaq.10
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 13:32:26 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=zVROmjbavyk0mEbWeaDtSgiqZfGyrFi4ciKoIGYSExM=;
        b=f+gMh4E+hW4ZGbO4nCp85UPLrxauuw5a4z0kpOedV764GgMn55oXL68VdOAbz8oh0j
         NPOhhLKzwTwn3Nj1Zk5m4nz1ith63pmNc4gI6faZUFSCD/qtPzLZv/7iUsS5FMiIt70w
         wy2P6yuHa07jNwj8DOrD8OC0WXziv/U1FT1hAnE3HxLpeavcd7VJp9StxKPoSp25K0Nd
         iBaKUpz913cN5la89OROBtwBKYioFI8PyUFR3hDTniGEhYl8VFlz974kROPhON9OlkFB
         EXBHmwzmp9A+D6a15CZQTAlbUsZvLYOQDfRErYBpPVqjeGQrDGrBxkVfo+n96EUDtg72
         FJBg==
X-Gm-Message-State: ALoCoQkTYYYNND1M3/QZSmDpV+zSuILiBh4AU9puEo6Cs97gd38HW62BkPEEkHSG6UzoxfVKf44a
MIME-Version: 1.0
X-Received: by 10.229.39.73 with SMTP id f9mr10738402qce.27.1406752346644;
 Wed, 30 Jul 2014 13:32:26 -0700 (PDT)
Received: by 10.96.224.104 with HTTP; Wed, 30 Jul 2014 13:32:26 -0700 (PDT)
In-Reply-To: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
Date: Wed, 30 Jul 2014 13:32:26 -0700
Message-ID: <CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Marcelo Vanzin <vanzin@cloudera.com>
To: dev@spark.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Cody,

Could you file a bug for this if there isn't one already?

For system properties SparkSubmit should be able to read those
settings and do the right thing, but that obviously won't work for
other JVM options... the current code should work fine in cluster mode
though, since the driver is a different process. :-)


On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <cody@koeninger.org> wrote:
> We were previously using SPARK_JAVA_OPTS to set java system properties via
> -D.
>
> This was used for properties that varied on a per-deployment-environment
> basis, but needed to be available in the spark shell and workers.
>
> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been deprecated, and
> replaced by spark-defaults.conf and command line arguments to spark-submit
> or spark-shell.
>
> However, setting spark.driver.extraJavaOptions and
> spark.executor.extraJavaOptions in spark-defaults.conf is not a replacement
> for SPARK_JAVA_OPTS:
>
>
> $ cat conf/spark-defaults.conf
> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>
> $ ./bin/spark-shell
>
> scala> System.getProperty("foo.bar.baz")
> res0: String = null
>
>
> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>
> scala> System.getProperty("foo.bar.baz")
> res0: String = 23
>
>
> Looking through the shell scripts for spark-submit and spark-class, I can
> see why this is; parsing spark-defaults.conf from bash could be brittle.
>
> But from an ergonomic point of view, it's a step back to go from a
> set-it-and-forget-it configuration in spark-env.sh, to requiring command
> line arguments.
>
> I can solve this with an ad-hoc script to wrap spark-shell with the
> appropriate arguments, but I wanted to bring the issue up to see if anyone
> else had run into it,
> or had any direction for a general solution (beyond parsing java properties
> files from bash).



-- 
Marcelo

From dev-return-8636-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 20:43:45 2014
Return-Path: <dev-return-8636-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0246F11BF8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 20:43:45 +0000 (UTC)
Received: (qmail 46246 invoked by uid 500); 30 Jul 2014 20:43:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 46183 invoked by uid 500); 30 Jul 2014 20:43:44 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 46171 invoked by uid 99); 30 Jul 2014 20:43:44 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 20:43:44 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 20:43:40 +0000
Received: by mail-oa0-f50.google.com with SMTP id g18so1377125oah.23
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 13:43:19 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=eSO62y/tOnMCrBedTk0zpEnBjlmN121RCrIrsiBONpk=;
        b=ZZFjn6TXC0dsDKn3cwbvPoY2C8qrkPK+MRRjl2iGeguE3uue3y0j2WuSOXsbXQ12kC
         TrP2Cq0EestoYHiFSHIWseA0u4cb+tr61FIZw+Vqag7iHCfo5AsQQhFByo9ePvaTKtZh
         8rbMRE67MXtrccMSMJCEgJL5s+vIGzuCtm28g0N09yX/jeaXE8jcmmq88FbtuGv38evg
         hdngohJpFWn0vWH0w3fpMTrEISofA9CYsIzHHCH3wth2GDEUiArwodD7avHnBjmrKReQ
         GmLwh7j2tmcDHmF0UREmMb4l/L76GV4C5qb9oD6pNGLvoaGg9RWLGUQKHt/wTQZ3/xWR
         RdUg==
MIME-Version: 1.0
X-Received: by 10.182.18.101 with SMTP id v5mr9654235obd.64.1406752999443;
 Wed, 30 Jul 2014 13:43:19 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Wed, 30 Jul 2014 13:43:19 -0700 (PDT)
In-Reply-To: <CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
Date: Wed, 30 Jul 2014 13:43:19 -0700
Message-ID: <CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Cody - in your example you are using the '=' character, but in our
documentation and tests we use a whitespace to separate the key and
value in the defaults file.

docs: http://spark.apache.org/docs/latest/configuration.html

spark.driver.extraJavaOptions -Dfoo.bar.baz=23

I'm not sure if the java properties file parser will try to interpret
the equals sign. If so you might need to do this.

spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"

Do those work for you?

On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com> wrote:
> Hi Cody,
>
> Could you file a bug for this if there isn't one already?
>
> For system properties SparkSubmit should be able to read those
> settings and do the right thing, but that obviously won't work for
> other JVM options... the current code should work fine in cluster mode
> though, since the driver is a different process. :-)
>
>
> On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <cody@koeninger.org> wrote:
>> We were previously using SPARK_JAVA_OPTS to set java system properties via
>> -D.
>>
>> This was used for properties that varied on a per-deployment-environment
>> basis, but needed to be available in the spark shell and workers.
>>
>> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been deprecated, and
>> replaced by spark-defaults.conf and command line arguments to spark-submit
>> or spark-shell.
>>
>> However, setting spark.driver.extraJavaOptions and
>> spark.executor.extraJavaOptions in spark-defaults.conf is not a replacement
>> for SPARK_JAVA_OPTS:
>>
>>
>> $ cat conf/spark-defaults.conf
>> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>
>> $ ./bin/spark-shell
>>
>> scala> System.getProperty("foo.bar.baz")
>> res0: String = null
>>
>>
>> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>>
>> scala> System.getProperty("foo.bar.baz")
>> res0: String = 23
>>
>>
>> Looking through the shell scripts for spark-submit and spark-class, I can
>> see why this is; parsing spark-defaults.conf from bash could be brittle.
>>
>> But from an ergonomic point of view, it's a step back to go from a
>> set-it-and-forget-it configuration in spark-env.sh, to requiring command
>> line arguments.
>>
>> I can solve this with an ad-hoc script to wrap spark-shell with the
>> appropriate arguments, but I wanted to bring the issue up to see if anyone
>> else had run into it,
>> or had any direction for a general solution (beyond parsing java properties
>> files from bash).
>
>
>
> --
> Marcelo

From dev-return-8637-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 21:11:47 2014
Return-Path: <dev-return-8637-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4128611D3E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 21:11:47 +0000 (UTC)
Received: (qmail 22349 invoked by uid 500); 30 Jul 2014 21:11:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 22289 invoked by uid 500); 30 Jul 2014 21:11:46 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 22274 invoked by uid 99); 30 Jul 2014 21:11:46 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 21:11:46 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.223.181 as permitted sender)
Received: from [209.85.223.181] (HELO mail-ie0-f181.google.com) (209.85.223.181)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 21:11:43 +0000
Received: by mail-ie0-f181.google.com with SMTP id rp18so2354387iec.40
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 14:11:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MowrZYnqyXMIXfCD3RYitfcLqmgswMYRzT2r+fjKMtk=;
        b=JR7d2wyWbSriA4ZrizXRVTUPS9jGIs1HtK4cdfQIBrOooYxbciRuig5PUcTbend2Ww
         BA09nTadeVw2I0LvLOloK0B4CdhNI4UKbC4thgj0l79Owd7SOyUw2H0rKxwyor9hlzLD
         vFQAluNq+CGofqQwnMqdV6a5jOK1KB6jp+i8rSZHyWHxvVa9CNzEHWqsT3ZSMEkKSRlr
         A75Uzp/t8gav1ZNPwkqeUdF1TX+fzaNrF9myDtaD39MQw3RJlXfhNdD6HO9uRqMHDrOt
         HAahcyagP6znkkMEE+L1eV6z8G68Qmss1SH2NJiBWCCCm5pELXsdharBFM2YF9tTmS6/
         NY/w==
MIME-Version: 1.0
X-Received: by 10.50.43.134 with SMTP id w6mr11588296igl.17.1406754678148;
 Wed, 30 Jul 2014 14:11:18 -0700 (PDT)
Received: by 10.107.35.213 with HTTP; Wed, 30 Jul 2014 14:11:18 -0700 (PDT)
Date: Wed, 30 Jul 2014 14:11:18 -0700
Message-ID: <CA+FETEKgsPyzvROSKaA3GKK_z9HDU3Y2tH8Dw7ZfiHSrDw9=Fg@mail.gmail.com>
Subject: spark 0.9.0 with hadoop 2.4 ?
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01184b0ce095f904ff6f9821
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184b0ce095f904ff6f9821
Content-Type: text/plain; charset=UTF-8

Hi Everyone,

We got some yarn related errors when running spark 0.9.0 on hadoop 2.4 (but
it was okay on hadoop 2.2). I didn't find any comments said spark 0.9.0
could support hadoop 2.4, so could I assume that we have to upgrade spark
to the latest release version at this point to solve this issue ?

Best
Shengzhe

--089e01184b0ce095f904ff6f9821--

From dev-return-8638-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 21:18:48 2014
Return-Path: <dev-return-8638-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F81D11D7F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 21:18:48 +0000 (UTC)
Received: (qmail 44732 invoked by uid 500); 30 Jul 2014 21:18:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 44675 invoked by uid 500); 30 Jul 2014 21:18:47 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 44658 invoked by uid 99); 30 Jul 2014 21:18:47 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 21:18:47 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.214.179] (HELO mail-ob0-f179.google.com) (209.85.214.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 21:18:45 +0000
Received: by mail-ob0-f179.google.com with SMTP id wn1so953847obc.38
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 14:18:19 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=i+XD/PkO6K5GGM43RURZOfe4828CAo3Xkl9HclCTpaA=;
        b=Jj7zym29Awq46QTew1gUMTEqo3jnn+IbUbw2el/yYRMHmJK8EHeZ2O3Wmyp8wZ/J5B
         my2y7N+bNSp0TCJIpccYjD9SogE9IUhFpTibZAJHOwSlWwxNeW1D4BHqQlsnQ0txjoDy
         PE8KNza4GbFt1NNUVycQ4hvV3ftGYrrKMMu3y1XUShRIIHUfAADNSQGA6UDjgU7MGzd8
         9zfbOXwLuOqq5y32hTChrk8BaitkKNNFSHqjB0X0t2PNsILIGyB5V2YfgigTgAWcsS94
         rFZ0RoKOoT4ib6alTy7MVszbSQcP96xmEV04I67E/56w+A0zc8pN/bwMil158XoqB5KR
         eEKQ==
X-Gm-Message-State: ALoCoQk6haZu9PpU8cST2Y6npGxSMMiWIl5vWOh5bG6yUhfEEa9Q44lH1w9XpdQ0ljLYC816Z5pM
MIME-Version: 1.0
X-Received: by 10.60.174.3 with SMTP id bo3mr10133061oec.31.1406755099743;
 Wed, 30 Jul 2014 14:18:19 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Wed, 30 Jul 2014 14:18:19 -0700 (PDT)
In-Reply-To: <CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
Date: Wed, 30 Jul 2014 16:18:19 -0500
Message-ID: <CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Cody Koeninger <cody@koeninger.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e011844ce01aac504ff6fb2f8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e011844ce01aac504ff6fb2f8
Content-Type: text/plain; charset=UTF-8

Either whitespace or equals sign are valid properties file formats.
Here's an example:

$ cat conf/spark-defaults.conf
spark.driver.extraJavaOptions -Dfoo.bar.baz=23

$ ./bin/spark-shell -v
Using properties file: /opt/spark/conf/spark-defaults.conf
Adding default property: spark.driver.extraJavaOptions=-Dfoo.bar.baz=23

scala>  System.getProperty("foo.bar.baz")
res0: String = null


If you add double quotes, the resulting string value will have double
quotes.


$ cat conf/spark-defaults.conf
spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"

$ ./bin/spark-shell -v
Using properties file: /opt/spark/conf/spark-defaults.conf
Adding default property: spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"

scala>  System.getProperty("foo.bar.baz")
res0: String = null


Neither one of those affects the issue; the underlying problem in my case
seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
SPARK_JAVA_OPTS environment variables, but nothing parses
spark-defaults.conf before the java process is started.

Here's an example of the process running when only spark-defaults.conf is
being used:

$ ps -ef | grep spark

514       5182  2058  0 21:05 pts/2    00:00:00 bash ./bin/spark-shell -v

514       5189  5182  4 21:05 pts/2    00:00:22 /usr/local/java/bin/java
-cp
::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
-XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
org.apache.spark.deploy.SparkSubmit spark-shell -v --class
org.apache.spark.repl.Main


Here's an example of it when the command line --driver-java-options is used
(and thus things work):


$ ps -ef | grep spark
514       5392  2058  0 21:15 pts/2    00:00:00 bash ./bin/spark-shell -v
--driver-java-options -Dfoo.bar.baz=23

514       5399  5392 80 21:15 pts/2    00:00:06 /usr/local/java/bin/java
-cp
::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
-XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
-Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
--driver-java-options -Dfoo.bar.baz=23 --class org.apache.spark.repl.Main




On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <pwendell@gmail.com> wrote:

> Cody - in your example you are using the '=' character, but in our
> documentation and tests we use a whitespace to separate the key and
> value in the defaults file.
>
> docs: http://spark.apache.org/docs/latest/configuration.html
>
> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>
> I'm not sure if the java properties file parser will try to interpret
> the equals sign. If so you might need to do this.
>
> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>
> Do those work for you?
>
> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com>
> wrote:
> > Hi Cody,
> >
> > Could you file a bug for this if there isn't one already?
> >
> > For system properties SparkSubmit should be able to read those
> > settings and do the right thing, but that obviously won't work for
> > other JVM options... the current code should work fine in cluster mode
> > though, since the driver is a different process. :-)
> >
> >
> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
> >> We were previously using SPARK_JAVA_OPTS to set java system properties
> via
> >> -D.
> >>
> >> This was used for properties that varied on a per-deployment-environment
> >> basis, but needed to be available in the spark shell and workers.
> >>
> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been deprecated,
> and
> >> replaced by spark-defaults.conf and command line arguments to
> spark-submit
> >> or spark-shell.
> >>
> >> However, setting spark.driver.extraJavaOptions and
> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
> replacement
> >> for SPARK_JAVA_OPTS:
> >>
> >>
> >> $ cat conf/spark-defaults.conf
> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> >>
> >> $ ./bin/spark-shell
> >>
> >> scala> System.getProperty("foo.bar.baz")
> >> res0: String = null
> >>
> >>
> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
> >>
> >> scala> System.getProperty("foo.bar.baz")
> >> res0: String = 23
> >>
> >>
> >> Looking through the shell scripts for spark-submit and spark-class, I
> can
> >> see why this is; parsing spark-defaults.conf from bash could be brittle.
> >>
> >> But from an ergonomic point of view, it's a step back to go from a
> >> set-it-and-forget-it configuration in spark-env.sh, to requiring command
> >> line arguments.
> >>
> >> I can solve this with an ad-hoc script to wrap spark-shell with the
> >> appropriate arguments, but I wanted to bring the issue up to see if
> anyone
> >> else had run into it,
> >> or had any direction for a general solution (beyond parsing java
> properties
> >> files from bash).
> >
> >
> >
> > --
> > Marcelo
>

--089e011844ce01aac504ff6fb2f8--

From dev-return-8639-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 22:10:35 2014
Return-Path: <dev-return-8639-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3FB1511F90
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 22:10:35 +0000 (UTC)
Received: (qmail 68213 invoked by uid 500); 30 Jul 2014 22:10:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 68147 invoked by uid 500); 30 Jul 2014 22:10:34 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 68135 invoked by uid 99); 30 Jul 2014 22:10:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 22:10:34 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.219.43] (HELO mail-oa0-f43.google.com) (209.85.219.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 22:10:30 +0000
Received: by mail-oa0-f43.google.com with SMTP id i7so1459930oag.16
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 15:10:08 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=X8SrdGN7ssGOXpmZg8Z7/uzjLC/2ZNEZtI/jm7XOKFE=;
        b=RDBrFal2j3gcvGUmsWm4gSZ6x9fzXYOZuviOXegcryKXRtRZImr3uXnBXv2Es4v2Ed
         V+mivStXprC1ZPPNVrtx9hRH59FiMUEjzwv+YT5qYg7GR+YNrAApKJu6GP9kxUuR5ryz
         /HNcqYqFKSQCt1rQUUUDgLIMHzCoZQ9hzehWC44U80vqbVUtSbxcpvSyZxRPXw/uMssr
         rWnRZUuNZMXuRyAKWYYDmMUhbOVTZrJoWltKR4W1wmCO6ngL6gie8I8t6j5vq5WNI31U
         hGx+uWXfcWg4kuqnUO+JUi95yc0bytc87p6wpGo90aMHuZ/N/iVDARGNgi6wiU4JlIcF
         negA==
X-Gm-Message-State: ALoCoQnDc+W7L7FKR23EluHZjKsrjIbSm93p/ywIZcz4LvVGpCWkqjvKAaIfyYBfkNANQVgxFVoF
MIME-Version: 1.0
X-Received: by 10.182.142.69 with SMTP id ru5mr10308848obb.6.1406758208733;
 Wed, 30 Jul 2014 15:10:08 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Wed, 30 Jul 2014 15:10:08 -0700 (PDT)
In-Reply-To: <CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
Date: Wed, 30 Jul 2014 17:10:08 -0500
Message-ID: <CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Cody Koeninger <cody@koeninger.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a11c2d5ce511b4104ff706b35
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2d5ce511b4104ff706b35
Content-Type: text/plain; charset=UTF-8

In addition, spark.executor.extraJavaOptions does not seem to behave as I
would expect; java arguments don't seem to be propagated to executors.


$ cat conf/spark-defaults.conf

spark.master
mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
spark.executor.extraJavaOptions -Dfoo.bar.baz=23
spark.driver.extraJavaOptions -Dfoo.bar.baz=23


$ ./bin/spark-shell

scala> sc.getConf.get("spark.executor.extraJavaOptions")
res0: String = -Dfoo.bar.baz=23

scala> sc.parallelize(1 to 100).map{ i => (
     |  java.net.InetAddress.getLocalHost.getHostName,
     |  System.getProperty("foo.bar.baz")
     | )}.collect

res1: Array[(String, String)] = Array((dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
(dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
(dn-02.mxstg,null), ...



Note that this is a mesos deployment, although I wouldn't expect that to
affect the availability of spark.driver.extraJavaOptions in a local spark
shell.


On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <cody@koeninger.org> wrote:

> Either whitespace or equals sign are valid properties file formats.
> Here's an example:
>
> $ cat conf/spark-defaults.conf
> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>
> $ ./bin/spark-shell -v
> Using properties file: /opt/spark/conf/spark-defaults.conf
> Adding default property: spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>
>
> scala>  System.getProperty("foo.bar.baz")
> res0: String = null
>
>
> If you add double quotes, the resulting string value will have double
> quotes.
>
>
> $ cat conf/spark-defaults.conf
> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>
> $ ./bin/spark-shell -v
> Using properties file: /opt/spark/conf/spark-defaults.conf
> Adding default property: spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
>
> scala>  System.getProperty("foo.bar.baz")
> res0: String = null
>
>
> Neither one of those affects the issue; the underlying problem in my case
> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
> SPARK_JAVA_OPTS environment variables, but nothing parses
> spark-defaults.conf before the java process is started.
>
> Here's an example of the process running when only spark-defaults.conf is
> being used:
>
> $ ps -ef | grep spark
>
> 514       5182  2058  0 21:05 pts/2    00:00:00 bash ./bin/spark-shell -v
>
> 514       5189  5182  4 21:05 pts/2    00:00:22 /usr/local/java/bin/java
> -cp
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
> org.apache.spark.repl.Main
>
>
> Here's an example of it when the command line --driver-java-options is
> used (and thus things work):
>
>
> $ ps -ef | grep spark
> 514       5392  2058  0 21:15 pts/2    00:00:00 bash ./bin/spark-shell -v
> --driver-java-options -Dfoo.bar.baz=23
>
> 514       5399  5392 80 21:15 pts/2    00:00:06 /usr/local/java/bin/java
> -cp
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
> --driver-java-options -Dfoo.bar.baz=23 --class org.apache.spark.repl.Main
>
>
>
>
> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
>
>> Cody - in your example you are using the '=' character, but in our
>> documentation and tests we use a whitespace to separate the key and
>> value in the defaults file.
>>
>> docs: http://spark.apache.org/docs/latest/configuration.html
>>
>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>
>> I'm not sure if the java properties file parser will try to interpret
>> the equals sign. If so you might need to do this.
>>
>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>
>> Do those work for you?
>>
>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com>
>> wrote:
>> > Hi Cody,
>> >
>> > Could you file a bug for this if there isn't one already?
>> >
>> > For system properties SparkSubmit should be able to read those
>> > settings and do the right thing, but that obviously won't work for
>> > other JVM options... the current code should work fine in cluster mode
>> > though, since the driver is a different process. :-)
>> >
>> >
>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <cody@koeninger.org>
>> wrote:
>> >> We were previously using SPARK_JAVA_OPTS to set java system properties
>> via
>> >> -D.
>> >>
>> >> This was used for properties that varied on a
>> per-deployment-environment
>> >> basis, but needed to be available in the spark shell and workers.
>> >>
>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been deprecated,
>> and
>> >> replaced by spark-defaults.conf and command line arguments to
>> spark-submit
>> >> or spark-shell.
>> >>
>> >> However, setting spark.driver.extraJavaOptions and
>> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
>> replacement
>> >> for SPARK_JAVA_OPTS:
>> >>
>> >>
>> >> $ cat conf/spark-defaults.conf
>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>> >>
>> >> $ ./bin/spark-shell
>> >>
>> >> scala> System.getProperty("foo.bar.baz")
>> >> res0: String = null
>> >>
>> >>
>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>> >>
>> >> scala> System.getProperty("foo.bar.baz")
>> >> res0: String = 23
>> >>
>> >>
>> >> Looking through the shell scripts for spark-submit and spark-class, I
>> can
>> >> see why this is; parsing spark-defaults.conf from bash could be
>> brittle.
>> >>
>> >> But from an ergonomic point of view, it's a step back to go from a
>> >> set-it-and-forget-it configuration in spark-env.sh, to requiring
>> command
>> >> line arguments.
>> >>
>> >> I can solve this with an ad-hoc script to wrap spark-shell with the
>> >> appropriate arguments, but I wanted to bring the issue up to see if
>> anyone
>> >> else had run into it,
>> >> or had any direction for a general solution (beyond parsing java
>> properties
>> >> files from bash).
>> >
>> >
>> >
>> > --
>> > Marcelo
>>
>
>

--001a11c2d5ce511b4104ff706b35--

From dev-return-8640-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 22:17:52 2014
Return-Path: <dev-return-8640-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9EDE311FF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 22:17:52 +0000 (UTC)
Received: (qmail 1617 invoked by uid 500); 30 Jul 2014 22:17:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1562 invoked by uid 500); 30 Jul 2014 22:17:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1550 invoked by uid 99); 30 Jul 2014 22:17:51 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 22:17:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.213.172 as permitted sender)
Received: from [209.85.213.172] (HELO mail-ig0-f172.google.com) (209.85.213.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 22:17:46 +0000
Received: by mail-ig0-f172.google.com with SMTP id h15so8186296igd.5
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 15:17:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=6iWFi5YPmKvdLoXIN/Oi2eBdaHw21wKwRZrJOdkgbYM=;
        b=Jfj0QXD30lB4FQRIOfVD4edeC7md2fJe6Zf8An/KJrnP6CQraSvJbF76oPpYne8JJE
         uYqQzOyjPBRheAn6LVe+G+YcJt+pmQtTogDKoVGrgK6wnka+cy/9PehAnZbZCgiUD5DH
         Cwcnjw/HZsFAY6m3yoLbNl8oBxxwaqI7NPUSceWaDBB6kcQrSOBlPDkyLzHUxpo/6Bem
         jXj3WRKIlZ0o9ymS/3hLaRAbLCgFfNzByyet9EIoqPcmKIliDoAy64rOTOiiDhh1sxf7
         qleZwPnVbF9crdNrhq7IWwb1OxpNhEut5Ex4etJJF0aYPkFbVmATNTDpnOHFHUOc/oNC
         DVIA==
MIME-Version: 1.0
X-Received: by 10.50.43.134 with SMTP id w6mr12059649igl.17.1406758645839;
 Wed, 30 Jul 2014 15:17:25 -0700 (PDT)
Received: by 10.107.35.213 with HTTP; Wed, 30 Jul 2014 15:17:25 -0700 (PDT)
In-Reply-To: <CA+FETEKgsPyzvROSKaA3GKK_z9HDU3Y2tH8Dw7ZfiHSrDw9=Fg@mail.gmail.com>
References: <CA+FETEKgsPyzvROSKaA3GKK_z9HDU3Y2tH8Dw7ZfiHSrDw9=Fg@mail.gmail.com>
Date: Wed, 30 Jul 2014 15:17:25 -0700
Message-ID: <CA+FETEJfHv_+a1ijHnq6kZixtGJ1ORum2W8xWoO3--vV1Y2b8g@mail.gmail.com>
Subject: Re: spark 0.9.0 with hadoop 2.4 ?
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e01184b0c5ebefe04ff70856d
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01184b0c5ebefe04ff70856d
Content-Type: text/plain; charset=UTF-8

I think I might find the root cause, YARN-1931 addressed the incompatible
issue. The solution for my case might be either take related Spark patches
or do an upgrade.


On Wed, Jul 30, 2014 at 2:11 PM, yao <yaoshengzhe@gmail.com> wrote:

> Hi Everyone,
>
> We got some yarn related errors when running spark 0.9.0 on hadoop 2.4
> (but it was okay on hadoop 2.2). I didn't find any comments said spark
> 0.9.0 could support hadoop 2.4, so could I assume that we have to upgrade
> spark to the latest release version at this point to solve this issue ?
>
> Best
> Shengzhe
>

--089e01184b0c5ebefe04ff70856d--

From dev-return-8641-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Wed Jul 30 23:41:11 2014
Return-Path: <dev-return-8641-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6E14113D0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 30 Jul 2014 23:41:11 +0000 (UTC)
Received: (qmail 64967 invoked by uid 500); 30 Jul 2014 23:41:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64906 invoked by uid 500); 30 Jul 2014 23:41:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64894 invoked by uid 99); 30 Jul 2014 23:41:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 23:41:10 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.177 as permitted sender)
Received: from [209.85.160.177] (HELO mail-yk0-f177.google.com) (209.85.160.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 30 Jul 2014 23:41:07 +0000
Received: by mail-yk0-f177.google.com with SMTP id 79so1116583ykr.22
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 16:40:41 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=FIBMo95UAMLTzphmhuyUDWnlmQyP+oDHGX6g/Rh3EvA=;
        b=AKIhhPsc+hGo7zkVVKSsnFBMKM8qxRyyVGewtKjxmGoZIhgObJRgvb2VadFbZaBqMT
         lU/ArIobgRti0MJyYaCWCJ1/hUoTcmftVoIAfoG7iX1BNhWxHuZu1o9NXS2yA25jsxwS
         XOQaFtJpgYJHNfuRkIzmJ5fjaKhGucXDJYpAdeeDMXj6ytdMPlLlLKpymbMoACgNJTIl
         5O7YtrrGi+FzBKWWAliEy3Hb//IUQnPRJUPx/DAmDhfiE2mC+Cdkm22tEgRz0iLtEFco
         wuz1S+ii/DRVzib3WeIv138z1KB36cIytHG75QXK18Jp0ehX/tNlRduywLyF7F1Uitou
         STfw==
MIME-Version: 1.0
X-Received: by 10.236.14.34 with SMTP id c22mr424297yhc.97.1406763641610; Wed,
 30 Jul 2014 16:40:41 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Wed, 30 Jul 2014 16:40:41 -0700 (PDT)
In-Reply-To: <CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com>
References: <CFFBAFC8.2D91%snunez@hortonworks.com>
	<CALte62zadHdJ8xvw+5dMEXLyzKnfs3uiRtK9wLGigzed2mQeKA@mail.gmail.com>
	<CAMAsSd+Uc0Z431F+k=4SSG6ETLo-b_CkEsu+c7WxstS8cRK4Dg@mail.gmail.com>
	<CALte62x=y2FYfxtsG1BfUK-95+5qA-seTa6BbNJuzOeFgO6P9A@mail.gmail.com>
	<CAMAsSdJ0J9yUEhOG=24_SEd0t+U4ncYB9Bpx8WYMFbN852Yoyg@mail.gmail.com>
	<CALte62wrXu5aUsC34=+ykbvtTA964mJ4Rvb+a8P1WVQYAbDqqA@mail.gmail.com>
	<CABPQxsu92BEukAca_tajYapj58R6m3L5b4JytBTSm+P2+GmGrg@mail.gmail.com>
	<CALte62wwMKh7EpCYM3Z-RNJP9EgLpH691YM0GEj2un76KTh=+g@mail.gmail.com>
	<CAA_qdLotL+YKSXBaZioH5wDJ4NqLOndR-NCe0PLK7N8m=thg6Q@mail.gmail.com>
	<CABPQxssuiZifEJjpfS1DJaGs1UQa_yQ5az063B+uuOB3fwygPQ@mail.gmail.com>
	<CAA_qdLpxVuEk9XQoze2VV7wbA1VnfLoTKoW2aXzj9EpdiLpinw@mail.gmail.com>
	<CFFC00D1.2ED4%snunez@hortonworks.com>
	<CALte62yTBHJjAheJzoCYP5nvcKpGT=FRTpsdGJvOFf=ra+yBRA@mail.gmail.com>
	<CALte62x46cvgMUvy_aTD4vMZBVwPPyfM=mvJN_WK3LLcRX=Z8w@mail.gmail.com>
	<CAAswR-62D75kRydbjF29Sw8aqteOagXFGhdZnhxWGDHkSBVanw@mail.gmail.com>
	<CALte62yNRuK1012a=EYArQTx+oFSpXJ-1+r27cc1OBJmuT4fbA@mail.gmail.com>
Date: Wed, 30 Jul 2014 16:40:41 -0700
Message-ID: <CALte62zYojKLwgvsABNPb77Vxi+--4ZEtgwNEmaMKhvmeD9Z1Q@mail.gmail.com>
Subject: Re: Working Formula for Hive 0.13?
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=089e013a29da242f1e04ff71af1b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a29da242f1e04ff71af1b
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

I found SPARK-2706

Let me attach tentative patch there - I still face compilation error.

Cheers


On Mon, Jul 28, 2014 at 5:59 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> bq. Either way its unclear to if there is any reason to use reflection to
> support multiple versions, instead of just upgrading to Hive 0.13.0
>
> Which Spark release would this Hive upgrade take place ?
> I agree it is cleaner to upgrade Hive dependency vs. introducing
> reflection.
>
> Cheers
>
>
> On Mon, Jul 28, 2014 at 5:22 PM, Michael Armbrust <michael@databricks.com=
>
> wrote:
>
>> A few things:
>>  - When we upgrade to Hive 0.13.0, Patrick will likely republish the
>> hive-exec jar just as we did for 0.12.0
>>  - Since we have to tie into some pretty low level APIs it is unsurprisi=
ng
>> that the code doesn't just compile out of the box against 0.13.0
>>  - ScalaReflection is for determining Schema from Scala classes, not
>> reflection based bridge code.  Either way its unclear to if there is any
>> reason to use reflection to support multiple versions, instead of just
>> upgrading to Hive 0.13.0
>>
>> One question I have is, What is the goal of upgrading to hive 0.13.0?  I=
s
>> it purely because you are having problems connecting to newer metastores=
?
>>  Are there some features you are hoping for?  This will help me prioriti=
ze
>> this effort.
>>
>> Michael
>>
>>
>> On Mon, Jul 28, 2014 at 4:05 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>>
>> > I was looking for a class where reflection-related code should reside.
>> >
>> > I found this but don't think it is the proper class for bridging
>> > differences between hive 0.12 and 0.13.1:
>> >
>> >
>> sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflectio=
n.scala
>> >
>> > Cheers
>> >
>> >
>> > On Mon, Jul 28, 2014 at 3:41 PM, Ted Yu <yuzhihong@gmail.com> wrote:
>> >
>> > > After manually copying hive 0.13.1 jars to local maven repo, I got t=
he
>> > > following errors when building spark-hive_2.10 module :
>> > >
>> > > [ERROR]
>> > >
>> >
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveCo=
ntext.scala:182:
>> > > type mismatch;
>> > >  found   : String
>> > >  required: Array[String]
>> > > [ERROR]       val proc: CommandProcessor =3D
>> > > CommandProcessorFactory.get(tokens(0), hiveconf)
>> > > [ERROR]
>> > >    ^
>> > > [ERROR]
>> > >
>> >
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMe=
tastoreCatalog.scala:60:
>> > > value getAllPartitionsForPruner is not a member of org.apache.
>> > >  hadoop.hive.ql.metadata.Hive
>> > > [ERROR]         client.getAllPartitionsForPruner(table).toSeq
>> > > [ERROR]                ^
>> > > [ERROR]
>> > >
>> >
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMe=
tastoreCatalog.scala:267:
>> > > overloaded method constructor TableDesc with alternatives:
>> > >   (x$1: Class[_ <: org.apache.hadoop.mapred.InputFormat[_, _]],x$2:
>> > > Class[_],x$3:
>> > java.util.Properties)org.apache.hadoop.hive.ql.plan.TableDesc
>> > > <and>
>> > >   ()org.apache.hadoop.hive.ql.plan.TableDesc
>> > >  cannot be applied to
>> (Class[org.apache.hadoop.hive.serde2.Deserializer],
>> > > Class[(some other)?0(in value tableDesc)(in value tableDesc)],
>> > Class[?0(in
>> > > value tableDesc)(in   value tableDesc)], java.util.Properties)
>> > > [ERROR]   val tableDesc =3D new TableDesc(
>> > > [ERROR]                   ^
>> > > [WARNING] Class org.antlr.runtime.tree.CommonTree not found -
>> continuing
>> > > with a stub.
>> > > [WARNING] Class org.antlr.runtime.Token not found - continuing with =
a
>> > stub.
>> > > [WARNING] Class org.antlr.runtime.tree.Tree not found - continuing
>> with a
>> > > stub.
>> > > [ERROR]
>> > >      while compiling:
>> > >
>> >
>> /homes/xx/spark/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl=
.scala
>> > >         during phase: typer
>> > >      library version: version 2.10.4
>> > >     compiler version: version 2.10.4
>> > >
>> > > The above shows incompatible changes between 0.12 and 0.13.1
>> > > e.g. the first error corresponds to the following method
>> > > in CommandProcessorFactory :
>> > >   public static CommandProcessor get(String[] cmd, HiveConf conf)
>> > >
>> > > Cheers
>> > >
>> > >
>> > > On Mon, Jul 28, 2014 at 1:32 PM, Steve Nunez <snunez@hortonworks.com=
>
>> > > wrote:
>> > >
>> > >> So, do we have a short-term fix until Hive 0.14 comes out? Perhaps
>> > adding
>> > >> the hive-exec jar to the spark-project repo? It doesn=C2=B9t look l=
ike
>> > there=C2=B9s
>> > >> a release date schedule for 0.14.
>> > >>
>> > >>
>> > >>
>> > >> On 7/28/14, 10:50, "Cheng Lian" <lian.cs.zju@gmail.com> wrote:
>> > >>
>> > >> >Exactly, forgot to mention Hulu team also made changes to cope wit=
h
>> > those
>> > >> >incompatibility issues, but they said that=C2=B9s relatively easy =
once
>> the
>> > >> >re-packaging work is done.
>> > >> >
>> > >> >
>> > >> >On Tue, Jul 29, 2014 at 1:20 AM, Patrick Wendell <
>> pwendell@gmail.com>
>> > >>
>> > >> >wrote:
>> > >> >
>> > >> >> I've heard from Cloudera that there were hive internal changes
>> > between
>> > >> >> 0.12 and 0.13 that required code re-writing. Over time it might =
be
>> > >> >> possible for us to integrate with hive using API's that are more
>> > >> >> stable (this is the domain of Michael/Cheng/Yin more than me!). =
It
>> > >> >> would be interesting to see what the Hulu folks did.
>> > >> >>
>> > >> >> - Patrick
>> > >> >>
>> > >> >> On Mon, Jul 28, 2014 at 10:16 AM, Cheng Lian <
>> lian.cs.zju@gmail.com>
>> > >> >> wrote:
>> > >> >> > AFAIK, according a recent talk, Hulu team in China has built
>> Spark
>> > >> SQL
>> > >> >> > against Hive 0.13 (or 0.13.1?) successfully. Basically they al=
so
>> > >> >> > re-packaged Hive 0.13 as what the Spark team did. The slides o=
f
>> the
>> > >> >>talk
>> > >> >> > hasn't been released yet though.
>> > >> >> >
>> > >> >> >
>> > >> >> > On Tue, Jul 29, 2014 at 1:01 AM, Ted Yu <yuzhihong@gmail.com>
>> > wrote:
>> > >> >> >
>> > >> >> >> Owen helped me find this:
>> > >> >> >> https://issues.apache.org/jira/browse/HIVE-7423
>> > >> >> >>
>> > >> >> >> I guess this means that for Hive 0.14, Spark should be able t=
o
>> > >> >>directly
>> > >> >> >> pull in hive-exec-core.jar
>> > >> >> >>
>> > >> >> >> Cheers
>> > >> >> >>
>> > >> >> >>
>> > >> >> >> On Mon, Jul 28, 2014 at 9:55 AM, Patrick Wendell <
>> > >> pwendell@gmail.com>
>> > >> >> >> wrote:
>> > >> >> >>
>> > >> >> >> > It would be great if the hive team can fix that issue. If
>> not,
>> > >> >>we'll
>> > >> >> >> > have to continue forking our own version of Hive to change
>> the
>> > way
>> > >> >>it
>> > >> >> >> > publishes artifacts.
>> > >> >> >> >
>> > >> >> >> > - Patrick
>> > >> >> >> >
>> > >> >> >> > On Mon, Jul 28, 2014 at 9:34 AM, Ted Yu <yuzhihong@gmail.co=
m
>> >
>> > >> >>wrote:
>> > >> >> >> > > Talked with Owen offline. He confirmed that as of 0.13,
>> > >> >>hive-exec is
>> > >> >> >> > still
>> > >> >> >> > > uber jar.
>> > >> >> >> > >
>> > >> >> >> > > Right now I am facing the following error building agains=
t
>> > Hive
>> > >> >> 0.13.1
>> > >> >> >> :
>> > >> >> >> > >
>> > >> >> >> > > [ERROR] Failed to execute goal on project spark-hive_2.10=
:
>> > Could
>> > >> >>not
>> > >> >> >> > > resolve dependencies for project
>> > >> >> >> > > org.apache.spark:spark-hive_2.10:jar:1.1.0-SNAPSHOT: The
>> > >> >>following
>> > >> >> >> > > artifacts could not be resolved:
>> > >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1,
>> > >> >> >> > > org.spark-project.hive:hive-exec:jar:0.13.1,
>> > >> >> >> > > org.spark-project.hive:hive-serde:jar:0.13.1: Failure to
>> find
>> > >> >> >> > > org.spark-project.hive:hive-metastore:jar:0.13.1 in
>> > >> >> >> > > http://repo.maven.apache.org/maven2 was cached in the
>> local
>> > >> >> >> repository,
>> > >> >> >> > > resolution will not be reattempted until the update
>> interval
>> > of
>> > >> >> >> > maven-repo
>> > >> >> >> > > has elapsed or updates are forced -> [Help 1]
>> > >> >> >> > >
>> > >> >> >> > > Some hint would be appreciated.
>> > >> >> >> > >
>> > >> >> >> > > Cheers
>> > >> >> >> > >
>> > >> >> >> > >
>> > >> >> >> > > On Mon, Jul 28, 2014 at 9:15 AM, Sean Owen <
>> > sowen@cloudera.com>
>> > >> >> wrote:
>> > >> >> >> > >
>> > >> >> >> > >> Yes, it is published. As of previous versions, at least,
>> > >> >>hive-exec
>> > >> >> >> > >> included all of its dependencies *in its artifact*,
>> making it
>> > >> >> unusable
>> > >> >> >> > >> as-is because it contained copies of dependencies that
>> clash
>> > >> >>with
>> > >> >> >> > >> versions present in other artifacts, and can't be manage=
d
>> > with
>> > >> >> Maven
>> > >> >> >> > >> mechanisms.
>> > >> >> >> > >>
>> > >> >> >> > >> I am not sure why hive-exec was not published normally,
>> with
>> > >> >>just
>> > >> >> its
>> > >> >> >> > >> own classes. That's why it was copied, into an artifact
>> with
>> > >> >>just
>> > >> >> >> > >> hive-exec code.
>> > >> >> >> > >>
>> > >> >> >> > >> You could do the same thing for hive-exec 0.13.1.
>> > >> >> >> > >> Or maybe someone knows that it's published more 'normall=
y'
>> > now.
>> > >> >> >> > >> I don't think hive-metastore is related to this question=
?
>> > >> >> >> > >>
>> > >> >> >> > >> I am no expert on the Hive artifacts, just remembering
>> what
>> > the
>> > >> >> issue
>> > >> >> >> > >> was initially in case it helps you get to a similar
>> solution.
>> > >> >> >> > >>
>> > >> >> >> > >> On Mon, Jul 28, 2014 at 4:47 PM, Ted Yu <
>> yuzhihong@gmail.com
>> > >
>> > >> >> wrote:
>> > >> >> >> > >> > hive-exec (as of 0.13.1) is published here:
>> > >> >> >> > >> >
>> > >> >> >> > >>
>> > >> >> >> >
>> > >> >> >>
>> > >> >>
>> > >> >>
>> > >>
>> >
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-exec%7=
C
>> > >> >>0.13.1%7Cjar
>> > >> >> >> > >> >
>> > >> >> >> > >> > Should a JIRA be opened so that dependency on
>> > hive-metastore
>> > >> >>can
>> > >> >> be
>> > >> >> >> > >> > replaced by dependency on hive-exec ?
>> > >> >> >> > >> >
>> > >> >> >> > >> > Cheers
>> > >> >> >> > >> >
>> > >> >> >> > >> >
>> > >> >> >> > >> > On Mon, Jul 28, 2014 at 8:26 AM, Sean Owen
>> > >> >><sowen@cloudera.com>
>> > >> >> >> > wrote:
>> > >> >> >> > >> >
>> > >> >> >> > >> >> The reason for org.spark-project.hive is that Spark
>> relies
>> > >> on
>> > >> >> >> > >> >> hive-exec, but the Hive project does not publish this
>> > >> >>artifact
>> > >> >> by
>> > >> >> >> > >> >> itself, only with all its dependencies as an uber jar=
.
>> > Maybe
>> > >> >> that's
>> > >> >> >> > >> >> been improved. If so, you need to point at the new
>> > hive-exec
>> > >> >>and
>> > >> >> >> > >> >> perhaps sort out its dependencies manually in your
>> build.
>> > >> >> >> > >> >>
>> > >> >> >> > >> >> On Mon, Jul 28, 2014 at 4:01 PM, Ted Yu <
>> > >> yuzhihong@gmail.com>
>> > >> >> >> wrote:
>> > >> >> >> > >> >> > I found 0.13.1 artifacts in maven:
>> > >> >> >> > >> >> >
>> > >> >> >> > >> >>
>> > >> >> >> > >>
>> > >> >> >> >
>> > >> >> >>
>> > >> >>
>> > >> >>
>> > >>
>> >
>> http://search.maven.org/#artifactdetails%7Corg.apache.hive%7Chive-metast=
o
>> > >> >>re%7C0.13.1%7Cjar
>> > >> >> >> > >> >> >
>> > >> >> >> > >> >> > However, Spark uses groupId of
>> org.spark-project.hive,
>> > not
>> > >> >> >> > >> >> org.apache.hive
>> > >> >> >> > >> >> >
>> > >> >> >> > >> >> > Can someone tell me how it is supposed to work ?
>> > >> >> >> > >> >> >
>> > >> >> >> > >> >> > Cheers
>> > >> >> >> > >> >> >
>> > >> >> >> > >> >> >
>> > >> >> >> > >> >> > On Mon, Jul 28, 2014 at 7:44 AM, Steve Nunez <
>> > >> >> >> > snunez@hortonworks.com>
>> > >> >> >> > >> >> wrote:
>> > >> >> >> > >> >> >
>> > >> >> >> > >> >> >> I saw a note earlier, perhaps on the user list,
>> that at
>> > >> >>least
>> > >> >> >> one
>> > >> >> >> > >> >> person is
>> > >> >> >> > >> >> >> using Hive 0.13. Anyone got a working build
>> > configuration
>> > >> >>for
>> > >> >> >> this
>> > >> >> >> > >> >> version
>> > >> >> >> > >> >> >> of Hive?
>> > >> >> >> > >> >> >>
>> > >> >> >> > >> >> >> Regards,
>> > >> >> >> > >> >> >> - Steve
>> > >> >> >> > >> >> >>
>> > >> >> >> > >> >> >>
>> > >> >> >> > >> >> >>
>> > >> >> >> > >> >> >> --
>> > >> >> >> > >> >> >> CONFIDENTIALITY NOTICE
>> > >> >> >> > >> >> >> NOTICE: This message is intended for the use of th=
e
>> > >> >> individual
>> > >> >> >> or
>> > >> >> >> > >> >> entity to
>> > >> >> >> > >> >> >> which it is addressed and may contain information
>> that
>> > is
>> > >> >> >> > >> confidential,
>> > >> >> >> > >> >> >> privileged and exempt from disclosure under
>> applicable
>> > >> >>law.
>> > >> >> If
>> > >> >> >> the
>> > >> >> >> > >> >> reader
>> > >> >> >> > >> >> >> of this message is not the intended recipient, you
>> are
>> > >> >>hereby
>> > >> >> >> > >> notified
>> > >> >> >> > >> >> that
>> > >> >> >> > >> >> >> any printing, copying, dissemination, distribution=
,
>> > >> >> disclosure
>> > >> >> >> or
>> > >> >> >> > >> >> >> forwarding of this communication is strictly
>> > prohibited.
>> > >> >>If
>> > >> >> you
>> > >> >> >> > have
>> > >> >> >> > >> >> >> received this communication in error, please conta=
ct
>> > the
>> > >> >> sender
>> > >> >> >> > >> >> immediately
>> > >> >> >> > >> >> >> and delete it from your system. Thank You.
>> > >> >> >> > >> >> >>
>> > >> >> >> > >> >>
>> > >> >> >> > >>
>> > >> >> >> >
>> > >> >> >>
>> > >> >>
>> > >>
>> > >>
>> > >>
>> > >> --
>> > >> CONFIDENTIALITY NOTICE
>> > >> NOTICE: This message is intended for the use of the individual or
>> entity
>> > >> to
>> > >> which it is addressed and may contain information that is
>> confidential,
>> > >> privileged and exempt from disclosure under applicable law. If the
>> > reader
>> > >> of this message is not the intended recipient, you are hereby
>> notified
>> > >> that
>> > >> any printing, copying, dissemination, distribution, disclosure or
>> > >> forwarding of this communication is strictly prohibited. If you hav=
e
>> > >> received this communication in error, please contact the sender
>> > >> immediately
>> > >> and delete it from your system. Thank You.
>> > >>
>> > >
>> > >
>> >
>>
>
>

--089e013a29da242f1e04ff71af1b--

From dev-return-8642-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 01:54:17 2014
Return-Path: <dev-return-8642-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7F9EF118BE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 01:54:17 +0000 (UTC)
Received: (qmail 93619 invoked by uid 500); 31 Jul 2014 01:54:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93558 invoked by uid 500); 31 Jul 2014 01:54:16 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 93544 invoked by uid 99); 31 Jul 2014 01:54:16 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 01:54:16 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sysolve@gmail.com designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 01:54:13 +0000
Received: by mail-oa0-f48.google.com with SMTP id m1so1566131oag.35
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 18:53:48 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=1N4D0drhJRsqtAYxnTaNiQT9rYx5kfzCFrQ1uX2nOys=;
        b=LYCj6JpGGU+/YlbsbiUTF0oK7YOvGFmbIWXB1uAiyqgPk8koutNDr2IMfGsnQRVZIF
         VSycRbve5cvyn+52DLtpxHmRhkX44m6PowDHye9aWgQr7Gd0v8ygw7aTMdgmClBToAXf
         Pg06N+4s9DLXYmGRpBNQelwZk+3IrJum1cySelyP3QceYOrH5eWJaQfYtjFbSbORfThV
         cvJ2ryzrzm09ypHzWYtBwvAbVm8KhgJJ3ynNL5h/0vXtEkEoB5DGSDHG0vVtPNZTdYEs
         Qy0KmDYqQDpoOArD76HGnSPUAj5O/dfbZ7VhdOMoRAgEHzR79os5xm8P2vysbpViK3UN
         FmMg==
MIME-Version: 1.0
X-Received: by 10.60.220.169 with SMTP id px9mr11486333oec.67.1406771628475;
 Wed, 30 Jul 2014 18:53:48 -0700 (PDT)
Received: by 10.182.221.196 with HTTP; Wed, 30 Jul 2014 18:53:48 -0700 (PDT)
Date: Thu, 31 Jul 2014 09:53:48 +0800
Message-ID: <CAD7nt4j8NK8EEiDUSG6gdg2-5YgpC-GTFd4fXnoNL3s=Mw07AQ@mail.gmail.com>
Subject: subscribe dev list for spark
From: Grace <sysolve@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=001a1133e0e432099904ff738bb2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133e0e432099904ff738bb2
Content-Type: text/plain; charset=UTF-8



--001a1133e0e432099904ff738bb2--

From dev-return-8643-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 01:57:18 2014
Return-Path: <dev-return-8643-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4FAA7118C9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 01:57:18 +0000 (UTC)
Received: (qmail 96667 invoked by uid 500); 31 Jul 2014 01:57:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96599 invoked by uid 500); 31 Jul 2014 01:57:17 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 96585 invoked by uid 99); 31 Jul 2014 01:57:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 01:57:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yuzhihong@gmail.com designates 209.85.160.171 as permitted sender)
Received: from [209.85.160.171] (HELO mail-yk0-f171.google.com) (209.85.160.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 01:57:14 +0000
Received: by mail-yk0-f171.google.com with SMTP id 19so1199170ykq.16
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 18:56:50 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JpdBnvOnO19OsgE/qY+/l4M62/QKriuDrlnE5SuKWcA=;
        b=ZrrfxrVBs9ua8H4mABenQj5d1xyZrV4fSPUXjHdy1ErwslXx5Gbcqx5NCQ8WODAsXn
         i+KFcWbS3Tsibke4AeZffuEjkGmgrv62C9IeTCTwkpuIzg9gE34+C2umGSY2Q4kHX8JM
         5FQWqwpyLerSWOr0q3T2mxAr99FQcm7Al0PQbZxwhr/qPUTnHvDXb6PBCYXl6PiNO7k5
         56sGodHDW8YR/seh6acl/TZRH9+RNwZyuintkHohifIBRgGpO/t5drjO9ijDH6t/eqoF
         c41trvVDvT+Gc24u31IQVQJTR/Zs+xYzf0L6QLAFOkun41U7eDK20qrV+5Zit+dRCdNQ
         TbZg==
MIME-Version: 1.0
X-Received: by 10.236.91.171 with SMTP id h31mr1344006yhf.144.1406771809789;
 Wed, 30 Jul 2014 18:56:49 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Wed, 30 Jul 2014 18:56:49 -0700 (PDT)
In-Reply-To: <CAD7nt4j8NK8EEiDUSG6gdg2-5YgpC-GTFd4fXnoNL3s=Mw07AQ@mail.gmail.com>
References: <CAD7nt4j8NK8EEiDUSG6gdg2-5YgpC-GTFd4fXnoNL3s=Mw07AQ@mail.gmail.com>
Date: Wed, 30 Jul 2014 18:56:49 -0700
Message-ID: <CALte62yjCtGiDVMO7PcFsjqd_oKYoS1uRuNx8YiHw3pukKuPqw@mail.gmail.com>
Subject: Re: subscribe dev list for spark
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf300e503100ae1504ff739618
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf300e503100ae1504ff739618
Content-Type: text/plain; charset=UTF-8

See Mailing list section of:
https://spark.apache.org/community.html


On Wed, Jul 30, 2014 at 6:53 PM, Grace <sysolve@gmail.com> wrote:

>
>

--20cf300e503100ae1504ff739618--

From dev-return-8644-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 04:26:28 2014
Return-Path: <dev-return-8644-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5243E11BD4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 04:26:28 +0000 (UTC)
Received: (qmail 14853 invoked by uid 500); 31 Jul 2014 04:26:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14788 invoked by uid 500); 31 Jul 2014 04:26:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14776 invoked by uid 99); 31 Jul 2014 04:26:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 04:26:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.213.175 as permitted sender)
Received: from [209.85.213.175] (HELO mail-ig0-f175.google.com) (209.85.213.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 04:26:19 +0000
Received: by mail-ig0-f175.google.com with SMTP id uq10so8657220igb.8
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 21:25:54 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=jtXclRhetSQ6G3FzIja55EmmDDnYIpUYVwdPDoNL74M=;
        b=bTCJrT1jP5C+aMy1t5wvBkxZRFQVzv2hvCrkvb74n1tVE1DQN9QYmF4exrgxQ1rHCp
         2kWDu4D8lbN4dU+h1nZB+fo8CDxKHqS4j169GccwPKVQpGxCGbaQRR0ISryDx5qA87Vi
         1n4vADMuaWbrJfBjXM2hMy7pjs1ZNGlWtWcUeu1TFDwWJKN4r5HkM5EvXO7qWHFfnom6
         9abGzKeTFWoG2AgZW0DB3dPveqtSnyonTtHetiZST+vdGro0HGE80yYo0VcmO67JLWcb
         touAXFPOpC20RsTTNjqe12Zbd9VW/7yfYzlyj5waQQPPuqtKqTdXCMpVIoGB24eVmJUh
         H5/w==
MIME-Version: 1.0
X-Received: by 10.50.117.106 with SMTP id kd10mr14428402igb.5.1406780753948;
 Wed, 30 Jul 2014 21:25:53 -0700 (PDT)
Received: by 10.107.35.213 with HTTP; Wed, 30 Jul 2014 21:25:53 -0700 (PDT)
Date: Wed, 30 Jul 2014 21:25:53 -0700
Message-ID: <CA+FETEKK7_7w_Z3ii=Yz-X+fwsTA2vzvYJxcEfCm6rFX=MNw1g@mail.gmail.com>
Subject: failed to build spark with maven for both 1.0.1 and latest master branch
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0139fb621db9d704ff75abc4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0139fb621db9d704ff75abc4
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi Folks,

Today I am trying to build spark using maven; however, the following
command failed consistently for both 1.0.1 and the latest master.  (BTW, it
seems sbt works fine: *sbt/sbt -Dhadoop.version=3D2.4.0 -Pyarn clean
assembly)*

Environment: Mac OS Mavericks
Maven: 3.2.2 (installed by homebrew)




*export M2_HOME=3D/usr/local/Cellar/maven/3.2.2/libexec/export
PATH=3D$M2_HOME/bin:$PATHexport MAVEN_OPTS=3D"-Xmx2g -XX:MaxPermSize=3D512M
-XX:ReservedCodeCacheSize=3D512m"mvn -Pyarn -Phadoop-2.4
-Dhadoop.version=3D2.4.0 -DskipTests clean package*

Build outputs:

[INFO] Scanning for projects...
[INFO]
------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO]
[INFO] Spark Project Parent POM
[INFO] Spark Project Core
[INFO] Spark Project Bagel
[INFO] Spark Project GraphX
[INFO] Spark Project ML Library
[INFO] Spark Project Streaming
[INFO] Spark Project Tools
[INFO] Spark Project Catalyst
[INFO] Spark Project SQL
[INFO] Spark Project Hive
[INFO] Spark Project REPL
[INFO] Spark Project YARN Parent POM
[INFO] Spark Project YARN Stable API
[INFO] Spark Project Assembly
[INFO] Spark Project External Twitter
[INFO] Spark Project External Kafka
[INFO] Spark Project External Flume
[INFO] Spark Project External ZeroMQ
[INFO] Spark Project External MQTT
[INFO] Spark Project Examples
[INFO]
[INFO]
------------------------------------------------------------------------
[INFO] Building Spark Project Parent POM 1.0.1
[INFO]
------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-parent ---
[INFO]
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
spark-parent ---
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @
spark-parent ---
[INFO] Source directory:
/Users/syao/git/grid/thirdparty/spark/src/main/scala added.
[INFO]
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @
spark-parent ---
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
spark-parent ---
[INFO] Add Test Source directory:
/Users/syao/git/grid/thirdparty/spark/src/test/scala
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
spark-parent ---
[INFO] No sources to compile
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:add-test-source
(add-scala-test-sources) @ spark-parent ---
[INFO] Test Source directory:
/Users/syao/git/grid/thirdparty/spark/src/test/scala added.
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:testCompile (scala-test-compile-first)
@ spark-parent ---
[INFO] No sources to compile
[INFO]
[INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @
spark-parent ---
[INFO]
[INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar) @
spark-parent ---
[INFO]
[INFO]
------------------------------------------------------------------------
[INFO] Building Spark Project Core 1.0.1
[INFO]
------------------------------------------------------------------------
[INFO]
[INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-core_2.10
---
[INFO]
[INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
spark-core_2.10 ---
[INFO]
[INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @
spark-core_2.10 ---
[INFO] Source directory:
/Users/syao/git/grid/thirdparty/spark/core/src/main/scala added.
[INFO]
[INFO] --- maven-remote-resources-plugin:1.5:process (default) @
spark-core_2.10 ---
[INFO]
[INFO] --- exec-maven-plugin:1.2.1:exec (default) @ spark-core_2.10 ---
Archive:  lib/py4j-0.8.1-src.zip
  inflating: build/py4j/tests/java_map_test.py
 extracting: build/py4j/tests/__init__.py
  inflating: build/py4j/tests/java_gateway_test.py
  inflating: build/py4j/tests/java_callback_test.py
  inflating: build/py4j/tests/java_list_test.py
  inflating: build/py4j/tests/byte_string_test.py
  inflating: build/py4j/tests/multithreadtest.py
  inflating: build/py4j/tests/java_array_test.py
  inflating: build/py4j/tests/py4j_callback_example2.py
  inflating: build/py4j/tests/py4j_example.py
  inflating: build/py4j/tests/py4j_callback_example.py
  inflating: build/py4j/tests/finalizer_test.py
  inflating: build/py4j/tests/java_set_test.py
  inflating: build/py4j/finalizer.py
 extracting: build/py4j/__init__.py
  inflating: build/py4j/java_gateway.py
  inflating: build/py4j/protocol.py
  inflating: build/py4j/java_collections.py
 extracting: build/py4j/version.py
  inflating: build/py4j/compat.py
[INFO]
[INFO] --- maven-resources-plugin:2.6:resources (default-resources) @
spark-core_2.10 ---
[INFO] Using 'UTF-8' encoding to copy filtered resources.
[INFO] Copying 6 resources
[INFO] Copying 20 resources
[INFO] Copying 7 resources
[INFO] Copying 3 resources
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
spark-core_2.10 ---
[INFO] Add Test Source directory:
/Users/syao/git/grid/thirdparty/spark/core/src/test/scala
[INFO]
[INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
spark-core_2.10 ---
[INFO] Using zinc server for incremental compilation
[info] Compiling 342 Scala sources and 34 Java sources to
/Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes...
[warn] Class javax.servlet.ServletException not found - continuing with a
stub.
[error]
[error]      while compiling:
/Users/syao/git/grid/thirdparty/spark/core/src/main/scala/org/apache/spark/=
HttpServer.scala
[error]         during phase: typer
[error]      library version: version 2.10.4
[error]     compiler version: version 2.10.4
[error]   reconstructed args: -classpath
/Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes:/Users=
/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-2.=
4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/ha=
doop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/1.=
2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commons=
-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xmle=
nc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/commo=
ns-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/com=
mons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/U=
sers/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.jar=
:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1.6=
/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digester/=
commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/co=
mmons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/=
syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-=
beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/ja=
ckson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repository/=
org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.jar:=
/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Users=
/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-=
2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/ha=
doop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons-c=
ompress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/tuk=
aani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-=
hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/jet=
ty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org/a=
pache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-app-=
2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-cli=
ent-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2/r=
epository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2.4=
.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-c=
lient-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-serv=
er-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/reposit=
ory/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapreduc=
e-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/had=
oop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/org=
/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client-c=
ore-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-comm=
on/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xml/=
bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/xml=
/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/javax/=
activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/com=
/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repository/=
org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduce-=
client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/had=
oop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/reposito=
ry/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/re=
pository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:/U=
sers/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcore-=
4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2.4=
.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curator/=
curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/reposit=
ory/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users=
/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.j=
ar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v20131031=
/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/jet=
ty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v201=
105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8.1=
.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repository/=
org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031.j=
ar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfish/=
1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/sya=
o/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v2011050712=
33/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/org/=
eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v201310=
31.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v2013=
1031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse=
/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/Use=
rs/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/User=
s/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-=
3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/=
jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf4j=
-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/jul-=
to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.=
5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/lo=
g4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/slf=
4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0.0=
/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snappy=
-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/ch=
ill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esoteric=
software/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esoter=
icsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/sya=
o/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Use=
rs/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users=
/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/Use=
rs/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Use=
rs/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shaded=
-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/reposi=
tory/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-acto=
r_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/co=
nfig/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6.6=
.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project/p=
rotobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/sy=
ao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-math=
s-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j_2=
.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/Users/=
syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2.1=
0-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/js=
on4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast_2=
.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/thoughtw=
orks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository/o=
rg/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/or=
g/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/.m=
2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-data=
bind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jackso=
n-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/repositor=
y/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/Use=
rs/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/repos=
itory/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/repo=
sitory/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/User=
s/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Fina=
l.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/str=
eam-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-core/=
3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metric=
s/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/co=
dahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/re=
pository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0.0=
.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/tac=
hyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0/a=
nt-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0/a=
nt-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.4/=
commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflect/=
2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-projec=
t/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4j/=
py4j/0.8.1/py4j-0.8.1.jar
-deprecation -feature -bootclasspath
/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/res=
ources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/=
jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/H=
ome/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jd=
k/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0=
_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk=
1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMa=
chines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirt=
ualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java/J=
avaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/.m=
2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
-unchecked -language:postfixOps
[error]
[error]   last tree to typer: Ident(Server)
[error]               symbol: <none> (flags: )
[error]    symbol definition: <none>
[error]        symbol owners:
[error]       context owners: variable server -> class HttpServer ->
package spark
[error]
[error] =3D=3D Enclosing template or block =3D=3D
[error]
[error] Template( // val <local HttpServer>: <notype> in class HttpServer
[error]   "org.apache.spark.Logging" // parents
[error]   ValDef(
[error]     private
[error]     "_"
[error]     <tpt>
[error]     <empty>
[error]   )
[error]   // 9 statements
[error]   ValDef( // private[this] val resourceBase: <?> in class HttpServe=
r
[error]     private <local> <paramaccessor>
[error]     "resourceBase"
[error]     "File"
[error]     <empty>
[error]   )
[error]   ValDef( // private[this] val securityManager: <?> in class
HttpServer
[error]     private <local> <paramaccessor>
[error]     "securityManager"
[error]     "SecurityManager"
[error]     <empty>
[error]   )
[error]   DefDef( // def <init>(resourceBase: java.io.File,securityManager:
org.apache.spark.SecurityManager): org.apache.spark.HttpServer in class
HttpServer
[error]     <method> <triedcooking>
[error]     "<init>"
[error]     []
[error]     // 1 parameter list
[error]     ValDef( // resourceBase: java.io.File
[error]       <param> <paramaccessor>
[error]       "resourceBase"
[error]       "File"
[error]       <empty>
[error]     )
[error]     ValDef( // securityManager: org.apache.spark.SecurityManager
[error]       <param> <paramaccessor>
[error]       "securityManager"
[error]       "SecurityManager" // private[package spark] class
SecurityManager extends Logging in package spark,
tree.tpe=3Dorg.apache.spark.SecurityManager
[error]       <empty>
[error]     )
[error]     <tpt> // tree.tpe=3Dorg.apache.spark.HttpServer
[error]     Block(
[error]       Apply(
[error]         super."<init>"
[error]         Nil
[error]       )
[error]       ()
[error]     )
[error]   )
[error]   ValDef( // private[this] var server: <?> in class HttpServer
[error]     private <mutable> <local>
[error]     "server"
[error]     "Server"
[error]     null
[error]   )
[error]   ValDef( // private[this] var port: <?> in class HttpServer
[error]     private <mutable> <local>
[error]     "port"
[error]     "Int"
[error]     -1
[error]   )
[error]   DefDef( // def start(): Unit in class HttpServer
[error]     <method> <triedcooking>
[error]     "start"
[error]     []
[error]     List(Nil)
[error]     "scala"."Unit" // final abstract class Unit extends AnyVal in
package scala, tree.tpe=3DUnit
[error]     If(
[error]       Apply(
[error]         "server"."$bang$eq"
[error]         null
[error]       )
[error]       Throw(
[error]         Apply(
[error]           new ServerStateException."<init>"
[error]           "Server is already started"
[error]         )
[error]       )
[error]       Block(
[error]         // 16 statements
[error]         Apply(
[error]           "logInfo"
[error]           "Starting HTTP Server"
[error]         )
[error]         Assign(
[error]           "server"
[error]           Apply(
[error]             new Server."<init>"
[error]             Nil
[error]           )
[error]         )
[error]         ValDef(
[error]           0
[error]           "connector"
[error]           <tpt>
[error]           Apply(
[error]             new SocketConnector."<init>"
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           "connector"."setMaxIdleTime"
[error]           Apply(
[error]             60."$times"
[error]             1000
[error]           )
[error]         )
[error]         Apply(
[error]           "connector"."setSoLingerTime"
[error]           -1
[error]         )
[error]         Apply(
[error]           "connector"."setPort"
[error]           0
[error]         )
[error]         Apply(
[error]           "server"."addConnector"
[error]           "connector"
[error]         )
[error]         ValDef(
[error]           0
[error]           "threadPool"
[error]           <tpt>
[error]           Apply(
[error]             new QueuedThreadPool."<init>"
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           "threadPool"."setDaemon"
[error]           true
[error]         )
[error]         Apply(
[error]           "server"."setThreadPool"
[error]           "threadPool"
[error]         )
[error]         ValDef(
[error]           0
[error]           "resHandler"
[error]           <tpt>
[error]           Apply(
[error]             new ResourceHandler."<init>"
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           "resHandler"."setResourceBase"
[error]           "resourceBase"."getAbsolutePath"
[error]         )
[error]         ValDef(
[error]           0
[error]           "handlerList"
[error]           <tpt>
[error]           Apply(
[error]             new HandlerList."<init>"
[error]             Nil
[error]           )
[error]         )
[error]         Apply(
[error]           "handlerList"."setHandlers"
[error]           Apply(
[error]             "Array"
[error]             // 2 arguments
[error]             "resHandler"
[error]             Apply(
[error]               new DefaultHandler."<init>"
[error]               Nil
[error]             )
[error]           )
[error]         )
[error]         If(
[error]           Apply(
[error]             "securityManager"."isAuthenticationEnabled"
[error]             Nil
[error]           )
[error]           Block(
[error]             // 3 statements
[error]             Apply(
[error]               "logDebug"
[error]               "HttpServer is using security"
[error]             )
[error]             ValDef(
[error]               0
[error]               "sh"
[error]               <tpt>
[error]               Apply(
[error]                 "setupSecurityHandler"
[error]                 "securityManager"
[error]               )
[error]             )
[error]             Apply(
[error]               "sh"."setHandler"
[error]               "handlerList"
[error]             )
[error]             Apply(
[error]               "server"."setHandler"
[error]               "sh"
[error]             )
[error]           )
[error]           Block(
[error]             Apply(
[error]               "logDebug"
[error]               "HttpServer is not using security"
[error]             )
[error]             Apply(
[error]               "server"."setHandler"
[error]               "handlerList"
[error]             )
[error]           )
[error]         )
[error]         Apply(
[error]           "server"."start"
[error]           Nil
[error]         )
[error]         Assign(
[error]           "port"
[error]           Apply(
[error]             server.getConnectors()(0)."getLocalPort"
[error]             Nil
[error]           )
[error]         )
[error]       )
[error]     )
[error]   )
[error]   DefDef( // private def setupSecurityHandler: <?> in class
HttpServer
[error]     <method> private
[error]     "setupSecurityHandler"
[error]     []
[error]     // 1 parameter list
[error]     ValDef(
[error]       <param>
[error]       "securityMgr"
[error]       "SecurityManager"
[error]       <empty>
[error]     )
[error]     "ConstraintSecurityHandler"
[error]     Block(
[error]       // 16 statements
[error]       ValDef(
[error]         0
[error]         "constraint"
[error]         <tpt>
[error]         Apply(
[error]           new Constraint."<init>"
[error]           Nil
[error]         )
[error]       )
[error]       Apply(
[error]         "constraint"."setName"
[error]         "Constraint"."__DIGEST_AUTH"
[error]       )
[error]       Apply(
[error]         "constraint"."setRoles"
[error]         Apply(
[error]           "Array"
[error]           "user"
[error]         )
[error]       )
[error]       Apply(
[error]         "constraint"."setAuthenticate"
[error]         true
[error]       )
[error]       Apply(
[error]         "constraint"."setDataConstraint"
[error]         "Constraint"."DC_NONE"
[error]       )
[error]       ValDef(
[error]         0
[error]         "cm"
[error]         <tpt>
[error]         Apply(
[error]           new ConstraintMapping."<init>"
[error]           Nil
[error]         )
[error]       )
[error]       Apply(
[error]         "cm"."setConstraint"
[error]         "constraint"
[error]       )
[error]       Apply(
[error]         "cm"."setPathSpec"
[error]         "/*"
[error]       )
[error]       ValDef(
[error]         0
[error]         "sh"
[error]         <tpt>
[error]         Apply(
[error]           new ConstraintSecurityHandler."<init>"
[error]           Nil
[error]         )
[error]       )
[error]       ValDef(
[error]         0
[error]         "hashLogin"
[error]         <tpt>
[error]         Apply(
[error]           new HashLoginService."<init>"
[error]           Nil
[error]         )
[error]       )
[error]       ValDef(
[error]         0
[error]         "userCred"
[error]         <tpt>
[error]         Apply(
[error]           new Password."<init>"
[error]           Apply(
[error]             "securityMgr"."getSecretKey"
[error]             Nil
[error]           )
[error]         )
[error]       )
[error]       If(
[error]         Apply(
[error]           "userCred"."$eq$eq"
[error]           null
[error]         )
[error]         Throw(
[error]           Apply(
[error]             new Exception."<init>"
[error]             "Error: secret key is null with authentication on"
[error]           )
[error]         )
[error]         ()
[error]       )
[error]       Apply(
[error]         "hashLogin"."putUser"
[error]         // 3 arguments
[error]         Apply(
[error]           "securityMgr"."getHttpUser"
[error]           Nil
[error]         )
[error]         "userCred"
[error]         Apply(
[error]           "Array"
[error]           "user"
[error]         )
[error]       )
[error]       Apply(
[error]         "sh"."setLoginService"
[error]         "hashLogin"
[error]       )
[error]       Apply(
[error]         "sh"."setAuthenticator"
[error]         Apply(
[error]           new DigestAuthenticator."<init>"
[error]           Nil
[error]         )
[error]       )
[error]       Apply(
[error]         "sh"."setConstraintMappings"
[error]         Apply(
[error]           "Array"
[error]           "cm"
[error]         )
[error]       )
[error]       "sh"
[error]     )
[error]   )
[error]   DefDef( // def stop(): Unit in class HttpServer
[error]     <method> <triedcooking>
[error]     "stop"
[error]     []
[error]     List(Nil)
[error]     "scala"."Unit" // final abstract class Unit extends AnyVal in
package scala, tree.tpe=3DUnit
[error]     If(
[error]       Apply(
[error]         "server"."$eq$eq"
[error]         null
[error]       )
[error]       Throw(
[error]         Apply(
[error]           new ServerStateException."<init>"
[error]           "Server is already stopped"
[error]         )
[error]       )
[error]       Block(
[error]         // 2 statements
[error]         Apply(
[error]           "server"."stop"
[error]           Nil
[error]         )
[error]         Assign(
[error]           "port"
[error]           -1
[error]         )
[error]         Assign(
[error]           "server"
[error]           null
[error]         )
[error]       )
[error]     )
[error]   )
[error]   DefDef( // def uri: String in class HttpServer
[error]     <method> <triedcooking>
[error]     "uri"
[error]     []
[error]     Nil
[error]     "String"
[error]     If(
[error]       Apply(
[error]         "server"."$eq$eq"
[error]         null
[error]       )
[error]       Throw(
[error]         Apply(
[error]           new ServerStateException."<init>"
[error]           "Server is not started"
[error]         )
[error]       )
[error]       Return(
[error]         Apply(
[error]           "http://".$plus(Utils.localIpAddress).$plus(":")."$plus"
[error]           "port"
[error]         )
[error]       )
[error]     )
[error]   )
[error] )
[error]
[error] uncaught exception during compilation: java.lang.AssertionError
java.lang.AssertionError: assertion failed: javax.servlet.ServletException
    at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)
    at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseExceptions$1(Classfil=
eParser.scala:1051)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$cla=
ssfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseAttributes(ClassfileP=
arser.scala:1080)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseMethod(ClassfileParse=
r.scala:666)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$cla=
ssfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser$$anonfun$parseClass$1.appl=
y$mcV$sp(ClassfileParser.scala:567)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfileParser=
.scala:572)
    at
scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.scal=
a:88)
    at
scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoade=
rs.scala:261)
    at
scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.sc=
ala:194)
    at
scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scala:=
210)
    at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:893)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:5064)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.scal=
a:5218)
    at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5561)
    at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
    at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5769=
)
    at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5772=
)
    at scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1317=
)
    at scala.tools.nsc.typechecker.Namers$Namer.getSig$1(Namers.scala:1457)
    at scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1466)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anon=
fun$apply$1.apply$mcV$sp(Namers.scala:731)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anon=
fun$apply$1.apply(Namers.scala:730)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$anon=
fun$apply$1.apply(Namers.scala:730)
    at
scala.tools.nsc.typechecker.Namers$Namer.scala$tools$nsc$typechecker$Namers=
$Namer$$logAndValidate(Namers.scala:1499)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.apply=
(Namers.scala:730)
    at
scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.apply=
(Namers.scala:729)
    at
scala.tools.nsc.typechecker.Namers$$anon$1.completeImpl(Namers.scala:1614)
    at
scala.tools.nsc.typechecker.Namers$LockingTypeCompleter$class.complete(Name=
rs.scala:1622)
    at
scala.tools.nsc.typechecker.Namers$$anon$1.complete(Namers.scala:1612)
    at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
    at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
    at
scala.tools.nsc.typechecker.MethodSynthesis$MethodSynth$class.addDerivedTre=
es(MethodSynthesis.scala:225)
    at
scala.tools.nsc.typechecker.Namers$Namer.addDerivedTrees(Namers.scala:55)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:191=
7)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:191=
7)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$1.=
apply(Typers.scala:1856)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$1.=
apply(Typers.scala:1853)
    at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.s=
cala:251)
    at
scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.s=
cala:251)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at
scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
    at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1917)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:1759)
    at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5583)
    at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
    at
scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Typers=
$Typer$$typedStat$1(Typers.scala:2928)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:303=
2)
    at
scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:303=
2)
    at scala.collection.immutable.List.loop$1(List.scala:170)
    at scala.collection.immutable.List.mapConserve(List.scala:186)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032)
    at
scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:530=
1)
    at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5587)
    at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
    at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5704)
    at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.sc=
ala:99)
    at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
    at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.ap=
ply(Analyzer.scala:91)
    at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.ap=
ply(Analyzer.scala:91)
    at scala.collection.Iterator$class.foreach(Iterator.scala:727)
    at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
    at
scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.scal=
a:91)
    at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
    at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
    at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
    at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
    at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
    at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
    at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:5=
7)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImp=
l.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
    at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
    at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
    at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply$m=
cV$sp(AggressiveCompile.scala:99)
    at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(A=
ggressiveCompile.scala:99)
    at
sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply(A=
ggressiveCompile.scala:99)
    at
sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(Aggres=
siveCompile.scala:166)
    at
sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompile.=
scala:98)
    at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:143=
)
    at
sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:87)
    at
sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
    at
sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
    at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
    at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
    at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
    at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
    at sbt.inc.Incremental$.compile(Incremental.scala:37)
    at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
    at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
    at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
    at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
    at com.typesafe.zinc.Main$.run(Main.scala:98)
    at com.typesafe.zinc.Nailgun$.zinc(Nailgun.scala:93)
    at com.typesafe.zinc.Nailgun$.nailMain(Nailgun.scala:82)
    at com.typesafe.zinc.Nailgun.nailMain(Nailgun.scala)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:5=
7)
    at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImp=
l.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at com.martiansoftware.nailgun.NGSession.run(NGSession.java:280)
[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM ........................... SUCCESS [
1.760 s]
[INFO] Spark Project Core ................................. FAILURE [
5.312 s]
[INFO] Spark Project Bagel ................................ SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project YARN Parent POM ...................... SKIPPED
[INFO] Spark Project YARN Stable API ...................... SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Spark Project External Twitter ..................... SKIPPED
[INFO] Spark Project External Kafka ....................... SKIPPED
[INFO] Spark Project External Flume ....................... SKIPPED
[INFO] Spark Project External ZeroMQ ...................... SKIPPED
[INFO] Spark Project External MQTT ........................ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO]
------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 7.562 s
[INFO] Finished at: 2014-07-30T21:18:41-07:00
[INFO] Final Memory: 39M/713M
[INFO]
------------------------------------------------------------------------
[ERROR] Failed to execute goal
net.alchim31.maven:scala-maven-plugin:3.1.6:compile (scala-compile-first)
on project spark-core_2.10: Execution scala-compile-first of goal
net.alchim31.maven:scala-maven-plugin:3.1.6:compile failed. CompileFailed
-> [Help 1]

Thanks
Shengzhe

--089e0139fb621db9d704ff75abc4--

From dev-return-8645-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 06:31:28 2014
Return-Path: <dev-return-8645-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5E88611F6E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 06:31:28 +0000 (UTC)
Received: (qmail 1650 invoked by uid 500); 31 Jul 2014 06:31:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 1598 invoked by uid 500); 31 Jul 2014 06:31:27 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 1584 invoked by uid 99); 31 Jul 2014 06:31:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 06:31:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.50 as permitted sender)
Received: from [209.85.219.50] (HELO mail-oa0-f50.google.com) (209.85.219.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 06:31:23 +0000
Received: by mail-oa0-f50.google.com with SMTP id g18so1713280oah.37
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 23:31:02 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0MIyWAVdFp2QSi4taT3/eMkyCIs+h9tEcHrDjNhqLN8=;
        b=Jaj0Is41KVxylde7CW4cGs0WQO7TLu3QqPTNHfdgH2L15/+o6fK+kD5WBeit7qhT8l
         zGC7JTbIr2uHeUoUrCdwdz0ZY/kDn5tri+uIqg2dxjm3zRrs08GWWKb+gNcqb9mWuBFb
         55yzTQXz4JRoXCfx7Uph3BTCpxF20c5wuohBojiM3ltUbtAWHLRKrchRHqe1Lw/gBarl
         Px4mqSrqybinLYof/8UjiY978ox4um6XWJuBpBEscwkK7UTPhjUoELiCunmvnPc8GYkE
         KEdzptRJVvwiUIKqqqaGlDP5vMyk366DIdVqRfPV13YhRq02fF7mLjkKoOdLqHudWQ9O
         O23w==
MIME-Version: 1.0
X-Received: by 10.60.62.197 with SMTP id a5mr5089897oes.78.1406788262473; Wed,
 30 Jul 2014 23:31:02 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Wed, 30 Jul 2014 23:31:02 -0700 (PDT)
In-Reply-To: <CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
Date: Wed, 30 Jul 2014 23:31:02 -0700
Message-ID: <CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Thanks for digging around here. I think there are a few distinct issues.

1. Properties containing the '=' character need to be escaped.
I was able to load properties fine as long as I escape the '='
character. But maybe we should document this:

== spark-defaults.conf ==
spark.foo a\=B
== shell ==
scala> sc.getConf.get("spark.foo")
res2: String = a=B

2. spark.driver.extraJavaOptions, when set in the properties file,
don't affect the driver when running in client mode (always the case
for mesos). We should probably document this. In this case you need to
either use --driver-java-options or set SPARK_SUBMIT_OPTS.

3. Arguments aren't propagated on Mesos (this might be because of the
other issues, or a separate bug).

- Patrick

On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <cody@koeninger.org> wrote:
> In addition, spark.executor.extraJavaOptions does not seem to behave as I
> would expect; java arguments don't seem to be propagated to executors.
>
>
> $ cat conf/spark-defaults.conf
>
> spark.master
> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>
>
> $ ./bin/spark-shell
>
> scala> sc.getConf.get("spark.executor.extraJavaOptions")
> res0: String = -Dfoo.bar.baz=23
>
> scala> sc.parallelize(1 to 100).map{ i => (
>      |  java.net.InetAddress.getLocalHost.getHostName,
>      |  System.getProperty("foo.bar.baz")
>      | )}.collect
>
> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
> (dn-02.mxstg,null), ...
>
>
>
> Note that this is a mesos deployment, although I wouldn't expect that to
> affect the availability of spark.driver.extraJavaOptions in a local spark
> shell.
>
>
> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <cody@koeninger.org> wrote:
>
>> Either whitespace or equals sign are valid properties file formats.
>> Here's an example:
>>
>> $ cat conf/spark-defaults.conf
>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>
>> $ ./bin/spark-shell -v
>> Using properties file: /opt/spark/conf/spark-defaults.conf
>> Adding default property: spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>
>>
>> scala>  System.getProperty("foo.bar.baz")
>> res0: String = null
>>
>>
>> If you add double quotes, the resulting string value will have double
>> quotes.
>>
>>
>> $ cat conf/spark-defaults.conf
>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>
>> $ ./bin/spark-shell -v
>> Using properties file: /opt/spark/conf/spark-defaults.conf
>> Adding default property: spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
>>
>> scala>  System.getProperty("foo.bar.baz")
>> res0: String = null
>>
>>
>> Neither one of those affects the issue; the underlying problem in my case
>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
>> SPARK_JAVA_OPTS environment variables, but nothing parses
>> spark-defaults.conf before the java process is started.
>>
>> Here's an example of the process running when only spark-defaults.conf is
>> being used:
>>
>> $ ps -ef | grep spark
>>
>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash ./bin/spark-shell -v
>>
>> 514       5189  5182  4 21:05 pts/2    00:00:22 /usr/local/java/bin/java
>> -cp
>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
>> org.apache.spark.repl.Main
>>
>>
>> Here's an example of it when the command line --driver-java-options is
>> used (and thus things work):
>>
>>
>> $ ps -ef | grep spark
>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash ./bin/spark-shell -v
>> --driver-java-options -Dfoo.bar.baz=23
>>
>> 514       5399  5392 80 21:15 pts/2    00:00:06 /usr/local/java/bin/java
>> -cp
>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
>> --driver-java-options -Dfoo.bar.baz=23 --class org.apache.spark.repl.Main
>>
>>
>>
>>
>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <pwendell@gmail.com>
>> wrote:
>>
>>> Cody - in your example you are using the '=' character, but in our
>>> documentation and tests we use a whitespace to separate the key and
>>> value in the defaults file.
>>>
>>> docs: http://spark.apache.org/docs/latest/configuration.html
>>>
>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>>
>>> I'm not sure if the java properties file parser will try to interpret
>>> the equals sign. If so you might need to do this.
>>>
>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>>
>>> Do those work for you?
>>>
>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com>
>>> wrote:
>>> > Hi Cody,
>>> >
>>> > Could you file a bug for this if there isn't one already?
>>> >
>>> > For system properties SparkSubmit should be able to read those
>>> > settings and do the right thing, but that obviously won't work for
>>> > other JVM options... the current code should work fine in cluster mode
>>> > though, since the driver is a different process. :-)
>>> >
>>> >
>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <cody@koeninger.org>
>>> wrote:
>>> >> We were previously using SPARK_JAVA_OPTS to set java system properties
>>> via
>>> >> -D.
>>> >>
>>> >> This was used for properties that varied on a
>>> per-deployment-environment
>>> >> basis, but needed to be available in the spark shell and workers.
>>> >>
>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been deprecated,
>>> and
>>> >> replaced by spark-defaults.conf and command line arguments to
>>> spark-submit
>>> >> or spark-shell.
>>> >>
>>> >> However, setting spark.driver.extraJavaOptions and
>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
>>> replacement
>>> >> for SPARK_JAVA_OPTS:
>>> >>
>>> >>
>>> >> $ cat conf/spark-defaults.conf
>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>> >>
>>> >> $ ./bin/spark-shell
>>> >>
>>> >> scala> System.getProperty("foo.bar.baz")
>>> >> res0: String = null
>>> >>
>>> >>
>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>>> >>
>>> >> scala> System.getProperty("foo.bar.baz")
>>> >> res0: String = 23
>>> >>
>>> >>
>>> >> Looking through the shell scripts for spark-submit and spark-class, I
>>> can
>>> >> see why this is; parsing spark-defaults.conf from bash could be
>>> brittle.
>>> >>
>>> >> But from an ergonomic point of view, it's a step back to go from a
>>> >> set-it-and-forget-it configuration in spark-env.sh, to requiring
>>> command
>>> >> line arguments.
>>> >>
>>> >> I can solve this with an ad-hoc script to wrap spark-shell with the
>>> >> appropriate arguments, but I wanted to bring the issue up to see if
>>> anyone
>>> >> else had run into it,
>>> >> or had any direction for a general solution (beyond parsing java
>>> properties
>>> >> files from bash).
>>> >
>>> >
>>> >
>>> > --
>>> > Marcelo
>>>
>>
>>

From dev-return-8646-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 06:33:59 2014
Return-Path: <dev-return-8646-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B89F011F83
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 06:33:59 +0000 (UTC)
Received: (qmail 5291 invoked by uid 500); 31 Jul 2014 06:33:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 5234 invoked by uid 500); 31 Jul 2014 06:33:58 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 5222 invoked by uid 99); 31 Jul 2014 06:33:58 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 06:33:58 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.214.174 as permitted sender)
Received: from [209.85.214.174] (HELO mail-ob0-f174.google.com) (209.85.214.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 06:33:56 +0000
Received: by mail-ob0-f174.google.com with SMTP id vb8so1287952obc.33
        for <dev@spark.apache.org>; Wed, 30 Jul 2014 23:33:31 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=+jqvsNa/Bi3YSz8pcLv8w/x/xJscu6u/+cqwnumxErc=;
        b=x2xqrER5aOknrdqM/87kcvpo/92m9sqNqTUQ5tXd0MSyWIXoIZdPQMxntgVcvpExgd
         ze7+Ak0zvgUPl8Do+gkWKH3gcOqVNWM3vTnesS9Srq4U71siLxQA8u/H+JJM2RLUbbdS
         aayDGsXmAXXayF1mGoD50nXNaifEk3UoEnkD/hNMkaGuHyprpN4Ws85/hzo1u9wKYobh
         oYJWit2zX3yri47cVYpuDD1H8HJSGhH/fVlam4NiRmss+elyAGK2Qy7MWbCT/hVB7WEX
         5C3s/LUqY9iwr+E5AhK4NiQLZT09BuD3AziJhCm4/ExTnxLH/CIy692iywf7Zqtj+SRx
         sS4g==
MIME-Version: 1.0
X-Received: by 10.182.246.232 with SMTP id xz8mr2720318obc.5.1406788411734;
 Wed, 30 Jul 2014 23:33:31 -0700 (PDT)
Received: by 10.202.187.134 with HTTP; Wed, 30 Jul 2014 23:33:31 -0700 (PDT)
In-Reply-To: <CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
Date: Wed, 30 Jul 2014 23:33:31 -0700
Message-ID: <CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

The third issue may be related to this:
https://issues.apache.org/jira/browse/SPARK-2022

We can take a look at this during the bug fix period for the 1.1
release next week. If we come up with a fix we can backport it into
the 1.0 branch also.

On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Thanks for digging around here. I think there are a few distinct issues.
>
> 1. Properties containing the '=' character need to be escaped.
> I was able to load properties fine as long as I escape the '='
> character. But maybe we should document this:
>
> == spark-defaults.conf ==
> spark.foo a\=B
> == shell ==
> scala> sc.getConf.get("spark.foo")
> res2: String = a=B
>
> 2. spark.driver.extraJavaOptions, when set in the properties file,
> don't affect the driver when running in client mode (always the case
> for mesos). We should probably document this. In this case you need to
> either use --driver-java-options or set SPARK_SUBMIT_OPTS.
>
> 3. Arguments aren't propagated on Mesos (this might be because of the
> other issues, or a separate bug).
>
> - Patrick
>
> On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <cody@koeninger.org> wrote:
>> In addition, spark.executor.extraJavaOptions does not seem to behave as I
>> would expect; java arguments don't seem to be propagated to executors.
>>
>>
>> $ cat conf/spark-defaults.conf
>>
>> spark.master
>> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
>> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>
>>
>> $ ./bin/spark-shell
>>
>> scala> sc.getConf.get("spark.executor.extraJavaOptions")
>> res0: String = -Dfoo.bar.baz=23
>>
>> scala> sc.parallelize(1 to 100).map{ i => (
>>      |  java.net.InetAddress.getLocalHost.getHostName,
>>      |  System.getProperty("foo.bar.baz")
>>      | )}.collect
>>
>> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
>> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
>> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
>> (dn-02.mxstg,null), ...
>>
>>
>>
>> Note that this is a mesos deployment, although I wouldn't expect that to
>> affect the availability of spark.driver.extraJavaOptions in a local spark
>> shell.
>>
>>
>> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <cody@koeninger.org> wrote:
>>
>>> Either whitespace or equals sign are valid properties file formats.
>>> Here's an example:
>>>
>>> $ cat conf/spark-defaults.conf
>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>>
>>> $ ./bin/spark-shell -v
>>> Using properties file: /opt/spark/conf/spark-defaults.conf
>>> Adding default property: spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>>
>>>
>>> scala>  System.getProperty("foo.bar.baz")
>>> res0: String = null
>>>
>>>
>>> If you add double quotes, the resulting string value will have double
>>> quotes.
>>>
>>>
>>> $ cat conf/spark-defaults.conf
>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>>
>>> $ ./bin/spark-shell -v
>>> Using properties file: /opt/spark/conf/spark-defaults.conf
>>> Adding default property: spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
>>>
>>> scala>  System.getProperty("foo.bar.baz")
>>> res0: String = null
>>>
>>>
>>> Neither one of those affects the issue; the underlying problem in my case
>>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
>>> SPARK_JAVA_OPTS environment variables, but nothing parses
>>> spark-defaults.conf before the java process is started.
>>>
>>> Here's an example of the process running when only spark-defaults.conf is
>>> being used:
>>>
>>> $ ps -ef | grep spark
>>>
>>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash ./bin/spark-shell -v
>>>
>>> 514       5189  5182  4 21:05 pts/2    00:00:22 /usr/local/java/bin/java
>>> -cp
>>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
>>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
>>> org.apache.spark.repl.Main
>>>
>>>
>>> Here's an example of it when the command line --driver-java-options is
>>> used (and thus things work):
>>>
>>>
>>> $ ps -ef | grep spark
>>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash ./bin/spark-shell -v
>>> --driver-java-options -Dfoo.bar.baz=23
>>>
>>> 514       5399  5392 80 21:15 pts/2    00:00:06 /usr/local/java/bin/java
>>> -cp
>>> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
>>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
>>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
>>> --driver-java-options -Dfoo.bar.baz=23 --class org.apache.spark.repl.Main
>>>
>>>
>>>
>>>
>>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <pwendell@gmail.com>
>>> wrote:
>>>
>>>> Cody - in your example you are using the '=' character, but in our
>>>> documentation and tests we use a whitespace to separate the key and
>>>> value in the defaults file.
>>>>
>>>> docs: http://spark.apache.org/docs/latest/configuration.html
>>>>
>>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
>>>>
>>>> I'm not sure if the java properties file parser will try to interpret
>>>> the equals sign. If so you might need to do this.
>>>>
>>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
>>>>
>>>> Do those work for you?
>>>>
>>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com>
>>>> wrote:
>>>> > Hi Cody,
>>>> >
>>>> > Could you file a bug for this if there isn't one already?
>>>> >
>>>> > For system properties SparkSubmit should be able to read those
>>>> > settings and do the right thing, but that obviously won't work for
>>>> > other JVM options... the current code should work fine in cluster mode
>>>> > though, since the driver is a different process. :-)
>>>> >
>>>> >
>>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <cody@koeninger.org>
>>>> wrote:
>>>> >> We were previously using SPARK_JAVA_OPTS to set java system properties
>>>> via
>>>> >> -D.
>>>> >>
>>>> >> This was used for properties that varied on a
>>>> per-deployment-environment
>>>> >> basis, but needed to be available in the spark shell and workers.
>>>> >>
>>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been deprecated,
>>>> and
>>>> >> replaced by spark-defaults.conf and command line arguments to
>>>> spark-submit
>>>> >> or spark-shell.
>>>> >>
>>>> >> However, setting spark.driver.extraJavaOptions and
>>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
>>>> replacement
>>>> >> for SPARK_JAVA_OPTS:
>>>> >>
>>>> >>
>>>> >> $ cat conf/spark-defaults.conf
>>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
>>>> >>
>>>> >> $ ./bin/spark-shell
>>>> >>
>>>> >> scala> System.getProperty("foo.bar.baz")
>>>> >> res0: String = null
>>>> >>
>>>> >>
>>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
>>>> >>
>>>> >> scala> System.getProperty("foo.bar.baz")
>>>> >> res0: String = 23
>>>> >>
>>>> >>
>>>> >> Looking through the shell scripts for spark-submit and spark-class, I
>>>> can
>>>> >> see why this is; parsing spark-defaults.conf from bash could be
>>>> brittle.
>>>> >>
>>>> >> But from an ergonomic point of view, it's a step back to go from a
>>>> >> set-it-and-forget-it configuration in spark-env.sh, to requiring
>>>> command
>>>> >> line arguments.
>>>> >>
>>>> >> I can solve this with an ad-hoc script to wrap spark-shell with the
>>>> >> appropriate arguments, but I wanted to bring the issue up to see if
>>>> anyone
>>>> >> else had run into it,
>>>> >> or had any direction for a general solution (beyond parsing java
>>>> properties
>>>> >> files from bash).
>>>> >
>>>> >
>>>> >
>>>> > --
>>>> > Marcelo
>>>>
>>>
>>>

From dev-return-8647-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 07:41:22 2014
Return-Path: <dev-return-8647-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DB89110F1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 07:41:22 +0000 (UTC)
Received: (qmail 30704 invoked by uid 500); 31 Jul 2014 07:41:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 30643 invoked by uid 500); 31 Jul 2014 07:41:21 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 5663 invoked by uid 99); 31 Jul 2014 07:33:01 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (nike.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 216.139.236.26 as permitted sender)
Date: Thu, 31 Jul 2014 00:32:35 -0700 (PDT)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1406791955072-7614.post@n3.nabble.com>
In-Reply-To: <CAKs7j9c_o+KxUnodEiS_HjO7=9OvmyBMAELZvkksP+OTFvWT3g@mail.gmail.com>
References: <1406514867558-7538.post@n3.nabble.com> <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com> <1406534584408-7546.post@n3.nabble.com> <CAKs7j9c_o+KxUnodEiS_HjO7=9OvmyBMAELZvkksP+OTFvWT3g@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Kenichi Takagiwa,

Thank you for commenting.
I am going to proceed with the translation, will you please help me.
Further details will be sent later.

Best,

Yu



--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538p7614.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8648-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 07:41:31 2014
Return-Path: <dev-return-8648-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2404A110F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 07:41:31 +0000 (UTC)
Received: (qmail 31787 invoked by uid 500); 31 Jul 2014 07:41:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31722 invoked by uid 500); 31 Jul 2014 07:41:30 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Delivered-To: moderator for dev@spark.apache.org
Received: (qmail 28851 invoked by uid 99); 31 Jul 2014 07:40:27 -0000
X-ASF-Spam-Status: No, hits=2.3 required=10.0
	tests=SPF_SOFTFAIL,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: softfail (athena.apache.org: transitioning domain of yuu.ishikawa+spark@gmail.com does not designate 216.139.236.26 as permitted sender)
Date: Thu, 31 Jul 2014 00:40:02 -0700 (PDT)
From: Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
To: dev@spark.incubator.apache.org
Message-ID: <1406792402694-7615.post@n3.nabble.com>
In-Reply-To: <CAOhmDzc36qMwnPC6937sewU8yAkQOWs2TDKuuScGz3U996+aqQ@mail.gmail.com>
References: <1406514867558-7538.post@n3.nabble.com> <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com> <CAOhmDzc36qMwnPC6937sewU8yAkQOWs2TDKuuScGz3U996+aqQ@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
MIME-Version: 1.0
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Nick,

> I know some projects get translations crowdsourced via one website or
> other.

Thank you for your comments.
I think crowdsourced translation is fit for the translation project on
github.

Best,

Yu




--
View this message in context: http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538p7615.html
Sent from the Apache Spark Developers List mailing list archive at Nabble.com.

From dev-return-8649-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 07:56:50 2014
Return-Path: <dev-return-8649-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D01571111D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 07:56:50 +0000 (UTC)
Received: (qmail 54486 invoked by uid 500); 31 Jul 2014 07:56:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54429 invoked by uid 500); 31 Jul 2014 07:56:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 54416 invoked by uid 99); 31 Jul 2014 07:56:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 07:56:49 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.192.50 as permitted sender)
Received: from [209.85.192.50] (HELO mail-qg0-f50.google.com) (209.85.192.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 07:56:45 +0000
Received: by mail-qg0-f50.google.com with SMTP id q108so3335618qgd.9
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 00:56:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=tNQ+ZtpRjJKnxbCx/C7/8zrrsGiuV50Ov4GIPTkNcaM=;
        b=hoIDjYYTX+czRofovGxveyo8hqATIgWQ6dz6d07TLRAPVRTGRIPYn096N7L9YbwLIe
         kqh72EaR++4qR/87NJ/Bi6SZ2XgDQBoq5bvCbTO1kdwD5g9CXeaPge6YoKpwCWydyiVs
         eWd76c2c3F0+1gwNGPmYewnwYuxlsqm5uu4aKy2fkBUY33vpkzGz7bD3j746YwrZRZQS
         285dIyH63Je8VLBu8BhmywnR4NPS9ybdYVAYSgCFaDCxnixnkW9Clo5MGIX+vxXwXJeD
         yml+BJjl84HvgyGuzaJQyUQ2JFnju2QhiSr1HxanbDOkNG5Xh+53r5tzjkykrJB4gpTj
         fVpw==
X-Received: by 10.224.69.136 with SMTP id z8mr15448860qai.60.1406793385113;
 Thu, 31 Jul 2014 00:56:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.23.113 with HTTP; Thu, 31 Jul 2014 00:56:05 -0700 (PDT)
In-Reply-To: <1406792402694-7615.post@n3.nabble.com>
References: <1406514867558-7538.post@n3.nabble.com> <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
 <CAOhmDzc36qMwnPC6937sewU8yAkQOWs2TDKuuScGz3U996+aqQ@mail.gmail.com> <1406792402694-7615.post@n3.nabble.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Thu, 31 Jul 2014 15:56:05 +0800
Message-ID: <CAA_qdLop3BviypMpWGNMtegx11aHzr_qKfeaRX30z8OsCKPa7g@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c28f22fdfe4204ff789bbd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c28f22fdfe4204ff789bbd
Content-Type: text/plain; charset=UTF-8

Transifex (https://www.transifex.com/) may be helpful if you're considering
online crowdsourcing. At least you may easily maintain a unified glossary
with it among all translators.


On Thu, Jul 31, 2014 at 3:40 PM, Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
wrote:

> Hi Nick,
>
> > I know some projects get translations crowdsourced via one website or
> > other.
>
> Thank you for your comments.
> I think crowdsourced translation is fit for the translation project on
> github.
>
> Best,
>
> Yu
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538p7615.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c28f22fdfe4204ff789bbd--

From dev-return-8650-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 07:56:54 2014
Return-Path: <dev-return-8650-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AEE281111F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 07:56:54 +0000 (UTC)
Received: (qmail 55670 invoked by uid 500); 31 Jul 2014 07:56:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55619 invoked by uid 500); 31 Jul 2014 07:56:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 55602 invoked by uid 99); 31 Jul 2014 07:56:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 07:56:51 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of lian.cs.zju@gmail.com designates 209.85.216.44 as permitted sender)
Received: from [209.85.216.44] (HELO mail-qa0-f44.google.com) (209.85.216.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 07:56:50 +0000
Received: by mail-qa0-f44.google.com with SMTP id f12so2113406qad.17
        for <dev@spark.incubator.apache.org>; Thu, 31 Jul 2014 00:56:25 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=tNQ+ZtpRjJKnxbCx/C7/8zrrsGiuV50Ov4GIPTkNcaM=;
        b=hoIDjYYTX+czRofovGxveyo8hqATIgWQ6dz6d07TLRAPVRTGRIPYn096N7L9YbwLIe
         kqh72EaR++4qR/87NJ/Bi6SZ2XgDQBoq5bvCbTO1kdwD5g9CXeaPge6YoKpwCWydyiVs
         eWd76c2c3F0+1gwNGPmYewnwYuxlsqm5uu4aKy2fkBUY33vpkzGz7bD3j746YwrZRZQS
         285dIyH63Je8VLBu8BhmywnR4NPS9ybdYVAYSgCFaDCxnixnkW9Clo5MGIX+vxXwXJeD
         yml+BJjl84HvgyGuzaJQyUQ2JFnju2QhiSr1HxanbDOkNG5Xh+53r5tzjkykrJB4gpTj
         fVpw==
X-Received: by 10.224.69.136 with SMTP id z8mr15448860qai.60.1406793385113;
 Thu, 31 Jul 2014 00:56:25 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.140.23.113 with HTTP; Thu, 31 Jul 2014 00:56:05 -0700 (PDT)
In-Reply-To: <1406792402694-7615.post@n3.nabble.com>
References: <1406514867558-7538.post@n3.nabble.com> <CABPQxss8qwZqUmArMf-1399awq3TrbK0iakygYfeKQ+2dZ8+Bg@mail.gmail.com>
 <CAOhmDzc36qMwnPC6937sewU8yAkQOWs2TDKuuScGz3U996+aqQ@mail.gmail.com> <1406792402694-7615.post@n3.nabble.com>
From: Cheng Lian <lian.cs.zju@gmail.com>
Date: Thu, 31 Jul 2014 15:56:05 +0800
Message-ID: <CAA_qdLop3BviypMpWGNMtegx11aHzr_qKfeaRX30z8OsCKPa7g@mail.gmail.com>
Subject: Re: Can I translate the documentations of Spark in Japanese?
To: dev@spark.apache.org
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c28f22fdfe4204ff789bbd
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c28f22fdfe4204ff789bbd
Content-Type: text/plain; charset=UTF-8

Transifex (https://www.transifex.com/) may be helpful if you're considering
online crowdsourcing. At least you may easily maintain a unified glossary
with it among all translators.


On Thu, Jul 31, 2014 at 3:40 PM, Yu Ishikawa <yuu.ishikawa+spark@gmail.com>
wrote:

> Hi Nick,
>
> > I know some projects get translations crowdsourced via one website or
> > other.
>
> Thank you for your comments.
> I think crowdsourced translation is fit for the translation project on
> github.
>
> Best,
>
> Yu
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/Can-I-translate-the-documentations-of-Spark-in-Japanese-tp7538p7615.html
> Sent from the Apache Spark Developers List mailing list archive at
> Nabble.com.
>

--001a11c28f22fdfe4204ff789bbd--

From dev-return-8651-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 08:26:44 2014
Return-Path: <dev-return-8651-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3FA2211201
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 08:26:44 +0000 (UTC)
Received: (qmail 8970 invoked by uid 500); 31 Jul 2014 08:26:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 8913 invoked by uid 500); 31 Jul 2014 08:26:43 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 8896 invoked by uid 99); 31 Jul 2014 08:26:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 08:26:43 +0000
X-ASF-Spam-Status: No, hits=-0.5 required=10.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of tathagata.das1565@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 08:26:39 +0000
Received: by mail-ig0-f182.google.com with SMTP id c1so4535973igq.15
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 01:26:18 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type:content-transfer-encoding;
        bh=6GhbTjRxB3oF5Q1G3HmzW1OHR5zdWgG6YYr49jRWXWQ=;
        b=wso0RYUC6nGGpkCWpYwCOg0qQXVi/T60h0MJcSR4Be9dE8fI7a/99XCnA2z7zLrWNy
         4u7ewqzxjjLvY80it+aPEuXRaiyeYA37DBYRuJgQpbu218KBdoTVi4q5gFaz8exZYWVp
         PiV1NW0C4Xe7J14o5XmwxTSHKia1WWE0Y2uoDtQjnwPlLR/1tSX57IAEWvy/cPWHD1SJ
         pBdtvupvCHvBhn6HuawWC4GJEDf2Rrmi6ZYnUMU7ofQ6OjwfkJ/gVHuW9k8n2391Ht08
         fyKRBJtQnd3QF1nDEZuHV9WI/t0SjLI7WCY6C0/BgBydGsVj2dzzhb2EH860tYhoFiUl
         Q07w==
X-Received: by 10.42.24.9 with SMTP id u9mr12907413icb.91.1406795178701; Thu,
 31 Jul 2014 01:26:18 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.107.129.10 with HTTP; Thu, 31 Jul 2014 01:25:48 -0700 (PDT)
In-Reply-To: <CA+FETEKK7_7w_Z3ii=Yz-X+fwsTA2vzvYJxcEfCm6rFX=MNw1g@mail.gmail.com>
References: <CA+FETEKK7_7w_Z3ii=Yz-X+fwsTA2vzvYJxcEfCm6rFX=MNw1g@mail.gmail.com>
From: Tathagata Das <tathagata.das1565@gmail.com>
Date: Thu, 31 Jul 2014 01:25:48 -0700
Message-ID: <CAMwrk0mO+w2ysWoODmRb3Mt4gBsP50YtD2inOL27xwzKEU+PUA@mail.gmail.com>
Subject: Re: failed to build spark with maven for both 1.0.1 and latest master branch
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Does a "mvn clean" or "sbt/sbt clean" help?

TD

On Wed, Jul 30, 2014 at 9:25 PM, yao <yaoshengzhe@gmail.com> wrote:
> Hi Folks,
>
> Today I am trying to build spark using maven; however, the following
> command failed consistently for both 1.0.1 and the latest master.  (BTW, =
it
> seems sbt works fine: *sbt/sbt -Dhadoop.version=3D2.4.0 -Pyarn clean
> assembly)*
>
> Environment: Mac OS Mavericks
> Maven: 3.2.2 (installed by homebrew)
>
>
>
>
> *export M2_HOME=3D/usr/local/Cellar/maven/3.2.2/libexec/export
> PATH=3D$M2_HOME/bin:$PATHexport MAVEN_OPTS=3D"-Xmx2g -XX:MaxPermSize=3D51=
2M
> -XX:ReservedCodeCacheSize=3D512m"mvn -Pyarn -Phadoop-2.4
> -Dhadoop.version=3D2.4.0 -DskipTests clean package*
>
> Build outputs:
>
> [INFO] Scanning for projects...
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Reactor Build Order:
> [INFO]
> [INFO] Spark Project Parent POM
> [INFO] Spark Project Core
> [INFO] Spark Project Bagel
> [INFO] Spark Project GraphX
> [INFO] Spark Project ML Library
> [INFO] Spark Project Streaming
> [INFO] Spark Project Tools
> [INFO] Spark Project Catalyst
> [INFO] Spark Project SQL
> [INFO] Spark Project Hive
> [INFO] Spark Project REPL
> [INFO] Spark Project YARN Parent POM
> [INFO] Spark Project YARN Stable API
> [INFO] Spark Project Assembly
> [INFO] Spark Project External Twitter
> [INFO] Spark Project External Kafka
> [INFO] Spark Project External Flume
> [INFO] Spark Project External ZeroMQ
> [INFO] Spark Project External MQTT
> [INFO] Spark Project Examples
> [INFO]
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Building Spark Project Parent POM 1.0.1
> [INFO]
> ------------------------------------------------------------------------
> [INFO]
> [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-parent --=
-
> [INFO]
> [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> spark-parent ---
> [INFO]
> [INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @
> spark-parent ---
> [INFO] Source directory:
> /Users/syao/git/grid/thirdparty/spark/src/main/scala added.
> [INFO]
> [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> spark-parent ---
> [INFO]
> [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
> spark-parent ---
> [INFO] Add Test Source directory:
> /Users/syao/git/grid/thirdparty/spark/src/test/scala
> [INFO]
> [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> spark-parent ---
> [INFO] No sources to compile
> [INFO]
> [INFO] --- build-helper-maven-plugin:1.8:add-test-source
> (add-scala-test-sources) @ spark-parent ---
> [INFO] Test Source directory:
> /Users/syao/git/grid/thirdparty/spark/src/test/scala added.
> [INFO]
> [INFO] --- scala-maven-plugin:3.1.6:testCompile (scala-test-compile-first=
)
> @ spark-parent ---
> [INFO] No sources to compile
> [INFO]
> [INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) @
> spark-parent ---
> [INFO]
> [INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar) @
> spark-parent ---
> [INFO]
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Building Spark Project Core 1.0.1
> [INFO]
> ------------------------------------------------------------------------
> [INFO]
> [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-core_2.10
> ---
> [INFO]
> [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> spark-core_2.10 ---
> [INFO]
> [INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources) @
> spark-core_2.10 ---
> [INFO] Source directory:
> /Users/syao/git/grid/thirdparty/spark/core/src/main/scala added.
> [INFO]
> [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> spark-core_2.10 ---
> [INFO]
> [INFO] --- exec-maven-plugin:1.2.1:exec (default) @ spark-core_2.10 ---
> Archive:  lib/py4j-0.8.1-src.zip
>   inflating: build/py4j/tests/java_map_test.py
>  extracting: build/py4j/tests/__init__.py
>   inflating: build/py4j/tests/java_gateway_test.py
>   inflating: build/py4j/tests/java_callback_test.py
>   inflating: build/py4j/tests/java_list_test.py
>   inflating: build/py4j/tests/byte_string_test.py
>   inflating: build/py4j/tests/multithreadtest.py
>   inflating: build/py4j/tests/java_array_test.py
>   inflating: build/py4j/tests/py4j_callback_example2.py
>   inflating: build/py4j/tests/py4j_example.py
>   inflating: build/py4j/tests/py4j_callback_example.py
>   inflating: build/py4j/tests/finalizer_test.py
>   inflating: build/py4j/tests/java_set_test.py
>   inflating: build/py4j/finalizer.py
>  extracting: build/py4j/__init__.py
>   inflating: build/py4j/java_gateway.py
>   inflating: build/py4j/protocol.py
>   inflating: build/py4j/java_collections.py
>  extracting: build/py4j/version.py
>   inflating: build/py4j/compat.py
> [INFO]
> [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @
> spark-core_2.10 ---
> [INFO] Using 'UTF-8' encoding to copy filtered resources.
> [INFO] Copying 6 resources
> [INFO] Copying 20 resources
> [INFO] Copying 7 resources
> [INFO] Copying 3 resources
> [INFO]
> [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
> spark-core_2.10 ---
> [INFO] Add Test Source directory:
> /Users/syao/git/grid/thirdparty/spark/core/src/test/scala
> [INFO]
> [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> spark-core_2.10 ---
> [INFO] Using zinc server for incremental compilation
> [info] Compiling 342 Scala sources and 34 Java sources to
> /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes...
> [warn] Class javax.servlet.ServletException not found - continuing with a
> stub.
> [error]
> [error]      while compiling:
> /Users/syao/git/grid/thirdparty/spark/core/src/main/scala/org/apache/spar=
k/HttpServer.scala
> [error]         during phase: typer
> [error]      library version: version 2.10.4
> [error]     compiler version: version 2.10.4
> [error]   reconstructed args: -classpath
> /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes:/Use=
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-=
2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/=
hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/=
1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commo=
ns-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xm=
lenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/com=
mons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/c=
ommons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:=
/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.j=
ar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1=
.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digeste=
r/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/=
commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/User=
s/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/common=
s-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/=
jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repositor=
y/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.ja=
r:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Use=
rs/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-jav=
a-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/=
hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons=
-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/t=
ukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoo=
p-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/j=
etty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org=
/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-ap=
p-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-c=
lient-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2=
/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2=
.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey=
-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-se=
rver-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapred=
uce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/o=
rg/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client=
-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-co=
mmon/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xm=
l/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/x=
ml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/java=
x/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/c=
om/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repositor=
y/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduc=
e-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/reposi=
tory/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/=
repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:=
/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcor=
e-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2=
.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curato=
r/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Use=
rs/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v201310=
31/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/j=
etty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v2=
01105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8=
.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repositor=
y/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfis=
h/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/s=
yao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v20110507=
1233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/or=
g/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v2013=
1031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20=
131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclip=
se/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/U=
sers/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Us=
ers/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang=
3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.=
9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf=
4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/ju=
l-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.=
7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/=
log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/s=
lf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0=
.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snap=
py-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/=
chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esoter=
icsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esot=
ericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/s=
yao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/U=
sers/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Use=
rs/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/U=
sers/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/U=
sers/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shad=
ed-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repo=
sitory/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-ac=
tor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/=
config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6=
.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project=
/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/=
syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-ma=
ths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j=
_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/User=
s/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2=
.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/=
json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast=
_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/though=
tworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository=
/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/=
org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/=
.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-da=
tabind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jack=
son-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/reposit=
ory/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/U=
sers/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/rep=
ository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/re=
pository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Us=
ers/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Fi=
nal.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/s=
tream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-cor=
e/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metr=
ics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/=
codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/=
repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0=
.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/t=
achyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0=
/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0=
/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.=
4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflec=
t/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-proj=
ect/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4=
j/py4j/0.8.1/py4j-0.8.1.jar
> -deprecation -feature -bootclasspath
> /Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/r=
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Hom=
e/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents=
/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.=
jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7=
.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/j=
dk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtual=
Machines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVi=
rtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java=
/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/=
.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
> -unchecked -language:postfixOps
> [error]
> [error]   last tree to typer: Ident(Server)
> [error]               symbol: <none> (flags: )
> [error]    symbol definition: <none>
> [error]        symbol owners:
> [error]       context owners: variable server -> class HttpServer ->
> package spark
> [error]
> [error] =3D=3D Enclosing template or block =3D=3D
> [error]
> [error] Template( // val <local HttpServer>: <notype> in class HttpServer
> [error]   "org.apache.spark.Logging" // parents
> [error]   ValDef(
> [error]     private
> [error]     "_"
> [error]     <tpt>
> [error]     <empty>
> [error]   )
> [error]   // 9 statements
> [error]   ValDef( // private[this] val resourceBase: <?> in class HttpSer=
ver
> [error]     private <local> <paramaccessor>
> [error]     "resourceBase"
> [error]     "File"
> [error]     <empty>
> [error]   )
> [error]   ValDef( // private[this] val securityManager: <?> in class
> HttpServer
> [error]     private <local> <paramaccessor>
> [error]     "securityManager"
> [error]     "SecurityManager"
> [error]     <empty>
> [error]   )
> [error]   DefDef( // def <init>(resourceBase: java.io.File,securityManage=
r:
> org.apache.spark.SecurityManager): org.apache.spark.HttpServer in class
> HttpServer
> [error]     <method> <triedcooking>
> [error]     "<init>"
> [error]     []
> [error]     // 1 parameter list
> [error]     ValDef( // resourceBase: java.io.File
> [error]       <param> <paramaccessor>
> [error]       "resourceBase"
> [error]       "File"
> [error]       <empty>
> [error]     )
> [error]     ValDef( // securityManager: org.apache.spark.SecurityManager
> [error]       <param> <paramaccessor>
> [error]       "securityManager"
> [error]       "SecurityManager" // private[package spark] class
> SecurityManager extends Logging in package spark,
> tree.tpe=3Dorg.apache.spark.SecurityManager
> [error]       <empty>
> [error]     )
> [error]     <tpt> // tree.tpe=3Dorg.apache.spark.HttpServer
> [error]     Block(
> [error]       Apply(
> [error]         super."<init>"
> [error]         Nil
> [error]       )
> [error]       ()
> [error]     )
> [error]   )
> [error]   ValDef( // private[this] var server: <?> in class HttpServer
> [error]     private <mutable> <local>
> [error]     "server"
> [error]     "Server"
> [error]     null
> [error]   )
> [error]   ValDef( // private[this] var port: <?> in class HttpServer
> [error]     private <mutable> <local>
> [error]     "port"
> [error]     "Int"
> [error]     -1
> [error]   )
> [error]   DefDef( // def start(): Unit in class HttpServer
> [error]     <method> <triedcooking>
> [error]     "start"
> [error]     []
> [error]     List(Nil)
> [error]     "scala"."Unit" // final abstract class Unit extends AnyVal in
> package scala, tree.tpe=3DUnit
> [error]     If(
> [error]       Apply(
> [error]         "server"."$bang$eq"
> [error]         null
> [error]       )
> [error]       Throw(
> [error]         Apply(
> [error]           new ServerStateException."<init>"
> [error]           "Server is already started"
> [error]         )
> [error]       )
> [error]       Block(
> [error]         // 16 statements
> [error]         Apply(
> [error]           "logInfo"
> [error]           "Starting HTTP Server"
> [error]         )
> [error]         Assign(
> [error]           "server"
> [error]           Apply(
> [error]             new Server."<init>"
> [error]             Nil
> [error]           )
> [error]         )
> [error]         ValDef(
> [error]           0
> [error]           "connector"
> [error]           <tpt>
> [error]           Apply(
> [error]             new SocketConnector."<init>"
> [error]             Nil
> [error]           )
> [error]         )
> [error]         Apply(
> [error]           "connector"."setMaxIdleTime"
> [error]           Apply(
> [error]             60."$times"
> [error]             1000
> [error]           )
> [error]         )
> [error]         Apply(
> [error]           "connector"."setSoLingerTime"
> [error]           -1
> [error]         )
> [error]         Apply(
> [error]           "connector"."setPort"
> [error]           0
> [error]         )
> [error]         Apply(
> [error]           "server"."addConnector"
> [error]           "connector"
> [error]         )
> [error]         ValDef(
> [error]           0
> [error]           "threadPool"
> [error]           <tpt>
> [error]           Apply(
> [error]             new QueuedThreadPool."<init>"
> [error]             Nil
> [error]           )
> [error]         )
> [error]         Apply(
> [error]           "threadPool"."setDaemon"
> [error]           true
> [error]         )
> [error]         Apply(
> [error]           "server"."setThreadPool"
> [error]           "threadPool"
> [error]         )
> [error]         ValDef(
> [error]           0
> [error]           "resHandler"
> [error]           <tpt>
> [error]           Apply(
> [error]             new ResourceHandler."<init>"
> [error]             Nil
> [error]           )
> [error]         )
> [error]         Apply(
> [error]           "resHandler"."setResourceBase"
> [error]           "resourceBase"."getAbsolutePath"
> [error]         )
> [error]         ValDef(
> [error]           0
> [error]           "handlerList"
> [error]           <tpt>
> [error]           Apply(
> [error]             new HandlerList."<init>"
> [error]             Nil
> [error]           )
> [error]         )
> [error]         Apply(
> [error]           "handlerList"."setHandlers"
> [error]           Apply(
> [error]             "Array"
> [error]             // 2 arguments
> [error]             "resHandler"
> [error]             Apply(
> [error]               new DefaultHandler."<init>"
> [error]               Nil
> [error]             )
> [error]           )
> [error]         )
> [error]         If(
> [error]           Apply(
> [error]             "securityManager"."isAuthenticationEnabled"
> [error]             Nil
> [error]           )
> [error]           Block(
> [error]             // 3 statements
> [error]             Apply(
> [error]               "logDebug"
> [error]               "HttpServer is using security"
> [error]             )
> [error]             ValDef(
> [error]               0
> [error]               "sh"
> [error]               <tpt>
> [error]               Apply(
> [error]                 "setupSecurityHandler"
> [error]                 "securityManager"
> [error]               )
> [error]             )
> [error]             Apply(
> [error]               "sh"."setHandler"
> [error]               "handlerList"
> [error]             )
> [error]             Apply(
> [error]               "server"."setHandler"
> [error]               "sh"
> [error]             )
> [error]           )
> [error]           Block(
> [error]             Apply(
> [error]               "logDebug"
> [error]               "HttpServer is not using security"
> [error]             )
> [error]             Apply(
> [error]               "server"."setHandler"
> [error]               "handlerList"
> [error]             )
> [error]           )
> [error]         )
> [error]         Apply(
> [error]           "server"."start"
> [error]           Nil
> [error]         )
> [error]         Assign(
> [error]           "port"
> [error]           Apply(
> [error]             server.getConnectors()(0)."getLocalPort"
> [error]             Nil
> [error]           )
> [error]         )
> [error]       )
> [error]     )
> [error]   )
> [error]   DefDef( // private def setupSecurityHandler: <?> in class
> HttpServer
> [error]     <method> private
> [error]     "setupSecurityHandler"
> [error]     []
> [error]     // 1 parameter list
> [error]     ValDef(
> [error]       <param>
> [error]       "securityMgr"
> [error]       "SecurityManager"
> [error]       <empty>
> [error]     )
> [error]     "ConstraintSecurityHandler"
> [error]     Block(
> [error]       // 16 statements
> [error]       ValDef(
> [error]         0
> [error]         "constraint"
> [error]         <tpt>
> [error]         Apply(
> [error]           new Constraint."<init>"
> [error]           Nil
> [error]         )
> [error]       )
> [error]       Apply(
> [error]         "constraint"."setName"
> [error]         "Constraint"."__DIGEST_AUTH"
> [error]       )
> [error]       Apply(
> [error]         "constraint"."setRoles"
> [error]         Apply(
> [error]           "Array"
> [error]           "user"
> [error]         )
> [error]       )
> [error]       Apply(
> [error]         "constraint"."setAuthenticate"
> [error]         true
> [error]       )
> [error]       Apply(
> [error]         "constraint"."setDataConstraint"
> [error]         "Constraint"."DC_NONE"
> [error]       )
> [error]       ValDef(
> [error]         0
> [error]         "cm"
> [error]         <tpt>
> [error]         Apply(
> [error]           new ConstraintMapping."<init>"
> [error]           Nil
> [error]         )
> [error]       )
> [error]       Apply(
> [error]         "cm"."setConstraint"
> [error]         "constraint"
> [error]       )
> [error]       Apply(
> [error]         "cm"."setPathSpec"
> [error]         "/*"
> [error]       )
> [error]       ValDef(
> [error]         0
> [error]         "sh"
> [error]         <tpt>
> [error]         Apply(
> [error]           new ConstraintSecurityHandler."<init>"
> [error]           Nil
> [error]         )
> [error]       )
> [error]       ValDef(
> [error]         0
> [error]         "hashLogin"
> [error]         <tpt>
> [error]         Apply(
> [error]           new HashLoginService."<init>"
> [error]           Nil
> [error]         )
> [error]       )
> [error]       ValDef(
> [error]         0
> [error]         "userCred"
> [error]         <tpt>
> [error]         Apply(
> [error]           new Password."<init>"
> [error]           Apply(
> [error]             "securityMgr"."getSecretKey"
> [error]             Nil
> [error]           )
> [error]         )
> [error]       )
> [error]       If(
> [error]         Apply(
> [error]           "userCred"."$eq$eq"
> [error]           null
> [error]         )
> [error]         Throw(
> [error]           Apply(
> [error]             new Exception."<init>"
> [error]             "Error: secret key is null with authentication on"
> [error]           )
> [error]         )
> [error]         ()
> [error]       )
> [error]       Apply(
> [error]         "hashLogin"."putUser"
> [error]         // 3 arguments
> [error]         Apply(
> [error]           "securityMgr"."getHttpUser"
> [error]           Nil
> [error]         )
> [error]         "userCred"
> [error]         Apply(
> [error]           "Array"
> [error]           "user"
> [error]         )
> [error]       )
> [error]       Apply(
> [error]         "sh"."setLoginService"
> [error]         "hashLogin"
> [error]       )
> [error]       Apply(
> [error]         "sh"."setAuthenticator"
> [error]         Apply(
> [error]           new DigestAuthenticator."<init>"
> [error]           Nil
> [error]         )
> [error]       )
> [error]       Apply(
> [error]         "sh"."setConstraintMappings"
> [error]         Apply(
> [error]           "Array"
> [error]           "cm"
> [error]         )
> [error]       )
> [error]       "sh"
> [error]     )
> [error]   )
> [error]   DefDef( // def stop(): Unit in class HttpServer
> [error]     <method> <triedcooking>
> [error]     "stop"
> [error]     []
> [error]     List(Nil)
> [error]     "scala"."Unit" // final abstract class Unit extends AnyVal in
> package scala, tree.tpe=3DUnit
> [error]     If(
> [error]       Apply(
> [error]         "server"."$eq$eq"
> [error]         null
> [error]       )
> [error]       Throw(
> [error]         Apply(
> [error]           new ServerStateException."<init>"
> [error]           "Server is already stopped"
> [error]         )
> [error]       )
> [error]       Block(
> [error]         // 2 statements
> [error]         Apply(
> [error]           "server"."stop"
> [error]           Nil
> [error]         )
> [error]         Assign(
> [error]           "port"
> [error]           -1
> [error]         )
> [error]         Assign(
> [error]           "server"
> [error]           null
> [error]         )
> [error]       )
> [error]     )
> [error]   )
> [error]   DefDef( // def uri: String in class HttpServer
> [error]     <method> <triedcooking>
> [error]     "uri"
> [error]     []
> [error]     Nil
> [error]     "String"
> [error]     If(
> [error]       Apply(
> [error]         "server"."$eq$eq"
> [error]         null
> [error]       )
> [error]       Throw(
> [error]         Apply(
> [error]           new ServerStateException."<init>"
> [error]           "Server is not started"
> [error]         )
> [error]       )
> [error]       Return(
> [error]         Apply(
> [error]           "http://".$plus(Utils.localIpAddress).$plus(":")."$plus=
"
> [error]           "port"
> [error]         )
> [error]       )
> [error]     )
> [error]   )
> [error] )
> [error]
> [error] uncaught exception during compilation: java.lang.AssertionError
> java.lang.AssertionError: assertion failed: javax.servlet.ServletExceptio=
n
>     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)
>     at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:137=
4)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseExceptions$1(Classf=
ileParser.scala:1051)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseAttributes(Classfil=
eParser.scala:1080)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseMethod(ClassfilePar=
ser.scala:666)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser$$anonfun$parseClass$1.ap=
ply$mcV$sp(ClassfileParser.scala:567)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfilePars=
er.scala:572)
>     at
> scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.sc=
ala:88)
>     at
> scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoa=
ders.scala:261)
>     at
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.=
scala:194)
>     at
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scal=
a:210)
>     at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:893)
>     at
> scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:5064)
>     at
> scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.sc=
ala:5218)
>     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5561)
>     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
>     at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:57=
69)
>     at scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:57=
72)
>     at scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:13=
17)
>     at scala.tools.nsc.typechecker.Namers$Namer.getSig$1(Namers.scala:145=
7)
>     at scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1466=
)
>     at
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
>     at
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
>     at
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
>     at
> scala.tools.nsc.typechecker.Namers$Namer.scala$tools$nsc$typechecker$Name=
rs$Namer$$logAndValidate(Namers.scala:1499)
>     at
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:730)
>     at
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:729)
>     at
> scala.tools.nsc.typechecker.Namers$$anon$1.completeImpl(Namers.scala:1614=
)
>     at
> scala.tools.nsc.typechecker.Namers$LockingTypeCompleter$class.complete(Na=
mers.scala:1622)
>     at
> scala.tools.nsc.typechecker.Namers$$anon$1.complete(Namers.scala:1612)
>     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
>     at scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:137=
4)
>     at
> scala.tools.nsc.typechecker.MethodSynthesis$MethodSynth$class.addDerivedT=
rees(MethodSynthesis.scala:225)
>     at
> scala.tools.nsc.typechecker.Namers$Namer.addDerivedTrees(Namers.scala:55)
>     at
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
>     at
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
>     at
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1856)
>     at
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1853)
>     at
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
>     at
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
>     at scala.collection.immutable.List.foreach(List.scala:318)
>     at
> scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
>     at scala.collection.AbstractTraversable.flatMap(Traversable.scala:105=
)
>     at
> scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1917)
>     at
> scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:1759)
>     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5583)
>     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
>     at
> scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Type=
rs$Typer$$typedStat$1(Typers.scala:2928)
>     at
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
>     at
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
>     at scala.collection.immutable.List.loop$1(List.scala:170)
>     at scala.collection.immutable.List.mapConserve(List.scala:186)
>     at
> scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032)
>     at
> scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5=
301)
>     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5587)
>     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
>     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5704)
>     at
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.=
scala:99)
>     at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
>     at
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
>     at
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
>     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
>     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
>     at
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.sc=
ala:91)
>     at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
>     at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
>     at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
>     at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
>     at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
>     at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
>     at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
>     at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
>     at java.lang.reflect.Method.invoke(Method.java:606)
>     at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
>     at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
>     at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
>     at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
$mcV$sp(AggressiveCompile.scala:99)
>     at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
>     at
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
>     at
> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(Aggr=
essiveCompile.scala:166)
>     at
> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompil=
e.scala:98)
>     at
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:1=
43)
>     at
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:8=
7)
>     at
> sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
>     at
> sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
>     at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
>     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
>     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
>     at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
>     at sbt.inc.Incremental$.compile(Incremental.scala:37)
>     at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
>     at sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:15=
7)
>     at sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71=
)
>     at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
>     at com.typesafe.zinc.Main$.run(Main.scala:98)
>     at com.typesafe.zinc.Nailgun$.zinc(Nailgun.scala:93)
>     at com.typesafe.zinc.Nailgun$.nailMain(Nailgun.scala:82)
>     at com.typesafe.zinc.Nailgun.nailMain(Nailgun.scala)
>     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
>     at
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
>     at
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
>     at java.lang.reflect.Method.invoke(Method.java:606)
>     at com.martiansoftware.nailgun.NGSession.run(NGSession.java:280)
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Reactor Summary:
> [INFO]
> [INFO] Spark Project Parent POM ........................... SUCCESS [
> 1.760 s]
> [INFO] Spark Project Core ................................. FAILURE [
> 5.312 s]
> [INFO] Spark Project Bagel ................................ SKIPPED
> [INFO] Spark Project GraphX ............................... SKIPPED
> [INFO] Spark Project ML Library ........................... SKIPPED
> [INFO] Spark Project Streaming ............................ SKIPPED
> [INFO] Spark Project Tools ................................ SKIPPED
> [INFO] Spark Project Catalyst ............................. SKIPPED
> [INFO] Spark Project SQL .................................. SKIPPED
> [INFO] Spark Project Hive ................................. SKIPPED
> [INFO] Spark Project REPL ................................. SKIPPED
> [INFO] Spark Project YARN Parent POM ...................... SKIPPED
> [INFO] Spark Project YARN Stable API ...................... SKIPPED
> [INFO] Spark Project Assembly ............................. SKIPPED
> [INFO] Spark Project External Twitter ..................... SKIPPED
> [INFO] Spark Project External Kafka ....................... SKIPPED
> [INFO] Spark Project External Flume ....................... SKIPPED
> [INFO] Spark Project External ZeroMQ ...................... SKIPPED
> [INFO] Spark Project External MQTT ........................ SKIPPED
> [INFO] Spark Project Examples ............................. SKIPPED
> [INFO]
> ------------------------------------------------------------------------
> [INFO] BUILD FAILURE
> [INFO]
> ------------------------------------------------------------------------
> [INFO] Total time: 7.562 s
> [INFO] Finished at: 2014-07-30T21:18:41-07:00
> [INFO] Final Memory: 39M/713M
> [INFO]
> ------------------------------------------------------------------------
> [ERROR] Failed to execute goal
> net.alchim31.maven:scala-maven-plugin:3.1.6:compile (scala-compile-first)
> on project spark-core_2.10: Execution scala-compile-first of goal
> net.alchim31.maven:scala-maven-plugin:3.1.6:compile failed. CompileFailed
> -> [Help 1]
>
> Thanks
> Shengzhe

From dev-return-8652-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 13:36:18 2014
Return-Path: <dev-return-8652-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EF891118E2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 13:36:18 +0000 (UTC)
Received: (qmail 81618 invoked by uid 500); 31 Jul 2014 13:36:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81561 invoked by uid 500); 31 Jul 2014 13:36:18 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81543 invoked by uid 99); 31 Jul 2014 13:36:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 13:36:17 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (athena.apache.org: local policy)
Received: from [209.85.214.176] (HELO mail-ob0-f176.google.com) (209.85.214.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 13:36:13 +0000
Received: by mail-ob0-f176.google.com with SMTP id wo20so1561277obc.21
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 06:35:51 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=BE9K8S2Mjg7vkljVFvM0YIuWJiGKDK6aHrcrkgQejKE=;
        b=QAbka9AhV74WHrghtK3Yr/HseP6uD5WTiw1D50InbsEFk3E98t5fv0youm4l2cwNdn
         0yasveoCd91Dqxavx5ej4lExFFhjW+I1zA/jc8gdRTJPuiZKhCFjfzaUomPDsAtEfs0c
         gdERPEQs1MH8i1J/CDnaichQOmHu0Lfcg9MfF4sZHy9MNjgP0UMrvK9JFCzGDf5ukmqM
         toCkb2E8M7yUEDLciBCV3lUyboLWfEffE2C3uDIKWua2sqXmq8/yD0x2jhGkHe9kkwwX
         bjkr2eZ/05VeYJGhVIGBgHGL41HID9NsDjek8A4PbbyG05ZEjTniZ7x2dBQVuxjMkjWU
         RH4A==
X-Gm-Message-State: ALoCoQk7UmaIg2wVb7W09SQbxpEwfd8LNOUhownlk/3bCyGmMB98n+v2OBab9XAr1yMkSOFZU8wG
MIME-Version: 1.0
X-Received: by 10.182.209.5 with SMTP id mi5mr6418945obc.33.1406813751488;
 Thu, 31 Jul 2014 06:35:51 -0700 (PDT)
Received: by 10.76.171.100 with HTTP; Thu, 31 Jul 2014 06:35:51 -0700 (PDT)
In-Reply-To: <CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
References: <CAKWX9VU+vVstw_Vu-yj61+=rptD8prX_-dVNKSMQDVrCsMuWNA@mail.gmail.com>
	<CAAOnQ7uC00E-4hrs8oG1Tn1u4+9-ng8DxCyfsc_sgdDBGrvqKQ@mail.gmail.com>
	<CABPQxsuZEuiYSwCxZqoCZBeiP2zKvtd7XnMJnwx+da5ybO+v4g@mail.gmail.com>
	<CAKWX9VUZqwvuCxiNM4-9fZd-e=vNM5xFQwkSS1YdyP7VaLuCYA@mail.gmail.com>
	<CAKWX9VXS4fGVSTHcmDqvpy3Ze7eO0jv=p7xwh1etRGZtaHW9AQ@mail.gmail.com>
	<CABPQxsv+Ss_V9FH-kXDXRa80FfS7g0CPpmVG-y-NGEAYEE=7Mw@mail.gmail.com>
	<CABPQxstGyLS_tMEEP8dxSHHrBLVuYs1PNgGXbsQv3kkg6kCq=w@mail.gmail.com>
Date: Thu, 31 Jul 2014 08:35:51 -0500
Message-ID: <CAKWX9VWRdtU9N3XAN7cVd3JGvG2omoVx=rQ4_n3Pz6Uwzh3jTw@mail.gmail.com>
Subject: Re: replacement for SPARK_JAVA_OPTS
From: Cody Koeninger <cody@koeninger.org>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=e89a8ff1cf92ec47a404ff7d59dd
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1cf92ec47a404ff7d59dd
Content-Type: text/plain; charset=UTF-8

1. I've tried with and without escaping equals sign, it doesn't affect the
results.

2. Yeah, exporting SPARK_SUBMIT_OPTS from spark-env.sh works for getting
system properties set in the local shell (although not for executors).

3. We're using the default fine-grained mesos mode, not setting
spark.mesos.coarse, so it doesn't seem immediately related to that ticket.
Should I file a bug report?


On Thu, Jul 31, 2014 at 1:33 AM, Patrick Wendell <pwendell@gmail.com> wrote:

> The third issue may be related to this:
> https://issues.apache.org/jira/browse/SPARK-2022
>
> We can take a look at this during the bug fix period for the 1.1
> release next week. If we come up with a fix we can backport it into
> the 1.0 branch also.
>
> On Wed, Jul 30, 2014 at 11:31 PM, Patrick Wendell <pwendell@gmail.com>
> wrote:
> > Thanks for digging around here. I think there are a few distinct issues.
> >
> > 1. Properties containing the '=' character need to be escaped.
> > I was able to load properties fine as long as I escape the '='
> > character. But maybe we should document this:
> >
> > == spark-defaults.conf ==
> > spark.foo a\=B
> > == shell ==
> > scala> sc.getConf.get("spark.foo")
> > res2: String = a=B
> >
> > 2. spark.driver.extraJavaOptions, when set in the properties file,
> > don't affect the driver when running in client mode (always the case
> > for mesos). We should probably document this. In this case you need to
> > either use --driver-java-options or set SPARK_SUBMIT_OPTS.
> >
> > 3. Arguments aren't propagated on Mesos (this might be because of the
> > other issues, or a separate bug).
> >
> > - Patrick
> >
> > On Wed, Jul 30, 2014 at 3:10 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
> >> In addition, spark.executor.extraJavaOptions does not seem to behave as
> I
> >> would expect; java arguments don't seem to be propagated to executors.
> >>
> >>
> >> $ cat conf/spark-defaults.conf
> >>
> >> spark.master
> >>
> mesos://zk://etl-01.mxstg:2181,etl-02.mxstg:2181,etl-03.mxstg:2181/masters
> >> spark.executor.extraJavaOptions -Dfoo.bar.baz=23
> >> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>
> >>
> >> $ ./bin/spark-shell
> >>
> >> scala> sc.getConf.get("spark.executor.extraJavaOptions")
> >> res0: String = -Dfoo.bar.baz=23
> >>
> >> scala> sc.parallelize(1 to 100).map{ i => (
> >>      |  java.net.InetAddress.getLocalHost.getHostName,
> >>      |  System.getProperty("foo.bar.baz")
> >>      | )}.collect
> >>
> >> res1: Array[(String, String)] = Array((dn-01.mxstg,null),
> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-01.mxstg,null),
> >> (dn-01.mxstg,null), (dn-01.mxstg,null), (dn-02.mxstg,null),
> >> (dn-02.mxstg,null), ...
> >>
> >>
> >>
> >> Note that this is a mesos deployment, although I wouldn't expect that to
> >> affect the availability of spark.driver.extraJavaOptions in a local
> spark
> >> shell.
> >>
> >>
> >> On Wed, Jul 30, 2014 at 4:18 PM, Cody Koeninger <cody@koeninger.org>
> wrote:
> >>
> >>> Either whitespace or equals sign are valid properties file formats.
> >>> Here's an example:
> >>>
> >>> $ cat conf/spark-defaults.conf
> >>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>>
> >>> $ ./bin/spark-shell -v
> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> >>> Adding default property: spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> >>>
> >>>
> >>> scala>  System.getProperty("foo.bar.baz")
> >>> res0: String = null
> >>>
> >>>
> >>> If you add double quotes, the resulting string value will have double
> >>> quotes.
> >>>
> >>>
> >>> $ cat conf/spark-defaults.conf
> >>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> >>>
> >>> $ ./bin/spark-shell -v
> >>> Using properties file: /opt/spark/conf/spark-defaults.conf
> >>> Adding default property:
> spark.driver.extraJavaOptions="-Dfoo.bar.baz=23"
> >>>
> >>> scala>  System.getProperty("foo.bar.baz")
> >>> res0: String = null
> >>>
> >>>
> >>> Neither one of those affects the issue; the underlying problem in my
> case
> >>> seems to be that bin/spark-class uses the SPARK_SUBMIT_OPTS and
> >>> SPARK_JAVA_OPTS environment variables, but nothing parses
> >>> spark-defaults.conf before the java process is started.
> >>>
> >>> Here's an example of the process running when only spark-defaults.conf
> is
> >>> being used:
> >>>
> >>> $ ps -ef | grep spark
> >>>
> >>> 514       5182  2058  0 21:05 pts/2    00:00:00 bash ./bin/spark-shell
> -v
> >>>
> >>> 514       5189  5182  4 21:05 pts/2    00:00:22
> /usr/local/java/bin/java
> >>> -cp
> >>>
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> >>> -XX:MaxPermSize=128m -Djava.library.path= -Xms512m -Xmx512m
> >>> org.apache.spark.deploy.SparkSubmit spark-shell -v --class
> >>> org.apache.spark.repl.Main
> >>>
> >>>
> >>> Here's an example of it when the command line --driver-java-options is
> >>> used (and thus things work):
> >>>
> >>>
> >>> $ ps -ef | grep spark
> >>> 514       5392  2058  0 21:15 pts/2    00:00:00 bash ./bin/spark-shell
> -v
> >>> --driver-java-options -Dfoo.bar.baz=23
> >>>
> >>> 514       5399  5392 80 21:15 pts/2    00:00:06
> /usr/local/java/bin/java
> >>> -cp
> >>>
> ::/opt/spark/conf:/opt/spark/lib/spark-assembly-1.0.1-hadoop2.3.0-mr1-cdh5.0.2.jar:/etc/hadoop/conf-mx
> >>> -XX:MaxPermSize=128m -Dfoo.bar.baz=23 -Djava.library.path= -Xms512m
> >>> -Xmx512m org.apache.spark.deploy.SparkSubmit spark-shell -v
> >>> --driver-java-options -Dfoo.bar.baz=23 --class
> org.apache.spark.repl.Main
> >>>
> >>>
> >>>
> >>>
> >>> On Wed, Jul 30, 2014 at 3:43 PM, Patrick Wendell <pwendell@gmail.com>
> >>> wrote:
> >>>
> >>>> Cody - in your example you are using the '=' character, but in our
> >>>> documentation and tests we use a whitespace to separate the key and
> >>>> value in the defaults file.
> >>>>
> >>>> docs: http://spark.apache.org/docs/latest/configuration.html
> >>>>
> >>>> spark.driver.extraJavaOptions -Dfoo.bar.baz=23
> >>>>
> >>>> I'm not sure if the java properties file parser will try to interpret
> >>>> the equals sign. If so you might need to do this.
> >>>>
> >>>> spark.driver.extraJavaOptions "-Dfoo.bar.baz=23"
> >>>>
> >>>> Do those work for you?
> >>>>
> >>>> On Wed, Jul 30, 2014 at 1:32 PM, Marcelo Vanzin <vanzin@cloudera.com>
> >>>> wrote:
> >>>> > Hi Cody,
> >>>> >
> >>>> > Could you file a bug for this if there isn't one already?
> >>>> >
> >>>> > For system properties SparkSubmit should be able to read those
> >>>> > settings and do the right thing, but that obviously won't work for
> >>>> > other JVM options... the current code should work fine in cluster
> mode
> >>>> > though, since the driver is a different process. :-)
> >>>> >
> >>>> >
> >>>> > On Wed, Jul 30, 2014 at 1:12 PM, Cody Koeninger <cody@koeninger.org
> >
> >>>> wrote:
> >>>> >> We were previously using SPARK_JAVA_OPTS to set java system
> properties
> >>>> via
> >>>> >> -D.
> >>>> >>
> >>>> >> This was used for properties that varied on a
> >>>> per-deployment-environment
> >>>> >> basis, but needed to be available in the spark shell and workers.
> >>>> >>
> >>>> >> On upgrading to 1.0, we saw that SPARK_JAVA_OPTS had been
> deprecated,
> >>>> and
> >>>> >> replaced by spark-defaults.conf and command line arguments to
> >>>> spark-submit
> >>>> >> or spark-shell.
> >>>> >>
> >>>> >> However, setting spark.driver.extraJavaOptions and
> >>>> >> spark.executor.extraJavaOptions in spark-defaults.conf is not a
> >>>> replacement
> >>>> >> for SPARK_JAVA_OPTS:
> >>>> >>
> >>>> >>
> >>>> >> $ cat conf/spark-defaults.conf
> >>>> >> spark.driver.extraJavaOptions=-Dfoo.bar.baz=23
> >>>> >>
> >>>> >> $ ./bin/spark-shell
> >>>> >>
> >>>> >> scala> System.getProperty("foo.bar.baz")
> >>>> >> res0: String = null
> >>>> >>
> >>>> >>
> >>>> >> $ ./bin/spark-shell --driver-java-options "-Dfoo.bar.baz=23"
> >>>> >>
> >>>> >> scala> System.getProperty("foo.bar.baz")
> >>>> >> res0: String = 23
> >>>> >>
> >>>> >>
> >>>> >> Looking through the shell scripts for spark-submit and
> spark-class, I
> >>>> can
> >>>> >> see why this is; parsing spark-defaults.conf from bash could be
> >>>> brittle.
> >>>> >>
> >>>> >> But from an ergonomic point of view, it's a step back to go from a
> >>>> >> set-it-and-forget-it configuration in spark-env.sh, to requiring
> >>>> command
> >>>> >> line arguments.
> >>>> >>
> >>>> >> I can solve this with an ad-hoc script to wrap spark-shell with the
> >>>> >> appropriate arguments, but I wanted to bring the issue up to see if
> >>>> anyone
> >>>> >> else had run into it,
> >>>> >> or had any direction for a general solution (beyond parsing java
> >>>> properties
> >>>> >> files from bash).
> >>>> >
> >>>> >
> >>>> >
> >>>> > --
> >>>> > Marcelo
> >>>>
> >>>
> >>>
>

--e89a8ff1cf92ec47a404ff7d59dd--

From dev-return-8653-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 14:47:53 2014
Return-Path: <dev-return-8653-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EBD7811B5E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 14:47:52 +0000 (UTC)
Received: (qmail 39148 invoked by uid 500); 31 Jul 2014 14:47:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 39089 invoked by uid 500); 31 Jul 2014 14:47:52 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 39078 invoked by uid 99); 31 Jul 2014 14:47:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 14:47:51 +0000
X-ASF-Spam-Status: No, hits=2.2 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_NEUTRAL
X-Spam-Check-By: apache.org
Received-SPF: neutral (nike.apache.org: local policy)
Received: from [209.85.220.173] (HELO mail-vc0-f173.google.com) (209.85.220.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 14:47:48 +0000
Received: by mail-vc0-f173.google.com with SMTP id hy10so4359847vcb.32
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 07:47:22 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=r7bJ6WYP64MJryjeCO3ElFcvehSWxS1m38RcWb0d6vU=;
        b=ZZzHTiN7Hy6lN6K6rGAqx06NN5voVB+1lQtwXTA/l3Pq/hFw+lfzQU2Dna4g74xnXw
         VOVbKXHmpngemBRn8B+25cHfsU+pmxbfDruKh2s+ob79kapdK4fpsLiKi/XYaBY5ML/B
         iV8HrsJhYcweAmo5UegoR3QLYqNaFwTPkmIRT19G24JMp85ODuDzFM6XPgeTYXr4ZDSz
         U1h1lhxix28CAw7zOkKxYELSkQziobnKEV0c1u0Q4C18T3BoFciAtdSm9MPVD5HqNID3
         Sx9jFwNAZoKTvktsLXlLDD6f9vqM6i38XhCzw6Tyl9sCJH1m8BPvuTEE8k38IW//Ixc7
         52+A==
X-Gm-Message-State: ALoCoQnl0XjvJ6tUISw2dctRqVV/oo0bXZyIt330GVeyNJd+WjgGTPfGBxaQJvBOPGEyQFF7uYIK
X-Received: by 10.52.171.233 with SMTP id ax9mr9881463vdc.76.1406818042613;
        Thu, 31 Jul 2014 07:47:22 -0700 (PDT)
Received: from mail-vc0-f179.google.com (mail-vc0-f179.google.com [209.85.220.179])
        by mx.google.com with ESMTPSA id td9sm3661108veb.8.2014.07.31.07.47.21
        for <dev@spark.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 31 Jul 2014 07:47:21 -0700 (PDT)
Received: by mail-vc0-f179.google.com with SMTP id hq11so4348559vcb.38
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 07:47:21 -0700 (PDT)
X-Received: by 10.52.129.165 with SMTP id nx5mr15317198vdb.25.1406818041133;
 Thu, 31 Jul 2014 07:47:21 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.220.124.211 with HTTP; Thu, 31 Jul 2014 07:47:01 -0700 (PDT)
From: Andrew Ash <andrew@andrewash.com>
Date: Thu, 31 Jul 2014 10:47:01 -0400
Message-ID: <CA+-p3AFnmbOdU61220GEwKibSMKB0M5hfrpvbsTTpG_G3ex3nA@mail.gmail.com>
Subject: Exception in Spark 1.0.1: com.esotericsoftware.kryo.KryoException:
 Buffer underflow
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=bcaec529a0079b014404ff7e5984
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec529a0079b014404ff7e5984
Content-Type: text/plain; charset=UTF-8

Hi everyone,

I'm seeing the below exception coming out of Spark 1.0.1 when I call it
from my application.  I can't share the source to that application, but the
quick gist is that it uses Spark's Java APIs to read from Avro files in
HDFS, do processing, and write back to Avro files.  It does this by
receiving a REST call, then spinning up a new JVM as the driver application
that connects to Spark.  I'm using CDH4.4.0 and have enabled Kryo and also
speculation.  The cluster is running in standalone mode on a 6 node cluster
in AWS (not using Spark's EC2 scripts though).

The below stacktraces are reliably reproduceable on every run of the job.
 The issue seems to be that on deserialization of a task result on the
driver, Kryo spits up while reading the ClassManifest.

I've tried swapping in Kryo 2.23.1 rather than 2.21 (2.22 had some
backcompat issues) but had the same error.

Any ideas on what can be done here?

Thanks!
Andrew



In the driver (Kryo exception while deserializing a DirectTaskResult):

INFO   | jvm 1    | 2014/07/30 20:52:52 | 20:52:52.667 [Result resolver
thread-0] ERROR o.a.spark.scheduler.TaskResultGetter - Exception while
getting task result
INFO   | jvm 1    | 2014/07/30 20:52:52 |
com.esotericsoftware.kryo.KryoException: Buffer underflow.
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.io.Input.require(Input.java:156)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.io.Input.readInt(Input.java:337)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:762)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readObject(Kryo.java:624) ~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:26)
~[chill_2.10-0.3.6.jar:0.3.6]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.twitter.chill.ClassManifestSerializer.read(ClassManifestSerializer.scala:19)
~[chill_2.10-0.3.6.jar:0.3.6]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
~[kryo-2.21.jar:na]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.serializer.KryoSerializerInstance.deserialize(KryoSerializer.scala:147)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:79)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskSetManager.handleSuccessfulTask(TaskSetManager.scala:480)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskSchedulerImpl.handleSuccessfulTask(TaskSchedulerImpl.scala:316)
~[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:68)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:47)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1160)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:46)
[spark-core_2.10-1.0.1.jar:1.0.1]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[na:1.7.0_65]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[na:1.7.0_65]
INFO   | jvm 1    | 2014/07/30 20:52:52 |       at
java.lang.Thread.run(Thread.java:745) [na:1.7.0_65]


In the DAGScheduler (job gets aborted):

org.apache.spark.SparkException: Job aborted due to stage failure:
Exception while getting task result:
com.esotericsoftware.kryo.KryoException: Buffer underflow.
    at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1044)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1028)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1026)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1026)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
    at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:634)
    at scala.Option.foreach(Option.scala:236)
    at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:634)
    at
org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1229)
    at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
    at akka.actor.ActorCell.invoke(ActorCell.scala:456)
    at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
    at akka.dispatch.Mailbox.run(Mailbox.scala:219)
    at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
    at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


In an Executor (running tasks get killed):

14/07/29 22:57:38 INFO broadcast.HttpBroadcast: Started reading broadcast
variable 0
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
153
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
147
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
141
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
135
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
150
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
144
14/07/29 22:57:39 INFO executor.Executor: Executor is trying to kill task
138
14/07/29 22:57:39 INFO storage.MemoryStore: ensureFreeSpace(241733) called
with curMem=0, maxMem=30870601728
14/07/29 22:57:39 INFO storage.MemoryStore: Block broadcast_0 stored as
values to memory (estimated size 236.1 KB, free 28.8 GB)
14/07/29 22:57:39 INFO broadcast.HttpBroadcast: Reading broadcast variable
0 took 0.91790748 s
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:39 INFO storage.BlockManager: Found block broadcast_0 locally
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 135
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 144
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 150
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 138
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
14/07/29 22:57:40 ERROR executor.Executor: Exception in task ID 141
org.apache.spark.TaskKilledException
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:174)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)

--bcaec529a0079b014404ff7e5984--

From dev-return-8654-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 20:37:14 2014
Return-Path: <dev-return-8654-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AEB9411AF0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 20:37:14 +0000 (UTC)
Received: (qmail 14470 invoked by uid 500); 31 Jul 2014 20:37:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 14410 invoked by uid 500); 31 Jul 2014 20:37:14 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 14396 invoked by uid 99); 31 Jul 2014 20:37:13 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 20:37:13 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.223.175 as permitted sender)
Received: from [209.85.223.175] (HELO mail-ie0-f175.google.com) (209.85.223.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 20:37:07 +0000
Received: by mail-ie0-f175.google.com with SMTP id x19so4597374ier.34
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 13:36:47 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9/mdh/CglqmBTWV6uQOLmiusEdDyCPNzLr6Fg1sOM18=;
        b=L/DuQG7R0N49VkCFbpAhiIlpsOd7Fm5avY5WQBkDQmL5tdVPkFaxWLGyKtz41fnFrn
         m0P4eCrJWohnHaM64d9/xpMIafifC1rb0Dz5h9d81ccFl0yCPCBhXm3fN/H49f46A8Sv
         pMCgLVhNNDkLT0sVyYFAf+ISFkFRXxDJue6UG8/v9QQTyQJ2gWiywUhQw33LEduhF0wV
         3N1cUdRmWrfFPDg7zSnDL1/35xB6KEnW/D5nDMon8feV+kv34Gtr/NJmONcVtsVYZ9vE
         9mYzyYGYzZQl6mjGtyn09cEvrisFg9E5L/wjX9QgZiReak2osd5kxMUpNBcW/3mfCVhP
         Lozw==
MIME-Version: 1.0
X-Received: by 10.50.87.10 with SMTP id t10mr98523igz.41.1406839007220; Thu,
 31 Jul 2014 13:36:47 -0700 (PDT)
Received: by 10.107.35.213 with HTTP; Thu, 31 Jul 2014 13:36:47 -0700 (PDT)
In-Reply-To: <CAMwrk0mO+w2ysWoODmRb3Mt4gBsP50YtD2inOL27xwzKEU+PUA@mail.gmail.com>
References: <CA+FETEKK7_7w_Z3ii=Yz-X+fwsTA2vzvYJxcEfCm6rFX=MNw1g@mail.gmail.com>
	<CAMwrk0mO+w2ysWoODmRb3Mt4gBsP50YtD2inOL27xwzKEU+PUA@mail.gmail.com>
Date: Thu, 31 Jul 2014 13:36:47 -0700
Message-ID: <CA+FETEJUEwndRG_0-zaM5pLoMzUDrbOjFUNf2GnRQTYwgV87YQ@mail.gmail.com>
Subject: Re: failed to build spark with maven for both 1.0.1 and latest master branch
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=089e0102ec5648399f04ff833b49
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0102ec5648399f04ff833b49
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Hi TD,

I've asked my colleagues to do the same thing but compile still fails.
However, maven build succeeded once I built it on my personal macbook (with
the latest MacOS Yosemite). So I guess there might be something wrong in my
build environment. Wonder if anyone tried to compile spark using maven
under Mavericks, please let me know your result.

Thanks
Shengzhe


On Thu, Jul 31, 2014 at 1:25 AM, Tathagata Das <tathagata.das1565@gmail.com=
>
wrote:

> Does a "mvn clean" or "sbt/sbt clean" help?
>
> TD
>
> On Wed, Jul 30, 2014 at 9:25 PM, yao <yaoshengzhe@gmail.com> wrote:
> > Hi Folks,
> >
> > Today I am trying to build spark using maven; however, the following
> > command failed consistently for both 1.0.1 and the latest master.  (BTW=
,
> it
> > seems sbt works fine: *sbt/sbt -Dhadoop.version=3D2.4.0 -Pyarn clean
> > assembly)*
> >
> > Environment: Mac OS Mavericks
> > Maven: 3.2.2 (installed by homebrew)
> >
> >
> >
> >
> > *export M2_HOME=3D/usr/local/Cellar/maven/3.2.2/libexec/export
> > PATH=3D$M2_HOME/bin:$PATHexport MAVEN_OPTS=3D"-Xmx2g -XX:MaxPermSize=3D=
512M
> > -XX:ReservedCodeCacheSize=3D512m"mvn -Pyarn -Phadoop-2.4
> > -Dhadoop.version=3D2.4.0 -DskipTests clean package*
> >
> > Build outputs:
> >
> > [INFO] Scanning for projects...
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO] Reactor Build Order:
> > [INFO]
> > [INFO] Spark Project Parent POM
> > [INFO] Spark Project Core
> > [INFO] Spark Project Bagel
> > [INFO] Spark Project GraphX
> > [INFO] Spark Project ML Library
> > [INFO] Spark Project Streaming
> > [INFO] Spark Project Tools
> > [INFO] Spark Project Catalyst
> > [INFO] Spark Project SQL
> > [INFO] Spark Project Hive
> > [INFO] Spark Project REPL
> > [INFO] Spark Project YARN Parent POM
> > [INFO] Spark Project YARN Stable API
> > [INFO] Spark Project Assembly
> > [INFO] Spark Project External Twitter
> > [INFO] Spark Project External Kafka
> > [INFO] Spark Project External Flume
> > [INFO] Spark Project External ZeroMQ
> > [INFO] Spark Project External MQTT
> > [INFO] Spark Project Examples
> > [INFO]
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO] Building Spark Project Parent POM 1.0.1
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO]
> > [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-parent
> ---
> > [INFO]
> > [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> > spark-parent ---
> > [INFO]
> > [INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources)=
 @
> > spark-parent ---
> > [INFO] Source directory:
> > /Users/syao/git/grid/thirdparty/spark/src/main/scala added.
> > [INFO]
> > [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> > spark-parent ---
> > [INFO]
> > [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
> > spark-parent ---
> > [INFO] Add Test Source directory:
> > /Users/syao/git/grid/thirdparty/spark/src/test/scala
> > [INFO]
> > [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> > spark-parent ---
> > [INFO] No sources to compile
> > [INFO]
> > [INFO] --- build-helper-maven-plugin:1.8:add-test-source
> > (add-scala-test-sources) @ spark-parent ---
> > [INFO] Test Source directory:
> > /Users/syao/git/grid/thirdparty/spark/src/test/scala added.
> > [INFO]
> > [INFO] --- scala-maven-plugin:3.1.6:testCompile
> (scala-test-compile-first)
> > @ spark-parent ---
> > [INFO] No sources to compile
> > [INFO]
> > [INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor) =
@
> > spark-parent ---
> > [INFO]
> > [INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar) @
> > spark-parent ---
> > [INFO]
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO] Building Spark Project Core 1.0.1
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO]
> > [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-core_2.=
10
> > ---
> > [INFO]
> > [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> > spark-core_2.10 ---
> > [INFO]
> > [INFO] --- build-helper-maven-plugin:1.8:add-source (add-scala-sources)=
 @
> > spark-core_2.10 ---
> > [INFO] Source directory:
> > /Users/syao/git/grid/thirdparty/spark/core/src/main/scala added.
> > [INFO]
> > [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> > spark-core_2.10 ---
> > [INFO]
> > [INFO] --- exec-maven-plugin:1.2.1:exec (default) @ spark-core_2.10 ---
> > Archive:  lib/py4j-0.8.1-src.zip
> >   inflating: build/py4j/tests/java_map_test.py
> >  extracting: build/py4j/tests/__init__.py
> >   inflating: build/py4j/tests/java_gateway_test.py
> >   inflating: build/py4j/tests/java_callback_test.py
> >   inflating: build/py4j/tests/java_list_test.py
> >   inflating: build/py4j/tests/byte_string_test.py
> >   inflating: build/py4j/tests/multithreadtest.py
> >   inflating: build/py4j/tests/java_array_test.py
> >   inflating: build/py4j/tests/py4j_callback_example2.py
> >   inflating: build/py4j/tests/py4j_example.py
> >   inflating: build/py4j/tests/py4j_callback_example.py
> >   inflating: build/py4j/tests/finalizer_test.py
> >   inflating: build/py4j/tests/java_set_test.py
> >   inflating: build/py4j/finalizer.py
> >  extracting: build/py4j/__init__.py
> >   inflating: build/py4j/java_gateway.py
> >   inflating: build/py4j/protocol.py
> >   inflating: build/py4j/java_collections.py
> >  extracting: build/py4j/version.py
> >   inflating: build/py4j/compat.py
> > [INFO]
> > [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @
> > spark-core_2.10 ---
> > [INFO] Using 'UTF-8' encoding to copy filtered resources.
> > [INFO] Copying 6 resources
> > [INFO] Copying 20 resources
> > [INFO] Copying 7 resources
> > [INFO] Copying 3 resources
> > [INFO]
> > [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) @
> > spark-core_2.10 ---
> > [INFO] Add Test Source directory:
> > /Users/syao/git/grid/thirdparty/spark/core/src/test/scala
> > [INFO]
> > [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> > spark-core_2.10 ---
> > [INFO] Using zinc server for incremental compilation
> > [info] Compiling 342 Scala sources and 34 Java sources to
> > /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes...
> > [warn] Class javax.servlet.ServletException not found - continuing with=
 a
> > stub.
> > [error]
> > [error]      while compiling:
> >
> /Users/syao/git/grid/thirdparty/spark/core/src/main/scala/org/apache/spar=
k/HttpServer.scala
> > [error]         during phase: typer
> > [error]      library version: version 2.10.4
> > [error]     compiler version: version 2.10.4
> > [error]   reconstructed args: -classpath
> >
> /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes:/Use=
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-=
2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/=
hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/=
1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commo=
ns-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xm=
lenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/com=
mons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/c=
ommons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:=
/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.j=
ar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1=
.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digeste=
r/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/=
commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/User=
s/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/common=
s-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/=
jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repositor=
y/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.ja=
r:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Use=
rs/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-jav=
a-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/=
hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons=
-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/t=
ukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoo=
p-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/j=
etty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org=
/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-ap=
p-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-c=
lient-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2=
/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2=
.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey=
-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-se=
rver-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapred=
uce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/o=
rg/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client=
-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-co=
mmon/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xm=
l/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/x=
ml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/java=
x/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/c=
om/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repositor=
y/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduc=
e-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/reposi=
tory/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/=
repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:=
/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcor=
e-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2=
.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curato=
r/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Use=
rs/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v201310=
31/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/j=
etty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v2=
01105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8=
.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repositor=
y/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfis=
h/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/s=
yao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v20110507=
1233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/or=
g/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v2013=
1031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20=
131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclip=
se/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/U=
sers/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Us=
ers/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang=
3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.=
9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf=
4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/ju=
l-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.=
7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/=
log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/s=
lf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0=
.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snap=
py-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/=
chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esoter=
icsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esot=
ericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/s=
yao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/U=
sers/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Use=
rs/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/U=
sers/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/U=
sers/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shad=
ed-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repo=
sitory/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-ac=
tor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/=
config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6=
.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project=
/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/=
syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-ma=
ths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j=
_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/User=
s/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2=
.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/=
json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast=
_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/though=
tworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository=
/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/=
org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/=
.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-da=
tabind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jack=
son-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/reposit=
ory/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/U=
sers/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/rep=
ository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/re=
pository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Us=
ers/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Fi=
nal.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/s=
tream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-cor=
e/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metr=
ics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/=
codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/=
repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0=
.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/t=
achyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0=
/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0=
/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.=
4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflec=
t/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-proj=
ect/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4=
j/py4j/0.8.1/py4j-0.8.1.jar
> > -deprecation -feature -bootclasspath
> >
> /Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/r=
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Hom=
e/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents=
/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.=
jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7=
.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/j=
dk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtual=
Machines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVi=
rtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java=
/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/=
.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
> > -unchecked -language:postfixOps
> > [error]
> > [error]   last tree to typer: Ident(Server)
> > [error]               symbol: <none> (flags: )
> > [error]    symbol definition: <none>
> > [error]        symbol owners:
> > [error]       context owners: variable server -> class HttpServer ->
> > package spark
> > [error]
> > [error] =3D=3D Enclosing template or block =3D=3D
> > [error]
> > [error] Template( // val <local HttpServer>: <notype> in class HttpServ=
er
> > [error]   "org.apache.spark.Logging" // parents
> > [error]   ValDef(
> > [error]     private
> > [error]     "_"
> > [error]     <tpt>
> > [error]     <empty>
> > [error]   )
> > [error]   // 9 statements
> > [error]   ValDef( // private[this] val resourceBase: <?> in class
> HttpServer
> > [error]     private <local> <paramaccessor>
> > [error]     "resourceBase"
> > [error]     "File"
> > [error]     <empty>
> > [error]   )
> > [error]   ValDef( // private[this] val securityManager: <?> in class
> > HttpServer
> > [error]     private <local> <paramaccessor>
> > [error]     "securityManager"
> > [error]     "SecurityManager"
> > [error]     <empty>
> > [error]   )
> > [error]   DefDef( // def <init>(resourceBase:
> java.io.File,securityManager:
> > org.apache.spark.SecurityManager): org.apache.spark.HttpServer in class
> > HttpServer
> > [error]     <method> <triedcooking>
> > [error]     "<init>"
> > [error]     []
> > [error]     // 1 parameter list
> > [error]     ValDef( // resourceBase: java.io.File
> > [error]       <param> <paramaccessor>
> > [error]       "resourceBase"
> > [error]       "File"
> > [error]       <empty>
> > [error]     )
> > [error]     ValDef( // securityManager: org.apache.spark.SecurityManage=
r
> > [error]       <param> <paramaccessor>
> > [error]       "securityManager"
> > [error]       "SecurityManager" // private[package spark] class
> > SecurityManager extends Logging in package spark,
> > tree.tpe=3Dorg.apache.spark.SecurityManager
> > [error]       <empty>
> > [error]     )
> > [error]     <tpt> // tree.tpe=3Dorg.apache.spark.HttpServer
> > [error]     Block(
> > [error]       Apply(
> > [error]         super."<init>"
> > [error]         Nil
> > [error]       )
> > [error]       ()
> > [error]     )
> > [error]   )
> > [error]   ValDef( // private[this] var server: <?> in class HttpServer
> > [error]     private <mutable> <local>
> > [error]     "server"
> > [error]     "Server"
> > [error]     null
> > [error]   )
> > [error]   ValDef( // private[this] var port: <?> in class HttpServer
> > [error]     private <mutable> <local>
> > [error]     "port"
> > [error]     "Int"
> > [error]     -1
> > [error]   )
> > [error]   DefDef( // def start(): Unit in class HttpServer
> > [error]     <method> <triedcooking>
> > [error]     "start"
> > [error]     []
> > [error]     List(Nil)
> > [error]     "scala"."Unit" // final abstract class Unit extends AnyVal =
in
> > package scala, tree.tpe=3DUnit
> > [error]     If(
> > [error]       Apply(
> > [error]         "server"."$bang$eq"
> > [error]         null
> > [error]       )
> > [error]       Throw(
> > [error]         Apply(
> > [error]           new ServerStateException."<init>"
> > [error]           "Server is already started"
> > [error]         )
> > [error]       )
> > [error]       Block(
> > [error]         // 16 statements
> > [error]         Apply(
> > [error]           "logInfo"
> > [error]           "Starting HTTP Server"
> > [error]         )
> > [error]         Assign(
> > [error]           "server"
> > [error]           Apply(
> > [error]             new Server."<init>"
> > [error]             Nil
> > [error]           )
> > [error]         )
> > [error]         ValDef(
> > [error]           0
> > [error]           "connector"
> > [error]           <tpt>
> > [error]           Apply(
> > [error]             new SocketConnector."<init>"
> > [error]             Nil
> > [error]           )
> > [error]         )
> > [error]         Apply(
> > [error]           "connector"."setMaxIdleTime"
> > [error]           Apply(
> > [error]             60."$times"
> > [error]             1000
> > [error]           )
> > [error]         )
> > [error]         Apply(
> > [error]           "connector"."setSoLingerTime"
> > [error]           -1
> > [error]         )
> > [error]         Apply(
> > [error]           "connector"."setPort"
> > [error]           0
> > [error]         )
> > [error]         Apply(
> > [error]           "server"."addConnector"
> > [error]           "connector"
> > [error]         )
> > [error]         ValDef(
> > [error]           0
> > [error]           "threadPool"
> > [error]           <tpt>
> > [error]           Apply(
> > [error]             new QueuedThreadPool."<init>"
> > [error]             Nil
> > [error]           )
> > [error]         )
> > [error]         Apply(
> > [error]           "threadPool"."setDaemon"
> > [error]           true
> > [error]         )
> > [error]         Apply(
> > [error]           "server"."setThreadPool"
> > [error]           "threadPool"
> > [error]         )
> > [error]         ValDef(
> > [error]           0
> > [error]           "resHandler"
> > [error]           <tpt>
> > [error]           Apply(
> > [error]             new ResourceHandler."<init>"
> > [error]             Nil
> > [error]           )
> > [error]         )
> > [error]         Apply(
> > [error]           "resHandler"."setResourceBase"
> > [error]           "resourceBase"."getAbsolutePath"
> > [error]         )
> > [error]         ValDef(
> > [error]           0
> > [error]           "handlerList"
> > [error]           <tpt>
> > [error]           Apply(
> > [error]             new HandlerList."<init>"
> > [error]             Nil
> > [error]           )
> > [error]         )
> > [error]         Apply(
> > [error]           "handlerList"."setHandlers"
> > [error]           Apply(
> > [error]             "Array"
> > [error]             // 2 arguments
> > [error]             "resHandler"
> > [error]             Apply(
> > [error]               new DefaultHandler."<init>"
> > [error]               Nil
> > [error]             )
> > [error]           )
> > [error]         )
> > [error]         If(
> > [error]           Apply(
> > [error]             "securityManager"."isAuthenticationEnabled"
> > [error]             Nil
> > [error]           )
> > [error]           Block(
> > [error]             // 3 statements
> > [error]             Apply(
> > [error]               "logDebug"
> > [error]               "HttpServer is using security"
> > [error]             )
> > [error]             ValDef(
> > [error]               0
> > [error]               "sh"
> > [error]               <tpt>
> > [error]               Apply(
> > [error]                 "setupSecurityHandler"
> > [error]                 "securityManager"
> > [error]               )
> > [error]             )
> > [error]             Apply(
> > [error]               "sh"."setHandler"
> > [error]               "handlerList"
> > [error]             )
> > [error]             Apply(
> > [error]               "server"."setHandler"
> > [error]               "sh"
> > [error]             )
> > [error]           )
> > [error]           Block(
> > [error]             Apply(
> > [error]               "logDebug"
> > [error]               "HttpServer is not using security"
> > [error]             )
> > [error]             Apply(
> > [error]               "server"."setHandler"
> > [error]               "handlerList"
> > [error]             )
> > [error]           )
> > [error]         )
> > [error]         Apply(
> > [error]           "server"."start"
> > [error]           Nil
> > [error]         )
> > [error]         Assign(
> > [error]           "port"
> > [error]           Apply(
> > [error]             server.getConnectors()(0)."getLocalPort"
> > [error]             Nil
> > [error]           )
> > [error]         )
> > [error]       )
> > [error]     )
> > [error]   )
> > [error]   DefDef( // private def setupSecurityHandler: <?> in class
> > HttpServer
> > [error]     <method> private
> > [error]     "setupSecurityHandler"
> > [error]     []
> > [error]     // 1 parameter list
> > [error]     ValDef(
> > [error]       <param>
> > [error]       "securityMgr"
> > [error]       "SecurityManager"
> > [error]       <empty>
> > [error]     )
> > [error]     "ConstraintSecurityHandler"
> > [error]     Block(
> > [error]       // 16 statements
> > [error]       ValDef(
> > [error]         0
> > [error]         "constraint"
> > [error]         <tpt>
> > [error]         Apply(
> > [error]           new Constraint."<init>"
> > [error]           Nil
> > [error]         )
> > [error]       )
> > [error]       Apply(
> > [error]         "constraint"."setName"
> > [error]         "Constraint"."__DIGEST_AUTH"
> > [error]       )
> > [error]       Apply(
> > [error]         "constraint"."setRoles"
> > [error]         Apply(
> > [error]           "Array"
> > [error]           "user"
> > [error]         )
> > [error]       )
> > [error]       Apply(
> > [error]         "constraint"."setAuthenticate"
> > [error]         true
> > [error]       )
> > [error]       Apply(
> > [error]         "constraint"."setDataConstraint"
> > [error]         "Constraint"."DC_NONE"
> > [error]       )
> > [error]       ValDef(
> > [error]         0
> > [error]         "cm"
> > [error]         <tpt>
> > [error]         Apply(
> > [error]           new ConstraintMapping."<init>"
> > [error]           Nil
> > [error]         )
> > [error]       )
> > [error]       Apply(
> > [error]         "cm"."setConstraint"
> > [error]         "constraint"
> > [error]       )
> > [error]       Apply(
> > [error]         "cm"."setPathSpec"
> > [error]         "/*"
> > [error]       )
> > [error]       ValDef(
> > [error]         0
> > [error]         "sh"
> > [error]         <tpt>
> > [error]         Apply(
> > [error]           new ConstraintSecurityHandler."<init>"
> > [error]           Nil
> > [error]         )
> > [error]       )
> > [error]       ValDef(
> > [error]         0
> > [error]         "hashLogin"
> > [error]         <tpt>
> > [error]         Apply(
> > [error]           new HashLoginService."<init>"
> > [error]           Nil
> > [error]         )
> > [error]       )
> > [error]       ValDef(
> > [error]         0
> > [error]         "userCred"
> > [error]         <tpt>
> > [error]         Apply(
> > [error]           new Password."<init>"
> > [error]           Apply(
> > [error]             "securityMgr"."getSecretKey"
> > [error]             Nil
> > [error]           )
> > [error]         )
> > [error]       )
> > [error]       If(
> > [error]         Apply(
> > [error]           "userCred"."$eq$eq"
> > [error]           null
> > [error]         )
> > [error]         Throw(
> > [error]           Apply(
> > [error]             new Exception."<init>"
> > [error]             "Error: secret key is null with authentication on"
> > [error]           )
> > [error]         )
> > [error]         ()
> > [error]       )
> > [error]       Apply(
> > [error]         "hashLogin"."putUser"
> > [error]         // 3 arguments
> > [error]         Apply(
> > [error]           "securityMgr"."getHttpUser"
> > [error]           Nil
> > [error]         )
> > [error]         "userCred"
> > [error]         Apply(
> > [error]           "Array"
> > [error]           "user"
> > [error]         )
> > [error]       )
> > [error]       Apply(
> > [error]         "sh"."setLoginService"
> > [error]         "hashLogin"
> > [error]       )
> > [error]       Apply(
> > [error]         "sh"."setAuthenticator"
> > [error]         Apply(
> > [error]           new DigestAuthenticator."<init>"
> > [error]           Nil
> > [error]         )
> > [error]       )
> > [error]       Apply(
> > [error]         "sh"."setConstraintMappings"
> > [error]         Apply(
> > [error]           "Array"
> > [error]           "cm"
> > [error]         )
> > [error]       )
> > [error]       "sh"
> > [error]     )
> > [error]   )
> > [error]   DefDef( // def stop(): Unit in class HttpServer
> > [error]     <method> <triedcooking>
> > [error]     "stop"
> > [error]     []
> > [error]     List(Nil)
> > [error]     "scala"."Unit" // final abstract class Unit extends AnyVal =
in
> > package scala, tree.tpe=3DUnit
> > [error]     If(
> > [error]       Apply(
> > [error]         "server"."$eq$eq"
> > [error]         null
> > [error]       )
> > [error]       Throw(
> > [error]         Apply(
> > [error]           new ServerStateException."<init>"
> > [error]           "Server is already stopped"
> > [error]         )
> > [error]       )
> > [error]       Block(
> > [error]         // 2 statements
> > [error]         Apply(
> > [error]           "server"."stop"
> > [error]           Nil
> > [error]         )
> > [error]         Assign(
> > [error]           "port"
> > [error]           -1
> > [error]         )
> > [error]         Assign(
> > [error]           "server"
> > [error]           null
> > [error]         )
> > [error]       )
> > [error]     )
> > [error]   )
> > [error]   DefDef( // def uri: String in class HttpServer
> > [error]     <method> <triedcooking>
> > [error]     "uri"
> > [error]     []
> > [error]     Nil
> > [error]     "String"
> > [error]     If(
> > [error]       Apply(
> > [error]         "server"."$eq$eq"
> > [error]         null
> > [error]       )
> > [error]       Throw(
> > [error]         Apply(
> > [error]           new ServerStateException."<init>"
> > [error]           "Server is not started"
> > [error]         )
> > [error]       )
> > [error]       Return(
> > [error]         Apply(
> > [error]           "http://
> ".$plus(Utils.localIpAddress).$plus(":")."$plus"
> > [error]           "port"
> > [error]         )
> > [error]       )
> > [error]     )
> > [error]   )
> > [error] )
> > [error]
> > [error] uncaught exception during compilation: java.lang.AssertionError
> > java.lang.AssertionError: assertion failed:
> javax.servlet.ServletException
> >     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)
> >     at
> scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseExceptions$1(Classf=
ileParser.scala:1051)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseAttributes(Classfil=
eParser.scala:1080)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseMethod(ClassfilePar=
ser.scala:666)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser$$anonfun$parseClass$1.ap=
ply$mcV$sp(ClassfileParser.scala:567)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfilePars=
er.scala:572)
> >     at
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.sc=
ala:88)
> >     at
> >
> scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoa=
ders.scala:261)
> >     at
> >
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.=
scala:194)
> >     at
> >
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scal=
a:210)
> >     at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:893)
> >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:5064=
)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.sc=
ala:5218)
> >     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:556=
1)
> >     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642=
)
> >     at
> scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5769)
> >     at
> scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5772)
> >     at
> scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1317)
> >     at
> scala.tools.nsc.typechecker.Namers$Namer.getSig$1(Namers.scala:1457)
> >     at
> scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1466)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$Namer.scala$tools$nsc$typechecker$Name=
rs$Namer$$logAndValidate(Namers.scala:1499)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:730)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:729)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$$anon$1.completeImpl(Namers.scala:1614=
)
> >     at
> >
> scala.tools.nsc.typechecker.Namers$LockingTypeCompleter$class.complete(Na=
mers.scala:1622)
> >     at
> > scala.tools.nsc.typechecker.Namers$$anon$1.complete(Namers.scala:1612)
> >     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
> >     at
> scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
> >     at
> >
> scala.tools.nsc.typechecker.MethodSynthesis$MethodSynth$class.addDerivedT=
rees(MethodSynthesis.scala:225)
> >     at
> > scala.tools.nsc.typechecker.Namers$Namer.addDerivedTrees(Namers.scala:5=
5)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1856)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1853)
> >     at
> >
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
> >     at
> >
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
> >     at scala.collection.immutable.List.foreach(List.scala:318)
> >     at
> > scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:25=
1)
> >     at
> scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
> >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:191=
7)
> >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:175=
9)
> >     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:558=
3)
> >     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642=
)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Type=
rs$Typer$$typedStat$1(Typers.scala:2928)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
> >     at scala.collection.immutable.List.loop$1(List.scala:170)
> >     at scala.collection.immutable.List.mapConserve(List.scala:186)
> >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032)
> >     at
> >
> scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5=
301)
> >     at scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:558=
7)
> >     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642=
)
> >     at scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5704=
)
> >     at
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.=
scala:99)
> >     at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
> >     at
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
> >     at
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
> >     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
> >     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> >     at
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.sc=
ala:91)
> >     at scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:158=
3)
> >     at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> >     at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> >     at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> >     at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
> >     at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
> >     at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >     at
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >     at
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >     at java.lang.reflect.Method.invoke(Method.java:606)
> >     at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> >     at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:4=
8)
> >     at sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:4=
1)
> >     at
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
$mcV$sp(AggressiveCompile.scala:99)
> >     at
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
> >     at
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
> >     at
> >
> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(Aggr=
essiveCompile.scala:166)
> >     at
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompil=
e.scala:98)
> >     at
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:1=
43)
> >     at
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:8=
7)
> >     at
> > sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
> >     at
> > sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
> >     at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
> >     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
> >     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
> >     at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
> >     at sbt.inc.Incremental$.compile(Incremental.scala:37)
> >     at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
> >     at
> sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
> >     at
> sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
> >     at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
> >     at com.typesafe.zinc.Main$.run(Main.scala:98)
> >     at com.typesafe.zinc.Nailgun$.zinc(Nailgun.scala:93)
> >     at com.typesafe.zinc.Nailgun$.nailMain(Nailgun.scala:82)
> >     at com.typesafe.zinc.Nailgun.nailMain(Nailgun.scala)
> >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> >     at
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> >     at
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> >     at java.lang.reflect.Method.invoke(Method.java:606)
> >     at com.martiansoftware.nailgun.NGSession.run(NGSession.java:280)
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO] Reactor Summary:
> > [INFO]
> > [INFO] Spark Project Parent POM ........................... SUCCESS [
> > 1.760 s]
> > [INFO] Spark Project Core ................................. FAILURE [
> > 5.312 s]
> > [INFO] Spark Project Bagel ................................ SKIPPED
> > [INFO] Spark Project GraphX ............................... SKIPPED
> > [INFO] Spark Project ML Library ........................... SKIPPED
> > [INFO] Spark Project Streaming ............................ SKIPPED
> > [INFO] Spark Project Tools ................................ SKIPPED
> > [INFO] Spark Project Catalyst ............................. SKIPPED
> > [INFO] Spark Project SQL .................................. SKIPPED
> > [INFO] Spark Project Hive ................................. SKIPPED
> > [INFO] Spark Project REPL ................................. SKIPPED
> > [INFO] Spark Project YARN Parent POM ...................... SKIPPED
> > [INFO] Spark Project YARN Stable API ...................... SKIPPED
> > [INFO] Spark Project Assembly ............................. SKIPPED
> > [INFO] Spark Project External Twitter ..................... SKIPPED
> > [INFO] Spark Project External Kafka ....................... SKIPPED
> > [INFO] Spark Project External Flume ....................... SKIPPED
> > [INFO] Spark Project External ZeroMQ ...................... SKIPPED
> > [INFO] Spark Project External MQTT ........................ SKIPPED
> > [INFO] Spark Project Examples ............................. SKIPPED
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO] BUILD FAILURE
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [INFO] Total time: 7.562 s
> > [INFO] Finished at: 2014-07-30T21:18:41-07:00
> > [INFO] Final Memory: 39M/713M
> > [INFO]
> > -----------------------------------------------------------------------=
-
> > [ERROR] Failed to execute goal
> > net.alchim31.maven:scala-maven-plugin:3.1.6:compile (scala-compile-firs=
t)
> > on project spark-core_2.10: Execution scala-compile-first of goal
> > net.alchim31.maven:scala-maven-plugin:3.1.6:compile failed. CompileFail=
ed
> > -> [Help 1]
> >
> > Thanks
> > Shengzhe
>

--089e0102ec5648399f04ff833b49--

From dev-return-8655-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 20:43:43 2014
Return-Path: <dev-return-8655-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2875111B48
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 20:43:43 +0000 (UTC)
Received: (qmail 34683 invoked by uid 500); 31 Jul 2014 20:43:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34514 invoked by uid 500); 31 Jul 2014 20:43:41 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 34103 invoked by uid 99); 31 Jul 2014 20:43:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 20:43:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yuzhihong@gmail.com designates 209.85.213.46 as permitted sender)
Received: from [209.85.213.46] (HELO mail-yh0-f46.google.com) (209.85.213.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 20:43:33 +0000
Received: by mail-yh0-f46.google.com with SMTP id a41so1986131yho.5
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 13:43:12 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=qsU2mqLTQKRwvHiULx1m+g4bglxY4209ZIYDOApexSg=;
        b=MclhRAEYlTPNQdMjoHHUSryHQ9NE/ErknT8Yj382MqB3ubtr6cDOajeWDu/iBvmAaZ
         ilomo8g7PL4Wzwx5JqZKAw3+AFonSGi4hOzTzJhChtLpsm4p2+mgGFFucsiQhIgjkqMh
         5kAbV5q8WfgBhLug+3s82V8PEDjw+s3daehULk5fYCmccSU6+53HbdLylXopDTBJsJNg
         rIDUvGqAaKcjhe0hqEljKv5T/6dZh3xn6GLwZ+rxp783P/aMQkRng1FMteZBz/Q6DG7I
         W+tAgQWnQN4qslOyNqoceXaj5BIAU1yJxsVwMM0RTfOBzYnGlCqTtE97N8MBEcj+Ht0b
         1AjA==
MIME-Version: 1.0
X-Received: by 10.236.119.146 with SMTP id n18mr970523yhh.23.1406839392756;
 Thu, 31 Jul 2014 13:43:12 -0700 (PDT)
Received: by 10.170.136.14 with HTTP; Thu, 31 Jul 2014 13:43:12 -0700 (PDT)
In-Reply-To: <CA+FETEJUEwndRG_0-zaM5pLoMzUDrbOjFUNf2GnRQTYwgV87YQ@mail.gmail.com>
References: <CA+FETEKK7_7w_Z3ii=Yz-X+fwsTA2vzvYJxcEfCm6rFX=MNw1g@mail.gmail.com>
	<CAMwrk0mO+w2ysWoODmRb3Mt4gBsP50YtD2inOL27xwzKEU+PUA@mail.gmail.com>
	<CA+FETEJUEwndRG_0-zaM5pLoMzUDrbOjFUNf2GnRQTYwgV87YQ@mail.gmail.com>
Date: Thu, 31 Jul 2014 13:43:12 -0700
Message-ID: <CALte62yKdF1KXuk3edyHOBzeYU3nKxJAwvpYNwD5GYW2TRL_hA@mail.gmail.com>
Subject: Re: failed to build spark with maven for both 1.0.1 and latest master branch
From: Ted Yu <yuzhihong@gmail.com>
To: "dev@spark.apache.org" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=20cf30050cd442e95804ff83522f
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf30050cd442e95804ff83522f
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

The following command succeeded (on Linux) on Spark master checked out this
morning:

mvn -Pyarn -Phive -Phadoop-2.4 -DskipTests install

FYI


On Thu, Jul 31, 2014 at 1:36 PM, yao <yaoshengzhe@gmail.com> wrote:

> Hi TD,
>
> I've asked my colleagues to do the same thing but compile still fails.
> However, maven build succeeded once I built it on my personal macbook (wi=
th
> the latest MacOS Yosemite). So I guess there might be something wrong in =
my
> build environment. Wonder if anyone tried to compile spark using maven
> under Mavericks, please let me know your result.
>
> Thanks
> Shengzhe
>
>
> On Thu, Jul 31, 2014 at 1:25 AM, Tathagata Das <
> tathagata.das1565@gmail.com>
> wrote:
>
> > Does a "mvn clean" or "sbt/sbt clean" help?
> >
> > TD
> >
> > On Wed, Jul 30, 2014 at 9:25 PM, yao <yaoshengzhe@gmail.com> wrote:
> > > Hi Folks,
> > >
> > > Today I am trying to build spark using maven; however, the following
> > > command failed consistently for both 1.0.1 and the latest master.
>  (BTW,
> > it
> > > seems sbt works fine: *sbt/sbt -Dhadoop.version=3D2.4.0 -Pyarn clean
> > > assembly)*
> > >
> > > Environment: Mac OS Mavericks
> > > Maven: 3.2.2 (installed by homebrew)
> > >
> > >
> > >
> > >
> > > *export M2_HOME=3D/usr/local/Cellar/maven/3.2.2/libexec/export
> > > PATH=3D$M2_HOME/bin:$PATHexport MAVEN_OPTS=3D"-Xmx2g -XX:MaxPermSize=
=3D512M
> > > -XX:ReservedCodeCacheSize=3D512m"mvn -Pyarn -Phadoop-2.4
> > > -Dhadoop.version=3D2.4.0 -DskipTests clean package*
> > >
> > > Build outputs:
> > >
> > > [INFO] Scanning for projects...
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO] Reactor Build Order:
> > > [INFO]
> > > [INFO] Spark Project Parent POM
> > > [INFO] Spark Project Core
> > > [INFO] Spark Project Bagel
> > > [INFO] Spark Project GraphX
> > > [INFO] Spark Project ML Library
> > > [INFO] Spark Project Streaming
> > > [INFO] Spark Project Tools
> > > [INFO] Spark Project Catalyst
> > > [INFO] Spark Project SQL
> > > [INFO] Spark Project Hive
> > > [INFO] Spark Project REPL
> > > [INFO] Spark Project YARN Parent POM
> > > [INFO] Spark Project YARN Stable API
> > > [INFO] Spark Project Assembly
> > > [INFO] Spark Project External Twitter
> > > [INFO] Spark Project External Kafka
> > > [INFO] Spark Project External Flume
> > > [INFO] Spark Project External ZeroMQ
> > > [INFO] Spark Project External MQTT
> > > [INFO] Spark Project Examples
> > > [INFO]
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO] Building Spark Project Parent POM 1.0.1
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO]
> > > [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @ spark-paren=
t
> > ---
> > > [INFO]
> > > [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> > > spark-parent ---
> > > [INFO]
> > > [INFO] --- build-helper-maven-plugin:1.8:add-source
> (add-scala-sources) @
> > > spark-parent ---
> > > [INFO] Source directory:
> > > /Users/syao/git/grid/thirdparty/spark/src/main/scala added.
> > > [INFO]
> > > [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> > > spark-parent ---
> > > [INFO]
> > > [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) =
@
> > > spark-parent ---
> > > [INFO] Add Test Source directory:
> > > /Users/syao/git/grid/thirdparty/spark/src/test/scala
> > > [INFO]
> > > [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> > > spark-parent ---
> > > [INFO] No sources to compile
> > > [INFO]
> > > [INFO] --- build-helper-maven-plugin:1.8:add-test-source
> > > (add-scala-test-sources) @ spark-parent ---
> > > [INFO] Test Source directory:
> > > /Users/syao/git/grid/thirdparty/spark/src/test/scala added.
> > > [INFO]
> > > [INFO] --- scala-maven-plugin:3.1.6:testCompile
> > (scala-test-compile-first)
> > > @ spark-parent ---
> > > [INFO] No sources to compile
> > > [INFO]
> > > [INFO] --- maven-site-plugin:3.3:attach-descriptor (attach-descriptor=
)
> @
> > > spark-parent ---
> > > [INFO]
> > > [INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar) =
@
> > > spark-parent ---
> > > [INFO]
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO] Building Spark Project Core 1.0.1
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO]
> > > [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @
> spark-core_2.10
> > > ---
> > > [INFO]
> > > [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> > > spark-core_2.10 ---
> > > [INFO]
> > > [INFO] --- build-helper-maven-plugin:1.8:add-source
> (add-scala-sources) @
> > > spark-core_2.10 ---
> > > [INFO] Source directory:
> > > /Users/syao/git/grid/thirdparty/spark/core/src/main/scala added.
> > > [INFO]
> > > [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> > > spark-core_2.10 ---
> > > [INFO]
> > > [INFO] --- exec-maven-plugin:1.2.1:exec (default) @ spark-core_2.10 -=
--
> > > Archive:  lib/py4j-0.8.1-src.zip
> > >   inflating: build/py4j/tests/java_map_test.py
> > >  extracting: build/py4j/tests/__init__.py
> > >   inflating: build/py4j/tests/java_gateway_test.py
> > >   inflating: build/py4j/tests/java_callback_test.py
> > >   inflating: build/py4j/tests/java_list_test.py
> > >   inflating: build/py4j/tests/byte_string_test.py
> > >   inflating: build/py4j/tests/multithreadtest.py
> > >   inflating: build/py4j/tests/java_array_test.py
> > >   inflating: build/py4j/tests/py4j_callback_example2.py
> > >   inflating: build/py4j/tests/py4j_example.py
> > >   inflating: build/py4j/tests/py4j_callback_example.py
> > >   inflating: build/py4j/tests/finalizer_test.py
> > >   inflating: build/py4j/tests/java_set_test.py
> > >   inflating: build/py4j/finalizer.py
> > >  extracting: build/py4j/__init__.py
> > >   inflating: build/py4j/java_gateway.py
> > >   inflating: build/py4j/protocol.py
> > >   inflating: build/py4j/java_collections.py
> > >  extracting: build/py4j/version.py
> > >   inflating: build/py4j/compat.py
> > > [INFO]
> > > [INFO] --- maven-resources-plugin:2.6:resources (default-resources) @
> > > spark-core_2.10 ---
> > > [INFO] Using 'UTF-8' encoding to copy filtered resources.
> > > [INFO] Copying 6 resources
> > > [INFO] Copying 20 resources
> > > [INFO] Copying 7 resources
> > > [INFO] Copying 3 resources
> > > [INFO]
> > > [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first) =
@
> > > spark-core_2.10 ---
> > > [INFO] Add Test Source directory:
> > > /Users/syao/git/grid/thirdparty/spark/core/src/test/scala
> > > [INFO]
> > > [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> > > spark-core_2.10 ---
> > > [INFO] Using zinc server for incremental compilation
> > > [info] Compiling 342 Scala sources and 34 Java sources to
> > > /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes.=
..
> > > [warn] Class javax.servlet.ServletException not found - continuing
> with a
> > > stub.
> > > [error]
> > > [error]      while compiling:
> > >
> >
> /Users/syao/git/grid/thirdparty/spark/core/src/main/scala/org/apache/spar=
k/HttpServer.scala
> > > [error]         during phase: typer
> > > [error]      library version: version 2.10.4
> > > [error]     compiler version: version 2.10.4
> > > [error]   reconstructed args: -classpath
> > >
> >
> /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes:/Use=
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-=
2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/=
hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/=
1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commo=
ns-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xm=
lenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/com=
mons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/c=
ommons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:=
/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.j=
ar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1=
.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digeste=
r/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/=
commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/User=
s/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/common=
s-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/=
jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repositor=
y/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.ja=
r:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Use=
rs/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-jav=
a-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/=
hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons=
-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/t=
ukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoo=
p-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/j=
etty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org=
/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-ap=
p-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-c=
lient-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2=
/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2=
.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey=
-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-se=
rver-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapred=
uce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/o=
rg/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client=
-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-co=
mmon/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xm=
l/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/x=
ml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/java=
x/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/c=
om/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repositor=
y/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduc=
e-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/reposi=
tory/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/=
repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:=
/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcor=
e-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2=
.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curato=
r/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Use=
rs/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v201310=
31/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/j=
etty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v2=
01105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8=
.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repositor=
y/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfis=
h/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/s=
yao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v20110507=
1233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/or=
g/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v2013=
1031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20=
131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclip=
se/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/U=
sers/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Us=
ers/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang=
3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.=
9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf=
4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/ju=
l-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.=
7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/=
log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/s=
lf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0=
.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snap=
py-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/=
chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esoter=
icsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esot=
ericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/s=
yao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/U=
sers/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Use=
rs/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/U=
sers/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/U=
sers/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shad=
ed-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repo=
sitory/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-ac=
tor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/=
config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6=
.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project=
/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/=
syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-ma=
ths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j=
_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/User=
s/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2=
.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/=
json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast=
_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/though=
tworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository=
/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/=
org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/=
.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-da=
tabind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jack=
son-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/reposit=
ory/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/U=
sers/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/rep=
ository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/re=
pository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Us=
ers/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Fi=
nal.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/s=
tream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-cor=
e/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metr=
ics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/=
codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/=
repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0=
.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/t=
achyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0=
/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0=
/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.=
4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflec=
t/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-proj=
ect/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4=
j/py4j/0.8.1/py4j-0.8.1.jar
> > > -deprecation -feature -bootclasspath
> > >
> >
> /Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/r=
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Hom=
e/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents=
/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.=
jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7=
.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/j=
dk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtual=
Machines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVi=
rtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java=
/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/=
.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
> > > -unchecked -language:postfixOps
> > > [error]
> > > [error]   last tree to typer: Ident(Server)
> > > [error]               symbol: <none> (flags: )
> > > [error]    symbol definition: <none>
> > > [error]        symbol owners:
> > > [error]       context owners: variable server -> class HttpServer ->
> > > package spark
> > > [error]
> > > [error] =3D=3D Enclosing template or block =3D=3D
> > > [error]
> > > [error] Template( // val <local HttpServer>: <notype> in class
> HttpServer
> > > [error]   "org.apache.spark.Logging" // parents
> > > [error]   ValDef(
> > > [error]     private
> > > [error]     "_"
> > > [error]     <tpt>
> > > [error]     <empty>
> > > [error]   )
> > > [error]   // 9 statements
> > > [error]   ValDef( // private[this] val resourceBase: <?> in class
> > HttpServer
> > > [error]     private <local> <paramaccessor>
> > > [error]     "resourceBase"
> > > [error]     "File"
> > > [error]     <empty>
> > > [error]   )
> > > [error]   ValDef( // private[this] val securityManager: <?> in class
> > > HttpServer
> > > [error]     private <local> <paramaccessor>
> > > [error]     "securityManager"
> > > [error]     "SecurityManager"
> > > [error]     <empty>
> > > [error]   )
> > > [error]   DefDef( // def <init>(resourceBase:
> > java.io.File,securityManager:
> > > org.apache.spark.SecurityManager): org.apache.spark.HttpServer in cla=
ss
> > > HttpServer
> > > [error]     <method> <triedcooking>
> > > [error]     "<init>"
> > > [error]     []
> > > [error]     // 1 parameter list
> > > [error]     ValDef( // resourceBase: java.io.File
> > > [error]       <param> <paramaccessor>
> > > [error]       "resourceBase"
> > > [error]       "File"
> > > [error]       <empty>
> > > [error]     )
> > > [error]     ValDef( // securityManager:
> org.apache.spark.SecurityManager
> > > [error]       <param> <paramaccessor>
> > > [error]       "securityManager"
> > > [error]       "SecurityManager" // private[package spark] class
> > > SecurityManager extends Logging in package spark,
> > > tree.tpe=3Dorg.apache.spark.SecurityManager
> > > [error]       <empty>
> > > [error]     )
> > > [error]     <tpt> // tree.tpe=3Dorg.apache.spark.HttpServer
> > > [error]     Block(
> > > [error]       Apply(
> > > [error]         super."<init>"
> > > [error]         Nil
> > > [error]       )
> > > [error]       ()
> > > [error]     )
> > > [error]   )
> > > [error]   ValDef( // private[this] var server: <?> in class HttpServe=
r
> > > [error]     private <mutable> <local>
> > > [error]     "server"
> > > [error]     "Server"
> > > [error]     null
> > > [error]   )
> > > [error]   ValDef( // private[this] var port: <?> in class HttpServer
> > > [error]     private <mutable> <local>
> > > [error]     "port"
> > > [error]     "Int"
> > > [error]     -1
> > > [error]   )
> > > [error]   DefDef( // def start(): Unit in class HttpServer
> > > [error]     <method> <triedcooking>
> > > [error]     "start"
> > > [error]     []
> > > [error]     List(Nil)
> > > [error]     "scala"."Unit" // final abstract class Unit extends AnyVa=
l
> in
> > > package scala, tree.tpe=3DUnit
> > > [error]     If(
> > > [error]       Apply(
> > > [error]         "server"."$bang$eq"
> > > [error]         null
> > > [error]       )
> > > [error]       Throw(
> > > [error]         Apply(
> > > [error]           new ServerStateException."<init>"
> > > [error]           "Server is already started"
> > > [error]         )
> > > [error]       )
> > > [error]       Block(
> > > [error]         // 16 statements
> > > [error]         Apply(
> > > [error]           "logInfo"
> > > [error]           "Starting HTTP Server"
> > > [error]         )
> > > [error]         Assign(
> > > [error]           "server"
> > > [error]           Apply(
> > > [error]             new Server."<init>"
> > > [error]             Nil
> > > [error]           )
> > > [error]         )
> > > [error]         ValDef(
> > > [error]           0
> > > [error]           "connector"
> > > [error]           <tpt>
> > > [error]           Apply(
> > > [error]             new SocketConnector."<init>"
> > > [error]             Nil
> > > [error]           )
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "connector"."setMaxIdleTime"
> > > [error]           Apply(
> > > [error]             60."$times"
> > > [error]             1000
> > > [error]           )
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "connector"."setSoLingerTime"
> > > [error]           -1
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "connector"."setPort"
> > > [error]           0
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "server"."addConnector"
> > > [error]           "connector"
> > > [error]         )
> > > [error]         ValDef(
> > > [error]           0
> > > [error]           "threadPool"
> > > [error]           <tpt>
> > > [error]           Apply(
> > > [error]             new QueuedThreadPool."<init>"
> > > [error]             Nil
> > > [error]           )
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "threadPool"."setDaemon"
> > > [error]           true
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "server"."setThreadPool"
> > > [error]           "threadPool"
> > > [error]         )
> > > [error]         ValDef(
> > > [error]           0
> > > [error]           "resHandler"
> > > [error]           <tpt>
> > > [error]           Apply(
> > > [error]             new ResourceHandler."<init>"
> > > [error]             Nil
> > > [error]           )
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "resHandler"."setResourceBase"
> > > [error]           "resourceBase"."getAbsolutePath"
> > > [error]         )
> > > [error]         ValDef(
> > > [error]           0
> > > [error]           "handlerList"
> > > [error]           <tpt>
> > > [error]           Apply(
> > > [error]             new HandlerList."<init>"
> > > [error]             Nil
> > > [error]           )
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "handlerList"."setHandlers"
> > > [error]           Apply(
> > > [error]             "Array"
> > > [error]             // 2 arguments
> > > [error]             "resHandler"
> > > [error]             Apply(
> > > [error]               new DefaultHandler."<init>"
> > > [error]               Nil
> > > [error]             )
> > > [error]           )
> > > [error]         )
> > > [error]         If(
> > > [error]           Apply(
> > > [error]             "securityManager"."isAuthenticationEnabled"
> > > [error]             Nil
> > > [error]           )
> > > [error]           Block(
> > > [error]             // 3 statements
> > > [error]             Apply(
> > > [error]               "logDebug"
> > > [error]               "HttpServer is using security"
> > > [error]             )
> > > [error]             ValDef(
> > > [error]               0
> > > [error]               "sh"
> > > [error]               <tpt>
> > > [error]               Apply(
> > > [error]                 "setupSecurityHandler"
> > > [error]                 "securityManager"
> > > [error]               )
> > > [error]             )
> > > [error]             Apply(
> > > [error]               "sh"."setHandler"
> > > [error]               "handlerList"
> > > [error]             )
> > > [error]             Apply(
> > > [error]               "server"."setHandler"
> > > [error]               "sh"
> > > [error]             )
> > > [error]           )
> > > [error]           Block(
> > > [error]             Apply(
> > > [error]               "logDebug"
> > > [error]               "HttpServer is not using security"
> > > [error]             )
> > > [error]             Apply(
> > > [error]               "server"."setHandler"
> > > [error]               "handlerList"
> > > [error]             )
> > > [error]           )
> > > [error]         )
> > > [error]         Apply(
> > > [error]           "server"."start"
> > > [error]           Nil
> > > [error]         )
> > > [error]         Assign(
> > > [error]           "port"
> > > [error]           Apply(
> > > [error]             server.getConnectors()(0)."getLocalPort"
> > > [error]             Nil
> > > [error]           )
> > > [error]         )
> > > [error]       )
> > > [error]     )
> > > [error]   )
> > > [error]   DefDef( // private def setupSecurityHandler: <?> in class
> > > HttpServer
> > > [error]     <method> private
> > > [error]     "setupSecurityHandler"
> > > [error]     []
> > > [error]     // 1 parameter list
> > > [error]     ValDef(
> > > [error]       <param>
> > > [error]       "securityMgr"
> > > [error]       "SecurityManager"
> > > [error]       <empty>
> > > [error]     )
> > > [error]     "ConstraintSecurityHandler"
> > > [error]     Block(
> > > [error]       // 16 statements
> > > [error]       ValDef(
> > > [error]         0
> > > [error]         "constraint"
> > > [error]         <tpt>
> > > [error]         Apply(
> > > [error]           new Constraint."<init>"
> > > [error]           Nil
> > > [error]         )
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "constraint"."setName"
> > > [error]         "Constraint"."__DIGEST_AUTH"
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "constraint"."setRoles"
> > > [error]         Apply(
> > > [error]           "Array"
> > > [error]           "user"
> > > [error]         )
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "constraint"."setAuthenticate"
> > > [error]         true
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "constraint"."setDataConstraint"
> > > [error]         "Constraint"."DC_NONE"
> > > [error]       )
> > > [error]       ValDef(
> > > [error]         0
> > > [error]         "cm"
> > > [error]         <tpt>
> > > [error]         Apply(
> > > [error]           new ConstraintMapping."<init>"
> > > [error]           Nil
> > > [error]         )
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "cm"."setConstraint"
> > > [error]         "constraint"
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "cm"."setPathSpec"
> > > [error]         "/*"
> > > [error]       )
> > > [error]       ValDef(
> > > [error]         0
> > > [error]         "sh"
> > > [error]         <tpt>
> > > [error]         Apply(
> > > [error]           new ConstraintSecurityHandler."<init>"
> > > [error]           Nil
> > > [error]         )
> > > [error]       )
> > > [error]       ValDef(
> > > [error]         0
> > > [error]         "hashLogin"
> > > [error]         <tpt>
> > > [error]         Apply(
> > > [error]           new HashLoginService."<init>"
> > > [error]           Nil
> > > [error]         )
> > > [error]       )
> > > [error]       ValDef(
> > > [error]         0
> > > [error]         "userCred"
> > > [error]         <tpt>
> > > [error]         Apply(
> > > [error]           new Password."<init>"
> > > [error]           Apply(
> > > [error]             "securityMgr"."getSecretKey"
> > > [error]             Nil
> > > [error]           )
> > > [error]         )
> > > [error]       )
> > > [error]       If(
> > > [error]         Apply(
> > > [error]           "userCred"."$eq$eq"
> > > [error]           null
> > > [error]         )
> > > [error]         Throw(
> > > [error]           Apply(
> > > [error]             new Exception."<init>"
> > > [error]             "Error: secret key is null with authentication on=
"
> > > [error]           )
> > > [error]         )
> > > [error]         ()
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "hashLogin"."putUser"
> > > [error]         // 3 arguments
> > > [error]         Apply(
> > > [error]           "securityMgr"."getHttpUser"
> > > [error]           Nil
> > > [error]         )
> > > [error]         "userCred"
> > > [error]         Apply(
> > > [error]           "Array"
> > > [error]           "user"
> > > [error]         )
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "sh"."setLoginService"
> > > [error]         "hashLogin"
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "sh"."setAuthenticator"
> > > [error]         Apply(
> > > [error]           new DigestAuthenticator."<init>"
> > > [error]           Nil
> > > [error]         )
> > > [error]       )
> > > [error]       Apply(
> > > [error]         "sh"."setConstraintMappings"
> > > [error]         Apply(
> > > [error]           "Array"
> > > [error]           "cm"
> > > [error]         )
> > > [error]       )
> > > [error]       "sh"
> > > [error]     )
> > > [error]   )
> > > [error]   DefDef( // def stop(): Unit in class HttpServer
> > > [error]     <method> <triedcooking>
> > > [error]     "stop"
> > > [error]     []
> > > [error]     List(Nil)
> > > [error]     "scala"."Unit" // final abstract class Unit extends AnyVa=
l
> in
> > > package scala, tree.tpe=3DUnit
> > > [error]     If(
> > > [error]       Apply(
> > > [error]         "server"."$eq$eq"
> > > [error]         null
> > > [error]       )
> > > [error]       Throw(
> > > [error]         Apply(
> > > [error]           new ServerStateException."<init>"
> > > [error]           "Server is already stopped"
> > > [error]         )
> > > [error]       )
> > > [error]       Block(
> > > [error]         // 2 statements
> > > [error]         Apply(
> > > [error]           "server"."stop"
> > > [error]           Nil
> > > [error]         )
> > > [error]         Assign(
> > > [error]           "port"
> > > [error]           -1
> > > [error]         )
> > > [error]         Assign(
> > > [error]           "server"
> > > [error]           null
> > > [error]         )
> > > [error]       )
> > > [error]     )
> > > [error]   )
> > > [error]   DefDef( // def uri: String in class HttpServer
> > > [error]     <method> <triedcooking>
> > > [error]     "uri"
> > > [error]     []
> > > [error]     Nil
> > > [error]     "String"
> > > [error]     If(
> > > [error]       Apply(
> > > [error]         "server"."$eq$eq"
> > > [error]         null
> > > [error]       )
> > > [error]       Throw(
> > > [error]         Apply(
> > > [error]           new ServerStateException."<init>"
> > > [error]           "Server is not started"
> > > [error]         )
> > > [error]       )
> > > [error]       Return(
> > > [error]         Apply(
> > > [error]           "http://
> > ".$plus(Utils.localIpAddress).$plus(":")."$plus"
> > > [error]           "port"
> > > [error]         )
> > > [error]       )
> > > [error]     )
> > > [error]   )
> > > [error] )
> > > [error]
> > > [error] uncaught exception during compilation: java.lang.AssertionErr=
or
> > > java.lang.AssertionError: assertion failed:
> > javax.servlet.ServletException
> > >     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1212)
> > >     at
> > scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseExceptions$1(Classf=
ileParser.scala:1051)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseAttributes(Classfil=
eParser.scala:1080)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseMethod(ClassfilePar=
ser.scala:666)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser$$anonfun$parseClass$1.ap=
ply$mcV$sp(ClassfileParser.scala:567)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfilePars=
er.scala:572)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.sc=
ala:88)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoa=
ders.scala:261)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.=
scala:194)
> > >     at
> > >
> >
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scal=
a:210)
> > >     at scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:893=
)
> > >     at
> > >
> scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:5064)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.sc=
ala:5218)
> > >     at
> scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5561)
> > >     at
> scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
> > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5769)
> > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5772)
> > >     at
> > scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1317)
> > >     at
> > scala.tools.nsc.typechecker.Namers$Namer.getSig$1(Namers.scala:1457)
> > >     at
> > scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1466)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer.scala$tools$nsc$typechecker$Name=
rs$Namer$$logAndValidate(Namers.scala:1499)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:730)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:729)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$$anon$1.completeImpl(Namers.scala:1614=
)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Namers$LockingTypeCompleter$class.complete(Na=
mers.scala:1622)
> > >     at
> > > scala.tools.nsc.typechecker.Namers$$anon$1.complete(Namers.scala:1612=
)
> > >     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1231)
> > >     at
> > scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.MethodSynthesis$MethodSynth$class.addDerivedT=
rees(MethodSynthesis.scala:225)
> > >     at
> > >
> scala.tools.nsc.typechecker.Namers$Namer.addDerivedTrees(Namers.scala:55)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1856)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1853)
> > >     at
> > >
> >
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
> > >     at
> > >
> >
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
> > >     at scala.collection.immutable.List.foreach(List.scala:318)
> > >     at
> > >
> scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:251)
> > >     at
> > scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
> > >     at
> > >
> scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:1917)
> > >     at
> > >
> scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:1759)
> > >     at
> scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5583)
> > >     at
> scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Type=
rs$Typer$$typedStat$1(Typers.scala:2928)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
> > >     at scala.collection.immutable.List.loop$1(List.scala:170)
> > >     at scala.collection.immutable.List.mapConserve(List.scala:186)
> > >     at
> > > scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032=
)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5=
301)
> > >     at
> scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5587)
> > >     at
> scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
> > >     at
> scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5704)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.=
scala:99)
> > >     at scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464=
)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
> > >     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
> > >     at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
> > >     at
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.sc=
ala:91)
> > >     at
> scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> > >     at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> > >     at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> > >     at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> > >     at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
> > >     at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
> > >     at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> > >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> > >     at
> > >
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> > >     at
> > >
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> > >     at java.lang.reflect.Method.invoke(Method.java:606)
> > >     at sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:10=
2)
> > >     at
> sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> > >     at
> sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> > >     at
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
$mcV$sp(AggressiveCompile.scala:99)
> > >     at
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
> > >     at
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
> > >     at
> > >
> >
> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(Aggr=
essiveCompile.scala:166)
> > >     at
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompil=
e.scala:98)
> > >     at
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:1=
43)
> > >     at
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:8=
7)
> > >     at
> > > sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:3=
9)
> > >     at
> > > sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:3=
7)
> > >     at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
> > >     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
> > >     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
> > >     at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
> > >     at sbt.inc.Incremental$.compile(Incremental.scala:37)
> > >     at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
> > >     at
> > sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
> > >     at
> > sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
> > >     at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
> > >     at com.typesafe.zinc.Main$.run(Main.scala:98)
> > >     at com.typesafe.zinc.Nailgun$.zinc(Nailgun.scala:93)
> > >     at com.typesafe.zinc.Nailgun$.nailMain(Nailgun.scala:82)
> > >     at com.typesafe.zinc.Nailgun.nailMain(Nailgun.scala)
> > >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> > >     at
> > >
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> > >     at
> > >
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> > >     at java.lang.reflect.Method.invoke(Method.java:606)
> > >     at com.martiansoftware.nailgun.NGSession.run(NGSession.java:280)
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO] Reactor Summary:
> > > [INFO]
> > > [INFO] Spark Project Parent POM ........................... SUCCESS [
> > > 1.760 s]
> > > [INFO] Spark Project Core ................................. FAILURE [
> > > 5.312 s]
> > > [INFO] Spark Project Bagel ................................ SKIPPED
> > > [INFO] Spark Project GraphX ............................... SKIPPED
> > > [INFO] Spark Project ML Library ........................... SKIPPED
> > > [INFO] Spark Project Streaming ............................ SKIPPED
> > > [INFO] Spark Project Tools ................................ SKIPPED
> > > [INFO] Spark Project Catalyst ............................. SKIPPED
> > > [INFO] Spark Project SQL .................................. SKIPPED
> > > [INFO] Spark Project Hive ................................. SKIPPED
> > > [INFO] Spark Project REPL ................................. SKIPPED
> > > [INFO] Spark Project YARN Parent POM ...................... SKIPPED
> > > [INFO] Spark Project YARN Stable API ...................... SKIPPED
> > > [INFO] Spark Project Assembly ............................. SKIPPED
> > > [INFO] Spark Project External Twitter ..................... SKIPPED
> > > [INFO] Spark Project External Kafka ....................... SKIPPED
> > > [INFO] Spark Project External Flume ....................... SKIPPED
> > > [INFO] Spark Project External ZeroMQ ...................... SKIPPED
> > > [INFO] Spark Project External MQTT ........................ SKIPPED
> > > [INFO] Spark Project Examples ............................. SKIPPED
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO] BUILD FAILURE
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [INFO] Total time: 7.562 s
> > > [INFO] Finished at: 2014-07-30T21:18:41-07:00
> > > [INFO] Final Memory: 39M/713M
> > > [INFO]
> > >
> ------------------------------------------------------------------------
> > > [ERROR] Failed to execute goal
> > > net.alchim31.maven:scala-maven-plugin:3.1.6:compile
> (scala-compile-first)
> > > on project spark-core_2.10: Execution scala-compile-first of goal
> > > net.alchim31.maven:scala-maven-plugin:3.1.6:compile failed.
> CompileFailed
> > > -> [Help 1]
> > >
> > > Thanks
> > > Shengzhe
> >
>

--20cf30050cd442e95804ff83522f--

From dev-return-8656-apmail-spark-dev-archive=spark.apache.org@spark.apache.org  Thu Jul 31 20:58:50 2014
Return-Path: <dev-return-8656-apmail-spark-dev-archive=spark.apache.org@spark.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9F8C111C02
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 31 Jul 2014 20:58:50 +0000 (UTC)
Received: (qmail 81340 invoked by uid 500); 31 Jul 2014 20:58:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81300 invoked by uid 500); 31 Jul 2014 20:58:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Reply-To: dev@spark.apache.org
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 81282 invoked by uid 99); 31 Jul 2014 20:58:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 20:58:49 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of yaoshengzhe@gmail.com designates 209.85.213.180 as permitted sender)
Received: from [209.85.213.180] (HELO mail-ig0-f180.google.com) (209.85.213.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 31 Jul 2014 20:58:43 +0000
Received: by mail-ig0-f180.google.com with SMTP id l13so331923iga.7
        for <dev@spark.apache.org>; Thu, 31 Jul 2014 13:58:22 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=RZx8jGAsfLy5fYDuxmm34Rje7cuKPxcZRxljAy2VciI=;
        b=Pv2X+Sd6tAHQBMKjoYArtNbrr3rhTcR5LVMyAqRgmv1wib5EjFS2gFtauo6wbF67f1
         Xy0VP7jhOU36NGwsVF4/DMGE7TCN4yLhpWDOiW4beMuOsv/xRyah9embrpHaVnuiAhPA
         DtcXCjoz5WlNfQHHrVKjeQT62HvuDnXS4khsg0Rjj8SJCUVcMTq4/J4jvIEC7bGdkIiv
         Zyw1njJDWvFhL2mJGZ4OOxWEyAuHQVJpTePWgefnS8cc6SohG8+WbGqx/nxyv5mIpCXo
         6syDwbLLeCab3PkmGFM1dswi+oPjahO/KzzSEFhRv3ENfr5ep+l95ggpXqCanBLghMiF
         P/rQ==
MIME-Version: 1.0
X-Received: by 10.42.35.8 with SMTP id o8mr1009215icd.41.1406840302755; Thu,
 31 Jul 2014 13:58:22 -0700 (PDT)
Received: by 10.107.35.213 with HTTP; Thu, 31 Jul 2014 13:58:22 -0700 (PDT)
In-Reply-To: <CALte62yKdF1KXuk3edyHOBzeYU3nKxJAwvpYNwD5GYW2TRL_hA@mail.gmail.com>
References: <CA+FETEKK7_7w_Z3ii=Yz-X+fwsTA2vzvYJxcEfCm6rFX=MNw1g@mail.gmail.com>
	<CAMwrk0mO+w2ysWoODmRb3Mt4gBsP50YtD2inOL27xwzKEU+PUA@mail.gmail.com>
	<CA+FETEJUEwndRG_0-zaM5pLoMzUDrbOjFUNf2GnRQTYwgV87YQ@mail.gmail.com>
	<CALte62yKdF1KXuk3edyHOBzeYU3nKxJAwvpYNwD5GYW2TRL_hA@mail.gmail.com>
Date: Thu, 31 Jul 2014 13:58:22 -0700
Message-ID: <CA+FETE+Z+bfdcnC-E2QPuGAxZm9JUKGg1GzgAmW8H=UWyr+2dQ@mail.gmail.com>
Subject: Re: failed to build spark with maven for both 1.0.1 and latest master branch
From: yao <yaoshengzhe@gmail.com>
To: dev@spark.apache.org
Content-Type: multipart/alternative; boundary=90e6ba1811d680656904ff838855
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba1811d680656904ff838855
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Great, thanks Ted. I just did a maven build on CentOS 6, everything looks
good. So this is more like a Mac specific issue.


On Thu, Jul 31, 2014 at 1:43 PM, Ted Yu <yuzhihong@gmail.com> wrote:

> The following command succeeded (on Linux) on Spark master checked out th=
is
> morning:
>
> mvn -Pyarn -Phive -Phadoop-2.4 -DskipTests install
>
> FYI
>
>
> On Thu, Jul 31, 2014 at 1:36 PM, yao <yaoshengzhe@gmail.com> wrote:
>
> > Hi TD,
> >
> > I've asked my colleagues to do the same thing but compile still fails.
> > However, maven build succeeded once I built it on my personal macbook
> (with
> > the latest MacOS Yosemite). So I guess there might be something wrong i=
n
> my
> > build environment. Wonder if anyone tried to compile spark using maven
> > under Mavericks, please let me know your result.
> >
> > Thanks
> > Shengzhe
> >
> >
> > On Thu, Jul 31, 2014 at 1:25 AM, Tathagata Das <
> > tathagata.das1565@gmail.com>
> > wrote:
> >
> > > Does a "mvn clean" or "sbt/sbt clean" help?
> > >
> > > TD
> > >
> > > On Wed, Jul 30, 2014 at 9:25 PM, yao <yaoshengzhe@gmail.com> wrote:
> > > > Hi Folks,
> > > >
> > > > Today I am trying to build spark using maven; however, the followin=
g
> > > > command failed consistently for both 1.0.1 and the latest master.
> >  (BTW,
> > > it
> > > > seems sbt works fine: *sbt/sbt -Dhadoop.version=3D2.4.0 -Pyarn clea=
n
> > > > assembly)*
> > > >
> > > > Environment: Mac OS Mavericks
> > > > Maven: 3.2.2 (installed by homebrew)
> > > >
> > > >
> > > >
> > > >
> > > > *export M2_HOME=3D/usr/local/Cellar/maven/3.2.2/libexec/export
> > > > PATH=3D$M2_HOME/bin:$PATHexport MAVEN_OPTS=3D"-Xmx2g -XX:MaxPermSiz=
e=3D512M
> > > > -XX:ReservedCodeCacheSize=3D512m"mvn -Pyarn -Phadoop-2.4
> > > > -Dhadoop.version=3D2.4.0 -DskipTests clean package*
> > > >
> > > > Build outputs:
> > > >
> > > > [INFO] Scanning for projects...
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO] Reactor Build Order:
> > > > [INFO]
> > > > [INFO] Spark Project Parent POM
> > > > [INFO] Spark Project Core
> > > > [INFO] Spark Project Bagel
> > > > [INFO] Spark Project GraphX
> > > > [INFO] Spark Project ML Library
> > > > [INFO] Spark Project Streaming
> > > > [INFO] Spark Project Tools
> > > > [INFO] Spark Project Catalyst
> > > > [INFO] Spark Project SQL
> > > > [INFO] Spark Project Hive
> > > > [INFO] Spark Project REPL
> > > > [INFO] Spark Project YARN Parent POM
> > > > [INFO] Spark Project YARN Stable API
> > > > [INFO] Spark Project Assembly
> > > > [INFO] Spark Project External Twitter
> > > > [INFO] Spark Project External Kafka
> > > > [INFO] Spark Project External Flume
> > > > [INFO] Spark Project External ZeroMQ
> > > > [INFO] Spark Project External MQTT
> > > > [INFO] Spark Project Examples
> > > > [INFO]
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO] Building Spark Project Parent POM 1.0.1
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO]
> > > > [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @
> spark-parent
> > > ---
> > > > [INFO]
> > > > [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> > > > spark-parent ---
> > > > [INFO]
> > > > [INFO] --- build-helper-maven-plugin:1.8:add-source
> > (add-scala-sources) @
> > > > spark-parent ---
> > > > [INFO] Source directory:
> > > > /Users/syao/git/grid/thirdparty/spark/src/main/scala added.
> > > > [INFO]
> > > > [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> > > > spark-parent ---
> > > > [INFO]
> > > > [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first=
)
> @
> > > > spark-parent ---
> > > > [INFO] Add Test Source directory:
> > > > /Users/syao/git/grid/thirdparty/spark/src/test/scala
> > > > [INFO]
> > > > [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> > > > spark-parent ---
> > > > [INFO] No sources to compile
> > > > [INFO]
> > > > [INFO] --- build-helper-maven-plugin:1.8:add-test-source
> > > > (add-scala-test-sources) @ spark-parent ---
> > > > [INFO] Test Source directory:
> > > > /Users/syao/git/grid/thirdparty/spark/src/test/scala added.
> > > > [INFO]
> > > > [INFO] --- scala-maven-plugin:3.1.6:testCompile
> > > (scala-test-compile-first)
> > > > @ spark-parent ---
> > > > [INFO] No sources to compile
> > > > [INFO]
> > > > [INFO] --- maven-site-plugin:3.3:attach-descriptor
> (attach-descriptor)
> > @
> > > > spark-parent ---
> > > > [INFO]
> > > > [INFO] --- maven-source-plugin:2.2.1:jar-no-fork (create-source-jar=
)
> @
> > > > spark-parent ---
> > > > [INFO]
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO] Building Spark Project Core 1.0.1
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO]
> > > > [INFO] --- maven-clean-plugin:2.5:clean (default-clean) @
> > spark-core_2.10
> > > > ---
> > > > [INFO]
> > > > [INFO] --- maven-enforcer-plugin:1.3.1:enforce (enforce-versions) @
> > > > spark-core_2.10 ---
> > > > [INFO]
> > > > [INFO] --- build-helper-maven-plugin:1.8:add-source
> > (add-scala-sources) @
> > > > spark-core_2.10 ---
> > > > [INFO] Source directory:
> > > > /Users/syao/git/grid/thirdparty/spark/core/src/main/scala added.
> > > > [INFO]
> > > > [INFO] --- maven-remote-resources-plugin:1.5:process (default) @
> > > > spark-core_2.10 ---
> > > > [INFO]
> > > > [INFO] --- exec-maven-plugin:1.2.1:exec (default) @ spark-core_2.10
> ---
> > > > Archive:  lib/py4j-0.8.1-src.zip
> > > >   inflating: build/py4j/tests/java_map_test.py
> > > >  extracting: build/py4j/tests/__init__.py
> > > >   inflating: build/py4j/tests/java_gateway_test.py
> > > >   inflating: build/py4j/tests/java_callback_test.py
> > > >   inflating: build/py4j/tests/java_list_test.py
> > > >   inflating: build/py4j/tests/byte_string_test.py
> > > >   inflating: build/py4j/tests/multithreadtest.py
> > > >   inflating: build/py4j/tests/java_array_test.py
> > > >   inflating: build/py4j/tests/py4j_callback_example2.py
> > > >   inflating: build/py4j/tests/py4j_example.py
> > > >   inflating: build/py4j/tests/py4j_callback_example.py
> > > >   inflating: build/py4j/tests/finalizer_test.py
> > > >   inflating: build/py4j/tests/java_set_test.py
> > > >   inflating: build/py4j/finalizer.py
> > > >  extracting: build/py4j/__init__.py
> > > >   inflating: build/py4j/java_gateway.py
> > > >   inflating: build/py4j/protocol.py
> > > >   inflating: build/py4j/java_collections.py
> > > >  extracting: build/py4j/version.py
> > > >   inflating: build/py4j/compat.py
> > > > [INFO]
> > > > [INFO] --- maven-resources-plugin:2.6:resources (default-resources)=
 @
> > > > spark-core_2.10 ---
> > > > [INFO] Using 'UTF-8' encoding to copy filtered resources.
> > > > [INFO] Copying 6 resources
> > > > [INFO] Copying 20 resources
> > > > [INFO] Copying 7 resources
> > > > [INFO] Copying 3 resources
> > > > [INFO]
> > > > [INFO] --- scala-maven-plugin:3.1.6:add-source (scala-compile-first=
)
> @
> > > > spark-core_2.10 ---
> > > > [INFO] Add Test Source directory:
> > > > /Users/syao/git/grid/thirdparty/spark/core/src/test/scala
> > > > [INFO]
> > > > [INFO] --- scala-maven-plugin:3.1.6:compile (scala-compile-first) @
> > > > spark-core_2.10 ---
> > > > [INFO] Using zinc server for incremental compilation
> > > > [info] Compiling 342 Scala sources and 34 Java sources to
> > > >
> /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes...
> > > > [warn] Class javax.servlet.ServletException not found - continuing
> > with a
> > > > stub.
> > > > [error]
> > > > [error]      while compiling:
> > > >
> > >
> >
> /Users/syao/git/grid/thirdparty/spark/core/src/main/scala/org/apache/spar=
k/HttpServer.scala
> > > > [error]         during phase: typer
> > > > [error]      library version: version 2.10.4
> > > > [error]     compiler version: version 2.10.4
> > > > [error]   reconstructed args: -classpath
> > > >
> > >
> >
> /Users/syao/git/grid/thirdparty/spark/core/target/scala-2.10/classes:/Use=
rs/syao/.m2/repository/org/apache/hadoop/hadoop-client/2.4.0/hadoop-client-=
2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-common/2.4.0/=
hadoop-common-2.4.0.jar:/Users/syao/.m2/repository/commons-cli/commons-cli/=
1.2/commons-cli-1.2.jar:/Users/syao/.m2/repository/org/apache/commons/commo=
ns-math3/3.1.1/commons-math3-3.1.1.jar:/Users/syao/.m2/repository/xmlenc/xm=
lenc/0.52/xmlenc-0.52.jar:/Users/syao/.m2/repository/commons-httpclient/com=
mons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/syao/.m2/repository/c=
ommons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:=
/Users/syao/.m2/repository/commons-lang/commons-lang/2.6/commons-lang-2.6.j=
ar:/Users/syao/.m2/repository/commons-configuration/commons-configuration/1=
.6/commons-configuration-1.6.jar:/Users/syao/.m2/repository/commons-digeste=
r/commons-digester/1.8/commons-digester-1.8.jar:/Users/syao/.m2/repository/=
commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/User=
s/syao/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/common=
s-beanutils-core-1.8.0.jar:/Users/syao/.m2/repository/org/codehaus/jackson/=
jackson-core-asl/1.8.8/jackson-core-asl-1.8.8.jar:/Users/syao/.m2/repositor=
y/org/codehaus/jackson/jackson-mapper-asl/1.8.8/jackson-mapper-asl-1.8.8.ja=
r:/Users/syao/.m2/repository/org/apache/avro/avro/1.7.6/avro-1.7.6.jar:/Use=
rs/syao/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-jav=
a-2.5.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-auth/2.4.0/=
hadoop-auth-2.4.0.jar:/Users/syao/.m2/repository/org/apache/commons/commons=
-compress/1.4.1/commons-compress-1.4.1.jar:/Users/syao/.m2/repository/org/t=
ukaani/xz/1.0/xz-1.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoo=
p-hdfs/2.4.0/hadoop-hdfs-2.4.0.jar:/Users/syao/.m2/repository/org/mortbay/j=
etty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/syao/.m2/repository/org=
/apache/hadoop/hadoop-mapreduce-client-app/2.4.0/hadoop-mapreduce-client-ap=
p-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-mapreduce-c=
lient-common/2.4.0/hadoop-mapreduce-client-common-2.4.0.jar:/Users/syao/.m2=
/repository/org/apache/hadoop/hadoop-yarn-client/2.4.0/hadoop-yarn-client-2=
.4.0.jar:/Users/syao/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey=
-client-1.9.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-se=
rver-common/2.4.0/hadoop-yarn-server-common-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.4.0/hadoop-mapred=
uce-client-shuffle-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-yarn-api/2.4.0/hadoop-yarn-api-2.4.0.jar:/Users/syao/.m2/repository/o=
rg/apache/hadoop/hadoop-mapreduce-client-core/2.4.0/hadoop-mapreduce-client=
-core-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/hadoop-yarn-co=
mmon/2.4.0/hadoop-yarn-common-2.4.0.jar:/Users/syao/.m2/repository/javax/xm=
l/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/syao/.m2/repository/javax/x=
ml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/syao/.m2/repository/java=
x/activation/activation/1.1/activation-1.1.jar:/Users/syao/.m2/repository/c=
om/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/syao/.m2/repositor=
y/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.4.0/hadoop-mapreduc=
e-client-jobclient-2.4.0.jar:/Users/syao/.m2/repository/org/apache/hadoop/h=
adoop-annotations/2.4.0/hadoop-annotations-2.4.0.jar:/Users/syao/.m2/reposi=
tory/commons-codec/commons-codec/1.5/commons-codec-1.5.jar:/Users/syao/.m2/=
repository/org/apache/httpcomponents/httpclient/4.1.2/httpclient-4.1.2.jar:=
/Users/syao/.m2/repository/org/apache/httpcomponents/httpcore/4.1.2/httpcor=
e-4.1.2.jar:/Users/syao/.m2/repository/org/apache/curator/curator-recipes/2=
.4.0/curator-recipes-2.4.0.jar:/Users/syao/.m2/repository/org/apache/curato=
r/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/syao/.m2/repos=
itory/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Use=
rs/syao/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-plus/8.1.14.v201310=
31/jetty-plus-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclipse/j=
etty/orbit/javax.transaction/1.1.1.v201105210645/javax.transaction-1.1.1.v2=
01105210645.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-webapp/8=
.1.14.v20131031/jetty-webapp-8.1.14.v20131031.jar:/Users/syao/.m2/repositor=
y/org/eclipse/jetty/jetty-jndi/8.1.14.v20131031/jetty-jndi-8.1.14.v20131031=
.jar:/Users/syao/.m2/repository/org/eclipse/jetty/orbit/javax.mail.glassfis=
h/1.4.1.v201005082020/javax.mail.glassfish-1.4.1.v201005082020.jar:/Users/s=
yao/.m2/repository/org/eclipse/jetty/orbit/javax.activation/1.1.0.v20110507=
1233/javax.activation-1.1.0.v201105071233.jar:/Users/syao/.m2/repository/or=
g/eclipse/jetty/jetty-security/8.1.14.v20131031/jetty-security-8.1.14.v2013=
1031.jar:/Users/syao/.m2/repository/org/eclipse/jetty/jetty-util/8.1.14.v20=
131031/jetty-util-8.1.14.v20131031.jar:/Users/syao/.m2/repository/org/eclip=
se/jetty/jetty-server/8.1.14.v20131031/jetty-server-8.1.14.v20131031.jar:/U=
sers/syao/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Us=
ers/syao/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang=
3-3.3.2.jar:/Users/syao/.m2/repository/com/google/code/findbugs/jsr305/1.3.=
9/jsr305-1.3.9.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-api/1.7.5/slf=
4j-api-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jul-to-slf4j/1.7.5/ju=
l-to-slf4j-1.7.5.jar:/Users/syao/.m2/repository/org/slf4j/jcl-over-slf4j/1.=
7.5/jcl-over-slf4j-1.7.5.jar:/Users/syao/.m2/repository/log4j/log4j/1.2.17/=
log4j-1.2.17.jar:/Users/syao/.m2/repository/org/slf4j/slf4j-log4j12/1.7.5/s=
lf4j-log4j12-1.7.5.jar:/Users/syao/.m2/repository/com/ning/compress-lzf/1.0=
.0/compress-lzf-1.0.0.jar:/Users/syao/.m2/repository/org/xerial/snappy/snap=
py-java/1.0.5/snappy-java-1.0.5.jar:/Users/syao/.m2/repository/com/twitter/=
chill_2.10/0.3.6/chill_2.10-0.3.6.jar:/Users/syao/.m2/repository/com/esoter=
icsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/syao/.m2/repository/com/esot=
ericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/s=
yao/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/U=
sers/syao/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Use=
rs/syao/.m2/repository/com/twitter/chill-java/0.3.6/chill-java-0.3.6.jar:/U=
sers/syao/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/U=
sers/syao/.m2/repository/org/spark-project/akka/akka-remote_2.10/2.2.3-shad=
ed-protobuf/akka-remote_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repo=
sitory/org/spark-project/akka/akka-actor_2.10/2.2.3-shaded-protobuf/akka-ac=
tor_2.10-2.2.3-shaded-protobuf.jar:/Users/syao/.m2/repository/com/typesafe/=
config/1.0.2/config-1.0.2.jar:/Users/syao/.m2/repository/io/netty/netty/3.6=
.6.Final/netty-3.6.6.Final.jar:/Users/syao/.m2/repository/org/spark-project=
/protobuf/protobuf-java/2.4.1-shaded/protobuf-java-2.4.1-shaded.jar:/Users/=
syao/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-ma=
ths-1.2.2a.jar:/Users/syao/.m2/repository/org/spark-project/akka/akka-slf4j=
_2.10/2.2.3-shaded-protobuf/akka-slf4j_2.10-2.2.3-shaded-protobuf.jar:/User=
s/syao/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.6/json4s-jackson_2=
.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-core_2.10/3.2.6/=
json4s-core_2.10-3.2.6.jar:/Users/syao/.m2/repository/org/json4s/json4s-ast=
_2.10/3.2.6/json4s-ast_2.10-3.2.6.jar:/Users/syao/.m2/repository/com/though=
tworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/syao/.m2/repository=
/org/scala-lang/scalap/2.10.4/scalap-2.10.4.jar:/Users/syao/.m2/repository/=
org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/syao/=
.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.3.0/jackson-da=
tabind-2.3.0.jar:/Users/syao/.m2/repository/com/fasterxml/jackson/core/jack=
son-annotations/2.3.0/jackson-annotations-2.3.0.jar:/Users/syao/.m2/reposit=
ory/com/fasterxml/jackson/core/jackson-core/2.3.0/jackson-core-2.3.0.jar:/U=
sers/syao/.m2/repository/colt/colt/1.2.0/colt-1.2.0.jar:/Users/syao/.m2/rep=
ository/concurrent/concurrent/1.3.4/concurrent-1.3.4.jar:/Users/syao/.m2/re=
pository/org/apache/mesos/mesos/0.18.1/mesos-0.18.1-shaded-protobuf.jar:/Us=
ers/syao/.m2/repository/io/netty/netty-all/4.0.17.Final/netty-all-4.0.17.Fi=
nal.jar:/Users/syao/.m2/repository/com/clearspring/analytics/stream/2.5.1/s=
tream-2.5.1.jar:/Users/syao/.m2/repository/com/codahale/metrics/metrics-cor=
e/3.0.0/metrics-core-3.0.0.jar:/Users/syao/.m2/repository/com/codahale/metr=
ics/metrics-jvm/3.0.0/metrics-jvm-3.0.0.jar:/Users/syao/.m2/repository/com/=
codahale/metrics/metrics-json/3.0.0/metrics-json-3.0.0.jar:/Users/syao/.m2/=
repository/com/codahale/metrics/metrics-graphite/3.0.0/metrics-graphite-3.0=
.0.jar:/Users/syao/.m2/repository/org/tachyonproject/tachyon/0.4.1-thrift/t=
achyon-0.4.1-thrift.jar:/Users/syao/.m2/repository/org/apache/ant/ant/1.9.0=
/ant-1.9.0.jar:/Users/syao/.m2/repository/org/apache/ant/ant-launcher/1.9.0=
/ant-launcher-1.9.0.jar:/Users/syao/.m2/repository/commons-io/commons-io/2.=
4/commons-io-2.4.jar:/Users/syao/.m2/repository/org/scala-lang/scala-reflec=
t/2.10.4/scala-reflect-2.10.4.jar:/Users/syao/.m2/repository/org/spark-proj=
ect/pyrolite/2.0.1/pyrolite-2.0.1.jar:/Users/syao/.m2/repository/net/sf/py4=
j/py4j/0.8.1/py4j-0.8.1.jar
> > > > -deprecation -feature -bootclasspath
> > > >
> > >
> >
> /Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/r=
esources.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Hom=
e/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents=
/Home/jre/lib/sunrsasign.jar:/Library/Java/JavaVirtualMachines/jdk1.7.0_40.=
jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.7=
.0_40.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/j=
dk1.7.0_40.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtual=
Machines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVi=
rtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/lib/JObjC.jar:/Library/Java=
/JavaVirtualMachines/jdk1.7.0_40.jdk/Contents/Home/jre/classes:/Users/syao/=
.m2/repository/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
> > > > -unchecked -language:postfixOps
> > > > [error]
> > > > [error]   last tree to typer: Ident(Server)
> > > > [error]               symbol: <none> (flags: )
> > > > [error]    symbol definition: <none>
> > > > [error]        symbol owners:
> > > > [error]       context owners: variable server -> class HttpServer -=
>
> > > > package spark
> > > > [error]
> > > > [error] =3D=3D Enclosing template or block =3D=3D
> > > > [error]
> > > > [error] Template( // val <local HttpServer>: <notype> in class
> > HttpServer
> > > > [error]   "org.apache.spark.Logging" // parents
> > > > [error]   ValDef(
> > > > [error]     private
> > > > [error]     "_"
> > > > [error]     <tpt>
> > > > [error]     <empty>
> > > > [error]   )
> > > > [error]   // 9 statements
> > > > [error]   ValDef( // private[this] val resourceBase: <?> in class
> > > HttpServer
> > > > [error]     private <local> <paramaccessor>
> > > > [error]     "resourceBase"
> > > > [error]     "File"
> > > > [error]     <empty>
> > > > [error]   )
> > > > [error]   ValDef( // private[this] val securityManager: <?> in clas=
s
> > > > HttpServer
> > > > [error]     private <local> <paramaccessor>
> > > > [error]     "securityManager"
> > > > [error]     "SecurityManager"
> > > > [error]     <empty>
> > > > [error]   )
> > > > [error]   DefDef( // def <init>(resourceBase:
> > > java.io.File,securityManager:
> > > > org.apache.spark.SecurityManager): org.apache.spark.HttpServer in
> class
> > > > HttpServer
> > > > [error]     <method> <triedcooking>
> > > > [error]     "<init>"
> > > > [error]     []
> > > > [error]     // 1 parameter list
> > > > [error]     ValDef( // resourceBase: java.io.File
> > > > [error]       <param> <paramaccessor>
> > > > [error]       "resourceBase"
> > > > [error]       "File"
> > > > [error]       <empty>
> > > > [error]     )
> > > > [error]     ValDef( // securityManager:
> > org.apache.spark.SecurityManager
> > > > [error]       <param> <paramaccessor>
> > > > [error]       "securityManager"
> > > > [error]       "SecurityManager" // private[package spark] class
> > > > SecurityManager extends Logging in package spark,
> > > > tree.tpe=3Dorg.apache.spark.SecurityManager
> > > > [error]       <empty>
> > > > [error]     )
> > > > [error]     <tpt> // tree.tpe=3Dorg.apache.spark.HttpServer
> > > > [error]     Block(
> > > > [error]       Apply(
> > > > [error]         super."<init>"
> > > > [error]         Nil
> > > > [error]       )
> > > > [error]       ()
> > > > [error]     )
> > > > [error]   )
> > > > [error]   ValDef( // private[this] var server: <?> in class
> HttpServer
> > > > [error]     private <mutable> <local>
> > > > [error]     "server"
> > > > [error]     "Server"
> > > > [error]     null
> > > > [error]   )
> > > > [error]   ValDef( // private[this] var port: <?> in class HttpServe=
r
> > > > [error]     private <mutable> <local>
> > > > [error]     "port"
> > > > [error]     "Int"
> > > > [error]     -1
> > > > [error]   )
> > > > [error]   DefDef( // def start(): Unit in class HttpServer
> > > > [error]     <method> <triedcooking>
> > > > [error]     "start"
> > > > [error]     []
> > > > [error]     List(Nil)
> > > > [error]     "scala"."Unit" // final abstract class Unit extends
> AnyVal
> > in
> > > > package scala, tree.tpe=3DUnit
> > > > [error]     If(
> > > > [error]       Apply(
> > > > [error]         "server"."$bang$eq"
> > > > [error]         null
> > > > [error]       )
> > > > [error]       Throw(
> > > > [error]         Apply(
> > > > [error]           new ServerStateException."<init>"
> > > > [error]           "Server is already started"
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Block(
> > > > [error]         // 16 statements
> > > > [error]         Apply(
> > > > [error]           "logInfo"
> > > > [error]           "Starting HTTP Server"
> > > > [error]         )
> > > > [error]         Assign(
> > > > [error]           "server"
> > > > [error]           Apply(
> > > > [error]             new Server."<init>"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]         )
> > > > [error]         ValDef(
> > > > [error]           0
> > > > [error]           "connector"
> > > > [error]           <tpt>
> > > > [error]           Apply(
> > > > [error]             new SocketConnector."<init>"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "connector"."setMaxIdleTime"
> > > > [error]           Apply(
> > > > [error]             60."$times"
> > > > [error]             1000
> > > > [error]           )
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "connector"."setSoLingerTime"
> > > > [error]           -1
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "connector"."setPort"
> > > > [error]           0
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "server"."addConnector"
> > > > [error]           "connector"
> > > > [error]         )
> > > > [error]         ValDef(
> > > > [error]           0
> > > > [error]           "threadPool"
> > > > [error]           <tpt>
> > > > [error]           Apply(
> > > > [error]             new QueuedThreadPool."<init>"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "threadPool"."setDaemon"
> > > > [error]           true
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "server"."setThreadPool"
> > > > [error]           "threadPool"
> > > > [error]         )
> > > > [error]         ValDef(
> > > > [error]           0
> > > > [error]           "resHandler"
> > > > [error]           <tpt>
> > > > [error]           Apply(
> > > > [error]             new ResourceHandler."<init>"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "resHandler"."setResourceBase"
> > > > [error]           "resourceBase"."getAbsolutePath"
> > > > [error]         )
> > > > [error]         ValDef(
> > > > [error]           0
> > > > [error]           "handlerList"
> > > > [error]           <tpt>
> > > > [error]           Apply(
> > > > [error]             new HandlerList."<init>"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "handlerList"."setHandlers"
> > > > [error]           Apply(
> > > > [error]             "Array"
> > > > [error]             // 2 arguments
> > > > [error]             "resHandler"
> > > > [error]             Apply(
> > > > [error]               new DefaultHandler."<init>"
> > > > [error]               Nil
> > > > [error]             )
> > > > [error]           )
> > > > [error]         )
> > > > [error]         If(
> > > > [error]           Apply(
> > > > [error]             "securityManager"."isAuthenticationEnabled"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]           Block(
> > > > [error]             // 3 statements
> > > > [error]             Apply(
> > > > [error]               "logDebug"
> > > > [error]               "HttpServer is using security"
> > > > [error]             )
> > > > [error]             ValDef(
> > > > [error]               0
> > > > [error]               "sh"
> > > > [error]               <tpt>
> > > > [error]               Apply(
> > > > [error]                 "setupSecurityHandler"
> > > > [error]                 "securityManager"
> > > > [error]               )
> > > > [error]             )
> > > > [error]             Apply(
> > > > [error]               "sh"."setHandler"
> > > > [error]               "handlerList"
> > > > [error]             )
> > > > [error]             Apply(
> > > > [error]               "server"."setHandler"
> > > > [error]               "sh"
> > > > [error]             )
> > > > [error]           )
> > > > [error]           Block(
> > > > [error]             Apply(
> > > > [error]               "logDebug"
> > > > [error]               "HttpServer is not using security"
> > > > [error]             )
> > > > [error]             Apply(
> > > > [error]               "server"."setHandler"
> > > > [error]               "handlerList"
> > > > [error]             )
> > > > [error]           )
> > > > [error]         )
> > > > [error]         Apply(
> > > > [error]           "server"."start"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]         Assign(
> > > > [error]           "port"
> > > > [error]           Apply(
> > > > [error]             server.getConnectors()(0)."getLocalPort"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]         )
> > > > [error]       )
> > > > [error]     )
> > > > [error]   )
> > > > [error]   DefDef( // private def setupSecurityHandler: <?> in class
> > > > HttpServer
> > > > [error]     <method> private
> > > > [error]     "setupSecurityHandler"
> > > > [error]     []
> > > > [error]     // 1 parameter list
> > > > [error]     ValDef(
> > > > [error]       <param>
> > > > [error]       "securityMgr"
> > > > [error]       "SecurityManager"
> > > > [error]       <empty>
> > > > [error]     )
> > > > [error]     "ConstraintSecurityHandler"
> > > > [error]     Block(
> > > > [error]       // 16 statements
> > > > [error]       ValDef(
> > > > [error]         0
> > > > [error]         "constraint"
> > > > [error]         <tpt>
> > > > [error]         Apply(
> > > > [error]           new Constraint."<init>"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "constraint"."setName"
> > > > [error]         "Constraint"."__DIGEST_AUTH"
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "constraint"."setRoles"
> > > > [error]         Apply(
> > > > [error]           "Array"
> > > > [error]           "user"
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "constraint"."setAuthenticate"
> > > > [error]         true
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "constraint"."setDataConstraint"
> > > > [error]         "Constraint"."DC_NONE"
> > > > [error]       )
> > > > [error]       ValDef(
> > > > [error]         0
> > > > [error]         "cm"
> > > > [error]         <tpt>
> > > > [error]         Apply(
> > > > [error]           new ConstraintMapping."<init>"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "cm"."setConstraint"
> > > > [error]         "constraint"
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "cm"."setPathSpec"
> > > > [error]         "/*"
> > > > [error]       )
> > > > [error]       ValDef(
> > > > [error]         0
> > > > [error]         "sh"
> > > > [error]         <tpt>
> > > > [error]         Apply(
> > > > [error]           new ConstraintSecurityHandler."<init>"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]       )
> > > > [error]       ValDef(
> > > > [error]         0
> > > > [error]         "hashLogin"
> > > > [error]         <tpt>
> > > > [error]         Apply(
> > > > [error]           new HashLoginService."<init>"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]       )
> > > > [error]       ValDef(
> > > > [error]         0
> > > > [error]         "userCred"
> > > > [error]         <tpt>
> > > > [error]         Apply(
> > > > [error]           new Password."<init>"
> > > > [error]           Apply(
> > > > [error]             "securityMgr"."getSecretKey"
> > > > [error]             Nil
> > > > [error]           )
> > > > [error]         )
> > > > [error]       )
> > > > [error]       If(
> > > > [error]         Apply(
> > > > [error]           "userCred"."$eq$eq"
> > > > [error]           null
> > > > [error]         )
> > > > [error]         Throw(
> > > > [error]           Apply(
> > > > [error]             new Exception."<init>"
> > > > [error]             "Error: secret key is null with authentication
> on"
> > > > [error]           )
> > > > [error]         )
> > > > [error]         ()
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "hashLogin"."putUser"
> > > > [error]         // 3 arguments
> > > > [error]         Apply(
> > > > [error]           "securityMgr"."getHttpUser"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]         "userCred"
> > > > [error]         Apply(
> > > > [error]           "Array"
> > > > [error]           "user"
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "sh"."setLoginService"
> > > > [error]         "hashLogin"
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "sh"."setAuthenticator"
> > > > [error]         Apply(
> > > > [error]           new DigestAuthenticator."<init>"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Apply(
> > > > [error]         "sh"."setConstraintMappings"
> > > > [error]         Apply(
> > > > [error]           "Array"
> > > > [error]           "cm"
> > > > [error]         )
> > > > [error]       )
> > > > [error]       "sh"
> > > > [error]     )
> > > > [error]   )
> > > > [error]   DefDef( // def stop(): Unit in class HttpServer
> > > > [error]     <method> <triedcooking>
> > > > [error]     "stop"
> > > > [error]     []
> > > > [error]     List(Nil)
> > > > [error]     "scala"."Unit" // final abstract class Unit extends
> AnyVal
> > in
> > > > package scala, tree.tpe=3DUnit
> > > > [error]     If(
> > > > [error]       Apply(
> > > > [error]         "server"."$eq$eq"
> > > > [error]         null
> > > > [error]       )
> > > > [error]       Throw(
> > > > [error]         Apply(
> > > > [error]           new ServerStateException."<init>"
> > > > [error]           "Server is already stopped"
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Block(
> > > > [error]         // 2 statements
> > > > [error]         Apply(
> > > > [error]           "server"."stop"
> > > > [error]           Nil
> > > > [error]         )
> > > > [error]         Assign(
> > > > [error]           "port"
> > > > [error]           -1
> > > > [error]         )
> > > > [error]         Assign(
> > > > [error]           "server"
> > > > [error]           null
> > > > [error]         )
> > > > [error]       )
> > > > [error]     )
> > > > [error]   )
> > > > [error]   DefDef( // def uri: String in class HttpServer
> > > > [error]     <method> <triedcooking>
> > > > [error]     "uri"
> > > > [error]     []
> > > > [error]     Nil
> > > > [error]     "String"
> > > > [error]     If(
> > > > [error]       Apply(
> > > > [error]         "server"."$eq$eq"
> > > > [error]         null
> > > > [error]       )
> > > > [error]       Throw(
> > > > [error]         Apply(
> > > > [error]           new ServerStateException."<init>"
> > > > [error]           "Server is not started"
> > > > [error]         )
> > > > [error]       )
> > > > [error]       Return(
> > > > [error]         Apply(
> > > > [error]           "http://
> > > ".$plus(Utils.localIpAddress).$plus(":")."$plus"
> > > > [error]           "port"
> > > > [error]         )
> > > > [error]       )
> > > > [error]     )
> > > > [error]   )
> > > > [error] )
> > > > [error]
> > > > [error] uncaught exception during compilation:
> java.lang.AssertionError
> > > > java.lang.AssertionError: assertion failed:
> > > javax.servlet.ServletException
> > > >     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:121=
2)
> > > >     at
> > > scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseExceptions$1(Classf=
ileParser.scala:1051)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$parseAttribute$1(ClassfileParser.scala:920)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseAttributes(Classfil=
eParser.scala:1080)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseMethod(ClassfilePar=
ser.scala:666)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.scala$tools$nsc$symtab$c=
lassfile$ClassfileParser$$queueLoad$1(ClassfileParser.scala:557)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser$$anonfun$parseClass$1.ap=
ply$mcV$sp(ClassfileParser.scala:567)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parseClass(ClassfilePars=
er.scala:572)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.classfile.ClassfileParser.parse(ClassfileParser.sc=
ala:88)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.SymbolLoaders$ClassfileLoader.doComplete(SymbolLoa=
ders.scala:261)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.complete(SymbolLoaders.=
scala:194)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.symtab.SymbolLoaders$SymbolLoader.load(SymbolLoaders.scal=
a:210)
> > > >     at
> scala.reflect.internal.Symbols$Symbol.exists(Symbols.scala:893)
> > > >     at
> > > >
> > scala.tools.nsc.typechecker.Typers$Typer.typedIdent$2(Typers.scala:5064=
)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer.typedIdentOrWildcard$1(Typers.sc=
ala:5218)
> > > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5561)
> > > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
> > > >     at
> > > scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5769)
> > > >     at
> > > scala.tools.nsc.typechecker.Typers$Typer.typedType(Typers.scala:5772)
> > > >     at
> > > scala.tools.nsc.typechecker.Namers$Namer.valDefSig(Namers.scala:1317)
> > > >     at
> > > scala.tools.nsc.typechecker.Namers$Namer.getSig$1(Namers.scala:1457)
> > > >     at
> > > scala.tools.nsc.typechecker.Namers$Namer.typeSig(Namers.scala:1466)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply$mcV$sp(Namers.scala:731)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1$$an=
onfun$apply$1.apply(Namers.scala:730)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer.scala$tools$nsc$typechecker$Name=
rs$Namer$$logAndValidate(Namers.scala:1499)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:730)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$Namer$$anonfun$monoTypeCompleter$1.app=
ly(Namers.scala:729)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$$anon$1.completeImpl(Namers.scala:1614=
)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Namers$LockingTypeCompleter$class.complete(Na=
mers.scala:1622)
> > > >     at
> > > >
> scala.tools.nsc.typechecker.Namers$$anon$1.complete(Namers.scala:1612)
> > > >     at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:123=
1)
> > > >     at
> > > scala.reflect.internal.Symbols$Symbol.initialize(Symbols.scala:1374)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.MethodSynthesis$MethodSynth$class.addDerivedT=
rees(MethodSynthesis.scala:225)
> > > >     at
> > > >
> > scala.tools.nsc.typechecker.Namers$Namer.addDerivedTrees(Namers.scala:5=
5)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$32.apply(Typers.scala:1=
917)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1856)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$rewrappingWrapperTrees$=
1.apply(Typers.scala:1853)
> > > >     at
> > > >
> > >
> >
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
> > > >     at
> > > >
> > >
> >
> scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike=
.scala:251)
> > > >     at scala.collection.immutable.List.foreach(List.scala:318)
> > > >     at
> > > >
> > scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:25=
1)
> > > >     at
> > > scala.collection.AbstractTraversable.flatMap(Traversable.scala:105)
> > > >     at
> > > >
> > scala.tools.nsc.typechecker.Typers$Typer.typedTemplate(Typers.scala:191=
7)
> > > >     at
> > > >
> > scala.tools.nsc.typechecker.Typers$Typer.typedClassDef(Typers.scala:175=
9)
> > > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5583)
> > > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer.scala$tools$nsc$typechecker$Type=
rs$Typer$$typedStat$1(Typers.scala:2928)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer$$anonfun$61.apply(Typers.scala:3=
032)
> > > >     at scala.collection.immutable.List.loop$1(List.scala:170)
> > > >     at scala.collection.immutable.List.mapConserve(List.scala:186)
> > > >     at
> > > >
> scala.tools.nsc.typechecker.Typers$Typer.typedStats(Typers.scala:3032)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Typers$Typer.typedPackageDef$1(Typers.scala:5=
301)
> > > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typed1(Typers.scala:5587)
> > > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5642)
> > > >     at
> > scala.tools.nsc.typechecker.Typers$Typer.typed(Typers.scala:5704)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.apply(Analyzer.=
scala:99)
> > > >     at
> scala.tools.nsc.Global$GlobalPhase.applyPhase(Global.scala:464)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3$$anonfun$run$1.=
apply(Analyzer.scala:91)
> > > >     at scala.collection.Iterator$class.foreach(Iterator.scala:727)
> > > >     at scala.collection.AbstractIterator.foreach(Iterator.scala:115=
7)
> > > >     at
> > > >
> > >
> >
> scala.tools.nsc.typechecker.Analyzer$typerFactory$$anon$3.run(Analyzer.sc=
ala:91)
> > > >     at
> > scala.tools.nsc.Global$Run.compileUnitsInternal(Global.scala:1583)
> > > >     at scala.tools.nsc.Global$Run.compileUnits(Global.scala:1557)
> > > >     at scala.tools.nsc.Global$Run.compileSources(Global.scala:1553)
> > > >     at scala.tools.nsc.Global$Run.compile(Global.scala:1662)
> > > >     at xsbt.CachedCompiler0.run(CompilerInterface.scala:123)
> > > >     at xsbt.CachedCompiler0.run(CompilerInterface.scala:99)
> > > >     at xsbt.CompilerInterface.run(CompilerInterface.scala:27)
> > > >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> > > >     at
> > > >
> > >
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> > > >     at
> > > >
> > >
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> > > >     at java.lang.reflect.Method.invoke(Method.java:606)
> > > >     at
> sbt.compiler.AnalyzingCompiler.call(AnalyzingCompiler.scala:102)
> > > >     at
> > sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:48)
> > > >     at
> > sbt.compiler.AnalyzingCompiler.compile(AnalyzingCompiler.scala:41)
> > > >     at
> > > >
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
$mcV$sp(AggressiveCompile.scala:99)
> > > >     at
> > > >
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
> > > >     at
> > > >
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3$$anonfun$compileScala$1$1.apply=
(AggressiveCompile.scala:99)
> > > >     at
> > > >
> > >
> >
> sbt.compiler.AggressiveCompile.sbt$compiler$AggressiveCompile$$timed(Aggr=
essiveCompile.scala:166)
> > > >     at
> > > >
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.compileScala$1(AggressiveCompil=
e.scala:98)
> > > >     at
> > > >
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:1=
43)
> > > >     at
> > > >
> > >
> >
> sbt.compiler.AggressiveCompile$$anonfun$3.apply(AggressiveCompile.scala:8=
7)
> > > >     at
> > > >
> sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:39)
> > > >     at
> > > >
> sbt.inc.IncrementalCompile$$anonfun$doCompile$1.apply(Compile.scala:37)
> > > >     at sbt.inc.IncrementalCommon.cycle(Incremental.scala:99)
> > > >     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:38)
> > > >     at sbt.inc.Incremental$$anonfun$1.apply(Incremental.scala:37)
> > > >     at sbt.inc.Incremental$.manageClassfiles(Incremental.scala:65)
> > > >     at sbt.inc.Incremental$.compile(Incremental.scala:37)
> > > >     at sbt.inc.IncrementalCompile$.apply(Compile.scala:27)
> > > >     at
> > > sbt.compiler.AggressiveCompile.compile2(AggressiveCompile.scala:157)
> > > >     at
> > > sbt.compiler.AggressiveCompile.compile1(AggressiveCompile.scala:71)
> > > >     at com.typesafe.zinc.Compiler.compile(Compiler.scala:184)
> > > >     at com.typesafe.zinc.Main$.run(Main.scala:98)
> > > >     at com.typesafe.zinc.Nailgun$.zinc(Nailgun.scala:93)
> > > >     at com.typesafe.zinc.Nailgun$.nailMain(Nailgun.scala:82)
> > > >     at com.typesafe.zinc.Nailgun.nailMain(Nailgun.scala)
> > > >     at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
> > > >     at
> > > >
> > >
> >
> sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java=
:57)
> > > >     at
> > > >
> > >
> >
> sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorI=
mpl.java:43)
> > > >     at java.lang.reflect.Method.invoke(Method.java:606)
> > > >     at com.martiansoftware.nailgun.NGSession.run(NGSession.java:280=
)
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO] Reactor Summary:
> > > > [INFO]
> > > > [INFO] Spark Project Parent POM ........................... SUCCESS=
 [
> > > > 1.760 s]
> > > > [INFO] Spark Project Core ................................. FAILURE=
 [
> > > > 5.312 s]
> > > > [INFO] Spark Project Bagel ................................ SKIPPED
> > > > [INFO] Spark Project GraphX ............................... SKIPPED
> > > > [INFO] Spark Project ML Library ........................... SKIPPED
> > > > [INFO] Spark Project Streaming ............................ SKIPPED
> > > > [INFO] Spark Project Tools ................................ SKIPPED
> > > > [INFO] Spark Project Catalyst ............................. SKIPPED
> > > > [INFO] Spark Project SQL .................................. SKIPPED
> > > > [INFO] Spark Project Hive ................................. SKIPPED
> > > > [INFO] Spark Project REPL ................................. SKIPPED
> > > > [INFO] Spark Project YARN Parent POM ...................... SKIPPED
> > > > [INFO] Spark Project YARN Stable API ...................... SKIPPED
> > > > [INFO] Spark Project Assembly ............................. SKIPPED
> > > > [INFO] Spark Project External Twitter ..................... SKIPPED
> > > > [INFO] Spark Project External Kafka ....................... SKIPPED
> > > > [INFO] Spark Project External Flume ....................... SKIPPED
> > > > [INFO] Spark Project External ZeroMQ ...................... SKIPPED
> > > > [INFO] Spark Project External MQTT ........................ SKIPPED
> > > > [INFO] Spark Project Examples ............................. SKIPPED
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO] BUILD FAILURE
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [INFO] Total time: 7.562 s
> > > > [INFO] Finished at: 2014-07-30T21:18:41-07:00
> > > > [INFO] Final Memory: 39M/713M
> > > > [INFO]
> > > >
> > -----------------------------------------------------------------------=
-
> > > > [ERROR] Failed to execute goal
> > > > net.alchim31.maven:scala-maven-plugin:3.1.6:compile
> > (scala-compile-first)
> > > > on project spark-core_2.10: Execution scala-compile-first of goal
> > > > net.alchim31.maven:scala-maven-plugin:3.1.6:compile failed.
> > CompileFailed
> > > > -> [Help 1]
> > > >
> > > > Thanks
> > > > Shengzhe
> > >
> >
>

--90e6ba1811d680656904ff838855--

