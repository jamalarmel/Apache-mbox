From dev-return-696-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  2 04:17:15 2013
Return-Path: <dev-return-696-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8956C10FA6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Nov 2013 04:17:15 +0000 (UTC)
Received: (qmail 27513 invoked by uid 500); 2 Nov 2013 04:17:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 27223 invoked by uid 500); 2 Nov 2013 04:17:09 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 27214 invoked by uid 99); 2 Nov 2013 04:17:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 04:17:08 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mistobaan@gmail.com designates 209.85.212.173 as permitted sender)
Received: from [209.85.212.173] (HELO mail-wi0-f173.google.com) (209.85.212.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 04:17:04 +0000
Received: by mail-wi0-f173.google.com with SMTP id ey11so1862520wid.12
        for <dev@spark.incubator.apache.org>; Fri, 01 Nov 2013 21:16:42 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=nZstB3VmEckNKzoK5yxGI2G42T4ELSFAy9VJb82Tk4U=;
        b=dSBmBvLvCFsqioV1MnPbrEXIgAID77H3jOtjl39WLHIlSz/KwS3q+PjKAP0YFg2RW4
         hRFBVKOiU80D4PDmlkPtWb9PYzbPOA5uEpniPxp8iOYS2PcJaXVEabcbk+lWbr2HncgI
         1vpbzQy62Ec2VMD5Dv6yZ/RBtggeKkUpy8uzp4Pm+BSxP73l46GJ9UR4l7uvcG7BS+J8
         ouAY0NT+OF+Zb6/BEffJ9O726gDZlKOhg3F+yHatx3yaUHoksgUcAQXN/BngbA2/cUw1
         OjKlKCXV6CNiM0I5hVrc2ZS2L56J7vWuywpscOU52fTy/1GjZE8oQGLlMGlM0oHA+S6t
         TWdQ==
MIME-Version: 1.0
X-Received: by 10.180.160.165 with SMTP id xl5mr4436794wib.48.1383365802083;
 Fri, 01 Nov 2013 21:16:42 -0700 (PDT)
Received: by 10.194.16.9 with HTTP; Fri, 1 Nov 2013 21:16:42 -0700 (PDT)
Date: Fri, 1 Nov 2013 21:16:42 -0700
Message-ID: <CAJwSGXXwqyD3-xxmj-rNPSwPQ74gwaRF-H-tw3H90G=vRMB-7w@mail.gmail.com>
Subject: RFC Placement Groups for ec2-scripts
From: Fabrizio Milo aka misto <mistobaan@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hi everyone,

I am experimenting with spark on ec2 and I added the ability to
declare placement groups for
cluster instances. A work in progress is at:

https://github.com/Mistobaan/incubator-spark/compare/placement_group

Placement groups will boost network performances between nodes.

Please try it out and let me know of any issues

Comments and critiques are appreciated :)

Thanks

Fabrizio Milo

--------------------------
Luck favors the prepared mind. (Pasteur)

From dev-return-697-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  2 14:51:45 2013
Return-Path: <dev-return-697-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8A8BC108DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Nov 2013 14:51:45 +0000 (UTC)
Received: (qmail 74771 invoked by uid 500); 2 Nov 2013 14:51:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74488 invoked by uid 500); 2 Nov 2013 14:51:36 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 74480 invoked by uid 99); 2 Nov 2013 14:51:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 14:51:35 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hdc1112@gmail.com designates 209.85.212.43 as permitted sender)
Received: from [209.85.212.43] (HELO mail-vb0-f43.google.com) (209.85.212.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 14:51:30 +0000
Received: by mail-vb0-f43.google.com with SMTP id g10so402510vbg.2
        for <dev@spark.incubator.apache.org>; Sat, 02 Nov 2013 07:51:08 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=Fm3Zy4A7YBe4FqGKyoWyw7ylLe/9Fa3bdfNnDZsm9is=;
        b=kU1CeKBTfDXHIoDb9bZhOMs7eTjwoccGtXdj7FosGCw6yvvEXxD4tostUJQmKleVLO
         r1qproyUM+BMqOnlPE/8VV4I8FWSfpHlA7GFExT8bPOQTwdEaM/oAP/u6zpOGnSjsIgR
         WFz2PcMx/GC3vYrPDdnlyegtyNl+QyXpmn7Lz4Y4e54fsM33dI2gVfd7gxHM/TTOS52D
         ID9yly0j7OzzwwGKEDanHx2W2ZuSsZeu5PkCzBY0tx5VhX29/0ttB8l8cOb0frkVV3tK
         MKoV7CK606yubf/RkKDNbf1XpXJ1KAxMR2tEXVRJEDXNZBiwohH54ATXgHQR9my261qn
         Bh9w==
MIME-Version: 1.0
X-Received: by 10.52.22.110 with SMTP id c14mr110217vdf.28.1383403868264; Sat,
 02 Nov 2013 07:51:08 -0700 (PDT)
Received: by 10.58.211.99 with HTTP; Sat, 2 Nov 2013 07:51:08 -0700 (PDT)
Date: Sat, 2 Nov 2013 10:51:08 -0400
Message-ID: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
Subject: a question about lineage graphs in streaming
From: dachuan <hdc1112@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=20cf307c9ad2262e7004ea32d0c1
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307c9ad2262e7004ea32d0c1
Content-Type: text/plain; charset=ISO-8859-1

Hi, developers,

I found this sentence hard to understand, it's from sosp'13 spark streaming
paper:

"Lineage cutoff: Because lineage graphs between RDDs
in D-Streams can grow indefinitely, we modified the
scheduler to forget lineage after an RDD has been checkpointed,
so that its state does not grow arbitrarily."

In my personal understanding, the length of DStream chain is fixed, so the
RDDs these DStreams generate also have fixed length. Besides, the RDDs
don't depend on the RDDs in the previous round. So why does it claim that
lineage graph can grow indefinitely? when you say "grow indefinitely", do
you refer to lineage graph's width or the number of lineage graphs?

thanks,
dachuan.

--20cf307c9ad2262e7004ea32d0c1--

From dev-return-698-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  2 17:36:12 2013
Return-Path: <dev-return-698-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 329B710BDC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Nov 2013 17:36:12 +0000 (UTC)
Received: (qmail 93703 invoked by uid 500); 2 Nov 2013 17:36:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93658 invoked by uid 500); 2 Nov 2013 17:36:11 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 93650 invoked by uid 99); 2 Nov 2013 17:36:11 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 17:36:11 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ctn@adatao.com designates 209.85.223.169 as permitted sender)
Received: from [209.85.223.169] (HELO mail-ie0-f169.google.com) (209.85.223.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 17:36:07 +0000
Received: by mail-ie0-f169.google.com with SMTP id ar20so9682663iec.14
        for <dev@spark.incubator.apache.org>; Sat, 02 Nov 2013 10:35:45 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=adatao.com; s=google;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=jdHfVI3jm2ign96V1e8O5rVYchI0+QwhmL65NuUNX78=;
        b=AaSMMMXCY0BdXnnGwkCk7WgTGkNJyzmNrAoQZqk/HAwiyujpGNe2G/YiFMhus+Ejma
         30uzEGTHgRfpKwfEIDN1EbddU0IeeyO7Qhb1vm2eGNJf1quYuijAmcBah18G7L9QSP4x
         wke5XX/UVY50GwqR8C3K2NSC5hpCZSNF4DZWw=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:content-type;
        bh=jdHfVI3jm2ign96V1e8O5rVYchI0+QwhmL65NuUNX78=;
        b=j9cQZ24D8EpS9tCPr0HfcJg0CeQLIF44eXWZeNXV8T02WIgu4gaR+q35SUZf4V/j8I
         73WF6IFFRjngOjnbjethD7/KWqAFemFxKxW2OubWF49jd+WWf/AV5BltRDls8pjZM+ca
         VQfUeM2/C8lm5WaZ23UUjyx9XVd1sm4pScoXPRr7qlOY39v4jRRs1gWAojTH7VhquKhI
         mHK6aP0mxxpftLZpGmz9dDuu1IA0lHeYmorD8w6cfUnXz2wDZX22Q2qj55lVk4N9CDJf
         5euAlckeQSaDpvMzEv0yn/V3/Uo/0TvmhyB+06VO9/w2sqgb7QTpuLFDRuutUlEyVUiA
         EAjg==
X-Gm-Message-State: ALoCoQne2e0eixhBEa4NjLR5j+64ETb0sYcfdgFelvUI3IFT94No1a2SvjzSFtRIzdC0Qcll3d4P
X-Received: by 10.50.55.106 with SMTP id r10mr6428638igp.45.1383413745038;
 Sat, 02 Nov 2013 10:35:45 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.64.20.78 with HTTP; Sat, 2 Nov 2013 10:35:24 -0700 (PDT)
X-Originating-IP: [67.188.95.187]
In-Reply-To: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
References: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
From: Christopher Nguyen <ctn@adatao.com>
Date: Sat, 2 Nov 2013 10:35:24 -0700
Message-ID: <CAGh_TuMiOBWFE824GT5Vzdq=-LLVdtVxO9ryL5Y4NX36H+Q7JQ@mail.gmail.com>
Subject: Re: a question about lineage graphs in streaming
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b10ce43d9db2804ea351c22
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b10ce43d9db2804ea351c22
Content-Type: text/plain; charset=ISO-8859-1

Dachuan, you may have correctly answered your own question. See Fig. 3 of
the same paper, where "infinity" occurs in the vertical direction.

--
Christopher T. Nguyen
Co-founder & CEO, Adatao <http://adatao.com>
linkedin.com/in/ctnguyen



On Sat, Nov 2, 2013 at 7:51 AM, dachuan <hdc1112@gmail.com> wrote:

> Hi, developers,
>
> I found this sentence hard to understand, it's from sosp'13 spark streaming
> paper:
>
> "Lineage cutoff: Because lineage graphs between RDDs
> in D-Streams can grow indefinitely, we modified the
> scheduler to forget lineage after an RDD has been checkpointed,
> so that its state does not grow arbitrarily."
>
> In my personal understanding, the length of DStream chain is fixed, so the
> RDDs these DStreams generate also have fixed length. Besides, the RDDs
> don't depend on the RDDs in the previous round. So why does it claim that
> lineage graph can grow indefinitely? when you say "grow indefinitely", do
> you refer to lineage graph's width or the number of lineage graphs?
>
> thanks,
> dachuan.
>

--047d7b10ce43d9db2804ea351c22--

From dev-return-699-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  2 20:35:47 2013
Return-Path: <dev-return-699-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C8C7F10E76
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Nov 2013 20:35:47 +0000 (UTC)
Received: (qmail 15076 invoked by uid 500); 2 Nov 2013 20:35:47 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15044 invoked by uid 500); 2 Nov 2013 20:35:47 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 15036 invoked by uid 99); 2 Nov 2013 20:35:47 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 20:35:47 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.54] (HELO mail-bk0-f54.google.com) (209.85.214.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 20:35:42 +0000
Received: by mail-bk0-f54.google.com with SMTP id 6so1868034bkj.41
        for <dev@spark.incubator.apache.org>; Sat, 02 Nov 2013 13:35:20 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=sMyajAeKFv0Kj8h4B46mJz2qbcS5iIjzVN8wkDLFUAI=;
        b=l056Vc9cBESy74eo9ZTZTBm5gMAyOZTwHhmG9VBMVl2ZDTJFWnoodZ4/TKYwwdDryX
         9hTV+YFH+VTv9GCsWn4vPgQpxMO3pMy39vek8oR0OSOCklwI6YRvnr23ojsw1wOxaiD9
         DsmewCFppQ7Am6p/L7zqcKQ03gYu/vKaMbl7dTSrdmWhEdJ8Iv+NzT9M0AZu24R4Z6p6
         2IqegL3ObT7AltLpx9CWkBg2aH71/BbAHhdSlC1TSDiL/IWWFd66d+m4TQ+45Xs3LrP0
         hN+vf+kcjxpCUXpHQnGXY/4qfnDH2r8OBVf/xrywoEaSGGf8la7wJ9UFkJ6LwF6iytje
         p9LQ==
X-Gm-Message-State: ALoCoQmd9qYnk1Se/TKPO8GuxXsjxGN6hDnbkq8YBZfFKQJxLgPvGGlPAsXJbPmrL76XlSzZyRu1
MIME-Version: 1.0
X-Received: by 10.204.228.198 with SMTP id jf6mr482690bkb.41.1383424520357;
 Sat, 02 Nov 2013 13:35:20 -0700 (PDT)
Received: by 10.204.101.201 with HTTP; Sat, 2 Nov 2013 13:35:20 -0700 (PDT)
In-Reply-To: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
References: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
Date: Sat, 2 Nov 2013 13:35:20 -0700
Message-ID: <CAAsvFP=9XuB7ebz+uFmTfnQ9fa1Js4t6A5AvmEZPk03zvb4Gfg@mail.gmail.com>
Subject: Re: a question about lineage graphs in streaming
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=485b3970d22c1c6aff04ea379f32
X-Virus-Checked: Checked by ClamAV on apache.org

--485b3970d22c1c6aff04ea379f32
Content-Type: text/plain; charset=ISO-8859-1

You're coming at the paper from a different context than that in which it
was written.  The paper doesn't claim that RDD lineage and state could grow
indefinitely after the Spark Streaming changes were made.  That growth was
indefinite in early, pre-Streaming versions of Spark, however.



On Sat, Nov 2, 2013 at 7:51 AM, dachuan <hdc1112@gmail.com> wrote:

> Hi, developers,
>
> I found this sentence hard to understand, it's from sosp'13 spark streaming
> paper:
>
> "Lineage cutoff: Because lineage graphs between RDDs
> in D-Streams can grow indefinitely, we modified the
> scheduler to forget lineage after an RDD has been checkpointed,
> so that its state does not grow arbitrarily."
>
> In my personal understanding, the length of DStream chain is fixed, so the
> RDDs these DStreams generate also have fixed length. Besides, the RDDs
> don't depend on the RDDs in the previous round. So why does it claim that
> lineage graph can grow indefinitely? when you say "grow indefinitely", do
> you refer to lineage graph's width or the number of lineage graphs?
>
> thanks,
> dachuan.
>

--485b3970d22c1c6aff04ea379f32--

From dev-return-700-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  2 21:24:41 2013
Return-Path: <dev-return-700-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C070F10FE9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Nov 2013 21:24:41 +0000 (UTC)
Received: (qmail 58114 invoked by uid 500); 2 Nov 2013 21:24:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58071 invoked by uid 500); 2 Nov 2013 21:24:41 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 58062 invoked by uid 99); 2 Nov 2013 21:24:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 21:24:41 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hdc1112@gmail.com designates 209.85.128.175 as permitted sender)
Received: from [209.85.128.175] (HELO mail-ve0-f175.google.com) (209.85.128.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 21:24:37 +0000
Received: by mail-ve0-f175.google.com with SMTP id jz11so552264veb.34
        for <dev@spark.incubator.apache.org>; Sat, 02 Nov 2013 14:24:16 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=wu8CvLdo4iGp7j7lfYXysQb1e41YhQrWxwgUOtVA6Xs=;
        b=oJroO0B9EZskbi5tEIlkyc02e9NvnE/kjvAMJa/sxKmFUenMkAuehAd4MubUcCORCk
         fiCZJHhFBFb0eUNc640Hj2gpJ0UwHPj5PqTqMcY3hepOa7FISc50NtbF+fyzSXlDOG/M
         cTQG08KwwMbxzDOkz24PjeH8pX/2F0qerbvUJ0x/ZZS7IhE95DgLpeLFYkfTUbTRu8Lz
         KOBcxsSoFF+UUwGepeKW6a6+ep47BMghbN6+VxCbqZ4wz4ZX5a8VtBJp20zs6T5n9KoL
         EFjtOeulIlWOuhmM1lPWOp+dmwNJxSIZraV/+wOTIpM/CgPDgS5V0MmaY2/tTXGqu8NQ
         sHow==
MIME-Version: 1.0
X-Received: by 10.58.232.228 with SMTP id tr4mr669072vec.34.1383427456710;
 Sat, 02 Nov 2013 14:24:16 -0700 (PDT)
Received: by 10.58.211.99 with HTTP; Sat, 2 Nov 2013 14:24:16 -0700 (PDT)
In-Reply-To: <CAAsvFP=9XuB7ebz+uFmTfnQ9fa1Js4t6A5AvmEZPk03zvb4Gfg@mail.gmail.com>
References: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
	<CAAsvFP=9XuB7ebz+uFmTfnQ9fa1Js4t6A5AvmEZPk03zvb4Gfg@mail.gmail.com>
Date: Sat, 2 Nov 2013 17:24:16 -0400
Message-ID: <CAAzo=r9oacn6rGpaodN5reCyMT+_eRRy0+osGAN5X_VJYjtkzw@mail.gmail.com>
Subject: Re: a question about lineage graphs in streaming
From: dachuan <hdc1112@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e013cbde221414204ea384e28
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013cbde221414204ea384e28
Content-Type: text/plain; charset=ISO-8859-1

It seems what Christopher said makes certain sense, because this round's
RDD depends on last round's RDD, so as time goes by, it would grow
infinitely.

I realize that the streaming/examples/clickstream/PageViewStream.scala in
code base is not what figure 3 in paper describes, so I have no idea what
application figure 3 is talking about.

Mark, sorry I don't quite understand what you've said.

thanks,
dachuan.


On Sat, Nov 2, 2013 at 4:35 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> You're coming at the paper from a different context than that in which it
> was written.  The paper doesn't claim that RDD lineage and state could grow
> indefinitely after the Spark Streaming changes were made.  That growth was
> indefinite in early, pre-Streaming versions of Spark, however.
>
>
>
> On Sat, Nov 2, 2013 at 7:51 AM, dachuan <hdc1112@gmail.com> wrote:
>
> > Hi, developers,
> >
> > I found this sentence hard to understand, it's from sosp'13 spark
> streaming
> > paper:
> >
> > "Lineage cutoff: Because lineage graphs between RDDs
> > in D-Streams can grow indefinitely, we modified the
> > scheduler to forget lineage after an RDD has been checkpointed,
> > so that its state does not grow arbitrarily."
> >
> > In my personal understanding, the length of DStream chain is fixed, so
> the
> > RDDs these DStreams generate also have fixed length. Besides, the RDDs
> > don't depend on the RDDs in the previous round. So why does it claim that
> > lineage graph can grow indefinitely? when you say "grow indefinitely", do
> > you refer to lineage graph's width or the number of lineage graphs?
> >
> > thanks,
> > dachuan.
> >
>



-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210

--089e013cbde221414204ea384e28--

From dev-return-701-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  2 21:36:08 2013
Return-Path: <dev-return-701-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A4AE010026
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Nov 2013 21:36:08 +0000 (UTC)
Received: (qmail 63509 invoked by uid 500); 2 Nov 2013 21:36:08 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63472 invoked by uid 500); 2 Nov 2013 21:36:08 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 63464 invoked by uid 99); 2 Nov 2013 21:36:08 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 21:36:08 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.50] (HELO mail-bk0-f50.google.com) (209.85.214.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 21:36:04 +0000
Received: by mail-bk0-f50.google.com with SMTP id v4so2335473bkz.9
        for <dev@spark.incubator.apache.org>; Sat, 02 Nov 2013 14:35:43 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=t7Xa8CbUImvBoDyoqzEquDohAYaCtFsDdmg1rDMgpGU=;
        b=K+VPWM7irDKfKaCkoOcCz7dgwxykJidiut1xlIQZ3N/NdSM7+NeCD7c1UmqQpD8D9D
         AFff5WfiEusiDG52BB0iH33whfEZsPRdpjiVNOhZXCS0g7GRvbY8x3yPNRDQ2kzY9ouC
         V8sWZOnoHUzpPO0FOYftILseNMA7vtPOGFwGflirNybnIij8rtW6ipEOzmunarm1A/xn
         Fcu3Wko8gpizsNih974XM8DUwxrrpbcYO4i4X6ok8IYEZoGLYHkic2+A5j1CIMK9Fdt2
         1RFEYScxPCOjJT6QJkgM9YBD8X2YS2g4hTV6dS6kTH4thIeJcOzC9v9AYLgm2E9Pn9e3
         sclg==
X-Gm-Message-State: ALoCoQkfYKBq+GF6P6ASkWY0cGbmkqnuuPaLwhVjcT8uOUO4BE44k51XPFM29mFXvwsA7fcw5pE0
MIME-Version: 1.0
X-Received: by 10.205.37.10 with SMTP id tc10mr4610299bkb.9.1383428142934;
 Sat, 02 Nov 2013 14:35:42 -0700 (PDT)
Received: by 10.204.101.201 with HTTP; Sat, 2 Nov 2013 14:35:42 -0700 (PDT)
In-Reply-To: <CAAzo=r9oacn6rGpaodN5reCyMT+_eRRy0+osGAN5X_VJYjtkzw@mail.gmail.com>
References: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
	<CAAsvFP=9XuB7ebz+uFmTfnQ9fa1Js4t6A5AvmEZPk03zvb4Gfg@mail.gmail.com>
	<CAAzo=r9oacn6rGpaodN5reCyMT+_eRRy0+osGAN5X_VJYjtkzw@mail.gmail.com>
Date: Sat, 2 Nov 2013 14:35:42 -0700
Message-ID: <CAAsvFPkJfNJW_w4bTcAfeX3R2QpPSRmB5KGMcyhi9g9kfq-f0g@mail.gmail.com>
Subject: Re: a question about lineage graphs in streaming
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=bcaec52c5c7708930804ea3877ba
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec52c5c7708930804ea3877ba
Content-Type: text/plain; charset=ISO-8859-1

All that I am saying is that before the checkpointing changes that came in
with the Streaming additions, RDD lineage would grow indefintiely.  Now
checkpointing causes pre-checkpoint lineage to be forgotten, so
checkpointing is an effective means to control the growth of RDD state.


On Sat, Nov 2, 2013 at 2:24 PM, dachuan <hdc1112@gmail.com> wrote:

> It seems what Christopher said makes certain sense, because this round's
> RDD depends on last round's RDD, so as time goes by, it would grow
> infinitely.
>
> I realize that the streaming/examples/clickstream/PageViewStream.scala in
> code base is not what figure 3 in paper describes, so I have no idea what
> application figure 3 is talking about.
>
> Mark, sorry I don't quite understand what you've said.
>
> thanks,
> dachuan.
>
>
> On Sat, Nov 2, 2013 at 4:35 PM, Mark Hamstra <mark@clearstorydata.com
> >wrote:
>
> > You're coming at the paper from a different context than that in which it
> > was written.  The paper doesn't claim that RDD lineage and state could
> grow
> > indefinitely after the Spark Streaming changes were made.  That growth
> was
> > indefinite in early, pre-Streaming versions of Spark, however.
> >
> >
> >
> > On Sat, Nov 2, 2013 at 7:51 AM, dachuan <hdc1112@gmail.com> wrote:
> >
> > > Hi, developers,
> > >
> > > I found this sentence hard to understand, it's from sosp'13 spark
> > streaming
> > > paper:
> > >
> > > "Lineage cutoff: Because lineage graphs between RDDs
> > > in D-Streams can grow indefinitely, we modified the
> > > scheduler to forget lineage after an RDD has been checkpointed,
> > > so that its state does not grow arbitrarily."
> > >
> > > In my personal understanding, the length of DStream chain is fixed, so
> > the
> > > RDDs these DStreams generate also have fixed length. Besides, the RDDs
> > > don't depend on the RDDs in the previous round. So why does it claim
> that
> > > lineage graph can grow indefinitely? when you say "grow indefinitely",
> do
> > > you refer to lineage graph's width or the number of lineage graphs?
> > >
> > > thanks,
> > > dachuan.
> > >
> >
>
>
>
> --
> Dachuan Huang
> Cellphone: 614-390-7234
> 2015 Neil Avenue
> Ohio State University
> Columbus, Ohio
> U.S.A.
> 43210
>

--bcaec52c5c7708930804ea3877ba--

From dev-return-702-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  2 21:41:00 2013
Return-Path: <dev-return-702-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D345F1006F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  2 Nov 2013 21:41:00 +0000 (UTC)
Received: (qmail 66686 invoked by uid 500); 2 Nov 2013 21:41:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66645 invoked by uid 500); 2 Nov 2013 21:41:00 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 66637 invoked by uid 99); 2 Nov 2013 21:41:00 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 21:41:00 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hdc1112@gmail.com designates 209.85.212.43 as permitted sender)
Received: from [209.85.212.43] (HELO mail-vb0-f43.google.com) (209.85.212.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 02 Nov 2013 21:40:56 +0000
Received: by mail-vb0-f43.google.com with SMTP id g10so532681vbg.16
        for <dev@spark.incubator.apache.org>; Sat, 02 Nov 2013 14:40:36 -0700 (PDT)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=umTmWOquCGmOBqVtwDjWz3hh3oOfikaQpomYnEVGBA0=;
        b=dTePMa2t+O2BIO/cb3vMIclwkX5CReJy1pbZ5UpavOQVb1jtY92ReHhnGg/aWZHpLA
         LE3To2eZrOF9TsiosFPnI2RbYD27fd6A0lpAa3A5JmvnCtFDH/p/dDA4SVlbG5oDHss1
         Qs3gf9M3J2MUAayuZq3TZoK106GjNqRv2JscrYTAdJS/AwH8yhY9IOJfeTBAReVqTaxg
         W2FJwKEgoH3qJWRhwcduecAlrBoZvENkZzeIuNJbmgJLf3eMjiwBnE7oNHmLYpDfKR0m
         cKc4KDyuFhDoG/35IEvwvOXVhhmWZWlVbVYehQpgY1YBrkIuiciEl/IRRvWHY2geOj4s
         x5hw==
MIME-Version: 1.0
X-Received: by 10.58.178.239 with SMTP id db15mr6016494vec.9.1383428436109;
 Sat, 02 Nov 2013 14:40:36 -0700 (PDT)
Received: by 10.58.211.99 with HTTP; Sat, 2 Nov 2013 14:40:36 -0700 (PDT)
In-Reply-To: <CAAsvFPkJfNJW_w4bTcAfeX3R2QpPSRmB5KGMcyhi9g9kfq-f0g@mail.gmail.com>
References: <CAAzo=r9LDE+zPWJG02J5qj1DDOiWKd9GaotmgE-qh0BCprUdPg@mail.gmail.com>
	<CAAsvFP=9XuB7ebz+uFmTfnQ9fa1Js4t6A5AvmEZPk03zvb4Gfg@mail.gmail.com>
	<CAAzo=r9oacn6rGpaodN5reCyMT+_eRRy0+osGAN5X_VJYjtkzw@mail.gmail.com>
	<CAAsvFPkJfNJW_w4bTcAfeX3R2QpPSRmB5KGMcyhi9g9kfq-f0g@mail.gmail.com>
Date: Sat, 2 Nov 2013 17:40:36 -0400
Message-ID: <CAAzo=r80M63gX1ga9nb2=O3c5ZOdu_N=21yPeFr+Cj2v2GbGTQ@mail.gmail.com>
Subject: Re: a question about lineage graphs in streaming
From: dachuan <hdc1112@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b672a9681b25c04ea388868
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b672a9681b25c04ea388868
Content-Type: text/plain; charset=ISO-8859-1

right. the paper mentioned this improvement.

thanks, Mark.


On Sat, Nov 2, 2013 at 5:35 PM, Mark Hamstra <mark@clearstorydata.com>wrote:

> All that I am saying is that before the checkpointing changes that came in
> with the Streaming additions, RDD lineage would grow indefintiely.  Now
> checkpointing causes pre-checkpoint lineage to be forgotten, so
> checkpointing is an effective means to control the growth of RDD state.
>
>
> On Sat, Nov 2, 2013 at 2:24 PM, dachuan <hdc1112@gmail.com> wrote:
>
> > It seems what Christopher said makes certain sense, because this round's
> > RDD depends on last round's RDD, so as time goes by, it would grow
> > infinitely.
> >
> > I realize that the streaming/examples/clickstream/PageViewStream.scala in
> > code base is not what figure 3 in paper describes, so I have no idea what
> > application figure 3 is talking about.
> >
> > Mark, sorry I don't quite understand what you've said.
> >
> > thanks,
> > dachuan.
> >
> >
> > On Sat, Nov 2, 2013 at 4:35 PM, Mark Hamstra <mark@clearstorydata.com
> > >wrote:
> >
> > > You're coming at the paper from a different context than that in which
> it
> > > was written.  The paper doesn't claim that RDD lineage and state could
> > grow
> > > indefinitely after the Spark Streaming changes were made.  That growth
> > was
> > > indefinite in early, pre-Streaming versions of Spark, however.
> > >
> > >
> > >
> > > On Sat, Nov 2, 2013 at 7:51 AM, dachuan <hdc1112@gmail.com> wrote:
> > >
> > > > Hi, developers,
> > > >
> > > > I found this sentence hard to understand, it's from sosp'13 spark
> > > streaming
> > > > paper:
> > > >
> > > > "Lineage cutoff: Because lineage graphs between RDDs
> > > > in D-Streams can grow indefinitely, we modified the
> > > > scheduler to forget lineage after an RDD has been checkpointed,
> > > > so that its state does not grow arbitrarily."
> > > >
> > > > In my personal understanding, the length of DStream chain is fixed,
> so
> > > the
> > > > RDDs these DStreams generate also have fixed length. Besides, the
> RDDs
> > > > don't depend on the RDDs in the previous round. So why does it claim
> > that
> > > > lineage graph can grow indefinitely? when you say "grow
> indefinitely",
> > do
> > > > you refer to lineage graph's width or the number of lineage graphs?
> > > >
> > > > thanks,
> > > > dachuan.
> > > >
> > >
> >
> >
> >
> > --
> > Dachuan Huang
> > Cellphone: 614-390-7234
> > 2015 Neil Avenue
> > Ohio State University
> > Columbus, Ohio
> > U.S.A.
> > 43210
> >
>



-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210

--047d7b672a9681b25c04ea388868--

From dev-return-703-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sun Nov  3 07:51:52 2013
Return-Path: <dev-return-703-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5544A10771
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Nov 2013 07:51:52 +0000 (UTC)
Received: (qmail 87016 invoked by uid 500); 3 Nov 2013 07:51:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86951 invoked by uid 500); 3 Nov 2013 07:51:42 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 86943 invoked by uid 99); 3 Nov 2013 07:51:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Nov 2013 07:51:39 +0000
X-ASF-Spam-Status: No, hits=-1998.3 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Sun, 03 Nov 2013 07:51:35 +0000
Received: (qmail 86916 invoked by uid 99); 3 Nov 2013 07:51:11 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Nov 2013 07:51:11 +0000
Received: from localhost (HELO mail-ve0-f170.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Nov 2013 07:51:10 +0000
Received: by mail-ve0-f170.google.com with SMTP id oy12so717557veb.29
        for <dev@spark.incubator.apache.org>; Sun, 03 Nov 2013 00:51:09 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=ncZlPXiaQcimEcttNI08lIXJ9LC6DwIRxiu9bOJmdjg=;
        b=Vx1SKUXPxQNwsgPUfn7aAOq636BhziELKO06rjmU4520NK1Uy7UMeLl9ilWO26dwq8
         iGp4MpNLmUazyWiE3EEVhaGaWVe79L7Hyz+p/Zq0+vos7UlrrpmdJD9VbcnC2XSiRjuX
         630HGhueodRGk1KzBe/2IoiSBk3K79XOAm3pfzNa3Qbugi+rU6/JAFTqb40S+nDAluSX
         MFRoNSd7Pe4G1J/RVOgXbR8MHMe5lOFZPRStsQ1cYHMMuIN2iRT6Cz/ELPaIDNoLDFCP
         JDxS7OBD6AniIZg2P4Q1cuL+Ch4H+ZJ4y5iDtNDuNWkhro7Kha5BZ5gBeXMg/wvuYgRE
         DxVg==
X-Received: by 10.52.27.243 with SMTP id w19mr5922733vdg.3.1383465069216; Sun,
 03 Nov 2013 00:51:09 -0700 (PDT)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Sun, 3 Nov 2013 00:50:49 -0700 (PDT)
In-Reply-To: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
From: Reynold Xin <rxin@apache.org>
Date: Sun, 3 Nov 2013 00:50:49 -0700
Message-ID: <CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
Subject: Re: SPARK-942
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=20cf307cfd680291cc04ea4110b1
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307cfd680291cc04ea4110b1
Content-Type: text/plain; charset=UTF-8

It's not a very elegant solution, but one possibility is for the
CacheManager to check whether it will have enough space. If it is running
out of space, skips buffering the output of the iterator & directly write
the output of the iterator to disk (if storage level allows that).

But it is still tricky to know whether we will run out of space before we
even start running the iterator. One possibility is to use sizing data from
previous partitions to estimate the size of the current partition (i.e.
estimated in memory size = avg of current in-memory size / current input
size).

Do you have any ideas on this one, Kyle?


On Sat, Oct 26, 2013 at 10:53 AM, Kyle Ellrott <kellrott@soe.ucsc.edu>wrote:

> I was wondering if anybody had any thoughts on the best way to tackle
> SPARK-942 ( https://spark-project.atlassian.net/browse/SPARK-942 ).
> Basically, Spark takes an iterator from a flatmap call and because I tell
> it that it needs to persist Spark proceeds to push it all into an array
> before deciding that it doesn't have enough memory and trying to serialize
> it to disk, and somewhere along the line it runs out of memory. For my
> particular operation, the function return an iterator that reads data out
> of a file, and the size of the files passed to that function can vary
> greatly (from a few kilobytes to a few gigabytes). The funny thing is that
> if I do a strait 'map' operation after the flat map, everything works,
> because Spark just passes the iterator forward and never tries to expand
> the whole thing into memory. But I need do a reduceByKey across all the
> records, so I'd like to persist to disk first, and that is where I hit this
> snag.
> I've already setup a unit test to replicate the problem, and I know the
> area of the code that would need to be fixed.
> I'm just hoping for some tips on the best way to fix the problem.
>
> Kyle
>

--20cf307cfd680291cc04ea4110b1--

From dev-return-704-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sun Nov  3 23:55:53 2013
Return-Path: <dev-return-704-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 53063C7F5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun,  3 Nov 2013 23:55:53 +0000 (UTC)
Received: (qmail 59294 invoked by uid 500); 3 Nov 2013 23:55:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59183 invoked by uid 500); 3 Nov 2013 23:55:52 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 59173 invoked by uid 99); 3 Nov 2013 23:55:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Nov 2013 23:55:52 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.217.182 as permitted sender)
Received: from [209.85.217.182] (HELO mail-lb0-f182.google.com) (209.85.217.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 03 Nov 2013 23:55:48 +0000
Received: by mail-lb0-f182.google.com with SMTP id w6so4993877lbh.13
        for <dev@spark.incubator.apache.org>; Sun, 03 Nov 2013 15:55:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=CdjY8xoNFXO55bboFY6xOb3Uzfz/EzoRRAK9gpoqyOg=;
        b=hen6Qohim9PQBrzU5xFxpbd0R+b+DkBilElEg44XiBhp/AQoDHynGNwzuxwXnJapeU
         GxWgr2TL80diLeZt1khUM6zndaQiuNVHFcyUIdIoow0hfUx4y1pCx85RwNIm1vc6m3RO
         55esYcBxpRTHsBI6nGw5C4wJm2aHmpmuGoETY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=CdjY8xoNFXO55bboFY6xOb3Uzfz/EzoRRAK9gpoqyOg=;
        b=j7haIjYE8OtEsIOzvZnRLy9CoekoRL+/Sm19ctWCgvE2pXLsK4KYFx1NshHlx+mtZp
         FI5jfAefdg17sQAA1PZi0sYPpNu5tK+dGAElO7/z6AuM6SgWRCH/uuQrlQAizceOHWx3
         Sf9a+OvWlbXQ/OuxdRCBkpfeeP4OXhpuPj3heRZnEZFegKz3KAU1c26YWPIbTCoXFo93
         DYmuvzsFUFu4eHuwGRo+HFXzzPzYovp2Fz+UoQVbT7o07wEaWzkCfhfJpx7lXelyOMFg
         d+oHP8W3T36kWCYsfPr+PPLk55RssvuoVPEQFuk5f+E//dEKhAtr1v1MqSkjiD+T96qG
         CztA==
X-Gm-Message-State: ALoCoQkhh+xNv2gazdTb/JEAJUl4bhdUXJHBXwSvArSY+8KR3X6OhJ5ll2AoaPmA1raB0dDHGb6v
MIME-Version: 1.0
X-Received: by 10.112.205.164 with SMTP id lh4mr8772668lbc.15.1383522926461;
 Sun, 03 Nov 2013 15:55:26 -0800 (PST)
Received: by 10.112.94.70 with HTTP; Sun, 3 Nov 2013 15:55:26 -0800 (PST)
In-Reply-To: <CAAsvFPm9TB3ubihBLRaPWTZDEQvmc2rtw8TH_sgePePDDbX+WQ@mail.gmail.com>
References: <CADWPM3j3wzLJ96_SvTVAEF-EMrDAxFAuOG5fubfZ7qzvBRhobA@mail.gmail.com>
	<CAOEPXP4uWz8=T7L_tFN4bUPC5qYGrmDYqCCd-nOdS6mO+qbOcA@mail.gmail.com>
	<CADWPM3jDBaDWNSGfDVJC-DS11S_rmiReUHq-=i-ghKLkr1iCZw@mail.gmail.com>
	<CAAsvFPm9TB3ubihBLRaPWTZDEQvmc2rtw8TH_sgePePDDbX+WQ@mail.gmail.com>
Date: Sun, 3 Nov 2013 15:55:26 -0800
Message-ID: <CADWPM3i8R2wvQdXxduhydF1iFNnW6KzuHv9ArPQe+VS8BSHgNg@mail.gmail.com>
Subject: Re: Getting failures in FileServerSuite
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Mark:  I'm using JDK 1.6. --

On Wed, Oct 30, 2013 at 1:44 PM, Mark Hamstra <mark@clearstorydata.com> wrote:
> What JDK version on you using, Evan?
>
> I tried to reproduce your problem earlier today, but I wasn't even able to
> get through the assembly build -- kept hanging when trying to build the
> examples assembly.  Foregoing the assembly and running the tests would hang
> on FileServerSuite "Dynamically adding JARS locally" -- no stack trace,
> just hung.  And I was actually seeing a very similar stack trace to yours
> from a test suite of our own running against 0.8.1-SNAPSHOT -- not exactly
> the same because line numbers were different once it went into the java
> runtime, and it eventually ended up someplace a little different.  That got
> me curious about differences in Java versions, so I updated to the latest
> Oracle release (1.7.0_45).  Now it cruises right through the build and test
> of Spark master from before Matei merged your PR.  Then I logged into a
> machine that has 1.7.0_15 (7u15-2.3.7-0ubuntu1~11.10.1, actually)
> installed, and I'm right back to the hanging during the examples assembly
> (but passes FileServerSuite, oddly enough.)  Upgrading the JDK didn't
> improve the results of the ClearStory test suite I was looking at, so my
> misery isn't over; but yours might be with a newer JDK....
>
>
>
> On Wed, Oct 30, 2013 at 12:44 PM, Evan Chan <ev@ooyala.com> wrote:
>
>> Must be a local environment thing, because AmpLab Jenkins can't
>> reproduce it..... :-p
>>
>> On Wed, Oct 30, 2013 at 11:10 AM, Josh Rosen <rosenville@gmail.com> wrote:
>> > Someone on the users list also encountered this exception:
>> >
>> >
>> https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201310.mbox/%3C64474308D680D540A4D8151B0F7C03F7025EF289%40SHSMSX104.ccr.corp.intel.com%3E
>> >
>> >
>> > On Wed, Oct 30, 2013 at 9:40 AM, Evan Chan <ev@ooyala.com> wrote:
>> >
>> >> I'm at the latest
>> >>
>> >> commit f0e23a023ce1356bc0f04248605c48d4d08c2d05
>> >> Merge: aec9bf9 a197137
>> >> Author: Reynold Xin <rxin@apache.org>
>> >> Date:   Tue Oct 29 01:41:44 2013 -0400
>> >>
>> >>
>> >> and seeing this when I do a "test-only FileServerSuite":
>> >>
>> >> 13/10/30 09:35:04.300 INFO DAGScheduler: Completed ResultTask(0, 0)
>> >> 13/10/30 09:35:04.307 INFO LocalTaskSetManager: Loss was due to
>> >> java.io.StreamCorruptedException
>> >> java.io.StreamCorruptedException: invalid type code: AC
>> >>         at
>> >> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
>> >>         at
>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
>> >>         at
>> >>
>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39)
>> >>         at
>> >>
>> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:101)
>> >>         at
>> >> org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
>> >>         at
>> scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)
>> >>         at
>> >>
>> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26)
>> >>         at
>> >>
>> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)
>> >>         at
>> >> org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:53)
>> >>         at
>> >>
>> org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:95)
>> >>         at
>> >>
>> org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:94)
>> >>         at
>> >>
>> org.apache.spark.rdd.MapPartitionsWithContextRDD.compute(MapPartitionsWithContextRDD.scala:40)
>> >>         at
>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
>> >>         at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
>> >>         at
>> >> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107)
>> >>         at org.apache.spark.scheduler.Task.run(Task.scala:53)
>> >>         at
>> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:212)
>> >>         at
>> >>
>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
>> >>         at
>> >>
>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
>> >>         at java.lang.Thread.run(Thread.java:680)
>> >>
>> >>
>> >> Anybody else seen this yet?
>> >>
>> >> I have a really simple PR and this fails without my change, so I may
>> >> go ahead and submit it anyways.
>> >>
>> >> --
>> >> --
>> >> Evan Chan
>> >> Staff Engineer
>> >> ev@ooyala.com  |
>> >>
>>
>>
>>
>> --
>> --
>> Evan Chan
>> Staff Engineer
>> ev@ooyala.com  |
>>



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

From dev-return-705-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov  4 00:05:40 2013
Return-Path: <dev-return-705-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 44D90C82E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Nov 2013 00:05:40 +0000 (UTC)
Received: (qmail 66260 invoked by uid 500); 4 Nov 2013 00:05:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 66214 invoked by uid 500); 4 Nov 2013 00:05:40 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 66206 invoked by uid 99); 4 Nov 2013 00:05:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 00:05:40 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.215.43 as permitted sender)
Received: from [209.85.215.43] (HELO mail-la0-f43.google.com) (209.85.215.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 00:05:36 +0000
Received: by mail-la0-f43.google.com with SMTP id el20so4981809lab.30
        for <dev@spark.incubator.apache.org>; Sun, 03 Nov 2013 16:05:14 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=eRGCkkEf0i1QoJwhkx9GEQd2RRZ3hzuuA5xk1yx6Yvg=;
        b=kGDean3kGAMZu3/bH/g0NdqOSo1g7On4vR/T3ZSmxVCZC3YlFC/o2IcZ+QER1JR7Dj
         aas+75wClA+5YczLh9UptiL49jvxEil4LPeXBjPqluVE4XkC8QJlxAr0YNzM572NNxwc
         wjfXjH9ky6izodlAAPLmRF3PWf57EYqhS5JfY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=eRGCkkEf0i1QoJwhkx9GEQd2RRZ3hzuuA5xk1yx6Yvg=;
        b=QLaDTmUqKITLtW3CHQCxn3Pg9Jaq7cz3W8zXTsetGrKzO5SFlDfDlKhNr5KJybw6jT
         829mBbsAl2Pb/eLoC3n8Q1Q9cgx6TMQAqSrMzXJmoUmwEBl91gNQmIikiwAn80x98KdP
         Tnrej/Adwb840LZupukzkCOSdvYa3HRMdK4gos/Ho7aPW2MAIxxR6IAdw38E2DpiWfvt
         W7hUsjqV3vsH+/hJ+qv8EWj/Udjek4E5cgto/qztPXWjfrUUM5e509wJwaiJSwvSH7C0
         qQVArU7qhfcQ+pcZIZG8VSWtdzT80tcZsQigXlhyme6phTJXQVKxgSQsaMga7dTi/Lwe
         KHVw==
X-Gm-Message-State: ALoCoQmqgSSObAcsjN7cTJijgMWfEy16qn85jl8+QChvkoW8yN4OwhZI07/+bjElmKle7Fe+nfVq
MIME-Version: 1.0
X-Received: by 10.112.210.197 with SMTP id mw5mr281576lbc.42.1383523514817;
 Sun, 03 Nov 2013 16:05:14 -0800 (PST)
Received: by 10.112.94.70 with HTTP; Sun, 3 Nov 2013 16:05:14 -0800 (PST)
In-Reply-To: <CADWPM3i8R2wvQdXxduhydF1iFNnW6KzuHv9ArPQe+VS8BSHgNg@mail.gmail.com>
References: <CADWPM3j3wzLJ96_SvTVAEF-EMrDAxFAuOG5fubfZ7qzvBRhobA@mail.gmail.com>
	<CAOEPXP4uWz8=T7L_tFN4bUPC5qYGrmDYqCCd-nOdS6mO+qbOcA@mail.gmail.com>
	<CADWPM3jDBaDWNSGfDVJC-DS11S_rmiReUHq-=i-ghKLkr1iCZw@mail.gmail.com>
	<CAAsvFPm9TB3ubihBLRaPWTZDEQvmc2rtw8TH_sgePePDDbX+WQ@mail.gmail.com>
	<CADWPM3i8R2wvQdXxduhydF1iFNnW6KzuHv9ArPQe+VS8BSHgNg@mail.gmail.com>
Date: Sun, 3 Nov 2013 16:05:14 -0800
Message-ID: <CADWPM3jc3QBhJEFswLO4oSUWo0FWTzbVmEHBMeYxRCQwruDTWQ@mail.gmail.com>
Subject: Re: Getting failures in FileServerSuite
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Oops, hit enter too soon.

Mark: 1.6.0_51.   I'm hesitant to upgrade to JDK 1.7, as several folks
reported problems on OSX.   Also, I have no problems building the
assembly, and it only takes me about 2 minutes (I'm running on SSD's
though  :)

I might bite the bullet and upgrade to JDK 1.7 for other reasons though.

Patrick:  I know the branch with my config overhaul, last merged from
master Oct-10th, doesn't exhibit this problem.  (Note that I tend to
set SPARK_LOCAL_IP to "localhost", which I don't think affects this,
and it fails either with it set or not, i believe)  We could
potentially run a git bisect starting roughly 2-3 weeks ago.



On Sun, Nov 3, 2013 at 3:55 PM, Evan Chan <ev@ooyala.com> wrote:
> Mark:  I'm using JDK 1.6. --
>
> On Wed, Oct 30, 2013 at 1:44 PM, Mark Hamstra <mark@clearstorydata.com> wrote:
>> What JDK version on you using, Evan?
>>
>> I tried to reproduce your problem earlier today, but I wasn't even able to
>> get through the assembly build -- kept hanging when trying to build the
>> examples assembly.  Foregoing the assembly and running the tests would hang
>> on FileServerSuite "Dynamically adding JARS locally" -- no stack trace,
>> just hung.  And I was actually seeing a very similar stack trace to yours
>> from a test suite of our own running against 0.8.1-SNAPSHOT -- not exactly
>> the same because line numbers were different once it went into the java
>> runtime, and it eventually ended up someplace a little different.  That got
>> me curious about differences in Java versions, so I updated to the latest
>> Oracle release (1.7.0_45).  Now it cruises right through the build and test
>> of Spark master from before Matei merged your PR.  Then I logged into a
>> machine that has 1.7.0_15 (7u15-2.3.7-0ubuntu1~11.10.1, actually)
>> installed, and I'm right back to the hanging during the examples assembly
>> (but passes FileServerSuite, oddly enough.)  Upgrading the JDK didn't
>> improve the results of the ClearStory test suite I was looking at, so my
>> misery isn't over; but yours might be with a newer JDK....
>>
>>
>>
>> On Wed, Oct 30, 2013 at 12:44 PM, Evan Chan <ev@ooyala.com> wrote:
>>
>>> Must be a local environment thing, because AmpLab Jenkins can't
>>> reproduce it..... :-p
>>>
>>> On Wed, Oct 30, 2013 at 11:10 AM, Josh Rosen <rosenville@gmail.com> wrote:
>>> > Someone on the users list also encountered this exception:
>>> >
>>> >
>>> https://mail-archives.apache.org/mod_mbox/incubator-spark-user/201310.mbox/%3C64474308D680D540A4D8151B0F7C03F7025EF289%40SHSMSX104.ccr.corp.intel.com%3E
>>> >
>>> >
>>> > On Wed, Oct 30, 2013 at 9:40 AM, Evan Chan <ev@ooyala.com> wrote:
>>> >
>>> >> I'm at the latest
>>> >>
>>> >> commit f0e23a023ce1356bc0f04248605c48d4d08c2d05
>>> >> Merge: aec9bf9 a197137
>>> >> Author: Reynold Xin <rxin@apache.org>
>>> >> Date:   Tue Oct 29 01:41:44 2013 -0400
>>> >>
>>> >>
>>> >> and seeing this when I do a "test-only FileServerSuite":
>>> >>
>>> >> 13/10/30 09:35:04.300 INFO DAGScheduler: Completed ResultTask(0, 0)
>>> >> 13/10/30 09:35:04.307 INFO LocalTaskSetManager: Loss was due to
>>> >> java.io.StreamCorruptedException
>>> >> java.io.StreamCorruptedException: invalid type code: AC
>>> >>         at
>>> >> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1353)
>>> >>         at
>>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:348)
>>> >>         at
>>> >>
>>> org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:39)
>>> >>         at
>>> >>
>>> org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:101)
>>> >>         at
>>> >> org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:71)
>>> >>         at
>>> scala.collection.Iterator$$anon$21.hasNext(Iterator.scala:440)
>>> >>         at
>>> >>
>>> org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:26)
>>> >>         at
>>> >>
>>> org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:27)
>>> >>         at
>>> >> org.apache.spark.Aggregator.combineCombinersByKey(Aggregator.scala:53)
>>> >>         at
>>> >>
>>> org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:95)
>>> >>         at
>>> >>
>>> org.apache.spark.rdd.PairRDDFunctions$$anonfun$combineByKey$2.apply(PairRDDFunctions.scala:94)
>>> >>         at
>>> >>
>>> org.apache.spark.rdd.MapPartitionsWithContextRDD.compute(MapPartitionsWithContextRDD.scala:40)
>>> >>         at
>>> org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:237)
>>> >>         at org.apache.spark.rdd.RDD.iterator(RDD.scala:226)
>>> >>         at
>>> >> org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:107)
>>> >>         at org.apache.spark.scheduler.Task.run(Task.scala:53)
>>> >>         at
>>> >> org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:212)
>>> >>         at
>>> >>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
>>> >>         at
>>> >>
>>> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
>>> >>         at java.lang.Thread.run(Thread.java:680)
>>> >>
>>> >>
>>> >> Anybody else seen this yet?
>>> >>
>>> >> I have a really simple PR and this fails without my change, so I may
>>> >> go ahead and submit it anyways.
>>> >>
>>> >> --
>>> >> --
>>> >> Evan Chan
>>> >> Staff Engineer
>>> >> ev@ooyala.com  |
>>> >>
>>>
>>>
>>>
>>> --
>>> --
>>> Evan Chan
>>> Staff Engineer
>>> ev@ooyala.com  |
>>>
>
>
>
> --
> --
> Evan Chan
> Staff Engineer
> ev@ooyala.com  |



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

From dev-return-706-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov  4 07:46:45 2013
Return-Path: <dev-return-706-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3567A101CE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Nov 2013 07:46:45 +0000 (UTC)
Received: (qmail 25637 invoked by uid 500); 4 Nov 2013 07:46:44 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25350 invoked by uid 500); 4 Nov 2013 07:46:40 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 25336 invoked by uid 99); 4 Nov 2013 07:46:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 07:46:39 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of raymond.liu@intel.com designates 143.182.124.21 as permitted sender)
Received: from [143.182.124.21] (HELO mga03.intel.com) (143.182.124.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 07:46:33 +0000
Received: from azsmga001.ch.intel.com ([10.2.17.19])
  by azsmga101.ch.intel.com with ESMTP; 03 Nov 2013 23:46:11 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,535,1378882800"; 
   d="scan'208";a="384275171"
Received: from fmsmsx107.amr.corp.intel.com ([10.19.9.54])
  by azsmga001.ch.intel.com with ESMTP; 03 Nov 2013 23:46:11 -0800
Received: from shsmsx104.ccr.corp.intel.com (10.239.4.70) by
 FMSMSX107.amr.corp.intel.com (10.19.9.54) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Sun, 3 Nov 2013 23:46:10 -0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.240]) by
 SHSMSX104.ccr.corp.intel.com ([169.254.5.244]) with mapi id 14.03.0123.003;
 Mon, 4 Nov 2013 15:46:09 +0800
From: "Liu, Raymond" <raymond.liu@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: issue regarding akka, protobuf and Hadoop version
Thread-Topic: issue regarding akka, protobuf and Hadoop version
Thread-Index: Ac7ZMduY/ltyQ77yToubUDTRusm1uw==
Date: Mon, 4 Nov 2013 07:46:08 +0000
Message-ID: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi

	I am working on porting spark onto Hadoop 2.2.0, With some renaming and ca=
ll into new YARN API works done. I can run up the spark master. While I enc=
ounter the issue that Executor Actor could not connecting to Driver actor.

	After some investigation, I found the root cause is that the akka-remote d=
o not support protobuf 2.5.0 before 2.3. And hadoop move to protobuf 2.5.0 =
from 2.1-beta.

	The issue is that if I exclude the akka dependency from hadoop and force p=
rotobuf dependency to 2.4.1, the compile/packing will fail since hadoop com=
mon jar require a new interface from protobuf 2.5.0.

	 So any suggestion on this?

Best Regards,
Raymond Liu

From dev-return-707-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov  4 09:04:39 2013
Return-Path: <dev-return-707-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3717F1038C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Nov 2013 09:04:39 +0000 (UTC)
Received: (qmail 11943 invoked by uid 500); 4 Nov 2013 09:04:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 11865 invoked by uid 500); 4 Nov 2013 09:04:33 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 11855 invoked by uid 99); 4 Nov 2013 09:04:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 09:04:32 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of raymond.liu@intel.com designates 143.182.124.37 as permitted sender)
Received: from [143.182.124.37] (HELO mga14.intel.com) (143.182.124.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 09:04:27 +0000
Received: from azsmga001.ch.intel.com ([10.2.17.19])
  by azsmga102.ch.intel.com with ESMTP; 04 Nov 2013 01:04:06 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,535,1378882800"; 
   d="scan'208";a="384298751"
Received: from fmsmsx106.amr.corp.intel.com ([10.19.9.37])
  by azsmga001.ch.intel.com with ESMTP; 04 Nov 2013 01:04:05 -0800
Received: from fmsmsx111.amr.corp.intel.com (10.18.116.5) by
 FMSMSX106.amr.corp.intel.com (10.19.9.37) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Mon, 4 Nov 2013 01:04:05 -0800
Received: from shsmsx103.ccr.corp.intel.com (10.239.4.69) by
 fmsmsx111.amr.corp.intel.com (10.18.116.5) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Mon, 4 Nov 2013 01:04:05 -0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.240]) by
 SHSMSX103.ccr.corp.intel.com ([169.254.4.14]) with mapi id 14.03.0123.003;
 Mon, 4 Nov 2013 17:04:04 +0800
From: "Liu, Raymond" <raymond.liu@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: what's the strategy for code sync between branches e.g. scala-2.10
 v.s. master?
Thread-Topic: what's the strategy for code sync between branches e.g.
 scala-2.10 v.s. master?
Thread-Index: Ac7ZPM6CNLAfGqLDSGWe7h58op6M4w==
Date: Mon, 4 Nov 2013 09:04:03 +0000
Message-ID: <391D65D0EBFC9B4B95E117F72A360F1A010CDD44@SHSMSX101.ccr.corp.intel.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi
	It seems to me that dev branches are sync with master by keep merging trun=
k codes. E.g. scala-2.10 branches continuously merge latest master code int=
o itself for update.

	While I am wondering, what's the general guide line on doing this? It seem=
s to me that not every code in master are merged into scala-2.10 branch. Sa=
y, on OCT 10, there are a merge from master to scala-2.10 branch. While som=
e commit in OCT.4 not included. E.g. StandaloneX rename to CoarseGrainedX. =
So I am puzzled, how do we track which commit is already merged into scala-=
2.10 branch and which is not? And how do we plan to merge scala-2.10 branch=
 back to master? And is there any good way to find out that any changes are=
 done by 2.10 branch or by master through merge operation. It seems to me p=
retty hard to identify them and sync codes.

	It seems to me that a rebase on master won't lead to the above issues, sin=
ce all branch changes will stay on the top. So any reason that merging is c=
hosen instead of rebase other than not want a force update on checked out s=
ource?


Best Regards,
Raymond Liu



From dev-return-708-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov  4 17:09:59 2013
Return-Path: <dev-return-708-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B41A8103F8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Nov 2013 17:09:59 +0000 (UTC)
Received: (qmail 13525 invoked by uid 500); 4 Nov 2013 17:09:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13256 invoked by uid 500); 4 Nov 2013 17:09:53 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 13247 invoked by uid 99); 4 Nov 2013 17:09:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 17:09:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of umarj.javed@gmail.com designates 209.85.212.51 as permitted sender)
Received: from [209.85.212.51] (HELO mail-vb0-f51.google.com) (209.85.212.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 17:09:45 +0000
Received: by mail-vb0-f51.google.com with SMTP id w5so1540739vbf.24
        for <dev@spark.incubator.apache.org>; Mon, 04 Nov 2013 09:09:24 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=MjU5xCR/HalulbO5WoacFpgI4YvqDxBaBT2V35zp6iE=;
        b=y/a4EZNddLKRUmd0HJ7fcvwgcB83VH5yIdb+trJhHuSECPSNf31SgolHQFOUYhirup
         MTVK3hituNd02m1NN8Nm1GqU6FkHSHoaaorjdNrs54TsdKWIAnZujSWp9IGMsTHuF5wv
         /fMCZzosQHRyjIKJwWClh6GfQYoljuLnN5ENuVFCd041bb/t7xeesAAlr/slyi6sRua4
         PO5M9UcbB7Rsc+HAlaBmE9P0TF3sN4nRi2Yjexo6D80dK93x8kdOyBwbRv87cdkW7UAv
         f5Z5fkkFhrM2WrKhbr3vTK02yNyRnRVOgLseVNwXBGz9kGiIuvvefIVTVdESlpHk8c8r
         7/Sg==
MIME-Version: 1.0
X-Received: by 10.52.118.73 with SMTP id kk9mr10210456vdb.13.1383584964091;
 Mon, 04 Nov 2013 09:09:24 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Mon, 4 Nov 2013 09:09:24 -0800 (PST)
Date: Mon, 4 Nov 2013 09:09:24 -0800
Message-ID: <CACwKa9dXFy1YM27DEK=KprOUEr038py9Ymn8mxE36vmpEuKrOw@mail.gmail.com>
Subject: hadoop configuration
From: Umar Javed <umarj.javed@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e0122f0f24d24ab04ea5cfa57
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0122f0f24d24ab04ea5cfa57
Content-Type: text/plain; charset=ISO-8859-1

In 'SparkHadoopUtil.scala'
in /core/src/main/scala/org/apache/spark/deploy/, there is a method:

 def newConfiguration(): Configuration = new Configuration()

There is a header that imports Configuration : import
org.apache.hadoop.conf.Configuration

But I'm unable to find the definition of Configuration under
/core/src/main/scala/org/apache/hadoop/
The only subdirectories in this directory are mapred and mapreduce. Does
anybody know where 'Configuration' is defined?

--089e0122f0f24d24ab04ea5cfa57--

From dev-return-709-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov  4 17:10:00 2013
Return-Path: <dev-return-709-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 15D69103F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Nov 2013 17:10:00 +0000 (UTC)
Received: (qmail 13636 invoked by uid 500); 4 Nov 2013 17:09:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13280 invoked by uid 500); 4 Nov 2013 17:09:58 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 13258 invoked by uid 99); 4 Nov 2013 17:09:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 17:09:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.214.43] (HELO mail-bk0-f43.google.com) (209.85.214.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 17:09:52 +0000
Received: by mail-bk0-f43.google.com with SMTP id mz11so2517476bkb.30
        for <dev@spark.incubator.apache.org>; Mon, 04 Nov 2013 09:09:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=ySZ/S678PIMxixe5sgH5iG2++jZ5JUlf7dPozuNHsAY=;
        b=k2GIyygVQaL5ZPP2kYL39RVvXMQ+7C4/a1nw8bRDpWikGtvECuNa//Qa7cfPvQ7sIH
         pDtt9drxYepjbDOBKwUMf+N4Y2TrrOiVXtrOgPMNJ2DHYeT1Ed6jdJDfBe6jiRJ8608q
         5IMGPVZadVPSR+VG3mmSiiMk+9EovXkv6Tn45/1k3OKUyI0sEJrNxYW5WTBIihPfWCmx
         PAwFP/MEgAIrdZ9ulZmD3CcCfQWGc6mEJkM/6BAMS1i3ww4v+8dHQUGUNTcOirbREnc0
         Dq+tNANiKXtP3xDmFHwyvjoKQq4yBdOnpYo4EPJfyC75J+R5jH+AfD8Ff+Qgj9pNW/ai
         yq6w==
X-Gm-Message-State: ALoCoQkIpxsTsSfMPC7BlxqbxIoJKk0sr5eFIP1GvC3G0v3pxoCafiOTFEOsyolQaZjGP7kCVoqF
MIME-Version: 1.0
X-Received: by 10.205.64.16 with SMTP id xg16mr672203bkb.42.1383584970549;
 Mon, 04 Nov 2013 09:09:30 -0800 (PST)
Received: by 10.204.101.201 with HTTP; Mon, 4 Nov 2013 09:09:30 -0800 (PST)
In-Reply-To: <391D65D0EBFC9B4B95E117F72A360F1A010CDD44@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDD44@SHSMSX101.ccr.corp.intel.com>
Date: Mon, 4 Nov 2013 09:09:30 -0800
Message-ID: <CAAsvFPnJm_b3-tS4xejyg-4FOPaWD2fAJCaWGinGPGU4_70ZjA@mail.gmail.com>
Subject: Re: what's the strategy for code sync between branches e.g.
 scala-2.10 v.s. master?
From: Mark Hamstra <mark@clearstorydata.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=bcaec53f8b8bb02a6b04ea5cfaa2
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec53f8b8bb02a6b04ea5cfaa2
Content-Type: text/plain; charset=ISO-8859-1

Rebasing changes the SHAs, which isn't a good idea in a public and
heavily-used repository.


On Mon, Nov 4, 2013 at 1:04 AM, Liu, Raymond <raymond.liu@intel.com> wrote:

> Hi
>         It seems to me that dev branches are sync with master by keep
> merging trunk codes. E.g. scala-2.10 branches continuously merge latest
> master code into itself for update.
>
>         While I am wondering, what's the general guide line on doing this?
> It seems to me that not every code in master are merged into scala-2.10
> branch. Say, on OCT 10, there are a merge from master to scala-2.10 branch.
> While some commit in OCT.4 not included. E.g. StandaloneX rename to
> CoarseGrainedX. So I am puzzled, how do we track which commit is already
> merged into scala-2.10 branch and which is not? And how do we plan to merge
> scala-2.10 branch back to master? And is there any good way to find out
> that any changes are done by 2.10 branch or by master through merge
> operation. It seems to me pretty hard to identify them and sync codes.
>
>         It seems to me that a rebase on master won't lead to the above
> issues, since all branch changes will stay on the top. So any reason that
> merging is chosen instead of rebase other than not want a force update on
> checked out source?
>
>
> Best Regards,
> Raymond Liu
>
>
>

--bcaec53f8b8bb02a6b04ea5cfaa2--

From dev-return-710-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov  4 17:43:31 2013
Return-Path: <dev-return-710-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8B70410527
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon,  4 Nov 2013 17:43:31 +0000 (UTC)
Received: (qmail 70561 invoked by uid 500); 4 Nov 2013 17:43:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70090 invoked by uid 500); 4 Nov 2013 17:43:28 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 69924 invoked by uid 99); 4 Nov 2013 17:43:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 17:43:26 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hdc1112@gmail.com designates 209.85.212.46 as permitted sender)
Received: from [209.85.212.46] (HELO mail-vb0-f46.google.com) (209.85.212.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 04 Nov 2013 17:43:21 +0000
Received: by mail-vb0-f46.google.com with SMTP id 10so1568315vbe.5
        for <dev@spark.incubator.apache.org>; Mon, 04 Nov 2013 09:42:59 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=9LHDBvjcu7mT1NolHpOTSK7YOHv0GPcNGb6W+eeSEoc=;
        b=oV2c0M1HJyUoPJ9cwb2xxvasvNZjO0PXECy9uM/zkC8T70akmrHBgr5kYyXVbS4xuz
         GDbldn+HyzJesriqYal8G3tr9S18I/IKT/0jDfXJ5xWjrKScgQq9Ny4bnyxA7qzGUYLl
         q9ezmYdBF/vi0+hr+kdW1K950KcLkgDYtzKL/efTRkHeLQP3Mz8csx4VyCcNgl5QX58s
         z71YTTmtEwzt5dCdrj/b+NN08WiA9X4qXheXAtLGXbYksRh7DIH4p5H6BKieKzO1tYVv
         28xd8XYGC0jS/7wD4dC65c4ItyQEUPTBiULxNj8viSHVeemq35V43sLOCuSu7KUU5T1/
         f2KA==
MIME-Version: 1.0
X-Received: by 10.220.47.10 with SMTP id l10mr676441vcf.32.1383586979447; Mon,
 04 Nov 2013 09:42:59 -0800 (PST)
Received: by 10.58.211.99 with HTTP; Mon, 4 Nov 2013 09:42:59 -0800 (PST)
In-Reply-To: <CACwKa9dXFy1YM27DEK=KprOUEr038py9Ymn8mxE36vmpEuKrOw@mail.gmail.com>
References: <CACwKa9dXFy1YM27DEK=KprOUEr038py9Ymn8mxE36vmpEuKrOw@mail.gmail.com>
Date: Mon, 4 Nov 2013 12:42:59 -0500
Message-ID: <CAAzo=r8CjNJ+R=sDDKerGv3Hg=Q3gzrvDFVYcQD+owc8wGMeRA@mail.gmail.com>
Subject: Re: hadoop configuration
From: dachuan <hdc1112@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c2a58c6d0c0804ea5d72ef
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2a58c6d0c0804ea5d72ef
Content-Type: text/plain; charset=ISO-8859-1

I guess it's defined in Hadoop library. You can try to download the Hadoop
source code, or use some IDE to solve the dependency issue automatically, I
am using IntelliJ Idea community version.


On Mon, Nov 4, 2013 at 12:09 PM, Umar Javed <umarj.javed@gmail.com> wrote:

> In 'SparkHadoopUtil.scala'
> in /core/src/main/scala/org/apache/spark/deploy/, there is a method:
>
>  def newConfiguration(): Configuration = new Configuration()
>
> There is a header that imports Configuration : import
> org.apache.hadoop.conf.Configuration
>
> But I'm unable to find the definition of Configuration under
> /core/src/main/scala/org/apache/hadoop/
> The only subdirectories in this directory are mapred and mapreduce. Does
> anybody know where 'Configuration' is defined?
>



-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210

--001a11c2a58c6d0c0804ea5d72ef--

From dev-return-711-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 00:34:21 2013
Return-Path: <dev-return-711-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 01ECC107A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 00:34:21 +0000 (UTC)
Received: (qmail 63405 invoked by uid 500); 5 Nov 2013 00:34:20 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 63359 invoked by uid 500); 5 Nov 2013 00:34:20 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 63351 invoked by uid 99); 5 Nov 2013 00:34:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 00:34:20 +0000
X-ASF-Spam-Status: No, hits=-1997.8 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Tue, 05 Nov 2013 00:34:19 +0000
Received: (qmail 62886 invoked by uid 99); 5 Nov 2013 00:33:59 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 00:33:59 +0000
Received: from localhost (HELO mail-ve0-f170.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 00:33:59 +0000
Received: by mail-ve0-f170.google.com with SMTP id oy12so2009903veb.15
        for <dev@spark.incubator.apache.org>; Mon, 04 Nov 2013 16:33:57 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=fsxyMrGNJ+ku3kJ80rYUGCiEjx6WhXHmhwJLtixVtMA=;
        b=AbZ3gE8vLsfHVU2xSSKh7EbsIEvE96VF98uLlO4nnRyYujWF2LSgCBOsgicjioT9Ek
         6YhgpRseia/2N2EX2YOYIM0QfODwRO6zvWE2dYcXogVH6fkSVg2Tr+hSBnjqcsdE7zpI
         teksmS2tUfS2pT83rUqMdO5WtbwMB9be7RugTeUuPXpsi2Yq+ivdPvD8Ndc9w/EtP90p
         byewzqktFEeJn5c5Mb571LZl5K9h75tggGZUBK7BCkWjKJP6TEHhCA8e1R7RxLlp5tEk
         nIYoLo2yItGHg8GE5yq+7pxLMOTj7EellMfNYX6id5PTJB01FS3z5r/fJAzqLbuY0cqa
         nwoQ==
X-Received: by 10.220.145.75 with SMTP id c11mr216982vcv.30.1383611637952;
 Mon, 04 Nov 2013 16:33:57 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Mon, 4 Nov 2013 16:33:37 -0800 (PST)
In-Reply-To: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
From: Reynold Xin <rxin@apache.org>
Date: Mon, 4 Nov 2013 16:33:37 -0800
Message-ID: <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
Subject: Re: issue regarding akka, protobuf and Hadoop version
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b3439702ff85304ea6330c6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3439702ff85304ea6330c6
Content-Type: text/plain; charset=UTF-8

I chatted with Matt Massie about this, and here are some options:

1. Use dependency injection in google-guice to make Akka use one version of
protobuf, and YARN use the other version.

2. Look into OSGi to accomplish the same goal.

3. Rewrite the messaging part of Spark to use a simple, custom RPC library
instead of Akka. We are really only using a very simple subset of Akka
features, and we can probably implement a simple RPC library tailored for
Spark quickly. We should only do this as the last resort.

4. Talk to Akka guys and hope they can make a maintenance release of Akka
that supports protobuf 2.5.


None of these are ideal, but we'd have to pick one. It would be great if
you have other suggestions.


On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond <raymond.liu@intel.com> wrote:

> Hi
>
>         I am working on porting spark onto Hadoop 2.2.0, With some
> renaming and call into new YARN API works done. I can run up the spark
> master. While I encounter the issue that Executor Actor could not
> connecting to Driver actor.
>
>         After some investigation, I found the root cause is that the
> akka-remote do not support protobuf 2.5.0 before 2.3. And hadoop move to
> protobuf 2.5.0 from 2.1-beta.
>
>         The issue is that if I exclude the akka dependency from hadoop and
> force protobuf dependency to 2.4.1, the compile/packing will fail since
> hadoop common jar require a new interface from protobuf 2.5.0.
>
>          So any suggestion on this?
>
> Best Regards,
> Raymond Liu
>

--047d7b3439702ff85304ea6330c6--

From dev-return-712-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 00:45:30 2013
Return-Path: <dev-return-712-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 06CB5107F8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 00:45:30 +0000 (UTC)
Received: (qmail 80207 invoked by uid 500); 5 Nov 2013 00:45:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 80173 invoked by uid 500); 5 Nov 2013 00:45:29 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 80163 invoked by uid 99); 5 Nov 2013 00:45:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 00:45:29 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of meisam.fathi@gmail.com designates 209.85.128.171 as permitted sender)
Received: from [209.85.128.171] (HELO mail-ve0-f171.google.com) (209.85.128.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 00:45:23 +0000
Received: by mail-ve0-f171.google.com with SMTP id pa12so2017008veb.16
        for <dev@spark.incubator.apache.org>; Mon, 04 Nov 2013 16:45:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=cu2VVTQL9KK8QtUS9jtpgj6zhjN8y+wJxb/syCdJzgI=;
        b=Jv0P5DJoqbdy8ZXs+V2qPr1YYnDNIgs+QG+uZR6+JbK46pNMxtHgnxtqEd3Ix4A/0a
         p4HMxMcE0NHY5g0Lgeg+3SY8nXiSPagzO4azwYgOimnWbfv/JcXbz61NNW85z4fNtB8e
         M2FMNZcTWkMv1kIgoyvFJbqAukBMgI4lM4FhhbsKfHuDNoY0DxjrMQoiEZjA7rocplpY
         LBU6XOdZokwCa4B3AvVhJ7Dc0vrUh5/V/YeFGhU4cSd5DE7GT/ruH/1fe/Jpq+r0/ltr
         Mw7huJcPCuq78mRFcNHtB1VgO7jIbH0P1zSlAygNmAyfv3bt0IUMmCFFoNeWVBIafAEJ
         fCTQ==
MIME-Version: 1.0
X-Received: by 10.221.54.129 with SMTP id vu1mr5505625vcb.20.1383612302519;
 Mon, 04 Nov 2013 16:45:02 -0800 (PST)
Received: by 10.52.231.9 with HTTP; Mon, 4 Nov 2013 16:45:02 -0800 (PST)
Date: Mon, 4 Nov 2013 19:45:02 -0500
Message-ID: <CAByMnGsY7GfG=Lf-hRoMiJpZB81P=tSAMpyzs6UFD=2g=Z41fA@mail.gmail.com>
Subject: Spark job terminates with an error message but the results seem to be correct
From: Meisam Fathi <meisam.fathi@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Community,

I checked out Spark v0.7.3 tag from github and I am the using it in
standalone mode on a small cluster with 2 nodes. My Spark application
reads files from hdfs and I writes the results back to hdfs.
Everything seems to be working fine except that at the very end of
execution I get this line on my log files:

ERROR executor.StandaloneExecutorBackend: Driver or worker
disconnected! Shutting down.

The last line in my spark application calls rdd.saveAsTextFile to save
the results to hdfs. It seems that it this call works fine because
hadoop _SUCCESS files are being generated on my hdfs.

When I run the same task in spark-repl on one node, I don't get the
error line. I've compared the output from spark-repl with the output
from my spark application and there is no difference.

Looking at StandaloneExecutorBackend code, it seems that
StandaloneExecutorBackend should receive a StopExecutor message but it
is getting a Terminated or RemoteClientDisconnected or
RemoteClientShutdown message.

Is it normal for Spark applications to terminated abruptly at the end
of their execution? Or am I missing something? or is it this the
intended behavior of Spark with two nodes in standalone mode?

Thanks,
Meisam

From dev-return-713-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 01:06:18 2013
Return-Path: <dev-return-713-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B11BD108CA
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 01:06:18 +0000 (UTC)
Received: (qmail 10403 invoked by uid 500); 5 Nov 2013 01:06:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10352 invoked by uid 500); 5 Nov 2013 01:06:18 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 10341 invoked by uid 99); 5 Nov 2013 01:06:18 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 01:06:18 +0000
X-ASF-Spam-Status: No, hits=-1997.8 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Tue, 05 Nov 2013 01:06:17 +0000
Received: (qmail 10012 invoked by uid 99); 5 Nov 2013 01:05:57 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 01:05:57 +0000
Received: from localhost (HELO mail-ve0-f181.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 01:05:56 +0000
Received: by mail-ve0-f181.google.com with SMTP id jz11so1977386veb.26
        for <dev@spark.incubator.apache.org>; Mon, 04 Nov 2013 17:05:55 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=cNyVVZ3IZWQ4dR4qz1rIiNcptHqh13k1Jb3Q17cV0Xg=;
        b=QLou6mcgTumTll37ALp/WC7l30CbrfGYAbaFAK9GGts3lJyBvO+R+O6o1xWt3kCeZI
         ftb81YrrryMIGuG7os0xica0p54jyH8kjruYnaDs2R4dRFc05hCnI4zOlj/0+xlXfNCJ
         cHFLrgRs3ILsQrdJVTKDMNJV8kD/vIXGDWUp5h4lYgnMNvcPS4vYH74djw7lAnlEdZir
         o1PctTpUGoNI9qzuU46k5aDyT6A60Cz4x+xoiflNOeJrLhPEAvGMTprM0T2dm0JnIuCR
         zSW8s9hDeeo2DFRRila3l5Vg69Tp8vEJ46QMU3JUDQhAB3eXvzkivTeB9yP+gmJgN1Pr
         q9AQ==
X-Received: by 10.220.147.20 with SMTP id j20mr5198512vcv.21.1383613555211;
 Mon, 04 Nov 2013 17:05:55 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Mon, 4 Nov 2013 17:05:35 -0800 (PST)
In-Reply-To: <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
From: Reynold Xin <rxin@apache.org>
Date: Mon, 4 Nov 2013 17:05:35 -0800
Message-ID: <CAC1ssC5C3N2rwcpvLO6wbg_4j4z6K6bnP+RENRgc1V5EkcF=wQ@mail.gmail.com>
Subject: Re: issue regarding akka, protobuf and Hadoop version
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Cc: Matt Massie <massie@cs.berkeley.edu>, Thomas Graves <tgraves@yahoo-inc.com>, 
	Harvey Feng <hyfeng224@gmail.com>
Content-Type: multipart/alternative; boundary=047d7b34364e77035404ea63a2b3
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b34364e77035404ea63a2b3
Content-Type: text/plain; charset=UTF-8

Adding in a few guys so they can chime in.


On Mon, Nov 4, 2013 at 4:33 PM, Reynold Xin <rxin@apache.org> wrote:

> I chatted with Matt Massie about this, and here are some options:
>
> 1. Use dependency injection in google-guice to make Akka use one version
> of protobuf, and YARN use the other version.
>
> 2. Look into OSGi to accomplish the same goal.
>
> 3. Rewrite the messaging part of Spark to use a simple, custom RPC library
> instead of Akka. We are really only using a very simple subset of Akka
> features, and we can probably implement a simple RPC library tailored for
> Spark quickly. We should only do this as the last resort.
>
> 4. Talk to Akka guys and hope they can make a maintenance release of Akka
> that supports protobuf 2.5.
>
>
> None of these are ideal, but we'd have to pick one. It would be great if
> you have other suggestions.
>
>
> On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond <raymond.liu@intel.com>wrote:
>
>> Hi
>>
>>         I am working on porting spark onto Hadoop 2.2.0, With some
>> renaming and call into new YARN API works done. I can run up the spark
>> master. While I encounter the issue that Executor Actor could not
>> connecting to Driver actor.
>>
>>         After some investigation, I found the root cause is that the
>> akka-remote do not support protobuf 2.5.0 before 2.3. And hadoop move to
>> protobuf 2.5.0 from 2.1-beta.
>>
>>         The issue is that if I exclude the akka dependency from hadoop
>> and force protobuf dependency to 2.4.1, the compile/packing will fail since
>> hadoop common jar require a new interface from protobuf 2.5.0.
>>
>>          So any suggestion on this?
>>
>> Best Regards,
>> Raymond Liu
>>
>
>

--047d7b34364e77035404ea63a2b3--

From dev-return-714-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 01:53:46 2013
Return-Path: <dev-return-714-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C04D510A17
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 01:53:46 +0000 (UTC)
Received: (qmail 70669 invoked by uid 500); 5 Nov 2013 01:53:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 70625 invoked by uid 500); 5 Nov 2013 01:53:45 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 70617 invoked by uid 99); 5 Nov 2013 01:53:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 01:53:45 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of raymond.liu@intel.com designates 192.55.52.93 as permitted sender)
Received: from [192.55.52.93] (HELO mga11.intel.com) (192.55.52.93)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 01:53:40 +0000
Received: from fmsmga002.fm.intel.com ([10.253.24.26])
  by fmsmga102.fm.intel.com with ESMTP; 04 Nov 2013 17:53:18 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,636,1378882800"; 
   d="scan'208";a="427929139"
Received: from fmsmsx106.amr.corp.intel.com ([10.19.9.37])
  by fmsmga002.fm.intel.com with ESMTP; 04 Nov 2013 17:53:18 -0800
Received: from fmsmsx154.amr.corp.intel.com (10.18.116.70) by
 FMSMSX106.amr.corp.intel.com (10.19.9.37) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Mon, 4 Nov 2013 17:53:18 -0800
Received: from shsmsx152.ccr.corp.intel.com (10.239.6.52) by
 FMSMSX154.amr.corp.intel.com (10.18.116.70) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Mon, 4 Nov 2013 17:53:18 -0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.240]) by
 SHSMSX152.ccr.corp.intel.com ([169.254.6.49]) with mapi id 14.03.0123.003;
 Tue, 5 Nov 2013 09:53:16 +0800
From: "Liu, Raymond" <raymond.liu@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: issue regarding akka, protobuf and Hadoop version
Thread-Topic: issue regarding akka, protobuf and Hadoop version
Thread-Index: Ac7ZMduY/ltyQ77yToubUDTRusm1uwAScDmAABLwNXA=
Date: Tue, 5 Nov 2013 01:53:15 +0000
Message-ID: <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
In-Reply-To: <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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==

From dev-return-715-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 02:07:27 2013
Return-Path: <dev-return-715-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5ADA210AAC
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 02:07:27 +0000 (UTC)
Received: (qmail 90712 invoked by uid 500); 5 Nov 2013 02:07:27 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90612 invoked by uid 500); 5 Nov 2013 02:07:27 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 90604 invoked by uid 99); 5 Nov 2013 02:07:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 02:07:27 +0000
X-ASF-Spam-Status: No, hits=-1997.8 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Tue, 05 Nov 2013 02:07:25 +0000
Received: (qmail 90188 invoked by uid 99); 5 Nov 2013 02:07:05 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 02:07:05 +0000
Received: from localhost (HELO mail-vc0-f175.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 02:07:05 +0000
Received: by mail-vc0-f175.google.com with SMTP id ht10so5070021vcb.6
        for <dev@spark.incubator.apache.org>; Mon, 04 Nov 2013 18:07:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=Sv8AU0wfGxenba9ym0rI/GX6UTVKGGb/n+2FEgWavxA=;
        b=XBQwz/L2qHSQ9anMgoH7t824w9v07IS2FTl54YfHSvUfLwb7fslK9ub07Cs4I25Bp9
         cqbg6Au8V9G0Lymj/fPrOYdm/7fPNyulbJ7vCMH9bb6IXXFwC4dw8ZWgFfrXjt+KROwY
         B/LYdWpvMCJZs0kgYDMSk8aTh20MpbHq6+WwKDQNqZerSA7UJVmhlgAEm3JqjjntvWae
         Cb3ceUqlD82Xup58S+hwxx/yGvOiAJmWHtJ6Z4wedgKxyP9HN87YA19G2gi9uZcoR46G
         sBgZ4HnZIWK4qgWAoPKWD5fmgU1igmfjKMPQ51XnHfMbelLIOZT6gOGEndIBejWjONIN
         EoJw==
X-Received: by 10.52.65.136 with SMTP id x8mr5510187vds.23.1383617224103; Mon,
 04 Nov 2013 18:07:04 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Mon, 4 Nov 2013 18:06:43 -0800 (PST)
In-Reply-To: <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com> <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
From: Reynold Xin <rxin@apache.org>
Date: Mon, 4 Nov 2013 18:06:43 -0800
Message-ID: <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com>
Subject: Re: issue regarding akka, protobuf and Hadoop version
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=bcaec5015f0b25e11d04ea647dbe
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec5015f0b25e11d04ea647dbe
Content-Type: text/plain; charset=UTF-8

I think we are near the end of Scala 2.9.3 development, and will merge the
Scala 2.10 branch into master and make it the future very soon (maybe next
week).  This problem will go away.

Meantime, we are relying on periodically merging the master into the Scala
2.10 branch.


On Mon, Nov 4, 2013 at 5:53 PM, Liu, Raymond <raymond.liu@intel.com> wrote:

> I plan to do the work on scala-2.10 branch, which already move to akka
> 2.2.3, hope that to move to akka 2.3-M1 (which support protobuf 2.5.x) will
> not cause many problem and make it a test to see is there further issues,
> then wait for the formal release of akka 2.3.x
>
> While the issue is that I can see many commits on master branch is not
> merged into scala-2.10 branch yet. The latest merge seems to happen on
> OCT.11, while as I mentioned in the dev branch merge/sync thread, seems
> that many earlier commit is not included and which will surely bring extra
> works on future code merging/rebase. So again, what's the code sync
> strategy and what's the plan of merge back into master?
>
> Best Regards,
> Raymond Liu
>
>
> -----Original Message-----
> From: Reynold Xin [mailto:rxin@apache.org]
> Sent: Tuesday, November 05, 2013 8:34 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: issue regarding akka, protobuf and Hadoop version
>
> I chatted with Matt Massie about this, and here are some options:
>
> 1. Use dependency injection in google-guice to make Akka use one version
> of protobuf, and YARN use the other version.
>
> 2. Look into OSGi to accomplish the same goal.
>
> 3. Rewrite the messaging part of Spark to use a simple, custom RPC library
> instead of Akka. We are really only using a very simple subset of Akka
> features, and we can probably implement a simple RPC library tailored for
> Spark quickly. We should only do this as the last resort.
>
> 4. Talk to Akka guys and hope they can make a maintenance release of Akka
> that supports protobuf 2.5.
>
>
> None of these are ideal, but we'd have to pick one. It would be great if
> you have other suggestions.
>
>
> On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond <raymond.liu@intel.com>
> wrote:
>
> > Hi
> >
> >         I am working on porting spark onto Hadoop 2.2.0, With some
> > renaming and call into new YARN API works done. I can run up the spark
> > master. While I encounter the issue that Executor Actor could not
> > connecting to Driver actor.
> >
> >         After some investigation, I found the root cause is that the
> > akka-remote do not support protobuf 2.5.0 before 2.3. And hadoop move
> > to protobuf 2.5.0 from 2.1-beta.
> >
> >         The issue is that if I exclude the akka dependency from hadoop
> > and force protobuf dependency to 2.4.1, the compile/packing will fail
> > since hadoop common jar require a new interface from protobuf 2.5.0.
> >
> >          So any suggestion on this?
> >
> > Best Regards,
> > Raymond Liu
> >
>

--bcaec5015f0b25e11d04ea647dbe--

From dev-return-716-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 08:12:47 2013
Return-Path: <dev-return-716-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6544F1035A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 08:12:47 +0000 (UTC)
Received: (qmail 99098 invoked by uid 500); 5 Nov 2013 08:12:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98806 invoked by uid 500); 5 Nov 2013 08:12:43 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 98798 invoked by uid 99); 5 Nov 2013 08:12:42 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 08:12:42 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of raymond.liu@intel.com designates 143.182.124.21 as permitted sender)
Received: from [143.182.124.21] (HELO mga03.intel.com) (143.182.124.21)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 08:12:38 +0000
Received: from azsmga001.ch.intel.com ([10.2.17.19])
  by azsmga101.ch.intel.com with ESMTP; 05 Nov 2013 00:12:10 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,535,1378882800"; 
   d="scan'208";a="384780185"
Received: from fmsmsx105.amr.corp.intel.com ([10.19.9.36])
  by azsmga001.ch.intel.com with ESMTP; 05 Nov 2013 00:12:06 -0800
Received: from fmsmsx113.amr.corp.intel.com (10.18.116.7) by
 FMSMSX105.amr.corp.intel.com (10.19.9.36) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Tue, 5 Nov 2013 00:12:06 -0800
Received: from shsmsx103.ccr.corp.intel.com (10.239.4.69) by
 FMSMSX113.amr.corp.intel.com (10.18.116.7) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Tue, 5 Nov 2013 00:12:06 -0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.240]) by
 SHSMSX103.ccr.corp.intel.com ([169.254.4.14]) with mapi id 14.03.0123.003;
 Tue, 5 Nov 2013 16:12:04 +0800
From: "Liu, Raymond" <raymond.liu@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: issue regarding akka, protobuf and Hadoop version
Thread-Topic: issue regarding akka, protobuf and Hadoop version
Thread-Index: Ac7ZMduY/ltyQ77yToubUDTRusm1uwAScDmAABLwNXD//4KBgP//FKCA
Date: Tue, 5 Nov 2013 08:12:03 +0000
Message-ID: <391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com>
In-Reply-To: <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="utf-8"
Content-Transfer-Encoding: base64
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org
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From dev-return-717-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 18:45:16 2013
Return-Path: <dev-return-717-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AB9A6106E5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 18:45:16 +0000 (UTC)
Received: (qmail 54698 invoked by uid 500); 5 Nov 2013 18:45:16 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54652 invoked by uid 500); 5 Nov 2013 18:45:15 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 54644 invoked by uid 99); 5 Nov 2013 18:45:15 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 18:45:15 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of imran@quantifind.com designates 209.85.217.172 as permitted sender)
Received: from [209.85.217.172] (HELO mail-lb0-f172.google.com) (209.85.217.172)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 18:45:09 +0000
Received: by mail-lb0-f172.google.com with SMTP id c11so6890073lbj.17
        for <dev@spark.incubator.apache.org>; Tue, 05 Nov 2013 10:44:48 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=8wXcdmjHb237+CysnegH7h+G0S0dbjXqUENP1Wkh7Uw=;
        b=g9XcCwvaFk7B5+Hmj+f9R1SSGRuam2wy/cqf7RvUwSCJWa6vGw8ch6ix1UOE2i1m0H
         ajJQ9zW8Oos+aL2VS4FVAe8y5tpRnjuDwKoOWs0Y8fNlP8maudpQj/RZavX3LCnGx6kl
         Wc4oguNZO5DB2+DpcmrjXRHJ1ALwioNyXKgIh+yWyM/qgzFVYEwXAZWqr3rcX5Gx6Wk9
         Sev+FxA0d2ReKYIN/dL2/Px7UJehJcyrEgcB47fL1lAK304fXFxdPlV/HImAhetHztB2
         lQJF3vB/niPCDSVoBzaBQek081yJMpkYqSMYuqY61wtaJnR635nBargEyG0qnridYClh
         /fPA==
X-Gm-Message-State: ALoCoQmdYMMPrb5cDYecqkkWUwlbuY9xjkiP4MpE7Tyg35MYNrs/Ke2lRpNkae7IAOu9eiEegaxS
MIME-Version: 1.0
X-Received: by 10.112.146.200 with SMTP id te8mr6563321lbb.32.1383677088393;
 Tue, 05 Nov 2013 10:44:48 -0800 (PST)
Received: by 10.112.7.34 with HTTP; Tue, 5 Nov 2013 10:44:48 -0800 (PST)
X-Originating-IP: [98.193.39.168]
Date: Tue, 5 Nov 2013 12:44:48 -0600
Message-ID: <CAO24D=R3BvObQtp3ghcEFJ5K0Y23HQr+krBZ25K-r9ZWAXVh0A@mail.gmail.com>
Subject: appId is no longer in the command line args for StandaloneExecutor
From: Imran Rashid <imran@quantifind.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b3a83d4567c5204ea726d0c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b3a83d4567c5204ea726d0c
Content-Type: text/plain; charset=ISO-8859-1

Hi,

a while back, ExecutorRunner was changed so the command line args included
the appId.

https://github.com/mesos/spark/pull/467

Those changes seem to be gone from the latest code.  Was that intentional,
or just an oversight?  I'll add it back in if it was removed accidentally,
but wanted to check in case there is some reason it shouldn't be there.

thanks,
Imran

--047d7b3a83d4567c5204ea726d0c--

From dev-return-718-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 22:37:26 2013
Return-Path: <dev-return-718-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5F16910EAD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 22:37:26 +0000 (UTC)
Received: (qmail 34378 invoked by uid 500); 5 Nov 2013 22:37:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34282 invoked by uid 500); 5 Nov 2013 22:37:26 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 34274 invoked by uid 99); 5 Nov 2013 22:37:26 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 22:37:26 +0000
X-ASF-Spam-Status: No, hits=-1997.8 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Tue, 05 Nov 2013 22:37:25 +0000
Received: (qmail 33211 invoked by uid 99); 5 Nov 2013 22:37:03 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 22:37:03 +0000
Received: from localhost (HELO mail-ve0-f174.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Tue, 05 Nov 2013 22:37:03 +0000
Received: by mail-ve0-f174.google.com with SMTP id pa12so2959177veb.19
        for <dev@spark.incubator.apache.org>; Tue, 05 Nov 2013 14:37:02 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=Meqc/C0iw7WiFfoEBZj1w+Rh8+GlbDfdfyNRmP+L0Zo=;
        b=OKXY4Y7zFajB/wTdcsQOxB9lJi0sQ+eZYPRIJ1eM+AUNoQPEkihHzADRePGIWi59rB
         P0QYOGcQWBa7utyz+GqJnGs4pYGnBQGtsCwMk1WgBVK2nA56akBeccQzvLAy7Ar0DvyU
         HHor+vX5f8FXKo/3cmXZe+vR+h0bSa408AbibKMiJjko9ZQe9OBHByzvETZOtHXneXnn
         e+FSj2WB3IVwwEqI2w8vEXYh+8JtApxGDo3+nevn/q+i3b9Je6jtVnLDwYY0x3jpcpPh
         XEIsNinvym/ZVNY/oIEbBrkTm7czxgdV5uiOipFJkgmNrtEyCnui9s9VjEB6xanBd/ak
         ernA==
X-Received: by 10.220.2.138 with SMTP id 10mr13227vcj.77.1383691022292; Tue,
 05 Nov 2013 14:37:02 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Tue, 5 Nov 2013 14:36:42 -0800 (PST)
In-Reply-To: <CAO24D=R3BvObQtp3ghcEFJ5K0Y23HQr+krBZ25K-r9ZWAXVh0A@mail.gmail.com>
References: <CAO24D=R3BvObQtp3ghcEFJ5K0Y23HQr+krBZ25K-r9ZWAXVh0A@mail.gmail.com>
From: Reynold Xin <rxin@apache.org>
Date: Tue, 5 Nov 2013 14:36:42 -0800
Message-ID: <CAC1ssC6zwB8JFVFyPF0Ej-mt8XCA5DJ8K6h9miDv6BfeDSz7og@mail.gmail.com>
Subject: Re: appId is no longer in the command line args for StandaloneExecutor
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Cc: Aaron Davidson <aaron@databricks.com>
Content-Type: multipart/alternative; boundary=001a11c3ca38dcfc3204ea75abb4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3ca38dcfc3204ea75abb4
Content-Type: text/plain; charset=UTF-8

+aaron on this one since he changed the executor runner. (I think it is
probably an oversight but Aaron should confirm.)




On Tue, Nov 5, 2013 at 10:44 AM, Imran Rashid <imran@quantifind.com> wrote:

> Hi,
>
> a while back, ExecutorRunner was changed so the command line args included
> the appId.
>
> https://github.com/mesos/spark/pull/467
>
> Those changes seem to be gone from the latest code.  Was that intentional,
> or just an oversight?  I'll add it back in if it was removed accidentally,
> but wanted to check in case there is some reason it shouldn't be there.
>
> thanks,
> Imran
>

--001a11c3ca38dcfc3204ea75abb4--

From dev-return-719-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov  5 23:46:29 2013
Return-Path: <dev-return-719-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C9A3910163
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue,  5 Nov 2013 23:46:29 +0000 (UTC)
Received: (qmail 74171 invoked by uid 500); 5 Nov 2013 23:46:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 74138 invoked by uid 500); 5 Nov 2013 23:46:29 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Delivered-To: moderator for dev@spark.incubator.apache.org
Received: (qmail 66394 invoked by uid 99); 5 Nov 2013 23:41:02 -0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=HCzBDLDQBhjVFbQyt8UZwLuW7jY1Hubto4ailTH96cs=;
        b=Mqmni7k3g+2o8GBtzgcG9rscb6slYXHo3PBHtxFvx5lA7CvZXhLRVJUbqCyqjFsefb
         2GTdBX0i57Unir5atB1VuzCLo3cKTjfdH+Nql8b/jiD0Tw+HP17gcXFMLqliq/FYCbMC
         EPLwy8/+Mr8PlUvTu6jBqzbovIEuXrCS5eUedxfCzMLDQ9uuX7LZZ+8ZvO/wLhlckcTv
         Pa3nyfwHCNeE3BYf5TlElc1K/TvVFMp6xHj8VeRLYOXL/RjOjTiBYF/uqd6TQUYUR6Ou
         LLRcD+YGoVYu0pDaQZ90VGzyoodcxqhhef4uBgngyo2XPOnvUjY5oGLwmDPqjjdSr8KT
         31nA==
X-Gm-Message-State: ALoCoQnZjRh4+WC3ZQMpGi4hDLHwVx54Fz3TqLGHuNsAK/Z628qGhGOHNQuj62Ybtl71ziQ5rdLt
X-Received: by 10.68.161.2 with SMTP id xo2mr62621pbb.179.1383694837540; Tue,
 05 Nov 2013 15:40:37 -0800 (PST)
MIME-Version: 1.0
In-Reply-To: <CAC1ssC6zwB8JFVFyPF0Ej-mt8XCA5DJ8K6h9miDv6BfeDSz7og@mail.gmail.com>
References: <CAO24D=R3BvObQtp3ghcEFJ5K0Y23HQr+krBZ25K-r9ZWAXVh0A@mail.gmail.com>
 <CAC1ssC6zwB8JFVFyPF0Ej-mt8XCA5DJ8K6h9miDv6BfeDSz7og@mail.gmail.com>
From: Aaron Davidson <aaron@databricks.com>
Date: Tue, 5 Nov 2013 15:40:17 -0800
Message-ID: <CAGnzRon9kHyHdSuWqJW5a-50qyyFsxO_X8_09+0Jw3f-UBF9+w@mail.gmail.com>
Subject: Re: appId is no longer in the command line args for StandaloneExecutor
To: Reynold Xin <rxin@apache.org>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7bd7562a452a7f04ea768f1c
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd7562a452a7f04ea768f1c
Content-Type: text/plain; charset=ISO-8859-1

Looks like the appId thing was removed accidentally quite a while ago:
https://github.com/apache/incubator-spark/commit/85a35c68401e171df0b72b172a689d8c4e412199
and has gone unnoticed since. I have no objections to adding it back in...


On Tue, Nov 5, 2013 at 2:36 PM, Reynold Xin <rxin@apache.org> wrote:

> +aaron on this one since he changed the executor runner. (I think it is
> probably an oversight but Aaron should confirm.)
>
>
>
>
> On Tue, Nov 5, 2013 at 10:44 AM, Imran Rashid <imran@quantifind.com>wrote:
>
>> Hi,
>>
>> a while back, ExecutorRunner was changed so the command line args included
>> the appId.
>>
>> https://github.com/mesos/spark/pull/467
>>
>> Those changes seem to be gone from the latest code.  Was that intentional,
>> or just an oversight?  I'll add it back in if it was removed accidentally,
>> but wanted to check in case there is some reason it shouldn't be there.
>>
>> thanks,
>> Imran
>>
>
>

--047d7bd7562a452a7f04ea768f1c--

From dev-return-720-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov  6 08:29:45 2013
Return-Path: <dev-return-720-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E3A410DE8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Nov 2013 08:29:45 +0000 (UTC)
Received: (qmail 55520 invoked by uid 500); 6 Nov 2013 08:29:43 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 55158 invoked by uid 500); 6 Nov 2013 08:29:35 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 55145 invoked by uid 99); 6 Nov 2013 08:29:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 08:29:33 +0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of sandy.ryza@cloudera.com designates 209.85.192.177 as permitted sender)
Received: from [209.85.192.177] (HELO mail-pd0-f177.google.com) (209.85.192.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 08:29:26 +0000
Received: by mail-pd0-f177.google.com with SMTP id p10so9804021pdj.22
        for <dev@spark.incubator.apache.org>; Wed, 06 Nov 2013 00:29:05 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=jJLcS8vOLrG5lQ7QuDxrbws4hGDM5c0auPx5K47KrUs=;
        b=KoZLVXZv0uezJxhuRxkNe7kV6gelaM9SzHWDiThEC5u2mGvvyoY7zXyymcR7gv8Qqy
         CKU0y4lnh+y0sv1E2yagnhZaMJdjM+Xdjo6BPqDC88Lp3FcxMDQ7vTXcVUP6tWMZyPDG
         FbnxBGOjOVS/8zBuqrzWn6ur4yyRr7Z6r/WKK7JT3+0T8c9RQU69+71MYSPcqL8AXpu1
         uid0yMX46p4oQI/cYihMw8qr/Bzv2mafpOE3P6ENt/E2tc6JgPgsYEidNGWzRAh+HPRz
         3krhbOfE1fCIKfo6Lxm2ovRbBhtzjdKM0DE+R1dsfmejrCZUObKqF7dw6UeyNjCVgt7s
         7gtg==
X-Gm-Message-State: ALoCoQmi+PgONaf0FZSIa7inmTgWzqVckyTeDqcNeUhmUkek6RKz2BI8CP0ADFR/oQs1QPH2tMGB
MIME-Version: 1.0
X-Received: by 10.67.30.70 with SMTP id kc6mr2783963pad.32.1383726545335; Wed,
 06 Nov 2013 00:29:05 -0800 (PST)
Received: by 10.70.52.2 with HTTP; Wed, 6 Nov 2013 00:29:05 -0800 (PST)
In-Reply-To: <391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
	<CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
	<391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
	<CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com>
	<391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com>
Date: Wed, 6 Nov 2013 00:29:05 -0800
Message-ID: <CACBYxKKZWMqhao6UekxnGQYo1BdnVSuMBvQ1D7sK4XxmYcezWA@mail.gmail.com>
Subject: Re: issue regarding akka, protobuf and Hadoop version
From: Sandy Ryza <sandy.ryza@cloudera.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11332d98338e2e04ea7df1ff
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11332d98338e2e04ea7df1ff
Content-Type: text/plain; charset=ISO-8859-1

For my own understanding, is this summary correct?
Spark will move to scala 2.10, which means it can support akka 2.3-M1,
which supports protobuf 2.5, which will allow Spark to run on Hadoop 2.2.

What will be the first Spark version with these changes?  Are the Akka
features that Spark relies on stable in 2.3-M1?

thanks,
Sandy



On Tue, Nov 5, 2013 at 12:12 AM, Liu, Raymond <raymond.liu@intel.com> wrote:

> Just pushed a pull request which based on scala 2.10 branch for hadoop
> 2.2.0.
> Yarn-standalone mode workable, but need a few more fine tune works.
> Not really for pull, but as a placeholder, and for someone who want to
> take a look.
>
> Best Regards,
> Raymond Liu
>
>
> -----Original Message-----
> From: Reynold Xin [mailto:rxin@apache.org]
> Sent: Tuesday, November 05, 2013 10:07 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: issue regarding akka, protobuf and Hadoop version
>
> I think we are near the end of Scala 2.9.3 development, and will merge the
> Scala 2.10 branch into master and make it the future very soon (maybe next
> week).  This problem will go away.
>
> Meantime, we are relying on periodically merging the master into the Scala
> 2.10 branch.
>
>
> On Mon, Nov 4, 2013 at 5:53 PM, Liu, Raymond <raymond.liu@intel.com>
> wrote:
>
> > I plan to do the work on scala-2.10 branch, which already move to akka
> > 2.2.3, hope that to move to akka 2.3-M1 (which support protobuf 2.5.x)
> > will not cause many problem and make it a test to see is there further
> > issues, then wait for the formal release of akka 2.3.x
> >
> > While the issue is that I can see many commits on master branch is not
> > merged into scala-2.10 branch yet. The latest merge seems to happen on
> > OCT.11, while as I mentioned in the dev branch merge/sync thread,
> > seems that many earlier commit is not included and which will surely
> > bring extra works on future code merging/rebase. So again, what's the
> > code sync strategy and what's the plan of merge back into master?
> >
> > Best Regards,
> > Raymond Liu
> >
> >
> > -----Original Message-----
> > From: Reynold Xin [mailto:rxin@apache.org]
> > Sent: Tuesday, November 05, 2013 8:34 AM
> > To: dev@spark.incubator.apache.org
> > Subject: Re: issue regarding akka, protobuf and Hadoop version
> >
> > I chatted with Matt Massie about this, and here are some options:
> >
> > 1. Use dependency injection in google-guice to make Akka use one
> > version of protobuf, and YARN use the other version.
> >
> > 2. Look into OSGi to accomplish the same goal.
> >
> > 3. Rewrite the messaging part of Spark to use a simple, custom RPC
> > library instead of Akka. We are really only using a very simple subset
> > of Akka features, and we can probably implement a simple RPC library
> > tailored for Spark quickly. We should only do this as the last resort.
> >
> > 4. Talk to Akka guys and hope they can make a maintenance release of
> > Akka that supports protobuf 2.5.
> >
> >
> > None of these are ideal, but we'd have to pick one. It would be great
> > if you have other suggestions.
> >
> >
> > On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond <raymond.liu@intel.com>
> > wrote:
> >
> > > Hi
> > >
> > >         I am working on porting spark onto Hadoop 2.2.0, With some
> > > renaming and call into new YARN API works done. I can run up the
> > > spark master. While I encounter the issue that Executor Actor could
> > > not connecting to Driver actor.
> > >
> > >         After some investigation, I found the root cause is that the
> > > akka-remote do not support protobuf 2.5.0 before 2.3. And hadoop
> > > move to protobuf 2.5.0 from 2.1-beta.
> > >
> > >         The issue is that if I exclude the akka dependency from
> > > hadoop and force protobuf dependency to 2.4.1, the compile/packing
> > > will fail since hadoop common jar require a new interface from
> protobuf 2.5.0.
> > >
> > >          So any suggestion on this?
> > >
> > > Best Regards,
> > > Raymond Liu
> > >
> >
>

--001a11332d98338e2e04ea7df1ff--

From dev-return-721-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov  6 08:33:26 2013
Return-Path: <dev-return-721-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9FDE510DFD
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Nov 2013 08:33:26 +0000 (UTC)
Received: (qmail 60784 invoked by uid 500); 6 Nov 2013 08:33:26 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 60618 invoked by uid 500); 6 Nov 2013 08:33:25 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 60602 invoked by uid 99); 6 Nov 2013 08:33:24 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 08:33:24 +0000
X-ASF-Spam-Status: No, hits=-1997.8 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Wed, 06 Nov 2013 08:33:23 +0000
Received: (qmail 59351 invoked by uid 99); 6 Nov 2013 08:33:03 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 08:33:03 +0000
Received: from localhost (HELO mail-vb0-f45.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 08:33:02 +0000
Received: by mail-vb0-f45.google.com with SMTP id p6so3226728vbe.32
        for <dev@spark.incubator.apache.org>; Wed, 06 Nov 2013 00:33:01 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=zqyq+ijoCAcCrhxDtE15CJ6F3Jmg+sokw2mcCs/2oig=;
        b=UrZLmwSmPJb41NxlJS01rtDWd0ZumTzz9qzX2i+h5suQOBdsjZy/ebfn1JPiMlDrNQ
         FhF0xrOR3Ad8teitLivCpy6YmCowPaoPsFGkZ6hSdkaqaN/PwaTBDr0eK8bYezftttf/
         PRbUpVNFvExtRWUceay4/NLU1ZQGin76sSFfPI2Wd0+ET+Nx80MkbNHvGStLbZ5pvtmV
         jY8FPvrVwb+FaXgZCjNpKs6C/DTLedzwe2SpDXHkYdM2sRnrAmd78x3BVp6YP5fvkr00
         spA3oS6E7zjdiQ6w9kg0mlJuUs5HScpWQe8dPc+fYynsAedweKlchv9FuAheesL/ZyOl
         7SPA==
X-Received: by 10.52.116.74 with SMTP id ju10mr1200939vdb.20.1383726781919;
 Wed, 06 Nov 2013 00:33:01 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Wed, 6 Nov 2013 00:32:41 -0800 (PST)
In-Reply-To: <CACBYxKKZWMqhao6UekxnGQYo1BdnVSuMBvQ1D7sK4XxmYcezWA@mail.gmail.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com> <CACBYxKKZWMqhao6UekxnGQYo1BdnVSuMBvQ1D7sK4XxmYcezWA@mail.gmail.com>
From: Reynold Xin <rxin@apache.org>
Date: Wed, 6 Nov 2013 00:32:41 -0800
Message-ID: <CAC1ssC4Z5yxvbLYh8sVr3g2x3fpPbkezLXSL2FgfYxJ3C+s_5w@mail.gmail.com>
Subject: Re: issue regarding akka, protobuf and Hadoop version
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=20cf307d01dc4d79df04ea7dff15
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307d01dc4d79df04ea7dff15
Content-Type: text/plain; charset=UTF-8

That is correct. However, there is no guarantee right now that Akka 2.3
will work correctly for us. We haven't tested it enough yet (or rather, we
haven't tested it at all) E.g. see:
https://github.com/apache/incubator-spark/pull/131

We want to make Spark 0.9.0 based on Scala 2.10, but we have also been
discussing ideas to make a Scala 2.10 version of Spark 0.8.x so it enables
users to move to Scala 2.10 earlier if they want.


On Wed, Nov 6, 2013 at 12:29 AM, Sandy Ryza <sandy.ryza@cloudera.com> wrote:

> For my own understanding, is this summary correct?
> Spark will move to scala 2.10, which means it can support akka 2.3-M1,
> which supports protobuf 2.5, which will allow Spark to run on Hadoop 2.2.
>
> What will be the first Spark version with these changes?  Are the Akka
> features that Spark relies on stable in 2.3-M1?
>
> thanks,
> Sandy
>
>
>
> On Tue, Nov 5, 2013 at 12:12 AM, Liu, Raymond <raymond.liu@intel.com>
> wrote:
>
> > Just pushed a pull request which based on scala 2.10 branch for hadoop
> > 2.2.0.
> > Yarn-standalone mode workable, but need a few more fine tune works.
> > Not really for pull, but as a placeholder, and for someone who want to
> > take a look.
> >
> > Best Regards,
> > Raymond Liu
> >
> >
> > -----Original Message-----
> > From: Reynold Xin [mailto:rxin@apache.org]
> > Sent: Tuesday, November 05, 2013 10:07 AM
> > To: dev@spark.incubator.apache.org
> > Subject: Re: issue regarding akka, protobuf and Hadoop version
> >
> > I think we are near the end of Scala 2.9.3 development, and will merge
> the
> > Scala 2.10 branch into master and make it the future very soon (maybe
> next
> > week).  This problem will go away.
> >
> > Meantime, we are relying on periodically merging the master into the
> Scala
> > 2.10 branch.
> >
> >
> > On Mon, Nov 4, 2013 at 5:53 PM, Liu, Raymond <raymond.liu@intel.com>
> > wrote:
> >
> > > I plan to do the work on scala-2.10 branch, which already move to akka
> > > 2.2.3, hope that to move to akka 2.3-M1 (which support protobuf 2.5.x)
> > > will not cause many problem and make it a test to see is there further
> > > issues, then wait for the formal release of akka 2.3.x
> > >
> > > While the issue is that I can see many commits on master branch is not
> > > merged into scala-2.10 branch yet. The latest merge seems to happen on
> > > OCT.11, while as I mentioned in the dev branch merge/sync thread,
> > > seems that many earlier commit is not included and which will surely
> > > bring extra works on future code merging/rebase. So again, what's the
> > > code sync strategy and what's the plan of merge back into master?
> > >
> > > Best Regards,
> > > Raymond Liu
> > >
> > >
> > > -----Original Message-----
> > > From: Reynold Xin [mailto:rxin@apache.org]
> > > Sent: Tuesday, November 05, 2013 8:34 AM
> > > To: dev@spark.incubator.apache.org
> > > Subject: Re: issue regarding akka, protobuf and Hadoop version
> > >
> > > I chatted with Matt Massie about this, and here are some options:
> > >
> > > 1. Use dependency injection in google-guice to make Akka use one
> > > version of protobuf, and YARN use the other version.
> > >
> > > 2. Look into OSGi to accomplish the same goal.
> > >
> > > 3. Rewrite the messaging part of Spark to use a simple, custom RPC
> > > library instead of Akka. We are really only using a very simple subset
> > > of Akka features, and we can probably implement a simple RPC library
> > > tailored for Spark quickly. We should only do this as the last resort.
> > >
> > > 4. Talk to Akka guys and hope they can make a maintenance release of
> > > Akka that supports protobuf 2.5.
> > >
> > >
> > > None of these are ideal, but we'd have to pick one. It would be great
> > > if you have other suggestions.
> > >
> > >
> > > On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond <raymond.liu@intel.com>
> > > wrote:
> > >
> > > > Hi
> > > >
> > > >         I am working on porting spark onto Hadoop 2.2.0, With some
> > > > renaming and call into new YARN API works done. I can run up the
> > > > spark master. While I encounter the issue that Executor Actor could
> > > > not connecting to Driver actor.
> > > >
> > > >         After some investigation, I found the root cause is that the
> > > > akka-remote do not support protobuf 2.5.0 before 2.3. And hadoop
> > > > move to protobuf 2.5.0 from 2.1-beta.
> > > >
> > > >         The issue is that if I exclude the akka dependency from
> > > > hadoop and force protobuf dependency to 2.4.1, the compile/packing
> > > > will fail since hadoop common jar require a new interface from
> > protobuf 2.5.0.
> > > >
> > > >          So any suggestion on this?
> > > >
> > > > Best Regards,
> > > > Raymond Liu
> > > >
> > >
> >
>

--20cf307d01dc4d79df04ea7dff15--

From dev-return-722-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov  6 17:31:34 2013
Return-Path: <dev-return-722-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6552510FC4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Nov 2013 17:31:34 +0000 (UTC)
Received: (qmail 62847 invoked by uid 500); 6 Nov 2013 17:31:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 62553 invoked by uid 500); 6 Nov 2013 17:31:30 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 62535 invoked by uid 99); 6 Nov 2013 17:31:28 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 17:31:28 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.128.49 as permitted sender)
Received: from [209.85.128.49] (HELO mail-qe0-f49.google.com) (209.85.128.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 17:31:24 +0000
Received: by mail-qe0-f49.google.com with SMTP id a11so6486885qen.22
        for <dev@spark.incubator.apache.org>; Wed, 06 Nov 2013 09:31:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=5Hm/aRqRbw3ScbDOxdFO5+QkQs3l/7juqAvD6eiKyYk=;
        b=cZ6HSV9X9jwLmnpyyNTa+cbFZYRMIwO+zsj/d1oGEUL82Aak1VXpKTlMx4z5ZkMCN4
         yTlcBqfd6bot9eqJ9yVRdaEdPuINZTjvnK7sFi4SwcRUv2/UMPZl/XTC+DAwXO67LIDB
         h2ueG6cvO5SDtDOIT4oSuxfBrXPWgQ2EAuGZHL58JB7BKsKasRm0hVqs5lnN5hhQTyz7
         rAaOYSB6zTzmgNsKSFLf/q0hcWeqZn2Nth/qGu8AXOxzAGDDdcUCs3xJ/Bl2N6kzGMJZ
         B9VJKOiIQm7jm6KteqBIg/aqB87+WeQlinyMUpaYIWJDKyl8MznGo4zzcvmCkZZWfDeV
         yHHQ==
X-Received: by 10.49.51.103 with SMTP id j7mr5993789qeo.29.1383759063774;
        Wed, 06 Nov 2013 09:31:03 -0800 (PST)
Received: from [172.16.3.53] ([69.89.172.91])
        by mx.google.com with ESMTPSA id kz8sm71525864qeb.0.2013.11.06.09.31.02
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 06 Nov 2013 09:31:02 -0800 (PST)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
Subject: Re: appId is no longer in the command line args for StandaloneExecutor
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAGnzRon9kHyHdSuWqJW5a-50qyyFsxO_X8_09+0Jw3f-UBF9+w@mail.gmail.com>
Date: Wed, 6 Nov 2013 12:31:00 -0500
Cc: Reynold Xin <rxin@apache.org>
Content-Transfer-Encoding: quoted-printable
Message-Id: <DEDA552B-173A-40C4-9413-57ACB491D115@gmail.com>
References: <CAO24D=R3BvObQtp3ghcEFJ5K0Y23HQr+krBZ25K-r9ZWAXVh0A@mail.gmail.com> <CAC1ssC6zwB8JFVFyPF0Ej-mt8XCA5DJ8K6h9miDv6BfeDSz7og@mail.gmail.com> <CAGnzRon9kHyHdSuWqJW5a-50qyyFsxO_X8_09+0Jw3f-UBF9+w@mail.gmail.com>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

Yeah, true. It would be good to add it back.

Matei

On Nov 5, 2013, at 6:40 PM, Aaron Davidson <aaron@databricks.com> wrote:

> Looks like the appId thing was removed accidentally quite a while ago:
> =
https://github.com/apache/incubator-spark/commit/85a35c68401e171df0b72b172=
a689d8c4e412199
> and has gone unnoticed since. I have no objections to adding it back =
in...
>=20
>=20
> On Tue, Nov 5, 2013 at 2:36 PM, Reynold Xin <rxin@apache.org> wrote:
>=20
>> +aaron on this one since he changed the executor runner. (I think it =
is
>> probably an oversight but Aaron should confirm.)
>>=20
>>=20
>>=20
>>=20
>> On Tue, Nov 5, 2013 at 10:44 AM, Imran Rashid =
<imran@quantifind.com>wrote:
>>=20
>>> Hi,
>>>=20
>>> a while back, ExecutorRunner was changed so the command line args =
included
>>> the appId.
>>>=20
>>> https://github.com/mesos/spark/pull/467
>>>=20
>>> Those changes seem to be gone from the latest code.  Was that =
intentional,
>>> or just an oversight?  I'll add it back in if it was removed =
accidentally,
>>> but wanted to check in case there is some reason it shouldn't be =
there.
>>>=20
>>> thanks,
>>> Imran
>>>=20
>>=20
>>=20


From dev-return-723-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov  6 18:59:21 2013
Return-Path: <dev-return-723-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3B033103E6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed,  6 Nov 2013 18:59:21 +0000 (UTC)
Received: (qmail 91934 invoked by uid 500); 6 Nov 2013 18:59:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91894 invoked by uid 500); 6 Nov 2013 18:59:20 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 91886 invoked by uid 99); 6 Nov 2013 18:59:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 18:59:20 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of kellrott@soe.ucsc.edu designates 209.85.219.48 as permitted sender)
Received: from [209.85.219.48] (HELO mail-oa0-f48.google.com) (209.85.219.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 06 Nov 2013 18:59:16 +0000
Received: by mail-oa0-f48.google.com with SMTP id h16so2609130oag.21
        for <dev@spark.incubator.apache.org>; Wed, 06 Nov 2013 10:58:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ucsc.edu; s=ucsc-google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=PvoqUP8PLu2OfekQCzC3q/7b/eOHdNA9lzOrweW5OEM=;
        b=BJYnpJN8syd+62jFPBaVufZ0kPPRLCBta/czKdxmEm+dp14/9r9T5qHyG1wj9Nfq9f
         wX/TAaosPwgi1qK9Mq0T7Ke2ulTWc/qxc7a4m92NwglZ4NSb/I/WGkVa8lToIPCo5+HI
         PkF/e0mWbpMm66wfzcgYK5B8bYr6o7QIfcbMM=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=PvoqUP8PLu2OfekQCzC3q/7b/eOHdNA9lzOrweW5OEM=;
        b=ZZl4JJCgI/1tmnhZhYZT96ySoulqtBXwGkU0vLH1G2ssLj5lthHFsPBVKC9mUIviVN
         NRiq3L8l/oo3621PLtATwTEOWQNDhlh9iyjWzZraKb8ec4SHuqxS+zx1EfnL+U+3Zdhn
         bsyUu9AXA4zZlWVg1v9CRQcsBodlMNvHKWNdeh3Bxs9vbuPUSjLim0uDp5O9CMKvtgjr
         5/ePBaolavIIbnym8ZP72b/il86Epny39Vt3pUgvzEIoBU9Q6U9FeoNLI+nBc6Fs/DYs
         5Uv8SuHMPRj5AR0ppLBL1dvgNIU6CWw5DXHH4P/ghEQ8DxgV+Vl+Ty5iqTCLZSxhhkMB
         yHSA==
X-Gm-Message-State: ALoCoQkGG96oVF3LSlChjmVSeDQhfDi3Pto8m3dFgR+AJgmVf9+LftgiXGEEpspC9/Z/BE+dqalF
MIME-Version: 1.0
X-Received: by 10.60.94.164 with SMTP id dd4mr1230460oeb.68.1383764335529;
 Wed, 06 Nov 2013 10:58:55 -0800 (PST)
Received: by 10.182.132.50 with HTTP; Wed, 6 Nov 2013 10:58:55 -0800 (PST)
In-Reply-To: <CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
Date: Wed, 6 Nov 2013 10:58:55 -0800
Message-ID: <CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
Subject: Re: SPARK-942
From: Kyle Ellrott <kellrott@soe.ucsc.edu>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e0116028cac474d04ea86bde0
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0116028cac474d04ea86bde0
Content-Type: text/plain; charset=ISO-8859-1

I think the usage has to be calculated as the iterator is being put into
the arraybuffer.
Right now, the BlockManager, in it's put method when it gets an iterator
named 'values' uses the simple stanza of:

def put(blockId: BlockId, values: Iterator[Any], level: StorageLevel,
tellMaster: Boolean)
    : Long = {
    val elements = new ArrayBuffer[Any]
    elements ++= values
    put(blockId, elements, level, tellMaster)
}


Completely unrolling the iterator in a single line.  Above it, the
CacheManager does the exact same thing with:

val elements = new ArrayBuffer[Any]
elements ++= computedValues
blockManager.put(key, elements, storageLevel, tellMaster = true)


We would probably have to implement some sort of 'IteratorBuffer' class,
which would wrap an iterator. It would include a method to unroll an
iterator into a buffer up to a point, something like

def unroll(maxMem:Long) : Boolean ={ ...}

And it would return True if the maxMem was hit. At which point BlockManager
could read through the already cached values, then continue on through the
rest of the iterators dumping all the values to file. If it unrolled
without hitting maxMem (which would probably be most of the time), the
class would simply wrap the ArrayBuffer of cached values.

Kyle



On Sun, Nov 3, 2013 at 12:50 AM, Reynold Xin <rxin@apache.org> wrote:

> It's not a very elegant solution, but one possibility is for the
> CacheManager to check whether it will have enough space. If it is running
> out of space, skips buffering the output of the iterator & directly write
> the output of the iterator to disk (if storage level allows that).
>
> But it is still tricky to know whether we will run out of space before we
> even start running the iterator. One possibility is to use sizing data from
> previous partitions to estimate the size of the current partition (i.e.
> estimated in memory size = avg of current in-memory size / current input
> size).
>
> Do you have any ideas on this one, Kyle?
>
>
> On Sat, Oct 26, 2013 at 10:53 AM, Kyle Ellrott <kellrott@soe.ucsc.edu
> >wrote:
>
> > I was wondering if anybody had any thoughts on the best way to tackle
> > SPARK-942 ( https://spark-project.atlassian.net/browse/SPARK-942 ).
> > Basically, Spark takes an iterator from a flatmap call and because I tell
> > it that it needs to persist Spark proceeds to push it all into an array
> > before deciding that it doesn't have enough memory and trying to
> serialize
> > it to disk, and somewhere along the line it runs out of memory. For my
> > particular operation, the function return an iterator that reads data out
> > of a file, and the size of the files passed to that function can vary
> > greatly (from a few kilobytes to a few gigabytes). The funny thing is
> that
> > if I do a strait 'map' operation after the flat map, everything works,
> > because Spark just passes the iterator forward and never tries to expand
> > the whole thing into memory. But I need do a reduceByKey across all the
> > records, so I'd like to persist to disk first, and that is where I hit
> this
> > snag.
> > I've already setup a unit test to replicate the problem, and I know the
> > area of the code that would need to be fixed.
> > I'm just hoping for some tips on the best way to fix the problem.
> >
> > Kyle
> >
>

--089e0116028cac474d04ea86bde0--

From dev-return-724-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov  7 02:13:38 2013
Return-Path: <dev-return-724-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 25F0C1032E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Nov 2013 02:13:38 +0000 (UTC)
Received: (qmail 98691 invoked by uid 500); 7 Nov 2013 02:13:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98649 invoked by uid 500); 7 Nov 2013 02:13:37 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 98640 invoked by uid 99); 7 Nov 2013 02:13:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 02:13:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.223.180 as permitted sender)
Received: from [209.85.223.180] (HELO mail-ie0-f180.google.com) (209.85.223.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 02:13:30 +0000
Received: by mail-ie0-f180.google.com with SMTP id e14so647639iej.11
        for <dev@spark.incubator.apache.org>; Wed, 06 Nov 2013 18:13:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=Yi4GmdI8JhiBZTVkRVHWFlZoYW54fkHCbjWtdYKUUfE=;
        b=ZUMOHRWGgZhxCzbKeTW1Eo5wc+iB/kbgMsod0snFlduy9McNoeUlVdJGEv20omg/3S
         1rMxSOADSh7Lk86PA+4KNZS+YhSSOA3L3sxYeP0144Q5TwW88pHbJJAXHzLrwsB5yp4U
         zkr4UttaipdnEgDsjU4pes77SpAMLfQeZBRxUqDOXQZhK3TyKQiYDsnb+Un4v8tPju4v
         v1AX/Rhf55xnW0vW8lanU0w+0Twt+yXyf/k/kFCC1D1A7k/g7iTN5VuphFWDhvJve1tE
         rxeCIXDTSY2c1wDYJ2tF7SojoLmLU6Va5KbA+FM/toUXuwZGApBj5QDMSy8ItK/8RRpL
         +UGg==
X-Received: by 10.50.8.102 with SMTP id q6mr360228iga.57.1383790389695;
        Wed, 06 Nov 2013 18:13:09 -0800 (PST)
Received: from [172.19.131.152] ([199.108.71.39])
        by mx.google.com with ESMTPSA id w7sm17523074igp.1.2013.11.06.18.13.08
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 06 Nov 2013 18:13:09 -0800 (PST)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
Subject: Re: issue regarding akka, protobuf and Hadoop version
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CAC1ssC4Z5yxvbLYh8sVr3g2x3fpPbkezLXSL2FgfYxJ3C+s_5w@mail.gmail.com>
Date: Wed, 6 Nov 2013 21:13:04 -0500
Content-Transfer-Encoding: quoted-printable
Message-Id: <BCA2A701-DD77-4D9C-965B-210652C70CAC@gmail.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com> <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com> <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com> <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com> <391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com> <CACBYxKKZWMqhao6UekxnGQYo1BdnVSuMBvQ1D7sK4XxmYcezWA@mail.gmail.com> <CAC1ssC4Z5yxvbLYh8sVr3g2x3fpPbkezLXSL2FgfYxJ3C+s_5w@mail.gmail.com>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

Moving to Akka 2.3 won=92t solve this problem unfortunately, because =
people still want to run Spark with older Hadoop versions too, which =
will have Protobuf 2.4.1. Have you tested it with those?

Matei

On Nov 6, 2013, at 3:32 AM, Reynold Xin <rxin@apache.org> wrote:

> That is correct. However, there is no guarantee right now that Akka =
2.3
> will work correctly for us. We haven't tested it enough yet (or =
rather, we
> haven't tested it at all) E.g. see:
> https://github.com/apache/incubator-spark/pull/131
>=20
> We want to make Spark 0.9.0 based on Scala 2.10, but we have also been
> discussing ideas to make a Scala 2.10 version of Spark 0.8.x so it =
enables
> users to move to Scala 2.10 earlier if they want.
>=20
>=20
> On Wed, Nov 6, 2013 at 12:29 AM, Sandy Ryza <sandy.ryza@cloudera.com> =
wrote:
>=20
>> For my own understanding, is this summary correct?
>> Spark will move to scala 2.10, which means it can support akka =
2.3-M1,
>> which supports protobuf 2.5, which will allow Spark to run on Hadoop =
2.2.
>>=20
>> What will be the first Spark version with these changes?  Are the =
Akka
>> features that Spark relies on stable in 2.3-M1?
>>=20
>> thanks,
>> Sandy
>>=20
>>=20
>>=20
>> On Tue, Nov 5, 2013 at 12:12 AM, Liu, Raymond <raymond.liu@intel.com>
>> wrote:
>>=20
>>> Just pushed a pull request which based on scala 2.10 branch for =
hadoop
>>> 2.2.0.
>>> Yarn-standalone mode workable, but need a few more fine tune works.
>>> Not really for pull, but as a placeholder, and for someone who want =
to
>>> take a look.
>>>=20
>>> Best Regards,
>>> Raymond Liu
>>>=20
>>>=20
>>> -----Original Message-----
>>> From: Reynold Xin [mailto:rxin@apache.org]
>>> Sent: Tuesday, November 05, 2013 10:07 AM
>>> To: dev@spark.incubator.apache.org
>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>=20
>>> I think we are near the end of Scala 2.9.3 development, and will =
merge
>> the
>>> Scala 2.10 branch into master and make it the future very soon =
(maybe
>> next
>>> week).  This problem will go away.
>>>=20
>>> Meantime, we are relying on periodically merging the master into the
>> Scala
>>> 2.10 branch.
>>>=20
>>>=20
>>> On Mon, Nov 4, 2013 at 5:53 PM, Liu, Raymond <raymond.liu@intel.com>
>>> wrote:
>>>=20
>>>> I plan to do the work on scala-2.10 branch, which already move to =
akka
>>>> 2.2.3, hope that to move to akka 2.3-M1 (which support protobuf =
2.5.x)
>>>> will not cause many problem and make it a test to see is there =
further
>>>> issues, then wait for the formal release of akka 2.3.x
>>>>=20
>>>> While the issue is that I can see many commits on master branch is =
not
>>>> merged into scala-2.10 branch yet. The latest merge seems to happen =
on
>>>> OCT.11, while as I mentioned in the dev branch merge/sync thread,
>>>> seems that many earlier commit is not included and which will =
surely
>>>> bring extra works on future code merging/rebase. So again, what's =
the
>>>> code sync strategy and what's the plan of merge back into master?
>>>>=20
>>>> Best Regards,
>>>> Raymond Liu
>>>>=20
>>>>=20
>>>> -----Original Message-----
>>>> From: Reynold Xin [mailto:rxin@apache.org]
>>>> Sent: Tuesday, November 05, 2013 8:34 AM
>>>> To: dev@spark.incubator.apache.org
>>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>>=20
>>>> I chatted with Matt Massie about this, and here are some options:
>>>>=20
>>>> 1. Use dependency injection in google-guice to make Akka use one
>>>> version of protobuf, and YARN use the other version.
>>>>=20
>>>> 2. Look into OSGi to accomplish the same goal.
>>>>=20
>>>> 3. Rewrite the messaging part of Spark to use a simple, custom RPC
>>>> library instead of Akka. We are really only using a very simple =
subset
>>>> of Akka features, and we can probably implement a simple RPC =
library
>>>> tailored for Spark quickly. We should only do this as the last =
resort.
>>>>=20
>>>> 4. Talk to Akka guys and hope they can make a maintenance release =
of
>>>> Akka that supports protobuf 2.5.
>>>>=20
>>>>=20
>>>> None of these are ideal, but we'd have to pick one. It would be =
great
>>>> if you have other suggestions.
>>>>=20
>>>>=20
>>>> On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond =
<raymond.liu@intel.com>
>>>> wrote:
>>>>=20
>>>>> Hi
>>>>>=20
>>>>>        I am working on porting spark onto Hadoop 2.2.0, With some
>>>>> renaming and call into new YARN API works done. I can run up the
>>>>> spark master. While I encounter the issue that Executor Actor =
could
>>>>> not connecting to Driver actor.
>>>>>=20
>>>>>        After some investigation, I found the root cause is that =
the
>>>>> akka-remote do not support protobuf 2.5.0 before 2.3. And hadoop
>>>>> move to protobuf 2.5.0 from 2.1-beta.
>>>>>=20
>>>>>        The issue is that if I exclude the akka dependency from
>>>>> hadoop and force protobuf dependency to 2.4.1, the compile/packing
>>>>> will fail since hadoop common jar require a new interface from
>>> protobuf 2.5.0.
>>>>>=20
>>>>>         So any suggestion on this?
>>>>>=20
>>>>> Best Regards,
>>>>> Raymond Liu
>>>>>=20
>>>>=20
>>>=20
>>=20


From dev-return-725-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov  7 02:31:14 2013
Return-Path: <dev-return-725-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 48A3410389
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Nov 2013 02:31:14 +0000 (UTC)
Received: (qmail 19831 invoked by uid 500); 7 Nov 2013 02:31:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 19801 invoked by uid 500); 7 Nov 2013 02:31:14 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 19793 invoked by uid 99); 7 Nov 2013 02:31:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 02:31:14 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of raymond.liu@intel.com designates 134.134.136.20 as permitted sender)
Received: from [134.134.136.20] (HELO mga02.intel.com) (134.134.136.20)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 02:31:08 +0000
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
  by orsmga101.jf.intel.com with ESMTP; 06 Nov 2013 18:30:46 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,648,1378882800"; 
   d="scan'208";a="423499562"
Received: from fmsmsx105.amr.corp.intel.com ([10.19.9.36])
  by fmsmga001.fm.intel.com with ESMTP; 06 Nov 2013 18:30:45 -0800
Received: from shsmsx103.ccr.corp.intel.com (10.239.4.69) by
 FMSMSX105.amr.corp.intel.com (10.19.9.36) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Wed, 6 Nov 2013 18:30:45 -0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.240]) by
 SHSMSX103.ccr.corp.intel.com ([169.254.4.14]) with mapi id 14.03.0123.003;
 Thu, 7 Nov 2013 10:30:43 +0800
From: "Liu, Raymond" <raymond.liu@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: issue regarding akka, protobuf and Hadoop version
Thread-Topic: issue regarding akka, protobuf and Hadoop version
Thread-Index: Ac7ZMduY/ltyQ77yToubUDTRusm1uwAScDmAABLwNXD//4KBgP//FKCAgASYJev///vFgA==
Date: Thu, 7 Nov 2013 02:30:43 +0000
Message-ID: <391D65D0EBFC9B4B95E117F72A360F1A010CF28A@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com>
 <CACBYxKKZWMqhao6UekxnGQYo1BdnVSuMBvQ1D7sK4XxmYcezWA@mail.gmail.com>
 <CAC1ssC4Z5yxvbLYh8sVr3g2x3fpPbkezLXSL2FgfYxJ3C+s_5w@mail.gmail.com>
 <BCA2A701-DD77-4D9C-965B-210652C70CAC@gmail.com>
In-Reply-To: <BCA2A701-DD77-4D9C-965B-210652C70CAC@gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Then that will be two branch code right? One for hadoop 2.2+ , one for olde=
r versions. And with different akka/protobuf version dependency.


Best Regards,
Raymond Liu

-----Original Message-----
From: Matei Zaharia [mailto:matei.zaharia@gmail.com]=20
Sent: Thursday, November 07, 2013 10:13 AM
To: dev@spark.incubator.apache.org
Subject: Re: issue regarding akka, protobuf and Hadoop version

Moving to Akka 2.3 won't solve this problem unfortunately, because people s=
till want to run Spark with older Hadoop versions too, which will have Prot=
obuf 2.4.1. Have you tested it with those?

Matei

On Nov 6, 2013, at 3:32 AM, Reynold Xin <rxin@apache.org> wrote:

> That is correct. However, there is no guarantee right now that Akka=20
> 2.3 will work correctly for us. We haven't tested it enough yet (or=20
> rather, we haven't tested it at all) E.g. see:
> https://github.com/apache/incubator-spark/pull/131
>=20
> We want to make Spark 0.9.0 based on Scala 2.10, but we have also been=20
> discussing ideas to make a Scala 2.10 version of Spark 0.8.x so it=20
> enables users to move to Scala 2.10 earlier if they want.
>=20
>=20
> On Wed, Nov 6, 2013 at 12:29 AM, Sandy Ryza <sandy.ryza@cloudera.com> wro=
te:
>=20
>> For my own understanding, is this summary correct?
>> Spark will move to scala 2.10, which means it can support akka=20
>> 2.3-M1, which supports protobuf 2.5, which will allow Spark to run on Ha=
doop 2.2.
>>=20
>> What will be the first Spark version with these changes?  Are the=20
>> Akka features that Spark relies on stable in 2.3-M1?
>>=20
>> thanks,
>> Sandy
>>=20
>>=20
>>=20
>> On Tue, Nov 5, 2013 at 12:12 AM, Liu, Raymond <raymond.liu@intel.com>
>> wrote:
>>=20
>>> Just pushed a pull request which based on scala 2.10 branch for=20
>>> hadoop 2.2.0.
>>> Yarn-standalone mode workable, but need a few more fine tune works.
>>> Not really for pull, but as a placeholder, and for someone who want=20
>>> to take a look.
>>>=20
>>> Best Regards,
>>> Raymond Liu
>>>=20
>>>=20
>>> -----Original Message-----
>>> From: Reynold Xin [mailto:rxin@apache.org]
>>> Sent: Tuesday, November 05, 2013 10:07 AM
>>> To: dev@spark.incubator.apache.org
>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>=20
>>> I think we are near the end of Scala 2.9.3 development, and will=20
>>> merge
>> the
>>> Scala 2.10 branch into master and make it the future very soon=20
>>> (maybe
>> next
>>> week).  This problem will go away.
>>>=20
>>> Meantime, we are relying on periodically merging the master into the
>> Scala
>>> 2.10 branch.
>>>=20
>>>=20
>>> On Mon, Nov 4, 2013 at 5:53 PM, Liu, Raymond <raymond.liu@intel.com>
>>> wrote:
>>>=20
>>>> I plan to do the work on scala-2.10 branch, which already move to=20
>>>> akka 2.2.3, hope that to move to akka 2.3-M1 (which support=20
>>>> protobuf 2.5.x) will not cause many problem and make it a test to=20
>>>> see is there further issues, then wait for the formal release of=20
>>>> akka 2.3.x
>>>>=20
>>>> While the issue is that I can see many commits on master branch is=20
>>>> not merged into scala-2.10 branch yet. The latest merge seems to=20
>>>> happen on OCT.11, while as I mentioned in the dev branch merge/sync=20
>>>> thread, seems that many earlier commit is not included and which=20
>>>> will surely bring extra works on future code merging/rebase. So=20
>>>> again, what's the code sync strategy and what's the plan of merge back=
 into master?
>>>>=20
>>>> Best Regards,
>>>> Raymond Liu
>>>>=20
>>>>=20
>>>> -----Original Message-----
>>>> From: Reynold Xin [mailto:rxin@apache.org]
>>>> Sent: Tuesday, November 05, 2013 8:34 AM
>>>> To: dev@spark.incubator.apache.org
>>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>>=20
>>>> I chatted with Matt Massie about this, and here are some options:
>>>>=20
>>>> 1. Use dependency injection in google-guice to make Akka use one=20
>>>> version of protobuf, and YARN use the other version.
>>>>=20
>>>> 2. Look into OSGi to accomplish the same goal.
>>>>=20
>>>> 3. Rewrite the messaging part of Spark to use a simple, custom RPC=20
>>>> library instead of Akka. We are really only using a very simple=20
>>>> subset of Akka features, and we can probably implement a simple RPC=20
>>>> library tailored for Spark quickly. We should only do this as the last=
 resort.
>>>>=20
>>>> 4. Talk to Akka guys and hope they can make a maintenance release=20
>>>> of Akka that supports protobuf 2.5.
>>>>=20
>>>>=20
>>>> None of these are ideal, but we'd have to pick one. It would be=20
>>>> great if you have other suggestions.
>>>>=20
>>>>=20
>>>> On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond=20
>>>> <raymond.liu@intel.com>
>>>> wrote:
>>>>=20
>>>>> Hi
>>>>>=20
>>>>>        I am working on porting spark onto Hadoop 2.2.0, With some=20
>>>>> renaming and call into new YARN API works done. I can run up the=20
>>>>> spark master. While I encounter the issue that Executor Actor=20
>>>>> could not connecting to Driver actor.
>>>>>=20
>>>>>        After some investigation, I found the root cause is that=20
>>>>> the akka-remote do not support protobuf 2.5.0 before 2.3. And=20
>>>>> hadoop move to protobuf 2.5.0 from 2.1-beta.
>>>>>=20
>>>>>        The issue is that if I exclude the akka dependency from=20
>>>>> hadoop and force protobuf dependency to 2.4.1, the compile/packing=20
>>>>> will fail since hadoop common jar require a new interface from
>>> protobuf 2.5.0.
>>>>>=20
>>>>>         So any suggestion on this?
>>>>>=20
>>>>> Best Regards,
>>>>> Raymond Liu
>>>>>=20
>>>>=20
>>>=20
>>=20


From dev-return-726-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov  7 03:04:02 2013
Return-Path: <dev-return-726-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 90C2D104FF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Nov 2013 03:04:02 +0000 (UTC)
Received: (qmail 64543 invoked by uid 500); 7 Nov 2013 03:03:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64521 invoked by uid 500); 7 Nov 2013 03:03:58 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 64482 invoked by uid 99); 7 Nov 2013 03:03:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 03:03:57 +0000
X-ASF-Spam-Status: No, hits=3.2 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rosenville@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 03:03:50 +0000
Received: by mail-pd0-f176.google.com with SMTP id g10so422391pdj.21
        for <dev@spark.incubator.apache.org>; Wed, 06 Nov 2013 19:03:29 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=uPVlyL7w0m3OhVLw2rGUO9/wZ5JdTuDSp9TsNy41DwY=;
        b=CkJu5WQ8jVWAJ7f1YNT6v4ePzm5j0HSw18zMt9dww1i2ZDo1XX3d01y9jm5z6PlAU2
         5Zv+A6e1qZeTE4MLf9CFdZ0K6Zaq1A7RXS40ZHUxwpbXOx06iwfZVEUNJ6ksDEcEWsot
         Ww2FFntBhPtVOS66mNe41y1cYJ3mbPXQFtJbK4LD/RF6JlUQISDR7ayzMumFlICDXe2o
         ZXHd2yTvM9rd9/boduJa0l51/oIPEKIe/6wu/Y4YRzCo0IDc5pum0Ag/+nzH5Cf42f39
         RQVRWX2hlJcBmoQqvU1Myg+Eyh6YW6cRzHuyxAssCYpJsdjUvXo6Di4dsc+lmRhIjbZM
         CjjQ==
MIME-Version: 1.0
X-Received: by 10.68.225.138 with SMTP id rk10mr17566pbc.203.1383793408992;
 Wed, 06 Nov 2013 19:03:28 -0800 (PST)
Received: by 10.70.24.3 with HTTP; Wed, 6 Nov 2013 19:03:28 -0800 (PST)
In-Reply-To: <CAOEPXP5mMbEzXFCbEafuAUS9ZAd5wkiaus2O5arC6w4QTp=5GQ@mail.gmail.com>
References: <CALD+6GO4b2T+w6cSPxUtMJxTEipYJzw0Fd-1DshTOTQTwt9PqQ@mail.gmail.com>
	<CAOEPXP7w91Q0i5VjBrNyAk4xJFjU_RNz8E6xyr6h-mbB3EvrTg@mail.gmail.com>
	<CABPQxssO3v4BmjWY3dxPyAe77TdOUUgLcha_2Xz95_quPEgmdg@mail.gmail.com>
	<CALD+6GNxo+0Y53C_dc42=cz4=7LB2JZzb2o2Kij-vXotpteqUQ@mail.gmail.com>
	<CAOEPXP5mMbEzXFCbEafuAUS9ZAd5wkiaus2O5arC6w4QTp=5GQ@mail.gmail.com>
Date: Wed, 6 Nov 2013 19:03:28 -0800
Message-ID: <CAOEPXP4zEx6dJFaF6h3q_3sUoX52UjfFs=y-WpoW+ngf0yGMkA@mail.gmail.com>
Subject: Re: [PySpark]: reading arbitrary Hadoop InputFormats
From: Josh Rosen <rosenville@gmail.com>
To: "Spark Dev (Apache Incubator)" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=047d7b2ee11d95f03004ea8d82de
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b2ee11d95f03004ea8d82de
Content-Type: text/plain; charset=ISO-8859-1

I opened a pull request to add custom serializer support to PySpark:
https://github.com/apache/incubator-spark/pull/146

My pull request adds the plumbing for transferring data from Java to Python
using formats other than Pickle.  For example, look at how textFile() uses
MUTF8Deserializer to read strings from Java.  Hopefully this provides all
of the functionality needed to support MsgPack.

- Josh


On Thu, Oct 31, 2013 at 11:11 AM, Josh Rosen <rosenville@gmail.com> wrote:

> Hi Nick,
>
> This is a nice start.  I'd prefer to keep the Java sequenceFileAsText()
> and newHadoopFileAsText() methods inside PythonRDD instead of adding them
> to JavaSparkContext, since I think these methods are unlikely to be used
> directly by Java users (you can add these methods to the PythonRDD
> companion object, which is how readRDDFromPickleFile is implemented:
> https://github.com/apache/incubator-spark/blob/branch-0.8/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala#L255
> )
>
> For MsgPack, the UnpicklingError is because the Python worker expects to
> receive its input in a pickled format.  In my prototype of custom
> serializers, I modified the PySpark worker to receive its
> serialization/deserialization function as input (
> https://github.com/JoshRosen/spark/blob/59b6b43916dc84fc8b83f22eb9ce13a27bc51ec0/python/pyspark/worker.py#L41)
> and added logic to pass the appropriate serializers based on each stage's
> input and output formats (
> https://github.com/JoshRosen/spark/blob/59b6b43916dc84fc8b83f22eb9ce13a27bc51ec0/python/pyspark/rdd.py#L42
> ).
>
> At some point, I'd like to port my custom serializers code to PySpark; if
> anyone's interested in helping, I'd be glad to write up some additional
> notes on how this should work.
>
> - Josh
>
> On Wed, Oct 30, 2013 at 2:25 PM, Nick Pentreath <nick.pentreath@gmail.com>wrote:
>
>> Thanks Josh, Patrick for the feedback.
>>
>> Based on Josh's pointers I have something working for JavaPairRDD ->
>> PySpark RDD[(String, String)]. This just calls the toString method on each
>> key and value as before, but without the need for a delimiter. For
>> SequenceFile, it uses SequenceFileAsTextInputFormat which itself calls
>> toString to convert to Text for keys and values. We then call toString
>> (again) ourselves to get Strings to feed to writeAsPickle.
>>
>> Details here: https://gist.github.com/MLnick/7230588
>>
>> This also illustrates where the "wrapper function" api would fit in. All
>> that is required is to define a T => String for key and value.
>>
>> I started playing around with MsgPack and can sort of get things to work
>> in
>> Scala, but am struggling with getting the raw bytes to be written properly
>> in PythonRDD (I think it is treating them as pickled byte arrays when they
>> are not, but when I removed the 'stripPickle' calls and amended the length
>> (-6) I got "UnpicklingError: invalid load key, ' '. ").
>>
>> Another issue is that MsgPack does well at writing "structures" - like
>> Java
>> classes with public fields that are fairly simple - but for example the
>> Writables have private fields so you end up with nothing being written.
>> This looks like it would require custom "Templates" (serialization
>> functions effectively) for many classes, which means a lot of custom code
>> for a user to write to use it. Fortunately for most of the common
>> Writables
>> a toString does the job. Will keep looking into it though.
>>
>> Anyway, Josh if you have ideas or examples on the "Wrapper API from
>> Python"
>> that you mentioned, I'd be interested to hear them.
>>
>> If you think this is worth working up as a Pull Request covering
>> SequenceFiles and custom InputFormats with default toString conversions
>> and
>> the ability to specify Wrapper functions, I can clean things up more, add
>> some functionality and tests, and also test to see if common things like
>> the "normal" Writables and reading from things like HBase and Cassandra
>> can
>> be made to work nicely (any other common use cases that you think make
>> sense?).
>>
>> Thoughts, comments etc welcome.
>>
>> Nick
>>
>>
>>
>> On Fri, Oct 25, 2013 at 11:03 PM, Patrick Wendell <pwendell@gmail.com
>> >wrote:
>>
>> > As a starting point, a version where people just write their own
>> "wrapper"
>> > functions to convert various HadoopFiles into String <K, V> files could
>> go
>> > a long way. We could even have a few built-in versions, such as dealing
>> > with Sequence files that are <String, String>. Basically, the user
>> needs to
>> > write a translator in Java/Scala that produces textual records from
>> > whatever format that want. Then, they make sure this is included in the
>> > classpath when running PySpark.
>> >
>> > As Josh is saying, I'm pretty sure this is already possible, but we may
>> > want to document it for users. In many organizations they might have 1-2
>> > people who can write the Java/Scala to do this but then many more people
>> > who are comfortable using python once it's setup.
>> >
>> > - Patrick
>> >
>> > On Fri, Oct 25, 2013 at 11:00 AM, Josh Rosen <rosenville@gmail.com>
>> wrote:
>> >
>> > > Hi Nick,
>> > >
>> > > I've seen several requests for SequenceFile support in PySpark, so
>> > there's
>> > > definitely demand for this feature.
>> > >
>> > > I like the idea of passing MsgPack'ed data (or some other structured
>> > > format) from Java to the Python workers.  My early prototype of custom
>> > > serializers (described at
>> > >
>> > >
>> >
>> https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals#PySparkInternals-customserializers
>> > > )
>> > > might be useful for implementing this.  Proper custom serializer
>> support
>> > > would handle the bookkeeping for tracking each stage's input and
>> output
>> > > formats and supplying the appropriate deserialization functions to the
>> > > Python worker, so the Python worker would be able to directly read the
>> > > MsgPack'd data that's sent to it.
>> > >
>> > > Regarding a wrapper API, it's actually possible to initially transform
>> > data
>> > > using Scala/Java and perform the remainder of the processing in
>> PySpark.
>> > >  This involves adding the appropriate compiled to the Java classpath
>> and
>> > a
>> > > bit of work in Py4J to create the Java/Scala RDD and wrap it for use
>> by
>> > > PySpark.  I can hack together a rough example of this if anyone's
>> > > interested, but it would need some work to be developed into a
>> > > user-friendly API.
>> > >
>> > > If you wanted to extend your proof-of-concept to handle the cases
>> where
>> > > keys and values have parseable toString() values, I think you could
>> > remove
>> > > the need for a delimiter by creating a PythonRDD from the
>> newHadoopFile
>> > > JavaPairRDD and adding a new method to writeAsPickle (
>> > >
>> > >
>> >
>> https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala#L224
>> > > )
>> > > to dump its contents as a pickled pair of strings.  (Aside: most of
>> > > writeAsPickle() would probably need be eliminated or refactored when
>> > adding
>> > > general custom serializer support).
>> > >
>> > > - Josh
>> > >
>> > > On Thu, Oct 24, 2013 at 11:18 PM, Nick Pentreath
>> > > <nick.pentreath@gmail.com>wrote:
>> > >
>> > > > Hi Spark Devs
>> > > >
>> > > > I was wondering what appetite there may be to add the ability for
>> > PySpark
>> > > > users to create RDDs from (somewhat) arbitrary Hadoop InputFormats.
>> > > >
>> > > > In my data pipeline for example, I'm currently just using Scala
>> (partly
>> > > > because I love it but also because I am heavily reliant on quite
>> custom
>> > > > Hadoop InputFormats for reading data). However, many users may
>> prefer
>> > to
>> > > > use PySpark as much as possible (if not for everything). Reasons
>> might
>> > > > include the need to use some Python library. While I don't do it
>> yet, I
>> > > can
>> > > > certainly see an attractive use case for using say scikit-learn /
>> numpy
>> > > to
>> > > > do data analysis & machine learning in Python. Added to this my
>> > cofounder
>> > > > knows Python well but not Scala so it can be very beneficial to do a
>> > lot
>> > > of
>> > > > stuff in Python.
>> > > >
>> > > > For text-based data this is fine, but reading data in from more
>> complex
>> > > > Hadoop formats is an issue.
>> > > >
>> > > > The current approach would of course be to write an ETL-style
>> > Java/Scala
>> > > > job and then process in Python. Nothing wrong with this, but I was
>> > > thinking
>> > > > about ways to allow Python to access arbitrary Hadoop InputFormats.
>> > > >
>> > > > Here is a quick proof of concept:
>> > https://gist.github.com/MLnick/7150058
>> > > >
>> > > > This works for simple stuff like SequenceFile with simple Writable
>> > > > key/values.
>> > > >
>> > > > To work with more complex files, perhaps an approach is to
>> manipulate
>> > > > Hadoop JobConf via Python and pass that in. The one downside is of
>> > course
>> > > > that the InputFormat (well actually the Key/Value classes) must
>> have a
>> > > > toString that makes sense so very custom stuff might not work.
>> > > >
>> > > > I wonder if it would be possible to take the objects that are
>> yielded
>> > via
>> > > > the InputFormat and convert them into some representation like
>> > ProtoBuf,
>> > > > MsgPack, Avro, JSON, that can be read relatively more easily from
>> > Python?
>> > > >
>> > > > Another approach could be to allow a simple "wrapper API" such that
>> one
>> > > can
>> > > > write a wrapper function T => String and pass that into an
>> > > > InputFormatWrapper that takes an arbitrary InputFormat and yields
>> > Strings
>> > > > for the keys and values. Then all that is required is to compile
>> that
>> > > > function and add it to the SPARK_CLASSPATH and away you go!
>> > > >
>> > > > Thoughts?
>> > > >
>> > > > Nick
>> > > >
>> > >
>> >
>>
>
>

--047d7b2ee11d95f03004ea8d82de--

From dev-return-727-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov  7 19:32:57 2013
Return-Path: <dev-return-727-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5DB0110443
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Nov 2013 19:32:57 +0000 (UTC)
Received: (qmail 31158 invoked by uid 500); 7 Nov 2013 19:32:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 31030 invoked by uid 500); 7 Nov 2013 19:32:56 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 31020 invoked by uid 99); 7 Nov 2013 19:32:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 19:32:56 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 19:32:52 +0000
Received: by mail-pd0-f170.google.com with SMTP id v10so1050315pde.1
        for <dev@spark.incubator.apache.org>; Thu, 07 Nov 2013 11:32:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=wRaODQDjgEEJcQAMaOxpVr8e9hWvQq3ROx6gpujfLDE=;
        b=HXNVPSr9Y/1jKRFgu0xlWYWUf6T6M+z5BH1rUm4ZuKJl8+m+ibmMtAsbe1KDi1H/5T
         0NUihMyrP5Yx10UeRmUr3PMN7pqdR5qcJCU3AGRqwpkvparq0oWYFWc3rBO7wcC5hAHw
         tQb11jDseWW+9NknmozXUKARJ7vIXZUt5CRU9V/tfhaBbVKunbdPIjotGtuC2SNHr1Wx
         QNUCMQAAb0wg1kIrcMFRpwYvgAV8tebF1O0DxzJ5lWV7dT+i/ZhYNaKk9rdms69w6RsI
         jpIAfEW0SholdvlXGzGW8hx9MYmFSX5hQ+QYyD0sdBOEgmkxr2nHLpOuux4sZylI7Goc
         vPCQ==
X-Received: by 10.66.217.166 with SMTP id oz6mr11531568pac.22.1383852751763;
        Thu, 07 Nov 2013 11:32:31 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id sg1sm6873104pbb.16.2013.11.07.11.32.30
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 07 Nov 2013 11:32:30 -0800 (PST)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
Subject: Re: issue regarding akka, protobuf and Hadoop version
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <391D65D0EBFC9B4B95E117F72A360F1A010CF28A@SHSMSX101.ccr.corp.intel.com>
Date: Thu, 7 Nov 2013 11:32:28 -0800
Content-Transfer-Encoding: quoted-printable
Message-Id: <AF58A69E-D351-4B33-87D7-1D82E981B473@gmail.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com> <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com> <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com> <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com> <391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com> <CACBYxKKZWMqhao6UekxnGQYo1BdnVSuMBvQ1D7sK4XxmYcezWA@mail.gmail.com> <CAC1ssC4Z5yxvbLYh8sVr3g2x3fpPbkezLXSL2FgfYxJ3C+s_5w@mail.gmail.com> <BCA2A701-DD77-4D9C-965B-210652C70CAC@gmail.com> <391D65D0EBFC9B4B95E117F72A360F1A010CF28A@SHSMSX101.ccr.corp.intel.com>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

I don=92t think we can maintain separate branches with different =
versions of Akka, because there are often API changes in Akka. Maybe =
that would be a short-term solution, but any release we make should =
ideally support all versions of Hadoop.

Matei

On Nov 6, 2013, at 6:30 PM, Liu, Raymond <raymond.liu@intel.com> wrote:

> Then that will be two branch code right? One for hadoop 2.2+ , one for =
older versions. And with different akka/protobuf version dependency.
>=20
>=20
> Best Regards,
> Raymond Liu
>=20
> -----Original Message-----
> From: Matei Zaharia [mailto:matei.zaharia@gmail.com]=20
> Sent: Thursday, November 07, 2013 10:13 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: issue regarding akka, protobuf and Hadoop version
>=20
> Moving to Akka 2.3 won't solve this problem unfortunately, because =
people still want to run Spark with older Hadoop versions too, which =
will have Protobuf 2.4.1. Have you tested it with those?
>=20
> Matei
>=20
> On Nov 6, 2013, at 3:32 AM, Reynold Xin <rxin@apache.org> wrote:
>=20
>> That is correct. However, there is no guarantee right now that Akka=20=

>> 2.3 will work correctly for us. We haven't tested it enough yet (or=20=

>> rather, we haven't tested it at all) E.g. see:
>> https://github.com/apache/incubator-spark/pull/131
>>=20
>> We want to make Spark 0.9.0 based on Scala 2.10, but we have also =
been=20
>> discussing ideas to make a Scala 2.10 version of Spark 0.8.x so it=20
>> enables users to move to Scala 2.10 earlier if they want.
>>=20
>>=20
>> On Wed, Nov 6, 2013 at 12:29 AM, Sandy Ryza <sandy.ryza@cloudera.com> =
wrote:
>>=20
>>> For my own understanding, is this summary correct?
>>> Spark will move to scala 2.10, which means it can support akka=20
>>> 2.3-M1, which supports protobuf 2.5, which will allow Spark to run =
on Hadoop 2.2.
>>>=20
>>> What will be the first Spark version with these changes?  Are the=20
>>> Akka features that Spark relies on stable in 2.3-M1?
>>>=20
>>> thanks,
>>> Sandy
>>>=20
>>>=20
>>>=20
>>> On Tue, Nov 5, 2013 at 12:12 AM, Liu, Raymond =
<raymond.liu@intel.com>
>>> wrote:
>>>=20
>>>> Just pushed a pull request which based on scala 2.10 branch for=20
>>>> hadoop 2.2.0.
>>>> Yarn-standalone mode workable, but need a few more fine tune works.
>>>> Not really for pull, but as a placeholder, and for someone who want=20=

>>>> to take a look.
>>>>=20
>>>> Best Regards,
>>>> Raymond Liu
>>>>=20
>>>>=20
>>>> -----Original Message-----
>>>> From: Reynold Xin [mailto:rxin@apache.org]
>>>> Sent: Tuesday, November 05, 2013 10:07 AM
>>>> To: dev@spark.incubator.apache.org
>>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>>=20
>>>> I think we are near the end of Scala 2.9.3 development, and will=20
>>>> merge
>>> the
>>>> Scala 2.10 branch into master and make it the future very soon=20
>>>> (maybe
>>> next
>>>> week).  This problem will go away.
>>>>=20
>>>> Meantime, we are relying on periodically merging the master into =
the
>>> Scala
>>>> 2.10 branch.
>>>>=20
>>>>=20
>>>> On Mon, Nov 4, 2013 at 5:53 PM, Liu, Raymond =
<raymond.liu@intel.com>
>>>> wrote:
>>>>=20
>>>>> I plan to do the work on scala-2.10 branch, which already move to=20=

>>>>> akka 2.2.3, hope that to move to akka 2.3-M1 (which support=20
>>>>> protobuf 2.5.x) will not cause many problem and make it a test to=20=

>>>>> see is there further issues, then wait for the formal release of=20=

>>>>> akka 2.3.x
>>>>>=20
>>>>> While the issue is that I can see many commits on master branch is=20=

>>>>> not merged into scala-2.10 branch yet. The latest merge seems to=20=

>>>>> happen on OCT.11, while as I mentioned in the dev branch =
merge/sync=20
>>>>> thread, seems that many earlier commit is not included and which=20=

>>>>> will surely bring extra works on future code merging/rebase. So=20
>>>>> again, what's the code sync strategy and what's the plan of merge =
back into master?
>>>>>=20
>>>>> Best Regards,
>>>>> Raymond Liu
>>>>>=20
>>>>>=20
>>>>> -----Original Message-----
>>>>> From: Reynold Xin [mailto:rxin@apache.org]
>>>>> Sent: Tuesday, November 05, 2013 8:34 AM
>>>>> To: dev@spark.incubator.apache.org
>>>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>>>=20
>>>>> I chatted with Matt Massie about this, and here are some options:
>>>>>=20
>>>>> 1. Use dependency injection in google-guice to make Akka use one=20=

>>>>> version of protobuf, and YARN use the other version.
>>>>>=20
>>>>> 2. Look into OSGi to accomplish the same goal.
>>>>>=20
>>>>> 3. Rewrite the messaging part of Spark to use a simple, custom RPC=20=

>>>>> library instead of Akka. We are really only using a very simple=20
>>>>> subset of Akka features, and we can probably implement a simple =
RPC=20
>>>>> library tailored for Spark quickly. We should only do this as the =
last resort.
>>>>>=20
>>>>> 4. Talk to Akka guys and hope they can make a maintenance release=20=

>>>>> of Akka that supports protobuf 2.5.
>>>>>=20
>>>>>=20
>>>>> None of these are ideal, but we'd have to pick one. It would be=20
>>>>> great if you have other suggestions.
>>>>>=20
>>>>>=20
>>>>> On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond=20
>>>>> <raymond.liu@intel.com>
>>>>> wrote:
>>>>>=20
>>>>>> Hi
>>>>>>=20
>>>>>>       I am working on porting spark onto Hadoop 2.2.0, With some=20=

>>>>>> renaming and call into new YARN API works done. I can run up the=20=

>>>>>> spark master. While I encounter the issue that Executor Actor=20
>>>>>> could not connecting to Driver actor.
>>>>>>=20
>>>>>>       After some investigation, I found the root cause is that=20
>>>>>> the akka-remote do not support protobuf 2.5.0 before 2.3. And=20
>>>>>> hadoop move to protobuf 2.5.0 from 2.1-beta.
>>>>>>=20
>>>>>>       The issue is that if I exclude the akka dependency from=20
>>>>>> hadoop and force protobuf dependency to 2.4.1, the =
compile/packing=20
>>>>>> will fail since hadoop common jar require a new interface from
>>>> protobuf 2.5.0.
>>>>>>=20
>>>>>>        So any suggestion on this?
>>>>>>=20
>>>>>> Best Regards,
>>>>>> Raymond Liu
>>>>>>=20
>>>>>=20
>>>>=20
>>>=20
>=20


From dev-return-728-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov  7 20:09:34 2013
Return-Path: <dev-return-728-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 74A851069E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Nov 2013 20:09:34 +0000 (UTC)
Received: (qmail 32897 invoked by uid 500); 7 Nov 2013 20:09:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 32864 invoked by uid 500); 7 Nov 2013 20:09:34 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 32856 invoked by uid 99); 7 Nov 2013 20:09:34 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 20:09:34 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of henry.saputra@gmail.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 20:09:29 +0000
Received: by mail-wi0-f178.google.com with SMTP id hn9so1206529wib.11
        for <dev@spark.incubator.apache.org>; Thu, 07 Nov 2013 12:09:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=JpyRZuF/HPIEXpq5WLuk5SDUEygzSPe1Un/RR+e3ens=;
        b=BwS+p7i+h9+9l6/UFyjivPsIOvChZvbbtihs9cdf7eg7cZ6mM/aSIXEsbCSHsya1r3
         97N0oXpj+0F853TFKLMomUbIagskN9AlddOwTOXRCxrz7ezxsJQpSuG+9NfJ2Wh9fRyH
         kEk9ggu0uXQjZ/Z52WM9t9Grg3oPn3uure2l4W7kpMGeNlTRysQTos342P8S/U/yy0kR
         f62u0DSDa5uWXNaEiXpyLSM4dNoyanPUsxMUPH4oEyY58IKnbo6nH0I16nX7uX6jhVNL
         ipKEBatCTC3K61vuTSB8p1T9FlYNIg83JstP9kqBjJD4ygeYYi3ZuK/VWC1UpzYaZ9Cp
         FdDg==
MIME-Version: 1.0
X-Received: by 10.194.237.226 with SMTP id vf2mr8073996wjc.58.1383854947884;
 Thu, 07 Nov 2013 12:09:07 -0800 (PST)
Received: by 10.216.201.2 with HTTP; Thu, 7 Nov 2013 12:09:07 -0800 (PST)
Date: Thu, 7 Nov 2013 12:09:07 -0800
Message-ID: <CALuGr6bDwQ_E6fFvDvJ3u4hqbj3mCcMSnM4zS2b6BU0xjEjpbg@mail.gmail.com>
Subject: Documenting the release process for Apache Spark
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Patrick,

Did you end up writing up the steps you were taking to generate the
Apache Spark release to provide help to the next Apache Spark RE?

I remember you were trying to create one after we released 0.8

Thanks,

- Henry

From dev-return-729-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov  7 20:58:25 2013
Return-Path: <dev-return-729-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 54E0110851
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Nov 2013 20:58:25 +0000 (UTC)
Received: (qmail 16500 invoked by uid 500); 7 Nov 2013 20:58:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16469 invoked by uid 500); 7 Nov 2013 20:58:25 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 16459 invoked by uid 99); 7 Nov 2013 20:58:25 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 20:58:25 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of imran@quantifind.com designates 209.85.215.46 as permitted sender)
Received: from [209.85.215.46] (HELO mail-la0-f46.google.com) (209.85.215.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 20:58:18 +0000
Received: by mail-la0-f46.google.com with SMTP id el20so913130lab.19
        for <dev@spark.incubator.apache.org>; Thu, 07 Nov 2013 12:57:58 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=0H+9MpybX/yjuFT00Y26pwGuPhHepo6nhzwFCB3dGF0=;
        b=UVV4OrRxcfpCDCDtSV0xXzdAMFOaUPIJFQ4otaWHxLKPER0hL1GmP22y/ji0pZI2j3
         qV0AYT2xvqNi/JaV7FNEnBiMUPWXH29vx8P7HX4Xll6rMyk99vkQp6dNwHd4wXo8liku
         JDEqIdwKx5QATxNwAAsRI0Tb1RXfOWZi6sJ15tGKL3cj+m9tuMdt/GUL/u5nobgeDM6l
         JUa9hovF3ILiQWGxog/fPpKuiujt/MlZ74H5vk+mEhcAkOLdt4CDFFpXiMZN0ly9Stet
         nXVEvvl8g1eWKEyyuc77RMdzy9yfiGpQ9CYUNquInqv5A5vCHMTD8dbIzIsivDEd3DPX
         i/IQ==
X-Gm-Message-State: ALoCoQmZ8Ky9PynvGHVquAgRqSR9aDeqEf69NhhjrWlEZZ7EOSyW1tSFCho4dHjWJEWd0tlc9beX
MIME-Version: 1.0
X-Received: by 10.112.205.34 with SMTP id ld2mr7690865lbc.27.1383857877952;
 Thu, 07 Nov 2013 12:57:57 -0800 (PST)
Received: by 10.112.7.34 with HTTP; Thu, 7 Nov 2013 12:57:57 -0800 (PST)
X-Originating-IP: [98.193.39.168]
Date: Thu, 7 Nov 2013 14:57:57 -0600
Message-ID: <CAO24D=QqmRAyjPT6fs0zDu0Gw7ajSUKvAfEZgcEP1LmntaguHw@mail.gmail.com>
Subject: master can double-register workers
From: Imran Rashid <imran@quantifind.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c3d99a3c517504ea9c85d2
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3d99a3c517504ea9c85d2
Content-Type: text/plain; charset=ISO-8859-1

Hi,

I've noticed a bug where the master can double-register workers.  I've got
a patch for it, but my patch has some conflicts w/ an open PR (the
whiltelist+spreadout one), so instead I modified on top of that, and
created a PR to our fork:

https://github.com/quantifind/incubator-spark/pull/2

(does anybody know of a better way to deal w/ pull requests on top of open
pull requests?)

Since its a bug fix, I'd like to get discussion going on it even though it
can't be merged until the other PR is merged.  Also, if you think there is
any hesitation on the other PR, I can instead apply the fix to master, and
then just update the other PR later.

thanks

imran

--001a11c3d99a3c517504ea9c85d2--

From dev-return-730-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov  7 22:31:04 2013
Return-Path: <dev-return-730-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E828A10D89
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu,  7 Nov 2013 22:31:03 +0000 (UTC)
Received: (qmail 95957 invoked by uid 500); 7 Nov 2013 22:31:03 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95869 invoked by uid 500); 7 Nov 2013 22:31:03 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 95740 invoked by uid 99); 7 Nov 2013 22:31:03 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 22:31:03 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.47 as permitted sender)
Received: from [209.85.160.47] (HELO mail-pb0-f47.google.com) (209.85.160.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 07 Nov 2013 22:30:58 +0000
Received: by mail-pb0-f47.google.com with SMTP id rq13so1234787pbb.6
        for <multiple recipients>; Thu, 07 Nov 2013 14:30:38 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:content-transfer-encoding:subject:message-id:date
         :to:mime-version;
        bh=8gs01PyiBjSsFRppw19gf7pTRXgQCyHZidYWkyAWKkw=;
        b=0fpYIho1b3eA1lDj4jICGRvIkokMFnxVMPBCt1xmK+Agd3uVkwCl/RzOyp0GjbHP+J
         ROu91BN2HbUzMKvWH9aNhUmoMKGV+uI9PVNGgPBVsdSZbSmjtPQJVr+dXKdVaMZz9jFa
         PuMP1nuYgI22MElLN2luPHHELsOz3DgRWKS7XsAAWfRKOOOtZEbrLG8lKwCvwu+hNi49
         HDmitLGRMLu4w1AbGlCBrl9825tCAr7IeiZsvxLRE88FpVJldLB9NvFpwTw8htvPTg1W
         kHY9OKnfnvW4CRSm5YwTLEcX79utTiJpG5Tfa3LSMiTV8LjeNQb8QSJ2brOwS7Huk43y
         KW9Q==
X-Received: by 10.68.253.67 with SMTP id zy3mr11451468pbc.137.1383863437894;
        Thu, 07 Nov 2013 14:30:37 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id sy10sm9071187pac.15.2013.11.07.14.30.36
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 07 Nov 2013 14:30:36 -0800 (PST)
From: Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: text/plain; charset=us-ascii
Content-Transfer-Encoding: quoted-printable
Subject: Spark Summit agenda posted
Message-Id: <085ED1F7-F5DB-4512-B907-0C40B32C3A7A@gmail.com>
Date: Thu, 7 Nov 2013 14:30:34 -0800
To: user@spark.incubator.apache.org,
 spark-users@googlegroups.com,
 dev@spark.incubator.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi everyone,

We're glad to announce the agenda of the Spark Summit, which will happen =
on December 2nd and 3rd in San Francisco. We have 5 keynotes and 24 =
talks lined up, from 18 different companies. Check out the agenda here: =
http://spark-summit.org/agenda/.

This will be the biggest Spark event yet, with some very cool use case =
talks, so we hope to see you there! Sign up now to still get access to =
the early-bird registration rate.

Matei


From dev-return-731-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov  8 01:11:56 2013
Return-Path: <dev-return-731-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 133541049E
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Nov 2013 01:11:56 +0000 (UTC)
Received: (qmail 34709 invoked by uid 500); 8 Nov 2013 01:11:55 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34654 invoked by uid 500); 8 Nov 2013 01:11:55 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 34646 invoked by uid 99); 8 Nov 2013 01:11:55 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 01:11:55 +0000
X-ASF-Spam-Status: No, hits=-5.0 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of raymond.liu@intel.com designates 134.134.136.20 as permitted sender)
Received: from [134.134.136.20] (HELO mga02.intel.com) (134.134.136.20)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 01:11:49 +0000
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
  by orsmga101.jf.intel.com with ESMTP; 07 Nov 2013 17:11:27 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,656,1378882800"; 
   d="scan'208";a="424055840"
Received: from fmsmsx106.amr.corp.intel.com ([10.19.9.37])
  by fmsmga001.fm.intel.com with ESMTP; 07 Nov 2013 17:11:26 -0800
Received: from fmsmsx114.amr.corp.intel.com (10.18.116.8) by
 FMSMSX106.amr.corp.intel.com (10.19.9.37) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Thu, 7 Nov 2013 17:11:26 -0800
Received: from shsmsx151.ccr.corp.intel.com (10.239.6.50) by
 FMSMSX114.amr.corp.intel.com (10.18.116.8) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Thu, 7 Nov 2013 17:11:26 -0800
Received: from shsmsx101.ccr.corp.intel.com ([169.254.1.240]) by
 SHSMSX151.ccr.corp.intel.com ([169.254.3.142]) with mapi id 14.03.0123.003;
 Fri, 8 Nov 2013 09:11:22 +0800
From: "Liu, Raymond" <raymond.liu@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: issue regarding akka, protobuf and Hadoop version
Thread-Topic: issue regarding akka, protobuf and Hadoop version
Thread-Index: Ac7ZMduY/ltyQ77yToubUDTRusm1uwAScDmAABLwNXD//4KBgP//FKCAgASYJev///vFgIAAoE0A//8blyA=
Date: Fri, 8 Nov 2013 01:11:22 +0000
Message-ID: <391D65D0EBFC9B4B95E117F72A360F1A010CFCB6@SHSMSX101.ccr.corp.intel.com>
References: <391D65D0EBFC9B4B95E117F72A360F1A010CDCA3@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC6JnfP-qt5EDRCGXv8Gq4ZrG_6c2SmNJfUWVm2Q0A0HaQ@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CE3A4@SHSMSX101.ccr.corp.intel.com>
 <CAC1ssC7-9EL_ACUasNR44Veewbq-Tump16P8rQpnvRquueavFw@mail.gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CE675@SHSMSX101.ccr.corp.intel.com>
 <CACBYxKKZWMqhao6UekxnGQYo1BdnVSuMBvQ1D7sK4XxmYcezWA@mail.gmail.com>
 <CAC1ssC4Z5yxvbLYh8sVr3g2x3fpPbkezLXSL2FgfYxJ3C+s_5w@mail.gmail.com>
 <BCA2A701-DD77-4D9C-965B-210652C70CAC@gmail.com>
 <391D65D0EBFC9B4B95E117F72A360F1A010CF28A@SHSMSX101.ccr.corp.intel.com>
 <AF58A69E-D351-4B33-87D7-1D82E981B473@gmail.com>
In-Reply-To: <AF58A69E-D351-4B33-87D7-1D82E981B473@gmail.com>
Accept-Language: zh-CN, en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Ok, so how about put different code into separate dir, and build separate p=
ackages depending on user config.

Best Regards,
Raymond Liu


-----Original Message-----
From: Matei Zaharia [mailto:matei.zaharia@gmail.com]=20
Sent: Friday, November 08, 2013 3:32 AM
To: dev@spark.incubator.apache.org
Subject: Re: issue regarding akka, protobuf and Hadoop version

I don't think we can maintain separate branches with different versions of =
Akka, because there are often API changes in Akka. Maybe that would be a sh=
ort-term solution, but any release we make should ideally support all versi=
ons of Hadoop.

Matei

On Nov 6, 2013, at 6:30 PM, Liu, Raymond <raymond.liu@intel.com> wrote:

> Then that will be two branch code right? One for hadoop 2.2+ , one for ol=
der versions. And with different akka/protobuf version dependency.
>=20
>=20
> Best Regards,
> Raymond Liu
>=20
> -----Original Message-----
> From: Matei Zaharia [mailto:matei.zaharia@gmail.com]
> Sent: Thursday, November 07, 2013 10:13 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: issue regarding akka, protobuf and Hadoop version
>=20
> Moving to Akka 2.3 won't solve this problem unfortunately, because people=
 still want to run Spark with older Hadoop versions too, which will have Pr=
otobuf 2.4.1. Have you tested it with those?
>=20
> Matei
>=20
> On Nov 6, 2013, at 3:32 AM, Reynold Xin <rxin@apache.org> wrote:
>=20
>> That is correct. However, there is no guarantee right now that Akka
>> 2.3 will work correctly for us. We haven't tested it enough yet (or=20
>> rather, we haven't tested it at all) E.g. see:
>> https://github.com/apache/incubator-spark/pull/131
>>=20
>> We want to make Spark 0.9.0 based on Scala 2.10, but we have also=20
>> been discussing ideas to make a Scala 2.10 version of Spark 0.8.x so=20
>> it enables users to move to Scala 2.10 earlier if they want.
>>=20
>>=20
>> On Wed, Nov 6, 2013 at 12:29 AM, Sandy Ryza <sandy.ryza@cloudera.com> wr=
ote:
>>=20
>>> For my own understanding, is this summary correct?
>>> Spark will move to scala 2.10, which means it can support akka=20
>>> 2.3-M1, which supports protobuf 2.5, which will allow Spark to run on H=
adoop 2.2.
>>>=20
>>> What will be the first Spark version with these changes?  Are the=20
>>> Akka features that Spark relies on stable in 2.3-M1?
>>>=20
>>> thanks,
>>> Sandy
>>>=20
>>>=20
>>>=20
>>> On Tue, Nov 5, 2013 at 12:12 AM, Liu, Raymond=20
>>> <raymond.liu@intel.com>
>>> wrote:
>>>=20
>>>> Just pushed a pull request which based on scala 2.10 branch for=20
>>>> hadoop 2.2.0.
>>>> Yarn-standalone mode workable, but need a few more fine tune works.
>>>> Not really for pull, but as a placeholder, and for someone who want=20
>>>> to take a look.
>>>>=20
>>>> Best Regards,
>>>> Raymond Liu
>>>>=20
>>>>=20
>>>> -----Original Message-----
>>>> From: Reynold Xin [mailto:rxin@apache.org]
>>>> Sent: Tuesday, November 05, 2013 10:07 AM
>>>> To: dev@spark.incubator.apache.org
>>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>>=20
>>>> I think we are near the end of Scala 2.9.3 development, and will=20
>>>> merge
>>> the
>>>> Scala 2.10 branch into master and make it the future very soon=20
>>>> (maybe
>>> next
>>>> week).  This problem will go away.
>>>>=20
>>>> Meantime, we are relying on periodically merging the master into=20
>>>> the
>>> Scala
>>>> 2.10 branch.
>>>>=20
>>>>=20
>>>> On Mon, Nov 4, 2013 at 5:53 PM, Liu, Raymond=20
>>>> <raymond.liu@intel.com>
>>>> wrote:
>>>>=20
>>>>> I plan to do the work on scala-2.10 branch, which already move to=20
>>>>> akka 2.2.3, hope that to move to akka 2.3-M1 (which support=20
>>>>> protobuf 2.5.x) will not cause many problem and make it a test to=20
>>>>> see is there further issues, then wait for the formal release of=20
>>>>> akka 2.3.x
>>>>>=20
>>>>> While the issue is that I can see many commits on master branch is=20
>>>>> not merged into scala-2.10 branch yet. The latest merge seems to=20
>>>>> happen on OCT.11, while as I mentioned in the dev branch=20
>>>>> merge/sync thread, seems that many earlier commit is not included=20
>>>>> and which will surely bring extra works on future code=20
>>>>> merging/rebase. So again, what's the code sync strategy and what's th=
e plan of merge back into master?
>>>>>=20
>>>>> Best Regards,
>>>>> Raymond Liu
>>>>>=20
>>>>>=20
>>>>> -----Original Message-----
>>>>> From: Reynold Xin [mailto:rxin@apache.org]
>>>>> Sent: Tuesday, November 05, 2013 8:34 AM
>>>>> To: dev@spark.incubator.apache.org
>>>>> Subject: Re: issue regarding akka, protobuf and Hadoop version
>>>>>=20
>>>>> I chatted with Matt Massie about this, and here are some options:
>>>>>=20
>>>>> 1. Use dependency injection in google-guice to make Akka use one=20
>>>>> version of protobuf, and YARN use the other version.
>>>>>=20
>>>>> 2. Look into OSGi to accomplish the same goal.
>>>>>=20
>>>>> 3. Rewrite the messaging part of Spark to use a simple, custom RPC=20
>>>>> library instead of Akka. We are really only using a very simple=20
>>>>> subset of Akka features, and we can probably implement a simple=20
>>>>> RPC library tailored for Spark quickly. We should only do this as the=
 last resort.
>>>>>=20
>>>>> 4. Talk to Akka guys and hope they can make a maintenance release=20
>>>>> of Akka that supports protobuf 2.5.
>>>>>=20
>>>>>=20
>>>>> None of these are ideal, but we'd have to pick one. It would be=20
>>>>> great if you have other suggestions.
>>>>>=20
>>>>>=20
>>>>> On Sun, Nov 3, 2013 at 11:46 PM, Liu, Raymond=20
>>>>> <raymond.liu@intel.com>
>>>>> wrote:
>>>>>=20
>>>>>> Hi
>>>>>>=20
>>>>>>       I am working on porting spark onto Hadoop 2.2.0, With some=20
>>>>>> renaming and call into new YARN API works done. I can run up the=20
>>>>>> spark master. While I encounter the issue that Executor Actor=20
>>>>>> could not connecting to Driver actor.
>>>>>>=20
>>>>>>       After some investigation, I found the root cause is that=20
>>>>>> the akka-remote do not support protobuf 2.5.0 before 2.3. And=20
>>>>>> hadoop move to protobuf 2.5.0 from 2.1-beta.
>>>>>>=20
>>>>>>       The issue is that if I exclude the akka dependency from=20
>>>>>> hadoop and force protobuf dependency to 2.4.1, the=20
>>>>>> compile/packing will fail since hadoop common jar require a new=20
>>>>>> interface from
>>>> protobuf 2.5.0.
>>>>>>=20
>>>>>>        So any suggestion on this?
>>>>>>=20
>>>>>> Best Regards,
>>>>>> Raymond Liu
>>>>>>=20
>>>>>=20
>>>>=20
>>>=20
>=20


From dev-return-732-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov  8 10:21:12 2013
Return-Path: <dev-return-732-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 995A010252
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Nov 2013 10:21:12 +0000 (UTC)
Received: (qmail 79444 invoked by uid 500); 8 Nov 2013 10:20:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 79376 invoked by uid 500); 8 Nov 2013 10:20:47 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 79367 invoked by uid 99); 8 Nov 2013 10:20:46 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 10:20:46 +0000
X-ASF-Spam-Status: No, hits=2.5 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nick.pentreath@gmail.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 10:20:41 +0000
Received: by mail-vc0-f180.google.com with SMTP id lc6so1242290vcb.39
        for <dev@spark.incubator.apache.org>; Fri, 08 Nov 2013 02:20:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=TRxcw/l47HwTMIUogC7lOrkkKYnJcr7bmlVpxbM534I=;
        b=Poaqzs37j34sU5dlFRNRsRUuLV2TgLOHJ86xwK/csbWgPZnbFaL/TcDBdPLf69rsnW
         fJ357gSQUaIfjAvVz+JRTy9EaTnOvsuxqsCyVp3/HdtnDw3lr8B17IwploLRwNUaqiOS
         kfz28suOqmly2Hd1SaAZlYy/1laZqVqMkFVfCBAo1959Q8FjYFpsaAYtmIV3ynMsXtiQ
         4nEf4LT9yLEI2b3l9ZZMNDRgKGzZsvdvt1F7GYDDNSmgKKAoj18BD56eFIWhA9o+77Vc
         GCODKaXMVWHZ8hlgeodQZsqxZflUjcs5sc7dxlMRkBgu5G1VT2PLaUK+dTa8IUZnhSbv
         gp7A==
MIME-Version: 1.0
X-Received: by 10.58.6.239 with SMTP id e15mr1203999vea.29.1383906020957; Fri,
 08 Nov 2013 02:20:20 -0800 (PST)
Received: by 10.220.178.71 with HTTP; Fri, 8 Nov 2013 02:20:20 -0800 (PST)
In-Reply-To: <CAOEPXP4zEx6dJFaF6h3q_3sUoX52UjfFs=y-WpoW+ngf0yGMkA@mail.gmail.com>
References: <CALD+6GO4b2T+w6cSPxUtMJxTEipYJzw0Fd-1DshTOTQTwt9PqQ@mail.gmail.com>
	<CAOEPXP7w91Q0i5VjBrNyAk4xJFjU_RNz8E6xyr6h-mbB3EvrTg@mail.gmail.com>
	<CABPQxssO3v4BmjWY3dxPyAe77TdOUUgLcha_2Xz95_quPEgmdg@mail.gmail.com>
	<CALD+6GNxo+0Y53C_dc42=cz4=7LB2JZzb2o2Kij-vXotpteqUQ@mail.gmail.com>
	<CAOEPXP5mMbEzXFCbEafuAUS9ZAd5wkiaus2O5arC6w4QTp=5GQ@mail.gmail.com>
	<CAOEPXP4zEx6dJFaF6h3q_3sUoX52UjfFs=y-WpoW+ngf0yGMkA@mail.gmail.com>
Date: Fri, 8 Nov 2013 12:20:20 +0200
Message-ID: <CALD+6GNSPSWFujzvRcCbk_zXxd9yfdKpFRKfLOOER3m2KvL7sQ@mail.gmail.com>
Subject: Re: [PySpark]: reading arbitrary Hadoop InputFormats
From: Nick Pentreath <nick.pentreath@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b6d7f7ac822fd04eaa7ba8f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6d7f7ac822fd04eaa7ba8f
Content-Type: text/plain; charset=ISO-8859-1

Wow Josh, that looks great. I've been a bit swamped this week but as soon
as I get a chance I'll test out the PR in more detail and port over the
InputFormat stuff to use the new framework (including the changes you
suggested).

I can then look deeper into the MsgPack functionality to see if it can be
made to work in a generic enough manner without requiring huge amounts of
custom Templates to be written by users.

Will feed back asap.
N


On Thu, Nov 7, 2013 at 5:03 AM, Josh Rosen <rosenville@gmail.com> wrote:

> I opened a pull request to add custom serializer support to PySpark:
> https://github.com/apache/incubator-spark/pull/146
>
> My pull request adds the plumbing for transferring data from Java to Python
> using formats other than Pickle.  For example, look at how textFile() uses
> MUTF8Deserializer to read strings from Java.  Hopefully this provides all
> of the functionality needed to support MsgPack.
>
> - Josh
>
>
> On Thu, Oct 31, 2013 at 11:11 AM, Josh Rosen <rosenville@gmail.com> wrote:
>
> > Hi Nick,
> >
> > This is a nice start.  I'd prefer to keep the Java sequenceFileAsText()
> > and newHadoopFileAsText() methods inside PythonRDD instead of adding them
> > to JavaSparkContext, since I think these methods are unlikely to be used
> > directly by Java users (you can add these methods to the PythonRDD
> > companion object, which is how readRDDFromPickleFile is implemented:
> >
> https://github.com/apache/incubator-spark/blob/branch-0.8/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala#L255
> > )
> >
> > For MsgPack, the UnpicklingError is because the Python worker expects to
> > receive its input in a pickled format.  In my prototype of custom
> > serializers, I modified the PySpark worker to receive its
> > serialization/deserialization function as input (
> >
> https://github.com/JoshRosen/spark/blob/59b6b43916dc84fc8b83f22eb9ce13a27bc51ec0/python/pyspark/worker.py#L41
> )
> > and added logic to pass the appropriate serializers based on each stage's
> > input and output formats (
> >
> https://github.com/JoshRosen/spark/blob/59b6b43916dc84fc8b83f22eb9ce13a27bc51ec0/python/pyspark/rdd.py#L42
> > ).
> >
> > At some point, I'd like to port my custom serializers code to PySpark; if
> > anyone's interested in helping, I'd be glad to write up some additional
> > notes on how this should work.
> >
> > - Josh
> >
> > On Wed, Oct 30, 2013 at 2:25 PM, Nick Pentreath <
> nick.pentreath@gmail.com>wrote:
> >
> >> Thanks Josh, Patrick for the feedback.
> >>
> >> Based on Josh's pointers I have something working for JavaPairRDD ->
> >> PySpark RDD[(String, String)]. This just calls the toString method on
> each
> >> key and value as before, but without the need for a delimiter. For
> >> SequenceFile, it uses SequenceFileAsTextInputFormat which itself calls
> >> toString to convert to Text for keys and values. We then call toString
> >> (again) ourselves to get Strings to feed to writeAsPickle.
> >>
> >> Details here: https://gist.github.com/MLnick/7230588
> >>
> >> This also illustrates where the "wrapper function" api would fit in. All
> >> that is required is to define a T => String for key and value.
> >>
> >> I started playing around with MsgPack and can sort of get things to work
> >> in
> >> Scala, but am struggling with getting the raw bytes to be written
> properly
> >> in PythonRDD (I think it is treating them as pickled byte arrays when
> they
> >> are not, but when I removed the 'stripPickle' calls and amended the
> length
> >> (-6) I got "UnpicklingError: invalid load key, ' '. ").
> >>
> >> Another issue is that MsgPack does well at writing "structures" - like
> >> Java
> >> classes with public fields that are fairly simple - but for example the
> >> Writables have private fields so you end up with nothing being written.
> >> This looks like it would require custom "Templates" (serialization
> >> functions effectively) for many classes, which means a lot of custom
> code
> >> for a user to write to use it. Fortunately for most of the common
> >> Writables
> >> a toString does the job. Will keep looking into it though.
> >>
> >> Anyway, Josh if you have ideas or examples on the "Wrapper API from
> >> Python"
> >> that you mentioned, I'd be interested to hear them.
> >>
> >> If you think this is worth working up as a Pull Request covering
> >> SequenceFiles and custom InputFormats with default toString conversions
> >> and
> >> the ability to specify Wrapper functions, I can clean things up more,
> add
> >> some functionality and tests, and also test to see if common things like
> >> the "normal" Writables and reading from things like HBase and Cassandra
> >> can
> >> be made to work nicely (any other common use cases that you think make
> >> sense?).
> >>
> >> Thoughts, comments etc welcome.
> >>
> >> Nick
> >>
> >>
> >>
> >> On Fri, Oct 25, 2013 at 11:03 PM, Patrick Wendell <pwendell@gmail.com
> >> >wrote:
> >>
> >> > As a starting point, a version where people just write their own
> >> "wrapper"
> >> > functions to convert various HadoopFiles into String <K, V> files
> could
> >> go
> >> > a long way. We could even have a few built-in versions, such as
> dealing
> >> > with Sequence files that are <String, String>. Basically, the user
> >> needs to
> >> > write a translator in Java/Scala that produces textual records from
> >> > whatever format that want. Then, they make sure this is included in
> the
> >> > classpath when running PySpark.
> >> >
> >> > As Josh is saying, I'm pretty sure this is already possible, but we
> may
> >> > want to document it for users. In many organizations they might have
> 1-2
> >> > people who can write the Java/Scala to do this but then many more
> people
> >> > who are comfortable using python once it's setup.
> >> >
> >> > - Patrick
> >> >
> >> > On Fri, Oct 25, 2013 at 11:00 AM, Josh Rosen <rosenville@gmail.com>
> >> wrote:
> >> >
> >> > > Hi Nick,
> >> > >
> >> > > I've seen several requests for SequenceFile support in PySpark, so
> >> > there's
> >> > > definitely demand for this feature.
> >> > >
> >> > > I like the idea of passing MsgPack'ed data (or some other structured
> >> > > format) from Java to the Python workers.  My early prototype of
> custom
> >> > > serializers (described at
> >> > >
> >> > >
> >> >
> >>
> https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals#PySparkInternals-customserializers
> >> > > )
> >> > > might be useful for implementing this.  Proper custom serializer
> >> support
> >> > > would handle the bookkeeping for tracking each stage's input and
> >> output
> >> > > formats and supplying the appropriate deserialization functions to
> the
> >> > > Python worker, so the Python worker would be able to directly read
> the
> >> > > MsgPack'd data that's sent to it.
> >> > >
> >> > > Regarding a wrapper API, it's actually possible to initially
> transform
> >> > data
> >> > > using Scala/Java and perform the remainder of the processing in
> >> PySpark.
> >> > >  This involves adding the appropriate compiled to the Java classpath
> >> and
> >> > a
> >> > > bit of work in Py4J to create the Java/Scala RDD and wrap it for use
> >> by
> >> > > PySpark.  I can hack together a rough example of this if anyone's
> >> > > interested, but it would need some work to be developed into a
> >> > > user-friendly API.
> >> > >
> >> > > If you wanted to extend your proof-of-concept to handle the cases
> >> where
> >> > > keys and values have parseable toString() values, I think you could
> >> > remove
> >> > > the need for a delimiter by creating a PythonRDD from the
> >> newHadoopFile
> >> > > JavaPairRDD and adding a new method to writeAsPickle (
> >> > >
> >> > >
> >> >
> >>
> https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala#L224
> >> > > )
> >> > > to dump its contents as a pickled pair of strings.  (Aside: most of
> >> > > writeAsPickle() would probably need be eliminated or refactored when
> >> > adding
> >> > > general custom serializer support).
> >> > >
> >> > > - Josh
> >> > >
> >> > > On Thu, Oct 24, 2013 at 11:18 PM, Nick Pentreath
> >> > > <nick.pentreath@gmail.com>wrote:
> >> > >
> >> > > > Hi Spark Devs
> >> > > >
> >> > > > I was wondering what appetite there may be to add the ability for
> >> > PySpark
> >> > > > users to create RDDs from (somewhat) arbitrary Hadoop
> InputFormats.
> >> > > >
> >> > > > In my data pipeline for example, I'm currently just using Scala
> >> (partly
> >> > > > because I love it but also because I am heavily reliant on quite
> >> custom
> >> > > > Hadoop InputFormats for reading data). However, many users may
> >> prefer
> >> > to
> >> > > > use PySpark as much as possible (if not for everything). Reasons
> >> might
> >> > > > include the need to use some Python library. While I don't do it
> >> yet, I
> >> > > can
> >> > > > certainly see an attractive use case for using say scikit-learn /
> >> numpy
> >> > > to
> >> > > > do data analysis & machine learning in Python. Added to this my
> >> > cofounder
> >> > > > knows Python well but not Scala so it can be very beneficial to
> do a
> >> > lot
> >> > > of
> >> > > > stuff in Python.
> >> > > >
> >> > > > For text-based data this is fine, but reading data in from more
> >> complex
> >> > > > Hadoop formats is an issue.
> >> > > >
> >> > > > The current approach would of course be to write an ETL-style
> >> > Java/Scala
> >> > > > job and then process in Python. Nothing wrong with this, but I was
> >> > > thinking
> >> > > > about ways to allow Python to access arbitrary Hadoop
> InputFormats.
> >> > > >
> >> > > > Here is a quick proof of concept:
> >> > https://gist.github.com/MLnick/7150058
> >> > > >
> >> > > > This works for simple stuff like SequenceFile with simple Writable
> >> > > > key/values.
> >> > > >
> >> > > > To work with more complex files, perhaps an approach is to
> >> manipulate
> >> > > > Hadoop JobConf via Python and pass that in. The one downside is of
> >> > course
> >> > > > that the InputFormat (well actually the Key/Value classes) must
> >> have a
> >> > > > toString that makes sense so very custom stuff might not work.
> >> > > >
> >> > > > I wonder if it would be possible to take the objects that are
> >> yielded
> >> > via
> >> > > > the InputFormat and convert them into some representation like
> >> > ProtoBuf,
> >> > > > MsgPack, Avro, JSON, that can be read relatively more easily from
> >> > Python?
> >> > > >
> >> > > > Another approach could be to allow a simple "wrapper API" such
> that
> >> one
> >> > > can
> >> > > > write a wrapper function T => String and pass that into an
> >> > > > InputFormatWrapper that takes an arbitrary InputFormat and yields
> >> > Strings
> >> > > > for the keys and values. Then all that is required is to compile
> >> that
> >> > > > function and add it to the SPARK_CLASSPATH and away you go!
> >> > > >
> >> > > > Thoughts?
> >> > > >
> >> > > > Nick
> >> > > >
> >> > >
> >> >
> >>
> >
> >
>

--047d7b6d7f7ac822fd04eaa7ba8f--

From dev-return-733-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov  8 18:04:51 2013
Return-Path: <dev-return-733-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4EA1C103E3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Nov 2013 18:04:51 +0000 (UTC)
Received: (qmail 88707 invoked by uid 500); 8 Nov 2013 18:04:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88408 invoked by uid 500); 8 Nov 2013 18:04:49 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Delivered-To: moderator for dev@spark.incubator.apache.org
Received: (qmail 20443 invoked by uid 99); 8 Nov 2013 11:28:41 -0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of rafarevertf22@gmail.com designates 209.85.215.45 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=e6qml5OCtpZGDSo+cleuBMU4ejm4O6v/oHBSkBAnY8w=;
        b=GD57IVoys6OmeLlYGldvDL26lLdCRo50YXXGL8eCLFvGqZqTZBP9Zaw6OAhBH2oOSJ
         cNlysD8mnX1tCirFgRZIGL2EY0MzeMgNId+8SaBfDYkG1TYi7HENlczAGvYST/v07G19
         rR9POTunh26fHb9rvfE7bHgg0uLOZz85aZkLsKKGhRDzc54qyPWjugQ8svMlUN6T1AWU
         EyzuhG+8xQA7BZ2kQyKC5wgc2tP5lsyViRrHsmwHbBeYG+1OciTG3uUhOQAY2nTkx42i
         TBN/+9YisVx8s2RZZntPgvbtoyyKkGEj5kgqWah2eATr3z56P9FuIiI27+AdtomYXDkw
         9WDw==
X-Received: by 10.112.145.105 with SMTP id st9mr268800lbb.46.1383910094781;
 Fri, 08 Nov 2013 03:28:14 -0800 (PST)
MIME-Version: 1.0
In-Reply-To: <085ED1F7-F5DB-4512-B907-0C40B32C3A7A@gmail.com>
References: <085ED1F7-F5DB-4512-B907-0C40B32C3A7A@gmail.com>
From: "R. Revert" <rafarevertf22@gmail.com>
Date: Fri, 8 Nov 2013 06:27:54 -0500
Message-ID: <CACXBjEZkifdPSVQ4w_xx1phxKftW0JR5nddyXu2YtWDL0bPdgQ@mail.gmail.com>
Subject: Re: Spark Summit agenda posted
To: user@spark.incubator.apache.org
Cc: spark-users@googlegroups.com, dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b34369699c3f104eaa8ad9e
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b34369699c3f104eaa8ad9e
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

would the pdf slider=C2=B4s or video talks, be posted for the people that c=
an't
attend to the conference?

thanks






*#__________________*
*Atte.*
*Rafael R.*



2013/11/7 Matei Zaharia <matei.zaharia@gmail.com>

> Hi everyone,
>
> We're glad to announce the agenda of the Spark Summit, which will happen
> on December 2nd and 3rd in San Francisco. We have 5 keynotes and 24 talks
> lined up, from 18 different companies. Check out the agenda here:
> http://spark-summit.org/agenda/.
>
> This will be the biggest Spark event yet, with some very cool use case
> talks, so we hope to see you there! Sign up now to still get access to th=
e
> early-bird registration rate.
>
> Matei
>
>

--047d7b34369699c3f104eaa8ad9e--

From dev-return-734-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov  8 18:47:38 2013
Return-Path: <dev-return-734-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 56F77105F8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Nov 2013 18:47:38 +0000 (UTC)
Received: (qmail 98613 invoked by uid 500); 8 Nov 2013 18:47:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 98509 invoked by uid 500); 8 Nov 2013 18:47:37 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 98163 invoked by uid 99); 8 Nov 2013 18:47:37 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 18:47:37 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.52 as permitted sender)
Received: from [209.85.160.52] (HELO mail-pb0-f52.google.com) (209.85.160.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 18:47:30 +0000
Received: by mail-pb0-f52.google.com with SMTP id rr4so2513811pbb.11
        for <multiple recipients>; Fri, 08 Nov 2013 10:47:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :message-id:references:to;
        bh=iNx0giOlBV1hCvWDr7OnotL3uVu7lVudDCI/mNXNIZE=;
        b=PjHLnlN74gesEMRxc6tcz7h/btx+hjhzWfDHFb6FXKAN3LStA9JbGO2YluI00ym/PS
         sQTbiogwkaNzPvFzsWwCCjEGpS3A+stB6IhXbit1lM5vA4E5XbmYjfUuHKcQtDK1JTDx
         K1oi4yQfB8mWVMqWkjvfX715Ho7I9fcL1D6gqFbkQhQBQbySdK9gmwPyCp5LGG6eoN1Q
         sv6BlyKgPmZpWvm/fuQJ8+WjhnAm1Be7dnH23D6oFW8HJ4AinV1u9/93mmeIIMjOtbvK
         MZmnqTk3gxNvdCEdXp0qsUOiJcI0WSRPWD0HMCDWCiWFFReApui+7QrrNexwmM++xpjq
         AjMA==
X-Received: by 10.66.196.168 with SMTP id in8mr17041866pac.18.1383936429158;
        Fri, 08 Nov 2013 10:47:09 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id uw6sm13697989pbc.8.2013.11.08.10.47.07
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Fri, 08 Nov 2013 10:47:08 -0800 (PST)
Content-Type: multipart/alternative; boundary="Apple-Mail=_35A07CDE-95B6-4E47-AC1E-9972FA5029FD"
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
Subject: Re: Spark Summit agenda posted
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACXBjEZkifdPSVQ4w_xx1phxKftW0JR5nddyXu2YtWDL0bPdgQ@mail.gmail.com>
Date: Fri, 8 Nov 2013 10:47:07 -0800
Cc: spark-users@googlegroups.com,
 dev@spark.incubator.apache.org
Message-Id: <653535F9-16AB-4318-AE75-8E3EFA6AD335@gmail.com>
References: <085ED1F7-F5DB-4512-B907-0C40B32C3A7A@gmail.com> <CACXBjEZkifdPSVQ4w_xx1phxKftW0JR5nddyXu2YtWDL0bPdgQ@mail.gmail.com>
To: user@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_35A07CDE-95B6-4E47-AC1E-9972FA5029FD
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=iso-8859-1

Yes, we do plan to make them available after.

Matei

On Nov 8, 2013, at 3:27 AM, R. Revert <rafarevertf22@gmail.com> wrote:

> would the pdf slider=B4s or video talks, be posted for the people that =
can't attend to the conference?
>=20
> thanks
>=20
>=20
>=20
>=20
>=20
>=20
> #__________________
> Atte.
> Rafael R.
>=20
>=20
>=20
> 2013/11/7 Matei Zaharia <matei.zaharia@gmail.com>
> Hi everyone,
>=20
> We're glad to announce the agenda of the Spark Summit, which will =
happen on December 2nd and 3rd in San Francisco. We have 5 keynotes and =
24 talks lined up, from 18 different companies. Check out the agenda =
here: http://spark-summit.org/agenda/.
>=20
> This will be the biggest Spark event yet, with some very cool use case =
talks, so we hope to see you there! Sign up now to still get access to =
the early-bird registration rate.
>=20
> Matei
>=20
>=20


--Apple-Mail=_35A07CDE-95B6-4E47-AC1E-9972FA5029FD--

From dev-return-735-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov  8 19:34:17 2013
Return-Path: <dev-return-735-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3AE85107B0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Nov 2013 19:34:17 +0000 (UTC)
Received: (qmail 96949 invoked by uid 500); 8 Nov 2013 19:34:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 96914 invoked by uid 500); 8 Nov 2013 19:34:17 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 96895 invoked by uid 99); 8 Nov 2013 19:34:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 19:34:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.45 as permitted sender)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 19:34:11 +0000
Received: by mail-la0-f45.google.com with SMTP id el20so2209172lab.32
        for <multiple recipients>; Fri, 08 Nov 2013 11:33:51 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=qdOLqbUJu2ZDA3syS+UML7YQ0tEMF5KKbse2uQ7J3x8=;
        b=Y/PJD+pGO23OqSwYyjGod8yMLEyqWsPHv4XocRQxOM279eKbKUsmj2bNBU9jT4BLrB
         lciIWDWKd5/L4s0YUjd6VL/F6vEVlwD9BDBKxSsvYHrJos0qGOClp6oNCzp2EAjeB7NE
         AIMKnZaxZqxqN+SbMyDI55iYQTCWV6WNuBFIZkvmXmDD+ZGebj8aTwmgy/oXiIbJEo1H
         X1wCJ15TEXqO2tKuHR0sGUkWbmeWI4utjsnstbNHunPGOynsoSlHThqIzGm7Rw1NhMas
         u7sSrcLL1Eii8lWpFkUfSvy3qJ0wB39oi08jKNq4EctRJRozM3llUmEzzMxpJmrkhkTJ
         dNTw==
MIME-Version: 1.0
X-Received: by 10.112.154.129 with SMTP id vo1mr1408506lbb.31.1383939230965;
 Fri, 08 Nov 2013 11:33:50 -0800 (PST)
Received: by 10.112.16.6 with HTTP; Fri, 8 Nov 2013 11:33:50 -0800 (PST)
In-Reply-To: <085ED1F7-F5DB-4512-B907-0C40B32C3A7A@gmail.com>
References: <085ED1F7-F5DB-4512-B907-0C40B32C3A7A@gmail.com>
Date: Fri, 8 Nov 2013 11:33:50 -0800
Message-ID: <CALEZFQyFRV=+LEvAQQcy88RSPMQaGnfrYVh9ioB6UKEprn=y4w@mail.gmail.com>
Subject: Re: Spark Summit agenda posted
From: Andy Konwinski <andykonwinski@gmail.com>
To: "spark-users@googlegroups.com" <spark-users@googlegroups.com>
Cc: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>, user@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e0115fb0840a51004eaaf765c
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115fb0840a51004eaaf765c
Content-Type: text/plain; charset=ISO-8859-1

Hey folks, one more thing,

As Matei mentioned, early bird rates are still available. However, we want
to let everybody know that these rates will only last through next Fri, Nov
15. So register soon to save on your ticket price.

See you at the Summit,
Andy

On Nov 7, 2013 2:30 PM, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:

> Hi everyone,
>
> We're glad to announce the agenda of the Spark Summit, which will happen
> on December 2nd and 3rd in San Francisco. We have 5 keynotes and 24 talks
> lined up, from 18 different companies. Check out the agenda here:
> http://spark-summit.org/agenda/.
>
> This will be the biggest Spark event yet, with some very cool use case
> talks, so we hope to see you there! Sign up now to still get access to the
> early-bird registration rate.
>
> Matei
>
> --
> You received this message because you are subscribed to the Google Groups
> "Spark Users" group.
> To unsubscribe from this group and stop receiving emails from it, send an
> email to spark-users+unsubscribe@googlegroups.com.
> For more options, visit https://groups.google.com/groups/opt_out.
>

--089e0115fb0840a51004eaaf765c--

From dev-return-736-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov  8 21:29:19 2013
Return-Path: <dev-return-736-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D140810C4A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Nov 2013 21:29:19 +0000 (UTC)
Received: (qmail 53552 invoked by uid 500); 8 Nov 2013 21:29:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53492 invoked by uid 500); 8 Nov 2013 21:29:19 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 53484 invoked by uid 99); 8 Nov 2013 21:29:19 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 21:29:19 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of pwendell@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 21:29:13 +0000
Received: by mail-oa0-f49.google.com with SMTP id l6so3000760oag.36
        for <dev@spark.incubator.apache.org>; Fri, 08 Nov 2013 13:28:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=nRLDUmcuQDjLLma3loto5xL0JAJMIkAMApD/a8XxRYA=;
        b=U4HwJ/t6jPDb+cmo7441D8cunJHqViRy8CndkX56jtlD0T+che4jaZb0XV0Q5xws72
         M14ORy+hGdsf/kBkoetnyfbjZ9d6FvnO4YCMuqeV5dWhQE8Nrzb/KYlTeGBxhxXtL5zg
         jRB4oBQjKfbNFIBcn2dSZ0bgdmdLyvU568vzJftqjD49O3FZwFGoFa4KK9941xZpNHho
         ysTn7j1SiXFCNHX58QMi7J2FeRHt+GvuZ39OX/dPpaoVmaHc3Tf3+LM2GejvHDGk06rd
         AhjeQsLobsC7iXHSyLh4o8GKFuc/C6srybpoNGWtUyrKOFhU2ouNjxjbJwy42opI8HoA
         FvGg==
MIME-Version: 1.0
X-Received: by 10.182.40.201 with SMTP id z9mr6476267obk.45.1383946132357;
 Fri, 08 Nov 2013 13:28:52 -0800 (PST)
Received: by 10.182.132.46 with HTTP; Fri, 8 Nov 2013 13:28:52 -0800 (PST)
In-Reply-To: <CALuGr6bDwQ_E6fFvDvJ3u4hqbj3mCcMSnM4zS2b6BU0xjEjpbg@mail.gmail.com>
References: <CALuGr6bDwQ_E6fFvDvJ3u4hqbj3mCcMSnM4zS2b6BU0xjEjpbg@mail.gmail.com>
Date: Fri, 8 Nov 2013 13:28:52 -0800
Message-ID: <CABPQxstD+ak-Eve07_yDaLX7DVC=nAQX_Sx9uw9yLojkC7SoHQ@mail.gmail.com>
Subject: Re: Documenting the release process for Apache Spark
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Henry,

I did create release notes for this. However, I wanted to "dogfood"
them for the 0.8.1 release before I push them publicly, just so I know
the thing is actually comprehensive. It's quite complicated and I
don't want to publish something that leads people down the wrong path.

My thought was I would use these personally for the 0.8.1 release to
verify them, then publish them and try to have someone else do the
0.9.0 release (perhaps wishful thinking!).

- Patrick

On Thu, Nov 7, 2013 at 12:09 PM, Henry Saputra <henry.saputra@gmail.com> wrote:
> Hi Patrick,
>
> Did you end up writing up the steps you were taking to generate the
> Apache Spark release to provide help to the next Apache Spark RE?
>
> I remember you were trying to create one after we released 0.8
>
> Thanks,
>
> - Henry

From dev-return-737-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov  8 21:39:11 2013
Return-Path: <dev-return-737-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 82BBE10CC6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri,  8 Nov 2013 21:39:11 +0000 (UTC)
Received: (qmail 86381 invoked by uid 500); 8 Nov 2013 21:39:11 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86339 invoked by uid 500); 8 Nov 2013 21:39:11 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 86331 invoked by uid 99); 8 Nov 2013 21:39:11 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 21:39:11 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of henry.saputra@gmail.com designates 74.125.82.41 as permitted sender)
Received: from [74.125.82.41] (HELO mail-wg0-f41.google.com) (74.125.82.41)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 08 Nov 2013 21:39:04 +0000
Received: by mail-wg0-f41.google.com with SMTP id b13so250215wgh.2
        for <dev@spark.incubator.apache.org>; Fri, 08 Nov 2013 13:38:44 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=0oz9eoL++T4iXAc/x0E73CNayOOAL+CMlI3ypzWoJps=;
        b=uTC/ek8yAi76ZPr2lwcN3Ts2gsPvKCah/ksVqH7FzRZGb5t0ZjF47YF+5lx5JMZpbx
         8f5y0xFrIOOT+WuPrhRIC/foLypwZS9kRKzMfgbO2cXgn/9X3fZ9sZEv/qAAXlQK3+DD
         CGy41Hxll2Ts1bWQ6M9nkJeFKr/k+ljG3f4ePCOQ/m2wki6eZMLJBs/12G41HvRlZQxQ
         4NRO39kGtViVnpuAs/WFk4gQVDcXsuNGIvvTTRT/8a13Fovz05QhsTRrnpU0m6qUl5jh
         gQ42qqeHFcNB0Y/LEZPp1YwR6RPVK+fuRzf4uZd5yz4hZ3YtRiEzTY8rtHNWVNeYaaTq
         ZehQ==
MIME-Version: 1.0
X-Received: by 10.180.14.226 with SMTP id s2mr3943864wic.41.1383946724736;
 Fri, 08 Nov 2013 13:38:44 -0800 (PST)
Received: by 10.216.201.2 with HTTP; Fri, 8 Nov 2013 13:38:44 -0800 (PST)
In-Reply-To: <CABPQxstD+ak-Eve07_yDaLX7DVC=nAQX_Sx9uw9yLojkC7SoHQ@mail.gmail.com>
References: <CALuGr6bDwQ_E6fFvDvJ3u4hqbj3mCcMSnM4zS2b6BU0xjEjpbg@mail.gmail.com>
	<CABPQxstD+ak-Eve07_yDaLX7DVC=nAQX_Sx9uw9yLojkC7SoHQ@mail.gmail.com>
Date: Fri, 8 Nov 2013 13:38:44 -0800
Message-ID: <CALuGr6Z_cwVznqytBh3+ysR9L8rE2j0fn8geDBO6gOCB7-Bgmg@mail.gmail.com>
Subject: Re: Documenting the release process for Apache Spark
From: Henry Saputra <henry.saputra@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Sweet! Sounds good to me.

Thanks for the awesome work Patrick.

- Henry

On Fri, Nov 8, 2013 at 1:28 PM, Patrick Wendell <pwendell@gmail.com> wrote:
> Hey Henry,
>
> I did create release notes for this. However, I wanted to "dogfood"
> them for the 0.8.1 release before I push them publicly, just so I know
> the thing is actually comprehensive. It's quite complicated and I
> don't want to publish something that leads people down the wrong path.
>
> My thought was I would use these personally for the 0.8.1 release to
> verify them, then publish them and try to have someone else do the
> 0.9.0 release (perhaps wishful thinking!).
>
> - Patrick
>
> On Thu, Nov 7, 2013 at 12:09 PM, Henry Saputra <henry.saputra@gmail.com> wrote:
>> Hi Patrick,
>>
>> Did you end up writing up the steps you were taking to generate the
>> Apache Spark release to provide help to the next Apache Spark RE?
>>
>> I remember you were trying to create one after we released 0.8
>>
>> Thanks,
>>
>> - Henry

From dev-return-738-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov  9 03:02:25 2013
Return-Path: <dev-return-738-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 0938D10631
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat,  9 Nov 2013 03:02:25 +0000 (UTC)
Received: (qmail 81588 invoked by uid 500); 9 Nov 2013 03:02:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 81549 invoked by uid 500); 9 Nov 2013 03:02:22 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 81515 invoked by uid 99); 9 Nov 2013 03:02:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Nov 2013 03:02:20 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hdc1112@gmail.com designates 209.85.128.182 as permitted sender)
Received: from [209.85.128.182] (HELO mail-ve0-f182.google.com) (209.85.128.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 09 Nov 2013 03:02:15 +0000
Received: by mail-ve0-f182.google.com with SMTP id jy13so2062704veb.13
        for <dev@spark.incubator.apache.org>; Fri, 08 Nov 2013 19:01:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=xXhsw8TD564oTkKe3UAF1pcwGyA+TMLWm+Jyj5nEdnY=;
        b=wNImMqwlN9mCs2Z8ThOuSKmrKuLdV5crrxvS961p7duvxhPlTCK3e9qlBxhnRV3aNa
         icaYTdvCqvKTZgqEeHTHWQ+X4yg8uYlJ4LBwro9ydh707h8wP0v8xtfF1TQjs29veNB/
         QFyryPIHxygiozHZrjUYuClFZlqwnKG2aVq/NVuFJ1/lzyyBj8OTei3rUdQB9XH70CIh
         xrbsSVGoA1sbqDp1fWghWE61kpTBAWso2Y5kjEBn31Z9ZK7DxWhZ+BSGf57IrtYeXwZH
         WLS6nMfpOSdl6cbxc9OmVhCVP1N30XT4x73RDt08KSXP4tuH4ixn7osx7u89U/UMc/rr
         Heww==
MIME-Version: 1.0
X-Received: by 10.58.186.173 with SMTP id fl13mr870856vec.31.1383966114978;
 Fri, 08 Nov 2013 19:01:54 -0800 (PST)
Received: by 10.58.32.199 with HTTP; Fri, 8 Nov 2013 19:01:54 -0800 (PST)
Date: Fri, 8 Nov 2013 22:01:54 -0500
Message-ID: <CAAzo=r_UN+ebhSc5z1cmJhLs4eW1iRaTa5becY8P7abKOHeiBQ@mail.gmail.com>
Subject: a question about RDD.checkpoint()
From: dachuan <hdc1112@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b6dd1c6aa265604eab5b820
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6dd1c6aa265604eab5b820
Content-Type: text/plain; charset=ISO-8859-1

Hello,

I have a quick question about RDD.checkpoint().

If the user calls RDD.checkpoint() and after the job finishes, the Spark
would call RDD.doCheckpoint() to do the real physical checkpointing, that
is to say, dump this RDD's partitions into HDFS.

Does this mean that all its parents RDD scala objects and RDD's data (which
is managed by BlockManager) will be garbage collected?

And could you please point me to the relevant source code region, if
possible?

thanks,
dachuan.

-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210

--047d7b6dd1c6aa265604eab5b820--

From dev-return-739-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov 11 13:19:19 2013
Return-Path: <dev-return-739-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E9C3D10466
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Nov 2013 13:19:19 +0000 (UTC)
Received: (qmail 58565 invoked by uid 500); 11 Nov 2013 13:19:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58106 invoked by uid 500); 11 Nov 2013 13:19:16 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 58045 invoked by uid 99); 11 Nov 2013 13:19:14 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 13:19:13 +0000
X-ASF-Spam-Status: No, hits=-3.4 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,SUBJ_ALL_CAPS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of junluan.xia@intel.com designates 143.182.124.37 as permitted sender)
Received: from [143.182.124.37] (HELO mga14.intel.com) (143.182.124.37)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 13:19:10 +0000
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
  by azsmga102.ch.intel.com with ESMTP; 11 Nov 2013 05:18:49 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,677,1378882800"; 
   d="scan'208";a="425553102"
Received: from fmsmsx104.amr.corp.intel.com ([10.19.9.35])
  by fmsmga001.fm.intel.com with ESMTP; 11 Nov 2013 05:18:36 -0800
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 FMSMSX104.amr.corp.intel.com (10.19.9.35) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Mon, 11 Nov 2013 05:18:28 -0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.186]) by
 SHSMSX102.ccr.corp.intel.com ([10.239.4.154]) with mapi id 14.03.0123.003;
 Mon, 11 Nov 2013 21:18:25 +0800
From: "Xia, Junluan" <junluan.xia@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: SPARK-942
Thread-Topic: SPARK-942
Thread-Index: AQHO2yJQKBYOqQJkDkur6f7JZyvgsZof7DJA
Date: Mon, 11 Nov 2013 13:18:25 +0000
Message-ID: <7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
 <CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
In-Reply-To: <CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi=20

I think it is bad user experience to throw OOM exception when user only per=
sist the RDD with DISK_ONLY or MEMORY_ADN_DISK.

As Kyle mentioned below, Key point is that CacheManager has unrolled the to=
tal Iterator into ArrayBuffer without free memory check, we should estimate=
 size of unrolled iterator object and check if it is beyond current free me=
mory size.

We could separate into three scenarios

1. For MEMORY_ONLY, I think it is normal case to throw OOM exception and ne=
ed user to adjust its application
2. For MEMORY_AND_DISK, we should check if free memory could hold unrolled =
Arraybuffer, if yes, then it will go with usual path, if no, we will degrad=
e it to DISK_ONLY
3. For DIS_ONLY, I think that we need not to unroll total iterator into Arr=
ayBuffer, because we could write this iterator one by one to disk.

So this issue is how to judge if free memory size could hold size of unroll=
ed iterator before it become Arraybuffer.

Is there any solution for this case? Could we just unroll first 10% of tota=
l iterator into ArrayBuffer, and estimate this size, and total size is equa=
l to 10* size of 10%? apparently it is not perfect.

-----Original Message-----
From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]=20
Sent: Thursday, November 07, 2013 2:59 AM
To: dev@spark.incubator.apache.org
Subject: Re: SPARK-942

I think the usage has to be calculated as the iterator is being put into th=
e arraybuffer.
Right now, the BlockManager, in it's put method when it gets an iterator na=
med 'values' uses the simple stanza of:

def put(blockId: BlockId, values: Iterator[Any], level: StorageLevel,
tellMaster: Boolean)
    : Long =3D {
    val elements =3D new ArrayBuffer[Any]
    elements ++=3D values
    put(blockId, elements, level, tellMaster) }


Completely unrolling the iterator in a single line.  Above it, the CacheMan=
ager does the exact same thing with:

val elements =3D new ArrayBuffer[Any]
elements ++=3D computedValues
blockManager.put(key, elements, storageLevel, tellMaster =3D true)


We would probably have to implement some sort of 'IteratorBuffer' class, wh=
ich would wrap an iterator. It would include a method to unroll an iterator=
 into a buffer up to a point, something like

def unroll(maxMem:Long) : Boolean =3D{ ...}

And it would return True if the maxMem was hit. At which point BlockManager=
 could read through the already cached values, then continue on through the=
 rest of the iterators dumping all the values to file. If it unrolled witho=
ut hitting maxMem (which would probably be most of the time), the class wou=
ld simply wrap the ArrayBuffer of cached values.

Kyle



On Sun, Nov 3, 2013 at 12:50 AM, Reynold Xin <rxin@apache.org> wrote:

> It's not a very elegant solution, but one possibility is for the=20
> CacheManager to check whether it will have enough space. If it is=20
> running out of space, skips buffering the output of the iterator &=20
> directly write the output of the iterator to disk (if storage level allow=
s that).
>
> But it is still tricky to know whether we will run out of space before=20
> we even start running the iterator. One possibility is to use sizing=20
> data from previous partitions to estimate the size of the current partiti=
on (i.e.
> estimated in memory size =3D avg of current in-memory size / current=20
> input size).
>
> Do you have any ideas on this one, Kyle?
>
>
> On Sat, Oct 26, 2013 at 10:53 AM, Kyle Ellrott <kellrott@soe.ucsc.edu
> >wrote:
>
> > I was wondering if anybody had any thoughts on the best way to=20
> > tackle
> > SPARK-942 ( https://spark-project.atlassian.net/browse/SPARK-942 ).
> > Basically, Spark takes an iterator from a flatmap call and because I=20
> > tell it that it needs to persist Spark proceeds to push it all into=20
> > an array before deciding that it doesn't have enough memory and=20
> > trying to
> serialize
> > it to disk, and somewhere along the line it runs out of memory. For=20
> > my particular operation, the function return an iterator that reads=20
> > data out of a file, and the size of the files passed to that=20
> > function can vary greatly (from a few kilobytes to a few gigabytes).=20
> > The funny thing is
> that
> > if I do a strait 'map' operation after the flat map, everything=20
> > works, because Spark just passes the iterator forward and never=20
> > tries to expand the whole thing into memory. But I need do a=20
> > reduceByKey across all the records, so I'd like to persist to disk=20
> > first, and that is where I hit
> this
> > snag.
> > I've already setup a unit test to replicate the problem, and I know=20
> > the area of the code that would need to be fixed.
> > I'm just hoping for some tips on the best way to fix the problem.
> >
> > Kyle
> >
>

From dev-return-740-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov 11 20:50:39 2013
Return-Path: <dev-return-740-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AF5CC10601
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Nov 2013 20:50:39 +0000 (UTC)
Received: (qmail 34912 invoked by uid 500); 11 Nov 2013 20:50:39 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34756 invoked by uid 500); 11 Nov 2013 20:50:39 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 34748 invoked by uid 99); 11 Nov 2013 20:50:39 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 20:50:39 +0000
X-ASF-Spam-Status: No, hits=1.7 required=5.0
	tests=FREEMAIL_ENVFROM_END_DIGIT,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of hdc1112@gmail.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 20:50:33 +0000
Received: by mail-vc0-f180.google.com with SMTP id ib11so910083vcb.25
        for <dev@spark.incubator.apache.org>; Mon, 11 Nov 2013 12:50:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=YkVT/lRiXWm7zM/mrdePPMFSAwjK6SFndP9XnbUMrII=;
        b=OqihmGIqMAJBJX15Vj6t96kjziEp2XM/Sw2530pSPevNYfTkDlSNX6ktH1HMOMZXuR
         WuUgduFB+2YMKVyBQWxKphQKh1e41Gd7HMJMRbb/kfzpEzbdZbfLPOt7v5GbECLWjagy
         sD6bO9HFHhzBcG8S8WaT7eXJzuoTFzetJ5X0oCC8vDwoNVQkNvmb8d9pqDKq8eXPNV+N
         Ar3xqNzE93+ZrnDScLZbiyL1ejMT03i0fRYOzPl/lBgGAfdf6FhUkP7PndmbCxEt0+L+
         BgZgEd39t5jMPxH9xxsgfgNK1SokULyy5GNFBXL6DWf5ZgKhTU5/r0QqpXKsAtGtQA6D
         O39A==
MIME-Version: 1.0
X-Received: by 10.58.216.74 with SMTP id oo10mr25435808vec.0.1384203013267;
 Mon, 11 Nov 2013 12:50:13 -0800 (PST)
Received: by 10.58.32.199 with HTTP; Mon, 11 Nov 2013 12:50:13 -0800 (PST)
Date: Mon, 11 Nov 2013 15:50:13 -0500
Message-ID: <CAAzo=r_16VEXJi-sZQ+n2Kx=3zpbEQhNf7xBBNOkevbdQsKJQA@mail.gmail.com>
Subject: a question about FetchFailed
From: dachuan <hdc1112@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b86e3d8e72a5904eaece0c2
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86e3d8e72a5904eaece0c2
Content-Type: text/plain; charset=ISO-8859-1

Hello, community,

I noticed one surprising code region in Spark. It's in DAGScheduler -->
handleTaskCompletion --> case FetchFailed --> failed += magStage. (I am in
branch-0.8)

That is to say, if one task failed to fetch its input from
shuffledependency, this task's stage and its parent stage will both need to
be re-executed.

If my understanding is correct, then why does Spark need to materialize the
intermediate data? This is "restart" fault tolerance mechanism.

thanks for your help,
dachuan.

-- 
Dachuan Huang
Cellphone: 614-390-7234
2015 Neil Avenue
Ohio State University
Columbus, Ohio
U.S.A.
43210

--047d7b86e3d8e72a5904eaece0c2--

From dev-return-741-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov 11 22:28:48 2013
Return-Path: <dev-return-741-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EE9ED10A10
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Nov 2013 22:28:48 +0000 (UTC)
Received: (qmail 34969 invoked by uid 500); 11 Nov 2013 22:28:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34913 invoked by uid 500); 11 Nov 2013 22:28:48 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 34905 invoked by uid 99); 11 Nov 2013 22:28:48 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 22:28:48 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kellrott@soe.ucsc.edu designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 22:28:42 +0000
Received: by mail-ob0-f171.google.com with SMTP id gq1so5132152obb.30
        for <dev@spark.incubator.apache.org>; Mon, 11 Nov 2013 14:28:21 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ucsc.edu; s=ucsc-google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=VlXidAiTN7nt/k9xHtyon6054MR/t8HgU+Cjpblph70=;
        b=cZSEpZd/Bt41HRKrETCKdbziwR562HDL5meY2y/aNmGlrTXAhZGv1UVXCSWcOTLe3p
         1ZX25irkbRJZPnBLNz/ph/cHXL5c/gMCoSBcKvLiXBr2MJWVvaBTniVe+D6ldP6bdyAF
         eWcZ7fR7T2cHMUwIwIonbRA7ystN0sUyEn+NY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=VlXidAiTN7nt/k9xHtyon6054MR/t8HgU+Cjpblph70=;
        b=Fap76EyL3SWnfeY9/PlvjKOvotpyQonLzJO2lfKakya7ohuZ6TyKRLF1YiXfQBji/o
         MpEAtMlY5evw4Jyi1cjlXsf1Qwf+fripE06hQjdgkdd0ADxznmRd6RKwiw+nz1Z45TRF
         Ey6sFf0HoH2Hr59t+jOyfPD4zvDTQ5yQKIbvruQkHyVNLqYNqHE/hxkdpACMsqUkUF1m
         HtZwlh9pKFaUSA+LD366QaJBtr6jGjkQMJiWrY/268YLsJzfNohO8UajxpI/0rr9WAtm
         fIo49b2c53Hom6tFTpFnPq7tmbJMQ26s0hJZzdykjh9JcSuU5nzTWwOL6eT9nveIGGft
         ziIQ==
X-Gm-Message-State: ALoCoQlrHon+QUtnOj7JsLf4qF+RaA7i4a8TlZTYNaiYaefow19yYIh82lTqZ8rz9zi11xaOY3ms
MIME-Version: 1.0
X-Received: by 10.182.144.136 with SMTP id sm8mr3497846obb.63.1384208901205;
 Mon, 11 Nov 2013 14:28:21 -0800 (PST)
Received: by 10.182.132.50 with HTTP; Mon, 11 Nov 2013 14:28:21 -0800 (PST)
In-Reply-To: <7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
Date: Mon, 11 Nov 2013 14:28:21 -0800
Message-ID: <CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
Subject: Re: SPARK-942
From: Kyle Ellrott <kellrott@soe.ucsc.edu>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e0149cd5eda0be404eaee3fb8
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0149cd5eda0be404eaee3fb8
Content-Type: text/plain; charset=ISO-8859-1

The problem is that the iterator interface only defines 'hasNext' and
'next' methods. So I don't think that there is really anyway to estimate
the total count until the iterator is done traversing. In my particular
case, I'm wrapping a OpenRDF RIO iterator, that is parsing a gzipfile
stream. And one of the files just happens to be several gigabytes large.
Each of the individual elements spit out by the iterator are all the same,
just sometimes it spits out a few million more then normal.

It's not a normal occurrence. 99.9% of the time, when people call 'flatMap'
they will probably be producing arrays that fit nicely into memory. Trying
to do a bunch of extra book keeping (ie unrolling the iterator one at a
time, trying to figure out if it's gotten too big yet), may be an extra
complication that makes the code much more complicated while only providing
a solution for extreme edge cases.

I think the 'best' way to go would to leave the 'MEMORY_ONLY' and
'MEMORY_AND_DISK' behaviors the same. If the user knows that their code
could produce these 'mega-iterators' then they pass a 'DISK_ONLY' and that
iterator gets passed straight to the BlockManager to be written straight to
disk. Then all we have to do is change "def put(blockId: BlockId, values:
Iterator[Any], level: StorageLevel, tellMaster: Boolean)"
(BlockManager.scala:452), to call 'diskStore.putValues' directly, rather
then unrolling the iterator and passing it onto the stardard 'doPut' like
it does now.

Kyle




On Mon, Nov 11, 2013 at 5:18 AM, Xia, Junluan <junluan.xia@intel.com> wrote:

> Hi
>
> I think it is bad user experience to throw OOM exception when user only
> persist the RDD with DISK_ONLY or MEMORY_ADN_DISK.
>
> As Kyle mentioned below, Key point is that CacheManager has unrolled the
> total Iterator into ArrayBuffer without free memory check, we should
> estimate size of unrolled iterator object and check if it is beyond current
> free memory size.
>
> We could separate into three scenarios
>
> 1. For MEMORY_ONLY, I think it is normal case to throw OOM exception and
> need user to adjust its application
> 2. For MEMORY_AND_DISK, we should check if free memory could hold unrolled
> Arraybuffer, if yes, then it will go with usual path, if no, we will
> degrade it to DISK_ONLY
> 3. For DIS_ONLY, I think that we need not to unroll total iterator into
> ArrayBuffer, because we could write this iterator one by one to disk.
>
> So this issue is how to judge if free memory size could hold size of
> unrolled iterator before it become Arraybuffer.
>
> Is there any solution for this case? Could we just unroll first 10% of
> total iterator into ArrayBuffer, and estimate this size, and total size is
> equal to 10* size of 10%? apparently it is not perfect.
>
> -----Original Message-----
> From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]
> Sent: Thursday, November 07, 2013 2:59 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: SPARK-942
>
> I think the usage has to be calculated as the iterator is being put into
> the arraybuffer.
> Right now, the BlockManager, in it's put method when it gets an iterator
> named 'values' uses the simple stanza of:
>
> def put(blockId: BlockId, values: Iterator[Any], level: StorageLevel,
> tellMaster: Boolean)
>     : Long = {
>     val elements = new ArrayBuffer[Any]
>     elements ++= values
>     put(blockId, elements, level, tellMaster) }
>
>
> Completely unrolling the iterator in a single line.  Above it, the
> CacheManager does the exact same thing with:
>
> val elements = new ArrayBuffer[Any]
> elements ++= computedValues
> blockManager.put(key, elements, storageLevel, tellMaster = true)
>
>
> We would probably have to implement some sort of 'IteratorBuffer' class,
> which would wrap an iterator. It would include a method to unroll an
> iterator into a buffer up to a point, something like
>
> def unroll(maxMem:Long) : Boolean ={ ...}
>
> And it would return True if the maxMem was hit. At which point
> BlockManager could read through the already cached values, then continue on
> through the rest of the iterators dumping all the values to file. If it
> unrolled without hitting maxMem (which would probably be most of the time),
> the class would simply wrap the ArrayBuffer of cached values.
>
> Kyle
>
>
>
> On Sun, Nov 3, 2013 at 12:50 AM, Reynold Xin <rxin@apache.org> wrote:
>
> > It's not a very elegant solution, but one possibility is for the
> > CacheManager to check whether it will have enough space. If it is
> > running out of space, skips buffering the output of the iterator &
> > directly write the output of the iterator to disk (if storage level
> allows that).
> >
> > But it is still tricky to know whether we will run out of space before
> > we even start running the iterator. One possibility is to use sizing
> > data from previous partitions to estimate the size of the current
> partition (i.e.
> > estimated in memory size = avg of current in-memory size / current
> > input size).
> >
> > Do you have any ideas on this one, Kyle?
> >
> >
> > On Sat, Oct 26, 2013 at 10:53 AM, Kyle Ellrott <kellrott@soe.ucsc.edu
> > >wrote:
> >
> > > I was wondering if anybody had any thoughts on the best way to
> > > tackle
> > > SPARK-942 ( https://spark-project.atlassian.net/browse/SPARK-942 ).
> > > Basically, Spark takes an iterator from a flatmap call and because I
> > > tell it that it needs to persist Spark proceeds to push it all into
> > > an array before deciding that it doesn't have enough memory and
> > > trying to
> > serialize
> > > it to disk, and somewhere along the line it runs out of memory. For
> > > my particular operation, the function return an iterator that reads
> > > data out of a file, and the size of the files passed to that
> > > function can vary greatly (from a few kilobytes to a few gigabytes).
> > > The funny thing is
> > that
> > > if I do a strait 'map' operation after the flat map, everything
> > > works, because Spark just passes the iterator forward and never
> > > tries to expand the whole thing into memory. But I need do a
> > > reduceByKey across all the records, so I'd like to persist to disk
> > > first, and that is where I hit
> > this
> > > snag.
> > > I've already setup a unit test to replicate the problem, and I know
> > > the area of the code that would need to be fixed.
> > > I'm just hoping for some tips on the best way to fix the problem.
> > >
> > > Kyle
> > >
> >
>

--089e0149cd5eda0be404eaee3fb8--

From dev-return-742-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov 11 22:54:39 2013
Return-Path: <dev-return-742-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 23FF410BC0
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 11 Nov 2013 22:54:39 +0000 (UTC)
Received: (qmail 90641 invoked by uid 500); 11 Nov 2013 22:54:38 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90578 invoked by uid 500); 11 Nov 2013 22:54:38 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 90562 invoked by uid 99); 11 Nov 2013 22:54:38 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 22:54:38 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of umarj.javed@gmail.com designates 209.85.128.173 as permitted sender)
Received: from [209.85.128.173] (HELO mail-ve0-f173.google.com) (209.85.128.173)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 11 Nov 2013 22:54:33 +0000
Received: by mail-ve0-f173.google.com with SMTP id c14so3807651vea.32
        for <multiple recipients>; Mon, 11 Nov 2013 14:54:12 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=ZvnTWrHJPo8Ts9fz/Lc+De4lVE056ahUQqX918LRNpI=;
        b=Z4fTmEleDc9dIBdyKU2pyWBDHabbcu/9yWlA8lHgTk79t6nD50uOVa8EE2j5od+wN6
         Ej8Hs7rQ4+EA8DuqKc4e8adScjqDTMjjIqM90nTo8cTbtJ9AF9njPAE1deScJ/MwFytt
         XxtmWvHDBA/pvLA+GycbCt+EQfzwImX8FM5p87qQnlKgxAARE4O4yE8Inldr4pEO7XyD
         C2KO2ymImVYzX0zK/jgV7FSw1dy7lwirBhBjMPYBLITPEuffaKIQsyyvKr/x7RnomhSG
         58NcWvks/emD9OFI6r2+lIWQuvBm5tHQOU39y10zD3NZt61bnYOsgi9HkjZKeFALW73f
         N67g==
MIME-Version: 1.0
X-Received: by 10.52.157.232 with SMTP id wp8mr22238799vdb.4.1384210452059;
 Mon, 11 Nov 2013 14:54:12 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Mon, 11 Nov 2013 14:54:12 -0800 (PST)
Date: Mon, 11 Nov 2013 14:54:12 -0800
Message-ID: <CACwKa9fmd5MeprLoBR8bHB2TDKQHg7X7bB5AZSi7XV_cJQru8w@mail.gmail.com>
Subject: problems with sbt
From: Umar Javed <umarj.javed@gmail.com>
To: user@spark.incubator.apache.org, dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e016339ee4a1fbd04eaee9c15
X-Virus-Checked: Checked by ClamAV on apache.org

--089e016339ee4a1fbd04eaee9c15
Content-Type: text/plain; charset=ISO-8859-1

I keep getting these io.Exception Permission denied errors when building
with sbt assembly:

java.io.IOException: Permission denied
        at java.io.FileOutputStream.close0(Native Method)
        at java.io.FileOutputStream.close(FileOutputStream.java:393)
        at java.io.FilterOutputStream.close(FilterOutputStream.java:160)
        at
java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:241)
        at java.util.zip.ZipOutputStream.close(ZipOutputStream.java:360)
        at sbt.IO$$anonfun$withZipOutput$1.apply(IO.scala:497)
        at sbt.IO$$anonfun$withZipOutput$1.apply(IO.scala:482)
        at sbt.Using.apply(Using.scala:25)
        at sbt.IO$.withZipOutput(IO.scala:482)
        at sbt.IO$.archive(IO.scala:401)
        at sbt.IO$.jar(IO.scala:384)
        at sbt.Package$.makeJar(Package.scala:107)
        at sbt.Package$$anonfun$3$$anonfun$apply$3.apply(Package.scala:72)
        at sbt.Package$$anonfun$3$$anonfun$apply$3.apply(Package.scala:70)
        at sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:57)
        at sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:52)
        at sbt.Package$.apply(Package.scala:80)
        at sbtassembly.Plugin$Assembly$.makeJar$1(Plugin.scala:174)
        at
sbtassembly.Plugin$Assembly$$anonfun$4$$anonfun$apply$3.apply(Plugin.scala:181)
        at
sbtassembly.Plugin$Assembly$$anonfun$4$$anonfun$apply$3.apply(Plugin.scala:177)
        at sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:57)
        at sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:52)
        at sbtassembly.Plugin$Assembly$.apply(Plugin.scala:189)
        at
sbtassembly.Plugin$.sbtassembly$Plugin$$assemblyTask(Plugin.scala:157)
        at
sbtassembly.Plugin$$anonfun$baseAssemblySettings$6.apply(Plugin.scala:369)
        at
sbtassembly.Plugin$$anonfun$baseAssemblySettings$6.apply(Plugin.scala:368)
        at sbt.Scoped$$anonfun$hf10$1.apply(Structure.scala:586)
        at sbt.Scoped$$anonfun$hf10$1.apply(Structure.scala:586)
        at scala.Function1$$anonfun$compose$1.apply(Function1.scala:49)
        at
sbt.Scoped$Reduced$$anonfun$combine$1$$anonfun$apply$12.apply(Structure.scala:311)
        at
sbt.Scoped$Reduced$$anonfun$combine$1$$anonfun$apply$12.apply(Structure.scala:311)
        at
sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:41)
        at sbt.std.Transform$$anon$5.work(System.scala:71)
        at
sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232)
        at
sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232)
        at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
        at sbt.Execute.work(Execute.scala:238)
        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232)
        at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232)
        at
sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions.scala:160)
        at sbt.CompletionService$$anon$2.call(CompletionService.scala:30)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
[error] (assembly/*:assembly) java.io.IOException: Permission denied

Can somebody help me out?
thanks,
Umar

--089e016339ee4a1fbd04eaee9c15--

From dev-return-743-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 12 01:15:17 2013
Return-Path: <dev-return-743-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 81E6610265
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Nov 2013 01:15:17 +0000 (UTC)
Received: (qmail 20896 invoked by uid 500); 12 Nov 2013 01:15:17 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 20858 invoked by uid 500); 12 Nov 2013 01:15:17 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 20850 invoked by uid 99); 12 Nov 2013 01:15:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 01:15:17 +0000
X-ASF-Spam-Status: No, hits=-3.4 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,SUBJ_ALL_CAPS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of junluan.xia@intel.com designates 134.134.136.24 as permitted sender)
Received: from [134.134.136.24] (HELO mga09.intel.com) (134.134.136.24)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 01:15:13 +0000
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
  by orsmga102.jf.intel.com with ESMTP; 11 Nov 2013 17:11:22 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,681,1378882800"; 
   d="scan'208";a="425914710"
Received: from fmsmsx108.amr.corp.intel.com ([10.19.9.228])
  by fmsmga001.fm.intel.com with ESMTP; 11 Nov 2013 17:14:52 -0800
Received: from fmsmsx153.amr.corp.intel.com (10.19.17.7) by
 FMSMSX108.amr.corp.intel.com (10.19.9.228) with Microsoft SMTP Server (TLS)
 id 14.3.123.3; Mon, 11 Nov 2013 17:14:52 -0800
Received: from shsmsx101.ccr.corp.intel.com (10.239.4.153) by
 FMSMSX153.amr.corp.intel.com (10.19.17.7) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Mon, 11 Nov 2013 17:14:51 -0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.186]) by
 SHSMSX101.ccr.corp.intel.com ([10.239.4.153]) with mapi id 14.03.0123.003;
 Tue, 12 Nov 2013 09:14:50 +0800
From: "Xia, Junluan" <junluan.xia@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: SPARK-942
Thread-Topic: SPARK-942
Thread-Index: AQHO2yJQKBYOqQJkDkur6f7JZyvgsZof7DJAgAAyFICAALMm4A==
Date: Tue, 12 Nov 2013 01:14:50 +0000
Message-ID: <7841A4E0A9C8784C9D8B07DF5E48F0A811623CF1@SHSMSX104.ccr.corp.intel.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
 <CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
In-Reply-To: <CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Kyle

I totally agree with you. 'best' solution currently is to only handle "DISK=
_ONLY" scenario and put iterator directly to BlockManager.

It is so expensive for us to make code complicated for only 0.1% possibilit=
y before we get perfect solution.

-----Original Message-----
From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]=20
Sent: Tuesday, November 12, 2013 6:28 AM
To: dev@spark.incubator.apache.org
Subject: Re: SPARK-942

The problem is that the iterator interface only defines 'hasNext' and 'next=
' methods. So I don't think that there is really anyway to estimate the tot=
al count until the iterator is done traversing. In my particular case, I'm =
wrapping a OpenRDF RIO iterator, that is parsing a gzipfile stream. And one=
 of the files just happens to be several gigabytes large.
Each of the individual elements spit out by the iterator are all the same, =
just sometimes it spits out a few million more then normal.

It's not a normal occurrence. 99.9% of the time, when people call 'flatMap'
they will probably be producing arrays that fit nicely into memory. Trying =
to do a bunch of extra book keeping (ie unrolling the iterator one at a tim=
e, trying to figure out if it's gotten too big yet), may be an extra compli=
cation that makes the code much more complicated while only providing a sol=
ution for extreme edge cases.

I think the 'best' way to go would to leave the 'MEMORY_ONLY' and 'MEMORY_A=
ND_DISK' behaviors the same. If the user knows that their code could produc=
e these 'mega-iterators' then they pass a 'DISK_ONLY' and that iterator get=
s passed straight to the BlockManager to be written straight to disk. Then =
all we have to do is change "def put(blockId: BlockId, values:
Iterator[Any], level: StorageLevel, tellMaster: Boolean)"
(BlockManager.scala:452), to call 'diskStore.putValues' directly, rather th=
en unrolling the iterator and passing it onto the stardard 'doPut' like it =
does now.

Kyle




On Mon, Nov 11, 2013 at 5:18 AM, Xia, Junluan <junluan.xia@intel.com> wrote=
:

> Hi
>
> I think it is bad user experience to throw OOM exception when user=20
> only persist the RDD with DISK_ONLY or MEMORY_ADN_DISK.
>
> As Kyle mentioned below, Key point is that CacheManager has unrolled=20
> the total Iterator into ArrayBuffer without free memory check, we=20
> should estimate size of unrolled iterator object and check if it is=20
> beyond current free memory size.
>
> We could separate into three scenarios
>
> 1. For MEMORY_ONLY, I think it is normal case to throw OOM exception=20
> and need user to adjust its application 2. For MEMORY_AND_DISK, we=20
> should check if free memory could hold unrolled Arraybuffer, if yes,=20
> then it will go with usual path, if no, we will degrade it to=20
> DISK_ONLY 3. For DIS_ONLY, I think that we need not to unroll total=20
> iterator into ArrayBuffer, because we could write this iterator one by=20
> one to disk.
>
> So this issue is how to judge if free memory size could hold size of=20
> unrolled iterator before it become Arraybuffer.
>
> Is there any solution for this case? Could we just unroll first 10% of=20
> total iterator into ArrayBuffer, and estimate this size, and total=20
> size is equal to 10* size of 10%? apparently it is not perfect.
>
> -----Original Message-----
> From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]
> Sent: Thursday, November 07, 2013 2:59 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: SPARK-942
>
> I think the usage has to be calculated as the iterator is being put=20
> into the arraybuffer.
> Right now, the BlockManager, in it's put method when it gets an=20
> iterator named 'values' uses the simple stanza of:
>
> def put(blockId: BlockId, values: Iterator[Any], level: StorageLevel,
> tellMaster: Boolean)
>     : Long =3D {
>     val elements =3D new ArrayBuffer[Any]
>     elements ++=3D values
>     put(blockId, elements, level, tellMaster) }
>
>
> Completely unrolling the iterator in a single line.  Above it, the=20
> CacheManager does the exact same thing with:
>
> val elements =3D new ArrayBuffer[Any]
> elements ++=3D computedValues
> blockManager.put(key, elements, storageLevel, tellMaster =3D true)
>
>
> We would probably have to implement some sort of 'IteratorBuffer'=20
> class, which would wrap an iterator. It would include a method to=20
> unroll an iterator into a buffer up to a point, something like
>
> def unroll(maxMem:Long) : Boolean =3D{ ...}
>
> And it would return True if the maxMem was hit. At which point=20
> BlockManager could read through the already cached values, then=20
> continue on through the rest of the iterators dumping all the values=20
> to file. If it unrolled without hitting maxMem (which would probably=20
> be most of the time), the class would simply wrap the ArrayBuffer of cach=
ed values.
>
> Kyle
>
>
>
> On Sun, Nov 3, 2013 at 12:50 AM, Reynold Xin <rxin@apache.org> wrote:
>
> > It's not a very elegant solution, but one possibility is for the=20
> > CacheManager to check whether it will have enough space. If it is=20
> > running out of space, skips buffering the output of the iterator &=20
> > directly write the output of the iterator to disk (if storage level
> allows that).
> >
> > But it is still tricky to know whether we will run out of space=20
> > before we even start running the iterator. One possibility is to use=20
> > sizing data from previous partitions to estimate the size of the=20
> > current
> partition (i.e.
> > estimated in memory size =3D avg of current in-memory size / current=20
> > input size).
> >
> > Do you have any ideas on this one, Kyle?
> >
> >
> > On Sat, Oct 26, 2013 at 10:53 AM, Kyle Ellrott=20
> > <kellrott@soe.ucsc.edu
> > >wrote:
> >
> > > I was wondering if anybody had any thoughts on the best way to=20
> > > tackle
> > > SPARK-942 ( https://spark-project.atlassian.net/browse/SPARK-942 ).
> > > Basically, Spark takes an iterator from a flatmap call and because=20
> > > I tell it that it needs to persist Spark proceeds to push it all=20
> > > into an array before deciding that it doesn't have enough memory=20
> > > and trying to
> > serialize
> > > it to disk, and somewhere along the line it runs out of memory.=20
> > > For my particular operation, the function return an iterator that=20
> > > reads data out of a file, and the size of the files passed to that=20
> > > function can vary greatly (from a few kilobytes to a few gigabytes).
> > > The funny thing is
> > that
> > > if I do a strait 'map' operation after the flat map, everything=20
> > > works, because Spark just passes the iterator forward and never=20
> > > tries to expand the whole thing into memory. But I need do a=20
> > > reduceByKey across all the records, so I'd like to persist to disk=20
> > > first, and that is where I hit
> > this
> > > snag.
> > > I've already setup a unit test to replicate the problem, and I=20
> > > know the area of the code that would need to be fixed.
> > > I'm just hoping for some tips on the best way to fix the problem.
> > >
> > > Kyle
> > >
> >
>

From dev-return-744-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 12 14:52:53 2013
Return-Path: <dev-return-744-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 91E6F107FE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Nov 2013 14:52:53 +0000 (UTC)
Received: (qmail 94302 invoked by uid 500); 12 Nov 2013 14:52:48 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 93920 invoked by uid 500); 12 Nov 2013 14:52:41 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 93912 invoked by uid 99); 12 Nov 2013 14:52:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 14:52:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [74.125.82.53] (HELO mail-wg0-f53.google.com) (74.125.82.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 14:52:36 +0000
Received: by mail-wg0-f53.google.com with SMTP id b13so2543928wgh.8
        for <dev@spark.incubator.apache.org>; Tue, 12 Nov 2013 06:52:14 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=iuMDGHAlCziql39OAwg2KGyPD8HHkNgBSPVOsj8GLUU=;
        b=NEYltefpQeRTvrjjZXnvX9AKNUTqBnimBz8J/aTVDH++T6EW6qX7q+sAvbcFItphpF
         hGNL4bUPCxVPJxbSP7sL7JDT9liLvM45OxBgZ5JKc027BbEfBW1B7Bg7fsZMtXZGIg6Y
         szmW3CQd4pci72c6ngHZQuD2Mc9eWN+gdGhVuZ+ZgyZt1RtuSAax5gs9a/no7q5azQFU
         VjjZaWXV5axEDUE6xlMHqpnNGHsoGUaqRdM6NayWbNXU/G5iszrENr7pxpcDe3wgJcpJ
         fszABE/MUQkycWiqK+4jue8C0Mr2WIlNjUQFN+OB7GI4bWPSPK594tvfo+gBlu1LEsqM
         cSgQ==
X-Gm-Message-State: ALoCoQnB3nnXmIdHwABzbiBGFKjkWVhJ2E9UKtRgXSh0ibwCS/KTeN6NvdQuL7p2RdHbJmGaQjbp
MIME-Version: 1.0
X-Received: by 10.180.183.72 with SMTP id ek8mr344892wic.31.1384267934163;
 Tue, 12 Nov 2013 06:52:14 -0800 (PST)
Received: by 10.216.175.138 with HTTP; Tue, 12 Nov 2013 06:52:14 -0800 (PST)
X-Originating-IP: [199.47.72.31]
In-Reply-To: <7841A4E0A9C8784C9D8B07DF5E48F0A811623CF1@SHSMSX104.ccr.corp.intel.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
	<CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A811623CF1@SHSMSX104.ccr.corp.intel.com>
Date: Tue, 12 Nov 2013 09:52:14 -0500
Message-ID: <CANx3uAiG_1L-c3wMFmWLh63d97ifNx33o2Xxun_dXypdpr5H3Q@mail.gmail.com>
Subject: Re: SPARK-942
From: Koert Kuipers <koert@tresata.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c3556e7d77ce04eafbfeaf
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c3556e7d77ce04eafbfeaf
Content-Type: text/plain; charset=ISO-8859-1

if spark wants to compete as an alternative for mapreduce on hadoop
clusters, then the assumption should not be that 99.9% of time data will
fit in memory. it will not.

however that said, i am fine with a solution where one has to use DISK_ONLY
for this, since that is exactly what mapreduce does too anyhow.


On Mon, Nov 11, 2013 at 8:14 PM, Xia, Junluan <junluan.xia@intel.com> wrote:

> Hi Kyle
>
> I totally agree with you. 'best' solution currently is to only handle
> "DISK_ONLY" scenario and put iterator directly to BlockManager.
>
> It is so expensive for us to make code complicated for only 0.1%
> possibility before we get perfect solution.
>
> -----Original Message-----
> From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]
> Sent: Tuesday, November 12, 2013 6:28 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: SPARK-942
>
> The problem is that the iterator interface only defines 'hasNext' and
> 'next' methods. So I don't think that there is really anyway to estimate
> the total count until the iterator is done traversing. In my particular
> case, I'm wrapping a OpenRDF RIO iterator, that is parsing a gzipfile
> stream. And one of the files just happens to be several gigabytes large.
> Each of the individual elements spit out by the iterator are all the same,
> just sometimes it spits out a few million more then normal.
>
> It's not a normal occurrence. 99.9% of the time, when people call 'flatMap'
> they will probably be producing arrays that fit nicely into memory. Trying
> to do a bunch of extra book keeping (ie unrolling the iterator one at a
> time, trying to figure out if it's gotten too big yet), may be an extra
> complication that makes the code much more complicated while only providing
> a solution for extreme edge cases.
>
> I think the 'best' way to go would to leave the 'MEMORY_ONLY' and
> 'MEMORY_AND_DISK' behaviors the same. If the user knows that their code
> could produce these 'mega-iterators' then they pass a 'DISK_ONLY' and that
> iterator gets passed straight to the BlockManager to be written straight to
> disk. Then all we have to do is change "def put(blockId: BlockId, values:
> Iterator[Any], level: StorageLevel, tellMaster: Boolean)"
> (BlockManager.scala:452), to call 'diskStore.putValues' directly, rather
> then unrolling the iterator and passing it onto the stardard 'doPut' like
> it does now.
>
> Kyle
>
>
>
>
> On Mon, Nov 11, 2013 at 5:18 AM, Xia, Junluan <junluan.xia@intel.com>
> wrote:
>
> > Hi
> >
> > I think it is bad user experience to throw OOM exception when user
> > only persist the RDD with DISK_ONLY or MEMORY_ADN_DISK.
> >
> > As Kyle mentioned below, Key point is that CacheManager has unrolled
> > the total Iterator into ArrayBuffer without free memory check, we
> > should estimate size of unrolled iterator object and check if it is
> > beyond current free memory size.
> >
> > We could separate into three scenarios
> >
> > 1. For MEMORY_ONLY, I think it is normal case to throw OOM exception
> > and need user to adjust its application 2. For MEMORY_AND_DISK, we
> > should check if free memory could hold unrolled Arraybuffer, if yes,
> > then it will go with usual path, if no, we will degrade it to
> > DISK_ONLY 3. For DIS_ONLY, I think that we need not to unroll total
> > iterator into ArrayBuffer, because we could write this iterator one by
> > one to disk.
> >
> > So this issue is how to judge if free memory size could hold size of
> > unrolled iterator before it become Arraybuffer.
> >
> > Is there any solution for this case? Could we just unroll first 10% of
> > total iterator into ArrayBuffer, and estimate this size, and total
> > size is equal to 10* size of 10%? apparently it is not perfect.
> >
> > -----Original Message-----
> > From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]
> > Sent: Thursday, November 07, 2013 2:59 AM
> > To: dev@spark.incubator.apache.org
> > Subject: Re: SPARK-942
> >
> > I think the usage has to be calculated as the iterator is being put
> > into the arraybuffer.
> > Right now, the BlockManager, in it's put method when it gets an
> > iterator named 'values' uses the simple stanza of:
> >
> > def put(blockId: BlockId, values: Iterator[Any], level: StorageLevel,
> > tellMaster: Boolean)
> >     : Long = {
> >     val elements = new ArrayBuffer[Any]
> >     elements ++= values
> >     put(blockId, elements, level, tellMaster) }
> >
> >
> > Completely unrolling the iterator in a single line.  Above it, the
> > CacheManager does the exact same thing with:
> >
> > val elements = new ArrayBuffer[Any]
> > elements ++= computedValues
> > blockManager.put(key, elements, storageLevel, tellMaster = true)
> >
> >
> > We would probably have to implement some sort of 'IteratorBuffer'
> > class, which would wrap an iterator. It would include a method to
> > unroll an iterator into a buffer up to a point, something like
> >
> > def unroll(maxMem:Long) : Boolean ={ ...}
> >
> > And it would return True if the maxMem was hit. At which point
> > BlockManager could read through the already cached values, then
> > continue on through the rest of the iterators dumping all the values
> > to file. If it unrolled without hitting maxMem (which would probably
> > be most of the time), the class would simply wrap the ArrayBuffer of
> cached values.
> >
> > Kyle
> >
> >
> >
> > On Sun, Nov 3, 2013 at 12:50 AM, Reynold Xin <rxin@apache.org> wrote:
> >
> > > It's not a very elegant solution, but one possibility is for the
> > > CacheManager to check whether it will have enough space. If it is
> > > running out of space, skips buffering the output of the iterator &
> > > directly write the output of the iterator to disk (if storage level
> > allows that).
> > >
> > > But it is still tricky to know whether we will run out of space
> > > before we even start running the iterator. One possibility is to use
> > > sizing data from previous partitions to estimate the size of the
> > > current
> > partition (i.e.
> > > estimated in memory size = avg of current in-memory size / current
> > > input size).
> > >
> > > Do you have any ideas on this one, Kyle?
> > >
> > >
> > > On Sat, Oct 26, 2013 at 10:53 AM, Kyle Ellrott
> > > <kellrott@soe.ucsc.edu
> > > >wrote:
> > >
> > > > I was wondering if anybody had any thoughts on the best way to
> > > > tackle
> > > > SPARK-942 ( https://spark-project.atlassian.net/browse/SPARK-942 ).
> > > > Basically, Spark takes an iterator from a flatmap call and because
> > > > I tell it that it needs to persist Spark proceeds to push it all
> > > > into an array before deciding that it doesn't have enough memory
> > > > and trying to
> > > serialize
> > > > it to disk, and somewhere along the line it runs out of memory.
> > > > For my particular operation, the function return an iterator that
> > > > reads data out of a file, and the size of the files passed to that
> > > > function can vary greatly (from a few kilobytes to a few gigabytes).
> > > > The funny thing is
> > > that
> > > > if I do a strait 'map' operation after the flat map, everything
> > > > works, because Spark just passes the iterator forward and never
> > > > tries to expand the whole thing into memory. But I need do a
> > > > reduceByKey across all the records, so I'd like to persist to disk
> > > > first, and that is where I hit
> > > this
> > > > snag.
> > > > I've already setup a unit test to replicate the problem, and I
> > > > know the area of the code that would need to be fixed.
> > > > I'm just hoping for some tips on the best way to fix the problem.
> > > >
> > > > Kyle
> > > >
> > >
> >
>

--001a11c3556e7d77ce04eafbfeaf--

From dev-return-745-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 12 16:54:15 2013
Return-Path: <dev-return-745-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2E96310CAF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Nov 2013 16:54:15 +0000 (UTC)
Received: (qmail 41515 invoked by uid 500); 12 Nov 2013 16:54:14 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 41195 invoked by uid 500); 12 Nov 2013 16:54:11 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 41186 invoked by uid 99); 12 Nov 2013 16:54:10 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 16:54:10 +0000
X-ASF-Spam-Status: No, hits=-2.8 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_HI,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of Hussam_Jarada@dell.com designates 143.166.85.207 as permitted sender)
Received: from [143.166.85.207] (HELO ausxippc101.us.dell.com) (143.166.85.207)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 16:54:02 +0000
X-LoopCount0: from 10.175.216.249
X-IronPort-AV: E=Sophos;i="4.93,686,1378875600"; 
   d="scan'208,217";a="320702555"
From: <Hussam_Jarada@Dell.com>
To: <dev@spark.incubator.apache.org>
Date: Tue, 12 Nov 2013 10:53:31 -0600
Subject: Any setting or configuration that I can use in spark that would
 dump more info on job errors
Thread-Topic: Any setting or configuration that I can use in spark that
 would dump more info on job errors
Thread-Index: Ac7fVD43geJ0fLHSQ2GZBtbxzV1X9wAc0N5g
Message-ID: <C4DD46602A865B4A9CEDDC1776969FA80898E0C422@AUSX7MCPC107.AMER.DELL.COM>
References: <C4DD46602A865B4A9CEDDC1776969FA80898E0C30C@AUSX7MCPC107.AMER.DELL.COM>
In-Reply-To: <C4DD46602A865B4A9CEDDC1776969FA80898E0C30C@AUSX7MCPC107.AMER.DELL.COM>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-titus-version: 3.5.21.1
x-tituslabs-classifications-30: TLPropertyRoot=Dell;Classification=No
 Restrictions;Sublabels=;
x-tituslabs-classificationhash-30: 0fDZ0/3bNBI7cufMZ514M4qWdQcDEC0S5zWJDoHtiSo6xWWyj5+yriog/ByOqOBlaqg9a31uelYF7zxufWswQwaRjThcgY+W6Q5U9FMQNxdA3xTp9KCxUEh4JiOnCdu1EM6+pTbmk5N58fmQAC+rb81Mr9aGXLFkylvlHtzf3q1EuJ6HeOsJOhO9VQVpzZwWPGzr1xfTo+Gzf8gCCyJHCcMEdAVkJYYsk0MmmNbjjROBUBT7DDmHexaut9uDNohekNopwgIL7Jr1Yf7zK+/PhMd5toTDZc8R6icL9eDQRFgnYd9JgVGVks6qxaPMPggE
x-titusconfig: 1.2AMER
acceptlanguage: en-US
Content-Type: multipart/alternative;
	boundary="_000_C4DD46602A865B4A9CEDDC1776969FA80898E0C422AUSX7MCPC107A_"
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

--_000_C4DD46602A865B4A9CEDDC1776969FA80898E0C422AUSX7MCPC107A_
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable

Hi,

Using spark 0.8 and hadoop 1.2.1 with cluster of 2 node each have 16 CPU an=
d allocated 8G of RAM

I am running into a use case that if I try to save a very large JavaRDD<Str=
ing> that was created using paralleize from Java List<String> my job worker=
s are failing as follows

13/11/11 19:23:48 INFO Worker: Executor app-20131111191414-0001/2 finished =
with state FAILED message Command exited with code 1 exitStatus 1

Looks like the spark driver trying 5 times to execute the  then decide to k=
ill the process

Any help on how to get more info on the reason of failure or what code 1 ex=
istStatus 1 would means here?

Any setting or configuration that I can use in spark that would dump more i=
nfo on error?

Here's my logs

13/11/11 19:14:50 INFO Worker: Asked to launch executor app-20131111190659-=
0000/0 for OMDBQueryService
13/11/11 19:14:50 INFO ExecutorRunner: Launch command: "java" "-cp" ":/opt/=
spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark-assembl=
y_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" "-Dspark.executor.memory=3D8g" "-=
Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UseParallelOldGC"=
 "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.executor.memor=
y=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UsePara=
llelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.exec=
utor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-X=
X:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-D=
spark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParal=
lelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D=
1024m" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.StandaloneExecutorB=
ackend" "akka://spark@poc1:54482/user/StandaloneScheduler" "0" "poc3" "16"
13/11/11 19:16:47 INFO Worker: Executor app-20131111190659-0000/0 finished =
with state FAILED message Command exited with code 1 exitStatus 1
13/11/11 19:16:47 INFO Worker: Asked to launch executor app-20131111190659-=
0000/2 for OMDBQueryService
13/11/11 19:16:47 INFO ExecutorRunner: Launch command: "java" "-cp" ":/opt/=
spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark-assembl=
y_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" "-Dspark.executor.memory=3D8g" "-=
Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UseParallelOldGC"=
 "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.executor.memor=
y=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UsePara=
llelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.exec=
utor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-X=
X:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-D=
spark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParal=
lelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D=
1024m" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.StandaloneExecutorB=
ackend" "akka://spark@poc1:54482/user/StandaloneScheduler" "2" "poc3" "16"
13/11/11 19:16:53 INFO Worker: Executor app-20131111190659-0000/2 finished =
with state FAILED message Command exited with code 1 exitStatus 1
13/11/11 19:16:53 INFO Worker: Asked to launch executor app-20131111190659-=
0000/4 for OMDBQueryService
13/11/11 19:16:53 INFO ExecutorRunner: Launch command: "java" "-cp" ":/opt/=
spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark-assembl=
y_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" "-Dspark.executor.memory=3D8g" "-=
Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UseParallelOldGC"=
 "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.executor.memor=
y=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UsePara=
llelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.exec=
utor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-X=
X:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-D=
spark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParal=
lelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D=
1024m" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.StandaloneExecutorB=
ackend" "akka://spark@poc1:54482/user/StandaloneScheduler" "4" "poc3" "16"
13/11/11 19:17:02 INFO Worker: Executor app-20131111190659-0000/4 finished =
with state FAILED message Command exited with code 1 exitStatus 1
13/11/11 19:17:02 INFO Worker: Asked to launch executor app-20131111190659-=
0000/6 for OMDBQueryService
13/11/11 19:17:02 INFO ExecutorRunner: Launch command: "java" "-cp" ":/opt/=
spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark-assembl=
y_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" "-Dspark.executor.memory=3D8g" "-=
Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UseParallelOldGC"=
 "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.executor.memor=
y=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UsePara=
llelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.exec=
utor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-X=
X:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-D=
spark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParal=
lelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D=
1024m" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.StandaloneExecutorB=
ackend" "akka://spark@poc1:54482/user/StandaloneScheduler" "6" "poc3" "16"
13/11/11 19:17:09 INFO Worker: Executor app-20131111190659-0000/6 finished =
with state FAILED message Command exited with code 1 exitStatus 1
13/11/11 19:17:09 INFO Worker: Asked to launch executor app-20131111190659-=
0000/8 for OMDBQueryService
13/11/11 19:17:09 INFO ExecutorRunner: Launch command: "java" "-cp" ":/opt/=
spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark-assembl=
y_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" "-Dspark.executor.memory=3D8g" "-=
Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UseParallelOldGC"=
 "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.executor.memor=
y=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UsePara=
llelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.exec=
utor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-X=
X:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-D=
spark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParal=
lelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D=
1024m" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.StandaloneExecutorB=
ackend" "akka://spark@poc1:54482/user/StandaloneScheduler" "8" "poc3" "16"
13/11/11 19:17:17 INFO Worker: Executor app-20131111190659-0000/8 finished =
with state FAILED message Command exited with code 1 exitStatus 1
13/11/11 19:17:17 INFO Worker: Asked to launch executor app-20131111190659-=
0000/10 for OMDBQueryService
13/11/11 19:17:17 INFO ExecutorRunner: Launch command: "java" "-cp" ":/opt/=
spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark-assembl=
y_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" "-Dspark.executor.memory=3D8g" "-=
Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UseParallelOldGC"=
 "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.executor.memor=
y=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-XX:+UsePara=
llelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-Dspark.exec=
utor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" "-X=
X:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m" "-D=
spark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParal=
lelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D=
1024m" "-Xms512M" "-Xmx512M" "org.apache.spark.executor.StandaloneExecutorB=
ackend" "akka://spark@poc1:54482/user/StandaloneScheduler" "10" "poc3" "16"
13/11/11 19:17:20 INFO Worker: Asked to kill executor app-20131111190659-00=
00/10
13/11/11 19:17:20 INFO ExecutorRunner: Killing process!
13/11/11 19:17:20 INFO ExecutorRunner: Runner thread for executor app-20131=
111190659-0000/10 interrupted
13/11/11 19:17:21 INFO Worker: Executor app-20131111190659-0000/10 finished=
 with state KILLED

Thanks,
Hussam

--_000_C4DD46602A865B4A9CEDDC1776969FA80898E0C422AUSX7MCPC107A_--

From dev-return-746-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 12 19:07:57 2013
Return-Path: <dev-return-746-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9188F102A6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Nov 2013 19:07:57 +0000 (UTC)
Received: (qmail 59651 invoked by uid 500); 12 Nov 2013 19:07:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 59514 invoked by uid 500); 12 Nov 2013 19:07:56 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 59502 invoked by uid 99); 12 Nov 2013 19:07:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 19:07:56 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of stephen.haberman@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 19:07:49 +0000
Received: by mail-ob0-f177.google.com with SMTP id wp4so4310213obc.8
        for <dev@spark.incubator.apache.org>; Tue, 12 Nov 2013 11:07:28 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=date:from:to:subject:message-id:in-reply-to:references:mime-version
         :content-type:content-transfer-encoding;
        bh=nP3gcPGxruUefLchrbFwzhLh/035n2KH92f454pGKMc=;
        b=DRipO3e903YxnVFgJS+TkNMs+GSdJQtHdStuFMypHKJEgP+/UiasYk+QPK46YMs5Qr
         F689ffGUpNBXG+IImyFxFNGMUxHpJRmn0nB1kVObZf0L5lDU8qYgE/bJv/tZuG3MsH+2
         yzTWgbInrAol+GLRt98b8Nd6ojQ/em4P2XULP8n+e76RWrFvEgmplqYhs06QO+knL9z9
         /38TGG6xo77YarlfMNmzt8st5IndZriLvAvPq0Rz/4Xj181b904KgQsPeHzK+fuw2+VE
         Ys0SCkZJtNOhCpQnnEx8Ud31o2LLX/JsGGqr2M6WnalZGVsOcuXGGrA9XMYHqZUM7GSv
         8paw==
X-Received: by 10.60.52.172 with SMTP id u12mr2462657oeo.97.1384283248155;
        Tue, 12 Nov 2013 11:07:28 -0800 (PST)
Received: from sh9 (wsip-184-187-11-226.om.om.cox.net. [184.187.11.226])
        by mx.google.com with ESMTPSA id nw5sm7244192obc.9.2013.11.12.11.07.27
        for <dev@spark.incubator.apache.org>
        (version=SSLv3 cipher=RC4-SHA bits=128/128);
        Tue, 12 Nov 2013 11:07:27 -0800 (PST)
Date: Tue, 12 Nov 2013 13:07:26 -0600
From: Stephen Haberman <stephen.haberman@gmail.com>
To: dev@spark.incubator.apache.org
Subject: Re: SPARK-942
Message-ID: <20131112130726.30ffa327@sh9>
In-Reply-To: <CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
	<CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
X-Mailer: Claws Mail 3.9.1 (GTK+ 2.24.20; x86_64-pc-linux-gnu)
Mime-Version: 1.0
Content-Type: text/plain; charset=US-ASCII
Content-Transfer-Encoding: 7bit
X-Virus-Checked: Checked by ClamAV on apache.org


> The problem is that the iterator interface only defines 'hasNext' and
> 'next' methods.

Just a comment from the peanut gallery, but FWIW it seems like being
able to ask "how much data is here" would be a useful thing for Spark
to know, even if that means moving away from Iterator itself, or
something like IteratorWithSizeEstimate/something/something.

Not only for this, but so that, ideally, Spark could basically do
dynamic partitioning.

E.g. when we load a month's worth of data, it's X GB, but after a few
maps and filters, it's X/100 GB, so could use X/100 partitions instead.

But right now all partitioning decisions are made up-front,
via .coalesce/etc. type hints from the programmer, and it seems if
Spark could delay making partitioning decisions each until RDD could
like lazily-eval/sample a few lines (hand waving), that would be super
sexy from our respective, in terms of doing automatic perf/partition
optimization.

Huge disclaimer that this is probably a big pita to implement, and
could likely not be as worthwhile as I naively think it would be.

- Stephen

From dev-return-747-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 12 19:36:07 2013
Return-Path: <dev-return-747-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0CCC10407
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 12 Nov 2013 19:36:07 +0000 (UTC)
Received: (qmail 15416 invoked by uid 500); 12 Nov 2013 19:36:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 15381 invoked by uid 500); 12 Nov 2013 19:36:07 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 15373 invoked by uid 99); 12 Nov 2013 19:36:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 19:36:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of alex.boisvert@gmail.com designates 74.125.82.52 as permitted sender)
Received: from [74.125.82.52] (HELO mail-wg0-f52.google.com) (74.125.82.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 12 Nov 2013 19:36:02 +0000
Received: by mail-wg0-f52.google.com with SMTP id z12so5363867wgg.31
        for <dev@spark.incubator.apache.org>; Tue, 12 Nov 2013 11:35:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=jPsZy5WoIRBrgZpTM38zHlHwa/iLCNObTpDY00jWG9M=;
        b=EaTFypLX1UHMLOryfzNpLEf+VC9zd/cxVkw6X5SOKCzMTUQoy5YHCaom9fjXbwJaKf
         myk1jTQYaDz/KKRdlGTJ9ZfR2HmX7rvfM5/ovvgM0SrpABg+0C8+q0r94M+VJ2pNoOFz
         yFCnc4t3v6UUBpI4FqMLOks3ff8k4B0AvbkoZ2sD3CsKDn6SXCwwbEF54vmfmUvaqFr1
         syRJlOG+huvw7Rr6CvEyM5oNMKWPbFj+82DHmmoU5uYAM20PD4ufwa9q0rPJSru8tERC
         3UNP2TBi2XHbZeJHkXB+YAvYF2zqeylKLETBPRAcDw7F2MmMy7oN74LdLJQlrNLe9yof
         R+8g==
MIME-Version: 1.0
X-Received: by 10.180.198.193 with SMTP id je1mr16010828wic.6.1384284941105;
 Tue, 12 Nov 2013 11:35:41 -0800 (PST)
Received: by 10.216.39.10 with HTTP; Tue, 12 Nov 2013 11:35:41 -0800 (PST)
In-Reply-To: <20131112130726.30ffa327@sh9>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
	<CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
	<20131112130726.30ffa327@sh9>
Date: Tue, 12 Nov 2013 11:35:41 -0800
Message-ID: <CAHuk3y7Ui9qFVoaEy1-cpJz_LS5DDbM+jXaeVegsmGrZvWipWw@mail.gmail.com>
Subject: Re: SPARK-942
From: Alex Boisvert <alex.boisvert@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b6706d52ec4fc04eafff47f
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6706d52ec4fc04eafff47f
Content-Type: text/plain; charset=ISO-8859-1

On Tue, Nov 12, 2013 at 11:07 AM, Stephen Haberman <
stephen.haberman@gmail.com> wrote:

> Huge disclaimer that this is probably a big pita to implement, and
> could likely not be as worthwhile as I naively think it would be.
>

My perspective on this is it's already big pita of Spark users today.

In the absence of explicit directions/hints, Spark should be able to make
ballpark estimates and conservatively pick # of partitions, storage
strategies (e.g., memory vs disk) and other runtime parameters that fit the
deployment architecture/capacities.   If this requires code and extra
runtime resources for sampling/measuring data, guestimating job size, and
so on, so be it.

Users want working jobs first.  Optimal performance / resource utilization
follow from that.

--047d7b6706d52ec4fc04eafff47f--

From dev-return-748-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 13 00:44:51 2013
Return-Path: <dev-return-748-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 589D710252
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Nov 2013 00:44:51 +0000 (UTC)
Received: (qmail 42426 invoked by uid 500); 13 Nov 2013 00:44:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 42389 invoked by uid 500); 13 Nov 2013 00:44:51 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 42381 invoked by uid 99); 13 Nov 2013 00:44:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 00:44:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kellrott@soe.ucsc.edu designates 209.85.214.171 as permitted sender)
Received: from [209.85.214.171] (HELO mail-ob0-f171.google.com) (209.85.214.171)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 00:44:45 +0000
Received: by mail-ob0-f171.google.com with SMTP id gq1so6853177obb.16
        for <dev@spark.incubator.apache.org>; Tue, 12 Nov 2013 16:44:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ucsc.edu; s=ucsc-google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=YeBwYwsXQMVQvcPJu2Ykv9l0k6d4y8hfqg9NTxZ5tE8=;
        b=F2LxlL3baAXzR7pXA2gplPfBKjWcXwJgUrU8LVsrnxFqtQac1oz8bfpOUAN88Wyke1
         YbrJLZ8VXhazC+/AQxAcrBt2L406s6v7+CG2DoDoLaW+a/dcbHU77FFjoD34I9GPQC7p
         MF01baS0kVDriyJm+kWEDa/jk3ZdyMmSXhjwY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=YeBwYwsXQMVQvcPJu2Ykv9l0k6d4y8hfqg9NTxZ5tE8=;
        b=StrJ4I84IkByOoT/5GVfc+VeZCC7e1LLAG3no4t33DU5K0JFU7gxEX759LXGvC4iG5
         dX2w+7e/DYqimIhl39IlxBgvDnYVi2G+29srFbQYfCYccWFJnmp8DT+/MWC1T+XjOcNk
         XfXeP13W5uq7mX76oePogrHNRuviW1RXSrtVQYOQsPQvT1h1DRo8TNt+lgYrPfkroOLu
         Yiiu+JEhQLKk5aOrg1L/m9oJJx4IVnzs7ePk6xvOdQtE/6SGQzj2kdp+6awHfPmwZKgV
         0PXePhDGBFvFgON910tQicowVzsRW+ysMUKM2204zV7VZSRxSIXajICAUoGRAlYV5rX6
         wqDg==
X-Gm-Message-State: ALoCoQmMXMkhB6G0Idqf7Hdy8wCYgwime+fvGvq0d4LoPJXgwadQto1cAgIi+UG39diT6rXbLnGT
MIME-Version: 1.0
X-Received: by 10.182.131.196 with SMTP id oo4mr12734686obb.50.1384303463264;
 Tue, 12 Nov 2013 16:44:23 -0800 (PST)
Received: by 10.182.132.50 with HTTP; Tue, 12 Nov 2013 16:44:23 -0800 (PST)
In-Reply-To: <CAHuk3y7Ui9qFVoaEy1-cpJz_LS5DDbM+jXaeVegsmGrZvWipWw@mail.gmail.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
	<CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
	<20131112130726.30ffa327@sh9>
	<CAHuk3y7Ui9qFVoaEy1-cpJz_LS5DDbM+jXaeVegsmGrZvWipWw@mail.gmail.com>
Date: Tue, 12 Nov 2013 16:44:23 -0800
Message-ID: <CAKXMip10ctCfhLGiV+9qpuCGCqs88D4ArhjFuFnXWTQxceBygQ@mail.gmail.com>
Subject: Re: SPARK-942
From: Kyle Ellrott <kellrott@soe.ucsc.edu>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c1f6d6308dd604eb044467
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c1f6d6308dd604eb044467
Content-Type: text/plain; charset=ISO-8859-1

I've posted a patch that I think produces the correct behavior at
https://github.com/kellrott/incubator-spark/commit/efe1102c8a7436b2fe112d3bece9f35fedea0dc8

It works fine on my programs, but if I run the unit tests, I get errors
like:

[info] - large number of iterations *** FAILED ***
[info]   org.apache.spark.SparkException: Job aborted: Task 4.0:0 failed
more than 0 times; aborting job java.lang.ClassCastException:
scala.collection.immutable.StreamIterator cannot be cast to
scala.collection.mutable.ArrayBuffer
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:818)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:816)
[info]   at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
[info]   at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:816)
[info]   at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:431)
[info]   at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:493)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:158)


I can't figure out the line number of where the original error occurred. Or
why I can't replicate them in my various test programs.
Any help would be appreciated.

Kyle






On Tue, Nov 12, 2013 at 11:35 AM, Alex Boisvert <alex.boisvert@gmail.com>wrote:

> On Tue, Nov 12, 2013 at 11:07 AM, Stephen Haberman <
> stephen.haberman@gmail.com> wrote:
>
> > Huge disclaimer that this is probably a big pita to implement, and
> > could likely not be as worthwhile as I naively think it would be.
> >
>
> My perspective on this is it's already big pita of Spark users today.
>
> In the absence of explicit directions/hints, Spark should be able to make
> ballpark estimates and conservatively pick # of partitions, storage
> strategies (e.g., memory vs disk) and other runtime parameters that fit the
> deployment architecture/capacities.   If this requires code and extra
> runtime resources for sampling/measuring data, guestimating job size, and
> so on, so be it.
>
> Users want working jobs first.  Optimal performance / resource utilization
> follow from that.
>

--001a11c1f6d6308dd604eb044467--

From dev-return-749-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 13 01:59:57 2013
Return-Path: <dev-return-749-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E84C110547
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Nov 2013 01:59:57 +0000 (UTC)
Received: (qmail 54847 invoked by uid 500); 13 Nov 2013 01:59:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54787 invoked by uid 500); 13 Nov 2013 01:59:57 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 54779 invoked by uid 99); 13 Nov 2013 01:59:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 01:59:57 +0000
X-ASF-Spam-Status: No, hits=-3.4 required=5.0
	tests=RCVD_IN_DNSWL_HI,SPF_PASS,SUBJ_ALL_CAPS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of junluan.xia@intel.com designates 134.134.136.20 as permitted sender)
Received: from [134.134.136.20] (HELO mga02.intel.com) (134.134.136.20)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 01:59:52 +0000
Received: from fmsmga001.fm.intel.com ([10.253.24.23])
  by orsmga101.jf.intel.com with ESMTP; 12 Nov 2013 17:59:30 -0800
X-ExtLoop1: 1
X-IronPort-AV: E=Sophos;i="4.93,690,1378882800"; 
   d="scan'208";a="426568589"
Received: from fmsmsx104.amr.corp.intel.com ([10.19.9.35])
  by fmsmga001.fm.intel.com with ESMTP; 12 Nov 2013 17:59:29 -0800
Received: from fmsmsx153.amr.corp.intel.com (10.19.17.7) by
 FMSMSX104.amr.corp.intel.com (10.19.9.35) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Tue, 12 Nov 2013 17:59:29 -0800
Received: from shsmsx102.ccr.corp.intel.com (10.239.4.154) by
 FMSMSX153.amr.corp.intel.com (10.19.17.7) with Microsoft SMTP Server (TLS) id
 14.3.123.3; Tue, 12 Nov 2013 17:59:29 -0800
Received: from shsmsx104.ccr.corp.intel.com ([169.254.5.186]) by
 SHSMSX102.ccr.corp.intel.com ([10.239.4.154]) with mapi id 14.03.0123.003;
 Wed, 13 Nov 2013 09:59:27 +0800
From: "Xia, Junluan" <junluan.xia@intel.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Subject: RE: SPARK-942
Thread-Topic: SPARK-942
Thread-Index: AQHO4AmSF/DmhLIyIE+pej3vYP3PcpoiZ2ew
Date: Wed, 13 Nov 2013 01:59:27 +0000
Message-ID: <7841A4E0A9C8784C9D8B07DF5E48F0A811624284@SHSMSX104.ccr.corp.intel.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
	<CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
	<20131112130726.30ffa327@sh9>
	<CAHuk3y7Ui9qFVoaEy1-cpJz_LS5DDbM+jXaeVegsmGrZvWipWw@mail.gmail.com>
 <CAKXMip10ctCfhLGiV+9qpuCGCqs88D4ArhjFuFnXWTQxceBygQ@mail.gmail.com>
In-Reply-To: <CAKXMip10ctCfhLGiV+9qpuCGCqs88D4ArhjFuFnXWTQxceBygQ@mail.gmail.com>
Accept-Language: en-US
Content-Language: en-US
X-MS-Has-Attach:
X-MS-TNEF-Correlator:
x-originating-ip: [10.239.127.40]
Content-Type: text/plain; charset="us-ascii"
Content-Transfer-Encoding: quoted-printable
MIME-Version: 1.0
X-Virus-Checked: Checked by ClamAV on apache.org

Hi kely=20

I also build a patch for this issue, and pass the test, you could help me t=
o review if you are free.

-----Original Message-----
From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]=20
Sent: Wednesday, November 13, 2013 8:44 AM
To: dev@spark.incubator.apache.org
Subject: Re: SPARK-942

I've posted a patch that I think produces the correct behavior at
https://github.com/kellrott/incubator-spark/commit/efe1102c8a7436b2fe112d3b=
ece9f35fedea0dc8

It works fine on my programs, but if I run the unit tests, I get errors
like:

[info] - large number of iterations *** FAILED ***
[info]   org.apache.spark.SparkException: Job aborted: Task 4.0:0 failed
more than 0 times; aborting job java.lang.ClassCastException:
scala.collection.immutable.StreamIterator cannot be cast to scala.collectio=
n.mutable.ArrayBuffer
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGSche=
duler.scala:818)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGSche=
duler.scala:816)
[info]   at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:=
60)
[info]   at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:816)
[info]   at
org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:431=
)
[info]   at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:493)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:158)


I can't figure out the line number of where the original error occurred. Or=
 why I can't replicate them in my various test programs.
Any help would be appreciated.

Kyle






On Tue, Nov 12, 2013 at 11:35 AM, Alex Boisvert <alex.boisvert@gmail.com>wr=
ote:

> On Tue, Nov 12, 2013 at 11:07 AM, Stephen Haberman <=20
> stephen.haberman@gmail.com> wrote:
>
> > Huge disclaimer that this is probably a big pita to implement, and=20
> > could likely not be as worthwhile as I naively think it would be.
> >
>
> My perspective on this is it's already big pita of Spark users today.
>
> In the absence of explicit directions/hints, Spark should be able to=20
> make ballpark estimates and conservatively pick # of partitions,=20
> storage strategies (e.g., memory vs disk) and other runtime parameters th=
at fit the
> deployment architecture/capacities.   If this requires code and extra
> runtime resources for sampling/measuring data, guestimating job size,=20
> and so on, so be it.
>
> Users want working jobs first.  Optimal performance / resource=20
> utilization follow from that.
>

From dev-return-750-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 13 02:21:30 2013
Return-Path: <dev-return-750-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 00EFC105B8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Nov 2013 02:21:30 +0000 (UTC)
Received: (qmail 77079 invoked by uid 500); 13 Nov 2013 02:21:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77035 invoked by uid 500); 13 Nov 2013 02:21:29 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 77027 invoked by uid 99); 13 Nov 2013 02:21:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 02:21:29 +0000
X-ASF-Spam-Status: No, hits=3.1 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,SUBJ_ALL_CAPS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of kellrott@soe.ucsc.edu designates 209.85.214.180 as permitted sender)
Received: from [209.85.214.180] (HELO mail-ob0-f180.google.com) (209.85.214.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 02:21:24 +0000
Received: by mail-ob0-f180.google.com with SMTP id vb8so4086103obc.11
        for <dev@spark.incubator.apache.org>; Tue, 12 Nov 2013 18:21:03 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ucsc.edu; s=ucsc-google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=WFkR94miVBNKBefx//sFQUecN3noZtlZB3Q9GBdBjRc=;
        b=hY0Ap1XpGCRlcm6Ubfknkp9ljj+ZoRhc22VMCKhjGsrlNXkYu0Mo9HyU3CA63YBsp1
         ndzMFDjJjjYqm9hva7kkYSGS1o531ZLjq/br9GcviNbxvc4F2XVwVoZvWfNoFCxvgHUM
         5WehEh1xJzyiJobYBE7/M48Yoy+F1y5MF7Nag=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=WFkR94miVBNKBefx//sFQUecN3noZtlZB3Q9GBdBjRc=;
        b=jP04me9lVAUT7nRCwHM0329GIzXskLg2Xkk4mpuNmNv6ruhS4fCy2WRL/ZCT6fadxN
         pBi5W4AskoRP+LdeKV7RR9/0mrAnCq1ADbY0rH7tClACFdSxjuPS5y6u4KyccUISyRni
         oKHP39tBwvF+yQ8t7wF+L0Eyve33Ddm+wO8/mSPRveHObKWWpECyIoHGF2Obv+lYvxlE
         2NmhZ3vAuHb80EP9tRFxmvmmTtol5K+eNHpe+BLjhL6su3TSX7rLzhNiMQfKythnPmP3
         JJTczJTAyWhv8k2yOyqcxj9/a/3UEGSmI0sUAfM1aSF4qi6bFdMtqMysDaWL5CGVqxvq
         JUbg==
X-Gm-Message-State: ALoCoQklX3dLNqzo6x7MBsS6ytvdxQ2bzm4dmufAOP1HxsxYdehodP513OZXPmQdlc9Vh5dfZP1H
MIME-Version: 1.0
X-Received: by 10.182.48.130 with SMTP id l2mr15043715obn.44.1384309262829;
 Tue, 12 Nov 2013 18:21:02 -0800 (PST)
Received: by 10.182.132.50 with HTTP; Tue, 12 Nov 2013 18:21:02 -0800 (PST)
Received: by 10.182.132.50 with HTTP; Tue, 12 Nov 2013 18:21:02 -0800 (PST)
In-Reply-To: <7841A4E0A9C8784C9D8B07DF5E48F0A811624284@SHSMSX104.ccr.corp.intel.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
	<CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
	<20131112130726.30ffa327@sh9>
	<CAHuk3y7Ui9qFVoaEy1-cpJz_LS5DDbM+jXaeVegsmGrZvWipWw@mail.gmail.com>
	<CAKXMip10ctCfhLGiV+9qpuCGCqs88D4ArhjFuFnXWTQxceBygQ@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A811624284@SHSMSX104.ccr.corp.intel.com>
Date: Tue, 12 Nov 2013 18:21:02 -0800
Message-ID: <CAKXMip27iKncnKGAaCGfkr+hONSyFKAdM+T5V4TyPqQpKThfew@mail.gmail.com>
Subject: RE: SPARK-942
From: Kyle Ellrott <kellrott@soe.ucsc.edu>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e0160b3dcdee27804eb059de4
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0160b3dcdee27804eb059de4
Content-Type: text/plain; charset=ISO-8859-1

Sure, do you have a URL for your patch?

Kyle
On Nov 12, 2013 5:59 PM, "Xia, Junluan" <junluan.xia@intel.com> wrote:

> Hi kely
>
> I also build a patch for this issue, and pass the test, you could help me
> to review if you are free.
>
> -----Original Message-----
> From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]
> Sent: Wednesday, November 13, 2013 8:44 AM
> To: dev@spark.incubator.apache.org
> Subject: Re: SPARK-942
>
> I've posted a patch that I think produces the correct behavior at
>
> https://github.com/kellrott/incubator-spark/commit/efe1102c8a7436b2fe112d3bece9f35fedea0dc8
>
> It works fine on my programs, but if I run the unit tests, I get errors
> like:
>
> [info] - large number of iterations *** FAILED ***
> [info]   org.apache.spark.SparkException: Job aborted: Task 4.0:0 failed
> more than 0 times; aborting job java.lang.ClassCastException:
> scala.collection.immutable.StreamIterator cannot be cast to
> scala.collection.mutable.ArrayBuffer
> [info]   at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:818)
> [info]   at
>
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:816)
> [info]   at
>
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
> [info]   at
> scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> [info]   at
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:816)
> [info]   at
>
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:431)
> [info]   at org.apache.spark.scheduler.DAGScheduler.org
> $apache$spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:493)
> [info]   at
> org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:158)
>
>
> I can't figure out the line number of where the original error occurred.
> Or why I can't replicate them in my various test programs.
> Any help would be appreciated.
>
> Kyle
>
>
>
>
>
>
> On Tue, Nov 12, 2013 at 11:35 AM, Alex Boisvert <alex.boisvert@gmail.com
> >wrote:
>
> > On Tue, Nov 12, 2013 at 11:07 AM, Stephen Haberman <
> > stephen.haberman@gmail.com> wrote:
> >
> > > Huge disclaimer that this is probably a big pita to implement, and
> > > could likely not be as worthwhile as I naively think it would be.
> > >
> >
> > My perspective on this is it's already big pita of Spark users today.
> >
> > In the absence of explicit directions/hints, Spark should be able to
> > make ballpark estimates and conservatively pick # of partitions,
> > storage strategies (e.g., memory vs disk) and other runtime parameters
> that fit the
> > deployment architecture/capacities.   If this requires code and extra
> > runtime resources for sampling/measuring data, guestimating job size,
> > and so on, so be it.
> >
> > Users want working jobs first.  Optimal performance / resource
> > utilization follow from that.
> >
>

--089e0160b3dcdee27804eb059de4--

From dev-return-751-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 13 03:33:07 2013
Return-Path: <dev-return-751-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EACBD10765
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Nov 2013 03:33:07 +0000 (UTC)
Received: (qmail 53934 invoked by uid 500); 13 Nov 2013 03:33:07 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53899 invoked by uid 500); 13 Nov 2013 03:33:07 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 53851 invoked by uid 99); 13 Nov 2013 03:33:06 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 03:33:06 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.176 as permitted sender)
Received: from [209.85.192.176] (HELO mail-pd0-f176.google.com) (209.85.192.176)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 03:32:50 +0000
Received: by mail-pd0-f176.google.com with SMTP id r10so2506154pdi.21
        for <multiple recipients>; Tue, 12 Nov 2013 19:32:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date:cc
         :content-transfer-encoding:message-id:references:to;
        bh=+T3HI8eYXsMsXWcxT1ODvJgjHxqzQ/BdFm1MRGjkgAE=;
        b=A/cIRPvWhSt0gIRjTQcQ9BR+1BDy2IeF9MU5/LHBjKbdnl+xLppULLPcTudk2HW6oE
         GppFh40Sk5J72MZbHtofsWZW/9ZXQM4BYHWbtdXXgj3kUR8sSb9HNrQ2QaXx3ROlgex1
         BynK0A2B2SGSU6oXMhhhX4wz040KMbI4Oau9rRFsKw7eUtfgOHlP3AQwhxFQzX0LyEtg
         X5krEZI43mxzPyV/kXqP0JVl2waE13a3hIk/9BhXfRscl39+BdpYl/g2mxhjDUW/vEEm
         gymIVVO7naGowvlazI1X0fZC7B09FDjYiau8wcblbS7vkMzl1N+s9nBKYKj8v6K/lb/J
         wqDQ==
X-Received: by 10.68.189.133 with SMTP id gi5mr39221486pbc.57.1384313545147;
        Tue, 12 Nov 2013 19:32:25 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ho3sm40932858pbb.23.2013.11.12.19.32.22
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 12 Nov 2013 19:32:23 -0800 (PST)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
Subject: Re: problems with sbt
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACwKa9fmd5MeprLoBR8bHB2TDKQHg7X7bB5AZSi7XV_cJQru8w@mail.gmail.com>
Date: Tue, 12 Nov 2013 19:32:22 -0800
Cc: dev@spark.incubator.apache.org
Content-Transfer-Encoding: quoted-printable
Message-Id: <6D458FCD-021E-4509-BBB6-6C38B1C5BF2E@gmail.com>
References: <CACwKa9fmd5MeprLoBR8bHB2TDKQHg7X7bB5AZSi7XV_cJQru8w@mail.gmail.com>
To: user@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

It=92s hard to tell, but maybe you=92ve run out of space in your working =
directory? The assembly command will try to write stuff in =
assembly/target.

Matei

On Nov 11, 2013, at 2:54 PM, Umar Javed <umarj.javed@gmail.com> wrote:

> I keep getting these io.Exception Permission denied errors when =
building with sbt assembly:
>=20
> java.io.IOException: Permission denied
>         at java.io.FileOutputStream.close0(Native Method)
>         at java.io.FileOutputStream.close(FileOutputStream.java:393)
>         at =
java.io.FilterOutputStream.close(FilterOutputStream.java:160)
>         at =
java.util.zip.DeflaterOutputStream.close(DeflaterOutputStream.java:241)
>         at =
java.util.zip.ZipOutputStream.close(ZipOutputStream.java:360)
>         at sbt.IO$$anonfun$withZipOutput$1.apply(IO.scala:497)
>         at sbt.IO$$anonfun$withZipOutput$1.apply(IO.scala:482)
>         at sbt.Using.apply(Using.scala:25)
>         at sbt.IO$.withZipOutput(IO.scala:482)
>         at sbt.IO$.archive(IO.scala:401)
>         at sbt.IO$.jar(IO.scala:384)
>         at sbt.Package$.makeJar(Package.scala:107)
>         at =
sbt.Package$$anonfun$3$$anonfun$apply$3.apply(Package.scala:72)
>         at =
sbt.Package$$anonfun$3$$anonfun$apply$3.apply(Package.scala:70)
>         at =
sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:57)
>         at =
sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:52)
>         at sbt.Package$.apply(Package.scala:80)
>         at sbtassembly.Plugin$Assembly$.makeJar$1(Plugin.scala:174)
>         at =
sbtassembly.Plugin$Assembly$$anonfun$4$$anonfun$apply$3.apply(Plugin.scala=
:181)
>         at =
sbtassembly.Plugin$Assembly$$anonfun$4$$anonfun$apply$3.apply(Plugin.scala=
:177)
>         at =
sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:57)
>         at =
sbt.Tracked$$anonfun$outputChanged$1.apply(Tracked.scala:52)
>         at sbtassembly.Plugin$Assembly$.apply(Plugin.scala:189)
>         at =
sbtassembly.Plugin$.sbtassembly$Plugin$$assemblyTask(Plugin.scala:157)
>         at =
sbtassembly.Plugin$$anonfun$baseAssemblySettings$6.apply(Plugin.scala:369)=

>         at =
sbtassembly.Plugin$$anonfun$baseAssemblySettings$6.apply(Plugin.scala:368)=

>         at sbt.Scoped$$anonfun$hf10$1.apply(Structure.scala:586)
>         at sbt.Scoped$$anonfun$hf10$1.apply(Structure.scala:586)
>         at =
scala.Function1$$anonfun$compose$1.apply(Function1.scala:49)
>         at =
sbt.Scoped$Reduced$$anonfun$combine$1$$anonfun$apply$12.apply(Structure.sc=
ala:311)
>         at =
sbt.Scoped$Reduced$$anonfun$combine$1$$anonfun$apply$12.apply(Structure.sc=
ala:311)
>         at =
sbt.$tilde$greater$$anonfun$$u2219$1.apply(TypeFunctions.scala:41)
>         at sbt.std.Transform$$anon$5.work(System.scala:71)
>         at =
sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232)
>         at =
sbt.Execute$$anonfun$submit$1$$anonfun$apply$1.apply(Execute.scala:232)
>         at sbt.ErrorHandling$.wideConvert(ErrorHandling.scala:18)
>         at sbt.Execute.work(Execute.scala:238)
>         at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232)
>         at sbt.Execute$$anonfun$submit$1.apply(Execute.scala:232)
>         at =
sbt.ConcurrentRestrictions$$anon$4$$anonfun$1.apply(ConcurrentRestrictions=
.scala:160)
>         at =
sbt.CompletionService$$anon$2.call(CompletionService.scala:30)
>         at java.util.concurrent.FutureTask.run(FutureTask.java:262)
>         at =
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
>         at java.util.concurrent.FutureTask.run(FutureTask.java:262)
>         at =
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:=
1145)
>         at =
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java=
:615)
>         at java.lang.Thread.run(Thread.java:744)
> [error] (assembly/*:assembly) java.io.IOException: Permission denied
>=20
> Can somebody help me out?
> thanks,
> Umar
>=20


From dev-return-752-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 13 17:52:26 2013
Return-Path: <dev-return-752-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9E54C1057B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 13 Nov 2013 17:52:26 +0000 (UTC)
Received: (qmail 34367 invoked by uid 500); 13 Nov 2013 17:52:25 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34280 invoked by uid 500); 13 Nov 2013 17:52:19 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 34271 invoked by uid 99); 13 Nov 2013 17:52:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 17:52:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 209.85.212.178 as permitted sender)
Received: from [209.85.212.178] (HELO mail-wi0-f178.google.com) (209.85.212.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 13 Nov 2013 17:52:13 +0000
Received: by mail-wi0-f178.google.com with SMTP id hn6so1212688wib.11
        for <dev@spark.incubator.apache.org>; Wed, 13 Nov 2013 09:51:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=8lQD17g8ujBuY5WpSyp+mw/RlPUwj4334tD6Kjx6BbU=;
        b=Rp2TbHMXccKvE9B/KE+Y9F2VPgnfaB85YZKckIGI/rX/E/9JhuioeSYCgnypOWQqZG
         h42CmDKCanUB/IoKRFZR5UE+ye7prhvV7qml4Y3Ulz1W4Lks9SctiTNv3e8ldykbJxPn
         hQGmSA3YqbgGLadrvFmBoa5JkGq1RuRSca3XQZ54Tb54C4Lceeh34YMfdhXCVomVcV/c
         1VElyVksVq/9xrcqDBSzlpgtRvytdeuxhDVSZ+Q8IjtlA2bOzHd6xqla+C4yGRgYuIcX
         UrwJAmEiULvl2YrAWPLBeSPCewV/nas5a2aGleNNylQTYFAL5kET4mgOJQ8sDFXlNq9x
         IyDw==
X-Received: by 10.180.72.204 with SMTP id f12mr7093636wiv.56.1384365112402;
 Wed, 13 Nov 2013 09:51:52 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.119.228 with HTTP; Wed, 13 Nov 2013 09:51:32 -0800 (PST)
In-Reply-To: <CAKXMip27iKncnKGAaCGfkr+hONSyFKAdM+T5V4TyPqQpKThfew@mail.gmail.com>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
 <CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
 <CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
 <7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
 <CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
 <20131112130726.30ffa327@sh9> <CAHuk3y7Ui9qFVoaEy1-cpJz_LS5DDbM+jXaeVegsmGrZvWipWw@mail.gmail.com>
 <CAKXMip10ctCfhLGiV+9qpuCGCqs88D4ArhjFuFnXWTQxceBygQ@mail.gmail.com>
 <7841A4E0A9C8784C9D8B07DF5E48F0A811624284@SHSMSX104.ccr.corp.intel.com> <CAKXMip27iKncnKGAaCGfkr+hONSyFKAdM+T5V4TyPqQpKThfew@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Wed, 13 Nov 2013 09:51:32 -0800
Message-ID: <CANGvG8oAWTcC3KJO7sJ7Qc_ZrDeDbjcrAChEOyk-WgB56z4rvw@mail.gmail.com>
Subject: Re: SPARK-942
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=f46d043be28ec3ada404eb129e7b
X-Virus-Checked: Checked by ClamAV on apache.org

--f46d043be28ec3ada404eb129e7b
Content-Type: text/plain; charset=ISO-8859-1

By the way, there are a few places one can look for logs while testing:
Unit test runner logs (should contain driver and worker
logs): core/target/unit-tests.log
Executor logs: work/app-*

This should help find the root exception when you see one caught by the
DAGScheduler, such as in this case.


On Tue, Nov 12, 2013 at 6:21 PM, Kyle Ellrott <kellrott@soe.ucsc.edu> wrote:

> Sure, do you have a URL for your patch?
>
> Kyle
> On Nov 12, 2013 5:59 PM, "Xia, Junluan" <junluan.xia@intel.com> wrote:
>
> > Hi kely
> >
> > I also build a patch for this issue, and pass the test, you could help me
> > to review if you are free.
> >
> > -----Original Message-----
> > From: Kyle Ellrott [mailto:kellrott@soe.ucsc.edu]
> > Sent: Wednesday, November 13, 2013 8:44 AM
> > To: dev@spark.incubator.apache.org
> > Subject: Re: SPARK-942
> >
> > I've posted a patch that I think produces the correct behavior at
> >
> >
> https://github.com/kellrott/incubator-spark/commit/efe1102c8a7436b2fe112d3bece9f35fedea0dc8
> >
> > It works fine on my programs, but if I run the unit tests, I get errors
> > like:
> >
> > [info] - large number of iterations *** FAILED ***
> > [info]   org.apache.spark.SparkException: Job aborted: Task 4.0:0 failed
> > more than 0 times; aborting job java.lang.ClassCastException:
> > scala.collection.immutable.StreamIterator cannot be cast to
> > scala.collection.mutable.ArrayBuffer
> > [info]   at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:818)
> > [info]   at
> >
> >
> org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:816)
> > [info]   at
> >
> >
> scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:60)
> > [info]   at
> > scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
> > [info]   at
> >
> org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:816)
> > [info]   at
> >
> >
> org.apache.spark.scheduler.DAGScheduler.processEvent(DAGScheduler.scala:431)
> > [info]   at org.apache.spark.scheduler.DAGScheduler.org
> > $apache$spark$scheduler$DAGScheduler$$run(DAGScheduler.scala:493)
> > [info]   at
> >
> org.apache.spark.scheduler.DAGScheduler$$anon$1.run(DAGScheduler.scala:158)
> >
> >
> > I can't figure out the line number of where the original error occurred.
> > Or why I can't replicate them in my various test programs.
> > Any help would be appreciated.
> >
> > Kyle
> >
> >
> >
> >
> >
> >
> > On Tue, Nov 12, 2013 at 11:35 AM, Alex Boisvert <alex.boisvert@gmail.com
> > >wrote:
> >
> > > On Tue, Nov 12, 2013 at 11:07 AM, Stephen Haberman <
> > > stephen.haberman@gmail.com> wrote:
> > >
> > > > Huge disclaimer that this is probably a big pita to implement, and
> > > > could likely not be as worthwhile as I naively think it would be.
> > > >
> > >
> > > My perspective on this is it's already big pita of Spark users today.
> > >
> > > In the absence of explicit directions/hints, Spark should be able to
> > > make ballpark estimates and conservatively pick # of partitions,
> > > storage strategies (e.g., memory vs disk) and other runtime parameters
> > that fit the
> > > deployment architecture/capacities.   If this requires code and extra
> > > runtime resources for sampling/measuring data, guestimating job size,
> > > and so on, so be it.
> > >
> > > Users want working jobs first.  Optimal performance / resource
> > > utilization follow from that.
> > >
> >
>

--f46d043be28ec3ada404eb129e7b--

From dev-return-753-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 14 03:37:38 2013
Return-Path: <dev-return-753-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C2188109E8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Nov 2013 03:37:38 +0000 (UTC)
Received: (qmail 82294 invoked by uid 500); 14 Nov 2013 03:37:36 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82236 invoked by uid 500); 14 Nov 2013 03:37:34 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 82219 invoked by uid 99); 14 Nov 2013 03:37:33 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 03:37:33 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.160.42 as permitted sender)
Received: from [209.85.160.42] (HELO mail-pb0-f42.google.com) (209.85.160.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 03:37:26 +0000
Received: by mail-pb0-f42.google.com with SMTP id uo5so1417094pbc.1
        for <multiple recipients>; Wed, 13 Nov 2013 19:37:05 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=from:content-type:subject:date:message-id:cc:to:mime-version;
        bh=1CsExiKG5ty5SZsAvDqivKsEd3DA3A2YAVaZDVZQNrw=;
        b=NgRteAn1qJJtXTKvlfH3M6F/f+7CkrtTRjG+KLofpvE9qT/XJ+2i6JEson20l7bOZm
         YUdwVHWg1DtD8AK3HlttiATA+3Hk6OBb/osQZFR1IY3Fv5G0nNIdl2hOEjhxOYe1BTWx
         H1ZT0sb8+/589eKYRrfyw/JkdebwQ0gf/6nYM90EZvMF4Jd0fP+pfozagrzQ3rFkIcb3
         1ZR16RGbdMbwZb8Avz3qoBVkcITYesKsVDcZaqGmlkoXVKBsJrMpfZU009EdFVehD9nN
         +4FoGhi5wjllmU3VCB0dg30b8JBBrV1CwD7qvPrgm2uL+slOp2EUVcew+jv7px+EXCmU
         5MWQ==
X-Received: by 10.66.27.136 with SMTP id t8mr12222948pag.171.1384400225414;
        Wed, 13 Nov 2013 19:37:05 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id hw10sm47921741pbc.24.2013.11.13.19.37.04
        for <multiple recipients>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 13 Nov 2013 19:37:04 -0800 (PST)
From: Matei Zaharia <matei.zaharia@gmail.com>
Content-Type: multipart/alternative; boundary="Apple-Mail=_2633ABBB-1CE5-4A00-BE8F-954DBE6F40C3"
Subject: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and Prashant Sharma
Date: Wed, 13 Nov 2013 19:37:02 -0800
Message-Id: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
Cc: dev@spark.incubator.apache.org
To: user@spark.incubator.apache.org
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

--Apple-Mail=_2633ABBB-1CE5-4A00-BE8F-954DBE6F40C3
Content-Transfer-Encoding: quoted-printable
Content-Type: text/plain;
	charset=windows-1252

Hi folks,

The Apache Spark PPMC is happy to welcome two new PPMC members and =
committers: Tom Graves and Prashant Sharma.

Tom has been maintaining and expanding the YARN support in Spark over =
the past few months, including adding big features such as support for =
YARN security, and recently contributed a major patch that adds security =
to all of Spark=92s internal communication services as well =
(https://github.com/apache/incubator-spark/pull/120). It will be great =
to have him continue expanding these and other features as a committer.

Prashant created and has maintained the Scala 2.10 branch of Spark for =
6+ months now, including tackling the hairy task of porting the Spark =
interpreter to 2.10, and debugging all the issues raised by that with =
third-party libraries. He's also contributed bug fixes and new input =
sources to Spark Streaming. The Scala 2.10 branch will be merged into =
master soon.

We=92re very excited to have both Tom and Prashant join the project as =
committers.

The Apache Spark PPMC


--Apple-Mail=_2633ABBB-1CE5-4A00-BE8F-954DBE6F40C3--

From dev-return-754-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 14 04:40:30 2013
Return-Path: <dev-return-754-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 2472410B03
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Nov 2013 04:40:30 +0000 (UTC)
Received: (qmail 49955 invoked by uid 500); 14 Nov 2013 04:40:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 49887 invoked by uid 500); 14 Nov 2013 04:40:24 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 49877 invoked by uid 99); 14 Nov 2013 04:40:22 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 04:40:22 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.178 as permitted sender)
Received: from [209.85.192.178] (HELO mail-pd0-f178.google.com) (209.85.192.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 04:40:13 +0000
Received: by mail-pd0-f178.google.com with SMTP id p10so1430027pdj.37
        for <dev@spark.incubator.apache.org>; Wed, 13 Nov 2013 20:39:52 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=/UM7ovWHggwV8tkxZxhPU4G/tEyb5Yb7MBASDL9OnF0=;
        b=GuZu7IsOF35YHtJakXeyd7oGKktoPbAPKzh1Dl5FZr6+IUBCxjxJxUufuqhVpNpOC3
         DK5Kj38GyvN6ZYVIXtXsZ0YCqnuBFFUNlYwfU7CTX4IClbAQQpInxZPx+dRjbzAgIsXm
         2OD5DOuqgQEs8hg7+I6wQc0L/YjoKyqd7hTvsLl+n4+LG+JMgH1dRawwP5uHnYnbNmT+
         9u+ds4m51Dw09UNeYLzZTn75vXhKjZaKiRpBYdAzdeP99CgwGzJOgrETEdiVf8mJ1FFY
         nP+yLcaB+6esXOx+4QjNOnSrmtrRACe92gNnRPpFAERuqHFPyUXb2/ng7+GX5D8k5A4H
         aVsw==
X-Received: by 10.66.163.164 with SMTP id yj4mr46351527pab.91.1384403992208;
        Wed, 13 Nov 2013 20:39:52 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id b3sm44050818pbu.38.2013.11.13.20.39.50
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 13 Nov 2013 20:39:51 -0800 (PST)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1816\))
Subject: Re: Any setting or configuration that I can use in spark that would dump more info on job errors
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <C4DD46602A865B4A9CEDDC1776969FA80898E0C422@AUSX7MCPC107.AMER.DELL.COM>
Date: Wed, 13 Nov 2013 20:39:48 -0800
Content-Transfer-Encoding: quoted-printable
Message-Id: <70C159A0-AAAA-41DD-8E8D-28E58CCD9036@gmail.com>
References: <C4DD46602A865B4A9CEDDC1776969FA80898E0C30C@AUSX7MCPC107.AMER.DELL.COM> <C4DD46602A865B4A9CEDDC1776969FA80898E0C422@AUSX7MCPC107.AMER.DELL.COM>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1816)
X-Virus-Checked: Checked by ClamAV on apache.org

Hi Hussam,

Have you looked at the stdout and stderr files from the worker process? =
You can find them in the =93work=94 directory under SPARK_HOME on the =
slave node. They might have some information about why it crashed. =
Otherwise, I=92d recommend profiling the workers with tools like jmap or =
jstack to see what objects take up memory. Commonly the problem may be =
having too low a level of parallelism set.

Matei

On Nov 12, 2013, at 8:53 AM, Hussam_Jarada@Dell.com wrote:

> Hi,
>=20
> Using spark 0.8 and hadoop 1.2.1 with cluster of 2 node each have 16 =
CPU and allocated 8G of RAM
>=20
> I am running into a use case that if I try to save a very large =
JavaRDD<String> that was created using paralleize from Java List<String> =
my job workers are failing as follows
>=20
> 13/11/11 19:23:48 INFO Worker: Executor app-20131111191414-0001/2 =
finished with state FAILED message Command exited with code 1 exitStatus =
1
>=20
> Looks like the spark driver trying 5 times to execute the  then decide =
to kill the process
>=20
> Any help on how to get more info on the reason of failure or what code =
1 existStatus 1 would means here?
>=20
> Any setting or configuration that I can use in spark that would dump =
more info on error?
>=20
> Here's my logs
>=20
> 13/11/11 19:14:50 INFO Worker: Asked to launch executor =
app-20131111190659-0000/0 for OMDBQueryService
> 13/11/11 19:14:50 INFO ExecutorRunner: Launch command: "java" "-cp" =
":/opt/spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark=
-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" =
"-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Xms512M" "-Xmx512M" =
"org.apache.spark.executor.StandaloneExecutorBackend" =
"akka://spark@poc1:54482/user/StandaloneScheduler" "0" "poc3" "16"
> 13/11/11 19:16:47 INFO Worker: Executor app-20131111190659-0000/0 =
finished with state FAILED message Command exited with code 1 exitStatus =
1
> 13/11/11 19:16:47 INFO Worker: Asked to launch executor =
app-20131111190659-0000/2 for OMDBQueryService
> 13/11/11 19:16:47 INFO ExecutorRunner: Launch command: "java" "-cp" =
":/opt/spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark=
-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" =
"-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Xms512M" "-Xmx512M" =
"org.apache.spark.executor.StandaloneExecutorBackend" =
"akka://spark@poc1:54482/user/StandaloneScheduler" "2" "poc3" "16"
> 13/11/11 19:16:53 INFO Worker: Executor app-20131111190659-0000/2 =
finished with state FAILED message Command exited with code 1 exitStatus =
1
> 13/11/11 19:16:53 INFO Worker: Asked to launch executor =
app-20131111190659-0000/4 for OMDBQueryService
> 13/11/11 19:16:53 INFO ExecutorRunner: Launch command: "java" "-cp" =
":/opt/spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark=
-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" =
"-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Xms512M" "-Xmx512M" =
"org.apache.spark.executor.StandaloneExecutorBackend" =
"akka://spark@poc1:54482/user/StandaloneScheduler" "4" "poc3" "16"
> 13/11/11 19:17:02 INFO Worker: Executor app-20131111190659-0000/4 =
finished with state FAILED message Command exited with code 1 exitStatus =
1
> 13/11/11 19:17:02 INFO Worker: Asked to launch executor =
app-20131111190659-0000/6 for OMDBQueryService
> 13/11/11 19:17:02 INFO ExecutorRunner: Launch command: "java" "-cp" =
":/opt/spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark=
-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" =
"-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Xms512M" "-Xmx512M" =
"org.apache.spark.executor.StandaloneExecutorBackend" =
"akka://spark@poc1:54482/user/StandaloneScheduler" "6" "poc3" "16"
> 13/11/11 19:17:09 INFO Worker: Executor app-20131111190659-0000/6 =
finished with state FAILED message Command exited with code 1 exitStatus =
1
> 13/11/11 19:17:09 INFO Worker: Asked to launch executor =
app-20131111190659-0000/8 for OMDBQueryService
> 13/11/11 19:17:09 INFO ExecutorRunner: Launch command: "java" "-cp" =
":/opt/spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark=
-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" =
"-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Xms512M" "-Xmx512M" =
"org.apache.spark.executor.StandaloneExecutorBackend" =
"akka://spark@poc1:54482/user/StandaloneScheduler" "8" "poc3" "16"
> 13/11/11 19:17:17 INFO Worker: Executor app-20131111190659-0000/8 =
finished with state FAILED message Command exited with code 1 exitStatus =
1
> 13/11/11 19:17:17 INFO Worker: Asked to launch executor =
app-20131111190659-0000/10 for OMDBQueryService
> 13/11/11 19:17:17 INFO ExecutorRunner: Launch command: "java" "-cp" =
":/opt/spark-0.8.0/conf:/opt/spark-0.8.0/assembly/target/scala-2.9.3/spark=
-assembly_2.9.3-0.8.0-incubating-hadoop1.0.4.jar" =
"-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Dspark.executor.memory=3D8g" "-Dspark.local.dir=3D/tmp/spark" =
"-XX:+UseParallelGC" "-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" =
"-XX:MaxPermSize=3D1024m" "-Dspark.executor.memory=3D8g" =
"-Dspark.local.dir=3D/tmp/spark" "-XX:+UseParallelGC" =
"-XX:+UseParallelOldGC" "-XX:+DisableExplicitGC" "-XX:MaxPermSize=3D1024m"=
 "-Xms512M" "-Xmx512M" =
"org.apache.spark.executor.StandaloneExecutorBackend" =
"akka://spark@poc1:54482/user/StandaloneScheduler" "10" "poc3" "16"
> 13/11/11 19:17:20 INFO Worker: Asked to kill executor =
app-20131111190659-0000/10
> 13/11/11 19:17:20 INFO ExecutorRunner: Killing process!
> 13/11/11 19:17:20 INFO ExecutorRunner: Runner thread for executor =
app-20131111190659-0000/10 interrupted
> 13/11/11 19:17:21 INFO Worker: Executor app-20131111190659-0000/10 =
finished with state KILLED
>=20
> Thanks,
> Hussam


From dev-return-755-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 14 05:21:29 2013
Return-Path: <dev-return-755-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 18EAC10BD9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Nov 2013 05:21:29 +0000 (UTC)
Received: (qmail 91113 invoked by uid 500); 14 Nov 2013 05:21:28 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 91072 invoked by uid 500); 14 Nov 2013 05:21:27 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 91064 invoked by uid 99); 14 Nov 2013 05:21:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 05:21:27 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of karthik.tunga@gmail.com designates 209.85.128.52 as permitted sender)
Received: from [209.85.128.52] (HELO mail-qe0-f52.google.com) (209.85.128.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 05:21:23 +0000
Received: by mail-qe0-f52.google.com with SMTP id cz11so470956qeb.25
        for <dev@spark.incubator.apache.org>; Wed, 13 Nov 2013 21:21:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=JOgCQgd1QQkc9SNanpSAz32uJB0/mYCbdeSD3iEcssc=;
        b=k9VirLNMaNzNZ3jSvB2q8nkCTGG6HCenErsq2jCdREQpQsOB9A1p+JM7tVH1XI1X/z
         fgYtsWPDWA2q7XHoWYf8LhESA2m9xgMGGzl31/tAd3nHNfJtNFWX3ICfdaAqKyBilKy8
         B0UuKLpreqso1At9g/SGxGRMqJtRvFAbk1UDVXOc7HDjJEHT1HEQWEXmv5E3ARITrC/u
         7j+2KODbv2fxcVrrjlCFs4m/ROD8gk3+KT7wXLNTA8TLmVXyAGpnPWHLgaUoXrXcBXLM
         lCBno0cFyRHLi8OQpURtyxsFxMaPTmkgYsvhk/Km92C06h7ap3PAXAKRP0UuqKHeMh4y
         UNtA==
MIME-Version: 1.0
X-Received: by 10.224.51.131 with SMTP id d3mr74272046qag.0.1384406462173;
 Wed, 13 Nov 2013 21:21:02 -0800 (PST)
Received: by 10.224.186.202 with HTTP; Wed, 13 Nov 2013 21:21:01 -0800 (PST)
Received: by 10.224.186.202 with HTTP; Wed, 13 Nov 2013 21:21:01 -0800 (PST)
In-Reply-To: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
Date: Wed, 13 Nov 2013 21:21:01 -0800
Message-ID: <CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
From: karthik tunga <karthik.tunga@gmail.com>
To: dev <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0158c65c67181704eb1c3fae
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0158c65c67181704eb1c3fae
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Congrats Tom and Prashant :)

Cheers,
Karthik
On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:

> Hi folks,
>
> The Apache Spark PPMC is happy to welcome two new PPMC members and
> committers: Tom Graves and Prashant Sharma.
>
> Tom has been maintaining and expanding the YARN support in Spark over the
> past few months, including adding big features such as support for YARN
> security, and recently contributed a major patch that adds security to al=
l
> of Spark=E2=80=99s internal communication services as well (
> https://github.com/apache/incubator-spark/pull/120). It will be great to
> have him continue expanding these and other features as a committer.
>
> Prashant created and has maintained the Scala 2.10 branch of Spark for 6+
> months now, including tackling the hairy task of porting the Spark
> interpreter to 2.10, and debugging all the issues raised by that with
> third-party libraries. He's also contributed bug fixes and new input
> sources to Spark Streaming. The Scala 2.10 branch will be merged into
> master soon.
>
> We=E2=80=99re very excited to have both Tom and Prashant join the project=
 as
> committers.
>
> The Apache Spark PPMC
>
>

--089e0158c65c67181704eb1c3fae--

From dev-return-756-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 14 17:45:52 2013
Return-Path: <dev-return-756-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E3175101CF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Nov 2013 17:45:52 +0000 (UTC)
Received: (qmail 17148 invoked by uid 500); 14 Nov 2013 17:45:40 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 16960 invoked by uid 500); 14 Nov 2013 17:45:35 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 16851 invoked by uid 99); 14 Nov 2013 17:45:31 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 17:45:31 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.215.50 as permitted sender)
Received: from [209.85.215.50] (HELO mail-la0-f50.google.com) (209.85.215.50)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 17:45:25 +0000
Received: by mail-la0-f50.google.com with SMTP id el20so1848600lab.37
        for <dev@spark.incubator.apache.org>; Thu, 14 Nov 2013 09:45:04 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type:content-transfer-encoding;
        bh=WIuwE3KSx70C52vVwZR3NKkEv+zV9oWza6Ifefjz9+g=;
        b=sByL2tL4IBoWhLptypezUdRrdrjczVZeNiBgZSIzsU8aZ150mNJ8udIN83CUnq7d5z
         PvFY6sFwTLUGnJUx22DVY0jmtRmFG+UUWUHzX0Y6LEt6TDBwYvSjP8w525ZpuyqFn447
         1lHvXppHiLFjiWXze96d5ux0ct6Akpwot29QA=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type:content-transfer-encoding;
        bh=WIuwE3KSx70C52vVwZR3NKkEv+zV9oWza6Ifefjz9+g=;
        b=ivqhAEzbBNT3R9tr6k/beHHTo5SdmXQY/OYiCG8VaJuoc6xlWeiq95r/eDOmTYfRvE
         GYkB+sqCS9zYSjWeoMqLRJwvACvZw3nBTNySPqzYNBO9+Yc6qd9YW8HyBG/6Aaow8GA5
         IZLpnEr/W7ZvXGAe7S4scTDSVZWvP1yK2LCgutAQoNkhePpRhWjk0etcE8z0YMoAGzRh
         Y7bQxVSLOR3LrrAAULZ939JG9FCDSJ7JnSVzeRqyJrPcxB/4PfEMGuTAtgzn2MjrWiRn
         e6d28Jregihg92ivE7j4Uc8NEGd4WpgKa8IYzfM04wHdc08opccQJr2QF4Y1vqtmwhMs
         hwqg==
X-Gm-Message-State: ALoCoQm7z4wV1sl3KBej7cpO9tT6stGU5aw9xP7qkzMoJkbW0P006wVt2ZeVfXyo2PE0MoA6s+EC
MIME-Version: 1.0
X-Received: by 10.112.234.168 with SMTP id uf8mr1437462lbc.35.1384451103886;
 Thu, 14 Nov 2013 09:45:03 -0800 (PST)
Received: by 10.112.94.70 with HTTP; Thu, 14 Nov 2013 09:45:03 -0800 (PST)
In-Reply-To: <CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
	<CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
Date: Thu, 14 Nov 2013 09:45:03 -0800
Message-ID: <CADWPM3jDJ5WKb7smbHBvzv4=VAJKVTd1arjYK4qkYDCe9qyXuA@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable
X-Virus-Checked: Checked by ClamAV on apache.org

Congrats!

On Wed, Nov 13, 2013 at 9:21 PM, karthik tunga <karthik.tunga@gmail.com> wr=
ote:
> Congrats Tom and Prashant :)
>
> Cheers,
> Karthik
> On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:
>
>> Hi folks,
>>
>> The Apache Spark PPMC is happy to welcome two new PPMC members and
>> committers: Tom Graves and Prashant Sharma.
>>
>> Tom has been maintaining and expanding the YARN support in Spark over th=
e
>> past few months, including adding big features such as support for YARN
>> security, and recently contributed a major patch that adds security to a=
ll
>> of Spark=92s internal communication services as well (
>> https://github.com/apache/incubator-spark/pull/120). It will be great to
>> have him continue expanding these and other features as a committer.
>>
>> Prashant created and has maintained the Scala 2.10 branch of Spark for 6=
+
>> months now, including tackling the hairy task of porting the Spark
>> interpreter to 2.10, and debugging all the issues raised by that with
>> third-party libraries. He's also contributed bug fixes and new input
>> sources to Spark Streaming. The Scala 2.10 branch will be merged into
>> master soon.
>>
>> We=92re very excited to have both Tom and Prashant join the project as
>> committers.
>>
>> The Apache Spark PPMC
>>
>>



--=20
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

From dev-return-757-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 14 17:48:40 2013
Return-Path: <dev-return-757-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id D6D36101F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Nov 2013 17:48:40 +0000 (UTC)
Received: (qmail 34233 invoked by uid 500); 14 Nov 2013 17:48:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 34138 invoked by uid 500); 14 Nov 2013 17:48:33 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 33754 invoked by uid 99); 14 Nov 2013 17:48:32 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 17:48:32 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.217.179 as permitted sender)
Received: from [209.85.217.179] (HELO mail-lb0-f179.google.com) (209.85.217.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 17:48:28 +0000
Received: by mail-lb0-f179.google.com with SMTP id w6so1864200lbh.10
        for <dev@spark.incubator.apache.org>; Thu, 14 Nov 2013 09:48:07 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=kt5AfznpAD4fhI0VheOh3YbO5QKI58J2ubd6zBAnk8M=;
        b=Ajq57PN/k6El+RTlRW3e+3O/0sxoc1p5QNi7fbxfbD27ILmMEZ2GcWMUGElipXbhR1
         uy+VW3ghcab8YKznLKI0+NGyKcchyHBdSsvAwow52C7caGaPpqrXP192LSaK+dPHBJfI
         slWnQT+izqa4TLxfYXcPCbkxyYrnMrp6kf+1g=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=kt5AfznpAD4fhI0VheOh3YbO5QKI58J2ubd6zBAnk8M=;
        b=I5NlQ/WJBY4P+Q1E2ich+nSFA3t7jC90vvVhl6CvQSDFh7/g7Iv4DdsQtLR68lFnRM
         VERJ0h3m1yZvnj/uWkNB+XlrUMBFRYrbAvdFJDa2VhEkXeqLj2+kEo4BhKjt9s6Z8y24
         8SCuStR0EVnCEN4b+Dj6A9xlGoEiGdTpj8neU+ii618je13UleZh9y3b1ikm0PbTel2M
         JBsWhj1BZOPaRBraVazUIN5LFfkISm5bAHvo1LHg+HytagKgywtuhRvP1i3bAOIJSaCe
         SuzYsvxbYqEANB1Xhjo3siJgJMMTYWCIHL6MBbwphtCj9Wj0a3AhPIMdzjesUFNXOncj
         PMXg==
X-Gm-Message-State: ALoCoQnUqZb0udXvEBa8hXi5sseC7FnyhaElzhkmy4jBSsPyOQQSlujobMS97dDQA6+cUMYnum3v
MIME-Version: 1.0
X-Received: by 10.112.131.4 with SMTP id oi4mr397451lbb.88.1384451287420; Thu,
 14 Nov 2013 09:48:07 -0800 (PST)
Received: by 10.112.94.70 with HTTP; Thu, 14 Nov 2013 09:48:07 -0800 (PST)
In-Reply-To: <20131112130726.30ffa327@sh9>
References: <CAKXMip3ayETWL1Pwbx9Mz_xocz=pF+bSytR1gDi4o2vfjhA3Og@mail.gmail.com>
	<CAC1ssC5B0eqW06tLE02tAkKRGvPfP2T5DWsrWhDiN1UHsr=ZHA@mail.gmail.com>
	<CAKXMip3TvN_zuevmxrFsk8Y=2Q398sXuqsbDRw75k0v4YbSy4g@mail.gmail.com>
	<7841A4E0A9C8784C9D8B07DF5E48F0A8116239E3@SHSMSX104.ccr.corp.intel.com>
	<CAKXMip2HgXsm_nK3q1M-T8f-cgiXGQiS=zGOtv9cPXqqSfVCzQ@mail.gmail.com>
	<20131112130726.30ffa327@sh9>
Date: Thu, 14 Nov 2013 09:48:07 -0800
Message-ID: <CADWPM3irxS5YE6w8AS1V=EHBPL1fY-FWhrtPe9S4UKeMuYtbVQ@mail.gmail.com>
Subject: Re: SPARK-942
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

+1 for IteratorWithSizeEstimate.

I believe today only HadoopRDDs are able to give fine grained
progress;  with an enhanced iterator interface (which can still expose
the base Iterator trait) we can extend the possibility of fine grained
progress to all RDDs that implement the enhanced iterator.

On Tue, Nov 12, 2013 at 11:07 AM, Stephen Haberman
<stephen.haberman@gmail.com> wrote:
>
>> The problem is that the iterator interface only defines 'hasNext' and
>> 'next' methods.
>
> Just a comment from the peanut gallery, but FWIW it seems like being
> able to ask "how much data is here" would be a useful thing for Spark
> to know, even if that means moving away from Iterator itself, or
> something like IteratorWithSizeEstimate/something/something.
>
> Not only for this, but so that, ideally, Spark could basically do
> dynamic partitioning.
>
> E.g. when we load a month's worth of data, it's X GB, but after a few
> maps and filters, it's X/100 GB, so could use X/100 partitions instead.
>
> But right now all partitioning decisions are made up-front,
> via .coalesce/etc. type hints from the programmer, and it seems if
> Spark could delay making partitioning decisions each until RDD could
> like lazily-eval/sample a few lines (hand waving), that would be super
> sexy from our respective, in terms of doing automatic perf/partition
> optimization.
>
> Huge disclaimer that this is probably a big pita to implement, and
> could likely not be as worthwhile as I naively think it would be.
>
> - Stephen



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

From dev-return-758-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 14 17:58:01 2013
Return-Path: <dev-return-758-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1FFA2102B9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 14 Nov 2013 17:58:01 +0000 (UTC)
Received: (qmail 72957 invoked by uid 500); 14 Nov 2013 17:57:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 72789 invoked by uid 500); 14 Nov 2013 17:57:57 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 72777 invoked by uid 99); 14 Nov 2013 17:57:56 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 17:57:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of mkolod@gmail.com designates 209.85.214.177 as permitted sender)
Received: from [209.85.214.177] (HELO mail-ob0-f177.google.com) (209.85.214.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 14 Nov 2013 17:57:52 +0000
Received: by mail-ob0-f177.google.com with SMTP id wp4so2559318obc.36
        for <dev@spark.incubator.apache.org>; Thu, 14 Nov 2013 09:57:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=kN/bUN9oHPwo5cMVrX8/po5eph8a89/obBf55Q1Wk2E=;
        b=KRJKuvIYQaFefoItGhkgOxl2DwUBq1rmyX2Djb/y7wZHrCTHDzxBptYmCQIFHQWHDJ
         r5kzp07JAgjHKk8MBLwazHd9QBn8IpHyHcIheCeH1MbZNA6hBtGaTQu5NCHn9KaaZ8W1
         jUDFKJphBbsaNqet9pPSfTRDvoT43s/b7akRrK1wGSx65cgC3TWWCmvWosqBaElJwq8J
         P3CkeD3SciV5TxuNYQkES8gF2sqyUtVTkvCZHH1BnnyPYP3EiC0IQnAg+HXSWPQR9rlt
         /QIj1ghI++I/XnydCacZaAl3GL4SJ+wzh5LAx1KhLaXnrCg53EddJdrF8G53Ebzc14K3
         wvrw==
MIME-Version: 1.0
X-Received: by 10.182.71.82 with SMTP id s18mr2764164obu.9.1384451851359; Thu,
 14 Nov 2013 09:57:31 -0800 (PST)
Received: by 10.76.85.133 with HTTP; Thu, 14 Nov 2013 09:57:31 -0800 (PST)
In-Reply-To: <CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
	<CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
Date: Thu, 14 Nov 2013 12:57:31 -0500
Message-ID: <CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
From: Marek Kolodziej <mkolod@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=e89a8fb1fde6cf1db704eb26d0d3
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8fb1fde6cf1db704eb26d0d3
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

Congratulations Tom and Prashant!

By the way, Prashant co-authored a very nice intro book about SBT, it was
just published by Packt in September.

Marek




On Thu, Nov 14, 2013 at 12:21 AM, karthik tunga <karthik.tunga@gmail.com>wr=
ote:

> Congrats Tom and Prashant :)
>
> Cheers,
> Karthik
> On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com> wrote:
>
> > Hi folks,
> >
> > The Apache Spark PPMC is happy to welcome two new PPMC members and
> > committers: Tom Graves and Prashant Sharma.
> >
> > Tom has been maintaining and expanding the YARN support in Spark over t=
he
> > past few months, including adding big features such as support for YARN
> > security, and recently contributed a major patch that adds security to
> all
> > of Spark=92s internal communication services as well (
> > https://github.com/apache/incubator-spark/pull/120). It will be great t=
o
> > have him continue expanding these and other features as a committer.
> >
> > Prashant created and has maintained the Scala 2.10 branch of Spark for =
6+
> > months now, including tackling the hairy task of porting the Spark
> > interpreter to 2.10, and debugging all the issues raised by that with
> > third-party libraries. He's also contributed bug fixes and new input
> > sources to Spark Streaming. The Scala 2.10 branch will be merged into
> > master soon.
> >
> > We=92re very excited to have both Tom and Prashant join the project as
> > committers.
> >
> > The Apache Spark PPMC
> >
> >
>

--e89a8fb1fde6cf1db704eb26d0d3--

From dev-return-759-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 15 00:50:13 2013
Return-Path: <dev-return-759-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5B3FE104BE
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Nov 2013 00:50:13 +0000 (UTC)
Received: (qmail 29850 invoked by uid 500); 15 Nov 2013 00:50:13 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 29764 invoked by uid 500); 15 Nov 2013 00:50:13 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 29756 invoked by uid 99); 15 Nov 2013 00:50:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 00:50:12 +0000
X-ASF-Spam-Status: No, hits=2.5 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of andykonwinski@gmail.com designates 209.85.215.45 as permitted sender)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 00:50:06 +0000
Received: by mail-la0-f45.google.com with SMTP id eh20so2248789lab.18
        for <dev@spark.incubator.apache.org>; Thu, 14 Nov 2013 16:49:46 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=QxGZmuEhBXuwDwS6FcYDo5EwnS40Kx4OYmVcd0qrJ3w=;
        b=wkjfKjyhP8o66O2y68NZFxq6XljQxYD5KlbPrMZysTvaJ1AXLDV5mJM9AY0204U1Of
         qMWMRZFe7pjQim6gLRMQBJvHVTCnhR5ZW2MOuWRPrMiN4QeOGIVRMMrxZFV16gn0qIUc
         Dwt3XDqsMVlPOgbS2qC2OUxISH1PD2Llf/d1gkdHv5dpNDPevI0x7ggS1jr8jWowDeVz
         rnh3GPvfKqNlr0PYbht2slTkASaO6pGhwdfIcXnE/uEojOx1QPyEtICto9NEqiHzKP5f
         oo8163GSchF5uWmRs3S3ty7OhD1InZrR4B36hAmpwHYSgavWjbnvr2ZzJPfGt4C+s9Qq
         EOtA==
MIME-Version: 1.0
X-Received: by 10.152.4.230 with SMTP id n6mr2298621lan.1.1384476585851; Thu,
 14 Nov 2013 16:49:45 -0800 (PST)
Received: by 10.112.16.6 with HTTP; Thu, 14 Nov 2013 16:49:45 -0800 (PST)
In-Reply-To: <CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
	<CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
	<CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>
Date: Thu, 14 Nov 2013 16:49:45 -0800
Message-ID: <CALEZFQy+F_QAn=aTv7uy8DQVk+xgSG-9vX8N01YVD2FOTNyYEg@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
From: Andy Konwinski <andykonwinski@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e013d1e8e1983e504eb2c9398
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013d1e8e1983e504eb2c9398
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

Congrats and welcome Tom and Prashant!


On Thu, Nov 14, 2013 at 9:57 AM, Marek Kolodziej <mkolod@gmail.com> wrote:

> Congratulations Tom and Prashant!
>
> By the way, Prashant co-authored a very nice intro book about SBT, it was
> just published by Packt in September.
>
> Marek
>
>
>
>
> On Thu, Nov 14, 2013 at 12:21 AM, karthik tunga <karthik.tunga@gmail.com
> >wrote:
>
> > Congrats Tom and Prashant :)
> >
> > Cheers,
> > Karthik
> > On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com>
> wrote:
> >
> > > Hi folks,
> > >
> > > The Apache Spark PPMC is happy to welcome two new PPMC members and
> > > committers: Tom Graves and Prashant Sharma.
> > >
> > > Tom has been maintaining and expanding the YARN support in Spark over
> the
> > > past few months, including adding big features such as support for YA=
RN
> > > security, and recently contributed a major patch that adds security t=
o
> > all
> > > of Spark=92s internal communication services as well (
> > > https://github.com/apache/incubator-spark/pull/120). It will be great
> to
> > > have him continue expanding these and other features as a committer.
> > >
> > > Prashant created and has maintained the Scala 2.10 branch of Spark fo=
r
> 6+
> > > months now, including tackling the hairy task of porting the Spark
> > > interpreter to 2.10, and debugging all the issues raised by that with
> > > third-party libraries. He's also contributed bug fixes and new input
> > > sources to Spark Streaming. The Scala 2.10 branch will be merged into
> > > master soon.
> > >
> > > We=92re very excited to have both Tom and Prashant join the project a=
s
> > > committers.
> > >
> > > The Apache Spark PPMC
> > >
> > >
> >
>

--089e013d1e8e1983e504eb2c9398--

From dev-return-760-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 15 06:14:55 2013
Return-Path: <dev-return-760-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DD58410B88
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Nov 2013 06:14:55 +0000 (UTC)
Received: (qmail 3703 invoked by uid 500); 15 Nov 2013 06:14:45 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 3654 invoked by uid 500); 15 Nov 2013 06:14:38 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 3645 invoked by uid 99); 15 Nov 2013 06:14:36 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 06:14:36 +0000
X-ASF-Spam-Status: No, hits=2.5 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of scrapcodes@gmail.com designates 74.125.82.43 as permitted sender)
Received: from [74.125.82.43] (HELO mail-wg0-f43.google.com) (74.125.82.43)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 06:14:30 +0000
Received: by mail-wg0-f43.google.com with SMTP id n12so3132016wgh.22
        for <dev@spark.incubator.apache.org>; Thu, 14 Nov 2013 22:14:09 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=yDoXckYU7xy1YA/ZSOdiXFtv5KEaqaCsJ3WL5hPOKnw=;
        b=xjgwyOovnXBi1QLq+sFa+W3vdnRH4MJiVd2qzwmb72V16zP/YjqpzoE6wi/g//FU2/
         B7y1mPopanUHrm00qC/aA2T3+wzy1e0trRrqEK+ihKvbFWAz8hgRYFDEYHG4rhKWwgRh
         uinXUNA1yStABWgEQLCLNzeYd+mqoMhwL1noWplljkoSiVQIzRmTa+HUckqqOqIRT2ad
         XIrwiDc+F4vKF2XS35I+VOxet1w9hgR725ZUpff4tFoAhsCg088eCxsrkcgp3GNDXE8I
         +jmCf0myFgT1rpYahTyYLlsF6zH1TPxEFFoSaoiFn3xNywudPxGO9hoggzKPU2f1Avs0
         w/2g==
X-Received: by 10.194.11.38 with SMTP id n6mr5389838wjb.25.1384496049543; Thu,
 14 Nov 2013 22:14:09 -0800 (PST)
MIME-Version: 1.0
Received: by 10.216.64.132 with HTTP; Thu, 14 Nov 2013 22:13:49 -0800 (PST)
In-Reply-To: <CALEZFQy+F_QAn=aTv7uy8DQVk+xgSG-9vX8N01YVD2FOTNyYEg@mail.gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
 <CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
 <CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com> <CALEZFQy+F_QAn=aTv7uy8DQVk+xgSG-9vX8N01YVD2FOTNyYEg@mail.gmail.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Fri, 15 Nov 2013 11:43:49 +0530
Message-ID: <CAOYDGoBKD31N2gO6vXy4Nsya=V+bNBPGBmR+KXncjOQ7=W1VUw@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b5d571039e0d604eb311be7
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b5d571039e0d604eb311be7
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

Congratulations Tom, and thanks a lot everyone and especially all the
voters.


On Fri, Nov 15, 2013 at 6:19 AM, Andy Konwinski <andykonwinski@gmail.com>wr=
ote:

> Congrats and welcome Tom and Prashant!
>
>
> On Thu, Nov 14, 2013 at 9:57 AM, Marek Kolodziej <mkolod@gmail.com> wrote=
:
>
> > Congratulations Tom and Prashant!
> >
> > By the way, Prashant co-authored a very nice intro book about SBT, it w=
as
> > just published by Packt in September.
> >
> > Marek
> >
> >
> >
> >
> > On Thu, Nov 14, 2013 at 12:21 AM, karthik tunga <karthik.tunga@gmail.co=
m
> > >wrote:
> >
> > > Congrats Tom and Prashant :)
> > >
> > > Cheers,
> > > Karthik
> > > On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com>
> > wrote:
> > >
> > > > Hi folks,
> > > >
> > > > The Apache Spark PPMC is happy to welcome two new PPMC members and
> > > > committers: Tom Graves and Prashant Sharma.
> > > >
> > > > Tom has been maintaining and expanding the YARN support in Spark ov=
er
> > the
> > > > past few months, including adding big features such as support for
> YARN
> > > > security, and recently contributed a major patch that adds security
> to
> > > all
> > > > of Spark=92s internal communication services as well (
> > > > https://github.com/apache/incubator-spark/pull/120). It will be
> great
> > to
> > > > have him continue expanding these and other features as a committer=
.
> > > >
> > > > Prashant created and has maintained the Scala 2.10 branch of Spark
> for
> > 6+
> > > > months now, including tackling the hairy task of porting the Spark
> > > > interpreter to 2.10, and debugging all the issues raised by that wi=
th
> > > > third-party libraries. He's also contributed bug fixes and new inpu=
t
> > > > sources to Spark Streaming. The Scala 2.10 branch will be merged in=
to
> > > > master soon.
> > > >
> > > > We=92re very excited to have both Tom and Prashant join the project=
 as
> > > > committers.
> > > >
> > > > The Apache Spark PPMC
> > > >
> > > >
> > >
> >
>



--=20
s

--047d7b5d571039e0d604eb311be7--

From dev-return-761-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 15 06:16:55 2013
Return-Path: <dev-return-761-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id EA89610B96
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Nov 2013 06:16:55 +0000 (UTC)
Received: (qmail 7902 invoked by uid 500); 15 Nov 2013 06:16:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7871 invoked by uid 500); 15 Nov 2013 06:16:48 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 7860 invoked by uid 99); 15 Nov 2013 06:16:45 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 06:16:45 +0000
X-ASF-Spam-Status: No, hits=2.5 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of scrapcodes@gmail.com designates 74.125.82.49 as permitted sender)
Received: from [74.125.82.49] (HELO mail-wg0-f49.google.com) (74.125.82.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 06:16:40 +0000
Received: by mail-wg0-f49.google.com with SMTP id x13so3044817wgg.28
        for <dev@spark.incubator.apache.org>; Thu, 14 Nov 2013 22:16:19 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :cc:content-type;
        bh=C0D0cPf2Fsy796tWqENvQOOkKlTaDULeiBxpVVpRhrs=;
        b=xuOGWZXdgcd8bCjcK478tdx0u+U4e9uQngJgTzfkhq/fSEOYuPPvV5eTyKdAu12L/n
         kY2aiMuZKbH1bxJWK2InPJw2ll8aver/DihWJ4lVHpU422f0MfPzncwoTMGfIcY2pwFu
         NBJwX9u2m6LGfJYwZgvnKXRQy44UxzhQqccPE8qfMoBMuHb354JsTwJRivMSf5lNmVn7
         5iZ/dFCaPEsiIdbwr9oY9ZeC13R7slmBLtpvaACDRb7xj5dfOiLm+DMEt4I4dOrYmWiG
         W6cpvlyH7IYs3ly+HAY2u9BBV/JF5R8UDCaqICNVv9nXmSV7Dw8rA+RlWFhTZJxmVnmY
         pgIw==
X-Received: by 10.194.84.72 with SMTP id w8mr63676wjy.55.1384496179569; Thu,
 14 Nov 2013 22:16:19 -0800 (PST)
MIME-Version: 1.0
Received: by 10.216.64.132 with HTTP; Thu, 14 Nov 2013 22:15:59 -0800 (PST)
In-Reply-To: <CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
 <CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com> <CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>
From: Prashant Sharma <scrapcodes@gmail.com>
Date: Fri, 15 Nov 2013 11:45:59 +0530
Message-ID: <CAOYDGoBd+JSmY6+e-BFxKPL6Nh0oGmCJb+9MbD=w6v9=5kXTDA@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
To: dev@spark.incubator.apache.org
Cc: mkolod@gmail.com
Content-Type: multipart/alternative; boundary=047d7bea40baf9e98a04eb312243
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bea40baf9e98a04eb312243
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

Hey Marek,

I did not co-auther that book, I was just one of the reviewers.

Thanks


On Thu, Nov 14, 2013 at 11:27 PM, Marek Kolodziej <mkolod@gmail.com> wrote:

> Congratulations Tom and Prashant!
>
> By the way, Prashant co-authored a very nice intro book about SBT, it was
> just published by Packt in September.
>
> Marek
>
>
>
>
> On Thu, Nov 14, 2013 at 12:21 AM, karthik tunga <karthik.tunga@gmail.com
> >wrote:
>
> > Congrats Tom and Prashant :)
> >
> > Cheers,
> > Karthik
> > On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com>
> wrote:
> >
> > > Hi folks,
> > >
> > > The Apache Spark PPMC is happy to welcome two new PPMC members and
> > > committers: Tom Graves and Prashant Sharma.
> > >
> > > Tom has been maintaining and expanding the YARN support in Spark over
> the
> > > past few months, including adding big features such as support for YA=
RN
> > > security, and recently contributed a major patch that adds security t=
o
> > all
> > > of Spark=92s internal communication services as well (
> > > https://github.com/apache/incubator-spark/pull/120). It will be great
> to
> > > have him continue expanding these and other features as a committer.
> > >
> > > Prashant created and has maintained the Scala 2.10 branch of Spark fo=
r
> 6+
> > > months now, including tackling the hairy task of porting the Spark
> > > interpreter to 2.10, and debugging all the issues raised by that with
> > > third-party libraries. He's also contributed bug fixes and new input
> > > sources to Spark Streaming. The Scala 2.10 branch will be merged into
> > > master soon.
> > >
> > > We=92re very excited to have both Tom and Prashant join the project a=
s
> > > committers.
> > >
> > > The Apache Spark PPMC
> > >
> > >
> >
>



--=20
s

--047d7bea40baf9e98a04eb312243--

From dev-return-762-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 15 10:47:42 2013
Return-Path: <dev-return-762-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5EC9510189
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Nov 2013 10:47:42 +0000 (UTC)
Received: (qmail 86189 invoked by uid 500); 15 Nov 2013 10:47:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 85949 invoked by uid 500); 15 Nov 2013 10:47:40 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 85941 invoked by uid 99); 15 Nov 2013 10:47:39 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 10:47:39 +0000
X-ASF-Spam-Status: No, hits=2.5 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of mkolod@gmail.com designates 209.85.219.49 as permitted sender)
Received: from [209.85.219.49] (HELO mail-oa0-f49.google.com) (209.85.219.49)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 10:47:33 +0000
Received: by mail-oa0-f49.google.com with SMTP id h16so3695178oag.8
        for <dev@spark.incubator.apache.org>; Fri, 15 Nov 2013 02:47:13 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Mj84F8JmrAouD0gPK+EwoQBnCa+6PiwMouzeTw1kovM=;
        b=ZCF9ejLR5VsgTmxo/kF2Buj0BPiOzLImRPnCvaWauvuLQOnMIJxO6BKFQCBdD/e5Wa
         J4QXihN6HrVWT7+6NIR1UmRJfYz/j8Om65tGbWPcFKnQVB/EtA1/21AfT/waUYVdy9wP
         GemKJ1CmR9nWldrxvT4CdG3haJuWbu51B5XPnbmSnHlOkfXJW045tcm7JAVV4wjCjmXa
         aLAYbwCM8UT1Qn2fm/Il864dxRo+d6LyviPvLYOraoSJ6P2o7I8zsvywAqruls5092EQ
         k94ja+L0rkXCqNbItOzE0zU6zXhTk/5XIxiNenlFrnzcglFi45bmt8fbN74Ms79cpKNE
         GY/w==
MIME-Version: 1.0
X-Received: by 10.60.70.134 with SMTP id m6mr6240215oeu.14.1384512432893; Fri,
 15 Nov 2013 02:47:12 -0800 (PST)
Received: by 10.76.85.133 with HTTP; Fri, 15 Nov 2013 02:47:12 -0800 (PST)
In-Reply-To: <CAOYDGoBd+JSmY6+e-BFxKPL6Nh0oGmCJb+9MbD=w6v9=5kXTDA@mail.gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
	<CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
	<CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>
	<CAOYDGoBd+JSmY6+e-BFxKPL6Nh0oGmCJb+9MbD=w6v9=5kXTDA@mail.gmail.com>
Date: Fri, 15 Nov 2013 05:47:12 -0500
Message-ID: <CANBKJfoEkmuTa9-+NgrJCS5NQWvKi3ytR0m3V-ASvnHFdqHz0A@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
From: Marek Kolodziej <mkolod@gmail.com>
To: Prashant Sharma <scrapcodes@gmail.com>
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a1133458ebff27d04eb34eba9
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133458ebff27d04eb34eba9
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

Oh sorry, I misspoke. Indeed, the author was Shiti Saxena. However,
reviewers often make such significant contributions to the final version of
a publication that one could treat them as co-authors. At least that's how
I viewed my reviewers when publishing peer-reviewed papers.

Marek


On Fri, Nov 15, 2013 at 1:15 AM, Prashant Sharma <scrapcodes@gmail.com>wrot=
e:

> Hey Marek,
>
> I did not co-auther that book, I was just one of the reviewers.
>
> Thanks
>
>
> On Thu, Nov 14, 2013 at 11:27 PM, Marek Kolodziej <mkolod@gmail.com>wrote=
:
>
>> Congratulations Tom and Prashant!
>>
>> By the way, Prashant co-authored a very nice intro book about SBT, it wa=
s
>> just published by Packt in September.
>>
>> Marek
>>
>>
>>
>>
>> On Thu, Nov 14, 2013 at 12:21 AM, karthik tunga <karthik.tunga@gmail.com
>> >wrote:
>>
>> > Congrats Tom and Prashant :)
>> >
>> > Cheers,
>> > Karthik
>> > On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com>
>> wrote:
>> >
>> > > Hi folks,
>> > >
>> > > The Apache Spark PPMC is happy to welcome two new PPMC members and
>> > > committers: Tom Graves and Prashant Sharma.
>> > >
>> > > Tom has been maintaining and expanding the YARN support in Spark ove=
r
>> the
>> > > past few months, including adding big features such as support for
>> YARN
>> > > security, and recently contributed a major patch that adds security =
to
>> > all
>> > > of Spark=92s internal communication services as well (
>> > > https://github.com/apache/incubator-spark/pull/120). It will be
>> great to
>> > > have him continue expanding these and other features as a committer.
>> > >
>> > > Prashant created and has maintained the Scala 2.10 branch of Spark
>> for 6+
>> > > months now, including tackling the hairy task of porting the Spark
>> > > interpreter to 2.10, and debugging all the issues raised by that wit=
h
>> > > third-party libraries. He's also contributed bug fixes and new input
>> > > sources to Spark Streaming. The Scala 2.10 branch will be merged int=
o
>> > > master soon.
>> > >
>> > > We=92re very excited to have both Tom and Prashant join the project =
as
>> > > committers.
>> > >
>> > > The Apache Spark PPMC
>> > >
>> > >
>> >
>>
>
>
>
> --
> s
>

--001a1133458ebff27d04eb34eba9--

From dev-return-763-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 15 14:25:38 2013
Return-Path: <dev-return-763-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B8972107DF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Nov 2013 14:25:38 +0000 (UTC)
Received: (qmail 18698 invoked by uid 500); 15 Nov 2013 14:25:37 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 18612 invoked by uid 500); 15 Nov 2013 14:25:35 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 18603 invoked by uid 99); 15 Nov 2013 14:25:34 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 14:25:34 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.212.179] (HELO mail-wi0-f179.google.com) (209.85.212.179)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 14:25:27 +0000
Received: by mail-wi0-f179.google.com with SMTP id fb10so1059593wid.0
        for <dev@spark.incubator.apache.org>; Fri, 15 Nov 2013 06:25:07 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:from:date
         :message-id:subject:to:cc:content-type;
        bh=/rNAVOIKv2tJGapl1sNMh4ASyZmME4T2z6l+Fp5X57Y=;
        b=A1DKFmOmyvT83vntkCRfg7iErM2F9kZK28gaV1scABO7hanZN9h+ohStLRDphDyS9J
         Tc/jAGc7owIOyo1o47uOtdjeJ0MiFGlNA/tBQ25Ifonu0PjMXH5Fh5CfqYJzZH5d/FGE
         RFcXMPksCt+BXNtTNLE0xEigRKqCkmcJ8qg7uzEJ6FPm9stxuEt1K5RFcozdiH54DoSQ
         xcZWnH5aVVv7nEiUl+pLfHGCT9S4nt0v8WJpVwYrfDJnocLnosTs0UgdTVqhzG/RzRV+
         koI2f3EmF0XG+HHf6kiRTp1xqI2+cZWRzcrwVQSEU9E9nTq1pWrTSbk9MZSD/HZcdYJI
         L9Qw==
X-Gm-Message-State: ALoCoQkLRvxb0vEMApjU8osqtujYP1kCZX2dwuIUrKVqMwKHGjmxDZVTxT2HTZRW1TU/sNLHtUn+
X-Received: by 10.194.78.141 with SMTP id b13mr7015628wjx.32.1384525507504;
 Fri, 15 Nov 2013 06:25:07 -0800 (PST)
MIME-Version: 1.0
Received: by 10.227.102.9 with HTTP; Fri, 15 Nov 2013 06:24:47 -0800 (PST)
X-Originating-IP: [49.206.0.67]
In-Reply-To: <CANBKJfoEkmuTa9-+NgrJCS5NQWvKi3ytR0m3V-ASvnHFdqHz0A@mail.gmail.com>
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>
 <CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>
 <CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>
 <CAOYDGoBd+JSmY6+e-BFxKPL6Nh0oGmCJb+9MbD=w6v9=5kXTDA@mail.gmail.com> <CANBKJfoEkmuTa9-+NgrJCS5NQWvKi3ytR0m3V-ASvnHFdqHz0A@mail.gmail.com>
From: Rohit Rai <rohit@tuplejump.com>
Date: Fri, 15 Nov 2013 19:54:47 +0530
Message-ID: <CAFRXrPq++EJBYgAhLn_r4cLw+F-B2yhtdCOR7-7=eWa6aXzjiQ@mail.gmail.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and
 Prashant Sharma
To: dev@spark.incubator.apache.org
Cc: Prashant Sharma <scrapcodes@gmail.com>
Content-Type: multipart/alternative; boundary=047d7bfcfc820ec68604eb37f7d9
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bfcfc820ec68604eb37f7d9
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

Congratulations Tom and Prashant! :)

*Founder & CEO, **Tuplejump, Inc.*
____________________________
www.tuplejump.com
*The Data Engineering Platform*


On Fri, Nov 15, 2013 at 4:17 PM, Marek Kolodziej <mkolod@gmail.com> wrote:

> Oh sorry, I misspoke. Indeed, the author was Shiti Saxena. However,
> reviewers often make such significant contributions to the final version =
of
> a publication that one could treat them as co-authors. At least that's ho=
w
> I viewed my reviewers when publishing peer-reviewed papers.
>
> Marek
>
>
> On Fri, Nov 15, 2013 at 1:15 AM, Prashant Sharma <scrapcodes@gmail.com
> >wrote:
>
> > Hey Marek,
> >
> > I did not co-auther that book, I was just one of the reviewers.
> >
> > Thanks
> >
> >
> > On Thu, Nov 14, 2013 at 11:27 PM, Marek Kolodziej <mkolod@gmail.com
> >wrote:
> >
> >> Congratulations Tom and Prashant!
> >>
> >> By the way, Prashant co-authored a very nice intro book about SBT, it
> was
> >> just published by Packt in September.
> >>
> >> Marek
> >>
> >>
> >>
> >>
> >> On Thu, Nov 14, 2013 at 12:21 AM, karthik tunga <
> karthik.tunga@gmail.com
> >> >wrote:
> >>
> >> > Congrats Tom and Prashant :)
> >> >
> >> > Cheers,
> >> > Karthik
> >> > On Nov 13, 2013 7:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com>
> >> wrote:
> >> >
> >> > > Hi folks,
> >> > >
> >> > > The Apache Spark PPMC is happy to welcome two new PPMC members and
> >> > > committers: Tom Graves and Prashant Sharma.
> >> > >
> >> > > Tom has been maintaining and expanding the YARN support in Spark
> over
> >> the
> >> > > past few months, including adding big features such as support for
> >> YARN
> >> > > security, and recently contributed a major patch that adds securit=
y
> to
> >> > all
> >> > > of Spark=92s internal communication services as well (
> >> > > https://github.com/apache/incubator-spark/pull/120). It will be
> >> great to
> >> > > have him continue expanding these and other features as a committe=
r.
> >> > >
> >> > > Prashant created and has maintained the Scala 2.10 branch of Spark
> >> for 6+
> >> > > months now, including tackling the hairy task of porting the Spark
> >> > > interpreter to 2.10, and debugging all the issues raised by that
> with
> >> > > third-party libraries. He's also contributed bug fixes and new inp=
ut
> >> > > sources to Spark Streaming. The Scala 2.10 branch will be merged
> into
> >> > > master soon.
> >> > >
> >> > > We=92re very excited to have both Tom and Prashant join the projec=
t as
> >> > > committers.
> >> > >
> >> > > The Apache Spark PPMC
> >> > >
> >> > >
> >> >
> >>
> >
> >
> >
> > --
> > s
> >
>

--047d7bfcfc820ec68604eb37f7d9--

From dev-return-764-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 15 19:57:56 2013
Return-Path: <dev-return-764-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A562910610
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 15 Nov 2013 19:57:56 +0000 (UTC)
Received: (qmail 26547 invoked by uid 500); 15 Nov 2013 19:57:56 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 26472 invoked by uid 500); 15 Nov 2013 19:57:56 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 26464 invoked by uid 99); 15 Nov 2013 19:57:55 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 19:57:55 +0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy)
Received: from [98.139.212.163] (HELO nm4.bullet.mail.bf1.yahoo.com) (98.139.212.163)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 15 Nov 2013 19:57:49 +0000
Received: from [98.139.212.152] by nm4.bullet.mail.bf1.yahoo.com with NNFMP; 15 Nov 2013 19:57:28 -0000
Received: from [98.139.212.213] by tm9.bullet.mail.bf1.yahoo.com with NNFMP; 15 Nov 2013 19:57:28 -0000
Received: from [127.0.0.1] by omp1022.mail.bf1.yahoo.com with NNFMP; 15 Nov 2013 19:57:28 -0000
X-Yahoo-Newman-Property: ymail-3
X-Yahoo-Newman-Id: 652912.79883.bm@omp1022.mail.bf1.yahoo.com
Received: (qmail 14573 invoked by uid 60001); 15 Nov 2013 19:57:28 -0000
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed; d=yahoo.com; s=s1024; t=1384545448; bh=U+MA2+w6yUH55xAJSrYtAbIl8jWX4NNehd0XAGev0Go=; h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type; b=CllL/qCWjbSbxbfjHgvKC+3HgdERYql8aAHXG/a66+DeZZvNxQMj9FnTOUHgoYKRBLQmFg2iTO2t96FQ5CwDtAgQk7un/S4yqIXf96uGAiMdkLhcB00OZjZJYsWC42U78iJKvIUPquX8l07eceQk5Fji0DAVXFwmf6O8FHc31ao=
DomainKey-Signature:a=rsa-sha1; q=dns; c=nofws;
  s=s1024; d=yahoo.com;
  h=X-YMail-OSG:Received:X-Rocket-MIMEInfo:X-Mailer:References:Message-ID:Date:From:Reply-To:Subject:To:In-Reply-To:MIME-Version:Content-Type;
  b=Dsbrsl9pmY5Vnazx8f0ddNVGWgoixciKpWp7DyY6ZNbL6bEKyZjn7auMqxZpMpyUy9fZaacfCohYtVakCH05DWdmgw/Xu5Cn8ZCYIqJTT/ELw4G5ApRAETX3i8OX/i96zSTv8/Yc60eyO9Wp5T/Hd+CduI9W0qc3/i/NDYtGmQE=;
X-YMail-OSG: E2IuDQEVM1k2rTc5jpEEvWfw_sRnWiyaXlDWBdU5FqCmg6V
 Z6h6tOsoxg63LqtiW7AidwW4F1jAwJ1P6b8S8Ku_F2lLh_6lIme2xEvWsqAM
 yjPY0txn9hW4ZGSe.pOqMQD34uVTCsSiC5s1yxMsdYkBleDbUDkod6L_hzpU
 3TLSp0ucrQyMiLTvD6.X.D.eVOQC.kXt0.w_Z1Rr2hmaYULYwnazx_PiA8ku
 D3DK9uzN5cLxbEy3E6ynSfIhGlG4CrAEzPwtOgUfERahqMfEQUqdT_1CBMuW
 ohm4xqeAhnZQoiW6eHtXeONdgamX9IfL5az7KczEXZPopNp_1sBGsXYYlwfg
 LyiUkR_f8mIZSvCNnHFjzahzN6kIa.R4jInDkNM6KRdHg4P2vTrEcoTIj3lw
 kWhPmy6sPr.i3DUMfB5_6MjSqAFjVI2BfgdMD49J3PEhhELe_yO37NnUnf0M
 8sLdcTPcNDeFhqmmKCpO3xeSnMbhHlLQl0d4SBTIM_gt07VNnkzT9GXe0iIf
 Tn4UL2iRTLzpwwRFLrGSS0kGhjcD.N7Vrcy0mQtRG3FWFl_S.tHtXF5MPLQK
 vZNiYbr8SXbNPm96FaTk1qMJWfTcOrJbu1SCeMHQtveK9ZTBFc3IlbWLBETe
 jubNFTxB8.XC7j6A83J5LKv28nd4s9pwxBfYTFPPbRWGy_JYsLcp4VsVfvKG
 HKcUFHZ9EYNOXNkM7lZPE
Received: from [204.11.79.50] by web140304.mail.bf1.yahoo.com via HTTP; Fri, 15 Nov 2013 11:57:27 PST
X-Rocket-MIMEInfo: 002.001,Q29uZ3JhdHMgUHJhc2hhbnQgYW5kIHRoYW5rcyBldmVyeW9uZS4KClRvbQoKCgpPbiBGcmlkYXksIE5vdmVtYmVyIDE1LCAyMDEzIDQ6NDcgQU0sIE1hcmVrIEtvbG9kemllaiA8bWtvbG9kQGdtYWlsLmNvbT4gd3JvdGU6CiAKT2ggc29ycnksIEkgbWlzc3Bva2UuIEluZGVlZCwgdGhlIGF1dGhvciB3YXMgU2hpdGkgU2F4ZW5hLiBIb3dldmVyLApyZXZpZXdlcnMgb2Z0ZW4gbWFrZSBzdWNoIHNpZ25pZmljYW50IGNvbnRyaWJ1dGlvbnMgdG8gdGhlIGZpbmFsIHZlcnNpb24gb2YKYSBwdWJsaWNhdGlvbiB0aGEBMAEBAQE-
X-Mailer: YahooMailWebService/0.8.163.597
References: <267E15A9-79C7-4E09-8BDF-E10670EF8A18@gmail.com>	<CAFNsrOf57DcR9nD0MuT=1vN5qEnGgDT99Q2eFvf=R+rc0eyL6Q@mail.gmail.com>	<CANBKJfqpZQGmH_emLWuDq3KP4KmXi6UJZE03vqLanfYp+RVSLw@mail.gmail.com>	<CAOYDGoBd+JSmY6+e-BFxKPL6Nh0oGmCJb+9MbD=w6v9=5kXTDA@mail.gmail.com> <CANBKJfoEkmuTa9-+NgrJCS5NQWvKi3ytR0m3V-ASvnHFdqHz0A@mail.gmail.com> 
Message-ID: <1384545447.14527.YahooMailNeo@web140304.mail.bf1.yahoo.com>
Date: Fri, 15 Nov 2013 11:57:27 -0800 (PST)
From: Tom Graves <tgraves_cs@yahoo.com>
Reply-To: Tom Graves <tgraves_cs@yahoo.com>
Subject: Re: [ANNOUNCE] Welcoming two new Spark committers: Tom Graves and Prashant Sharma
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>,
  Prashant Sharma <scrapcodes@gmail.com>
In-Reply-To: <CANBKJfoEkmuTa9-+NgrJCS5NQWvKi3ytR0m3V-ASvnHFdqHz0A@mail.gmail.com>
MIME-Version: 1.0
Content-Type: multipart/alternative; boundary="683208101-1402883154-1384545447=:14527"
X-Virus-Checked: Checked by ClamAV on apache.org

--683208101-1402883154-1384545447=:14527
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: quoted-printable

Congrats Prashant and thanks everyone.=0A=0ATom=0A=0A=0A=0AOn Friday, Novem=
ber 15, 2013 4:47 AM, Marek Kolodziej <mkolod@gmail.com> wrote:=0A =0AOh so=
rry, I misspoke. Indeed, the author was Shiti Saxena. However,=0Areviewers =
often make such significant contributions to the final version of=0Aa publi=
cation that one could treat them as co-authors. At least that's how=0AI vie=
wed my reviewers when publishing peer-reviewed papers.=0A=0AMarek=0A=0A=0A=
=0AOn Fri, Nov 15, 2013 at 1:15 AM, Prashant Sharma <scrapcodes@gmail.com>w=
rote:=0A=0A> Hey Marek,=0A>=0A> I did not co-auther that book, I was just o=
ne of the reviewers.=0A>=0A> Thanks=0A>=0A>=0A> On Thu, Nov 14, 2013 at 11:=
27 PM, Marek Kolodziej <mkolod@gmail.com>wrote:=0A>=0A>> Congratulations To=
m and Prashant!=0A>>=0A>> By the way, Prashant co-authored a very nice intr=
o book about SBT, it was=0A>> just published by Packt in September.=0A>>=0A=
>>=0A Marek=0A>>=0A>>=0A>>=0A>>=0A>> On Thu, Nov 14, 2013 at 12:21 AM, kart=
hik tunga <karthik.tunga@gmail.com=0A>> >wrote:=0A>>=0A>> > Congrats Tom an=
d Prashant :)=0A>> >=0A>> > Cheers,=0A>> > Karthik=0A>> > On Nov 13, 2013 7=
:37 PM, "Matei Zaharia" <matei.zaharia@gmail.com>=0A>> wrote:=0A>> >=0A>> >=
 > Hi folks,=0A>> > >=0A>> > > The Apache Spark PPMC is happy to welcome tw=
o new PPMC=0A members and=0A>> > > committers: Tom Graves and Prashant Shar=
ma.=0A>> > >=0A>> > > Tom has been maintaining and expanding the YARN suppo=
rt in Spark over=0A>> the=0A>> > > past few months, including adding big fe=
atures such as support for=0A>> YARN=0A>> > > security, and recently contri=
buted a major patch that adds security to=0A>> > all=0A>> > > of Spark=E2=
=80=99s internal communication services as well (=0A>> > > https://github.c=
om/apache/incubator-spark/pull/120). It will be=0A>> great to=0A>> > > have=
 him continue expanding these and other features as a committer.=0A>> > >=
=0A>> > > Prashant created and has maintained the Scala 2.10 branch of Spar=
k=0A>> for 6+=0A>> > > months now, including tackling the hairy task of por=
ting the Spark=0A>> > > interpreter to 2.10, and debugging all the issues r=
aised by that with=0A>> > > third-party libraries. He's also contributed bu=
g fixes and new input=0A>> > > sources to Spark Streaming. The Scala 2.10 b=
ranch will be merged into=0A>> > > master soon.=0A>> > >=0A>> > > We=E2=80=
=99re very excited to have both Tom and Prashant join the project as=0A>> >=
 > committers.=0A>> > >=0A>> > > The Apache Spark PPMC=0A>> > >=0A>> > >=0A=
>> >=0A>>=0A>=0A>=0A>=0A> --=0A> s=0A>
--683208101-1402883154-1384545447=:14527--

From dev-return-765-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov 18 07:25:44 2013
Return-Path: <dev-return-765-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id DDCD3102F9
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 18 Nov 2013 07:25:43 +0000 (UTC)
Received: (qmail 58727 invoked by uid 500); 18 Nov 2013 07:25:32 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 58157 invoked by uid 500); 18 Nov 2013 07:25:11 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 58071 invoked by uid 99); 18 Nov 2013 07:25:07 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Nov 2013 07:25:07 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of umarj.javed@gmail.com designates 209.85.212.51 as permitted sender)
Received: from [209.85.212.51] (HELO mail-vb0-f51.google.com) (209.85.212.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 18 Nov 2013 07:25:02 +0000
Received: by mail-vb0-f51.google.com with SMTP id w5so4450196vbf.10
        for <multiple recipients>; Sun, 17 Nov 2013 23:24:41 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=W6ykLd5jOBh0ivmCAHWVQS79aatMRHHJLv0YhpCWUHU=;
        b=Fb/UvWOsc2bQts281is/Cvw4xICmnm/Guus60L7VShvMzzYN1dBFdpeMlj9GH+4Plb
         0LumKO4qPYr3gHHm4zrHFtOtId/eGgJ3LjIDqkr6N4lC4gUKAeb0wLQSYfoYVn/y3A0Q
         gM58UdRd0QhieyYEtRXYzSY4RqC166BdYxrhXZi0d7Z50WDe5rZHIPRDZQ+pUCrvBNqe
         DVDO35/qreSxdj7pMRcK4kANuH4PURnNRgRgY48aTWep00k2v3/Ic+9P0KvUvOTg+QEc
         lYhZHem3tsncyH24WaiN4hrqClT1d1HxUFcoiNwWT4/Fe7ZMp9P/pqcf19OVHN3vOazt
         ogig==
MIME-Version: 1.0
X-Received: by 10.52.182.39 with SMTP id eb7mr11499039vdc.6.1384759481703;
 Sun, 17 Nov 2013 23:24:41 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Sun, 17 Nov 2013 23:24:41 -0800 (PST)
Date: Sun, 17 Nov 2013 23:24:41 -0800
Message-ID: <CACwKa9cx87JWcVtr8rs-ayGrwT8_FJ8r5TNQuX-vTwQj=PbKHw@mail.gmail.com>
Subject: configuring final partition length
From: Umar Javed <umarj.javed@gmail.com>
To: user@spark.incubator.apache.org, dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=bcaec547ca8d01a36304eb6e71f6
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec547ca8d01a36304eb6e71f6
Content-Type: text/plain; charset=ISO-8859-1

I'm using pyspark. I was wondering how to modify the number of partitions
for the result task (reduce in my case). I'm running Spark on a cluster of
two machines (each with 16 cores). Here's the relevant log output for my
result stage:

13/11/17 23:16:47 INFO SparkContext: time: 18851958895218046
*13/11/17 23:16:47 INFO SparkContext: partition length: 2*
13/11/17 23:16:47 DEBUG DAGScheduler: Got event of type
org.apache.spark.scheduler.JobSubmitted
13/11/17 23:16:47 INFO DAGScheduler: class of RDD: class
org.apache.spark.api.python.PythonRDD PythonRDD[6] at RDD at
PythonRDD.scala:34
13/11/17 23:16:47 INFO DAGScheduler: number of dependencies: 1
13/11/17 23:16:47 INFO DAGScheduler: class of dep: class
org.apache.spark.rdd.MappedRDD MappedRDD[5] at values at
NativeMethodAccessorImpl.java:-2
13/11/17 23:16:47 INFO DAGScheduler: class of RDD: class
org.apache.spark.rdd.MappedRDD MappedRDD[5] at values at
NativeMethodAccessorImpl.java:-2
13/11/17 23:16:47 INFO DAGScheduler: number of dependencies: 1
13/11/17 23:16:47 INFO DAGScheduler: class of dep: class
org.apache.spark.rdd.ShuffledRDD ShuffledRDD[4] at partitionBy at
NativeMethodAccessorImpl.java:-2
13/11/17 23:16:47 INFO DAGScheduler: class of RDD: class
org.apache.spark.rdd.ShuffledRDD ShuffledRDD[4] at partitionBy at
NativeMethodAccessorImpl.java:-2
13/11/17 23:16:47 INFO DAGScheduler: number of dependencies: 1


In this case, Spark seems to automatically configure the number of
partitions for the result tasks to be 2. The result is that only two reduce
tasks run (one on each machine). Is there a way to modify this number? More
generally how do you configure the number of reduce tasks?

thanks!
Umar

--bcaec547ca8d01a36304eb6e71f6--

From dev-return-766-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 19 21:45:13 2013
Return-Path: <dev-return-766-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 1019C10EC1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Nov 2013 21:45:13 +0000 (UTC)
Received: (qmail 38717 invoked by uid 500); 19 Nov 2013 21:45:12 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38545 invoked by uid 500); 19 Nov 2013 21:45:12 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 38536 invoked by uid 99); 19 Nov 2013 21:45:12 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Nov 2013 21:45:12 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of umarj.javed@gmail.com designates 209.85.220.182 as permitted sender)
Received: from [209.85.220.182] (HELO mail-vc0-f182.google.com) (209.85.220.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Nov 2013 21:45:05 +0000
Received: by mail-vc0-f182.google.com with SMTP id ie18so5110219vcb.41
        for <dev@spark.incubator.apache.org>; Tue, 19 Nov 2013 13:44:45 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=rE9TPvLfv0niQonTSxHwoqTYd99B6YthZcFqS+DQGHE=;
        b=PfHkH17mlJdpkfWSBsdH+BKkHWhi+70ZzIvaHM0AM/RJKRPK9fy5QBJoU4XKhvAdt7
         8eN7tVdA1H1CZMOiJiE9Jv94GR505hknRCVqO1v1/K1lXqWCUyrWL2JC+lHwgGNJCZP9
         VP9y5ZbgECZBPAhdzD5TleMgyfLeD3pDOny7rEdhVoJrYLvGR6LTZJG5UY2uyBo/vGjD
         p+RSB71hmcob6L4jZ6XCtE36xNXzia9yLYHb5TqNwqy6xOWQ89ZSIFrkv4F84qekO6sX
         BURV4Zl7ecY4oMcBBHqwS7Oe3AyJaXVfyiYZVgYRQ5Pw+O6C7NFRHUsJuadpU6aXoNOT
         92mw==
MIME-Version: 1.0
X-Received: by 10.52.33.44 with SMTP id o12mr19389893vdi.7.1384897484957; Tue,
 19 Nov 2013 13:44:44 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Tue, 19 Nov 2013 13:44:44 -0800 (PST)
Date: Tue, 19 Nov 2013 13:44:44 -0800
Message-ID: <CACwKa9ebevqyB5JQ6kr64D+Y=5z5G2bthafW7RuyG+z-d2q_9Q@mail.gmail.com>
Subject: addSparkListener in python
From: Umar Javed <umarj.javed@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=20cf3079b5f2a4332c04eb8e9262
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf3079b5f2a4332c04eb8e9262
Content-Type: text/plain; charset=ISO-8859-1

Is there a way to write the addSparkListener() api call for pyspark, or
should I switch to writing my scripts in scala in order to use this api
call?

thanks!
Umar

--20cf3079b5f2a4332c04eb8e9262--

From dev-return-767-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 19 22:30:21 2013
Return-Path: <dev-return-767-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8E7BD10135
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 19 Nov 2013 22:30:21 +0000 (UTC)
Received: (qmail 47610 invoked by uid 500); 19 Nov 2013 22:30:21 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 47326 invoked by uid 500); 19 Nov 2013 22:30:20 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 47311 invoked by uid 99); 19 Nov 2013 22:30:20 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Nov 2013 22:30:20 +0000
X-ASF-Spam-Status: No, hits=2.2 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of rosenville@gmail.com designates 209.85.192.169 as permitted sender)
Received: from [209.85.192.169] (HELO mail-pd0-f169.google.com) (209.85.192.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 19 Nov 2013 22:30:15 +0000
Received: by mail-pd0-f169.google.com with SMTP id v10so3183379pde.0
        for <multiple recipients>; Tue, 19 Nov 2013 14:29:55 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=SCwNixW4Iopa3dej9G36D/xsEXjoGHwcimVvfGSOASs=;
        b=YIbgUY/0D0LBsX8w1+HGKsL2vf1M+yBTLUa2vJ65GTOMP+rniwGbhez52626AYJHdW
         pmj4yBRJXe2odl4S7ejZTn+gmk8O5zJoK5xCGqe5+6nAAippN0cRCpbxOp5F1EcO+1XN
         Il2B0cMH7rF+ULs6iVY9x+zo6Q5CA4LRGrf3bsc3Qot2Owr1THLHSoRKgIbIlLAcDa8I
         0n7Jsv/6gbFS5v0RHaMXiZJpNzSu/n4ipXTP4bCYZcGzoUFH4ek6Svo7Z5j3HAirh4bV
         Cw8XhCmD/r4xkW7OiEgU2cvztkRXRhbhvR0K65i7Ece/4WTRrgzg28hoPgNGFHKqFx38
         vMRw==
MIME-Version: 1.0
X-Received: by 10.68.189.197 with SMTP id gk5mr28394909pbc.37.1384900195326;
 Tue, 19 Nov 2013 14:29:55 -0800 (PST)
Received: by 10.70.24.3 with HTTP; Tue, 19 Nov 2013 14:29:55 -0800 (PST)
In-Reply-To: <CACwKa9cx87JWcVtr8rs-ayGrwT8_FJ8r5TNQuX-vTwQj=PbKHw@mail.gmail.com>
References: <CACwKa9cx87JWcVtr8rs-ayGrwT8_FJ8r5TNQuX-vTwQj=PbKHw@mail.gmail.com>
Date: Tue, 19 Nov 2013 14:29:55 -0800
Message-ID: <CAOEPXP7yp4-qkSFWDCM2j1L8L48ac_+jVaiZdEzyDnJe=v-FFw@mail.gmail.com>
Subject: Re: configuring final partition length
From: Josh Rosen <rosenville@gmail.com>
To: user@spark.incubator.apache.org
Cc: "Spark Dev (Apache Incubator)" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=e89a8ff1cc7a31236f04eb8f340a
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff1cc7a31236f04eb8f340a
Content-Type: text/plain; charset=ISO-8859-1

I think that the reduce() action is implemented as
mapPartitions.collect().reduce(), so the number of result tasks is
determined by the degree of parallelism of the RDD being reduced.

Some operations, like reduceByKey(), accept a `numPartitions` argument for
configuring the number of reducers:
https://spark.incubator.apache.org/docs/0.8.0/api/pyspark/index.html


On Sun, Nov 17, 2013 at 11:24 PM, Umar Javed <umarj.javed@gmail.com> wrote:

> I'm using pyspark. I was wondering how to modify the number of partitions
> for the result task (reduce in my case). I'm running Spark on a cluster of
> two machines (each with 16 cores). Here's the relevant log output for my
> result stage:
>
> 13/11/17 23:16:47 INFO SparkContext: time: 18851958895218046
> *13/11/17 23:16:47 INFO SparkContext: partition length: 2*
> 13/11/17 23:16:47 DEBUG DAGScheduler: Got event of type
> org.apache.spark.scheduler.JobSubmitted
> 13/11/17 23:16:47 INFO DAGScheduler: class of RDD: class
> org.apache.spark.api.python.PythonRDD PythonRDD[6] at RDD at
> PythonRDD.scala:34
> 13/11/17 23:16:47 INFO DAGScheduler: number of dependencies: 1
> 13/11/17 23:16:47 INFO DAGScheduler: class of dep: class
> org.apache.spark.rdd.MappedRDD MappedRDD[5] at values at
> NativeMethodAccessorImpl.java:-2
> 13/11/17 23:16:47 INFO DAGScheduler: class of RDD: class
> org.apache.spark.rdd.MappedRDD MappedRDD[5] at values at
> NativeMethodAccessorImpl.java:-2
> 13/11/17 23:16:47 INFO DAGScheduler: number of dependencies: 1
> 13/11/17 23:16:47 INFO DAGScheduler: class of dep: class
> org.apache.spark.rdd.ShuffledRDD ShuffledRDD[4] at partitionBy at
> NativeMethodAccessorImpl.java:-2
> 13/11/17 23:16:47 INFO DAGScheduler: class of RDD: class
> org.apache.spark.rdd.ShuffledRDD ShuffledRDD[4] at partitionBy at
> NativeMethodAccessorImpl.java:-2
> 13/11/17 23:16:47 INFO DAGScheduler: number of dependencies: 1
>
>
> In this case, Spark seems to automatically configure the number of
> partitions for the result tasks to be 2. The result is that only two reduce
> tasks run (one on each machine). Is there a way to modify this number? More
> generally how do you configure the number of reduce tasks?
>
> thanks!
> Umar
>

--e89a8ff1cc7a31236f04eb8f340a--

From dev-return-768-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 20 00:26:18 2013
Return-Path: <dev-return-768-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8897E106E1
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Nov 2013 00:26:18 +0000 (UTC)
Received: (qmail 94759 invoked by uid 500); 20 Nov 2013 00:26:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 94683 invoked by uid 500); 20 Nov 2013 00:26:18 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 94675 invoked by uid 99); 20 Nov 2013 00:26:18 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 00:26:18 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.177 as permitted sender)
Received: from [209.85.192.177] (HELO mail-pd0-f177.google.com) (209.85.192.177)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 00:26:10 +0000
Received: by mail-pd0-f177.google.com with SMTP id q10so3644015pdj.22
        for <dev@spark.incubator.apache.org>; Tue, 19 Nov 2013 16:25:49 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=iK0Q1Bo5Vm/66FI2AcQ4oSLPAcWl7gtHpoqdsTt7gtQ=;
        b=wUeg7CCYicIt6+iwm9hthoXTHiX7Oqn9KSX2V55npnNWXSaskSdfzdTpANR05dif5C
         hqhesjaNxkgJaCceSF+8WXw2kCU92v8IDKDLxlthG2yS2ZE07yOB2o73jzruyQDGQjGj
         2aNmA2XWjzgLdAlIVc5GO0VosGMv7U8k4jL9apLs+Ivs0zo1tED9JH7tKvMv/NL0BhrU
         /mUO0udtK11/ULBvoIOtcsOojkOgmMWcY+9QgT6dYBEoSeS1gQpA9CpgVreF+ZGn2LyK
         kvWZG4evXRbhjeFHaHoDnzY5JdWnYrc5culSl1kbUlC42cg3hpvS12wnrOGSADdMRF8V
         3+AA==
X-Received: by 10.66.154.1 with SMTP id vk1mr29262460pab.85.1384907149206;
        Tue, 19 Nov 2013 16:25:49 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id ka3sm33323846pbc.32.2013.11.19.16.25.47
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 19 Nov 2013 16:25:48 -0800 (PST)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1822\))
Subject: Re: addSparkListener in python
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACwKa9ebevqyB5JQ6kr64D+Y=5z5G2bthafW7RuyG+z-d2q_9Q@mail.gmail.com>
Date: Tue, 19 Nov 2013 16:25:46 -0800
Content-Transfer-Encoding: quoted-printable
Message-Id: <AD29BF1B-3444-4247-BA75-95640362C4CE@gmail.com>
References: <CACwKa9ebevqyB5JQ6kr64D+Y=5z5G2bthafW7RuyG+z-d2q_9Q@mail.gmail.com>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1822)
X-Virus-Checked: Checked by ClamAV on apache.org

Sorry, this is currently not available in Python, though you might be =
able to do it through Py4J by accessing the private Java SparkContext =
member of the Python SparkContext. I=92m curious, which pieces of the =
API do you want to use?

Matei

On Nov 19, 2013, at 1:44 PM, Umar Javed <umarj.javed@gmail.com> wrote:

> Is there a way to write the addSparkListener() api call for pyspark, =
or
> should I switch to writing my scripts in scala in order to use this =
api
> call?
>=20
> thanks!
> Umar


From dev-return-769-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 20 01:32:54 2013
Return-Path: <dev-return-769-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9AE1C108D8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Nov 2013 01:32:54 +0000 (UTC)
Received: (qmail 77662 invoked by uid 500); 20 Nov 2013 01:32:54 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 77564 invoked by uid 500); 20 Nov 2013 01:32:54 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 77549 invoked by uid 99); 20 Nov 2013 01:32:54 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 01:32:54 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of umarj.javed@gmail.com designates 209.85.212.46 as permitted sender)
Received: from [209.85.212.46] (HELO mail-vb0-f46.google.com) (209.85.212.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 01:32:47 +0000
Received: by mail-vb0-f46.google.com with SMTP id i12so437366vbh.33
        for <dev@spark.incubator.apache.org>; Tue, 19 Nov 2013 17:32:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=k4GnHEYaqWXTpukqg9cLBXfmqm9mpWMH12M3FDnV4Tk=;
        b=zDlhfK5fePqMJLXpGCrSj7KVr6Qy7dcQ6DALSxlu2QkiOsnZjbYMciBXHX2i0p/gJT
         NEbrMMkrYrHk1Vzr3a6pQ3Uz/JbJbFmeT6ThfKb1yraykxQ64iTrwqOJHd9a94ROKRSE
         7WdyMW+KhUyFEB1923FXpOu31PyNO6cca17jO09YIzgAZkpXjUzumf8BVwehmBIVWV3S
         VmYr6Z1iTCaRVTLzZdKmCbynXF9/yjHff62ovnaSC5nC4k9F88f6ns4yoMB8anAZYkjt
         CGQJZryNPwjUTjb2Pts9uRO4lvZJCtr7CetYCzT+1Jl+gI0C0uxm6QRsXFg2zbyo2qlj
         XLBA==
MIME-Version: 1.0
X-Received: by 10.58.100.244 with SMTP id fb20mr23637981veb.6.1384911146063;
 Tue, 19 Nov 2013 17:32:26 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Tue, 19 Nov 2013 17:32:25 -0800 (PST)
In-Reply-To: <AD29BF1B-3444-4247-BA75-95640362C4CE@gmail.com>
References: <CACwKa9ebevqyB5JQ6kr64D+Y=5z5G2bthafW7RuyG+z-d2q_9Q@mail.gmail.com>
	<AD29BF1B-3444-4247-BA75-95640362C4CE@gmail.com>
Date: Tue, 19 Nov 2013 17:32:25 -0800
Message-ID: <CACwKa9cYOYcqOZ0uw-Yr=EYYAh=yymQAfdVD2L+DR4Z13kvAig@mail.gmail.com>
Subject: Re: addSparkListener in python
From: Umar Javed <umarj.javed@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e013a2708e8202004eb91c005
X-Virus-Checked: Checked by ClamAV on apache.org

--089e013a2708e8202004eb91c005
Content-Type: text/plain; charset=windows-1252
Content-Transfer-Encoding: quoted-printable

It takes a SparkListener() as the argument. Any idea how I construct a
scala object (a SparkListener() in this case) inside python and then pass
it on to the Java SparkContext member? This is the only API call I need
right now.


On Tue, Nov 19, 2013 at 4:25 PM, Matei Zaharia <matei.zaharia@gmail.com>wro=
te:

> Sorry, this is currently not available in Python, though you might be abl=
e
> to do it through Py4J by accessing the private Java SparkContext member o=
f
> the Python SparkContext. I=92m curious, which pieces of the API do you wa=
nt
> to use?
>
> Matei
>
> On Nov 19, 2013, at 1:44 PM, Umar Javed <umarj.javed@gmail.com> wrote:
>
> > Is there a way to write the addSparkListener() api call for pyspark, or
> > should I switch to writing my scripts in scala in order to use this api
> > call?
> >
> > thanks!
> > Umar
>
>

--089e013a2708e8202004eb91c005--

From dev-return-770-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 20 02:46:52 2013
Return-Path: <dev-return-770-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6A12410AF3
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Nov 2013 02:46:52 +0000 (UTC)
Received: (qmail 69805 invoked by uid 500); 20 Nov 2013 02:46:52 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 69764 invoked by uid 500); 20 Nov 2013 02:46:52 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 69756 invoked by uid 99); 20 Nov 2013 02:46:52 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 02:46:52 +0000
X-ASF-Spam-Status: No, hits=-0.0 required=5.0
	tests=RCVD_IN_DNSWL_NONE,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of matei.zaharia@gmail.com designates 209.85.192.170 as permitted sender)
Received: from [209.85.192.170] (HELO mail-pd0-f170.google.com) (209.85.192.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 02:46:44 +0000
Received: by mail-pd0-f170.google.com with SMTP id g10so4656496pdj.29
        for <dev@spark.incubator.apache.org>; Tue, 19 Nov 2013 18:46:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=7gXToCpvrFPS4ywgD2iZXZKP2aUVf77soWiqR0l6zZQ=;
        b=1ErzpWJ2e/PBzYSGBdArdXDPA6US543hevs8Wg6AY2xaAASoDtpSqF86CWp+um3YVL
         Xn21ZlVQ5hVH9ZsZ3omK1RyWDv+c3gK9oMOto4uJfHZEpQMhPHCQia2TbOikUsGD6R2e
         Joq4ItqWImdrwjA7cjNUO7VTLIOLCfQgAagD1N2M+9RoQhVrhAoJVfJmHKOfHRhlRlre
         CHZEs1/gaZ4hxKzn7uTBhNuVAV+gTb8cEPPJGC4K3Wqhmrkdy5hNl0Tc35km+2dNOZ95
         hcIi1cGnTHajQIqIv1s8eLDTPxM3ypzXzW3un2jOeBPGZ0o+hepKROvI+JvxykD1UGZh
         2A7A==
X-Received: by 10.66.231.6 with SMTP id tc6mr29130608pac.68.1384915583336;
        Tue, 19 Nov 2013 18:46:23 -0800 (PST)
Received: from [192.168.1.106] (c-67-164-94-237.hsd1.ca.comcast.net. [67.164.94.237])
        by mx.google.com with ESMTPSA id xv2sm33874544pbb.39.2013.11.19.18.46.21
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Tue, 19 Nov 2013 18:46:22 -0800 (PST)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1822\))
Subject: Re: addSparkListener in python
From: Matei Zaharia <matei.zaharia@gmail.com>
In-Reply-To: <CACwKa9cYOYcqOZ0uw-Yr=EYYAh=yymQAfdVD2L+DR4Z13kvAig@mail.gmail.com>
Date: Tue, 19 Nov 2013 18:46:18 -0800
Content-Transfer-Encoding: quoted-printable
Message-Id: <0E42D353-FFE9-4BE5-B9D0-5403BE840C67@gmail.com>
References: <CACwKa9ebevqyB5JQ6kr64D+Y=5z5G2bthafW7RuyG+z-d2q_9Q@mail.gmail.com> <AD29BF1B-3444-4247-BA75-95640362C4CE@gmail.com> <CACwKa9cYOYcqOZ0uw-Yr=EYYAh=yymQAfdVD2L+DR4Z13kvAig@mail.gmail.com>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1822)
X-Virus-Checked: Checked by ClamAV on apache.org

Take a look at =
http://py4j.sourceforge.net/advanced_topics.html#implementing-java-interfa=
ces-from-python-callback. Not 100% sure this will work, but this would =
be the way to do it.

Matei

On Nov 19, 2013, at 5:32 PM, Umar Javed <umarj.javed@gmail.com> wrote:

> It takes a SparkListener() as the argument. Any idea how I construct a
> scala object (a SparkListener() in this case) inside python and then =
pass
> it on to the Java SparkContext member? This is the only API call I =
need
> right now.
>=20
>=20
> On Tue, Nov 19, 2013 at 4:25 PM, Matei Zaharia =
<matei.zaharia@gmail.com>wrote:
>=20
>> Sorry, this is currently not available in Python, though you might be =
able
>> to do it through Py4J by accessing the private Java SparkContext =
member of
>> the Python SparkContext. I=92m curious, which pieces of the API do =
you want
>> to use?
>>=20
>> Matei
>>=20
>> On Nov 19, 2013, at 1:44 PM, Umar Javed <umarj.javed@gmail.com> =
wrote:
>>=20
>>> Is there a way to write the addSparkListener() api call for pyspark, =
or
>>> should I switch to writing my scripts in scala in order to use this =
api
>>> call?
>>>=20
>>> thanks!
>>> Umar
>>=20
>>=20


From dev-return-771-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 20 23:03:02 2013
Return-Path: <dev-return-771-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id A3119105F5
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 20 Nov 2013 23:03:02 +0000 (UTC)
Received: (qmail 38218 invoked by uid 500); 20 Nov 2013 23:03:02 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38139 invoked by uid 500); 20 Nov 2013 23:03:01 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 38126 invoked by uid 99); 20 Nov 2013 23:03:01 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 23:03:01 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of umarj.javed@gmail.com designates 209.85.212.48 as permitted sender)
Received: from [209.85.212.48] (HELO mail-vb0-f48.google.com) (209.85.212.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 20 Nov 2013 23:02:56 +0000
Received: by mail-vb0-f48.google.com with SMTP id x16so3020076vbf.7
        for <dev@spark.incubator.apache.org>; Wed, 20 Nov 2013 15:02:35 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=BdQ9bR+kZzUcrQKnB1uYddSRXkQP0CA/QXlslAfwseg=;
        b=DVaDtie8z5FZApAPUtmx0cjAvmJ2wTtDFH99IJaw6FQl4PBtQ7dcynYZwFWhGK4kNE
         9cbQjwH0KLxNSj0dbUtuoVFcEzwrSOWkBFrePWnsr2/GxQw6R8XMm83Mzi7fk+uDaA1y
         7huWvB1gE2CUBXSw3OIosfoxUOOK9uddSkZDInLbS1mL8W9L9O0drtbTGr6YAqPZ/6q3
         yQDirDjmJKSSUPtRkqjyS8SDwMxOdGTsansiGoydSBvCOKcOSp9ptxLfvQNG6J83LHhl
         00nEGW7rxq80AlEjzPJLWqArH+6MG0dz/5tX3yvELQYuiIeVBkvWuZIADVvilehdNZP3
         ESnQ==
MIME-Version: 1.0
X-Received: by 10.52.103.35 with SMTP id ft3mr2210035vdb.5.1384988555518; Wed,
 20 Nov 2013 15:02:35 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Wed, 20 Nov 2013 15:02:35 -0800 (PST)
Date: Wed, 20 Nov 2013 15:02:35 -0800
Message-ID: <CACwKa9eFKr79kRqj+xu9zvpEwCpKZ0Xc08TD9LN-oxBdJHWQug@mail.gmail.com>
Subject: difference between 'fetchWaitTime' and 'remoteFetchTime'
From: Umar Javed <umarj.javed@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=e89a8ff24d9bdea8b004eba3c63b
X-Virus-Checked: Checked by ClamAV on apache.org

--e89a8ff24d9bdea8b004eba3c63b
Content-Type: text/plain; charset=ISO-8859-1

In the class ShuffleReadMetrics in executor/TaskMetrics.scala, there are
two variables:

1) fetchWaitTime: /**


   * Total time that is spent blocked waiting for shuffle to fetch data


   */

2) remoteFetchTime

/**


   * The total amount of time for all the shuffle fetches.  This adds up
time from overlapping

   *     shuffles, so can be longer than task time


   */

As I understand it, the difference between these two is that fetchWaitTime
is remoteFetchTime without the overlapped time counted exactly once. Is
that right? Can somebody explain the difference better?

thanks!

--e89a8ff24d9bdea8b004eba3c63b--

From dev-return-772-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 21 05:21:15 2013
Return-Path: <dev-return-772-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA3BE10E03
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 21 Nov 2013 05:21:15 +0000 (UTC)
Received: (qmail 24808 invoked by uid 500); 21 Nov 2013 05:21:04 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 24751 invoked by uid 500); 21 Nov 2013 05:20:56 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 24735 invoked by uid 99); 21 Nov 2013 05:20:53 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Nov 2013 05:20:53 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of umarj.javed@gmail.com designates 209.85.212.48 as permitted sender)
Received: from [209.85.212.48] (HELO mail-vb0-f48.google.com) (209.85.212.48)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 21 Nov 2013 05:20:47 +0000
Received: by mail-vb0-f48.google.com with SMTP id x16so3225567vbf.7
        for <multiple recipients>; Wed, 20 Nov 2013 21:20:26 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=rhq6vTUfAH8OAjjjE7JY7w/SWbLAK60kLUV5eqWnWA8=;
        b=ULQOdn7j4zcRzRkUDYAkCQ6330a0DZbjGcKOCNXazRGTERkEzPQAi52BQEa2k/OdOq
         T8sA+WtN096BPOzP/1R48iyARQw1Ju76rJ5X9Bse5XFe6Cdy4Pr+0/cvMFCpZc2ct/eF
         kh12ZzE5WNj1bhXK3Kjk4f9x1xGmRQpfdQyP5I/+i5WbNQUZbzwKgyfTYH3ZILTxEgo2
         kTVCcIuud3VzSd0cNnbDYE38GQQi9JgNgjDfyZ6GqtJ7fkF5cwiASd4PJGjTyAlsHboT
         iRP4ZgLgsw+a6K7kTgc9BewoECMLj+DGjRIdt8UCBMel/h+NyVsl7EjcjEUqiDDdADq8
         9uwQ==
MIME-Version: 1.0
X-Received: by 10.220.74.69 with SMTP id t5mr4083070vcj.18.1385011226814; Wed,
 20 Nov 2013 21:20:26 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Wed, 20 Nov 2013 21:20:26 -0800 (PST)
Date: Wed, 20 Nov 2013 21:20:26 -0800
Message-ID: <CACwKa9f-CyLAfWvZnqw+yY_+d_H8KHBr3b2_O5nOdVAy98S60w@mail.gmail.com>
Subject: time taken to fetch input partition by map
From: Umar Javed <umarj.javed@gmail.com>
To: user@spark.incubator.apache.org, dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b624cbe2f2ff404eba90e01
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b624cbe2f2ff404eba90e01
Content-Type: text/plain; charset=ISO-8859-1

Hi,

The metrics provide information for the reduce (i.e. shuffleReaders) tasks
about the time taken to fetch the shuffle outputs. Is there a way I can
find out the the time taken by a map task (ie shuffleWriter) on a remote
machine to read its input partition from disk?

I believe I should look in HadoopRDD.scala where there is the
getRecordReader, and the headers show that it should be
in org.apache.hadoop.mapred.RecordReader, but I can't find that file
anywhere.

Any help would be appreciated.

thanks!
Umar

--047d7b624cbe2f2ff404eba90e01--

From dev-return-773-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 22 20:56:30 2013
Return-Path: <dev-return-773-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8100610959
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Nov 2013 20:56:30 +0000 (UTC)
Received: (qmail 9305 invoked by uid 500); 22 Nov 2013 20:56:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 9244 invoked by uid 500); 22 Nov 2013 20:56:29 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 9233 invoked by uid 99); 22 Nov 2013 20:56:29 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Nov 2013 20:56:29 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.216.51 as permitted sender)
Received: from [209.85.216.51] (HELO mail-qa0-f51.google.com) (209.85.216.51)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Nov 2013 20:56:23 +0000
Received: by mail-qa0-f51.google.com with SMTP id o15so5212821qap.17
        for <dev@spark.incubator.apache.org>; Fri, 22 Nov 2013 12:56:02 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=XpJXTfnRaRA9usIkET+vFo6fImpLh6RgzxZ36PcF95A=;
        b=Rsl0TiZeBX6i4FUx5Jc4EOE2FKz/wf/6a0jCgaLPWmOHHFoLw5i1PU7prJ6IPhmBf3
         CFO3TMx3rcOFAQ2jd2+mCLCZYxLgUlAR5hCMMm1tWzvyA3P1crfn3rTLTaOmX67QKiCu
         41N2zYy5r3nRkNHifLqIPGKWY8G6iSwqwtLGY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=XpJXTfnRaRA9usIkET+vFo6fImpLh6RgzxZ36PcF95A=;
        b=B+oGhfyINUcoClo9wC5hBxKOf7R01NR5yBsdQGgozKXqKnSTs18IikCYVsmWffpSSR
         NZw121wDgSjjn/gMRx/0FuRfvkQlZabyZrFbx2mqALnWOuSKonXMEAUfGOA9zJnmcp79
         KHC1iz+Q3JDOEzQX0an2ZRmbZogBQMpsd/FpCYMEBRxBJJXFTXutN8USRjGouk4rqHNp
         nHldCJNE1Smq85NJRQhUcZEoNhidWeSSzosIyQ3ifTlAorDa5mZSPAXzc9SJmbDJL3R5
         gNEi5SOE7jkOoig/sTnc1l0OzHYCgttn52UmC+cvgK8Oky1fuD+ypx29jI/eMCcbWGZQ
         oSeA==
X-Gm-Message-State: ALoCoQnf4WL6TpG/da1PSnO+dgszSOPsq9UEvROLLXraawB6KmvAo2/bWb/iVOXoJY2f/5dbjdvm
MIME-Version: 1.0
X-Received: by 10.49.50.161 with SMTP id d1mr25368779qeo.51.1385153762391;
 Fri, 22 Nov 2013 12:56:02 -0800 (PST)
Received: by 10.96.148.168 with HTTP; Fri, 22 Nov 2013 12:56:02 -0800 (PST)
Date: Fri, 22 Nov 2013 15:56:02 -0500
Message-ID: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
Subject: Problem with tests
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7bd74e3ef78b4d04ebca3de6
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bd74e3ef78b4d04ebca3de6
Content-Type: text/plain; charset=ISO-8859-1

Hi there.

I have a problem with the unit tests on a pull request I'm trying to tie
up.  The changes deal with partition-related functions.

In particular, the tests I have that test an append-to-partition function
work fine on my own machine, but fail on the build machine (
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
).

The failure seems to stem from pulling a single partition out of the set.
In either case, when I work on the full dataset:

UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
  UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
    ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1 partitions)
    MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
partitions)
      ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2 partitions)
  ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1 partitions)


It seems to work.  When I pull one partition out of this, by wrapping
a PartitionPruningRDD around it (pruning out everything but partition
2):

PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1 partitions)
  UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
    UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
      ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1 partitions)
      MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265
(2 partitions)
        ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2 partitions)
    ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1 partitions)


In this case, my local machine and the build machine seem to act
differently.

On my local machine, what is in the inner ParallelCollection partition #2
shows up in the MapPartitionsWithContextRDD as partition #2 still.  On the
build machine, this same partition shows up in the later RDD as partition
#0 - presumably because everything else is pruned out, but that pruning
should happen at an outer level, shouldn't it?

Does anyone know why the build machine would act different from locally
here?

Also, sadly, this worked fine two days ago.

My only thought is that perhaps the PullRequestBuilder does a merge with
current code, and someone broke this in the last day or two?  Past that,
I'm at a bit of a loss.

Thanks,
                    -Nathan


-- 

Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--047d7bd74e3ef78b4d04ebca3de6--

From dev-return-774-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 22 21:02:58 2013
Return-Path: <dev-return-774-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4BB8B109CF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Nov 2013 21:02:58 +0000 (UTC)
Received: (qmail 26020 invoked by uid 500); 22 Nov 2013 21:02:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 25935 invoked by uid 500); 22 Nov 2013 21:02:58 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 25924 invoked by uid 99); 22 Nov 2013 21:02:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Nov 2013 21:02:57 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.216.53 as permitted sender)
Received: from [209.85.216.53] (HELO mail-qa0-f53.google.com) (209.85.216.53)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Nov 2013 21:02:52 +0000
Received: by mail-qa0-f53.google.com with SMTP id j5so4478625qaq.19
        for <dev@spark.incubator.apache.org>; Fri, 22 Nov 2013 13:02:31 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=SJH/2ZftFFJPPASP9fJFCy7Kkm76I9leAoLeujDT65w=;
        b=eR3Czy/yL27ZHDg3C4iex2yUg/ufgY8FxsVMdZluHG7iO39b1grG153F4iSqovOgM8
         Vf9+Iezw42Mt30bTrRhGKxQztiKsjS8S9SNl4AG1gTceSF+2JFe2Y4a7pPsqP7BNq9cJ
         TI1bN0I6Ma/3FdZekZpnwfS6y4srdTXol7LDQ=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=SJH/2ZftFFJPPASP9fJFCy7Kkm76I9leAoLeujDT65w=;
        b=nHRSkKphUSfQqandej3frIYILOziVrVN6KNh+Mu8lzaJAJVT0yfV83Z5AOxwGbdAQJ
         ZptSvfSj6LIx06gudRWzdFwrw7wDhODpO7++ylr+yFrUtgtOqE7S6IfNlsEWKoyrXM/J
         S2JKKH/LdlhHVbdBID9XZiYNzeoqO2zf3Xa0kMjZkcLkK2Xk1CfsHbzbgDS6/LRpi6rr
         A9r9+VHSBouzbsr0bLOai9zDgP5NdtNfo2tZ0O9CXvBbR6pMjwZHI3cDSzDtGJpjLbpk
         ctON3KdwHtlDVYIBwDFgL6xFVeqV3aD08FgASei8AxH0ppMLckBd3Q7Jaa/mviQXhVRc
         BAxw==
X-Gm-Message-State: ALoCoQkUCTKPBeRvNFn18RLbwINncm5mIx5YEn9pchrDdV3uOPaY7hviswbd5AJn+KbyUbMEf5B8
MIME-Version: 1.0
X-Received: by 10.224.65.199 with SMTP id k7mr25885106qai.24.1385154151382;
 Fri, 22 Nov 2013 13:02:31 -0800 (PST)
Received: by 10.96.148.168 with HTTP; Fri, 22 Nov 2013 13:02:31 -0800 (PST)
In-Reply-To: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
References: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
Date: Fri, 22 Nov 2013 16:02:31 -0500
Message-ID: <CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
Subject: Re: Problem with tests
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ca5c2715b704ebca5512
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ca5c2715b704ebca5512
Content-Type: text/plain; charset=ISO-8859-1

Actually, looking into recent commits, it looks like my hunch may be
exactly correct:
https://github.com/apache/incubator-spark/commit/f639b65eabcc8666b74af8f13a37c5fdf7e0185f
"PartitionPruningRDD is using index from parent"

Is there anyone who can explain why this new behavior is preferable?  And,
if it's staying, can suggest a way to fix my tests for this case?

Thanks again,
                 Nathan


On Fri, Nov 22, 2013 at 3:56 PM, Nathan Kronenfeld <
nkronenfeld@oculusinfo.com> wrote:

> Hi there.
>
> I have a problem with the unit tests on a pull request I'm trying to tie
> up.  The changes deal with partition-related functions.
>
> In particular, the tests I have that test an append-to-partition function
> work fine on my own machine, but fail on the build machine (
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
> ).
>
> The failure seems to stem from pulling a single partition out of the set.
> In either case, when I work on the full dataset:
>
> UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
>   UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
>     ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1 partitions)
>     MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2 partitions)
>       ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2 partitions)
>   ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1 partitions)
>
>
> It seems to work.  When I pull one partition out of this, by wrapping a PartitionPruningRDD around it (pruning out everything but partition 2):
>
> PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1 partitions)
>   UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
>     UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
>       ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1 partitions)
>       MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2 partitions)
>         ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2 partitions)
>     ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1 partitions)
>
>
> In this case, my local machine and the build machine seem to act
> differently.
>
> On my local machine, what is in the inner ParallelCollection partition #2
> shows up in the MapPartitionsWithContextRDD as partition #2 still.  On the
> build machine, this same partition shows up in the later RDD as partition
> #0 - presumably because everything else is pruned out, but that pruning
> should happen at an outer level, shouldn't it?
>
> Does anyone know why the build machine would act different from locally
> here?
>
> Also, sadly, this worked fine two days ago.
>
> My only thought is that perhaps the PullRequestBuilder does a merge with
> current code, and someone broke this in the last day or two?  Past that,
> I'm at a bit of a loss.
>
> Thanks,
>                     -Nathan
>
>
> --
>
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com
>



-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--001a11c2ca5c2715b704ebca5512--

From dev-return-775-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 22 23:36:57 2013
Return-Path: <dev-return-775-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 7D88310E4F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Nov 2013 23:36:57 +0000 (UTC)
Received: (qmail 82249 invoked by uid 500); 22 Nov 2013 23:36:57 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 82119 invoked by uid 500); 22 Nov 2013 23:36:57 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 82111 invoked by uid 99); 22 Nov 2013 23:36:57 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Nov 2013 23:36:57 +0000
X-ASF-Spam-Status: No, hits=-1998.3 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Fri, 22 Nov 2013 23:36:53 +0000
Received: (qmail 82062 invoked by uid 99); 22 Nov 2013 23:36:31 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Nov 2013 23:36:31 +0000
Received: from localhost (HELO mail-vb0-f50.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Fri, 22 Nov 2013 23:36:31 +0000
Received: by mail-vb0-f50.google.com with SMTP id 10so1339458vbe.23
        for <dev@spark.incubator.apache.org>; Fri, 22 Nov 2013 15:36:30 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=b35Sa/0IANh+i2Z5v5CnZRUFDTuVZ5V5/7w7CRfJTHg=;
        b=gnkG/gRIKltEJni8sS2VD8yY17XmqiMmivRsQq+3xxysB1hoCH9EjHgJPAq4jCg+84
         KQsDdF2GvYs8lJ008GU4fYS0jDWhhZ0R9nHONlmgB3JYLt0ElhU+RyPSnTsfIu1HvbMP
         dPJ5GQy27UdHAjQtyWNrI3nggFVj3EQVwQCjufoVDLFjIEv75wDsdDfqs1Mz98c4MQsk
         j8chkcVVkELeD8mVjLTxlNB0r9CGZDB2rqiP80mKmNBFpwLgdIqVpgwwFXtvIwroldOY
         U2RhPqbk2qbpa3H76l+RVSs+k4Zoc4S9FeFQJ5ZAvQ8QiMO7diG+0mNvjTRZlzA85c4k
         YHOg==
X-Received: by 10.52.227.6 with SMTP id rw6mr11554370vdc.19.1385163390030;
 Fri, 22 Nov 2013 15:36:30 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Fri, 22 Nov 2013 15:36:09 -0800 (PST)
In-Reply-To: <CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
References: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
 <CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
From: Reynold Xin <rxin@apache.org>
Date: Sat, 23 Nov 2013 07:36:09 +0800
Message-ID: <CAC1ssC4w-ZF5mF-1sHkyx4zyOmhjrq-7xfPh+dF5niuxLXjCfw@mail.gmail.com>
Subject: Re: Problem with tests
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e01161660d19d3704ebcc7b0b
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01161660d19d3704ebcc7b0b
Content-Type: text/plain; charset=UTF-8

Can you provide a link to your pull request?


On Sat, Nov 23, 2013 at 5:02 AM, Nathan Kronenfeld <
nkronenfeld@oculusinfo.com> wrote:

> Actually, looking into recent commits, it looks like my hunch may be
> exactly correct:
>
> https://github.com/apache/incubator-spark/commit/f639b65eabcc8666b74af8f13a37c5fdf7e0185f
> "PartitionPruningRDD is using index from parent"
>
> Is there anyone who can explain why this new behavior is preferable?  And,
> if it's staying, can suggest a way to fix my tests for this case?
>
> Thanks again,
>                  Nathan
>
>
> On Fri, Nov 22, 2013 at 3:56 PM, Nathan Kronenfeld <
> nkronenfeld@oculusinfo.com> wrote:
>
> > Hi there.
> >
> > I have a problem with the unit tests on a pull request I'm trying to tie
> > up.  The changes deal with partition-related functions.
> >
> > In particular, the tests I have that test an append-to-partition function
> > work fine on my own machine, but fail on the build machine (
> >
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
> > ).
> >
> > The failure seems to stem from pulling a single partition out of the set.
> > In either case, when I work on the full dataset:
> >
> > UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> >   UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> >     ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> partitions)
> >     MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
> partitions)
> >       ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> partitions)
> >   ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> partitions)
> >
> >
> > It seems to work.  When I pull one partition out of this, by wrapping a
> PartitionPruningRDD around it (pruning out everything but partition 2):
> >
> > PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1 partitions)
> >   UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> >     UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> >       ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> partitions)
> >       MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
> partitions)
> >         ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> partitions)
> >     ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> partitions)
> >
> >
> > In this case, my local machine and the build machine seem to act
> > differently.
> >
> > On my local machine, what is in the inner ParallelCollection partition #2
> > shows up in the MapPartitionsWithContextRDD as partition #2 still.  On
> the
> > build machine, this same partition shows up in the later RDD as partition
> > #0 - presumably because everything else is pruned out, but that pruning
> > should happen at an outer level, shouldn't it?
> >
> > Does anyone know why the build machine would act different from locally
> > here?
> >
> > Also, sadly, this worked fine two days ago.
> >
> > My only thought is that perhaps the PullRequestBuilder does a merge with
> > current code, and someone broke this in the last day or two?  Past that,
> > I'm at a bit of a loss.
> >
> > Thanks,
> >                     -Nathan
> >
> >
> > --
> >
> > Nathan Kronenfeld
> > Senior Visualization Developer
> > Oculus Info Inc
> > 2 Berkeley Street, Suite 600,
> > Toronto, Ontario M5A 4J5
> > Phone:  +1-416-203-3003 x 238
> > Email:  nkronenfeld@oculusinfo.com
> >
>
>
>
> --
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com
>

--089e01161660d19d3704ebcc7b0b--

From dev-return-776-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 22 23:39:41 2013
Return-Path: <dev-return-776-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 8ACA010E58
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 22 Nov 2013 23:39:41 +0000 (UTC)
Received: (qmail 86309 invoked by uid 500); 22 Nov 2013 23:39:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 86266 invoked by uid 500); 22 Nov 2013 23:39:41 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Delivered-To: moderator for dev@spark.incubator.apache.org
Received: (qmail 81996 invoked by uid 99); 22 Nov 2013 23:36:22 -0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of reynoldx@gmail.com designates 209.85.220.169 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=2XmipQAN4py5TQE1Qf15aHCg9KfgJl4g/JeuPIyJfPU=;
        b=VIjU10BzYVI2EdFpSCCAL+FyppAuvRhww1XHeju4tf7LljJuxY/H778RrTs0JhDy++
         trhlKPBg099VP6GY+TsjDWVJdie92keL5qcoa34cy2AqSwqiDFfCc3+k9O2xxYGRFn7v
         KIJtKiEPD4rAlQgplnubDH8oOVuh/wnM5mUb0DU+9sRhd9fAKf0QIsCOw3LHFdWDcBgH
         5s/KCairNvinQGRTzL9Gp59bL1fLah9w7hmLpU/g7G74dBD90Ws0dAkpkxtLi34Q9HSY
         5fCPaMkK24FSFK/kQZnqTuyJuaXR5YeI4ZEmwNwNtGYKHc6Q8CWdlmddz7Z3ntFUuZw6
         YLYw==
X-Received: by 10.52.116.74 with SMTP id ju10mr11529982vdb.20.1385163355444;
 Fri, 22 Nov 2013 15:35:55 -0800 (PST)
MIME-Version: 1.0
In-Reply-To: <CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
References: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
 <CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
From: Reynold Xin <reynoldx@gmail.com>
Date: Sat, 23 Nov 2013 07:35:35 +0800
Message-ID: <CAC1ssC42ZTKeTmjSiwx5wh8-k63W9XSY+WUdxrURVbXGQ3Opeg@mail.gmail.com>
Subject: Re: Problem with tests
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=20cf307d01dcc1de9504ebcc7961
X-Virus-Checked: Checked by ClamAV on apache.org

--20cf307d01dcc1de9504ebcc7961
Content-Type: text/plain; charset=UTF-8

Can you provide a link to your pull request?


On Sat, Nov 23, 2013 at 5:02 AM, Nathan Kronenfeld <
nkronenfeld@oculusinfo.com> wrote:

> Actually, looking into recent commits, it looks like my hunch may be
> exactly correct:
>
> https://github.com/apache/incubator-spark/commit/f639b65eabcc8666b74af8f13a37c5fdf7e0185f
> "PartitionPruningRDD is using index from parent"
>
> Is there anyone who can explain why this new behavior is preferable?  And,
> if it's staying, can suggest a way to fix my tests for this case?
>
> Thanks again,
>                  Nathan
>
>
> On Fri, Nov 22, 2013 at 3:56 PM, Nathan Kronenfeld <
> nkronenfeld@oculusinfo.com> wrote:
>
> > Hi there.
> >
> > I have a problem with the unit tests on a pull request I'm trying to tie
> > up.  The changes deal with partition-related functions.
> >
> > In particular, the tests I have that test an append-to-partition function
> > work fine on my own machine, but fail on the build machine (
> >
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
> > ).
> >
> > The failure seems to stem from pulling a single partition out of the set.
> > In either case, when I work on the full dataset:
> >
> > UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> >   UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> >     ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> partitions)
> >     MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
> partitions)
> >       ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> partitions)
> >   ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> partitions)
> >
> >
> > It seems to work.  When I pull one partition out of this, by wrapping a
> PartitionPruningRDD around it (pruning out everything but partition 2):
> >
> > PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1 partitions)
> >   UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> >     UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> >       ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> partitions)
> >       MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
> partitions)
> >         ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> partitions)
> >     ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> partitions)
> >
> >
> > In this case, my local machine and the build machine seem to act
> > differently.
> >
> > On my local machine, what is in the inner ParallelCollection partition #2
> > shows up in the MapPartitionsWithContextRDD as partition #2 still.  On
> the
> > build machine, this same partition shows up in the later RDD as partition
> > #0 - presumably because everything else is pruned out, but that pruning
> > should happen at an outer level, shouldn't it?
> >
> > Does anyone know why the build machine would act different from locally
> > here?
> >
> > Also, sadly, this worked fine two days ago.
> >
> > My only thought is that perhaps the PullRequestBuilder does a merge with
> > current code, and someone broke this in the last day or two?  Past that,
> > I'm at a bit of a loss.
> >
> > Thanks,
> >                     -Nathan
> >
> >
> > --
> >
> > Nathan Kronenfeld
> > Senior Visualization Developer
> > Oculus Info Inc
> > 2 Berkeley Street, Suite 600,
> > Toronto, Ontario M5A 4J5
> > Phone:  +1-416-203-3003 x 238
> > Email:  nkronenfeld@oculusinfo.com
> >
>
>
>
> --
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com
>

--20cf307d01dcc1de9504ebcc7961--

From dev-return-777-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov 23 13:01:01 2013
Return-Path: <dev-return-777-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 50D9010B52
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Nov 2013 13:01:01 +0000 (UTC)
Received: (qmail 91062 invoked by uid 500); 23 Nov 2013 13:01:00 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 90635 invoked by uid 500); 23 Nov 2013 13:00:54 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 90550 invoked by uid 99); 23 Nov 2013 13:00:52 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Nov 2013 13:00:52 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.128.47 as permitted sender)
Received: from [209.85.128.47] (HELO mail-qe0-f47.google.com) (209.85.128.47)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Nov 2013 13:00:48 +0000
Received: by mail-qe0-f47.google.com with SMTP id t7so1961360qeb.34
        for <dev@spark.incubator.apache.org>; Sat, 23 Nov 2013 05:00:27 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=BJgIQQfDiOiIxv2OSIS2xWt+RE4zbkLSBTSNxbcZXgM=;
        b=hIpTuxV9sWhEoseWHgtppSu32OwDl82Bp7vFBnyz2qYcfpfgk7ULX8sTG4mO1bc04T
         JCJ6pESkDh7H7h2cQu+g8+9QE5CWRgaaJXR4uLfF7wFM9WCAwN7znKb5/1FzVwN20osd
         y5bZsUnUlTQR7EntOjDtRPOrjnqSuzIlHZIJ8=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=BJgIQQfDiOiIxv2OSIS2xWt+RE4zbkLSBTSNxbcZXgM=;
        b=QgjmS9si9cAcnNuSImamtAVHxNkiGeZkXVWoknWutvAAApf7b3t4yISS7y6a1TKvBm
         5zpC8bXG66yDv9dgPiQy861R/+VSUPAFbmbUMhBbrEw0Ysy2psYOfFySYBVrwJzl4t29
         tWW+lb1EbrtiwBV2SXulUiTjQfN/FqL8Czzx4hgErrTL8rKJZnF8q9WDuaVkiPtuqnP8
         rvEVyEQ/Arh+4hA7OTPhKVoB5kSn8qcDn+S5pjcIiiUYZWJ7tnKwA2i0Ojq4e8AvIrAb
         BTvee8fOQAwrfylypaRBB9YJa4KiuZFw+VIVIbirhI9Cd2VUbIm8QCJyJ51eOF2lc2UA
         OAXw==
X-Gm-Message-State: ALoCoQmXm/UbP1yRUMx5xgnHkvsbYujp1Cn4f+XD1bZRAMLShYLwkogS49MxJLtvn/WZNV68KYki
MIME-Version: 1.0
X-Received: by 10.224.172.6 with SMTP id j6mr1807858qaz.97.1385211627581; Sat,
 23 Nov 2013 05:00:27 -0800 (PST)
Received: by 10.96.148.168 with HTTP; Sat, 23 Nov 2013 05:00:27 -0800 (PST)
In-Reply-To: <CAC1ssC42ZTKeTmjSiwx5wh8-k63W9XSY+WUdxrURVbXGQ3Opeg@mail.gmail.com>
References: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
	<CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
	<CAC1ssC42ZTKeTmjSiwx5wh8-k63W9XSY+WUdxrURVbXGQ3Opeg@mail.gmail.com>
Date: Sat, 23 Nov 2013 08:00:27 -0500
Message-ID: <CAEpWh48wwEwLOx2H0ix2GWm=igMw-UKXtsnofsurz9pcpn=6Hg@mail.gmail.com>
Subject: Re: Problem with tests
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c2c8440045a704ebd7b70b
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2c8440045a704ebd7b70b
Content-Type: text/plain; charset=ISO-8859-1

https://github.com/apache/incubator-spark/pull/18


On Fri, Nov 22, 2013 at 6:35 PM, Reynold Xin <reynoldx@gmail.com> wrote:

> Can you provide a link to your pull request?
>
>
> On Sat, Nov 23, 2013 at 5:02 AM, Nathan Kronenfeld <
> nkronenfeld@oculusinfo.com> wrote:
>
> > Actually, looking into recent commits, it looks like my hunch may be
> > exactly correct:
> >
> >
> https://github.com/apache/incubator-spark/commit/f639b65eabcc8666b74af8f13a37c5fdf7e0185f
> > "PartitionPruningRDD is using index from parent"
> >
> > Is there anyone who can explain why this new behavior is preferable?
>  And,
> > if it's staying, can suggest a way to fix my tests for this case?
> >
> > Thanks again,
> >                  Nathan
> >
> >
> > On Fri, Nov 22, 2013 at 3:56 PM, Nathan Kronenfeld <
> > nkronenfeld@oculusinfo.com> wrote:
> >
> > > Hi there.
> > >
> > > I have a problem with the unit tests on a pull request I'm trying to
> tie
> > > up.  The changes deal with partition-related functions.
> > >
> > > In particular, the tests I have that test an append-to-partition
> function
> > > work fine on my own machine, but fail on the build machine (
> > >
> >
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
> > > ).
> > >
> > > The failure seems to stem from pulling a single partition out of the
> set.
> > > In either case, when I work on the full dataset:
> > >
> > > UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> > >   UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> > >     ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> > partitions)
> > >     MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
> > partitions)
> > >       ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> > partitions)
> > >   ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> > partitions)
> > >
> > >
> > > It seems to work.  When I pull one partition out of this, by wrapping a
> > PartitionPruningRDD around it (pruning out everything but partition 2):
> > >
> > > PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1 partitions)
> > >   UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> > >     UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> > >       ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> > partitions)
> > >       MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
> > partitions)
> > >         ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> > partitions)
> > >     ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> > partitions)
> > >
> > >
> > > In this case, my local machine and the build machine seem to act
> > > differently.
> > >
> > > On my local machine, what is in the inner ParallelCollection partition
> #2
> > > shows up in the MapPartitionsWithContextRDD as partition #2 still.  On
> > the
> > > build machine, this same partition shows up in the later RDD as
> partition
> > > #0 - presumably because everything else is pruned out, but that pruning
> > > should happen at an outer level, shouldn't it?
> > >
> > > Does anyone know why the build machine would act different from locally
> > > here?
> > >
> > > Also, sadly, this worked fine two days ago.
> > >
> > > My only thought is that perhaps the PullRequestBuilder does a merge
> with
> > > current code, and someone broke this in the last day or two?  Past
> that,
> > > I'm at a bit of a loss.
> > >
> > > Thanks,
> > >                     -Nathan
> > >
> > >
> > > --
> > >
> > > Nathan Kronenfeld
> > > Senior Visualization Developer
> > > Oculus Info Inc
> > > 2 Berkeley Street, Suite 600,
> > > Toronto, Ontario M5A 4J5
> > > Phone:  +1-416-203-3003 x 238
> > > Email:  nkronenfeld@oculusinfo.com
> > >
> >
> >
> >
> > --
> > Nathan Kronenfeld
> > Senior Visualization Developer
> > Oculus Info Inc
> > 2 Berkeley Street, Suite 600,
> > Toronto, Ontario M5A 4J5
> > Phone:  +1-416-203-3003 x 238
> > Email:  nkronenfeld@oculusinfo.com
> >
>



-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--001a11c2c8440045a704ebd7b70b--

From dev-return-778-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sat Nov 23 14:01:30 2013
Return-Path: <dev-return-778-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C274610CAF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sat, 23 Nov 2013 14:01:30 +0000 (UTC)
Received: (qmail 54644 invoked by uid 500); 23 Nov 2013 14:01:30 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 54439 invoked by uid 500); 23 Nov 2013 14:01:29 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 54430 invoked by uid 99); 23 Nov 2013 14:01:28 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Nov 2013 14:01:28 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.128.44 as permitted sender)
Received: from [209.85.128.44] (HELO mail-qe0-f44.google.com) (209.85.128.44)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sat, 23 Nov 2013 14:01:22 +0000
Received: by mail-qe0-f44.google.com with SMTP id nd7so1402235qeb.17
        for <dev@spark.incubator.apache.org>; Sat, 23 Nov 2013 06:01:01 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=EP8dHyEsAGqiDRxMlh4btiCkDGVdOo2kNVDUdPr5l7o=;
        b=iY/CPUOw8T7TuHbksP81q8pH229fpN7Dm8CjJINEoikV/QOEVi5Z3JOoB9PZ9tOcSN
         T5SqrLR8peHlrobOjJ0JWxKEoojWPwsYietS1r+0XOwyVKS/+cA+2ocMPE6sD6s79iAP
         Be4hvrmDbkcVL6HK+lSThY0oM/YRQPeFq1W4Y=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=EP8dHyEsAGqiDRxMlh4btiCkDGVdOo2kNVDUdPr5l7o=;
        b=iSrFFRnnvc4TcjhnsKYB7nW5kgD1QT3eRL4ex+OzRXP6XNONKjYejdROfiCvzEDilL
         668w6NU+uAFVc6zu2aymhBz762zdmAdHjSnOdzVYqkFI8EE7ltcJ5QBima68GYDaxSfz
         8Ps1G5KC3sTlUIqtIPitf11GUwvKS6HvtLk6CiSg6YxfqMu/cP9c5yBfEbF38yCaxIaU
         iHrkgvvRpfyv9G8B5WvAebxc5gtMrn3WI14mBp6m5HqTfGzpNS90oILyGsWZAJHPCnWj
         Q7GWYJ4/wCnbMCli+pg0Z0la+NBRVES55ssCKfCHGeXOqJrQX6mz4r3AvouK6qPWbp1P
         eIkQ==
X-Gm-Message-State: ALoCoQk8kTQYqcwBilzrjlglHwnN3Bdf2R42zHPBcupI5ALYJFGfUJiqe0xFSY1p9McRlUwaJtyX
MIME-Version: 1.0
X-Received: by 10.49.117.41 with SMTP id kb9mr30140744qeb.79.1385215261444;
 Sat, 23 Nov 2013 06:01:01 -0800 (PST)
Received: by 10.96.148.168 with HTTP; Sat, 23 Nov 2013 06:01:01 -0800 (PST)
In-Reply-To: <CAEpWh48wwEwLOx2H0ix2GWm=igMw-UKXtsnofsurz9pcpn=6Hg@mail.gmail.com>
References: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
	<CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
	<CAC1ssC42ZTKeTmjSiwx5wh8-k63W9XSY+WUdxrURVbXGQ3Opeg@mail.gmail.com>
	<CAEpWh48wwEwLOx2H0ix2GWm=igMw-UKXtsnofsurz9pcpn=6Hg@mail.gmail.com>
Date: Sat, 23 Nov 2013 09:01:01 -0500
Message-ID: <CAEpWh4_af-8RYdWR+MpJV8HS3fs+JnOg97rDYFwhC4cMbD42bw@mail.gmail.com>
Subject: Re: Problem with tests
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b6da55898a25d04ebd88f13
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b6da55898a25d04ebd88f13
Content-Type: text/plain; charset=ISO-8859-1

Though I think it's a more general problem...

Take the following:

val data = sc.parallelize(Range(0, 8), 2)
val data2 = data.mapPartitionsWithIndex((index, i) => i.map(x => (x,
index)))

data2.collect
  res0: Array[(Int, Int)] = Array((0,0), (1,0), (2,0), (3,0), (4,1), (5,1),
(6,1), (7,1))

new org.apache.spark.rdd.PartitionPruningRDD(data2, n => 1 == n).collect
  res1: Array[(Int, Int)] = Array((4,0), (5,0), (6,0), (7,0))

So, in this case, pruning the RDD has changed the data within it.  This
seems to be what is causing my errors.



On Sat, Nov 23, 2013 at 8:00 AM, Nathan Kronenfeld <
nkronenfeld@oculusinfo.com> wrote:

> https://github.com/apache/incubator-spark/pull/18
>
>
> On Fri, Nov 22, 2013 at 6:35 PM, Reynold Xin <reynoldx@gmail.com> wrote:
>
>> Can you provide a link to your pull request?
>>
>>
>> On Sat, Nov 23, 2013 at 5:02 AM, Nathan Kronenfeld <
>> nkronenfeld@oculusinfo.com> wrote:
>>
>> > Actually, looking into recent commits, it looks like my hunch may be
>> > exactly correct:
>> >
>> >
>> https://github.com/apache/incubator-spark/commit/f639b65eabcc8666b74af8f13a37c5fdf7e0185f
>> > "PartitionPruningRDD is using index from parent"
>> >
>> > Is there anyone who can explain why this new behavior is preferable?
>>  And,
>> > if it's staying, can suggest a way to fix my tests for this case?
>> >
>> > Thanks again,
>> >                  Nathan
>> >
>> >
>> > On Fri, Nov 22, 2013 at 3:56 PM, Nathan Kronenfeld <
>> > nkronenfeld@oculusinfo.com> wrote:
>> >
>> > > Hi there.
>> > >
>> > > I have a problem with the unit tests on a pull request I'm trying to
>> tie
>> > > up.  The changes deal with partition-related functions.
>> > >
>> > > In particular, the tests I have that test an append-to-partition
>> function
>> > > work fine on my own machine, but fail on the build machine (
>> > >
>> >
>> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
>> > > ).
>> > >
>> > > The failure seems to stem from pulling a single partition out of the
>> set.
>> > > In either case, when I work on the full dataset:
>> > >
>> > > UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
>> > >   UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
>> > >     ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
>> > partitions)
>> > >     MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265 (2
>> > partitions)
>> > >       ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
>> > partitions)
>> > >   ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
>> > partitions)
>> > >
>> > >
>> > > It seems to work.  When I pull one partition out of this, by wrapping
>> a
>> > PartitionPruningRDD around it (pruning out everything but partition 2):
>> > >
>> > > PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1 partitions)
>> > >   UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
>> > >     UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
>> > >       ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
>> > partitions)
>> > >       MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265
>> (2
>> > partitions)
>> > >         ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
>> > partitions)
>> > >     ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
>> > partitions)
>> > >
>> > >
>> > > In this case, my local machine and the build machine seem to act
>> > > differently.
>> > >
>> > > On my local machine, what is in the inner ParallelCollection
>> partition #2
>> > > shows up in the MapPartitionsWithContextRDD as partition #2 still.  On
>> > the
>> > > build machine, this same partition shows up in the later RDD as
>> partition
>> > > #0 - presumably because everything else is pruned out, but that
>> pruning
>> > > should happen at an outer level, shouldn't it?
>> > >
>> > > Does anyone know why the build machine would act different from
>> locally
>> > > here?
>> > >
>> > > Also, sadly, this worked fine two days ago.
>> > >
>> > > My only thought is that perhaps the PullRequestBuilder does a merge
>> with
>> > > current code, and someone broke this in the last day or two?  Past
>> that,
>> > > I'm at a bit of a loss.
>> > >
>> > > Thanks,
>> > >                     -Nathan
>> > >
>> > >
>> > > --
>> > >
>> > > Nathan Kronenfeld
>> > > Senior Visualization Developer
>> > > Oculus Info Inc
>> > > 2 Berkeley Street, Suite 600,
>> > > Toronto, Ontario M5A 4J5
>> > > Phone:  +1-416-203-3003 x 238
>> > > Email:  nkronenfeld@oculusinfo.com
>> > >
>> >
>> >
>> >
>> > --
>> > Nathan Kronenfeld
>> > Senior Visualization Developer
>> > Oculus Info Inc
>> > 2 Berkeley Street, Suite 600,
>> > Toronto, Ontario M5A 4J5
>> > Phone:  +1-416-203-3003 x 238
>> > Email:  nkronenfeld@oculusinfo.com
>> >
>>
>
>
>
> --
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com
>



-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--047d7b6da55898a25d04ebd88f13--

From dev-return-779-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Sun Nov 24 10:03:34 2013
Return-Path: <dev-return-779-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5B62910EFF
	for <apmail-spark-dev-archive@minotaur.apache.org>; Sun, 24 Nov 2013 10:03:34 +0000 (UTC)
Received: (qmail 7330 invoked by uid 500); 24 Nov 2013 10:03:33 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 7108 invoked by uid 500); 24 Nov 2013 10:03:29 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 7100 invoked by uid 99); 24 Nov 2013 10:03:27 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Nov 2013 10:03:27 +0000
X-ASF-Spam-Status: No, hits=-1998.3 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Sun, 24 Nov 2013 10:03:25 +0000
Received: (qmail 6641 invoked by uid 99); 24 Nov 2013 10:03:05 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Nov 2013 10:03:05 +0000
Received: from localhost (HELO mail-vc0-f180.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Sun, 24 Nov 2013 10:03:04 +0000
Received: by mail-vc0-f180.google.com with SMTP id if17so1945514vcb.25
        for <dev@spark.incubator.apache.org>; Sun, 24 Nov 2013 02:03:03 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=hFf4fS/Hfxw1NyJFnpbEUuaUy1gxCaIWOKtmGdOsrTk=;
        b=Dzep5XYqpFqX5o15LYzNjTmYVupMhlbUdCsIj38aFe5Rsu0qRbpQ2yWT51cwOtfSu4
         Heu3u7Gbt5+drrRkxOIoyaef3/6TYmVtUJ039cyEcu0fRTHA5mkviXn5g2Zi+fEzkyU4
         Z1WJQWvmzkA5S4iEhZJHcR4/ukbuu7EoKejEBCrMzGmABDCsauPEbdKO0XkAVKr85p2/
         qs+I9rWJavvlo/ZtGs9KcfOEP9scfAxMdgf0nArOwQHQVlmlU4DtURfhdinYbWGEHt63
         R7lX1qlqWPLP61+utzwZjFvZwtNWEzJsTQ5vkKH3v1LXTKVLiqeL0QEM609JrBATkezv
         lz4A==
X-Received: by 10.52.230.35 with SMTP id sv3mr119961vdc.27.1385287383621; Sun,
 24 Nov 2013 02:03:03 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Sun, 24 Nov 2013 02:02:43 -0800 (PST)
In-Reply-To: <CAEpWh4_af-8RYdWR+MpJV8HS3fs+JnOg97rDYFwhC4cMbD42bw@mail.gmail.com>
References: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
 <CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
 <CAC1ssC42ZTKeTmjSiwx5wh8-k63W9XSY+WUdxrURVbXGQ3Opeg@mail.gmail.com>
 <CAEpWh48wwEwLOx2H0ix2GWm=igMw-UKXtsnofsurz9pcpn=6Hg@mail.gmail.com> <CAEpWh4_af-8RYdWR+MpJV8HS3fs+JnOg97rDYFwhC4cMbD42bw@mail.gmail.com>
From: Reynold Xin <rxin@apache.org>
Date: Sun, 24 Nov 2013 18:02:43 +0800
Message-ID: <CAC1ssC4NMN=3RRQzEaGUPh0VqLh+2B0JZwydZsVc7dD_jnh2fA@mail.gmail.com>
Subject: Re: Problem with tests
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=089e0102fe6e69a8f604ebe95af9
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0102fe6e69a8f604ebe95af9
Content-Type: text/plain; charset=UTF-8

Take a look at this pull request and see if it fixes your problem:
https://github.com/apache/incubator-spark/pull/201

I changed the semantics of the index from the output partition index back
to the rdd partition index.



On Sat, Nov 23, 2013 at 10:01 PM, Nathan Kronenfeld <
nkronenfeld@oculusinfo.com> wrote:

> Though I think it's a more general problem...
>
> Take the following:
>
> val data = sc.parallelize(Range(0, 8), 2)
> val data2 = data.mapPartitionsWithIndex((index, i) => i.map(x => (x,
> index)))
>
> data2.collect
>   res0: Array[(Int, Int)] = Array((0,0), (1,0), (2,0), (3,0), (4,1), (5,1),
> (6,1), (7,1))
>
> new org.apache.spark.rdd.PartitionPruningRDD(data2, n => 1 == n).collect
>   res1: Array[(Int, Int)] = Array((4,0), (5,0), (6,0), (7,0))
>
> So, in this case, pruning the RDD has changed the data within it.  This
> seems to be what is causing my errors.
>
>
>
> On Sat, Nov 23, 2013 at 8:00 AM, Nathan Kronenfeld <
> nkronenfeld@oculusinfo.com> wrote:
>
> > https://github.com/apache/incubator-spark/pull/18
> >
> >
> > On Fri, Nov 22, 2013 at 6:35 PM, Reynold Xin <reynoldx@gmail.com> wrote:
> >
> >> Can you provide a link to your pull request?
> >>
> >>
> >> On Sat, Nov 23, 2013 at 5:02 AM, Nathan Kronenfeld <
> >> nkronenfeld@oculusinfo.com> wrote:
> >>
> >> > Actually, looking into recent commits, it looks like my hunch may be
> >> > exactly correct:
> >> >
> >> >
> >>
> https://github.com/apache/incubator-spark/commit/f639b65eabcc8666b74af8f13a37c5fdf7e0185f
> >> > "PartitionPruningRDD is using index from parent"
> >> >
> >> > Is there anyone who can explain why this new behavior is preferable?
> >>  And,
> >> > if it's staying, can suggest a way to fix my tests for this case?
> >> >
> >> > Thanks again,
> >> >                  Nathan
> >> >
> >> >
> >> > On Fri, Nov 22, 2013 at 3:56 PM, Nathan Kronenfeld <
> >> > nkronenfeld@oculusinfo.com> wrote:
> >> >
> >> > > Hi there.
> >> > >
> >> > > I have a problem with the unit tests on a pull request I'm trying to
> >> tie
> >> > > up.  The changes deal with partition-related functions.
> >> > >
> >> > > In particular, the tests I have that test an append-to-partition
> >> function
> >> > > work fine on my own machine, but fail on the build machine (
> >> > >
> >> >
> >>
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
> >> > > ).
> >> > >
> >> > > The failure seems to stem from pulling a single partition out of the
> >> set.
> >> > > In either case, when I work on the full dataset:
> >> > >
> >> > > UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> >> > >   UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> >> > >     ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> >> > partitions)
> >> > >     MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265
> (2
> >> > partitions)
> >> > >       ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> >> > partitions)
> >> > >   ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> >> > partitions)
> >> > >
> >> > >
> >> > > It seems to work.  When I pull one partition out of this, by
> wrapping
> >> a
> >> > PartitionPruningRDD around it (pruning out everything but partition
> 2):
> >> > >
> >> > > PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1
> partitions)
> >> > >   UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> >> > >     UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> >> > >       ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> >> > partitions)
> >> > >       MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265
> >> (2
> >> > partitions)
> >> > >         ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> >> > partitions)
> >> > >     ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> >> > partitions)
> >> > >
> >> > >
> >> > > In this case, my local machine and the build machine seem to act
> >> > > differently.
> >> > >
> >> > > On my local machine, what is in the inner ParallelCollection
> >> partition #2
> >> > > shows up in the MapPartitionsWithContextRDD as partition #2 still.
>  On
> >> > the
> >> > > build machine, this same partition shows up in the later RDD as
> >> partition
> >> > > #0 - presumably because everything else is pruned out, but that
> >> pruning
> >> > > should happen at an outer level, shouldn't it?
> >> > >
> >> > > Does anyone know why the build machine would act different from
> >> locally
> >> > > here?
> >> > >
> >> > > Also, sadly, this worked fine two days ago.
> >> > >
> >> > > My only thought is that perhaps the PullRequestBuilder does a merge
> >> with
> >> > > current code, and someone broke this in the last day or two?  Past
> >> that,
> >> > > I'm at a bit of a loss.
> >> > >
> >> > > Thanks,
> >> > >                     -Nathan
> >> > >
> >> > >
> >> > > --
> >> > >
> >> > > Nathan Kronenfeld
> >> > > Senior Visualization Developer
> >> > > Oculus Info Inc
> >> > > 2 Berkeley Street, Suite 600,
> >> > > Toronto, Ontario M5A 4J5
> >> > > Phone:  +1-416-203-3003 x 238
> >> > > Email:  nkronenfeld@oculusinfo.com
> >> > >
> >> >
> >> >
> >> >
> >> > --
> >> > Nathan Kronenfeld
> >> > Senior Visualization Developer
> >> > Oculus Info Inc
> >> > 2 Berkeley Street, Suite 600,
> >> > Toronto, Ontario M5A 4J5
> >> > Phone:  +1-416-203-3003 x 238
> >> > Email:  nkronenfeld@oculusinfo.com
> >> >
> >>
> >
> >
> >
> > --
> > Nathan Kronenfeld
> > Senior Visualization Developer
> > Oculus Info Inc
> > 2 Berkeley Street, Suite 600,
> > Toronto, Ontario M5A 4J5
> > Phone:  +1-416-203-3003 x 238
> > Email:  nkronenfeld@oculusinfo.com
> >
>
>
>
> --
> Nathan Kronenfeld
> Senior Visualization Developer
> Oculus Info Inc
> 2 Berkeley Street, Suite 600,
> Toronto, Ontario M5A 4J5
> Phone:  +1-416-203-3003 x 238
> Email:  nkronenfeld@oculusinfo.com
>

--089e0102fe6e69a8f604ebe95af9--

From dev-return-780-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Mon Nov 25 21:24:23 2013
Return-Path: <dev-return-780-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 648411071A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 25 Nov 2013 21:24:23 +0000 (UTC)
Received: (qmail 51018 invoked by uid 500); 25 Nov 2013 21:24:23 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 50982 invoked by uid 500); 25 Nov 2013 21:24:22 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 50974 invoked by uid 99); 25 Nov 2013 21:24:22 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Nov 2013 21:24:22 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of umarj.javed@gmail.com designates 209.85.220.178 as permitted sender)
Received: from [209.85.220.178] (HELO mail-vc0-f178.google.com) (209.85.220.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 25 Nov 2013 21:24:16 +0000
Received: by mail-vc0-f178.google.com with SMTP id lh4so3147695vcb.23
        for <dev@spark.incubator.apache.org>; Mon, 25 Nov 2013 13:23:56 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=qTwJTIM/2ZVd7vIqMkcjdb4uV0hc4JwEBmSzuBRusHo=;
        b=S7hDi2CduTmqFXbeb0OQE1ugX+Kf+67vZhqYdpOeO3gE9rg+cZ4OtMP45EwhfA7xIJ
         7Jw6P2FfXust1yHo0YrsjVqYOg6GbDmOidigHBldhCfJQ7bvnCwHDqoxx2RX8OCz81Zd
         4leBhoaavI/vqI/JjKg8RSORSEwec0wB/aqpLvKCbHsIi/YbkOZ8rhsxuNOZqWCEqCzJ
         3d7qwrExB6tHE1DIde5R/7uWDkrsB2a44YZD1c49Wuc6v+dh3WjsXcd0mnX3xi44mtNv
         zd0t25cBw+XgC5qYCJZg8OYGiDDtjcjhTmdWKwf2VJgDsM3hZoKk94PgzxlMHFYkzZxh
         smnA==
MIME-Version: 1.0
X-Received: by 10.58.117.7 with SMTP id ka7mr50289veb.44.1385414635928; Mon,
 25 Nov 2013 13:23:55 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Mon, 25 Nov 2013 13:23:55 -0800 (PST)
In-Reply-To: <CACwKa9eFKr79kRqj+xu9zvpEwCpKZ0Xc08TD9LN-oxBdJHWQug@mail.gmail.com>
References: <CACwKa9eFKr79kRqj+xu9zvpEwCpKZ0Xc08TD9LN-oxBdJHWQug@mail.gmail.com>
Date: Mon, 25 Nov 2013 13:23:55 -0800
Message-ID: <CACwKa9ewg0q=-FuYZfZkS7dG9fDsR5jZtQU0gU66xitCnQd8Sw@mail.gmail.com>
Subject: Re: difference between 'fetchWaitTime' and 'remoteFetchTime'
From: Umar Javed <umarj.javed@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b86f1303dc0b204ec06fbac
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b86f1303dc0b204ec06fbac
Content-Type: text/plain; charset=ISO-8859-1

Any clarification on this? thanks.


On Wed, Nov 20, 2013 at 3:02 PM, Umar Javed <umarj.javed@gmail.com> wrote:

> In the class ShuffleReadMetrics in executor/TaskMetrics.scala, there are
> two variables:
>
> 1) fetchWaitTime: /**
>
>
>    * Total time that is spent blocked waiting for shuffle to fetch data
>
>
>    */
>
> 2) remoteFetchTime
>
> /**
>
>
>    * The total amount of time for all the shuffle fetches.  This adds up
> time from overlapping
>
>    *     shuffles, so can be longer than task time
>
>
>    */
>
> As I understand it, the difference between these two is that fetchWaitTime
> is remoteFetchTime without the overlapped time counted exactly once. Is
> that right? Can somebody explain the difference better?
>
> thanks!
>

--047d7b86f1303dc0b204ec06fbac--

From dev-return-781-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 26 06:56:30 2013
Return-Path: <dev-return-781-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id B0DF51094C
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Nov 2013 06:56:30 +0000 (UTC)
Received: (qmail 14073 invoked by uid 500); 26 Nov 2013 06:56:29 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13876 invoked by uid 500); 26 Nov 2013 06:56:24 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 13868 invoked by uid 99); 26 Nov 2013 06:56:23 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Nov 2013 06:56:23 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of pwendell@gmail.com designates 209.85.219.54 as permitted sender)
Received: from [209.85.219.54] (HELO mail-oa0-f54.google.com) (209.85.219.54)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Nov 2013 06:56:19 +0000
Received: by mail-oa0-f54.google.com with SMTP id h16so5636731oag.27
        for <dev@spark.incubator.apache.org>; Mon, 25 Nov 2013 22:55:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=yZ1zC2od04vwY6AVOMku4FYAtz+3aqJSIheXjzrZMJ0=;
        b=FBbOw3Ve0DG4Qtn3b0o5dO0a9mpAc51FiX0EOJFMTkhROlCGM6xxpjtL6ayilLy6UI
         XHcY74DZtmDsLpcc7sDy2QSo8VaeyD/srsvt6ZHDv3j1PS58qergbebDCNdaXpO68nSz
         9RZFb8LYyc0SdzL7cKlVIcuupX/+dCsnVvbvrVeLiVvVD+ZO84HD8uDzp7D+W26s9Sil
         i97YxHACI4NAEBGKW0uas0NHyqMbxP5+9gvyx/3ZAHgBvzfwrGCfW3jdJWwEhsPjlmVg
         GYbhPxjHA+EceZ/WFW9bh3+DwvH1F22UTB07Zs+EiInEHwZWUsnSe5Pwf1hl6ZpbL2ve
         BA5g==
MIME-Version: 1.0
X-Received: by 10.182.135.165 with SMTP id pt5mr142125obb.66.1385448957724;
 Mon, 25 Nov 2013 22:55:57 -0800 (PST)
Received: by 10.182.132.46 with HTTP; Mon, 25 Nov 2013 22:55:57 -0800 (PST)
In-Reply-To: <CACwKa9ewg0q=-FuYZfZkS7dG9fDsR5jZtQU0gU66xitCnQd8Sw@mail.gmail.com>
References: <CACwKa9eFKr79kRqj+xu9zvpEwCpKZ0Xc08TD9LN-oxBdJHWQug@mail.gmail.com>
	<CACwKa9ewg0q=-FuYZfZkS7dG9fDsR5jZtQU0gU66xitCnQd8Sw@mail.gmail.com>
Date: Mon, 25 Nov 2013 22:55:57 -0800
Message-ID: <CABPQxss3XdBJMdE96=YHOXt8-Y8L=OkQTHRL5ziXZGv+CDoo5Q@mail.gmail.com>
Subject: Re: difference between 'fetchWaitTime' and 'remoteFetchTime'
From: Patrick Wendell <pwendell@gmail.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

Hey Umar,

I dug into this a bit today out of curiosity since I also wasn't sure.

I updated the in-line documentation here:
https://github.com/apache/incubator-spark/pull/209/files

The more important metric is `fetchWaitTime` which indicates how much
of the task runtime was spent waiting for input data.

remoteFetchTime is an aggregation of all of the fetch delays for each
block... this second metric is a bit more convoluted because those
fetches can actually overlap, so if this is high it doesn't
necessarily indicate any latency hit.

- Patrick

On Mon, Nov 25, 2013 at 1:23 PM, Umar Javed <umarj.javed@gmail.com> wrote:
> Any clarification on this? thanks.
>
>
> On Wed, Nov 20, 2013 at 3:02 PM, Umar Javed <umarj.javed@gmail.com> wrote:
>
>> In the class ShuffleReadMetrics in executor/TaskMetrics.scala, there are
>> two variables:
>>
>> 1) fetchWaitTime: /**
>>
>>
>>    * Total time that is spent blocked waiting for shuffle to fetch data
>>
>>
>>    */
>>
>> 2) remoteFetchTime
>>
>> /**
>>
>>
>>    * The total amount of time for all the shuffle fetches.  This adds up
>> time from overlapping
>>
>>    *     shuffles, so can be longer than task time
>>
>>
>>    */
>>
>> As I understand it, the difference between these two is that fetchWaitTime
>> is remoteFetchTime without the overlapped time counted exactly once. Is
>> that right? Can somebody explain the difference better?
>>
>> thanks!
>>

From dev-return-782-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Tue Nov 26 09:49:11 2013
Return-Path: <dev-return-782-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 5116A10D60
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 26 Nov 2013 09:49:11 +0000 (UTC)
Received: (qmail 48932 invoked by uid 500); 26 Nov 2013 09:48:51 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 48651 invoked by uid 500); 26 Nov 2013 09:48:39 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 48599 invoked by uid 99); 26 Nov 2013 09:48:35 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Nov 2013 09:48:35 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of umarj.javed@gmail.com designates 209.85.220.175 as permitted sender)
Received: from [209.85.220.175] (HELO mail-vc0-f175.google.com) (209.85.220.175)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 26 Nov 2013 09:48:29 +0000
Received: by mail-vc0-f175.google.com with SMTP id ld13so3571638vcb.20
        for <dev@spark.incubator.apache.org>; Tue, 26 Nov 2013 01:48:08 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=6S/vCimU0SgJvVFoFIallxKQU1lF30BxskymiYozWM0=;
        b=mtMrP3O0ActFESZxlo/tcx/JYnXQD0+43kGmiTKenwJRH3zQjp/Njlb7FhljgeLcR4
         Re8nm2Cpr5OR1+fXuUZTOFV28IXa839JsljA+6jgAHBGOqwOKXWFZMR7Gsa1n9rhF5gx
         rGx7K+4opG9EJmE7db+Az69Aiu1S47ugspegeSPZVgmdlXcc7ufYOX6hTfOcVJhvsX32
         NzGDBPwrjPmLeK2Ck7MK+Pp1Ae4BLVLZ3JGKtsin+TMPyBSkMkDttlEutDWAtu1tpn8l
         y+UF4qLqjFhTtC4Jc/DbRZbrNyKT3xdMT9O9X6zL+lxW9Mot98TE8wQfaJNbVT0lAMXS
         nIYg==
MIME-Version: 1.0
X-Received: by 10.52.164.203 with SMTP id ys11mr798963vdb.37.1385459288508;
 Tue, 26 Nov 2013 01:48:08 -0800 (PST)
Received: by 10.220.72.73 with HTTP; Tue, 26 Nov 2013 01:48:08 -0800 (PST)
In-Reply-To: <CABPQxss3XdBJMdE96=YHOXt8-Y8L=OkQTHRL5ziXZGv+CDoo5Q@mail.gmail.com>
References: <CACwKa9eFKr79kRqj+xu9zvpEwCpKZ0Xc08TD9LN-oxBdJHWQug@mail.gmail.com>
	<CACwKa9ewg0q=-FuYZfZkS7dG9fDsR5jZtQU0gU66xitCnQd8Sw@mail.gmail.com>
	<CABPQxss3XdBJMdE96=YHOXt8-Y8L=OkQTHRL5ziXZGv+CDoo5Q@mail.gmail.com>
Date: Tue, 26 Nov 2013 01:48:08 -0800
Message-ID: <CACwKa9fKkuUjRFwFvYkrRnLFq-jqQoB0y_mbew4+DkBBa1Hzew@mail.gmail.com>
Subject: Re: difference between 'fetchWaitTime' and 'remoteFetchTime'
From: Umar Javed <umarj.javed@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c2cc40be0e1204ec1160f0
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2cc40be0e1204ec1160f0
Content-Type: text/plain; charset=ISO-8859-1

Patrick: inline


On Mon, Nov 25, 2013 at 10:55 PM, Patrick Wendell <pwendell@gmail.com>wrote:

> Hey Umar,
>
> I dug into this a bit today out of curiosity since I also wasn't sure.
>
> I updated the in-line documentation here:
> https://github.com/apache/incubator-spark/pull/209/files
>
> The more important metric is `fetchWaitTime` which indicates how much
> of the task runtime was spent waiting for input data.
>
> remoteFetchTime is an aggregation of all of the fetch delays for each
> block... this second metric is a bit more convoluted because those
> fetches can actually overlap, so if this is high it doesn't
> necessarily indicate any latency hit.
>

Instead of each block, I think it is each fetch request. A fetch request
can ask for multiple blocks. I don't remember off the top of my head when a
second request is made, but I think there is a max limit on the number of
blocks a fetch can ask for. The result is that if all required fetch blocks
are asked for in the same request, 'remoteFetchTime' is the time the task
is blocked over network requests.

>
> - Patrick
>
> On Mon, Nov 25, 2013 at 1:23 PM, Umar Javed <umarj.javed@gmail.com> wrote:
> > Any clarification on this? thanks.
> >
> >
> > On Wed, Nov 20, 2013 at 3:02 PM, Umar Javed <umarj.javed@gmail.com>
> wrote:
> >
> >> In the class ShuffleReadMetrics in executor/TaskMetrics.scala, there are
> >> two variables:
> >>
> >> 1) fetchWaitTime: /**
> >>
> >>
> >>    * Total time that is spent blocked waiting for shuffle to fetch data
> >>
> >>
> >>    */
> >>
> >> 2) remoteFetchTime
> >>
> >> /**
> >>
> >>
> >>    * The total amount of time for all the shuffle fetches.  This adds up
> >> time from overlapping
> >>
> >>    *     shuffles, so can be longer than task time
> >>
> >>
> >>    */
> >>
> >> As I understand it, the difference between these two is that
> fetchWaitTime
> >> is remoteFetchTime without the overlapped time counted exactly once. Is
> >> that right? Can somebody explain the difference better?
> >>
> >> thanks!
> >>
>

--001a11c2cc40be0e1204ec1160f0--

From dev-return-783-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 27 00:12:59 2013
Return-Path: <dev-return-783-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 9DFBA10034
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Nov 2013 00:12:59 +0000 (UTC)
Received: (qmail 62001 invoked by uid 500); 27 Nov 2013 00:12:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 61854 invoked by uid 500); 27 Nov 2013 00:12:58 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 61846 invoked by uid 99); 27 Nov 2013 00:12:58 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 00:12:58 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_IMAGE_ONLY_32,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS,T_REMOTE_IMAGE
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.220.180 as permitted sender)
Received: from [209.85.220.180] (HELO mail-vc0-f180.google.com) (209.85.220.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 00:12:55 +0000
Received: by mail-vc0-f180.google.com with SMTP id if17so4464950vcb.11
        for <dev@spark.incubator.apache.org>; Tue, 26 Nov 2013 16:12:34 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=Tn4XaZe8Vyvg9ob2mSODgcojuGEgpNfVbV1JhqTt1WA=;
        b=kFZRlqHoI1uwdLhxgFEou45Z9AbujZfDDYVJnsVnBrk2fu5Maw3dPsvo4xvXkfuuhf
         p5PT6xogPSg/lso8r4oH5HFzOap9cSkvvlUFF/Cgg5rno6zDvzujqFpNyugQgD96VbFc
         HBwyt+ShoFolYLLRVnd052AQ/aJJIG8irDegI=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=Tn4XaZe8Vyvg9ob2mSODgcojuGEgpNfVbV1JhqTt1WA=;
        b=EbcqTe7CP9Nrfv7TVj6nUOj+ipsj3jD4XeKkHceKegPzOHSnr3YUOXqT9Mib4c23A1
         QdtVm58pSZro5ZrwbwyQMEqGQDrOcoVFJ+PgwqsaxtVm32qIxyKmMydjo11Jd74hkaY9
         DwGKYYzxrDMEraxu3FZPnqTv7XmIRLzYvR8pbNNcF05KvT1zEeL3dEMArkh4ajKYRR/K
         UQkKwR8bvqdC3htFe1T6McUv4MDZCSxaV8zHuaYtmfl/zLtVXV0LK94JYCd8WXvKDxAt
         PAAERpCK46uW8LRrDl6MqBblgjr2KnigNBrh/UAU+Gp1etg/xmSxs9sRPgjeJDtCqrKK
         mN6A==
X-Gm-Message-State: ALoCoQnjvh0nN0oJR296FpmzPG2qG/U/C2iUqkUGxhwjVSUfr1LUCT/hoBRl5Unf0uvcLnzhaVyA
MIME-Version: 1.0
X-Received: by 10.220.95.139 with SMTP id d11mr8994371vcn.21.1385511154069;
 Tue, 26 Nov 2013 16:12:34 -0800 (PST)
Received: by 10.220.17.193 with HTTP; Tue, 26 Nov 2013 16:12:33 -0800 (PST)
In-Reply-To: <CACwKa9f-CyLAfWvZnqw+yY_+d_H8KHBr3b2_O5nOdVAy98S60w@mail.gmail.com>
References: <CACwKa9f-CyLAfWvZnqw+yY_+d_H8KHBr3b2_O5nOdVAy98S60w@mail.gmail.com>
Date: Tue, 26 Nov 2013 16:12:33 -0800
Message-ID: <CADWPM3iq=q3SzCNM6M4UGEPhjk9P2gfzKTRSVeZ2DuVje3vTng@mail.gmail.com>
Subject: Re: time taken to fetch input partition by map
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Cc: user@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c2ae762bc57d04ec1d74f4
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c2ae762bc57d04ec1d74f4
Content-Type: text/plain; charset=ISO-8859-1

Hi Umar,

It's fine to look into hooking into HadoopRDD, though I think we need a
general purpose way to provide metrics and progress for non-Hadoop RDDs (ie
RDDs that aren't based on an InputFormat).   Any ideas would be great.  :)

-Evan



On Wed, Nov 20, 2013 at 9:20 PM, Umar Javed <umarj.javed@gmail.com> wrote:

> Hi,
>
> The metrics provide information for the reduce (i.e. shuffleReaders) tasks
> about the time taken to fetch the shuffle outputs. Is there a way I can
> find out the the time taken by a map task (ie shuffleWriter) on a remote
> machine to read its input partition from disk?
>
> I believe I should look in HadoopRDD.scala where there is the
> getRecordReader, and the headers show that it should be
> in org.apache.hadoop.mapred.RecordReader, but I can't find that file
> anywhere.
>
> Any help would be appreciated.
>
> thanks!
> Umar
>



-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

<http://www.ooyala.com/>
<http://www.facebook.com/ooyala><http://www.linkedin.com/company/ooyala><http://www.twitter.com/ooyala>

--001a11c2ae762bc57d04ec1d74f4--

From dev-return-784-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 27 06:49:21 2013
Return-Path: <dev-return-784-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 6AD481096B
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Nov 2013 06:49:21 +0000 (UTC)
Received: (qmail 75931 invoked by uid 500); 27 Nov 2013 06:49:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 75874 invoked by uid 500); 27 Nov 2013 06:49:15 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 75861 invoked by uid 99); 27 Nov 2013 06:49:14 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 06:49:14 +0000
X-ASF-Spam-Status: No, hits=2.5 required=5.0
	tests=FREEMAIL_REPLY,HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of nick.pentreath@gmail.com designates 209.85.214.178 as permitted sender)
Received: from [209.85.214.178] (HELO mail-ob0-f178.google.com) (209.85.214.178)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 06:49:08 +0000
Received: by mail-ob0-f178.google.com with SMTP id uz6so7038194obc.9
        for <dev@spark.incubator.apache.org>; Tue, 26 Nov 2013 22:48:47 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :cc:content-type;
        bh=H0jJMxwPEtRuSkgZBMCb8HtXOZeL3KmmDsffrMYBCkA=;
        b=yh9w5d+g3CswVoaJV6qoQL88j8/7bUPsi62CgNoZjIWrzM0dLI3yZ4Dwggnp8tQT0c
         EcF2nXpLOottKQUEsBuL45wEaimW9/xY1YTeSO7ZYFIJtdb3h1QXW55ES4Q9bMy8W4Rk
         X7OK/Lra0b0iRhl32/IeRgFrzF8jhl/7cuuPaKCVtlgHKO+5rLdDtGLjhWbqjzYOe8ER
         dhYOjEYDANJDZKEAiyQsjHcl55/19EA2uN6Z4KB7wgcH6gD+CF0SOq+CxVb/8R19f4az
         xPnW0PzsqpojrTuQaO+1+FEOd4N7+uutbeaq5QY6bV7gVDpIN3ly/Vk9EwG44ZTvPneO
         KuYQ==
MIME-Version: 1.0
X-Received: by 10.60.117.38 with SMTP id kb6mr32640890oeb.7.1385534927390;
 Tue, 26 Nov 2013 22:48:47 -0800 (PST)
Received: by 10.182.48.131 with HTTP; Tue, 26 Nov 2013 22:48:47 -0800 (PST)
In-Reply-To: <CAOc9b+GWqsvcSka1rFkyfLxhmw6qK=wNNQPXCRAdf5CsjGUEjw@mail.gmail.com>
References: <CAOc9b+GWqsvcSka1rFkyfLxhmw6qK=wNNQPXCRAdf5CsjGUEjw@mail.gmail.com>
Date: Wed, 27 Nov 2013 08:48:47 +0200
Message-ID: <CALD+6GNMHbrDCg-0RnJzz=JsVkRvNvEG=pNc2MNcqv6Y6Fyo_g@mail.gmail.com>
Subject: Re: [Scikit-learn-general] Spark-backed implementations of
 scikit-learn estimators
From: Nick Pentreath <nick.pentreath@gmail.com>
To: scikit-learn-general@lists.sourceforge.net
Cc: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b414d242bcee904ec22fdf0
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b414d242bcee904ec22fdf0
Content-Type: text/plain; charset=ISO-8859-1

CC'ing Spark Dev list

I have been thinking about this for quite a while and would really love to
see this happen.

Most of my pipeline ends up in Scala/Spark these days - which I love, but
it is partly because I am reliant on custom Hadoop input formats that are
just way easier to use from Scala/Java - but I still use Python a lot for
data analysis and interactive work. There is some good stuff happening with
Breeze in Scala and MLlib in Spark (and IScala) but the breadth just
doesn't compare as yet - not to mention IPython and plotting!

There is a PR that was just merged into PySpark to allow arbitrary
serialization protocols between the Java and Python layers. I hope to try
to use this to allow PySpark users to pull data from arbitrary Hadoop
InputFormats with minimum fuss. This I believe will open the way for many
(including me!) to use PySpark directly for virtually all distributed data
processing without "needing" to use Java (
https://github.com/apache/incubator-spark/pull/146) (
http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201311.mbox/browser
).

Linked to this is what I believe is huge potential to add distributed
PySpark versions of many algorithms in scikit-learn (and elsewhere). The
idea as intimated above, would be to have estimator classes with sklearn
compatible APIs. They may in turn use sklearn algorithms themselves (eg:
this shows how easy it would be for linear models:
https://gist.github.com/MLnick/4707012).

I'd be very happy to try to find some time to work on such a library (I had
started one in Scala that was going to contain a Python library also, but
I've just not had the time available and with Spark MLlib appearing and the
Hadoop stuff I had what I needed for my systems).

The main benefit I see is that sklearn already has:
- many algorithms to work with
- great, simple API
- very useful stuff like preprocessing, vectorizing and feature hashing
(very important for large scale linear models)
- obviously the nice Python ecosystem stuff like plotting, IPython
notebook, pandas, scikit-statsmodels and so on.

The easiest place to start in my opinion is to take a few of the basic
models in the PySpark examples and turn them into production-ready code
that utilises sklearn or other good libraries as much as possible.

(I think this library would live outside of both Spark and sklearn, at
least until it is clear where it should live).

I would be happy to help and provide Spark-related advice even if I cannot
find enough time to work on many algorithms. Though I do hope to find more
time toward the end of the year and early next year.

N


On Wed, Nov 27, 2013 at 12:42 AM, Uri Laserson <uri.laserson@gmail.com>wrote:

> Hi all,
>
> I was wondering whether there has been any organized effort to create
> scikit-learn estimators that are backed by Spark clusters.  Rather than
> using the PySpark API to call sklearn functions, you would instantiate
> sklearn estimators that end up calling PySpark functionality in their
> .fit() methods.
>
> Uri
>
> ......................................
> Uri Laserson
> +1 617 910 0447
> uri.laserson@gmail.com
>
>
> ------------------------------------------------------------------------------
> Rapidly troubleshoot problems before they affect your business. Most IT
> organizations don't have a clear picture of how application performance
> affects their revenue. With AppDynamics, you get 100% visibility into your
> Java,.NET, & PHP application. Start your 15-day FREE TRIAL of AppDynamics
> Pro!
> http://pubads.g.doubleclick.net/gampad/clk?id=84349351&iu=/4140/ostg.clktrk
> _______________________________________________
> Scikit-learn-general mailing list
> Scikit-learn-general@lists.sourceforge.net
> https://lists.sourceforge.net/lists/listinfo/scikit-learn-general
>
>

--047d7b414d242bcee904ec22fdf0--

From dev-return-785-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 27 08:40:59 2013
Return-Path: <dev-return-785-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CA77810BD8
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Nov 2013 08:40:59 +0000 (UTC)
Received: (qmail 33544 invoked by uid 500); 27 Nov 2013 08:40:59 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 33413 invoked by uid 500); 27 Nov 2013 08:40:57 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Delivered-To: moderator for dev@spark.incubator.apache.org
Received: (qmail 21921 invoked by uid 99); 27 Nov 2013 08:35:08 -0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of olivier.grisel@gmail.com designates 74.125.82.54 as permitted sender)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:sender:in-reply-to:references:from:date:message-id
         :subject:to:cc:content-type;
        bh=YbZwXndVQYpL7iLm7xI4Uq7IXO1OsKJnZr+g3KHEJIU=;
        b=AsEpD+jOQmYcfQg3/Bbd4ZmpUP5ZZ9GgHM+ILFRCMaxIUNgWZUsUveuEa+xiFno0AE
         LfSt9jkCXWO58yzDWWbTYifoYWkxu4/jNLjX+6Ni5VdSHzigl9ybGonWd3CYuQQGVdvQ
         bS7OLbnpKhxXabi+LtO2ue8M/SEKA/R80pq/HT0wZGwMqFNETDVrK3Os7xRKbnfnQUH5
         /WrHXX8qsxEyFA2f9OnBAQFrlCZHO0xGIl7jMFW+4W4+ji1bdsc56MKHHrCyvJtrU2v9
         eCbt0I5pwrcdojGVbmfTb+YD9Zqh4c98vJZoNbFA/0h9q8qk26f3z2LyKvppICIomHie
         nrWg==
X-Received: by 10.194.57.243 with SMTP id l19mr1306389wjq.54.1385541281928;
 Wed, 27 Nov 2013 00:34:41 -0800 (PST)
MIME-Version: 1.0
Sender: olivier.grisel@gmail.com
In-Reply-To: <CALD+6GNMHbrDCg-0RnJzz=JsVkRvNvEG=pNc2MNcqv6Y6Fyo_g@mail.gmail.com>
References: <CAOc9b+GWqsvcSka1rFkyfLxhmw6qK=wNNQPXCRAdf5CsjGUEjw@mail.gmail.com>
 <CALD+6GNMHbrDCg-0RnJzz=JsVkRvNvEG=pNc2MNcqv6Y6Fyo_g@mail.gmail.com>
From: Olivier Grisel <olivier.grisel@ensta.org>
Date: Wed, 27 Nov 2013 09:34:21 +0100
X-Google-Sender-Auth: Vi8ByGxm9UBpkjfhu_41lw-99Vo
Message-ID: <CAFvE7K5xHbnWwg+ieGEDQ0y6S_0GeNCexOAo11BPvjmUL1aODA@mail.gmail.com>
Subject: Re: [Scikit-learn-general] Spark-backed implementations of
 scikit-learn estimators
To: scikit-learn-general <scikit-learn-general@lists.sourceforge.net>
Cc: dev@spark.incubator.apache.org
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

>013/11/27 Nick Pentreath <nick.pentreath@gmail.com>:
> CC'ing Spark Dev list
>
> I have been thinking about this for quite a while and would really love to
> see this happen.
>
> Most of my pipeline ends up in Scala/Spark these days - which I love, but it
> is partly because I am reliant on custom Hadoop input formats that are just
> way easier to use from Scala/Java - but I still use Python a lot for data
> analysis and interactive work. There is some good stuff happening with
> Breeze in Scala and MLlib in Spark (and IScala) but the breadth just doesn't
> compare as yet - not to mention IPython and plotting!
>
> There is a PR that was just merged into PySpark to allow arbitrary
> serialization protocols between the Java and Python layers. I hope to try to
> use this to allow PySpark users to pull data from arbitrary Hadoop
> InputFormats with minimum fuss. This I believe will open the way for many
> (including me!) to use PySpark directly for virtually all distributed data
> processing without "needing" to use Java
> (https://github.com/apache/incubator-spark/pull/146)
> (http://mail-archives.apache.org/mod_mbox/incubator-spark-dev/201311.mbox/browser).

This is very interesting, thanks for the heads up.


-- 
Olivier
http://twitter.com/ogrisel - http://github.com/ogrisel

From dev-return-786-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 27 15:23:08 2013
Return-Path: <dev-return-786-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 30DF510A1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Nov 2013 15:23:08 +0000 (UTC)
Received: (qmail 38275 invoked by uid 500); 27 Nov 2013 15:23:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 38219 invoked by uid 500); 27 Nov 2013 15:22:52 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 38207 invoked by uid 99); 27 Nov 2013 15:22:51 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 15:22:51 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zuhair.khayyat@gmail.com designates 209.85.212.42 as permitted sender)
Received: from [209.85.212.42] (HELO mail-vb0-f42.google.com) (209.85.212.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 15:22:44 +0000
Received: by mail-vb0-f42.google.com with SMTP id w18so5032225vbj.15
        for <dev@spark.incubator.apache.org>; Wed, 27 Nov 2013 07:22:23 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:date:message-id:subject:from:to:content-type;
        bh=sIzTpT4YEzkA5OnthOWU/WziVmx8Gal6DpaZGDZR0vU=;
        b=q6BZxfIjjbjXCm7gqxaDahFn+PpKGsW/Ix9a1VDGz+bFbTQUQ+U97ljVAJa9KvJIoP
         tbRAJQIe7V2MCjC0Q3JWmUvkMk5yY5xDk4QIlO/KugTPevAxGZFbwfIpGwc7ee17hAuI
         BCsjDnqxbvrlAZaVXlC9684W4QeYfLfYcIpEtWw3q5UItw4u5wsE/EazBqoSw+xRk657
         xnhuDVFAV8nnAB1ZYAJuAOTy70dlL08BO2EdMrRwPDPikVCBNvXTRQOKMDJXNJlRYAnt
         Rp9qpn94i9Na68zXfftrBL47ecJeEfQchy9KbEJtR1mN3u7SAq2Th8kguJ53U0lwXhXw
         DT6A==
MIME-Version: 1.0
X-Received: by 10.58.46.18 with SMTP id r18mr35210408vem.4.1385565743787; Wed,
 27 Nov 2013 07:22:23 -0800 (PST)
Received: by 10.221.49.134 with HTTP; Wed, 27 Nov 2013 07:22:23 -0800 (PST)
Date: Wed, 27 Nov 2013 18:22:23 +0300
Message-ID: <CAGgBT0LKXtVHr_PLHn1paZAXLQmrsvaNy-qUZ3crwp13rCLM7w@mail.gmail.com>
Subject: Modifying RDD.scala
From: Zuhair Khayyat <zuhair.khayyat@gmail.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=089e01294482f8bad104ec2a2944
X-Virus-Checked: Checked by ClamAV on apache.org

--089e01294482f8bad104ec2a2944
Content-Type: text/plain; charset=ISO-8859-1

Dear SPARK members,

I am trying to start developing on SPARK source code. I have added a new
dummy function in RDD.scala to test if it compiles and runs. The modified
Spark compiled correctly but when I execute my code I got the following
error:

java.io.InvalidClassException: spark.RDD; local class incompatible: stream
classdesc serialVersionUID = 5151096093324583655, local class
serialVersionUID = 9012954318378784201
        at
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        at
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
        at
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1769)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at
spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
        at
spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:54)
        at
spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:111)
        at
java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1835)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1794)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at
spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
        at spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:45)
        at spark.executor.Executor$TaskRunner.run(Executor.scala:96)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
13/11/27 17:47:43 ERROR executor.StandaloneExecutorBackend: Driver or
worker disconnected! Shutting down.

Can you please help me to find out what went wrong? Thank you

Zuhair Khayyat

--089e01294482f8bad104ec2a2944--

From dev-return-787-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 27 15:30:20 2013
Return-Path: <dev-return-787-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id F04A910A57
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Nov 2013 15:30:20 +0000 (UTC)
Received: (qmail 53579 invoked by uid 500); 27 Nov 2013 15:30:19 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53335 invoked by uid 500); 27 Nov 2013 15:30:18 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 53326 invoked by uid 99); 27 Nov 2013 15:30:17 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 15:30:17 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.182 as permitted sender)
Received: from [209.85.212.182] (HELO mail-wi0-f182.google.com) (209.85.212.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 15:30:12 +0000
Received: by mail-wi0-f182.google.com with SMTP id en1so8817476wid.15
        for <dev@spark.incubator.apache.org>; Wed, 27 Nov 2013 07:29:50 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=SHSxC1mvvWuyEzFj6bTOHR0zW4M23hZ5Py9aE4nYF+w=;
        b=tCyca1btQ5uOWsoH8QG4VrDZU06cCJa91N9s6dkMDr8F6vmYZWnyLq8IUvfgnXfNpr
         jsgTneRVuOFwEpJ1ZUFsGi58XPLX+eHW1dz0diktAMzZpY0RM8oYfUBuUdB2cc2TsFgv
         vnE8BSbx8Ey6VGMnp7fVPHyW8r3LkzUWD5RSmFNi9jJSFzPWJrVlHxPjnzcyWd7gTqHg
         tQwA2CDXAXs3NdcW4thFAe6HUnC1BmLJbAgmNvXbqtT9AxUvXppvtL0ygJLIydVkJiT3
         Vu3lgciv3mREiRSx92FvBZXDcVRA/FlWyZ5xqjRMPAxdOchPENy4PkjFcZ6OYf8J5R6V
         r4mw==
X-Received: by 10.194.94.167 with SMTP id dd7mr12193097wjb.43.1385566190490;
 Wed, 27 Nov 2013 07:29:50 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.9.135 with HTTP; Wed, 27 Nov 2013 07:29:20 -0800 (PST)
In-Reply-To: <CAGgBT0LKXtVHr_PLHn1paZAXLQmrsvaNy-qUZ3crwp13rCLM7w@mail.gmail.com>
References: <CAGgBT0LKXtVHr_PLHn1paZAXLQmrsvaNy-qUZ3crwp13rCLM7w@mail.gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Wed, 27 Nov 2013 16:29:20 +0100
Message-ID: <CAMc-71kGehHKP9i3ntSrjt1D+YJm4JnDus1D5SfO0Wi0=DUe=g@mail.gmail.com>
Subject: Re: Modifying RDD.scala
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7bb03c4698db9404ec2a4478
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7bb03c4698db9404ec2a4478
Content-Type: text/plain; charset=ISO-8859-1

>From the looks of your exception, you modified your local class, but you
forgot to deploy those local changes to the cluster. This error msg:
classdesc serialVersionUID = 5151096093324583655, local class
serialVersionUID = 9012954318378784201

indicates that a version being de-serialized is different from the local
version. Make sure you deploy your changes across your Spark cluster.

-kr, Gerard.


On Wed, Nov 27, 2013 at 4:22 PM, Zuhair Khayyat <zuhair.khayyat@gmail.com>wrote:

> Dear SPARK members,
>
> I am trying to start developing on SPARK source code. I have added a new
> dummy function in RDD.scala to test if it compiles and runs. The modified
> Spark compiled correctly but when I execute my code I got the following
> error:
>
> java.io.InvalidClassException: spark.RDD; local class incompatible: stream
> classdesc serialVersionUID = 5151096093324583655, local class
> serialVersionUID = 9012954318378784201
>         at
> java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
>         at
> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
>         at
> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
>         at
> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
>         at
> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
>         at
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1769)
>         at
> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
>         at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>         at
> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
>         at
> spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:54)
>         at
> spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:111)
>         at
> java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1835)
>         at
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1794)
>         at
> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
>         at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>         at
> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
>         at
> spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:45)
>         at spark.executor.Executor$TaskRunner.run(Executor.scala:96)
>         at
>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
>         at
>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
>         at java.lang.Thread.run(Thread.java:724)
> 13/11/27 17:47:43 ERROR executor.StandaloneExecutorBackend: Driver or
> worker disconnected! Shutting down.
>
> Can you please help me to find out what went wrong? Thank you
>
> Zuhair Khayyat
>

--047d7bb03c4698db9404ec2a4478--

From dev-return-788-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 27 16:02:14 2013
Return-Path: <dev-return-788-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 94F6110B31
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Nov 2013 16:02:14 +0000 (UTC)
Received: (qmail 99183 invoked by uid 500); 27 Nov 2013 16:01:58 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 99140 invoked by uid 500); 27 Nov 2013 16:01:52 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 99050 invoked by uid 99); 27 Nov 2013 16:01:43 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 16:01:43 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of zuhair.khayyat@gmail.com designates 74.125.82.170 as permitted sender)
Received: from [74.125.82.170] (HELO mail-we0-f170.google.com) (74.125.82.170)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 16:01:36 +0000
Received: by mail-we0-f170.google.com with SMTP id w61so7072174wes.15
        for <dev@spark.incubator.apache.org>; Wed, 27 Nov 2013 08:01:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=xz1NdjN904hm36iBMGrFJyNamGaDEUCRo0ldN0G9T/g=;
        b=E7yZNPUR4IMPU+xq/ooDgJdfqsBB5UBNJFpRusaLL7B/Ct4VLGL+jD9VyUzSkzobqy
         oWneH/6y58YaazX2a0JAGq0Gn6VkY28M9SD13qgafCftXNJthpzO45e2Nl+2OfvR34BI
         APl5W0osgITdBJlLZxALmXGQ4UHBYXIjsNEU5yWuqDN3WbED72C4DOEBRUK3GG65KFV0
         kwcGbWQyxSR7aIvqcgy0NDyAzgX3dcO1MV5QzluBxkY9VHfDE+/4sJFfaDsgWYvYLLlL
         EKtDe2beoZolMVGr5jJfibd61t8nTL0RedkIJI6udbRw+pyKSVlKLLfrD0XLDfkWHCu8
         fKeg==
X-Received: by 10.180.107.168 with SMTP id hd8mr23266120wib.32.1385568074957;
        Wed, 27 Nov 2013 08:01:14 -0800 (PST)
Received: from [10.2.12.38] ([89.211.49.2])
        by mx.google.com with ESMTPSA id qc10sm72488162wic.9.2013.11.27.08.01.13
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Wed, 27 Nov 2013 08:01:14 -0800 (PST)
Content-Type: text/plain; charset=iso-8859-1
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1822\))
Subject: Re: Modifying RDD.scala
From: Zuhair Khayyat <zuhair.khayyat@gmail.com>
In-Reply-To: <CAMc-71kGehHKP9i3ntSrjt1D+YJm4JnDus1D5SfO0Wi0=DUe=g@mail.gmail.com>
Date: Wed, 27 Nov 2013 19:00:36 +0300
Content-Transfer-Encoding: quoted-printable
Message-Id: <B010381F-71DF-4C5C-8472-C37390EFD9E2@gmail.com>
References: <CAGgBT0LKXtVHr_PLHn1paZAXLQmrsvaNy-qUZ3crwp13rCLM7w@mail.gmail.com> <CAMc-71kGehHKP9i3ntSrjt1D+YJm4JnDus1D5SfO0Wi0=DUe=g@mail.gmail.com>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1822)
X-Virus-Checked: Checked by ClamAV on apache.org

Dear Gerard,

All servers share the spark binaries through NFS; It is unlikly that =
other servers contains the old class. I will test later with one server =
and see if I got the same problem..

Regards,
Zuhair Khayyat

On Nov 27, 2013, at 6:29 PM, Gerard Maas <gerard.maas@gmail.com> wrote:

> =46rom the looks of your exception, you modified your local class, but =
you
> forgot to deploy those local changes to the cluster. This error msg:
> classdesc serialVersionUID =3D 5151096093324583655, local class
> serialVersionUID =3D 9012954318378784201
>=20
> indicates that a version being de-serialized is different from the =
local
> version. Make sure you deploy your changes across your Spark cluster.
>=20
> -kr, Gerard.
>=20
>=20
> On Wed, Nov 27, 2013 at 4:22 PM, Zuhair Khayyat =
<zuhair.khayyat@gmail.com>wrote:
>=20
>> Dear SPARK members,
>>=20
>> I am trying to start developing on SPARK source code. I have added a =
new
>> dummy function in RDD.scala to test if it compiles and runs. The =
modified
>> Spark compiled correctly but when I execute my code I got the =
following
>> error:
>>=20
>> java.io.InvalidClassException: spark.RDD; local class incompatible: =
stream
>> classdesc serialVersionUID =3D 5151096093324583655, local class
>> serialVersionUID =3D 9012954318378784201
>>        at
>> java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
>>        at
>> =
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
>>        at
>> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
>>        at
>> =
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
>>        at
>> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
>>        at
>> =
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1769)
>>        at
>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
>>        at =
java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>>        at
>> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
>>        at
>> =
spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:54)
>>        at
>> spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:111)
>>        at
>> =
java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1835)
>>        at
>> =
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1794)
>>        at
>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
>>        at =
java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>>        at
>> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
>>        at
>> spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:45)
>>        at spark.executor.Executor$TaskRunner.run(Executor.scala:96)
>>        at
>>=20
>> =
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:=
1145)
>>        at
>>=20
>> =
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java=
:615)
>>        at java.lang.Thread.run(Thread.java:724)
>> 13/11/27 17:47:43 ERROR executor.StandaloneExecutorBackend: Driver or
>> worker disconnected! Shutting down.
>>=20
>> Can you please help me to find out what went wrong? Thank you
>>=20
>> Zuhair Khayyat
>>=20


From dev-return-789-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Wed Nov 27 21:15:43 2013
Return-Path: <dev-return-789-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id E7C9D106F2
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 27 Nov 2013 21:15:42 +0000 (UTC)
Received: (qmail 10982 invoked by uid 500); 27 Nov 2013 21:15:42 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 10928 invoked by uid 500); 27 Nov 2013 21:15:42 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 10920 invoked by uid 99); 27 Nov 2013 21:15:41 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 21:15:41 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nkronenfeld@oculusinfo.com designates 209.85.216.52 as permitted sender)
Received: from [209.85.216.52] (HELO mail-qa0-f52.google.com) (209.85.216.52)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 27 Nov 2013 21:15:38 +0000
Received: by mail-qa0-f52.google.com with SMTP id k4so9560112qaq.4
        for <dev@spark.incubator.apache.org>; Wed, 27 Nov 2013 13:15:16 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=oculusinfo.com; s=google;
        h=mime-version:in-reply-to:references:date:message-id:subject:from:to
         :content-type;
        bh=zcVvD0QxSxJd/ewlq40AJX467VcQc4YQL9ohD7ZbQMQ=;
        b=HlW1MBKNYCszYU5l6qduX18c5pd/+XnRrfyc+LriTZQtiwtP5mvVkd2AXaQEAGOABG
         3qS1tF1phm4mEV597QnwEnsmoZUL9yJuifgkV5p7yi91M/gH5P1pXAXwUvlUJLLyBEzH
         k6pNQXxOQN12ntYcJRvVZYgB+0E2SboECzQpY=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:content-type;
        bh=zcVvD0QxSxJd/ewlq40AJX467VcQc4YQL9ohD7ZbQMQ=;
        b=cq3DvE1sLW5UHpDf24Di6BQf+yMU8+3spNaGB9JIadKrSTcCZtdtLPR3Vs5/e6O+Px
         SLMLfgKPjlzoGapVGu+JvkDxbdwQvyDzrZR/S0PeZLToJcLRQW53iLEXP9MCvKqwHAcS
         GUBgTyzlHMdvNO2qe5YQmQ+84LemylYw9LEfGFoUIoGP9OkgpGumAr27dmnjjVDbrwAf
         qzxsFK8yak31c7Ir7xckAXwPHHWVrpMajISG8VJAhrP+R8jSPWBYwUk9CYzxO7ldCnXw
         W90Qj2A199jsauyx+kpRBIDChZSL9XX/WQH2SDhjJFSOFeS8NvcOmEXWSZ3hVRiksDbf
         FDoA==
X-Gm-Message-State: ALoCoQkqBuslTBOmom48w/lUZx4VZ0KOyN/3mvd5Amtnnb0lwTKAnPUv55Z3Z4i5S+ygCWlB30Is
MIME-Version: 1.0
X-Received: by 10.49.105.138 with SMTP id gm10mr40167879qeb.7.1385586916667;
 Wed, 27 Nov 2013 13:15:16 -0800 (PST)
Received: by 10.96.157.226 with HTTP; Wed, 27 Nov 2013 13:15:16 -0800 (PST)
In-Reply-To: <CAC1ssC4NMN=3RRQzEaGUPh0VqLh+2B0JZwydZsVc7dD_jnh2fA@mail.gmail.com>
References: <CAEpWh4_Gg1t=sU7bRLKMzedXzbKYbQkzohRDQP8A2rAH_=98og@mail.gmail.com>
	<CAEpWh492_K=ET66L2JCvOYxzRjcmXKwNv0BxPbO1-syuBQv0Hg@mail.gmail.com>
	<CAC1ssC42ZTKeTmjSiwx5wh8-k63W9XSY+WUdxrURVbXGQ3Opeg@mail.gmail.com>
	<CAEpWh48wwEwLOx2H0ix2GWm=igMw-UKXtsnofsurz9pcpn=6Hg@mail.gmail.com>
	<CAEpWh4_af-8RYdWR+MpJV8HS3fs+JnOg97rDYFwhC4cMbD42bw@mail.gmail.com>
	<CAC1ssC4NMN=3RRQzEaGUPh0VqLh+2B0JZwydZsVc7dD_jnh2fA@mail.gmail.com>
Date: Wed, 27 Nov 2013 16:15:16 -0500
Message-ID: <CAEpWh4_NBvO9bpfk4BrfsLUzND5aBZSOMP1432ftWGoegu=9UQ@mail.gmail.com>
Subject: Re: Problem with tests
From: Nathan Kronenfeld <nkronenfeld@oculusinfo.com>
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=047d7b676514f9658204ec2f179b
X-Virus-Checked: Checked by ClamAV on apache.org

--047d7b676514f9658204ec2f179b
Content-Type: text/plain; charset=ISO-8859-1

Thanks.  Now that that's checked into HEAD, it all seems to work again.


On Sun, Nov 24, 2013 at 5:02 AM, Reynold Xin <rxin@apache.org> wrote:

> Take a look at this pull request and see if it fixes your problem:
> https://github.com/apache/incubator-spark/pull/201
>
> I changed the semantics of the index from the output partition index back
> to the rdd partition index.
>
>
>
> On Sat, Nov 23, 2013 at 10:01 PM, Nathan Kronenfeld <
> nkronenfeld@oculusinfo.com> wrote:
>
> > Though I think it's a more general problem...
> >
> > Take the following:
> >
> > val data = sc.parallelize(Range(0, 8), 2)
> > val data2 = data.mapPartitionsWithIndex((index, i) => i.map(x => (x,
> > index)))
> >
> > data2.collect
> >   res0: Array[(Int, Int)] = Array((0,0), (1,0), (2,0), (3,0), (4,1),
> (5,1),
> > (6,1), (7,1))
> >
> > new org.apache.spark.rdd.PartitionPruningRDD(data2, n => 1 == n).collect
> >   res1: Array[(Int, Int)] = Array((4,0), (5,0), (6,0), (7,0))
> >
> > So, in this case, pruning the RDD has changed the data within it.  This
> > seems to be what is causing my errors.
> >
> >
> >
> > On Sat, Nov 23, 2013 at 8:00 AM, Nathan Kronenfeld <
> > nkronenfeld@oculusinfo.com> wrote:
> >
> > > https://github.com/apache/incubator-spark/pull/18
> > >
> > >
> > > On Fri, Nov 22, 2013 at 6:35 PM, Reynold Xin <reynoldx@gmail.com>
> wrote:
> > >
> > >> Can you provide a link to your pull request?
> > >>
> > >>
> > >> On Sat, Nov 23, 2013 at 5:02 AM, Nathan Kronenfeld <
> > >> nkronenfeld@oculusinfo.com> wrote:
> > >>
> > >> > Actually, looking into recent commits, it looks like my hunch may be
> > >> > exactly correct:
> > >> >
> > >> >
> > >>
> >
> https://github.com/apache/incubator-spark/commit/f639b65eabcc8666b74af8f13a37c5fdf7e0185f
> > >> > "PartitionPruningRDD is using index from parent"
> > >> >
> > >> > Is there anyone who can explain why this new behavior is preferable?
> > >>  And,
> > >> > if it's staying, can suggest a way to fix my tests for this case?
> > >> >
> > >> > Thanks again,
> > >> >                  Nathan
> > >> >
> > >> >
> > >> > On Fri, Nov 22, 2013 at 3:56 PM, Nathan Kronenfeld <
> > >> > nkronenfeld@oculusinfo.com> wrote:
> > >> >
> > >> > > Hi there.
> > >> > >
> > >> > > I have a problem with the unit tests on a pull request I'm trying
> to
> > >> tie
> > >> > > up.  The changes deal with partition-related functions.
> > >> > >
> > >> > > In particular, the tests I have that test an append-to-partition
> > >> function
> > >> > > work fine on my own machine, but fail on the build machine (
> > >> > >
> > >> >
> > >>
> >
> https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/2152/console
> > >> > > ).
> > >> > >
> > >> > > The failure seems to stem from pulling a single partition out of
> the
> > >> set.
> > >> > > In either case, when I work on the full dataset:
> > >> > >
> > >> > > UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> > >> > >   UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> > >> > >     ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> > >> > partitions)
> > >> > >     MapPartitionsWithContextRDD[7] at apply at FunSuite.scala:1265
> > (2
> > >> > partitions)
> > >> > >       ParallelCollectionRDD[4] at apply at FunSuite.scala:1265 (2
> > >> > partitions)
> > >> > >   ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> > >> > partitions)
> > >> > >
> > >> > >
> > >> > > It seems to work.  When I pull one partition out of this, by
> > wrapping
> > >> a
> > >> > PartitionPruningRDD around it (pruning out everything but partition
> > 2):
> > >> > >
> > >> > > PartitionPruningRDD[12] at apply at FunSuite.scala:1265 (1
> > partitions)
> > >> > >   UnionRDD[11] at apply at FunSuite.scala:1265 (4 partitions)
> > >> > >     UnionRDD[9] at apply at FunSuite.scala:1265 (3 partitions)
> > >> > >       ParallelCollectionRDD[8] at apply at FunSuite.scala:1265 (1
> > >> > partitions)
> > >> > >       MapPartitionsWithContextRDD[7] at apply at
> FunSuite.scala:1265
> > >> (2
> > >> > partitions)
> > >> > >         ParallelCollectionRDD[4] at apply at FunSuite.scala:1265
> (2
> > >> > partitions)
> > >> > >     ParallelCollectionRDD[10] at apply at FunSuite.scala:1265 (1
> > >> > partitions)
> > >> > >
> > >> > >
> > >> > > In this case, my local machine and the build machine seem to act
> > >> > > differently.
> > >> > >
> > >> > > On my local machine, what is in the inner ParallelCollection
> > >> partition #2
> > >> > > shows up in the MapPartitionsWithContextRDD as partition #2 still.
> >  On
> > >> > the
> > >> > > build machine, this same partition shows up in the later RDD as
> > >> partition
> > >> > > #0 - presumably because everything else is pruned out, but that
> > >> pruning
> > >> > > should happen at an outer level, shouldn't it?
> > >> > >
> > >> > > Does anyone know why the build machine would act different from
> > >> locally
> > >> > > here?
> > >> > >
> > >> > > Also, sadly, this worked fine two days ago.
> > >> > >
> > >> > > My only thought is that perhaps the PullRequestBuilder does a
> merge
> > >> with
> > >> > > current code, and someone broke this in the last day or two?  Past
> > >> that,
> > >> > > I'm at a bit of a loss.
> > >> > >
> > >> > > Thanks,
> > >> > >                     -Nathan
> > >> > >
> > >> > >
> > >> > > --
> > >> > >
> > >> > > Nathan Kronenfeld
> > >> > > Senior Visualization Developer
> > >> > > Oculus Info Inc
> > >> > > 2 Berkeley Street, Suite 600,
> > >> > > Toronto, Ontario M5A 4J5
> > >> > > Phone:  +1-416-203-3003 x 238
> > >> > > Email:  nkronenfeld@oculusinfo.com
> > >> > >
> > >> >
> > >> >
> > >> >
> > >> > --
> > >> > Nathan Kronenfeld
> > >> > Senior Visualization Developer
> > >> > Oculus Info Inc
> > >> > 2 Berkeley Street, Suite 600,
> > >> > Toronto, Ontario M5A 4J5
> > >> > Phone:  +1-416-203-3003 x 238
> > >> > Email:  nkronenfeld@oculusinfo.com
> > >> >
> > >>
> > >
> > >
> > >
> > > --
> > > Nathan Kronenfeld
> > > Senior Visualization Developer
> > > Oculus Info Inc
> > > 2 Berkeley Street, Suite 600,
> > > Toronto, Ontario M5A 4J5
> > > Phone:  +1-416-203-3003 x 238
> > > Email:  nkronenfeld@oculusinfo.com
> > >
> >
> >
> >
> > --
> > Nathan Kronenfeld
> > Senior Visualization Developer
> > Oculus Info Inc
> > 2 Berkeley Street, Suite 600,
> > Toronto, Ontario M5A 4J5
> > Phone:  +1-416-203-3003 x 238
> > Email:  nkronenfeld@oculusinfo.com
> >
>



-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Toronto, Ontario M5A 4J5
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com

--047d7b676514f9658204ec2f179b--

From dev-return-790-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 28 06:01:43 2013
Return-Path: <dev-return-790-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 82EA5106E4
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Nov 2013 06:01:43 +0000 (UTC)
Received: (qmail 53937 invoked by uid 500); 28 Nov 2013 06:01:41 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 53705 invoked by uid 500); 28 Nov 2013 06:01:37 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 53697 invoked by uid 99); 28 Nov 2013 06:01:36 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Nov 2013 06:01:36 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ev@ooyala.com designates 209.85.212.42 as permitted sender)
Received: from [209.85.212.42] (HELO mail-vb0-f42.google.com) (209.85.212.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Nov 2013 06:01:32 +0000
Received: by mail-vb0-f42.google.com with SMTP id w18so5773269vbj.1
        for <dev@spark.incubator.apache.org>; Wed, 27 Nov 2013 22:01:10 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=ooyala.com; s=google;
        h=mime-version:date:message-id:subject:from:to:cc:content-type;
        bh=HF5W1qqVbPtCauci8j02AM7XYklydIafXb0m0xh161g=;
        b=L95aknK5WUt9Q2FnoHDo5ZgWi1RMY1oV85B/ywLKlF3WAUGTPbpkrBYNr1ZFBHblqF
         EWy8JSR3xK4+rkTDYvPunYbbr8dGui3XxLw2AUsbUfgfnV5VzS/ZSWc+Dylojm5I12sU
         VTqzF2ebPgksFZT4wKmij3MOC1oQwvMKCXJAg=
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to:cc
         :content-type;
        bh=HF5W1qqVbPtCauci8j02AM7XYklydIafXb0m0xh161g=;
        b=djtZaB1NEcTvQL720CZWLJKsmg9/2jdOQ2pjacqpWrTyqDMFpWXm2X+UZOxY6aDkxI
         56ovEsQgFVvn/Yx5uSqQKiI3GBNjajKxkQ2DdzMQtqYoOkOkZi/iJbvDoJkHiZaRpZfr
         zSPxDP096ddVYCsQ/HZUs9qHZt2erdEWgE+47+X1y1QmC4PmP9zDMf4Wnr7TsvsFSMTp
         YyJTxjHa5nDqsoNWESISJdiWZQUHPWe2nMC2z3gXOUl4wQVJUZiuklBOwgwaoTVCHJ3X
         8RGGynhvctACuY/x06sm4TSW4RIBZTo6YNgVgAKc8K7+G6BFUAeA3EbBVL6fN3kCcTOt
         wVRw==
X-Gm-Message-State: ALoCoQnnHQQ1VPjT5H3aMNRmfj9bzmfxvnUDFz4DRnwI2n9vaOuOh5cLQ0CWywLLx9to2DGOhYaE
MIME-Version: 1.0
X-Received: by 10.220.11.7 with SMTP id r7mr36989533vcr.12.1385618470733; Wed,
 27 Nov 2013 22:01:10 -0800 (PST)
Received: by 10.220.17.193 with HTTP; Wed, 27 Nov 2013 22:01:10 -0800 (PST)
Date: Wed, 27 Nov 2013 22:01:10 -0800
Message-ID: <CADWPM3jBZGAPMHh5eew+uC940fVV2a6=iTTgmPLVWooPQ4Sqew@mail.gmail.com>
Subject: Mesos not working on head of master
From: Evan Chan <ev@ooyala.com>
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Cc: Tobi Knaup <tobi@mesosphere.io>, Paco Nathan <pacoid@mesosphe.re>
Content-Type: text/plain; charset=ISO-8859-1
X-Virus-Checked: Checked by ClamAV on apache.org

1) zk:// URIs can no longer be passed to SparkContext as the first
argument, as the MESOS_REGEX was changed to take only mesos:// URIs.
This essentially renders Spark incompatible with real production Mesos
setups.

2) Spinning up tasks on a Mesos cluster, all of them fail right away,
with no log message visible.  This might possibly be me, but this was
working with 0.8.0.

I can submit a pull request for #1, but a better fix would be to have
some kind of MesosSuite so that any kind of breakages can be caught
systematically.   Maybe some kind of VM, but ideas are welcome.  What
do you guys think?

(I've cc'ed two folks from Mesosphere, esp on setting up a test suite)

-Evan

-- 
--
Evan Chan
Staff Engineer
ev@ooyala.com  |

From dev-return-791-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 28 11:31:09 2013
Return-Path: <dev-return-791-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 4038F10F2D
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Nov 2013 11:31:09 +0000 (UTC)
Received: (qmail 51514 invoked by uid 500); 28 Nov 2013 11:31:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51451 invoked by uid 500); 28 Nov 2013 11:30:59 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 51442 invoked by uid 99); 28 Nov 2013 11:30:56 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Nov 2013 11:30:56 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of gerard.maas@gmail.com designates 209.85.212.169 as permitted sender)
Received: from [209.85.212.169] (HELO mail-wi0-f169.google.com) (209.85.212.169)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Nov 2013 11:30:49 +0000
Received: by mail-wi0-f169.google.com with SMTP id hm6so809702wib.2
        for <dev@spark.incubator.apache.org>; Thu, 28 Nov 2013 03:30:29 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=fFzqyoBQS0WmPKX8TQXxCyjF8kXfBhA+aEUsCnyDIq8=;
        b=esOjYPEbQuLSxQCquoT5K9OvN5TI2R6yqCCU3mU8Dn53kZ91BhDmqzTzikrS3RgOv4
         b13yzTOrn0x8GW7h3qYspRkvbr9F7PY75WDdfEz9vp3btyxrOsGi1u/7sQ/fcAHNdlWs
         UUjRj37sHClmktxKByHYBhu6ucGst8ym14R91hgsRlVn7Mf1A2L/vc2g2+ck6glqB3Lc
         w6zDU6jBjbgEyQ5QnxrUPmLC5YQq8Jh/Ov8jfL681wlRzuE7axSuSJeAVY3a/PnBZZAK
         87DyrqaDcpfkqpsnwQsYUKiYHtT8ezHzT54zb5i0hCDuJo/n7wMRI7ddTRc7vulOi+uG
         r9oA==
X-Received: by 10.180.11.169 with SMTP id r9mr2110027wib.26.1385638228927;
 Thu, 28 Nov 2013 03:30:28 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.9.135 with HTTP; Thu, 28 Nov 2013 03:29:58 -0800 (PST)
In-Reply-To: <B010381F-71DF-4C5C-8472-C37390EFD9E2@gmail.com>
References: <CAGgBT0LKXtVHr_PLHn1paZAXLQmrsvaNy-qUZ3crwp13rCLM7w@mail.gmail.com>
 <CAMc-71kGehHKP9i3ntSrjt1D+YJm4JnDus1D5SfO0Wi0=DUe=g@mail.gmail.com> <B010381F-71DF-4C5C-8472-C37390EFD9E2@gmail.com>
From: Gerard Maas <gerard.maas@gmail.com>
Date: Thu, 28 Nov 2013 12:29:58 +0100
Message-ID: <CAMc-71nac16=QMAs1p0gCMpERVgGTqUq-kaWbHawMb3y9WZyCg@mail.gmail.com>
Subject: Re: Modifying RDD.scala
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=001a11c24de86c2cee04ec3b0a64
X-Virus-Checked: Checked by ClamAV on apache.org

--001a11c24de86c2cee04ec3b0a64
Content-Type: text/plain; charset=ISO-8859-1

Hi Zuhair,

Following the exception, you have two different versions somewhere. Do you
get the same behavior if you use it in a single node?
Maybe Spark veterans have more specific tips for you.

kr, Gerard.


On Wed, Nov 27, 2013 at 5:00 PM, Zuhair Khayyat <zuhair.khayyat@gmail.com>wrote:

> Dear Gerard,
>
> All servers share the spark binaries through NFS; It is unlikly that other
> servers contains the old class. I will test later with one server and see
> if I got the same problem..
>
> Regards,
> Zuhair Khayyat
>
> On Nov 27, 2013, at 6:29 PM, Gerard Maas <gerard.maas@gmail.com> wrote:
>
> > From the looks of your exception, you modified your local class, but you
> > forgot to deploy those local changes to the cluster. This error msg:
> > classdesc serialVersionUID = 5151096093324583655, local class
> > serialVersionUID = 9012954318378784201
> >
> > indicates that a version being de-serialized is different from the local
> > version. Make sure you deploy your changes across your Spark cluster.
> >
> > -kr, Gerard.
> >
> >
> > On Wed, Nov 27, 2013 at 4:22 PM, Zuhair Khayyat <
> zuhair.khayyat@gmail.com>wrote:
> >
> >> Dear SPARK members,
> >>
> >> I am trying to start developing on SPARK source code. I have added a new
> >> dummy function in RDD.scala to test if it compiles and runs. The
> modified
> >> Spark compiled correctly but when I execute my code I got the following
> >> error:
> >>
> >> java.io.InvalidClassException: spark.RDD; local class incompatible:
> stream
> >> classdesc serialVersionUID = 5151096093324583655, local class
> >> serialVersionUID = 9012954318378784201
> >>        at
> >> java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
> >>        at
> >> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
> >>        at
> >> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
> >>        at
> >> java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
> >>        at
> >> java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
> >>        at
> >>
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1769)
> >>        at
> >> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
> >>        at
> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
> >>        at
> >> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
> >>        at
> >> spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:54)
> >>        at
> >> spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:111)
> >>        at
> >> java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1835)
> >>        at
> >>
> java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1794)
> >>        at
> >> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
> >>        at
> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
> >>        at
> >> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
> >>        at
> >> spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:45)
> >>        at spark.executor.Executor$TaskRunner.run(Executor.scala:96)
> >>        at
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
> >>        at
> >>
> >>
> java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
> >>        at java.lang.Thread.run(Thread.java:724)
> >> 13/11/27 17:47:43 ERROR executor.StandaloneExecutorBackend: Driver or
> >> worker disconnected! Shutting down.
> >>
> >> Can you please help me to find out what went wrong? Thank you
> >>
> >> Zuhair Khayyat
> >>
>
>

--001a11c24de86c2cee04ec3b0a64--

From dev-return-792-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Thu Nov 28 12:36:36 2013
Return-Path: <dev-return-792-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 49D721012F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Thu, 28 Nov 2013 12:36:36 +0000 (UTC)
Received: (qmail 51611 invoked by uid 500); 28 Nov 2013 12:36:34 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51269 invoked by uid 500); 28 Nov 2013 12:36:29 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 51256 invoked by uid 99); 28 Nov 2013 12:36:27 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Nov 2013 12:36:27 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=5.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of zuhair.khayyat@gmail.com designates 74.125.82.180 as permitted sender)
Received: from [74.125.82.180] (HELO mail-we0-f180.google.com) (74.125.82.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Thu, 28 Nov 2013 12:36:19 +0000
Received: by mail-we0-f180.google.com with SMTP id t61so2478927wes.11
        for <dev@spark.incubator.apache.org>; Thu, 28 Nov 2013 04:35:58 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=content-type:mime-version:subject:from:in-reply-to:date
         :content-transfer-encoding:message-id:references:to;
        bh=xIru7QKIlhJWxvyKQNi2LnsWgq/38dqxV/9pfha/EQs=;
        b=Ums3Pd3KEoPwd5bGK6qaI+qHyxG66XRMx0oDi7qrTvRWIp/Bw1udO+rBDwY3ZLFnhM
         MDkiO4G9PcEdhVLvMtlw/0wa5meg6+sBzfSfrOy1JrzL/mo2qaMiQK/lQKUKIkgQvM3A
         Tg9b6VMSOYYi2CndvVQ2RZjKkWJpQYhbCFFQepwD0zp2CDVFWOg+sOEbb5ZcptCMLkP8
         3rb0BI3bNCXgeCg2fdc2EhwY6E1If1C1+jLo3VtH1wJwGcC73OuKFwZ+Zu4FWHH3/InH
         qgtgCij0Nwrz8guwaN55R+Y6uUvAlqyk0Sl+FsvCy+SEDt3wTLQaTYZQQtygyRDe/idl
         QRqg==
X-Received: by 10.194.122.99 with SMTP id lr3mr35559558wjb.21.1385642158621;
        Thu, 28 Nov 2013 04:35:58 -0800 (PST)
Received: from [10.5.3.159] ([78.100.128.130])
        by mx.google.com with ESMTPSA id hv5sm80945353wib.2.2013.11.28.04.35.51
        for <dev@spark.incubator.apache.org>
        (version=TLSv1 cipher=ECDHE-RSA-RC4-SHA bits=128/128);
        Thu, 28 Nov 2013 04:35:58 -0800 (PST)
Content-Type: text/plain; charset=windows-1252
Mime-Version: 1.0 (Mac OS X Mail 7.0 \(1822\))
Subject: Re: Modifying RDD.scala
From: Zuhair Khayyat <zuhair.khayyat@gmail.com>
In-Reply-To: <CAMc-71nac16=QMAs1p0gCMpERVgGTqUq-kaWbHawMb3y9WZyCg@mail.gmail.com>
Date: Thu, 28 Nov 2013 15:35:46 +0300
Content-Transfer-Encoding: quoted-printable
Message-Id: <1EADC4AE-0EED-4807-9530-DE1BAED5A646@gmail.com>
References: <CAGgBT0LKXtVHr_PLHn1paZAXLQmrsvaNy-qUZ3crwp13rCLM7w@mail.gmail.com> <CAMc-71kGehHKP9i3ntSrjt1D+YJm4JnDus1D5SfO0Wi0=DUe=g@mail.gmail.com> <B010381F-71DF-4C5C-8472-C37390EFD9E2@gmail.com> <CAMc-71nac16=QMAs1p0gCMpERVgGTqUq-kaWbHawMb3y9WZyCg@mail.gmail.com>
To: dev@spark.incubator.apache.org
X-Mailer: Apple Mail (2.1822)
X-Virus-Checked: Checked by ClamAV on apache.org

Dear Gerard,

I don=92t have the problem when running on a single machine.

I think I found the problem; since I build my project using maven, the =
old Spark copy is still cached in my home directory =
(~/.m2/repository/org=85). Every time I modify my local spark and build =
it, I have to clean whatever Maven is caching in my home directory.

Thank you
Zuhair Khayyat

On Nov 28, 2013, at 2:29 PM, Gerard Maas <gerard.maas@gmail.com> wrote:

> Hi Zuhair,
>=20
> Following the exception, you have two different versions somewhere. Do =
you
> get the same behavior if you use it in a single node?
> Maybe Spark veterans have more specific tips for you.
>=20
> kr, Gerard.
>=20
>=20
> On Wed, Nov 27, 2013 at 5:00 PM, Zuhair Khayyat =
<zuhair.khayyat@gmail.com>wrote:
>=20
>> Dear Gerard,
>>=20
>> All servers share the spark binaries through NFS; It is unlikly that =
other
>> servers contains the old class. I will test later with one server and =
see
>> if I got the same problem..
>>=20
>> Regards,
>> Zuhair Khayyat
>>=20
>> On Nov 27, 2013, at 6:29 PM, Gerard Maas <gerard.maas@gmail.com> =
wrote:
>>=20
>>> =46rom the looks of your exception, you modified your local class, =
but you
>>> forgot to deploy those local changes to the cluster. This error msg:
>>> classdesc serialVersionUID =3D 5151096093324583655, local class
>>> serialVersionUID =3D 9012954318378784201
>>>=20
>>> indicates that a version being de-serialized is different from the =
local
>>> version. Make sure you deploy your changes across your Spark =
cluster.
>>>=20
>>> -kr, Gerard.
>>>=20
>>>=20
>>> On Wed, Nov 27, 2013 at 4:22 PM, Zuhair Khayyat <
>> zuhair.khayyat@gmail.com>wrote:
>>>=20
>>>> Dear SPARK members,
>>>>=20
>>>> I am trying to start developing on SPARK source code. I have added =
a new
>>>> dummy function in RDD.scala to test if it compiles and runs. The
>> modified
>>>> Spark compiled correctly but when I execute my code I got the =
following
>>>> error:
>>>>=20
>>>> java.io.InvalidClassException: spark.RDD; local class incompatible:
>> stream
>>>> classdesc serialVersionUID =3D 5151096093324583655, local class
>>>> serialVersionUID =3D 9012954318378784201
>>>>       at
>>>> java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
>>>>       at
>>>> =
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
>>>>       at
>>>> =
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
>>>>       at
>>>> =
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1620)
>>>>       at
>>>> =
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1515)
>>>>       at
>>>>=20
>> =
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1769)
>>>>       at
>>>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
>>>>       at
>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>>>>       at
>>>> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
>>>>       at
>>>> =
spark.scheduler.ShuffleMapTask$.deserializeInfo(ShuffleMapTask.scala:54)
>>>>       at
>>>> =
spark.scheduler.ShuffleMapTask.readExternal(ShuffleMapTask.scala:111)
>>>>       at
>>>> =
java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1835)
>>>>       at
>>>>=20
>> =
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1794)
>>>>       at
>>>> java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1348)
>>>>       at
>> java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
>>>>       at
>>>> spark.JavaDeserializationStream.readObject(JavaSerializer.scala:23)
>>>>       at
>>>> spark.JavaSerializerInstance.deserialize(JavaSerializer.scala:45)
>>>>       at spark.executor.Executor$TaskRunner.run(Executor.scala:96)
>>>>       at
>>>>=20
>>>>=20
>> =
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:=
1145)
>>>>       at
>>>>=20
>>>>=20
>> =
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java=
:615)
>>>>       at java.lang.Thread.run(Thread.java:724)
>>>> 13/11/27 17:47:43 ERROR executor.StandaloneExecutorBackend: Driver =
or
>>>> worker disconnected! Shutting down.
>>>>=20
>>>> Can you please help me to find out what went wrong? Thank you
>>>>=20
>>>> Zuhair Khayyat
>>>>=20
>>=20
>>=20


From dev-return-793-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 29 04:49:48 2013
Return-Path: <dev-return-793-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 3B938104E6
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Nov 2013 04:49:48 +0000 (UTC)
Received: (qmail 13498 invoked by uid 500); 29 Nov 2013 04:49:46 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 13423 invoked by uid 500); 29 Nov 2013 04:49:41 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 13407 invoked by uid 99); 29 Nov 2013 04:49:40 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Nov 2013 04:49:40 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of ilikerps@gmail.com designates 74.125.82.42 as permitted sender)
Received: from [74.125.82.42] (HELO mail-wg0-f42.google.com) (74.125.82.42)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Nov 2013 04:49:36 +0000
Received: by mail-wg0-f42.google.com with SMTP id a1so1463406wgh.3
        for <dev@spark.incubator.apache.org>; Thu, 28 Nov 2013 20:49:15 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=7B9NJQmQWi+0uIcqPpAuIsMy4wBSoRshRbw5y7Qe/aQ=;
        b=PZHF3EN0Ymw0Nty+dYCrD6WzFiMIvu4IlvOoZbxYebwEdF89EkysH5HjennuwHwVFj
         aKKT2Nf+0uNNAR6FJ7m2xzA5kWapyzp98oqyvi3F2+sK9I82z/rJMnJl4+A7scntJStN
         /vMcStnwIsNuDwLWQddSM6G3fvtkLCDa+QOFwkPRIv1aFYwhM3XP7fMh9y/kt68xNywh
         0V47dkoATTNScAHpQNYtk/UE+Re7LjyfPjGEAwrnSwWXX/dTMi5MoXuxzu2Qpt8yFk9T
         23xha2Zh9FfB0+DZWN02FLHIQnkBrJyh+cewpJvVi6SxDrFlcZNfjGiBYJzTWsfhQUxI
         3yIQ==
X-Received: by 10.180.74.174 with SMTP id u14mr5067078wiv.45.1385700555112;
 Thu, 28 Nov 2013 20:49:15 -0800 (PST)
MIME-Version: 1.0
Received: by 10.194.119.228 with HTTP; Thu, 28 Nov 2013 20:48:55 -0800 (PST)
In-Reply-To: <CADWPM3jBZGAPMHh5eew+uC940fVV2a6=iTTgmPLVWooPQ4Sqew@mail.gmail.com>
References: <CADWPM3jBZGAPMHh5eew+uC940fVV2a6=iTTgmPLVWooPQ4Sqew@mail.gmail.com>
From: Aaron Davidson <ilikerps@gmail.com>
Date: Thu, 28 Nov 2013 20:48:55 -0800
Message-ID: <CANGvG8pOJVxH5+SxXpWBoMDcyvZkKMzHgXA0UKR5NNa1O0hSaA@mail.gmail.com>
Subject: Re: Mesos not working on head of master
To: dev@spark.incubator.apache.org
Content-Type: multipart/alternative; boundary=bcaec54fb5345a469404ec498df3
X-Virus-Checked: Checked by ClamAV on apache.org

--bcaec54fb5345a469404ec498df3
Content-Type: text/plain; charset=ISO-8859-1

Hey Evan,

Sorry about the issue with the Mesos urls. I submitted the patch that broke
it originally -- at the time, I did not know about the zk:// urls mapping
to Mesos. I went ahead and started PR
#217<https://github.com/apache/incubator-spark/pull/217> to
correct this, which also includes a unit test for creating the schedulers
to hopefully avoid this type of issue in the future (at least at this most
basic level).

I cannot speak for #2, though, which seems to be the more serious issue.



On Wed, Nov 27, 2013 at 10:01 PM, Evan Chan <ev@ooyala.com> wrote:

> 1) zk:// URIs can no longer be passed to SparkContext as the first
> argument, as the MESOS_REGEX was changed to take only mesos:// URIs.
> This essentially renders Spark incompatible with real production Mesos
> setups.
>
> 2) Spinning up tasks on a Mesos cluster, all of them fail right away,
> with no log message visible.  This might possibly be me, but this was
> working with 0.8.0.
>
> I can submit a pull request for #1, but a better fix would be to have
> some kind of MesosSuite so that any kind of breakages can be caught
> systematically.   Maybe some kind of VM, but ideas are welcome.  What
> do you guys think?
>
> (I've cc'ed two folks from Mesosphere, esp on setting up a test suite)
>
> -Evan
>
> --
> --
> Evan Chan
> Staff Engineer
> ev@ooyala.com  |
>

--bcaec54fb5345a469404ec498df3--

From dev-return-794-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 29 10:02:53 2013
Return-Path: <dev-return-794-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id BB9BE10A1F
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Nov 2013 10:02:53 +0000 (UTC)
Received: (qmail 96122 invoked by uid 500); 29 Nov 2013 10:02:53 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 95762 invoked by uid 500); 29 Nov 2013 10:02:51 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 95450 invoked by uid 99); 29 Nov 2013 10:02:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Nov 2013 10:02:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=5.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: local policy includes SPF record at spf.trusted-forwarder.org)
Received: from [209.85.223.174] (HELO mail-ie0-f174.google.com) (209.85.223.174)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Nov 2013 10:02:46 +0000
Received: by mail-ie0-f174.google.com with SMTP id at1so15704110iec.5
        for <dev@spark.incubator.apache.org>; Fri, 29 Nov 2013 02:02:26 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:from:date:message-id:subject:to
         :content-type;
        bh=EE+CqyAxxBvZxviPW8T9LKgctKC2IWoUMErgI0dY6oU=;
        b=ZvuQOKXjyYRo2Eg3ePsHGr2Q5pvfBsu/XjEn1kZjWcQvkdODSWZDb9Wz+yP+f8Y1Yk
         jqRbBT+sU0lqZZyoofUiwFs5t0pV4rr6F7XYv30gO+dNEIeAr4MXAI6lM/14F/EBWnEJ
         Q2A8WHSvFslV2z8btxkgG2cxcvYilqKxBZSZ5znnQarCWY3Lk4V+etSrfY7ddKS8fZux
         pAIAfqocrRtumJjU/fsUX4LXOpoSbxTUOZblIYBU6/mRP5VpxJ3kk1A0K03OZZ2yu4SN
         peaW+eijbBOck1oViuVlR4K2V1VO+EKgSnSL4UfCdnTlxnQmwCAhLTIJCdDlkgSn6a56
         WbEw==
X-Gm-Message-State: ALoCoQll7xzh+CjAfp08LEiED5XlTl7bL2LBcgo6Zajo+cvM33k+Jq+zPeM5fQkA5/UbhC72Is5F
X-Received: by 10.50.117.40 with SMTP id kb8mr5513388igb.60.1385719345982;
 Fri, 29 Nov 2013 02:02:25 -0800 (PST)
MIME-Version: 1.0
Received: by 10.64.13.207 with HTTP; Fri, 29 Nov 2013 02:02:05 -0800 (PST)
From: =?UTF-8?Q?Grega_Ke=C5=A1pret?= <grega@celtra.com>
Date: Fri, 29 Nov 2013 11:02:05 +0100
Message-ID: <CAMihvYb8aR4gBBtmD2R4ZvxYo8k8w7AsbFN+myaqm344yOMciA@mail.gmail.com>
Subject: spark.task.maxFailures
To: dev@spark.incubator.apache.org
Content-Type: multipart/related; boundary=089e0115f49a607b1604ec4dedbf
X-Virus-Checked: Checked by ClamAV on apache.org

--089e0115f49a607b1604ec4dedbf
Content-Type: multipart/alternative; boundary=089e0115f49a607b1204ec4dedbe

--089e0115f49a607b1204ec4dedbe
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Looking at http://spark.incubator.apache.org/docs/latest/configuration.html
docs says:
Number of individual task failures before giving up on the job. Should be
greater than or equal to 1. Number of allowed retries =3D this value - 1.

However, looking at the code
https://github.com/apache/incubator-spark/blob/master/core/src/main/scala/o=
rg/apache/spark/scheduler/cluster/ClusterTaskSetManager.scala#L532

if I set spark.task.maxFailures to 1, this means that job will fail after
task fails for the second time. Shouldn't this line be corrected to if (
numFailures(index) >=3D MAX_TASK_FAILURES) {
?

I can open a pull request if this is the case.

Thanks,
Grega
--
[image: Inline image 1]
*Grega Ke=C5=A1pret*
Analytics engineer

Celtra =E2=80=94 Rich Media Mobile Advertising
celtra.com <http://www.celtra.com/> |
@celtramobile<http://www.twitter.com/celtramobile>

--089e0115f49a607b1204ec4dedbe
Content-Type: text/html; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

<div dir=3D"ltr"><div>Looking at=C2=A0<a href=3D"http://spark.incubator.apa=
che.org/docs/latest/configuration.html">http://spark.incubator.apache.org/d=
ocs/latest/configuration.html</a></div><div>docs says:</div><div><span styl=
e=3D"color:rgb(51,51,51);font-family:&#39;Helvetica Neue&#39;,Helvetica,Ari=
al,sans-serif;font-size:14px;line-height:20px">Number of individual task fa=
ilures before giving up on the job. Should be greater than or equal to 1. N=
umber of allowed retries =3D this value - 1.</span><div>

<br></div><div>However, looking at the code<span style=3D"color:rgb(51,51,5=
1);font-family:&#39;Helvetica Neue&#39;,Helvetica,Arial,sans-serif;font-siz=
e:14px;line-height:20px"><br></span></div><div><a href=3D"https://github.co=
m/apache/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/s=
cheduler/cluster/ClusterTaskSetManager.scala#L532">https://github.com/apach=
e/incubator-spark/blob/master/core/src/main/scala/org/apache/spark/schedule=
r/cluster/ClusterTaskSetManager.scala#L532</a><br>

</div><div><br></div><div>if I set spark.task.maxFailures to 1, this means =
that job will fail after task fails for the second time. Shouldn&#39;t this=
 line be corrected to=C2=A0<span class=3D"" style=3D"font-weight:bold;color=
:rgb(51,51,51);font-family:Consolas,&#39;Liberation Mono&#39;,Courier,monos=
pace;font-size:11.818181991577148px;line-height:17.99715805053711px;white-s=
pace:pre">if</span><span style=3D"color:rgb(51,51,51);font-family:Consolas,=
&#39;Liberation Mono&#39;,Courier,monospace;font-size:11.818181991577148px;=
line-height:17.99715805053711px;white-space:pre;background-color:rgb(255,25=
5,204)"> </span><span class=3D"" style=3D"font-weight:bold;color:rgb(51,51,=
51);font-family:Consolas,&#39;Liberation Mono&#39;,Courier,monospace;font-s=
ize:11.818181991577148px;line-height:17.99715805053711px;white-space:pre">(=
</span><span class=3D"" style=3D"color:rgb(51,51,51);font-family:Consolas,&=
#39;Liberation Mono&#39;,Courier,monospace;font-size:11.818181991577148px;l=
ine-height:17.99715805053711px;white-space:pre">numFailures</span><span cla=
ss=3D"" style=3D"font-weight:bold;color:rgb(51,51,51);font-family:Consolas,=
&#39;Liberation Mono&#39;,Courier,monospace;font-size:11.818181991577148px;=
line-height:17.99715805053711px;white-space:pre">(</span><span class=3D"" s=
tyle=3D"color:rgb(51,51,51);font-family:Consolas,&#39;Liberation Mono&#39;,=
Courier,monospace;font-size:11.818181991577148px;line-height:17.99715805053=
711px;white-space:pre">index</span><span class=3D"" style=3D"font-weight:bo=
ld;color:rgb(51,51,51);font-family:Consolas,&#39;Liberation Mono&#39;,Couri=
er,monospace;font-size:11.818181991577148px;line-height:17.99715805053711px=
;white-space:pre">)</span><span style=3D"color:rgb(51,51,51);font-family:Co=
nsolas,&#39;Liberation Mono&#39;,Courier,monospace;font-size:11.81818199157=
7148px;line-height:17.99715805053711px;white-space:pre;background-color:rgb=
(255,255,204)"> </span><span class=3D"" style=3D"font-weight:bold;color:rgb=
(51,51,51);font-family:Consolas,&#39;Liberation Mono&#39;,Courier,monospace=
;font-size:11.818181991577148px;line-height:17.99715805053711px;white-space=
:pre">&gt;=3D</span><span style=3D"color:rgb(51,51,51);font-family:Consolas=
,&#39;Liberation Mono&#39;,Courier,monospace;font-size:11.818181991577148px=
;line-height:17.99715805053711px;white-space:pre;background-color:rgb(255,2=
55,204)"> </span><span class=3D"" style=3D"color:rgb(68,85,136);font-weight=
:bold;font-family:Consolas,&#39;Liberation Mono&#39;,Courier,monospace;font=
-size:11.818181991577148px;line-height:17.99715805053711px;white-space:pre"=
>MAX_TASK_FAILURES</span><span class=3D"" style=3D"font-weight:bold;color:r=
gb(51,51,51);font-family:Consolas,&#39;Liberation Mono&#39;,Courier,monospa=
ce;font-size:11.818181991577148px;line-height:17.99715805053711px;white-spa=
ce:pre">)</span><span style=3D"color:rgb(51,51,51);font-family:Consolas,&#3=
9;Liberation Mono&#39;,Courier,monospace;font-size:11.818181991577148px;lin=
e-height:17.99715805053711px;white-space:pre;background-color:rgb(255,255,2=
04)"> </span><span class=3D"" style=3D"font-weight:bold;color:rgb(51,51,51)=
;font-family:Consolas,&#39;Liberation Mono&#39;,Courier,monospace;font-size=
:11.818181991577148px;line-height:17.99715805053711px;white-space:pre">{</s=
pan></div>

<div>?</div><div><br></div><div>I can open a pull request if this is the ca=
se.</div><div><br></div><div>Thanks,</div></div><div><div dir=3D"ltr"><div>=
Grega</div>--<br><table style=3D"line-height:12px;color:rgb(119,121,133);fo=
nt-size:11px;font-family:Helvetica,Arial,sans-serif;margin:6px 0px;padding:=
0px">

<tbody><tr><td valign=3D"top"><img src=3D"cid:ii_13b04c50817df16a" alt=3D"I=
nline image 1"></td><td style=3D"line-height:13px"><div><strong style=3D"co=
lor:rgb(34,37,103)">Grega Ke=C5=A1pret</strong><br>Analytics engineer<br><b=
r><span style=3D"color:rgb(159,159,171)">Celtra =E2=80=94 Rich Media Mobile=
 Advertising</span><br>

</div><a href=3D"http://www.celtra.com/" style=3D"font-family:Helvetica,Ari=
al,sans-serif;color:rgb(17,85,204)" target=3D"_blank">celtra.com</a><span s=
tyle=3D"color:rgb(159,159,171);font-family:Helvetica,Arial,sans-serif">=C2=
=A0|=C2=A0</span><a href=3D"http://www.twitter.com/celtramobile" style=3D"f=
ont-family:Helvetica,Arial,sans-serif;color:rgb(17,85,204)" target=3D"_blan=
k">@celtramobile</a><span style=3D"color:rgb(159,159,171)"><br>

</span></td></tr></tbody></table></div></div>
</div>

--089e0115f49a607b1204ec4dedbe--
--089e0115f49a607b1604ec4dedbf--

From dev-return-795-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org  Fri Nov 29 17:25:18 2013
Return-Path: <dev-return-795-apmail-spark-dev-archive=spark.apache.org@spark.incubator.apache.org>
X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 905E310386
	for <apmail-spark-dev-archive@minotaur.apache.org>; Fri, 29 Nov 2013 17:25:18 +0000 (UTC)
Received: (qmail 21812 invoked by uid 500); 29 Nov 2013 17:25:18 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 21779 invoked by uid 500); 29 Nov 2013 17:25:17 -0000
Mailing-List: contact dev-help@spark.incubator.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.incubator.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.incubator.apache.org>
List-Post: <mailto:dev@spark.incubator.apache.org>
List-Id: <dev.spark.incubator.apache.org>
Reply-To: dev@spark.incubator.apache.org
Delivered-To: mailing list dev@spark.incubator.apache.org
Received: (qmail 21771 invoked by uid 99); 29 Nov 2013 17:25:17 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Nov 2013 17:25:17 +0000
X-ASF-Spam-Status: No, hits=-1997.8 required=5.0
	tests=ALL_TRUSTED,HTML_MESSAGE,RP_MATCHES_RCVD
X-Spam-Check-By: apache.org
Received: from [140.211.11.3] (HELO mail.apache.org) (140.211.11.3)
    by apache.org (qpsmtpd/0.29) with SMTP; Fri, 29 Nov 2013 17:25:14 +0000
Received: (qmail 20991 invoked by uid 99); 29 Nov 2013 17:24:52 -0000
Received: from minotaur.apache.org (HELO minotaur.apache.org) (140.211.11.9)
    by apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Nov 2013 17:24:52 +0000
Received: from localhost (HELO mail-vc0-f171.google.com) (127.0.0.1)
  (smtp-auth username rxin, mechanism plain)
  by minotaur.apache.org (qpsmtpd/0.29) with ESMTP; Fri, 29 Nov 2013 17:24:51 +0000
Received: by mail-vc0-f171.google.com with SMTP id ik5so6704916vcb.2
        for <dev@spark.incubator.apache.org>; Fri, 29 Nov 2013 09:24:50 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=mime-version:in-reply-to:references:from:date:message-id:subject:to
         :content-type;
        bh=gp7hcouu8qfNjLj75j+gyfVorTONDVzZ4hqqAV5pWz8=;
        b=W4P0H5SKkeoo7uaDX7IVILvqw8Hc1khQZwYx12U7ahQjRnQWhs0as/eiS3BBNI1ps9
         nia7ZqjsizD0QqFZmx3mhYRjtJW1HTpteS/0jWTyZoUZspb0kB5OnUABD08gulZeLxDY
         wPEkF1oGItdbqPVKOk4a8XfhYSJY9SKG/ukodqlmXWa4dSvuCBVJf24tdu07KijhHkA1
         fiV7eENoIBpUCAlSRW94hmIpdnF3tngrHwhfSV5Sz+WBqOFUHKzVkURv9UP5DdUhBDqT
         J0ghSY9WbY+saQwBZRrwYnbsmjbDLNXsRrCtJqX1ObVnYhNnJRXX6r75b3pyoh1EIoON
         X4Zw==
X-Received: by 10.53.13.199 with SMTP id fa7mr2027926vdd.31.1385745890472;
 Fri, 29 Nov 2013 09:24:50 -0800 (PST)
MIME-Version: 1.0
Received: by 10.58.109.135 with HTTP; Fri, 29 Nov 2013 09:24:30 -0800 (PST)
In-Reply-To: <CAMihvYb8aR4gBBtmD2R4ZvxYo8k8w7AsbFN+myaqm344yOMciA@mail.gmail.com>
References: <CAMihvYb8aR4gBBtmD2R4ZvxYo8k8w7AsbFN+myaqm344yOMciA@mail.gmail.com>
From: Reynold Xin <rxin@apache.org>
Date: Fri, 29 Nov 2013 09:24:30 -0800
Message-ID: <CAC1ssC7RV88z79RVkn6vF46BYgujVimbku92p2S2RS0ygtTECg@mail.gmail.com>
Subject: Re: spark.task.maxFailures
To: "dev@spark.incubator.apache.org" <dev@spark.incubator.apache.org>
Content-Type: multipart/alternative; boundary=001a1133faa28cfd3904ec541bd7
X-Virus-Checked: Checked by ClamAV on apache.org

--001a1133faa28cfd3904ec541bd7
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

Looks like a bug to me. Can you submit a pull request?



On Fri, Nov 29, 2013 at 2:02 AM, Grega Ke=C5=A1pret <grega@celtra.com> wrot=
e:

> Looking at
> http://spark.incubator.apache.org/docs/latest/configuration.html
> docs says:
> Number of individual task failures before giving up on the job. Should be
> greater than or equal to 1. Number of allowed retries =3D this value - 1.
>
> However, looking at the code
>
> https://github.com/apache/incubator-spark/blob/master/core/src/main/scala=
/org/apache/spark/scheduler/cluster/ClusterTaskSetManager.scala#L532
>
> if I set spark.task.maxFailures to 1, this means that job will fail after
> task fails for the second time. Shouldn't this line be corrected to if (
> numFailures(index) >=3D MAX_TASK_FAILURES) {
> ?
>
> I can open a pull request if this is the case.
>
> Thanks,
> Grega
> --
> [image: Inline image 1]
> *Grega Ke=C5=A1pret*
> Analytics engineer
>
> Celtra =E2=80=94 Rich Media Mobile Advertising
> celtra.com <http://www.celtra.com/> | @celtramobile<http://www.twitter.co=
m/celtramobile>
>

--001a1133faa28cfd3904ec541bd7--

