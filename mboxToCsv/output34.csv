Jeff Zhang <zjffdu@gmail.com>,"Tue, 1 Mar 2016 13:07:20 +0800",Support virtualenv in PySpark,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","I have created jira for this feature , comments and feedback are welcome
about how to improve it and whether it's valuable for users.

https://issues.apache.org/jira/browse/SPARK-13587


Here's some background info and status of this work.


Currently, it's not easy for user to add third party python packages in
pyspark.

   not suitable for complicated dependency, especially with transitive
   dependency)
   - Another way is install packages manually on each node (time wasting,
   and not easy to switch to different environment)

virtualenv another is through conda.

I have implemented POC for this features. Here's one simple command for how
to use virtualenv in pyspark

bin/spark-submit --master yarn --deploy-mode client --conf
""spark.pyspark.virtualenv.enabled=true"" --conf
""spark.pyspark.virtualenv.type=conda"" --conf
""spark.pyspark.virtualenv.requirements=/Users/jzhang/work/virtualenv/conda.txt""
--conf ""spark.pyspark.virtualenv.path=/Users/jzhang/anaconda/bin/conda""
 ~/work/virtualenv/spark.py

There're 4 properties needs to be set

   - spark.pyspark.virtualenv.enabled (enable virtualenv)
   - spark.pyspark.virtualenv.type (native/conda are supported, default is
   native)
   - spark.pyspark.virtualenv.requirements (requirement file for the
   dependencies)
   - spark.pyspark.virtualenv.path (path to the executable for for
   virtualenv/conda)






Best Regards

Jeff Zhang
"
yasincelik <yasincelik27@gmail.com>,"Mon, 29 Feb 2016 22:25:54 -0700 (MST)",Spark performance comparison for research,dev@spark.apache.org,"Hello,

I am working on a project as a part of my research. The system I am working
on is basically an in-memory computing system. I want to compare its
performance with Spark. Here is how I conduct experiments. For my project: I
have a software defined network(SDN) that allows HPC applications to share
data, such as sending and receiving messages through this network. For
example, in a word count application, a master reads a 10GB text file from
hard drive, slices into small chunks, and distribute the chunks. Each worker
will fetch some chunks, process them, and send them back to the SDN. Then
master collects the results.

To compare with Spark, I run word count application. I run Spark in
standalone mode. I do not use any cluster manager. There is no pre-installed
HDFS. I use PBS to reserve nodes, which gives me list of nodes. Then I
simply run Spark on these nodes. Here is the command to run Spark:
~/SPARK/bin/spark-submit --class word.JavaWordCount  --num-executors 1
spark.jar ~/data.txt  > ~/wc

Technically, these experiments are run under same conditions. Read file, cut
it into small chunks, distribute chunks, process chunks, collect results.
Do you think this is a reasonable comparison? Can someone make this claim:
""Well, Spark is designed to work on top of HDFS, in which the data is
already stored in nodes, and Spark jobs are submitted to these nodes to take
advantage of data locality""


Any comment is appreciated.

Thanks



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 29 Feb 2016 21:34:25 -0800",Re: Spark performance comparison for research,yasincelik <yasincelik27@gmail.com>,"That seems reasonable, but it seems pretty unfair to the HPC setup in which
the master is reading all the data. Basically you can make HPC perform
infinitely worse by just adding more modes to Spark.


"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Tue, 1 Mar 2016 07:41:52 +0100",Re: Spark performance comparison for research,yasincelik <yasincelik27@gmail.com>,"I am not sure what you compare here. You would need to provide additional details, such as algorithms and functionality supported by your framework. For instance, Spark has built-in fault-tolerance and is a generic framework, which has advantage with respect to development and operations, but may have disadvantage in certain use cases wrt performance.
Another concern it is the SDN which could be configured in disadvantageous for your approach or for Spark. I would not use it for generic performance comparison, except if it is the production network of your company and you want to compare it only for your company.

 I doubt that focusing only on performance for a framework makes scientifically sense. 
Your approach sounds too simple to be of scientific value, but more for unscientific marketing purposes. That being said, it could be that you did not provide all the details.

g
 I


er
ed
ut

ke
n3.nabble.com/Spark-performance-comparison-for-research-tp16498.html
com.

---------------------------------------------------------------------


"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 1 Mar 2016 16:29:06 +0800",Re: Support virtualenv in PySpark,Mohannad Ali <mandoz@gmail.com>,"I may not express it clearly. This method is trying to create virtualenv
before python worker start, and this virtualenv is application scope, after
the spark application job finish, the virtualenv will be cleanup. And the
virtualenvs don't need to be the same path for each node (In my POC, it is
the yarn container working directory). So that means user don't need to
manually install packages on each node (sometimes you even can't install
packages on cluster due to security reason). This is the biggest benefit
and purpose that user can create virtualenv on demand without touching each
node even when you are not administrator.  The cons is the extra cost for
installing the required packages before starting python worker. But if it
is an application which will run for several hours then the extra cost can
be ignored.


g
k.virtualenv.enabled=true"" --conf ""spark.pyspark.virtualenv.type=conda"" --conf ""spark.pyspark.virtualenv.requirements=/Users/jzhang/work/virtualenv/conda.txt"" --conf ""spark.pyspark.virtualenv.path=/Users/jzhang/anaconda/bin/conda""  ~/work/virtualenv/spark.py


-- 
Best Regards

Jeff Zhang
"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 1 Mar 2016 00:57:16 -0800",Re: What should be spark.local.dir in spark on yarn?,Jeff Zhang <zjffdu@gmail.com>,"spark 1.6.0 uses /tmp in the following places
# spark.local.dir is not set
yarn.nodemanager.local-dirs=/data01/yarn/nm,/data02/yarn/nm

1. spark-shell on start
16/03/01 08:33:48 INFO storage.DiskBlockManager: Created local directory at
/tmp/blockmgr-ffd3143d-b47f-4844-99fd-2d51c6a05d05

2. spark-shell on start
16/03/01 08:33:50 INFO yarn.Client: Uploading resource
file:/tmp/spark-456184c9-d59f-48f4-a9b0-560b7d310655/__spark_conf__6943938018805427428.zip
->
hdfs://ip-10-101-124-30:8020/user/hadoop/.sparkStaging/application_1456776184284_0047/__spark_conf__6943938018805427428.zip

3. spark-shell spark-sql (Hive) on start
16/03/01 08:34:06 INFO session.SessionState: Created local directory:
/tmp/01705299-a384-4e85-923b-e858017cf351_resources
16/03/01 08:34:06 INFO session.SessionState: Created HDFS directory:
/tmp/hive/hadoop/01705299-a384-4e85-923b-e858017cf351
16/03/01 08:34:06 INFO session.SessionState: Created local directory:
/tmp/hadoop/01705299-a384-4e85-923b-e858017cf351
16/03/01 08:34:06 INFO session.SessionState: Created HDFS directory:
/tmp/hive/hadoop/01705299-a384-4e85-923b-e858017cf351/_tmp_space.db

4. Spark Executor container uses hadoop.tmp.dir /data01/tmp/hadoop-${
user.name} for s3 output

scala> sc.parallelize(1 to
10).saveAsTextFile(""s3n://my_bucket/test/p10_13"");

16/03/01 08:41:13 INFO s3native.NativeS3FileSystem: OutputStream for
key 'test/p10_13/part-00000' writing to tempfile
'/data01/tmp/hadoop-hadoop/s3/output-7399167152756918334.tmp'


--------------------------------------------------

if I set spark.local.dir=/data01/tmp then #1 and #2 uses /data01/tmp
instead of /tmp

--------------------------------------------------


1. 16/03/01 08:47:03 INFO storage.DiskBlockManager: Created local
directory at /data01/tmp/blockmgr-db88dbd2-0ef4-433a-95ea-b33392bbfb7f


2. 16/03/01 08:47:05 INFO yarn.Client: Uploading resource
file:/data01/tmp/spark-aa3e619c-a368-4f95-bd41-8448a78ae456/__spark_conf__368426817234224667.zip
-> hdfs://ip-10-101-124-30:8020/user/hadoop/.sparkStaging/application_1456776184284_0050/__spark_conf__368426817234224667.zip


3. spark-sql (hive) still uses /tmp

16/03/01 08:47:20 INFO session.SessionState: Created local directory:
/tmp/d315926f-39d7-4dcb-b3fa-60e9976f7197_resources
16/03/01 08:47:20 INFO session.SessionState: Created HDFS directory:
/tmp/hive/hadoop/d315926f-39d7-4dcb-b3fa-60e9976f7197
16/03/01 08:47:20 INFO session.SessionState: Created local directory:
/tmp/hadoop/d315926f-39d7-4dcb-b3fa-60e9976f7197
16/03/01 08:47:20 INFO session.SessionState: Created HDFS directory:
/tmp/hive/hadoop/d315926f-39d7-4dcb-b3fa-60e9976f7197/_tmp_space.db


4. executor uses hadoop.tmp.dir for s3 output

16/03/01 08:50:01 INFO s3native.NativeS3FileSystem: OutputStream for
key 'test/p10_16/_SUCCESS' writing to tempfile
'/data01/tmp/hadoop-hadoop/s3/output-2541604454681305094.tmp'


5. /data0X/yarn/nm used for usercache

16/03/01 08:41:12 INFO storage.DiskBlockManager: Created local
directory at /data01/yarn/nm/usercache/hadoop/appcache/application_1456776184284_0047/blockmgr-af5




"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Mar 2016 01:00:52 -0800",Re: Is spark.driver.maxResultSize used correctly ?,Jeff Zhang <zjffdu@gmail.com>,"How big of a deal is this though? If I am reading your email correctly,
either way this job will fail. You simply want it to fail earlier in the
executor side, rather than collecting it and fail on the driver side?


"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 1 Mar 2016 17:24:53 +0800",Re: Is spark.driver.maxResultSize used correctly ?,Reynold Xin <rxin@databricks.com>,"Check the code again. Looks like currently the task result will be loaded
into memory no matter it is DirectTaskResult or InDirectTaskResult.
Previous I thought InDirectTaskResult can be loaded into memory later which
can save memory, RDD#collectAsIterator is what I thought that may save
memory.




-- 
Best Regards

Jeff Zhang
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 1 Mar 2016 17:39:28 +0800",Re: What should be spark.local.dir in spark on yarn?,Alexander Pivovarov <apivovarov@gmail.com>,"You are using yarn-client mode, the driver is not yarn container, so it can
not use yarn.nodemanager.local-dirs, only have to use spark.local.dir which
/tmp by default. But usually driver won't cost too much disk, so it should
be fine to use /tmp in driver side.


8018805427428.zip
6184284_0047/__spark_conf__6943938018805427428.zip
'test/p10_13/part-00000' writing to tempfile '/data01/tmp/hadoop-hadoop/s3/output-7399167152756918334.tmp'
stead of /tmp
ry at /data01/tmp/blockmgr-db88dbd2-0ef4-433a-95ea-b33392bbfb7f
p/spark-aa3e619c-a368-4f95-bd41-8448a78ae456/__spark_conf__368426817234224667.zip -> hdfs://ip-10-101-124-30:8020/user/hadoop/.sparkStaging/application_1456776184284_0050/__spark_conf__368426817234224667.zip
p/d315926f-39d7-4dcb-b3fa-60e9976f7197_resources
/hive/hadoop/d315926f-39d7-4dcb-b3fa-60e9976f7197
p/hadoop/d315926f-39d7-4dcb-b3fa-60e9976f7197
/hive/hadoop/d315926f-39d7-4dcb-b3fa-60e9976f7197/_tmp_space.db
'test/p10_16/_SUCCESS' writing to tempfile '/data01/tmp/hadoop-hadoop/s3/output-2541604454681305094.tmp'
at /data01/yarn/nm/usercache/hadoop/appcache/application_1456776184284_0047/blockmgr-af5
on
m
s


-- 
Best Regards

Jeff Zhang
"
Jerry Lam <chilinglam@gmail.com>,"Tue, 1 Mar 2016 09:16:18 -0500",SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Spark dev list <dev@spark.apache.org>,"Hi Spark developers,

Will you consider to add support for implementing ""Pattern matching in
sequences of rows""? More specifically, I'm referring to this:
http://web.cs.ucla.edu/classes/fall15/cs240A/notes/temporal/row-pattern-recogniton-11.pdf

This is a very cool/useful feature to pattern matching over live
stream/archived data. It is sorted of related to machine learning because
this is usually used in clickstream analysis or path analysis. Also it is
related to streaming because of the nature of the processing (time series
data mostly). It is SQL because there is a good way to express and optimize
the query.

Best Regards,

Jerry
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Tue, 1 Mar 2016 15:39:46 +0100",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Jerry Lam <chilinglam@gmail.com>,"Hi Jerry,

This is not on any roadmap. I (shortly) browsed through this; and this
looks like some sort of a window function with very awkward syntax. I think
spark provided better constructs for this using dataframes/datasets/nested
data...

Feel free to submit a PR.

Kind regards,

Herman van HÃ¶vell

2016-03-01 15:16 GMT+01:00 Jerry Lam <chilinglam@gmail.com>:

ecogniton-11.pdf
ze
"
Karan Kumar <karankumar1100@gmail.com>,"Tue, 1 Mar 2016 21:47:13 +0530",Re: [Proposal] Enabling time series analysis on spark metrics,"user@spark.apache.org, dev@spark.apache.org",#NAME?
Jerry Lam <chilinglam@gmail.com>,"Tue, 1 Mar 2016 11:19:57 -0500",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Hi Herman,

Thank you for your reply!
This functionality usually finds its place in financial services which use
CEP (complex event processing) for correlation and pattern matching. Many
commercial products have this including Oracle and Teradata Aster Data MR
Analytics. I do agree the syntax a bit awkward but after you understand it,
it is actually very compact for expressing something that is very complex.
Esper has this feature partially implemented (
http://www.espertech.com/esper/release-5.1.0/esper-reference/html/match-recognize.html
).

I found the Teradata Analytics documentation best to describe the usage of
it. For example (note npath is similar to match_recognize):

SELECT last_pageid, MAX( count_page80 )
 FROM nPath(
 ON ( SELECT * FROM clicks WHERE category >= 0 )
 PARTITION BY sessionid
 ORDER BY ts
 PATTERN ( 'A.(B|C)*' )
 MODE ( OVERLAPPING )
 SYMBOLS ( pageid = 50 AS A,
           pageid = 80 AS B,
           pageid <> 80 AND category IN (9,10) AS C )
 RESULT ( LAST ( pageid OF ANY ( A,B,C ) ) AS last_pageid,
          COUNT ( * OF B ) AS count_page80,
          COUNT ( * OF ANY ( A,B,C ) ) AS count_any )
 )
 WHERE count_any >= 5
 GROUP BY last_pageid
 ORDER BY MAX( count_page80 )

The above means:
Find user click-paths starting at pageid 50 and passing exclusively through
either pageid 80 or pages in category 9 or category 10. Find the pageid of
the last page in the path and count the number of times page 80 was
visited. Report the maximum count for each last page, and sort the output
by the latter. Restrict to paths containing at least 5 pages. Ignore pages
in the sequence with category < 0.

If this query is written in pure SQL (if possible at all), it requires
several self-joins. The interesting thing about this feature is that it
integrates SQL+Streaming+ML in one (perhaps potentially graph too).

Best Regards,

Jerry



nk
d
recogniton-11.pdf
e
s
s
ize
"
Henri Dubois-Ferriere <henridf@gmail.com>,"Tue, 1 Mar 2016 08:34:56 -0800",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Jerry Lam <chilinglam@gmail.com>,"fwiw Apache Flink just added CEP. Queries are constructed programmatically
rather than in SQL, but the underlying functionality is similar.

https://issues.apache.org/jira/browse/FLINK-3215


e
t,
.
ecognize.html
f
re
ink
ed
-recogniton-11.pdf
se
is
es
mize
"
Jerry Lam <chilinglam@gmail.com>,"Tue, 1 Mar 2016 11:48:39 -0500",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Henri Dubois-Ferriere <henridf@gmail.com>,"Hi Henri,

Finally, there is a good reason for me to use Flink! Thanks for sharing
this information. This is exactly the solution I'm looking for especially
the ticket references a paper I was reading a week ago. It would be nice if
Flink adds support SQL because this makes business analyst (traders as
well) a way to express it.

Best Regards,

Jerry


y
ata
nd
recognize.html
0
ore
hink
ted
n-recogniton-11.pdf
use
 is
ies
imize
"
Alex Kozlov <alexvk@gmail.com>,"Tue, 1 Mar 2016 08:58:01 -0800",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Henri Dubois-Ferriere <henridf@gmail.com>,"For the purpose of full disclosure, I think Scala offers a much more
efficient pattern matching paradigm.  Using nPath is like using assembler
to program distributed systems.  Cannot tell much here today, but the
pattern would look like:

     |     def matchSessions(h: Seq[Session[PageView]], id: String, p:
Seq[PageView]) :

Seq[Session[PageView]] = {    |       p match {

     |         case Nil => Nil

     |         case PageView(ts1, ""company.com>homepage"") :: PageView(ts2,

""company.com>plus>products landing"") :: tail if ts2 > ts1 + 600 =>

     |           matchSessions(h, id, tail).+:(new Session(id, p))

     |         case _ => matchSessions(h, id, p.tail)

     |       }

Look for Scala case statements with guards and upcoming book releases.

http://docs.scala-lang.org/tutorials/tour/pattern-matching
https://www.safaribooksonline.com/library/view/scala-cookbook/9781449340292/ch03s14.html


y
ata
nd
recognize.html
0
ore
hink
ted
n-recogniton-11.pdf
use
 is
ies
imize


-- 
Alex Kozlov
(408) 507-4987
(650) 887-2135 efax
alexvk@gmail.com
"
Jerry Lam <chilinglam@gmail.com>,"Tue, 1 Mar 2016 12:07:25 -0500",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Alex Kozlov <alexvk@gmail.com>,"Hi Alex,

We went through this path already :) This is the reason we try other
approaches. The recursion makes it very inefficient for some cases.
For details, this paper describes it very well:
https://people.cs.umass.edu/%7Eyanlei/publications/sase-sigmod08.pdf
which is the same paper references in Flink ticket.

Please let me know if I overlook something. Thank you for sharing this!

Best Regards,

Jerry





92/ch03s14.html
.
Data
and
-recognize.html
e
80
nore
<
think
sted
n
rn-recogniton-11.pdf
ause
t is
ries
timize
"
Alex Kozlov <alexvk@gmail.com>,"Tue, 1 Mar 2016 09:35:57 -0800",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Jerry Lam <chilinglam@gmail.com>,"Looked at the paper: while we can argue on the performance side, I think
semantically the Scala pattern matching is much more expressive.  The time
will decide.


r
292/ch03s14.html
s
g.
 Data
tand
h-recognize.html
e
he
 80
gnore
t
 <
s
 think
ested
ern-recogniton-11.pdf
cause
it is
eries
ptimize


-- 
Alex Kozlov
(408) 507-4987
(650) 887-2135 efax
alexvk@gmail.com
"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Mar 2016 13:44:51 -0800",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Alex Kozlov <alexvk@gmail.com>,"There are definitely pros and cons for Scala vs SQL-style CEP. Scala might
be more powerful, but the target audience is very different.

How much usage is there for a CEP style SQL syntax in practice? I've never
seen it coming up so far.




e
er
0292/ch03s14.html
m
is
h
ng.
r Data
stand
ch-recognize.html
the
e 80
e
Ignore
s
it
r <
tax. I
tern-recogniton-11.pdf
ecause
 it is
series
optimize
"
Reynold Xin <rxin@databricks.com>,"Tue, 1 Mar 2016 14:06:34 -0800",Re: [Proposal] Enabling time series analysis on spark metrics,Karan Kumar <karankumar1100@gmail.com>,"Is the suggestion just to use a different config (and maybe fallback to
appid) in order to publish metrics? Seems reasonable.



"
Jerry Lam <chilinglam@gmail.com>,"Tue, 1 Mar 2016 17:25:41 -0500",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

You are right. It is about the audience. For instance, in many of my cases,
the SQL style is very attractive if not mandatory for people with minimum
programming knowledge. SQL has its place for communication. Last time I
show someone spark dataframe-style, they immediately said it is too
difficult to use. When I change it to SQL, they are suddenly happy and say
how you do this. It sounds stupid but that's what it is for now.

The following example will make some banks happy (copy from the Oracle
solution):

SELECT *
FROM Ticker MATCH_RECOGNIZE (
     PARTITION BY symbol
     ORDER BY tstamp
     MEASURES  STRT.tstamp AS start_tstamp,
               LAST(DOWN.tstamp) AS bottom_tstamp,
               LAST(UP.tstamp) AS end_tstamp
     ONE ROW PER MATCH
     AFTER MATCH SKIP TO LAST UP
     PATTERN (STRT DOWN+ UP+)
     DEFINE
        DOWN AS DOWN.price < PREV(DOWN.price),
        UP AS UP.price > PREV(UP.price)
     ) MR
ORDER BY MR.symbol, MR.start_tstamp;

Basically this query finds all cases where stock prices dipped to a bottom
price and then rose (the popular V-shape).  It might be confusing at first
but it is still readable for many users who know SQL. Note that the PATTERN
is interesting; it is a regular expression on the symbols defined (DOWN and
UP, STRT is not define so it means it matches any event).

Most CEP solutions have a SQL-like interface.

Best Regards,

Jerry




t
r
me
ler
40292/ch03s14.html
 is
radata
er you
that is
tch-recognize.html
 the
ge 80
he
 Ignore
e is
h too).
er <
ntax. I
g
ttern-recogniton-11.pdf
because
o it is
 series
 optimize
"
Teng Liao <tliao@palantir.com>,"Tue, 1 Mar 2016 23:19:26 +0000",Dataframe Partitioning,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I was wondering what the rationale behind defaulting all repartitioning to spark.sql.shuffle.partitions is. Iâ€™m seeing a huge overhead when running a job whose input partitions is 2 and, using the default value for spark.sql.shuffle.partitions, this is now 200. Thanks.

-Teng Fei Liao
"
Michael Armbrust <michael@databricks.com>,"Tue, 1 Mar 2016 15:43:06 -0800",Re: Dataframe Partitioning,Teng Liao <tliao@palantir.com>,"If you have to pick a number, its better to over estimate than
underestimate since task launching in spark is relatively cheap compared to
spilling to disk or OOMing (now much less likely due to Tungsten).
Eventually, we plan to make this dynamic, but you should tune for your
particular workload.


o
running a
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 1 Mar 2016 16:15:57 -0800",Re: Dataframe Partitioning,Michael Armbrust <michael@databricks.com>,"I don't entirely agree.  You're best off picking the right size :).  That's
almost impossible, though, since at the input end of the query processing
you often want a large number of partitions to get sufficient parallelism
for both performance and to avoid spilling or OOM, while at the output end
of the query processing (after all the pruning and filtering) you often
have only a few result rows, which means that splitting those few rows
across many partitions in order to do a sort or similar is actually pretty
silly and inefficient. I'll frequently see sorts where the per-partition
sorts have only one or two records and it would have been quicker and more
efficient to sort using a small number of partitions rather than using
RangePartitioning to split the few rows across many partitions, then doing
a degenerate/trivial form of sort on each of those partitions with their
one or two rows, and finally merging all those tiny partitions back in
order to produce the final results.

Since the optimum number of shuffle partitions is different at different
points in the query processing flow, it's really impossible to pick a
static best number of shuffle partitions.  Using spark.sql.adaptive.enabled
to turn on ExchangeCoordinator and dynamically set the number of shuffle
partitions mostly works pretty well, but it still has at least a couple of
it doesn't stop coalescing partitions until after the coalesced partition
size exceeds a target value; so if you've got some big ugly partitions that
exceed the target size all on their own, they'll often be even bigger and
uglier after the ExchangeCoordinator is done merging them with a few
smaller partitions.  The other issue is that adaptive partitioning doesn't
even try to do anything currently with any partitioning other than
HashPartitioning, so you've still got the sorting problem using
RangePartitioning that I just got done describing.

I've actually started working on addressing each of those problems.


to
hen running
"
Matt Cheah <mcheah@palantir.com>,"Wed, 2 Mar 2016 01:17:43 +0000",HashedRelation Memory Pressure on Broadcast Joins,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

I had a quick question regarding our implementation of UnsafeHashedRelation and HashedRelation<https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala>. It appears that we copy the rows that weâ€™ve collected into memory upon inserting them into the hash table in UnsafeHashedRelation#apply(). I was wondering why we are copying the rows every time? I canâ€™t imagine these rows being mutable in this scenario.

The context is that Iâ€™m looking into a case where a small data frame should fit in the driverâ€™s memory, but my driver ran out of memory after I increased the autoBroadcastJoinThreshold. YourKit is indicating that this logic is consuming more memory than my driver can handle.

Thanks,

-Matt Cheah
"
yash datta <saucam@gmail.com>,"Wed, 2 Mar 2016 10:00:46 +0530",Re: Dataframe Partitioning,Mark Hamstra <mark@clearstorydata.com>,"+1
This is  one of the most common problems we encounter in our flow. Mark, I
am happy to help if you would like to share some of the workload.

Best
Yash


he
ew
ed
f
at
t
 to
when running

-- 
When events unfold with calm and ease
When the winds that bl"
Ewan Leith <ewan.leith@realitymine.com>,"Wed, 2 Mar 2016 09:44:26 +0000","Selecting column in dataframe created with incompatible schema
 causes AnalysisException","""dev@spark.apache.org"" <dev@spark.apache.org>","When you create a dataframe using the sqlContext.read.schema() API, if you pass in a schema that's compatible with some of the records, but incompatible with others, it seems you can't do a .select on the problematic columns, instead you get an AnalysisException error.

I know loading the wrong data isn't good behaviour, but if you're reading data from (for example) JSON files, there's going to be malformed files along the way. I think it would be nice to handle this error in a nicer way, though I don't know the best way to approach it.

Before I raise a JIRA ticket about it, would people consider this to be a bug or expected behaviour?

I've attached a couple of sample JSON files and the steps below to reproduce it, by taking the inferred schema from the simple1.json file, and applying it to a union of simple1.json and simple2.json. You can visually see the data has been parsed as I think you'd want if you do a .select on the parent column and print out the output, but when you do a select on the problem column you instead get an exception.

scala> val s1Rdd = sc.wholeTextFiles(""/tmp/simple1.json"").map(x => x._2)
s1Rdd: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[171] at map at <console>:27

scala> val s1schema = sqlContext.read.json(s1Rdd).schema
s1schema: org.apache.spark.sql.types.StructType = StructType(StructField(data,ArrayType(StructType(StructField(stuff,ArrayType(StructType(StructField(onetype,ArrayType(StructType(StructField(id,LongType,true), StructField(name,StringType,true)),true),true), StructField(othertype,ArrayType(StructType(StructField(company,StringType,true), StructField(id,LongType,true)),true),true)),true),true)),true),true))

scala> sqlContext.read.schema(s1schema).json(s2Rdd).select(""data.stuff"").take(2).foreach(println)
[WrappedArray(WrappedArray([WrappedArray([1,John Doe], [2,Don Joeh]),null], [null,WrappedArray([ACME,2])]))]
[WrappedArray(WrappedArray([null,WrappedArray([null,1], [null,2])], [WrappedArray([2,null]),null]))]

scala> sqlContext.read.schema(s1schema).json(s2Rdd).select(""data.stuff.onetype"")
org.apache.spark.sql.AnalysisException: cannot resolve 'data.stuff[onetype]' due to data type mismatch: argument 2 requires integral type, however, 'onetype' is of string type.;
                at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
                at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:65)
                at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
                at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)
                at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)

(The full exception is attached too).

What do people think, is this a bug?

Thanks,
Ewan
org.apache.spark.sql.AnalysisException: cannot resolve 'data.stuff[onetype]' due to data type mismatch: argument 2 requires integral type, however, 'onetype' is of string type.;
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:65)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:57)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:319)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:53)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:318)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:316)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:265)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:305)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:316)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionUp$1(QueryPlan.scala:107)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:117)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2$1.apply(QueryPlan.scala:121)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
  at scala.collection.AbstractTraversable.map(Traversable.scala:105)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$recursiveTransform$2(QueryPlan.scala:121)
  at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:125)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:125)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:57)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:105)
  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:44)
  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:34)
  at org.apache.spark.sql.DataFrame.<init>(DataFrame.scala:133)
  at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$withPlan(DataFrame.scala:2165)
  at org.apache.spark.sql.DataFrame.select(DataFrame.scala:751)
  at org.apache.spark.sql.DataFrame.select(DataFrame.scala:768)
  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:34)
  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:39)
  at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:41)
  at $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:43)
  at $iwC$$iwC$$iwC$$iwC.<init>(<console>:45)
  at $iwC$$iwC$$iwC.<init>(<console>:47)
  at $iwC$$iwC.<init>(<console>:49)
  at $iwC.<init>(<console>:51)
  at <init>(<console>:53)
  at .<init>(<console>:57)
  at .<clinit>(<console>)
  at .<init>(<console>:7)
  at .<clinit>(<console>)
  at $print(<console>)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
  at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
  at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
  at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
  at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
  at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
  at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
  at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
  at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
  at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
  at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
  at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
  at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
  at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
  at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
  at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
  at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
  at org.apache.spark.repl.Main$.main(Main.scala:31)
  at org.apache.spark.repl.Main.main(Main.scala)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:606)
  at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
  at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
  at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
  at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
  at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

---------------------------------------------------------------------"
Steve Loughran <stevel@hortonworks.com>,"Wed, 2 Mar 2016 15:03:28 +0000","Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event
 Data",,"
s, the SQL style is very attractive if not mandatory for people with minimum programming knowledge.

but SQL skills instead. Which is just relational set theory with a syntax, Structured English Query Language from the IBM R project of the mid 1970s (\cite{Gray et al, An evaluation of System R})

If you look at why SQL snuck back in as a layer atop the ""Post-SQL systems"", it's 

(a) tooling
(b) declarative queries can be optimised by query planners
(c) a lot of people who do queries on existing systems can migrate to the new platforms. This is why FB wrote Hive; their PHP GUI teams didn't want to learn Java.


rame-style, they immediately said it is too difficult to use. When I change it to SQL, they are suddenly happy and say how you do this. It sounds stupid but that's what it is for now.

try showing the python syntax. Python is an easier language to learn, and its list comprehensions are suspiciously close to applied set theory.




---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 2 Mar 2016 10:07:47 -0800","Re: Selecting column in dataframe created with incompatible schema
 causes AnalysisException",Ewan Leith <ewan.leith@realitymine.com>,"-dev +user

StructType(StructField(data,ArrayType(StructType(StructField(
uctField(id,LongType,true),


Its not a great error message, but as the schema above shows, stuff is an
array, not a struct.  So, you need to pick a particular element (using [])
be"
Jerry Lam <chilinglam@gmail.com>,"Wed, 2 Mar 2016 13:10:07 -0500",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Steve Loughran <stevel@hortonworks.com>,"Hi guys,

FYI... this wiki page (StreamSQL: https://en.wikipedia.org/wiki/StreamSQL)
has some histories related Event Stream Processing and SQL.

Hi Steve,

It is difficult to ask your customers that they should learn a new language
when they are not programmers :)
I don't know where/why they learn SQL-like languages. Do business schools
teach SQL??

Best Regards,

Jerry



"
Davies Liu <davies@databricks.com>,"Wed, 2 Mar 2016 10:15:28 -0800",Re: HashedRelation Memory Pressure on Broadcast Joins,Matt Cheah <mcheah@palantir.com>,"UnsafeHashedRelation and HashedRelation could also be used in Executor
(for non-broadcast hash join), then the UnsafeRow could come from
UnsafeProjection,
so We should copy the rows for safety.

We could have a smarter copy() for UnsafeRow (avoid the copy if it's
already copied),
but I don't think this copy here will increase the memory pressure.
The total memory
will be determined by how many rows are stored in the hash tables.

In general, if you do not have enough memory, just don't increase
autoBroadcastJoinThreshold,
or the performance could be worse because of full GC.

Sometimes the tables looks small as compressed files (for example,
parquet file),
once it's loaded into memory, it could required much more memory than the size
of file on disk.


on
ollected
ario.
ame should
 I

---------------------------------------------------------------------


"
Matt Cheah <mcheah@palantir.com>,"Wed, 2 Mar 2016 18:45:41 +0000",Re: HashedRelation Memory Pressure on Broadcast Joins,Davies Liu <davies@databricks.com>,"I would expect the memory pressure to grow because not only are we storing
the backing array to the iterator of the rows on the driver, but weâ€™re
also storing a copy of each of those rows in the hash table. Whereas if we
didnâ€™t do the copy on the drive side then the hash table would only have
to store pointers to those rows in the array. Perhaps we can think about
whether or not we want to be using the HashedRelation constructs in
broadcast join physical plans?

The file isnâ€™t compressed - it was a 150MB CSV living on HDFS. I would
expect it to fit in a 1GB heap, but I agree that it is difficult to reason
about dataset size on disk vs. memory.

-Matt Cheah

On 3/2/16, 10:15 AM, ""Davies Liu"" <davies@databricks.com> wrote:

>UnsafeHashedRelation and HashedRelation could also be used in Executor
>(for non-broadcast hash join), then the UnsafeRow could come from
>UnsafeProjection,
>so We should copy the rows for safety.
>
>We could have a smarter copy() for UnsafeRow (avoid the copy if it's
>already copied),
>but I don't think this copy here will increase the memory pressure.
>The total memory
>will be determined by how many rows are stored in the hash tables.
>
>In general, if you do not have enough memory, just don't increase
>autoBroadcastJoinThreshold,
>or the performance could be worse because of full GC.
>
>Sometimes the tables looks small as compressed files (for example,
>parquet file),
>once it's loaded into memory, it could required much more memory than the
>size
>of file on disk.
>
>
>On Tue, Mar 1, 2016 at 5:17 PM, Matt Cheah <mcheah@palantir.com> wrote:
>> Hi everyone,
>>
>> I had a quick question regarding our implementation of
>>UnsafeHashedRelation
>> and HashedRelation. It appears that we copy the rows that weâ€™ve
>>collected
>> into memory upon inserting them into the hash table in
>> UnsafeHashedRelation#apply(). I was wondering why we are copying the
>>rows
>> every time? I canâ€™t imagine these rows being mutable in this scenario.
>>
>> The context is that Iâ€™m looking into a case where a small data frame
>>should
>> fit in the driverâ€™s memory, but my driver ran out of memory after I
>> increased the autoBroadcastJoinThreshold. YourKit is indicating that
>>this
>> logic is consuming more memory than my driver can handle.
>>
>> Thanks,
>>
>> -Matt Cheah

"
Cody Koeninger <cody@koeninger.org>,"Wed, 2 Mar 2016 13:17:23 -0600",Re: Upgrading to Kafka 0.9.x,Jay Kreps <jay@confluent.io>,"Jay, thanks for the response.

Regarding the new consumer API for 0.9, I've been reading through the code
for it and thinking about how it fits in to the existing Spark integration.
So far I've seen some interesting challenges, and if you (or anyone else on
the dev list) have time to provide some hints, I'd appreciate it.

To recap how the existing Spark integration works (this is all using the
Kafka simple consumer api):

- Single driver node.  For each spark microbatch, it queries Kafka for the
offset high watermark for all topicpartitions of interest.  Creates one
spark partition for each topicpartition.  The partition doesn't have
messages, it just has a lower offset equal to the last consumed position,
upper offset equal to the high water mark. Sends those partitions to the
workers.

- Multiple worker nodes.  For each spark partition, it opens a simple
consumer, consumes from kafka the lower to the upper offset for a single
topicpartition, closes the consumer.

This is really blunt, but it actually works better than the integration
based on the older higher level consumer.  Churn of simple consumers on the
worker nodes in practice wasn't much of a problem (because the granularity
of microbatches is rarely under 1 second), so we don't even bother to cache
connections between batches.

The new consumer api presents some interesting challenges

consumer group with dynamic topic subscription (so that users can take
advantage of topic patterns, etc).  Heartbeat happens only on a poll.  But
clearly the driver doesn't actually want to poll any messages, because that
load should be distributed to workers. I've seen KIP-41, which might help
if polling a single message is sufficiently lightweight.  In the meantime
the only things I can think of are trying to subclass to make a .heartbeat
method, or possibly setting max fetch bytes to a very low value.

and closing an instance of the new consumer every microbatch, since
prefetching, security, metadata requests all make that heavier weight than
a simple consumer.  The new consumer doesn't have a way to poll for only a
given number of messages (again KIP-41 would help here).  But the new
consumer also doesn't provide a way to poll for only a given
topicpartition, and the .pause method flushes fetch buffers so it's not an
option either.  I don't see a way to avoid caching one consumer per
topicpartition, which is probably less desirable than e.g. one consumer per
broker.

Any suggestions welcome, even if it's ""why don't you go work on
implementing KIP-41"", or ""You're doing it wrong"" :)

Thanks,
Cody









"
Davies Liu <davies@databricks.com>,"Wed, 2 Mar 2016 14:29:19 -0800",Re: HashedRelation Memory Pressure on Broadcast Joins,Matt Cheah <mcheah@palantir.com>,"I see, we could reduce the memory by moving the copy out of the HashedRelation,
then we should do the copy before call HashedRelation for shuffle hash join.

Another things is that when we do broadcasting, we will have another
serialized copy
of hash table.

For the table that's larger than 100M, we may not suggest to use Broadcast join,
because it take time to send it to every executor also take the same amount of
memory on every executor.

g
™re
e
ly have
would
n
enario.
frame
er I

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 2 Mar 2016 14:45:09 -0800",[VOTE] Release Apache Spark 1.6.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.1!

The vote is open until Saturday, March 5, 2016 at 20:00 UTC and passes if a
majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.1
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/

The tag to be voted on is *v1.6.1-rc1
(15de51c238a7340fa81cb0b80d029a05d97bfc5c)
<https://github.com/apache/spark/tree/v1.6.1-rc1>*

The release files, including signatures, digests, etc. can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.1-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1180/

The test repository (versioned as v1.6.1-rc1) for this release can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1179/

The documentation corresponding to this release can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.1-rc1-docs/


=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.0.

================================================
== What justifies a -1 vote for this release? ==
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.0, missing features, or bugs related to new features will not
necessarily block this release.

===============================================================
== What should happen to JIRA tickets still targeting 1.6.0? ==
===============================================================
1. It is OK for documentation patches to target 1.6.1 and still go into
branch-1.6, since documentations will be published separately from the
release.
2. New features for non-alpha-modules should target 1.7+.
3. Non-blocker bug fixes should target 1.6.2 or 2.0.0, or drop the target
version.
"
Reynold Xin <rxin@databricks.com>,"Wed, 2 Mar 2016 18:42:41 -0800",Re: SPARK-SQL: Pattern Detection on Live Event or Archived Event Data,Jerry Lam <chilinglam@gmail.com>,"SQL is very common and even some business analysts learn them. Scala and
Python are great, but the easiest language to use is often the languages a
user already knows. And for a lot of users, that is SQL.


"
"""=?ISO-8859-1?B?U2Vh?="" <261810726@qq.com>","Thu, 3 Mar 2016 10:44:24 +0800","About the exception ""Received LaunchTask command but executor was null""","""=?ISO-8859-1?B?ZGV2?="" <dev@spark.apache.org>","Hi, all:
     Sometimes task will fail with exception ""About the exception ""Received LaunchTask command but executor was null"",  and I find it is a common problem:
     https://issues.apache.org/jira/browse/SPARK-13112
     https://issues.apache.org/jira/browse/SPARK-13060
     
     I have a question that why should CoarseGrainedExecutorBackend wait for the response of driver? Can we initialize executor in the onStart function after the request to driver? 
     If CoarseGrainedExecutorBackend receive messages like 'LaunchTask' or 'KillTask' which means that it is registered sucessfully."
Mark Hamstra <mark@clearstorydata.com>,"Wed, 2 Mar 2016 21:39:42 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Michael Armbrust <michael@databricks.com>,1
Tim Preece <tepreece@mail.com>,"Thu, 3 Mar 2016 04:00:21 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),dev@spark.apache.org,"I have been testing 1.6.1RC1 using the IBM Java SDK.

I notice a problem ( with the org.apache.spark.sql.hive.client.VersionsSuite
tests ) after a recent Spark 1.6.1 change. 
Pull request -
https://github.com/apache/spark/commit/f7898f9e2df131fa78200f6034508e74a78c2a44

The change introduced a dependency on
org.apache.hadoop.hive.cli.CliSessionState in
org.apache.spark.sql.hive.client.ClientWrapper. 

In particular the following test was added
if (originalState.isInstanceOf[CliSessionState]) {

The problem is that VersionsSuite test uses an isolated classloader in order
to test various versions of Hive. However the classpath of the isolated
classloader does not contain CliSessionState. 

The behaviour of isInstanceOf[CliSessionState]) is JVM vendor specific. In
particular whether this code causes the CliSessionState class to be loaded.
( It does not for openjdk, but does for IBM JDK ). Hence this call can throw
a classnotfound exception.

I will have a pull request to fix the testcase very shortly.

I opened JIRA SPARK-13648 ( i wasn't too sure if I should have reopened one
of SPARK-11624 or SPARK-11972 instead ?)

 





--

---------------------------------------------------------------------


"
Tim Preece <tepreece@mail.com>,"Thu, 3 Mar 2016 05:48:03 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),dev@spark.apache.org,"I just created the following pull request ( against master but would like on
1.6.1 ) for the isolated classloader fix ( Spark-13648 )

https://github.com/apache/spark/pull/11495



--

---------------------------------------------------------------------


"
Yin Yang <yy201602@gmail.com>,"Thu, 3 Mar 2016 08:55:45 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Michael Armbrust <michael@databricks.com>,"When I ran test suite using the following command:

build/mvn clean -Phive -Phive-thriftserver -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.0 package

I got failure in Spark Project Docker Integration Tests :

16/03/02 17:36:46 INFO RemoteActorRefProvider$RemotingTerminator: Remote
daemon shut down; proceeding with flushing remote transports.
^[[31m*** RUN ABORTED ***^[[0m
^[[31m  com.spotify.docker.client.DockerException:
java.util.concurrent.ExecutionException:
com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException: java.io.
           IOException: No such file or directory^[[0m
^[[31m  at
com.spotify.docker.client.DefaultDockerClient.propagate(DefaultDockerClient.java:1141)^[[0m
^[[31m  at
com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1082)^[[0m
^[[31m  at
com.spotify.docker.client.DefaultDockerClient.ping(DefaultDockerClient.java:281)^[[0m
^[[31m  at
org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:76)^[[0m
^[[31m  at
org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)^[[0m
^[[31m  at
org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:58)^[[0m
^[[31m  at
org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)^[[0m
^[[31m  at
org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.run(DockerJDBCIntegrationSuite.scala:58)^[[0m
^[[31m  at
^[[31m  at
org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)^[[0m
^[[31m  ...^[[0m
^[[31m  Cause: java.util.concurrent.ExecutionException:
com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException:
java.io.IOException: No such file or directory^[[0m
^[[31m  at
jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:299)^[[0m
^[[31m  at
jersey.repackaged.com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:286)^[[0m
^[[31m  at
jersey.repackaged.com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)^[[0m
^[[31m  at
com.spotify.docker.client.DefaultDockerClient.request(DefaultDockerClient.java:1080)^[[0m
^[[31m  at
com.spotify.docker.client.DefaultDockerClient.ping(DefaultDockerClient.java:281)^[[0m
^[[31m  at
org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:76)^[[0m
^[[31m  at
org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)^[[0m
^[[31m  at
org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.beforeAll(DockerJDBCIntegrationSuite.scala:58)^[[0m
^[[31m  at
org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)^[[0m
^[[31m  at
org.apache.spark.sql.jdbc.DockerJDBCIntegrationSuite.run(DockerJDBCIntegrationSuite.scala:58)^[[0m
^[[31m  ...^[[0m
^[[31m  Cause:
com.spotify.docker.client.shaded.javax.ws.rs.ProcessingException:
java.io.IOException: No such file or directory^[[0m
^[[31m  at
org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:481)^[[0m
^[[31m  at
org.glassfish.jersey.apache.connector.ApacheConnector$1.run(ApacheConnector.java:491)^[[0m
^[[31m  at
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)^[[0m
^[[31m  at java.util.concurrent.FutureTask.run(FutureTask.java:262)^[[0m
^[[31m  at
jersey.repackaged.com.google.common.util.concurrent.MoreExecutors$DirectExecutorService.execute(MoreExecutors.java:299)^[[0m
^[[31m  at
java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:110)^[[0m
^[[31m  at
jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:50)^[[0m
^[[31m  at
jersey.repackaged.com.google.common.util.concurrent.AbstractListeningExecutorService.submit(AbstractListeningExecutorService.java:37)^[[0m
^[[31m  at
org.glassfish.jersey.apache.connector.ApacheConnector.apply(ApacheConnector.java:487)^[[0m
^[[31m  at
org.glassfish.jersey.client.ClientRuntime$2.run(ClientRuntime.java:177)^[[0m
^[[31m  ...^[[0m
^[[31m  Cause: java.io.IOException: No such file or directory^[[0m
^[[31m  at
jnr.unixsocket.UnixSocketChannel.doConnect(UnixSocketChannel.java:94)^[[0m

Has anyone seen the above ?


"
Karan Kumar <karankumar1100@gmail.com>,"Thu, 3 Mar 2016 23:22:34 +0530",Re: [Proposal] Enabling time series analysis on spark metrics,Reynold Xin <rxin@databricks.com>,"Precisely. Found a JIRA in this regard : SPARK-10610
<https://issues.apache.org/jira/browse/SPARK-10610>




-- 
Thanks
Karan
"
Sean Owen <sowen@cloudera.com>,"Thu, 3 Mar 2016 19:16:15 +0000",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Yin Yang <yy201602@gmail.com>,"@Yin Yang see https://issues.apache.org/jira/browse/SPARK-12426 Docker
has to be running locally for these tests to pass. I think it's a
little surprising. However I still get a docker error, below.

For me, +0 I guess. The signatures and hashes are all f"
Tim Preece <tepreece@mail.com>,"Thu, 3 Mar 2016 12:43:54 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),dev@spark.apache.org,"Regarding the failure in 
org.apache.spark.streaming.kafka.DirectKafkaStreamSuite"",""offset recovery

We have been seeing the very same problem with the IBM JDK for quite a long
time ( since at least July 2015 ).
It is intermittent and we had dismissed it as a testcase problem.




--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 3 Mar 2016 19:47:33 +0000",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Tim Preece <tepreece@mail.com>,"FWIW I was running this with OpenJDK 1.8.0_66


---------------------------------------------------------------------


"
Yin Yang <yy201602@gmail.com>,"Thu, 3 Mar 2016 12:09:00 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Michael Armbrust <michael@databricks.com>,"Skipping docker tests, the rest are green:

[INFO] Spark Project External Kafka ....................... SUCCESS [01:28
min]
[INFO] Spark Project Examples ............................. SUCCESS [02:59
min]
[INFO] Spark Project External Kafka Assembly .............. SUCCESS [
11.680 s]
[INFO]
------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO]
------------------------------------------------------------------------
[INFO] Total time: 02:16 h
[INFO] Finished at: 2016-03-03T11:17:07-08:00
[INFO] Final Memory: 152M/4062M


"
Cody Koeninger <cody@koeninger.org>,"Thu, 3 Mar 2016 17:08:42 -0600",getting a list of executors for use in getPreferredLocations,"""dev@spark.apache.org"" <dev@spark.apache.org>","I need getPreferredLocations to choose a consistent executor for a
given partition in a stream.  In order to do that, I need to know what
the current executors are.

I'm currently grabbing them from the block manager master .getPeers(),
which works, but I don't know if that's the most reasonable way to do
it.

Relevant code:

https://github.com/koeninger/spark-1/blob/aaef0fc6e7e3aae18e4e03271bc0707d09d243e4/external/kafka-beta/src/main/scala/org/apache/spark/streaming/kafka/KafkaRDD.scala#L107

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 3 Mar 2016 15:10:36 -0800",Re: getting a list of executors for use in getPreferredLocations,Cody Koeninger <cody@koeninger.org>,"What do you mean by consistent? Throughout the life cycle of an app, the
executors can come and go and as a result really has no consistency. Do you
just need it for a specific job?




"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Thu, 3 Mar 2016 15:15:35 -0800",Re: getting a list of executors for use in getPreferredLocations,Cody Koeninger <cody@koeninger.org>,"You can take a look at
""org.apache.spark.streaming.scheduler.ReceiverTracker#getExecutors""


"
Cody Koeninger <cody@koeninger.org>,"Thu, 3 Mar 2016 19:51:35 -0600",Re: getting a list of executors for use in getPreferredLocations,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Thanks.  That looks pretty similar to what I'm doing, with the
difference being getPeers vs getMemoryStatus.  Seems like they're both
backed by the same blockManagerInfo, but getPeers is filtering in a
way that looks close to what I need.  Is there a reason to prefer
getMemoryStatus?


---------------------------------------------------------------------


"
Rishi Mishra <rmishra@snappydata.io>,"Fri, 4 Mar 2016 10:41:00 +0530",Re: HashedRelation Memory Pressure on Broadcast Joins,Davies Liu <davies@databricks.com>,"Hi Davies,
When you say *""UnsafeRow could come from UnsafeProjection, so We should
copy the rows for safety.""  *do you intend to say that the underlying state
might change , because of some state update APIs ?
Or its due to some other rationale ?

Regards,
Rishitesh Mishra,
SnappyData . (http://www.snappydata.io/)

https://in.linkedin.com/in/rishiteshmishra


t
€™re
only have
t
I would
he
ve
scenario.
a frame
fter I
"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Fri, 4 Mar 2016 09:25:44 +0100",Fwd: spark master ui to proxy app and worker ui,"""dev@spark.apache.org"" <dev@spark.apache.org>","Forwarding to development mailing list, as it might be more relevant
here to ask for it. I am wondering if I miss something in the
documentation that it might be possible already. If yes then please
point me to the documentation as how to achieve it. If no, then would it
make sense to implement it ?

Thanks,
Gurvinder


-------- Forwarded Message --------
Subject: spark master ui to proxy app and worker ui
Date: Thu, 3 Mar 2016 20:12:07 +0100
From: Gurvinder Singh <gurvinder.singh@uninett.no>
To: user <user@spark.apache.org>

Hi,

I am wondering if it is possible for the spark standalone master UI to
proxy app/driver UI and worker UI. The reason for this is that currently
if you want to access UI of driver and worker to see logs, you need to
have access to their IP:port which makes it harder to open up from
networking point of view. So operationally it makes life easier if
master can simply proxy those connections and allow access both app and
worker UI details from master UI itself.

Master does not need to have content stream to it all the time, only
when user wants to access contents from other UIs then it simply proxy
the request/response during that duration. Thus master will not have to
incur extra load all the time.

Thanks,
Gurvinder

---------------------------------------------------------------------




---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 4 Mar 2016 10:34:54 +0000",Set up a Coverity scan for Spark,dev <dev@spark.apache.org>,"https://scan.coverity.com/projects/apache-spark-2f9d080d-401d-47bc-9dd1-7956c411fbb4?tab=overview

This has to be run manually, and is Java-only, but the inspection
results are pretty good. Anyone should be able to browse them, and let
me know if anyone would like more access.
Most are false-positives, but it's found some reasonable little bugs.

When my stack of things to do clears I'll try to address them, but I
bring it up as an FYI for anyone interested in static analysis.

---------------------------------------------------------------------


"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Fri, 4 Mar 2016 17:32:42 +0530",Re: Mapper side join with DataFrames API,"Yong Zhang <java8964@hotmail.com>, dev@spark.apache.org","Have added this to SO, can you guys share any thoughts ?

http://stackoverflow.com/questions/35795518/spark-1-6-spills-to-disk-even-when-there-is-enough-memory
<http://www.google.com/url?q=http%3A%2F%2Fstackoverflow.com%2Fquestions%2F35795518%2Fspark-1-6-spills-to-disk-even-when-there-is-enough-memory&sa=D&sntz=1&usg=AFQjCNEzDJqylz5aF0998u08RGlf5YF1-g>




-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 4 Mar 2016 04:18:59 -0800",Re: Set up a Coverity scan for Spark,Sean Owen <sowen@cloudera.com>,"Since majority of code is written in Scala which is not analyzed by Coverity, the efficacy of the tool seems limited. 

56c411fbb4?tab=overview

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 4 Mar 2016 12:35:47 +0000",Re: Set up a Coverity scan for Spark,Ted Yu <yuzhihong@gmail.com>,"Yeah, it's not going to help with Scala, but it can at least find
stuff in the Java code. I'm not suggesting anyone run it regularly,
but one run to catch some bugs is useful.

I've already triaged ~70 issues there just in the Java code, of which
a handful are important.


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 4 Mar 2016 06:52:27 -0800",Re: Set up a Coverity scan for Spark,Sean Owen <sowen@cloudera.com>,"Last time I checked there wasn't high impact defects.

Mind pointing out the defects you think should be fixed ?

Thanks


"
Sean Owen <sowen@cloudera.com>,"Fri, 4 Mar 2016 15:06:46 +0000",Re: Set up a Coverity scan for Spark,Ted Yu <yuzhihong@gmail.com>,"Hi Ted, I've already marked them. You should be able to see the ones
marked ""Fix Required"" if you click through to the defects. Most are
just bad form and probably have no impact. The few that looked
reasonably important were:

- using platform char encoding, not UTF-8
- Incorrect notify/wait
- volatile count with non-atomic update
- bad equals/hashCode


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 4 Mar 2016 07:36:23 -0800",Re: Set up a Coverity scan for Spark,Sean Owen <sowen@cloudera.com>,"Is there JIRA for fixing the resource leaks w.r.t. unclosed SparkContext ?

I wonder if such defects are really high priority.

Cheers


"
Sean Owen <sowen@cloudera.com>,"Fri, 4 Mar 2016 15:41:35 +0000",Re: Set up a Coverity scan for Spark,Ted Yu <yuzhihong@gmail.com>,"No. Those are all in Java examples, and while we should show stopping
the context, it has no big impact. It's worth touching up.

I'm concerned about the ones with a potential correctness implication.
They are easy to fix and already identified; why wouldn't we fix them?
we take PRs to fix typos in comments.


---------------------------------------------------------------------


"
Khaled Ammar <khaled.ammar@gmail.com>,"Fri, 4 Mar 2016 12:53:30 -0500",GraphX optimizations,dev@spark.apache.org,"Hi all,

I wonder if the optimizations mentioned in the GraphX paper (
https://amplab.cs.berkeley.edu/wp-content/uploads/2014/09/graphx.pdf ) are
currently implemented. In particular, I am looking for mrTriplets
optimizations and memory-based shuffle.

-- 
Thanks,
-Khaled
"
Cody Koeninger <cody@koeninger.org>,"Fri, 4 Mar 2016 15:39:14 -0600",Use cases for kafka direct stream messageHandler,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Wanted to survey what people are using the direct stream
messageHandler for, besides just extracting key / value / offset.

Would your use case still work if that argument was removed, and the
stream just contained ConsumerRecord objects
(http://kafka.apache.org/090/javadoc/org/apache/kafka/clients/consumer/ConsumerRecord.html)
which you could then use normal map transformations to access?

The only other valid use of messageHandler that I can think of is
catching serialization problems on a per-message basis.  But with the
new Kafka consumer library, that doesn't seem feasible anyway, and
could be handled with a custom (de)serializer.

---------------------------------------------------------------------


"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Sat, 5 Mar 2016 19:42:12 +0530",Re: Mapper side join with DataFrames API,"Yong Zhang <java8964@hotmail.com>, dev@spark.apache.org","Hello Guys,

No help yet. Can someone tell me with a reply to the above question in SO ?

Thanks
Deepak




-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com
"
Dhaval Modi <dhavalmodi24@gmail.com>,"Sat, 5 Mar 2016 20:32:10 +0530","Fwd: Spark SQL drops the HIVE table in ""overwrite"" mode while writing
 into table",dev@spark.apache.org,"Regards,
Dhaval Modi
dhavalmodi24@gmail.com

---------- Forwarded message ----------
From: Dhaval Modi <dhavalmodi24@gmail.com>
Date: 5 March 2016 at 20:31
Subject: Spark SQL drops the HIVE table in ""overwrite"" mode while writing
into table
To: user@spark.apache.org


Hi Team,

I am facing a issue while writing dataframe back to HIVE table.

When using ""SaveMode.Overwrite"" option the table is getting dropped and
Spark is unable to recreate it thus throwing error.

JIRA: https://issues.apache.org/jira/browse/SPARK-13699


E.g.
tgtFinal.write.mode(SaveMode.Overwrite).saveAsTable(""tgt_table"")

Error:
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
16/03/05 13:57:26 INFO spark.SparkContext: Created broadcast 138 from run
at ThreadPoolExecutor.java:1145
16/03/05 13:57:26 INFO log.PerfLogger: <PERFLOG method=OrcGetSplits
from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl>
*java.lang.RuntimeException: serious problem*
*        at *
org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)
        at
org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
        at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
        at
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:239)
        at
org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:237)
        at scala.Option.getOrElse(Option.scala:120)
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Caused by: java.util.concurrent.ExecutionException:
java.io.FileNotFoundException: File does not exist: hdfs://
sandbox.hortonworks.com:8020/apps/hive/warehouse/tgt_table
        at java.util.concurrent.FutureTask.report(FutureTask.java:122)
        at java.util.concurrent.FutureTask.get(FutureTask.java:188)
        at
org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:998)
        ... 138 more
*Caused by: java.io.FileNotFoundException: File does not exist:
hdfs://sandbox.hortonworks.com:8020/apps/hive/warehouse/tgt_table
<http://sandbox.hortonworks.com:8020/apps/hive/warehouse/tgt_table>*
        at
org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1122)
        at
org.apache.hadoop.hdfs.DistributedFileSystem$18.doCall(DistributedFileSystem.java:1114)
++++++++++++++++++++++++++++++++++++++++++++++++++


Regards,
Dhaval Modi
dhavalmodi24@gmail.com
"
Ted Yu <yuzhihong@gmail.com>,"Sat, 5 Mar 2016 07:17:05 -0800","Re: Spark SQL drops the HIVE table in ""overwrite"" mode while writing
 into table",Dhaval Modi <dhavalmodi24@gmail.com>,"Please stack trace, code snippet, etc in the JIRA you created so that
people can reproduce what you saw.


"
Dhaval Modi <dhavalmodi24@gmail.com>,"Sat, 5 Mar 2016 23:01:18 +0530","Re: Spark SQL drops the HIVE table in ""overwrite"" mode while writing
 into table",Ted Yu <yuzhihong@gmail.com>,"Added.

Regards,
Dhaval Modi
dhavalmodi24@gmail.com


"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Sun, 6 Mar 2016 03:23:22 +0300",Typo in community databricks cloud docs,dev@spark.apache.org,"Hi, I'm not sure where to put this, but I've found a typo on a page
Caching:
""

   - *Serialization:* The default serialization in Spark is Java
   serialization. However for better peformance, we recommend Kyro
   serialization, which you can learn more about here
   <https://spark.apache.org/docs/latest/tuning.html#data-serialization>

""

It has to be ""Kryo"".

Hope, this helps.
--
Be well!
Jean Morozov
"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Sun, 6 Mar 2016 15:19:33 +0100",Re: Fwd: spark master ui to proxy app and worker ui,"""dev@spark.apache.org"" <dev@spark.apache.org>","I wonder if anyone got any feedback on it. I can look into implement it
but would like to know if such a functionality can be merged into master
back. If yes then please let me know and point me to the direction to
get started.

Regards,
Gurvinder


---------------------------------------------------------------------


"
Egor Pahomov <pahomov.egor@gmail.com>,"Sun, 6 Mar 2016 12:08:50 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Yin Yang <yy201602@gmail.com>,"+1

Spark ODBC server is fine, SQL is fine.

2016-03-03 12:09 GMT-08:00 Yin Yang <yy201602@gmail.com>:



-- 


*Sincerely yoursEgor Pakhomov*
"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Mon, 7 Mar 2016 02:18:05 +0530",Spark Custom Partitioner not picked,"user <user@spark.apache.org>, Spark dev list <dev@spark.apache.org>","Hi All,

    When i am submitting a spark job on YARN with Custom Partitioner, it is
not picked by Executors. Executors still using the default HashPartitioner.
I added logs into both HashPartitioner (org/apache/spark/Partitioner.scala)
and Custom Partitioner. The completed executor logs shows HashPartitioner.

Below is the Spark application code with Custom Partitioner and the log
line which is added into HashPartitioner class of Partition.scala

 log.info(""HashPartitioner=""+key+""---""+numPartitions+""----""+Utils.nonNegativeMod(key.hashCode,
numPartitions))

The Executor logs has

16/03/06 15:20:27 INFO spark.HashPartitioner: HashPartitioner=INFO---4----2
16/03/06 15:20:27 INFO spark.HashPartitioner: HashPartitioner=INFO---4----2
........

How to make sure, the executors are picking the right partitioner.



*Code:*
package org.apache.spark

class ExactPartitioner(partitions: Int) extends Partitioner with Logging{

  def numPartitions: Int = partitions

  def getPartition(key: Any): Int = {

*   log.info <http://log.info>(""ExactPartitioner=""+key)*

   key match{
   case ""INFO"" => 0
   case ""DEBUG"" => 1
   case ""ERROR"" => 2
   case ""WARN"" => 3
   case ""FATAL"" => 4
   }
  }
}

object GroupByCLDB {

def main(args: Array[String]) {

val logFile = ""/DATA""

val sparkConf = new SparkConf().setAppName(""GroupBy"")
sparkConf.set(""spark.executor.memory"",""4g"");
sparkConf.set(""spark.executor.cores"",""2"");
sparkConf.set(""spark.executor.instances"",""2"");

val sc = new SparkContext(sparkConf)
val logData = sc.textFile(logFile)


case class LogClass(one:String,two:String)

def parse(line: String) = {
      val pieces = line.split(' ')
      val level = pieces(2).toString
      val one = pieces(0).toString
      val two = pieces(1).toString
      (level,LogClass(one,two))
      }

val output = logData.map(x => parse(x))

*val partitioned = output.partitionBy(new ExactPartitioner(5)).persist()val
groups = partitioned.groupByKey(new ExactPartitioner(5))*
groups.count()

output.partitions.size
partitioned.partitions.size

}
}



Thanks,
Prabhu Joseph
"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 7 Mar 2016 11:19:45 +0900","PySpark, spill-related (possibly psutil) issue, throwing an exception
 '_fill_function() takes exactly 4 arguments (5 given)'",dev@spark.apache.org,"Hi all,

While I am testing some codes in PySpark, I met a weird issue.

This works fine at Spark 1.6.0 but it looks it does not for Spark 2.0.0.

When I simply run *logData = sc.textFile(path).coalesce(1) *with some big
files in stand-alone local mode without HDFS, this simply throws the
exception,


*_fill_function() takes exactly 4 arguments (5 given)*


I firstly wanted to open a Jira for this but feel like it is a too
primitive functionality and I felt like I might be doing this wrong.



The full error message is below:

16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split:
file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:2415919104+33554432
*16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split:
file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:805306368+33554432*
*16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split:
file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:0+33554432*
*16/03/07 11:12:44 INFO rdd.HadoopRDD: Input split:
file:/Users/hyukjinkwon/Desktop/workspace/local/spark-local-ade/spark/data/00_REF/20160119000000-20160215235900-TROI_STAT_ADE_0.DAT:1610612736+33554432*
*16/03/07 11:12:44 ERROR executor.Executor: Exception in task 2.0 in stage
0.0 (TID 2)*
*org.apache.spark.api.python.PythonException: Traceback (most recent call
last):*
*  File ""./python/pyspark/worker.py"", line 98, in main*
*    command = pickleSer._read_with_length(infile)*
*  File ""./python/pyspark/serializers.py"", line 164, in _read_with_length*
*    return self.loads(obj)*
*  File ""./python/pyspark/serializers.py"", line 422, in loads*
*    return pickle.loads(obj)*
*TypeError: ('_fill_function() takes exactly 4 arguments (5 given)',
<function _fill_function at 0x101e105f0>, (<function add_shuffle_key at
0x10612c488>, {'defaultdict': <type 'collections.defaultdict'>,
'get_used_memory': <function get_used_memory at 0x1027c8b18>, 'pack_long':
<function pack_long at 0x101e1ec08>}, None, {}, 'pyspark.rdd'))*

* at
org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:168)*
* at
org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:209)*
* at org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:127)*
* at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:62)*
* at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)*
* at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)*
* at org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:349)*
* at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)*
* at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)*
* at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:77)*
* at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:45)*
* at org.apache.spark.scheduler.Task.run(Task.scala:82)*
* at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:227)*
* at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)*
* at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)*
* at java.lang.Thread.run(Thread.java:745)*
*16/03/07 11:12:44 ERROR executor.Executor: Exception in task 3.0 in stage
0.0 (TID 3)*
*org.apache.spark.api.python.PythonException: Traceback (most recent call
last):*
*  File ""./python/pyspark/worker.py"", line 98, in main*
*    command = pickleSer._read_with_length(infile)*
*  File ""./python/pyspark/serializers.py"", line 164, in _read_with_length*
*    return self.loads(obj)*
*  File ""./python/pyspark/serializers.py"", line 422, in loads*
*    return pickle.loads(obj)*
*TypeError: ('_fill_function() takes exactly 4 arguments (5 given)',
<function _fill_function at 0x101e105f0>, (<function add_shuffle_key at
0x10612c488>, {'defaultdict': <type 'collections.defaultdict'>,
'get_used_memory': <function get_used_memory at 0x1027c8b18>, 'pack_long':
<function pack_long at 0x101e1ec08>}, None, {}, 'pyspark.rdd'))*


Thanks!
"
Hyukjin Kwon <gurwls223@gmail.com>,"Mon, 7 Mar 2016 11:24:11 +0900","Re: PySpark, spill-related (possibly psutil) issue, throwing an
 exception '_fill_function() takes exactly 4 arguments (5 given)'",dev@spark.apache.org,"Just in case, My python version is 2.7.10.

2016-03-07 11:19 GMT+09:00 Hyukjin Kwon <gurwls223@gmail.com>:

"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Mon, 7 Mar 2016 12:42:38 +0900",Re: GraphX optimizations,Khaled Ammar <khaled.ammar@gmail.com>,"Hi,

mapReduceTriplets you said has been removed in master and you need to use a
newer api,
aggregateMessages, instead of it (See SPARK-3936 and SPARK-12995 for
details).
The memory-based shuffling opt. is a topic of not only graphx but also
spark itself.
You can see SPARK-3376 for related discussions.

Thanks,






-- 
---
Takeshi Yamamuro
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Sun, 6 Mar 2016 22:46:13 -0800","Re: PySpark, spill-related (possibly psutil) issue, throwing an
 exception '_fill_function() takes exactly 4 arguments (5 given)'",Hyukjin Kwon <gurwls223@gmail.com>,"Could you rebuild the whole project? I changed the python function
serialization format in https://github.com/apache/spark/pull/11535 to fix a
bug. This exception looks like some place was still using the old codes.


"
"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","Mon, 7 Mar 2016 14:30:01 -0500",Nulls getting converted to 0 with spark 2.0 SNAPSHOT,dev@spark.apache.org,"Just wanted to confirm that this is the expected behaviour.

Basically I'm putting nulls into a non-nullable LongType column and doing a
transformation operation on that column, the result is a column with nulls
converted to 0.

Heres an example

from pyspark.sql import types
from pyspark.sql import DataFrame, types, functions as F

sql_schema = types.StructType([
  types.StructField(""a"", types.LongType(), True),
  types.StructField(""b"", types.StringType(),  True),
])

df = sqlCtx.createDataFrame([
    (1, ""one""),
    (None, ""two""),
], sql_schema)

*# Everything is fine here*
*df.collect() # [Row(a=1, b=u'one'), Row(a=None, b=u'two')]*

def assert_not_null(val):
    return val

udf = F.udf(assert_not_null, types.LongType())

df = df.withColumnRenamed('a', ""_tmp_col"")
df = df.withColumn('a', udf(df._tmp_col))
df = df.drop(""_tmp_col"")

*# None gets converted to 0*
*df.collect() # [Row(b=u'one', a=1), Row(b=u'two', a=0)]*

Thanks,

Franklyn
"
Michael Armbrust <michael@databricks.com>,"Mon, 7 Mar 2016 11:50:33 -0800",Re: Nulls getting converted to 0 with spark 2.0 SNAPSHOT,"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","That looks like a bug to me.  Open a JIRA?


"
Davies Liu <davies@databricks.com>,"Mon, 7 Mar 2016 12:26:11 -0800",Re: HashedRelation Memory Pressure on Broadcast Joins,Rishi Mishra <rmishra@snappydata.io>,"The underlying buffer for UnsafeRow is reused in UnsafeProjection.

py
ht
st
€™re
f
 only have
ut
 I would
:
™ve
 scenario.
ta frame
after I

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Mar 2016 12:39:49 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Egor Pahomov <pahomov.egor@gmail.com>,"+1 (binding)



"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Mar 2016 12:48:00 -0800",Re: Typo in community databricks cloud docs,Eugene Morozov <evgeny.a.morozov@gmail.com>,"Thanks - I've fixed it and it will go out next time we update. For future
reference, you can email directly support@databricks.com for this.

Again - thanks for reporting this.


"
Suniti Singh <suniti.singh@gmail.com>,"Mon, 7 Mar 2016 16:15:37 -0800",Adding hive context gives error,"user@spark.apache.org, dev@spark.apache.org","Hi All,

I am trying to create a hive context in a scala prog as follows in eclipse:
Note --  i have added the maven dependency for spark -core , hive , and sql.

import org.apache.spark.SparkConf

import org.apache.spark.SparkContext

import org.apache.spark.rdd.RDD.rddToPairRDDFunctions

object DataExp {

   def main(args: Array[String]) = {

      val conf = new SparkConf().setAppName(""DataExp"").setMaster(""local"")

      val sc = new SparkContext(conf)

     * val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)*

 }

}

I get the the following *errors*: @ line of hiveContext above in the prog

1 --- Error in Scala compiler: bad symbolic reference. A signature in
HiveContext.class refers to term ui in package
org.apache.spark.sql.execution which is not available. It may be completely
missing from the current classpath, or the version on the classpath might
be incompatible with the version used when compiling HiveContext.class.
spark Unknown Scala Problem
2 --- SBT builder crashed while compiling. The error message is 'bad
symbolic reference. A signature in HiveContext.class refers to term ui in
package org.apache.spark.sql.execution which is not available. It may be
completely missing from the current classpath, or the version on the
classpath might be incompatible with the version used when compiling
HiveContext.class.'. Check Error Log for details. spark Unknown Scala
Problem

3 --- while compiling:
/Users/sunitisingh/sparktest/spark/src/main/scala/com/sparktest/spark/DataExp.scala
        during phase: erasure      library version: version 2.10.6
compiler version: version 2.10.6   reconstructed args: -javabootclasspath
/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/resources.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/rt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/jsse.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/jce.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/charsets.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/jfr.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/cldrdata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/dnsns.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/jaccess.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/jfxrt.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/localedata.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/nashorn.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/sunec.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/sunjce_provider.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/sunpkcs11.jar:/Library/Java/JavaVirtualMachines/jdk1.8.0_60.jdk/Contents/Home/jre/lib/ext/zipfs.jar:/System/Library/Java/Extensions/AppleScriptEngine.jar:/System/Library/Java/Extensions/dns_sd.jar:/System/Library/Java/Extensions/j3daudio.jar:/System/Library/Java/Extensions/j3dcore.jar:/System/Library/Java/Extensions/j3dutils.jar:/System/Library/Java/Extensions/jai_codec.jar:/System/Library/Java/Extensions/jai_core.jar:/System/Library/Java/Extensions/mlibwrapper_jai.jar:/System/Library/Java/Extensions/MRJToolkit.jar:/System/Library/Java/Extensions/vecmath.jar
-classpath
/Users/sunitisingh/sparktest/spark/target/classes:/Users/sunitisingh/sparktest/spark/target/test-classes:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-core_2.10/1.6.0/spark-core_2.10-1.6.0.jar:/Users/sunitisingh/.m2/repository/org/apache/avro/avro-mapred/1.7.7/avro-mapred-1.7.7-hadoop2.jar:/Users/sunitisingh/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7.jar:/Users/sunitisingh/.m2/repository/org/apache/avro/avro-ipc/1.7.7/avro-ipc-1.7.7-tests.jar:/Users/sunitisingh/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.9.13/jackson-core-asl-1.9.13.jar:/Users/sunitisingh/.m2/repository/com/twitter/chill_2.10/0.5.0/chill_2.10-0.5.0.jar:/Users/sunitisingh/.m2/repository/com/esotericsoftware/kryo/kryo/2.21/kryo-2.21.jar:/Users/sunitisingh/.m2/repository/com/esotericsoftware/reflectasm/reflectasm/1.07/reflectasm-1.07-shaded.jar:/Users/sunitisingh/.m2/repository/com/esotericsoftware/minlog/minlog/1.2/minlog-1.2.jar:/Users/sunitisingh/.m2/repository/org/objenesis/objenesis/1.2/objenesis-1.2.jar:/Users/sunitisingh/.m2/repository/com/twitter/chill-java/0.5.0/chill-java-0.5.0.jar:/Users/sunitisingh/.m2/repository/org/apache/xbean/xbean-asm5-shaded/4.4/xbean-asm5-shaded-4.4.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-client/2.2.0/hadoop-client-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-common/2.2.0/hadoop-common-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/sunitisingh/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar:/Users/sunitisingh/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/sunitisingh/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/sunitisingh/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/sunitisingh/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/sunitisingh/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-auth/2.2.0/hadoop-auth-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-hdfs/2.2.0/hadoop-hdfs-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-app/2.2.0/hadoop-mapreduce-client-app-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-common/2.2.0/hadoop-mapreduce-client-common-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-yarn-client/2.2.0/hadoop-yarn-client-2.2.0.jar:/Users/sunitisingh/.m2/repository/com/google/inject/guice/3.0/guice-3.0.jar:/Users/sunitisingh/.m2/repository/javax/inject/javax.inject/1/javax.inject-1.jar:/Users/sunitisingh/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/jersey-test-framework/jersey-test-framework-grizzly2/1.9/jersey-test-framework-grizzly2-1.9.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/jersey-test-framework/jersey-test-framework-core/1.9/jersey-test-framework-core-1.9.jar:/Users/sunitisingh/.m2/repository/javax/servlet/javax.servlet-api/3.0.1/javax.servlet-api-3.0.1.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/jersey-client/1.9/jersey-client-1.9.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/jersey-grizzly2/1.9/jersey-grizzly2-1.9.jar:/Users/sunitisingh/.m2/repository/org/glassfish/grizzly/grizzly-http/2.1.2/grizzly-http-2.1.2.jar:/Users/sunitisingh/.m2/repository/org/glassfish/grizzly/grizzly-framework/2.1.2/grizzly-framework-2.1.2.jar:/Users/sunitisingh/.m2/repository/org/glassfish/gmbal/gmbal-api-only/3.0.0-b023/gmbal-api-only-3.0.0-b023.jar:/Users/sunitisingh/.m2/repository/org/glassfish/external/management-api/3.0.0-b012/management-api-3.0.0-b012.jar:/Users/sunitisingh/.m2/repository/org/glassfish/grizzly/grizzly-http-server/2.1.2/grizzly-http-server-2.1.2.jar:/Users/sunitisingh/.m2/repository/org/glassfish/grizzly/grizzly-rcm/2.1.2/grizzly-rcm-2.1.2.jar:/Users/sunitisingh/.m2/repository/org/glassfish/grizzly/grizzly-http-servlet/2.1.2/grizzly-http-servlet-2.1.2.jar:/Users/sunitisingh/.m2/repository/org/glassfish/javax.servlet/3.1/javax.servlet-3.1.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/jersey-json/1.9/jersey-json-1.9.jar:/Users/sunitisingh/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/sunitisingh/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/sunitisingh/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/sunitisingh/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/sunitisingh/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.8.3/jackson-jaxrs-1.8.3.jar:/Users/sunitisingh/.m2/repository/org/codehaus/jackson/jackson-xc/1.8.3/jackson-xc-1.8.3.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/contribs/jersey-guice/1.9/jersey-guice-1.9.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-yarn-server-common/2.2.0/hadoop-yarn-server-common-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-shuffle/2.2.0/hadoop-mapreduce-client-shuffle-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-yarn-api/2.2.0/hadoop-yarn-api-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-core/2.2.0/hadoop-mapreduce-client-core-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-yarn-common/2.2.0/hadoop-yarn-common-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-mapreduce-client-jobclient/2.2.0/hadoop-mapreduce-client-jobclient-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/hadoop/hadoop-annotations/2.2.0/hadoop-annotations-2.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-launcher_2.10/1.6.0/spark-launcher_2.10-1.6.0.jar:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-network-common_2.10/1.6.0/spark-network-common_2.10-1.6.0.jar:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-network-shuffle_2.10/1.6.0/spark-network-shuffle_2.10-1.6.0.jar:/Users/sunitisingh/.m2/repository/org/fusesource/leveldbjni/leveldbjni-all/1.8/leveldbjni-all-1.8.jar:/Users/sunitisingh/.m2/repository/com/fasterxml/jackson/core/jackson-annotations/2.4.4/jackson-annotations-2.4.4.jar:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-unsafe_2.10/1.6.0/spark-unsafe_2.10-1.6.0.jar:/Users/sunitisingh/.m2/repository/net/java/dev/jets3t/jets3t/0.7.1/jets3t-0.7.1.jar:/Users/sunitisingh/.m2/repository/org/apache/curator/curator-recipes/2.4.0/curator-recipes-2.4.0.jar:/Users/sunitisingh/.m2/repository/org/apache/curator/curator-framework/2.4.0/curator-framework-2.4.0.jar:/Users/sunitisingh/.m2/repository/org/apache/curator/curator-client/2.4.0/curator-client-2.4.0.jar:/Users/sunitisingh/.m2/repository/org/apache/zookeeper/zookeeper/3.4.5/zookeeper-3.4.5.jar:/Users/sunitisingh/.m2/repository/com/google/guava/guava/14.0.1/guava-14.0.1.jar:/Users/sunitisingh/.m2/repository/org/eclipse/jetty/orbit/javax.servlet/3.0.0.v201112011016/javax.servlet-3.0.0.v201112011016.jar:/Users/sunitisingh/.m2/repository/org/apache/commons/commons-lang3/3.3.2/commons-lang3-3.3.2.jar:/Users/sunitisingh/.m2/repository/org/apache/commons/commons-math3/3.4.1/commons-math3-3.4.1.jar:/Users/sunitisingh/.m2/repository/com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar:/Users/sunitisingh/.m2/repository/org/slf4j/slf4j-api/1.7.10/slf4j-api-1.7.10.jar:/Users/sunitisingh/.m2/repository/org/slf4j/jul-to-slf4j/1.7.10/jul-to-slf4j-1.7.10.jar:/Users/sunitisingh/.m2/repository/org/slf4j/jcl-over-slf4j/1.7.10/jcl-over-slf4j-1.7.10.jar:/Users/sunitisingh/.m2/repository/log4j/log4j/1.2.17/log4j-1.2.17.jar:/Users/sunitisingh/.m2/repository/org/slf4j/slf4j-log4j12/1.7.10/slf4j-log4j12-1.7.10.jar:/Users/sunitisingh/.m2/repository/com/ning/compress-lzf/1.0.3/compress-lzf-1.0.3.jar:/Users/sunitisingh/.m2/repository/org/xerial/snappy/snappy-java/1.1.2/snappy-java-1.1.2.jar:/Users/sunitisingh/.m2/repository/net/jpountz/lz4/lz4/1.3.0/lz4-1.3.0.jar:/Users/sunitisingh/.m2/repository/org/roaringbitmap/RoaringBitmap/0.5.11/RoaringBitmap-0.5.11.jar:/Users/sunitisingh/.m2/repository/commons-net/commons-net/2.2/commons-net-2.2.jar:/Users/sunitisingh/.m2/repository/com/typesafe/akka/akka-remote_2.10/2.3.11/akka-remote_2.10-2.3.11.jar:/Users/sunitisingh/.m2/repository/com/typesafe/akka/akka-actor_2.10/2.3.11/akka-actor_2.10-2.3.11.jar:/Users/sunitisingh/.m2/repository/com/typesafe/config/1.2.1/config-1.2.1.jar:/Users/sunitisingh/.m2/repository/io/netty/netty/3.8.0.Final/netty-3.8.0.Final.jar:/Users/sunitisingh/.m2/repository/com/google/protobuf/protobuf-java/2.5.0/protobuf-java-2.5.0.jar:/Users/sunitisingh/.m2/repository/org/uncommons/maths/uncommons-maths/1.2.2a/uncommons-maths-1.2.2a.jar:/Users/sunitisingh/.m2/repository/com/typesafe/akka/akka-slf4j_2.10/2.3.11/akka-slf4j_2.10-2.3.11.jar:/Users/sunitisingh/.m2/repository/org/json4s/json4s-jackson_2.10/3.2.10/json4s-jackson_2.10-3.2.10.jar:/Users/sunitisingh/.m2/repository/org/json4s/json4s-core_2.10/3.2.10/json4s-core_2.10-3.2.10.jar:/Users/sunitisingh/.m2/repository/org/json4s/json4s-ast_2.10/3.2.10/json4s-ast_2.10-3.2.10.jar:/Users/sunitisingh/.m2/repository/org/scala-lang/scalap/2.10.0/scalap-2.10.0.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/jersey-server/1.9/jersey-server-1.9.jar:/Users/sunitisingh/.m2/repository/asm/asm/3.1/asm-3.1.jar:/Users/sunitisingh/.m2/repository/com/sun/jersey/jersey-core/1.9/jersey-core-1.9.jar:/Users/sunitisingh/.m2/repository/org/apache/mesos/mesos/0.21.1/mesos-0.21.1-shaded-protobuf.jar:/Users/sunitisingh/.m2/repository/io/netty/netty-all/4.0.29.Final/netty-all-4.0.29.Final.jar:/Users/sunitisingh/.m2/repository/com/clearspring/analytics/stream/2.7.0/stream-2.7.0.jar:/Users/sunitisingh/.m2/repository/io/dropwizard/metrics/metrics-core/3.1.2/metrics-core-3.1.2.jar:/Users/sunitisingh/.m2/repository/io/dropwizard/metrics/metrics-jvm/3.1.2/metrics-jvm-3.1.2.jar:/Users/sunitisingh/.m2/repository/io/dropwizard/metrics/metrics-json/3.1.2/metrics-json-3.1.2.jar:/Users/sunitisingh/.m2/repository/io/dropwizard/metrics/metrics-graphite/3.1.2/metrics-graphite-3.1.2.jar:/Users/sunitisingh/.m2/repository/com/fasterxml/jackson/core/jackson-databind/2.4.4/jackson-databind-2.4.4.jar:/Users/sunitisingh/.m2/repository/com/fasterxml/jackson/core/jackson-core/2.4.4/jackson-core-2.4.4.jar:/Users/sunitisingh/.m2/repository/com/fasterxml/jackson/module/jackson-module-scala_2.10/2.4.4/jackson-module-scala_2.10-2.4.4.jar:/Users/sunitisingh/.m2/repository/org/scala-lang/scala-reflect/2.10.4/scala-reflect-2.10.4.jar:/Users/sunitisingh/.m2/repository/com/thoughtworks/paranamer/paranamer/2.6/paranamer-2.6.jar:/Users/sunitisingh/.m2/repository/org/apache/ivy/ivy/2.4.0/ivy-2.4.0.jar:/Users/sunitisingh/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/sunitisingh/.m2/repository/org/tachyonproject/tachyon-client/0.8.2/tachyon-client-0.8.2.jar:/Users/sunitisingh/.m2/repository/commons-lang/commons-lang/2.4/commons-lang-2.4.jar:/Users/sunitisingh/.m2/repository/commons-io/commons-io/2.4/commons-io-2.4.jar:/Users/sunitisingh/.m2/repository/org/tachyonproject/tachyon-underfs-hdfs/0.8.2/tachyon-underfs-hdfs-0.8.2.jar:/Users/sunitisingh/.m2/repository/org/tachyonproject/tachyon-underfs-s3/0.8.2/tachyon-underfs-s3-0.8.2.jar:/Users/sunitisingh/.m2/repository/org/tachyonproject/tachyon-underfs-local/0.8.2/tachyon-underfs-local-0.8.2.jar:/Users/sunitisingh/.m2/repository/net/razorvine/pyrolite/4.9/pyrolite-4.9.jar:/Users/sunitisingh/.m2/repository/net/sf/py4j/py4j/0.9/py4j-0.9.jar:/Users/sunitisingh/.m2/repository/org/spark-project/spark/unused/1.0.0/unused-1.0.0.jar:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-sql_2.10/1.2.0/spark-sql_2.10-1.2.0.jar:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-catalyst_2.10/1.2.0/spark-catalyst_2.10-1.2.0.jar:/Users/sunitisingh/.m2/repository/org/scala-lang/scala-compiler/2.10.4/scala-compiler-2.10.4.jar:/Users/sunitisingh/.m2/repository/org/scalamacros/quasiquotes_2.10/2.0.1/quasiquotes_2.10-2.0.1.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-column/1.6.0rc3/parquet-column-1.6.0rc3.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-common/1.6.0rc3/parquet-common-1.6.0rc3.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-encoding/1.6.0rc3/parquet-encoding-1.6.0rc3.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-generator/1.6.0rc3/parquet-generator-1.6.0rc3.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-hadoop/1.6.0rc3/parquet-hadoop-1.6.0rc3.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-format/2.2.0-rc1/parquet-format-2.2.0-rc1.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-jackson/1.6.0rc3/parquet-jackson-1.6.0rc3.jar:/Users/sunitisingh/.m2/repository/org/apache/spark/spark-hive_2.10/1.6.0/spark-hive_2.10-1.6.0.jar:/Users/sunitisingh/.m2/repository/com/twitter/parquet-hadoop-bundle/1.6.0/parquet-hadoop-bundle-1.6.0.jar:/Users/sunitisingh/.m2/repository/org/spark-project/hive/hive-exec/1.2.1.spark/hive-exec-1.2.1.spark.jar:/Users/sunitisingh/.m2/repository/javolution/javolution/5.5.1/javolution-5.5.1.jar:/Users/sunitisingh/.m2/repository/log4j/apache-log4j-extras/1.2.17/apache-log4j-extras-1.2.17.jar:/Users/sunitisingh/.m2/repository/org/antlr/antlr-runtime/3.4/antlr-runtime-3.4.jar:/Users/sunitisingh/.m2/repository/org/antlr/stringtemplate/3.2.1/stringtemplate-3.2.1.jar:/Users/sunitisingh/.m2/repository/antlr/antlr/2.7.7/antlr-2.7.7.jar:/Users/sunitisingh/.m2/repository/org/antlr/ST4/4.0.4/ST4-4.0.4.jar:/Users/sunitisingh/.m2/repository/org/apache/commons/commons-compress/1.4.1/commons-compress-1.4.1.jar:/Users/sunitisingh/.m2/repository/org/tukaani/xz/1.0/xz-1.0.jar:/Users/sunitisingh/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar:/Users/sunitisingh/.m2/repository/com/googlecode/javaewah/JavaEWAH/0.3.2/JavaEWAH-0.3.2.jar:/Users/sunitisingh/.m2/repository/org/iq80/snappy/snappy/0.2/snappy-0.2.jar:/Users/sunitisingh/.m2/repository/org/json/json/20090211/json-20090211.jar:/Users/sunitisingh/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/sunitisingh/.m2/repository/net/sf/opencsv/opencsv/2.3/opencsv-2.3.jar:/Users/sunitisingh/.m2/repository/jline/jline/2.12/jline-2.12.jar:/Users/sunitisingh/.m2/repository/org/spark-project/hive/hive-metastore/1.2.1.spark/hive-metastore-1.2.1.spark.jar:/Users/sunitisingh/.m2/repository/com/jolbox/bonecp/0.8.0.RELEASE/bonecp-0.8.0.RELEASE.jar:/Users/sunitisingh/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/sunitisingh/.m2/repository/commons-logging/commons-logging/1.1.3/commons-logging-1.1.3.jar:/Users/sunitisingh/.m2/repository/org/apache/derby/derby/
10.10.2.0/derby-10.10.2.0.jar:/Users/sunitisingh/.m2/repository/org/datanucleus/datanucleus-api-jdo/3.2.6/datanucleus-api-jdo-3.2.6.jar:/Users/sunitisingh/.m2/repository/org/datanucleus/datanucleus-rdbms/3.2.9/datanucleus-rdbms-3.2.9.jar:/Users/sunitisingh/.m2/repository/commons-pool/commons-pool/1.5.4/commons-pool-1.5.4.jar:/Users/sunitisingh/.m2/repository/commons-dbcp/commons-dbcp/1.4/commons-dbcp-1.4.jar:/Users/sunitisingh/.m2/repository/javax/jdo/jdo-api/3.0.1/jdo-api-3.0.1.jar:/Users/sunitisingh/.m2/repository/javax/transaction/jta/1.1/jta-1.1.jar:/Users/sunitisingh/.m2/repository/org/apache/avro/avro/1.7.7/avro-1.7.7.jar:/Users/sunitisingh/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/sunitisingh/.m2/repository/org/apache/calcite/calcite-avatica/1.2.0-incubating/calcite-avatica-1.2.0-incubating.jar:/Users/sunitisingh/.m2/repository/org/apache/calcite/calcite-core/1.2.0-incubating/calcite-core-1.2.0-incubating.jar:/Users/sunitisingh/.m2/repository/org/apache/calcite/calcite-linq4j/1.2.0-incubating/calcite-linq4j-1.2.0-incubating.jar:/Users/sunitisingh/.m2/repository/net/hydromatic/eigenbase-properties/1.1.5/eigenbase-properties-1.1.5.jar:/Users/sunitisingh/.m2/repository/org/codehaus/janino/commons-compiler/2.7.6/commons-compiler-2.7.6.jar:/Users/sunitisingh/.m2/repository/org/apache/httpcomponents/httpclient/4.3.2/httpclient-4.3.2.jar:/Users/sunitisingh/.m2/repository/org/apache/httpcomponents/httpcore/4.3.1/httpcore-4.3.1.jar:/Users/sunitisingh/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.9.13/jackson-mapper-asl-1.9.13.jar:/Users/sunitisingh/.m2/repository/commons-codec/commons-codec/1.10/commons-codec-1.10.jar:/Users/sunitisingh/.m2/repository/joda-time/joda-time/2.9/joda-time-2.9.jar:/Users/sunitisingh/.m2/repository/org/jodd/jodd-core/3.5.2/jodd-core-3.5.2.jar:/Users/sunitisingh/.m2/repository/org/datanucleus/datanucleus-core/3.2.10/datanucleus-core-3.2.10.jar:/Users/sunitisingh/.m2/repository/org/apache/thrift/libthrift/0.9.2/libthrift-0.9.2.jar:/Users/sunitisingh/.m2/repository/org/apache/thrift/libfb303/0.9.2/libfb303-0.9.2.jar
-bootclasspath
/Users/sunitisingh/.m2/repository/org/scala-lang/scala-library/2.10.5/scala-library-2.10.5.jar
-javaextdirs      last tree to typer: TypeTree(class BoxedUnit)
  symbol: class BoxedUnit in package runtime (flags: )    symbol
definition: class BoxedUnit extends                   tpe:
runtime.BoxedUnit        symbol owners: class BoxedUnit -> package runtime
      context owners: value hiveContext -> method main -> object DataExp ->
package spark  == Enclosing template or block ==  DefDef( // def main(args:
Array[String]): Unit in object DataExp   <method>   ""main""   []   // 1
parameter list   ValDef( // args: Array[String]     <param>     ""args""
<tpt> // tree.tpe=Array[String]     <empty>   )   <tpt> //
tree.tpe=runtime.BoxedUnit   Block( // tree.tpe=Unit     // 3 statements
  ValDef( // val conf: org.apache.spark.SparkConf       <triedcooking>
  ""conf""       <tpt> // tree.tpe=org.apache.spark.SparkConf       Apply( //
def setMaster(master: String): org.apache.spark.SparkConf in class
SparkConf         new
org.apache.spark.SparkConf().setAppName(""DataExp"").""setMaster"" // def
setMaster(master: String): org.apache.spark.SparkConf in class SparkConf
      ""local""       )     )     ValDef( // val sc:
org.apache.spark.SparkContext       <triedcooking>       ""sc""       <tpt>
// tree.tpe=org.apache.spark.SparkContext       Apply( // def
<init>(config: org.apache.spark.SparkConf): org.apache.spark.SparkContext
in class SparkContext         new org.apache.spark.SparkContext.""<init>"" //
def <init>(config: org.apache.spark.SparkConf):
org.apache.spark.SparkContext in class SparkContext         ""conf"" // val
conf: org.apache.spark.SparkConf       )     )     ValDef( // val
hiveContext: org.apache.spark.sql.hive.HiveContext       0
""hiveContext""       <tpt> // tree.t spark Unknown Scala Problem

*Maven dependency* --

<dependency>

  <groupId>org.apache.spark</groupId>

  <artifactId>spark-core_2.10</artifactId>

  <version>1.6.0</version>

  </dependency>

  <dependency>

  <groupId>org.apache.spark</groupId>

  <artifactId>spark-sql_2.10</artifactId>

  <version>1.2.0</version>

  </dependency>

  <dependency>

  <groupId>org.apache.spark</groupId>

  <artifactId>spark-hive_2.10</artifactId>

  <version>1.6.0</version>

  </dependency>
I am not able to understand what is missing.

Regards,
Suniti
"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Tue, 8 Mar 2016 03:25:11 +0300",Dynamic allocation availability on standalone mode. Misleading doc.,dev@spark.apache.org,"Hi, the feature looks like the one I'd like to use, but there are two
different descriptions in the docs of whether it's available.

I'm on a standalone deployment mode and here:
http://spark.apache.org/docs/latest/configuration.html it's specified the
feature is available only for YARN, but here:
http://spark.apache.org/docs/latest/job-scheduling.html#dynamic-resource-allocation
it says it's available on all coarse-grained cluster managers including
standalone.

So, is the feature available in standalone mode?

Thank you.
--
Be well!
Jean Morozov
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 7 Mar 2016 17:07:40 -0800",Re: Dynamic allocation availability on standalone mode. Misleading doc.,Eugene Morozov <evgeny.a.morozov@gmail.com>,"Yes, it works in standalone mode.


"
Suniti Singh <suniti.singh@gmail.com>,"Mon, 7 Mar 2016 17:11:19 -0800",Re: Adding hive context gives error,Tristan Nixon <tnixon@memeticlabs.org>,"yeah i realized it and changed the version of it to 1.6.0 as mentioned in
http://mvnrepository.com/artifact/org.apache.spark/spark-sql_2.10/1.6.0

I added the spark sql dependency back to the pom.xml and the scala code
works just fine.




"
Saisai Shao <sai.sai.shao@gmail.com>,"Tue, 8 Mar 2016 09:40:31 +0800",Re: Dynamic allocation availability on standalone mode. Misleading doc.,Mark Hamstra <mark@clearstorydata.com>,"Yes, we need to fix the document.


"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Mar 2016 17:56:08 -0800",Re: Dynamic allocation availability on standalone mode. Misleading doc.,Saisai Shao <sai.sai.shao@gmail.com>,"The doc fix was merged in 1.6.1, so it will get updated automatically once
we push the 1.6.1 docs.



"
Josh Rosen <joshrosen@databricks.com>,"Tue, 08 Mar 2016 02:57:44 +0000","Does anyone implement org.apache.spark.serializer.Serializer in their
 own code?","user <user@spark.apache.org>, Dev <dev@spark.apache.org>","Does anyone implement Spark's serializer interface
(org.apache.spark.serializer.Serializer) in your own third-party code? If
so, please let me know because I'd like to change this interface from a
DeveloperAPI to private[spark] in Spark 2.0 in order to do some cleanup and
refactoring. I think that the only reason it was a DeveloperAPI was Shark,
but I'd like to confirm this by asking the community.

Thanks,
Josh
"
Koert Kuipers <koert@tresata.com>,"Mon, 7 Mar 2016 22:22:42 -0500","Re: Does anyone implement org.apache.spark.serializer.Serializer in
 their own code?",Josh Rosen <joshrosen@databricks.com>,"we are not, but it seems reasonable to me that a user has the ability to
implement their own serializer.

can you refactor and break compatibility, but not make it private?


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 7 Mar 2016 20:02:08 -0800","Re: Does anyone implement org.apache.spark.serializer.Serializer in
 their own code?",Josh Rosen <joshrosen@databricks.com>,"Josh:
SerializerInstance and SerializationStream would also become private[spark],
right ?

Thanks


"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 8 Mar 2016 07:20:14 +0100",ML ALS API,"""dev@spark.apache.org"" <dev@spark.apache.org>","Can I ask for a clarifications regarding ml.recommendation.ALS:

- is train method
(https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala#L598)
intended to be be public?
- Rating class 
(https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala#L436)is
using float instead of double like its MLLib counterpart. Is it going to
be a default encoding in 2.0+?

-- 
Best,
Maciej Szymkiewicz


"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Mar 2016 22:51:55 -0800",Re: More Robust DataSource Parameters,Hamel Kothari <hamelkothari@gmail.com>,"Hi Hamel,

Sorry for the slow reply. Do you mind writing down the thoughts in a
document, with API sketches? I think all the devils are in the details of
the API for this one.

If we can design an API that is type-safe, supports all languages, and also
can be stable, then it sounds like a great idea.




"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 08:38:22 +0100",BUILD FAILURE due to...Unable to find configuration file at location dev/scalastyle-config.xml,dev <dev@spark.apache.org>,"Hi,

Got the BUILD FAILURE. Anyone looking into it?

âžœ  spark git:(master) âœ— ./build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.2 -Phive -Phive-thriftserver -DskipTests clean
install
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 2.837 s
[INFO] Finished at: 2016-03-08T08:19:36+01:00
[INFO] Final Memory: 50M/581M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal
org.scalastyle:scalastyle-maven-plugin:0.8.0:check (default) on
project spark-parent_2.11: Failed during scalastyle execution: Unable
to find configuration file at location dev/scalastyle-config.xml ->
[Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with
the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions,
please read the following articles:
[ERROR] [Help 1]
http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 7 Mar 2016 23:39:58 -0800","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml","Jacek Laskowski <jacek@japila.pl>, Sean Owen <srowen@gmail.com>",#NAME?
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Mon, 7 Mar 2016 23:41:27 -0800","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml",Jacek Laskowski <jacek@japila.pl>,"There is a fix: https://github.com/apache/spark/pull/11567


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 08:42:43 +0100","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml","""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Hi,

Nope. It's not. It's at
https://github.com/apache/spark/commit/0eea12a3d956b54bbbd73d21b296868852a04494#diff-600376dffeb79835ede4a0b285078036L2249.
I've got that and testing...

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


:
-
-
-

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 08:46:24 +0100","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml","""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Okey...it's building now
properly...https://github.com/apache/spark/pull/11567 + git mv
scalastyle-config.xml dev/

How to fix it in the repo? Should I send a pull request to...pull
request #11567? Guide me or fix it yourself...somehow :-)

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


a04494#diff-600376dffeb79835ede4a0b285078036L2249.
:
e:
6
--
--
--
n

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 7 Mar 2016 23:48:20 -0800","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml",Jacek Laskowski <jacek@japila.pl>,"Ur, may I include that, too?

Dongjoon.


a04494#diff-600376dffeb79835ede4a0b285078036L2249
2.6
n
e
g.
,
-
"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 7 Mar 2016 23:54:01 -0800","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml",Jacek Laskowski <jacek@japila.pl>,"Or, I hope one of committer commits both mine(11567) and that soon.
It's related to build setting files, Jenkins test tooks over 2 hours. :(

Dongjoon.


2a04494#diff-600376dffeb79835ede4a0b285078036L2249
-2.6
an
le
h
s,
--
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 08:55:20 +0100","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml",Dongjoon Hyun <dongjoon@apache.org>,"Sure! Go ahead and...fix the build. Thanks.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


852a04494#diff-600376dffeb79835ede4a0b285078036L2249.
-2.6
an
-----
-----
-----
le
h
s,
tion
--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 08:56:27 +0100","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml",Dongjoon Hyun <dongjoon@apache.org>,"Hi,

I could confirm that the two changes fixed the build. #happy again.

âžœ  spark git:(master) âœ— ./build/mvn -Pyarn -Phadoop-2.6
-Dhadoop.version=2.7.2 -Phive -Phive-thriftserver -DskipTests clean
install
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 11:38 min
[INFO] Finished at: 2016-03-08T08:55:55+01:00
[INFO] Final Memory: 453M/1340M
[INFO] ------------------------------------------------------------------------


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


:
:
:
8852a04494#diff-600376dffeb79835ede4a0b285078036L2249.
p-2.6
ean
------
------
------
ble
th
ns,
ption
---

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 08:40:20 +0100","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml",dev <dev@spark.apache.org>,"Hi,

At first glance it appears the commit *yesterday* (Warsaw time) broke
the build :(

https://github.com/apache/spark/commit/0eea12a3d956b54bbbd73d21b296868852a04494


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski


------
------
------

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 8 Mar 2016 00:24:41 -0800","Re: BUILD FAILURE due to...Unable to find configuration file at
 location dev/scalastyle-config.xml",Jacek Laskowski <jacek@japila.pl>,"Hi, I updated PR https://github.com/apache/spark/pull/11567.

But, `lint-java` fails if that file is in the dev folder. (Jenkins fails,
too.)

So, inevitably, I changed pom.xml instead.

Dongjoon.



a04494
"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 08 Mar 2016 09:07:12 +0000",Re: ML ALS API,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Maciej

Yes, that *train* method is intended to be public, but it is marked as
*DeveloperApi*, which means that backward compatibility is not necessarily
guaranteed, and that method may change. Having said that, even APIs marked
as DeveloperApi do tend to be relatively stable.

As the comment mentions:

 * :: DeveloperApi ::
 * An implementation of ALS that supports *generic ID types*, specialized
for Int and Long. This is
 * exposed as a developer API for users who do need other ID types. But it
is not recommended
 * because it increases the shuffle size and memory requirement during
training.

This *train* method is intended for the use case where user and item ids
are not the default Int (e.g. String). As you can see it returns the factor
RDDs directly, as opposed to an ALSModel instance, so overall it is a
little less user-friendly.

The *Float* ratings are to save space and make ALS more efficient overall.
That will not change in 2.0+ (especially since the precision of ratings is
not very important).

Hope that helps.


"
"""Praveen Devarao"" <praveendrl@in.ibm.com>","Tue, 8 Mar 2016 15:08:05 +0530",Spark structured streaming,"user@spark.apache.org, dev@spark.apache.org","Hi,

        I would like to get my hands on the structured streaming feature 
coming out in Spark 2.0. I have tried looking around for code samples to 

is the test cases that have been committed under the JIRA umbrella 
https://issues.apache.org/jira/browse/SPARK-8360 but the test cases don't 
lead to building a example code as they seem to be working out of internal 
classes.

        Could anyone point me to some resources or pointers in code that I 
can start with to understand structured streaming from a consumability 
angle.

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""

"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 11:44:35 +0100",Re: Spark structured streaming,Praveen Devarao <praveendrl@in.ibm.com>,"Hi Praveen,

I've spent few hours on the changes related to streaming dataframes
(included in the SPARK-8360) and concluded that it's currently only
possible to read.stream(), but not write.stream() since there are no
streaming Sinks yet.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
"""Praveen Devarao"" <praveendrl@in.ibm.com>","Tue, 8 Mar 2016 17:02:43 +0530",Re: Spark structured streaming,Jacek Laskowski <jacek@japila.pl>,"Thanks Jacek for the pointer.

Any idea which package can be used in .format(). The test cases seem to 
work out of the DefaultSource class defined within the 
DataFrameReaderWriterSuite [
org.apache.spark.sql.streaming.test.DefaultSource]

Thanking You
---------------------------------------------------------------------------------
Praveen Devarao
Spark Technology Centre
IBM India Software Labs
---------------------------------------------------------------------------------
""Courage doesn't always roar. Sometimes courage is the quiet voice at the 
end of the day saying I will try again""



From:   Jacek Laskowski <jacek@japila.pl>
To:     Praveen Devarao/India/IBM@IBMIN
Cc:     user <user@spark.apache.org>, dev <dev@spark.apache.org>
Date:   08/03/2016 04:17 pm
Subject:        Re: Spark structured streaming



Hi Praveen,

I've spent few hours on the changes related to streaming dataframes
(included in the SPARK-8360) and concluded that it's currently only
possible to read.stream(), but not write.stream() since there are no
streaming Sinks yet.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



get
is
internal

I
---------------------------------------------------------------------------------
---------------------------------------------------------------------------------
the

---------------------------------------------------------------------





"
Cody Koeninger <cody@koeninger.org>,"Tue, 8 Mar 2016 08:49:37 -0600",Re: Use cases for kafka direct stream messageHandler,Marius Soutier <mps.dev@gmail.com>,"No, looks like you'd have to catch them in the serializer and have the
serializer return option or something. The new consumer builds a buffer
full of records, not one at a time.

"
Jacek Laskowski <jacek@japila.pl>,"Tue, 8 Mar 2016 16:38:32 +0100",Re: Spark structured streaming,Praveen Devarao <praveendrl@in.ibm.com>,"Hi Praveen,

I don't really know. I think TD or Michael should know as they
personally involved in the task (as far as I could figure it out from
the JIRA and the changes). Ping people on the JIRA so they notice your
question(s).

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 8 Mar 2016 10:44:50 -0800",Re: Spark structured streaming,Jacek Laskowski <jacek@japila.pl>,"This is in active development, so there is not much that can be done from
an end user perspective.  In particular the only sink that is available in
apache/master is a testing sink that just stores the data in memory.  We
are working on a parquet based file sink and will eventually support all
the of Data Source API file formats (text, json, csv, orc, parquet).


"
Yin Huai <yhuai@databricks.com>,"Tue, 8 Mar 2016 10:59:14 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Reynold Xin <rxin@databricks.com>,1
Andrew Or <andrew@databricks.com>,"Tue, 8 Mar 2016 10:59:53 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Yin Huai <yhuai@databricks.com>,"+1

2016-03-08 10:59 GMT-08:00 Yin Huai <yhuai@databricks.com>:

"
Burak Yavuz <brkyvz@gmail.com>,"Tue, 8 Mar 2016 11:28:17 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Andrew Or <andrew@databricks.com>,1
Reynold Xin <rxin@databricks.com>,"Tue, 8 Mar 2016 15:06:51 -0800",Spark 2.0 high level API doc,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

As most of you know, we are doing some API changes in Spark 2.0 to prepare
Spark for the future, and a lot of focus there are on DataFrames and
Datasets. We wrote a high level API doc and uploaded it to JIRA last week,
but I don't think a lot of people monitor JIRA as closely. Here's a link:
https://issues.apache.org/jira/browse/SPARK-13485

Please check it out and comment on it if you have any feedback. Cheers.
"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Wed, 9 Mar 2016 09:47:10 +0530",Spark Scheduler creating Straggler Node,"user <user@spark.apache.org>, Spark dev list <dev@spark.apache.org>","Hi All,

    When a Spark Job is running, and one of the Spark Executor on Node A
has some partitions cached. Later for some other stage, Scheduler tries to
assign a task to Node A to process a cached partition (PROCESS_LOCAL). But
meanwhile the Node A is occupied with some other
tasks and got busy. Scheduler waits for spark.locality.wait interval and
times out and tries to find some other node B which is NODE_LOCAL. The
executor on Node B will try to get the cached partition from Node A which
adds network IO to node and also some extra CPU for I/O. Eventually,
every node will have a task that is waiting to fetch some cached partition
from node A and so the spark job / cluster is basically blocked on a single
node.

Spark JIRA is created https://issues.apache.org/jira/browse/SPARK-13718

Beginning from Spark 1.2, Spark introduced External Shuffle Service to
enable executors fetch shuffle files from an external service instead of
from each other which will offload the load on Spark Executors.

We want to check whether a similar thing of an External Service is
implemented for transferring the cached partition to other executors.


Thanks, Prabhu Joseph
"
Reynold Xin <rxin@databricks.com>,"Tue, 8 Mar 2016 20:20:29 -0800",Re: Spark Scheduler creating Straggler Node,Prabhu Joseph <prabhujose.gates@gmail.com>,"You just want to be able to replicate hot cached blocks right?


"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 9 Mar 2016 14:49:09 +0900","Inconsistent file extensions and omitting file extensions written by
 CSV, TEXT and JSON data sources.",dev@spark.apache.org,"Hi all,

Currently, the output from CSV, TEXT and JSON data sources does not have
file extensions such as .csv, .txt and .json (except for compression
extensions such as .gz, .deflate and .bz4).

In addition, it looks Parquet has the extensions such as .gz.parquet or
.snappy.parquet according to compression codecs whereas ORC does not have
such extensions but it is just .orc.

I tried to search some JIRAs related with this but I could not find yet
but I did not open a JIRA directly because I feel like this is already
concerned

Maybe could I open a JIRA for this inconsistent file extensions?

It would be thankful if you give me some feedback

Thanks!
"
Reynold Xin <rxin@databricks.com>,"Tue, 8 Mar 2016 21:49:55 -0800","Re: Inconsistent file extensions and omitting file extensions written
 by CSV, TEXT and JSON data sources.",Hyukjin Kwon <gurwls223@gmail.com>,"Isn't this just specified by the user?



"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Wed, 9 Mar 2016 11:22:18 +0530",Re: Spark Scheduler creating Straggler Node,Reynold Xin <rxin@databricks.com>,"I don't just want to replicate all Cached Blocks. I am trying to find a way
to solve the issue which i mentioned above mail. Having replicas for all
cached blocks will add more cost to customers.




"
Sean Owen <sowen@cloudera.com>,"Wed, 9 Mar 2016 09:30:53 +0000","Re: Inconsistent file extensions and omitting file extensions written
 by CSV, TEXT and JSON data sources.",Hyukjin Kwon <gurwls223@gmail.com>,"These files are effectively an internal representation, and I would
not expect them to have such an extension. For example, you're not
really guaranteed that the way the data breaks up leaves each file a
valid JSON doc.


---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 9 Mar 2016 19:34:19 +0900","Re: Inconsistent file extensions and omitting file extensions written
 by CSV, TEXT and JSON data sources.",Sean Owen <sowen@cloudera.com>,"This discussion is going to the Jira. Please refer the Jira if anyone is
interested in this.

"
<Ioannis.Deligiannis@nomura.com>,"Wed, 9 Mar 2016 10:43:14 +0000",RE: Spark Scheduler creating Straggler Node,"<rxin@databricks.com>, <prabhujose.gates@gmail.com>","It would be nice to have a code-configurable fall-back plan for such cases. Any generalized solution can cause problems elsewhere.

Simply replicating hot cached blocks would be complicated to maintain and could cause OOME. In the case I described on the JIRA, the hot partition will be changing e.g. every hour. Even though the persistence is calculated to be 1xMEM_SERIALIZED, replicating it will eventually break this contract and cause OOME. Of course in some cases the hot partition will be the same so it makes sense to replicate (possibly even to every node).

What would be very helpful, would be a way to configure caching/scheduling on the RDD level. So something like this would suit most cases (Simplified as it would require much more thought):
RDD.maxPartitionCache=5: Maximum number of times a partition can be cached
RDD.maxTTLMillis=60000: Simple time based eviction policy to drop extra copied after X millis of inactivity. Alternatively, these copies could have a lower priority when BlockManager evicts cached RDDs.
RDD.nonNodePolicy=Recompute: A hint that if a task is not accepted by LOCAL or NODE to re-compute the RDD. (Note that the profiling evidence of mentioned Jira was evenly distributed when RDD was not cached)

PS. I donâ€™t have adequate Scala/Spark source knowledge to suggest an actual solution or make sure that what I am suggesting is even possibleicks.com]
Sent: 09 March 2016 04:20
To: Prabhu Joseph
Cc: user; Spark dev list
Subject: Re: Spark Scheduler creating Straggler Node

You just want to be able to replicate hot cached blocks right?

On Tuesday, March 8, 2016, Prabhu Joseph <prabhujose.gates@gmail.com<mailto:prabhujose.gates@gmail.com>> wrote:
Hi All,

    When a Spark Job is running, and one of the Spark Executor on Node A has some partitions cached. Later for some other stage, Scheduler tries to assign a task to Node A to process a cached partition (PROCESS_LOCAL). But meanwhile the Node A is occupied with some other
tasks and got busy. Scheduler waits for spark.locality.wait interval and times out and tries to find some other node B which is NODE_LOCAL. The executor on Node B will try to get the cached partition from Node A which adds network IO to node and also some extra CPU for I/O. Eventually,
every node will have a task that is waiting to fetch some cached partition from node A and so the spark job / cluster is basically blocked on a single node.

Spark JIRA is created https://issues.apache.org/jira/browse/SPARK-13718<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D13718&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=pkiQbFij92oOibY1jRPQTCeMEkA_q3cY903B10riOxs&s=QB9ZvdmXtDBOycsz-EWyezFNoc-QavPvCLbDO04LPjg&e=>

Beginning from Spark 1.2, Spark introduced External Shuffle Service to enable executors fetch shuffle files from an external service instead of from each other which will offload the load on Spark Executors.

We want to check whether a similar thing of an External Service is implemented for transferring the cached partition to other executors.



Thanks, Prabhu Joseph




This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm

"
<Ioannis.Deligiannis@nomura.com>,"Wed, 9 Mar 2016 10:47:13 +0000",RE: Spark Scheduler creating Straggler Node,"<user@spark.apache.org>, <dev@spark.apache.org>","It would be nice to have a code-configurable fall-back plan for such cases. Any generalized solution can cause problems elsewhere.

Simply replicating hot cached blocks would be complicated to maintain and could cause OOME. In the case I described on the JIRA, the hot partition will be changing e.g. every hour. Even though the persistence is calculated to be 1xMEM_SERIALIZED, replicating it will eventually break this contract and cause OOME. Of course in some cases the hot partition will be the same so it makes sense to replicate (possibly even to every node).

What would be very helpful, would be a way to configure caching/scheduling on the RDD level. So something like this would suit most cases (Simplified as it would require much more thought):
RDD.maxPartitionCache=5: Maximum number of times a partition can be cached
RDD.maxTTLMillis=60000: Simple time based eviction policy to drop extra copied after X millis of inactivity. Alternatively, these copies could have a lower priority when BlockManager evicts cached RDDs.
RDD.nonNodePolicy=Recompute: A hint that if a task is not accepted by LOCAL or NODE to re-compute the RDD. (Note that the profiling evidence of mentioned Jira was evenly distributed when RDD was not cached)

PS. I donâ€™t have adequate Scala/Spark source knowledge to suggest an actual solution or make sure that what I am suggesting is even possiblegates@gmail.com]
Sent: 09 March 2016 05:52
To: Reynold Xin
Cc: user; Spark dev list
Subject: Re: Spark Scheduler creating Straggler Node

I don't just want to replicate all Cached Blocks. I am trying to find a way to solve the issue which i mentioned above mail. Having replicas for all cached blocks will add more cost to customers.



On Wed, Mar 9, 2016 at 9:50 AM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:
You just want to be able to replicate hot cached blocks right?


On Tuesday, March 8, 2016, Prabhu Joseph <prabhujose.gates@gmail.com<mailto:prabhujose.gates@gmail.com>> wrote:
Hi All,

    When a Spark Job is running, and one of the Spark Executor on Node A has some partitions cached. Later for some other stage, Scheduler tries to assign a task to Node A to process a cached partition (PROCESS_LOCAL). But meanwhile the Node A is occupied with some other
tasks and got busy. Scheduler waits for spark.locality.wait interval and times out and tries to find some other node B which is NODE_LOCAL. The executor on Node B will try to get the cached partition from Node A which adds network IO to node and also some extra CPU for I/O. Eventually,
every node will have a task that is waiting to fetch some cached partition from node A and so the spark job / cluster is basically blocked on a single node.

Spark JIRA is created https://issues.apache.org/jira/browse/SPARK-13718<https://urldefense.proofpoint.com/v2/url?u=https-3A__issues.apache.org_jira_browse_SPARK-2D13718&d=CwMFaQ&c=dCBwIlVXJsYZrY6gpNt0LA&r=B8E4n9FrSS85mPCi6Mfs7cyEPQnVrpcQ1zeB-JKws6A&m=2wN8TZn5RLY6a_v5FejSTCLWQdxzNUnF4p3qj-tAhdE&s=5jHceSn6MVEStQ1gitgn0QipiFNx41leGkaeGypIlFk&e=>

Beginning from Spark 1.2, Spark introduced External Shuffle Service to enable executors fetch shuffle files from an external service instead of from each other which will offload the load on Spark Executors.

We want to check whether a similar thing of an External Service is implemented for transferring the cached partition to other executors.



Thanks, Prabhu Joseph





This e-mail (including any attachments) is private and confidential, may contain proprietary or privileged information and is intended for the named recipient(s) only. Unintended recipients are strictly prohibited from taking action on the basis of information in this e-mail and must contact the sender immediately, delete this e-mail (and all attachments) and destroy any hard copies. Nomura will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in, this e-mail. If verification is sought please request a hard copy. Any reference to the terms of executed transactions should be treated as preliminary only and subject to formal written confirmation by Nomura. Nomura reserves the right to retain, monitor and intercept e-mail communications through its networks (subject to and in accordance with applicable laws). No confidentiality or privilege is waived or lost by Nomura by any mistransmission of this e-mail. Any reference to ""Nomura"" is a reference to any entity in the Nomura Holdings, Inc. group. Please read our Electronic Communications Legal Notice which forms part of this e-mail: http://www.Nomura.com/email_disclaimer.htm

"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Thu, 10 Mar 2016 00:04:21 +0900",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),dev@spark.apache.org,"+1 (non-binding)


"
Sachin Aggarwal <different.sachin@gmail.com>,"Wed, 9 Mar 2016 22:39:28 +0530","submissionTime vs batchTime, DirectKafka",dev@spark.apache.org,"Hi All,

we have batchTime and submissionTime.

@param batchTime   Time of the batch

@param submissionTime  Clock time of when jobs of this batch was
submitted to the streaming scheduler queue

1) we are seeing difference between batchTime and submissionTime for small
batches(300ms) even in minutes for direct kafka this we see, only when the
processing time is more than the batch interval. how can we explain this
delay??

2) In one of case batch processing time is more then batch interval, then
will spark fetch the next batch data from kafka parallelly processing the
current batch or it will wait for current batch to finish first ?

I would be thankful if you give me some pointers

Thanks!
-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
Cody Koeninger <cody@koeninger.org>,"Wed, 9 Mar 2016 11:16:24 -0600","Re: submissionTime vs batchTime, DirectKafka",Sachin Aggarwal <different.sachin@gmail.com>,"Spark streaming by default will not start processing a batch until the
current batch is finished.  So if your processing time is larger than
your batch time, delays will build up.


---------------------------------------------------------------------


"
Sachin Aggarwal <different.sachin@gmail.com>,"Thu, 10 Mar 2016 00:13:09 +0530","Re: submissionTime vs batchTime, DirectKafka",Cody Koeninger <cody@koeninger.org>,"where are we capturing this delay?
I am aware of scheduling delay which is defined as processing
time-submission time not the batch create time





-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
Cody Koeninger <cody@koeninger.org>,"Wed, 9 Mar 2016 12:52:48 -0600","Re: submissionTime vs batchTime, DirectKafka",Sachin Aggarwal <different.sachin@gmail.com>,"I'm really not sure what you're asking.


---------------------------------------------------------------------


"
Alan Braithwaite <alan@cloudflare.com>,"Wed, 9 Mar 2016 11:19:31 -0800",Re: Use cases for kafka direct stream messageHandler,Cody Koeninger <cody@koeninger.org>,"I'd probably prefer to keep it the way it is, unless it's becoming more
like the function without the messageHandler argument.

Right now I have code like this, but I wish it were more similar looking:

    if (parsed.partitions.isEmpty()) {
      JavaPairInputDStream<String, MessageWrapper> kvstream = KafkaUtils
          .createDirectStream(jssc, String.class, MessageWrapper.class,
StringDecoder.class,
              MessageDecoder.class, kafkaArgs(parsed), topicSet);
      requests = kvstream.map((Function<Tuple2<String, MessageWrapper>,
MessageWrapper>) Tuple2::_2);
    } else {
      requests = KafkaUtils.createDirectStream(jssc, String.class,
          MessageWrapper.class, StringDecoder.class, MessageDecoder.class,
MessageWrapper.class,
          kafkaArgs(parsed), parsed.partitions,
(Function<MessageAndMetadata<String, MessageWrapper>,
              MessageWrapper>) MessageAndMetadata::message);
    }

Of course, this is in the Java API so it may not have relevance to what
you're talking about.

Perhaps if both functions (the one with partitions arg and the one without)
returned just ConsumerRecord, I would like that more.

- Alan


"
Cody Koeninger <cody@koeninger.org>,"Wed, 9 Mar 2016 13:38:20 -0600",Re: Use cases for kafka direct stream messageHandler,Alan Braithwaite <alan@cloudflare.com>,"Yeah, to be clear, I'm talking about having only one constructor for a
direct stream, that will give you a stream of ConsumerRecord.

Different needs for topic subscription, starting offsets, etc could be
handled by calling appropriate methods after construction but before
starting the stream.



---------------------------------------------------------------------


"
Mohammed Guller <mohammed@glassbeam.com>,"Wed, 9 Mar 2016 19:45:12 +0000",Request to add a new book to the Books section on Spark's website,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","My book on Spark was recently published. I would like to request it to be added to the Books section on Spark's website.

Here are the details about the book.

Title: Big Data Analytics with Spark
Author: Mohammed Guller
Link: www.amazon.com/Big-Data-Analytics-Spark-Practitioners/dp/1484209656/<http://www.amazon.com/Big-Data-Analytics-Spark-Practitioners/dp/1484209656/
Brief Description:
This book is a hands-on guide for learning how to use Spark for different types of analytics, including batch, interactive, graph, and stream data analytics as well as machine learning. It covers Spark core, Spark SQL, DataFrames, Spark Streaming, GraphX, MLlib, and Spark ML. Plenty of examples are provided for the readers to practice with.

In addition to covering Spark in depth, the book provides an introduction to other big data technologies that are commonly used along with Spark, such as HDFS, Parquet, Kafka, Avro, Cassandra, HBase, Mesos, and YARN. The book also includes a primer on functional programming and Scala.

Please let me know if you need any other information.

Thanks,
Mohammed


"
Sean Owen <sowen@cloudera.com>,"Wed, 9 Mar 2016 19:46:13 +0000",Re: Request to add a new book to the Books section on Spark's website,Mohammed Guller <mohammed@glassbeam.com>,"Oh yeah I already added it after your earlier message, have a look.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 9 Mar 2016 15:29:11 -0800",Re: [VOTE] Release Apache Spark 1.6.1 (RC1),Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"+1 - Ported all our internal jobs to run on 1.6.1 with no regressions.


"
Michael Armbrust <michael@databricks.com>,"Wed, 9 Mar 2016 16:07:50 -0800",[RESULT] [VOTE] Release Apache Spark 1.6.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","This vote passes with nine +1s (five binding) and one binding +0!  Thanks
to everyone who tested/voted.  I'll start work on publishing the release
today.

+1:
Mark Hamstra*
Moshe Eshel
Egor Pahomov
Reynold Xin*
Yin Huai*
Andrew Or*
Burak Yavuz
Kousuke Saruta
Michael Armbrust*

0:
Sean Owen*


-1: (none)

*Binding


"
Sachin Aggarwal <different.sachin@gmail.com>,"Thu, 10 Mar 2016 10:29:51 +0530","Re: submissionTime vs batchTime, DirectKafka",Cody Koeninger <cody@koeninger.org>,"Hi cody,

let me try once again to explain with example.

In BatchInfo class of spark ""scheduling delay"" is defined as

*def schedulingDelay: Option[Long] = processingStartTime.map(_ -
submissionTime)*

I am dumping batchinfo object in my LatencyListener which extends
StreamingListener.

batchTime = 1457424695400 ms

submissionTime = 1457425630780 ms

difference = 935380 ms

can this be considered a lag in processing of events . what is possible
explaination for this lag?





-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
"""Mario Ds Briggs"" <mario.briggs@in.ibm.com>","Thu, 10 Mar 2016 10:51:28 +0530","Re: submissionTime vs batchTime, DirectKafka",Sachin Aggarwal <different.sachin@gmail.com>,"
Look at
  org.apache.spark.streaming.scheduler.JobGenerator

it has a RecurringTimer (timer) that will simply post 'JobGenerate' events
to a EventLoop at the batchInterval time.

This EventLoop's thread then picks up these events, uses the
streamingContext.graph' to generate a Job (InputDstream's compute method).
batchInfo.submissionTime is the time recorded after this generateJob
completes. The Job is then sent to the org.apache.spark.streaming.scheduler
.JobScheduler who has a ThreadExecutorPool to execute the Job.

JobGenerate events are not the only event that gets posted to the
JobGenerator.eventLoop. Other events are like DoCheckpoint,
ClearCheckpointData, ClearMetadata are also posted and all these events are
serviced by the EventLoop's single thread. So for instance if a
DoCheckPoint, ClearCheckpointData and ClearMetadata events are queued
before your nth JobGenerate event, then there will be a time difference
between the batchTime and SubmissionTime for that nth batch


thanks
Mario






  Hi cody,

  let me try once again to explain with example.

  In BatchInfo class of spark ""scheduling delay"" is defined as

  def schedulingDelay: Option[Long] = processingStartTime.map(_ -
  submissionTime)

  I am dumping batchinfo object in my LatencyListener which
  extends StreamingListener.
  batchTime = 1457424695400 ms
  submissionTime = 1457425630780 ms
  difference = 935380 ms

  can this be considered a lag in processing of events . what is possible
  explaination for this lag?

   I'm really not sure what you're asking.

   > where are we capturing this delay?
   > I am aware of scheduling delay which is defined as processing
   > time-submission time not the batch create time
   >
   >>
   >> Spark streaming by default will not start processing a batch until
   the
   >> current batch is finished.  So if your processing time is larger than
   >> your batch time, delays will build up.
   >>
   >> > Hi All,
   >> >
   >> > we have batchTime and submissionTime.
   >> >
   >> > @param batchTime   Time of the batch
   >> >
   >> > @param submissionTime  Clock time of when jobs of this batch was
   >> > submitted
   >> > to the streaming scheduler queue
   >> >
   >> > 1) we are seeing difference between batchTime and submissionTime
   for
   >> > small
   >> > batches(300ms) even in minutes for direct kafka this we see, only
   when
   >> > the
   >> > processing time is more than the batch interval. how can we explain
   this
   >> > delay??
   >> >
   >> > 2) In one of case batch processing time is more then batch
   interval,
   >> > then
   >> > will spark fetch the next batch data from kafka parallelly
   processing
   >> > the
   >> > current batch or it will wait for current batch to finish first ?
   >> >
   >> > I would be thankful if you give me some pointers
   >> >
   >> > Thanks!
   >> > --
   >> >
   >> > Thanks & Regards
   >> >
   >> > Sachin Aggarwal
   >> > 7760502772
   >
   >
   >
   >
   > --
   >
   > Thanks & Regards
   >
   > Sachin Aggarwal
   > 7760502772



  --

  Thanks & Regards

  Sachin Aggarwal
  7760502772



--

Thanks & Regards

Sachin Aggarwal
7760502772

"
"""FangFang Chen"" <lulynn_2015_spark@163.com>","Thu, 10 Mar 2016 16:19:22 +0800 (GMT+08:00)","dataframe.groupby.agg vs sql(""select from groupby)"")","""dev@spark.apache.org"" <dev@spark.apache.org>","hi,
Based on my testing, the memory cost is very different for 
1. sql(""select * from ..."").groupby.agg 
2. sql(""select ... From ... Groupby ..."").


For table.partition sized more than 500g, 2# run good, while outofmemory happened in 1#. I am using the same spark configurations.
Could somebody tell why this happened? 


·¢×Ô ÍøÒ×ÓÊÏä´óÊ¦"
Reynold Xin <rxin@databricks.com>,"Thu, 10 Mar 2016 00:20:44 -0800","Re: dataframe.groupby.agg vs sql(""select from groupby)"")",FangFang Chen <lulynn_2015_spark@163.com>,"They should be identical. Can you paste the detailed explain output.


 <http://u.163.com/signature>
"
Sachin Aggarwal <different.sachin@gmail.com>,"Thu, 10 Mar 2016 22:56:34 +0530","Re: submissionTime vs batchTime, DirectKafka",Mario Ds Briggs <mario.briggs@in.ibm.com>,"hi

can this be considered a lag in processing of events?
should we report this as delay.




-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
Renyi Xiong <renyixiong0@gmail.com>,"Thu, 10 Mar 2016 11:59:33 -0800",DynamicPartitionKafkaRDD - 1:n mapping between kafka and RDD partition,Tathagata Das <tdas@databricks.com>,"Hi TD,

Thanks a lot for offering to look at our PR (if we fire one) at the
conference NYC.

As we discussed briefly the issues of unbalanced and
under-distributed kafka partitions when developing Spark streaming
application in Mobius (C# for Spark), we're trying the option of
repartitioning within DirectKafkaInputDStream instead of DStream.repartiton
API which introduces extra network cost and doesn't really solve the root
cause.

However, instead of firing a JIRA with PR directly, we decided to create a
customized Kafka RDD / DStream (to start with and contribute back later if
success) - DynamicPartitionKafkaRDD and DynamicPartitionKafkaInputDStream
using inheritance model and expose a new API
KafkaUtils.CreateDirectStreamWithRepartition with one more parameter -
numPartitions (hint number of RDD partitions to create)

it'll be great that you can take look at the code and share your comments:

https://github.com/Microsoft/Mobius/tree/master/scala/src/main/org/apache/spark/streaming/api/kafka

the major relevant change is in DynamicPartitionKafkaRDD.getPartitions
where an average size of RDD partition is calculated first (total size of
the topic divided by numPartitions) and used to split partitions (final RDD
partitions will be greater or equal to numPartitions)

there's a concern that Kafka partition[i] no longer maps to task[i] which
might break existing application. here's our thinking:

a. OffsetRanges in original implementation may have multiple topics meaning
'partition i maps to tasks i' is generally a false statement

b. Even if only one topic is involved, partition sequence in offsetRanges
comes from Kafka topic meta data response which doesn't necessary guarantee
the sequence, even if it does, application should not take that dependency

c. Topic partition split happens only when configured


there're some other more complicated changes related to fault
tolerance which are irrelevant here (but you're more than welcome to
comment on them too) and are introduced to unblock the scenarios we're
experiencing on a daily basis.

1. temporally redirect kafka read to C# worker by passing metadata instead
of actual kafka messages to it, in C# worker, a C# version of kafka client
is used which enables much easier debugging

2. bypass metadata request exceptions on driver side and let next batch
retry

3. bypass some read errors on worker side


Note all above are at very early stage, your comments will be much valuable
and  appreciated.


Thanks a lot,

Reny.
"
Cody Koeninger <cody@koeninger.org>,"Thu, 10 Mar 2016 14:12:08 -0600",Re: DynamicPartitionKafkaRDD - 1:n mapping between kafka and RDD partition,Renyi Xiong <renyixiong0@gmail.com>,"The central problem with doing anything like this is that you break
one of the basic guarantees of kafka, which is in-order processing on
a per-topicpartition basis.

As far as PRs go, because of the new consumer interface for kafka 0.9
and 0.10, there's a lot of potential change already underway.

See

https://issues.apache.org/jira/browse/SPARK-12177


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 10 Mar 2016 13:08:04 -0800",[ANNOUNCE] Announcing Spark 1.6.1,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Spark 1.6.1 is a maintenance release containing stability fixes. This
release is based on the branch-1.6 maintenance branch of Spark. We
*strongly recommend* all 1.6.0 users to upgrade to this release.

Notable fixes include:
 - Workaround for OOM when writing large partitioned tables SPARK-12546
<https://issues.apache.org/jira/browse/SPARK-12546>
 - Several fixes to the experimental Dataset API - SPARK-12478
<https://issues.apache.org/jira/browse/SPARK-12478>, SPARK-12696
<https://issues.apache.org/jira/browse/SPARK-12696>, SPARK-13101
<https://issues.apache.org/jira/browse/SPARK-13101>, SPARK-12932
<https://issues.apache.org/jira/browse/SPARK-12932>

The full list of bug fixes is here: http://s.apache.org/spark-1.6.1
http://spark.apache.org/releases/spark-release-1-6-1.html

(note: it can take a few hours for everything to be propagated, so you
might get 404 on some download links, but everything should be in maven
central already.  If you see any issues with the release notes or webpage
*please contact me directly, off-list*)
"
Matt Cheah <mcheah@palantir.com>,"Thu, 10 Mar 2016 23:26:28 +0000",Understanding fault tolerance in shuffle operations,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

I have a question about the shuffle mechanisms in Spark and the fault-tolerance I should expect. Suppose I have a simple job with two stages  â€“ something like rdd.textFile().mapToPair().reduceByKey().saveAsTextFile().

The questions I have are,

  1.  Suppose Iâ€™m not using the external shuffle service. Iâ€™m running the job. The first stage succeeds. During the second stage, one of the executors is lost (for the simplest case, someone uses kill â€“9 on it and the job itself should have no problems completing otherwise). Should I expect the job to be able to recover and complete successfully? My understanding is that the lost shuffle files from that executor can still be re-computed and the job should be able to complete successfully.
  2.  Suppose Iâ€™m using the shuffle service. How does this change the result of question #1?
  3.  Suppose Iâ€™m using the shuffle service, and Iâ€™m using standalone mode. The first stage succeeds. During the second stage, I kill both the executor and the worker that spawned that executor. Now that the shuffle files associated with that workerâ€™s shuffle service daemon have been lost, will the job be able to recompute the lost shuffle data? This is the scenario Iâ€™m running into most, where my tasks fail because they try to reach the shuffle service instead of trying to recompute the lost shuffle files.

Thanks,

-Matt Cheah
"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Fri, 11 Mar 2016 09:22:35 +0530",Running ALS on comparitively large RDD,"dev@spark.apache.org, user <user@spark.apache.org>","Hello All,

I've been running Spark's ALS on a dataset of users and rated items. I
first encode my users to integers by using an auto increment function (
just like zipWithIndex), I do the same for my items. I then create an RDD
of the ratings and feed it to ALS.

My issue is that the ALS algorithm never completes. Attached is a
screenshot of the stages window.

Any help will be greatly appreciated

-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com

---------------------------------------------------------------------"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 11 Mar 2016 07:16:24 +0000",Re: Running ALS on comparitively large RDD,"dev@spark.apache.org, user <user@spark.apache.org>","Could you provide more details about:
1. Data set size (# ratings, # users and # products)
2. Spark cluster set up and version

Thanks


"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Fri, 11 Mar 2016 12:51:04 +0530",Re: Running ALS on comparitively large RDD,Nick Pentreath <nick.pentreath@gmail.com>,"1. I'm using about 1 million users against few thousand products. I
basically have around a million ratings
2. Spark 1.6 on Amazon EMR




-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com
"
Jan Kotek <discus@kotek.net>,"Fri, 11 Mar 2016 09:25:36 +0200","Contributing to managed memory, Tungsten..",dev@spark.apache.org,"Hi,

I would like to help with optimizing Spark memory usage. I have some experience 
with offheap, managed memory etc. For example I modified Hazelcast to run with '-
Xmx128M' [1] and XAP from Gigaspaces uses my memory store. 

I already studied Spark code, read blogs, videos etc... But I have questions,  about 
current situation, future direction and what would be the best way to contribute. 
Perhaps if some developer would spare a 30 minutes for a chat, and assign me some 
Issues for start.

[1]
https://github.com/jankotek/mapdb-hz-offheap[1] 

Thanks,
Jan Kotek
MapDB author

--------
[1] https://github.com/jankotek/mapdb-hz-offheap
"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 11 Mar 2016 08:43:24 +0000",Re: Running ALS on comparitively large RDD,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hmmm, something else is going on there. What data source are you reading
from? How much driver and executor memory have you provided to Spark?




"
Deepak Gopalakrishnan <dgkris@gmail.com>,"Fri, 11 Mar 2016 15:29:46 +0530",Re: Running ALS on comparitively large RDD,Nick Pentreath <nick.pentreath@gmail.com>,"Executor memory : 45g X 4 executors , 1 Driver with 45g memory
Data Source is from S3 and I've logs that tells me the Rating objects are
loaded fine.




-- 
Regards,
*Deepak Gopalakrishnan*
*Mobile*:+918891509774
*Skype* : deepakgk87
http://myexps.blogspot.com
"
Amit Chavan <achavan1@gmail.com>,"Fri, 11 Mar 2016 09:22:27 -0500","Re: Contributing to managed memory, Tungsten..",Jan Kotek <discus@kotek.net>,"Hi Jan,

Welcome to the group. I have used mapdb on some personal project and really
enjoyed working with it. I am also willing to contribute to the spark
community and would like to pair with you on this. I can help out with any
tasks that you can give me.

Thanks,
Amit


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 11 Mar 2016 16:31:50 -0800",Re: pull request template,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey all,

Just wanted to ask: how do people like this new template?

While I think it's great to have instructions for people to write
proper commit messages, I think the current template has a few
downsides.

- I tend to write verbose commit messages already when I'm preparing a
PR. Now when I open the PR I have to edit the summary field to remove
all the boilerplate.
- The template ends up in the commit messages, and sometimes people
forget to remove even the instructions.

Instead, what about changing the template a bit so that it just has
instructions prepended with some character, and have those lines
removed by the merge_spark_pr.py script? We could then even throw in a
link to the wiki as Sean suggested since it won't end up in the final
commit messages.


e
it
n
LATE
on



-- 
Marcelo

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 12 Mar 2016 10:30:27 +0100",Re: pull request template,Marcelo Vanzin <vanzin@cloudera.com>,"The template is a great thing as it gets instructions even more right
in front of people.

Another idea is to just write a checklist of items, like ""did you
describe your changes? did you test? etc."" with instructions to delete
the text and replace with a description. This keeps the boilerplate
titles out of the commit message.

The special character and post processing just takes that a step further.

:
:
le
 it
on
PLATE
ion

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 12 Mar 2016 10:53:48 +0000",Re: Spark ML - Scaling logistic regression for many features,"Daniel Siegmann <daniel.siegmann@teamaol.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Also adding dev list in case anyone else has ideas / views.


"
Renyi Xiong <renyixiong0@gmail.com>,"Mon, 14 Mar 2016 16:16:30 -0700",Re: DynamicPartitionKafkaRDD - 1:n mapping between kafka and RDD partition,Cody Koeninger <cody@koeninger.org>,"right.

However, I think it's developer's choice to purposely drop the guarantee
like when they use the existing DStream.repartition where original
per-topicpartition in-order processing is also not observed any more.

Do you agree?


"
Suniti Singh <suniti.singh@gmail.com>,"Mon, 14 Mar 2016 20:46:38 -0700","Compare a column in two different tables/find the distance between
 column data","user@spark.apache.org, dev@spark.apache.org","Hi All,

I have two tables with same schema but different data. I have to join the
tables based on one column and then do a group by the same column name.

now the data in that column in two table might/might not exactly match. (Ex
- column name is ""title"". Table1. title = ""doctor""   and Table2. title =
""doc"") doctor and doc are actually same titles.

sure if i can achieve this using the sql statement. What would be the best
approach of solving this problem. Should i look for MLLIB apis?

Spark Gurus any pointers?

Thanks,
Suniti
"
Wail Alkowaileet <wael.y.k@gmail.com>,"Tue, 15 Mar 2016 08:39:37 +0300","Re: Compare a column in two different tables/find the distance
 between column data",Suniti Singh <suniti.singh@gmail.com>,"I think you need some sort of fuzzy join ?
Is it always the case that one title is a substring of another ?




-- 

*Regards,*
Wail Alkowaileet
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 15 Mar 2016 12:44:34 +0530","Re: Compare a column in two different tables/find the distance
 between column data",Suniti Singh <suniti.singh@gmail.com>,"You can achieve this with the normal RDD way. Have one extra stage in the
pipeline where you will properly standardize all the values (like replacing
doc with doctor) for all the columns before the join.

Thanks
Best Regards


"
Koert Kuipers <koert@tresata.com>,"Tue, 15 Mar 2016 10:37:31 -0400",SparkConf constructor now private,"""dev@spark.apache.org"" <dev@spark.apache.org>","in this commit

8301fadd8d269da11e72870b7a889596e3337839
Author: Marcelo Vanzin <vanzin@cloudera.com>
Date:   Mon Mar 14 14:27:33 2016 -0700
[SPARK-13626][CORE] Avoid duplicate config deprecation warnings.

the following change was made

-class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging {
+class SparkConf private[spark] (loadDefaults: Boolean) extends Cloneable
with Logging {

i use the constructor new SparkConf(false) to build a SparkConf for our
in-house unit tests (where i do not want system properties to change meddle
with things).

is this API change on purpose?
"
Koert Kuipers <koert@tresata.com>,"Tue, 15 Mar 2016 10:49:45 -0400",spark 2.0 logging binary incompatibility,"""dev@spark.apache.org"" <dev@spark.apache.org>","i have been using spark 2.0 snapshots with some libraries build for spark
1.0 so far (simply because it worked). in last few days i noticed this new
error:

[error] Uncaught exception when running
com.tresata.spark.sql.fieldsapi.FieldsApiSpec: java.lang.AbstractMethodError
sbt.ForkMain$ForkError: java.lang.AbstractMethodError: null
    at org.apache.spark.Logging$class.log(Logging.scala:46)
    at
com.tresata.spark.sorted.PairRDDFunctions.log(PairRDDFunctions.scala:13)

so it seems spark made binary incompatible changes in logging.
i do not think spark 2.0 is trying to have binary compatibility with 1.0 so
i assume this is a non-issue, but just in case the assumptions are
different (or incompatibilities are actively minimized) i wanted to point
it out.
"
Cody Koeninger <cody@koeninger.org>,"Tue, 15 Mar 2016 10:35:41 -0500",Re: DynamicPartitionKafkaRDD - 1:n mapping between kafka and RDD partition,Renyi Xiong <renyixiong0@gmail.com>,"No, I don't agree that someone explicitly calling repartition or
shuffle is the same as a constructor that implicitly breaks
guarantees.

Realistically speaking, the changes you have made are also totally
incompatible with the way kafka's new consumer works. Pulling
different out-of-order chunks of the same topicpartition from
different consumer nodes is going to make prefetch optimizations
useless.




---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 15 Mar 2016 11:53:11 -0400","Release Announcement: XGBoost4J - Portable Distributed XGBoost
 in Spark, Flink and Dataflow","""=?utf-8?Q?user=40spark.apache.org?="" <user@spark.apache.org>, 
 dev@spark.apache.org","Dear Spark Users and Developers, 

We (Distributed (Deep) Machine Learning Community (http://dmlc.ml/)) are happy to announce the release of XGBoost4J (http://dmlc.ml/2016/03/14/xgboost4j-portable-distributed-xgboost-in-spark-flink-and-dataflow.html), a Portable Distributed XGBoost in Spark, Flink and Dataflow 

XGBoost is an optimized distributed gradient boosting library designed to be highly efficient, flexible and portable.XGBoost provides a parallel tree boosting (also known as GBDT, GBM) that solve many data science problems in a fast and accurate way. It has been the winning solution for many machine learning scenarios, ranging from Machine Learning Challenges (https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions) to Industrial User Cases (https://github.com/dmlc/xgboost/tree/master/demo#usecases) 

XGBoost4J is a new package in XGBoost aiming to provide the clean Scala/Java APIs and the seamless integration with the mainstream data processing platform, like Apache Spark. With XGBoost4J, users can run XGBoost as a stage of Spark job and build a unified pipeline from ETL to Model training to data product service within Spark, instead of jumping across two different systems, i.e. XGBoost and Spark. (Example: https://github.com/dmlc/xgboost/blob/master/jvm-packages/xgboost4j-example/src/main/scala/ml/dmlc/xgboost4j/scala/example/spark/DistTrainWithSpark.scala)

Today, we release the first version of XGBoost4J to bring more choices to the Spark users who are seeking the solutions to build highly efficient data analytic platform and enrich the Spark ecosystem. We will keep moving forward to integrate with more features of Spark. Of course, you are more than welcome to join us and contribute to the project!

For more details of distributed XGBoost, you can refer to the recently published paper: http://arxiv.org/abs/1603.02754

Best, 

-- 
Nan Zhu
http://codingcat.me

"
Koert Kuipers <koert@tresata.com>,"Tue, 15 Mar 2016 12:01:44 -0400",question about catalyst and TreeNode,"""dev@spark.apache.org"" <dev@spark.apache.org>","i am trying to understand some parts of the catalyst optimizer. but i
struggle with one bigger picture issue:

LogicalPlan extends TreeNode, which makes sense since the optimizations
rely on tree transformations like transformUp and transformDown.

but how can a LogicalPlan be a tree? isnt it really a DAG? if it is
possible to create diamond-like operator dependencies, then assumptions
made in tree transformations could be wrong? for example pushing a limit
operator down into a child sounds safe, but if that same child is also used
by another operator (so it has another parent, no longer a tree) then its
not safe at all.

what am i missing here?
thanks! koert
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 15 Mar 2016 09:49:42 -0700",Re: SparkConf constructor now private,Koert Kuipers <koert@tresata.com>,"Oh, my bad. I think I left that from a previous part of the patch and
forgot to revert it. Will fix.




-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 15 Mar 2016 09:55:33 -0700",Re: spark 2.0 logging binary incompatibility,Koert Kuipers <koert@tresata.com>,"Logging is a ""private[spark]"" class so binary compatibility is not
important at all, because code outside of Spark isn't supposed to use
it. Mixing Spark library versions is also not recommended, not just
because of this reason.

There have been other binary changes in the Logging class in the past too.




-- 
Marcelo

---------------------------------------------------------------------


"
Jan Kotek <discus@kotek.net>,"Tue, 15 Mar 2016 18:59:24 +0200","Re: Contributing to managed memory, Tungsten..",dev@spark.apache.org,"Hi Amit,

I am slowly getting into it, so I will contact in a few weeks. 

Jan



Hi Jan,


Welcome to the group. I have used mapdb on some personal project and really 
enjoyed working with it. I am also willing to contribute to the spark community and 
would like to pair with you on this. I can help out with any tasks that you can give 
me. 


Thanks,
Amit




Hi,
 
I would like to help with optimizing Spark memory usage. I have some experience 
with offheap, managed memory etc. For example I modified Hazelcast to run with '-
Xmx128M' [1] and XAP from Gigaspaces uses my memory store. 
 
I already studied Spark code, read blogs, videos etc... But I have questions, about 
current situation, future direction and what would be the best way to contribute. 
Perhaps if some developer would spare a 30 minutes for a chat, and assign me some 
Issues for start.
 
[1]
https://github.com/jankotek/mapdb-hz-offheap[2] 
 
Thanks,
Jan Kotek
MapDB author





--------
[1] mailto:discus@kotek.net
[2] https://github.com/jankotek/mapdb-hz-offheap
"
Pete Robbins <robbinspg@gmail.com>,"Tue, 15 Mar 2016 17:00:25 +0000",Re: SparkConf constructor now private,"Marcelo Vanzin <vanzin@cloudera.com>, Koert Kuipers <koert@tresata.com>","Is the SparkConf effectively a singleton? Could there be a Utils method to
return a clone of the SparkConf?

Cheers


"
Sean Owen <sowen@cloudera.com>,"Tue, 15 Mar 2016 17:24:40 +0000",Re: Various forks,"=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>, 
	Jacek Laskowski <jacek@japila.pl>","Picking up this old thread, since we have the same problem updating to
Scala 2.11.8

https://github.com/apache/spark/pull/11681#issuecomment-196932777

We can see the org.spark-project packages here:

http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22org.spark-project%22

I've forgotten who maintains the custom fork builds, and I don't know
the reasons we needed a fork of genjavadoc. Is it still relevant?

Heh, there's no plugin for 2.11.8 from the upstream project either anyway:
http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.typesafe.genjavadoc%22

This may be blocked for now

s a
g
und

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 15 Mar 2016 10:32:30 -0700",Re: question about catalyst and TreeNode,Koert Kuipers <koert@tresata.com>,"Trees are immutable, and TreeNode takes care of copying unchanged parts of
the tree when you are doing transformations.  As a result, even if you do
construct a DAG with the Dataset API, the first transformation will turn it
back into a tree.

The only exception to this rule is when we share the results of plans after
an Exchange operator.  This is the last step before execution and sometimes
turns the query into a DAG to avoid redundant computation.


"
Suniti Singh <suniti.singh@gmail.com>,"Tue, 15 Mar 2016 10:50:09 -0700","Re: Compare a column in two different tables/find the distance
 between column data",Wail Alkowaileet <wael.y.k@gmail.com>,"Is it always the case that one title is a substring of another ? -- Not
doc_{dep,areacode}


"
Reynold Xin <rxin@databricks.com>,"Tue, 15 Mar 2016 10:50:04 -0700",Re: Various forks,"Sean Owen <sowen@cloudera.com>, Xiangrui Meng <meng@databricks.com>","+Xiangrui


:
%22
g
k
 I
"
Suniti Singh <suniti.singh@gmail.com>,"Tue, 15 Mar 2016 11:34:46 -0700","Re: Compare a column in two different tables/find the distance
 between column data",,"The data in the title is different, so to correct the data in the column
requires to find out what is the correct data  and then replace.

To find the correct data could be tedious but if some mechanism is in place
which can help to group the partially matched data then it might help to do
the further processing.

I am kind of stuck.




"
Koert Kuipers <koert@tresata.com>,"Tue, 15 Mar 2016 15:29:45 -0400",Re: spark 2.0 logging binary incompatibility,Marcelo Vanzin <vanzin@cloudera.com>,"makes sense

note that Logging was not private[spark] in 1.x, which is why i used it.


"
Koert Kuipers <koert@tresata.com>,"Tue, 15 Mar 2016 15:32:47 -0400",Re: spark 2.0 logging binary incompatibility,Marcelo Vanzin <vanzin@cloudera.com>,"oh i just noticed the big warning in spark 1.x Logging


 * NOTE: DO NOT USE this class outside of Spark. It is intended as an
internal utility.
 *       This will likely be changed or removed in future releases.


"
Reynold Xin <rxin@databricks.com>,"Tue, 15 Mar 2016 12:37:00 -0700",Re: spark 2.0 logging binary incompatibility,Koert Kuipers <koert@tresata.com>,"Yea we are going to tighten a lot of class' visibility. A lot of APIs were
made experimental, developer, or public for no good reason in the past.
Many of them (not Logging in this case) are tied to the internal
implementation of Spark at a specific time, and no longer make sense given
the project's evolution.



"
Joseph Bradley <joseph@databricks.com>,"Tue, 15 Mar 2016 15:59:16 -0700",Re: pull request template,Sean Owen <sowen@cloudera.com>,"+1 for keeping the template

I figure any template will require conscientiousness & enforcement.


't
ts
o
o
ne
e
l
d
ll
"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 15 Mar 2016 16:00:57 -0700",Re: pull request template,Joseph Bradley <joseph@databricks.com>,"Nobody has suggested removing the template.

te:
.
g
f
:
,
oo
I
fo
he
TEMPLATE
ll
ed



-- 
Marcelo

---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Mar 2016 07:38:12 +0000",Accessing SparkConf in metrics sink,Dev <dev@spark.apache.org>,"I'm writing a metrics sink and reporter to push metrics to Elasticsearch.
An example format of a metric in JSON:

{
 ""timestamp"": ""2016-03-15T16:11:19.314+0000"",
 ""hostName"": ""10.192.0.87""
 ""applicationName"": ""My application"",
 ""applicationId"": ""app-20160315093931-0003"",
 ""executorId"": ""17"",
 ""executor_threadpool_completeTasks"": 20
}

For correlating the metrics I want the timestamp, hostname, applicationId,
executorId and applicationName.

Currently I am extracting the applicationId and executor Id from the metric
name as MetricsSystem prepends these to the name. As the sink is
instantiated without the SparkConf I can not determine the applicationName.

Another proposed change in https://issues.apache.org/jira/browse/SPARK-10610
would also make me require access to the SparkConf to get the
applicationId/executorId.

So, Is the SparkConf a singleton and can there be a Utils method for
accessing it? Instantiating a SparkConf myself will not pick up the appName
etc as these are set via methods on the conf.

I'm trying to write this without modifying any Spark code by just using a
definition in the metrics properties to load my sink.

Cheers,
"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Mar 2016 00:57:41 -0700",Re: Accessing SparkConf in metrics sink,Pete Robbins <robbinspg@gmail.com>,"SparkConf is not a singleton.

However, SparkContext in almost all cases are. So you can use
SparkContext.getOrCreate().getConf


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Mar 2016 08:22:31 +0000",Re: Accessing SparkConf in metrics sink,Reynold Xin <rxin@databricks.com>,"OK thanks. Does that work in an executor?


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 16 Mar 2016 09:37:21 +0000",Re: Accessing SparkConf in metrics sink,Reynold Xin <rxin@databricks.com>,"So the answer to my previous question is NO.

It looks like I could use SparkEnv.get.conf but

* * NOTE: This is not intended for external use. This is exposed for Shark
and may be made private * in a future release. */




"
Craig Lukasik <clukasik@zaloni.com>,"Wed, 16 Mar 2016 12:18:10 -0400",[POWERED BY] Please add our organization,dev@spark.apache.org,"Name:
â€‹ â€‹
Zaloni's Bedrock & Mica

URL: http://www.zaloni.com/products/

Description:
â€‹ â€‹
Zaloni's data
â€‹lake â€‹
management platform (Bedrock) and self-service data preparation solution
â€‹ â€‹
(Mica) leverage Spark for
â€‹fast execution of transformations and data exploration.
"
Xiangrui Meng <meng@databricks.com>,"Wed, 16 Mar 2016 17:22:42 +0000",Re: Various forks,"Reynold Xin <rxin@databricks.com>, Sean Owen <sowen@cloudera.com>","We made that fork to hide package private classes/members in the generated
Java API doc. Otherwise, the Java API doc is very messy. The patch is to
map all private[*] to the default scope in the generated Java code.
However, this might not be the expected behavior for other packages. So it
didn't get merged into the official genjavadoc repo. The proposal is to
have a flag in genjavadoc settings to enable this mapping, but it was
delayed. This is the JIRA for this issue:
https://issues.apache.org/jira/browse/SPARK-7992. -Xiangrui


y:
c%22
ng
rk
c
.
"
Dan Burkert <dan@cloudera.com>,"Wed, 16 Mar 2016 11:19:06 -0700",graceful shutdown in external data sources,dev@spark.apache.org,"Hi all,

I'm working on the Spark connector for Apache Kudu, and I've run into an
issue that is a bit beyond my Spark knowledge. The Kudu connector
internally holds an open connection to the Kudu cluster
<https://github.com/apache/incubator-kudu/blob/master/java/kudu-spark/src/main/scala/org/kududb/spark/KuduContext.scala#L37>
which
internally holds a Netty context with non-daemon threads. When using the
Spark shell with the Kudu connector, exiting the shell via <ctrl>-D causes
the shell to hang, and a thread dump reveals it's waiting for these
non-daemon threads.  Registering a JVM shutdown hook to close the Kudu
client does not do the trick, as it seems that the shutdown hooks are not
fired on <ctrl>-D.

I see that there is an internal Spark API for handling shutdown
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala>,
is there something similar available for cleaning up external data sources?

- Dan
"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Mar 2016 14:40:28 -0700",Re: graceful shutdown in external data sources,Dan Burkert <dan@cloudera.com>,"Maybe just add a watch dog thread and closed the connection upon some
timeout?


"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Mar 2016 14:52:27 -0700",[discuss] making SparkEnv private in Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>, 
	Xuefu Zhang <xzhang@cloudera.com>","Any objections? Please articulate your use case. SparkEnv is a weird one
because it was documented as ""private"" but not marked as so in class
visibility.

 * NOTE: This is not intended for external use. This is exposed for Shark
and may be made private
 *       in a future release.


I do see Hive using it to get the config variable. That can probably be
propagated through other means.
"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 16 Mar 2016 15:29:05 -0700",Re: [discuss] making SparkEnv private in Spark 2.0,Reynold Xin <rxin@databricks.com>,"We use it in executors to get to :
a) spark conf (for getting to hadoop config in map doing custom
writing of side-files)
b) Shuffle manager (to get shuffle reader)

Not sure if there are alternative ways to get to these.

Regards,
Mridul


---------------------------------------------------------------------


"
Dan Burkert <dan@cloudera.com>,"Wed, 16 Mar 2016 15:29:46 -0700",Re: graceful shutdown in external data sources,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Is there any way to know when an executor will no longer have any tasks?
It seems to me there is no timeout which is appropriate that is long enough
to ensure that no more tasks will be scheduled on the executor, and short
enough to be appropriate to wait on during an interactive shell shutdown.

- Dan


"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 16 Mar 2016 22:35:26 +0000",Re: graceful shutdown in external data sources,"Dan Burkert <dan@cloudera.com>, Reynold Xin <rxin@databricks.com>","Dan,

You could probably just register a JVM shutdown hook yourself:
https://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.html#addShutdownHook(java.lang.Thread
)

This at least would let you close the connections when the application as a
whole has completed (in standalone) or when your executors have been killed
(in YARN). I think that's as close as you'll get to knowing when an
executor will no longer have any tasks in the current state of the world.


"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Mar 2016 15:36:01 -0700",Re: graceful shutdown in external data sources,Dan Burkert <dan@cloudera.com>,"There is no way to really know that, because users might run queries at any
given point.

BTW why can't your threads be just daemon threads?




"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Mar 2016 15:36:50 -0700",Re: [discuss] making SparkEnv private in Spark 2.0,Mridul Muralidharan <mridul@gmail.com>,"

What's the use case for shuffle manager/reader? This seems like using super
internal APIs in applications.
"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 16 Mar 2016 15:40:22 -0700",Re: [discuss] making SparkEnv private in Spark 2.0,Reynold Xin <rxin@databricks.com>,"We have custom join's that leverage it.
It is used to get to direct shuffle'ed iterator - without needing
sort/aggregate/etc.

IIRC the only way to get to it from ShuffleHandle is via shuffle manager.


Regards,
Mridul


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 16 Mar 2016 15:44:43 -0700",Re: df.dtypes -> pyspark.sql.types,Ruslan Dautkhanov <dautkhanov@gmail.com>,"We probably should have the alias. Is this still a problem on master
branch?


"
Dan Burkert <dan@cloudera.com>,"Wed, 16 Mar 2016 16:04:53 -0700",Re: graceful shutdown in external data sources,Reynold Xin <rxin@databricks.com>,"Thanks for the replies, responses inline:



The bigger issue is that we require the Kudu client to be manually closed
so that it can do necessary cleanup tasks.  During shutdown the client
closes the non-daemon threads, but more importantly, it flushes any
outstanding batched writes to the server.



The Spark shell will not run shutdown hooks after a <ctrl>-D if there are
non-daemon threads running.  You can test this with the following input to
the shell:

new Thread(new Runnable { override def run() = { while (true) {
println(""running""); Thread.sleep(10000) } } }).start()
Runtime.getRuntime.addShutdownHook(new Thread(new Runnable { override def
run() = println(""shutdown fired"") }))

- Dan



"
Dan Burkert <dan@cloudera.com>,"Wed, 16 Mar 2016 16:43:32 -0700",Re: graceful shutdown in external data sources,Reynold Xin <rxin@databricks.com>,"After further thought, I think following both of your suggestions- adding a
shutdown hook and making the threads non-daemon- may have the result I'm
looking for.  I'll check and see if there are other reasons not to use
daemon threads in our networking internals.  More generally though, what do
y'all think about having Spark shutdown or close RelationProviders once
they are not needed?  Seems to me that RelationProviders will often be
stateful objects with network and/or file resources.  I checked with the C*
Spark connector, and they jump through a bunch of hoops to handle this
issue, including shutdown hooks and a ref counted cache.

- Dan


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Mar 2016 00:15:10 +0000",Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Spark dev list <dev@spark.apache.org>,"https://s3.amazonaws.com/spark-related-packages/spark-1.6.1-bin-hadoop2.6.tgz

Does anyone else have trouble unzipping this? How did this happen?

What I get is:

$ gzip -t spark-1.6.1-bin-hadoop2.6.tgz
gzip: spark-1.6.1-bin-hadoop2.6.tgz: unexpected end of file
gzip: spark-1.6.1-bin-hadoop2.6.tgz: uncompress failed

Seems like a strange type of problem to come across.

Nick
â€‹
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 16 Mar 2016 17:28:08 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"
$ tar zxf spark-1.6.1-bin-hadoop2.6.tgz

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now


.tgz
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Mar 2016 00:47:55 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Ted Yu <yuzhihong@gmail.com>,"Looks like the other packages may also be corrupt. Iâ€™m getting the same
error for the Spark 1.6.1 / Hadoop 2.4 package.

https://s3.amazonaws.com/spark-related-packages/spark-1.6.1-bin-hadoop2.4.tgz

Nick
â€‹


6.tgz
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 16 Mar 2016 18:38:57 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Same with hadoop 2.3 tar ball:

$ tar zxf spark-1.6.1-bin-hadoop2.3.tgz

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now


e same
.tgz
.6.tgz
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Wed, 16 Mar 2016 19:05:42 -0700","=?UTF-8?Q?Fwd=3A_Apache_Spark_Exception_in_thread_=E2=80=9Cmain=E2=80=9D_jav?=
	=?UTF-8?Q?a=2Elang=2ENoClassDefFoundError=3A_scala=2Fcollection=2FGenTraversable?=","user@spark.apache.org, dev@spark.apache.org","Hi,

Scala version:2.11.7(had to upgrade the scala verison to enable case
clasess to accept more than 22 parameters.)

Spark version:1.6.1.

PFB pom.xml

Getting below error when trying to setup spark on intellij IDE,

16/03/16 18:36:44 INFO spark.SparkContext: Running Spark version 1.6.1
Exception in thread ""main"" java.lang.NoClassDefFoundError:
org.apache.spark.util.TimeStampedWeakValueHashMap.(TimeStampedWeakValueHashMap.scala:42)
at org.apache.spark.SparkContext.(SparkContext.scala:298) at
com.examples.testSparkPost$.main(testSparkPost.scala:27) at
com.examples.testSparkPost.main(testSparkPost.scala) at
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606) at
com.intellij.rt.execution.application.AppMain.main(AppMain.java:140) Caused
by: java.lang.ClassNotFoundException:
java.net.URLClassLoader$1.run(URLClassLoader.java:366) at
java.net.URLClassLoader$1.run(URLClassLoader.java:355) at
java.security.AccessController.doPrivileged(Native Method) at
java.net.URLClassLoader.findClass(URLClassLoader.java:354) at
java.lang.ClassLoader.loadClass(ClassLoader.java:425) at
sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308) at
java.lang.ClassLoader.loadClass(ClassLoader.java:358) ... 9 more

pom.xml:

<project xmlns=""http://maven.apache.org/POM/4.0.0""
xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
         xsi:schemaLocation=""http://maven.apache.org/POM/4.0.0
http://maven.apache.org/maven-v4_0_0.xsd"">
    <modelVersion>4.0.0</modelVersion>
    <groupId>StreamProcess</groupId>
    <artifactId>StreamProcess</artifactId>
    <version>0.0.1-SNAPSHOT</version>
    <name>${project.artifactId}</name>
    <description>This is a boilerplate maven project to start using
Spark in Scala</description>
    <inceptionYear>2010</inceptionYear>

    <properties>
        <maven.compiler.source>1.6</maven.compiler.source>
        <maven.compiler.target>1.6</maven.compiler.target>
        <encoding>UTF-8</encoding>
        <scala.tools.version>2.10</scala.tools.version>
        <!-- Put the Scala version of the cluster -->
        <scala.version>2.11.7</scala.version>
    </properties>

    <!-- repository to add org.apache.spark -->
    <repositories>
        <repository>
            <id>cloudera-repo-releases</id>
            <url>https://repository.cloudera.com/artifactory/repo/</url>
        </repository>
    </repositories>

    <build>
        <sourceDirectory>src/main/scala</sourceDirectory>
        <testSourceDirectory>src/test/scala</testSourceDirectory>
        <plugins>
                <!-- any other plugins -->
                <plugin>
                    <artifactId>maven-assembly-plugin</artifactId>
                    <executions>
                        <execution>
                            <phase>package</phase>
                            <goals>
                                <goal>single</goal>
                            </goals>
                        </execution>
                    </executions>
                    <configuration>
                        <descriptorRefs>
                            <descriptorRef>jar-with-dependencies</descriptorRef>
                        </descriptorRefs>
                    </configuration>
                </plugin>
            <plugin>
                <!-- see http://davidb.github.com/scala-maven-plugin -->
                <groupId>net.alchim31.maven</groupId>
                <artifactId>scala-maven-plugin</artifactId>
                <version>3.2.2</version>
                <executions>
                    <execution>
                        <goals>
                            <goal>compile</goal>
                            <goal>testCompile</goal>
                        </goals>
                        <configuration>
                            <args>
                                <!--<arg>-make:transitive</arg>-->
                                <arg>-dependencyfile</arg>

<arg>${project.build.directory}/.scala_dependencies</arg>
                            </args>
                        </configuration>
                    </execution>
                </executions>
            </plugin>

            <!-- ""package"" command plugin -->
            <plugin>
                <artifactId>maven-assembly-plugin</artifactId>
                <version>2.4.1</version>
                <configuration>
                    <descriptorRefs>
                        <descriptorRef>jar-with-dependencies</descriptorRef>
                    </descriptorRefs>
                </configuration>
                <executions>
                    <execution>
                        <id>make-assembly</id>
                        <phase>package</phase>
                        <goals>
                            <goal>single</goal>
                        </goals>
                    </execution>
                </executions>
            </plugin>
        </plugins>
    </build>
    <dependencies>
        <dependency>
            <groupId>org.scala-lang</groupId>
            <artifactId>scala-library</artifactId>
            <version>${scala.version}</version>
        </dependency>
        <dependency>
            <groupId>org.mongodb.mongo-hadoop</groupId>
            <artifactId>mongo-hadoop-core</artifactId>
            <version>1.4.2</version>
            <exclusions>
                <exclusion>
                    <groupId>javax.servlet</groupId>
                    <artifactId>servlet-api</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
                <groupId>org.mongodb</groupId>
                <artifactId>mongodb-driver</artifactId>
                <version>3.2.2</version>
            <exclusions>
                <exclusion>
                    <groupId>javax.servlet</groupId>
                    <artifactId>servlet-api</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
        <dependency>
                <groupId>org.mongodb</groupId>
                <artifactId>mongodb-driver</artifactId>
                <version>3.2.2</version>
            <exclusions>
                <exclusion>
                    <groupId>javax.servlet</groupId>
                    <artifactId>servlet-api</artifactId>
                </exclusion>
            </exclusions>
            </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-streaming_2.10</artifactId>
            <version>1.6.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-core_2.10</artifactId>
            <version>1.6.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.spark</groupId>
            <artifactId>spark-sql_2.10</artifactId>
            <version>1.6.1</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-hdfs</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-auth</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-common</artifactId>
            <version>2.6.0</version>
        </dependency>
        <dependency>
            <groupId>org.apache.hadoop</groupId>
            <artifactId>hadoop-core</artifactId>
            <version>1.2.1</version>
        </dependency>
    </dependencies>
</project>

Would like to know , what to be changed in pom to get things going.

Regards,
Satyajit.
"
Josh Rosen <joshrosen@databricks.com>,"Thu, 17 Mar 2016 02:07:10 +0000","=?UTF-8?Q?Re=3A_Apache_Spark_Exception_in_thread_=E2=80=9Cmain=E2=80=9D_java?=
	=?UTF-8?Q?=2Elang=2ENoClassDefFoundError=3A_scala=2Fcollection=2FGenTraversableO?=
	=?UTF-8?Q?nce=24class?=","satyajit vegesna <satyajit.apasprk@gmail.com>, user@spark.apache.org, dev@spark.apache.org","See the instructions in the Spark documentation:
https://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211


"
Josh Rosen <joshrosen@databricks.com>,"Thu, 17 Mar 2016 02:08:39 +0000","=?UTF-8?Q?Re=3A_Apache_Spark_Exception_in_thread_=E2=80=9Cmain=E2=80=9D_java?=
	=?UTF-8?Q?=2Elang=2ENoClassDefFoundError=3A_scala=2Fcollection=2FGenTraversableO?=
	=?UTF-8?Q?nce=24class?=","satyajit vegesna <satyajit.apasprk@gmail.com>, user@spark.apache.org, dev@spark.apache.org","Err, whoops, looks like this is a user app and not building Spark itself,
so you'll have to change your deps to use the 2.11 versions of Spark.
e.g. spark-streaming_2.10 -> spark-streaming_2.11.


"
Jeff Zhang <zjffdu@gmail.com>,"Thu, 17 Mar 2016 14:46:00 +0800",Spark build with scala-2.10 fails ?,dev <dev@spark.apache.org>,"Anyone can pass the spark build with scala-2.10 ?


[info] Compiling 475 Scala sources and 78 Java sources to
/Users/jzhang/github/spark/core/target/scala-2.10/classes...
[error]
/Users/jzhang/github/spark/core/src/main/scala/org/apache/spark/deploy/mesos/MesosExternalShuffleService.scala:30:
object ShuffleServiceHeartbeat is not a member of package
org.apache.spark.network.shuffle.protocol.mesos
[error] import
org.apache.spark.network.shuffle.protocol.mesos.{RegisterDriver,
ShuffleServiceHeartbeat}
[error]        ^
[error]
/Users/jzhang/github/spark/core/src/main/scala/org/apache/spark/deploy/mesos/MesosExternalShuffleService.scala:87:
not found: type ShuffleServiceHeartbeat
[error]     def unapply(h: ShuffleServiceHeartbeat): Option[String] =
Some(h.getAppId)
[error]                    ^
[error]
/Users/jzhang/github/spark/core/src/main/scala/org/apache/spark/deploy/mesos/MesosExternalShuffleService.scala:83:
value getHeartbeatTimeoutMs is not a member of
org.apache.spark.network.shuffle.protocol.mesos.RegisterDriver
[error]       Some((r.getAppId, new AppState(r.getHeartbeatTimeoutMs,
System.nanoTime())))
[error]                                        ^
[error]
/Users/jzhang/github/spark/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/CoarseMesosSchedulerBackend.scala:451:
too many arguments for method registerDriverWithShuffleService: (x$1:
String, x$2: Int)Unit
[error]           .registerDriverWithShuffleService(
[error]                                            ^
[error] four errors found
[error] Compile failed at Mar 17, 2016 2:45:22 PM [13.105s]
-- 
Best Regards

Jeff Zhang
"
Josh Rosen <joshrosen@databricks.com>,"Thu, 17 Mar 2016 07:09:47 +0000",Re: Spark build with scala-2.10 fails ?,"Jeff Zhang <zjffdu@gmail.com>, dev <dev@spark.apache.org>","It looks like the Scala 2.10 Jenkins build is working:
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/spark-master-compile-sbt-scala-2.10/

Can you share more details about how you're compiling with 2.10 (e.g. which
commands you ran, git SHA, etc)?


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 17 Mar 2016 12:02:33 +0000",Re: graceful shutdown in external data sources,Dan Burkert <dan@cloudera.com>,"

After further thought, I think following both of your suggestions- adding a shutdown hook and making the threads non-daemon- may have the result I'm looking for.  I'll check and see if there are other reasons not to use daemon threads in our networking internals.  More generally though, what do y'all think about having Spark shutdown or close RelationProviders once they are not needed?  Seems to me that RelationProviders will often be stateful objects with network and/or file resources.  I checked with the C* Spark connector, and they jump through a bunch of hoops to handle this issue, including shutdown hooks and a ref counted cache.

I'd recommend using org.apache.spark.util.ShutdownHookManager as the shutdown hook mechanism; it gives you priority over shutdown , and is already used in the Yarn AM, DiskBlockManager and elsewhere


d time period even if you can't connect to the far end: do make sure there are timeouts on TCP connects &c. i've hit problems with Hadoop HDFS where, if the endpoint isn't configured correctly, the shutdown hook blocks, causing Control-C/kill <pid> interrupts to appear to hang, and of course a second kill just deadlocks on the original sync. (To deal with that, I ended up recognising a 2nd Ctrl-C interrupt as a trigger for calling System.halt(), which bails out the JVM without trying to invoke those hooks


- Dan

Thanks for the replies, responses inline:

There is no way to really know that, because users might run queries at any given point.

BTW why can't your threads be just daemon threads?

The bigger issue is that we require the Kudu client to be manually closed so that it can do necessary cleanup tasks.  During shutdown the client closes the non-daemon threads, but more importantly, it flushes any outstanding batched writes to the server.

Dan,

You could probably just register a JVM shutdown hook yourself: https://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.html#addShutdownHook(java.lang.Thread)

This at least would let you close the connections when the application as a whole has completed (in standalone) or when your executors have been killed (in YARN). I think that's as close as you'll get to knowing when an executor will no longer have any tasks in the current state of the world.

The Spark shell will not run shutdown hooks after a <ctrl>-D if there are non-daemon threads running.  You can test this with the following input to the shell:

new Thread(new Runnable { override def run() = { while (true) { println(""running""); Thread.sleep(10000) } } }).start()
Runtime.getRuntime.addShutdownHook(new Thread(new Runnable { override def run() = println(""shutdown fired"") }))

- Dan



Hi Reynold,

Is there any way to know when an executor will no longer have any tasks?  It seems to me there is no timeout which is appropriate that is long enough to ensure that no more tasks will be scheduled on the executor, and short enough to be appropriate to wait on during an interactive shell shutdown.

- Dan

Maybe just add a watch dog thread and closed the connection upon some timeout?


Hi all,

I'm working on the Spark connector for Apache Kudu, and I've run into an issue that is a bit beyond my Spark knowledge. The Kudu connector internally holds an open connection to the Kudu cluster<https://github.com/apache/incubator-kudu/blob/master/java/kudu-spark/src/main/scala/org/kududb/spark/KuduContext.scala#L37> which internally holds a Netty context with non-daemon threads. When using the Spark shell with the Kudu connector, exiting the shell via <ctrl>-D causes the shell to hang, and a thread dump reveals it's waiting for these non-daemon threads.  Registering a JVM shutdown hook to close the Kudu client does not do the trick, as it seems that the shutdown hooks are not fired on <ctrl>-D.

I see that there is an internal Spark API for handling shutdown<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ShutdownHookManager.scala>, is there something similar available for cleaning up external data sources?

- Dan





"
Michael Armbrust <michael@databricks.com>,"Thu, 17 Mar 2016 08:57:37 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Patrick reuploaded the artifacts, so it should be fixed now.

e same
.tgz
.6.tgz
"
Bryan Cutler <cutlerb@gmail.com>,"Thu, 17 Mar 2016 09:01:26 -0700",Re: pull request template,dev <dev@spark.apache.org>,"+1 on Marcelo's comments.  It would be nice not to pollute commit messages
with the  instructions because some people might forget to remove them.
Nobody has suggested removing the template.

.
g
f
:
,
too
I
info
the
https://github.com/apache/spark/blob/m"
Yin Yang <yy201602@gmail.com>,"Thu, 17 Mar 2016 09:32:37 -0700",Re: Spark build with scala-2.10 fails ?,Jeff Zhang <zjffdu@gmail.com>,"The build was broken as of this morning.

Created PR:
https://github.com/apache/spark/pull/11787


"
Dan Burkert <dan@cloudera.com>,"Thu, 17 Mar 2016 10:46:19 -0700",Re: graceful shutdown in external data sources,Steve Loughran <stevel@hortonworks.com>,"Hi Steve,

I referenced the ShutdownHookManager in my original message, but it appears
to be an internal-only API.  Looks like it uses a Hadoop equivalent
internally, though, so I'll look into using that.  Good tip about timeouts,
thanks.

 - Dan


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 17 Mar 2016 11:14:49 -0700",SPARK-13843 and future of streaming backends,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello all,

Recently a lot of the streaming backends were moved to a separate
project on github and removed from the main Spark repo.

While I think the idea is great, I'm a little worried about the
execution. Some concerns were already raised on the bug mentioned
above, but I'd like to have a more explicit discussion about this so
things don't fall through the cracks.

Mainly I have three concerns.

i. Ownership

That code used to be run by the ASF, but now it's hosted in a github
repo owned not by the ASF. That sounds a little sub-optimal, if not
problematic.

ii. Governance

Similar to the above; who has commit access to the above repos? Will
all the Spark committers, present and future, have commit access to
all of those repos? Are they still going to be considered part of
Spark and have release management done through the Spark community?


For both of the questions above, why are they not turned into
sub-projects of Spark and hosted on the ASF repos? I believe there is
a mechanism to do that, without the need to keep the code in the main
Spark repo, right?

iii. Usability

This is another thing I don't see discussed. For Scala-based code
things don't change much, I guess, if the artifact names don't change
(another reason to keep things in the ASF?), but what about python?
How are pyspark users expected to get that code going forward, since
it's not in Spark's pyspark.zip anymore?


Is there an easy way of keeping these things within the ASF Spark
project? I think that would be better for everybody.

-- 
Marcelo

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 17 Mar 2016 14:01:25 -0500",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"i.  An ASF project can clearly decide that some of its code is no
longer worth maintaining and delete it.  This isn't really any
different. It's still apache licensed so ultimately whoever wants the
code can get it.

ii.  I think part of the rationale is to not tie release management to
Spark, so it can proceed on a schedule that makes sense.  I'm fine
with helping out with release management for the Kafka subproject, for
instance.  I agree that practical governance questions need to be
worked out.

iii.  How is this any different from how python users get access to
any other third party Spark package?



---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 17 Mar 2016 12:25:50 -0700",Re: SPARK-13843 and future of streaming backends,Cody Koeninger <cody@koeninger.org>,"
Absolutely. But I don't remember this being discussed either way. Was
the intention, as you mention later, just to decouple the release of
those components from the main Spark release, or to completely disown
that code?

If the latter, is the ASF ok with it still retaining the current
package and artifact names? Changing those would break backwards
compatibility. Which is why I believe that keeping them as a
sub-project, even if their release cadence is much slower, would be a
better solution for both developers and users.


True, but that requires the modules to be published somewhere, not
just to live as a bunch of .py files in a gitbub repo. Basically, I'm
worried that there's work to be done to keep those modules working in
this new environment - how to build, test, and publish things, remove
potential uses of internal Spark APIs, just to cite a couple of
things.

-- 
Marcelo

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 17 Mar 2016 12:49:17 -0700",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"I was not aware of a discussion in Dev list about this - agree with most of
the observations.
In addition, I did not see PMC signoff on moving (sub-)modules out.

Regards
Mridul



"
Steve Loughran <stevel@hortonworks.com>,"Thu, 17 Mar 2016 19:49:35 +0000",Re: graceful shutdown in external data sources,Dan Burkert <dan@cloudera.com>,"

 Looks like it uses a Hadoop equivalent internally, though, so I'll look into using that.  Good tip about timeouts, thanks.


Dont think that's actually tagged as @Public, but it would upset too many people if it broke, myself included
"
Cody Koeninger <cody@koeninger.org>,"Thu, 17 Mar 2016 14:55:23 -0500",Re: SPARK-13843 and future of streaming backends,Mridul Muralidharan <mridul@gmail.com>,"Why would a PMC vote be necessary on every code deletion?

There was a Jira and pull request discussion about the submodules that
have been removed so far.

https://issues.apache.org/jira/browse/SPARK-13843

There's another ongoing one about Kafka specifically

https://issues.apache.org/jira/browse/SPARK-13877



---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 17 Mar 2016 12:58:02 -0700",Re: SPARK-13843 and future of streaming backends,Cody Koeninger <cody@koeninger.org>,"Note the non-kafka bug was filed right before the change was pushed.
So there really wasn't any discussion before the decision was made to
remove that code.

I'm just trying to merge both discussions here in the list where it's
a little bit more dynamic than bug updates that end up getting lost in
the noise.




-- 
Marcelo

---------------------------------------------------------------------


"
,"Thu, 17 Mar 2016 21:10:30 +0100",Re: SPARK-13843 and future of streaming backends,dev@spark.apache.org,"Hi Marcelo,

I quickly discussed with Reynold this morning about this.

I share your concerns.

I fully understand that it's painful for users to wait a Spark releases 
to include fix in streaming backends as it's not really related.
It makes sense to provide backends ""outside"" of ASF, especially for 
legal issues: it's what we do at Camel with Camel-Extra.

Don't you think it could be interesting to have another ASF git repo 
dedicated to streaming backends, each backend can managed its release 
cycle following the ASF ""rules"" (staging, vote, ...) ?

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
,"Thu, 17 Mar 2016 21:13:03 +0100",Re: [POWERED BY] Please add our organization,dev@spark.apache.org,"Just a detail (but important): ""leverage Apache Spark"" (not ""leverage 
Spark"").

My $0.01.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Mar 2016 13:13:09 -0700",Re: pull request template,Joseph Bradley <joseph@databricks.com>,"I think it'd make sense to have the merge script automatically remove some
parts of the template, if they were not removed by the contributor. That
seems trivial to do.



.
g
f
:
,
oo
I
he
E
ll
ed
"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 17 Mar 2016 13:13:32 -0700",Re: SPARK-13843 and future of streaming backends,Cody Koeninger <cody@koeninger.org>,"I am not referring to code edits - but to migrating submodules and
code currently in Apache Spark to 'outside' of it.
If I understand correctly, assets from Apache Spark are being moved
out of it into thirdparty external repositories - not owned by Apache.

At a minimum, dev@ discussion (like this one) should be initiated.
As PMC is responsible for the project assets (including code), signoff
is required for it IMO.

More experienced Apache members might be opine better in case I got it wrong !


Regards,
Mridul



---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 17 Mar 2016 20:23:02 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Michael Armbrust <michael@databricks.com>,"OK cool. I'll test the hadoop-2.6 package and check back here if it's still
broken.

Just curious: How did those packages all get corrupted (if we know)? Seems
like a strange thing to happen.
2016ë…„ 3ì›” 17ì¼ (ëª©) ì˜¤ì „ 11:57, Michael Armbrust <michael@databricks.com>ë‹˜ì´ ìž‘ì„±:

he same
4.tgz
2.6.tgz
"
Cody Koeninger <cody@koeninger.org>,"Thu, 17 Mar 2016 15:50:13 -0500",Re: SPARK-13843 and future of streaming backends,Mridul Muralidharan <mridul@gmail.com>,"Anyone can fork apache licensed code.  Committers can approve pull
requests that delete code from asf repos.  Because those two things
happen near each other in time, it's somehow a process violation?

I think the discussion would be better served by concentrating on how
we're going to solve the problem and move forward.


---------------------------------------------------------------------


"
Hari Shreedharan <hshreedharan@cloudera.com>,"Thu, 17 Mar 2016 13:51:58 -0700",Re: SPARK-13843 and future of streaming backends,Mridul Muralidharan <mridul@gmail.com>,"I have worked with various ASF projects for 4+ years now. Sure, ASF
projects can delete code as they feel fit. But this is the first time I
have really seen code being ""moved out"" of a project without discussion. I
am sure you can do this without violating ASF policy, but the explanation
for that would be convoluted (someone decided to make a copy and then the
ASF project deleted it?).

Also, moving the code out would break compatibility. AFAIK, there is no way
to push org.apache.* artifacts directly to maven central. That happens via
mirroring from the ASF maven repos. Even if it you could somehow directly
push the artifacts to mvn, you really can push to org.apache.* groups only
if you are part of the repo and acting as an agent of that project (which
committer/PMC member would not be representing the ASF when pushing the
code. I am not sure if there is a way to fix this issue.


Thanks,
Hari


"
Daniel Siegmann <daniel.siegmann@teamaol.com>,"Thu, 17 Mar 2016 16:55:47 -0400",Re: Spark ML - Scaling logistic regression for many features,Nick Pentreath <nick.pentreath@gmail.com>,"Hi Nick,

Thanks again for your help with this. Did you create a ticket in JIRA for
investigating sparse models in LR and / or multivariate summariser? If so,
can you give me the issue key(s)? If not, would you like me to create these
tickets?

I'm going to look into this some more and see if I can figure out how to
implement these fixes.

~Daniel Siegmann


"
Cody Koeninger <cody@koeninger.org>,"Thu, 17 Mar 2016 16:09:09 -0500",Re: SPARK-13843 and future of streaming backends,Hari Shreedharan <hshreedharan@cloudera.com>,"There's a difference between ""without discussion"" and ""without as much
discussion as I would have liked to have a chance to notice it"".
There are plenty of PRs that got merged before I noticed them that I
would rather have not gotten merged.

As far as group / artifact name compatibility, at least in the case of
Kafka we need different artifact names anyway, and people are going to
have to make changes to their build files for spark 2.0 anyway.   As
far as keeping the actual classes in org.apache.spark to not break
code despite the group name being different, I don't know whether that
would be enforced by maven central, just looked at as poor taste, or
ASF suing for trademark violation :)

For people who would rather the problem be solved with official asf
subprojects, which committers are volunteering to help do that work?
Reynold already said he doesn't want to mess with that overhead.

I'm fine with continuing to help work on the Kafka integration
wherever it ends up, I'd just like the color of the bikeshed to get
decided so we can build a decent bike...



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 17 Mar 2016 14:18:55 -0700",Re: SPARK-13843 and future of streaming backends,Cody Koeninger <cody@koeninger.org>,"Thanks for initiating this discussion. I merged the pull request because it
was unblocking another major piece of work for Spark 2.0: not requiring
assembly jars, which is arguably a lot more important than sources that are
less frequently used. I take full responsibility for that.

I think it's inaccurate to call them ""backend"" because it makes these
things sound a lot more serious, when in reality they are a bunch of
connectors to less frequently used streaming data sources (e.g. mqtt,
flume). But that's not that important here.

Another important factor is that over time, with the development of
structure streaming, we'd provide a new API for streaming sources that
unifies the way to connect arbitrary sources, and as a result all of these
sources need to be rewritten anyway. This is similar to the RDD ->
DataFrame transition for data sources, although it was initially painful,
but in the long run provides much better experience for end-users because
they only need to learn a single API for all sources, and it becomes
trivial to transition from one source to another, without actually
impacting business logic.

So the truth is that in the long run, the existing connectors will be
replaced by new ones, and they have been causing minor issues here and
there in the code base. Now issues like these are never black and white. By
moving them out, we'd require users to at least change the maven coordinate
in their build file (although things can still be made binary and source
compatible). So I made the call and asked the contributor to keep Kafka and
Kinesis in, because those are the most widely used (and could be more
contentious), and move everything else out.

I have personally done enough data sources or 3rd party packages for Spark
on github that I can setup a github repo with CI and maven publishing in
just under an hour. I do not expect a lot of changes to these packages
because the APIs have been fairly stable. So the thing I was optimizing for
was to minimize the time we need to spent on these packages given the
(expected) low activity and the shift to focus on structured streaming, and
also minimize the chance to break user apps to provide the best user
experience.

Github repo seems the simplest choice to me. I also made another decision
to provide separate repos (and thus issue trackers) on github for these
packages. The reason is that these connectors have very disjoint
communities. For example, the community that care about mqtt is likely very
different from the community that care about akka. It is much easier to
track all of these.

Logistics wise -- things are still in flux. I think it'd make a lot of
sense to give existing Spark committers (or at least the ones that have
contributed to streaming) write access to the github repos. IMHO, it is not
in any of the major Spark contributing organizations' strategic interest to
""own"" these projects, especially considering most of the activities will
switch to structured streaming.

If one really feels strongly that we should go through all the overhead to
setup an ASF subproject for these modules that won't work with the new
structured streaming, and want to spearhead to setup separate repos
(preferably one subproject per connector), CI, separate JIRA, governance,
READMEs, voting, we can discuss that. Until then, I'd keep the github
option open because IMHO it is what works the best for end users (including
discoverability, issue tracking, release publishing, ...).







"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 17 Mar 2016 14:33:04 -0700",Re: SPARK-13843 and future of streaming backends,Reynold Xin <rxin@databricks.com>,"Hi Reynold, thanks for the info.


For those of us who are not exactly familiar with the inner workings
of administrating ASF projects, would you mind explaining in more
detail what this overhead is?

it's a simple as having a separate git repo for it, tied to the same
parent project. Everything else - JIRA, committers, bylaws, etc -
remains the same. And since the project we're talking about are very
small, CI should be very simple (Travis?) and, assuming sporadic
releases, things overall should not be that expensive to maintain.

-- 
Marcelo

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Thu, 17 Mar 2016 15:03:52 -0700",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"If the intention is to actually decouple and give a life of it's own to
these connectors, I would have expected that they would still be hosted as
different git repositories inside Apache even tough users will not really
see much difference as they would still be mirrored in GitHub. This makes
it much easier on the legal departments of the upstream consumers and
customers as well because the code still follow the so well received and
trusted Apache Governance and Apache Release Policies. As for
implementation details, we can have multiple repositories if we see a lot
of fragmented releases, or a single ""connectors"" repository which in our
side would make administration more easily.



Agree that there might be a little overhead, but there are ways to minimize
this, and I am sure there are volunteers willing to help in favor of having
a more unifying project. Breaking things into multiple projects, and having
to manage the matrix of supported versions will be hell worst overhead.


Subprojects or even if we send this back to incubator as ""connectors
project"" is better then public github per package in my opinion.



Now, if with this move is signalizing to customers that the Streaming API
as in 1.x is going away in favor the new structure streaming APIs , then I
guess this is a complete different discussion.


-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 17 Mar 2016 16:21:07 -0700",Re: SPARK-13843 and future of streaming backends,Reynold Xin <rxin@databricks.com>,"Also, just wanted to point out something:


While I do agree that's more important, the streaming assemblies
weren't really blocking that work. The fact that there are still
streaming assemblies in the build kinda proves that point. :-)

I even filed a task to look at getting rid of the streaming assemblies
(SPARK-13575; just the assemblies though, not the code) but while
working on it found it would be more complicated than expected, and
decided against it given that it didn't really affect work on the
other assemblies.

-- 
Marcelo

---------------------------------------------------------------------


"
Wes McKinney <wes@cloudera.com>,"Thu, 17 Mar 2016 16:25:16 -0700",PySpark API divergence + improving pandas interoperability,dev@spark.apache.org,"hi everyone,

I've recently gotten moving on solving some of the low-level data
interoperability problems between Python's NumPy-focused scientific
computing and data libraries like pandas and the rest of the big data
ecosystem, Spark being a very important part of that.

for pandas users using Apache Arrow as the structured data exchange
medium (read more here:
http://wesmckinney.com/blog/pandas-and-apache-arrow/). I created
https://issues.apache.org/jira/browse/SPARK-13534 to add an Arrow
""thunderbolt port""  (to make an analogy) to Spark for moving data from
Spark SQL to pandas much more efficiently than the current
serialization scheme. If anyone wants to be a partner in crime on
this, feel free to reach out! I'll be dropping the Arrow
memory<->pandas conversion code in the next couple weeks.

As I'm looking more at the implementation details and API design of
PySpark, I note that it has been intended to have near 1-1 parity with
the Scala API, enabling developers to jump between APIs without a lot
of cognitive dissonance (you lose type information in Python, but
c'est la vie). Much of PySpark appears to be wrapping Scala / Java API
calls with py4j (much as many Python libraries wrap C/C++ libraries in
an analogous fashion).

In the long run, I'm concerned this may become problematic as users'
expectations about the semantics of interacting with the data may not
be compatible with the behavior of the Spark Scala API (particularly
the API design and semantics of Spark SQL and Datasets). As the Spark
user base grows, so, too, will the user needs, particularly in the
more accessible APIs (Python / R). I expect the Scala users tend to be
a more sophisticated audience with a more software engineering /
computer science tilt.

With a ""big picture"" goal of bringing about a semantic convergence
between big data and small data in a certain subset of scalable
computations, I am curious what is the Spark development community's
attitude towards efforts to achieve 1-1 PySpark API parity (with a
slight API lag as new features show up strictly in Scala before in
Python), particularly in the strictly semantic realm of data
interactions (at the end of the day, code has to move around bits
someplace). Here is an illustrative, albeit somewhat trivial example
of what I'm talking about:

https://issues.apache.org/jira/browse/SPARK-13943

If closer semantic compatibility with existing software in R and
Python is not a high priority, that is a completely reasonable answer.

Another thought is treating PySpark as the place where the ""rubber
meets the road"" -- the point of contact for any Python developers
building applications with Spark. This would leave library developers
aiming to create higher level user experiences (e.g. emulating pandas
more closely) and thus use PySpark as an implementation tool that
users otherwise do not directly interact with. But this is seemingly
at odds with the efforts to make Spark DataFrames behave in an
pandas/R-like fashion.

The nearest analogue to this I would give is the relationship between
pandas and NumPy in the earlier days of pandas (version 0.7 and
earlier). pandas relies on NumPy data structures and many of its array
algorithms. Early on I was lightly criticized in the community for
creating pandas as a separate project rather than contributing patches
to NumPy, but over time it has proven to have been the right decision,
as domain specific needs can evolve in a decoupled way without onerous
API design compromises.

very best,
Wes

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 18 Mar 2016 09:10:20 +0000",Re: [POWERED BY] Please add our organization,,"(Good practice indeed but the general idea is to reference ""Apache
Spark"" after which it's reasonable to reference ""Spark"", and this wiki
does.)


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 18 Mar 2016 09:35:34 +0000",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"Code can be removed from an ASF project.
That code can live on elsewhere (in accordance with the license)

It can't be presented as part of the official ASF project, like any
other 3rd party project
The package name certainly must change from org.apache.spark

I don't know of a protocol, but common sense dictates a good-faith
effort to offer equivalent access to the code (e.g. interested
committers should probably be repo owners too.)

This differs from ""any other code deletion"" in that there's an intent
to keep working on the code but outside the project.
More discussion -- like this one -- would have been useful beforehand
but nothing's undoable

Backwards-compatibility is not a good reason for things, because we're
talking about Spark 2.x, and we're already talking about distributing
the code differently.

Is the reason for this change decoupling releases? or changing governance?
Seems like the former, but we don't actually need the latter to achieve that.
There's an argument for a new repo, but this is not an argument for
moving X out of the project per se

I'm sure doing this in the ASF is more overhead, but if changing
governance is a non-goal, there's no choice.
Convenience can't trump that.

Kafka integration is clearly more important than the others.
It seems to need to stay within the project.
However this still leaves a packaging problem to solve, that might
need a new repo. This is orthgonal.


Here's what I think:

1. Leave the moved modules outside the project entirely
  (why not Kinesis though? that one was not made clear)
2. Change package names and make sure it's clearly presented as external
3. Add any committers that want to be repo owners as owners
4. Keep Kafka within the project
5. Add some subproject within the current project as needed to
accomplish distribution goals


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 18 Mar 2016 10:04:56 +0000",Re: SPARK-13843 and future of streaming backends,Hari Shreedharan <hshreedharan@cloudera.com>,"
Spark has hit one of the enternal problems of OSS projects, one hit by: ant, maven, hadoop, ... anything with a plugin model.

Take in the plugin: you're in control, but also down for maintenance

Leave out the plugin: other people can maintain it, be more agile, etc.

But you've lost control, and you can't even manage the links. Here I think maven suffered the most by keeping stuff in codehaus; migrating off there is still hard â€”not only did they lose the links: they lost the JIRA.

Maven's relationship with codehaus was very tightly coupled, lots of committers on both; I don't know how that relationship was handled at a higher level.


On 17 Mar 2016, at 20:51, Hari Shreedharan <hshreedharan@cloudera.com<mailto:hshreedharan@cloudera.com>> wrote:

I have worked with various ASF projects for 4+ years now. Sure, ASF projects can delete code as they feel fit. But this is the first time I have really seen code being ""moved out"" of a project without discussion. I am sure you can do this without violating ASF policy, but the explanation for that would be convoluted (someone decided to make a copy and then the ASF project deleted it?).

+1 for discussion. Dev changes should -> dev list; PMC for process in general. Don't think the ASF will overlook stuff like that.

Might want to raise this issue on the next broad report


FWIW, it may be better to just see if you can have committers to work on these projects: recruit the people and say 'please, only work in this area â€”for now"". That gets developers on your team, which is generally considered a metric of health in a project.

Or, as Cody Koeniger suggests, having a spark-extras project in the ASF with a focus on extras with their own support channel.


Also, moving the code out would break compatibility. AFAIK, there is no way to push org.apache.* artifacts directly to maven central. That happens via mirroring from the ASF maven repos. Even if it you could somehow directly push the artifacts to mvn, you really can push to org.apache.* groups only if you are part of the repo and acting as an agent of that project (which in this case would be Apache Spark). Once you move the code out, even a committer/PMC member would not be representing the ASF when pushing the code. I am not sure if there is a way to fix this issue.




This topic has cropped up in the general context of third party repos publishing artifacts with org.apache names but vendor specfic suffixes (e.g org.apache.hadoop/hadoop-common.5.3-cdh.jar

Some people were pretty unhappy about this, but the conclusion reached was ""maven doesn't let you do anything else and still let downstream people use it"". Futhermore, as all ASF releases are nominally the source releases *not the binaries*, you can look at the POMs and say ""we've released source code designed to publish artifacts to repos â€”this is 'use as intended'.

People are also free to cut their own full project distributions, etc, etc. For example, I stick up the binaries of Windows builds independent of the ASF releases; these were originally just those from HDP on windows installs, now I check out the commit of the specific ASF release on a windows 2012 VM, do the build, copy the binaries. Free for all to use. But I do suspect that the ASF legal protections get a bit blurred here. These aren't ASF binaries, but binaries built directly from unmodified ASF releases.

In contrast to sticking stuff into a github repo, the moved artifacts cannot be published as org.apache artfacts on maven central. That's non-negotiable as far as the ASF are concerned. The process for releasing ASF artifacts there goes downstream of the ASF public release process: you stage the artifacts, they are part of the vote process, everything with org.apache goes through it.

That said: there is nothing to stop a set of shell org.apache artifacts being written which do nothing but contain transitive dependencies on artifacts in different groups, such as org.spark-project. The shells would be released by the ASF; they pull in the new stuff. And, therefore, it'd be possible to build a spark-assembly with the files. (I'm ignoring a loop in the build DAG here, playing with git submodules would let someone eliminate this by adding the removed libraries under a modified project.

I think there might some issues related to package names; you could make a case for having public APIs with the original names â€”they're the API, after all, and that's exactly what Apache Harmony did with the java.* packages.


Thanks,
Hari

On Thu, Mar 17, 2016 at 1:13 PM, Mridul Muralidharan <mridul@gmail.com<mailto:mridul@gmail.com>> wrote:
I am not referring to code edits - but to migrating submodules and
code currently in Apache Spark to 'outside' of it.
If I understand correctly, assets from Apache Spark are being moved
out of it into thirdparty external repositories - not owned by Apache.

At a minimum, dev@ discussion (like this one) should be initiated.
As PMC is responsible for the project assets (including code), signoff
is required for it IMO.

More experienced Apache members might be opine better in case I got it wrong !


Regards,
Mridul


On Thu, Mar 17, 2016 at 12:55 PM, Cody Koeninger <cody@koeninger.org<mailto:cody@koeninger.org>> wrote:
> Why would a PMC vote be necessary on every code deletion?
>
> There was a Jira and pull request discussion about the submodules that
> have been removed so far.
>
> https://issues.apache.org/jira/browse/SPARK-13843
>
> There's another ongoing one about Kafka specifically
>
> https://issues.apache.org/jira/browse/SPARK-13877
>
>
> On Thu, Mar 17, 2016 at 2:49 PM, Mridul Muralidharan <mridul@gmail.com<mailto:mridul@gmail.com>> wrote:
>>
>> I was not aware of a discussion in Dev list about this - agree with most of
>> the observations.
>> In addition, I did not see PMC signoff on moving (sub-)modules out.
>>
>> Regards
>> Mridul
>>
>>
>>
>> On Thursday, March 17, 2016, Marcelo Vanzin <vanzi>>> Hello all,
>>>
>>> Recently a lot of the streaming backends were moved to a separate
>>> project on github and removed from the main Spark repo.
>>>
>>> While I think the idea is great, I'm a little worried about the
>>> execution. Some concerns were already raised on the bug mentioned
>>> above, but I'd like to have a more explicit discussion about this so
>>> things don't fall through the cracks.
>>>
>>> Mainly I have three concerns.
>>>
>>> i. Ownership
>>>
>>> That code used to be run by the ASF, but now it's hosted in a github
>>> repo owned not by the ASF. That sounds a little sub-optimal, if not
>>> problematic.
>>>
>>> ii. Governance
>>>
>>> Similar to the above; who has commit access to the above repos? Will
>>> all the Spark committers, present and future, have commit access to
>>> all of those repos? Are they still going to be considered part of
>>> Spark and have release management done through the Spark community?
>>>
>>>
>>> For both of the questions above, why are they not turned into
>>> sub-projects of Spark and hosted on the ASF repos? I believe there is
>>> a mechanism to do that, without the need to keep the code in the main
>>> Spark repo, right?
>>>
>>> iii. Usability
>>>
>>> This is another thing I don't see discussed. For Scala-based code
>>> things don't change much, I guess, if the artifact names don't change
>>> (another reason to keep things in the ASF?), but what about python?
>>> How are pyspark users expected to get that code going forward, since
>>> it's not in Spark's pyspark.zip anymore?
>>>
>>>
>>> Is there an easy way of keeping these things within the ASF Spark
>>> project? I think that would be better for everybody.
>>>
>>> --
>>> Marcelo
>>>
>>> ---------------------------------------------------------------------
>>> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
>>> For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>
>>>
>>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>



"
Steve Loughran <stevel@hortonworks.com>,"Fri, 18 Mar 2016 10:12:52 +0000",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"
to
,
tion


If you want a separate project, eg. SPARK-EXTRAS, then it *generally* needs to go through incubation. While normally its the incubator PMC which sponsors/oversees the incubating project, it doesn't have to be the case: the spark project can do it.


Also Apache Arrow managed to make it straight to toplevel without that process. Given that the spark extras are already ASF source files, you could try the same thing, add all the existing committers, then look for volunteers to keep things.


You'd get
 -a JIRA entry of your own, easy to reassign bugs from SPARK to SPARK-EXTRAS
 -a bit of git
 -ability to set up builds on ASF Jenkins. Regression testing against spark nightlies would be invaluable here.
 -the ability to stage and publish through ASF Nexus


-Steve

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 18 Mar 2016 11:15:08 +0000",Re: Spark ML - Scaling logistic regression for many features,Daniel Siegmann <daniel.siegmann@teamaol.com>,"No, I didn't yet - feel free to create a JIRA.




"
Ted Yu <yuzhihong@gmail.com>,"Fri, 18 Mar 2016 07:57:45 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Michael Armbrust <michael@databricks.com>,"I tried again this morning :

$ wget
https://s3.amazonaws.com/spark-related-packages/spark-1.6.1-bin-hadoop2.6.tgz
--2016-03-18 07:55:30--
https://s3.amazonaws.com/spark-related-packages/spark-1.6.1-bin-hadoop2.6.tgz
Resolving s3.amazonaws.com... 54.231.19.163
...
$ tar zxf spark-1.6.1-bin-hadoop2.6.tgz

gzip: stdin: unexpected end of file
tar: Unexpected EOF in archive
tar: Unexpected EOF in archive
tar: Error is not recoverable: exiting now


he same
4.tgz
2.6.tgz
"
Cody Koeninger <cody@koeninger.org>,"Fri, 18 Mar 2016 09:58:42 -0500",Re: SPARK-13843 and future of streaming backends,Steve Loughran <stevel@hortonworks.com>,"with a focus on extras with their own support channel.

To be clear, I didn't suggest that and don't think that's the best
solution.  I said to the people who want things done that way, which
committer is going to step up and do that organizational work?

I think there are advantages to moving everything currently in extras/
and external/ out of the spark project, but the current Kafka
packaging issue can be solved straightforwardly by just adding another
artifact and code tree under external/.

ote:
nt,
k
 is
cts
y
u
uld
a
considered
ith
ay
a
y
 in
.g
s
se
ot
de
.
c.
ls,
VM,
at
es,
not
le
d
be
n
te
a
API, after
s
n
e
-

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 18 Mar 2016 15:00:31 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,"Ted Yu <yuzhihong@gmail.com>, Michael Armbrust <michael@databricks.com>","I'm seeing the same. :(


.tgz
.tgz
the same
.4.tgz
p2.6.tgz
"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 18 Mar 2016 08:12:45 -0700",Re: SPARK-13843 and future of streaming backends,Cody Koeninger <cody@koeninger.org>,"

I am currently not a committer, but If we are willing to go into the
direction of having another project as spark-extras, I can help drive the
bureaucratic work to make this a reality.


-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
VHPC 16 <vhpc.dist@gmail.com>,"Fri, 18 Mar 2016 16:30:04 +0100","CfP 11th Workshop on Virtualization in High-Performance Cloud
 Computing (VHPC '16)",dev@spark.apache.org,"====================================================================
CALL FOR PAPERS


11th Workshop on Virtualization in HighÂ­-Performance Cloud Computing  (VHPC '16)
held in conjunction with the International Supercomputing Conference -
High Performanc"
Imran Rashid <irashid@cloudera.com>,"Fri, 18 Mar 2016 11:06:01 -0500",Re: SPARK-13843 and future of streaming backends,Cody Koeninger <cody@koeninger.org>,"

Certainly PMC votes are not necessary on *every* code deletion.  I dont'
think there is a very clear rule on when such discussion is warranted, just
a soft expectation that committers understand which changes require more
discussion before getting merged.  I believe the only formal requirement
for a PMC vote is when there is a release.  But I think as a community we'd
much rather deal with these issues ahead of time, rather than having
contentious discussions around releases because some are strongly opposed
to changes that have already been merged.

I'm all for the idea of removing these modules in general (for all of the
reasons already mentioned), but it seems that there are important questions
about how the new packages get distributed and how they are managed that
merit further discussion.

I'm somewhat torn on the question of the sub-project vs independent, and
how its governed.  I think Steve has summarized the tradeoffs very well.  I
do want to emphasize, though, that if they are entirely external from the
ASF, the artifact ids and the package names must change at the very least.
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 18 Mar 2016 10:07:44 -0700",Re: SPARK-13843 and future of streaming backends,Steve Loughran <stevel@hortonworks.com>,"Hi Steve, thanks for the write up.


Am I to understand from your reply that it's not possible for a single
project to have multiple repos?

-- 
Marcelo

---------------------------------------------------------------------


"
,"Fri, 18 Mar 2016 18:09:58 +0100",Re: SPARK-13843 and future of streaming backends,"dev@spark.apache.org
 <CAJiQeYJzeOsubVi-Ot=ztmOQd-46_EvwzJz57JMTSA1vvp-BmQ@mail.gmail.com>
 <CAKWX9VVc538sB0MLaEF4-2_Za-4Y=3fzh9brjqeByKzR0SY6EA@mail.gmail.com>
 <CAJiQeYLeCdVZATuckyjPcf1_hQBSHTLqP8gJfqwSGOuHA02tzA@mail.gmail.com>
 <CAKWX9VX6kDuU-v50CZhsL-AykuvHEHtHTqu0cz55UXx_jNz=VA@mail.gmail.com>
 <CAPh_B=a7Af1LxnQAdPf9N94cHTCb6WXcHR5aE_Ah9Fq5kgG3Aw@mail.gmail.com>
 <814AD545-E0AF-4411-B863-B25C5A963930@hortonworks.com>","Hi Marcelo,

a project can have multiple repos: it's what we have in ServiceMix, in 
Karaf.

For the *-extra on github, if the code has been in the ASF, the PMC 
members have to vote to move the code on *-extra.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 18 Mar 2016 10:22:34 -0700",Re: SPARK-13843 and future of streaming backends,"""dev@spark.apache.org"" <dev@spark.apache.org>","rs

That's good to know. To me that sounds like the best solution.

I've heard that top-level projects have some requirements with regards
to have active development, and these components probably will not see
that much activity. And top-level does sound like too much bureaucracy
for this.

-- 
Marcelo

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 18 Mar 2016 10:27:15 -0700",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"
It can have multiple repos, but this still brings the overhead into the PMC
to maintain it which was brought on previously on this thread and it might
not be the direction the PMC want to take (but I might be mistaken).

Another approach is to make this extras, just a subproject, with it's own
set of committers etc.... which gives less burden on the Spark PMC.

Anyway, my main issue here is not who and how it's going to be managed, but
that it continues under Apache governance.

-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Shane Curcuru <asf@shanecurcuru.org>,"Fri, 18 Mar 2016 16:15:30 -0400",Re: SPARK-13843 and future of streaming backends,dev@spark.apache.org,"Marcelo Vanzin wrote earlier:


Question: why was the code removed from the Spark repo?  What's the harm
in keeping it available here?

The ASF is perfectly happy if anyone wants to fork our code - that's one
of the core tenets of the Apache license.  You just can't take the name
or trademarks, so you may need to change some package names or the like.

So it's fine if some people want to work on the code outside the
project.  But it's puzzling as to why the Spark PMC shouldn't keep the
code in the project as well, even if it might not have the same release
cycles or whatnot.

- Shane

---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Fri, 18 Mar 2016 15:39:03 -0500",Re: SPARK-13843 and future of streaming backends,Shane Curcuru <asf@shanecurcuru.org>,"


Assuming the Spark PMC has no plan on releasing the code, why would we keep
it in our codebase?  It only makes the codebase harder to navigate.  It
would be easy for someone to stumble on that code and expect it be part of
the release.  Seems like a general code-hygiene practice, eg. just like not
leaving a giant commented-out block of old code.

(But as you can see from the rest of the thread, I think that discussion on
whether it should still be part of Apache Spark is ongoing ...)
"
chrismattmann <mattmann@apache.org>,"Fri, 18 Mar 2016 14:12:11 -0700 (MST)",Re: SPARK-13843 and future of streaming backends,dev@spark.apache.org,"So, my comment here is that any code *cannot* be removed from an Apache
project if there is a VETO issued which so far I haven't seen, though maybe
Marcelo can clarify that.

However if a VETO was issued, then the code cannot be removed and must be
put back. Anyone can fork anything our license allows that, but the
community itself must steward the code and part of that is hearing
everyone's voice within that community before acting.

Cheers,
Chris



--

---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Fri, 18 Mar 2016 15:11:51 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I just experienced the issue, however retrying the download a second
time worked. Could it be that there is some load balancer/cache in
front of the archive and some nodes still serve the corrupt packages?

6.tgz
6.tgz
m>
 the same
2.4.tgz
op2.6.tgz

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 18 Mar 2016 22:20:18 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Jakob Odersky <jakob@odersky.com>,"I just retried the Spark 1.6.1 / Hadoop 2.6 download and got a corrupt ZIP
file.

Jakob, are you sure the ZIP unpacks correctly for you? Is it the same Spark
1.6.1/Hadoop 2.6 package you had a success with?


.tgz
.tgz
ng the
.tgz
.tgz
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 18 Mar 2016 15:24:17 -0700",Re: SPARK-13843 and future of streaming backends,chrismattmann <mattmann@apache.org>,"
No, my intention was not to veto the change. I'm actually for the
removal of components if the community thinks they don't add much to
the project. (I'm also not sure I can even veto things, not being a
PMC member.)

I mainly wanted to know what was the path forward for those components
because, with Cloudera's hat on, we care about one of them (streaming
integration with flume), and we'd prefer if that code remained under
the ASF umbrella in some way.

-- 
Marcelo

---------------------------------------------------------------------


"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Fri, 18 Mar 2016 22:28:48 +0000",Re: SPARK-13843 and future of streaming backends,"Marcelo Vanzin <vanzin@cloudera.com>, chrismattmann <mattmann@apache.org>","Hi Marcelo,

Thanks for your reply. As a committer on the project, you *can* VETO
code. For sure. Unfortunately you donâ€™t have a binding vote on adding
new PMC members/committers, and/or on releasing the software, but do
have the ability to VETO.

That said, if thatâ€™s not your intent, sorry for misreading your intent.

Cheers,
Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 168-519, Mailstop: 168-527
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Director, Information Retrieval and Data Science Group (IRDS)
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
WWW: http://irds.usc.edu/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++





-----Original Message-----
From: Marcelo Vanzin <vanzin@cloudera.com>
Date: Friday, March 18, 2016 at 3:24 PM
To: jpluser <mattmann@apache.org>
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>
Subject: Re: SPARK-13843 and future of streaming backends

>On Fri, Mar 18, 2016 at 2:12 PM, chrismattmann <mattmann@apache.org>
>wrote:
>> So, my comment here is that any code *cannot* be removed from an Apache
>> project if there is a VETO issued which so far I haven't seen, though
>>maybe
>> Marcelo can clarify that.
>
>No, my intention was not to veto the change. I'm actually for the
>removal of components if the community thinks they don't add much to
>the project. (I'm also not sure I can even veto things, not being a
>PMC member.)
>
>I mainly wanted to know what was the path forward for those components
>because, with Cloudera's hat on, we care about one of them (streaming
>integration with flume), and we'd prefer if that code remained under
>the ASF umbrella in some way.
>
>-- 
>Marcelo

"
Jakob Odersky <jakob@odersky.com>,"Fri, 18 Mar 2016 15:49:08 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I just realized you're using a different download site. Sorry for the
confusion, the link I get for a direct download of Spark 1.6.1 /
Hadoop 2.6 is http://d3kbcqa49mib13.cloudfront.net/spark-1.6.1-bin-hadoop2.6.tgz

P
rk
p2.6.tgz
p2.6.tgz
ing the
oop2.4.tgz
adoop2.6.tgz
?

---------------------------------------------------------------------


"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Fri, 18 Mar 2016 23:16:24 +0000",SparkContext.stop() takes too long to complete,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark experts,
I am using Spark 1.5.2 on YARN with dynamic allocation enabled. I see in
the driver/application master logs that the app is marked as SUCCEEDED and
then SparkContext stop is called. However, this stop sequence takes > 10
minutes to complete, and YARN resource manager kills the application master
as it didnâ€™t receive a heartbeat within the last 10 minutes. The resource
manager then kills the application master. Any ideas about what may be
going on?

Here are the relevant logs:

*6/03/18 21:26:58 INFO yarn.ApplicationMaster: Final app status:
SUCCEEDED, exitCode: 0
16/03/18 21:26:58 INFO spark.SparkContext: Invoking stop() from
shutdown hook*16/03/18 21:26:58 INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/static/sql,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL/execution/json,null}16/03/18
21:26:58 INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL/execution,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL/json,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/static/sql,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL/execution/json,null}16/03/18
21:26:58 INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL/execution,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL/json,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/SQL,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/metrics/json,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/stages/stage/kill,null}16/03/18
21:26:58 INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/api,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/static,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/executors/threadDump/json,null}16/03/18
21:26:58 INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/executors/threadDump,null}16/03/18
21:26:58 INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/executors/json,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/executors,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/environment/json,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/environment,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/storage/rdd/json,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/storage/rdd,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/storage/json,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/storage,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/stages/pool/json,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/stages/pool,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/stages/stage/json,null}16/03/18
21:26:58 INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/stages/stage,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/stages/json,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/stages,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/jobs/job/json,null}16/03/18 21:26:58
INFO handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/jobs/job,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/jobs/json,null}16/03/18 21:26:58 INFO
handler.ContextHandler: stopped
o.s.j.s.ServletContextHandler{/jobs,null}16/03/18 21:26:58 INFO
ui.SparkUI: Stopped Spark web UI at
http://10.143.240.240:5270616/03/18 21:27:58 INFO
cluster.YarnClusterSchedulerBackend: Requesting to kill executor(s)
113516/03/18 21:27:58 INFO yarn.YarnAllocator: Driver requested a
total number of 208 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1135.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1135 because it has
been idle for 60 seconds (new desired total will be 208)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 112316/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 207 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1123.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1123 because it has
been idle for 60 seconds (new desired total will be 207)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 111716/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 206 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1117.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1117 because it has
been idle for 60 seconds (new desired total will be 206)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 118516/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 205 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1185.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1185 because it has
been idle for 60 seconds (new desired total will be 205)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 115216/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 204 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1152.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1152 because it has
been idle for 60 seconds (new desired total will be 204)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 114016/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 203 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1140.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1140 because it has
been idle for 60 seconds (new desired total will be 203)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 114916/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 202 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1149.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1149 because it has
been idle for 60 seconds (new desired total will be 202)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 115416/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 201 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1154.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1154 because it has
been idle for 60 seconds (new desired total will be 201)16/03/18
21:27:58 INFO cluster.YarnClusterSchedulerBackend: Requesting to kill
executor(s) 113616/03/18 21:27:58 INFO yarn.YarnAllocator: Driver
requested a total number of 200 executor(s).16/03/18 21:27:58 INFO
yarn.ApplicationMaster$AMEndpoint: Driver requested to kill
executor(s) 1136.16/03/18 21:27:58 INFO
spark.ExecutorAllocationManager: Removing executor 1136 because it has
been idle for 60 seconds (new desired total will be 200)*16/03/18
21:38:17 ERROR yarn.ApplicationMaster: RECEIVED SIGNAL 15: SIGTERM
*

â€‹
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Fri, 18 Mar 2016 17:30:38 -0700",Fwd: DF creation,"user@spark.apache.org, dev@spark.apache.org","Hi ,

I am trying to create separate val reference to object DATA (as shown
below),

case class data(name:String,age:String)

Creation of this object is done separately and the reference to the object
is stored into val data.

i use val samplerdd = sc.parallelize(Seq(data)) , to create RDD.
org.apache.spark.rdd.RDD[data] = ParallelCollectionRDD[10] at parallelize
at <console>:24

is there a way to create dataframe out of this, without using  createDataFrame,
and by using toDF() which i was unable to convert.(would like to avoid
providing the structtype).

Regards,
Satyajit.
"
Tim Hunter <timhunter@databricks.com>,"Fri, 18 Mar 2016 18:18:50 -0700","Request for comments: Tensorframes, an integration library between
 TensorFlow and Spark DataFrames","user <user@spark.apache.org>, dev@spark.apache.org","Hello all,

I would like to bring your attention to a small project to integrate
TensorFlow with Apache Spark, called TensorFrames. With this library, you
can map, reduce or aggregate numerical data stored in Spark dataframes
using TensorFlow computation graphs. It is published as a Spark package and
available in this github repository:

https://github.com/tjhunter/tensorframes

More detailed examples can be found in the user guide:

https://github.com/tjhunter/tensorframes/wiki/TensorFrames-user-guide

This is a technical preview at this point. I am looking forward to some
feedback about the current python API if some adventurous users want to try
it out. Of course, contributions are most welcome, for example to fix bugs
or to add support for platforms other than linux-x86_64. It should support
all the most common inputs in dataframes (dense tensors of rank 0, 1, 2 of
ints, longs, floats and doubles).

Please note that this is not an endorsement by Databricks of TensorFlow, or
any other deep learning framework for that matter. If users want to use
deep learning in production, some other more robust solutions are

Best regards


Tim Hunter
"
Diwakar Dhanuskodi <diwakar.dhanuskodi@gmail.com>,"Sat, 19 Mar 2016 10:56:25 +0530",RE: Fwd: DF creation,"satyajit vegesna <satyajit.apasprk@gmail.com>, user@spark.apache.org,
 dev@spark.apache.org","Import sqlContext.implicits._ Â before Â using Â df ()


Sent from Samsung Mobile.

<div>-------- Original message --------</div><div>From: satyajit vegesna <satyajit.apasprk@gmail.com> </div><div>Date:19/03/2016  06:00  (GMT+05:30) </div><div>To: user@spark.apache.org, dev@spark.apache.org </div><div>Cc:  </div><div>Subject: Fwd: DF creation </div><div>
</div>
Hi ,

I am trying to create separate val reference to object DATA (as shown below),  

case class data(name:String,age:String)

Creation of this object is done separately and the reference to the object is stored into val data.

i use val samplerdd = sc.parallelize(Seq(data)) , to create RDD.
org.apache.spark.rdd.RDD[data] = ParallelCollectionRDD[10] at parallelize at <console>:24

is there a way to create dataframe out of this, without using  createDataFrame, and by using toDF() which i was unable to convert.(would like to avoid providing the structtype).

Regards,
Satyajit.



"
Pete Robbins <robbinspg@gmail.com>,"Sat, 19 Mar 2016 07:32:58 +0000",Can we remove private[spark] from Metrics Source and SInk traits?,Dev <dev@spark.apache.org>,"This seems to me to be unnecessarily restrictive. These are very useful
extension points for adding 3rd party sources and sinks.

I intend to make an Elasticsearch sink available on spark-packages but this
will require a single class, the sink, to be in the org.apache.spark
package tree. I could submit the package as a PR to the Spark codebase, and
I'd be happy to do that but it could be a completely separate add-on.

There are similar issues with writing a 3rd party metrics source which may
not be of interest to the community at large so would probably not warrant
inclusion in the Spark codebase.

Any thoughts?
"
Steve Loughran <stevel@hortonworks.com>,"Sat, 19 Mar 2016 12:32:21 +0000",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"
eds to go through incubation. While normally its the incubator PMC which sponsors/oversees the incubating project, it doesn't have to be the case: the spark project can do it.
rocess. Given that the spark extras are already ASF source files, you could try the same thing, add all the existing committers, then look for volunteers to keep things.


I don't know. there's generally a 1 project -> 1x issue, 1x JIRA.

but: hadoop core has 3x JIRA, 1x repo, and one set of write permissions to that repo, with the special exception of branches (encryption, ipv6) that have their own committers.

oh, and I know that hadoop site is on SVN, as are other projects, just to integrate with asf site publishing, so you can certainly have 1x git + 1 x svn

ASF won't normally let you have 1 repo with different bits of the tree having different access rights, so you couldn't open up spark-extras to people with less permissions/rights than others.

A separate repo will, separate issue tracking helps you isolate stuff

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Sat, 19 Mar 2016 12:35:39 +0000",Re: SPARK-13843 and future of streaming backends,Marcelo Vanzin <vanzin@cloudera.com>,"
e:
ybe

I'd be supportive of a spark-extras project; it'd actually be  place to keep stuff I've worked on 
 -the yarn ATS 1/1.5 integration
 -that mutant hive JAR which has the consistent kryo dependency and different shadings

... etc

There's also the fact that the twitter streaming is a common example to play with, flume is popular in places too.

If you want to set up a new incubator with a goal of graduating fast, I'd help. As a key metric of getting out of incubator is active development, you just need to ""recruit"" contributors and keep them engaged.




---------------------------------------------------------------------


"
Gerard Maas <gerard.maas@gmail.com>,"Sat, 19 Mar 2016 14:05:10 +0100",Re: Can we remove private[spark] from Metrics Source and SInk traits?,Pete Robbins <robbinspg@gmail.com>,1
Adam Kocoloski <kocolosk@apache.org>,"Sat, 19 Mar 2016 10:45:34 -0400",Re: SPARK-13843 and future of streaming backends,Steve Loughran <stevel@hortonworks.com>,"
*generally* needs to go through incubation. While normally its the incubator PMC which sponsors/oversees the incubating project, it doesn't have to be the case: the spark project can do it.
that process. Given that the spark extras are already ASF source files, you could try the same thing, add all the existing committers, then look for volunteers to keep things.
single
permissions to that repo, with the special exception of branches (encryption, ipv6) that have their own committers.
to integrate with asf site publishing, so you can certainly have 1x git + 1 x svn
having different access rights, so you couldn't open up spark-extras to people with less permissions/rights than others.

Multiple repositories per project are certainly allowed without incurring the overhead of a subproject; Cordova and CouchDB are two projects that have taken this approach:

https://github.com/apache?utf8=âœ“&query=cordova-
https://github.com/apache?utf8=âœ“&query=couchdb-

I believe Cordova also generates independent release artifacts in different cycles (e.g. cordova-ios releases independently from cordova-android).

If the goal is to enable a divergent set of committers to spark-extras then an independent project makes sense. If youâ€™re just looking to streamline the main repo and decouple some of these other streaming â€œbackendsâ€ from the normal release cycle then there are low impact ways to accomplish this inside a single Apache Spark project. Cheers,

Adam


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Sat, 19 Mar 2016 16:16:49 +0000",Re: Can we remove private[spark] from Metrics Source and SInk traits?,Gerard Maas <gerard.maas@gmail.com>,"There are several open Jiras to add new Sinks

OpenTSDB https://issues.apache.org/jira/browse/SPARK-12194
StatsD https://issues.apache.org/jira/browse/SPARK-11574
Kafka https://issues.apache.org/jira/browse/SPARK-13392

Some have PRs from 2015 so I'm assuming there is not the desire to
integrate these into core Spark. Opening up the Sink/Source interfaces
would at least allow these to exist somewhere such as spark-packages
without having to pollute the o.a.s namespace



"
"""HanPan"" <panda@thinkingdata.cn>","Mon, 21 Mar 2016 11:31:51 +0800",MLPC model can not be saved,<dev@spark.apache.org>," 

Hi Guys,

 

         I built a ML pipeline that includes multilayer perceptron
classifier, I got the following error message when I tried to save the
pipeline model. It seems like MLPC model can not be saved which means I have
no ways to save the trained model. Is there any way to save the model that I
can use it for future prediction.

 

         Exception in thread ""main"" java.lang.UnsupportedOperationException:
Pipeline write will fail on this Pipeline because it contains a stage which
does not implement Writable. Non-Writable stage: mlpc_2d8b74f6da60 of type
class
org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel

         at
org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply
(Pipeline.scala:218)

         at
org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply
(Pipeline.scala:215)

         at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala
:33)

         at
scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)

         at
org.apache.spark.ml.Pipeline$SharedReadWrite$.validateStages(Pipeline.scala:
215)

         at
org.apache.spark.ml.PipelineModel$PipelineModelWriter.<init>(Pipeline.scala:
325)

         at org.apache.spark.ml.PipelineModel.write(Pipeline.scala:309)

         at
org.apache.spark.ml.util.MLWritable$class.save(ReadWrite.scala:130)

         at org.apache.spark.ml.PipelineModel.save(Pipeline.scala:280)

         at
cn.thinkingdata.nlp.spamclassifier.FFNNSpamClassifierPipeLine.main(FFNNSpamC
lassifierPipeLine.java:76)

         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

         at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62
)

         at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
.java:43)

         at java.lang.reflect.Method.invoke(Method.java:497)

         at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$ru
nMain(SparkSubmit.scala:731)

         at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)

         at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)

         at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)

         at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

 

Thanks

Pan

"
Kashish Jain <Kashish.Jain@guavus.com>,"Mon, 21 Mar 2016 06:08:07 +0000",CTAS support in sparksql,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I was exploring the possibility of CTAS with spark-sql (SPARK-1.3.1) for saving the big results into CSV formatted files for offline viewing. These are the two things that I did

  1.  CREATE TABLE IF NOT EXISTS csv_dump27 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS TEXTFILE LOCATION '/data/offline/' as select X, Y from tableName where timestamp=1427094000;
  2.  CREATE TABLE IF NOT EXISTS csv_dump44 ROW FORMAT DELIMITED FIELDS TERMINATED BY ',' LINES TERMINATED BY '\n' STORED AS PARQUET LOCATION '/data/offline/' as select X, Y from tableName where timestamp=1427094000;

Here are my observations

Case1 – I am able to create the table in hive and can see the CSV data at the provided path. But when I try to do some queries over this table through spark-sql I get exceptions. (StackTrace below).
Case2 – I am able to create the table in hive, and can see the parquet files. But strangely this time I am able to query this table through spark-sql without any exception. I can’t use this though since saving data as parquet does not serve my purpose of offline viewing in CSV.

So my question is “Is CTAS supported in spark-sql with storage as TEXT?”

he “TextInputFormat” contains the appended paths of each part file under that directory. StrilUtils.split is not able to split the paths in to individual paths.

hdfs://NN-199:9000/data/offline/part-00000\,hdfs:/NN-199:9000/data/offlline/part-00001\,hdfs:/NN-199:9000/data/offline/part-00002\………….. . . . . ..


However in the case of  Parquet, the “dir"" variable is correctly passed as only the top level directory

hdfs://NN-199:9000/data/offline/csv_dump44

P.S.:- StackTrace
java.lang.IllegalArgumentException: java.net.URISyntaxException: Illegal character in scheme name at index 10: part-00000,hdfs:
at org.apache.hadoop.fs.Path.initialize(Path.java:206)
at org.apache.hadoop.fs.Path.<init>(Path.java:172)
at org.apache.hadoop.fs.Path.<init>(Path.java:94)
at org.apache.hadoop.fs.Globber.glob(Globber.java:211)
at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1642)
at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:257)
at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)
at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:304)
at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:203)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:32)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:219)
at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:217)
at scala.Option.getOrElse(Option.scala:120)
at org.apache.spark.rdd.RDD.partitions(RDD.scala:217)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1556)
at org.apache.spark.rdd.RDD.collect(RDD.scala:825)
at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:88)
at org.apache.spark.sql.hive.HiveContext$QueryExecution.stringResult(HiveContext.scala:423)
at org.apache.spark.sql.hive.thriftserver.AbstractSparkSQLDriver.run(AbstractSparkSQLDriver.scala:58)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:275)
at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:423)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:211)
at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
at java.lang.reflect.Method.invoke(Unknown Source)
at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:569)
at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:166)
at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:189)
at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:110)
at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.net.URISyntaxException: Illegal character in scheme name at index 10: part-00000,hdfs:
at java.net.URI$Parser.fail(Unknown Source)
at java.net.URI$Parser.checkChars(Unknown Source)
at java.net.URI$Parser.parse(Unknown Source)
at java.net.URI.<init>(Unknown Source)
at org.apache.hadoop.fs.Path.initialize(Path.java:203)


Thanks
Kashish Jain

"
JOAQUIN GUANTER GONZALBEZ <joaquin.guantergonzalbez@telefonica.com>,"Mon, 21 Mar 2016 08:06:34 +0000",Performance improvements for sorted RDDs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello devs,

I have found myself in a situation where Spark is doing sub-optimal computations for my RDDs, and I was wondering whether a patch to enable improved performance for this scenario would be a welcome addition to Spark or not.

The scenario happens when trying to cogroup two RDDs that are sorted by key and share the same partitioner. CoGroupedRDD will correctly detect that the RDDs have the same partitioner and will therefore create narrow cogroup split dependencies, as opposed to shuffle dependencies. This is great because it prevents any shuffling from happening. However, the cogroup is unable to detect that the RDDs are sorted in the same way, and will still insert all elements of the RDD in a map in order to join the elements with the same key.

When both RDDs are sorted using the same order, the cogroup can just join by doing a single pass over the data (since the data is ordered by key, you can just keep iterating until you find a different key). This would greatly reduce the memory requirements for these kind of operations.

Adding this to spark would require adding an â€œorderingâ€ member to RDD of type Option[Ordering], similarly to how the â€œpartitionerâ€ field works. That way, the sorting operations could populate this field and the operations that could benefit from this knowledge (cogroup, join, groupbykey, etc.) could read it to change their behavior accordingly.

Do you think this would be a good addition to Spark?

Thanks,
Ximo

________________________________

Este mensaje y sus adjuntos se dirigen exclusivamente a su destinatario, puede contener informaciÃ³n privilegiada o confidencial y es para uso exclusivo de la persona o entidad de destino. Si no es usted. el destinatario indicado, queda notificado de que la lectura, utilizaciÃ³n, divulgaciÃ³n y/o copia sin autorizaciÃ³n puede estar prohibida en virtud de la legislaciÃ³n vigente. Si ha recibido este mensaje por error, le rogamos que nos lo comunique inmediatamente por esta misma vÃ­a y proceda a su destrucciÃ³n.

The information contained in this transmission is privileged and confidential information intended only for the use of the individual or entity named above. If the reader of this message is not the intended recipient, you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this transmission in error, do not read it. Please immediately reply to the sender that you have received this communication in error and then delete it.

Esta mensagem e seus anexos se dirigem exclusivamente ao seu destinatÃ¡rio, pode conter informaÃ§Ã£o privilegiada ou confidencial e Ã© para uso exclusivo da pessoa ou entidade de destino. Se nÃ£o Ã© vossa senhoria o destinatÃ¡rio indicado, fica notificado de que a leitura, utilizaÃ§Ã£o, divulgaÃ§Ã£o e/ou cÃ³pia sem autorizaÃ§Ã£o pode estar proibida em virtude da legislaÃ§Ã£o vigente. Se recebeu esta mensagem por erro, rogamos-lhe que nos o comunique imediatamente por esta mesma via e proceda a sua destruiÃ§Ã£o
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 21 Mar 2016 03:41:28 -0700",Re: Performance improvements for sorted RDDs,JOAQUIN GUANTER GONZALBEZ <joaquin.guantergonzalbez@telefonica.com>,"Do you have performance numbers to backup this proposal for cogroup
operation ?

Thanks


rk
at
up
h
ou
ly
ember to RDD of
 field works. That
³n,
 virtud de
gamos
u
r
d
he
¡rio,
ra uso exclusivo
tinatÃ¡rio
Ã§Ã£o e/ou
gislaÃ§Ã£o vigente.
"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Mon, 21 Mar 2016 16:20:15 +0100",Re: Performance improvements for sorted RDDs,Ted Yu <yuzhihong@gmail.com>,"There is related discussion in
https://issues.apache.org/jira/browse/SPARK-8836. It's not too hard to
implement this without modifying Spark and we measured ~10x improvement
over plain RDD joins. I haven't benchmarked against DataFrames -- maybe
they also realize this performance advantage.


ark
hat
oup
s
l
th
n
you
tly
member to RDD of
 field works. That
o
³n,
n virtud de
ogamos
su
or
ed
the
e
idencial e Ã© para
 senhoria o
§Ã£o,
r proibida em virtude da
-lhe que nos
§Ã£o
"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Mar 2016 09:59:14 -0700",Re: PySpark API divergence + improving pandas interoperability,Wes McKinney <wes@cloudera.com>,"Hi Wes,

Thanks for the email. It is difficult to generalize without seeing a lot
more cases, but the boolean issue is simply a query analysis rule.

I can see us having a config option that changes analysis to match more
Python/R like, which changes the behavior of implicit type coercion and
allows boolean to integral automatically.


"
JOAQUIN GUANTER GONZALBEZ <joaquin.guantergonzalbez@telefonica.com>,"Mon, 21 Mar 2016 16:59:03 +0000",RE: Performance improvements for sorted RDDs,"Daniel Darabos <daniel.darabos@lynxanalytics.com>, Ted Yu
	<yuzhihong@gmail.com>","Hi Daniel,

I am glad you already ran the numbers on this change â˜º (for anyone reading, they can be found on slide 19 in http://www.slideshare.net/SparkSummit/interactive-graph-analytics-daniel-darabos ). I havenâ€™t done any formal benchmarking, but the speedup in our jobs is highly noticeable.

I agree it can be done without modifying Spark (we also have our own implementation in my codebase), but it seems a pity that anyone using the RDD API wonâ€™t get the benefits having a sorted RDD (which happens quite often since the shuffle phase can sort!).

Ximo.

De: Daniel Darabos [mailto:daniel.darabos@lynxanalytics.com]
Enviado el: lunes, 21 de marzo de 2016 16:20
Para: Ted Yu <yuzhihong@gmail.com>
CC: JOAQUIN GUANTER GONZALBEZ <joaquin.guantergonzalbez@telefonica.com>; dev@spark.apache.org
Asunto: Re: Performance improvements for sorted RDDs

There is related discussion in https://issues.apache.org/jira/browse/SPARK-8836. It's not too hard to implement this without modifying Spark and we measured ~10x improvement over plain RDD joins. I haven't benchmarked against DataFrames -- maybe they also realize this performance advantage.

On Mon, Mar 21, 2016 at 11:41 AM> wrote:
Do you have performance numbers to backup this proposal for cogroup operation ?

Thanks

On Mon, Mar 21, 2016 at 1:06 AM, JOAQUIN GUANTER GONZALBEZ <joaquin.guantergonzalbez@telefonica.com<mailto:joaquin.guantergonzalbez@telefonica.com>> wrote:
Hello devs,

I have found myself in a situation where Spark is doing sub-optimal computations for my RDDs, and I was wondering whether a patch to enable improved performance for this scenario would be a welcome addition to Spark or not.

The scenario happens when trying to cogroup two RDDs that are sorted by key and share the same partitioner. CoGroupedRDD will correctly detect that the RDDs have the same partitioner and will therefore create narrow cogroup split dependencies, as opposed to shuffle dependencies. This is great because it prevents any shuffling from happening. However, the cogroup is unable to detect that the RDDs are sorted in the same way, and will still insert all elements of the RDD in a map in order to join the elements with the same key.

When both RDDs are sorted using the same order, the cogroup can just join by doing a single pass over the data (since the data is ordered by key, you can just keep iterating until you find a different key). This would greatly reduce the memory requirements for these kind of operations.

Adding this to spark would require adding an â€œorderingâ€ member to RDD of type Option[Ordering], similarly to how the â€œpartitionerâ€ field works. That way, the sorting operations could populate this field and the operations that could benefit from this knowledge (cogroup, join, groupbykey, etc.) could read it to change their behavior accordingly.

Do you think this would be a good addition to Spark?

Thanks,
Ximo

________________________________

Este mensaje y sus adjuntos se dirigen exclusivamente a su destinatario, puede contener informaciÃ³n privilegiada o confidencial y es para uso exclusivo de la persona o entidad de destino. Si no es usted. el destinatario indicado, queda notificado de que la lectura, utilizaciÃ³n, divulgaciÃ³n y/o copia sin autorizaciÃ³n puede estar prohibida en virtud de la legislaciÃ³n vigente. Si ha recibido este mensaje por error, le rogamos que nos lo comunique inmediatamente por esta misma vÃ­a y proceda a su destrucciÃ³n.

The information contained in this transmission is privileged and confidential information intended only for the use of the individual or entity named above. If the reader of this message is not the intended recipient, you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this transmission in error, do not read it. Please immediately reply to the sender that you have received this communication in error and then delete it.

Esta mensagem e seus anexos se dirigem exclusivamente ao seu destinatÃ¡rio, pode conter informaÃ§Ã£o privilegiada ou confidencial e Ã© para uso exclusivo da pessoa ou entidade de destino. Se nÃ£o Ã© vossa senhoria o destinatÃ¡rio indicado, fica notificado de que a leitura, utilizaÃ§Ã£o, divulgaÃ§Ã£o e/ou cÃ³pia sem autorizaÃ§Ã£o pode estar proibida em virtude da rogamos-lhe que nos o comunique imediatamente por esta me___________________

Este mensaje y sus adjuntos se dirigen exclusivamente a su destinatario, puede contener informaciÃ³n privilegiada o confidencial y es para uso exclusivo de la persona o entidad de destino. Si no es usted. el destinatario indicado, queda notificado de que la lectura, utilizaciÃ³n, divulgaciÃ³n y/o copia sin autorizaciÃ³n puede estar prohibida en virtud de la legislaciÃ³n vigente. Si ha recibido este mensaje por error, le rogamos que nos lo comunique inmediatamente por esta misma vÃ­a y proceda a su destrucciÃ³n.

The information contained in this transmission is privileged and confidential information intended only for the use of the individual or entity named above. If the reader of this message is not the intended recipient, you are hereby notified that any dissemination, distribution or copying of this communication is strictly prohibited. If you have received this transmission in error, do not read it. Please immediately reply to the sender that you have received this communication in error and then delete it.

Esta mensagem e seus anexos se dirigem exclusivamente ao seu destinatÃ¡rio, pode conter informaÃ§Ã£o privilegiada ou confidencial e Ã© para uso exclusivo da pessoa ou entidade de destino. Se nÃ£o Ã© vossa senhoria o destinatÃ¡rio indicado, fica notificado de que a leitura, utilizaÃ§Ã£o, divulgaÃ§Ã£o e/ou cÃ³pia sem autorizaÃ§Ã£o pode estar proibida em virtude da legamos-lhe que nos o comunique imediatamente por esta mesma"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Mon, 21 Mar 2016 17:29:51 +0000",java.lang.OutOfMemoryError: Unable to acquire bytes of memory,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,
I am using 1.6.0 with dynamic allocation on yarn. I am trying to run a
relatively big application with 10s of jobs and 100K+ tasks and my app
fails with the exception below. The closest jira issue I could find is
SPARK-11293 <https://issues.apache.org/jira/browse/SPARK-11293>, which is a
critical bug that has been open for a long time. There are other similar
jira issues (all fixed): SPARK-10474
<https://issues.apache.org/jira/browse/SPARK-10474>, SPARK-10733
<https://issues.apache.org/jira/browse/SPARK-10733>, SPARK-10309
<https://issues.apache.org/jira/browse/SPARK-10309>, SPARK-10379
<https://issues.apache.org/jira/browse/SPARK-10379>.

Any workarounds to this issue or any plans to fix it?

Thanks a lot,
Nezih

16/03/19 05:12:09 INFO memory.TaskMemoryManager: Memory used in task
4687016/03/19 05:12:09 INFO memory.TaskMemoryManager: Acquired by
org.apache.spark.shuffle.sort.ShuffleExternalSorter@1c36f801: 32.0
KB16/03/19 05:12:09 INFO memory.TaskMemoryManager: 1512915599 bytes of
memory were used by task 46870 but are not associated with specific
consumers16/03/19 05:12:09 INFO memory.TaskMemoryManager: 1512948367
bytes of memory are used for execution and 156978343 bytes of memory
are used for storage16/03/19 05:12:09 ERROR executor.Executor: Managed
memory leak detected; size = 1512915599 bytes, TID = 4687016/03/19
05:12:09 ERROR executor.Executor: Exception in task 77.0 in stage
273.0 (TID 46870)
java.lang.OutOfMemoryError: Unable to acquire 128 bytes of memory, got 0
    at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:120)
    at org.apache.spark.shuffle.sort.ShuffleExternalSorter.acquireNewPageIfNecessary(ShuffleExternalSorter.java:354)
    at org.apache.spark.shuffle.sort.ShuffleExternalSorter.insertRecord(ShuffleExternalSorter.java:375)
    at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.insertRecordIntoSorter(UnsafeShuffleWriter.java:237)
    at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:164)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:89)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)16/03/19 05:12:09 ERROR
util.SparkUncaughtExceptionHandler: Uncaught exception in thread
Thread[Executor task launch worker-8,5,main]
java.lang.OutOfMemoryError: Unable to acquire 128 bytes of memory, got 0
    at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:120)
    at org.apache.spark.shuffle.sort.ShuffleExternalSorter.acquireNewPageIfNecessary(ShuffleExternalSorter.java:354)
    at org.apache.spark.shuffle.sort.ShuffleExternalSorter.insertRecord(ShuffleExternalSorter.java:375)
    at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.insertRecordIntoSorter(UnsafeShuffleWriter.java:237)
    at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:164)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
    at org.apache.spark.scheduler.Task.run(Task.scala:89)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)16/03/19 05:12:10 INFO
storage.DiskBlockManager: Shutdown hook called16/03/19 05:12:10 INFO
util.ShutdownHookManager: Shutdown hook called

â€‹
"
Andrew Or <andrew@databricks.com>,"Mon, 21 Mar 2016 10:32:48 -0700",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,Nezih Yigitbasi <nyigitbasi@netflix.com.invalid>,"@Nezih, can you try again after setting `spark.memory.useLegacyMode` to
true? Can you still reproduce the OOM that way?

2016-03-21 10:29 GMT-07:00 Nezih Yigitbasi <nyigitbasi@netflix.com.invalid>:

r
016/03/19 05:12:09 INFO memory.TaskMemoryManager: Acqu"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Mon, 21 Mar 2016 17:38:25 +0000",RE: MLPC model can not be saved,"HanPan <panda@thinkingdata.cn>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hi Pan,

There is a pull request that is supposed to fix the issue:
https://github.com/apache/spark/pull/9854

There is a workaround for saving/loading a model (however I am not sure if it will work for the pipeline):
sc.parallelize(Seq(model), 1).saveAsObjectFile(""path"")
val sameModel = sc.objectFile[YourCLASS](""path"").first()


Best regards, Alexander

From: HanPan [mailto:panda@thinkingdata.cn]
Sent: Sunday, March 20, 2016 8:32 PM
To: dev@spark.apache.org
Cc: panda@thinkingdata.cn
Subject: MLPC model can not be saved


Hi Guys,

         I built a ML pipeline that includes multilayer perceptron classifier, I got the following error message when I tried to save the pipeline model. It seems like MLPC model can not be saved which means I have no ways to save the trained model. Is there any way to save the model that I can use it for future prediction.

         Exception in thread ""main"" java.lang.UnsupportedOperationException: Pipeline write will fail on this Pipeline because it contains a stage which does not implement Writable. Non-Writable stage: mlpc_2d8b74f6da60 of type class org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel
         at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply(Pipeline.scala:218)
         at org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply(Pipeline.scala:215)
         at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
         at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
         at org.apache.spark.ml.Pipeline$SharedReadWrite$.validateStages(Pipeline.scala:215)
         at org.apache.spark.ml.PipelineModel$PipelineModelWriter.<init>(Pipeline.scala:325)
         at org.apache.spark.ml.PipelineModel.write(Pipeline.scala:309)
         at org.apache.spark.ml.util.MLWritable$class.save(ReadWrite.scala:130)
         at org.apache.spark.ml.PipelineModel.save(Pipeline.scala:280)
         at cn.thinkingdata.nlp.spamclassifier.FFNNSpamClassifierPipeLine.main(FFNNSpamClassifierPipeLine.java:76)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
         at java.lang.reflect.Method.invoke(Method.java:497)
         at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
         at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
         at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
         at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
         at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

Thanks
Pan
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 21 Mar 2016 17:43:02 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Jakob Odersky <jakob@odersky.com>,"Is someone going to retry fixing these packages? It's still a problem.

Also, it would be good to understand why this is happening.


:
.tgz
.tgz
tting the
.tgz
.tgz
"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Mon, 21 Mar 2016 21:22:20 +0300",Re: SparkML algos limitations question.,Joseph Bradley <joseph@databricks.com>,"Hi, Joseph,

I thought I understood, why it has a limit of 30 levels for decision tree,
but now I'm not that sure. I thought that's because the decision tree
stored in the array, which has length of type int, which cannot be more,
than 2^31-1.
But here are my new discoveries. I've trained two different random forest
models of 50 trees and different maxDepth (20 and 30) and specified node
size = 5. Here are couple of those trees

Model with maxDepth = 20:
    depth=20, numNodes=471
    depth=19, numNodes=497

Model with maxDepth = 30:
    depth=30, numNodes=11347
    depth=30, numNodes=10963

It looks like the tree is not pretty balanced and I understand why that
happens, but I'm surprised that actual number of nodes way less, than 2^31
- 1. And now I'm not sure of why the limitation actually exists. With tree
that consist of 2^31 nodes it'd required to have 8G of memory just to store
those indexes, so I'd say that depth isn't the biggest issue in such a
case.

Is it possible to workaround or simply miss maxDepth limitation (without
codebase modification) to train the tree until I hit the max number of
nodes? I'd assume that in most cases I simply won't hit it, but the depth
of the tree would be much more, than 30.


--
Be well!
Jean Morozov


"
Joseph Bradley <joseph@databricks.com>,"Mon, 21 Mar 2016 11:53:29 -0700",Merging ML Estimator and Model,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Spark devs & users,

I want to bring attention to a proposal to merge the MLlib (spark.ml)
concepts of Estimator and Model in Spark 2.0.  Please comment & discuss on
SPARK-14033 <https://issues.apache.org/jira/browse/SPARK-14033> (not in
this email thread).

*TL;DR:*
*Proposal*: Merge Estimator and Model under a single abstraction
(Estimator).
*Goals*: Simplify API by combining the tightly coupled concepts of
Estimator & Model.  Match other ML libraries like scikit-learn.  Simplify
mutability semantics.

*Details*: See https://issues.apache.org/jira/browse/SPARK-14033 for a
design document (Google doc & PDF).

Thanks in advance for feedback!
Joseph
"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Mon, 21 Mar 2016 19:46:36 +0000",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,"Andrew Or <andrew@databricks.com>, Nezih Yigitbasi <nyigitbasi@netflix.com.invalid>","Andrew, thanks for the suggestion, but unfortunately it didn't work --
still getting the same exception.

d
7016/03/19 05:12:09 INFO memory.TaskMemoryManager: Acquired by org.apache.spark.shuffle.sort.ShuffleExternalSorter@1c36f801: 32.0 KB16/03/19 05:12:09 INFO memory.TaskMemoryManager: 1512915599 bytes of memory were used by task 46870 but are not associated with specific consumers16/03/19 05:12:09 INFO memory.TaskMemoryManager: 1512948367 bytes of memory are used for execution and 156978343 bytes of memory are used for storage16/03/19 05:12:09 ERROR executor.Executor: Managed memory leak detected; size = 1512915599 bytes, TID = 4687016/03/19 05:12:09 ERROR executor.Executor: Exception in task 77.0 in stage 273.0 (TID 46870)
r.java:120)
eIfNecessary(ShuffleExternalSorter.java:354)
ShuffleExternalSorter.java:375)
oSorter(UnsafeShuffleWriter.java:237)
ffleWriter.java:164)
scala:73)
scala:41)
213)
tor.java:1142)
utor.java:617)
.SparkUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-8,5,main]
r.java:120)
eIfNecessary(ShuffleExternalSorter.java:354)
ShuffleExternalSorter.java:375)
oSorter(UnsafeShuffleWriter.java:237)
ffleWriter.java:164)
scala:73)
scala:41)
213)
tor.java:1142)
utor.java:617)
ge.DiskBlockManager: Shutdown hook called16/03/19 05:12:10 INFO util.ShutdownHookManager: Shutdown hook called
"
Joseph Bradley <joseph@databricks.com>,"Mon, 21 Mar 2016 13:24:41 -0700",Re: SparkML algos limitations question.,Eugene Morozov <evgeny.a.morozov@gmail.com>,"The indexing I mentioned is more restrictive than that: each index
corresponds to a unique position in a binary tree.  (I.e., the first index
of row 0 is 1, the first of row 1 is 2, the first of row 2 is 4, etc., IIRC)

You're correct that this restriction could be removed; with some careful
thought, we could probably avoid using indices altogether.  I just created
https://issues.apache.org/jira/browse/SPARK-14043  to track this.


"
Wes McKinney <wes@cloudera.com>,"Mon, 21 Mar 2016 20:48:34 -0700",Re: PySpark API divergence + improving pandas interoperability,Reynold Xin <rxin@databricks.com>,"hi Reynold,

It's of course possible to find solutions to specific issues, but what
I'm curious about is a general decision-making framework around
building strong user experiences for programmers using each of the
Spark APIs. Right now, the semantics of using Spark are very tied to
the semantics of the Scala API. In turn, the semantics of Spark
DataFrames may be constrained by the semantics of Spark SQL, depending
on your attitude toward API divergence and using a sort of ""code
generation"" to produce a certain behavior.

To make this concrete, in this particular example, PySpark could
implicitly cast boolean to numeric (completely from the Python side)
before the SUM operation is sent to Spark. But then SUM(boolean) in
Scala and SUM(boolean) in Python now have distinct behavior. Now
imagine that you have made many such divergent API design decisions
(some of which may interact with each other). Another classic example
is preventing floor division with integers by automatically casting to
double (__truediv__ in Python).

Adding a different mode to Spark SQL might be dangerous, and it would
likely add a lot of complexity.

Thanks,
Wes


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 21 Mar 2016 22:38:42 -0700",Re: PySpark API divergence + improving pandas interoperability,Wes McKinney <wes@cloudera.com>,"Hi Wes,

I agree it is difficult to do this design case by case, but what I was
pointing out was ""it is difficult to generalize without seeing a lot more
cases"".

I do think we need to see a lot of these cases and then make a call. My
intuition is that we can just have config options that control behavior,
similar to what a lot of relational databases do. The good thing is that
Spark data frames are very abstracted away from the underlying execution so
a lot of the behaviors can be controlled just by analysis rules.




"
Kostas Sakellis <kostas@cloudera.com>,"Tue, 22 Mar 2016 00:27:19 -0700",SPARK-13843 Next steps,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello all,

I'd like to close out the discussion on SPARK-13843 by getting a poll from
the community on which components we should seriously reconsider re-adding
back to Apache Spark. For reference, here are the modules that were removed
as part of SPARK-13843 and pushed to: https://github.com/spark-packages

   - streaming-flume
   - streaming-akka
   - streaming-mqtt
   - streaming-zeromq
   - streaming-twitter

For us, we'd like to see the streaming-flume added back to Apache Spark.

Thanks,
Kostas
"
,"Tue, 22 Mar 2016 08:29:41 +0100",Re: SPARK-13843 Next steps,dev@spark.apache.org,"Thanks for the update Kostas,

for now, kafka stays in Spark and Kinesis will be removed, right ?

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 22 Mar 2016 00:30:42 -0700",Re: SPARK-13843 Next steps,,"Kinesis is still in it. I think it's OK to add Flume back.

t>

"
,"Tue, 22 Mar 2016 08:33:30 +0100",Re: SPARK-13843 Next steps,dev@spark.apache.org,"OK, so kafka, kinesis and flume will stay in Spark.

Thanks,
Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
james <yiazhou@gmail.com>,"Tue, 22 Mar 2016 01:07:38 -0700 (MST)",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,dev@spark.apache.org,"Hi,
I also found 'Unable to acquire memory' issue using Spark 1.6.1 with Dynamic
allocation on YARN. My case happened with setting
spark.sql.shuffle.partitions larger than 200. From error stack, it has a
diff with issue reported by Nezih and not sure if these has same root cause.

Thanks 
James

16/03/17 16:02:11 INFO spark.MapOutputTrackerMaster: Size of output statuses
for shuffle 0 is 1912805 bytes
16/03/17 16:02:12 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 1 to hw-node3:55062
16/03/17 16:02:12 INFO spark.MapOutputTrackerMaster: Size of output statuses
for shuffle 0 is 1912805 bytes
16/03/17 16:02:16 INFO scheduler.TaskSetManager: Starting task 280.0 in
stage 153.0 (TID 9390, hw-node5, partition 280,PROCESS_LOCAL, 2432 bytes)
16/03/17 16:02:16 WARN scheduler.TaskSetManager: Lost task 170.0 in stage
153.0 (TID 9280, hw-node5): java.lang.OutOfMemoryError: Unable to acquire
1073741824 bytes of memory, got 1060110796
	at
org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:91)
	at
org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.growPointerArrayIfNecessary(UnsafeExternalSorter.java:295)
	at
org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertRecord(UnsafeExternalSorter.java:330)
	at
org.apache.spark.sql.execution.UnsafeExternalRowSorter.insertRow(UnsafeExternalRowSorter.java:91)
	at
org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:168)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:90)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:64)
	at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
	at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$21.apply(RDD.scala:728)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at
org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:270)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:89)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)



--

---------------------------------------------------------------------


"
Nezih Yigitbasi <nyigitbasi@netflix.com.INVALID>,"Tue, 22 Mar 2016 08:23:05 +0000",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,"james <yiazhou@gmail.com>, dev@spark.apache.org","Interesting. After experimenting with various parameters increasing
spark.sql.shuffle.partitions and decreasing spark.buffer.pageSize helped my
job go through. BTW I will be happy to help getting this issue fixed.

Nezih


Hi,
d
91)
nterArrayIfNecessary(UnsafeExternalSorter.java:295)
ecord(UnsafeExternalSorter.java:330)
ternalRowSorter.java:91)
lRowSorter.java:168)
.apply(RDD.scala:728)
.apply(RDD.scala:728)
)
la:88)
)
)
)
)
)
)
:1145)
a:615)
MemoryError-Unable-to-acquire-bytes-of-memory-tp16773p16787.html
"
james <yiazhou@gmail.com>,"Tue, 22 Mar 2016 01:32:12 -0700 (MST)",Re: java.lang.OutOfMemoryError: Unable to acquire bytes of memory,dev@spark.apache.org,"I guess different workload cause diff result ?



--

---------------------------------------------------------------------


"
Allen <allenzhang010@126.com>,"Tue, 22 Mar 2016 18:07:16 +0800 (CST)",error occurs to compile spark 1.6.1 using scala 2.11.8,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am facing an error when doing compilation from IDEA, please see the attached. I fired the build process by clicking ""Rebuild Project"" in ""Build"" menu in IDEA IDE.

more info here:
Spark 1.6.1 + scala 2.11.8 + IDEA 15.0.3 + Maven 3.3.3

I can build spark 1.6.1 with scala 2.10.4 successfully in the same way.


Help!
BR,
Allen Zhang

---------------------------------------------------------------------"
Steve Loughran <stevel@hortonworks.com>,"Tue, 22 Mar 2016 10:29:37 +0000","Re: Can we remove private[spark] from Metrics Source and SInk
 traits?",Pete Robbins <robbinspg@gmail.com>,"


There are several open Jiras to add new Sinks

OpenTSDB https://issues.apache.org/jira/browse/SPARK-12194
StatsD https://issues.apache.org/jira/browse/SPARK-11574


statsd is nicely easy to test: either listen in on a (localhost, port) or simply create a socket and force it into the sink for the test run


Kafka https://issues.apache.org/jira/browse/SPARK-13392

Some have PRs from 2015 so I'm assuming there is not the desire to integrate these into core Spark. Opening up the Sink/Source interfaces would at least allow these to exist somewhere such as spark-packages without having to pollute the o.a.s namespace



+1

This seems to me to be unnecessarily restrictive. These are very useful extension points for adding 3rd party sources and sinks.

I intend to make an Elasticsearch sink available on spark-packages but this will require a single class, the sink, to be in the org.apache.spark package tree. I could submit the package as a PR to the Spark codebase, and I'd be happy to do that but it could be a completely separate add-on.

There are similar issues with writing a 3rd party metrics source which may not be of interest to the community at large so would probably not warrant inclusion in the Spark codebase.

Any thoughts?

"
Rishi Mishra <rmishra@snappydata.io>,"Tue, 22 Mar 2016 17:27:42 +0530",StatefulNetworkWordCount behaviour,dev@spark.apache.org,"I am trying out StatefulNetworkWordCount from latest Spark master branch.
When I run this example I see a odd behaviour.
If in a batch a key is repeated the output stream prints for each
repetition e.g.  If I key in ""ab"" five times for input it will show like

(ab,1)
(ab,2)
(ab,3)
(ab,4)
(ab,5)

Is it the intended behaviour to show all the occurrence of the word, or is
it a bug ? If I am a user I would expect only the last entry (ab, 5) . Else
users has to put some logic in application code to get to the latest
value.  I know we can do this by snapshot, but IMO the updated stream
should give us similar functionality.

Is there a reason for not doing this ? i.e. for a given key if multiple
output is generated , only the last one should be returned back.

Regards,
Rishitesh Mishra,
SnappyData . (http://www.snappydata.io/)

https://in.linkedin.com/in/rishiteshmishra
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 22 Mar 2016 05:02:03 -0700",Re: error occurs to compile spark 1.6.1 using scala 2.11.8,Allen <allenzhang010@126.com>,"around.

FYI maven 3.3.9 is required for master branch.


"
Josh Levy-Kramer <josh@starcount.com>,"Tue, 22 Mar 2016 12:40:12 +0000",toPandas very slow,dev@spark.apache.org,"Hi,

A common pattern in my work is querying large tables in Spark DataFrames
and then needing to do more detailed analysis locally when the data can fit
into memory. However, i've hit a few blockers. In Scala no well developed
DataFrame library exists and in Python the `toPandas` function is very
slow. As Pandas is one of the best DataFrame libraries out there is may be
worth spending some time into making the `toPandas` method more efficient.

Having a quick look at the code it looks like a lot of iteration is
occurring on the Python side. Python is really slow at iterating over large
loop and this should be avoided. If iteration does have to occur its best
done in Cython. Has anyone looked at Cythonising the process? Or even
better serialising directly to Numpy arrays instead of the intermediate
lists of Rows.

Here are some links to the current code:

topandas:
https://github.com/apache/spark/blob/8e0b030606927741f91317660cd14a8a5ed6e5f9/python/pyspark/sql/dataframe.py#L1342

collect:
https://github.com/apache/spark/blob/8e0b030606927741f91317660cd14a8a5ed6e5f9/python/pyspark/sql/dataframe.py#L233

_load_from_socket:
https://github.com/apache/spark/blob/a60f91284ceee64de13f04559ec19c13a820a133/python/pyspark/rdd.py#L123

Josh Levy-Kramer
Data Scientist @ Starcount
"
hansbogert <hansbogert@gmail.com>,"Tue, 22 Mar 2016 05:41:44 -0700 (MST)",Job description only visible after job finish,dev@spark.apache.org,"Hi, 

Iâ€™m trying to do some dynamic scheduling by an external application by
looking at the jobs in a Spark framework.

I need the job description to know which kind of query Iâ€™m dealing with. The
problem is that the job description (set with: sparkCtx.setJobDescription)
but in case of a job with multiple stages, the job description is not seen
in the UI, and more importantly in my case, is not retrievable in the API
endpoint of `host:api/v1/application/x/jobs`

Is there a reason why the job description is not shown for non-finished,
multi-stage, jobs?

Further info: Spark 1.5.1, the jobs for now are sent ad-hoc through the
spark-shell.

Thanks, 

Hans



--
3.nabble.com/Job-description-only-visible-after-job-finish-tp16795.html
om.

---------------------------------------------------------------------


"
"""Gil Vernik"" <GILV@il.ibm.com>","Tue, 22 Mar 2016 15:34:18 +0200",new object store driver for Spark,Dev <dev@spark.apache.org>,"We recently released an object store connector for Spark. 
https://github.com/SparkTC/stocator
Currently this connector contains driver for the Swift based object store 
( like SoftLayer or any other Swift cluster ), but it can easily support 
additional object stores.
There is a pending patch to support Amazon S3 object store. 

The major highlights that this connector doesn't create any temporary 
files  and so it achieves very fast response times when Spark persist data 
in the object store.
The new connector supports speculate mode and covers various failure 
scenarios ( like two Spark tasks writing into same object, partial 
corrupted data due to run time exceptions in Spark master, etc ).  It also 
covers https://issues.apache.org/jira/browse/SPARK-10063 and other known 
issues.

The detail algorithm for fault tolerance will be released very soon. For 
now, those who interested, can view the implementation in the code itself.

 https://github.com/SparkTC/stocator contains all the details how to setup 
and use with Spark.

A series of tests showed that the new connector obtains 70% improvements 
for write operations from Spark to Swift and about 30% improvements for 
read operations from Swift into Spark ( comparing to the existing driver 
that Spark uses to integrate with objects stored in Swift). 

There is an ongoing work to add more coverage and fix some known bugs / 
limitations.

All the best
Gil



"
Mark Vervuurt <m.a.vervuurt@gmail.com>,"Tue, 22 Mar 2016 15:11:22 +0100",Re: toPandas very slow,Josh Levy-Kramer <josh@starcount.com>,"Hi Josh,

The work around we figured out to solve network latency and out of memory problems with the toPandas method was to create Pandas DataFrames or Numpy Arrays using MapPartitions for each partition. Maybe a standard solution around this line of thought could be built. The integration is quite tedious ;)

I hope this helps.

Regards,
Mark

DataFrames and then needing to do more detailed analysis locally when the data can fit into memory. However, i've hit a few blockers. In Scala no well developed DataFrame library exists and in Python the `toPandas` function is very slow. As Pandas is one of the best DataFrame libraries out there is may be worth spending some time into making the `toPandas` method more efficient.
occurring on the Python side. Python is really slow at iterating over large loop and this should be avoided. If iteration does have to occur its best done in Cython. Has anyone looked at Cythonising the process? Or even better serialising directly to Numpy arrays instead of the intermediate lists of Rows.
https://github.com/apache/spark/blob/8e0b030606927741f91317660cd14a8a5ed6e5f9/python/pyspark/sql/dataframe.py#L1342 <https://github.com/apache/spark/blob/8e0b030606927741f91317660cd14a8a5ed6e5f9/python/pyspark/sql/dataframe.py#L1342>
https://github.com/apache/spark/blob/8e0b030606927741f91317660cd14a8a5ed6e5f9/python/pyspark/sql/dataframe.py#L233 <https://github.com/apache/spark/blob/8e0b030606927741f91317660cd14a8a5ed6e5f9/python/pyspark/sql/dataframe.py#L233>
https://github.com/apache/spark/blob/a60f91284ceee64de13f04559ec19c13a820a133/python/pyspark/rdd.py#L123 <https://github.com/apache/spark/blob/a60f91284ceee64de13f04559ec19c13a820a133/python/pyspark/rdd.py#L123>

"
Wes McKinney <wes@cloudera.com>,"Tue, 22 Mar 2016 07:55:18 -0700",Re: toPandas very slow,Mark Vervuurt <m.a.vervuurt@gmail.com>,"hi all,

I recently did an analysis of the performance of toPandas

summary: http://wesmckinney.com/blog/pandas-and-apache-arrow/
ipython notebook: https://gist.github.com/wesm/0cb5531b1c2e346a0007

Spark DataFrames, with an optimized (C++ / Cython) conversion to a
pandas.DataFrame on the Python side:

https://issues.apache.org/jira/browse/SPARK-13534

I'm happy to discuss in more detail with those interested. The basic
idea is that deserializing binary data directly into NumPy arrays is
what you want, but you need some array-oriented / columnar memory
representation to push over the wire. Apache Arrow is designed
specifically for this use case.

best,
Wes


---------------------------------------------------------------------


"
Josh Levy-Kramer <josh@starcount.com>,"Tue, 22 Mar 2016 16:06:59 +0000",Re: toPandas very slow,dev@spark.apache.org,"Hi all,

Wez, I read your thread earlier today after I sent this message and its
exciting someone of your caliber working on the issue :)

For a short term solution i've created a Gist which performs the toPandas
operation using the mapPartitions method suggested by Mark:
https://gist.github.com/joshlk/871d58e01417478176e7

Regards,
Josh





-- 


*Josh Levy-Kramer*Data Scientist


+44 (0) 203 770 7554
+44 (0) 781 797 0736
Henry Wood House, 2 Riding House Street, W1W 7FA London

www.starcount.com

*Confidentiality*

The information contained in this e-mail is confidential, may be privileged
and is intended solely for the use of the named addressee. Access to this
e-mail by any other person is not authorised. If you are not the intended
recipient, you should not disclose, copy, distribute, take any action or
rely on it and you should please notify the sender by reply. Any opinions
expressed are not necessarily those of the company.
We may monitor all incoming and outgoing emails in line with current
legislation. We have taken steps to ensure that this email and attachments
are free from any virus, but it remains your responsibility to ensure that
viruses do not adversely affect you.
"
DavidFallside <david@fallside.com>,"Tue, 22 Mar 2016 09:46:46 -0700 (MST)","EclairJS for ""Powered by Spark"" Wiki page",dev@spark.apache.org,"Can someone please post the following information on the ""Powered by Spark""
wiki pages, thank you.

Organization: IBM www.ibm.com/spark
 
Project URL: https://github.com/EclairJS/eclairjs-node
 
Brief project description: EclairJS enables Node.js developers to code
against Spark, and data scientists to use Javascript in Jupyter notebooks.



--

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 22 Mar 2016 10:20:21 -0700",Re: SPARK-13843 Next steps,Kostas Sakellis <kostas@cloudera.com>,"+1 for getting flume back.




-- 
Marcelo

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Tue, 22 Mar 2016 12:26:29 -0500",Re: SPARK-13843 Next steps,Marcelo Vanzin <vanzin@cloudera.com>,"I'm in favor of everything in /extras and /external being removed, but
I'm more in favor of making a decision and moving on.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 23 Mar 2016 12:59:57 +0100","Re: EclairJS for ""Powered by Spark"" Wiki page",DavidFallside <david@fallside.com>,"Added to https://cwiki.apache.org/confluence/display/SPARK/Supplemental+Spark+Projects

I'm not clear what our criteria are for being added as an org, vs
project, vs posting projects on spark-packages.org. Should this page
actually go away in favor of spark-packages.org?

The wiki doesn't do harm, just is a bit duplicative, needs updating,
and might be viewed as selectively emphasizing a small number of
projects.


---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Wed, 23 Mar 2016 17:20:03 -0400",spark 2.0 snapshot change in RowEncoder behavior,"""dev@spark.apache.org"" <dev@spark.apache.org>","one of our unit tests broke with changes in spark 2.0 snapshot in last few
days (or maybe i simple missed it longer). i think it boils down to this:

val df1 = sc.makeRDD(1 to 3).toDF
val df2 = df1.map(row => Row(row(0).asInstanceOf[Int] +
1))(RowEncoder(df1.schema))
println(s""schema before ${df1.schema} and after ${df2.schema}"")

i get:
schema before StructType(StructField(value,IntegerType,false)) and after
StructType(StructField(value,IntegerType,true))

it is the change in nullability that i did not expect.
"
Reynold Xin <rxin@databricks.com>,"Wed, 23 Mar 2016 21:09:48 -0700",Re: spark 2.0 snapshot change in RowEncoder behavior,Koert Kuipers <koert@tresata.com>,"Can you see if this is the patch that caused the issue?

https://github.com/apache/spark/pull/11737




"
Jacek Laskowski <jacek@japila.pl>,"Thu, 24 Mar 2016 07:06:32 +0100",[ml] Two ClassificationModels are final and two are not - why?,dev <dev@spark.apache.org>,"Hi,

While reviewing ClassificationModel custom implementations, I found
that out of 4 ""production"" models two are final while the other two
are not. Is there any reason for this?

** `DecisionTreeClassificationModel` (`final`)
** `RandomForestClassificationModel` (`final`)
** `LogisticRegressionModel`
** `NaiveBayesModel`

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
"""HanPan"" <panda@thinkingdata.cn>","Thu, 24 Mar 2016 15:15:34 +0800",=?gb2312?B?tPC4tDogTUxQQyBtb2RlbCBjYW4gbm90IGJlIHNhdmVk?=,"""'Ulanov, Alexander'"" <alexander.ulanov@hpe.com>,
	<dev@spark.apache.org>","Hi Alexander,

 

         Thanks for your reply. The pull request shows that
MultilayerPerceptronClassifier implement default params writable interface.
I will try that.

 

Thanks

Pan

 

·¢¼þÈË: Ulanov, Alexander [mailto:alexander.ulanov@hpe.com] 
·¢ËÍÊ±¼ä: 2016Äê3ÔÂ22ÈÕ 1:38
ÊÕ¼þÈË: HanPan; dev@spark.apache.org
Ö÷Ìâ: RE: MLPC model can not be saved

 

Hi Pan,

 

There is a pull request that is supposed to fix the issue:

https://github.com/apache/spark/pull/9854

 

There is a workaround for saving/loading a model (however I am not sure if
it will work for the pipeline): 

sc.parallelize(Seq(model), 1).saveAsObjectFile(""path"")

val sameModel = sc.objectFile[YourCLASS](""path"").first()

 

 

Best regards, Alexander

 

From: HanPan [mailto:panda@thinkingdata.cn] 
Sent: Sunday, March 20, 2016 8:32 PM
To: dev@spark.apache.org <mailto:dev@spark.apache.org> 
Cc: panda@thinkingdata.cn <mailto:panda@thinkingdata.cn> 
Subject: MLPC model can not be saved

 

 

Hi Guys,

 

         I built a ML pipeline that includes multilayer perceptron
classifier, I got the following error message when I tried to save the
pipeline model. It seems like MLPC model can not be saved which means I have
no ways to save the trained model. Is there any way to save the model that I
can use it for future prediction.

 

         Exception in thread ""main"" java.lang.UnsupportedOperationException:
Pipeline write will fail on this Pipeline because it contains a stage which
does not implement Writable. Non-Writable stage: mlpc_2d8b74f6da60 of type
class
org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel

         at
org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply
(Pipeline.scala:218)

         at
org.apache.spark.ml.Pipeline$SharedReadWrite$$anonfun$validateStages$1.apply
(Pipeline.scala:215)

         at
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala
:33)

         at
scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)

         at
org.apache.spark.ml.Pipeline$SharedReadWrite$.validateStages(Pipeline.scala:
215)

         at
org.apache.spark.ml.PipelineModel$PipelineModelWriter.<init>(Pipeline.scala:
325)

         at org.apache.spark.ml.PipelineModel.write(Pipeline.scala:309)

         at
org.apache.spark.ml.util.MLWritable$class.save(ReadWrite.scala:130)

         at org.apache.spark.ml.PipelineModel.save(Pipeline.scala:280)

         at
cn.thinkingdata.nlp.spamclassifier.FFNNSpamClassifierPipeLine.main(FFNNSpamC
lassifierPipeLine.java:76)

         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)

         at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62
)

         at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl
.java:43)

         at java.lang.reflect.Method.invoke(Method.java:497)

         at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$ru
nMain(SparkSubmit.scala:731)

         at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)

         at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)

         at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)

         at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

 

Thanks

Pan

"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Mar 2016 00:27:45 -0700",[discuss] ending support for Java 7 in Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","About a year ago we decided to drop Java 6 support in Spark 1.5. I am
wondering if we should also just drop Java 7 support in Spark 2.0 (i.e.
Spark 2.0 would require Java 8 to run).

Oracle ended public updates for JDK 7 in one year ago (Apr 2015), and
removed public downloads for JDK 7 in July 2015. In the past I've actually
been against dropping Java 8, but today I ran into an issue with the new
Dataset API not working well with Java 8 lambdas, and that changed my
opinion on this.

I've been thinking more about this issue today and also talked with a lot
people offline to gather feedback, and I actually think the pros outweighs
the cons, for the following reasons (in some rough order of importance):

1. It is complicated to test how well Spark APIs work for Java lambdas if
we support Java 7. Jenkins machines need to have both Java 7 and Java 8
installed and we must run through a set of test suites in 7, and then the
lambda tests in Java 8. This complicates build environments/scripts, and
makes them less robust. Without good testing infrastructure, I have no
confidence in building good APIs for Java 8.

2. Dataset/DataFrame performance will be between 1x to 10x slower in Java
7. The primary APIs we want users to use in Spark 2.x are
Dataset/DataFrame, and this impacts pretty much everything from machine
learning to structured streaming. We have made great progress in their
performance through extensive use of code generation. (In many dimensions
Spark 2.0 with DataFrames/Datasets looks more like a compiler than a
MapReduce or query engine.) These optimizations don't work well in Java 7
due to broken code cache flushing. This problem has been fixed by Oracle in
Java 8. In addition, Java 8 comes with better support for Unsafe and SIMD.

3. Scala 2.12 will come out soon, and we will want to add support for that.
Scala 2.12 only works on Java 8. If we do support Java 7, we'd have a
fairly complicated compatibility matrix and testing infrastructure.

4. There are libraries that I've looked into in the past that support only
Java 8. This is more common in high performance libraries such as Aeron (a
messaging library). Having to support Java 7 means we are not able to use
these. It is not that big of a deal right now, but will become increasingly
more difficult as we optimize performance.


The downside of not supporting Java 7 is also obvious. Some organizations
are stuck with Java 7, and they wouldn't be able to use Spark 2.0 without
upgrading Java.
"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Mar 2016 00:36:58 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","8's Optional class to replace our built-in Optional.



"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Thu, 24 Mar 2016 07:42:18 +0000",RE: [discuss] ending support for Java 7 in Spark 2.0,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Very good points

Going to support java 8 looks like a good direction
2.0 would be a good release to start with that

Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569om]
Sent: Thursday, March 24, 2016 9:37 AM
To: dev@spark.apache.org
Subject: Re: [discuss] ending support for Java 7 in Spark 2.0

One other benefit that I didn't mention is that we'd be able to use Java 8's Optional class to replace our built-in Optional.


On Thu, Mar 24, 2016 at 12:27 AM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:
About a year ago we decided to drop Java 6 support in Spark 1.5. I am wondering if we should also just drop Java 7 support in Spark 2.0 (i.e. Spark 2.0 would require Java 8 to run).

Oracle ended public updates for JDK 7 in one year ago (Apr 2015), and removed public downloads for JDK 7 in July 2015. In the past I've actually been against dropping Java 8, but today I ran into an issue with the new Dataset API not working well with Java 8 lambdas, and that changed my opinion on this.

I've been thinking more about this issue today and also talked with a lot people offline to gather feedback, and I actually think the pros outweighs the cons, for the following reasons (in some rough order of importance):

1. It is complicated to test how well Spark APIs work for Java lambdas if we support Java 7. Jenkins machines need to have both Java 7 and Java 8 installed and we must run through a set of test suites in 7, and then the lambda tests in Java 8. This complicates build environments/scripts, and makes them less robust. Without good testing infrastructure, I have no confidence in building good APIs for Java 8.

2. Dataset/DataFrame performance will be between 1x to 10x slower in Java 7. The primary APIs we want users to use in Spark 2.x are Dataset/DataFrame, and this impacts pretty much everything from machine learning to structured streaming. We have made great progress in their performance through extensive use of code generation. (In many dimensions Spark 2.0 with DataFrames/Datasets looks more like a compiler than a MapReduce or query engine.) These optimizations don't work well in Java 7 due to broken code cache flushing. This problem has been fixed by Oracle in Java 8. In addition, Java 8 comes with better support for Unsafe and SIMD.

3. Scala 2.12 will come out soon, and we will want to add support for that. Scala 2.12 only works on Java 8. If we do support Java 7, we'd have a fairly complicated compatibility matrix and testing infrastructure.

4. There are libraries that I've looked into in the past that support only Java 8. This is more common in high performance libraries such as Aeron (a messaging library). Having to support Java 7 means we are not able to use these. It is not that big of a deal right now, but will become increasingly more difficult as we optimize performance.


The downside of not supporting Java 7 is also obvious. Some organizations are stuck with Java 7, and they wouldn't be able to use Spark 2.0 without upgrading Java.



"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 24 Mar 2016 00:47:30 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"+1
Agree, dropping support for java 7 is long overdue - and 2.0 would be
a logical release to do this on.

Regards,
Mridul



---------------------------------------------------------------------


"
Ram Sriharsha <sriharsha.ram@gmail.com>,"Thu, 24 Mar 2016 00:48:52 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"+1, yes Java 7 has been end of life for a year now, 2.0 is a good time to
upgrade to Java 8





-- 
Ram Sriharsha
Architect, Spark and Data Science
Hortonworks, 2550 Great America Way, 2nd Floor
Santa Clara, CA 95054
Ph: 408-510-8635
email: harsha@apache"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Mar 2016 09:01:42 +0100",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"I generally favor this for the simplification. I didn't realize there
were actually some performance wins and important bug fixes.

I've had lots of trouble with scalac 2.10 + Java 8. I don't know if
it's still a problem since 2.11 + 8 seems OK, but for a long time the
sql/ modules would never compile in this config. If it's actually
required for 2.12, makes sense.

As ever my general stance is that nobody has to make a major-version
upgrade; Spark 1.6 does not stop working for those that need Java 7. I
also think it's reasonable for anyone to expect that major-version
upgrades require major-version dependency updates. Also remember that
not removing Java 7 support means committing to it here for a couple
more years. It's not just about the situation on release day.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Mar 2016 01:04:16 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Sean Owen <sowen@cloudera.com>,"I actually talked quite a bit today with an engineer on the scala compiler
team tonight and the scala 2.10 + java 8 combo should be ok. The latest
Scala 2.10 release should have all the important fixes that are needed for
Java 8.


"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Mar 2016 10:29:14 +0100",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"Maybe so; I think we have a ticket open to update to 2.10.6, which
maybe fixes it.

It brings up a different point: supporting multiple Scala versions is
much more painful than Java versions because of mutual
incompatibility. Right now I get the sense there's an intent to keep
supporting 2.10, and 2.11, and 2.12 later in Spark 2. This seems like
relatively way more trouble. In the same breath -- why not remove 2.10
support anyway? It's also EOL, 2.11 also brought big improvements,
etc.


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 24 Mar 2016 12:41:13 +0000",Re: [discuss] ending support for Java 7 in Spark 2.0,,"
dering if we should also just drop Java 7 support in Spark 2.0 (i.e. Spark 2.0 would require Java 8 to run).
oved public downloads for JDK 7 in July 2015.

Still there, Jan 2016 was the last public one.

nto an issue with the new Dataset API not working well with Java 8 lambdas, and that changed my opinion on this.
 people offline to gather feedback, and I actually think the pros outweighs the cons, for the following reasons (in some rough order of importance):
 we support Java 7. Jenkins machines need to have both Java 7 and Java 8 installed and we must run through a set of test suites in 7, and then the lambda tests in Java 8. This complicates build environments/scripts, and makes them less robust. Without good testing infrastructure, I have no confidence in building good APIs for Java 8.

+complicates the test matrix for problems: if something works on java 8 and fails on java 7, is that a java 8 problem or a java 7 one?
+most developers would want to be on java 8 on their desktop if they could; the risk is that people accidentally code for java 8 even if they don't realise it just by using java 8 libraries, etc

 7. The primary APIs we want users to use in Spark 2.x are Dataset/DataFrame, and this impacts pretty much everything from machine learning to structured streaming. We have made great progress in their performance through extensive use of code generation. (In many dimensions Spark 2.0 with DataFrames/Datasets looks more like a compiler than a MapReduce or query engine.) These optimizations don't work well in Java 7 due to broken code cache flushing. This problem has been fixed by Oracle in Java 8. In addition, Java 8 comes with better support for Unsafe and SIMD.
t. Scala 2.12 only works on Java 8. If we do support Java 7, we'd have a fairly complicated compatibility matrix and testing infrastructure.
y Java 8. This is more common in high performance libraries such as Aeron (a messaging library). Having to support Java 7 means we are not able to use these. It is not that big of a deal right now, but will become increasingly more difficult as we optimize performance.
 are stuck with Java 7, and they wouldn't be able to use Spark 2.0 without upgrading Java.


want to upgrade to java 8 want to be upgrading to spark 2.0 anyway? 

If there is a price, it means all apps that use any remote Spark APIs will also have to be java 8. Something like a REST API is less of an issue, but anything loading an JAR in the group org.apache.spark will have to be Java 8+. That's what held hadoop back on Java 7 in 2015 : twitter made the case that it shouldn't be the hadoop cluster forcing them to upgrade all their client apps just to use the IPC and filesystem code.I don't believe that's so much of a constraint on Spark.

Finally, Java 8 lines you up better for worrying about Java 9, which is on the horizon.

---------------------------------------------------------------------


"
,"Thu, 24 Mar 2016 13:52:20 +0100",Re: [discuss] ending support for Java 7 in Spark 2.0,dev@spark.apache.org,"+1 to support Java 8 (and future) *only* in Spark 2.0, and end support 
of Java 7. It makes sense.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

-------------------------------------------------------------"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 24 Mar 2016 14:25:57 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Jakob Odersky <jakob@odersky.com>,"Just checking in on this again as the builds on S3 are still broken. :/

Could it have something to do with us moving release-build.sh
<https://github.com/apache/spark/commits/master/dev/create-release/release-build.sh>
?
â€‹

m>

:
6.tgz
6.tgz
etting the
4.tgz
6.tgz
"
Koert Kuipers <koert@tresata.com>,"Thu, 24 Mar 2016 11:27:38 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,,"i think the arguments are convincing, but it also makes me wonder if i live
in some kind of alternate universe... we deploy on customers clusters,
where the OS, python version, java version and hadoop distro are not chosen
by us. so think centos 6, cdh5 or hdp 2.3, java 7 and python 2.6. we simply
have access to a single proxy machine and launch through yarn. asking them
to upgrade java is pretty much out of the question or a 6+ month ordeal. of
the 10 client clusters i can think of on the top of my head all of them are
on java 7, none are on java 8. so by doing this you would make spark 2
basically unusable for us (unless most of them have plans of upgrading in
near term to java 8, i will ask around and report back...).

on a side note, its particularly interesting to me that spark 2 chose to
continue support for scala 2.10, because even for us in our very
constricted client environments the scala version is something we can
easily upgrade (we just deploy a custom build of spark for the relevant
scala version and hadoop distro). and because scala is not a dependency of
any hadoop distro (so not on classpath, which i am very happy about) we can
use whatever scala version we like. also i found the upgrade path from
scala 2.10 to 2.11 to be very easy, so i have a hard time understanding why
anyone would stay on scala 2.10. and finally with scala 2.12 around the
corner you really dont want to be supporting 3 versions. so clearly i am
missing something here.




e.
"
Sean Owen <sowen@cloudera.com>,"Thu, 24 Mar 2016 17:25:52 +0100",Re: [discuss] ending support for Java 7 in Spark 2.0,Koert Kuipers <koert@tresata.com>,"(PS CDH5 runs fine with Java 8, but I understand your more general point.)

This is a familiar context indeed, but in that context, would a group
not wanting to update to Java 8 want to manually put Spark 2.0 into
the mix? That is, if this is a context where the cluster is
purposefully some stable mix of components, would you be updating just
one?

You make a good point about Scala being more library than
the one hand it's harder to handle different Scala versions from the
framework side, it's less hard on the deployment side.

ve
ere
us.
the
n
ted
(we
o
ala
 be
et>
f
a
,
r
d

---------------------------------------------------------------------


"
Al Pivonka <alpivonka@gmail.com>,"Thu, 24 Mar 2016 12:38:35 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Sean Owen <sowen@cloudera.com>,"As an end user (developer) and Cluster Admin.
I would have to agree with Koet.

To me the real question is timing,  current version is 1.6.1, the question
I have is how many more releases till 2.0 and what is the time frame?

If you give people six to twelve months to plan and make sure they know
(paste it all over the web site) most can plan ahead.


Just my two pennies






)
y
ve
o
f
in
o
e
nt
e.
.net>
e.
at
s
en
ve
ne
r
l
r
s
t


-- 
Those who say it can't be done, are usually interrupted by those doing it.
"
Koert Kuipers <koert@tresata.com>,"Thu, 24 Mar 2016 12:39:05 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Sean Owen <sowen@cloudera.com>,"The group will not upgrade to spark 2.0 themselves, but they are mostly
fine with vendors like us deploying our application via yarn with whatever
spark version we choose (and bundle, so they do not install it separately,
they might not even be aware of what spark version we use). This all works
because spark does not need to be on the cluster nodes, just on the one
machine where our application gets launched. Having yarn is pretty awesome
in this respect.


)
y
ve
o
f
in
o
e
nt
e.
.net>
e.
at
s
en
ve
ne
r
l
r
s
t
"
,"Thu, 24 Mar 2016 17:40:12 +0100",Re: [discuss] ending support for Java 7 in Spark 2.0,dev@spark.apache.org,"Hi Al,

Spark 2.0 doesn't mean Spark 1.x will stop. Clearly, new features will 
go on Spark 2.0, but maintenance release can be performed on 1.x branch.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Al Pivonka <alpivonka@gmail.com>,"Thu, 24 Mar 2016 12:41:05 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,,"Thank you for the context Jean...
I appreciate it...


t>

p
st
n
 2
ve
-
-
t.


-- 
Those who say it can't be done, are usually interrupted by those doing it.
"
Koert Kuipers <koert@tresata.com>,"Thu, 24 Mar 2016 12:54:49 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Sean Owen <sowen@cloudera.com>,"i guess what i am saying is that in a yarn world the only hard restrictions
left are the the containers you run in, which means the hadoop version,
java version and python version (if you use python).



r
,
s
e
.)
:
to
e
to
n
x.net>
t
m
d
a
f
as
ir
ll
or
r
t
as
ot
e
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 24 Mar 2016 10:06:23 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"
So, do you actually get the benefits you're looking for without
compiling explicitly to the 1.8 jvm? Because:

$ scala -version
Scala code runner version 2.10.6 -- Copyright 2002-2013, LAMP/EPFL
$ scalac -target jvm-1.8
scalac error: Usage: -target:<target>
 where <target> choices are jvm-1.5, jvm-1.5-fjbg, jvm-1.5-asm,
jvm-1.6, jvm-1.7, msil (default: jvm-1.6)

So even if you use jdk 8 to compile with scala 2.10, you can't target
jvm 1.8 as far as I can tell.

-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Mar 2016 10:13:28 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"Yes


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 24 Mar 2016 10:13:45 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Koert Kuipers <koert@tresata.com>,"
It is theoretically possible to run containers with a different JDK
than the NM (I've done it for testing), although I'm not sure about
whether that's recommended from YARN's perspective.

But I understand your concern is that you're not allowed to modify the
machines where the NMs are hosted. You could hack things and
distribute the JVM with your Spark application, but that would be
incredibly ugly.

-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 24 Mar 2016 10:44:46 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"
So is it safe to say the only hard requirements for Java 8 in your list is (4)?

(1) and (3) are infrastructure issues. Yes, it sucks to maintain more
testing infrastructure and potentially more complicated build scripts,
but does that really outweigh maintaining support for Java 7?

A cheap hack would also be to require jdk 1.8 for the build, but still
target java 7. You could then isolate java 8 tests in a separate
module that will get run in all builds because of that requirement.
There are downsides, of course: it's basically the same situation we
were in when we still supported Java 6 but were using jdk 1.7 to build
things. Setting the proper bootclasspath to use jdk 7's rt.jar during
compilation could solve a lot of those. (We already have both JDKs in
jenkins machines as far as I can tell.)

For Scala 2.12, and option might be dropping Java 7 when we decide to
add support for that (unless you're also suggesting Scala 2.12 as part
of 2.0?).

For (2) it seems the jvm used to compile things doesn't really make a
difference. It could be as simple as ""we strongly recommend running
Spark 2.0 on Java 8"".

Note I'm not for or against the change per se; I'd like to see more
data about what users are really using out there before making that
decision. But there was an explicit desire to maintain java 7
compatibility when we talked about going for Spark 2.0. And with those
kinds of decisions there's always a cost, including spending more
resources on infrastructure and testing.

-- 
Marcelo

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 24 Mar 2016 18:32:27 +0000",Re: [discuss] ending support for Java 7 in Spark 2.0,"Koert Kuipers <koert@tresata.com>
	""dev@spark.apache.org"" <dev@spark.apache.org>","

i think the arguments are convincing, but it also makes me wonder if i live in some kind of alternate universe... we deploy on customers clusters, where the OS, python version, java version and hadoop distro are not chosen by us. so think centos 6, cdh5 or hdp 2.3, java 7 and python 2.6. we simply have access to a single proxy machine and launch through yarn. asking them to upgrade java is pretty much out of the question or a 6+ month ordeal. of the 10 client clusters i can think of on the top of my head all of them are on java 7, none are on java 8. so by doing this you would make spark 2 basically unusable for us (unless most of them have plans of upgrading in near term to java 8, i will ask around and report back...).


It's not actually mandatory for the process executing in the Yarn cluster to run with the same JVM as the rest of the Hadoop stack; all that is needed is for the environment variables to set up the JAVA_HOME and PATH. Switching JVMs not something which YARN makes it easy to do, but it may be possible, especially if Spark itself provides some hooks, so you don't have to manually lay with setting things up. That may be something which could significantly ease adoption of Spark 2 in YARN clusters. Same for Python.

This is something I could probably help others to address

"
Stephen Boesch <javadba@gmail.com>,"Thu, 24 Mar 2016 11:46:43 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,"Steve Loughran <stevel@hortonworks.com>
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1 for java8 only   +1 for 2.11+ only .    At this point scala libraries
supporting only 2.10 are typically less active and/or poorly maintained.
That trend will only continue when considering the lifespan of spark 2.X.

2016-03-24 11:32 GMT-07:00 Steve L"
Michael Armbrust <michael@databricks.com>,"Thu, 24 Mar 2016 13:08:12 -0700",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Patrick is investigating.


e-build.sh>
t
?
.6.tgz
.6.tgz
getting
.4.tgz
.6.tgz
"
Andrew Ash <andrew@andrewash.com>,"Thu, 24 Mar 2016 14:26:47 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Stephen Boesch <javadba@gmail.com>,"Spark 2.x has to be the time for Java 8.

I'd rather increase JVM major version on a Spark major version than on a
Spark minor version, and I'd rather Spark do that upgrade for the 2.x
series than the 3.x series (~2yr from now based on the lifetime of Spark
1.x).  If we wait until the next opportunity for a breaking change to Spark
(3.x) we might be upgrading to Java 9 at that point rather than Java 8.

If Spark users need Java 7 they are free to continue using the 1.x series,
the same way that folks who need Java 6 are free to continue using 1.4


"
Jakob Odersky <jakob@odersky.com>,"Thu, 24 Mar 2016 14:29:07 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Stephen Boesch <javadba@gmail.com>,"Reynold's 3rd point is particularly strong in my opinion. Supporting
Scala 2.12 will require Java 8 anyway, and introducing such a change
is probably best done in a major release.
Consider what would happen if Spark 2.0 doesn't require Java 8 and
hence not support Scala 2.12. Will it be stuck on an older version
until 3.0 is out? Will it be introduced in a minor release?
I think 2.0 is the best time for such a change.


---------------------------------------------------------------------


"
Romi Kuntsman <romi@totango.com>,"Thu, 24 Mar 2016 14:30:55 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,"Stephen Boesch <javadba@gmail.com>
	Koert Kuipers <koert@tresata.com>, Steve Loughran <stevel@hortonworks.com>","+1 for Java 8 only

I think it will make it easier to make a unified API for Java and Scala,
instead of the wrappers of Java over Scala.

"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 24 Mar 2016 14:34:34 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Jakob Odersky <jakob@odersky.com>,"Hi Jakob,


That's a false choice. You can support 2.10 (or 2.11) on Java 7 and
2.12 on Java 8.

I'm not saying it's a great idea, just that what you're suggesting
isn't really a problem.

-- 
Marcelo

---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Thu, 24 Mar 2016 14:41:01 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"You can, but since it's going to be a maintainability issue I would
argue it is in fact a problem.


---------------------------------------------------------------------


"
Jakob Odersky <jakob@odersky.com>,"Thu, 24 Mar 2016 14:44:27 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"I mean from the perspective of someone developing Spark, it makes
things more complicated. It's just my point of view, people that
actually support Spark deployments may have a different opinion ;)


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 24 Mar 2016 14:46:27 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Jakob Odersky <jakob@odersky.com>,"
Every thing you choose to support generates a maintenance burden.
Support 3 versions of Scala would be a huge maintenance burden, for
example, as is supporting 2 versions of the JDK. Just note that,
technically, we do support 2 versions of the jdk today; we just don't
do a lot of automated testing on jdk 8 (PRs are all built with jdk 7
AFAIK).

So at the end it's a compromise. How many users will be affected by
your choices? That's the question that I think is the most important.
If switching to java 8-only means a bunch of users won't be able to
upgrade, it means that Spark 2.0 will get less use than 1.x and will
take longer to gain traction. That has other ramifications - such as
less use means less issues might be found and the overall quality may
suffer in the beginning of this transition.

-- 
Marcelo

---------------------------------------------------------------------


"
Kostas Sakellis <kostas@cloudera.com>,"Thu, 24 Mar 2016 16:44:35 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"If an argument here is the ongoing build/maintenance burden I think we
should seriously consider dropping scala 2.10 in Spark 2.0. Supporting
scala 2.10 is bigger build/infrastructure burden than supporting jdk7 since
you actually have to build different artifacts and test them whereas you
can target Spark onto 1.7 and just test it on JDK8.

In addition, as others pointed out, it seems like a bigger pain to drop
support for a JDK than scala version. So if we are considering dropping
java 7, which is a breaking change on the infra side, now is also a good
time to drop Scala 2.10 support.

Kostas

P.S. I haven't heard anyone on this thread fight for Scala 2.10 support.


"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Mar 2016 16:46:26 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"Actually it's *way* harder to upgrade Scala from 2.10 to 2.11, than
upgrading the JVM runtime from 7 to 8, because Scala 2.10 and 2.11 are not
binary compatible, whereas JVM 7 and 8 are binary compatible except certain
esoteric cases.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 24 Mar 2016 16:48:35 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"
True, but ask anyone who manages a large cluster how long it would
take them to upgrade the jdk across their cluster and validate all
their applications and everything... binary compatibility is a tiny
drop in that bucket.

-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Mar 2016 16:50:20 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"If you want to go down that route, you should also ask somebody who has had
experience managing a large organization's applications and try to update
Scala version.



"
Kostas Sakellis <kostas@cloudera.com>,"Thu, 24 Mar 2016 16:53:26 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"In addition, with Spark 2.0, we are throwing away binary compatibility
anyways so user applications will have to be recompiled.

The only argument I can see is for libraries that have already been built
on Scala 2.10 that are no longer being maintained. How big of an issue do
we think that is?

Kostas


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 24 Mar 2016 16:54:19 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"It's a pain in the ass.  Especially if some of your transitive dependencies
never upgraded from 2.10 to 2.11.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 24 Mar 2016 16:54:42 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"
I understand both sides. But if you look at what I've been asking
since the beginning, it's all about the cost and benefits of dropping
support for java 1.7.

The biggest argument in your original e-mail is about testing. And the
testing cost is much bigger for supporting scala 2.10 than it is for
supporting java 1.7. If you read one of my earlier replies, it should
be even possible to just do everything in a single job - compile for
java 7 and still be able to test things in 1.8, including lambdas,
which seems to be the main thing you were worried about.





-- 
Marcelo

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Thu, 24 Mar 2016 16:57:03 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"There aren't many such libraries, but there are a few.  When faced with one
of those dependencies that still doesn't go beyond 2.10, you essentially
have the choice of taking on the maintenance burden to bring the library up
to date, or you do what is potentially a fairly larger refactoring to use
an alternative but well-maintained library.


"
Michael Armbrust <michael@databricks.com>,"Thu, 24 Mar 2016 17:01:04 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Mark Hamstra <mark@clearstorydata.com>,"

Yeah, I'm going to have to agree here.  It is not as bad as it was in the
2.9 days, but its still non-trivial due to the eco-system part of it.  For
this reason I think that it is premature to drop support for 2.10.x.
"
Koert Kuipers <koert@tresata.com>,"Thu, 24 Mar 2016 21:34:11 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Reynold Xin <rxin@databricks.com>,"the good news is, that from an shared infrastructure perspective, most
places have zero scala, so the upgrade is actually very easy. i can see how
it would be different for say twitter....


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 24 Mar 2016 18:36:14 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Koert Kuipers <koert@tresata.com>,"Container Java version can be different from yarn Java version : we run
jobs with jdk8 on jdk7 cluster without issues.

Regards
Mridul


op
er
y,
ks
me
i
,
g
d
r
1
on
ax.net
am
nd
e
 a
s
of
n
or
d
rt
se
-
"
Koert Kuipers <koert@tresata.com>,"Thu, 24 Mar 2016 21:40:36 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Mridul Muralidharan <mridul@gmail.com>,"i think marcelo also pointed this out before. its very interesting to hear,
i was not aware of that until today. it would mean we would only have to
convince a group/client with a cluster to install jdk8 on the nodes,
without actually transitioning to it, if i understand it correctly. that
would definitely lower the hurdle by a lot.


oop
ver
ly,
rks
ome
n
m
.
e
nd
ue
h
os
d
in
'd
h
l
--
"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 24 Mar 2016 18:59:42 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"Removing compatibility (with jdk, etc) can be done with a major release-
given that 7 has been EOLed a while back and is now unsupported, we have to
decide if we drop support for it in 2.0 or 3.0 (2+ years from now).

Given the functionality & performance benefits of going to jdk8, future
enhancements relevant in 2.x timeframe ( scala, dependencies) which
requires it, and simplicity wrt code, test & support it looks like a good
checkpoint to drop jdk7 support.

As already mentioned in the thread, existing yarn clusters are unaffected
if they want to continue running jdk7 and yet use spark2 (install jdk8 on
all nodes and use it via JAVA_HOME, or worst case distribute jdk8 as
archive - suboptimal).
I am unsure about mesos (standalone might be easier upgrade I guess ?).


Proposal is for 1.6x line to continue to be supported with critical
fixes; newer
features will require 2.x and so jdk8

Regards
Mridul



"
Koert Kuipers <koert@tresata.com>,"Thu, 24 Mar 2016 22:28:37 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Mridul Muralidharan <mridul@gmail.com>,"i think that logic is reasonable, but then the same should also apply to
scala 2.10, which is also unmaintained/unsupported at this point (basically
has been since march 2015 except for one hotfix due to a license
incompatibility)

who wants to support scala 2.10 three years after they did the last
maintenance release?



"
Liwei Lin <lwlin7@gmail.com>,"Fri, 25 Mar 2016 11:51:26 +0800",Re: [discuss] ending support for Java 7 in Spark 2.0,Koert Kuipers <koert@tresata.com>,"Arguments are really convincing; new Dataset API as well as performance

improvements is exiting, so I'm personally +1 on moving onto Java8.



However, I'm afraid Tencent is one of ""the organizations stuck with Java7""

-- our IT Infra division wouldn't upgrade to Java7 until Java8 is out, and

wouldn't upgrade to Java8 until Java9 is out.


So:

(non-binding) +1 on dropping scala 2.10 support

(non-binding)  -1 on dropping Java 7 support

                      * as long as we figure out a practical way to run
Spark with

                        JDK8 on JDK7 clusters, this -1 would then
definitely be +1


Thanks !


"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 24 Mar 2016 20:55:52 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Koert Kuipers <koert@tresata.com>,"I do agree w.r.t scala 2.10 as well; similar arguments apply (though there
is a nuanced diff - source compatibility for scala vs binary compatibility
wrt Java)
Was there a proposal which did not go through ? Not sure if I missed it.

Regards
Mridul


"
Saisai Shao <sai.sai.shao@gmail.com>,"Fri, 25 Mar 2016 14:20:23 +0800",Re: Can we remove private[spark] from Metrics Source and SInk traits?,Steve Loughran <stevel@hortonworks.com>,"+1 on exposing the source/sink interface, since MetricsSystem by natural
support plugin source and sink, so supporting this don't require a big
change of current code. Also there's lot of requirements to add custom sink
and source to the MetricsSystem, it"
sage <lkkey80@gmail.com>,"Thu, 24 Mar 2016 23:56:26 -0700 (MST)",Does SparkSql has official jdbc/odbc driver ?,dev@spark.apache.org,"Hi all,
   Does SparkSql has official jdbc/odbc driver? 
   I only found third-party's odbc/jdbc driver, like simba, and most of
third-party's odbc/jdbc driver are not free to use.




--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 24 Mar 2016 23:57:17 -0700",Re: Does SparkSql has official jdbc/odbc driver ?,sage <lkkey80@gmail.com>,"No - it is too painful to develop a jdbc/odbc driver.



"
Mridul Muralidharan <mridul@gmail.com>,"Thu, 24 Mar 2016 23:58:13 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Koert Kuipers <koert@tresata.com>,"I do agree w.r.t scala 2.10 as well; similar arguments apply (though there
is a nuanced diff - source compatibility for scala vs binary compatibility
wrt Java)
Was there a proposal which did not go through ? Not sure if I missed it.

Regards
Mridul


"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Fri, 25 Mar 2016 11:54:12 +0100",Re: Does SparkSql has official jdbc/odbc driver ?,Reynold Xin <rxin@databricks.com>,"I haven't tried this, but I thought you can run the Thriftserver in Spark
and then connect with the HiveServer2 JDBC driver:
http://spark.apache.org/docs/1.6.1/sql-programming-guide.html#running-the-thrift-jdbcodbc-server


"
Andrew Ray <ray.andrew@gmail.com>,"Fri, 25 Mar 2016 09:03:31 -0500",Re: [discuss] ending support for Java 7 in Spark 2.0,Mridul Muralidharan <mridul@gmail.com>,"+1 on removing Java 7 and Scala 2.10 support.

It looks to be entirely possible to support Java 8 containers in a YARN
cluster otherwise running Java 7 (example code for alt JAVA_HOME
https://issues.apache.org/jira/secure/attachment/12671739/YARN-1964.pat"
Jacek Laskowski <jacek@japila.pl>,"Fri, 25 Mar 2016 16:05:02 +0100",Any plans to migrate Transformer API to Spark SQL (closer to DataFrames)?,dev <dev@spark.apache.org>,"Hi,

After few weeks with spark.ml now, I came to conclusion that
Transformer concept from Pipeline API (spark.ml/MLlib) should be part
of DataFrame (SQL) where they fit better. Are there any plans to
migrate Transformer API (ML) to DataFrame (SQL)?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 25 Mar 2016 16:50:07 +0100",[spark.ml] Why is private class ColumnPruner?,dev <dev@spark.apache.org>,"Hi,

Came across `private class ColumnPruner` with ""TODO(ekl) make this a
public transformer"" in scaladoc, cf.
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/feature/RFormula.scala#L317.

Why is this private and is there a JIRA for the TODO(ekl)?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
"""David Nalley""<ke4qqq@apache.org>","Fri, 25 Mar 2016 23:54:34 -0000",Re: SPARK-13843 and future of streaming backends,<dev@spark.apache.org>,"


Sonatype, has strict instructions to only permit org.apache.* to originate from repository.apache.org. Exceptions to that must be approved by VP, Infrastructure. 
------
Sent via Pony Mail for dev@spark.apache.org. 
View this email online at:
https://pony-poc.apache.org/list.html?dev@spark.apache.org

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Fri, 25 Mar 2016 22:18:59 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Andrew Ray <ray.andrew@gmail.com>,"i asked around a little, and the general trend at our clients seems to be
that they plan to upgrade the clusters to java 8 within the year.

so with that in mind i wish this was a little later (i would have preferred
a java-8-only spark at the end of year). but since a major spark version
only comes around every so often, i guess it makes sense to make the jump
now. so:
+1 on dropping java 7
+1 on dropping scala 2.10

i would especially like to point out (as others have before me) that nobody
has come in and said they actually need scala 2.10 support, so that seems
like the easiest choice to me of all.


"
Joseph Bradley <joseph@databricks.com>,"Fri, 25 Mar 2016 19:23:22 -0700",Re: Any plans to migrate Transformer API to Spark SQL (closer to DataFrames)?,Jacek Laskowski <jacek@japila.pl>,"There have been some comments about using Pipelines outside of ML, but I
have not yet seen a real need for it.  If a user does want to use Pipelines
for non-ML tasks, they still can use Transformers + PipelineModels.  Will
that work?


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 26 Mar 2016 10:26:39 +0100",Re: Any plans to migrate Transformer API to Spark SQL (closer to DataFrames)?,Joseph Bradley <joseph@databricks.com>,"Hi Joseph,

Thanks for the response. I'm one who doesn't understand all the
hype/need for Machine Learning...yet and through Spark ML(lib) glasses
I'm looking at ML space. In the meantime I've got few assignments (in
a project with Spark and Scala) that have required quite extensive
dataset manipulation.

It was when I sinked into using DataFrame/Dataset for data
manipulation not RDD (I remember talking to Brian about how RDD is an
""assembly"" language comparing to the higher-level concept of
DataFrames with Catalysts and other optimizations). After few days
with DataFrame I learnt he was so right! (sorry Brian, it took me
longer to understand your point).

I started using DataFrames in far too many places than one could ever
accept :-) I was so...carried away with DataFrames (esp. show vs
foreach(println) and UDFs via udf() function)

And then, when I moved to Pipeline API and discovered Transformers.
And PipelineStage that can create pipelines of DataFrame manipulation.
They read so well that I'm pretty sure people would love using them
more often, but...they belong to MLlib so they are part of ML space
(not many devs tackled yet). I applied the approach to using
withColumn to have better debugging experience (if I ever need it). I
learnt it after having watched your presentation about Pipeline API.
It was so helpful in my RDD/DataFrame space.

So, to promote a more extensive use of Pipelines, PipelineStages, and
Transformers, I was thinking about moving that part to SQL/DataFrame
API where they really belong. If not, I think people might miss the
beauty of the very fine and so helpful Transformers.

Transformers are *not* a ML thing -- they are DataFrame thing and
should be where they really belong (for their greater adoption).

What do you think?


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
=?UTF-8?B?TWljaGHFgiBaaWVsacWEc2tp?= <zielinski.michal0@gmail.com>,"Sat, 26 Mar 2016 11:10:32 +0000",Re: Any plans to migrate Transformer API to Spark SQL (closer to DataFrames)?,Jacek Laskowski <jacek@japila.pl>,"Spark ML Pipelines API (not just Transformers, Estimators and custom
Pipelines classes as well) are definitely not just machine-learning
specific.

We use them heavily in our developement. We're building machine learning
pipelines *BUT* many steps involve joining, schema manipulation,
pre/postprocessing data for the actual statistical algorithm, having
monoidal architecture (I have a slide deck if you're interested).

Pipelines API is a powerful abstraction that makes things very easy for us.
They are not always perfect (imho transformSchema is a little bit of a
mess, maybe future Dataset API will help), but they make our pipelines very
customisable and pluggable (you can add/swap/remove any PipelineStage and
any point).


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 26 Mar 2016 14:17:29 +0100",Re: SPARK-13843 and future of streaming backends,Mridul Muralidharan <mridul@gmail.com>,"Hi,

Although I'm not that much experienced member of ASF, I share your
concerns. I haven't looked at the issue from this point of view, but
after having read the thread I think PMC should've signed off the
migration of ASF-owned code to a non-ASF repo. At least a vote is
required (and this discussion is a sign that the process has not been
conducted properly as people have concerns, me including).

Thanks Mridul!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 26 Mar 2016 13:58:15 +0000",Re: SPARK-13843 and future of streaming backends,Jacek Laskowski <jacek@japila.pl>,"This has been resolved; see the JIRA and related PRs but also
http://apache-spark-developers-list.1001551.n3.nabble.com/SPARK-13843-Next-steps-td16783.html

This is not a scenario where a [VOTE] needs to take place, and code
changes don't proceed through PMC votes. From the project perspective,
code was deleted/retired for lack of interest, and this is controlled
by the normal lazy consensus protocol which wasn't vetoed.

The subsequent discussion was in part about whether other modules
should go, or whether one should come back, which it did. The latter
suggests that change could have been left open for some discussion
longer. Ideally, you would have commented before the initial change
happened, but it sounds like several people would have liked more
time. I don't think I'd call that ""improper conduct"" though, no. It
was reversed via the same normal code management process.

The rest of the question concerned what becomes of the code that was
removed. It was revived outside the project for anyone who cares to
continue collaborating. There seemed to be no disagreement about that,
mostly because the code in question was of minimal interest. PMC
doesn't need to rule on anything. There may still be some loose ends
there like namespace changes. I'll add to the other thread about this.




---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 26 Mar 2016 14:03:05 +0000",Re: SPARK-13843 Next steps,"""dev@spark.apache.org"" <dev@spark.apache.org>","Looks like this is done; docs have been moved, flume is back in, etc.

For the moment Kafka streaming is still in the project and I know
there's still discussion about how to manage multiple versions within
the project.

the code that was moved out. I believe it'll have to move out of the
org.apache namespace as well as change its artifact group. At least,
David indicated Sonatype wouldn't let someone non-ASF push an artifact
from that group anyway.

Also might be worth adding a description at
https://github.com/spark-packages explaining that these are just some
unofficial Spark-related packages.


---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Sat, 26 Mar 2016 10:07:21 -0700","Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Sean Owen <sowen@cloudera.com>,"I believe some of this has been resolved in the context of some parts that
had interest in one extra connector, but we still have a few removed, and
as you mentioned, we still don't have a simple way or willingness to manage
and be current on new packages like kafka. And based on the fact that this
thread is still alive, I believe that other community members might have
other concerns as well.

After some thought, I believe having a separate project (what was mentioned
here as Spark Extras) to handle Spark Connectors and Spark add-ons in
general could be very beneficial to Spark and the overall Spark community,
which would have a central place in Apache to collaborate around related
Spark components.

Some of the benefits on this approach

- Enables maintaining the connectors inside Apache, following the Apache
governance and release rules, while allowing Spark proper to focus on the
core runtime.
- Provides more flexibility in controlling the direction (currency) of the
existing connectors (e.g. willing to find a solution and maintain multiple
versions of same connectors like kafka 0.8x and 0.9x)
- Becomes a home for other types of Spark related connectors helping
expanding the community around Spark (e.g. Zeppelin see most of it's
current contribution around new/enhanced connectors)

What are some requirements for Spark Extras to be successful:

- Be up to date with Spark Trunk APIs (based on daily CIs against SNAPSHOT)
- Adhere to Spark release cycles (have a very little window compared to
Spark release)
- Be more open and flexible to the set of connectors it will accept and
maintain (e.g. also handle multiple versions like the kafka 0.9 issue we
have today)

Where to start Spark Extras

Depending on the interest here, we could follow the steps of (Apache Arrow)
and start this directly as a TLP, or start as an incubator project. I would
consider the first option first.

Who would participate

Have thought about this for a bit, and if we go to the direction of TLP, I
would say Spark Committers and Apache Members can request to participate as
PMC members, while other committers can request to become committers. Non
committers would be added based on meritocracy after the start of the
project.

Project Name

It would be ideal if we could have a project name that shows close ties to
Spark (e.g. Spark Extras or Spark Connectors) but we will need permission
and support from whoever is going to evaluate the project proposal (e.g.
Apache Board)


Thoughts ?

Does anyone have any big disagreement or objection to moving into this
direction ?

Otherwise, who would be interested in joining the project, so I can start
working on some concrete proposal ?






-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
,"Sat, 26 Mar 2016 18:20:54 +0100","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",dev@spark.apache.org,"Hi Luciano,

If we take the ""pure"" technical vision, there's pros and cons of having 
spark-extra (or whatever the name we give) still as an Apache project:

Pro:
  - Governance & Quality Insurance: we follow the Apache rules, meaning 
that a release has to be staged and voted by the PMC. It's a form of 
governance of the project and quality (as the releases are reviewed).
  - Software origin: users know where the connector comes from, and they 
have the guarantee in term of licensing, etc.
  - IP/ICLA: We know the committers of this project, and we know they 
agree with the ICL agreement.

Cons:
  - Third licenses support. As an Apache project, the ""connectors"" will 
be allowed to use only Apache or Category B licensed dependencies. For 
instance, if I would like to create a Spark connector for couchbase, I 
can't do it at Apache.
  - Release cycle. As an Apache project, it means we have to follow the 
rules, meaning that the release cycle can appear strict and long due to 
the staging and vote process. For me, it's a huge benefit but some can 
see as too strict ;)

Maybe, we can imagine both, as we have in ServiceMix or Camel:
- all modules/connectors matching the Apache rule (especially in term of 
licensing) should be in the Apache Spark-Modules (or Spark-Extensions, 
or whatever). It's like the ServiceMix Bundles.
- all modules/connectors that can't fit into the Apache rule (due to 
licensing issue) can go into GitHub Spark-Extra (or Spark-Package). It's 
like the ServiceMix Extra or Camel Extra on github.

My $0.01.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Sat, 26 Mar 2016 10:38:07 -0700","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",,"t>

e

Yes, this is not solving the incompatible license problems


he

IMHO, This is the small price we pay for all the good stuff you mentioned
in pro



If you are talking here about Spark proper, then we are currently seeing
that this is going to be hard. If there was a way to have more flexibility
to host these directly into Spark proper, I would never be creating this
thread as we would have all the pros you mentioned hosting them directly
into Spark.


We could look into this, but it might be a ""Spark Extra  discussion"" on how
we can help foster a community around the non-compatible licensed
connectors.


xt-steps-td16783.html
e,
d
t,
s.
r
nd
he
t
-
-
-


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
,"Sat, 26 Mar 2016 18:44:20 +0100","Re: Creating Spark Extras project, was Re: SPARK-13843 and future of
 streaming backends",Luciano Resende <luckbr1975@gmail.com>,"Hi Luciano,

I didn't mean Spark proper, but more something like you proposed.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Mridul Muralidharan <mridul@gmail.com>,"Sat, 26 Mar 2016 11:13:42 -0700",Re: SPARK-13843 and future of streaming backends,Sean Owen <sowen@cloudera.com>,"
This change happened subsequent to current thread (thanks Marcelo) and
could as well have gone unnoticed until release vote.





I have not seen Apache owned artifacts moved out of it's governance without
discussion - this was not refactoring or cleanup (as was suggested
disingenuously) but migration of submodules/functionality (though from
Reynold's clarification, looks like for good enough reasons).

A vote might or might not have required but a discussion must have
happened - atleast going forward, it will help us not to miss things
(artifact and project namespace, license, ownership, release cycle, version
compatibility, etc of the sub project could be of interest to users and
developers).

Regards
Mridul


"
salexln <salexln@gmail.com>,"Sun, 27 Mar 2016 12:24:57 -0700 (MST)",BlockManager WARNINGS and ERRORS,dev@spark.apache.org,"HI all,

I started testing my code (https://github.com/salexln/FinalProject_FCM) 
with the latest Spark available in GitHub, 
and when I run it I get the following errors:

*scala> val clusters = FuzzyCMeans.train(parsedData, 2, 20, 2.0)*

16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 ERROR Executor: 2 block locks were not released by TID =
32:
[rdd_8_0, rdd_35_0]
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 ERROR Executor: 2 block locks were not released by TID =
35:
[rdd_8_0, rdd_35_0]
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 ERROR Executor: 2 block locks were not released by TID =
38:
[rdd_8_0, rdd_35_0]
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 ERROR Executor: 2 block locks were not released by TID =
41:
[rdd_8_0, rdd_35_0]
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 ERROR Executor: 2 block locks were not released by TID =
44:
[rdd_8_0, rdd_35_0]
16/03/27 22:24:10 WARN BlockManager: Block rdd_8_0 already exists on this
machine; not re-adding it
16/03/27 22:24:10 WARN BlockManager: Block rdd_35_0 already exists on this
machine; not re-adding it

I did not get these previously, is it something new?





--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Sun, 27 Mar 2016 13:23:51 -0700",Re: BlockManager WARNINGS and ERRORS,salexln <salexln@gmail.com>,"The warning was added by:

SPARK-12757 Add block-level read/write locks to BlockManager


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 28 Mar 2016 00:49:20 +0000",Re: Spark 1.6.1 Hadoop 2.6 package on S3 corrupt?,Michael Armbrust <michael@databricks.com>,"Pingity-ping-pong since this is still a problem.


se-build.sh>
:
e
d
s?
2.6.tgz
2.6.tgz
 getting
2.4.tgz
2.6.tgz
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Mon, 28 Mar 2016 06:54:11 +0200","Re: Any plans to migrate Transformer API to Spark SQL (closer to
 DataFrames)?",dev@spark.apache.org,"Hi Jacek,

In this context, don't you think it would be useful, if at least some
traits from org.apache.spark.ml.param.shared.sharedParams were
public?HasInputCol(s) and HasOutputCol for example. These are useful
pretty much every time you create custom Transformer. 

-- 
Pozdrawiam,
Maciej Szymkiewicz


 I
lines
ill
te:




"
Jacek Laskowski <jacek@japila.pl>,"Mon, 28 Mar 2016 08:44:37 +0200",Re: Any plans to migrate Transformer API to Spark SQL (closer to DataFrames)?,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Hi,

Never develop any custom Transformer (or UnaryTransformer in particular),
but I'd be for it if that's the case.

Jacek
28.03.2016 6:54 AM ""Maciej Szymkiewicz"" <mszymkiewicz@gmail.com> napisaÅ‚(a):

 I
"
=?UTF-8?B?TWljaGHFgiBaaWVsacWEc2tp?= <zielinski.michal0@gmail.com>,"Mon, 28 Mar 2016 08:49:45 +0100",Re: Any plans to migrate Transformer API to Spark SQL (closer to DataFrames)?,Jacek Laskowski <jacek@japila.pl>,"Hi Maciej,

Absolutely. We had to copy HasInputCol/s, HasOutputCol/s (along with a
couple of others like HasProbabilityCol) to our repo. Which for most
use-cases is good enough, but for some (e.g. operating on any Transformer
that accepts either our or Sparks HasInputCol) makes the code clunky.
Opening those traits to the public would be a big gain.

Thanks,
Michal


t
t
-
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 28 Mar 2016 12:12:21 +0000",Re: [discuss] ending support for Java 7 in Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","
given that 7 has been EOLed a while back and is now unsupported, we have to decide if we drop support for it in 2.0 or 3.0 (2+ years from now).
nhancements relevant in 2.x timeframe ( scala, dependencies) which requires it, and simplicity wrt code, test & support it looks like a good checkpoint to drop jdk7 support.
 if they want to continue running jdk7 and yet use spark2 (install jdk8 on all nodes and use it via JAVA_HOME, or worst case distribute jdk8 as archive - suboptimal).

you wouldn't want to dist it as an archive; it's not just the binaries, it's the install phase. And you'd better remember to put the JCE jar in on top of the JDK for kerberos to work.

setting up environment vars to point to JDK8 in the launched app/container avoids that. Yes, the ops team do need to install java, but if you offer them the choice of ""installing a centrally managed Java"" and ""having my code try and install it"", they should go for the managed option.

 for both python and java. And, as the techniques for mixing JDK versions is clearly not that well known, documenting it. 

(FWIW I've done code which even uploads it's own hadoop-* JAR, but what gets you is changes in the hadoop-native libs; you do need to get the PATH var spot on)


; newer features will require 2.x and so jdk8


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 28 Mar 2016 10:33:11 -0500",Re: SPARK-13843 Next steps,Sean Owen <sowen@cloudera.com>,"I really think the only thing that should have to change is the maven
group and identifier, not the java namespace.

There are compatibility problems with the java namespace changing
(e.g. access to private[spark]), and I don't think that someone who
takes the time to change their build file to download a maven artifact
without ""apache"" in the identifier is at significant risk of consumer
confusion.

I've tried to get a straight answer from ASF trademarks on this point,
but the answers I've been getting are mixed, and personally disturbing
to me in terms of over-reaching.


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Mon, 28 Mar 2016 10:35:42 -0500",Re: SPARK-13843 and future of streaming backends,David Nalley <ke4qqq@apache.org>,"Are you talking about group/identifier name, or contained classes?

Because there are plenty of org.apache.* classes distributed via maven
with non-apache group / identifiers.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 28 Mar 2016 09:32:15 -0700",Re: SPARK-13843 Next steps,Cody Koeninger <cody@koeninger.org>,"
I think it would be fine to keep the package names for backwards
compatibility, but I think if these external projects want to keep a
separate release cycle from Spark, they should refrain from using
""private[spark]"" APIs; which I guess is an argument for changing the
package names at some point.

-- 
Marcelo

---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 28 Mar 2016 11:24:11 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,,"+1 from Mesosphere




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Steve Johnston <sjohnston@algebraixdata.com>,"Mon, 28 Mar 2016 12:07:46 -0700 (MST)","OOM and ""spark.buffer.pageSize""",dev@spark.apache.org,"I'm attempting to address an OOM issue. I saw referenced in 
java.lang.OutOfMemoryError: Unable to acquire bytes of memory
<http://apache-spark-developers-list.1001551.n3.nabble.com/java-lang-OutOfMemoryError-Unable-to-acquire-bytes-of-memory-td16773.html>  
the configuration setting ""spark.buffer.pageSize"" which was used in
conjunction with ""spark.sql.shuffle.partitions"" to solve the OOM problem
Nezih was having.

What is ""spark.buffer.pageSize""? How can it be used? I can find it in the
code but there doesn't seem to be any other documentation.

Thanks,
Steve



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 28 Mar 2016 12:12:07 -0700","Re: OOM and ""spark.buffer.pageSize""",Steve Johnston <sjohnston@algebraixdata.com>,"I guess you have looked at MemoryManager#pageSizeBytes where
the ""spark.buffer.pageSize"" config can override default page size.

FYI


"
Steve Johnston <sjohnston@algebraixdata.com>,"Mon, 28 Mar 2016 12:16:01 -0700 (MST)","Re: OOM and ""spark.buffer.pageSize""",dev@spark.apache.org,"Yes I have. Thatâ€™s the best source of information at the moment. Thanks.



--
3.nabble.com/OOM-and-spark-buffer-pageSize-tp16890p16892.html
om.

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 28 Mar 2016 13:15:12 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Michael Gummelt <mgummelt@mesosphere.io>,"+1, I also checked with few projects inside IBM that consume Spark and they
seem to be ok with the direction of droping JDK 7.





-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 28 Mar 2016 14:01:27 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,"""dev@spark.apache.org"" <dev@spark.apache.org>","Finally got some internal feedback on this, and we're ok with
requiring people to deploy jdk8 for 2.0, so +1 too.




-- 
Marcelo

---------------------------------------------------------------------


"
Kostas Sakellis <kostas@cloudera.com>,"Mon, 28 Mar 2016 16:06:19 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Marcelo Vanzin <vanzin@cloudera.com>,"Also, +1 on dropping jdk7 in Spark 2.0.

Kostas


"
Sean Owen <sowen@cloudera.com>,"Tue, 29 Mar 2016 00:47:46 +0000",Re: SPARK-13843 Next steps,Cody Koeninger <cody@koeninger.org>,"I tend to agree. If it's going to present a significant technical hurdle
and the software is clearly non ASF like via a different artifact, there's
a decent argument the namespace should stay. The artifact has to change
though and that is what David was referring to in his other message.


"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Mon, 28 Mar 2016 19:19:45 -0700",Master options Cluster/Client descrepencies.,"user@spark.apache.org, dev@spark.apache.org","Hi All,

I have written a spark program on my dev box ,
       IDE:Intellij
       scala version:2.11.7
       spark verison:1.6.1

run fine from IDE, by providing proper input and output paths including
 master.

But when i try to deploy the code in my cluster made of below,

       Spark version:1.6.1
        built from source pkg using scala 2.11
        But when i try spark-shell on cluster i get scala version to be
2.10.5
         hadoop yarn cluster 2.6.0

and with additional options,

--executor-memory
--total-executor-cores
--deploy-mode cluster/client
--master yarn

i get Exception in thread ""main"" java.lang.NoSuchMethodError:
scala.Predef$.$conforms()Lscala/Predef$$less$colon$less;
        at com.movoto.SparkPost$.main(SparkPost.scala:36)
        at com.movoto.SparkPost.main(SparkPost.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

i understand this to be a scala version issue, as i have faced this before.

Is there something that i have change and try  things to get the same
program running on cluster.

Regards,
Satyajit.
"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Tue, 29 Mar 2016 13:12:36 +0300",SparkML RandomForest java.lang.StackOverflowError,"dev@spark.apache.org, user <user@spark.apache.org>","Hi,

I have a web service that provides rest api to train random forest algo.
I train random forest on a 5 nodes spark cluster with enough memory -
everything is cached (~22 GB).
biggest one (400k samples and ~70k features) I'm stuck with
StackOverflowError.

Additional options for my web service
    spark.executor.extraJavaOptions=""-XX:ThreadStackSize=8192""
    spark.default.parallelism = 200.

- (with default thread stack size) it took 4 hours of training to get the
error.
- with increased stack size it took 60 hours to hit it.
I can increase it, but it's hard to say what amount of memory it needs and
it's applied to all of the treads and might waste a lot of memory.

I'm looking at different stages at event timeline now and see that task
deserialization time gradually increases. And at the end task
deserialization time is roughly same as executor computing time.

Code I use to train model:

int MAX_BINS = 16;
int NUM_CLASSES = 0;
double MIN_INFO_GAIN = 0.0;
int MAX_MEMORY_IN_MB = 256;
double SUBSAMPLING_RATE = 1.0;
boolean USE_NODEID_CACHE = true;
int CHECKPOINT_INTERVAL = 10;
int RANDOM_SEED = 12345;

int NODE_SIZE = 5;
int maxDepth = 30;
int numTrees = 50;
Strategy strategy = new Strategy(Algo.Regression(),
Variance.instance(), maxDepth, NUM_CLASSES, MAX_BINS,
        QuantileStrategy.Sort(), new
scala.collection.immutable.HashMap<>(), nodeSize, MIN_INFO_GAIN,
        MAX_MEMORY_IN_MB, SUBSAMPLING_RATE, USE_NODEID_CACHE,
CHECKPOINT_INTERVAL);
RandomForestModel model =
RandomForest.trainRegressor(labeledPoints.rdd(), strategy, numTrees,
""auto"", RANDOM_SEED);


Any advice would be highly appreciated.

The exception (~3000 lines long):
 java.lang.StackOverflowError
        at
java.io.ObjectInputStream$PeekInputStream.read(ObjectInputStream.java:2320)
        at
java.io.ObjectInputStream$PeekInputStream.readFully(ObjectInputStream.java:2333)
        at
java.io.ObjectInputStream$BlockDataInputStream.readInt(ObjectInputStream.java:2828)
        at java.io.ObjectInputStream.readHandle(ObjectInputStream.java:1453)
        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1512)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
        at
scala.collection.immutable.$colon$colon.readObject(List.scala:366)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
        at
scala.collection.immutable.$colon$colon.readObject(List.scala:362)
        at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:497)
        at
java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1900)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:2000)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1924)

--
Be well!
Jean Morozov
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 29 Mar 2016 11:04:27 +0000",Re: SPARK-13843 Next steps,Sean Owen <sowen@cloudera.com>,"while sonatype are utterly strict about the org.apache namespace (it guarantees that all such artifacts have come through the ASF release process, ideally including code-signing), nobody checks the org.apache internals, or worries too much about them. Note that spark itself has some bits of code in org.apache.hive so as to subclass the thriftserver.

What are the costs of having a project's package used externally?

1. interesting debugging sessions if JARs with conflicting classes are loaded.
2. you can't sign the JARs in the metadata. Nobody does that with the maven artifacts anyway.
3. whoever's package name it is often gets to see the stack traces in bug reports filed against them.




I tend to agree. If it's going to present a significant technical hurdle and the software is clearly non ASF like via a different artifact, there's a decent argument the namespace should stay. The artifact has to change though and that is what David was referring to in his other message.

I really think the only thing that should have to change is the maven
group and identifier, not the java namespace.

There are compatibility problems with the java namespace changing
(e.g. access to private[spark]), and I don't think that someone who
takes the time to change their build file to download a maven artifact
without ""apache"" in the identifier is at significant risk of consumer
confusion.

I've tried to get a straight answer from ASF trademarks on this point,
but the answers I've been getting are mixed, and personally disturbing
to me in terms of over-reaching.

om
ng
ved
scribe@spark.apache.org>
p@spark.apache.org>

"
Adam Roberts <AROBERTS@uk.ibm.com>,"Tue, 29 Mar 2016 15:21:02 +0000",Understanding PySpark Internals,dev@spark.apache.org,"Hi, I'm interested in figuring out how the Python API for Spark works, 
I've came to the following conclusion and want to share this with the 
community; could be of use in the PySpark docs here, specifically the 
""Execution and pipelining part"".

Any sanity checking would be much appreciated, here's the trivial Python 
example I've traced:
from pyspark import SparkContext
sc = SparkContext(""local[1]"", ""Adam test"")
sc.setCheckpointDir(""foo checkpoint dir"")

Added this JVM option:
export 
IBM_JAVA_OPTIONS=""-Xtrace:methods={org/apache/spark/*,py4j/*},print=mt""

Prints added in py4j-java/src/py4j/commands/CallCommand.java - 
specifically in the execute method. Built and replaced existing class in 
the py4j 0.9 jar in my Spark assembly jar. Example output is:
In execute for CallCommand, commandName: c
target object id: o0
methodName: get

I'll launch the Spark application with:
$SPARK_HOME/bin/spark-submit --master local[1] Adam.py > checkme.txt 2>&1

I've quickly put together the following WIP diagram of what I think is 
happening:
http://postimg.org/image/nihylmset/

To summarise I think:
We're heavily using reflection (as evidenced by Py4j's ReflectionEngine 
and MethodInvoker classes) to invoke Spark's API in a JVM from Python
There's an agreed protocol (in Py4j's Protocol.java) for handling 
commands: said commands are exchanged using a local socket between Python 
and our JVM (the driver based on docs, not the master)
The Spark API is accessible by means of commands exchanged using said 
socket using the agreed protocol
Commands are read/written using BufferedReader/Writer
Type conversion is also performed from Python to Java (not looked at in 
detail yet)
We keep track of the objects with, for example, o0 representing the first 
object we know about

Does this sound correct?

I've only checked the trace output in local mode, curious as to what 
happens when we're running in standalone mode (I didn't see a Python 
interpreter appearing on all workers in order to process partitions of 
data, I assume in standalone mode we use Python solely as an orchestrator 
- the driver - and not as an executor for distributed computing?).

Happy to provide the full trace output on request (omitted timestamps, 
logging info, added spacing), I expect there's a O*JDK method tracing 
equivalent so the above can easily be reproduced regardless of Java 
vendor.

Cheers,


Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Koert Kuipers <koert@tresata.com>,"Tue, 29 Mar 2016 13:01:44 -0400",Re: [discuss] ending support for Java 7 in Spark 2.0,Kostas Sakellis <kostas@cloudera.com>,"if scala prior to sbt 2.10.4 didn't support java 8, does that mean that 3rd
party scala libraries compiled with a scala version < 2.10.4 might not work
on java 8?



"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Tue, 29 Mar 2016 10:09:08 -0700",Fwd: Master options Cluster/Client descrepencies.,"user@spark.apache.org, dev@spark.apache.org","Hi All,

I have written a spark program on my dev box ,
       IDE:Intellij
       scala version:2.11.7
       spark verison:1.6.1

run fine from IDE, by providing proper input and output paths including
 master.

But when i try to deploy the code in my cluster made of below,

       Spark version:1.6.1
        built from source pkg using scala 2.11
        But when i try spark-shell on cluster i get scala version to be
2.10.5
         hadoop yarn cluster 2.6.0

and with additional options,

--executor-memory
--total-executor-cores
--deploy-mode cluster/client
--master yarn

i get Exception in thread ""main"" java.lang.NoSuchMethodError:
scala.Predef$.$conforms()Lscala/Predef$$less$colon$less;
        at com.movoto.SparkPost$.main(SparkPost.scala:36)
        at com.movoto.SparkPost.main(SparkPost.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

i understand this to be a scala version issue, as i have faced this before.

Is there something that i have change and try  things to get the same
program running on cluster.

Regards,
Satyajit.
"
Reynold Xin <rxin@databricks.com>,"Tue, 29 Mar 2016 11:17:20 -0700",Re: [discuss] ending support for Java 7 in Spark 2.0,Koert Kuipers <koert@tresata.com>,"They work.



"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 29 Mar 2016 18:35:04 +0000 (UTC)",Re: [discuss] ending support for Java 7 in Spark 2.0,"Reynold Xin <rxin@databricks.com>, Koert Kuipers <koert@tresata.com>","+1.
Tom 

rote:
 

 They work.


if scala prior to sbt 2.10.4 didn't support java 8, does that mean that 3rd party scala libraries compiled with a scala version < 2.10.4 might not work on java 8?


e:

Also, +1 on dropping jdk7 in Spark 2.0.Â 
Kostas
:

F"
Joseph Bradley <joseph@databricks.com>,"Tue, 29 Mar 2016 12:03:54 -0700",Re: Any plans to migrate Transformer API to Spark SQL (closer to DataFrames)?,=?UTF-8?B?TWljaGHFgiBaaWVsacWEc2tp?= <zielinski.michal0@gmail.com>,"This is great feedback to hear.  I think there was discussion about moving
Pipelines outside of ML at some point, but I'll have to spend more time to
dig it up.

In the meantime, I thought I'd mention this JIRA here in case people have
feedback:
https://issues.apache.org/jira/browse/SPARK-14033
--> It's about merging the concepts of Estimator and Model.  It would be a
breaking change in 2.0, but it would help to simplify the API and reduce
code duplication.

Regarding making shared params public:
https://issues.apache.org/jira/browse/SPARK-7146
--> I'd like to do this for 2.0, though maybe not for all shared params

Joseph


,
s
.
m>
--
"
Michael Segel <msegel_hadoop@hotmail.com>,"Tue, 29 Mar 2016 14:19:31 -0700",Any documentation on Spark's security model beyond YARN? ,dev@spark.apache.org,"Hi, 

So yeah, I know that Spark jobs running on a Hadoop cluster will inherit its security from the underlying YARN job. 
Howeverâ€¦ thatâ€™s not really saying much when you think about some use cases. 

Like using the thrift service â€¦ 

Iâ€™m wondering what else is new and what people have been thinking about how to enhance sparkâ€™s security. 

Thx

-Mike


---------------------------------------------------------------------


"
Suniti Singh <suniti.singh@gmail.com>,"Tue, 29 Mar 2016 17:36:20 -0700",aggregateByKey on PairRDD,"user@spark.apache.org, dev@spark.apache.org","Hi All,

I have an RDD having the data in  the following form :

tempRDD: RDD[(String, (String, String))]

(brand , (product, key))

(""amazon"",(""book1"",""tech""))

(""eBay"",(""book1"",""tech""))

(""barns&noble"",(""book"",""tech""))

(""amazon"",(""book2"",""tech""))


I would like to group the data by Brand and would like to get the result
set in the following format :

resultSetRDD : RDD[(String, List[(String), (String)]

i tried using the aggregateByKey but kind  of not getting how to achieve
this. OR is there any other way to achieve this?

val resultSetRDD  = tempRDD.aggregateByKey("""")({case (aggr , value) => aggr
+ String.valueOf(value) + "",""}, (aggr1, aggr2) => aggr1 + aggr2)

resultSetRDD = (amazon,(""book1"",""tech""),(""book2"",""tech""))

Thanks,

Suniti
"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 30 Mar 2016 13:03:12 +0900",Re: Null pointer exception when using com.databricks.spark.csv,Selvam Raman <selmna@gmail.com>,"Hi,

I guess this is not a CSV-datasource specific problem.

Does loading any file (eg. textFile()) work as well?

I think this is related with this thread,
http://apache-spark-user-list.1001560.n3.nabble.com/Error-while-running-example-scala-application-using-spark-submit-td10056.html
.


2016-03-30 12:44 GMT+09:00 Selvam Raman <selmna@gmail.com>:

e.
µà®¿à®°à¯à®¤à¯à®¤à¯ à®¨à¯†à®žà¯à®šà®®à¯ à®¨à®¿à®®à®¿à®°à¯à®¤à¯à®¤à¯""
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 30 Mar 2016 13:57:19 +0700",Re: Null pointer exception when using com.databricks.spark.csv,Selvam Raman <selmna@gmail.com>,"Looks like the winutils.exe is missing from the environment, See
https://issues.apache.org/jira/browse/SPARK-2356

Thanks
Best Regards


e.
µà®¿à®°à¯à®¤à¯à®¤à¯ à®¨à¯†à®žà¯à®šà®®à¯ à®¨à®¿à®®à®¿à®°à¯à®¤à¯à®¤à¯""
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 30 Mar 2016 14:01:16 +0700",Re: aggregateByKey on PairRDD,Suniti Singh <suniti.singh@gmail.com>,"Isn't it what tempRDD.groupByKey does?

Thanks
Best Regards


"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 30 Mar 2016 14:06:31 +0700",Re: Any documentation on Spark's security model beyond YARN?,Michael Segel <msegel_hadoop@hotmail.com>,"I don't think at this point there is anything on security beyond what is
written here already http://spark.apache.org/docs/latest/security.html

Thanks
Best Regards


ut some use cases.
 about
"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 30 Mar 2016 14:11:34 +0700",Re: Master options Cluster/Client descrepencies.,satyajit vegesna <satyajit.apasprk@gmail.com>,"Have a look at
http://spark.apache.org/docs/latest/building-spark.html#building-for-scala-211

Thanks
Best Regards


"
Steve Loughran <stevel@hortonworks.com>,"Wed, 30 Mar 2016 09:33:06 +0000",Re: Any documentation on Spark's security model beyond YARN?,Michael Segel <msegel_hadoop@hotmail.com>,"
> On 29 Mar 2016, at 22:19, Michael Segel <msegel_hadoop@hotmail.com> wrote:
> 
> Hi, 
> 
> So yeah, I know that Spark jobs running on a Hadoop cluster will inherit its security from the underlying YARN job. 
> Howeverâ€¦ thatâ€™s not really saying much when you think about some use cases. 
> 
> Like using the thrift service â€¦ 
> 
> Iâ€™m wondering what else is new and what people have been thinking about how to enhance sparkâ€™s security. 
> 

Been thinking a bit.

One thing to look at is renewal of hbase and hive tokens on long-lived services, alongside hdfs


---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Steve Loughran <stevel@hortonworks.com>,"Wed, 30 Mar 2016 09:33:08 +0000",Re: Null pointer exception when using com.databricks.spark.csv,Selvam Raman <selmna@gmail.com>,"

Hi,

i am using spark 1.6.0 prebuilt hadoop 2.6.0 version in my windows machine.

i was trying to use databricks csv format to read csv file. i used the below command.

<image.png>

I got null pointer exception. Any help would be greatly appreciated.

<image.png>

issue is Hadoop and it's need on windows for some native libs, compounded by bad error handlng of the situation

In Hadoop 2.8+ the NPE is replaced by a warning and a link to a page telling you what to do

https://wiki.apache.org/hadoop/WindowsProblems

"
Steve Loughran <stevel@hortonworks.com>,"Wed, 30 Mar 2016 10:42:16 +0000",Re: [discuss] ending support for Java 7 in Spark 2.0,Tom Graves <tgraves_cs@yahoo.com>,"
Can I note that if Spark 2.0 is going to be Java 8+ only, then that means Hadoop 2.6.x should be the minimum Hadoop version.

https://issues.apache.org/jira/browse/HADOOP-11090

Where things get complicated, is that situation of: Hadoop services on Java 7, Spark on Java 8 in its own JVM

I'm not sure that you could get away with having the newer version of the Hadoop classes in the spark assembly/lib dir, without coming up against incompatibilities with the Hadoop JNI libraries. These are currently backwards compatible, but trying to link up Hadoop 2.7 against a Hadoop 2.6 hadoop lib will generate an UnsatisfiedLinkException. Meaning: the whole cluster's hadoop libs have to be in sync, or at least the main cluster release in a version of hadoop 2.x >= the spark bundled edition.

Ignoring that detail,

Hadoop 2.6.1+
Guava >= 15? 17?

 I think the outcome of Hadoop < 2.6 and JDK >= 8 is ""undefined""; all bug reports will be met with a ""please upgrade, re-open if the problem is still there"".

Kerberos is  a particular troublespot here : You need Hadoop 2.6.1+ for Kerberos to work in Java 8 and recent versions of Java 7 (HADOOP-10786)

Note also that HADOOP-11628 is in 2.8 only. SPNEGO + CNAMES. I'll see about pulling that into 2.7.x, though I'm reluctant to go near 2.6 just to keep that extra stable.


Thomas: you've got the big clusters, what versions of Hadoop will they be on by the time you look at Spark 2.0?

-Steve




"
Sean Owen <sowen@cloudera.com>,"Wed, 30 Mar 2016 06:45:16 -0700",Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,dev <dev@spark.apache.org>,"(This should fork as its own thread, though it began during discussion
of whether to continue Java 7 support in Spark 2.x.)

Simply: would like to more clearly take the temperature of all
interested parties about whether to support Scala 2.10 in the Spark
2.x lifecycle. Some of the arguments appear to be:

Pro
- Some third party dependencies do not support Scala 2.11+ yet and so
would not be usable in a Spark app

Con
- Lower maintenance overhead -- no separate 2.10 build,
cross-building, tests to check, esp considering support of 2.12 will
be needed
- Can use 2.11+ features freely
- 2.10 was EOL in late 2014 and Spark 2.x lifecycle is years to come

I would like to not support 2.10 for Spark 2.x, myself.

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 30 Mar 2016 13:46:14 +0000 (UTC)",Re: [discuss] ending support for Java 7 in Spark 2.0,Steve Loughran <stevel@hortonworks.com>,"Steve, those are good points, I had forgotten Hadoop had those issues. Â  Â We run with jdk 8, hadoop is built for jdk7 compatibility, we are running hadoop 2.7 on our clusters and by the time Spark 2.0 is out I would expected a mix of Hadoop 2.7 and 2.8. Â We also don't use spnego.
I didn't quite follow what you were saying with the hadoop services being on jdk7. Â Are you saying building spark with say hadoop 2.8 libraries but your hadoop cluster is running hadoop 2.6 or less? If so I would agree that isn't a good idea.
Personally and from Yahoo point I'm still fine with going to jdk8 but I could see where other people are on older versions of Hadoop where it might be a problem.
Tom 

 

 
Can I note that if Spark 2.0 is going to be Java 8+ only, then that means Hadoop 2.6.x should be the minimum Hadoop version.
https://issues.apache.org/jira/browse/HADOOP-11090
Where things get complicated, is that situation of: Hadoop services on Java 7, Spark on Java 8 in its own JVM
I'm not sure that you could get away with having the newer version of the Hadoop classes in the spark assembly/lib dir, without coming up against incompatibilities with the Hadoop JNI libraries. These are currently backwards compatible, but trying to link up Hadoop 2.7 against a Hadoop 2.6 hadoop lib will generate an UnsatisfiedLinkException. Meaning: the whole cluster's hadoop libs have to be in sync, or at least the main cluster release in a version of hadoop 2.x >= the spark bundled edition.
Ignoring that detail,Â 
Hadoop 2.6.1+Guava >= 15? 17?
Â I think the outcome of Hadoop < 2.6 and JDK >= 8 is ""undefined""; all bug reports will be met with a ""please upgrade, re-open if the problem is still there"".Â 
Kerberos is Â a particular troublespot here : You need Hadoop 2.6.1+ for Kerberos to work in Java 8 and recent versions of Java 7 (HADOOP-10786)
Note also thatÂ HADOOP-11628 is in 2.8 only. SPNEGO + CNAMES. I'll see about pulling that into 2.7.x, though I'm reluctant to go near 2.6 just to keep that extra stable.

Thomas: you've got the big clusters, what versions of Hadoop will they be on by the time you look at Spark 2.0?
-Steve


Â 

  "
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Wed, 30 Mar 2016 13:57:10 +0000",RE: [discuss] ending support for Java 7 in Spark 2.0,"Tom Graves <tgraves_cs@yahoo.com>, Steve Loughran <stevel@hortonworks.com>","Maybe the question should be how far back should spark be compatible?


There is nothings stopping people to run spark 1.6.x with jdk 7 or scala 2.10 or Hadoop <2.6
But if they want spark 2.x they should consider a migration to jdk8 and scala 2.11

Or am I getting it all wrong?


Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya

From: Tom Graves [mailto:tgraves_cs@yahoo.com.INVALID]
Sent: Wednesday, March 30, 2016 4:46 PM
To: Steve Loughran <stevel@hortonworks.com>
Cc: Reynold Xin <rxin@databricks.com>; Koert Kuipers <koert@tresata.com>; Kostas Sakellis <kostas@cloudera.com>; Marcelo Vanzin <vanzin@cloudera.com>; dev@spark.apache.org
Subject: Re: [discuss] ending support for Java 7 in Spark 2.0

Steve, those are good points, I had forgotten Hadoop had those issues.    We run with jdk 8, hadoop is built for jdk7 compatibility, we are running hadoop 2.7 on our clusters and by the time Spark 2.0 is out I would expected a mix of Hadoop 2.7 and 2.8.  We also don't use spnego.

I didn't quite follow what you were saying with the hadoop services being on jdk7.  Are you saying building spark with say hadoop 2.8 libraries but your hadoop cluster is running hadoop 2.6 or less? If so I would agree that isn't a good idea.

Personally and from Yahoo point I'm still fine with going to jdk8 but I could see where other people are on older versions of Hadoop where it might be a problem.

Tom

On Wednesday, March 30, 2016 5:42 AM, Steve Loughran <stevel@hortonworks.com<mailto:stevel@hortonworks.com>> wrote:


Can I note that if Spark 2.0 is going to be Java 8+ only, then that means Hadoop 2.6.x should be the minimum Hadoop version.

https://issues.apache.org/jira/browse/HADOOP-11090

Where things get complicated, is that situation of: Hadoop services on Java 7, Spark on Java 8 in its own JVM

I'm not sure that you could get away with having the newer version of the Hadoop classes in the spark assembly/lib dir, without coming up against incompatibilities with the Hadoop JNI libraries. These are currently backwards compatible, but trying to link up Hadoop 2.7 against a Hadoop 2.6 hadoop lib will generate an UnsatisfiedLinkException. Meaning: the whole cluster's hadoop libs have to be in sync, or at least the main cluster release in a version of hadoop 2.x >= the spark bundled edition.

Ignoring that detail,

Hadoop 2.6.1+
Guava >= 15? 17?

 I think the outcome of Hadoop < 2.6 and JDK >= 8 is ""undefined""; all bug reports will be met with a ""please upgrade, re-open if the problem is still there"".

Kerberos is  a particular troublespot here : You need Hadoop 2.6.1+ for Kerberos to work in Java 8 and recent versions of Java 7 (HADOOP-10786)

Note also that HADOOP-11628 is in 2.8 only. SPNEGO + CNAMES. I'll see about pulling that into 2.7.x, though I'm reluctant to go near 2.6 just to keep that extra stable.


Thomas: you've got the big clusters, what versions of Hadoop will they be on by the time you look at Spark 2.0?

-Steve





"
Koert Kuipers <koert@tresata.com>,"Wed, 30 Mar 2016 10:21:28 -0400",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Sean Owen <sowen@cloudera.com>,"â€‹about that pro, i think it's more the opposite: â€‹many libraries have
stopped maintaining scala 2.10 versions. bugs will no longer be fixed for
scala 2.10 and new libraries will not be available for scala 2.10 at all,
making  them unusable in spark.

take for example akka, a distributed messaging library spark is build on.
the newest version does not support scala 2.10 today.

also since the intention is to support scala 2.12 at some point as well for
spark 2, the burden of supporting 3 scala versions would be significant.


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 30 Mar 2016 08:36:40 -0600",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Sean Owen <sowen@cloudera.com>,"Dropping Scala 2.10 support has to happen at some point, so I'm not
fundamentally opposed to the idea; but I've got questions about how we go
about making the change and what degree of negative consequences we are
willing to accept.  Until now, we have been saying that 2.10 support will
be continued in Spark 2.0.0.  Switching to 2.11 will be non-trivial for
some Spark users, so abruptly dropping 2.10 support is very likely to delay
migration to Spark 2.0 for those users.

What about continuing 2.10 support in 2.0.x, but repeatedly making an
obvious announcement in multiple places that such support is deprecated,
that we are not committed to maintaining it throughout 2.x, and that it is,
in fact, scheduled to be removed in 2.1.0?


"
Cody Koeninger <cody@koeninger.org>,"Wed, 30 Mar 2016 09:44:03 -0500",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Mark Hamstra <mark@clearstorydata.com>,"I agree with Mark in that I don't see how supporting scala 2.10 for
spark 2.0 implies supporting it for all of spark 2.x

Regarding Koert's comment on akka, I thought all akka dependencies
have been removed from spark after SPARK-7997 and the recent removal
of external/akka


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 30 Mar 2016 15:01:57 +0000",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,"Cody Koeninger <cody@koeninger.org>, Mark Hamstra <mark@clearstorydata.com>","Yeah it is not crazy to drop support for something foundational like this
in a feature release but is something ideally coupled to a major release.
You could at least say it is probably a decision to keep supporting through
the end of the year given how releases are likely to go. Given the
availability of the 'right' time to do it in the near future, does the
value of passing that up to extend support for 3-6 more months outweigh the
negatives ? I guess I think 2.10 is already about as droppable as it will
get so it doesn't buy much. It is an option.


"
Koert Kuipers <koert@tresata.com>,"Wed, 30 Mar 2016 11:10:38 -0400",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Cody Koeninger <cody@koeninger.org>,"Spark still runs on akka. So if you want the benefits of the latest akka
(not saying we do, was just an example) then you need to drop scala 2.10

"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 30 Mar 2016 15:47:19 +0000",Spark SQL UDF Returning Rows,dev <dev@spark.apache.org>,"Hi all,

I've been trying for the last couple of days to define a UDF which takes in
a deeply nested Row object and performs some extraction to pull out a
portion of of the Row and return it. This row object is nested not just
with StructTypes but a bunch of ArrayTypes and MapTypes. From this complex
Row object I pull out what is essentially still a nested row with multiple
levels of depth. I'm just partially extracting the nested row.

After trying a variety of things, I've teased out the following constraints:

- UDFs can pretty much only take in Primitives, Seqs, Maps and Row objects
as parameters. I cannot take in a case class object in place of the
corresponding Row object, even if the schema matches because the Row object
will always be passed in at Runtime and it will yield a ClassCastException.
- UDFs cannot return a Row object (understandably because you have no idea
of the schema being output in that case) but they can return a case object
which I can only assume will be restructured as a StructType after being
returned.

This results in a really weird/non-performant workflow when writing my UDF
because I'd have to perform all of my extraction on the Row objects. Then
convert that Row object to a case object manually (extracting everything
from the row once I've extracted the subobjects and building the deeply
nested case object) only to return it and have Spark convert that case
object to a Row object. Before I go ahead and do that I have a few
questions:

1) Is there any way to return a Row object in scala from a UDF and specify
the known schema that would be returned at UDF registration time? In
python/java this seems to be the case because you need to explicitly
specify return DataType of your UDF but using scala functions this isn't
possible. I guess I could use the Java UDF1/2/3... API but I wanted to see
if there was a first class scala way to do this.

2) Is Spark actually converting the returned case class object when the UDF
is called, or does it use the fact that it's essentially ""Product"" to
efficiently coerce it to a Row in some way?
    2.1) If this is the case, we could just take in a case object as a
parameter (rather than a Row) and perform manipulation on that and return
it. Is this explicitly something we avoided?

Thanks,
Hamel
"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 30 Mar 2016 09:50:27 -0600",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Koert Kuipers <koert@tresata.com>,"No, with 2.0 Spark really doesn't use Akka:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L744


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 30 Mar 2016 09:56:25 -0600",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Sean Owen <sowen@cloudera.com>,"My concern is that for some of those stuck using 2.10 because of some
library dependency, three months isn't sufficient time to refactor their
infrastructure to be compatible with Spark 2.0.0 if that requires Scala
2.11.  The additional 3-6 months would make it much more feasible for those
users to stay onboard for 2.0.x using the deprecated 2.10 support,
migrating to 2.11 by the release of Spark 2.1.0.


"
Koert Kuipers <koert@tresata.com>,"Wed, 30 Mar 2016 12:08:10 -0400",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Mark Hamstra <mark@clearstorydata.com>,"oh wow, had no idea it got ripped out


"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 30 Mar 2016 12:21:27 -0400",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Koert Kuipers <koert@tresata.com>,"I agree that putting it in 2.0 doesn't mean keeping Scala 2.10 for the entire 2.x line. My vote is to keep Scala 2.10 in Spark 2.0, because it's the default version we built with in 1.x. We want to make the transition from 1.x to 2.0 as easy as possible. In 2.0, we'll have the default downloads be for Scala 2.11, so people will more easily move, but we shouldn't create obstacles that lead to fragmenting the community and slowing down Spark 2.0's adoption. I've seen companies that stayed on an old Scala version for multiple years because switching it, or mixing versions, would affect the company's entire codebase.

Matei

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L744 <https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SparkConf.scala#L744>
akka (not saying we do, was just an example) then you need to drop scala 2.10
we go
are
will be
some
delay
an
deprecated,
it is,
discussion
so
will
come
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Hamel Kothari <hamelkothari@gmail.com>,"Wed, 30 Mar 2016 17:34:32 +0000",Re: Spark SQL UDF Returning Rows,dev <dev@spark.apache.org>,"Just to clarify, this is possible via UDF1/2/3 etc and registering those
with the desired return schema. It just felt wrong that the only way to do
this in scala was to use these classes which were in the Java package.
Maybe the relevant question is, why are these in a Java package?


"
Josh Rosen <joshrosen@databricks.com>,"Wed, 30 Mar 2016 17:47:13 +0000",Re: Understanding PySpark Internals,"Adam Roberts <AROBERTS@uk.ibm.com>, dev@spark.apache.org","that Python UDFs and RDD API code can be executed. Some slightly-outdated
but mostly-correct reference material for this can be found at
https://cwiki.apache.org/confluence/display/SPARK/PySpark+Internals.

See also: search the Spark codebase for PythonRDD and look at
python/pyspark/worker.py


"
Michael Armbrust <michael@databricks.com>,"Wed, 30 Mar 2016 12:25:13 -0700",Re: Discuss: commit to Scala 2.10 support for Spark 2.x lifecycle,Matei Zaharia <matei.zaharia@gmail.com>,"+1 to Matei's reasoning.


"
Michael Armbrust <michael@databricks.com>,"Wed, 30 Mar 2016 12:47:12 -0700",Re: Spark SQL UDF Returning Rows,Hamel Kothari <hamelkothari@gmail.com>,"Some answers and more questions inline

- UDFs can pretty much only take in Primitives, Seqs, Maps and Row objects


This is true today, but could be improved using the new encoder framework.
Out of curiosity, have you look at that?  If so, what is missing thats
leading you back to UDFs.

Is there any way to return a Row object in scala from a UDF and specify the


I think UDF1/2/3 are the only way to do this today.  Is the problem here
that you are only changing a subset of the nested data and you want to
preserve the structure.  What kind of changes are you doing?

2) Is Spark actually converting the returned case class object when the UDF
We use reflection to figure out the schema and extract the data into the
internal row format.  We actually runtime build bytecode for this in many
cases (though not all yet) so it can be pretty fast.




You can do this with Datasets:

df.as[CaseClass].map(o => do stuff)
"
Sean Busbey <busbey@cloudera.com>,"Wed, 30 Mar 2016 15:02:30 -0500",Re: Any documentation on Spark's security model beyond YARN?,"""dev@spark.apache.org"" <dev@spark.apache.org>","ote:
e:
 its security from the underlying YARN job.
out some use cases.
g about how to enhance sparkâ€™s security.
rvices, alongside hdfs

I've been looking at this as well. The current work-around I'm using
is to use keytab logins on the executors, which is less than
desirable.

Since the HBase project maintains Spark integration points, it'd be
great if there were just a hook for services to provide ""here's how to
renew"" to a common renewal service.



-- 
busbey

---------------------------------------------------------------------


"
"""write2sivakumar@gmail"" <write2sivakumar@gmail.com>","Thu, 31 Mar 2016 10:58:21 +0800",Re: aggregateByKey on PairRDD,"Daniel Haviv <daniel.haviv@veracity-group.com>, Akhil Das
 <akhil@sigmoidanalytics.com>","
    
Hi,
We can use CombineByKey to achieve this.
val finalRDD = tempRDD.combineByKey((x: (Any, Any)) => (x),(acc: (Any, Any), x) => (acc, x),(acc1: (Any, Any), acc2: (Any, Any)) => (acc1, acc2))
finalRDD.collect.foreach(println)
(amazon,((book1, tech),(book2,tech)))(barns&noble, (book,tech))(eBay, (book1,tech))
Thanks,Sivakumar

-------- Original message --------
From: Daniel Haviv <daniel.haviv@veracity-group.com> 
Date: 30/03/2016  18:58  (GMT+08:00) 
To: Akhil Das <akhil@sigmoidanalytics.com> 
Cc: Suniti Singh <suniti.singh@gmail.com>, user@spark.apache.org, dev <dev@spark.apache.org> 
Subject: Re: aggregateByKey on PairRDD 

Hi,shouldn't groupByKey be avoided (https://databricks.gitbooks.io/databricks-spark-knowledge-base/content/best_practices/prefer_reducebykey_over_groupbykey.html) ?

Thank you,.Daniel
On Wed, Mar 30, 2016 at 9:01 AM, Akhil Das <akhil@sigmoidanalytics.com> wrote:
Isn't it what tempRDD.groupByKey does?Â 
ThanksBest Regards

On Wed, Mar 30, 2016 at 7:36 AM, Suniti Singh <suniti.singh@gmail.com> wrote:
Hi All,
I have an RDD having the data in Â the following form :








tempRDD: RDD[(String, (String, String))](brand , (product, key))(""amazon"",(""book1"",""tech""))(""eBay"",(""book1"",""tech""))
(""barns&noble"",(""book"",""tech""))
(""amazon"",(""book2"",""tech""))
I would like to group the data by Brand and would like to get the result set in the following format :resultSetRDD :Â RDD[(String, List[(String), (String)]i tried using the aggregateByKey but kind Â of not getting how to achieve this. OR is there any other way to achieve this?







valÂ resultSetRDD Â =Â tempRDD.aggregateByKey("""")({case (aggr , value) => aggr + String.valueOf(value) + "",""}, (aggr1, aggr2) => aggr1 + aggr2)resultSetRDD = (amazon,(""book1"",""tech""),(""book2"",""tech""))Thanks,Suniti




"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Thu, 31 Mar 2016 09:00:51 +0000",Question Create External table location S3,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I pulled the latest version

git pull git://github.com/apache/spark.git

Compiled:
mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package


now I am getting the following error:
Error: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: java.io.IOException No FileSystem for scheme: s3n) (state=,code=0)

Did anyone else experience this?


Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 31 Mar 2016 11:32:44 +0000",Re: Any documentation on Spark's security model beyond YARN?,Sean Busbey <busbey@cloudera.com>,"
> On 30 Mar 2016, at 21:02, Sean Busbey <busbey@cloudera.com> wrote:
> 
> On Wed, Mar 30, 2016 at 4:33 AM, Steve Loughran <stevel@hortonworks.com> wrote:
>> 
>>> On 29 Mar 2016, at 22:19, Michael Segel <msegel_hadoop@hotmail.com> wrote:
>>> 
>>> Hi,
>>> 
>>> So yeah, I know that Spark jobs running on a Hadoop cluster will inherit its security from the underlying YARN job.
>>> Howeverâ€¦ thatâ€™s not really saying much when you think about some use cases.
>>> 
>>> Like using the thrift service â€¦
>>> 
>>> Iâ€™m wondering what else is new and what people have been thinking about how to enhance sparkâ€™s security.
>>> 
>> 
>> Been thinking a bit.
>> 
>> One thing to look at is renewal of hbase and hive tokens on long-lived services, alongside hdfs
>> 
>> 
> 
> I've been looking at this as well. The current work-around I'm using
> is to use keytab logins on the executors, which is less than
> desirable.


OK, let's work together on this ... the current spark renewal code assumes its only for HDFS (indeed, that the filesystem is HDFS and therefore the #of tokens > 0); there' s no fundamental reason why the code in YarnSparkHadoopUtils can't run in the AM too.

> 
> Since the HBase project maintains Spark integration points, it'd be
> great if there were just a hook for services to provide ""here's how to
> renew"" to a common renewal service.
> 

1. Wittenauer is doing some work on a tool for doing this; I'm pushing for it to be a fairly generic API. Even if Spark has to use reflection to get at it, at least it would be consistent across services. See https://issues.apache.org/jira/browse/HADOOP-12563

2. The topic of HTTPS based acquisition/use of HDFS tokens has arisen elsewhere; needed for long-haul job submission when  you don' t have a keytab to hand. This could be useful as it'd avoid actually needing hbase-*.jar on the classpath at submit time.


> 
> 
> -- 
> busbey
> 
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
> For additional commands, e-mail: dev-help@spark.apache.org
> 
> 

"
Steve Loughran <stevel@hortonworks.com>,"Thu, 31 Mar 2016 11:39:50 +0000",Re: Question Create External table location S3,Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"

Hi,

I pulled the latest version

git pull git://github.com/apache/spark.git

Compiled:
mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package


now I am getting the following error:
Error: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: java.io.IOException No FileSystem for scheme: s3n) (state=,code=0)

Did anyone else experience this?


Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>


You are going to need to add hadoop-aws.JAR to your classpath, along with amazon's aws-java-sdk v 1.7.4 on your CP

I am actually working on a PR to add the hadoop-aws, openstack and (hadoop 2.7+) azure JARs to spark-assembly, but you'll still need to add the relevant amazon SDK (which has proven brittle across versions):

https://github.com/apache/spark/pull/12004

It's not ready yet; once I've got the 2.7 profile working with tests for openstack and azure, *and documentation on use and testing* then you'll be able to play with.
"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Thu, 31 Mar 2016 11:50:48 +0000",Re: Question Create External table location S3,"""stevel@hortonworks.com"" <stevel@hortonworks.com>","Thanks for the insites
Ill try to add it

Sent from Outlook Mobile<https://aka.ms/blhgte>






Hi,

I pulled the latest version

git pull git://github.com/apache/spark.git

Compiled:
mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package


now I am getting the following error:
Error: org.apache.spark.sql.execution.QueryExecutionException: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:Got exception: java.io.IOException No FileSystem for scheme: s3n) (state=,code=0)

Did anyone else experience this?


Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya


[Read More]<http://feeds.feedburner.com/~r/sizmek-blog/~6/1>

[http://www.sizmek.com/Sizmek.png]<http://www.sizmek.com/>


You are going to need to add hadoop-aws.JAR to your classpath, along with amazon's aws-java-sdk v 1.7.4 on your CP

I am actually working on a PR to add the hadoop-aws, openstack and (hadoop 2.7+) azure JARs to spark-assembly, but you'll still need to add the relevant amazon SDK (which has proven brittle across versions):

https://github.com/apache/spark/pull/12004

It's not ready yet; once I've got the 2.7 profile working with tests for openstack and azure, *and documentation on use and testing* then you'll be able to play with.
"
Steve Loughran <stevel@hortonworks.com>,"Thu, 31 Mar 2016 11:52:55 +0000","Jenkins PR failing, Mima unhappy: bad constant pool tag 50 at byte
 12","""dev@spark.apache.org"" <dev@spark.apache.org>","
A WiP PR of mine is failing in mima: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/54525/consoleFull

[info] spark-examples: previous-artifact not set, not analyzing binary compatibility
java.lang.RuntimeException: bad constant pool tag 50 at byte 12
at com.typesafe.tools.mima.core.ClassfileParser$ConstantPool.errorBadTag(ClassfileParser.scala:204)
at com.typesafe.tools.mima.core.ClassfileParser$ConstantPool.<init>(ClassfileParser.scala:106)
at com.typesafe.tools.mima.core.ClassfileParser.parseAll(ClassfileParser.scala:67)
at com.typesafe.tools.mima.core.ClassfileParser.parse(ClassfileParser.scala:59)
at com.typesafe.tools.mima.core.ClassInfo.ensureLoaded(ClassInfo.scala:86)
at com.typesafe.tools.mima.core.ClassInfo.methods(ClassInfo.scala:101)
at com.typesafe.tools.mima.core.ClassInfo$$anonfun$lookupClassMethods$2.apply(ClassInfo.scala:123)
at com.typesafe.tools.mima.core.ClassInfo$$anonfun$lookupClassMethods$2.apply(ClassInfo.scala:123)

...

That's the kind of message which hints at some kind of JVM versioning mismatch, but, AFAIK, I'm (a) just pulling in java 6/7 libraries and (b) skipping the hadoop-2.6+ module anyway.

Any suggestions to make the stack trace go away
"
Hamel Kothari <hamelkothari@gmail.com>,"Thu, 31 Mar 2016 12:23:39 +0000",Re: Spark SQL UDF Returning Rows,Michael Armbrust <michael@databricks.com>,"Hi Michael,

Thanks for the response. I am just extracting part of the nested structure
and returning only a piece that same structure.

I haven't looked at Encoders or Datasets since we're bound to 1.6 for now
but I'll look at encoders to see if that covers it. Datasets seems like it
would solve this problem for sure.

I avoided returning a case object because even if we use reflection to
build byte code and do it efficiently. I still need to convert my Row to a
case object manually within my UDF, just to have it converted to a Row
again. Even if it's fast, it's still fairly necessary.

The thing I guess that threw me off was that UDF1/2/3 was in a ""java""
prefixed package although there was nothing that made it java specific and
in fact was the only way to do what I wanted in scala. For things like
JavaRDD, etc it makes sense, but for generic things like UDF is there a
reason they get put into a package with ""java"" in the name?

Regards,
Hamel


"
Steve Johnston <sjohnston@algebraixdata.com>,"Thu, 31 Mar 2016 09:26:28 -0700 (MST)",What influences the space complexity of Spark operations?,dev@spark.apache.org,"*What weâ€™ve observed*
Increasing the number of partitions (and thus decreasing the partition size)
seems to reliably help avoid OOM errors. To demonstrate this we used a
single executor and loaded a small table into a DataFrame, persisted it with
MEMORY_AND_DISK, repartitioned it and joined it to itself. Varying the
number of partitions identifies a threshold between completing the join and
incurring an OOM error. 
lineitem = sc.textFile('lineitem.tbl').map(converter)lineitem =
sqlContext.createDataFrame(lineitem,
schema)lineitem.persist(StorageLevel.MEMORY_AND_DISK)repartitioned =
lineitem.repartition(partition_count)joined =
repartitioned.join(repartitioned)joined.show() 
*Questions*
 Generally, what influences the space complexity of Spark operations? Is it
the case that a single partition of each operandâ€™s data set + a single
partition of the resulting data set all need to fit in memory at the same
time? We can see where the transformations (for say joins) are implemented
in the source code (for the example above BroadcastNestedLoopJoin), but they
seem to be based on virtualized iterators; where in the code is the
partition data for the inputs and outputs actually materialized?



--
3.nabble.com/What-influences-the-space-complexity-of-Spark-operations-tp16944.html
om."
Davies Liu <davies@databricks.com>,"Thu, 31 Mar 2016 16:41:32 -0700",Re: Making BatchPythonEvaluation actually Batch,Justin Uang <justin.uang@gmail.com>,"@Justin, it's fixed by https://github.com/apache/spark/pull/12057

e:
tch
s a
ot a
nce
ver,
ly a
ff40fb309fc)
e
""))
ost
ing
f
hem down
same
g
 20
e,
s
€™s variables,
uld
â€)).select(F.col(â€œcol1x2â€),
). To get around that, I add a
gh a
f
plex. The way"
