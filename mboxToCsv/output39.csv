Sean Owen <sowen@cloudera.com>,"Mon, 1 Aug 2016 07:07:17 -0700",Default date formats in CSV and JSON: see SPARK-16216,dev <dev@spark.apache.org>,"https://issues.apache.org/jira/browse/SPARK-16216
https://github.com/apache/spark/pull/14279

This concerns default representation of times and dates in CSV and
JSON. CSV has UNIX timestamps; JSON has formatted strings but
unfortunately they lack timezones.

The question here is which to change to do what; it was a significant
enough potential change that I wanted to draw attention to it for more
comment.

Sean

---------------------------------------------------------------------


"
Martin Le <martin.lequoc@gmail.com>,"Mon, 1 Aug 2016 18:24:52 +0200",Re: sampling operation for DStream,Cody Koeninger <cody@koeninger.org>,"Hi Cody and all,

Thank you for your answer. I implement simple random sampling (SRS) for
DStream using transform method, and it works fine.
However, I have a problem when I implement reservoir sampling (RS). In RS,
I need to maintain a reservoir (a queue) to store selected data items
(RDDs). If I define a large stream window, the queue also increases  and it
leads to the driver run out of memory.  I explain my problem in detail
here:
https://docs.google.com/document/d/1YBV_eHH6U_dVF1giiajuG4ayoVN5R8Qw3IthoKvW5ok

Could you please give me some suggestions or advice to fix this problem?

Thanks


"
Cody Koeninger <cody@koeninger.org>,"Mon, 1 Aug 2016 11:43:44 -0500",Re: sampling operation for DStream,Martin Le <martin.lequoc@gmail.com>,"Can you keep a queue per executor in memory?


---------------------------------------------------------------------


"
Martin Le <martin.lequoc@gmail.com>,"Mon, 1 Aug 2016 22:22:10 +0200",Re: sampling operation for DStream,Cody Koeninger <cody@koeninger.org>,"How to do that? if I put the queue inside .transform operation, it
doesn't work.


"
Cody Koeninger <cody@koeninger.org>,"Mon, 1 Aug 2016 16:01:31 -0500",Re: sampling operation for DStream,Martin Le <martin.lequoc@gmail.com>,"Put the queue in a static variable that is first referenced on the
workers (inside an rdd closure).  That way it will be created on each
of the workers, not the driver.

Easiest way to do that is with a lazy val in a companion object.


---------------------------------------------------------------------


"
Hao Ren <invkrh@gmail.com>,"Tue, 2 Aug 2016 00:29:23 +0200",[MLlib] Term Frequency in TF-IDF seems incorrect,"user <user@spark.apache.org>, dev <dev@spark.apache.org>","When computing term frequency, we can use either HashTF or CountVectorizer
feature extractors.
However, both of them just use the number of times that a term appears in a
document.
It is not a true frequency. Acutally, it should be divided by the length of
the document.

Is this a wanted feature ?

-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 2 Aug 2016 03:08:58 +0200",What happens in Dataset limit followed by rdd,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi everyone,

This doesn't look like something expected, does it?

http://stackoverflow.com/q/38710018/1560062

Quick glance at the UI suggest that there is a shuffle involved and
input for first is ShuffledRowRDD.

-- 
Best regards,
Maciej Szymkiewicz

"
kevin <kiss.kevin119@gmail.com>,"Tue, 2 Aug 2016 09:49:55 +0800",Re: tpcds for spark2.0,Olivier Girardot <o.girardot@lateral-thoughts.com>,"finally I use  spark-sql-perf-0.4.3 :
./bin/spark-shell --jars
/home/dcos/spark-sql-perf-0.4.3/target/scala-2.11/spark-sql-perf_2.11-0.4.3.jar
--executor-cores 4 --executor-memory 10G --master spark://master1:7077
If I don't use ""--jars"" I will get error what I mentioned.

2016-07-29 21:17 GMT+08:00 Olivier Girardot <o.girardot@lateral-thoughts.com

t =
n
n
,
D
"
Yanbo Liang <ybliang8@gmail.com>,"Mon, 1 Aug 2016 20:44:04 -0700",Re: [MLlib] Term Frequency in TF-IDF seems incorrect,Hao Ren <invkrh@gmail.com>,"Hi Hao,

HashingTF directly apply a hash function (Murmurhash3) to the features to
determine their column index. It excluded any thought about the term
frequency or the length of the document. It does similar work compared with
sklearn FeatureHasher. The result is increased speed and reduced memory
usage, but it does not remember what the input features looked like and can
not convert the output back to the original features. Actually we misnamed
this transformer, it only does the work of feature hashing rather than
computing hashing term frequency.

CountVectorizer will select the top vocabSize words ordered by term
frequency across the corpus to build the hash table of the features. So it
will consume more memory than HashingTF. However, we can convert the output
back to the original feature.

Both of the transformers do not consider the length of each document. If
you want to compute term frequency divided by the length of the document,
you should write your own function based on transformers provided by MLlib.

Thanks
Yanbo

2016-08-01 15:29 GMT-07:00 Hao Ren <invkrh@gmail.com>:

"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 02 Aug 2016 09:25:46 +0000",Re: [MLlib] Term Frequency in TF-IDF seems incorrect,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Note that both HashingTF and CountVectorizer are usually used for creating
TF-IDF normalized vectors. The definition (
https://en.wikipedia.org/wiki/Tf%E2%80%93idf#Definition) of term frequency
in TF-IDF is actually the ""number of times the term occurs in the document"".

So it's perhaps a bit of a misnomer, but the implementation is correct.


"
Sun Rui <sunrise_win@163.com>,"Tue, 2 Aug 2016 17:57:46 +0800",Re: What happens in Dataset limit followed by rdd,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Based on your code, here is simpler test case on Spark 2.0

case class my (x: Int)
val rdd = sc.parallelize(0.until(10000), 1000).map { x => my(x) }
val df1 = spark.createDataFrame(rdd)
val df2 = df1.limit(1)
df1.map { r => r.getAs[Int](0) }.first
df2.map { r => r.getAs[Int](0) }.first // Much slower than the previous line

Actually, Dataset.first is equivalent to Dataset.limit(1).collect, so check the physical plan of the two cases:

scala> df1.map { r => r.getAs[Int](0) }.limit(1).explain
== Physical Plan ==
CollectLimit 1
+- *SerializeFromObject [input[0, int, true] AS value#124]
   +- *MapElements <function1>, obj#123: int
      +- *DeserializeToObject createexternalrow(x#74, StructField(x,IntegerType,false)), obj#122: org.apache.spark.sql.Row
         +- Scan ExistingRDD[x#74]

scala> df2.map { r => r.getAs[Int](0) }.limit(1).explain
== Physical Plan ==
CollectLimit 1
+- *SerializeFromObject [input[0, int, true] AS value#131]
   +- *MapElements <function1>, obj#130: int
      +- *DeserializeToObject createexternalrow(x#74, StructField(x,IntegerType,false)), obj#129: org.apache.spark.sql.Row
         +- *GlobalLimit 1
            +- Exchange SinglePartition
               +- *LocalLimit 1
                  +- Scan ExistingRDD[x#74]

For the first case, it is related to an optimisation in the CollectLimitExec physical operator. That is, it will first fetch the first partition to get limit number of row, 1 in this case, if not satisfied, then fetch more partitions, until the desired limit is reached. So generally, if the first partition is not empty, only the first partition will be calculated and fetched. Other partitions will even not be computed.

However, in the second case, the optimisation in the CollectLimitExec does not help, because the previous limit operation involves a shuffle operation. All partitions will be computed, and running LocalLimit(1) on each partition to get 1 row, and then all partitions are shuffled into a single partition. CollectLimitExec will fetch 1 row from the resulted single partition.


<http://stackoverflow.com/q/38710018/1560062>
input for first is ShuffledRowRDD. 

"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Tue, 2 Aug 2016 12:51:05 +0200",Re: What happens in Dataset limit followed by rdd,Sun Rui <sunrise_win@163.com>,"Thank you for your prompt response and great examples Sun Rui but I am
still confused about one thing. Do you see any particular reason to not
to merge subsequent limits? Following case

    (limit n (map f (limit m ds)))

could be optimized to:

    (map f (limit n (limit m ds)))

and further to

    (map f (limit (min n m) ds))

couldn't it?



-- 
Maciej Szymkiewicz


---------------------------------------------------------------------


"
Noorul Islam Kamal Malmiyoda <noorul@noorul.com>,"Tue, 2 Aug 2016 17:37:53 +0530",Re: Testing --supervise flag,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Widening to dev@spark


---------------------------------------------------------------------


"
Sun Rui <sunrise_win@163.com>,"Tue, 2 Aug 2016 20:12:26 +0800",Re: What happens in Dataset limit followed by rdd,Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Spark does optimise subsequent limits, for example:
scala> df1.limit(3).limit(1).explain
== Physical Plan ==
CollectLimit 1
+- *SerializeFromObject [assertnotnull(input[0, $line14.$read$$iw$$iw$my, true], top level non-flat input object).x AS x#2]
   +- Scan ExternalRDDScan[obj#1]

However, limit can not be simply pushes down across mapping functions, because the number of rows may change across functions. for example, flatMap()

It seems that limit can be pushed across map() which won‚Äôt change the number of rows. Maybe this is a room for Spark optimisation.

not
my(x) }
org.apache.spark.sql.Row
org.apache.spark.sql.Row
not
shuffle

"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Tue, 2 Aug 2016 12:29:10 +0000",Graph edge type pattern matching in GraphX,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

Could you suggest how to perform pattern matching on the type of the graph edge in the following scenario. I need to perform some math by means of aggregateMessages on the graph edges if edges are Double. Here is the code:
def my[VD: ClassTag, ED: ClassTag] (graph: Graph[VD, ED]): Double {
graph match {
   g: Graph[_, Double] => g.aggregateMessages[Double](t => t.sendToSrc(t.attr), _ + _).values.max
   _ => 0.0
}
}

However, it does not work, because aggregateMessages creates context t of type [VD, ED, Double]. I expect it to create context of [VD, Double, Double] because of the type pattern matching. Could you suggest what is the issue?

Best regards, Alexander
"
Bryan Cutler <cutlerb@gmail.com>,"Tue, 2 Aug 2016 13:46:31 -0700",AccumulatorV2 += operator,dev <dev@spark.apache.org>,"It seems like the += operator is missing from the new accumulator API,
although the docs still make reference to it.  Anyone know if it was
intentionally not put in?  I'm happy to do a PR for it or update the docs
to just use the add() method, just want to check if there was some reason
first.

Bryan
"
Holden Karau <holden@pigscanfly.ca>,"Tue, 2 Aug 2016 14:52:52 -0700",Re: AccumulatorV2 += operator,Bryan Cutler <cutlerb@gmail.com>,"I believe it was intentional with the idea that it would be more unified
between Java and Scala APIs. If your talking about the javadoc mention in
https://github.com/apache/spark/pull/14466/files - I believe the += is
meant to refer to what the internal implementation of the add function can
be for someone extending the accumulator (but it certainly could cause
confusion).

Reynold can provide a more definitive answer in this case.





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
=?UTF-8?B?6ams5pmT5a6H?= <hzmaxiaoyu@corp.netease.com>,"Wed, 3 Aug 2016 09:40:03 +0800",SQL Based Authorization for SparkSQL,dev <dev@spark.apache.org>,"Hi guys,

I wonder if anyone working on SQL based authorization already or not.

This is something we needed badly right now and we tried to embedded a 
Hive frontend in front of SparkSQL to achieve this but it's not quite a 
elegant solution. If SparkSQL has a way to do it or anyone already 
working on it?

If not, we might consider make some contributions here and might need 
guidance during the work.

Thanks.

Shawn



---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 2 Aug 2016 21:00:41 -0700",Re: SQL Based Authorization for SparkSQL,=?UTF-8?B?6ams5pmT5a6H?= <hzmaxiaoyu@corp.netease.com>,"There was SPARK-12008 which was closed.

Not sure if there is active JIRA in this regard.


g
"
satyajit vegesna <satyajit.apasprk@gmail.com>,"Tue, 2 Aug 2016 21:11:10 -0700","Spark on yarn, only 1 or 2 vcores getting allocated to the containers
 getting created.","user@spark.apache.org, dev@spark.apache.org","Hi All,

I am trying to run a spark job using yarn, and i specify --executor-cores
value as 20.
But when i go check the ""nodes of the cluster"" page in
http://hostname:8088/cluster/nodes then i see 4 containers getting created
on each of the node in cluster.

But can only see 1 vcore getting assigned for each containier, even when i
specify --executor-cores 20 while submitting job using spark-submit.

yarn-site.xml
<property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>60000</value>
</property>
<property>
        <name>yarn.scheduler.minimum-allocation-vcores</name>
        <value>1</value>
</property>
<property>
        <name>yarn.scheduler.maximum-allocation-vcores</name>
        <value>40</value>
</property>
<property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>70000</value>
</property>
<property>
        <name>yarn.nodemanager.resource.cpu-vcores</name>
        <value>20</value>
</property>


Did anyone face the same issue??

Regards,
Satyajit.
"
Saisai Shao <sai.sai.shao@gmail.com>,"Wed, 3 Aug 2016 15:53:29 +0800","Re: Spark on yarn, only 1 or 2 vcores getting allocated to the
 containers getting created.",satyajit vegesna <satyajit.apasprk@gmail.com>,"Use dominant resource calculator instead of default resource calculator
will get the expected vcores as you wanted. Basically by default yarn does
not honor cpu cores as resource, so you will always see vcore is 1 no
matter what number of cores you set in spark.


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Wed, 03 Aug 2016 08:49:50 +0000",Spark SQL and Kryo registration,"""dev.spark"" <dev@spark.apache.org>","Hi everyone, I'm currently to use Spark 2.0.0 and making Dataframes work with
kryo.registrationRequired=true Is it even possible at all considering the codegen ?
Regards,
Olivier Girardot | Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
Maciej Szymkiewicz <mszymkiewicz@gmail.com>,"Wed, 3 Aug 2016 14:04:39 +0200",Re: What happens in Dataset limit followed by rdd,Sun Rui <sunrise_win@163.com>,"Pushing down across mapping would be great. If you're used to SQL or
work frequently with lazy collections this is a behavior you learn to
expect.

ge the

t
) }

ow
ow
t


e


-- 
Maciej Szymkiewicz

"
Soumitra Johri <soumitra.siddharth@gmail.com>,"Wed, 03 Aug 2016 14:42:30 +0000",How does MapWithStateRDD distribute the data,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","Hi,

I am running a steaming job with 4 executors and 16 cores so that each
executor has two cores to work with. The input Kafka topic has 4 partitions.
With this given configuration I was expecting MapWithStateRDD to be evenly
distributed across all executors, how ever I see that it uses only two
executors on which MapWithStateRDD data is distributed. Sometimes the data
goes only to one executor.

How can this be explained and pretty sure there would be some math to
understand this behavior.

I am using the standard standalone 1.6.2 cluster.

Thanks
Soumitra
"
Bryan Cutler <cutlerb@gmail.com>,"Wed, 3 Aug 2016 08:05:50 -0700",Re: AccumulatorV2 += operator,Holden Karau <holden@pigscanfly.ca>,"No, I was referring to the programming guide section on accumulators, it
says "" Tasks running on a cluster can then add to it using the add method
or the += operator (in Scala and Python).""


"
Cody Koeninger <cody@koeninger.org>,"Wed, 3 Aug 2016 11:34:06 -0500",Re: How does MapWithStateRDD distribute the data,Soumitra Johri <soumitra.siddharth@gmail.com>,"Are you using KafkaUtils.createDirectStream?


---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Wed, 3 Aug 2016 10:02:27 -0700",Re: AccumulatorV2 += operator,Bryan Cutler <cutlerb@gmail.com>,"Ah in that case the programming guides text is still talking about the
deprecated accumulator API despite having an updated code sample (the way
it suggests making an accumulator is also deprecated). I think the fix is
updating the programming guide rather than adding += to the API.



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Tao Li <tli@hortonworks.com>,"Wed, 3 Aug 2016 22:30:31 +0000",Quick question about hive-exec 1.2.1.spark2,dev <dev@spark.apache.org>,"Hi,

The spark-hive module has a dependency on hive-exec module (a custom built module from ""Hive on Spark‚Äù project). Can someone point me to the source code repo of the hive-exec module? Thanks.

Here is the maven repo link: https://mvnrepository.com/artifact/org.spark-project.hive/hive-exec/1.2.1.spark2
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 4 Aug 2016 17:14:05 +0200",Re: Spark SQL and Kryo registration,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Hi Olivier,

I don't know either, but am curious what you've tried already.

Jacek


"
Amit Sela <amitsela33@gmail.com>,"Thu, 04 Aug 2016 15:41:44 +0000",Re: Spark SQL and Kryo registration,"Jacek Laskowski <jacek@japila.pl>, Olivier Girardot <o.girardot@lateral-thoughts.com>","It should. Codegen uses the SparkConf in SparkEnv when instantiating a new
Serializer.


"
Fred Reiss <freiss.oss@gmail.com>,"Thu, 4 Aug 2016 16:38:32 -0700",Source API requires unbounded distributed storage?,dev@spark.apache.org,"Hi,

I've been looking over the Source API in
org.apache.spark.sql.execution.streaming, and I'm at a loss for how the
current API can be implemented in a practical way. The API defines a single
getBatch() method for fetching records from the source, with the following
Scaladoc comments defining the semantics:


*/**  * Returns the data that is between the offsets (*`*start*`*, *`*end*`*].
When *`*start*` *is *`*None*`

*then  * the batch should begin with the first available record. This
method must always return the  * same data for a particular *`*start*` *and
*`*end*`
*pair.  */*
* def *getBatch(start: Option[Offset], end: Offset): DataFrame

If I read the semantics described here correctly, a Source is required to
retain all past history for the stream that it backs. Further, a Source is
also required to retain this data across restarts of the process where the
Source is instantiated, even when the Source is restarted on a different
machine.

The current implementation of FileStreamSource follows my reading of the
requirements above. FileStreamSource never deletes a file.

I feel like this requirement for unbounded state retention must be a
mistake or misunderstanding of some kind. The scheduler is internally
maintaining a high water mark (StreamExecution.committedOffsets in
StreamExecution.scala) of data that has been successfully processed. There
must have been an intent to communicate that high water mark back to the
Source so that the Source can clean up its state. Indeed, the DataBricks
blog post from last week (
https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html)
Structured Streaming will maintain its own internal state after that.""

But the code checked into git and shipped with Spark 2.0 does not have an
API call for the scheduler to tell a Source where the boundary of ""only a
few minutes' worth of data"" lies.

Is there a JIRA that I'm not aware of to change the Source API? If not,
should we maybe open one?

Fred
"
Michael Armbrust <michael@databricks.com>,"Thu, 4 Aug 2016 16:52:55 -0700",Re: Source API requires unbounded distributed storage?,Fred Reiss <freiss.oss@gmail.com>,"Yeah, this API is in the private execution package because we are planning
to continue to iterate on it.  Today, we will only ever go back one batch,
though that might change in the future if we do async checkpointing of
internal state.

You are totally right that we should relay this info back to the source.
Opening a JIRA sounds like a good first step.


treaming,
al
*
nd
e
ming will
"
Hyukjin Kwon <gurwls223@gmail.com>,"Fri, 5 Aug 2016 10:26:21 +0900","Inquery about Spark's behaviour for configurations in Hadoop
 configuration instance via read/write.options()",dev <dev@spark.apache.org>,"Hi all,


If my understanding is correct, now Spark supports to set some options to
Hadoop configuration instance via read/write.option(..) API.

However, I recently saw some comments and opinion about this. If I
understood them correctly, it was as below:

   -

   Respecting all the configurations in Hadoop configuration instance
   including what Spark configurations/options cover and Spark
   configurations/options will override it if the equivalent ones are
   duplicated
   -

   Not respecting the configurations in the instance that Spark
   configurations/options cover, meaning using the default value in Spark
   configuration regardless of the values set in this instance for the
   equivalent ones.

For example, now, Spark is supporting compression for ORC as an option but
currently we are not respecting orc.compress. This is being ignored
(meaning following the latter case).

Maybe I understood those comments and opinions wrongly and might be a dump
question from my misunderstanding but I would really appreciate that if
anyone helps me to know which one is correct.
‚Äã

Thanks !
"
Sean Owen <sowen@cloudera.com>,"Thu, 4 Aug 2016 21:15:57 -0700","We don't use ASF Jenkins builds, right?",dev <dev@spark.apache.org>,"There was a recent message about deprecating many Maven, ant and JDK
combos for ASF Jenkins machines, and I was just triple-checking we're
only making use of the Amplab ones.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 5 Aug 2016 12:19:07 +0800","Re: We don't use ASF Jenkins builds, right?",Sean Owen <sowen@cloudera.com>,"We don't.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 05 Aug 2016 04:56:01 +0000",PySpark: Make persist() return a context manager,Spark dev list <dev@spark.apache.org>,"Context managers
<https://docs.python.org/3/reference/datamodel.html#context-managers> are a
natural way to capture closely related setup and teardown code in Python.

For example, they are commonly used when doing file I/O:

with open('/path/to/file') as f:
    contents = f.read()
    ...


Does it make sense to apply this pattern to persisting and unpersisting
DataFrames and RDDs? I feel like there are many cases when you want to
persist a DataFrame for a specific set of operations and then unpersist it
immediately afterwards.

For example, take model training. Today, you might do something like this:

labeled_data.persist()
model = pipeline.fit(labeled_data)
labeled_data.unpersist()

If persist() returned a context manager, you could rewrite this as follows:

with labeled_data.persist():
    model = pipeline.fit(labeled_data)

Upon exiting the with block, labeled_data would automatically be
unpersisted.

This can be done in a backwards-compatible way since persist() would still
return the parent DataFrame or RDD as it does today, but add two methods to
the object: __enter__() and __exit__()

Does this make sense? Is it attractive?

Nick
‚Äã
"
Reynold Xin <rxin@databricks.com>,"Fri, 5 Aug 2016 14:08:26 +0800",Re: PySpark: Make persist() return a context manager,Nicholas Chammas <nicholas.chammas@gmail.com>,"Sounds like a great idea!


n.
t
:
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Fri, 5 Aug 2016 09:55:27 +0200",Result code of whole stage codegen,Spark dev list <dev@spark.apache.org>,"Hi,
I have some operation on DataFrame / Dataset.
How can I see source code for whole stage codegen ?
Is there any API for this ? Or maybe I should configure log4j in specific
way ?

Regards,
-- 
Maciek Bry≈Ñski
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Fri, 5 Aug 2016 10:06:52 +0200",Re: Result code of whole stage codegen,=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Do you want to see the code that whole stage codegen produces?

You can prepend a SQL statement with EXPLAIN CODEGEN ...

Or you can add the following code to a DataFrame/Dataset command:

import org.apache.spark.sql.execution.debug._

and call the the debugCodegen() command on a Dataframe/Dataset, for example:

range(0, 100).debugCodegen

...

Found 1 WholeStageCodegen subtrees.

== Subtree 1 / 1 ==

*Range (0, 100, splits=8)


Generated code:

/* 001 */ public Object generate(Object[] references) {

/* 002 */   return new GeneratedIterator(references);

/* 003 */ }

/* 004 */

/* 005 */ final class GeneratedIterator extends
org.apache.spark.sql.execution.BufferedRowIterator {

/* 006 */   private Object[] references;

/* 007 */   private org.apache.spark.sql.execution.metric.SQLMetric
range_numOutputRows;

/* 008 */   private boolean range_initRange;

/* 009 */   private long range_partitionEnd;

...

te:

"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Fri, 5 Aug 2016 10:07:54 +0200",Re: Spark SQL and Kryo registration,Amit Sela <amitsela33@gmail.com>,"Hi Olivier,
Did you check performance of Kryo ?
I have observations that Kryo is slightly slower than Java Serializer.

Regards,
Maciek

2016-08-04 17:41 GMT+02:00 Amit Sela <amitsela33@gmail.com>:

w


-- 
Maciek Bry≈Ñski
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Fri, 5 Aug 2016 10:18:19 +0200",Re: Result code of whole stage codegen,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Thank you.
That was it.

Regards,
Maciek

2016-08-05 10:06 GMT+02:00 Herman van H√∂vell tot Westerflier <
hvanhovell@databricks.com>:

ution.BufferedRowIterator
rote:
c


-- 
Maciek Bry≈Ñski
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 05 Aug 2016 17:44:24 +0000",Re: PySpark: Make persist() return a context manager,Reynold Xin <rxin@databricks.com>,"Okie doke, I've filed a JIRA for this here:
https://issues.apache.org/jira/browse/SPARK-16921


it
s:
"
"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Fri, 5 Aug 2016 12:40:33 -0700 (MST)",Apache Arrow data in buffer to RDD/DataFrame/Dataset?,dev@spark.apache.org,"In a few earlier posts [ 1
<http://apache-spark-developers-list.1001551.n3.nabble.com/Tungsten-off-heap-memory-access-for-C-libraries-td13898.html> 
] [ 2
<http://apache-spark-developers-list.1001551.n3.nabble.com/How-to-access-the-off-heap-representation-of-cached-data-in-Spark-2-0-td17701.html> 
], I asked about moving data from C++ into a Spark data source (RDD,
DataFrame, or Dataset). The issue is that even the off-heap cache might not
have a stable representation: it might change from one version to the next.

I recently learned about Apache Arrow, a data layer that Spark currently or
will someday share with Pandas, Impala, etc. Suppose that I can fill a
buffer (such as a direct ByteBuffer) with Arrow-formatted data, is there an
easy--- or even zero-copy--- way to use that in Spark? Is that an API that
could be developed?

I'll be at the KDD Spark 2.0 tutorial on August 15. Is that a good place to
ask this question?

Thanks,
-- Jim




--

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Fri, 5 Aug 2016 12:43:44 -0700",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,"""jpivarski@gmail.com"" <jpivarski@gmail.com>","Spark does not currently support Apache Arrow - probably a good place to
chat would be on the Arrow mailing list where they are making progress
towards unified JVM & Python/R support which is sort of a precondition of a
functioning Arrow interface between Spark and Python.




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Jeremy Smith <jeremy.smith@acorns.com>,"Fri, 5 Aug 2016 13:14:30 -0700",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,Holden Karau <holden@pigscanfly.ca>,"If you had a persistent, off-heap buffer of Arrow data on each executor,
and you could get an iterator over that buffer from inside of a task, then
you could conceivably define an RDD over it by just extending RDD and
returning the iterator from the compute method.  If you want to make a
Dataset or DataFrame, though, it's going to be tough to avoid copying the
data.  You can't avoid Spark copying data into InternalRows unless your RDD
is an RDD[InternalRow] and you create a BaseRelation for it that specifies
needsConversion = false.  It might be possible to implement InternalRow
over your Arrow buffer, but I'm still fuzzy on whether nor not that would
prevent copying/marshaling of the data.  Maybe one of the actual
contributors on Spark SQL will chime in with deeper knowledge.

Jeremy


"
Jim Pivarski <jpivarski@gmail.com>,"Fri, 5 Aug 2016 16:18:09 -0500",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,Holden Karau <holden@pigscanfly.ca>,"I see. I've already started working with Arrow-C++ and talking to members
of the Arrow community, so I'll keep doing that.

As a follow-up question, is there an approximate timescale for when Spark
will support Arrow? I'd just like to know that all the pieces will come
together eventually.

(In this forum, most of the discussion about Arrow is about PySpark and
Pandas, not Spark in general.)

Best,
Jim


"
Holden Karau <holden@pigscanfly.ca>,"Fri, 5 Aug 2016 14:22:44 -0700",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,Jim Pivarski <jpivarski@gmail.com>,"I don't think there is an approximate timescale right now and its likely
any implementation would depend on a solid Java implementation of Arrow
being ready first (or even a guarantee that it necessarily will - although
I'm interested in making it happen in some places where it makes sense).




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 05 Aug 2016 22:14:39 +0000",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,"Holden Karau <holden@pigscanfly.ca>, Jim Pivarski <jpivarski@gmail.com>","Relevant jira: https://issues.apache.org/jira/browse/SPARK-13534
2016ÎÖÑ 8Ïõî 5Ïùº (Í∏à) Ïò§ÌõÑ 5:22, Holden Karau <holden@pigscanfly.ca>ÎãòÏù¥ ÏûëÏÑ±:

h
s
k
o
of a
-heap-memory-access-for-C-libraries-td13898.html
s-the-off-heap-representation-of-cached-data-in-Spark-2-0-td17701.html
t
-data-in-buffer-to-RDD-DataFrame-Dataset-tp18563.html
"
Koert Kuipers <koert@tresata.com>,"Fri, 5 Aug 2016 18:50:41 -0400",Re: PySpark: Make persist() return a context manager,Nicholas Chammas <nicholas.chammas@gmail.com>,"The tricky part is that the action needs to be inside the with block, not
just the transformation that uses the persisted data.


Okie doke, I've filed a JIRA for this here: https://issues.apache.
org/jira/browse/SPARK-16921


it
s:
"
Jim Pivarski <jpivarski@gmail.com>,"Fri, 5 Aug 2016 17:53:11 -0500",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,Nicholas Chammas <nicholas.chammas@gmail.com>,"

Thank you. This ticket describes output from Spark to Arrow for flat
(non-nested) tables. Are there no plans to input from Arrow to Spark for
general types? Did I misunderstand the blogs?

I don't see it in this search:
https://issues.apache.org/jira/browse/SPARK-13534?jql=project%20%3D%20SPARK%20AND%20text%20~%20%22arrow%22

I'm beginning to think there's some misleading information out there (like
this diagram: https://arrow.apache.org/img/shared2.png).

-- JIm
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 05 Aug 2016 23:04:55 +0000",Re: PySpark: Make persist() return a context manager,Koert Kuipers <koert@tresata.com>,"Good point.

Do you think it's sufficient to note this somewhere in the documentation
(or simply assume that user understanding of transformations vs. actions
means they know this), or are there other implications that need to be
considered?


 it
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 05 Aug 2016 23:07:47 +0000",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,Jim Pivarski <jpivarski@gmail.com>,"Don't know much about Spark + Arrow efforts myself; just wanted to share
the reference.


"
Micah Kornfield <emkornfield@gmail.com>,"Fri, 5 Aug 2016 20:17:23 -0700",Re: Apache Arrow data in buffer to RDD/DataFrame/Dataset?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hi Everyone,
I'm an Arrow contributor mostly on the C++ side of things, but I'll try to
give a brief update of where I believe the project currently is (the views
are my own, but hopefully are fairly accurate :).

I think in the long run the diagram mentioned by Jim, is were we would like
Arrow to be, but it is clearly not there yet there.   The ticket referenced
[2] was an actionable first step for using Arrow, not the end state (after
I finished a couple of more items in Arrow I was hoping to try to work on
that ticket, but that might be a little ways out still).

In terms of specification [1], the memory model specification seems to be
fairly stable but might undergo a couple of more tweaks (there is still
some discussion on making a first class string/binary column type  and how
we address endianness on different systems).  The RPC model has a first
draft for how to serialize types to a stream/memory space, but needs to be
fleshed out some more to deal the practicalities of resource management.

The Java implementation of Arrow was originally taken from the Apache Drill
code base and I think it is fairly close to conforming to the specification
[1] (if not already doing so).  But it is should be ""mature"" in the sense
that is being used in a real system.

The C++/Python code is still in development and I hope to have at least a
prototype showing some form of communication between a JVM and C++/Python
process over the next couple weeks.

The last time I looked at the spark code base, the main difference I saw
with Arrow versus the existing columnar memory structure in Spark was how
null flags and boolean values are stored.  Arrow bit-packs null
flags/boolean values. Spark seems to have one byte per value.  I did not
take a close look to see if the memory allocation APIs were compatible
between Spark and Arrow.

If people are interested, I for one would like to here feedback on the
current specifications/code for Arrow.  So please feel free to chime in the
Arrow-dev mailing list.

Thanks,
Micah

[1] https://github.com/apache/arrow/tree/master/format
[2] https://issues.apache.org/jira/browse/SPARK-13534


"
Koert Kuipers <koert@tresata.com>,"Fri, 5 Aug 2016 23:28:38 -0400",Re: PySpark: Make persist() return a context manager,Nicholas Chammas <nicholas.chammas@gmail.com>,"i think it limits the usability of with statement. and it could be somewhat
confusing because of this, so i would mention it in docs.

i like the idea though.

m

t
n
g
t it
"
Ruslan Dautkhanov <dautkhanov@gmail.com>,"Sat, 6 Aug 2016 00:29:40 -0600",Spark requires sysctl tuning? Servers unresponsive,dev <dev@spark.apache.org>,"Hello,

When I start a spark notebook, it makes some of the servers exhausting some
Linux kernel resources, as I can't even ssh to those nodes.

And it's not due to servers being hammered. It happens when there are no
spark jobs/taks are running. To reproduce this problem, it's enough to just
start a spark notebook (and keep spark context up).

Spark version: 1.5 but I had this problem in previous versions of Spark too.
Hadoop 2.6. Spark on YARN.
There are 150 containers, each has 2 vcores.

There is plenty of memory (yarn reserved 1.3Tb of memory across 10 nodes
for those 150 containers). GC is miniscule. There are many cores in the
system
too (24-48 cores per node). Servers are RHEL 6.7,
on 2.6.32-573.22.1.el6.x86_64

We have monitoring running on the servers.
Number of open connections jumps up to 2500-4000 when Spark is up.
It's just around 200 when Spark is not running.

I have tried to bump up many of the /etc/sysctl.conf limiting parameters
that we
might be reaching. Current set of non-default sysctl settings - see *[1]*.
When those parameters were increased to those levels, it feels better,
but stiil problem happens.

After some time of running tasks, jobs start to fail. Looks like there
might be
some leaking netty connections or something like that?
Jobs fail with ""Failed to connect to.."" error - see *[2]*. But this happens
way later,
it's an epogee of the problem when it starts affecting jobs. Before that
Spark
itself exhausts some resources and we can see that as we can't even ssh to
some
of the servers.

Also tuned up ulimits in the system, but no change in the behavior.
Problem always reproducible.
See a limits.d conf file in *[3] *

We only have this problem when Spark is running. Hive etc running just fine.

Any ideas?

Thank you.



*[1] *
Current set of non-default sysctl settings:

net.ipv4.ip_forward = 0



*[2]* Error stack :

An error occurred while calling














[3] /etc/security/limits.d conf file in *[3] *

$ cat heavy-mr-jobs.conf
"
Hao Ren <invkrh@gmail.com>,"Sun, 7 Aug 2016 23:31:33 +0200","[SPARK-2.0][SQL] UDF containing non-serializable object does not work
 as expected","user <user@spark.apache.org>, dev <dev@spark.apache.org>","I am playing with spark 2.0
What I tried to test is:

Create a UDF in which there is a non serializable object.
What I expected is when this UDF is called during materializing the
dataFrame where the UDF is used in ""select"", an task non serializable
exception should be thrown.
It depends also which ""action"" is called on that dataframe.

Here is the code for reproducing the pb:

============
object DataFrameSerDeTest extends App {

  class A(val value: Int) // It is not serializable

  def run() = {
    val spark = SparkSession
      .builder()
      .appName(""DataFrameSerDeTest"")
      .master(""local[*]"")
      .getOrCreate()

    import org.apache.spark.sql.functions.udf
    import spark.sqlContext.implicits._

    val notSer = new A(2)
    val add = udf {
      (a: Int) => a + notSer.value
    }
    val df = spark.createDataFrame(Seq(
      (1, 2),
      (2, 2),
      (3, 2),
      (4, 2)
    )).toDF(""key"", ""value"")
      .select($""key"", add($""value"").as(""added""))

    df.show() // *It should not work because the udf contains a
non-serializable object, but it works*

    df.filter($""key"" === 2).show() // *It does not work as expected
(org.apache.spark.SparkException: Task not serializable)*
  }

  run()
}
============

Also, I tried collect(), count(), first(), limit(). All of them worked
without non-serializable exceptions.
It seems only filter() throws the exception. (feature or bug ?)

Any ideas ? Or I just messed things up ?
Any help is highly appreciated.

-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
"""r7raul1984@163.com"" <r7raul1984@163.com>","Mon, 8 Aug 2016 14:12:21 +0800",Kafka Support new topic subscriptions without requiring restart of the streaming context,dev <dev@spark.apache.org>,"How to add new topic to kafka without requiring restart of the streaming context?



r7raul1984@163.com
"
Hao Ren <invkrh@gmail.com>,"Mon, 8 Aug 2016 10:02:31 +0200","Re: [SPARK-2.0][SQL] UDF containing non-serializable object does not
 work as expected",Muthu Jayakumar <babloo80@gmail.com>,"Yes, it is.
You can define a udf like that.
Basically, it's a udf Int => Int which is a closure contains a non
serializable object.
The latter should cause Task not serializable exception.

Hao




-- 
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
ekass <evie@cslab.ece.ntua.gr>,"Mon, 8 Aug 2016 04:14:13 -0700 (MST)",Spark 2.0 sql module empty columns in result over parquet tables,dev@spark.apache.org,"I run into this very strange issue. After loading parquet tables and trying
to run an sql query with the sql module the results are not correct with
Spark 2.0 although over the same exactly dataset Spark 1.6 results are
correct. With Textfiles however both versions of Spark work as expected. I
haven't noticed something strange in the command log stages, tasks and
shuffle data are almost the same. How can I debug the query execution to
find out why some columns in the result are null or why the resultset is
empty? Could this be related to datatype casting that is required in Spark
2.0 queries?

Thank you in advance



--

---------------------------------------------------------------------


"
Simon Scott <Simon.Scott@viavisolutions.com>,"Mon, 8 Aug 2016 12:16:23 +0000","RE: [SPARK-2.0][SQL] UDF containing non-serializable object does not
 work as expected","Hao Ren <invkrh@gmail.com>, Muthu Jayakumar <babloo80@gmail.com>","But does the ‚ÄúnotSer‚Äù object have to be serialized?

The object is immutable by the definition of A, so the only thing that needs to be serialized is the (immutable) Int value? And Ints are serializable?

Just thinking out loud

Simon Scott

Research Developer @ viavisolutions.com

From: Hao Ren [mailto:invkrh@gmail.com]
Sent: 08 August 2016 09:03
To: Muthu Jayakumar <babloo80@gmail.com>
Cc: user <user@spark.apache.org>; dev <dev@spark.apache.org>
Subject: Re: [SPARK-2.0][SQL] UDF containing non-serializable object does not work as expected

Yes, it is.
You can define a udf like that.
Basically, it's a udf Int => Int which is a closure contains a non serializable object.
The latter should cause Task not serializable exception.

Hao

On Mon, Aug 8, 2016 at 5:08 AM, Muthu Jayakumar <babloo80@gmail.com<mailto:babloo80@gmail.com>> wrote:
Hello Hao Ren,

Doesn't the code...

val add = udf {
      (a: Int) => a + notSer.value
    }
Mean UDF function that Int => Int ?

Thanks,
Muthu

On Sun, Aug 7, 2016 at 2:31 PM, Hao Ren <invkrh@gmail.com<mailto:invkrh@gmail.com>> wrote:
I am playing with spark 2.0
What I tried to test is:

Create a UDF in which there is a non serializable object.
What I expected is when this UDF is called during materializing the dataFrame where the UDF is used in ""select"", an task non serializable exception should be thrown.
It depends also which ""action"" is called on that dataframe.

Here is the code for reproducing the pb:

============
object DataFrameSerDeTest extends App {

  class A(val value: Int) // It is not serializable

  def run() = {
    val spark = SparkSession
      .builder()
      .appName(""DataFrameSerDeTest"")
      .master(""local[*]"")
      .getOrCreate()

    import org.apache.spark.sql.functions.udf
    import spark.sqlContext.implicits._

    val notSer = new A(2)
    val add = udf {
      (a: Int) => a + notSer.value
    }
    val df = spark.createDataFrame(Seq(
      (1, 2),
      (2, 2),
      (3, 2),
      (4, 2)
    )).toDF(""key"", ""value"")
      .select($""key"", add($""value"").as(""added""))

    df.show() // It should not work because the udf contains a non-serializable object, but it works

    df.filter($""key"" === 2).show() // It does not work as expected (org.apache.spark.SparkException: Task not serializable)
  }

  run()
}
============

Also, I tried collect(), count(), first(), limit(). All of them worked without non-serializable exceptions.
It seems only filter() throws the exception. (feature or bug ?)

Any ideas ? Or I just messed things up ?
Any help is highly appreciated.

--
Hao Ren

Data Engineer @ leboncoin

Paris, France




--
Hao Ren

Data Engineer @ leboncoin

Paris, France
"
Cody Koeninger <cody@koeninger.org>,"Mon, 8 Aug 2016 08:20:55 -0500","Re: Kafka Support new topic subscriptions without requiring restart
 of the streaming context",r7raul1984@163.com,"The Kafka 0.10 support in spark 2.0 allows for pattern based topic
subscription

"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 8 Aug 2016 14:15:00 -0400",Welcoming Felix Cheung as a committer,dev <dev@spark.apache.org>,"Hi all,

The PMC recently voted to add Felix Cheung as a committer. Felix has been a major contributor to SparkR and we're excited to have him join officially. Congrats and welcome, Felix!

Matei
---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 8 Aug 2016 11:15:57 -0700",Re: Welcoming Felix Cheung as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations, Felix.


"
Dongjoon Hyun <dongjoon@apache.org>,"Mon, 8 Aug 2016 11:17:03 -0700",Re: Welcoming Felix Cheung as a committer,Ted Yu <yuzhihong@gmail.com>,"Congratulation, Felix!

Bests,
Dongjoon.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 08 Aug 2016 18:45:53 +0000",Re: Welcoming Felix Cheung as a committer,"Dongjoon Hyun <dongjoon@apache.org>, Ted Yu <yuzhihong@gmail.com>","üëèüëèüëè

Do we now have 2 SparkR-focused committers (Shivaram + Felix)? Or are there
more?

Nick


"
Michael Allman <michael@videoamp.com>,"Mon, 8 Aug 2016 11:53:51 -0700",Scaling partitioned Hive table support,dev <dev@spark.apache.org>,"Hello,

I'd like to propose a modification in the way Hive table partition metadata are loaded and cached. Currently, when a user reads from a partitioned Hive table whose metadata are not cached (and for which Hive table conversion is enabled and supported), all partition metadata is fetched from the metastore:

https://github.com/apache/spark/blob/5effc016c893ce917d535cc1b5026d8e4c846721/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L252-L260 <https://github.com/apache/spark/blob/5effc016c893ce917d535cc1b5026d8e4c846721/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L252-L260>

This is highly inefficient in some scenarios. In the most extreme case, a user starts a new Spark app, runs a query which reads from a single partition in a table with a large number of partitions and terminates their app. All partition metadata are loaded and their files' schema are merged, but only a single partition is read. Instead, I propose we load and cache partition metadata on-demand, as needed to build query plans.

We've long encountered this performance problem at VideoAmp and have taken different approaches to address it. In addition to the load time, we've found that loading all of a table's partition metadata can require a significant amount of JVM heap space. Our largest tables OOM our Spark drivers unless we allocate several GB of heap space.

Certainly one could argue that our situation is pathological and rare, and that the problem in our scenario is with the design of our tables‚Äînot Spark. However, even in tables with more modest numbers of partitions, loading only the necessary partition metadata and file schema can significantly reduce the query planning time, and is definitely more memory efficient.

I've written POCs for a couple of different implementation approaches. Though incomplete, both have been successful in their basic goal. The first extends `org.apache.spark.sql.catalyst.catalog.ExternalCatalog` and as such is more general. It requires some new abstractions and refactoring of `HadoopFsRelation` and `FileCatalog`, among others. It places a greater burden on other implementations of `ExternalCatalog`. Currently the only other implementation of `ExternalCatalog` is `InMemoryCatalog`, and my code throws an `UnsupportedOperationException` on that implementation.

The other approach is simpler and only touches code in the codebase's `hive` project. Basically, conversion of `MetastoreRelation` to `HadoopFsRelation` is deferred to physical planning when the metastore relation is partitioned. During physical planning, the partition pruning filters in a logical query plan are used to select the required partition metadata and a `HadoopFsRelation` is built from those. The new logical plan is then re-injected into the planner.

I'd like to get the community's thoughts on my proposal and implementation approaches.

Thanks!

Michael"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Mon, 8 Aug 2016 11:57:25 -0700",Re: Welcoming Felix Cheung as a committer,Ted Yu <yuzhihong@gmail.com>,"Congratulations , Felix!



been a major contributor to SparkR and we're excited to have him join officially. Congrats and welcome, Felix!
<mailto:dev-unsubscribe@spark.apache.org>

"
Timothy Chen <tnachen@gmail.com>,"Mon, 8 Aug 2016 12:27:25 -0700",Re: Welcoming Felix Cheung as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congrats Felix!

Tim


---------------------------------------------------------------------


"
Tarun Kumar <tarunk1407@gmail.com>,"Mon, 08 Aug 2016 19:28:08 +0000",Re: Welcoming Felix Cheung as a committer,"Timothy Chen <tnachen@gmail.com>, Matei Zaharia <matei.zaharia@gmail.com>","Congrats Felix!

Tarun

"
Eric Liang <ekl@databricks.com>,"Mon, 08 Aug 2016 19:51:05 +0000",Re: Scaling partitioned Hive table support,"Michael Allman <michael@videoamp.com>, dev <dev@spark.apache.org>","I like the former approach -- it seems more generally applicable to other
catalogs and IIUC would let you defer pruning until execution time. Pruning
is work that should be done by the catalog anyways, as is the case when
querying over an (unconverted) hive table.

You might also want to look at https://github.com/apache/spark/pull/14241 ,
which refactors some of the file scan execution to defer pruning.


6721/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L252-L260
ir
,
n
d
înot
ry
st
ch
de
an
n
"
Reynold Xin <rxin@databricks.com>,"Tue, 9 Aug 2016 04:00:10 +0800","Re: [SPARK-2.0][SQL] UDF containing non-serializable object does not
 work as expected",Simon Scott <Simon.Scott@viavisolutions.com>,"That is unfortunately the way how Scala compiler captures (and defines)
closures. Nothing is really final in the JVM. You can always use reflection
or unsafe to modify the value of fields.


"
Michael Allman <michael@videoamp.com>,"Mon, 8 Aug 2016 14:01:03 -0700",Re: Scaling partitioned Hive table support,Eric Liang <ekl@databricks.com>,"Hi Eric,

Thanks for your feedback. I'm rebasing my code for the first approach on a more recent Spark master and am resolving some conflicts. I'll have a better understanding of the relationship to your PR once my rebase is complete.

Cheers,

Michael

other catalogs and IIUC would let you defer pruning until execution time. Pruning is work that should be done by the catalog anyways, as is the case when querying over an (unconverted) hive table.
https://github.com/apache/spark/pull/14241 <https://github.com/apache/spark/pull/14241> , which refactors some of the file scan execution to defer pruning.
metadata are loaded and cached. Currently, when a user reads from a partitioned Hive table whose metadata are not cached (and for which Hive table conversion is enabled and supported), all partition metadata is fetched from the metastore:
https://github.com/apache/spark/blob/5effc016c893ce917d535cc1b5026d8e4c846721/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L252-L260 <https://github.com/apache/spark/blob/5effc016c893ce917d535cc1b5026d8e4c846721/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L252-L260>
case, a user starts a new Spark app, runs a query which reads from a single partition in a table with a large number of partitions and terminates their app. All partition metadata are loaded and their files' schema are merged, but only a single partition is read. Instead, I propose we load and cache partition metadata on-demand, as needed to build query plans.
taken different approaches to address it. In addition to the load time, we've found that loading all of a table's partition metadata can require a significant amount of JVM heap space. Our largest tables OOM our Spark drivers unless we allocate several GB of heap space.
and that the problem in our scenario is with the design of our tables‚Äînot Spark. However, even in tables with more modest numbers of partitions, loading only the necessary partition metadata and file schema can significantly reduce the query planning time, and is definitely more memory efficient.
Though incomplete, both have been successful in their basic goal. The first extends `org.apache.spark.sql.catalyst.catalog.ExternalCatalog` and as such is more general. It requires some new abstractions and refactoring of `HadoopFsRelation` and `FileCatalog`, among others. It places a greater burden on other implementations of `ExternalCatalog`. Currently the only other implementation of `ExternalCatalog` is `InMemoryCatalog`, and my code throws an `UnsupportedOperationException` on that implementation.
`hive` project. Basically, conversion of `MetastoreRelation` to `HadoopFsRelation` is deferred to physical planning when the metastore relation is partitioned. During physical planning, the partition pruning filters in a logical query plan are used to select the required partition metadata and a `HadoopFsRelation` is built from those. The new logical plan is then re-injected into the planner.
implementation approaches.

"
Hao Ren <invkrh@gmail.com>,"Mon, 8 Aug 2016 23:05:07 +0200","Re: [SPARK-2.0][SQL] UDF containing non-serializable object does not
 work as expected",Reynold Xin <rxin@databricks.com>,"@Reynold

Some questions to make things clear:

1. As nothing is really final in the JVM, is the generated code during the
execution of `df.show()` different from the one of `df.filter($""key"" ===
2).show()` in my snippet ?

2. When `df.show()` is being ex"
dhruve ashar <dhruveashar@gmail.com>,"Mon, 8 Aug 2016 16:57:25 -0500",Re: Welcoming Felix Cheung as a committer,Tarun Kumar <tarunk1407@gmail.com>,"Congrats Felix!




-- 
-Dhruve Ashar
"
Reynold Xin <rxin@databricks.com>,"Tue, 9 Aug 2016 06:02:24 +0800","Re: [SPARK-2.0][SQL] UDF containing non-serializable object does not
 work as expected",Hao Ren <invkrh@gmail.com>,"The show thing was the result of an optimization that short-circuited any
real Spark computation when the input is a local collection, and the result
was simply the first few rows. That's why it completed without serializing
anything.

always serialize the query plan even for local execution. We did that back
in the days for the RDD code path, and we can do similar things for the SQL
code path. However, serialization is not free and it will slow down the
execution by small percentage.




ion
ed
"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Tue, 9 Aug 2016 00:04:55 +0200",Re: Welcoming Felix Cheung as a committer,dhruve ashar <dhruveashar@gmail.com>,"Congrats Felix!


"
Xiao Li <gatorsmile@gmail.com>,"Mon, 8 Aug 2016 15:47:45 -0700",Re: Welcoming Felix Cheung as a committer,=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@databricks.com>,"Congrats Felix!

2016-08-08 15:04 GMT-07:00 Herman van H√∂vell tot Westerflier
<hvanhovell@databricks.com>:
te:
:
:
m>
n
-

---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Mon, 8 Aug 2016 15:48:52 -0700",SASL Support,dev <dev@spark.apache.org>,"I was checking if RPC calls can be encrypted and I saw here that the docs
here (*http://spark.apache.org/docs/latest/configuration.html
<http://spark.apache.org/docs/latest/configuration.html>) *say that SASL
encryption is ""currently only supported by the block transfer service.""

However, it seems that RPC can be SASL encrypted as well:
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/rpc/netty/NettyRpcEnv.scala#L64

Is this accurate?  If so, I'll submit a PR to update the docs.

-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Tue, 9 Aug 2016 06:49:58 +0800",Re: SASL Support,Michael Gummelt <mgummelt@mesosphere.io>,"Please send a pull request to update the doc. Thanks.


"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 9 Aug 2016 09:49:46 +0900",Re: Welcoming Felix Cheung as a committer,Xiao Li <gatorsmile@gmail.com>,"Congratulations!

2016-08-09 7:47 GMT+09:00 Xiao Li <gatorsmile@gmail.com>:

as
-
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 9 Aug 2016 09:14:27 +0800",Re: Welcoming Felix Cheung as a committer,Hyukjin Kwon <gurwls223@gmail.com>,"Congrats Felix!




-- 
Best Regards

Jeff Zhang
"
Kai Jiang <jiangkai@gmail.com>,"Tue, 09 Aug 2016 01:21:52 +0000",Re: Welcoming Felix Cheung as a committer,"Jeff Zhang <zjffdu@gmail.com>, Hyukjin Kwon <gurwls223@gmail.com>","Congrats Felix!


"
Fred Reiss <freiss.oss@gmail.com>,"Mon, 8 Aug 2016 19:24:07 -0700",Re: Source API requires unbounded distributed storage?,Michael Armbrust <michael@databricks.com>,"Created SPARK-16963 to cover this issue.

Fred


g
,
streaming,
cal
m
and
o
e
re
aming will
n
a
"
Yanbo Liang <ybliang8@gmail.com>,"Mon, 8 Aug 2016 19:41:45 -0700",Re: Welcoming Felix Cheung as a committer,Kai Jiang <jiangkai@gmail.com>,"Congrats Felix!

2016-08-08 18:21 GMT-07:00 Kai Jiang <jiangkai@gmail.com>:

:
x
m
"
Felix Cheung <felixcheung_m@hotmail.com>,"Tue, 9 Aug 2016 04:44:22 +0000",Re: Welcoming Felix Cheung as a committer,"Kai Jiang <jiangkai@gmail.com>, Yanbo Liang <ybliang8@gmail.com>","Thank you!
Looking forward to work with you all!






Congrats Felix!

2016-08-08 18:21 GMT-07:00 Kai Jiang <jiangkai@gmail.com<mailto:jiangkai@gmail.com>>:

Congrats Felix!

Congrats Felix!

Congratulations!

2016-08-09 7:47 GMT+09:00 Xiao Li <gatorsmile@gmail.com<mailto:gatorsmile@gmail.com>>:
Congrats Felix!

2016-08-08 15:04 GMT-07:00 Herman van Hˆvell tot Westerflier
<hvanhovell@databricks.com<mailto:hvanhovell@databricks.com>>:
m<mailto:matei.zaharia@gmail.com>>
n
-
nsubscribe@spark.apache.org>
ubscribe@spark.apache.org>

---------------------------------------------------------------------
ibe@spark.apache.org>





--
Best Regards

Jeff Zhang

"
Denny Lee <denny.g.lee@gmail.com>,"Tue, 09 Aug 2016 05:53:22 +0000",Re: Welcoming Felix Cheung as a committer,"Felix Cheung <felixcheung_m@hotmail.com>, Kai Jiang <jiangkai@gmail.com>, 
	Yanbo Liang <ybliang8@gmail.com>","Awesome - congrats Felix!


m>
"
Michael Allman <michael@videoamp.com>,"Tue, 9 Aug 2016 11:20:14 -0700",Re: Scaling partitioned Hive table support,Eric Liang <ekl@databricks.com>,"Hi Eric,

I've rebased my first patch to master and created a Jira issue for tracking: https://issues.apache.org/jira/browse/SPARK-16980 <https://issues.apache.org/jira/browse/SPARK-16980>. As mentioned in the issue, I will open a PR for discussion and design review, and include you in the conversation.

Cheers,

Michael


other catalogs and IIUC would let you defer pruning until execution time. Pruning is work that should be done by the catalog anyways, as is the case when querying over an (unconverted) hive table.
https://github.com/apache/spark/pull/14241 <https://github.com/apache/spark/pull/14241> , which refactors some of the file scan execution to defer pruning.
metadata are loaded and cached. Currently, when a user reads from a partitioned Hive table whose metadata are not cached (and for which Hive table conversion is enabled and supported), all partition metadata is fetched from the metastore:
https://github.com/apache/spark/blob/5effc016c893ce917d535cc1b5026d8e4c846721/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L252-L260 <https://github.com/apache/spark/blob/5effc016c893ce917d535cc1b5026d8e4c846721/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala#L252-L260>
case, a user starts a new Spark app, runs a query which reads from a single partition in a table with a large number of partitions and terminates their app. All partition metadata are loaded and their files' schema are merged, but only a single partition is read. Instead, I propose we load and cache partition metadata on-demand, as needed to build query plans.
taken different approaches to address it. In addition to the load time, we've found that loading all of a table's partition metadata can require a significant amount of JVM heap space. Our largest tables OOM our Spark drivers unless we allocate several GB of heap space.
and that the problem in our scenario is with the design of our tables‚Äînot Spark. However, even in tables with more modest numbers of partitions, loading only the necessary partition metadata and file schema can significantly reduce the query planning time, and is definitely more memory efficient.
Though incomplete, both have been successful in their basic goal. The first extends `org.apache.spark.sql.catalyst.catalog.ExternalCatalog` and as such is more general. It requires some new abstractions and refactoring of `HadoopFsRelation` and `FileCatalog`, among others. It places a greater burden on other implementations of `ExternalCatalog`. Currently the only other implementation of `ExternalCatalog` is `InMemoryCatalog`, and my code throws an `UnsupportedOperationException` on that implementation.
`hive` project. Basically, conversion of `MetastoreRelation` to `HadoopFsRelation` is deferred to physical planning when the metastore relation is partitioned. During physical planning, the partition pruning filters in a logical query plan are used to select the required partition metadata and a `HadoopFsRelation` is built from those. The new logical plan is then re-injected into the planner.
implementation approaches.

"
Chris Fregly <chris@fregly.com>,"Tue, 9 Aug 2016 11:52:41 -0700",Re: Spark 2.0.1 / 2.1.0 on Maven,Mark Hamstra <mark@clearstorydata.com>,"alrighty then!

bcc'ing user list.  cc'ing dev list.

@user list people:  do not read any further or you will be in violation of
ASF policies!




-- 
*Chris Fregly*
Research Scientist @ PipelineIO
San Francisco, CA
pipeline.io
advancedspark.com
"
Minudika Malshan <minudika001@gmail.com>,"Wed, 10 Aug 2016 16:46:10 +0530",Get data from CSV files to feed SparkML library methods,dev@spark.apache.org,"Hi all,

I'm using spark ml library and need to train a model using data extracted
from a CSV file.
I found that we can load datasets from LibSVM files to spark ML methods.
As far as i understood, the data should be represented as labeled points
in-order to feed the ml methods.
Is there a way to load dataset from a CSV file instead of a LibSVM file?
Or do I need to convert the CSV file to LibSVM format? If so, could you
please let me know a way to do that.?
Your help would be much appreciated.

Thank you!
Minudika
"
Yanbo Liang <ybliang8@gmail.com>,"Wed, 10 Aug 2016 06:39:31 -0700",Re: Get data from CSV files to feed SparkML library methods,Minudika Malshan <minudika001@gmail.com>,"You can load dataset from CSV file and use VectorAssembler to assemble
necessary columns into a single columns of vector type. The output column
of VectorAssembler will be the features column which should be feed into ML
estimator for model training. You can refer VectorAssembler document:
http://spark.apache.org/docs/latest/ml-features.html#vectorassembler .

Thanks
Yanbo

2016-08-10 4:16 GMT-07:00 Minudika Malshan <minudika001@gmail.com>:

"
Minudika Malshan <minudika001@gmail.com>,"Wed, 10 Aug 2016 20:28:19 +0530",Re: Get data from CSV files to feed SparkML library methods,Yanbo Liang <ybliang8@gmail.com>,"Thanks a lot Yanbo.!
I will try it.

Best Regards.



"
Deepak Sharma <deepakmca05@gmail.com>,"Wed, 10 Aug 2016 20:50:39 +0530",Use cases around image/video processing in spark,"dev@spark.apache.org, spark users <user@spark.apache.org>","Hi
If anyone is using or knows about github repo that can help me get started
with image and video processing using spark.
The images/videos will be stored in s3 and i am planning to use s3 with
Spark.
In this case , how will spark achieve distributed processing?
Any code base or references is really appreciated.

-- 
Thanks
Deepak
"
Benjamin Fradet <benjamin.fradet@gmail.com>,"Wed, 10 Aug 2016 18:27:12 +0200",Re: Use cases around image/video processing in spark,Deepak Sharma <deepakmca05@gmail.com>,"Hi,

Check out the the thunder project
<https://github.com/thunder-project/thunder>





-- 
Ben Fradet.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 11 Aug 2016 02:50:05 +0000",Serving Spark ML models via a regular Python web app,Spark dev list <dev@spark.apache.org>,"Are there any existing JIRAs covering the possibility of serving up Spark
ML models via, for example, a regular Python web app?

The story goes like this: You train your model with Spark on several TB of
data, and now you want to use it in a prediction service that you‚Äôre
building, say with Flask <http://flask.pocoo.org/>. In principle, you don‚Äôt
need Spark anymore since you‚Äôre just passing individual data points to your
model and looking for it to spit some prediction back.

I assume this is something people do today, right? I presume Spark needs to
run in their web service to serve up the model. (Sorry, I‚Äôm new to the ML
side of Spark. üòÖ)

Are there any JIRAs discussing potential improvements to this story? I did
a search, but I‚Äôm not sure what exactly to look for. SPARK-4587
<https://issues.apache.org/jira/browse/SPARK-4587> (model import/export)
looks relevant, but doesn‚Äôt address the story directly.

Nick
‚Äã
"
Michael Allman <michael@videoamp.com>,"Wed, 10 Aug 2016 21:28:43 -0700",Re: Serving Spark ML models via a regular Python web app,Nicholas Chammas <nicholas.chammas@gmail.com>,"Nick,

Check out MLeap: https://github.com/TrueCar/mleap <https://github.com/TrueCar/mleap>. It's not python, but we use it in production to serve a random forest model trained by a Spark ML pipeline.

Thanks,

Michael

Spark ML models via, for example, a regular Python web app?
TB of data, and now you want to use it in a prediction service that you‚Äôre building, say with Flask <http://flask.pocoo.org/>. In principle, you don‚Äôt need Spark anymore since you‚Äôre just passing individual data points to your model and looking for it to spit some prediction back.
needs to run in their web service to serve up the model. (Sorry, I‚Äôm new to the ML side of Spark. üòÖ)
did a search, but I‚Äôm not sure what exactly to look for. SPARK-4587 <https://issues.apache.org/jira/browse/SPARK-4587> (model import/export) looks relevant, but doesn‚Äôt address the story directly.

"
Jason Moore <Jason.Moore@quantium.com.au>,"Thu, 11 Aug 2016 06:23:22 +0000",Sorting within partitions is not maintained in parquet?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

It seems that something changed between Spark 1.6.2 and 2.0.0 that I wasn't expecting.

If I have a DataFrame with records sorted within each partition, and I write it to parquet and read back from the parquet, previously the records would be iterated through in the same order they were written (assuming no shuffle has taken place).  But this doesn't seem to be the case anymore.  Below is the code to reproduce in a spark-shell.

Was this change expected?

Thanks,
Jason.


import org.apache.spark.sql._
def isSorted[T](self: DataFrame, mapping: Row => T)(implicit ordering: Ordering[T]) = {
  import self.sqlContext.implicits._
  import ordering._
  self
    .mapPartitions(rows => {
      val isSorted = rows
        .map(mapping)
        .sliding(2) // all adjacent pairs
        .forall {
          case x :: y :: Nil => x <= y
          case x :: Nil => true
          case Nil => true
        }

      Iterator(isSorted)
    })
    .reduce(_ && _)
}

// in Spark 2.0.0
spark.range(100000).toDF(""id"").registerTempTable(""input"")
spark.sql(""SELECT id FROM input DISTRIBUTE BY id SORT BY id"").write.mode(""overwrite"").parquet(""input.parquet"")
isSorted(spark.read.parquet(""input.parquet""), _.getAs[Long](""id""))
// FALSE

// in Spark 1.6.2
sqlContext.range(100000).toDF(""id"").registerTempTable(""input"")
sqlContext.sql(""SELECT id FROM input DISTRIBUTE BY id SORT BY id"").write.mode(""overwrite"").parquet(""input.parquet"")
isSorted(sqlContext.read.parquet(""input.parquet""), _.getAs[Long](""id""))
// TRUE

"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 11 Aug 2016 07:01:56 +0000",Re: Serving Spark ML models via a regular Python web app,"""dev@spark.apache.org"" <dev@spark.apache.org>","Currently there is no direct way in Spark to serve models without bringing
in all of Spark as a dependency.

For Spark ML, there is actually no way to do it independently of DataFrames
either (which for single-instance prediction makes things sub-optimal).
That is covered here: https://issues.apache.org/jira/browse/SPARK-10413

So, your options are (in Scala) things like MLeap, PredictionIO, or ""roll
your own"". Or you can try to export to some other format such as PMML or
PFA. Some MLlib models support PMML export, but for ML it is still missing
(see https://issues.apache.org/jira/browse/SPARK-11171).

There is an external project for PMML too (note licensing) -
https://github.com/jpmml/jpmml-sparkml - which is by now actually quite
comprehensive. It shows that PMML can represent a pretty large subset of
typical ML pipeline functionality.

pretty much ""roll your own"" currently, or export in PMML or PFA.

Finally, part of the ""mllib-local"" idea was around enabling this local
model-serving (for some initial discussion about the future see
https://issues.apache.org/jira/browse/SPARK-16365).

N


f
ôre
dual data points
w to the
d
"
Sean Owen <sowen@cloudera.com>,"Thu, 11 Aug 2016 09:54:14 +0100",Who controls 'databricks-jenkins'?,dev <dev@spark.apache.org>,"Not a big deal but 'he' is commenting on a lot of ancient PRs for some
reason, like https://github.com/apache/spark/pull/51 and it generates
mails to the list. I assume this is a misconfiguration somewhere.

---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Thu, 11 Aug 2016 20:27:10 +0900",Re: Sorting within partitions is not maintained in parquet?,Jason Moore <Jason.Moore@quantium.com.au>,"I just took a quick look for this. It seems not parquet-specific problem
but for datasources implimenting FileFormat.

In 1.6, it seems apparently partitions are made per file but in 2.0
partition can hold multiple files.

So, in your case files are miltiple but partitions are fewer, meaning each
partition is not sorted although each part-file is sorted.

It seems this PR https://github.com/apache/spark/pull/12095 is related.

I could not test before/after this PR because I don't have the access to my
computer for now (it's my phone) but I am sure the PR is related.

Maybe we need an option to enable/disable this?

I appreciate if any gives some feedback.

Thanks!

:

s
o
nymore.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 11 Aug 2016 14:42:28 +0000",Re: Serving Spark ML models via a regular Python web app,"Nick Pentreath <nick.pentreath@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Thanks Michael for the reference, and thanks Nick for the comprehensive
overview of existing JIRA discussions about this. I've added myself as a
watcher on the various tasks.


g
g
e
k
m>
k
Äôre
idual data points
ew to the
587
"
Chris Fregly <chris@fregly.com>,"Thu, 11 Aug 2016 09:35:55 -0700",Re: Serving Spark ML models via a regular Python web app,Nicholas Chammas <nicholas.chammas@gmail.com>,"this is exactly what my http://pipeline.io project is addressing.  check it out and send me feedback or create issues at that github location.

erview of existing JIRA discussions about this. I've added myself as a watcher on the various tasks.
g in all of Spark as a dependency.
es either (which for single-instance prediction makes things sub-optimal). That is covered here: https://issues.apache.org/jira/browse/SPARK-10413
 your own"". Or you can try to export to some other format such as PMML or PFA. Some MLlib models support PMML export, but for ML it is still missing (see https://issues.apache.org/jira/browse/SPARK-11171).
ub.com/jpmml/jpmml-sparkml - which is by now actually quite comprehensive. It shows that PMML can represent a pretty large subset of typical ML pipeline functionality.
e pretty much ""roll your own"" currently, or export in PMML or PFA.
del-serving (for some initial discussion about the future see https://issues.apache.org/jira/browse/SPARK-16365).
:
e use it in production to serve a random forest model trained by a Spark ML pipeline.
rk ML models via, for example, a regular Python web app?
 of data, and now you want to use it in a prediction service that you‚Äôre building, say with Flask. In principle, you don‚Äôt need Spark anymore since you‚Äôre just passing individual data points to your model and looking for it to spit some prediction back.
s to run in their web service to serve up the model. (Sorry, I‚Äôm new to the ML side of Spark. üòÖ)
id a search, but I‚Äôm not sure what exactly to look for. SPARK-4587 (model import/export) looks relevant, but doesn‚Äôt address the story directly.
"
Chris Fregly <chris@fregly.com>,"Thu, 11 Aug 2016 09:42:12 -0700",Re: Serving Spark ML models via a regular Python web app,Nicholas Chammas <nicholas.chammas@gmail.com>,"And here's a recent slide deck on the pipeline.io that summarizes what we're working on (all open source):  

https://www.slideshare.net/mobile/cfregly/advanced-spark-and-tensorflow-meetup-08042016-one-click-spark-ml-pipeline-deploy-to-production

mleap is heading the wrong direction and reinventing the wheel.  not quite sure where that project will go.  doesn't seem like it will have a long shelf-life in my opinion.

check out pipeline.io.  some cool stuff in there.

t out and send me feedback or create issues at that github location.
verview of existing JIRA discussions about this. I've added myself as a watcher on the various tasks.
ng in all of Spark as a dependency.
mes either (which for single-instance prediction makes things sub-optimal). That is covered here: https://issues.apache.org/jira/browse/SPARK-10413
l your own"". Or you can try to export to some other format such as PMML or PFA. Some MLlib models support PMML export, but for ML it is still missing (see https://issues.apache.org/jira/browse/SPARK-11171).
hub.com/jpmml/jpmml-sparkml - which is by now actually quite comprehensive. It shows that PMML can represent a pretty large subset of typical ML pipeline functionality.
re pretty much ""roll your own"" currently, or export in PMML or PFA.
odel-serving (for some initial discussion about the future see https://issues.apache.org/jira/browse/SPARK-16365).
e:
 we use it in production to serve a random forest model trained by a Spark ML pipeline.
ark ML models via, for example, a regular Python web app?
B of data, and now you want to use it in a prediction service that you‚Äôre building, say with Flask. In principle, you don‚Äôt need Spark anymore since you‚Äôre just passing individual data points to your model and looking for it to spit some prediction back.
ds to run in their web service to serve up the model. (Sorry, I‚Äôm new to the ML side of Spark. üòÖ)
 did a search, but I‚Äôm not sure what exactly to look for. SPARK-4587 (model import/export) looks relevant, but doesn‚Äôt address the story directly.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 11 Aug 2016 17:53:36 +0000",Re: Serving Spark ML models via a regular Python web app,Chris Fregly <chris@fregly.com>,"Thanks for the additional reference Chris. Sounds like there are a few
independent projects addressing this story.


eetup-08042016-one-click-spark-ml-pipeline-deploy-to-production
e
l
ng
:
rk
Äôre
vidual data points
s
new to the
4587
rectly.
"
Michael Allman <michael@videoamp.com>,"Thu, 11 Aug 2016 10:58:11 -0700",Re: Serving Spark ML models via a regular Python web app,Chris Fregly <chris@fregly.com>,"Hi Chris,

I was just checking out your project. I mentioned we use MLeap to serve predictions from a trained Spark ML RandomForest model. How would I do that with pipeline.io <http://pipeline.io/>? It isn't clear to me.

Thanks!

Michael

<http://pipeline.io/> that summarizes what we're working on (all open source):  
https://www.slideshare.net/mobile/cfregly/advanced-spark-and-tensorflow-meetup-08042016-one-click-spark-ml-pipeline-deploy-to-production <https://www.slideshare.net/mobile/cfregly/advanced-spark-and-tensorflow-meetup-08042016-one-click-spark-ml-pipeline-deploy-to-production>
quite sure where that project will go.  doesn't seem like it will have a long shelf-life in my opinion.
there.
project is addressing.  check it out and send me feedback or create issues at that github location.
comprehensive overview of existing JIRA discussions about this. I've added myself as a watcher on the various tasks.
bringing in all of Spark as a dependency.
DataFrames either (which for single-instance prediction makes things sub-optimal). That is covered here: https://issues.apache.org/jira/browse/SPARK-10413 <https://issues.apache.org/jira/browse/SPARK-10413>
""roll your own"". Or you can try to export to some other format such as PMML or PFA. Some MLlib models support PMML export, but for ML it is still missing (see https://issues.apache.org/jira/browse/SPARK-11171 <https://issues.apache.org/jira/browse/SPARK-11171>).
https://github.com/jpmml/jpmml-sparkml <https://github.com/jpmml/jpmml-sparkml> - which is by now actually quite comprehensive. It shows that PMML can represent a pretty large subset of typical ML pipeline functionality.
options are pretty much ""roll your own"" currently, or export in PMML or PFA.
local model-serving (for some initial discussion about the future see https://issues.apache.org/jira/browse/SPARK-16365 <https://issues.apache.org/jira/browse/SPARK-16365>).
<https://github.com/TrueCar/mleap>. It's not python, but we use it in production to serve a random forest model trained by a Spark ML pipeline.
Spark ML models via, for example, a regular Python web app?
several TB of data, and now you want to use it in a prediction service that you‚Äôre building, say with Flask <http://flask.pocoo.org/>. In principle, you don‚Äôt need Spark anymore since you‚Äôre just passing individual data points to your model and looking for it to spit some prediction back.
needs to run in their web service to serve up the model. (Sorry, I‚Äôm new to the ML side of Spark. üòÖ)
story? I did a search, but I‚Äôm not sure what exactly to look for. SPARK-4587 <https://issues.apache.org/jira/browse/SPARK-4587> (model import/export) looks relevant, but doesn‚Äôt address the story directly.

"
Michael Armbrust <michael@databricks.com>,"Thu, 11 Aug 2016 11:54:05 -0700",Re: Sorting within partitions is not maintained in parquet?,Hyukjin Kwon <gurwls223@gmail.com>,"This is an optimization to avoid overloading the scheduler with many small
tasks.  It bin-packs data into tasks based on the file size.

You can disable it by setting spark.sql.files.openCostInBytes very high
(higher than spark.sql.files.maxPartitionBytes).


h
ds
no
anymore.
"
SURESH CHAGANTI <chaganti.bigdata@gmail.com>,"Fri, 12 Aug 2016 10:54:18 -0700 (MST)",Re: Welcoming Felix Cheung as a committer,dev@spark.apache.org,"Congrats Felix!



-----
-SUURI
--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Fri, 12 Aug 2016 18:45:03 -0700",Re: Welcoming Felix Cheung as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congrats Felix!

(I don't believe SparkR is ever getting close to how excellent
Spark/Scala is but it's worth seeing some competition in this area :))

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Sun, 14 Aug 2016 18:11:04 -0700",Re: Spark 2.0.1 / 2.1.0 on Maven,Chris Fregly <chris@fregly.com>,"Hi Chris,

With my ASF member hat on...

Oh, come on, Chris. It's not ""in violation of ASF policies""
whatsoever. Policies are for ASF developers not for users. Honestly, I
was surprised to read the note in Mark Hamstra's email. It's very
restrictive but it says about what committers and PMCs should do not
users:

""Do not include any links on the project website that might encourage
non-developers to download and use nightly builds, snapshots, release
candidates, or any other similar package.""

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 15 Aug 2016 09:08:48 +0100",Re: Spark 2.0.1 / 2.1.0 on Maven,Jacek Laskowski <jacek@japila.pl>,"I believe Chris was being a bit facetious.

The ASF guidance is right, that it's important people don't consume
non-blessed snapshot builds as like other releases. The intended
audience is developers and so the easiest default policy is to only
advertise the snapshots where only developers are likely to be
looking.

That said, they're not secret or confidential, and while this probably
should go to dev@, it's not a sin to mention the name of snapshots on
user@, as long as these disclaimers are clear too. I'd rather a user
understand the full picture, than find the snapshots and not
understand any of the context.


---------------------------------------------------------------------


"
Paul Roy <roykoikai@gmail.com>,"Mon, 15 Aug 2016 12:27:25 +0300",Re: Welcoming Felix Cheung as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congrats Felix

Paul Roy.




-- 
""Change is slow and gradual. It requires hardwork, a bit of
luck, a fair amount of self-sacrifice and a lot of patience.""

Roy.
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 15 Aug 2016 10:10:48 +0000",Re: Spark 2.0.1 / 2.1.0 on Maven,dev <dev@spark.apache.org>,"
As well as the legal issue 'nightly builds haven't been through the strict review and license check process for ASF releases', and the engineering issue 'release off a nightly and your users will hate you', there's an ASF community one: ASF projects want to build a dev community as well as a user one ‚Äîand encouraging users to jump to coding for/near a project is a way to do this.


1. Anyone on user@ lists are strongly encouraged to get on the dev@ lists, not just to contribute code but to contribute to discourse on project direction, comment on issues, review other code.

2. Its really good if someone builds/tests their downstream apps against pre-releases; catching problems early is the best way to fix them.

3. it really, really helps if the people doing build/tess of downstream apps have their own copy of the source code, in sync with the snapshots they use. That puts them in the place to start debugging any problems which surface, identify if it is a bug in their own code surfacing, vs a regression in the dependency ‚Äîand, if it is the latter, they are in the position to start working on a fix * and test it in the exact environment where the problem arises*

That's why you wan't to restrict these snapshots to developers: it's not ""go away, user"", it's ""come and join the developers'


> On 15 Aug 2016, at 09:08, Sean Owen <sowen@cloudera.com> wrote:
> 
> I believe Chris was being a bit facetious.
> 
> The ASF guidance is right, that it's important people don't consume
> non-blessed snapshot builds as like other releases. The intended
> audience is developers and so the easiest default policy is to only
> advertise the snapshots where only developers are likely to be
> looking.
> 
> That said, they're not secret or confidential, and while this probably
> should go to dev@, it's not a sin to mention the name of snapshots on
> user@, as long as these disclaimers are clear too. I'd rather a user
> understand the full picture, than find the snapshots and not
> understand any of the context.
> 
> On Mon, Aug 15, 2016 at 2:11 AM, Jacek Laskowski <jacek@japila.pl> wrote:
>> Hi Chris,
>> 
>> With my ASF member hat on...
>> 
>> Oh, come on, Chris. It's not ""in violation of ASF policies""
>> whatsoever. Policies are for ASF developers not for users. Honestly, I
>> was surprised to read the note in Mark Hamstra's email. It's very
>> restrictive but it says about what committers and PMCs should do not
>> users:
>> 
>> ""Do not include any links on the project website that might encourage
>> non-developers to download and use nightly builds, snapshots, release
>> candidates, or any other similar package.""
>> 
>> Pozdrawiam,
>> Jacek Laskowski
>> ----
>> https://medium.com/@jaceklaskowski/
>> Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
>> Follow me at https://twitter.com/jaceklaskowski
> 
> ---------------------------------------------------------------------
> To unsubscribe e-mail: dev-unsubscribe@spark.apache.org
> 
> 

"
mayur bhole <mayur.bhole01@gmail.com>,"Mon, 15 Aug 2016 18:46:22 +0530",Re: Welcoming Felix Cheung as a committer,Paul Roy <roykoikai@gmail.com>,"Congrats Felix!


"
mikhainin <bespoleznyak@narod.ru>,"Mon, 15 Aug 2016 06:39:55 -0700 (MST)",Spark hangs after OOM in Serializer,dev@spark.apache.org,"Hi guys, 

I'm using Spark 1.6.2 and faced some problem so I kindly ask you to help.
Sometimes, when DAGScheduler tries to serialise pair <rdd, func> OOM
exception is thrown inside closureSerializer.serialize() call (you may see a
stack-trace below). But it isn't a problem itself, the problem is that Spark
hangs after this has happened.

I've fixed the problem by adding OOM handling to try-catch statement inside
submitMissingTasks() function and now when this happen, Spark is correctly
finishes its work. But I noticed that Non-fatal are handling and Spark abort
the task when such ones happen and there are a lot of places where
NonFatal(e) is handled. So it looks like other types of errors are
deliberately ignored. 

And as long as I don't clearly understand the reason why so, I'm not sure
that the fix is correct. Could you please have a look and point me to a
better solution for the issue?
The fix:  abort-task-on-oom-in-dag-scheduler.patch
<http://apache-spark-developers-list.1001551.n3.nabble.com/file/n18639/abort-task-on-oom-in-dag-scheduler.patch>  


	at
org.apache.spark.scheduler.DAGScheduler.submitMissingTasks(DAGScheduler.scala:1016)
	at
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitStage(DAGScheduler.scala:921)
	at
org.apache.spark.scheduler.DAGScheduler.handleJobSubmitted(DAGScheduler.scala:861)
	at
	at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1603)
	at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1592)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)
	at
org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:920)
	at
org.apache.spark.rdd.RDD$$anonfun$foreachPartition$1.apply(RDD.scala:918)
	at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)
	at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:316)
	at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:918)
	at com.massivedatascience.util.SparkHelper$class.sync(SparkHelper.scala:39)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans.sync(ColumnTrackingKMeans.scala:255)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans$$anonfun$11.apply(ColumnTrackingKMeans.scala:457)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans$$anonfun$11.apply(ColumnTrackingKMeans.scala:456)
	at
com.massivedatascience.util.SparkHelper$class.withBroadcast(SparkHelper.scala:83)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans.withBroadcast(ColumnTrackingKMeans.scala:255)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans.com$massivedatascience$clusterer$ColumnTrackingKMeans$$lloyds$1(ColumnTrackingKMeans.scala:456)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans$$anonfun$cluster$3$$anonfun$apply$5.apply(ColumnTrackingKMeans.scala:485)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans$$anonfun$cluster$3$$anonfun$apply$5.apply(ColumnTrackingKMeans.scala:480)
	at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans$$anonfun$cluster$3.apply(ColumnTrackingKMeans.scala:480)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans$$anonfun$cluster$3.apply(ColumnTrackingKMeans.scala:479)
	at
com.massivedatascience.util.SparkHelper$class.withCached(SparkHelper.scala:71)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans.withCached(ColumnTrackingKMeans.scala:255)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans.cluster(ColumnTrackingKMeans.scala:479)
	at
com.massivedatascience.clusterer.MultiKMeansClusterer$class.best(MultiKMeansClusterer.scala:37)
	at
com.massivedatascience.clusterer.ColumnTrackingKMeans.best(ColumnTrackingKMeans.scala:255)
	at com.massivedatascience.clusterer.KMeans$.simpleTrain(KMeans.scala:168)
	at
com.massivedatascience.clusterer.KMeans$.iterativelyTrain(KMeans.scala:249)
	at
com.massivedatascience.clusterer.KMeans$$anonfun$trainWeighted$1.apply(KMeans.scala:111)
	at
com.massivedatascience.clusterer.KMeans$$anonfun$trainWeighted$1.apply(KMeans.scala:110)
	at
com.massivedatascience.util.SparkHelper$class.withCached(SparkHelper.scala:55)
	at com.massivedatascience.clusterer.KMeans$.withCached(KMeans.scala:32)
	at com.massivedatascience.clusterer.KMeans$.trainWeighted(KMeans.scala:110)
	at
com.massivedatascience.clusterer.KMeans$$anonfun$train$2.apply(KMeans.scala:81)
	at
com.massivedatascience.clusterer.KMeans$$anonfun$train$2.apply(KMeans.scala:77)
	at
com.massivedatascience.util.SparkHelper$class.withCached(SparkHelper.scala:71)
	at com.massivedatascience.clusterer.KMeans$.withCached(KMeans.scala:32)
	at com.massivedatascience.clusterer.KMeans$.train(KMeans.scala:77)
	at com.badoo.antispam.clustering.InvokeKMeans$.main(InvokeKmeans.scala:175)
	at com.badoo.antispam.clustering.InvokeKMeans.main(InvokeKmeans.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at
org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:558)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at java.util.Arrays.copyOf(Arrays.java:3332)
	at
java.lang.AbstractStringBuilder.expandCapacity(AbstractStringBuilder.java:137)
	at
java.lang.AbstractStringBuilder.ensureCapacityInternal(AbstractStringBuilder.java:121)
	at java.lang.AbstractStringBuilder.append(AbstractStringBuilder.java:421)
	at java.lang.StringBuilder.append(StringBuilder.java:136)
	at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at
java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)
	at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)
	at scala.collection.immutable.$colon$colon.writeObject(List.scala:379)
	at sun.reflect.GeneratedMethodAccessor93.invoke(Unknown Source)
	at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at java.io.ObjectStreamClass.invokeWriteObject(ObjectStreamClass.java:988)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1496)
	at
java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)




--

---------------------------------------------------------------------


"
Minudika Malshan <minudika001@gmail.com>,"Mon, 15 Aug 2016 22:16:51 +0530",How to resolve the SparkExecption : Size exceeds Integer.MAX_VALUE,dev@spark.apache.org,"Hi all,

I am trying to create and train a model for a Kaggle competition dataset
using Apache spark. The dataset has more than 10 million rows of data.
But when training the model, I get an exception ""*Size exceeds
Integer.MAX_VALUE*"".

I found the same question has been raised in Stack overflow but those
answers didn't help much.

It would be a great if you could help to resolve this issue.

Thanks.
Minudika
"
Ewan Leith <ewan.leith@realitymine.com>,"Mon, 15 Aug 2016 19:04:06 +0000","Re: How to resolve the SparkExecption : Size exceeds
 Integer.MAX_VALUE",Minudika Malshan <minudika001@gmail.com>,"I think this is more suited to the user mailing list than the dev one, but this almost always means you need to repartition your data into smaller partitions as one of the partitions is over 2GB.

When you create your dataset, put something like . repartition(1000) at the end of the command creating the initial dataframe or dataset.

Ewan

Hi all,

I am trying to create and train a model for a Kaggle competition dataset using Apache spark. The dataset has more than 10 million rows of data.
But when training the model, I get an exception ""Size exceeds Integer.MAX_VALUE"".

I found the same question has been raised in Stack overflow but those answers didn't help much.

It would be a great if you could help to resolve this issue.

Thanks.
Minudika


This email and any attachments to it may contain confidential information and are intended solely for the addressee and. If you are not the intended recipient of this email or if you have believe you have received this email in error, please contact the sender and remove it from your system. Do not use, copy or disclose the information contained in this email or in any attachment. RealityMine Limited may monitor email traffic data. RealityMine Limited may monitor email traffic data and also the content of email for the purposes of security. RealityMine Limited is a company registered in England and Wales. Registered number: 07920936 Registered office: Warren Bruce Court, Warren Bruce Road, Trafford Park, Manchester M17 1LB
"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Mon, 15 Aug 2016 19:13:58 +0000",Number of tasks on executors become negative after executor failures,"""'dev@spark.apache.org'"" <dev@spark.apache.org>, ""'user@spark.apache.org'""
	<user@spark.apache.org>","Summary:
I am running Spark 1.5 on CDH5.5.1.  Under extreme load intermittently I am getting this connection failure exception and later negative executor in the Spark UI.

Exception:
TRACE: org.apache.hadoop.hbase.ipc.AbstractRpcClient - Call: Multi, callTime: 76ms
INFO : org.apache.spark.network.client.TransportClientFactory - Found inactive connection to xxxx/xxx.xxx.xxx.xxxx, creating a new one.
ERROR: org.apache.spark.network.shuffle.RetryingBlockFetcher - Exception while beginning fetch of 1 outstanding blocks (after 1 retries)
java.io.IOException: Failed to connect to xxxx/xxx.xxx.xxx.xxxx
                at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:193)
                at org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:156)
                at org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:88)
                at org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
                at org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
                at org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
                at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
                at java.util.concurrent.FutureTask.run(FutureTask.java:262)
                at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
                at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
                at java.lang.Thread.run(Thread.java:745)
Caused by: java.net.ConnectException: Connection refused: xxxx/xxx.xxx.xxx.xxxx
                at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
                at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739)
                at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
                at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
                at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
                at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
                at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
                at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
                ... 1 more


Related Defects:
https://issues.apache.org/jira/browse/SPARK-2319
https://issues.apache.org/jira/browse/SPARK-9591


[cid:image001.png@01D1F6EE.1CCFE110]
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 15 Aug 2016 17:41:18 -0700",Re: Spark 2.0.1 / 2.1.0 on Maven,Sean Owen <sowen@cloudera.com>,"Thanks Sean. That reflects my sentiments so well!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
"""Guo, Chenzhao"" <chenzhao.guo@intel.com>","Tue, 16 Aug 2016 02:12:25 +0000",Structured Streaming with Kafka sources/sinks,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I'm trying to write Structured Streaming test code and will deal with Kafka source. Currently Spark 2.0 doesn't support Kafka sources/sinks.

I found some Databricks slides saying that Kafka sources/sinks will be implemented in Spark 2.0, so is there anybody working on this? And when will it be released?

Thanks,
Chenzhao Guo
"
Cody Koeninger <cody@koeninger.org>,"Mon, 15 Aug 2016 21:26:45 -0500",Re: Structured Streaming with Kafka sources/sinks,"""Guo, Chenzhao"" <chenzhao.guo@intel.com>","https://issues.apache.org/jira/browse/SPARK-15406

I'm not working on it (yet?), never got an answer to the question of
who was planning to work on it.

te:
with Kafka
ill

---------------------------------------------------------------------


"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 16 Aug 2016 11:31:28 +0530",Resultant RDD after a group by query always returns 200 partitions,"""dev@spark.apache.org"" <dev@spark.apache.org>, Michael Armbrust <michael@databricks.com>","Hi,

I ran the following simple code in a pseudo distributed spark cluster.

*object ScalaDummy {*
*  def main(args: Array[String]) {*
*    val appName = ""testApp "" + Calendar.getInstance().getTime*
*    val conf: SparkConf = new
SparkConf().setMaster(""spark://nira-wso2:7077"").setAppName(appName)*

*    val propsFile: String =
Thread.currentThread.getContextClassLoader.getResource(""spark-defaults.conf"").getPath*
*    val properties: Map[String, String] =
Utils.getPropertiesFromFile(propsFile)*
*    conf.setAll(properties)*

*    val sqlCtx: SQLContext = new SQLContext(new JavaSparkContext(conf))*

*    sqlCtx.createDataFrame(Seq(*
*      (83, 0, 38),*
*      (26, 0, 79),*
*      (43, 81, 24))).toDF(""a"", ""b"", ""c"").registerTempTable(""table1"")*

*    val dataFrame: DataFrame = sqlCtx.sql(""select count(*) from table1
group by a, b"")*
*    val partitionCount = dataFrame.rdd.partitions.length*
*    dataFrame.show*

*    println(""Press any key to exit!"")*
*    Console.readLine()*
*  }*
*}*

I observed that the resultant dataFrame encapsulated a MapPartitionsRDD
with 200 partitions. Please see the attached screenshots.

I'm wondering where this 200 partitions came from? Is this the default
number of partitions for MapPartitionsRDD? or is there any way to control
this?
The result only contains 3 rows. So, IMO having 200 partitions for this is
not so efficient because 197< tasks are run on empty partitions, from what
I could see.

Am I doing something wrong here or is this the expected behavior after a
'group by' query?

Look forward to hearing from you!

Best
-- 
Niranda Perera
@n1r44 <https://twitter.com/N1R44>
+94 71 554 8430
https://www.linkedin.com/in/niranda
https://pythagoreanscript.wordpress.com/

---------------------------------------------------------------------"
Reynold Xin <rxin@databricks.com>,"Mon, 15 Aug 2016 23:04:26 -0700",Re: Structured Streaming with Kafka sources/sinks,Cody Koeninger <cody@koeninger.org>,"We (the team at Databricks) are working on one currently.



l with
.
"
Rishi Mishra <rmishra@snappydata.io>,"Tue, 16 Aug 2016 14:17:25 +0530",Re: Resultant RDD after a group by query always returns 200 partitions,Niranda Perera <niranda.perera@gmail.com>,"That's the default shuffle partitions with Spark, You can tune it using
spark.sql.shuffle.partitions.

Regards,
Rishitesh Mishra,
SnappyData . (http://www.snappydata.io/)

https://in.linkedin.com/in/rishiteshmishra


"
colzer <471519678@qq.com>,"Tue, 16 Aug 2016 03:14:45 -0700 (MST)","Re: Resultant RDD after a group by query always returns 200
 partitions",dev@spark.apache.org,"In additionÔºåyou can also set spark.sql.adaptive.enabled=true (default=false)
Ôºåenable adaptive query execution„ÄÇ



--
3.nabble.com/Resultant-RDD-after-a-group-by-query-always-returns-200-partitions-tp18647p18650.html
om.

---------------------------------------------------------------------


"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 16 Aug 2016 18:56:48 +0530",Executors go OOM when using JDBC relation provider,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I have been using a standalone spark cluster (v1.4.x) with the following
configurations. 2 nodes with 1 core each and 4g memory workers in each
node. So I had 2 executors for my app with 2 cores and 8g memory in total.

I have a table in a MySQL database which has around 10million rows. It has
around 10 columns with integer, string and date types. (say table1 with
column c1 to c10)

I run the following query,


   1. select count(*) from table1 - completes within seconds
   2. select c1, count(*) from table1 group by c1 - complete within seconds
   but more than the 1st query
   3. select c1, c2, count(*) from table1 group by c1, c2 - same behavior
   as Q2
   4. select c1, c2, c3, c4, count(*) from table1 group by c1, c2, c3, c4 -
   took a few minutes to finish
   5. select c1, c2, c3, c4, count(*) from table1 group by c1, c2, c3, c4, *c5
   *-* Executor goes OOM within a few minutes!!! *(this has one more column
   for group by statement)

It seemed like the more the group by columns added, the time grows
*exponentially!* Is this the expected behavior?
I was monitoring the MySQL process list, and observed that the data was
transmitted to the executors within a few seconds without an issue.
NOTE: I am not using any partition columns here. So, AFAIU essentially
there's only a single partition for the JDBC RDD

I ran the same query (query 5) in MySQL console and I was able to get a
result with in 3 minutes!!! So, I'm wondering what could have been the
issue here. This OOM exception is actually a blocker!

Are there any other tuning I should do? And it certainly worries me to see
that MySQL gave a significantly fast result than Spark here!

Look forward to hearing from you!

Best

-- 
Niranda Perera
@n1r44 <https://twitter.com/N1R44>
+94 71 554 8430
https://www.linkedin.com/in/niranda
https://pythagoreanscript.wordpress.com/
"
Tim Hunter <timhunter@databricks.com>,"Tue, 16 Aug 2016 09:32:55 -0700",GraphFrames 0.2.0 released,"user <user@spark.apache.org>, dev@spark.apache.org","Hello all,
I have released version 0.2.0 of the GraphFrames package. Apart from a few
bug fixes, it is the first release published for Spark 2.0 and both scala
2.10 and 2.11. Please let us know if you have any comment or questions.

It is available as a Spark package:
https://spark-packages.org/package/graphframes/graphframes

The source code is available as always at
https://github.com/graphframes/graphframes


What is GraphFrames?

GraphFrames is a DataFrame-based graph engine Spark. In addition to the
algorithms available in GraphX, users can write highly expressive queries
by leveraging the DataFrame API, combined with a new API for motif finding.
The user also benefits from DataFrame performance optimizations within the
Spark SQL engine.

Cheers

Tim
"
Shagun Sodhani <sshagunsodhani@gmail.com>,"Tue, 16 Aug 2016 22:43:39 +0530",Re: GraphFrames 0.2.0 released,Tim Hunter <timhunter@databricks.com>,"Hi Tim.

Could you share link to the release docs as well?

Thanks,
Shagun
https://twitter.com/shagunsodhani


"
Joseph Bradley <joseph@databricks.com>,"Tue, 16 Aug 2016 15:51:26 -0700",Re: Welcoming Felix Cheung as a committer,mayur bhole <mayur.bhole01@gmail.com>,"Welcome Felix!


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 16 Aug 2016 18:18:13 -0700",Re: GraphFrames 0.2.0 released,Tim Hunter <timhunter@databricks.com>,"Hi Tim,

AWESOME. Thanks a lot for releasing it. That makes me even more eager
to see it in Spark's codebase (and replacing the current RDD-based
API)!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 16 Aug 2016 19:33:28 -0700","[master] ERROR RetryingHMSHandler: AlreadyExistsException(message:Database
 default already exists)",dev <dev@spark.apache.org>,"Hi,

I'm working with today's build and am facing the issue:

scala> Seq(A(4)).toDS
16/08/16 19:26:26 ERROR RetryingHMSHandler:
AlreadyExistsException(message:Database default already exists)
    at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:891)
...

res1: org.apache.spark.sql.Dataset[A] = [id: int]

scala> spark.version
res2: String = 2.1.0-SNAPSHOT

See the complete stack trace at
https://gist.github.com/jaceklaskowski/a969fdd5c2c9cdb736bf647b01257a3e.

I'm quite positive that it didn't happen a day or two ago.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Tue, 16 Aug 2016 22:51:31 -0700","Re: [master] ERROR RetryingHMSHandler: AlreadyExistsException(message:Database
 default already exists)",Jacek Laskowski <jacek@japila.pl>,"Hi Jacek,

We will try to create the default database if it does not exist. Hive
actually relies on that AlreadyExistsException to determine if a db already
exists and ignore the error to implement the logic of ""CREATE DATABASE IF
NOT EXISTS"". So, that message does not mean any bad thing happened. I think
we can avoid of having this error log by changing
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/SessionCatalog.scala#L84-L91.
Basically, we will check if the default db already exists and we only call
create database if the default db does not exist. Do you want to try it?

Thanks,

Yin


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 16 Aug 2016 23:06:29 -0700","Re: [master] ERROR RetryingHMSHandler: AlreadyExistsException(message:Database
 default already exists)",Yin Huai <yhuai@databricks.com>,"

Yes, indeed! I'd be more than happy. Guide me if you don't mind. Thanks.

Should I create a JIRA for this?

Jacek

---------------------------------------------------------------------


"
Senthil Kumar <senthilec566@gmail.com>,"Wed, 17 Aug 2016 14:53:29 +0530",Spark R - Loading Third Party R Library in YARN Executors,"dev@spark.apache.org, Senthil kumar <senthilec566@gmail.com>, dufan@ebay.com, 
	jiajsun@ebay.com","Hi All ,  We are using Spark 1.6 Version R library .. Below is our code
which Loads the THIRD Party Library .


library(""BreakoutDetection"", lib.loc = ""*hdfs://xxxxxx/BreakoutDetection/*"")
:
library(""BreakoutDetection"", lib.loc = ""*//xxxxxx/BreakoutDetection/*"") :


When i try to execute the code using LOCAL Mode , Spark R code is Working
fine without any issue . If i submit the Job in Cluster , we will end up
with error.

*error in evaluating the argument 'X' in selecting a method for function
'lapply': Error in library(""BreakoutDetection"", lib.loc =
""hdfs://xxxxxxx/BreakoutDetection/"") :*
*  no library trees found in 'lib.loc'*
*Calls: f ... lapply -> FUN -> mainProcess -> angleValid -> library*


Can't we read libraries in R as below ?
library(""BreakoutDetection"", lib.loc = ""*hdfs://xxxxxx/BreakoutDetection/*"")
:

If not what is the other way to solve this problem ?

Since our cluster having close to 2500 nodes we cant copy the Third Party
Libs to all nodes .. Copying to all DNs is not good practice too ..

Can someone help me here How to load R libs from HDFS or any other way  ?


--Senthil
"
Felix Cheung <felixcheung_m@hotmail.com>,"Wed, 17 Aug 2016 11:16:40 +0000",Re: Spark R - Loading Third Party R Library in YARN Executors,"Senthil kumar <senthilec566@gmail.com>, ""dufan@ebay.com"" <dufan@ebay.com>,
	""jiajsun@ebay.com"" <jiajsun@ebay.com>, Senthil Kumar
	<senthilec566@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","When you call library(), that is the library loading function in native R. As of now it does not support HDFS but there are several packages out there that might help.

Another approach is to have a prefetch/installation mechanism to call HDFS command to download the R package from HDFS onto the worker node first.


_____________________________
From: Senthil Kumar <senthilec566@gmail.com<mailto:senthilec566@gmail.com>>
Sent: Wednesday, August 17, 2016 2:23 AM
Subject: Spark R - Loading Third Party R Library in YARN Executors
To: Senthil kumar <senthilec566@gmail.com<mailto:senthilec566@gmail.com>>, <dufan@ebay.com<mailto:dufan@ebay.com>>, <jiajsun@ebay.com<mailto:jiajsun@ebay.com>>, <dev@spark.apache.org<mailto:dev@spark.apache.org>>


Hi All ,  We are using Spark 1.6 Version R library .. Below is our code which Loads the THIRD Party Library .


library(""BreakoutDetection"", lib.loc = ""hdfs://xxxxxx/BreakoutDetection/"") :
library(""BreakoutDetection"", lib.loc = ""//xxxxxx/BreakoutDetection/"") :


When i try to execute the code using LOCAL Mode , Spark R code is Working fine without any issue . If i submit the Job in Cluster , we will end up with error.

error in evaluating the argument 'X' in selecting a method for function 'lapply': Error in library(""BreakoutDetection"", lib.loc = ""hdfs://xxxxxxx/BreakoutDetection/"") :
  no library trees found in 'lib.loc'
Calls: f ... lapply -> FUN -> mainProcess -> angleValid -> library


Can't we read libraries in R as below ?
library(""BreakoutDetection"", lib.loc = ""hdfs://xxxxxx/BreakoutDetection/"") :

If not what is the other way to solve this problem ?

Since our cluster having close to 2500 nodes we cant copy the Third Party Libs to all nodes .. Copying to all DNs is not good practice too ..

Can someone help me here How to load R libs from HDFS or any other way  ?


--Senthil




"
Yin Huai <yhuai@databricks.com>,"Wed, 17 Aug 2016 07:46:28 -0700","Re: [master] ERROR RetryingHMSHandler: AlreadyExistsException(message:Database
 default already exists)",Jacek Laskowski <jacek@japila.pl>,"Yea. Please create a jira. Thanks!


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 17 Aug 2016 11:36:36 -0700",Re: Spark R - Loading Third Party R Library in YARN Executors,Felix Cheung <felixcheung_m@hotmail.com>,"I think you can also pass in a zip file using the --files option
(http://spark.apache.org/docs/latest/running-on-yarn.html has some
examples). The files should then be present in the current working
directory of the driver R process.

Thanks
Shivaram


---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 17 Aug 2016 15:00:07 -0700",How is mapped LogicalPlan to RDDs eventually if ever? How about Dataset?,dev <dev@spark.apache.org>,"Hi,

I'm wondering how far off base I am with the question:

Is a LogicalPlan in #SparkSQL similar to a RDD in #ApacheSpark Core in
that they both seem a metadata of the computation that eventually gets
executed to produce records?

What am I missing if anything? How imprecise I am by comparing
LogicalPlan to RDD?

I'm considering a Dataset a pair of a LogicalPlan and an Encoder where
the encoder is to handle the data in a more sophisticated, low-level
way while LogicalPlan is how the data is computed/retrieved.

Please help me understand the concepts in a more accurate way. Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 18 Aug 2016 06:16:59 +0000",Re: Spark SQL and Kryo registration,"""=?UTF-8?Q?Maciej_Bry=C5=84ski?="" <maciek@brynski.pl>","Hi everyone, it seems that it works now out of the box. So nevermind, registration is compatible with spark 2.0 when using dataframes.
Regards,
Olivier.





Hi Olivier, Did you check performance of Kryo ? I have observations that Kryo is slightly slower than Java Serializer.
Regards, Maciek
2016-08-04 17:41 GMT+02:00 Amit Sela < amitsela33@gmail.com > :
It should. Codegen uses the SparkConf in SparkEnv when instantiating a new
Serializer.
Hi Olivier,

I don't know either, but am curious what you've tried already.

Jacek


Hi everyone, I'm currently to use Spark 2.0.0 and making Dataframes work with kryo. registrationRequired=true Is it even possible at all considering the codegen ?
Regards,
Olivier Girardot | Associ√©
o.girardot@lateral-thoughts. com
+33 6 24 09 17 94


--
Maciek Bry≈Ñski

Olivier Girardot | Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Thu, 18 Aug 2016 06:32:10 +0000",Re: Aggregations with scala pairs,"""=?UTF-8?Q?Andr=C3=A9s_Ivaldi?="" <iaivaldi@gmail.com>","CC'ing dev list, you should open a Jira and a PR related to it to discuss it c.f.
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ContributingCodeChanges





Hello, I'd like to report a wrong behavior of DataSet's API, I don¬¥t know how I
can do that. My Jira account doesn't allow me to add a Issue
I'm using Apache 2.0.0 but the problem came since at least version 1.4 (given
the doc since 1.3)
The problem is simple to reporduce, also the work arround, if we apply agg over
a DataSet with scala pairs over the same column, only one agg over that column
is actualy used, this is because the toMap that reduce the pair values of the
mane key to one and overwriting the value
class 
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/RelationalGroupedDataset.scala


def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
agg((aggExpr +: aggExprs).toMap)
}
rewrited as somthing like this should work def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
toDF((aggExpr +: aggExprs).map { pairExpr => strToExpr(pairExpr._2)(df(pairExpr._1).expr) }.toSeq) }

regards --
Ing. Ivaldi Andres


Olivier Girardot | Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94"
,"Thu, 18 Aug 2016 07:35:47 +0100",Re: Aggregations with scala pairs,Olivier Girardot <o.girardot@lateral-thoughts.com>,"Agreed.

Regards
JB



"
Jacek Laskowski <jacek@japila.pl>,"Wed, 17 Aug 2016 23:46:42 -0700",Found a typo in Catalyst's exception and want to write a test -- help needed,dev <dev@spark.apache.org>,"Hi devs,

While reviewing the code in Catalyst for doing query parsing I found
that UnresolvedStar has this typo in the exception [1].

I do understand that it's a very trivial issue but I thought I'd write
a test for it as part of the change so I could improve my
understanding of the low-level bits and bytes of Catalyst.

scala> ds.select(""hello.*"")
org.apache.spark.sql.AnalysisException: cannot resolve 'hello.*' give
input columns '';
  at org.apache.spark.sql.catalyst.analysis.UnresolvedStar.expand(unresolved.scala:249)

What test in Spark is the closest proximity to the class that I could
extend to assert the exception's message? How to run the test? Is sbt

Please guide. Thanks.

[1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/unresolved.scala#L249

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 17 Aug 2016 23:49:47 -0700","Re: Found a typo in Catalyst's exception and want to write a test --
 help needed",Jacek Laskowski <jacek@japila.pl>,"I'd use the new SQLQueryTestSuite. Test cases defined in sql files.



"
Minudika Malshan <minudika001@gmail.com>,"Thu, 18 Aug 2016 20:29:24 +0530",How to convert spark data-frame to datasets?,dev@spark.apache.org,"Hi all,

Most of Spark ML algorithms requires a dataset to train the model.
I would like to know how to convert a spark *data-frame* to a *dataset*
using Java.
Your support is much appreciated.

Thank you!
Minudika
"
Oscar Batori <oscarbatori@gmail.com>,"Thu, 18 Aug 2016 15:13:18 +0000",Re: How to convert spark data-frame to datasets?,"Minudika Malshan <minudika001@gmail.com>, dev@spark.apache.org","<https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.package@DataFrame=org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]>,
DataFrame is just Dataset[Row]. The are various converters for subtypes of
Product if you want, using ""as[T]"", where T <: Product, or there is an
implicit decoder in scope, I believe.

Also, this is probably a user list question.



"
Ryan Williams <ryan.blake.williams@gmail.com>,"Thu, 18 Aug 2016 17:49:35 +0000",Setting YARN executors' JAVA_HOME,"""dev@spark.apache.org"" <dev@spark.apache.org>","I need to tell YARN a JAVA_HOME to use when spawning containers (to run a
Java 8 app on Java 7 YARN).

The only way I've found that works is
setting SPARK_YARN_USER_ENV=""JAVA_HOME=/path/to/java8"".

The code
<https://github.com/apache/spark/blob/b72bb62d421840f82d663c6b8e3922bd14383fbb/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L762>
implies that this is deprecated and users should use ""the config"", but I
can't figure out what config is being referenced.

Passing ""--conf spark.yarn.appMasterEnv.JAVA_HOME=/path/to/java8"" seems to
set it for the AM but not for executors.

Likewise, spark.executor.extraLibraryPath and spark.driver.extraLibraryPath
don't appear to set JAVA_HOME (and maybe aren't even supposed to?).

The 1.0.1 docs
<https://spark.apache.org/docs/1.0.1/running-on-yarn.html#environment-variables>
 are the last ones to reference the SPARK_YARN_USER_ENV var, afaict.

What's the preferred way of passing YARN a custom JAVA_HOME that will be
applied to executors' containers?

Thanks!
"
dhruve ashar <dhruveashar@gmail.com>,"Thu, 18 Aug 2016 13:51:30 -0500",Re: Setting YARN executors' JAVA_HOME,Ryan Williams <ryan.blake.williams@gmail.com>,"Hi Ryan,

You can get more info on this here:  Spark documentation
<http://spark.apache.org/docs/latest/configuration.html>.

The page addresses what you need. You can look for
spark.executorEnv.[EnvironmentVariableName]
and set your java home as
spark.executorEnv.JAVA_HOME=....

Regards,
Dhruve





-- 
-Dhruve Ashar
"
Ryan Williams <ryan.blake.williams@gmail.com>,"Thu, 18 Aug 2016 18:56:45 +0000",Re: Setting YARN executors' JAVA_HOME,dhruve ashar <dhruveashar@gmail.com>,"Ah, I guess I missed that by only looking in the YARN config docs, but this
is a more general parameter and not documented there. Thanks!


"
Holden Karau <holden@pigscanfly.ca>,"Thu, 18 Aug 2016 12:33:37 -0700",Early Draft Structured Streaming Machine Learning,"""dev@spark.apache.org"" <dev@spark.apache.org>, Seth Hendrickson <shendri@us.ibm.com>","Hi Everyone (that cares about structured streaming and ML),

Seth and I have been giving some thought to support structured streaming in
machine learning - we've put together an early design doc (its been in JIRA
(SPARK-16424) <https://issues.apache.org/jira/browse/SPARK-16424> for
awhile, but incase you missed it) we'd love your comments.

Also if anyone happens to be in San Francisco this week and would like to
chat over coffee or bubble tea (I know some people on the list already get
bobba tea as a perk so in that case we can get pastries) feel free to reach
out to me on or off-list (
https://docs.google.com/document/d/1snh7x7b0dQIlTsJNHLr-IxIFgP43RfRV271YK2qGiFQ/edit?usp=sharing
) and we will update the list with the result of any such discussion.

Cheers,

Holden :)

-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Jeremy Smith <jeremy.smith@acorns.com>,"Thu, 18 Aug 2016 13:01:01 -0700",Parquet partitioning / appends,dev@spark.apache.org,"Hi,

I'm running into an issue wherein Spark (both 1.6.1 and 2.0.0) will fail
with a GC Overhead limit when creating a DataFrame from a parquet-backed
partitioned Hive table with a relatively large number of parquet files (~
175 partitions, and each partition contains many parquet files).  If I then
use Hive directly to create a new table from the partitioned table with
CREATE TABLE AS, Hive completes that with no problem and Spark then has no
problem reading the resulting table.

Part of the problem is that whenever we insert records to a parquet table,
it creates a new parquet file; this results in many small parquet files for
a streaming job. Since HDFS supports file appending, couldn't the records
be appended to the existing parquet file as a new row group? If I
understand correctly, this would be pretty straightforward - append the new
data pages and then write a copy of the existing footer with the new row
groups included.  It wouldn't be as optimal as creating a whole new parquet
file including all the data, but it would be much better than creating many
small files (for many different reasons, including the crash case above).
And I'm sure I can't be the only one struggling with streaming output to
parquet.

I know the typical solution to this is to periodically compact the small
files into larger files, but it seems like parquet ought to be appendable
as-is - which would obviate the need for that.

Here's a partial trace of the error for reference:
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at
java.io.ObjectStreamClass.getClassDataLayout0(ObjectStreamClass.java:1251)
        at
java.io.ObjectStreamClass.getClassDataLayout(ObjectStreamClass.java:1195)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1885)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
        at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:75)
        at
org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:108)

Thanks,
Jeremy
"
Ignacio Zendejas <iz@node.io>,"Thu, 18 Aug 2016 13:43:48 -0700","Re: RFC: Remote ""HBaseTest"" from examples?",Ted Yu <yuzhihong@gmail.com>,"I'm very late to this party and I get hbase-spark... what's the
recommendation for pyspark + hbase? I realize this isn't necessarily a
concern of the spark project, but it'd be nice to at least document it here
with a very short and sweet response because I haven't found anything
useful in the wild besides using the approach in the examples with
pythonconverters, which were dropped in 2.0.

Thanks.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 19 Aug 2016 18:28:52 +0000",Persisting PySpark ML Pipelines that include custom Transformers,Spark dev list <dev@spark.apache.org>,"I understand persistence for PySpark ML pipelines is already present in
2.0, and further improvements are being made for 2.1 (e.g. SPARK-13786
<https://issues.apache.org/jira/browse/SPARK-13786>).

I‚Äôm having trouble, though, persisting a pipeline that includes a custom
Transformer (see SPARK-17025
<https://issues.apache.org/jira/browse/SPARK-17025>). It appears that there
is a magic _to_java() method that I need to implement.

Is the intention that developers implementing custom Transformers would
also specify how it should be persisted, or are there ideas about how to
make this automatic? I searched on JIRA but I‚Äôm not sure if I missed an
issue that already addresses this problem.

Nick
‚Äã
"
Holden Karau <holden@pigscanfly.ca>,"Fri, 19 Aug 2016 12:16:40 -0700",Re: Persisting PySpark ML Pipelines that include custom Transformers,Nicholas Chammas <nicholas.chammas@gmail.com>,"I don't think we've given a lot of thought to model persistence for custom
Python models yet - if the Python models is wrapping a JVM model using the
JavaMLWritable along with '_to_java' should work provided your Java model
model you shouldn't feel the need to shoehorn yourself into this approach -
in either case much of the persistence work is up to you it's just a matter
if you do it in the JVM or Python.


 custom
sed an


-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 19 Aug 2016 21:36:11 +0000",Re: Persisting PySpark ML Pipelines that include custom Transformers,Holden Karau <holden@pigscanfly.ca>,"My pipeline (i.e. a 2.0 Pipeline) is mostly made of the built-in
is custom (i.e. I subclassed Transformer), and all it does is use a UDF to
append a VectorUDT column to a DataFrame.

To speak in more concrete terms, my custom transformer takes two columns
that contain people‚Äôs names, and appends a column of features describing
how similar those names are.

So I‚Äôm not sure where I stand as far as being able to persist this Pipeline
which includes my custom Transformer. It sounds like you‚Äôre saying I need
to do the work of defining how to persist and unpersist this Transformer
myself. Is that correct?

Is there an example I can reference of how I might do that? Looking at this
instance
<https://github.com/apache/spark/blob/acaf2a81ad5238fd1bc81e7be2c328f40c07e755/python/pyspark/ml/classification.py#L1421-L1433>
of _to_java() from a built-in Estimator, for example, doesn‚Äôt give me any
clues as to how I‚Äôd do it for my custom Transformer.

Nick
‚Äã


m
e
va
 -
er
a custom
ssed an
"
Timur Shenkao <tsh@timshenkao.su>,"Sat, 20 Aug 2016 13:41:41 +0300",Java 8,dev@spark.apache.org,"Hello, guys!

Are there any plans / tickets / branches in repository on Java 8?

I ask because ML library will gain in performance. I'd like to take part in
refactoring.
"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 20 Aug 2016 10:47:46 +0000",Re: Java 8,dev@spark.apache.org,"Spark already supports compiling with Java 8. What refactoring are you
referring to, and where do you expect to see performance gains?


"
Jerry Lam <chilinglam@gmail.com>,"Sun, 21 Aug 2016 13:07:53 -0400",Broadcast Variable Life Cycle,Spark dev list <dev@spark.apache.org>,"Hello spark developers,

Can someone explain to me what is the lifecycle of a broadcast variable?
When a broadcast variable will be garbage-collected at the driver-side and
at the executor-side? Does a spark application need to actively manage the
broadcast variables to ensure that it will not run in OOM?

Best Regards,

Jerry
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 22 Aug 2016 11:02:48 +0200",Why is isStreaming naming-inconsistent with analyzed and resolved in LogicalPlan?,dev <dev@spark.apache.org>,"Hi,

Just noticed that LogicalPlan.isStreaming flag [1] is inconsistent
name-wise to analyzed and resolved flags.

Why is isStreaming not a mere ""streaming""? That would make sense to me
(esp. given the others).

(I did manage to find a reason why it could be isStreaming but would
like to hear the real reason).

Thanks.

[1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/plans/logical/LogicalPlan.scala#L46

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 22 Aug 2016 11:28:03 +0200",Analyzer.resolver a duplicate of CatalystConf.resolver?,dev <dev@spark.apache.org>,"Hi,

Analyzer.resolver returns a Resolver per CatalystConf setting [1] that
seems a duplicate of CatalystConf.resolver.

Unless I'm mistaken, the code could get simpler and be as follows:

def resolver: Resolver = conf.resolver

Would you agree? I'm ready with a PR if so (plus some other typo fixes).

[1] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/Analyzer.scala#L67-L73

[2] https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/CatalystConf.scala#L43-L45

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 22 Aug 2016 12:14:43 -0700",critical bugs to be fixed in Spark 2.0.1?,"""dev@spark.apache.org"" <dev@spark.apache.org>","We should work on a 2.0.1 release soon, since we have found couple critical
bugs in 2.0.0. Are there any critical bugs outstanding that we should
address in 2.0.1?
"
Robert Kruszewski <robertk@palantir.com>,"Mon, 22 Aug 2016 20:20:26 +0000",Re: critical bugs to be fixed in Spark 2.0.1?,"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","SPARK-16991 (https://github.com/apache/spark/pull/14661) would be nice

 

Robert

 

From: Reynold Xin <rxin@databricks.com>
Date: Monday, August 22, 2016 at 8:14 PM
To: ""dev@spark.apache.org"" <dev@spark.apache.org>
Subject: critical bugs to be fixed in Spark 2.0.1?

 

We should work on a 2.0.1 release soon, since we have found couple critical bugs in 2.0.0. Are there any critical bugs outstanding that we should address in 2.0.1? 

 

"
trsell@gmail.com,"Mon, 22 Aug 2016 23:02:39 +0000",Re: critical bugs to be fixed in Spark 2.0.1?,"Robert Kruszewski <robertk@palantir.com>, Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","https://issues.apache.org/jira/plugins/servlet/mobile#issue/SPARK-17100

It's a blocker for upgrading.

I'd be happy to try and fix it if anyone has any hints.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 23 Aug 2016 14:15:08 +0000",Why can't a Transformer have multiple output columns?,Spark dev list <dev@spark.apache.org>,"If you create your own Spark 2.x ML Transformer, there are multiple mix-ins
(is that the correct term?) that you can use to define its behavior which
are in ml/param/shared.py
<https://github.com/apache/spark/blob/master/python/pyspark/ml/param/shared.py>
.

Among them are the following mix-ins:

   - HasInputCol
   - HasInputCols
   - HasOutputCol

What‚Äôs *not* available is a HasOutputCols mix-in, and I assume that is
intentional.

Is there a design reason why Transformers should not be able to define
multiple output columns?

I‚Äôm guessing if you are an ML beginner who thinks they need a Transformer
with multiple output columns, you‚Äôve misunderstood something. üòÖ

Nick
‚Äã
"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 23 Aug 2016 14:41:22 +0000",Re: Why can't a Transformer have multiple output columns?,Spark dev list <dev@spark.apache.org>,"It's not impossible that a Transformer could output multiple columns - it's
simply because none of the current ones do. It's true that it might be a
relatively less common use case in general.

But take StringIndexer for example. It turns strings (categorical features)
into ints (0-based indexes). It could (should) accept multiple input
columns for efficiency (see
https://issues.apache.org/jira/browse/SPARK-11215). This is a case where
multiple output columns would be required.

N


or
ed.py>
at is
nsformer
üòÖ
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 23 Aug 2016 14:47:43 +0000",Re: Why can't a Transformer have multiple output columns?,"Nick Pentreath <nick.pentreath@gmail.com>, Spark dev list <dev@spark.apache.org>","Thanks for the pointer! A linked issue from the one you shared also appears
to be relevant.

SPARK-8418 <https://issues.apache.org/jira/browse/SPARK-8418>: ""Add single-
and multi-value support to ML Transformers""


be
ior
red.py>
hat is
ansformer
üòÖ
"
Michael Allman <michael@videoamp.com>,"Tue, 23 Aug 2016 09:37:06 -0700",Fwd: Anyone else having trouble with replicated off heap RDD persistence?,Spark dev list <dev@spark.apache.org>,"FYI, I posted this to user@ and have followed up with a bug report: https://issues.apache.org/jira/browse/SPARK-17204 <https://issues.apache.org/jira/browse/SPARK-17204>

Michael

persistence?
several hours when one of the executors would segfault. That problem aside, I speculated that her job would be more robust against these kinds of executor crashes if she used replicated RDD storage. She's using off heap storage (for good reason), so I asked her to try running her job with the following storage level: `StorageLevel(useDisk = true, useMemory = true, useOffHeap = true, deserialized = false, replication = 2)`. The job would immediately fail with a rather suspicious looking exception. For example:
class ID: 9086
com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)
com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:834)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:788)
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:141)
org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:140)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:146)
org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:433)
org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:672)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
had a problem like this using off heap storage with replication factor 2?

"
Rahul Palamuttam <rahulpalamut@gmail.com>,"Tue, 23 Aug 2016 10:51:31 -0700",Serialization troubles with mutable.LinkedHashMap,dev@spark.apache.org,"Hi,

I initially send this on the user mailing list, however I didn't get any
response.
I figured this could be a bug so it might of more concern to the dev-list.

I recently switched to using kryo serialization and I've been running into
errors
with the mutable.LinkedHashMap class.

If I don't register the mutable.LinkedHashMap class then I get an
ArrayStoreException seen below.
If I do register the class, then when the LinkedHashMap is collected on the
driver, it does not contain any elements.

Here is the snippet of code I used :

val sc = new SparkContext(new SparkConf()
  .setMaster(""local[*]"")
  .setAppName(""Sample"")
  .set(""spark.serializer"", ""org.apache.spark.serializer.KryoSerializer"")
  .registerKryoClasses(Array(classOf[mutable.LinkedHashMap[String, String]])))

val collect = sc.parallelize(0 to 10)
  .map(p => new mutable.LinkedHashMap[String, String]() ++=
Array((""hello"", ""bonjour""), (""good"", ""bueno"")))

val mapSideSizes = collect.map(p => p.size).collect()(0)
val driverSideSizes = collect.collect()(0).size

println(""The sizes before collect : "" + mapSideSizes)
println(""The sizes after collect : "" + driverSideSizes)


** The following only occurs if I did not register the
mutable.LinkedHashMap class **
16/08/20 18:10:38 ERROR TaskResultGetter: Exception while getting task
result
java.lang.ArrayStoreException: scala.collection.mutable.HashMap
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$
ObjectArraySerializer.read(DefaultArraySerializers.java:338)
at com.esotericsoftware.kryo.serializers.DefaultArraySerializers$
ObjectArraySerializer.read(DefaultArraySerializers.java:293)
at com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:729)
at org.apache.spark.serializer.KryoSerializerInstance.
deserialize(KryoSerializer.scala:311)
at org.apache.spark.scheduler.DirectTaskResult.value(TaskResult.scala:97)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$
anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:60)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(
TaskResultGetter.scala:51)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(
TaskResultGetter.scala:51)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1741)
at org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(
TaskResultGetter.scala:50)
at java.util.concurrent.ThreadPoolExecutor.runWorker(
ThreadPoolExecutor.java:1142)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(
ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

I hope this is a known issue and/or I'm missing something important in my
setup.
Appreciate any help or advice!

As a bit of background this was encountered in the SciSpark project being
developed at NASA JPL.
The mutable.LinkedHashMap is necessary as it enables us to deal with Netcdf
attributes in the order they appear in the original Netcdf files.
The test case I posted above was just to show the error I'm seeing more
clearly.
Our actual use case is slightly different, but we see the same result
(empty HashMaps)..

Rahul Palamuttam
"
Reynold Xin <rxin@databricks.com>,"Tue, 23 Aug 2016 11:55:56 -0700",Re: Anyone else having trouble with replicated off heap RDD persistence?,Michael Allman <michael@videoamp.com>,"Does this problem still exist on today's master/branch-2.0?

SPARK-16550 was merged. It might be fixed already.


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Tue, 23 Aug 2016 22:20:53 +0000","How do we process/scale variable size batches in Apache Spark
 Streaming","""'dev@spark.apache.org'"" <dev@spark.apache.org>, ""'user@spark.apache.org'""
	<user@spark.apache.org>","I am running a spark streaming process where I am getting batch of data after n seconds. I am using repartition to scale the application. Since the repartition size is fixed we are getting lots of small files when batch size is very small. Is there anyway I can change the partitioner logic based on the input batch size in order to avoid lots of small files.
"
Michael Allman <michael@videoamp.com>,"Tue, 23 Aug 2016 16:54:17 -0700",Re: Anyone else having trouble with replicated off heap RDD persistence?,Reynold Xin <rxin@databricks.com>,"I've replied on the issue's page, but in a word, ""yes"". See https://issues.apache.org/jira/browse/SPARK-17204 <https://issues.apache.org/jira/browse/SPARK-17204>.

Michael


https://issues.apache.org/jira/browse/SPARK-17204 <https://issues.apache.org/jira/browse/SPARK-17204>
<mailto:michael@videoamp.com>>
persistence?
several hours when one of the executors would segfault. That problem aside, I speculated that her job would be more robust against these kinds of executor crashes if she used replicated RDD storage. She's using off heap storage (for good reason), so I asked her to try running her job with the following storage level: `StorageLevel(useDisk = true, useMemory = true, useOffHeap = true, deserialized = false, replication = 2)`. The job would immediately fail with a rather suspicious looking exception. For example:
class ID: 9086
com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)
com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:834)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:788)
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:141)
org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:140)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
header: 780000D0
java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:146)
org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:433)
org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:672)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
had a problem like this using off heap storage with replication factor 2?

"
kant kodali <kanth909@gmail.com>,"Wed, 24 Aug 2016 01:00:55 +0000",is the Lineage of RDD stored as a byte code in memory or a file?,dev@spark.apache.org,"Hi Guys,
I have this question for a very long time and after diving into the source
code(specifically from the links below) I have a feeling that the lineage of an
RDD (the transformations) are converted into byte code and stored in memory or
disk. or if I were to ask another question on a similar note do we ever store
JVM byte code or python byte code in memory or disk? This make sense to me
because if we were to construct an RDD after a node failure we need to go
through the lineage and execute the respective transformations so storing their
byte codes does make sense however many people seem to disagree with me so it
would be great if someone can clarify.

https://github.com/apache/spark/blob/6ee40d2cc5f467c78be662c1639fc3d5b7f796cf/python/pyspark/rdd.py#L1452

https://github.com/apache/spark/blob/6ee40d2cc5f467c78be662c1639fc3d5b7f796cf/python/pyspark/rdd.py#L1471

https://github.com/apache/spark/blob/6ee40d2cc5f467c78be662c1639fc3d5b7f796cf/python/pyspark/rdd.py#L229
https://github.com/apache/spark/blob/master/python/pyspark/cloudpickle.py#L241"
Nishadi Kirielle <ndimeshi@gmail.com>,"Wed, 24 Aug 2016 11:40:54 +0530",Re: Spark dev-setup,Akhil Das <akhld@hacked.work>,"Hi,
I'm engaged in learning how query execution flow occurs in Spark SQL. In
order to understand the query execution flow, I'm attempting to run an
example in debug mode with intellij IDEA. It would be great if anyone can
help me with debug configurations.

Thanks & Regards
Nishadi


"
Sean Owen <sowen@cloudera.com>,"Wed, 24 Aug 2016 08:41:30 +0100",Re: is the Lineage of RDD stored as a byte code in memory or a file?,kant kodali <kanth909@gmail.com>,"Byte code, no. It's sufficient to store the information that the RDD
represents, which can include serialized function closures, but that's not
quite storing byte code.


"
kant kodali <kanth909@gmail.com>,"Wed, 24 Aug 2016 08:20:34 +0000","Re: is the Lineage of RDD stored as a byte code in memory or a
 file?",Sean Owen <sowen@cloudera.com>,"can you please elaborate a bit more?





Byte code, no. It's sufficient to store the information that the RDD represents,
which can include serialized function closures, but that's not quite storing
byte code.
Hi Guys,
I have this question for a very long time and after diving into the source
code(specifically from the links below) I have a feeling that the lineage of an
RDD (the transformations) are converted into byte code and stored in memory or
disk. or if I were to ask another question on a similar note do we ever store
JVM byte code or python byte code in memory or disk? This make sense to me
because if we were to construct an RDD after a node failure we need to go
through the lineage and execute the respective transformations so storing their
byte codes does make sense however many people seem to disagree with me so it
would be great if someone can clarify.
https://github.com/apache/ spark/blob/ 6ee40d2cc5f467c78be662c1639fc3
d5b7f796cf/python/pyspark/rdd. py#L1452
https://github.com/apache/ spark/blob/ 6ee40d2cc5f467c78be662c1639fc3
d5b7f796cf/python/pyspark/rdd. py#L1471
https://github.com/apache/ spark/blob/ 6ee40d2cc5f467c78be662c1639fc3
d5b7f796cf/python/pyspark/rdd. py#L229
https://github.com/apache/ spark/blob/master/python/ pyspark/cloudpickle.py#L241"
Sun Rui <sunrise_win@163.com>,"Wed, 24 Aug 2016 16:45:53 +0800",spread out of executors with Spark on Mesos,"mgummelt@mesosphere.io,
 ""dev.spark"" <dev@spark.apache.org>","Hi, Michael,

The MesosCoarseGrainedSchedulerBackend launch executors in a round-robin way among accepted offers that are received at once. I thought that the behaviour is similar to ‚Äúspreadout‚Äù mode in the standalone cluster deployment and expected that executors are evenly distributed among all Mesos slaves. But that is not the case. It is observed that typically executors are launched on a small number of slaves.

After looking at the logs, it is found that MesosCoarseGrainedSchedulerBackend mostly is receiving only one offer once on a cluster composed of tens of nodes.

So the round-robin assignment of executors among offers do not have expected result because the Mesos core seems  only pass a very small subset of all available resources on the cluster each time. This leads to the fact that executors are located on a small number of slave nodes than expected, which suffers bad data locality.

The resource offer behaviour may be related to the paper Dominant Resource Fairness: Fair Allocation of Multiple Resource Types¬† <https://people.eecs.berkeley.edu/~alig/papers/drf.pdf>, but I don‚Äôt read it deeply.

dedicated for Spark. But not sure.

I did an experiment by a slight change to MesosCoarseGrainedSchedulerBackend::buildMesosTasks(). That is, only one round of executor assignment is performed to the accepted offers each time. No more round of assignments and the remaining resources are released. The code change like follows:
  private def buildMesosTasks(offers: mutable.Buffer[Offer]): Map[OfferID, List[MesosTaskInfo]] = {
    ...
    // round-robin create executors on the available offers
    while (launchTasks) {
      launchTasks = false
      for (offer <- offers) {
       ...
       }
+      if (conf.getBoolean(""spark.deploy.spreadOut"", true)) {
+        launchTasks = false
+      }
     }
     tasks.toMap
   }

After the change, the job time for a spark program reduces ~30%.  (5.3 hours -> 3.8 hours). Due to better data locality.

before:


After:


Do you think such change makes senses and can be a patch to the upstream?

Rui

"
Steve Loughran <stevel@hortonworks.com>,"Wed, 24 Aug 2016 09:13:40 +0000",Re: Spark dev-setup,,"

Hi,
I'm engaged in learning how query execution flow occurs in Spark SQL. In order to understand the query execution flow, I'm attempting to run an example in debug mode with intellij IDEA. It would be great if anyone can help me with debug configurations.

I'd recommend

-check out the version of spark you want to use
-set breakpoints wherever you want
-use dev/make-distribution.sh to build the release in dist/
-stark spark standalone from there
-attach to it in the IDE debugger

submit the work/type in queries in the REPL

this gives you the full launch with the complete classpath and env setup.

Otherwise: pull it out into a junit test and try to use IDEAs test runner to run it.
"
Jacek Laskowski <jacek@japila.pl>,"Wed, 24 Aug 2016 12:38:03 +0200",Re: Spark dev-setup,Steve Loughran <stevel@hortonworks.com>,"

...which I mostly agree to with some exceptions :)


Why spark standalone since the OP asked about ""learning how query
execution flow occurs in Spark SQL""? How about spark-shell in local
mode? Possibly explain(true) + conf/log4j.properties as the code might
get tricky to get right at the very beginning.

#justcurious

Jacek

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Wed, 24 Aug 2016 12:32:05 +0000",Re: Spark dev-setup,Jacek Laskowski <jacek@japila.pl>,"


no reason; the key thing is : not in cluster mode, as there your work happens elsewhere


---------------------------------------------------------------------


"
Daniel Darabos <daniel.darabos@lynxanalytics.com>,"Wed, 24 Aug 2016 15:02:03 +0200",Re: is the Lineage of RDD stored as a byte code in memory or a file?,kant kodali <kanth909@gmail.com>,"You are saying the RDD lineage must be serialized, otherwise we could not
recreate it after a node failure. This is false. The RDD lineage is not
serialized. It is only relevant to the driver application and as such it is
just kept in memory in the driver application. If the driver application
stops, the lineage is lost. There is no recovery.


"
Jacek Laskowski <jacek@japila.pl>,"Wed, 24 Aug 2016 15:35:39 +0200",Re: Spark dev-setup,Steve Loughran <stevel@hortonworks.com>,"

Right! Anything but cluster mode should make it easy (that leaves us
with local).

Jacek

---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Wed, 24 Aug 2016 14:28:36 +0000 (UTC)","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability","Michael Armbrust <michael@databricks.com>, 
	Marcelo Vanzin <vanzin@cloudera.com>","ping, did this discussion conclude or did we decide what we are doing?
Tom 

 

 +1 to the general structure of Reynold's proposal.¬† I've found what we do currently a little confusing.¬† In particular, it doesn't make much sense that @DeveloperApi things are always labeled as possibly changing.¬† For example the Data Source API should arguably be one of the most stable interfaces since its very difficult for users to recompile libraries that might break when there are changes.
For a similar reason, I don't really see the point of¬†LimitedPrivate.¬† The goal here should be communication of promises of stability or future stability.
Regarding Developer vs. Public. I don't care too much about the naming, but it does seem useful to differentiate APIs that we expect end users to consume from those that are used to augment Spark.¬†""Library"" and ""Application"" also seem reasonable.
e:


I think ""LimitedPrivate"" is a rather confusing name for that. I think
Reynold's first e-mail better matches that use case: this would be
""InterfaceAudience(Developer)"" and ""InterfaceStability(Experimental)"".

But I don't really like ""Developer"" as a name here, because it's
ambiguous. Developer of what? Theoretically everybody writing Spark or
on top of its APIs is a developer. In that sense, I prefer using
something like ""Library"" and ""Application"" instead of ""Developer"" and
""Public"".

Personally, in fact, I don't see a lot of gain in differentiating
between the target users of an interface... knowing whether it's a
stable interface or not is a lot more useful. If you're equating a
""developer API"" with ""it's not really stable"", then you don't really
need two annotations for that - just say it's not stable.

--
Marcelo

---------------------------------------------------------------------





  "
Michael Allman <michael@videoamp.com>,"Wed, 24 Aug 2016 09:36:03 -0700",Re: Anyone else having trouble with replicated off heap RDD persistence?,Reynold Xin <rxin@databricks.com>,"FYI, I've updated the issue's description to include a very simple program which reproduces the issue for me.

Cheers,

Michael

https://issues.apache.org/jira/browse/SPARK-17204 <https://issues.apache.org/jira/browse/SPARK-17204>.
https://issues.apache.org/jira/browse/SPARK-17204 <https://issues.apache.org/jira/browse/SPARK-17204>
<mailto:michael@videoamp.com>>
persistence?
several hours when one of the executors would segfault. That problem aside, I speculated that her job would be more robust against these kinds of executor crashes if she used replicated RDD storage. She's using off heap storage (for good reason), so I asked her to try running her job with the following storage level: `StorageLevel(useDisk = true, useMemory = true, useOffHeap = true, deserialized = false, replication = 2)`. The job would immediately fail with a rather suspicious looking exception. For example:
class ID: 9086
com.esotericsoftware.kryo.util.DefaultClassResolver.readClass(DefaultClassResolver.java:137)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:781)
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
com.esotericsoftware.kryo.util.MapReferenceResolver.getReadObject(MapReferenceResolver.java:60)
com.esotericsoftware.kryo.Kryo.readReferenceOrNull(Kryo.java:834)
com.esotericsoftware.kryo.Kryo.readClassAndObject(Kryo.java:788)
org.apache.spark.serializer.KryoDeserializationStream.readObject(KryoSerializer.scala:229)
org.apache.spark.serializer.DeserializationStream$$anon$1.getNext(Serializer.scala:169)
org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)
org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:32)
org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:39)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:461)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:141)
org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$doExecute$1$$anonfun$6.apply(InMemoryTableScanExec.scala:140)
scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:463)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator.hasNext(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithoutKey$(Unknown Source)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:125)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
exception:
header: 780000D0
java.io.ObjectInputStream.readStreamHeader(ObjectInputStream.java:808)
org.apache.spark.serializer.JavaDeserializationStream$$anon$1.<init>(JavaSerializer.scala:63)
org.apache.spark.serializer.JavaDeserializationStream.<init>(JavaSerializer.scala:63)
org.apache.spark.serializer.JavaSerializerInstance.deserializeStream(JavaSerializer.scala:122)
org.apache.spark.serializer.SerializerManager.dataDeserializeStream(SerializerManager.scala:146)
org.apache.spark.storage.BlockManager.getLocalValues(BlockManager.scala:433)
org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:672)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
had a problem like this using off heap storage with replication factor 2?

"
Michael Heuer <heuermh@gmail.com>,"Wed, 24 Aug 2016 11:41:26 -0500",Spark 1.x/2.x qualifiers in downstream artifact names,dev@spark.apache.org,"Hello,

We're a project downstream of Spark and need to provide separate artifacts
for Spark 1.x and Spark 2.x.  Has any convention been established or even
proposed for artifact names and/or qualifiers?

We are currently thinking

org.bdgenomics.adam:adam-{core,apis,cli}_2.1[0,1]  for Spark 1.x and Scala
2.10 & 2.11

  and

org.bdgenomics.adam:adam-{core,apis,cli}-spark2_2.1[0,1]  for Spark 1.x and
Scala 2.10 & 2.11

https://github.com/bigdatagenomics/adam/issues/1093


Thanks in advance,

   michael
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 24 Aug 2016 09:44:38 -0700",Re: Spark 1.x/2.x qualifiers in downstream artifact names,Michael Heuer <heuermh@gmail.com>,"'Spark 1.x and Scala 2.10 & 2.11' was repeated.

I guess your second line should read:

org.bdgenomics.adam:adam-{core,apis,cli}-spark2_2.1[0,1]  for Spark 2.x and
Scala 2.10 & 2.11


"
Michael Heuer <heuermh@gmail.com>,"Wed, 24 Aug 2016 11:45:29 -0500",Re: Spark 1.x/2.x qualifiers in downstream artifact names,Ted Yu <yuzhihong@gmail.com>,"Ah yes, thank you for the clarification.


"
Reynold Xin <rxin@databricks.com>,"Wed, 24 Aug 2016 09:48:58 -0700","Re: [discuss] separate API annotation into two components:
 InterfaceAudience & InterfaceStability",Tom Graves <tgraves_cs@yahoo.com>,"Looks like I'm general people like it. Next step is for somebody to take
the lead and implement it.

Tom do you have cycles to do this?


"
Sean Owen <sowen@cloudera.com>,"Wed, 24 Aug 2016 17:49:18 +0100",Re: Spark 1.x/2.x qualifiers in downstream artifact names,Michael Heuer <heuermh@gmail.com>,"This is also what ""classifiers"" are for in Maven, to have variations
on one artifact and version. https://maven.apache.org/pom.html

It has been used to ship code for Hadoop 1 vs 2 APIs.

In a way it's the same idea as Scala's ""_2.xx"" naming convention, with
a less unfortunate implementation.



---------------------------------------------------------------------


"
Michael Heuer <heuermh@gmail.com>,"Wed, 24 Aug 2016 12:02:43 -0500",Re: Spark 1.x/2.x qualifiers in downstream artifact names,Sean Owen <sowen@cloudera.com>,"Have you seen any successful applications of this for Spark 1.x/2.x?

built from the same POM but differ in their content.""

We'd be building from different POMs, since we'd be modifying the Spark
dependency version (and presumably any other dependencies that needed the
same Spark 1.x/2.x distinction).



"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 24 Aug 2016 19:11:01 +0200",Re: GraphFrames 0.2.0 released,Jacek Laskowski <jacek@japila.pl>,"Hi,
Do you plan to add tag for this release on github ?
https://github.com/graphframes/graphframes/releases

Regards,
Maciek

2016-08-17 3:18 GMT+02:00 Jacek Laskowski <jacek@japila.pl>:

la
.


-- 
Maciek Bry≈Ñski
"
Sean Owen <sowen@cloudera.com>,"Wed, 24 Aug 2016 18:12:52 +0100",Re: Spark 1.x/2.x qualifiers in downstream artifact names,Michael Heuer <heuermh@gmail.com>,"If you're just varying versions (or things that can be controlled by a
profile, which is most everything including dependencies), you don't
need and probably don't want multiple POM files. Even that wouldn't
mean you can't use classifiers.

I have seen it used for HBase, core Hadoop. I am not sure I've seen it
used for Spark 2 vs 1 but no reason it couldn't be. Frequently
projects would instead declare that as of some version, Spark 2 is
required, rather than support both. Or shim over an API difference
with reflection if that's all there was to it. Spark does both of
those sorts of things itself to avoid having to publish multiple
variants at all. (Well, except for Scala 2.10 vs 2.11!)


---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 24 Aug 2016 21:31:17 +0200",Tree for SQL Query,Spark dev list <dev@spark.apache.org>,"Hi,
I read this article:
https://databricks.com/blog/2015/04/13/deep-dive-into-spark-sqls-catalyst-optimizer.html

And I have a question. Is it possible to get / print Tree for SQL Query ?

Something like this:

Add(Attribute(x), Add(Literal(1), Literal(2)))

Regards,
-- 
Maciek Bry≈Ñski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 24 Aug 2016 13:39:30 -0700",Re: Tree for SQL Query,=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"It's basically the output of the explain command.


rote:

"
kant kodali <kanth909@gmail.com>,"Wed, 24 Aug 2016 20:49:43 +0000",quick question,"dev@spark.apache.org, users@spark.apache.org","In this picture what does ""Dashboards"" really mean? is there a open source
project which can allow me to push the results back to Dashboards such that
Dashboards are always in sync with real time updates? (a push based solution is
better than poll but i am open to whatever is possible given the above picture)"
Matt Smith <matt.smith123@gmail.com>,"Thu, 25 Aug 2016 01:01:38 +0000",StateStore with DStreams,dev@spark.apache.org,"Are there any examples of how to use StateStore with DStreams?  It seems
like the idea would be to create a new version with each minibatch, but I
don't quite know how to make that happen.  My lame attempt is below.

  def run (ss: SparkSession): Unit = {
    val c = new StreamingContext(ss.sparkContext, Seconds(2))
    val stm = c.socketTextStream(""localhost"", 9999)

    var version = 0L
    stm.foreachRDD { (rdd, t) =>
      val data = rdd
      .map { (s) =>
        val Array(k, v) = s.split("" "")
        (k, v)
      }
      .mapPartitionsWithStateStore(ss.sqlContext, ""/Users/msmith/cp"", 1,
version, keySchema, valueSchema) { (store, rows) =>
        val data = rows.map { case (k,v) =>
          val keyRow = InternalRow(UTF8String.fromString(k))
          val keyURow = UnsafeProjection.create(keySchema).apply(keyRow)

          val newCount = v.toLong
          val count = store.get(keyURow).map(_.getLong(0)).getOrElse(0L)
          val valRow = InternalRow(count + newCount)
          val valURow = UnsafeProjection.create(valueSchema).apply(valRow)
          store.put(keyURow, valURow)

          val ret = (k, count + newCount)
          println(""ret"", ret)
          ret
        }
        lazy val finish = Some(("""",0)).flatMap{ case(k,v) =>
            println(""finish"")
            version = store.commit()
            println(""commit"", version)
            None
        }

        data ++ finish
      }

      println(data.collectAsMap())

    }

    c.start()             // Start the computation
    c.awaitTermination()  // Wait for the computation to terminate

  }
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 25 Aug 2016 01:31:29 +0000",Spark streaming get RDD within the sliding window,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I am working with Spark streaming 1.6.1. The task is to get RDDs for some external analytics from each timewindow. This external function accepts RDD so I cannot use DStream. I learned that DStream.window.compute(time) returns Option[RDD]. I am trying to use it in the following code derived from the example in programming guide:
val conf = new SparkConf().setMaster(""local[2]"").setAppName(""NetworkWordCount"")
val ssc = new StreamingContext(conf, Seconds(1))

val lines = ssc.socketTextStream(""localhost"", 9999)

val rdd = lines.window(Seconds(5), Seconds(3)).compute(Time(System.currentTimeMillis())) // this does not seem to be a proper way to set time

ssc.start()

ssc.awaitTermination()

At the line with rdd I get the following exception: Exception in thread ""main"" org.apache.spark.SparkException: org.apache.spark.streaming.dstream.SocketInputDStream@2264e43c has not been initialized.

The other option to get RDD from DStream is to use ""slice"" function. However, it is not clear how to use it and I get the same exception with the following use:

val rdd = lines.slice(Time(System.currentTimeMillis() - 100), Time(System.currentTimeMillis())) // does not seem correct

Could you suggest what is the proper use of ""compute"" or ""slice"" functions from DStream or another way to get RDD from DStream?

Best regards, Alexander

P.S. I have found the following example that does streaming within the loop, however it looks hacky:
https://github.com/chlam4/spark-exercises/blob/master/using-dstream-slice.scala
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Thu, 25 Aug 2016 08:23:08 +0200",Re: Tree for SQL Query,Reynold Xin <rxin@databricks.com>,"@rxin
It's not I'm looking for.
Explain prints output like this.
== Physical Plan ==
*Project [id#1576L AS id#1582L]
+- *Range (0, 1000, splits=400)

I'd like to have whole tree with expressions.

So when I have ""select x + y"" there should by Add expresio"
Reynold Xin <rxin@databricks.com>,"Wed, 24 Aug 2016 23:26:25 -0700",Re: Tree for SQL Query,=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"scala> sql(""select rand() +
1"").queryExecution.analyzed(0).expressions.head.treeString
res11: String =
""(rand(3296762545665822882) + cast(1 as double)) AS
(rand(3296762545665822882) + CAST(1 AS DOUBLE))#29
+- (rand(3296762545665822882) + cast(1 as double))
   :- rand(3296762545665822882)
   +- cast(1 as double)
      +- 1
""


rote:

l>
y
"
=?UTF-8?B?QWJlbCBSaW5jw7Nu?= <ganger85@gmail.com>,"Thu, 25 Aug 2016 12:10:30 +0200",Spark Kerberos proxy user,dev@spark.apache.org,"Hi devs,

I'm working (at Stratio)  on use spark over mesos and standalone, with a
kerberized HDFS

We are working to solve these scenarios,


   - We have an long term running spark sql context using concurrently by
   many users like Thrift server called CrossData, we need access to hdfs data
   with kerberos authorization using proxy-user method. we trust on HDFS
   permission system, or our custom authorizer.


   - We need load/write dataframes using datasources with HDFS
   backend(built-in, or others)  such json, csv, parquet, orc ‚Ä¶, so we want to
   enable the secure access (krb)  only by configuration.


   - We have an scenario where we want to run streaming jobs over
   kerberized HDFS,  from W/R and  checkpointing too.


   - We have to load every single RDD that spark core over kerberized HDFS
   without breaking the Spark API.




As you can see, We have a ""special"" requirement need to set the proxy user
by job over the same spark context.

Do you have any idea to cover it?
"
Dibyendu Bhattacharya <dibyendu.bhattachary@gmail.com>,"Thu, 25 Aug 2016 17:03:41 +0530",Latest Release of Receiver based Kafka Consumer for Spark Streaming.,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi ,

Released latest version of Receiver based Kafka Consumer for Spark
Streaming.

Receiver is compatible with Kafka versions 0.8.x, 0.9.x and 0.10.x and All
Spark Versions

Available at Spark Packages :
https://spark-packages.org/package/dibbhatt/kafka-spark-consumer

Also at github  : https://github.com/dibbhatt/kafka-spark-consumer

Salient Features :

   - End to End No Data Loss without Write Ahead Log
   - ZK Based offset management for both consumed and processed offset
   - No dependency on WAL and Checkpoint
   - In-built PID Controller for Rate Limiting and Backpressure management
   - Custom Message Interceptor

Please refer to
https://github.com/dibbhatt/kafka-spark-consumer/blob/master/README.md for
more details


Regards,

Dibyendu
"
Nishadi Kirielle <ndimeshi@gmail.com>,"Thu, 25 Aug 2016 19:59:56 +0530",Re: Spark dev-setup,Jacek Laskowski <jacek@japila.pl>,"Thanks a lot for the guidelines.
I could successfully configure and debug




"
wgtmac <ustcwg@gmail.com>,"Thu, 25 Aug 2016 13:47:12 -0700 (MST)","Spark 2.0 history server summary page gets stuck at ""loading
 history summary"" with 10K+ application history",dev@spark.apache.org,"The summary page of Spark 2.0 history server web UI keep displaying ""Loading
history summary..."" all the time and crashes the browser when there are more
than 10K application history event logs on HDFS.

I did some investigation, ""historypage.js"" file sends a REST request to
/api/v1/applications endpoint of history server REST endpoint and gets back
json response. When there are more than 10K applications inside the event
log directory it takes forever to parse them and render the page. When there
are only hundreds or thousands of application history it is running fine.

I've created a JIRA here: https://issues.apache.org/jira/browse/SPARK-17243



--

---------------------------------------------------------------------


"
Srikanth Sampath <ssampath.apache@gmail.com>,"Fri, 26 Aug 2016 19:29:19 +0530",Assembly build on spark 2.0.0,dev@spark.apache.org,"Hi,
mvn assembly is creating a .tgz distribution.  How can I create a plain jar
archive?  I would like to create a spark-assembly-<version>.jar
-Srikanth
"
Russell Spitzer <russell.spitzer@gmail.com>,"Fri, 26 Aug 2016 15:38:09 +0000",Re: Insert non-null values from dataframe,"Selvam Raman <selmna@gmail.com>, user <user@spark.apache.org>, dev <dev@spark.apache.org>","Cassandra does not differentiate between null and empty, so when reading
from C* all empty values are reported as null. To avoid inserting nulls
(avoiding tombstones) see

https://github.com/datastax/spark-cassandra-connector/blob/master/doc/5_saving.md#globally-treating-all-nulls-as-unset

This will not prevent those columns from being read as null though, it will
only skip writing tombstones.


es
µ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ ‡Æ®‡ØÜ‡Æû‡Øç‡Æö‡ÆÆ‡Øç ‡Æ®‡Æø‡ÆÆ‡Æø‡Æ∞‡Øç‡Æ§‡Øç‡Æ§‡ØÅ""
"
Radoslaw Gruchalski <radek@gruchalski.com>,"Fri, 26 Aug 2016 15:32:49 -0400",Re: Assembly build on spark 2.0.0,"dev@spark.apache.org, Srikanth Sampath <ssampath.apache@gmail.com>","mvn package might be the command you‚Äôre looking for.

‚Äì
Best regards,
Radek Gruchalski
radek@gruchalski.com



Hi,
mvn assembly is creating a .tgz distribution.  How can I create a plain jar
archive?  I would like to create a spark-assembly-<version>.jar
-Srikanth
"
Michael Gummelt <mgummelt@mesosphere.io>,"Fri, 26 Aug 2016 13:20:33 -0700",Mesos is now a maven module,dev <dev@spark.apache.org>,"Hello devs,

Much like YARN, Mesos has been refactored into a Maven module.  So when
building, you must add ""-Pmesos"" to enable Mesos support.

The pre-built distributions from Apache will continue to enable Mesos.

PR: https://github.com/apache/spark/pull/14637

Cheers

-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Reynold Xin <rxin@databricks.com>,"Fri, 26 Aug 2016 13:23:36 -0700",Re: Mesos is now a maven module,Michael Gummelt <mgummelt@mesosphere.io>,"This is great!



"
Jacek Laskowski <jacek@japila.pl>,"Fri, 26 Aug 2016 23:14:41 +0200",Re: Mesos is now a maven module,Michael Gummelt <mgummelt@mesosphere.io>,"Hi Michael,

Congrats!

BTW What I like about the change the most is that it uses the
pluggable interface for TaskScheduler and SchedulerBackend (as
introduced by YARN). Think Standalone should follow the steps. WDYT?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Fri, 26 Aug 2016 15:19:52 -0700",Re: Mesos is now a maven module,Jacek Laskowski <jacek@japila.pl>,"If it's separable, then sure.  Consistency is nice.





-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Joseph Bradley <joseph@databricks.com>,"Fri, 26 Aug 2016 17:10:05 -0700",Re: GraphFrames 0.2.0 released,=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"This should do it:
https://github.com/graphframes/graphframes/releases/tag/release-0.2.0
Thanks for the reminder!
Joseph

rote:

.
e
"
Srikanth Sampath <ssampath.apache@gmail.com>,"Sat, 27 Aug 2016 08:54:24 +0530",Re: Assembly build on spark 2.0.0,Radoslaw Gruchalski <radek@gruchalski.com>,"Hi,
Thanks Radek.  However mvn package does not build the uber jar.  I am
looking for an uber jar and not a distribution.  I have seen references to
the uber jar here
<http://www.cloudera.com/documentation/enterprise/5-5-x/topics/cdh_ig_running_spark_on_yarn.html>

What I see in the spark 2.0 codeline (assembly/pom.xml) builds a
distribution. I

    <profile>

      <id>bigtop-dist</id>

      <!-- This profile uses the assembly plugin to create a special ""dist""
package for BigTop

           that contains Spark but not the Hadoop JARs it depends on. -->

      <build>

        <plugins>

          <plugin>

            <groupId>org.apache.maven.plugins</groupId>

            <artifactId>maven-assembly-plugin</artifactId>

            <executions>

              <execution>

                <id>dist</id>

                <phase>package</phase>

                <goals>

                  <goal>single</goal>

                </goals>

                <configuration>

                  <descriptors>

                    <descriptor>src/main/assembly/assembly.xml</descriptor>

                  </descriptors>

                </configuration>

              </execution>

            </executions>

          </plugin>

...        </plugins>

      </build>

    </profile>


In src/main/assembly/assembly.xml we see

<assembly>

  <id>dist</id>

  <formats>

    <format>tar.gz</format>

    <format>dir</format>

  </formats>

  <includeBaseDirectory>false</includeBaseDirectory>

.....




"
Srikanth Sampath <ssampath.apache@gmail.com>,"Sat, 27 Aug 2016 15:38:40 +0530",Re: Assembly build on spark 2.0.0,Radoslaw Gruchalski <radek@gruchalski.com>,"Found the answer.  This is the reason
https://issues.apache.org/jira/browse/SPARK-11157

-Srikanth

m

o
ning_spark_on_yarn.html>
m
"
Radoslaw Gruchalski <radek@gruchalski.com>,"Sat, 27 Aug 2016 11:28:00 +0000 (UTC)",Re: Assembly build on spark 2.0.0,Srikanth Sampath <ssampath.apache@gmail.com>,"Ah, an uberjar. Normally one would build the uberjar with a Maven Shade plugin. Haven't looked into Spark code much recently, it wouldn't make much sense having a separate maven command to build an uberjar while building a distribution because, from memory, if you open the tgz file, the uberjar sits in the lib directory.
-- 
Best regards,
Rad

		_____________________________
From: Srikanth Sampath <ssampath.apache@gmail.com>
Sent: Saturday, August 27, 2016 5:24 am
Subject: Re: Assembly build on spark 2.0.0
To: Radoslaw Gruchalski <radek@gruchalski.com>
Cc:  <dev@spark.apache.org>


Hi,Thanks Radek.¬† However mvn package does not build the uber jar.¬† I am looking for an uber jar and not a distribution.¬† I have seen references to the uber jar here¬†What I see in the spark 2.0 codeline (assembly/pom.xml) builds a distribution. I

¬† ¬†¬†<profile>

¬† ¬† ¬† <id>bigtop-dist</id>

¬† ¬† ¬† <!-- This profile uses the assembly plugin to create a special ""dist"" package for BigTop

¬†¬† ¬† ¬† ¬† ¬† that contains Spark but not the Hadoop JARs it depends on. -->

¬† ¬† ¬† <build>

¬† ¬† ¬† ¬† <plugins>

¬† ¬† ¬† ¬† ¬† <plugin>

¬† ¬† ¬† ¬† ¬† ¬† <groupId>org.apache.maven.plugins</groupId>

¬† ¬† ¬† ¬† ¬† ¬† <artifactId>maven-assembly-plugin</artifactId>

¬† ¬† ¬† ¬† ¬† ¬† <executions>

¬† ¬† ¬† ¬† ¬† ¬† ¬† <execution>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <id>dist</id>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <phase>package</phase>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <goals>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <goal>single</goal>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† </goals>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <configuration>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <descriptors
¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† <descriptor>src/main/assembly/assembly.xml</descriptor>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† </descriptors>

¬† ¬† ¬† ¬† ¬† ¬† ¬† ¬† </configuration>

¬† ¬† ¬† ¬† ¬† ¬† ¬† </execution>

¬† ¬† ¬† ¬† ¬† ¬† </executions>

¬† ¬† ¬† ¬† ¬† </plugin>

... ¬† ¬† ¬† ¬†</plugins>

¬† ¬† ¬† </build>

¬† ¬† </profile>




In src/main/assembly/assembly.xml we see

<assembly>

¬† <id>dist</id>

¬† <formats>

¬† ¬† <format>tar.gz</format>

¬† ¬† <format>dir</format>

¬† </formats>

¬† <includeBaseDirectory>false</includeBaseDirectory>



.....



mvn package might be the command you‚Äôre looking for. 						

						

‚Äì 
Best regards,
RadekGruchalski
radek@gruchalski.com

 


create a plain jar archive?¬† I would like to create aspark-assembly-<version>.jar-Srikanth




	"
Julien Dumazert <julien.dumazert@gmail.com>,"Sat, 27 Aug 2016 15:27:12 +0200",Performance of loading parquet files into case classes in Spark,dev@spark.apache.org,"Hi all,

I'm forwarding a question <http://stackoverflow.com/questions/39125911/performance-of-loading-parquet-files-into-case-classes-in-spark> I recently asked on Stack Overflow about benchmarking Spark performance when working with case classes stored in Parquet files. 

I am assessing the performance of different ways of loading Parquet files in Spark and the differences are staggering.

In our Parquet files, we have nested case classes of the type:

case class C(/* a dozen of attributes*/)
case class B(/* a dozen of attributes*/, cs: Seq[C])
case class A(/* a dozen of attributes*/, bs: Seq[B])
It takes a while to load them from Parquet files. So I've done a benchmark of different ways of loading case classes from Parquet files and summing a field using Spark 1.6 and 2.0.

Here is a summary of the benchmark I did:

val df: DataFrame = sqlContext.read.parquet(""path/to/file.gz.parquet"").persist()
df.count()

// Spark 1.6

// Play Json
// 63.169s
df.toJSON.flatMap(s => Try(Json.parse(s).as[A]).toOption)
         .map(_.fieldToSum).sum()

// Direct access to field using Spark Row
// 2.811s
df.map(row => row.getAs[Long](""fieldToSum"")).sum()

// Some small library we developed that access fields using Spark Row
// 10.401s
df.toRDD[A].map(_.fieldToSum).sum()

// Dataframe hybrid SQL API
// 0.239s
df.agg(sum(""fieldToSum"")).collect().head.getAs[Long](0)

// Dataset with RDD-style code
// 34.223s
df.as[A].map(_.fieldToSum).reduce(_ + _)

// Dataset with column selection
// 0.176s
df.as[A].select($""fieldToSum"".as[Long]).reduce(_ + _)


// Spark 2.0

// Performance is similar except for:

// Direct access to field using Spark Row
// 23.168s
df.map(row => row.getAs[Long](""fieldToSum"")).reduce(_ + _)

// Some small library we developed that access fields using Spark Row
// 32.898s
f1DF.toRDD[A].map(_.fieldToSum).sum()
I understand why the performance of methods using Spark Row is degraded when upgrading to Spark 2.0, since Dataframe is now a mere alias of Dataset[Row]. That's the cost of unifying the interfaces, I guess.

not kept: performance when using RDD-style coding (maps and flatMaps) is worse than when using Dataset like Dataframe with SQL-like DSL.

Basically, to have good performance, we need to give up type safety.

What is the reason for such difference between Dataset used as RDD and Dataset used as Dataframe?
Is there a way to improve encoding performance in Dataset to equate RDD-style coding and SQL-style coding performance? For data engineering, it's much cleaner to have RDD-style coding.
Also, working with the SQL-like DSL would require to flatten our data model and not use nested case classes. Am I right that good performance is only achieved with flat data models?
Some more questions:

4. Is the performance regression between Spark 1.6 and Spark 2.0 an identified problem? Will it be addressed in future releases? Or is the performance regression very specific to my case and I should handle my data differently?

5. Is the performance difference between RDD-style coding and SQL-style coding with Dataset an identified problem? Will it be addressed in future releases? Maybe there's no way to do something about it for reasons I can't see with my limited understanding of Spark internals. Or should I migrate to the SQL-style interface, yet losing type safety?

Best regards,
Julien



"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Sat, 27 Aug 2016 22:32:40 +0200",Re: Performance of loading parquet files into case classes in Spark,Julien Dumazert <julien.dumazert@gmail.com>,"2016-08-27 15:27 GMT+02:00 Julien Dumazert <julien.dumazert@gmail.com>:



I think reduce and sum has very different performance.
Did you try sql.functions.sum ?
Or of you want to benchmark access to Row object then  count() function
will be better idea.

Regards,
-- 
Maciek Bry≈Ñski
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Sat, 27 Aug 2016 22:39:16 +0200",Cache'ing performance,Spark dev list <dev@spark.apache.org>,"Hi,
I did some benchmark of cache function today.

*RDD*
sc.parallelize(0 until Int.MaxValue).cache().count()

*Datasets*
spark.range(Int.MaxValue).cache().count()

For me Datasets was 2 times slower.

Results (3 nodes, 20 cores and 48GB RAM each)
*RDD - 6s*
*Datasets - 13,5 s*

Is that expected behavior for Datasets and Encoders ?

Regards,
-- 
Maciek Bry≈Ñski
"
Koert Kuipers <koert@tresata.com>,"Sat, 27 Aug 2016 17:32:12 -0400",Re: Structured Streaming with Kafka sources/sinks,Reynold Xin <rxin@databricks.com>,"thats great

is this effort happening anywhere that is publicly visible? github?


al with
s.
n
"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Sun, 28 Aug 2016 10:13:59 +0900",Re: Cache'ing performance,=?ISO-8859-2?Q?Maciej_Bry=F1ski?= <maciek@brynski.pl>,"Hi
I think that it is a performance issue in both DataFrame and Dataset 
cache. It is not due to only Encoders. The DataFrame version 
""spark.range(Int.MaxValue).toDF.cache().count()"" is also slow.

While a cache for DataFrame and Dataset is stored as a columnar format 
with some compressed data representation, we have revealed there is room 
to improve performance. We have already created pull requests to address 
them. These pull requests are under review. 
https://github.com/apache/spark/pull/11956
https://github.com/apache/spark/pull/14091

We would appreciate your feedback to these pull requests.

Best Regards,
Kazuaki Ishizaki



From:   Maciej BryÒski <maciek@brynski.pl>
To:     Spark dev list <dev@spark.apache.org>
Date:   2016/08/28 05:40
Subject:        Cache'ing performance



Hi,
I did some benchmark of cache function today.

RDD
sc.parallelize(0 until Int.MaxValue).cache().count()

Datasets
spark.range(Int.MaxValue).cache().count()

For me Datasets was 2 times slower.

Results (3 nodes, 20 cores and 48GB RAM each)
RDD - 6s
Datasets - 13,5 s

Is that expected behavior for Datasets and Encoders ?

Regards,
-- 
Maciek BryÒski


"
linguin.m.s@gmail.com,"Sun, 28 Aug 2016 11:29:07 +0900",Re: Cache'ing performance,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Hi,

How does the performance difference change when turning off compression?
It is enabled by default.

// maropu

Sent by iPhone

2016/08/28 10:13„ÄÅKazuaki Ishizaki <ISHIZAKI@jp.ibm.com> „ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏:

. It is not due to only Encoders. The DataFrame version ""spark.range(Int.MaxValue).toDF.cache().count()"" is also slow.
h some compressed data representation, we have revealed there is room to improve performance. We have already created pull requests to address them. These pull requests are under review. 
"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Sun, 28 Aug 2016 11:46:51 +0900",Re: Cache'ing performance,linguin.m.s@gmail.com,"Hi,
Good point. I have just measured performance with 
""spark.sql.inMemoryColumnarStorage.compressed=false.""
It improved the performance than with default. However, it is still slower 
RDD version on my environment.

It seems to be consistent with the PR 
https://github.com/apache/spark/pull/11956. This PR shows room to 
performance improvement for float/double values that are not compressed.

Kazuaki Ishizaki



From:   linguin.m.s@gmail.com
To:     Maciej Bry®Ωski <maciek@brynski.pl>
Cc:     Spark dev list <dev@spark.apache.org>
Date:   2016/08/28 11:30
Subject:        Re: Cache'ing performance



Hi,

How does the performance difference change when turning off compression?
It is enabled by default.

// maropu

Sent by iPhone

2016/08/28 10:13°¢Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com> §Œ•·•√•ª©`•∏:

Hi
I think that it is a performance issue in both DataFrame and Dataset 
cache. It is not due to only Encoders. The DataFrame version 
""spark.range(Int.MaxValue).toDF.cache().count()"" is also slow.

While a cache for DataFrame and Dataset is stored as a columnar format 
with some compressed data representation, we have revealed there is room 
to improve performance. We have already created pull requests to address 
them. These pull requests are under review. 
https://github.com/apache/spark/pull/11956
https://github.com/apache/spark/pull/14091

We would appreciate your feedback to these pull requests.

Best Regards,
Kazuaki Ishizaki



From:        Maciej Bry®Ωski <maciek@brynski.pl>
To:        Spark dev list <dev@spark.apache.org>
Date:        2016/08/28 05:40
Subject:        Cache'ing performance



Hi,
I did some benchmark of cache function today.

RDD
sc.parallelize(0 until Int.MaxValue).cache().count()

Datasets
spark.range(Int.MaxValue).cache().count()

For me Datasets was 2 times slower.

Results (3 nodes, 20 cores and 48GB RAM each)
RDD - 6s
Datasets - 13,5 s

Is that expected behavior for Datasets and Encoders ?

Regards,
-- 
Maciek Bry®Ωski



"
Srikanth Sampath <ssampath.apache@gmail.com>,"Sun, 28 Aug 2016 06:59:32 -0700 (MST)",Spark 2.0 and Yarn,dev@spark.apache.org,"Hi,
With SPARK-11157, the big fat assembly jar build was removed.

Has anyone used spark.yarn.archive - the alternative provided and
successfully deployed Spark on a Yarn cluster.  If so, what does the archive
contain.  What should be the minimal set.  Any suggestion is greatly
appreciated.

Thanks,
-Srikanth




--

---------------------------------------------------------------------


"
Julien Dumazert <julien.dumazert@gmail.com>,"Sun, 28 Aug 2016 21:27:30 +0200",Re: Performance of loading parquet files into case classes in Spark,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Hi Maciek,

I've tested several variants for summing ""fieldToSum"":

First, RDD-style code:
df.as[A].map(_.fieldToSum).reduce(_ + _)
df.as[A].rdd.map(_.fieldToSum).sum()
df.as[A].map(_.fieldToSum).rdd.sum()
All around 30 seconds. ""reduce"" and ""sum"" seem to have the same performance, for this use case at least.

Then with sql.functions.sum:
df.agg(sum(""fieldToSum"")).collect().head.getAs[Long](0)
0.24 seconds, super fast.

Finally, dataset with column selection:
df.as[A].select($""fieldToSum"".as[Long]).reduce(_ + _)
0.18 seconds, super fast again.

(I've also tried replacing my sums and reduces by counts on your advice, but the performance is unchanged. Apparently, summing does not take much time compared to accessing data.)

It seems that we need to use the SQL interface to reach the highest level of performance, which somehow breaks the promise of Dataset (preserving type safety and having Catalyst and Tungsten performance like datasets).

As for direct access to Row, it seems that it got much slower from 1.6 to 2.0. I guess, it's because of the fact that Dataframe is now Dataset[Row], and thus uses the same encoding/decoding mechanism as for any other case class.

Best regards,

Julien

<maciek@brynski.pl> a √©crit :
<mailto:julien.dumazert@gmail.com>>:
function will be better idea.

"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Sun, 28 Aug 2016 22:12:59 +0200",Re: Performance of loading parquet files into case classes in Spark,Julien Dumazert <julien.dumazert@gmail.com>,"Hi Julien,
I thought about something like this:

import org.apache.spark.sql.functions.sumdf.as[A].map(_.fieldToSum).agg(sum(""value"")).collect()

To try using Dataframes aggregation on Dataset instead of reduce.

Regards,
Maciek

2016-08-28 21:27 GMT+02:00 Julien Dumazert <julien.dumazert@gmail.com>:

,


-- 
Maciek Bry≈Ñski
"
Denis Bolshakov <bolshakov.denis@gmail.com>,"Mon, 29 Aug 2016 11:23:29 +0300",spark roadmap,dev@spark.apache.org,"Hello spark devs,

Does any one can provide a roadmap for the nearest two months?
Or at least say when we can expect 2.1 release and which features will be
added?


-- 
//with Best Regards
--Denis Bolshakov
e-mail: bolshakov.denis@gmail.com
"
Saisai Shao <sai.sai.shao@gmail.com>,"Mon, 29 Aug 2016 16:39:46 +0800",Re: Spark 2.0 and Yarn,Srikanth Sampath <ssampath.apache@gmail.com>,"This archive contains all the jars required by Spark runtime, you could zip
all the jars under <SPARK-HOME>/jars and upload this archive to HDFS, then
configure spark.yarn.archive with the path of this archive on HDFS.


"
Jerry Lam <chilinglam@gmail.com>,"Mon, 29 Aug 2016 11:30:07 -0400",Re: Broadcast Variable Life Cycle,Spark dev list <dev@spark.apache.org>,"Hello spark developers,

Anyone can shed some lights on the life cycle of the broadcast variables?
Basically, if I have a broadcast variable defined in a loop and for each
iteration, I provide a different value.
// For example:
for(i< 1 to 10) {
    val bc = sc.broadcast(i)
    sc.parallelize(Seq(1,2,3)).map{id => val i = bc.value; (id,
i)}.toDF(""id"", ""i"").write.parquet(""/dummy_output"")
}

Do I need to active manage the broadcast variable in this case? I know this
example is not real but please imagine this broadcast variable can hold an
array of 1M Long.

Regards,

Jerry




"
Artur Sukhenko <artur.sukhenko@gmail.com>,"Mon, 29 Aug 2016 19:06:04 +0300",Remaining folders in .sparkStaging directory after app was killed,dev@spark.apache.org,"Hello spark devs,

Whenever I  run spark app in yarn-cluster mode, do Ctrl+C to stop
spark-submit and yarn application -kill
I have remaining folders in hdfs:
.sparkStaging/application_1472140614688_0001
.sparkStaging/application_1472140614688_0002

Those folders will never be deleted?
And if so, the only way to clean them is to manually remove them from hdfs?


---
Sincerely,
Artur Sukhenko
"
Sean Owen <sowen@cloudera.com>,"Mon, 29 Aug 2016 17:11:58 +0100",Re: Broadcast Variable Life Cycle,Jerry Lam <chilinglam@gmail.com>,"Yes you want to actively unpersist() or destroy() broadcast variables
when they're no longer needed. They can eventually be removed when the
reference on the driver is garbage collected, but you usually would
not want to rely on that.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 29 Aug 2016 09:36:47 -0700","[build system] jenkins wedged itself this weekend, just restarted","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Andrew Audibert <andrew@alluxio.com>, Michael Armbrust <michael@databricks.com>","jenkins got in to one of it's ""states"" and wasn't accepting new builds
starting this past saturday night.  i restarted it, and now it's
catching up on the weekend's queue.

shane

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 29 Aug 2016 09:38:39 -0700",Re: spark roadmap,Denis Bolshakov <bolshakov.denis@gmail.com>,"At this point, there is no target date set for 2.1.  That's something that
we should do fairly soon, but right now there is at least a little room for
discussion as to whether we want to continue with the same pace of releases
that we targeted throughout the 1.x development cycles, or whether
lengthening the release cycles by a month or two might be better (mainly by
reducing the overhead fraction that comes from the constant-size
engineering mechanics of coordinating and making a release.)


"
shane knapp <sknapp@berkeley.edu>,"Mon, 29 Aug 2016 09:46:55 -0700","Re: [build system] jenkins wedged itself this weekend, just restarted","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>, 
	Andrew Audibert <andrew@alluxio.com>, Michael Armbrust <michael@databricks.com>","and now a screen grab of the ganglia system load metrics for your amusement:


---------------------------------------------------------------------"
Fred Reiss <freiss.oss@gmail.com>,"Mon, 29 Aug 2016 12:39:53 -0700",Re: Structured Streaming with Kafka sources/sinks,Koert Kuipers <koert@tresata.com>,"I think that the community really needs some feedback on the progress of
this very important task. Many existing Spark Streaming applications can't
be ported to Structured Streaming without Kafka support.

Is there a design document somewhere?  Or can someone from the DataBricks
team break down the existing monolithic JIRA issue into smaller steps that
reflect the current development plan?

Fred



eal with
ks.
e
"
Julien Dumazert <julien.dumazert@gmail.com>,"Mon, 29 Aug 2016 21:58:40 +0200",Re: Performance of loading parquet files into case classes in Spark,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Hi Maciek,

I followed your recommandation and benchmarked Dataframes aggregations on Dataset. Here is what I got:

// Dataset with RDD-style code
// 34.223s
df.as[A].map(_.fieldToSum).reduce(_ + _)
// Dataset with map and Dataframes sum
// 35.372s
df.as[A].map(_.fieldToSum).agg(sum(""value"")).collect().head.getAs[Long](0)

Not much of a difference. It seems that as soon as you access data as in RDDs, you force the full decoding of the object into a case class, which is super costly.

I find this behavior quite normal: as soon as you provide the user with the ability to pass a blackbox function, anything can happen, so you functions only, everything is ""white box"", so Spark understands what you want to do and can optimize.

Still, it breaks the promise of Datasets to me, and I hope there is something to do here (not confident on this point), and that it will be addressed in a later release.

Best regards,
Julien


<maciek@brynski.pl> a √©crit :
<mailto:julien.dumazert@gmail.com>>:
performance, for this use case at least.
advice, but the performance is unchanged. Apparently, summing does not take much time compared to accessing data.)
level of performance, which somehow breaks the promise of Dataset (preserving type safety and having Catalyst and Tungsten performance like datasets).
to 2.0. I guess, it's because of the fact that Dataframe is now Dataset[Row], and thus uses the same encoding/decoding mechanism as for any other case class.
<maciek@brynski.pl <mailto:maciek@brynski.pl>> a √©crit :
<mailto:julien.dumazert@gmail.com>>:
function will be better idea.

"
=?iso-8859-2?Q?Tomasz_Gaw=EAda?= <tomasz.gaweda@outlook.com>,"Mon, 29 Aug 2016 20:13:07 +0000",Real time streaming in Spark,dev.spark <dev@spark.apache.org>,"Hi everyone,


I wonder if there are plans to implement real time streaming in Spark. I see that in Spark 2.0 Trigger can have more implementations than ProcessingTime.


In my opinion Real Time streaming (so reaction on every event - like continous queries in Apache Ignite) will be very useful and will fill gap that is currently in Spark. Now, if we must implement both real-time streaming and batch jobs, the streaming must be done in other frameworks as Spark allows us only to process event in Micro Batches. Matei Zaharia wrote in Databricks blog about  Continuous Applications [1], in my opinion adding EventTrigger will be next big step to Continuous Applications.


What do you think about it? Are there any plans to implement such event-based trigger? Of course I can help with implementation, however I'm just starting learning Spark internals and it will take a while before I would be able to write something.


Pozdrawiam / Best regards,

Tomek


[1] https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html

[https://databricks.com/wp-content/uploads/2016/07/spark-2-continuous-apps-OG.png]<https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html>

Continuous Applications: Evolving Streaming in Apache Spark 2.0<https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html>
databricks.com
Apache Spark 2.0 lays the foundation for Continuous Applications, a simplified and unified way to write end-to-end streaming applications that reacts to data in real-time.


"
gsamaras <georgeSamarasDIT@gmail.com>,"Mon, 29 Aug 2016 14:34:53 -0700 (MST)",KMeans calls takeSample() twice?,dev@spark.apache.org,"After reading the internal code of Spark about it, I wasn't able to
understand why it calls takeSample() twice? Can someone please explain?

There is a relevant  StackOverflow question
<http://stackoverflow.com/questions/38986395/sparkkmeans-calls-takesample-twice> 
.



--

---------------------------------------------------------------------


"
Fang Zhang <fang.zhang.dx@gmail.com>,"Mon, 29 Aug 2016 18:46:55 -0700 (MST)",Saving less data to improve Pregel performance in GraphX?,dev@spark.apache.org,"Dear developers,

I am running some tests using Pregel API. 

It seems to me that more than 90% of the volume of a graph object is
composed of index structures that will not change during the execution of
Pregel. When the size of a graph is too huge to fit in memory, Pregel will
persist intermediate graphs on disk each time, which seems to involve a lot
of repeated disk savings.

In my test(Shortest Path), I save only one copy of the initial graph and
maintain only a var of RDD[(VertexID, VD)]. To create new messages, I create
a new graph using updated RDD[(VertexId, VD)] and the fixed data in initial
graph during each iteration. Using a slow NTFS hard drive, I did observe
around 40% overall improvement. Note my updateVertices(corresponding to
joinVertices) and edges.upgrade are not optimized yet (they can be optimized
following the follow of GraphX) and the improvement should be from I/O.

So my question is: do you think the current flow of Pregel could be improved
by saving a small portion of a large Graph object? If there are other
concerns, could you explain them?

Best regards,
Fang



--

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 29 Aug 2016 19:16:49 -0700",Re: Real time streaming in Spark,=?UTF-8?Q?Tomasz_Gaw=C4=99da?= <tomasz.gaweda@outlook.com>,"There were some prototypes/discussions being done on top of Spark
Streaming, and they were discussing how that would fit with regards to
Structured Streaming which was in design mode at that time. See
https://issues.apache.org/jira/browse/SPARK-14745 for some details and link
to PR.

com>

as
m
streaming-in-apache-spark-2-0.html>
streaming-in-apache-spark-2-0.html>
t


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Hyukjin Kwon <gurwls223@gmail.com>,"Tue, 30 Aug 2016 11:55:24 +0900","Inconsistency for nullvalue handling CSV: see SPARK-16462,
 SPARK-16460, SPARK-15144, SPARK-17290 and SPARK-16903",dev <dev@spark.apache.org>,"Hi all,


PR:
https://github.com/apache/spark/pull/14118

JIRAs
https://issues.apache.org/jira/browse/SPARK-17290
https://issues.apache.org/jira/browse/SPARK-16903
https://issues.apache.org/jira/browse/SPARK-16462
https://issues.apache.org/jira/browse/SPARK-16460
https://issues.apache.org/jira/browse/SPARK-15144

It seems this is not critical but the duplicated issues are being opened
due to this.

Also, it seems many users are affected by this. I hope this email make some
interests for reviewers.


Thanks.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 30 Aug 2016 03:03:00 +0000","Re: Inconsistency for nullvalue handling CSV: see SPARK-16462,
 SPARK-16460, SPARK-15144, SPARK-17290 and SPARK-16903","Hyukjin Kwon <gurwls223@gmail.com>, dev <dev@spark.apache.org>","I wish JIRA would automatically show you potentially similar issues as you
are typing up a new one, like Stack Overflow does...

It would really help cut down on duplicate reports.


"
kant kodali <kanth909@gmail.com>,"Mon, 29 Aug 2016 21:22:53 -0700","What are the names of the network protocols used between Spark
 Driver, Master and Workers?",dev <dev@spark.apache.org>,"What are the names of the network protocols used between Spark Driver,
Master and Workers?
"
Yanbo Liang <ybliang8@gmail.com>,"Mon, 29 Aug 2016 23:54:58 -0700",Re: KMeans calls takeSample() twice?,gsamaras <georgeSamarasDIT@gmail.com>,"I run KMeans with probes and found that takeSample() was called only once
actually. It looks like this issue was caused by mistake display at Spark
UI.

Thanks
Yanbo


"
Ofir Manor <ofir.manor@equalum.io>,"Tue, 30 Aug 2016 09:56:13 +0300",Re: Structured Streaming with Kafka sources/sinks,"""dev@spark.apache.org"" <dev@spark.apache.org>","I personally find it disappointing that a big chuck of Spark's design and
development is happening behind closed curtains. It makes it harder than
necessary for me to work with Spark. We had to improvise in the recent
weeks a temporary solution for reading from Kafka (from Structured
Streaming) to unblock our development, and I feed that if the design and
development of that feature was done in the open, it would have saved us a
lot of hassle (and would reduce the refactoring of our code base).

It hard not compare it to other Apache projects - for example, I believe
most of the Apache Kafka full-time contributors work at a single company,
but they manage as a community to have a very transparent design and
development process, which seems to work great.

Ofir Manor

Co-Founder & CTO | Equalum

Mobile: +972-54-7801286 | Email: ofir.manor@equalum.io


t
t
:
deal with
nks.
be
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 30 Aug 2016 10:20:03 +0200",3Ps for Datasets not available?! (=Parquet Predicate Pushdown),dev <dev@spark.apache.org>,"Hi,

I've been playing with UDFs and why they're a blackbox for Spark's
optimizer and started with filters to showcase the optimizations in
play.

My current understanding is that the predicate pushdowns are supported
by the following data sources:

1. Hive tables
2. Parquet files
3. ORC files
4. JDBC

While working on examples I came to a conclusion that not only does
predicate pushdown work for the data sources mentioned above but
solely for DataFrames. That was quite interesting since I was so much
into Datasets as strongly type-safe data abstractions in Spark SQL.

Can you help me to find the truth? Any links to videos, articles,
commits and such to further deepen my understanding of optimizations
in Spark SQL 2.0? I'd greatly appreciate.

The following query pushes the filter down to Parquet (see
PushedFilters attribute at the bottom)

scala> cities.filter('name === ""Warsaw"").queryExecution.executedPlan
res30: org.apache.spark.sql.execution.SparkPlan =
*Project [id#196L, name#197]
+- *Filter (isnotnull(name#197) && (name#197 = Warsaw))
   +- *FileScan parquet [id#196L,name#197] Batched: true, Format:
ParquetFormat, InputPaths:
file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [],
PushedFilters: [IsNotNull(name), EqualTo(name,Warsaw)], ReadSchema:
struct<id:bigint,name:string>

Why does this not work for Datasets? Is the function/lambda too
complex? Are there any examples where it works for Datasets? Are we
perhaps trading strong type-safety over optimizations like predicate
pushdown (and the feature's are yet to come in the next releases of
Spark 2)?

scala> cities.as[(Long, String)].filter(_._2 ==
""Warsaw"").queryExecution.executedPlan
res31: org.apache.spark.sql.execution.SparkPlan =
*Filter <function1>.apply
+- *FileScan parquet [id#196L,name#197] Batched: true, Format:
ParquetFormat, InputPaths:
file:/Users/jacek/dev/oss/spark/cities.parquet, PartitionFilters: [],
PushedFilters: [], ReadSchema: struct<id:bigint,name:string>

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 30 Aug 2016 16:21:20 +0800",Reynold on vacation next two weeks,"""dev@spark.apache.org"" <dev@spark.apache.org>","A lot of people have been pinging me on github and email directly and
expect instant reply. Just FYI I'm on vacation for two weeks with limited
internet access.
"
Reynold Xin <rxin@databricks.com>,"Tue, 30 Aug 2016 16:23:12 +0800",Re: 3Ps for Datasets not available?! (=Parquet Predicate Pushdown),Jacek Laskowski <jacek@japila.pl>,"The UDF is a black box so Spark can't know what it is dealing with. There
are simple cases in which we can analyze the UDFs byte code and infer what
it is doing, but it is pretty difficult to do in general.


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 30 Aug 2016 11:44:34 +0200",Re: 3Ps for Datasets not available?! (=Parquet Predicate Pushdown),Reynold Xin <rxin@databricks.com>,"Hi Reynold,

That's what I was told few times already (most notably by Adam on
twitter), but couldn't understand what it meant exactly. It's only now
when I understand what you're saying, Reynold :)

Does this put DataFrame's Column-based or SQL-based queries usually
faster than Datasets with Encoders?

How much I'm wrong to claim that for parquet files, Hive tables, and
JDBC tables using DataFrame + Columns/SQL-based queries is usually
faster than Datasets? Is that Datasets only shine for strongly typed
queries with data sources with no support for such optimizations like
filter pushdown? I'm tempted to say that for some data sources
DataFrames are faster than Datasets...always. True? What am I missing?

https://twitter.com/jaceklaskowski/status/770554918419755008

Thanks a lot, Reynold, for helping me out to get the gist of it all!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark 2.0 http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
=?UTF-8?B?QWJlbCBSaW5jw7Nu?= <ganger85@gmail.com>,"Tue, 30 Aug 2016 12:02:03 +0200",Re: Spark Kerberos proxy user,dev@spark.apache.org,"Hi again,

Is there any open issue related?

Nowadays, we (stratio)  have a end to end  running, with a spark
distribution based in 1.6.2.

Work in progress:

- Create and share our solution documentation.
- Test Suite for all the stuff.
- Rebase our code with apache-master branch.

Regards,


2016-08-25 12:10 GMT+02:00 Abel Rinc√≥n <ganger85@gmail.com>:

 data
so we want to
r
"
Rishi Mishra <rmishra@snappydata.io>,"Tue, 30 Aug 2016 18:32:02 +0530",Spark 2.0.1 fails for provided hadoop,dev@spark.apache.org,"Hi All,
I tried to configure my Spark with MapR hadoop cluster. For that I built
Spark 2.0 from source with hadoop-provided option. Then as per the document
I set my hadoop libraries in spark-env.sh.
However I get an error while SessionCatalog is getting created.  Please
refer below for exception stack trace.  Point to note is default scheme for
MapR is ""maprfs://"".  Hence the error.

I can see some fixes were there earlier to solve the problem.
https://github.com/apache/spark/pull/13348
But another PR removed the code.
https://github.com/apache/spark/pull/13868/files.

If I take the changes in the 1st PR mentioned here it works perfectly
fine.

Is it intentional or is it a bug ?
If its intentional , does user always have to run drivers on a hadoop
cluster node ? Which might make ""some"" sense in a production environment ,
but it is not very helpful during development.

GIT version on my fork : d16f9a0b7c464728d7b11899740908e23820a797.

Regards,
Rishitesh Mishra,
SnappyData . (http://www.snappydata.io/)

https://in.linkedin.com/in/rishiteshmishra



Exception Stack
=====================================================================

2016-08-29 18:30:17,0869 ERROR JniCommon
fs/client/fileclient/cc/jni_MapRClient.cc:2073 Thread: 18258 mkdirs failed
for /rishim1/POCs/spark-2.0.1-SNAPSHOT-bin-custom-spark/spar, error 13
org.apache.spark.SparkException: Unable to create database default as
failed to create its directory
maprfs:///rishim1/POCs/spark-2.0.1-SNAPSHOT-bin-custom-spark/spark-warehouse
  at
org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:126)
  at
org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.createDatabase(InMemoryCatalog.scala:120)
  at
org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:147)
  at
org.apache.spark.sql.catalyst.catalog.SessionCatalog.<init>(SessionCatalog.scala:89)
  at
org.apache.spark.sql.internal.SessionState.catalog$lzycompute(SessionState.scala:95)
  at
org.apache.spark.sql.internal.SessionState.catalog(SessionState.scala:95)
  at
org.apache.spark.sql.internal.SessionState$$anon$1.<init>(SessionState.scala:112)
  at
org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:112)
  at
org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:111)
  at
org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:49)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:64)
  at
org.apache.spark.sql.SparkSession.baseRelationToDataFrame(SparkSession.scala:382)
  at org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:143)
  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:427)
  at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:411)
  ... 48 elided
Caused by: org.apache.hadoop.security.AccessControlException: User
rishim(user id 1000)  has been denied access to create spark-warehouse
  at com.mapr.fs.MapRFileSystem.makeDir(MapRFileSystem.java:1239)
  at com.mapr.fs.MapRFileSystem.mkdirs(MapRFileSystem.java:1259)
  at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1913)
  at
org.apache.spark.sql.catalyst.catalog.InMemoryCatalog.liftedTree1$1(InMemoryCatalog.scala:123)
  ... 62 more
"
Rich Bowen <rbowen@apache.org>,"Tue, 30 Aug 2016 11:03:41 -0400",ApacheCon Seville CFP closes September 9th,apachecon-discuss@apache.org,"It's traditional. We wait for the last minute to get our talk proposals
in for conferences.

Well, the last minute has arrived. The CFP for ApacheCon Seville closes
on September 9th, which is less than 2 weeks away. It's time to get your
talks in, so that we can make this the best ApacheCon yet.

It's also time to discuss with your developer and user community whether
there's a track of talks that you might want to propose, so that you
have more complete coverage of your project than a talk or two.

For Apache Big Data, the relevant URLs are:
Event details:
http://events.linuxfoundation.org/events/apache-big-data-europe
CFP:
http://events.linuxfoundation.org/events/apache-big-data-europe/program/cfp

For ApacheCon Europe, the relevant URLs are:
Event details: http://events.linuxfoundation.org/events/apachecon-europe
CFP: http://events.linuxfoundation.org/events/apachecon-europe/program/cfp

This year, we'll be reviewing papers ""blind"" - that is, looking at the
abstracts without knowing who the speaker is. This has been shown to
eliminate the ""me and my buddies"" nature of many tech conferences,
producing more diversity, and more new speakers. So make sure your
abstracts clearly explain what you'll be talking about.

For further updated about ApacheCon, follow us on Twitter, @ApacheCon,
or drop by our IRC channel, #apachecon on the Freenode IRC network.

-- 
Rich Bowen
WWW: http://apachecon.com/
Twitter: @ApacheCon

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Tue, 30 Aug 2016 10:09:22 -0500",Re: Structured Streaming with Kafka sources/sinks,Ofir Manor <ofir.manor@equalum.io>,"Not that I wouldn't rather have more open communication around this
issue...but what are people actually expecting to get out of
structured streaming with regard to Kafka?

There aren't any realistic pushdown-type optimizations available, and
from what I could tell the last time I looked at structured streaming,
resolving the event time vs processing time issue was still a ways
off.

eks
o
:
't
s
at
:
e:
m>
 deal with
inks.

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 30 Aug 2016 17:36:39 +0200",Re: Reynold on vacation next two weeks,Reynold Xin <rxin@databricks.com>,"Hi,

Definitely well deserved. Don't check your emails for the 2 weeks. Not even
for a minute :-)

Jacek


"
Jerry Lam <chilinglam@gmail.com>,"Tue, 30 Aug 2016 11:43:59 -0400",Re: Broadcast Variable Life Cycle,Sean Owen <sowen@cloudera.com>,"Hi Sean,

Thank you for the response. The only problem is that actively managing
broadcast variables require to return the broadcast variables to the caller
if the function that creates the broadcast variables does not contain any
action. That is the scope that uses the broadcast variables cannot destroy
the broadcast variables in many cases. For example:

==============
def perfromTransformation(rdd: RDD[int]) = {
   val sharedMap = sc.broadcast(map)
   rdd.map{id =>
      val localMap = sharedMap.vlaue
      (id, localMap(id))
   }
}

def main = {
    ....
    performTransformation(rdd).toDF(""id"",
""i"").write.parquet(""dummy_example"")
}
==============

In this example above, we cannot destroy the sharedMap before the
write.parquet is executed because RDD is evaluated lazily. We will get a
exception if I put sharedMap.destroy like this:

==============
def perfromTransformation(rdd: RDD[int]) = {
   val sharedMap = sc.broadcast(map)
   val result = rdd.map{id =>
      val localMap = sharedMap.vlaue
      (id, localMap(id))
   }
   sharedMap.destroy
   result
}
==============

Am I missing something? Are there better way to do this without returning
the broadcast variables to the main function?

Best Regards,

Jerry




"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 30 Aug 2016 15:44:36 +0000",Re: Structured Streaming with Kafka sources/sinks,"Cody Koeninger <cody@koeninger.org>, Ofir Manor <ofir.manor@equalum.io>","development is happening behind closed curtains.

I'm not too familiar with Streaming, but I see design docs and proposals
for ML and SQL published here and on JIRA all the time, and they are
discussed extensively.

For example, here are some ML JIRAs with extensive design discussions:
SPARK-6725 <https://issues.apache.org/jira/browse/SPARK-6725>, SPARK-13944
<https://issues.apache.org/jira/browse/SPARK-13944>, SPARK-16365
<https://issues.apache.org/jira/browse/SPARK-16365>

Nick


:
nd
n
of
le
e
y,
of
f
ll deal
/sinks.
ll
d
--
"
Sean Owen <sowen@cloudera.com>,"Tue, 30 Aug 2016 16:58:49 +0100",Re: Broadcast Variable Life Cycle,Jerry Lam <chilinglam@gmail.com>,"Yes, although there's a difference between unpersist and destroy,
you'll hit the same type of question either way. You do indeed have to
reason about when you know the broadcast variable is no longer needed
in the face of lazy evaluation, and that's hard.

Sometimes it's obvious and you can take advantage of this to
proactively free resources. You may have to consider restructuring the
computation to allow for more resources to be freed, if this is
important to scale.

Keep in mind that things that are computed and cached may be lost and
recomputed even after their parent RDDs were definitely already
computed and don't seem to be needed. This is why unpersist is often
the better thing to call because it allows for variables to be
rebroadcast if needed in this case. Destroy permanently closes the
broadcast.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 31 Aug 2016 00:02:11 +0800",Re: Structured Streaming with Kafka sources/sinks,Nicholas Chammas <nicholas.chammas@gmail.com>,"In this case simply not much progress has been made, because people might
be busy with other stuff.

Ofir it looks like you have spent non-trivial amount of time thinking about
this topic and have even designed something to work -- can you chime in on
the JIRA ticket with your thoughts and your prototype? That would be
tremendously useful to the project.




4
an
)
ve
of
ill deal
s/sinks.
nd
"
Jerry Lam <chilinglam@gmail.com>,"Tue, 30 Aug 2016 12:12:06 -0400",Re: Broadcast Variable Life Cycle,Sean Owen <sowen@cloudera.com>,"Hi Sean,

Thank you for sharing the knowledge between unpersist and destroy.
Does that mean unpersist keeps the broadcast variable in the driver whereas
destroy will delete everything about the broadcast variable like it has
never existed?

Best Regards,

Jerry



"
Cody Koeninger <cody@koeninger.org>,"Tue, 30 Aug 2016 11:12:08 -0500",Re: Structured Streaming with Kafka sources/sinks,Reynold Xin <rxin@databricks.com>,"In case it wasn't obvious from the ticket, I'm happy to work on this,
I just don't want to get in a situation where the work I do conflicts
with or duplicates work that's already being done.

 be
ut
n
t
g)
t
s
s
g>
will deal
es/sinks.
----

---------------------------------------------------------------------


"
gsamaras <georgeSamarasDIT@gmail.com>,"Tue, 30 Aug 2016 09:18:29 -0700 (MST)",Re: KMeans calls takeSample() twice?,dev@spark.apache.org,"Yanbo thank you for your reply. So you are saying that this is a bug in the
Spark UI in general, and not in the local Spark UI of our cluster, where I
work, right?

George






--"
Sean Owen <sowen@cloudera.com>,"Tue, 30 Aug 2016 17:22:18 +0100",Re: Broadcast Variable Life Cycle,Jerry Lam <chilinglam@gmail.com>,"Yeah, after destroy, accessing the broadcast variable results in an
error. Accessing it after it's unpersisted (on an executor) causes it
to be rebroadcast.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 30 Aug 2016 17:24:43 +0100",Re: KMeans calls takeSample() twice?,gsamaras <georgeSamarasDIT@gmail.com>,"I'm not sure it's a UI bug; it really does record two different
stages, the second of which executes quickly. I am not sure why that
would happen off the top of my head. I don't see anything that failed
here.

Digging into those two stages and what they executed might give a clue
to what's really going on there.


---------------------------------------------------------------------


"
gsamaras <georgeSamarasDIT@gmail.com>,"Tue, 30 Aug 2016 09:49:55 -0700 (MST)",Re: KMeans calls takeSample() twice?,dev@spark.apache.org,"I am not sure what you want me to check. Note that I see two takeSample()s
being invoked every single time I execute KMeans(). In a current job I
have, I did view the details and updated the:

StackOverflow question.
<http://stackoverflow.com/questions/38986395/sparkkmeans-calls-takesample-twice>








--"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 30 Aug 2016 09:56:12 -0700",Re: Mesos is now a maven module,Michael Gummelt <mgummelt@mesosphere.io>,"Hi, Michael.

It's a great news!

BTW, I'm wondering if the Jenkins (SparkPullRequestBuilder) knows this new
profile, -Pmesos.

The PR was passed with the following Jenkins build arguments without
`-Pmesos` option. (at the last test)
```
[info] Building Spark (w/Hive 1.2.1) using SBT with these arguments:
 -Pyarn -Phadoop-2.3 -Pkinesis-asl -Phive-thriftserver -Phive test:package
streaming-kafka-0-8-assembly/assembly streaming-flume-assembly/assembly
streaming-kinesis-asl-assembly/assembly
```
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/64435/consoleFull

Also, up to now, Jenkins seems not to use '-Pmesos' for all PRs.

Bests,
Dongjoon.



"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 30 Aug 2016 10:00:16 -0700",Re: Mesos is now a maven module,Dongjoon Hyun <dongjoon@apache.org>,"Michael added the profile to the build scripts, but maybe some script
or code path was missed...




-- 
Marcelo

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 30 Aug 2016 18:01:17 +0100",Re: Mesos is now a maven module,Dongjoon Hyun <dongjoon@apache.org>,"I have the heady power to modify Jenkins jobs now, so I will carefully take
a look at them and see if any of the config needs -Pmesos. But yeah I
thought this should be baked into the script.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 30 Aug 2016 10:05:09 -0700",Re: Mesos is now a maven module,Sean Owen <sowen@cloudera.com>,"A quick look shows that maybe dev/sparktestsupport/modules.py needs to
be modified, and a ""build_profile_flags"" added to the mesos section
(similar to hive / hive-thriftserver).

Note not all PR builds will trigger mesos currently, since it's listed
as an independent module in the above file.




-- 
Marcelo

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 30 Aug 2016 10:11:15 -0700",Re: Mesos is now a maven module,Marcelo Vanzin <vanzin@cloudera.com>,"Thank you for confirming, Sean and Marcelo!

Bests,
Dongjoon.


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 30 Aug 2016 10:20:54 -0700",Re: KMeans calls takeSample() twice?,Georgios Samaras <georgesamarasdit@gmail.com>,"I think takeSample itself runs multiple jobs if the amount of samples
collected in the first pass is not enough. The comment and code path
at https://github.com/apache/spark/blob/412b0e8969215411b97efd3d0984dc6cac5d31e0/core/src/main/scala/org/apache/spark/rdd/RDD.scala#L508
should explain when this happens. Also you can confirm this by
checking if the logWarning shows up in your logs.

Thanks
Shivaram


---------------------------------------------------------------------


"
Georgios Samaras <georgesamarasdit@gmail.com>,"Tue, 30 Aug 2016 10:31:59 -0700",Re: KMeans calls takeSample() twice?,shivaram@eecs.berkeley.edu,"Good catch Shivaram. However, the very next line states:

// this shouldn't happen often because we use a big multiplier for the
initial size

which makes me wondering if that is the case, really, since I am
experimenting heavily right now and I launched 30~40 jobs, and from a
glance on them I can see takeSample() being called twice!

George



"
Steve Loughran <stevel@hortonworks.com>,"Tue, 30 Aug 2016 18:16:33 +0000",Re: Performance of loading parquet files into case classes in Spark,Spark dev list <dev@spark.apache.org>,"

Hi Maciek,

I followed your recommandation and benchmarked Dataframes aggregations on Dataset. Here is what I got:


// Dataset with RDD-style code
// 34.223s
df.as[A].map(_.fieldToSum).reduce(_ + _)

// Dataset with map and Dataframes sum
// 35.372s


df.as[A].map(_.fieldToSum).agg(sum(""value"")).collect().head.getAs[Long](0)

Not much of a difference. It seems that as soon as you access data as in RDDs, you force the full decoding of the object into a case class, which is super costly.

I find this behavior quite normal: as soon as you provide the user with the ability to pass a blackbox function, anything can happen, so you have to ly, everything is ""white box"", so Spark understands what you want to do and can optimize.



SWL and the dataframe code where you are asking for a specific field can be handled by the file format itself, so optimising the operation. If you ask for only one column of Parquet and orc data, then only that column's data should be loaded. And because they store columns together, you save on all the IO needed to read all the discarded columns. Add even more selectiveness (such as ranges in values), then you can even get ""predicate pushdown"" where blocks of the file are skipped if the input format reader can determine that none of the columns there match the predicate's conditions.

you should be able to ge away with something like df.select(""field"").... to filter out the fields you want first, then stay in code rather than SQL.

Anyway, experiment: its always more accurate than the opinions of others, especially when applied to your own datasets.
"
Sean Owen <sowen@cloudera.com>,"Tue, 30 Aug 2016 19:32:57 +0100",Re: Mesos is now a maven module,Marcelo Vanzin <vanzin@cloudera.com>,"Ah, I helped miss that. We don't enable -Pyarn for YARN because it's
already always set? I wonder if it makes sense to make that optional
in order to speed up builds, or, maybe I'm missing a reason it's
always essential.

I think it's not setting -Pmesos indeed because no Mesos code was
changed but I think that script change is necessary as a follow up
yes.

Yeah, nothing is in the Jenkins config itself.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 30 Aug 2016 11:36:27 -0700",Re: Mesos is now a maven module,Sean Owen <sowen@cloudera.com>,"
YARN is currently handled as part of the Hadoop profiles in
dev/run-tests.py; it could potentially be changed to behave like the
others (e.g. only enabled when the YARN code changes).

-- 
Marcelo

---------------------------------------------------------------------


"
Michael Gummelt <mgummelt@mesosphere.io>,"Tue, 30 Aug 2016 13:05:48 -0700",Re: Spark Kerberos proxy user,=?UTF-8?B?QWJlbCBSaW5jw7Nu?= <ganger85@gmail.com>,"Here's one: https://issues.apache.org/jira/browse/SPARK-16742

e:

hdfs
 HDFS
 so we want to


-- 
Michael Gummelt
Software Engineer
Mesosphere
"
kant kodali <kanth909@gmail.com>,"Tue, 30 Aug 2016 20:15:08 +0000","Re: What are the names of the network protocols used between Spark
 Driver, Master and Workers?",dev <dev@spark.apache.org>,"Ok I will answer my own question. Looks like Netty based RPC





What are the names of the network protocols used between Spark Driver, Master
and Workers?"
Michael Gummelt <mgummelt@mesosphere.io>,"Tue, 30 Aug 2016 13:48:48 -0700",Re: Mesos is now a maven module,Marcelo Vanzin <vanzin@cloudera.com>,"https://github.com/apache/spark/pull/14885

Thanks




-- 
Michael Gummelt
Software Engineer
Mesosphere
"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 30 Aug 2016 13:56:41 -0700",Re: Mesos is now a maven module,Michael Gummelt <mgummelt@mesosphere.io>,"Thank you all for quick fix! :D

Dongjoon.


"
vsr <vsr@cloudera.com>,"Tue, 30 Aug 2016 15:31:41 -0700 (MST)",Re: How to check for No of Records per partition in Dataframe,dev@spark.apache.org,"Hi Saurabh, 

You can do the following to print the number of entries in each partition.
You may need to grep executor logs for the counts.

val rdd = sc.parallelize(1 to 100, 4)
rdd.foreachPartition(it => println(""Record count in partition"" + it.size))

Hope this is what you are looking for.

Thanks
Srinivas



--

---------------------------------------------------------------------


"
Mohit Jaggi <mohitjaggi@gmail.com>,"Tue, 30 Aug 2016 16:51:30 -0700",Re: Model abstract class in spark ml,Sean Owen <sowen@cloudera.com>,"thanks Sean. I am cross posting on dev to see why the code was written that way. Perhaps, this.type doesn‚Äôt do what is needed.

Mohit Jaggi
Founder,
Data Orchard LLC
www.dataorchardllc.com






---------------------------------------------------------------------


"
"""huanqinghappy"" <huanqinghappy@aliyun.com>","Wed, 31 Aug 2016 10:39:51 +0800",=?UTF-8?B?ZGV2LXN1YnNjcmliZUBzcGFyay5hcGFjaGUub3Jn?=,"""dev"" <dev@spark.apache.org>",dev-subscribe@spark.apache.org
Mohit Jaggi <mohitjaggi@gmail.com>,"Tue, 30 Aug 2016 21:51:37 -0700",Re: Model abstract class in spark ml,Sean Owen <sowen@cloudera.com>,"thanks Sean. I am cross posting on dev to see why the code was written that
way. Perhaps, this.type doesn‚Äôt do what is needed.

Mohit Jaggi
Founder,
Data Orchard LLC
www.dataorchardllc.com





I think it's imitating, for example, how Enum is delcared in Java:

abstract class Enum<E extends Enum<E>>

this is done so that Enum can refer to the actual type of the derived
enum class when declaring things like public final int compareTo(E o)
to implement Comparable<E>. The type is redundant in a sense, because
you effectively have MyEnum extending Enum<MyEnum>.

Java allows this self-referential definition. However Scala has
""this.type"" for this purpose and (unless I'm about to learn something
deeper about Scala) it would have been the better way to express this
so that Model methods can for example state that copy() returns a
Model of the same concrete type.

I don't know if it can be changed now without breaking compatibility
but you're welcome to give it a shot with MiMa to see. It does
compile, using this.type.



Folks,
I am having a bit of trouble understanding the following:

abstract class Model[M <: Model[M]]

Why is M <: Model[M]?

Cheers,
Mohit.
"
Mohit Jaggi <mohitjaggi@gmail.com>,"Tue, 30 Aug 2016 22:33:15 -0700",Re: Model abstract class in spark ml,Sean Owen <sowen@cloudera.com>,"I think I figured it out. There is indeed ""something deeper in Scala‚Äù :-)

abstract class A {
  def a: this.type
}

class AA(i: Int) extends A {
  def a = this
}
the above works ok. But if you return anything other than ‚Äúthis‚Äù, you will get a compile error.

abstract class A {
  def a: this.type
}

class AA(i: Int) extends A {
  def a = new AA(1)
}
Error:(33, 11) type mismatch;
 found   : com.dataorchard.datagears.AA
 required: AA.this.type
  def a = new AA(1)
          ^

So you have to do:

abstract class A[T <: A[T]]  {
  def a: T
}

class AA(i: Int) extends A[AA] {
  def a = new AA(1)
}


Mohit Jaggi
Founder,
Data Orchard LLC
www.dataorchardllc.com




that way. Perhaps, this.type doesn‚Äôt do what is needed.

"
Sean Owen <sowen@cloudera.com>,"Wed, 31 Aug 2016 08:19:17 +0100",Re: Model abstract class in spark ml,Mohit Jaggi <mohitjaggi@gmail.com>,"Weird, I recompiled Spark with a similar change to Model and it seemed
to work but maybe I missed a step in there.

Äù :-)
‚Äù, you will

---------------------------------------------------------------------


"
Yanbo Liang <ybliang8@gmail.com>,"Wed, 31 Aug 2016 04:38:38 -0700",Re: KMeans calls takeSample() twice?,Georgios Samaras <georgesamarasdit@gmail.com>,"I added println at the start of function takeSample, and found it was
printed only once for each run of KMeans.

Thanks
Yanbo


"
Cody Koeninger <cody@koeninger.org>,"Wed, 31 Aug 2016 09:32:19 -0500",Re: Model abstract class in spark ml,Sean Owen <sowen@cloudera.com>,"http://blog.originate.com/blog/2014/02/27/types-inside-types-in-scala/

:
Äù :-)
‚Äù, you will

---------------------------------------------------------------------


"
Georgios Samaras <georgesamarasdit@gmail.com>,"Wed, 31 Aug 2016 09:29:09 -0700",Re: KMeans calls takeSample() twice?,Yanbo Liang <ybliang8@gmail.com>,"But as can see in this:

  Stackoverflow question
<http://stackoverflow.com/questions/38986395/sparkkmeans-calls-takesample-twice>

this is simply not the case for me. Could it because your data is not big
enough to reproduce?

Or could it be that you are actually reproducing it, but the problem comes
with the weakness of the UI to display it correctly?


"
Tejas Patil <tejas.patil.cs@gmail.com>,"Wed, 31 Aug 2016 11:04:49 -0700",Questions about bucketing in Spark,dev@spark.apache.org,"Hi everyone,

I am working towards making Spark's Sort Merge join in par with Hive's
Sort-Merge-Bucket join to use sorted. So far I have identified these main
items to be addressed:

1. Make query planner to use `sorted`ness information for sort merge join
(SPARK-15453, SPARK-17271)
2. Configurable hashing and bucketing function in Spark. There need to be
separate impls for Hive as the default one in Spark is not compatible with
Hive.
3. Propagate bucketing information for Hive tables to / from Catalog
4. Ensure that writes to Hive bucketed tables produce single file per bucket

I have started off with #1 and got few questions as I am moving to next
items:

(a) Current Spark way of creating bucketed tables (ie. `bucketBy`) produces
multiple files per bucket which produces bucketed data faster [1]. But on
the read side, one cannot utilize the benefits of sorted-ness as the files
for a given bucket-id are locally sorted (not globally). Was this by design
? I read the design doc for ""Bucketed Tables"" [0] and there is no mention
about this (actually the plan seemed to do same as Hive, see sections
""Writing Bucketed Tables"" and ""Reading persisted bucketed tables"").

(b) In one of the TODO / future work section, the design doc [0] says :
""Take advantage of sorted files. We will do this later as it has many
subtle issues"". I could not find much information on what these issues are.
Can anyone highlight the known issues ?

[0] : https://issues.apache.org/jira/secure/attachment/
12778890/BucketedTables.pdf
[1] : https://issues.apache.org/jira/browse/SPARK-12394
"
Mohit Jaggi <mohitjaggi@gmail.com>,"Wed, 31 Aug 2016 11:13:13 -0700",Re: Model abstract class in spark ml,Cody Koeninger <cody@koeninger.org>,"Thanks Cody. That was a good explanation!

Mohit Jaggi
Founder,
Data Orchard LLC
www.dataorchardllc.com




seemed
Scala‚Äù :-)
‚Äúthis‚Äù, you will


---------------------------------------------------------------------


"
