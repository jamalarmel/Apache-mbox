myasuka <myasuka@live.com>,"Tue, 30 Sep 2014 23:00:47 -0700 (PDT)",Re: How to use multi thread in RDD map function ?,dev@spark.incubator.apache.org,"Thank for your advise, I have tried your recommended configuration, but 

SPARK_WORKER_CORES=32
SPARK_WORKER_INSTANCES=1

still work better, in the offical spark-standalone, about the parameter
'SPARK_WORKER_INSTANCES' /You can make this more than 1 if you have have
very large machines and would like multiple Spark worker processes./ Maybe
our 16 nodes cluster with 16cores, 24GB memory is not fit for more than 1
worker instance.

Yi Tian wrote












--

---------------------------------------------------------------------


"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 1 Oct 2014 11:25:12 +0200",Re: parquet predicate / projection pushdown into unionAll,Michael Armbrust <michael@databricks.com>,"Hi Cody and Michael,

We ran into the same issue. Each day of data we have is stored into
one parquet, and we want to query it against monthly parquet data. The
data for each data is around 600GB, and we use 300 executors with 8GB
memory for each executor. Without the patch, it took forever, and
crashed in the end.

With patch, for table obtained by unionAll with 5 parquets (around
3TB), it takes 199.126 seconds to execute a simple HIVE query, while
it takes 64.36 seconds for individual parquet file. Great work, this
patch solves the issue. Hopefully to see this in next 1.1 series.

Thanks.

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Wed, 1 Oct 2014 07:14:05 -0700","Re: jenkins downtime/system upgrade wednesday morning, 730am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","jenkins is currently in quiet mode, and will be restarted once the current
crop of builds finishes.


"
shane knapp <sknapp@berkeley.edu>,"Wed, 1 Oct 2014 07:54:31 -0700","Re: jenkins downtime/system upgrade wednesday morning, 730am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","upgrade complete!  we're back online and happily building.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 1 Oct 2014 15:53:03 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"

Are there any updates on this move of the Jenkins infrastructure to a
managed datacenter?

I remember it being mentioned that another benefit of this move would be
reduced flakiness when Jenkins tries to checkout patches for testing. For
some reason, I'm getting a lot of those
<https://github.com/apache/spark/pull/2606#issuecomment-57514540> today.

Nick
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 1 Oct 2014 15:56:18 -0400",Re: do MIMA checking before all test cases start?,Patrick Wendell <pwendell@gmail.com>,"How early can MiMa checks be run? Before Spark is even built
<https://github.com/apache/spark/blob/8cc70e7e15fd800f31b94e9102069506360289db/dev/run-tests#L118>?
After the build but before the unit tests?


"
shane knapp <sknapp@berkeley.edu>,"Wed, 1 Oct 2014 13:44:01 -0700",Re: amplab jenkins is down,Nicholas Chammas <nicholas.chammas@gmail.com>,"as of this morning, i've got the new jenkins up, with all of the current
builds set up (but failing).  i'm in the middle of playing setup/debug
whack-a-mole, but we're getting there.  my guess would be early next week
for the switchover.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 1 Oct 2014 16:50:19 -0400",Re: amplab jenkins is down,shane knapp <sknapp@berkeley.edu>,"Sounds good! Thanks for the update Shane.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 1 Oct 2014 17:01:06 -0400",Extending Scala style checks,dev <dev@spark.apache.org>,"As discussed here <https://github.com/apache/spark/pull/2619>, it would be
good to extend our Scala style checks to programmatically enforce as many
of our style rules as possible.

Does anyone know if it's relatively straightforward to enforce additional
rules like the ""no trailing spaces"" rule mentioned in the linked PR?

Nick
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 1 Oct 2014 14:36:04 -0700",Re: Extending Scala style checks,Nicholas Chammas <nicholas.chammas@gmail.com>,"Please take a look at WhitespaceEndOfLineChecker under:
http://www.scalastyle.org/rules-0.1.0.html

Cheers


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 1 Oct 2014 15:37:11 -0700",Re: Extending Scala style checks,Ted Yu <yuzhihong@gmail.com>,"Hey Nick,

We can always take built-in rules. Back when we added this Prashant
Sharma actually did some great work that lets us write our own style
rules in cases where rules don't exist.

You can see some existing rules here:
https://github.com/apache/spark/tree/master/project/spark-style/src/main/scala/org/apache/spark/scalastyle

Prashant has over time contributed a lot of our custom rules upstream
to stalastyle, so now there are only a couple there.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 1 Oct 2014 21:13:05 -0400",Re: Extending Scala style checks,Patrick Wendell <pwendell@gmail.com>,"Ah, since there appears to be a built-in rule for end-of-line whitespace,
Michael and Cheng, y'all should be able to add this in pretty easily.

Nick


"
Michael Armbrust <michael@databricks.com>,"Wed, 1 Oct 2014 18:20:13 -0700",Re: Extending Scala style checks,Nicholas Chammas <nicholas.chammas@gmail.com>,"The hard part here is updating the existing code base... which is going to
create merge conflicts with like all of the open PRs...


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 1 Oct 2014 21:24:53 -0400",Re: Extending Scala style checks,Michael Armbrust <michael@databricks.com>,"Yeah, I remember that hell when I added PEP 8 to the build checks and fixed
all the outstanding Python style issues. I had to keep rebasing and
resolving merge conflicts until the PR was merged.

It's a rough process, but thankfully it's also a one-time process. I might
be able to help with that in the next week or two if no-one else wants to
pick it up.

Nick


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 1 Oct 2014 21:33:55 -0400",Re: Extending Scala style checks,Michael Armbrust <michael@databricks.com>,"Does anyone know if Scala has something equivalent to autopep8
<https://pypi.python.org/pypi/autopep8>? It would help patch up the
existing code base a lot quicker as we add in new style rules.
â€‹

m

t
e,
n/scala/org/apache/spark/scalastyle
as
R?
"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Oct 2014 19:30:20 -0700",Re: Extending Scala style checks,Nicholas Chammas <nicholas.chammas@gmail.com>,"There is scalariform but it can be disruptive. Last time I ran it on Spark
it didn't compile due to some xml interpolation problem.


nd
to
m
g
e
scala/org/apache/spark/scalastyle
am
t
e
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 02 Oct 2014 12:06:25 +0800",Re: Extending Scala style checks,"Nicholas Chammas <nicholas.chammas@gmail.com>, 
 Michael Armbrust <michael@databricks.com>","Since we can easily catch the list of all changed files in a PR, I think 
we can start with adding the no trailing space check for newly changed 
files only?



---------------------------------------------------------------------


"
Du Li <lidu@yahoo-inc.com.INVALID>,"Thu, 2 Oct 2014 19:39:05 +0000",HiveContext: cache table not supported for partitioned table?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

In Spark 1.1 HiveContext, I ran a create partitioned table command followed by a cache table command and got a java.sql.SQLSyntaxErrorException: Table/View 'PARTITIONS' does not exist. But cache table worked fine if the table is not a partitioned table.

Can anybody confirm that cache of partitioned table is not supported yet in current version?

Thanks,
Du
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 2 Oct 2014 19:30:51 -0400",Re: EC2 clusters ready in launch time + 30 seconds,"dev <dev@spark.apache.org>, 
	Shivaram Venkataraman <shivaram@eecs.berkeley.edu>","Is there perhaps a way to define an AMI programmatically? Like, a
collection of base AMI id + list of required stuff to be installed + list
of required configuration changes. Iâ€™m guessing thatâ€™s what people use
things like Puppet, Ansible, or maybe also AWS CloudFormation for, right?

If we could do something like that, then with every new release of Spark we
could quickly and easily create new AMIs that have everything we need.
spark-ec2 would only have to bring up the instances and do a minimal amount
of configuration, and the only thing weâ€™d need to track in the Spark repo
is the code that defines what goes on the AMI, as well as a list of the AMI
ids specific to each release.

Iâ€™m just thinking out loud here. Does this make sense?

Nate,

Any progress on your end with this work?

Nick
â€‹


be
o
fs
:
op
gs
"
David Rowe <davidrowe@gmail.com>,"Fri, 3 Oct 2014 09:43:35 +1000",Re: EC2 clusters ready in launch time + 30 seconds,Nicholas Chammas <nicholas.chammas@gmail.com>,"I think this is exactly what packer is for. See e.g.
http://www.packer.io/intro/getting-started/build-image.html

bad package for httpd, whcih causes ganglia not to start. For some reason I
can't get access to the raw AMI to fix it.

m

at people use
we
nt
ark repo
MI
f
t
 a
d
es
ck
"
"""Nate D'Amico"" <nate@reactor8.com>","Thu, 2 Oct 2014 17:00:44 -0700",RE: EC2 clusters ready in launch time + 30 seconds,"""'dev'"" <dev@spark.apache.org>","Bit of progress on our end, bit of lagging as well.  Our guy leading effort got little bogged down on client project to update hive/sql testbed to latest spark/sparkSQL, also launching public service so we have been bit scattered recently.

Will have some more updates probably after next week.  We are planning on taking our client work around hive/spark, plus taking over the bigtop automation work to modernize and get that fit for human consumption outside or org.  All our work and puppet modules will be open sourced, documented, hopefully start to rally some other folks around effort that find it useful

Side note, another effort we are looking into is gradle tests/support.  We have been leveraging serverspec for some basic infrastructure tests, but with bigtop switching over to gradle builds/testing setup in 0.8 we want to include support for that in our own efforts, probably some stuff that can be learned and leveraged in spark world for repeatable/tested infrastructure 

If anyone has any specific automation questions to your environment you can drop me a line directly.., will try to help out best I can.  Else will post update to dev list once we get on top of our own product release and the bigtop work

Nate


I think this is exactly what packer is for. See e.g.
http://www.packer.io/intro/getting-started/build-image.html

a bad package for httpd, whcih causes ganglia not to start. For some reason I can't get access to the raw AMI to fix it.

<nicholas.chammas@gmail.com

thatâ€™s what 
CloudFormation for, right?
we need.
in the 



more.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 2 Oct 2014 20:23:07 -0400",Re: EC2 clusters ready in launch time + 30 seconds,"""Nate D'Amico"" <nate@reactor8.com>","Thanks for the update, Nate. I'm looking forward to seeing how these
projects turn out.

David, Packer looks very, very interesting. I'm gonna look into it more
next week.

Nick



d
it
de
,
ul
e
to
ure
l
a
 I
™s what
n
in the
e.
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Thu, 2 Oct 2014 17:37:10 -0700 (PDT)","What is the best way to build my developing Spark for testing on
 EC2?",dev@spark.incubator.apache.org,"Hi all, 

I am trying to contribute some machine learning algorithms to MLlib. 
I must evaluate their performance on a cluster, changing input data 
size, the number of CPU cores and any their parameters.

I would like to build my develoipng Spark on EC2 automatically. 
Is there already a building script for a developing version like spark-ec2
script?
Or if you have any good idea to evaluate the performance of a developing 
MLlib algorithm on a spark cluster like EC2, could you tell me?

Best,



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Evan Sparks <evan.sparks@gmail.com>,"Thu, 2 Oct 2014 17:53:10 -0700",Re: What is the best way to build my developing Spark for testing on EC2?,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"I recommend using the data generators provided with MLlib to generate synthetic data for your scalability tests - provided they're well suited for your algorithms. They let you control things like number of examples and dimensionality of your dataset, as well as number of partitions. 

As far as cluster set up goes, I usually launch spot instances with the spark-ec2 scripts, and then check out a repo which contains a simple driver application for my code. Then I have something crude like bash scripts running my program and collecting output. 

You could have a look at the spark-perf repo if you want something a little better principled/automatic. 

- Evan

te:


n3.nabble.com/What-is-the-best-way-to-build-my-developing-Spark-for-testing-on-EC2-tp8638.html
com.

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 03 Oct 2014 11:01:57 +0800",Re: HiveContext: cache table not supported for partitioned table?,"Du Li <lidu@yahoo-inc.com.INVALID>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Cache table works with partitioned table.

I guess youâ€™re experimenting with a default local metastore and the 
metastore_db directory doesnâ€™t exist at the first place. In this case, 
all metastore tables/views donâ€™t exist at first and will throw the error 
message you saw when the |PARTITIONS| metastore table is accessed for 
the first time by Hive client. However, you should also see this line 
before this error:

    14/10/03 10:51:30 ERROR ObjectStore: Direct SQL failed, falling back
    to ORM

And then the table is created on the fly. The cache operation is also 
performed normally. You can verify this by selecting it and check the 
Spark UI for cached RDDs. If you try to uncache the table and cache it 
again, you wonâ€™t see this error any more.

Normally, in production environment you wonâ€™t see this error because 
metastore database is usually setup ahead of time.



â€‹
"
Priya Ch <learnings.chitturi@gmail.com>,"Fri, 3 Oct 2014 16:52:44 +0530",Breeze Library usage in Spark,"user@spark.apache.org, dev@spark.apache.org","Hi Team,

When I am trying to use DenseMatrix of breeze library in spark, its
throwing me the following error:

java.lang.noclassdeffounderror: breeze/storage/Zero


Can someone help me on this ?

Thanks,
Padma Ch
"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 3 Oct 2014 09:45:41 -0700",Re: Breeze Library usage in Spark,Priya Ch <learnings.chitturi@gmail.com>,"Did you add a different version of breeze to the classpath? In Spark
1.0, we use breeze 0.7, and in Spark 1.1 we use 0.9. If the breeze
version you used is different from the one comes with Spark, you might
see class not found. -Xiangrui


---------------------------------------------------------------------


"
Du Li <lidu@yahoo-inc.com.INVALID>,"Fri, 3 Oct 2014 17:12:34 +0000",Re: HiveContext: cache table not supported for partitioned table?,Cheng Lian <lian.cs.zju@gmail.com>,"Thanks for your explanation.

From: Cheng Lian <lian.cs.zju@gmail.com<mailto:lian.cs.zju@gmail.com>>
Date: Thursday, October 2, 2014 at 8:01 PM
To: Du Li <lidu@yahoo-inc.com.INVALID<mailto:lidu@yahoo-inc.com.INVALID>>, ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Cc: ""user@spark.apache.org<mailto:user@spark.apache.org>"" <user@spark.apache.org<mailto:user@spark.apache.org>>
Subject: Re: HiveContext: cache table not supported for partitioned table?


Cache table works with partitioned table.

I guess youâ€™re experimenting with a default local metastore and the metastore_db directory doesnâ€™t exist at the first place. In this case, all metastore tables/views donâ€™t exist at first and will throw the error message you saw when the PARTITIONS metastore table is accessed for the first time by Hive client. However, you should also see this line before this error:

14/10/03 10:51:30 ERROR ObjectStore: Direct SQL failed, falling back to ORM

And then the table is created on the fly. The cache operation is also performed normally. You can verify this by selecting it and check the Spark UI for cached RDDs. If you try to uncache the table and cache it again, you wonâ€™t see this error any more.

Normally, in production environment you wonâ€™t see this error because metastore database is usually setup ahead of time.

On 10/3/14 3:39 AM, Du Li wrote:

Hi,

In Spark 1.1 HiveContext, I ran a create partitioned table command followed by a cache table command and got a java.sql.SQLSyntaxErrorException: Table/View 'PARTITIONS' does not exist. But cache table worked fine if the table is not a partitioned table.

Can anybody confirm that cache of partitioned table is not supported yet in current version?

Thanks,
Du

â€‹
"
David Hall <dlwh@cs.berkeley.edu>,"Fri, 3 Oct 2014 10:31:19 -0700",Re: Breeze Library usage in Spark,Xiangrui Meng <mengxr@gmail.com>,"yeah, breeze.storage.Zero was introduced in either 0.8 or 0.9.


"
shane knapp <sknapp@berkeley.edu>,"Fri, 3 Oct 2014 10:51:13 -0700",emergency jenkins restart -- massive security patch released,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","https://wiki.jenkins-ci.org/display/SECURITY/Jenkins+Security+Advisory+2014-10-01

there's some pretty big stuff that's been identified and we need to get
this upgraded asap.

i'll be killing off what's currently running, and will retrigger them all
once we're done.

sorry for the inconvenience.

shane
"
shane knapp <sknapp@berkeley.edu>,"Fri, 3 Oct 2014 10:54:17 -0700",Re: emergency jenkins restart -- massive security patch released,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","update complete.  i'm retriggering builds now.


"
Priya Ch <learnings.chitturi@gmail.com>,"Fri, 3 Oct 2014 23:34:14 +0530",Fwd: Breeze Library usage in Spark,"user@spark.apache.org, dev@spark.apache.org","yes. I have included breeze-0.9 in build.sbt file. I ll change this to 0.7.
Apart from this, do we need to include breeze jars explicitly in the spark
context as sc.addJar() ? and what about the dependencies
netlib-native_ref-linux-
      x86_64-1.1-natives.jar,
netlib-native_system-linux-x86_64-1.1-natives.jar ? Need to be included in
classpath ?


"
Cody Koeninger <cody@koeninger.org>,"Fri, 3 Oct 2014 15:33:40 -0500",Parquet schema migrations,"""dev@spark.apache.org"" <dev@spark.apache.org>","Wondering if anyone has thoughts on a path forward for parquet schema
migrations, especially for people (like us) that are using raw parquet
files rather than Hive.

So far we've gotten away with reading old files, converting, and writing to
new directories, but that obviously becomes problematic above a certain
data size.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 3 Oct 2014 20:20:50 -0400",Re: EC2 clusters ready in launch time + 30 seconds,"""Nate D'Amico"" <nate@reactor8.com>","FYI: There is an existing issue -- SPARK-3314
<https://issues.apache.org/jira/browse/SPARK-3314> -- about scripting the
creation of Spark AMIs.

With Packer, it looks like we may be able to script the creation of
multiple image types (VMWare, GCE, AMI, Docker, etc...) at once from a
single Packer template. That's very cool.

I'll be looking into this.

Nick


m

ed
bit
n
ide
d,
ful
but
 to
n
ture
ll
d
son
€™s what
 in the
"
DB Tsai <dbtsai@dbtsai.com>,"Sat, 4 Oct 2014 07:27:12 +0200",Re: Fwd: Breeze Library usage in Spark,Priya Ch <learnings.chitturi@gmail.com>,"You dont have to include breeze jar which is already in spark assembly jar.
For native one, its optional.

Sent from my Google Nexus 5

"
Patrick Wendell <pwendell@gmail.com>,"Fri, 3 Oct 2014 23:49:00 -0700",Re: EC2 clusters ready in launch time + 30 seconds,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey All,

Just a couple notes. I recently posted a shell script for creating the
AMI's from a clean Amazon Linux AMI.

https://github.com/mesos/spark-ec2/blob/v3/create_image.sh

I think I will update the AMI's soon to get the most recent security
updates. For spark-ec2's purpose this is probably sufficient (we'll
only need to re-create them every few months).

However, it would be cool if someone wanted to tackle providing a more
general mechanism for defining Spark-friendly ""images"" that can be
used more generally. I had thought that docker might be a good way to
go for something like this - but maybe this packer thing is good too.

For one thing, if we had a standard image we could use it to create
containers for running Spark's unit test, which would be really cool.
This would help a lot with random issues around port and filesystem
contention we have for unit tests.

I'm not sure if the long term place for this would be inside the spark
codebase or a community library or what. But it would definitely be
very valuable to have if someone wanted to take it on.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 4 Oct 2014 10:28:08 -0400",Re: EC2 clusters ready in launch time + 30 seconds,Patrick Wendell <pwendell@gmail.com>,"Thanks for posting that script, Patrick. It looks like a good place to
start.

Regarding Docker vs. Packer, as I understand it you can use Packer to
create Docker containers at the same time as AMIs and other image types.

Nick



"
tomo cocoa <cocoatomo77@gmail.com>,"Sun, 5 Oct 2014 04:10:43 +0900",What versions of Hadoop Spark supports?,dev <dev@spark.apache.org>,"Hi,

I reported this issue (https://issues.apache.org/jira/browse/SPARK-3794)
about compilation error of spark core.
This error depends on a Hadoop version, and problematic versions are
1.1.1--2.2.0.

At first, we should argue about what versions of Hadoop Spark supports.
If we decide to omit a support for those versions, things are so simple and
no modification is needed.
Otherwise, we should be careful to use only commons-io 2.1.


Regards,
cocoatomo

-- 
class Cocoatomo:
    name = 'cocoatomo'
    email_address = 'cocoatomo77@gmail.com'
    twitter_id = '@cocoatomo'
"
Robert C Senkbeil <rcsenkbe@us.ibm.com>,"Sun, 5 Oct 2014 12:16:25 -0500",Jython importing pyspark?,dev@spark.apache.org,"

Hi there,

I wanted to ask whether or not anyone has successfully used Jython with the
pyspark library. I wasn't sure if the C extension support was needed for
pyspark itself or was just a bonus of using Cython.

There was a claim (
http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html#a7269
) that using Jython would be better - if you didn't need C extension
support - because the cost of serialization is lower. However, I have not
been able to import pyspark into a Jython session. I'm using version 2.7b3
of Jython and version 1.1.0 of Spark for reference.

Jython 2.7b3 (default:e81256215fb0, Aug 4 2014, 02:39:51)
[Java HotSpot(TM) 64-Bit Server VM (Oracle Corporation)] on java1.7.0_51
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
Traceback (most recent call last):
  File ""<stdin>"", line 1, in <module>
  File ""pyspark/__init__.py"", line 63, in <module>
  File ""pyspark/context.py"", line 25, in <module>
  File ""pyspark/accumulators.py"", line 94, in <module>
  File ""pyspark/serializers.py"", line 341, in <module>
  File ""pyspark/serializers.py"", line 328, in _hijack_namedtuple
RuntimeError: maximum recursion depth exceeded (Java StackOverflowError)

Is there something I am missing with this? Did Jython ever work for
pyspark? The same error happens regardless of whether I use the Python
files or compile them down to Java class files using Jython first.

I know that previous documentation (0.9.1) indicated, ""PySpark requires
Python 2.6 or higher. PySpark applications are executed using a standard
CPython interpreter in order to support Python modules that use C
extensions. We have not tested PySpark with Python 3 or with alternative
Python interpreters, such as PyPy or Jython.""

In later versions, it now reflects, ""Spark 1.1.0 works with Python 2.6 or
higher (but not Python 3). It uses the standard CPython interpreter, so C
libraries like NumPy can be used.""

I'm assuming this means that attempts to use other interpreters failed. If
so, are there any plans to support something like Jython in the future?

Signed,
Chip Senkbeil"
Tom Hubregtsen <thubregtsen@gmail.com>,"Sun, 5 Oct 2014 13:58:16 -0700 (PDT)",Impact of input format on timing,dev@spark.incubator.apache.org,"Hi,

I ran the same version of a program with two different types of input
containing equivalent information. 
Program 1: 10,000 files with on average 50 IDs, one every line

My program takes the input, creates key/value pairs of them, and performs
about 7 more steps. I have compared the sets of key/value pairs after the
initialization phase, and they are equivalent. All other steps are equal.
The only difference is using wholeTextFile versus textFile and the
initialization input map function itself.  

Since I am reading in from HDFS, and the minimum part size there is 64MB,
every file in program 1 will take 64MB, even though they are KBs original.
Because of this, I expected program 2 to be faster, or equivalent as the
initialization phase may be neglectable. 

In my first comparison, I provided the program with sufficient memory, and
they both take around 2 minutes. No surprises here.

In my second comparison, I limit the memory to in this case 4 GB. Program 1
executes in a little over 4 minutes, but program 2 takes over 15 minutes
(after which I terminated the program, as I see it is getting there, but
spilling massively in every stage). The difference between the two runs is
the amount of spilling in phases later on in the program (*not* in the
initialization phase). Program 1 spills 2 chunks per stage:
map of 327 MB to disk (1 time so far)
map of 328 MB to disk (1 time so far)
Program 2 also spills these 2 chunks:
map of 320 MB to disk (1 time so far)
map of 323 MB to disk (1 time so far)
But then spills 2 * ~15,000 chuncks of <1MB:
map of 0 MB to disk (15561 time so far)
map of 0 MB to disk (13866 times so far)

I understand that RDDs with narrow dependencies to their parents are
pipelined, and form a stage. These all point to the parent RDD. This parent
RDD will have different partitioning and different data placement between
the two programs. Because of this, I did expect a difference in this stage,
but as we change from JavaRDD to JavaPairRDD and do a reduceByKey after the
initialization, I expected this difference to be gone in the next stages.
Can anyone explain this behavior, or point me in a direction?

Thanks in advance!




--

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 5 Oct 2014 15:07:17 -0700",Re: Impact of input format on timing,Tom Hubregtsen <thubregtsen@gmail.com>,"Hi Tom,

HDFS and Spark don't actually have a minimum block size -- so in that first dataset, the files won't each be costing you 64 MB. However, the main reason for difference in performance here is probably the number of RDD partitions. In the first case, Spark will create an RDD with 10000 partitions, one per file, while in the second case it will likely have only 1-2 of them. The number of partitions affects the level of parallelism of operations like reduceByKey (by default, reduceByKey uses as many partitions as the parent RDD it runs on), and in this case, I think it's causing reduceByKey to spill to disk within tasks in the second case and not in the first case, because each task has more input data.

You can see the number of partitions in each stage of your computation on the application web UI at http://<driver>:4040 if you want to confirm this. Also, for both programs, you can manually set the number of partitions for the reduce by passing a second argument to reduceByKey (e.g. reduceByKey(myFunc, 100)). In general it's best to choose this so that the input data for each reduce task fits in memory to avoid spilling.

Matei


performs
the
equal.
64MB,
original.
the
and
Program 1
minutes
but
runs is
in-memory
in-memory
in-memory
in-memory
in-memory
in-memory
parent
between
stage,
after the
stages.
http://apache-spark-developers-list.1001551.n3.nabble.com/Impact-of-input-format-on-timing-tp8655.html
Nabble.com.


---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Sun, 5 Oct 2014 18:58:54 -0400",Re: Parquet schema migrations,Cody Koeninger <cody@koeninger.org>,"Hi Cody,

I wasn't aware there were different versions of the parquet format.  What's
the difference between ""raw parquet"" and the Hive-written parquet files?

As for your migration question, the approaches I've often seen are
convert-on-read and convert-all-at-once.  Apache Cassandra for example does
both -- when upgrading between Cassandra versions that change the on-disk
sstable format, it will do a convert-on-read as you access the sstables, or
you can run the upgradesstables command to convert them all at once
post-upgrade.

Andrew


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 5 Oct 2014 16:13:06 -0700",Re: Jython importing pyspark?,Robert C Senkbeil <rcsenkbe@us.ibm.com>,"PySpark doesn't attempt to support Jython at present. IMO while it might be a bit faster, it would lose a lot of the benefits of Python, which are the very strong data processing libraries (NumPy, SciPy, Pandas, etc). So I'm not sure it's worth supporting unless someone demonstrates a really major performance benefit.

There was actually a recent patch to add PyPy support (https://github.com/apache/spark/pull/2144), which is worth a try if you want Python applications to run faster. It might actually be faster overall than Jython.

Matei


with the
for
http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html#a7269
not
2.7b3
java1.7.0_51
StackOverflowError)
requires
standard
alternative
or
so C
failed. If
future?


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Sun, 5 Oct 2014 17:36:07 -0700",Re: Parquet schema migrations,Andrew Ash <andrew@andrewash.com>,"Hi Cody,

Assuming you are talking about 'safe' changes to the schema (i.e. existing
column names are never reused with incompatible types), this is something
I'd love to support.  Perhaps you can describe more what sorts of changes
you are making, and if simple merging of the schemas would be sufficient.
If so, we can open a JIRA, though I'm not sure when we'll have resources to
dedicate to this.

In the near term, I'd suggest writing converters for each version of the
schema, that translate to some desired master schema.  You can then union
all of these together and avoid the cost of batch conversion.  It seems
like in most cases this should be pretty efficient, at least now that we
have good pushdown past union operators :)

Michael


"
Lochana Menikarachchi <lochanac@gmail.com>,"Mon, 06 Oct 2014 07:58:18 +0530",Hyper Parameter Tuning Algorithms,dev@spark.apache.org,"Found this thread from April..

http://mail-archives.apache.org/mod_mbox/spark-user/201404.mbox/%3CCABjXkq6b7SfAxie4+AqTCmD8jSqBZnsxSFw6V5o0WWWouOBbCw@mail.gmail.com%3E

Wondering what the status of this.. We are thinking about implementing 
these algorithms.. Would be a waste if they are already available?

Please advice.

Thanks.

Lochana

---------------------------------------------------------------------


"
Soumitra Kumar <kumar.soumitra@gmail.com>,"Sun, 5 Oct 2014 20:40:48 -0700 (PDT)",Re: SPARK-3660 : Initial RDD for updateStateByKey transformation,dev@spark.apache.org,"Hello,

I have submitted a pull request (Adding support of initial value for state update. #2665), please review and let me know.

Excited to submit my first pull request.

-Soumitra.


Thanks TD for relevant pointers.

I have created an issue :
https://issues.apache.org/jira/browse/SPARK-3660

Copying the description from JIRA:
""
How to initialize state tranformation updateStateByKey?

I have word counts from previous spark-submit run, and want to load that in next spark-submit job to start counting over that.

initial : Option [RDD [(K, S)]] = None

This will maintain the backward compatibility as well.

I have a working code as well.

This thread started on spark-user list at:
http://apache-spark-user-list.1001560.n3.nabble.com/How-to-initialize-updateStateByKey-operation-td14772.html
""

Please let me know if I shall add a parameter ""initial : Option [RDD [(K, S)]] = None"" to all updateStateByKey methods or create new ones?

Thanks,
-Soumitra.

---------------------------------------------------------------------


"
"""=?utf-8?B?VHJpZGVudA==?="" <cwk32@vip.qq.com>","Mon, 6 Oct 2014 14:00:29 +0800",Too big data Spark SQL on Hive table on version 1.0.2 has some strange output,"""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","Dear Developers,

I'm limited in using Spark 1.0.2 currently.

I use Spark SQL on Hive table to load amplab benchmark, which is  25.6GiB approximately.

I run:
CREATE EXTERNAL TABLE uservisits (sourceIP STRING,destURL STRING, visitDate STRING,adRevenue DOUBLE,userAgent STRING,countryCode STRING, languageCode STRING,searchWord STRING,duration INT ) ROW FORMAT DELIMITED FIELDS TERMINATED BY ""\001"" STORED AS SEQUENCEFILE LOCATION ""/public/xxxx/data/uservisits""â€

okay!

I run:
SELECT COUNT(*) from uservisitsâ€

okay! the result is correct

but when I run:
SELECT SUBSTR(sourceIP, 1, 8), SUM(adRevenue) FROM uservisits GROUP BY SUBSTR(sourceIP, 1, 8)â€

There are some error messages (i stronger and underline some important message)

mainly two problems:

akka => Timed out
GC => Out of memory

what should I do?

...
14/10/05 23:45:18 INFO MemoryStore: Block broadcast_2 of size 158188 dropped from memory (free 308752285)
14/10/05 23:45:40 ERROR BlockManagerMaster: Failed to remove shuffle 4
akka.pattern.AskTimeoutException: Timed out
    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334)
    at akka.actor.Scheduler$$anon$11.run(Scheduler.scala:118)
    at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
    at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:455)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.executeBucket$1(Scheduler.scala:407)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:411)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:45:47 ERROR BlockManagerMaster: Failed to remove shuffle 0
akka.pattern.AskTimeoutException: Timed out
    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334)
    at akka.actor.Scheduler$$anon$11.run(Scheduler.scala:118)
    at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
    at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:455)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.executeBucket$1(Scheduler.scala:407)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:411)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:45:46 ERROR BlockManagerMaster: Failed to remove shuffle 2
akka.pattern.AskTimeoutException: Timed out
    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334)
    at akka.actor.Scheduler$$anon$11.run(Scheduler.scala:118)
    at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
    at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:455)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.executeBucket$1(Scheduler.scala:407)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:411)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:45:45 ERROR BlockManagerMaster: Failed to remove shuffle 1
akka.pattern.AskTimeoutException: Timed out
    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334)
    at akka.actor.Scheduler$$anon$11.run(Scheduler.scala:118)
    at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
    at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:455)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.executeBucket$1(Scheduler.scala:407)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:411)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:45:40 ERROR BlockManagerMaster: Failed to remove shuffle 3
akka.pattern.AskTimeoutException: Timed out
    at akka.pattern.PromiseActorRef$$anonfun$1.apply$mcV$sp(AskSupport.scala:334)
    at akka.actor.Scheduler$$anon$11.run(Scheduler.scala:118)
    at scala.concurrent.Future$InternalCallbackExecutor$.scala$concurrent$Future$InternalCallbackExecutor$$unbatchedExecute(Future.scala:694)
    at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:691)
    at akka.actor.LightArrayRevolverScheduler$TaskHolder.executeTask(Scheduler.scala:455)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.executeBucket$1(Scheduler.scala:407)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.nextTick(Scheduler.scala:411)
    at akka.actor.LightArrayRevolverScheduler$$anon$12.run(Scheduler.scala:363)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:46:31 ERROR Executor: Exception in task ID 5280
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at org.apache.spark.sql.catalyst.expressions.SumFunction.<init>(aggregates.scala:351)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:243)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:230)
    at org.apache.spark.sql.execution.Aggregate.org$apache$spark$sql$execution$Aggregate$$newAggregateBuffer(Aggregate.scala:99)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:163)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:46:53 ERROR Executor: Exception in task ID 5270
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at java.util.Arrays.copyOfRange(Arrays.java:3664)
    at java.lang.String.<init>(String.java:201)
    at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567)
    at java.nio.CharBuffer.toString(CharBuffer.java:1241)
    at org.apache.hadoop.io.Text.decode(Text.java:350)
    at org.apache.hadoop.io.Text.decode(Text.java:327)
    at org.apache.hadoop.io.Text.toString(Text.java:254)
    at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveJavaObject(LazyStringObjectInspector.java:52)
    at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveJavaObject(LazyStringObjectInspector.java:28)
    at org.apache.spark.sql.hive.HiveInspectors$class.unwrapData(hiveUdfs.scala:287)
    at org.apache.spark.sql.hive.execution.HiveTableScan.unwrapData(HiveTableScan.scala:48)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$attributeFunctions$1$$anonfun$apply$3.apply(HiveTableScan.scala:101)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$attributeFunctions$1$$anonfun$apply$3.apply(HiveTableScan.scala:99)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$12$$anonfun$apply$5.apply(HiveTableScan.scala:203)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$12$$anonfun$apply$5.apply(HiveTableScan.scala:200)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:159)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
14/10/05 23:47:03 INFO ShuffleBlockManager: Deleted all files for shuffle 4
14/10/05 23:47:05 INFO ShuffleBlockManager: Deleted all files for shuffle 1
14/10/05 23:47:04 INFO ShuffleBlockManager: Deleted all files for shuffle 2
14/10/05 23:47:03 INFO ShuffleBlockManager: Deleted all files for shuffle 3
14/10/05 23:47:03 INFO ShuffleBlockManager: Deleted all files for shuffle 0
14/10/05 23:47:09 INFO TaskSetManager: Starting task 14.0:32 as TID 5290 on executor localhost: localhost (PROCESS_LOCAL)
14/10/05 23:47:17 ERROR ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-92,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at java.util.Arrays.copyOfRange(Arrays.java:3664)
    at java.lang.String.<init>(String.java:201)
    at java.nio.HeapCharBuffer.toString(HeapCharBuffer.java:567)
    at java.nio.CharBuffer.toString(CharBuffer.java:1241)
    at org.apache.hadoop.io.Text.decode(Text.java:350)
    at org.apache.hadoop.io.Text.decode(Text.java:327)
    at org.apache.hadoop.io.Text.toString(Text.java:254)
    at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveJavaObject(LazyStringObjectInspector.java:52)
    at org.apache.hadoop.hive.serde2.lazy.objectinspector.primitive.LazyStringObjectInspector.getPrimitiveJavaObject(LazyStringObjectInspector.java:28)
    at org.apache.spark.sql.hive.HiveInspectors$class.unwrapData(hiveUdfs.scala:287)
    at org.apache.spark.sql.hive.execution.HiveTableScan.unwrapData(HiveTableScan.scala:48)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$attributeFunctions$1$$anonfun$apply$3.apply(HiveTableScan.scala:101)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$attributeFunctions$1$$anonfun$apply$3.apply(HiveTableScan.scala:99)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$12$$anonfun$apply$5.apply(HiveTableScan.scala:203)
    at org.apache.spark.sql.hive.execution.HiveTableScan$$anonfun$12$$anonfun$apply$5.apply(HiveTableScan.scala:200)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:159)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
14/10/05 23:47:22 ERROR Executor: Exception in task ID 5267
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:164)
    at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45)
    at scala.collection.SeqLike$$anonfun$distinct$1.apply(SeqLike.scala:495)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at scala.collection.SeqLike$class.distinct(SeqLike.scala:493)
    at scala.collection.AbstractSeq.distinct(Seq.scala:40)
    at org.apache.spark.sql.catalyst.expressions.Coalesce.resolved$lzycompute(nullFunctions.scala:34)
    at org.apache.spark.sql.catalyst.expressions.Coalesce.resolved(nullFunctions.scala:34)
    at org.apache.spark.sql.catalyst.expressions.Coalesce.dataType(nullFunctions.scala:38)
    at org.apache.spark.sql.catalyst.expressions.Expression.n2(Expression.scala:100)
    at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:58)
    at org.apache.spark.sql.catalyst.expressions.MutableLiteral.update(literals.scala:72)
    at org.apache.spark.sql.catalyst.expressions.SumFunction.update(aggregates.scala:358)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:169)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:47:17 ERROR ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-100,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at org.apache.spark.sql.catalyst.expressions.SumFunction.<init>(aggregates.scala:351)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:243)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:230)
    at org.apache.spark.sql.execution.Aggregate.org$apache$spark$sql$execution$Aggregate$$newAggregateBuffer(Aggregate.scala:99)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:163)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:47:31 INFO TaskSetManager: Serialized task 14.0:32 as 4996 bytes in 13419 ms
14/10/05 23:47:52 ERROR Executor: Exception in task ID 5288
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at org.apache.spark.sql.catalyst.expressions.SumFunction.<init>(aggregates.scala:353)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:243)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:230)
    at org.apache.spark.sql.execution.Aggregate.org$apache$spark$sql$execution$Aggregate$$newAggregateBuffer(Aggregate.scala:99)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:163)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:47:52 INFO Executor: Running task ID 5290
14/10/05 23:47:52 ERROR ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-97,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:164)
    at scala.collection.mutable.ListBuffer.$plus$eq(ListBuffer.scala:45)
    at scala.collection.SeqLike$$anonfun$distinct$1.apply(SeqLike.scala:495)
    at scala.collection.immutable.List.foreach(List.scala:318)
    at scala.collection.SeqLike$class.distinct(SeqLike.scala:493)
    at scala.collection.AbstractSeq.distinct(Seq.scala:40)
    at org.apache.spark.sql.catalyst.expressions.Coalesce.resolved$lzycompute(nullFunctions.scala:34)
    at org.apache.spark.sql.catalyst.expressions.Coalesce.resolved(nullFunctions.scala:34)
    at org.apache.spark.sql.catalyst.expressions.Coalesce.dataType(nullFunctions.scala:38)
    at org.apache.spark.sql.catalyst.expressions.Expression.n2(Expression.scala:100)
    at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:58)
    at org.apache.spark.sql.catalyst.expressions.MutableLiteral.update(literals.scala:72)
    at org.apache.spark.sql.catalyst.expressions.SumFunction.update(aggregates.scala:358)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:169)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
14/10/05 23:47:52 INFO TaskSetManager: Starting task 14.0:33 as TID 5291 on executor localhost: localhost (PROCESS_LOCAL)
14/10/05 23:47:52 ERROR ExecutorUncaughtExceptionHandler: Uncaught exception in thread Thread[Executor task launch worker-91,5,main]
java.lang.OutOfMemoryError: GC overhead limit exceeded
    at org.apache.spark.sql.catalyst.expressions.SumFunction.<init>(aggregates.scala:353)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:243)
    at org.apache.spark.sql.catalyst.expressions.Sum.newInstance(aggregates.scala:230)
    at org.apache.spark.sql.execution.Aggregate.org$apache$spark$sql$execution$Aggregate$$newAggregateBuffer(Aggregate.scala:99)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:163)
    at org.apache.spark.sql.execution.Aggregate$$anonfun$execute$1$$anonfun$7.apply(Aggregate.scala:153)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:158)
    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
    at org.apache.spark.scheduler.Task.run(Task.scala:51)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
    at java.lang.Thread.run(Thread.java:745)
â€
â€"
Fairiz Azizi <coderfi@gmail.com>,"Sun, 5 Oct 2014 23:19:04 -0700",Spark on Mesos 0.20,dev@spark.apache.org,"The Spark online docs indicate that Spark is compatible with Mesos 0.18.1

I've gotten it to work just fine on 0.18.1 and 0.18.2

Has anyone tried Spark on a newer version of Mesos, i.e. Mesos v0.20.0?

-Fi
"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Mon, 06 Oct 2014 08:44:52 +0200",Re: Spark on Mesos 0.20,"Fairiz Azizi <coderfi@gmail.com>, dev@spark.apache.org","Yeah we are using Spark 1.1.0 with Mesos 0.20.1. It runs fine in coarse
mode, in fine grain mode there is an issue with blockmanager names
conflict. I have been waiting for it to be fixed but it is still there.

-Gurvinder

---------------------------------------------------------------------


"
Andrew Ash <andrew@andrewash.com>,"Mon, 6 Oct 2014 02:58:24 -0400",Re: Spark on Mesos 0.20,Gurvinder Singh <gurvinder.singh@uninett.no>,"Hi Gurvinder,

Is there a SPARK ticket tracking the issue you describe?


"
Timothy Chen <tnachen@gmail.com>,"Mon, 6 Oct 2014 00:30:13 -0700",Re: Spark on Mesos 0.20,Gurvinder Singh <gurvinder.singh@uninett.no>,"Hi Gurvinder,

I tried fine grain mode before and didn't get into that problem.



---------------------------------------------------------------------


"
Timothy Chen <tnachen@gmail.com>,"Mon, 6 Oct 2014 00:30:45 -0700",Re: Spark on Mesos 0.20,Gurvinder Singh <gurvinder.singh@uninett.no>,"(Hit enter too soon...)

What is your setup and steps to repro this?

Tim


---------------------------------------------------------------------


"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Mon, 06 Oct 2014 09:50:19 +0200",Re: Spark on Mesos 0.20,Timothy Chen <tnachen@gmail.com>,"The issue does not occur if the task at hand has small number of map
tasks. I have a task which has 978 map tasks and I see this error as

14/10/06 09:34:40 ERROR BlockManagerMasterActor: Got two different block
manager registrations on 20140711-081617-711206558-5050-2543-5

Here is the log from the mesos-slave where this container was running.

http://pastebin.com/Q1Cuzm6Q

If you look for the code from where error produced by spark, you will
see that it simply exit and saying in comments ""this should never
happen, lets just quit"" :-)

- Gurvinder


---------------------------------------------------------------------


"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Mon, 06 Oct 2014 10:27:06 +0200",TorrentBroadcast slow performance,dev@spark.apache.org,"Hi,

I've had no answer to this on user@spark.apache.org, so I post it on dev before 
filing a JIRA (in case the problem or solution is already identified)

We've had some performance issues since switching to 1.1.0, and we finally found 
the origin : TorrentBroadcast seems to be very slow in our setting (and it 
became default with 1.1.0)

The logs of a 4MB variable with TorrentBroadcast : (15s)

14/10/01 15:47:13 INFO storage.MemoryStore: Block broadcast_84_piece1 stored as 
bytes in memory (estimated size 171.6 KB, free 7.2 GB)
14/10/01 15:47:13 INFO storage.BlockManagerMaster: Updated info of block 
broadcast_84_piece1
14/10/01 15:47:23 INFO storage.MemoryStore: ensureFreeSpace(4194304) called with 
curMem=1401611984, maxMem=9168696115
14/10/01 15:47:23 INFO storage.MemoryStore: Block broadcast_84_piece0 stored as 
bytes in memory (estimated size 4.0 MB, free 7.2 GB)
14/10/01 15:47:23 INFO storage.BlockManagerMaster: Updated info of block 
broadcast_84_piece0
14/10/01 15:47:23 INFO broadcast.TorrentBroadcast: Reading broadcast variable 84 
took 15.202260006 s
14/10/01 15:47:23 INFO storage.MemoryStore: ensureFreeSpace(4371392) called with 
curMem=1405806288, maxMem=9168696115
14/10/01 15:47:23 INFO storage.MemoryStore: Block broadcast_84 stored as values 
in memory (estimated size 4.2 MB, free 7.2 GB)

(notice that a 10s lag happens after the ""Updated info of block broadcast_..."" 
and before the MemoryStore log

And with HttpBroadcast (0.3s):

14/10/01 16:05:58 INFO broadcast.HttpBroadcast: Started reading broadcast 
variable 147
14/10/01 16:05:58 INFO storage.MemoryStore: ensureFreeSpace(4369376) called with 
curMem=1373493232, maxMem=9168696115
14/10/01 16:05:58 INFO storage.MemoryStore: Block broadcast_147 stored as values 
in memory (estimated size 4.2 MB, free 7.3 GB)
14/10/01 16:05:58 INFO broadcast.HttpBroadcast: Reading broadcast variable 147 
took 0.320907112 s 14/10/01 16:05:58 INFO storage.BlockManager: Found block 
broadcast_147 locally

Since Torrent is supposed to perform much better than Http, we suspect a 
configuration error from our side, but are unable to pin it down. Does someone 
have any idea of the origin of the problem ?

For now we're sticking with the HttpBroadcast workaround.

Guillaume
-- 
eXenSa

	
*Guillaume PITEL, PrÃ©sident*
+33(0)626 222 431

eXenSa S.A.S. <http://www.exensa.com/>
41, rue PÃ©rier - 92120 Montrouge - FRANCE
Tel +33(0)184 163 677 / Fax +33(0)972 283 705

"
RJ Nowling <rnowling@gmail.com>,"Mon, 6 Oct 2014 09:08:23 -0400",Re: Spark on Mesos 0.20,Gurvinder Singh <gurvinder.singh@uninett.no>,"I've recently run into this issue as well. I get it from running Spark
examples such as log query.  Maybe that'll help reproduce the issue.



-- 
em rnowling@gmail.com
c 954.496.2314
"
Cody Koeninger <cody@koeninger.org>,"Mon, 6 Oct 2014 09:07:20 -0500",Re: Parquet schema migrations,Michael Armbrust <michael@databricks.com>,"Sorry, by ""raw parquet"" I just meant there is no external metadata store,
only the schema written as part of the parquet format.

We've done several different kinds of changes, including column rename and
widening the data type of an existing column.  I don't think it's feasible
to support those.

The kind of change we've made that it probably makes most sense to support
is adding a nullable column. I think that also implies supporting
""removing"" a nullable column, as long as you don't end up with columns of
the same name but different type.

I'm not sure semantically that it makes sense to do schema merging as part
of union all, and definitely doesn't make sense to do it by default.  I
wouldn't want two accidentally compatible schema to get merged without
warning.  It's also a little odd since unlike a normal sql database union
all can happen before there are any projections or filters... e.g. what
order do columns come back in if someone does select *.

Seems like there should be either a separate api call, or an optional
argument to union all.

As far as resources go, I can probably put some personal time into this if
we come up with a plan that makes sense.



"
Timothy Chen <tnachen@gmail.com>,"Mon, 6 Oct 2014 09:20:49 -0700",Re: Spark on Mesos 0.20,RJ Nowling <rnowling@gmail.com>,"Ok I created SPARK-3817 to track this, will try to repro it as well.

Tim


---------------------------------------------------------------------


"
Daniil Osipov <daniil.osipov@shazam.com>,"Mon, 6 Oct 2014 13:48:40 -0700",Re: EC2 clusters ready in launch time + 30 seconds,Nicholas Chammas <nicholas.chammas@gmail.com>,"I've also been looking at this. Basically, the Spark EC2 script is
excellent for small development clusters of several nodes, but isn't
suitable for production. It handles instance setup in a single threaded
manner, while it can easily be parallelized. It also doesn't handle failure
well, ex when an instance fails to start or is taking too long to respond.

Our desire was to have an equivalent to Amazon EMR[1] API that would
trigger Spark jobs, including specified cluster setup. I've done some work
towards that end, and it would benefit from an updated AMI greatly.

Dan

[1]
http://docs.aws.amazon.com/ElasticMapReduce/latest/DeveloperGuide/emr-cli-commands.html


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 6 Oct 2014 17:14:20 -0400",Re: EC2 clusters ready in launch time + 30 seconds,Daniil Osipov <daniil.osipov@shazam.com>,"FYI: I've created SPARK-3821: Develop an automated way of creating Spark
images (AMI, Docker, and others)
<https://issues.apache.org/jira/browse/SPARK-3821>


"
David Rowe <davidrowe@gmail.com>,"Tue, 7 Oct 2014 10:40:28 +1100",Re: EC2 clusters ready in launch time + 30 seconds,Daniil Osipov <daniil.osipov@shazam.com>,"I agree with this - there is also the issue of different sized masters and
slaves, and numbers of executors for hefty machines (e.g. r3.8xlarges),
tagging of instances and volumes (we use this for cost attribution at my
workplace), and running in VPCs.

I think think it might be useful to take a layered approach: the first step
could be getting a good reliable image produced - Nick's ticket - then
doing some work on the launch script.

Regarding the EMR like service - I think I heard that AWS is planning to
add spark support to EMR, but as usual there's nothing firm until it's
released.



"
Fairiz Azizi <coderfi@gmail.com>,"Mon, 6 Oct 2014 17:07:25 -0700",Re: Spark on Mesos 0.20,Timothy Chen <tnachen@gmail.com>,"That's what great about Spark, the community is so active! :)

I compiled Mesos 0.20.1 from the source tarball.

Using the Mapr3 Spark 1.1.0 distribution from the Spark downloads page
 (spark-1.1.0-bin-mapr3.tgz).

I see no problems for the workloads we are trying.

However, the cluster is small (less than 100 cores across 3 nodes).

The workloads reads in just a few gigabytes from HDFS, via an ipython
notebook spark shell.

thanks,
Fi



Fairiz ""Fi"" Azizi


"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Mon, 6 Oct 2014 17:19:34 -0700 (PDT)","Re: What is the best way to build my developing Spark for testing
 on EC2?",dev@spark.incubator.apache.org,"Hi Evan,

Sorry for my replay late. And Thank you for your comment.


It's just as you thought.  I agree with you.


I overlooked this. I will give it a try.

best,



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Bill Bejeck <bbejeck@gmail.com>,"Mon, 6 Oct 2014 22:32:00 -0400",Pull Requests,dev@spark.apache.org,"the trunk?
"
Bill Bejeck <bbejeck@gmail.com>,"Mon, 6 Oct 2014 22:58:15 -0400",Re: Pull Requests,dev@spark.apache.org,"Can someone review patch #2309 (jira task SPARK-3178)

Thanks


"
Ameet Talwalkar <atalwalkar@gmail.com>,"Mon, 6 Oct 2014 22:36:03 -0700",Re: Hyper Parameter Tuning Algorithms,Lochana Menikarachchi <lochanac@gmail.com>,"Hi Lochana,

This post is also referring to the MLbase project I mentioned in my
previous email.  We have not open-sourced this work, but plan to do so.

Moreover, you might want to check out the following JIRA ticket
<https://issues.apache.org/jira/browse/SPARK-3530>that includes the design
doc for ML pipelines and parameters in MLlib.  This design will include
many of the ideas from our MLbase work.

-Ameet


"
RJ Nowling <rnowling@gmail.com>,"Tue, 7 Oct 2014 09:29:50 -0400",Re: Spark on Mesos 0.20,Fairiz Azizi <coderfi@gmail.com>,"I was able to reproduce it on a small 4 node cluster (1 mesos master and 3
mesos slaves) with relatively low-end specs.  As I said, I just ran the log
query examples with the fine-grained mesos mode.

Spark 1.1.0 and mesos 0.20.1.

Fairiz, could you try running the logquery example included with Spark and
see what you get?

Thanks!




-- 
em rnowling@gmail.com
c 954.496.2314
"
Debasish Das <debasish.das83@gmail.com>,"Tue, 7 Oct 2014 11:42:35 -0700",Local tests logging to log4j,dev <dev@spark.apache.org>,"Hi,

I have added some changes to ALS tests and I am re-running tests as:

mvn -Dhadoop.version=2.3.0-cdh5.1.0 -Phadoop-2.3 -Pyarn
-DwildcardSuites=org.apache.spark.mllib.recommendation.ALSSuite test

I have some INFO logs in the code which I want to see on my console. They
work fine if I add println.

I copied conf/log4j.properties.template to conf/log4j.properties

The options are:

log4j.rootCategory=INFO, console

log4j.appender.console=org.apache.log4j.ConsoleAppender

log4j.appender.console.target=System.err

I still don't see the INFO msgs on the console.

Any idea if I am setting up my log4j properties correctly ?

Thanks.

Deb
"
Davies Liu <davies@databricks.com>,"Tue, 7 Oct 2014 11:42:50 -0700",Re: TorrentBroadcast slow performance,Guillaume Pitel <guillaume.pitel@exensa.com>,"Could you create a JIRA for it? maybe it's a regression after
https://issues.apache.org/jira/browse/SPARK-3119.

We will appreciate that if you could tell how to reproduce it.

ed)
y
red
ed
red
ed
ed
e

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 7 Oct 2014 11:52:22 -0700",Re: TorrentBroadcast slow performance,Davies Liu <davies@databricks.com>,"Maybe there is a firewall issue that makes it slow for your nodes to connect through the IP addresses they're configured with. I see there's this 10 second pause between ""Updated info of block broadcast_84_piece1"" and ""ensureFreeSpace(4194304) called"" (where it actually receives the block). HTTP broadcast used only HTTP fetches from the executors to the driver, but TorrentBroadcast has connections between the executors themselves and between executors and the driver over a different port. Where are you running your driver app and nodes?

Matei


dev
identified)
finally
setting
stored
block
called
stored
block
called
as
broadcast
called
stored as
variable
Found
suspect a
Does


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 7 Oct 2014 20:24:04 +0100",Re: Local tests logging to log4j,Debasish Das <debasish.das83@gmail.com>,"What has worked for me is to bundle log4j.properties in the root of
the application's .jar file, since log4j will look for it there, and
configuring log4j will turn off Spark's default log4j configuration.

I don't think conf/log4j.properties is going to do anything by itself,
but -Dlog4j.configuration=/path/to/file should cause it read a config
file on the file system.

But for messing with a local build of Spark, just edit
core/src/main/resources/org/apache/spark/log4j-defaults.properties and
rebuild.

Yes I think your syntax is OK; here's some of mine where I turn off a
bunch of INFO messages:

log4j.rootLogger=INFO, stdout
log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.Target=System.out
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=%d{ISO8601} %-5p %c{1}:%L %m%n
log4j.logger.org.apache.hadoop=WARN
log4j.logger.org.apache.kafka=WARN
log4j.logger.kafka=WARN
log4j.logger.akka=WARN
log4j.logger.org.apache.spark=WARN
log4j.logger.org.apache.spark.storage.BlockManager=ERROR
log4j.logger.org.apache.zookeeper=WARN
log4j.logger.org.eclipse.jetty=WARN
log4j.logger.org.I0Itec.zkclient=WARN


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 7 Oct 2014 16:45:01 -0400",Re: Extending Scala style checks,Cheng Lian <lian.cs.zju@gmail.com>,"For starters, do we have a list of all the Scala style rules that are
currently not enforced automatically but are likely well-suited for
automation?

Let's put such a list together in a JIRA issue and work through
implementing them.

Nick


"
Debasish Das <debasish.das83@gmail.com>,"Tue, 7 Oct 2014 14:02:31 -0700",Re: Local tests logging to log4j,Sean Owen <sowen@cloudera.com>,"Thanks Sean...trying them out...


"
Fairiz Azizi <coderfi@gmail.com>,"Tue, 7 Oct 2014 17:36:42 -0700",Re: Spark on Mesos 0.20,RJ Nowling <rnowling@gmail.com>,"Sure, could you point me to the example?

The only thing I could find was
https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala

So do you mean running it like:
   MASTER=""mesos://xxxxxxx*:5050*"" ./run-example LogQuery

I tried that and I can see the job run and the tasks complete on the slave
nodes, but the client process seems to hang forever, it's probably a
different problem. BTW, only a dozen or so tasks kick off.

I actually haven't done much with Scala and Spark (it's been all python).

Fi



Fairiz ""Fi"" Azizi


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 7 Oct 2014 21:25:22 -0400",Unneeded branches/tags,dev <dev@spark.apache.org>,"Just curious: Are there branches and/or tags on the repo that we donâ€™t need
anymore?

What are the scala-2.9 and streaming branches for, for example? And do we
still need branches for older versions of Spark that we are not backporting
stuff to, like branch-0.5?

Nick
â€‹
"
Reynold Xin <rxin@databricks.com>,"Tue, 7 Oct 2014 18:27:45 -0700",Re: Unneeded branches/tags,Nicholas Chammas <nicholas.chammas@gmail.com>,"Those branches are no longer active. However, I don't think we can delete
branches from github due to the way ASF mirroring works. I might be wrong
there.



m

€™t need
ng
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 7 Oct 2014 22:52:13 -0700",Re: Unneeded branches/tags,Reynold Xin <rxin@databricks.com>,"Actually - weirdly - we can delete old tags and it works with the
mirroring. Nick if you put together a list of un-needed tags I can
delete them.


---------------------------------------------------------------------


"
"""Haopu Wang"" <HWang@qilinsoft.com>","Wed, 8 Oct 2014 14:04:55 +0800",RE: Spark SQL question: why build hashtable for both sides in HashOuterJoin?,"""Liquan Pei"" <liquanpei@gmail.com>","Liquan, yes, for full outer join, one hash table on both sides is more efficient.

 

For the left/right outer join, it looks like one hash table should be enought.

 

________________________________

From: Liquan Pei [mailto:liquanpei@gmail.com] 
Sent: 2014Äê9ÔÂ30ÈÕ 18:34
To: Haopu Wang
Cc: dev@spark.apache.org; user
Subject: Re: Spark SQL question: why build hashtable for both sides in HashOuterJoin?

 

Hi Haopu,

 

case. 

 

Liquan

 


Hi, Liquan, thanks for the response.

 

In your example, I think the hash table should be built on the ""right"" side, so Spark can iterate through the left side and find matches in the right side from the hash table efficiently. Please comment and suggest, thanks again!

 

________________________________

From: Liquan Pei [mailto:liquanpei@gmail.com] 
Sent: 2014Äê9ÔÂ30ÈÕ 12:31
To: Haopu Wang
Cc: dev@spark.apache.org; user
Subject: Re: Spark SQL question: why build hashtable for both sides in HashOuterJoin?

 

Hi Haopu,

 

My understanding is that the hashtable on both left and right side is used for including null values in result in an efficient manner. If hash table is only built on one side, let's say left side and we perform a left outer join, for each row in left side, a scan over the right side is needed to make sure that no matching tuples for that row on left side. 

 

Hope this helps!

Liquan

 


I take a look at HashOuterJoin and it's building a Hashtable for both
sides.

This consumes quite a lot of memory when the partition is big. And it
doesn't reduce the iteration on streamed relation, right?

Thanks!

---------------------------------------------------------------------





 

-- 
Liquan Pei 
Department of Physics 
University of Massachusetts Amherst 





 

-- 
Liquan Pei 
Department of Physics 
University of Massachusetts Amherst 

"
Jianshi Huang <jianshi.huang@gmail.com>,"Wed, 8 Oct 2014 14:18:37 +0800",Re: How to do broadcast join in SparkSQL,Ted Yu <yuzhihong@gmail.com>,"Looks like https://issues.apache.org/jira/browse/SPARK-1800 is not merged
into master?

I cannot find spark.sql.hints.broadcastTables in latest master, but it's in
the following patch.


https://github.com/apache/spark/commit/76ca4341036b95f71763f631049fdae033990ab5


Jianshi






-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 8 Oct 2014 00:09:05 -0700",Re: Spark SQL question: why build hashtable for both sides in HashOuterJoin?,Haopu Wang <HWang@qilinsoft.com>,"I'm pretty sure inner joins on Spark SQL already build only one of the sides. Take a look at ShuffledHashJoin, which calls could optimize it for those that are not full.

Matei



efficient.
enought.
HashOuterJoin?
this case. 
side, so Spark can iterate through the left side and find matches in the right side from the hash table efficiently. Please comment and suggest, thanks again!
HashOuterJoin?
used for including null values in result in an efficient manner. If hash table is only built on one side, let's say left side and we perform a left outer join, for each row in left side, a scan over the right side is needed to make sure that no matching tuples for that row on left side. 

"
Liquan Pei <liquanpei@gmail.com>,"Wed, 8 Oct 2014 00:12:17 -0700",Re: Spark SQL question: why build hashtable for both sides in HashOuterJoin?,Matei Zaharia <matei.zaharia@gmail.com>,"I am working on a PR to leverage the HashJoin trait code to optimize the
Left/Right outer join. It's already been tested locally and will send out
the PR soon after some clean up.

Thanks,
Liquan


s.
e
d
r


-- 
Liquan Pei
Department of Physics
University of Massachusetts Amherst
"
Jianshi Huang <jianshi.huang@gmail.com>,"Wed, 8 Oct 2014 16:44:15 +0800",Re: How to do broadcast join in SparkSQL,Ted Yu <yuzhihong@gmail.com>,"Ok, currently there's cost-based optimization however Parquet statistics is
not implemented...

What's the good way if I want to join a big fact table with several tiny
dimension tables in Spark SQL (1.1)?

I wish we can allow user hint for the join.

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 8 Oct 2014 04:19:47 -0700 (PDT)",Standardized Distance Functions in MLlib,dev@spark.incubator.apache.org,"Hi all, 

In my limited understanding of the MLlib, it is a good idea to use the
various distance functions on some machine learning algorithms. For example,
we can only use Euclidean distance metric in KMeans. And I am tackling with
contributing hierarchical clustering to MLlib
(https://issues.apache.org/jira/browse/SPARK-2429). I would like to support
the various distance functions in it.

Should we support the standardized distance function in MLlib or not?
You know, Spark depends on Breeze. So I think we have two approaches in
functions in MLlib. The other is wrapping the functions of Breeze. And I am
a bit worried about using Breeze directly in Spark. For example,  we can't
absolutely control the release of Breeze. 

I sent a PR before. But it is stopping. I'd like to get your thoughts on it,
community.
https://github.com/apache/spark/pull/1964#issuecomment-54953348

Best,



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 8 Oct 2014 10:21:26 -0400",Re: Unneeded branches/tags,Patrick Wendell <pwendell@gmail.com>,"So:

   - tags: can delete
   - branches: stuck with â€˜em

Correct?

Nick
â€‹


te
ng
"
RJ Nowling <rnowling@gmail.com>,"Wed, 8 Oct 2014 10:54:19 -0400",Re: Spark on Mesos 0.20,Fairiz Azizi <coderfi@gmail.com>,"Yep!  That's the example I was talking about.

Is an error message printed when it hangs? I get :

14/09/30 13:23:14 ERROR BlockManagerMasterActor: Got two different
block manager registrations on 20140930-131734-1723727882-5050-1895-1






-- 
em rnowling@gmail.com
c 954.496.2314
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 8 Oct 2014 11:49:08 -0400",Re: Extending Scala style checks,Cheng Lian <lian.cs.zju@gmail.com>,"I've created SPARK-3849: Automate remaining Scala style rules
<https://issues.apache.org/jira/browse/SPARK-3849>.

Please create sub-tasks on this issue for rules that we have not automated
and let's work through them as possible.

I went ahead and created the first sub-task, SPARK-3850: Scala style:
Disallow trailing spaces <https://issues.apache.org/jira/browse/SPARK-3850>.

Nick


"
James Yu <jym2307@gmail.com>,"Wed, 8 Oct 2014 10:03:11 -0700",will/when Spark/SparkSQL will support ORCFile format,dev@spark.apache.org,"Didn't see anyone asked the question before, but I was wondering if anyone
knows if Spark/SparkSQL will support ORCFile format soon? ORCFile is
getting more and more popular hi Hive world.

Thanks,
James
"
Evan Chan <velvia.github@gmail.com>,"Wed, 8 Oct 2014 11:01:46 -0700",Re: will/when Spark/SparkSQL will support ORCFile format,James Yu <jym2307@gmail.com>,"James,

Michael at the meetup last night said there was some development
activity around ORCFiles.

I'm curious though, what are the pros and cons of ORCFiles vs Parquet?


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 8 Oct 2014 11:12:12 -0700",Re: will/when Spark/SparkSQL will support ORCFile format,Evan Chan <velvia.github@gmail.com>,"https://github.com/apache/spark/pull/2576




"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 8 Oct 2014 11:15:02 -0700",Re: Standardized Distance Functions in MLlib,Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Hi Yu,

We upgraded breeze to 0.10 yesterday. So we can call the distance
functions you contributed to breeze easily. We don't want to maintain
another copy of the implementation in MLlib to keep the maintenance
cost low. Both spark and breeze are open-source projects. We should
try our best to avoid duplicate effort and forking, even though we
don't have control the release of breeze.

As we discussed in the PR, if we want users to call them directly,
they should live in breeze. If we want users to specify them in
clustering algorithms, we should hide the implementation from users.
So simple wrappers over the breeze implementation should be
sufficient. We are reviewing

https://github.com/apache/spark/pull/2634

and try to see how we can embed distance measures there. In the
k-means implementation, we don't use (Vector, Vector) => Double.
Instead, we cache the norms and use inner product to derive the
distance, which is faster and takes advantage of sparsity. It would be
really nice if you can help review it and discuss how to embed
distance measures there. Thanks!

Best,
Xiangrui


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 8 Oct 2014 12:10:45 -0700",Re: How to do broadcast join in SparkSQL,Jianshi Huang <jianshi.huang@gmail.com>,"Thanks for the input.  We purposefully made sure that the config option did
not make it into a release as it is not something that we are willing to
support long term.  That said we'll try and make this easier in the future
either through hints or better support for statistics.

In this particular case you can get what you want by registering the tables
as external tables and setting an flag.  Here's a helper function to do
what you need.

/**
 * Sugar for creating a Hive external table from a parquet path.
 */
def createParquetTable(name: String, file: String): Unit = {
  import org.apache.spark.sql.hive.HiveMetastoreTypes

  val rdd = parquetFile(file)
  val schema = rdd.schema.fields.map(f => s""${f.name}
${HiveMetastoreTypes.toMetastoreType(f.dataType)}"").mkString("",\n"")
  val ddl = s""""""
    |CREATE EXTERNAL TABLE $name (
    |  $schema
    |)
    |ROW FORMAT SERDE 'parquet.hive.serde.ParquetHiveSerDe'
    |STORED AS INPUTFORMAT 'parquet.hive.DeprecatedParquetInputFormat'
    |OUTPUTFORMAT 'parquet.hive.DeprecatedParquetOutputFormat'
    |LOCATION '$file'"""""".stripMargin
  sql(ddl)
  setConf(""spark.sql.hive.convertMetastoreParquet"", ""true"")
}

You'll also need to run this to populate the statistics:

ANALYZE TABLE  tableName COMPUTE STATISTICS noscan;



"
Michael Armbrust <michael@databricks.com>,"Wed, 8 Oct 2014 13:19:55 -0700",Re: Parquet schema migrations,Cody Koeninger <cody@koeninger.org>,"
Filed here: https://issues.apache.org/jira/browse/SPARK-3851



I was proposing you manually convert each different format into one unified
format  (by adding literal nulls and such for missing columns) and then
union these converted datasets.  It would be weird to have union all try
and do this automatically.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 8 Oct 2014 16:50:17 -0400",spark-ec2 can't initialize spark-standalone module,dev <dev@spark.apache.org>,"This line
<https://github.com/mesos/spark-ec2/blob/bd2b0bd89f4fe42be51b3b6431f04ca6823b43dd/setup.sh#L109>
in setup.sh initializes several modules, which are defined here
<https://github.com/apache/spark/blob/bc4418727b40c9b6ba5194ead6e2698539272280/ec2/spark_ec2.py#L575>
.

# Install / Init module
for module in $MODULES; do
  echo ""Initializing $module""
  if [[ -e $module/init.sh ]]; then
    source $module/init.sh
  fi
  cd /root/spark-ec2  # guard against init.sh changing the cwd
done

init.sh file
<https://github.com/mesos/spark-ec2/tree/bd2b0bd89f4fe42be51b3b6431f04ca6823b43dd/spark-standalone>
.

Should it have one? Itâ€™s the only module without an init.sh.

Nick
â€‹
"
Cody Koeninger <cody@koeninger.org>,"Wed, 8 Oct 2014 16:08:14 -0500",Re: Parquet schema migrations,Michael Armbrust <michael@databricks.com>,"


Sure, I was just musing on what an api for doing the merging without manual
user input should look like / do.   I'll comment on the ticket, thanks for
making it
"
Nathan Kronenfeld <nkronenfeld@oculusinfo.com>,"Wed, 8 Oct 2014 17:53:34 -0400",Fwd: Accumulator question,dev@spark.apache.org,"I notice that accumulators register themselves with a private Accumulators
object.

I don't notice any way to unregister them when one is done.

Am I missing something? If not, is there any plan for how to free up that
memory?

I've a case where we're gathering data from repeated queries using some
relatively sizable accumulators; at the moment, we're creating one per
query, and running out of memory after far too few queries.

I've tried methods that don't involve accumulators; they involve a shuffle
instead, and take 10x as long.

Thanks,
                  -Nathan




-- 
Nathan Kronenfeld
Senior Visualization Developer
Oculus Info Inc
2 Berkeley Street, Suite 600,
Phone:  +1-416-203-3003 x 238
Email:  nkronenfeld@oculusinfo.com
"
shane knapp <sknapp@berkeley.edu>,"Wed, 8 Oct 2014 15:01:04 -0700",new jenkins update + tentative release date,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","greetings!

i've got some updates regarding our new jenkins infrastructure, as well as
the initial date and plan for rolling things out:

*** current testing/build break whack-a-mole:
a lot of out of date artifacts are cached in the current jenkins, which has
caused a few builds during my testing to break due to dependency resolution
failure[1][2].

bumping these versions can cause your builds to fail, due to public api
changes and the like.  consider yourself warned that some projects might
require some debugging...  :)

tomorrow, i will be at databricks working w/@joshrosen to make sure that
the spark builds have any bugs hammered out.

***  deployment plan:
unless something completely horrible happens, THE NEW JENKINS WILL GO LIVE
ON MONDAY (october 13th).

all jenkins infrastructure will be DOWN for the entirety of the day
(starting at ~8am).  this means no builds, period.  i'm hoping that the
downtime will be much shorter than this, but we'll have to see how
everything goes.

all test/build history WILL BE PRESERVED.  i will be rsyncing the jenkins
jobs/ directory over, complete w/history as part of the deployment.

once i'm feeling good about the state of things, i'll point the original
url to the new instances and send out an all clear.

if you are a student at UC berkeley, you can log in to jenkins using your
LDAP login, and (by default) view but not change plans.  if you do not have
a UC berkeley LDAP login, you can still view plans anonymously.

IF YOU ARE A PLAN ADMIN, THEN PLEASE REACH OUT, ASAP, PRIVATELY AND I WILL
SET UP ADMIN ACCESS TO YOUR BUILDS.

***  post deployment plan:
fix all of the things that break!

i will be keeping a VERY close eye on the builds, checking for breaks, and
helping out where i can.  if the situation is dire, i can always roll back
to the old jenkins infra...  but i hope we never get to that point!  :)

i'm hoping that things will go smoothly, but please be patient as i'm
certain we'll hit a few bumps in the road.

please let me know if you guys have any comments/questions/concerns...  :)

shane

1 - https://github.com/bigdatagenomics/bdg-services/pull/18
2 - https://github.com/bigdatagenomics/avocado/pull/111
"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 8 Oct 2014 17:19:45 -0700 (PDT)",Re: Standardized Distance Functions in MLlib,dev@spark.incubator.apache.org,"Hi Xiangrui, 

Thank you very much for replying and letting me know that you upgraded
breeze to 0.10 yesterday.
Sorry that I didn't know that.


I got it. I agree with keeping linear algebra in MLlib lightweight.


All right. I will check it.

thanks,
Yu Ishikawa



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
James Yu <jym2307@gmail.com>,"Wed, 8 Oct 2014 17:22:41 -0700",Re: will/when Spark/SparkSQL will support ORCFile format,Mark Hamstra <mark@clearstorydata.com>,"Thanks Mark! I will keep eye on it.

@Evan, I saw people use both format, so I really want to have Spark support
ORCFile.



"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Wed, 8 Oct 2014 23:10:02 -0600",Re: spark-ec2 can't initialize spark-standalone module,Nicholas Chammas <nicholas.chammas@gmail.com>,"There is a check to see if init.sh file exists (` if [[ -e
$module/init.sh ]]; then`), so it just won't get called. Regarding
spark-standalone not having a init.sh that is because we dont have any
initialization work to do for it  (its not necessary for all modules
to have a init.sh) as the spark module downloads and installs Spark.

Thanks
Shivaram

823b43dd/setup.sh#L109>
72280/ec2/spark_ec2.py#L575>
823b43dd/spark-standalone>

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 09 Oct 2014 14:03:21 +0800",Re: will/when Spark/SparkSQL will support ORCFile format,"James Yu <jym2307@gmail.com>, Mark Hamstra <mark@clearstorydata.com>","The foreign data source API PR also matters here 
https://www.github.com/apache/spark/pull/2475

Foreign data source like ORC can be added more easily and systematically 
after this PR is merged.



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 8 Oct 2014 23:56:09 -0700",Re: Extending Scala style checks,Nicholas Chammas <nicholas.chammas@gmail.com>,"Thanks. I added one.



"
DB Tsai <dbtsai@dbtsai.com>,"Thu, 9 Oct 2014 11:23:25 +0200","Re: [MLlib] LogisticRegressionWithSGD and LogisticRegressionWithLBFGS
 converge with different weights.",Yanbo Liang <yanbohappy@gmail.com>,"Nice to hear that your experiment is consistent to my assumption. The
current L1/L2 will penalize the intercept as well which is not idea.
I'm working on GLMNET in Spark using OWLQN, and I can exactly get the
same solution as R but with scalability in # of rows and columns. Stay
tuned!

Sincerely,

DB Tsai
-------------------------------------------------------
My Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



---------------------------------------------------------------------


"
Yana <yana.kadiyska@gmail.com>,"Thu, 9 Oct 2014 07:10:57 -0700 (PDT)",Trouble running tests,dev@spark.incubator.apache.org,"Hi, apologies if I missed a FAQ somewhere.

I am trying to submit a bug fix for the very first time. Reading
instructions, I forked the git repo (at
c9ae79fba25cd49ca70ca398bc75434202d26a97) and am trying to run tests.

I run this: ./dev/run-tests  _SQL_TESTS_ONLY=true

and after a while get the following error: 

[info] ScalaTest
[info] Run completed in 3 minutes, 37 seconds.
[info] Total number of tests run: 224
[info] Suites: completed 19, aborted 0
[info] Tests: succeeded 224, failed 0, canceled 0, ignored 5, pending 0
[info] All tests passed.
[info] Passed: Total 224, Failed 0, Errors 0, Passed 224, Ignored 5
[success] Total time: 301 s, completed Oct 9, 2014 9:31:23 AM
[error] Expected ID character
[error] Not a valid command: hive-thriftserver
[error] Expected project ID
[error] Expected configuration
[error] Expected ':' (if selecting a configuration)
[error] Expected key
[error] Not a valid key: hive-thriftserver
[error] hive-thriftserver/test
[error]                  ^


(I am running this without my changes)

I have 2 questions:
1. How to fix this
2. Is there a best practice on what to fork so you start off with a ""good
state""? I'm wondering if I should sync the latest changes or go back to a
label?

thanks in advance




--

---------------------------------------------------------------------


"
"""devl.development"" <devl.development@gmail.com>","Thu, 9 Oct 2014 11:18:52 -0700 (PDT)",Introduction to Spark Blog,dev@spark.incubator.apache.org,"Hi Spark community

Having spent some time getting up to speed with the various Spark components
in the core package, I've written a blog to help other newcomers and
contributors.

By no means am I a Spark expert so would be grateful for any advice,
comments or edit suggestions. 

Thanks very much here's the post.

http://batchinsights.wordpress.com/2014/10/09/a-short-dive-into-apache-spark/

Dev





--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 9 Oct 2014 14:40:57 -0400",Re: Trouble running tests,Yana <yana.kadiyska@gmail.com>,"_RUN_SQL_TESTS needs to be true as well. Those two _... variables set get
correctly when tests are run on Jenkins. Theyâ€™re not meant to be
manipulated directly by testers.

Did you want to run SQL tests only locally? You can try faking being
Jenkins by setting AMPLAB_JENKINS=true before calling run-tests. That
should be simpler than futzing with the _... variables.

Nick
â€‹


-tests-tp8717.html
"
Guillaume Pitel <guillaume.pitel@exensa.com>,"Thu, 09 Oct 2014 21:11:33 +0200",Re: TorrentBroadcast slow performance,dev@spark.apache.org,"Hi,

Thanks to your answer, we've found the problem. It was on reverse IP 
resolution on the drivers we used (wrong configuration of the local 
bind9). Apparently, not being able to reverse-resolve the IP address of 
the nodes was the culprit of the 10s delay.

We've hit two other secondary problems with TorrentBroadcast though, in 
case you're interested  :

1 - Broadcasting a variable of about 2GB (1.8GB exactly) triggers a 
""java.lang.OutOfMemoryError: Requested array size exceeds VM limit"", 
which is not the case with HttpBroadcast (I guess HttpBroadcast splits 
the serialized variable in small chunks)
2 - Memory use of Torrent seems to be higher than Http (i.e. switching 
from Http to Torrent triggers several OOM).

Additionally, a question : while HttpBroadcast stores the broadcast 
pieces on disk (in spark.local.dir/spark-... ), TorrentBroadcast seems 
not to use disk backend storage. Does it mean that HttpBroadcast can 
handle bigger broadcast out of memory ? If so, it's too bad that this 
design choice wasn't used for Torrent.

  That being said, hats off to the people in charge of the broadcast 
unloading wrt the lineage, this stuff works great !

Guillaume




---------------------------------------------------------------------


"
James Yu <jym2307@gmail.com>,"Thu, 9 Oct 2014 14:18:09 -0700",Re: will/when Spark/SparkSQL will support ORCFile format,Cheng Lian <lian.cs.zju@gmail.com>,"For performance, will foreign data format support, same as native ones?

Thanks,
James



"
Michael Armbrust <michael@databricks.com>,"Thu, 9 Oct 2014 14:22:58 -0700",Re: will/when Spark/SparkSQL will support ORCFile format,James Yu <jym2307@gmail.com>,"Yes, the foreign sources work is only about exposing a stable set of APIs
for external libraries to link against (to avoid the spark assembly
becoming a dependency mess).  The code path these APIs use will be the same
as that for datasources included in the core spark sql library.

Michael


"
Josh Rosen <rosenville@gmail.com>,"Thu, 9 Oct 2014 14:26:49 -0700",Re: Fwd: Accumulator question,"Nathan Kronenfeld <nkronenfeld@oculusinfo.com>, 
 dev@spark.apache.org","Hi Nathan,

Youâ€™re right, it looks like we donâ€™t currently provide a method to unregister accumulators. Â Iâ€™ve opened a JIRA to discuss a fix:Â https://issues.apache.org/jira/browse/SPARK-3885

In the meantime, hereâ€™s a workaround that might work: Â Accumulators have a public setValue() method that can be called (only by the driver) to change an accumulatorâ€™s value. Â You might be able to use this to reset accumulatorsâ€™ values to smaller objects (e.g. the â€œzeroâ€ object of whatever your accumulator type is, or â€˜nullâ€™ if youâ€™re sure that the accumulator will never be accessed again).

Hope this helps,
Josh


I notice that accumulators register themselves with a private Accumulators  
object.  

I don't notice any way to unregister them when one is done.  

Am I missing something? If not, is there any plan for how to free up that  
memory?  

I've a case where we're gathering data from repeated queries using some  
relatively sizable accumulators; at the moment, we're creating one per  
query, and running out of memory after far too few queries.  

I've tried methods that don't involve accumulators; they involve a shuffle  
instead, and take 10x as long.  

Thanks,  
-Nathan  




--  
Nathan Kronenfeld  
Senior Visualization Developer  
Oculus Info Inc  
2 Berkeley Street, Suite 600,  
Phone: +1-416-203-3003 x 238  
Email: nkronenfeld@oculusinfo.com  
"
Michael Armbrust <michael@databricks.com>,"Thu, 9 Oct 2014 14:26:53 -0700",Re: Trouble running tests,Nicholas Chammas <nicholas.chammas@gmail.com>,"Also, in general for SQL only changes it is sufficient to run ""sbt/sbt
catatlyst/test sql/test hive/test"".  The ""hive/test"" part takes the
longest, so I usually leave that out until just before submitting unless my
changes are hive specific.


od
 a
-tests-tp8717.html
"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 9 Oct 2014 15:04:23 -0700",Re: TorrentBroadcast slow performance,Guillaume Pitel <guillaume.pitel@exensa.com>,"Thanks for the feedback. For 1, there is an open patch: https://github.com/apache/spark/pull/2659. For 2, broadcast blocks actually use MEMORY_AND_DISK storage, so they will spill to disk if you have low memory, but they're faster to access otherwise.

Matei


resolution on the drivers we used (wrong configuration of the local bind9). Apparently, not being able to reverse-resolve the IP address of the nodes was the culprit of the 10s delay.
in case you're interested  :
""java.lang.OutOfMemoryError: Requested array size exceeds VM limit"", which is not the case with HttpBroadcast (I guess HttpBroadcast splits the serialized variable in small chunks)
from Http to Torrent triggers several OOM).
pieces on disk (in spark.local.dir/spark-... ), TorrentBroadcast seems not to use disk backend storage. Does it mean that HttpBroadcast can handle bigger broadcast out of memory ? If so, it's too bad that this design choice wasn't used for Torrent.
unloading wrt the lineage, this stuff works great !
connect through the IP addresses they're configured with. I see there's this 10 second pause between ""Updated info of block broadcast_84_piece1"" and ""ensureFreeSpace(4194304) called"" (where it actually receives the block). HTTP broadcast used only HTTP fetches from the executors to the driver, but TorrentBroadcast has connections between the executors themselves and between executors and the driver over a different port. Where are you running your driver app and nodes?
on dev
identified)
finally
setting
broadcast_84_piece1 stored
block
ensureFreeSpace(4194304) called
broadcast_84_piece0 stored
block
broadcast
ensureFreeSpace(4371392) called
stored as
broadcast
ensureFreeSpace(4369376) called
stored as
variable
Found
suspect a
Does
---------------------------------------------------------------------

"
Matei Zaharia <matei.zaharia@gmail.com>,"Thu, 9 Oct 2014 15:07:27 -0700",Re: TorrentBroadcast slow performance,Guillaume Pitel <guillaume.pitel@exensa.com>,"Oops I forgot to add, for 2, maybe we can add a flag to use DISK_ONLY for TorrentBroadcast, or if the broadcasts are bigger than some size.

Matei


https://github.com/apache/spark/pull/2659. For 2, broadcast blocks actually use MEMORY_AND_DISK storage, so they will spill to disk if you have low memory, but they're faster to access otherwise.
resolution on the drivers we used (wrong configuration of the local bind9). Apparently, not being able to reverse-resolve the IP address of the nodes was the culprit of the 10s delay.
in case you're interested  :
""java.lang.OutOfMemoryError: Requested array size exceeds VM limit"", which is not the case with HttpBroadcast (I guess HttpBroadcast splits the serialized variable in small chunks)
switching from Http to Torrent triggers several OOM).
pieces on disk (in spark.local.dir/spark-... ), TorrentBroadcast seems not to use disk backend storage. Does it mean that HttpBroadcast can handle bigger broadcast out of memory ? If so, it's too bad that this design choice wasn't used for Torrent.
unloading wrt the lineage, this stuff works great !
connect through the IP addresses they're configured with. I see there's this 10 second pause between ""Updated info of block broadcast_84_piece1"" and ""ensureFreeSpace(4194304) called"" (where it actually receives the block). HTTP broadcast used only HTTP fetches from the executors to the driver, but TorrentBroadcast has connections between the executors themselves and between executors and the driver over a different port. Where are you running your driver app and nodes?
on dev
identified)
finally
setting
broadcast_84_piece1 stored
block
ensureFreeSpace(4194304) called
broadcast_84_piece0 stored
block
broadcast
ensureFreeSpace(4371392) called
stored as
broadcast
ensureFreeSpace(4369376) called
stored as
variable
storage.BlockManager: Found
suspect a
Does
---------------------------------------------------------------------
---------------------------------------------------------------------

"
James Yu <jym2307@gmail.com>,"Thu, 9 Oct 2014 16:46:31 -0700",Re: will/when Spark/SparkSQL will support ORCFile format,Michael Armbrust <michael@databricks.com>,"Sounds great, thanks!




"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 9 Oct 2014 21:19:44 -0400",spark-prs and mesos/spark-ec2,dev <dev@spark.apache.org>,"Does it make sense to point the Spark PR review board to read from
mesos/spark-ec2 as well? PRs submitted against that repo may reference
Spark JIRAs and need review just like any other Spark PR.

Nick
"
"""=?utf-8?B?VHJpZGVudA==?="" <cwk32@vip.qq.com>","Fri, 10 Oct 2014 10:09:41 +0800",[Spark SQL] Strange NPE in Spark SQL with Hive,"""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","Hi Community,

      I use Spark 1.0.2, using Spark SQL to do Hive SQL.

      When I run the following code in Spark Shell:

val file = sc.textFile(""./README.md"")
val count = file.flatMap(line => line.split("" "")).map(word => (word, 1)).reduceByKey(_+_)
count.collect()
â€
      Correct and no error!

      When I run the following code:
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.hql(""SHOW TABLES"").collect().foreach(println)â€

      Correct and no error!

      But when I run:
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)
hiveContext.hql(""SELECT COUNT(*) from uservisits"").collect().foreach(println)â€

      It comes with some error messages.


      What I found was the following error:      
14/10/09 19:47:34 ERROR Executor: Exception in task ID 4 java.lang.NullPointerException 	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:594) 	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:594) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111) 	at org.apache.spark.scheduler.Task.run(Task.scala:51) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 14/10/09 19:47:34 INFO CoarseGrainedExecutorBackend: Got assigned task 5 14/10/09 19:47:34 INFO Executor: Running task ID 5 14/10/09 19:47:34 DEBUG BlockManager: Getting local block broadcast_1 14/10/09 19:47:34 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(true, true, false, true, 1) 14/10/09 19:47:34 DEBUG BlockManager: Getting block broadcast_1 from memory 14/10/09 19:47:34 INFO BlockManager: Found block broadcast_1 locally 14/10/09 19:47:34 INFO BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329 14/10/09 19:47:34 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks 14/10/09 19:47:34 DEBUG BlockFetcherIterator$BasicBlockFetcherIterator: Sending request for 2 blocks (2.5 KB) from node19:50868 14/10/09 19:47:34 DEBUG BlockMessageArray: Adding BlockMessage [type = 1, id = shuffle_0_0_1, level = null, data = null] 14/10/09 19:47:34 DEBUG BlockMessageArray: Added BufferMessage(id = 5, size = 34) 14/10/09 19:47:34 DEBUG BlockMessageArray: Adding BlockMessage [type = 1, id = shuffle_0_1_1, level = null, data = null] 14/10/09 19:47:34 DEBUG BlockMessageArray: Added BufferMessage(id = 6, size = 34) 14/10/09 19:47:34 DEBUG BlockMessageArray: Buffer list: 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=4 cap=4] 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=34 cap=34] 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=4 cap=4] 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=34 cap=34] 14/10/09 19:47:34 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Started 1 remote fetches in 2 ms 14/10/09 19:47:34 DEBUG BlockFetcherIterator$BasicBlockFetcherIterator: Got local blocks in  0 ms ms 14/10/09 19:47:34 ERROR Executor: Exception in task ID 5 java.lang.NullPointerException 	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:594) 	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:594) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111) 	at org.apache.spark.scheduler.Task.run(Task.scala:51) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 14/10/09 19:47:34 INFO CoarseGrainedExecutorBackend: Got assigned task 6 14/10/09 19:47:34 INFO Executor: Running task ID 6 14/10/09 19:47:34 DEBUG BlockManager: Getting local block broadcast_1 14/10/09 19:47:34 DEBUG BlockManager: Level for block broadcast_1 is StorageLevel(true, true, false, true, 1) 14/10/09 19:47:34 DEBUG BlockManager: Getting block broadcast_1 from memory 14/10/09 19:47:34 INFO BlockManager: Found block broadcast_1 locally 14/10/09 19:47:34 INFO BlockFetcherIterator$BasicBlockFetcherIterator: maxBytesInFlight: 50331648, targetRequestSize: 10066329 14/10/09 19:47:34 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks 14/10/09 19:47:34 DEBUG BlockFetcherIterator$BasicBlockFetcherIterator: Sending request for 2 blocks (2.5 KB) from node19:50868 14/10/09 19:47:34 DEBUG BlockMessageArray: Adding BlockMessage [type = 1, id = shuffle_0_0_1, level = null, data = null] 14/10/09 19:47:34 DEBUG BlockMessageArray: Added BufferMessage(id = 8, size = 34) 14/10/09 19:47:34 DEBUG BlockMessageArray: Adding BlockMessage [type = 1, id = shuffle_0_1_1, level = null, data = null] 14/10/09 19:47:34 DEBUG BlockMessageArray: Added BufferMessage(id = 9, size = 34) 14/10/09 19:47:34 DEBUG BlockMessageArray: Buffer list: 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=4 cap=4] 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=34 cap=34] 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=4 cap=4] 14/10/09 19:47:34 DEBUG BlockMessageArray: java.nio.HeapByteBuffer[pos=0 lim=34 cap=34] 14/10/09 19:47:34 INFO BlockFetcherIterator$BasicBlockFetcherIterator: Started 1 remote fetches in 2 ms 14/10/09 19:47:34 DEBUG BlockFetcherIterator$BasicBlockFetcherIterator: Got local blocks in  0 ms ms 14/10/09 19:47:34 ERROR Executor: Exception in task ID 6 java.lang.NullPointerException 	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:594) 	at org.apache.spark.rdd.RDD$$anonfun$15.apply(RDD.scala:594) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111) 	at org.apache.spark.scheduler.Task.run(Task.scala:51) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745)â€



             What can contribute to this? Is it a known problem?

                                                                                                         Chen Weikeng"
"""=?utf-8?B?VHJpZGVudA==?="" <cwk32@vip.qq.com>","Fri, 10 Oct 2014 10:53:45 +0800","[Spark SQL Continue] Sorry, it is not only limited in SQL, may due to network ","""=?utf-8?B?ZGV2?="" <dev@spark.apache.org>","Dear Community,

       Please ignore my last post about Spark SQL.

       When I run:
val file = sc.textFile(""./README.md"")
val count = file.flatMap(line => line.split("" "")).map(word => (word, 1)).reduceByKey(_+_)
count.collect()
â€
        it happends too.

        is there any possible reason for that? we make have some adjustment in network last night

                                                                                             Chen Weikeng
14/10/09 20:45:23 ERROR Executor: Exception in task ID 1 java.lang.NullPointerException 	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571) 	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.sql.SchemaRDD.compute(SchemaRDD.scala:116) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111) 	at org.apache.spark.scheduler.Task.run(Task.scala:51) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745) 14/10/09 20:45:23 INFO CoarseGrainedExecutorBackend: Got assigned task 2 14/10/09 20:45:23 INFO Executor: Running task ID 2 14/10/09 20:45:23 DEBUG BlockManager: Getting local block broadcast_0 14/10/09 20:45:23 DEBUG BlockManager: Level for block broadcast_0 is StorageLevel(true, true, false, true, 1) 14/10/09 20:45:23 DEBUG BlockManager: Getting block broadcast_0 from memory 14/10/09 20:45:23 INFO BlockManager: Found block broadcast_0 locally 14/10/09 20:45:23 DEBUG Executor: Task 2's epoch is 0 14/10/09 20:45:23 INFO HadoopRDD: Input split: file:/public/rdma14/app/spark-rdma/examples/src/main/resources/people.txt:16+16 14/10/09 20:45:23 ERROR Executor: Exception in task ID 2 java.lang.NullPointerException 	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571) 	at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:571) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35) 	at org.apache.spark.sql.SchemaRDD.compute(SchemaRDD.scala:116) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31) 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262) 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229) 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111) 	at org.apache.spark.scheduler.Task.run(Task.scala:51) 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:183) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.Thread.run(Thread.java:745)â€"
Fairiz Azizi <coderfi@gmail.com>,"Thu, 9 Oct 2014 21:11:52 -0700",Re: Spark on Mesos 0.20,RJ Nowling <rnowling@gmail.com>,"Hello,

Sorry for the late reply.

When I tried the LogQuery example this time, things now seem to be fine!

...

14/10/10 04:01:21 INFO scheduler.DAGScheduler: Stage 0 (collect at
LogQuery.scala:80) finished in 0.429 s

14/10/10 04:01:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0,
whose tasks have all completed, from pool defa

14/10/10 04:01:21 INFO spark.SparkContext: Job finished: collect at
LogQuery.scala:80, took 12.802743914 s

(10.10.10.10,""FRED"",GET http://images.com/2013/Generic.jpg HTTP/1.1)
bytes=621       n=2


Not sure if this is the correct response for that example.

Our mesos/spark builds have since been updated since I last wrote.

Possibly, the JDK version was updated to 1.7.0_67

If you are using an older JDK, maybe try updating that?


- Fi



Fairiz ""Fi"" Azizi


"
Gurvinder Singh <gurvinder.singh@uninett.no>,"Fri, 10 Oct 2014 08:35:33 +0200",Re: Spark on Mesos 0.20,"Fairiz Azizi <coderfi@gmail.com>, RJ Nowling <rnowling@gmail.com>","I have tested on current JDK 7 and now I am running JDK 8, the problem
still exist. Can you run logquery on data of size say 100+ GB, so that
you have more map tasks. As we start to see the issue on larger tasks.

- Gurvinder


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 10 Oct 2014 07:54:16 -0700",Breaking the previous large-scale sort record with Spark,"user <user@spark.apache.org>,
 dev@spark.apache.org","Hi folks,

I interrupt your regularly scheduled user / dev list to bring you some pretty cool news for the project, which is that we've been able to use Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x faster on 10x fewer nodes. There's a detailed writeup at http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html. Summary: while Hadoop MapReduce held last year's 100 TB world record by sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.

I want to thank Reynold Xin for leading this effort over the past few weeks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for providing the machines to make this possible. Finally, this result would of course not be possible without the many many other contributions, testing and feature requests from throughout the community.

For an engine to scale from these multi-hour petabyte batch jobs down to 100-millisecond streaming and interactive queries is quite uncommon, and it's thanks to all of you folks that we are able to make this happen.

Matei
---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Fri, 10 Oct 2014 08:17:34 -0700",Re: Breaking the previous large-scale sort record with Spark,Matei Zaharia <matei.zaharia@gmail.com>,"Awesome news Matei !

Congratulations to the databricks team and all the community members...


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 10 Oct 2014 20:49:28 +0530",Re: Breaking the previous large-scale sort record with Spark,Matei Zaharia <matei.zaharia@gmail.com>,"Brilliant stuff ! Congrats all :-)
This is indeed really heartening news !

Regards,
Mridul


ote:
etty cool news for the project, which is that we've been able to use Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x faster on 10x fewer nodes. There's a detailed writeup at http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html. Summary: while Hadoop MapReduce held last year's 100 TB world record by sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.
ks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for providing the machines to make this possible. Finally, this result would of course not be possible without the many many other contributions, testing and feature requests from throughout the community.
100-millisecond streaming and interactive queries is quite uncommon, and it's thanks to all of you folks that we are able to make this happen.

---------------------------------------------------------------------


"
Ted Malaska <ted.malaska@cloudera.com>,"Fri, 10 Oct 2014 11:21:45 -0400",Re: Breaking the previous large-scale sort record with Spark,Mridul Muralidharan <mridul@gmail.com>,"This is a bad deal, great job.


"
"""Dinesh J. Weerakkody"" <dineshjweerakkody@gmail.com>","Fri, 10 Oct 2014 21:08:51 +0530",Re: Breaking the previous large-scale sort record with Spark,Ted Malaska <ted.malaska@cloudera.com>,"Wow.. Cool.. Congratulations.. :)





-- 
Thanks & Best Regards,

*Dinesh J. Weerakkody*
*www.dineshjweerakkody.com <http://www.dineshjweerakkody.com>*
"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 10 Oct 2014 12:00:19 -0400",Re: Breaking the previous large-scale sort record with Spark,Mridul Muralidharan <mridul@gmail.com>,"Great! Congratulations! 

-- 
Nan Zhu





"
"""Arthur.hk.chan@gmail.com"" <arthur.hk.chan@gmail.com>","Fri, 10 Oct 2014 23:46:15 +0800",Re: Breaking the previous large-scale sort record with Spark,Matei Zaharia <matei.zaharia@gmail.com>,"Wonderful !!


some pretty cool news for the project, which is that we've been able to use Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x faster on 10x fewer nodes. There's a detailed writeup at http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html. Summary: while Hadoop MapReduce held last year's 100 TB world record by sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.
few weeks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for providing the machines to make this possible. Finally, this result would of course not be possible without the many many other contributions, testing and feature requests from throughout the community.
down to 100-millisecond streaming and interactive queries is quite uncommon, and it's thanks to all of you folks that we are able to make this happen.
---------------------------------------------------------------------

"
Steve Nunez <snunez@hortonworks.com>,"Fri, 10 Oct 2014 09:17:46 -0700",Re: Breaking the previous large-scale sort record with Spark,"Debasish Das <debasish.das83@gmail.com>,
	Matei Zaharia <matei.zaharia@gmail.com>","Great stuff. Wonderful to see such progress in so short a time.

How about some links to code and instructions so that these benchmarks can
be reproduced?

Regards,
- Steve

From:  Debasish Das <debasish.das83@gmail.com>
Date:  Friday, October 10, 2014 at 8:17
To:  Matei Zaharia <matei.zaharia@gmail.com>
Cc:  user <user@spark.apache.org>, dev <dev@spark.apache.org>
Subject:  Re: Breaking the previous large-scale sort record with Spark




-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 10 Oct 2014 13:38:22 -0400",Re: Trouble running tests,"""yana.kadiyska@gmail.com"" <yana.kadiyska@gmail.com>","Running dev/run-tests as-is should work and will test everything. That's
what the contributing guide recommends, if I remember correctly.

At some point we should make it easier to test individual components
locally using the dev script, but calling sbt on the various tests suites
as Michael pointed out will always work.

Nick


 my
et
e
 0
ng-tests-tp8717.html
"
Josh Rosen <rosenville@gmail.com>,"Fri, 10 Oct 2014 12:06:04 -0700",Re: spark-prs and mesos/spark-ec2,"Nicholas Chammas <nicholas.chammas@gmail.com>, dev
 <dev@spark.apache.org>","I think this would require fairly significant refactoring of the PR board code. Â Iâ€™d love it if the PR board code was more easily configurable to support different JIRA / GitHub repositories, etc, but I donâ€™t have the time to work on this myself.

- Josh


Does it make sense to point the Spark PR review board to read from  
mesos/spark-ec2 as well? PRs submitted against that repo may reference  
Spark JIRAs and need review just like any other Spark PR.  

Nick  
"
shane knapp <sknapp@berkeley.edu>,"Fri, 10 Oct 2014 13:01:00 -0700",Re: new jenkins update + tentative release date,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","reminder:  this IS happening, first thing monday morning PDT.  :)


"
Henry Saputra <henry.saputra@gmail.com>,"Fri, 10 Oct 2014 23:04:07 -0700",Re: Breaking the previous large-scale sort record with Spark,Matei Zaharia <matei.zaharia@gmail.com>,"Congrats to Reynold et al leading this effort!

- Henry

ote:
etty cool news for the project, which is that we've been able to use Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x faster on 10x fewer nodes. There's a detailed writeup at http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html. Summary: while Hadoop MapReduce held last year's 100 TB world record by sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.
ks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for providing the machines to make this possible. Finally, this result would of course not be possible without the many many other contributions, testing and feature requests from throughout the community.
100-millisecond streaming and interactive queries is quite uncommon, and it's thanks to all of you folks that we are able to make this happen.

---------------------------------------------------------------------


"
Jianshi Huang <jianshi.huang@gmail.com>,"Sat, 11 Oct 2014 14:18:22 +0800",Re: How to do broadcast join in SparkSQL,Michael Armbrust <michael@databricks.com>,"It works fine, thanks for the help Michael.

Liancheng also told me a trick, using a subquery with LIMIT n. It works in
latest 1.2.0

BTW, looks like the broadcast optimization won't be recognized if I do a
left join instead of a inner join. Is that true? How can I make it work for
left joins?

Cheers,
Jianshi




-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Ilya Ganelin <ilganeli@gmail.com>,"Sat, 11 Oct 2014 01:09:32 -0400",Re: Breaking the previous large-scale sort record with Spark,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei - I read your post with great interest. Could you possibly comment
in more depth on some of the issues you guys saw when scaling up spark and
how you resolved them? I am interested specifically in spark-related
problems. I'm working on scaling up spark to very large datasets and have
been running into a variety of issues. Thanks in advance!

"
Sean Owen <sowen@cloudera.com>,"Sun, 12 Oct 2014 19:50:08 +0100",Decision forests don't work with non-trivial categorical features,"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm having trouble getting decision forests to work with categorical
features. I have a dataset with a categorical feature with 40 values.
It seems to be treated as a continuous/numeric value by the
implementation.

Digging deeper, I see there is some logic in the code that indicates
that categorical features over N values do not work unless the number
of bins is at least 2*((2^N - 1) - 1) bins. I understand this as the
naive brute force condition, wherein the decision tree will test all
possible splits of the categorical value.

However, this gets unusable quickly as the number of bins should be
tens or hundreds at best, and this requirement rules out categorical
values over more than 10 or so features as a result. But, of course,
it's not unusual to have categorical features with high cardinality.
It's almost common.

There are some pretty fine heuristics for selecting 'bins' over
categorical features when the number of bins is far fewer than the
complete, exhaustive set.

Before I open a JIRA or continue, does anyone know what I am talking
about, am I mistaken? Is this a real limitation and is it worth
pursuing these heuristics? I can't figure out how to proceed with
decision forests in MLlib otherwise.

---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Sun, 12 Oct 2014 13:51:21 -0700",reading/writing parquet decimal type,dev@spark.apache.org,"Hello,

I'm interested in reading/writing parquet SchemaRDDs that support the Parquet Decimal converted type. The first thing I did was update the Spark parquet dependency to version 1.5.0, as this version introduced support for decimals in parquet. However, conversion between the catalyst decimal type and the parquet decimal type is complicated by the fact that the catalyst type does not specify a decimal precision and scale but the parquet type requires them.

I'm wondering if perhaps we could add an optional precision and scale to the catalyst decimal type? The catalyst decimal type would have unspecified precision and scale by default for backwards compatibility, but users who want to serialize a SchemaRDD with decimal(s) to parquet would have to narrow their decimal type(s) by specifying a precision and scale.

Thoughts?

Michael
---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 12 Oct 2014 15:32:29 -0700",Re: reading/writing parquet decimal type,Michael Allman <michael@videoamp.com>,"Hi Michael,

I've been working on this in my repo: https://github.com/mateiz/spark/tree/decimal. I'll make some pull requests with these features soon, but meanwhile you can try this branch. See https://github.com/mateiz/spark/compare/decimal for the individual commits that went into it. It has exactly the precision stuff you need, plus some optimizations for working on decimals.

Matei


Parquet Decimal converted type. The first thing I did was update the Spark parquet dependency to version 1.5.0, as this version introduced support for decimals in parquet. However, conversion between the catalyst decimal type and the parquet decimal type is complicated by the fact that the catalyst type does not specify a decimal precision and scale but the parquet type requires them.
to the catalyst decimal type? The catalyst decimal type would have unspecified precision and scale by default for backwards compatibility, but users who want to serialize a SchemaRDD with decimal(s) to parquet would have to narrow their decimal type(s) by specifying a precision and scale.


---------------------------------------------------------------------


"
Evan Sparks <evan.sparks@gmail.com>,"Sun, 12 Oct 2014 22:34:43 -0400",Re: Decision forests don't work with non-trivial categorical features,Sean Owen <sowen@cloudera.com>,"I was under the impression that we were using the usual sort by average response value heuristic when storing histogram bins (and searching for optimal splits) in the tree code. 

Maybe Manish or Joseph can clarify?


---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Sun, 12 Oct 2014 20:20:11 -0700",Re: reading/writing parquet decimal type,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei,

Thanks, I can see you've been hard at work on this! I examined your patch and do have a question. It appears you're limiting the precision of decimals written to parquet to those that will fit in a long, yet you're writing the values as a parquet binary type. Why not write them using the int64 parquet type instead?

Cheers,

Michael


https://github.com/mateiz/spark/tree/decimal. I'll make some pull requests with these features soon, but meanwhile you can try this branch. See https://github.com/mateiz/spark/compare/decimal for the individual commits that went into it. It has exactly the precision stuff you need, plus some optimizations for working on decimals.
Parquet Decimal converted type. The first thing I did was update the Spark parquet dependency to version 1.5.0, as this version introduced support for decimals in parquet. However, conversion between the catalyst decimal type and the parquet decimal type is complicated by the fact that the catalyst type does not specify a decimal precision and scale but the parquet type requires them.
to the catalyst decimal type? The catalyst decimal type would have unspecified precision and scale by default for backwards compatibility, but users who want to serialize a SchemaRDD with decimal(s) to parquet would have to narrow their decimal type(s) by specifying a precision and scale.


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sun, 12 Oct 2014 21:37:04 -0700",Scalastyle improvements / large code reformatting,dev <dev@spark.apache.org>,"There are a number of open pull requests that aim to extend Sparkâ€™s automated style checks (seeÂ https://issues.apache.org/jira/browse/SPARK-3849Â for an umbrella JIRA). Â These fixes are mostly good, but I have some concerns about merging these patches. Â Several of these patches make large reformatting changes in nearly every file of Spark, which makes it more difficult to use `git blame` and has the potential to introduce merge conflicts with all open PRs and all backport patches.

I feel that most of the value of automated style-checks comes from allowing reviewers/committers to focus on the technical content of pull requests rather than their formatting.Â  My concern is that the convenience added by these new style rules will not outweigh the other overheads that these reformatting patches will create for the committers.

If possible, it would be great if we could extend the style checker to enforce the more stringent rules only for new code additions / deletions. Â If not, I donâ€™t think that we should proceed with the reformatting. Â Others might disagree, though, so I welcome comments / discussion.

- Josh"
Reynold Xin <rxin@databricks.com>,"Sun, 12 Oct 2014 21:39:29 -0700",Re: Scalastyle improvements / large code reformatting,Josh Rosen <rosenville@gmail.com>,"I actually think we should just take the bite and follow through with the
reformatting. Many rules are simply not possible to enforce only on deltas
(e.g. import ordering).

That said, maybe there are better windows to do this, e.g. during the QA
period.


™s
Rs
e
g.  Others
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 12 Oct 2014 22:16:52 -0700",Re: Scalastyle improvements / large code reformatting,Reynold Xin <rxin@databricks.com>,"Another big problem with these patches are that they make it almost
impossible to backport changes to older branches cleanly (there
becomes like 100% chance of a merge conflict).

1. We only consider new style rules at the end of a release cycle,
when there is the smallest chance of wanting to backport stuff.
2. We require that they are submitted in individual patches with a (a)
new style rule and (b) the associated changes. Then we can also
evaluate on a case-by-case basis how large the change is for each
rule. For rules that require sweeping changes across the codebase,
personally I'd vote against them. For rules like import ordering that
won't cause too much pain on the diff (it's pretty easy to deal with
those conflicts) I'd be okay with it.

If we went with this, we'd also have to warn people that we might not
accept new style rules if they are too costly to enforce. I'm guessing
people will still contribute even with those expectations.

- Patrick


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 12 Oct 2014 22:23:25 -0700",Re: Scalastyle improvements / large code reformatting,Patrick Wendell <pwendell@gmail.com>,"I'm also against these huge reformattings. They slow down development and backporting for trivial reasons. Let's not do that at this point, the style of the current code is quite consistent and we have plenty of other things to worry about. Instead, what you can do is as you edit a file when you're working on a feature, fix up style issues you see. Or, as Josh suggested, some way to make this apply only to new files would help.

Matei


the
deltas
QA
JIRA).
these
in
`git
open PRs
pull
convenience
that
to
deletions.
Others


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 12 Oct 2014 22:48:22 -0700",Re: reading/writing parquet decimal type,Michael Allman <michael@videoamp.com>,"The fixed-length binary type can hold fewer bytes than an int64, though many encodings of int64 can probably do the right thing. We can look into supporting multiple ways to do this -- the spec does say that you should at least be able to read int32s and int64s.

Matei


patch and do have a question. It appears you're limiting the precision of decimals written to parquet to those that will fit in a long, yet you're writing the values as a parquet binary type. Why not write them using the int64 parquet type instead?
https://github.com/mateiz/spark/tree/decimal. I'll make some pull requests with these features soon, but meanwhile you can try this branch. See https://github.com/mateiz/spark/compare/decimal for the individual commits that went into it. It has exactly the precision stuff you need, plus some optimizations for working on decimals.
the Parquet Decimal converted type. The first thing I did was update the Spark parquet dependency to version 1.5.0, as this version introduced support for decimals in parquet. However, conversion between the catalyst decimal type and the parquet decimal type is complicated by the fact that the catalyst type does not specify a decimal precision and scale but the parquet type requires them.
scale to the catalyst decimal type? The catalyst decimal type would have unspecified precision and scale by default for backwards compatibility, but users who want to serialize a SchemaRDD with decimal(s) to parquet would have to narrow their decimal type(s) by specifying a precision and scale.
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Sun, 12 Oct 2014 23:26:02 -0700",Re: new jenkins update + tentative release date,"dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","Reminder: this Jenkins migration is happening tomorrow morning (Monday).


"
Sean Owen <sowen@cloudera.com>,"Mon, 13 Oct 2014 09:04:23 +0100",Re: Decision forests don't work with non-trivial categorical features,Evan Sparks <evan.sparks@gmail.com>,"I'm looking at this bit of code in DecisionTreeMetadata ...

val maxCategoriesForUnorderedFeature =
  ((math.log(maxPossibleBins / 2 + 1) / math.log(2.0)) + 1).floor.toInt
strategy.categoricalFeaturesInfo.foreach { case (featureIndex, numCategories) =>
  // Decide if some categorical features should be treated as
unordered features,
  //  which require 2 * ((1 << numCategories - 1) - 1) bins.
  // We do this check with log values to prevent overflows in case
numCategories is large.
  // The next check is equivalent to: 2 * ((1 << numCategories - 1) -
1) <= maxBins
  if (numCategories <= maxCategoriesForUnorderedFeature) {
    unorderedFeatures.add(featureIndex)
    numBins(featureIndex) = numUnorderedBins(numCategories)
  } else {
    numBins(featureIndex) = numCategories
  }
}

So if I have a feature with 40 values and less than about a trillion
bins, it gets treated as a continuous feature, which is meaningless.
It shortly throws an exception though since other parts of the code
expect this to be a categorical feature.

I think there's a bug here somewhere but wasn't sure whether it was
just 'not implemented' yet and so needs a better error message (and
should be implemented), or something else preventing this from working
as expected.

I'll wait a beat to get more info and then if needed open a JIRA. Thanks all.


---------------------------------------------------------------------


"
Jianshi Huang <jianshi.huang@gmail.com>,"Mon, 13 Oct 2014 16:15:02 +0800",SPARK-3106 fixed?,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","https://issues.apache.org/jira/browse/SPARK-3106

I'm having the saming errors described in SPARK-3106 (no other types of
errors confirmed), running a bunch sql queries on spark 1.2.0 built from
latest master HEAD.

Any updates to this issue?

My main task is to join a huge fact table with a dozen dim tables (using
HiveContext) and then map it to my class object. It failed a couple of
times and now I cached the intermediate table and currently it seems
working fine... no idea why until I found SPARK-3106

Cheers,
-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Sean Owen <sowen@cloudera.com>,"Mon, 13 Oct 2014 09:16:03 +0100",Re: Decision forests don't work with non-trivial categorical features,Evan Sparks <evan.sparks@gmail.com>,"Hm, no I don't think I'm quite right there. There's an issue but
that's not quite it.

So I have a categorical feature with 40 value, and 300 bins. The error
I see in the end is:

java.lang.IllegalArgumentException: requirement failed:
DTStatsAggregator.getLeftRightFeatureOffsets is for unordered features
only, but was called for ordered feature 4.
at scala.Predef$.require(Predef.scala:233)
at org.apache.spark.mllib.tree.impl.DTStatsAggregator.getLeftRightFeatureOffsets(DTStatsAggregator.scala:143)
at org.apache.spark.mllib.tree.DecisionTree$.org$apache$spark$mllib$tree$DecisionTree$$mixedBinSeqOp(DecisionTree.scala:363)
...

So this categorical is treated as an ordered feature because you would
have to have a load of bins to match the condition in the code I
quote. But something isn't expecting that. Is that much worth a JIRA?

Hm, but what's the theory of giving meaning to the ordering of the
arbitrary categorical values? is that what this is trying to do, or is
it in fact ordering by some function of the target (average value for
regression, average 1-vs-all entropy for classification)? I suppose I
didn't expect to encounter logic like this.


---------------------------------------------------------------------


"
Jianshi Huang <jianshi.huang@gmail.com>,"Mon, 13 Oct 2014 16:24:24 +0800",Re: SPARK-3106 fixed?,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hmm... it failed again, just lasted a little bit longer.

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Oct 2014 08:46:48 -0400",Re: Scalastyle improvements / large code reformatting,Matei Zaharia <matei.zaharia@gmail.com>,"The arguments against large scale refactorings make sense. Doing them, if
at all, during QA cycles or around releases sounds like a promising idea.

Coupled with that, would it be useful to implement new rules outside of
these potential windows for refactoring in such a way that they report on
style errors without failing the build?

That way they work like a nag and encourage developers to fix style
problems in the course of working on their original patch. Coupled with a
clear policy to fix style only where code is being changed anyway, this
could be a helpful way to steadily fix problems related to new rules over
time. Then, when we get close to the ""finish line"", we can make a final
patch to fix the remaining issues for a given rule and enforce it as part
of the build. Having the style report should also make it easier for
committers to review style. We will have to do some work to show reports on
new rules in a digestible way, as they will probably be very large at
first, but I think that's a tractable problem.

What do committers/reviewers think of that?

As an aside, enforcing new style rules for new files only is an interesting
idea, but you'd have to track that a file was added after the new rules
were enforced. Otherwise bad style will be allowed after the initial
checkin of that file. Also, enforcing style rules on changes may work in
some (e.g. space required  before ""{"") but is impossible in other (import
ordering) cases, as Reynold pointed out.

Nick


2014ë…„ 10ì›” 13ì¼ ì›”ìš”ì¼, Matei Zaharia<matei.zaharia@gmail.com>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:

le
s
e
QA
ll
o
"
"""=?UTF-8?B?5qyn6Ziz5pmLKOasp+mYs+aZiyk=?="" <jin.oyj@alibaba-inc.com>","Mon, 13 Oct 2014 21:10:30 +0800","=?UTF-8?B?UmXvvJpCcmVha2luZyB0aGUgcHJldmlvdXMgbGFyZ2Utc2NhbGUgc29ydCByZWNvcmQgd2l0?=
  =?UTF-8?B?aCBTcGFyaw==?=","""user"" <user@spark.apache.org>,
  ""dev"" <dev@spark.apache.org>,
  ""Matei Zaharia"" <matei.zaharia@gmail.com>","
Great News!

Still some questions for this Sort benchmark
1 How many Map tasks run in the sort? I think the 2,50,000 is the # of reduce tasks. (if the unsorted file is read from HDFS and use the default 64MB chunk size, the # of map task will be about 1PB/64MB?)2 How long a single Reduce task run ? I think becuase the #of reduce task is very large , a single reduce task will not last a long time, so the record the reduce read in will also be very small?3 How much memory a single Reduce task use in one run?Â ------------------------------------------------------------------
å‘ä»¶äººï¼šMatei Zaharia <matei.zaharia@gmail.com>
å‘é€æ—¶é—´ï¼š2014å¹´10æœˆ10æ—¥(æ˜ŸæœŸäº”) 22:54
æ”¶ä»¶äººï¼šuser <user@spark.apache.org>; dev <dev@spark.apache.org>
ä¸»ã€€é¢˜ï¼šBreaking the previous large-scale sort record with Spark

Hi folks,

I interrupt your regularly scheduled user / dev list to bring you some pretty cool news for the project, which is that we've been able to use Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x faster on 10x fewer nodes. There's a detailed writeup at http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html. Summary: while Hadoop MapReduce held last year's 100 TB world record by sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.

I want to thank Reynold Xin for leading this effort over the past few weeks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for providing the machines to make this possible. Finally, this result would of course not be possible without the many many other contributions, testing and feature requests from throughout the community.

For an engine to scale from these multi-hour petabyte batch jobs down to 100-millisecond streaming and interactive queries is quite uncommon, and it's thanks to all of you folks that we are able to make this happen.

Matei
-------------------------------------------"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Oct 2014 07:54:16 -0700",Re: new jenkins update + tentative release date,Josh Rosen <rosenville@gmail.com>,"Jenkins is in quiet mode and the move will be starting after i have my
coffee.  :)


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 13 Oct 2014 08:57:19 -0700",Re: Scalastyle improvements / large code reformatting,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nick,

I think the best solution is really to find a way to only apply
certain rules to code modified after a certain date. I also don't
think it would be that hard to implement because git can output
per-line information about modification times. So you'd just run the
scalastyle rules and then if you saw errors from rules with a special
""if modified since"" property, we'd only fail the line has been
modified after that date. That would even work for imports as well,
you'd just have a thing where if anyone modified some imports they
would have to fix all the imports in that file. It's at least worth a
try.

Overall I think that's the only real solution here. Doing things
closer to releases reduces the overhead of backporting, but overall
it's still going to be a very high overhead.

- Patrick

 at
ems
e a
,
the
ew
ng
ere
f
ria@gmail.com>´ÔÀÌ ÀÛ¼ºÇÑ ¸Þ½ÃÁö:
d
yle
gs
re
,
:
n
t
to

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 13 Oct 2014 10:12:36 -0700",Re: Scalastyle improvements / large code reformatting,Patrick Wendell <pwendell@gmail.com>,"Another option is to add new style rules that trigger too many errors
as warnings, and slowly clean them up. This means that reviewers will
be burdened with manually enforcing the rules for a while, and we need
to remember to turn them to errors once some threshold is reached.

(The Hadoop build had this check where it compared the number of
warnings against the last known good build, and yelled at you if you
added any new warnings. That might be something to look at also,
although not sure how easy it would be and I seem to remember it
having false positives.)

:
f at
n
lems
be a
n,
 the
iew
k
ing
were
of
 Zaharia<matei.zaharia@gmail.com>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
nd
tyle
ngs
're
d,
e:
)
g
h
e
in
it
s
 to



-- 
Marcelo

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Oct 2014 10:15:46 -0700",Re: new jenkins update + tentative release date,Josh Rosen <rosenville@gmail.com>,"quick update:  we should be back up and running in the next ~60mins.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Oct 2014 13:16:21 -0400",Re: Scalastyle improvements / large code reformatting,Patrick Wendell <pwendell@gmail.com>,"

OK, that sounds like a fair compromise. I've updated the description on
SPARK-3849 <https://issues.apache.org/jira/browse/SPARK-3849> accordingly.

Nick
"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Oct 2014 10:35:04 -0700",Re: new jenkins update + tentative release date,Josh Rosen <rosenville@gmail.com>,"AND WE ARE LIIIIIIIVE!

https://amplab.cs.berkeley.edu/jenkins/

have at it, folks!


"
Joseph Bradley <joseph@databricks.com>,"Mon, 13 Oct 2014 11:12:56 -0700",Re: Decision forests don't work with non-trivial categorical features,Sean Owen <sowen@cloudera.com>,"Hi Sean,

Sorry I didn't see this thread earlier!  (Thanks Ameet for pinging me.)

Short version: That exception should not be thrown, so there is a bug
somewhere.  The intended logic for handling high-arity categorical features
is about the best one can do, as far as I know.

Bug finding: For my checking purposes, which branch of Spark are you using,
and do you have the options being submitted to DecisionTree?

High-arity categorical features: As you have figured out, if you use a
categorical feature with just a few categories, it is treated as
""unordered"" so that we explicitly consider all exponentially many ways to
split the categories into 2 groups.  If you use one with many categories,
then it is necessary to impose an order.  (The communication increases
linearly in the number of possible splits, so it would blow up if we
considered all exponentially many splits.)  This order is chosen separately
for each node, so it is not a uniform order imposed over the entire tree.
This actually means that it is not a heuristic for regression and binary
classification; i.e., it chooses the same split as if we had explicitly
considered all of the possible splits.  For multiclass classification, it
is a heuristic, but I don't know of a better solution.

I'll check the code, but if you can forward info about the bug, that would
be very helpful.

Thanks!
Joseph


"
Sean Owen <sowen@cloudera.com>,"Mon, 13 Oct 2014 19:19:54 +0100",Re: Decision forests don't work with non-trivial categorical features,Joseph Bradley <joseph@databricks.com>,"Great, we'll confer then. I'm using master / 1.2.0-SNAPSHOT. I'll send
some details directly under separate cover.


---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Mon, 13 Oct 2014 11:55:04 -0700",Re: Decision forests don't work with non-trivial categorical features,Sean Owen <sowen@cloudera.com>,"I think this is the fix:

In this
file: mllib/src/main/scala/org/apache/spark/mllib/tree/impl/DTStatsAggregator.scala

methods ""getFeatureOffset"" and ""getLeftRightFeatureOffsets"" have sanity
checks (""require"") which are correct for DecisionTree but not for
RandomForest.  You can remove those.  I've sent a PR with this and a few
other small fixes:

https://github.com/apache/spark/pull/2785

I hope this fixes the bug!


"
Jianshi Huang <jianshi.huang@gmail.com>,"Tue, 14 Oct 2014 04:36:01 +0800",Re: SPARK-3106 fixed?,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Turned out it was caused by this issue:
https://issues.apache.org/jira/browse/SPARK-3923

Set spark.akka.heartbeat.interval to 100 solved it.

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Jianshi Huang <jianshi.huang@gmail.com>,"Tue, 14 Oct 2014 04:45:32 +0800",Re: SPARK-3106 fixed?,"user <user@spark.apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","important one

  WARN ReliableDeliverySupervisor: Association with remote system
[akka.tcp://sparkDriver@xxx:50278] has failed, address is now gated for
[5000] ms. Reason is: [Disassociated].

is of Log Level WARN.

Jianshi






-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Oct 2014 17:28:58 -0400",Re: new jenkins update + tentative release date,shane knapp <sknapp@berkeley.edu>,"Thanks for doing this work Shane.

So is Jenkins in the new datacenter now? Do you know if the problems with
checking out patches from GitHub should be resolved now? Here's an example
from the past hour
<https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21702/console>
.

Nick



"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 13 Oct 2014 14:36:41 -0700",Re: Breaking the previous large-scale sort record with Spark,Ilya Ganelin <ilganeli@gmail.com>,"The biggest scaling issue was supporting a large number of reduce tasks efficiently, which the JIRAs in that post handle. In particular, our current default shuffle (the hash-based one) has each map task open a separate file output stream for each reduce task, which wastes a lot of memory (since each stream has its own buffer).

A second thing that helped efficiency tremendously was Reynold's new network module (https://issues.apache.org/jira/browse/SPARK-2468). Doing I/O on 32 cores, 10 Gbps Ethernet and 8+ disks efficiently is not easy, as can be seen when you try to scale up other software.

Finally, with 30,000 tasks even sending info about every map's output size to each reducer was a problem, so Reynold has a patch that avoids that if the number of tasks is large.

Matei


comment in more depth on some of the issues you guys saw when scaling up spark and how you resolved them? I am interested specifically in spark-related problems. I'm working on scaling up spark to very large datasets and have been running into a variety of issues. Thanks in advance!
pretty cool news for the project, which is that we've been able to use Spark to break MapReduce's 100 TB and 1 PB sort records, sorting data 3x faster on 10x fewer nodes. There's a detailed writeup at http://databricks.com/blog/2014/10/10/spark-breaks-previous-large-scale-sort-record.html. Summary: while Hadoop MapReduce held last year's 100 TB world record by sorting 100 TB in 72 minutes on 2100 nodes, we sorted it in 23 minutes on 206 nodes; and we also scaled up to sort 1 PB in 234 minutes.
weeks, along with Parviz Deyhim, Xiangrui Meng, Aaron Davidson and Ali Ghodsi. In addition, we'd really like to thank Amazon's EC2 team for providing the machines to make this possible. Finally, this result would of course not be possible without the many many other contributions, testing and feature requests from throughout the community.
to 100-millisecond streaming and interactive queries is quite uncommon, and it's thanks to all of you folks that we are able to make this happen.


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Oct 2014 14:43:30 -0700",Re: new jenkins update + tentative release date,Nicholas Chammas <nicholas.chammas@gmail.com>,"
really hoping that the better network would just make this go away...
 guess i'll be doing a deeper dive now.

i would just up the timeout, but that's not coming out for a little while
yet:
https://issues.jenkins-ci.org/browse/JENKINS-20387

(we are currently running the latest -- 2.2.7, and the timeout field is
coming in 2.3, whenever that is)

i'll try and strace/replicate it locally as well.
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Oct 2014 17:48:02 -0400",Re: new jenkins update + tentative release date,shane knapp <sknapp@berkeley.edu>,"Ah, that sucks. Thank you for looking into this.


"
shane knapp <sknapp@berkeley.edu>,"Mon, 13 Oct 2014 14:54:00 -0700",Re: new jenkins update + tentative release date,Nicholas Chammas <nicholas.chammas@gmail.com>,"ok, i found something that may help:
https://issues.jenkins-ci.org/browse/JENKINS-20445?focusedCommentId=195638&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-195638

i set this to 20 minutes...  let's see if that helps.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 13 Oct 2014 17:55:32 -0400",Re: new jenkins update + tentative release date,shane knapp <sknapp@berkeley.edu>,"*fingers crossed*


"
Ilya Ganelin <ilganeli@gmail.com>,"Mon, 13 Oct 2014 18:52:58 -0400",Re: Breaking the previous large-scale sort record with Spark,Matei Zaharia <matei.zaharia@gmail.com>,"Thank you for the details! Would you mind speaking to what tools proved
most useful as far as identifying bottlenecks or bugs? Thanks again.

"
Krishna Sankar <ksankar42@gmail.com>,"Mon, 13 Oct 2014 16:21:15 -0700",Re: Breaking the previous large-scale sort record with Spark,Matei Zaharia <matei.zaharia@gmail.com>,"Well done guys. MapReduce sort at that time was a good feat and Spark now
has raised the bar with the ability to sort a PB.
Like some of the folks in the list, a summary of what worked (and didn't)
as well as the monitoring practices would be good.
Cheers
<k/>
P.S: What are you folks planning next ?


"
Manish Amde <manish9ue@gmail.com>,"Mon, 13 Oct 2014 22:52:30 -0700",Re: Decision forests don't work with non-trivial categorical features,Joseph Bradley <joseph@databricks.com>,"Sean, sorry for missing out on the discussion.

Evan, you are correct, we are using the heuristic Sean suggested during the
multiclass PR for ordering high-arity categorical variables using the
impurity values for each categorical feature.

Joseph, thanks for fixing the bug which I think was a regression since we
added support for RFs. I don't think we have see this in 1.1.

-Manish


"
Priya Ch <learnings.chitturi@gmail.com>,"Tue, 14 Oct 2014 17:03:46 +0530",Default spark.deploy.recoveryMode,"user@spark.apache.org, dev@spark.apache.org","Hi Spark users/experts,

In Spark source code  (Master.scala & Worker.scala), when  registering the
worker with master, I see the usage of *persistenceEngine*. When we don't
specify spark.deploy.recovery mode explicitly, what is the default value
used ? This recovery mode is used to persists and restore the application &
worker details.

 I see when recovery mode not specified explicitly,
*BlackHolePersistenceEngine* being used. Am i right ?


Thanks,
Padma Ch
"
Joseph Beynon <jbeynon@gmail.com>,"Tue, 14 Oct 2014 17:21:21 -0700 (PDT)",avro-mapred hadoop2 not in assembly jar,dev@spark.incubator.apache.org,"When building off of master it looks like the wrong version of the jar is
added to the assembly jar. The fix for SPARK-3039 added the ""hadoop2""
classifier at compile time when using the hadoop2 profile, but in the
assembly stage I get:
    ""[INFO] Including org.apache.avro:avro-mapred:jar:1.7.6 in the shaded
jar.""
It's not clear if there are other dependencies that have classifiers being
added to the assembly. But when running with the current version I end up
with a common exception about TaskAttemptContext. After manually unjar-ing
the assembly and replacing the correct version of avro-mapred it works
correctly.
If I knew more about maven I'd try to help out with a PR to fix this.




--

---------------------------------------------------------------------


"
Matthew Cheah <matthew.c.cheah@gmail.com>,"Tue, 14 Oct 2014 18:17:14 -0700",Unit testing Master-Worker Message Passing,dev@spark.apache.org,"Hi everyone,

Iâ€™m adding some new message passing between the Master and Worker actors in
order to address https://issues.apache.org/jira/browse/SPARK-3736 .

I was wondering if these kinds of interactions are tested in the automated
Jenkins test suite, and if so, where I could find some examples to help me
do the same.

Thanks!

-Matt Cheah
"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 14 Oct 2014 21:40:45 -0400",Re: Unit testing Master-Worker Message Passing,Matthew Cheah <matthew.c.cheah@gmail.com>,"You can use akka testkit

Example:

https://github.com/apache/spark/blob/ef4ff00f87a4e8d38866f163f01741c2673e41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala  

--  
Nan Zhu



er actors in
ted
 me

"
Yu Ishikawa <yuu.ishikawa+spark@gmail.com>,"Wed, 15 Oct 2014 07:05:33 -0700 (PDT)","[mllib] Share the simple benchmark result about the cast cost from
 Spark vector to Breeze vector",dev@spark.incubator.apache.org,"Hi all,

I wondered the cast cost from Spark Vectors to Breeze vector is high or low. 
So I benchmarked the simple operation about addition, multiplication and 
division of RDD[Vector] or RDD[BV[Double]]. I share the simple benchmark
result with you.

In conclusion, the cast cost was lower than I had expected. 
For more information, please read the below report, if you are interested in
it.
https://github.com/yu-iskw/benchmark-breeze-on-spark/blob/master/doc%2Fbenchmark-result.md

Best,
Yu Ishikawa



-----
-- Yu Ishikawa
--

---------------------------------------------------------------------


"
Matthew Cheah <matthew.c.cheah@gmail.com>,"Wed, 15 Oct 2014 11:04:03 -0700",Re: Unit testing Master-Worker Message Passing,Nan Zhu <zhunanmcgill@gmail.com>,"Thanks, the example was helpful.

However, testing the Worker itself is a lot more complicated than
WorkerWatcher, since the Worker class is quite a bit more complex. Are
there any tests that inspect the Worker itself?

Thanks,

-Matt Cheah


41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala
 actors in
d
e
"
Nan Zhu <zhunanmcgill@gmail.com>,"Wed, 15 Oct 2014 14:28:29 -0400",Re: Unit testing Master-Worker Message Passing,Matthew Cheah <matthew.c.cheah@gmail.com>,"I donâ€™t think there are test cases for Worker itself  


You can  


val actorRef = TestActorRef[Master](Props(classOf[Master], ...))(actorSystem) actorRef.underlyingActor.receive(Heartbeat)

and use expectMsg to test if Master can reply correct message  by assuming Worker is absolutely correct

Then in another test case to test if Worker can send register message to Master after receiving Masterâ€™s â€œre-registerâ€ instruction, (in this test case assuming that the Master is absolutely right)

Best,

--  
Nan Zhu



rWatcher, since the Worker class is quite a bit more complex. Are there any tests that inspect the Worker itself?
673e41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala  
Worker actors in

tomated
help me

"
Matthew Cheah <matthew.c.cheah@gmail.com>,"Wed, 15 Oct 2014 13:38:05 -0700",Re: Unit testing Master-Worker Message Passing,Nan Zhu <zhunanmcgill@gmail.com>,"What's happening when I do this is that the Worker tries to get the Master
actor by calling context.actorSelection(), and the RegisterWorker message
gets sent to the dead letters mailbox instead of being picked up by
expectMsg. I'm new to Akka and I've tried various ways to registering a
""mock"" master to no avail.

I would think there would be at least some kind of test for master - worker
message passing, no?


g
truction, (in this test
41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala
 actors in
d
e
"
shane knapp <sknapp@berkeley.edu>,"Wed, 15 Oct 2014 13:52:20 -0700","short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i'm going to be downgrading our git plugin (from 2.2.7 to 2.2.2) to see if
that helps w/the git fetch timeouts.

this will require a short downtime (~20 mins for builds to finish, ~20 mins
to downgrade), and will hopefully give us some insight in to wtf is going
on.

thanks for your patience...

shane
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 15 Oct 2014 16:59:38 -0400","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",shane knapp <sknapp@berkeley.edu>,"I support this effort. :thumbsup:


"
Chester Chen <chester@alpinenow.com>,"Wed, 15 Oct 2014 14:18:03 -0700",Re: Unit testing Master-Worker Message Passing,Matthew Cheah <matthew.c.cheah@gmail.com>,"actor is still there or the path is correct. The method returns a future
and you can wait for it with timeout. This way, you know the actor is live
or already dead or incorrect.

Another way, is to send Identify method to ActorSystem, if it returns with
correct identified message; then you can act on it, otherwise, ...

hope this helps

Chester


r
er
o
nstruction, (in this test
:
41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala
er actors
"
shane knapp <sknapp@berkeley.edu>,"Wed, 15 Oct 2014 14:19:43 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",amp-infra <amp-infra@googlegroups.com>,"ok, we're up and building...  :crossesfingersfortheumpteenthtime:


"
shane knapp <sknapp@berkeley.edu>,"Wed, 15 Oct 2014 14:27:48 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",amp-infra <amp-infra@googlegroups.com>,"four builds triggered....  and no timeouts.  :crossestoes:  :)


"
Matthew Cheah <matthew.c.cheah@gmail.com>,"Wed, 15 Oct 2014 14:38:54 -0700",Re: Unit testing Master-Worker Message Passing,Chester Chen <chester@alpinenow.com>,"I think on a higher level I also want to ask why such unit testing has not
actually been done in this codebase. If it's not a common practice to test
message passing then I'm fine with leaving out the unit test, however I'm
more curious as to why such testing was not done before.

:

e
h
er
e
:
to
instruction, (in this test
e41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala
ker
p
"
Josh Rosen <rosenville@gmail.com>,"Wed, 15 Oct 2014 14:42:57 -0700",Re: Unit testing Master-Worker Message Passing,"Chester Chen <chester@alpinenow.com>, Matthew Cheah
 <matthew.c.cheah@gmail.com>","There are some end-to-end integration tests of Master <-> Worker fault-tolerance inÂ https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala

Iâ€™ve actually been working to develop a more generalized Docker-based integration-testing framework for Spark in order to test Master <-> Worker interactions. Â Iâ€™d like to eventually clean up my code and release it publicly.

I think on a higher level I also want to ask why such unit testing has not  
actually been done in this codebase. If it's not a common practice to test  
message passing then I'm fine with leaving out the unit test, however I'm  
more curious as to why such testing was not done before.  

rote:  

e  
e  
ive  
ith  
.com>  
ster  
age  
a  
rote:  
 ...))(  
e to  
 instruction, (in this test  
re  
 
73e41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala  
orker  
 
elp  
"
Matthew Cheah <matthew.c.cheah@gmail.com>,"Wed, 15 Oct 2014 15:04:42 -0700",Re: Unit testing Master-Worker Message Passing,Josh Rosen <rosenville@gmail.com>,"Thanks Josh! These tests seem to cover the cases I'm looking for already =).

What's interesting though is that we still ran into SPARK-3736 despite such
integration tests being in place to catch it - specifically, the case when
the master disconnects and reconnects, the workers should reconnect to the
master after the master restarts. Are the tests here run regularly, i.e.
Jenkins build or nightly build, and if so how did that test case pass while
SPARK-3736 apparently still exists?

At any rate, I think I'll submit my fix PR but with no particular extra
automated test written for it, since it seems like FaultToleranceTest
sufficiently covers what I need.


e/spark/deploy/FaultToleranceTest.scala
ased
r
e it
t
t
e
e
a
e
 instruction, (in this
41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala
orker
"
shane knapp <sknapp@berkeley.edu>,"Wed, 15 Oct 2014 15:10:11 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",amp-infra <amp-infra@googlegroups.com>,"ok, we've had about 10 spark pull request builds go through w/o any git
timeouts.  it seems that the git timeout issue might be licked.

i will be definitely be keeping an eye on this for the next few days.

thanks for being patient!

shane


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 15 Oct 2014 16:57:04 -0700",Issues with ALS positive definite,dev <dev@spark.apache.org>,"Hi,

If I take the Movielens data and run the default ALS with regularization as
0.0, I am hitting exception from LAPACK that the gram matrix is not
positive definite. This is on the master branch.

This is how I run it :

./bin/spark-submit --total-executor-cores 1 --master spark://
tusca09lmlvt00c.uswin.ad.vzwcorp.com:7077 --jars
/Users/v606014/.m2/repository/com/github/scopt/scopt_2.10/3.2.0/scopt_2.10-3.2.0.jar
--class org.apache.spark.examples.mllib.MovieLensALS
./examples/target/spark-examples_2.10-1.1.0-SNAPSHOT.jar --rank 20
--numIterations 20 --lambda 0.0 --kryo
hdfs://localhost:8020/sandbox/movielens/

Error from LAPACK:

WARN TaskSetManager: Lost task 0.0 in stage 11.0 (TID 22,
tusca09lmlvt00c.uswin.ad.vzwcorp.com):
org.jblas.exceptions.LapackArgumentException: LAPACK DPOSV: Leading minor
of order i of A is not positive definite.


||r - wi'hj||^{2} has to be positive definite...

I think the tests are not running any 0.0 regularization tests otherwise we
should have caught it as well...

For the sparse coding NMF variant that I am running, I have to turn off L2
regularization when I run a L1 on products to extract sparse topics...

Thanks.

Deb
"
Liquan Pei <liquanpei@gmail.com>,"Wed, 15 Oct 2014 17:01:22 -0700",Re: Issues with ALS positive definite,Debasish Das <debasish.das83@gmail.com>,"Hi Debaish,

I think ||r - wi'hj||^{2} is semi-positive definite.

Thanks,
Liquan





-- 
Liquan Pei
Department of Physics
University of Massachusetts Amherst
"
Debasish Das <debasish.das83@gmail.com>,"Wed, 15 Oct 2014 17:05:33 -0700",Re: Issues with ALS positive definite,Liquan Pei <liquanpei@gmail.com>,"But do you expect the mllib code to fail if I run with 0.0 regularization ?

I think ||r - wi'hj||^{2} is positive definite...It can become positive
semi definite only if there are dependent rows in the matrix...

@sean is that right ? We had this discussion before as well...



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 15 Oct 2014 22:05:22 -0400","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",shane knapp <sknapp@berkeley.edu>,"A quick scan through the Spark PR board <https://spark-prs.appspot.com/> shows
no recent failures related to this git checkout problem.

Looks promising!

Nick


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 15 Oct 2014 23:25:31 -0700",Re: Issues with ALS positive definite,Debasish Das <debasish.das83@gmail.com>,"Do not use lambda=0.0. Use a small number instead. Cholesky
factorization doesn't work on semi-positive systems with 0
eigenvalues. -Xiangrui


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 16 Oct 2014 07:19:12 -0400",Re: Issues with ALS positive definite,Debasish Das <debasish.das83@gmail.com>,"It Gramian is at least positive semidefinite and will be definite if the
matrix is non singular, yes. That's usually but not always true.

The lambda*I matrix is positive definite, well, when lambda is positive.
Adding that makes it definite.

At least, lambda=0 could be rejected as invalid.

But this goes back to using the Cholesky decomposition. Why not use QR? It
smallish dense matrices I don't think it is much slower. I have not
benchmarked that but I opted for QR in a different implementation and it
has worked fine.

Now I have to go hunt for how the QR decomposition is exposed in BLAS...
Looks like its GEQRF which JBLAS helpfully exposes. Debasish you could try
it for fun at least.

"
Debasish Das <debasish.das83@gmail.com>,"Thu, 16 Oct 2014 08:34:35 -0700",Re: Issues with ALS positive definite,Sean Owen <sowen@cloudera.com>,"@xiangrui should we add this epsilon inside ALS code itself ? So that if
user by mistake put 0.0 as regularization, LAPACK failures does not show
up...

@sean For the proximal algorithms I am using Cholesky for L1 and LU for
equality and bound constraints"
Debasish Das <debasish.das83@gmail.com>,"Thu, 16 Oct 2014 08:44:57 -0700",Re: Issues with ALS positive definite,Sean Owen <sowen@cloudera.com>,"Just checked, QR is exposed by netlib: import org.netlib.lapack.Dgeqrf

For the equality and bound version, I will use QR...it will be faster than
the LU that I am using through jblas.solveSymmetric...


"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Thu, 16 Oct 2014 16:10:32 +0000",accumulators,"""dev@spark.apache.org"" <dev@spark.apache.org>","Accumulators on the stage info page show the rolling life time value of accumulators as well as per task which is handy.  I think it would be useful to add another field to the “Accumulators” table that also shows the total for the stage you are looking at (basically just a merge of the accumulators for tasks in that stage).  This would be useful for any job that is iterative (eg- basically every spark streaming job).

Does this idea make sense?


Separate but related question-  From the operational side I think it could be very useful to have an accumulators summary page.  For example we have a spark streaming job with many different stages.  It is difficult to navigate into each stage to pick out a trend.  An accumulators page that allowed one to filter by stage description and/or accumulator name would be very useful.

Thoughts?


Thanks,

Sean
---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Thu, 16 Oct 2014 11:04:56 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Nicholas Chammas <nicholas.chammas@gmail.com>,"the bad news is that we've had a couple more failures due to timeouts, but
the good news is that the frequency that these happen has decreased
significantly (3 in the past ~18hr).

seems like the git plugin downgrade has helped relieve the problem, but
hasn't fixed it.  i'll be looking in to this more today.


"
Josh Rosen <rosenville@gmail.com>,"Thu, 16 Oct 2014 11:53:34 -0700",Re: Unit testing Master-Worker Message Passing,Matthew Cheah <matthew.c.cheah@gmail.com>,"Hi Matt,

Iâ€™m not sure whether those tests will actually find this specific issue. Â The tests that I linked to test Sparkâ€™s Zookeeper-based multi-master mode, whereas it sounds like youâ€™re seeing this issue in regular standalone cluster. Â In those tests, the workers disconnect from the master because we've actually killed the master, whereas it sounds like your workers are disconnecting due to intermittent network issues and youâ€™d like them to reconnect to the sameÂ master.

That FaultToleranceSuite is not run automatically and I donâ€™t think that it was designed for automatic use: for example, it doesnâ€™t clean up Docker containers after failed tests, doesnâ€™t use a framework like ScalaTest for structured test result reporting, etc. Â The new framework that Iâ€™m working on will address these issues.

- Josh


Thanks Josh! These tests seem to cover the cases I'm looking for already =).

What's interesting though is that we still ran into SPARK-3736 despite such integration tests being in place to catch it - specifically, the case when the master disconnects and reconnects, the workers should reconnect to the master after the master restarts. Are the tests here run regularly, i.e. Jenkins build or nightly build, and if so how did that test case pass while SPARK-3736 apparently still exists?

At any rate, I think I'll submit my fix PR but with no particular extra automated test written for it, since it seems like FaultToleranceTest sufficiently covers what I need.

e:
There are some end-to-end integration tests of Master <-> Worker fault-tolerance inÂ https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/FaultToleranceTest.scala

Iâ€™ve actually been working to develop a more generalized Docker-based integration-testing framework for Spark in order to test Master <-> Worker interactions.Â  Iâ€™d like to eventually clean up my code and release it publicly.


I think on a higher level I also want to ask why such unit testing has not
actually been done in this codebase. If it's not a common practice to test
message passing then I'm fine with leaving out the unit test, however I'm
more curious as to why such testing was not done before.

rote:

e
e
ive
ith
.com>
ster
age
a
rote:
 ...))(
e to
 instruction, (in this test
re
73e41da/core/src/test/scala/org/apache/spark/deploy/worker/WorkerWatcherSuite.scala
orker
elp

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 16 Oct 2014 15:51:26 -0400","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",shane knapp <sknapp@berkeley.edu>,"Thanks for continuing to look into this, Shane.

bottom of this, is doing the git checkout ourselves in the run-tests-jenkins
script and cutting out the Jenkins git plugin entirely. That way we can
script retries and post friendlier messages about timeouts if they still
occur by ourselves.

Do you think thatâ€™s worth trying at some point?

Nick
â€‹


t
 shows
:
o
s
"
shane knapp <sknapp@berkeley.edu>,"Thu, 16 Oct 2014 12:55:50 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Nicholas Chammas <nicholas.chammas@gmail.com>,"yeah, at this point it might be worth trying.  :)

the absolutely irritating thing is that i am not seeing this happen w/any
other jobs other that the spark prb, nor does it seem to correlate w/time
of day, network or system load, or what slave it runs on.  nor are we
hitting our limit of connections on github.  i really, truly hate
non-deterministic failures.

i'm also going to write an email to support@github and see if they have any
insight in to this as well.


s
t
,
is
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 16 Oct 2014 16:04:43 -0400","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",shane knapp <sknapp@berkeley.edu>,"


Amen bruddah.
"
Debasish Das <debasish.das83@gmail.com>,"Thu, 16 Oct 2014 23:25:50 -0700",NNLS bug,dev <dev@spark.apache.org>,"Hi,

I am validating the proximal algorithm for positive and bound constrained
ALS and I came across the bug detailed in the JIRA while running ALS with
NNLS:

https://issues.apache.org/jira/browse/SPARK-3987

ADMM based proximal algorithm came up with correct result...

Thanks.
Deb
"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 16 Oct 2014 23:53:39 -0700",Re: NNLS bug,Debasish Das <debasish.das83@gmail.com>,"Thanks for reporting the bug! I will take a look. -Xiangrui


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 17 Oct 2014 15:13:06 -0400",Using Docker to Parallelize Tests,dev <dev@spark.apache.org>,"https://news.ycombinator.com/item?id=8471812

The parent thread has lots of interesting use cases for Docker, and the
linked comment seems most relevant to our testing predicament.

I might look into this after I finish something presentable with Packer and
our EC2 scripts, but if anyone else is interested, by all means...

Nick
"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Oct 2014 17:31:01 -0400",Re: accumulators,"Sean McNamara <sean.mcnamara@webtrends.com>, 
 ""=?utf-8?Q?dev=40spark.apache.org?="" <dev@spark.apache.org>","It certainly makes sense for a single streaming job. But it is definitely non-trivial to make this useful to all Spark programs. If I were to have a long running SParkContext and submit a wide variety of jobs to it, this would make the list of accumulators very, very large. Maybe the solution is to have pagination of these and always sort them by the last update time.

--Â 
Reynold Xin



Accumulators on the stage info page show the rolling life time value of accumulators as well as per task which is handy. I think it would be useful to add another field to the â€œAccumulatorsâ€ table that also shows the total for the stage you are looking at (basically just a merge of the accumulators for tasks in that stage). This would be useful for any job that is iterative (eg- basically every spark streaming job).  

Does this idea make sense?  


Separate but related question- From the operational side I think it could be very useful to have an accumulators summary page. For example we have a spark streaming job with many different stages. It is difficult to navigate into each stage to pick out a trend. An accumulators page that allowed one to filter by stage description and/or accumulator name would be very useful.  

Thoughts?  


Thanks,  

Sean  
---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Fri, 17 Oct 2014 18:23:15 -0400",sampling broken in PySpark with recent NumPy,dev@spark.apache.org,"Hi all,

I found a significant bug in PySpark's sampling methods, due to a recent NumPy change (as of v1.9). I created a JIRA (https://issues.apache.org/jira/browse/SPARK-3995), but wanted to share here as well in case anyone hits it.

Steps to reproduce are:


Which returns:

""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/worker.py"", line 79, in main
""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/serializers.py"", line 196, in dump_stream
""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/serializers.py"", line 127, in dump_stream
""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/serializers.py"", line 185, in _batched
""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/rddsampler.py"", line 116, in func
""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/rddsampler.py"", line 58, in getUniformSample
""/Users/freemanj11/code/spark-1.1.0-bin-hadoop1/python/pyspark/rddsampler.py"", line 44, in initRandomGenerator
(numpy/random/mtrand/mtrand.c:7397)
(numpy/random/mtrand/mtrand.c:7697)

The problem is that NumPy used to silently truncate random seeds larger than 2 ** 32, but now throws an error (due to this patch: https://github.com/numpy/numpy/commit/6b1a1205eac6fe5d162f16155d500765e8bca53c). And this reliably breaks our sampling. I’ll put a PR in shortly with the fix.

— Jeremy

-------------------------
jeremyfreeman.net
@thefreemanlab

"
shane knapp <sknapp@berkeley.edu>,"Fri, 17 Oct 2014 16:44:06 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Nicholas Chammas <nicholas.chammas@gmail.com>,"quick update:

here are some stats i scraped over the past week of ALL pull request
builder projects and timeout failures.  due to the large number of spark
ghprb jobs, i don't have great records earlier than oct 7th.  the data is
current up until ~230pm today:

spark and new spark ghprb total builds vs git fetch timeouts:
$ for x in 10-{09..17}; do passed=$(grep $x SORTED.passed | grep -i spark |
wc -l); failed=$(grep $x SORTED | grep -i spark | wc -l); let
total=passed+failed; fail_percent=$(echo ""scale=2; $failed/$total"" | bc |
sed ""s/^\.//g""); line=""$x -- total builds: $total\tp/f:
 $passed/$failed\tfail%: $fail_percent%""; echo -e $line; done
10-09 -- total builds: 140 p/f: 92/48 fail%: 34%
10-10 -- total builds: 65 p/f: 59/6 fail%: 09%
10-11 -- total builds: 29 p/f: 29/0 fail%: 0%
10-12 -- total builds: 24 p/f: 21/3 fail%: 12%
10-13 -- total builds: 39 p/f: 35/4 fail%: 10%
10-14 -- total builds: 7 p/f: 5/2 fail%: 28%
10-15 -- total builds: 37 p/f: 34/3 fail%: 08%
10-16 -- total builds: 71 p/f: 59/12 fail%: 16%
10-17 -- total builds: 26 p/f: 20/6 fail%: 23%

all other ghprb builds vs git fetch timeouts:
$ for x in 10-{09..17}; do passed=$(grep $x SORTED.passed | grep -vi spark
| wc -l); failed=$(grep $x SORTED | grep -vi spark | wc -l); let
total=passed+failed; fail_percent=$(echo ""scale=2; $failed/$total"" | bc |
sed ""s/^\.//g""); line=""$x -- total builds: $total\tp/f:
 $passed/$failed\tfail%: $fail_percent%""; echo -e $line; done
10-09 -- total builds: 16 p/f: 16/0 fail%: 0%
10-10 -- total builds: 46 p/f: 40/6 fail%: 13%
10-11 -- total builds: 4 p/f: 4/0 fail%: 0%
10-12 -- total builds: 2 p/f: 2/0 fail%: 0%
10-13 -- total builds: 2 p/f: 2/0 fail%: 0%
10-14 -- total builds: 10 p/f: 10/0 fail%: 0%
10-15 -- total builds: 5 p/f: 5/0 fail%: 0%
10-16 -- total builds: 5 p/f: 5/0 fail%: 0%
10-17 -- total builds: 0 p/f: 0/0 fail%: 0%

note:  the 15th was the day i rolled back to the earlier version of the git
plugin.  it doesn't seem to have helped much, so i'll probably bring us
back up to the latest version soon.
also note:  rocking some floating point math on the CLI!  ;)

i also compared the distribution of git timeout failures vs time of day,
and there appears to be no correlation.  the failures are pretty evenly
distributed over each hour of the day.

we could be hitting the rate limit due to the ghprb hitting github a couple
of times for each build, but we're averaging ~10-20 builds per hour (a
build hits github 2-4 times, from what i can tell).  i'll have to look more
in to this on monday, but suffice to say we may need to move from
unauthorized https fetches to authorized requests.  this means retrofitting
all of our jobs.  yay!  fun!  :)

another option is to have local mirrors of all of the repos.  the problem
w/this is that there might be a window where changes haven't made it to the
local mirror and tests run against it.  more fun stuff to think about...

now that i have some stats, and a list of all of the times/dates of the
failures, i will be drafting my email to github and firing that off later
today or first thing monday.

have a great weekend everyone!

shane, who spent way too much time on the CLI and is ready for some beer.


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 17 Oct 2014 19:52:47 -0400","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",shane knapp <sknapp@berkeley.edu>,"Wow, thanks for this deep dive Shane. Is there a way to check if we are
getting hit by rate limiting directly, or do we need to contact GitHub for
that?

2014ë…„ 10ì›” 17ì¼ ê¸ˆìš”ì¼, shane knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:

rk
| bc |
ark
| bc |
us
r
ng
he
"
shane knapp <sknapp@berkeley.edu>,"Fri, 17 Oct 2014 17:00:08 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Nicholas Chammas <nicholas.chammas@gmail.com>,"actually, nvm, you have to be run that command from our servers to affect
our limit.  run it all you want from your own machines!  :P


s
 knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
k
is
"" | bc |
t
"" | bc |
g us
,
our
ok
ting
de
nk
er
r.
"
Andrew Ash <andrew@andrewash.com>,"Fri, 17 Oct 2014 17:00:40 -0700",Raise Java dependency from 6 to 7,dev <dev@spark.apache.org>,"Hi Spark devs,

I've heard a few times that keeping support for Java 6 is a priority for
Apache Spark.  Given that Java 6 has been publicly EOL'd since Feb 2013
<http://www.oracle.com/technetwork/java/eol-135779.html> and the last
public update was Apr 2013
<https://en.wikipedia.org/wiki/Java_version_history#Java_6_updates>, why
are we still maintaing support for 6?  The only people using it now must be
paying for the extended support to continue receiving security fixes.

Bumping the lower bound of Java versions up to Java 7 would allow us to
upgrade from Jetty 8 to 9, which is currently a conflict with the
Dropwizard framework and a personal pain point.

Java 6 vs 7 for Spark links:
Try with resources
<https://github.com/apache/spark/pull/2575/files#r18152125> for
SparkContext et al
Upgrade to Jetty 9
<https://github.com/apache/spark/pull/167#issuecomment-54544494>
Warn when not compiling with Java6
<https://github.com/apache/spark/pull/859>


Who are the people out there that still need Java 6 support?

Thanks!
Andrew
"
shane knapp <sknapp@berkeley.edu>,"Fri, 17 Oct 2014 16:59:15 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Nicholas Chammas <nicholas.chammas@gmail.com>,"yep, and i will tell you guys ONLY if you promise to NOT try this
yourselves...  checking the rate limit also counts as a hit and increments
our numbers:

# curl -i https://api.github.com/users/whatever 2> /dev/null | egrep ^X-Rate
X-RateLimit-Limit: 60
X-RateLimit-Remaining: 51
X-RateLimit-Reset: 1413590269

(yes, that is the exact url that they recommended on the github site lol)

so, earlier today, we had a spark build fail w/a git timeout at 10:57am,
but there were only ~7 builds run that hour, so that points to us NOT
hitting the rate limit...  at least for this fail.  whee!

is it beer-thirty yet?

shane




knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
s
ark
 | bc |
 | bc |
 us
ur
k
ing
m
the
r
.
"
Davies Liu <davies@databricks.com>,"Fri, 17 Oct 2014 17:26:29 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",shane knapp <sknapp@berkeley.edu>,"
git fetch --tags --progress https://github.com/apache/spark.git
+refs/pull/*:refs/remotes/origin/pr/*

I'm thinking that maybe this may be a expensive call, we could try to
use a more cheap one:

git fetch --tags --progress https://github.com/apache/spark.git
+refs/pull/XXX/*:refs/remotes/origin/pr/XXX/*

XXX is the PullRequestID,

The configuration support parameters [1], so we could put this in :

+refs/pull//${ghprbPullId}/*:refs/remotes/origin/pr/${ghprbPullId}/*

I have not tested this yet, could you give this a try?

Davies


[1] https://wiki.jenkins-ci.org/display/JENKINS/GitHub+pull+request+builder+plugin

ts
)
e knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
rk
 is
t
l"" | bc |
et
l"" | bc |
e
ng us
y,
y
hour
ook
tting
ade
ink
e
ter
er.

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 17 Oct 2014 18:17:07 -0700","Re: short jenkins downtime -- trying to get to the bottom of
 the git fetch timeouts","Davies Liu <davies@databricks.com>, shane knapp
 <sknapp@berkeley.edu>","FYI, I edited the Spark Pull Request Builder job to try this out. Â Letâ€™s see if it works (Iâ€™ll be around to revert if it doesnâ€™t).

rote:


git fetch --tags --progress https://github.com/apache/spark.git  
+refs/pull/*:refs/remotes/origin/pr/*  

I'm thinking that maybe this may be a expensive call, we could try to  
use a more cheap one:  

git fetch --tags --progress https://github.com/apache/spark.git  
+refs/pull/XXX/*:refs/remotes/origin/pr/XXX/*  

XXX is the PullRequestID,  

The configuration support parameters [1], so we could put this in :  

+refs/pull//${ghprbPullId}/*:refs/remotes/origin/pr/${ghprbPullId}/*  

I have not tested this yet, could you give this a try?  

Davies  


[1] https://wiki.jenkins-ci.org/display/JENKINS/GitHub+pull+request+builder+plugin  

ote:  
ct  
nts  
ep  
ol)  
m,  
T  
re  
b  
ane knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:  
  
ark  
a is  
d | grep -i  
 wc -l); let  
iled/$total"" | bc |  
tp/f:  
; done  
d | grep -vi  
 wc -l); let  
iled/$total"" | bc |  
tp/f:  
; done  
he  
ing us  
day,  
ly  
  
per hour  
look  
om  
itting  
 made  
hink  
the  
later  
beer.  

---------------------------------------------------------------------  
For additional commands, e-mail: dev-help@spark.apache.org  

"
Fairiz Azizi <coderfi@gmail.com>,"Fri, 17 Oct 2014 22:31:09 -0700",Re: Spark on Mesos 0.20,Gurvinder Singh <gurvinder.singh@uninett.no>,"Hello,

Sorry for the delay (again), we were busy upgrading our cluster from MAPR
3.0.x to Mapr 3.1.1.26113.GA

I updated my builds to include referencing the native hadoop libraries by
this distribution as well as installing SNAPPY (I no longer see the 'unable
to load native hadoop libraries' and also see that it loads the SNAPPY
library).

I ran the example against a directory of ApacheLog files containing about
4.4GB, and things seem to work fine.

time MASTER=""mesos://xxxxxxxx*:5050*"" /opt/spark/current/bin/run-example
LogQuery ""maprfs:///user/hive/warehouse/apachelog/dt=20141017/16""

14/10/18 05:23:21 INFO scheduler.DAGScheduler: Stage 0 (collect at
LogQuery.scala:80) finished in 1.704 s
14/10/18 05:23:21 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0,
whose tasks have all completed, from pool default
14/10/18 05:23:21 INFO spark.SparkContext: Job finished: collect at
LogQuery.scala:80, took 40.533904277 s
(null,null,null) bytes=0 n=16682940

real 0m51.393s
user 0m19.130s
sys 0m4.120s

So this combination of software seems to work fine for me!

Spark 1.1.0
Mesos 0.20.1
MAPR 3.1.1.26113.GA
spark-1.1.0-bin-mapr3.tgz

Note: one thing you might try is increasing your spark.executor.memory
setting
Mine was set to 8GB in the spark-defaults.conf file.

Hope this helps,
Fi


Fairiz ""Fi"" Azizi


"
Davies Liu <davies@databricks.com>,"Fri, 17 Oct 2014 23:03:13 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Josh Rosen <rosenville@gmail.com>,"How can we know the changes has been applied? I had checked several
recent builds, they all use the original configs.

Davies

€™s see
te:
lugin
t
:
ts
l)
,
e
ne knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
rk
et
al"" | bc
i
let
al"" | bc
e
ng
y
m
he

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 17 Oct 2014 23:06:02 -0700","Re: short jenkins downtime -- trying to get to the bottom of
 the git fetch timeouts",Davies Liu <davies@databricks.com>,"I think that the fix was applied. Â Take a look atÂ https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21874/consoleFull

Here, I see a fetch command that mentions this specific PR branch rather than the wildcard that we had before:

 > git fetch --tags --progress https://github.com/apache/spark.git +refs/pull/2840/*:refs/remotes/origin/pr/2840/* # timeout=15

Do you have an example of a Spark PRB build thatâ€™s still failing with the old fetch failure?

- Josh

How can we know the changes has been applied? I had checked several  
recent builds, they all use the original configs.  

Davies  

ote:  
â€™s see  
  
llId}/*  
+plugin  
ect  
ents  
rep  
lol)  
am,  
OT  
are  
ub  
hane knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:  
t  
park  
ta  
ed | grep -i  
 wc -l); let  
ailed/$total"" | bc  
tp/f:  
e; done  
ed | grep -vi  
| wc -l); let  
ailed/$total"" | bc  
tp/f:  
e; done  
the  
ring  
  
nly  
a  
 per  
  
rom  
t  
 
 the  
  
  
u>  
"
Davies Liu <davies@databricks.com>,"Sat, 18 Oct 2014 00:44:25 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Josh Rosen <rosenville@gmail.com>,"Cool, the recent 4 build had used the new configs, thanks!

Let's run more builds.

Davies

consoleFull
with the
€™s
plugin
:
ct
e:
m,
re
b
ane knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
a
i
let
tal"" |
vi
 let
tal"" |
he
ly
r
om

---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Sat, 18 Oct 2014 08:46:48 -0700",Oryx + Spark mllib,dev <dev@spark.apache.org>,"Hi,

Is someone working on a project on integrating Oryx model serving layer
with Spark ? Models will be built using either Streaming data / Batch data
in HDFS and cross validated with mllib APIs but the model serving layer
will give API endpoints like Oryx
and read the models may be from hdfs/impala/SparkSQL

elastic...as requests grow we should be able to add more nodes...using play
and akka clustering module...

If there is a ongoing project on github please point to it...

Is there a plan of adding model serving and experimentation layer to mllib ?

Thanks.
Deb
"
Rajiv Abraham <rajiv.abraham@gmail.com>,"Sat, 18 Oct 2014 12:06:01 -0400",Re: Oryx + Spark mllib,Debasish Das <debasish.das83@gmail.com>,"Oryx 2 seems to be geared for Spark

https://github.com/OryxProject/oryx

2014-10-18 11:46 GMT-04:00 Debasish Das <debasish.das83@gmail.com>:




-- 
Take care,
Rajiv
"
Saurabh Wadhawan <Saurabh.Wadhawan@guavus.com>,"Sat, 18 Oct 2014 20:46:49 +0000",Joining the spark dev community,"""dev@spark.apache.org"" <dev@spark.apache.org>","How can I become a spark contributor.
What's the good path that I can follow to become an active code submitter for spark from a newbie.

Regards
- Saurabh

"
Koert Kuipers <koert@tresata.com>,"Sat, 18 Oct 2014 19:37:20 -0400",Re: Raise Java dependency from 6 to 7,Andrew Ash <andrew@andrewash.com>,"my experience is that there are still a lot of java 6 clusters out there.
also distros that bundle spark still support java 6

"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 18 Oct 2014 17:44:09 -0700",Re: Raise Java dependency from 6 to 7,Koert Kuipers <koert@tresata.com>,"I'd also wait a bit until these are gone. Jetty is unfortunately a much hairier topic by the way, because the Hadoop libraries also depend on Jetty. I think it will be hard to update. However, a patch that shades Jetty might be nice to have, if that doesn't require shading a lot of other stuff.

Matei

there.
for
2013
why
must be
to


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Sat, 18 Oct 2014 18:20:13 -0700",Re: Raise Java dependency from 6 to 7,Matei Zaharia <matei.zaharia@gmail.com>,"Hadoop, for better or worse, depends on an ancient version of Jetty
(6), that is even on a different package. So Spark (or anyone trying
to use a newer Jetty) is lucky on that front...

IIRC Hadoop is planning to move to Java 7-only starting with 2.7. Java
7 is also supposed to be EOL some time next year, so a plan to move to
Java 7 and, eventually, Java 8 would be nice.

ote:
airier topic by the way, because the Hadoop libraries also depend on Jetty. I think it will be hard to update. However, a patch that shades Jetty might be nice to have, if that doesn't require shading a lot of other stuff.
.
r
y
t be



-- 
Marcelo

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 18 Oct 2014 21:52:13 -0700",Submissions open for Spark Summit East 2015,"user@spark.incubator.apache.org,
 dev <dev@spark.apache.org>","After successful events in the past two years, the Spark Summit conference has expanded for 2015, offering both an event in New York on March 18-19 and one in San Francisco on June 15-17. The conference is a great chance to meet people from throughout the Spark community and see the latest news, tips and use cases.

Submissions are now open for Spark Summit East 2015, to be held in New York on March 18-19. If youâ€™d like to give a talk on use cases, neat applications, or ongoing Spark development, submit your talk online today at http://prevalentdesignevents.com/sparksummit2015/east/speaker/. Submissions will be open until December 6th, 2014.

If you missed this yearâ€™s Spark Summit, you can still find videos from all talks online at http://spark-summit.org/2014.

Hope to see you there,

Matei
---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sun, 19 Oct 2014 01:10:23 -0400",Re: Oryx + Spark mllib,Rajiv Abraham <rajiv.abraham@gmail.com>,"Yes, that is exactly what the next 2.x version does. Still in progress but
the recommender app and framework are code - complete. It is not even
specific to MLlib and could plug in other model build functions.

The current 1.x version will not use MLlib. Neither uses Play but is
intended to scale just by adding web servers however you usually do.

See graphflow too.

"
Nick Pentreath <nick.pentreath@gmail.com>,"Sun, 19 Oct 2014 08:22:32 +0200",Re: Oryx + Spark mllib,Sean Owen <sowen@cloudera.com>,"We've built a model server internally, based on Scalatra and Akka
Clustering. Our use case is more geared towards serving possibly thousands
of smaller models.

It's actually very basic, just reads models from S3 as strings (!!) (uses
HDFS FileSystem so can read from local, HDFS, S3) and uses Breeze for
linear algebra. (Technically it is also not dependent on Spark, it could be
reading models generated by any computation layer).

It's designed to allow scaling via cluster sharding, by adding nodes (but
could also support a load-balanced approach). Not using persistent actors
as doing a model reload on node failure is not a disaster as we have
multiple levels of fallback.

Currently it is a bit specific to our setup (and only focused on
recommendation models for now), but could with some work be made generic.
I'm certainly considering if we can find the time to make it a releasable
project.

vector computations, not the filtering-related and other things that come
as part of a recommender system (that is done elsewhere in our system). It
also does not handle the ingesting of data at all.


"
Debasish Das <debasish.das83@gmail.com>,"Sun, 19 Oct 2014 08:29:23 -0700",Re: Oryx + Spark mllib,Nick Pentreath <nick.pentreath@gmail.com>,"Hi Nick,

Any specific reason of choosing scalatra and not play/spray (now that they
are getting integrated) ?

Sean,

Would you be interested in a play and akka clustering based module in oryx2
and see how it compares against the servlets ? I am interested to
understand the scalability....

Thanks.
Deb


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sun, 19 Oct 2014 17:38:12 +0200",Re: Oryx + Spark mllib,Debasish Das <debasish.das83@gmail.com>,"Well, when I started development ~2 years ago, Scalatra just appealed more,
being more lightweight (I didn't need MVC just barebones REST endpoints),
and I still find its API / DSL much nicer to work with. Also, the swagger
API docs integration was important to me. So it's more familiarity than any
other reason.

If I were to build a model server from scratch perhaps Spray/Akka HTTP
would be the better way to go purely for integration purposes.

Having said that I think Scalatra is great and performant, so it's not a
no-brainer either way.


"
Sean Owen <sowen@cloudera.com>,"Sun, 19 Oct 2014 11:46:31 -0400",Re: Oryx + Spark mllib,Debasish Das <debasish.das83@gmail.com>,"Briefly, re: Oryx2, since the intent is for users to write their own
serving apps, I though JAX-RS would be more familiar to more
developers. I don't know how hard/easy REST APIs are in JAX-RS vs
anything else but I suspect it's not much different.

The interesting design decision that impacts scale is: do you
distribute scoring of each request across a cluster? the servlet-based
design does not and does everything in-core, in-memory.

Pros: Dead simple architecture. Hard to beat for low latency. Anything
more complex is big overkill for most models (RDF, k-means) -- except
recommenders.

Cons: For recommenders, harder to scale since everything is in-memory.
And that's a big ""but"".


---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sun, 19 Oct 2014 18:34:32 +0200",Re: Oryx + Spark mllib,Sean Owen <sowen@cloudera.com>,"The shared-nothing load-balanced server architecture works for all but the
most massive models - and even then a few big EC2 r3 instances should do
the trick.

recovery and potential for persistence.

For us arguably the sharding is somewhat overkill initially, but does allow
easy scaling in future where conceivably all models may not fit into single
machine memory.


"
Corey Nolet <cjnolet@gmail.com>,"Sun, 19 Oct 2014 12:48:13 -0400",Re: Raise Java dependency from 6 to 7,Marcelo Vanzin <vanzin@cloudera.com>,"A concrete plan and a definite version upon which the upgrade would be
applied sounds like it would benefit the community. If you plan far enough
out (as Hadoop has done) and give the community enough of a notice, I can't
see it being a problem as they would have ample time upgrade.




"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 19 Oct 2014 13:13:22 -0700",Re: Submissions open for Spark Summit East 2015,"user@spark.incubator.apache.org,
 dev <dev@spark.apache.org>","BTW several people asked about registration and student passes. Registration will open in a few weeks, and like in previous Spark Summits, I expect there to be a special pass for students.

Matei

conference has expanded for 2015, offering both an event in New York on March 18-19 and one in San Francisco on June 15-17. The conference is a great chance to meet people from throughout the Spark community and see the latest news, tips and use cases.
York on March 18-19. If youâ€™d like to give a talk on use cases, neat applications, or ongoing Spark development, submit your talk online today at http://prevalentdesignevents.com/sparksummit2015/east/speaker/. Submissions will be open until December 6th, 2014.
videos from all talks online at http://spark-summit.org/2014.


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sun, 19 Oct 2014 19:21:06 -0700",Re: Raise Java dependency from 6 to 7,Corey Nolet <cjnolet@gmail.com>,"So from my point of view, I'd do it maybe 1-2 years after all the major Hadoop vendors have stopped supporting Java 6. We're not there yet, but we will be soon. The reason is that the cost of staying on Java 6 is much smaller to us (as developers) than the cost of fragmenting the Spark community by having a big chunk of users unable to upgrade past a certain version of Spark (or requiring them to use a modified third-party version, which is a similar thing). There's very little stuff in Java 7 or 8 that would make the project much better if we dropped support for 6 -- this Jetty issue might be one, but we can certainly work around it.

We've done a lot of stuff to reach the broadest set of users, including pushing back the versions of Python and NumPy we support, and in my experience it's been well worth it. In surveys I've seen the majority of users (something like 75%) updating to each new Spark release within a few months of it coming out, which is awesome for keeping the community healthy.

Matei


applied sounds like it would benefit the community. If you plan far enough out (as Hadoop has done) and give the community enough of a notice, I can't see it being a problem as they would have ample time upgrade. 
much hairier topic by the way, because the Hadoop libraries also depend on Jetty. I think it will be hard to update. However, a patch that shades Jetty might be nice to have, if that doesn't require shading a lot of other stuff.
there.
priority for
2013
<http://www.oracle.com/technetwork/java/eol-135779.html>> and the last
<https://en.wikipedia.org/wiki/Java_version_history#Java_6_updates>>, why
now must be
fixes.
us to
<https://github.com/apache/spark/pull/2575/files#r18152125>> for
<https://github.com/apache/spark/pull/167#issuecomment-54544494>>
<https://github.com/apache/spark/pull/859>>
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Henry Saputra <henry.saputra@gmail.com>,"Sun, 19 Oct 2014 22:27:45 -0700",Re: Joining the spark dev community,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Saurabh,

Good way to start is to use Spark with your applications and file
issues you might have found and maybe provide patch for those or
existing ones.

Please take a look at Spark's how to contribute page [1] to help you
get started.

Hope this helps.

- Henry


[1] https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


---------------------------------------------------------------------


"
Yin Huai <huaiyin.thu@gmail.com>,"Mon, 20 Oct 2014 10:17:04 -0400",Get attempt number in a closure,dev@spark.apache.org,"Hello,

Is there any way to get the attempt number in a closure? Seems
TaskContext.attemptId actually returns the taskId of a task (see this
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/executor/Executor.scala#L181>
 and this
<https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/Task.scala#L47>).
It looks like a bug.

Thanks,

Yin
"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Oct 2014 10:57:42 -0700",Re: Get attempt number in a closure,Yin Huai <huaiyin.thu@gmail.com>,"I also ran into this earlier. It is a bug. Do you want to file a jira?

I think part of the problem is that we don't actually have the attempt id
on the executors. If we do, that's great. If not, we'd need to propagate
that over.


"
Yin Huai <huaiyin.thu@gmail.com>,"Mon, 20 Oct 2014 15:38:31 -0400",Re: Get attempt number in a closure,Reynold Xin <rxin@databricks.com>,"Yeah, seems we need to pass the attempt id to executors through
TaskDescription. I have created
https://issues.apache.org/jira/browse/SPARK-4014.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 20 Oct 2014 13:29:33 -0700",Re: Get attempt number in a closure,Yin Huai <huaiyin.thu@gmail.com>,"There is a deeper issue here which is AFAIK we don't even store a
notion of attempt inside of Spark, we just use a new taskId with the
same index.


---------------------------------------------------------------------


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Mon, 20 Oct 2014 13:45:35 -0700",Re: Get attempt number in a closure,Patrick Wendell <pwendell@gmail.com>,"Are you guys sure this is a bug?  In the task scheduler, we keep two
identifiers for each task: the ""index"", which uniquely identifiers the
computation+partition, and the ""taskId"" which is unique across all tasks
for that Spark context (See
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L439).
If multiple attempts of one task are run, they will have the same index,
but different taskIds.  Historically, we have used ""taskId"" and
""taskAttemptId"" interchangeably (which arose from naming in Mesos, which
uses similar naming).

This was complicated when Mr. Xin added the ""attempt"" field to TaskInfo,
which we show in the UI.  This field uniquely identifies attempts for a
particular task, but is not unique across different task indexes (it always
starts at 0 for a given task).  I'm guessing the right fix is to rename
Task.taskAttemptId to Task.taskId to resolve this inconsistency -- does
that sound right to you Reynold?

-Kay


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Mon, 20 Oct 2014 13:47:54 -0700",Re: Get attempt number in a closure,Patrick Wendell <pwendell@gmail.com>,"Sorry to clarify, there are two issues here:

(1) attemptId has different meanings in the codebase
(2) we currently don't propagate the 0-based per-task attempt identifier to
the executors.

(1) should definitely be fixed.  It sounds like Yin's original email was
requesting that we add (2).


"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Oct 2014 13:56:56 -0700",Re: Get attempt number in a closure,Kay Ousterhout <keo@eecs.berkeley.edu>,"Yes, as I understand it this is for (2).

Imagine a use case in which I want to save some output. In order to make
this atomic, the program uses part_[index]_[attempt].dat, and once it
finishes writing, it renames this to part_[index].dat.

Right now [attempt] is just the TID, which could show up like (assuming
this is not the first stage):

part_0_1000
part_1_1001
part_0_1002 (some retry)
...

This is fairly confusing. The natural thing to expect is

part_0_0
part_1_0
part_0_1
...




"
Yin Huai <huaiyin.thu@gmail.com>,"Mon, 20 Oct 2014 17:13:55 -0400",Re: Get attempt number in a closure,Reynold Xin <rxin@databricks.com>,"Yes, it is for (2). I was confused because the doc of TaskContext.attemptId
(release 1.1)
<http://spark.apache.org/docs/1.1.0/api/scala/index.html#org.apache.spark.TaskContext>
is
""the number of attempts to execute this task"". Seems the per-task attempt
id used to populate ""attempt"" field in the UI is maintained by
TaskSetManager and its value is assigned in resourceOffer.


"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 20 Oct 2014 18:51:44 -0400",something wrong with Jenkins or something untested merged?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I just submitted a patch https://github.com/apache/spark/pull/2864/files
with one line change

but the Jenkins told me it's failed to compile on the unrelated files?

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21935/console


Best,

Nan
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 20 Oct 2014 19:43:55 -0400",Building and Running Spark on OS X,dev <dev@spark.apache.org>,"If one were to put together a short but comprehensive guide to setting up
Spark to run locally on OS X, would it look like this?

brew install maven
# Set some important Java and Maven environment variables.export
JAVA_HOME=$(/usr/libexec/java_home)export MAVEN_OPTS=""-Xmx512m
-XX:MaxPermSize=128m""
# Go to where you downloaded the Spark source.cd ./spark
# Build, configure slaves, and startup Spark.
mvn -DskipTests clean packageecho ""localhost"" > ./conf/slaves
./sbin/start-all.sh
# Rock 'n' Roll.
./bin/pyspark
# Cleanup when you're done.
./sbin/stop-all.sh

Nick
â€‹
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 20 Oct 2014 16:56:34 -0700",Re: something wrong with Jenkins or something untested merged?,Nan Zhu <zhunanmcgill@gmail.com>,"I performed build on latest master branch but didn't get compilation error.

FYI


"
Nan Zhu <zhunanmcgill@gmail.com>,"Mon, 20 Oct 2014 20:11:55 -0400",Re: something wrong with Jenkins or something untested merged?,Ted Yu <yuzhihong@gmail.com>,"yes, I can compile locally, too 

but it seems that Jenkins is not happy now...https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/ 

All failed to compile

Best, 

-- 
Nan Zhu




"
Reynold Xin <rxin@databricks.com>,"Mon, 20 Oct 2014 17:00:21 -0700",Re: Building and Running Spark on OS X,Nicholas Chammas <nicholas.chammas@gmail.com>,"I usually use SBT on Mac and that one doesn't require any setup ...



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 20 Oct 2014 20:04:05 -0400",Re: Building and Running Spark on OS X,Reynold Xin <rxin@databricks.com>,"Yeah, I would use sbt too, but I thought if I wanted to publish a little
reference page for OS X users then I probably should use the â€œofficial
<https://github.com/apache/spark#building-spark>â€œ build instructions.

Nick
â€‹


p
"
Denny Lee <denny.g.lee@gmail.com>,"Mon, 20 Oct 2014 17:07:52 -0700",Re: Building and Running Spark on OS X,Reynold Xin <rxin@databricks.com>,"+1 
huge fan of sbt with OSX




---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 20 Oct 2014 17:08:36 -0700",Re: something wrong with Jenkins or something untested merged?,Nan Zhu <zhunanmcgill@gmail.com>,"hmm, strange.  i'll take a look.


"
Sean Owen <sowen@cloudera.com>,"Mon, 20 Oct 2014 20:15:45 -0400",Re: Building and Running Spark on OS X,Nicholas Chammas <nicholas.chammas@gmail.com>,"Maven is at least built in to OS X (well, with dev tools). You don't
even have to brew install it. Surely SBT isn't in the dev tools even?
I recall I had to install it. I'd be surprised to hear it required
zero setup.

icial
ons.
up

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 20 Oct 2014 17:16:25 -0700",Re: something wrong with Jenkins or something untested merged?,shane knapp <sknapp@berkeley.edu>,"The failure is in the Kinesis compoent, can you reproduce this if you
build with -Pkinesis-asl?

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 20 Oct 2014 20:19:26 -0400",Re: Building and Running Spark on OS X,Sean Owen <sowen@cloudera.com>,"I think starting in Mavericks, Maven is no longer included by default
<http://stackoverflow.com/questions/19678594/maven-not-found-in-mac-osx-mavericks>
.


e
fficial
tions.
g
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Mon, 20 Oct 2014 17:20:41 -0700 (PDT)",Re: Building and Running Spark on OS X,"""Sean Owen"" <sowen@cloudera.com>","The sbt executable that is in the spark repo can be used to build sbt without any other set up (it will download the sbt jars etc).


Thanks,
Hari


little
â€œofficial
instructions.
 up
MAVEN_OPTS=""-Xmx512m
org"
Sean Owen <sowen@cloudera.com>,"Mon, 20 Oct 2014 20:25:16 -0400",Re: Building and Running Spark on OS X,Hari Shreedharan <hshreedharan@cloudera.com>,"Oh right, we're talking about the bundled sbt of course.
And I didn't know Maven wasn't installed anymore!


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 20 Oct 2014 17:28:57 -0700",Re: something wrong with Jenkins or something untested merged?,Patrick Wendell <pwendell@gmail.com>,"ok, so earlier today i installed a 2nd JDK within jenkins (7u71), which
fixed the SparkR build but apparently made Spark itself quite unhappy.  i
removed that JDK, triggered a build (
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21943/console),
and it compiled kinesis w/o dying a fiery death.

apparently 7u71 is stricter when compiling.  sad times.

sorry about that!

shane



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 20 Oct 2014 20:28:48 -0400",Re: Building and Running Spark on OS X,Sean Owen <sowen@cloudera.com>,"So back to my original question... :)

If we wanted to post this guide to the user list or to a gist for easy
reference, would we rather have Maven or SBT listed? And is there anything
else about the steps that should be modified?

Nick


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 20 Oct 2014 17:32:35 -0700",Re: something wrong with Jenkins or something untested merged?,shane knapp <sknapp@berkeley.edu>,"Thanks Shane - we should fix the source code issues in the Kinesis
code that made stricter Java compilers reject it.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 20 Oct 2014 17:35:15 -0700",Re: something wrong with Jenkins or something untested merged?,shane knapp <sknapp@berkeley.edu>,"I created an issue to fix this:

https://issues.apache.org/jira/browse/SPARK-4021


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 20 Oct 2014 17:37:06 -0700",Re: something wrong with Jenkins or something untested merged?,Patrick Wendell <pwendell@gmail.com>,"thanks, patrick!

:)


"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Mon, 20 Oct 2014 20:42:22 -0400",Re: Building and Running Spark on OS X,Nicholas Chammas <nicholas.chammas@gmail.com>,"I also prefer sbt on Mac.

You might want to add checking for / getting Python 2.6+ (though most modern Macs should have it), and maybe numpy as an optional dependency. I often just point people to Anaconda.

— Jeremy

-------------------------
jeremyfreeman.net
@thefreemanlab


anything
sbt

"
Sean Owen <sowen@cloudera.com>,"Tue, 21 Oct 2014 07:41:07 -0400",Easy win: SBT plugin config expert to help on SPARK-3359?,"""dev@spark.apache.org"" <dev@spark.apache.org>","This one can be resolved, I think, with a bit of help from someone who
understands SBT + plugin config:

https://issues.apache.org/jira/browse/SPARK-3359

Just a matter of figuring out how to set a property on the plugin.
This would make Java 8 javadoc work much more nicely. Minor but
useful!

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 21 Oct 2014 20:30:43 +0800",Re: something wrong with Jenkins or something untested merged?,"shane knapp <sknapp@berkeley.edu>, 
 Patrick Wendell <pwendell@gmail.com>","Hm, seems that 7u71 comes back again. Observed similar Kinesis 
compilation error just now: 
https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/410/consoleFull

Checked Jenkins slave nodes, saw /usr/java/latest points to jdk1.7.0_71. 
However, /usr/bin/javac -version says:

    Eclipse Java Compiler 0.894_R34x, 3.4.2 release, Copyright IBM Corp
    2000, 2008. All rights reserved.

Which JDK is actually used by Jenkins?

Cheng



â€‹
"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 21 Oct 2014 09:33:14 -0400",Re: something wrong with Jenkins or something untested merged?,Cheng Lian <lian.cs.zju@gmail.com>,"just curiousâ€¦what is this â€œNewSparkPullRequestBuilderâ€?  

Best,  

--  
Nan Zhu



ion error just now: https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/410/consoleFull
71. However, /usr/bin/javac -version says:
 2000, 2008. All rights reserved.
ch fixed the SparkR build but apparently made Spark itself quite unhappy. i removed that JDK, triggered a build ( https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21943/console), and it compiled kinesis w/o dying a fiery death. apparently 7u71 is stricter when compiling. sa shane knapp <sknapp@berkeley.edu> (mailto:sknapp@berkeley.edu) wrote:  
Nan Zhu <zhunanmcgill@gmail.com> (mailto:zhunanmcgill@gmail.com) wrote:  
t happy now... https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequesilation  
gmail.com (mailto:zhunanmcgill@gmail.com)  
 compile on the unrelated  
21935/console  


"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 21 Oct 2014 21:22:37 +0800",Re: something wrong with Jenkins or something untested merged?,Nan Zhu <zhunanmcgill@gmail.com>,"It's a new pull request builder written by Josh, integrated into our 
state-of-the-art PR dashboard :)


"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 21 Oct 2014 09:39:11 -0400",Re: something wrong with Jenkins or something untested merged?,Cheng Lian <lian.cs.zju@gmail.com>,"seems that all PRs built by NewSparkPRBuilder suffers from 7u71, while SparkPRBuilder is working fine

Best,  

--  
Nan Zhu



ate-of-the-art PR dashboard :)
â€?  
ilation error just now: https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/410/consoleFull
0_71. However, /usr/bin/javac -version says:
Corp 2000, 2008. All rights reserved.
 which fixed the SparkR build but apparently made Spark itself quite unhappy. i removed that JDK, triggered a build ( https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21943/console), and it compiled kinesis w/o dying a fiery death. apparently 7u71 is stricter when compilingatrick Wendell <pwendell@gmail.com> (mailto:pwendell@gmail.com) wrote:  
 PM, shane knapp <sknapp@berkeley.edu> (mailto:sknapp@berkeley.edu) wrote:  
PM, Nan Zhu <zhunanmcgill@gmail.com> (mailto:zhunanmcgill@gmail.com) s not happy now... https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRecompilation  
ill@gmail.com (mailto:zhunanmcgill@gmail.com)  
d to compile on the unrelated  
der/21935/console  

"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 21 Oct 2014 09:48:15 -0400",Re: something wrong with Jenkins or something untested merged?,Cheng Lian <lian.cs.zju@gmail.com>,"weirdâ€¦..two buildings (one triggered by New, one triggered by Old) were executed in the same node, amp-jenkins-slave-01, one compiles, one notâ€¦

Best,  

--  
Nan Zhu



SparkPRBuilder is working fine
state-of-the-art PR dashboard :)
erâ€?  
mpilation error just now: https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/410/consoleFull
7.0_71. However, /usr/bin/javac -version says:
M Corp 2000, 2008. All rights reserved.
), which fixed the SparkR build but apparently made Spark itself quite unhappy. i removed that JDK, triggered a build ( https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21943/console), and it compiled kinesis w/o dying a fiery death. apparently 7u71 is stricter when compili Patrick Wendell <pwendell@gmail.com> (mailto:pwendell@gmail.com) wrote:  
08 PM, shane knapp <sknapp@berkeley.edu> (mailto:sknapp@berkeley.edu)1 PM, Nan Zhu <zhunanmcgill@gmail.com> (mailto:zhunanmcgill@gmail.com is not happy now... https://amplab.cs.berkeley.edu/jenkins/job/SparkPullt compilation  
cgill@gmail.com (mailto:zhunanmcgill@gmail.com)  
led to compile on the unrelated  
ilder/21935/console  

"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Tue, 21 Oct 2014 02:23:00 -0700 (PDT)",[MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi,
I am new to Apache Spark (any open source project). I want to contribute to
it. I found that MLlib has no algorithm for outlier detection yet.  By
literature review I found the algorithm Attribute Value Frequency (AVF) is
promising. Here is the link  DOI: 10.1109/ICTAI.2007.125

By following the process I figured out that, I have to open a new feature
request at JIRA (https://issues.apache.org/jira/browse/SPARK). Also, I have
checked that no other issue is opened on ""outlier detection"".

I want to know is it the right way to go? What project owners have in mind
about outlier detection? Also is anybody working on parallel K nearest
neighbour?

Apart from opening up the feature request then pull request from git, How to
provide the test cases? 

Suggestions and guidance are welcome.

Thanks,
Ashutosh 



--

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 21 Oct 2014 09:40:00 -0700",Re: [MLlib] Contributing Algorithm for Outlier Detection,Ashutosh <ashutosh.trivedi@iiitb.org>,"Hi Ashutosh,

The process you described is correct, with details documented in
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark
. There is no outlier detection algorithm in MLlib. Before you start
coding, please open an JIRA and let's discuss which algorithms are
appropriate to include, because there are many outlier detection
algorithms. I'm not sure which one is general enough and easy to
implement in parallel. For example, I'm not familiar with the
algorithm you mentioned, while the one I'm familiar with is based on
leverage scores: http://en.wikipedia.org/wiki/Leverage_(statistics)

Best,
Xiangrui


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Tue, 21 Oct 2014 10:05:17 -0700",Re: something wrong with Jenkins or something untested merged?,"Nan Zhu <zhunanmcgill@gmail.com>, Cheng Lian
 <lian.cs.zju@gmail.com>","I find it concerning that thereâ€™s a JDK version that breaks out build, since weâ€™re supposed to support Java 7. Â Is 7u71 an upgrade or downgrade from the JDK that we used before? Â Is there an easy way to fix our build so that it compiles with 7u71â€™s stricter settings?

Iâ€™m not sure why the â€œNewâ€ PRB is failing here. Â It was originally created as a clone of the main pull request builder job. I checked the configuration history and confirmed that there arenâ€™t any settings that weâ€™ve forgotten to copy over (e.g. their configurations havenâ€™t diverged), so Iâ€™m not sure whatâ€™s causing this.

- Josh

te:
weirdâ€¦..two buildings (one triggered by New, one triggered by Old) were executed in the same node, amp-jenkins-slave-01, one compiles, one notâ€¦  

Best,  

--  
Nan Zhu  



SparkPRBuilder is working fine  
state-of-the-art PR dashboard :)  
erâ€?  
mpilation error just now: https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/410/consoleFull  
7.0_71. However, /usr/bin/javac -version says:  
M Corp 2000, 2008. All rights reserved.  
), which fixed the SparkR build but apparently made Spark itself quite unhappy. i removed that JDK, triggered a build ( https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21943/console), and it compiled kinesis w/o dying a fiery death. apparently 7u71 is stricter when compili Patrick Wendell <pwendell@gmail.com> (mailto:pwendell@gmail.com) wrote:  
08 PM, shane knapp <sknapp@berkeley.edu> (mailto:sknapp@berkeley.edu)1 PM, Nan Zhu <zhunanmcgill@gmail.com> (mailto:zhunanmcgill@gmail.com is not happy now... https://amplab.cs.berkeley.edu/jenkins/job/SparkPullt compilation  
cgill@gmail.com (mailto:zhunanmcgill@gmail.com)  
led to compile on the unrelated  
ilder/21935/console  

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 21 Oct 2014 10:08:58 -0700",Re: something wrong with Jenkins or something untested merged?,Josh Rosen <rosenville@gmail.com>,"Josh - the errors that broke our build indicated that JDK5 was being
used. Somehow the upgrade caused our build to use a much older Java
version. See the JIRA for more details.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 21 Oct 2014 13:09:37 -0400",Re: something wrong with Jenkins or something untested merged?,Josh Rosen <rosenville@gmail.com>,"Given the nature of the error, I would be really, really shocked if
Java 7u71 were actually being used in the failing build, so no I do
not thing the problem has to do with 7u71 per se.

As I'd expect I see no changes to javac in this update from 7u65, and
no chatter about crazy javac regressions.

It is complaining that @Override doesn't apply to implemented
interface methods. This is allowed in Java 6+. Either something is
really using Java 5 -- or some new config has kicked in setting
-source=1.5. (Something also seems to have enabled -Xlint:all or
similar.)

Given Shane's finding about the Eclipse compiler apparently being
invoked, I think that's the problem, or it's of that form.

uild, since weâ€™re supposed to support Java 7.  Is 7u71 an upgrade or downgrade from the JDK that we used before?  Is there an easy way to fix our build so that it compiles with 7u71â€™s stricter settings?
It was originally created as a clone of the main pull request builder job. I checked the configuration history and confirmed that there arenâ€™t any settings that weâ€™ve forgotten to copy over (e.g. their configurations havenâ€™t diverged), so Iâ€™m not sure whatâ€™s causing this.
:
) were executed in the same node, amp-jenkins-slave-01, one compiles, one notâ€¦
parkPRBuilder is working fine
tate-of-the-art PR dashboard :)
râ€?
pilation error just now: https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/410/consoleFull
.0_71. However, /usr/bin/javac -version says:
orp 2000, 2008. All rights reserved.
, which fixed the SparkR build but apparently made Spark itself quite unhappy. i removed that JDK, triggered a build ( https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/21943/console), and it compiled kinesis w/o dying a fiery death. apparently 7u71 is stricter when compiling. sad t PM, Nan Zhu <zhunanmcgill@gmail.com> (mailto:zhunanmcgill@gmail.com) wrote:
is not happy now... https://amplab.cs.berkeley.edu/jenkins/job/SparkPullReq compilation
ll@gmail.com (mailto:zhunanmcgill@gmail.com)
ed to compile on the unrelated
lder/21935/console

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Tue, 21 Oct 2014 10:11:02 -0700",Re: something wrong with Jenkins or something untested merged?,Patrick Wendell <pwendell@gmail.com>,"Ah, that makes sense. Â I had forgotten that there was a JIRA for this:

https://issues.apache.org/jira/browse/SPARK-4021 Â 


Josh - the errors that broke our build indicated that JDK5 was being  
used. Somehow the upgrade caused our build to use a much older Java  
version. See the JIRA for more details.  

te:  
since  
the  
t it  
reated  
ation  
en to  
  
rote:  
re  
..  
  
  
ilder/410/consoleFull  
BM  
1),  
quite unhappy.  
lder/21943/console),  
71 is stricter  
ct 20, 2014 at  
ll@gmail.com)  
is  
 2014 at 5:08 PM,  
11  
s is  
estBuilder/ All  
, 2014 at 7:56 PM,  
et  
.com)  
iled  
uilder/21935/console  
"
Nan Zhu <zhunanmcgill@gmail.com>,"Tue, 21 Oct 2014 14:07:54 -0400",Re: something wrong with Jenkins or something untested merged?,Josh Rosen <rosenville@gmail.com>,"I agree with Sean

I just compiled spark core successfully with 7u71 in Mac OS X


"
shane knapp <sknapp@berkeley.edu>,"Tue, 21 Oct 2014 11:51:37 -0700",Re: something wrong with Jenkins or something untested merged?,Nan Zhu <zhunanmcgill@gmail.com>,"i'm currently in a meeting and will be starting to do some tests in ~1 hour
or so.


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Tue, 21 Oct 2014 12:41:10 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi Xiangrui,

Thanks for the reply. AVF is not so difficult to implement in parallel. It
just calculate the frequency of each attribute and calculate the overall
advantage of it is that it does not calculate distance, so in that sense it
is general.

I have to look at the one you pointed out. It calculates Hat matrix and I am
not sure about calculating Hat matrix in parallel, but Mahalanobis Distance
can be implemented. http://en.wikipedia.org/wiki/Mahalanobis_distance 

I have Opened the JIRA.
 https://issues.apache.org/jira/browse/SPARK-4038
Lets discuss it over there.

Regards,
Ashutosh



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 21 Oct 2014 13:52:50 -0700",Re: something wrong with Jenkins or something untested merged?,Nan Zhu <zhunanmcgill@gmail.com>,"ok, i did some testing and found out what's happening.

https://issues.apache.org/jira/browse/SPARK-4021

here's the TL;DR:
jenkins ignores what JDKs are installed via the web interface when there's
more than one defined, and falls back to whatever is default on the slave
the test is run on.  in this case, it's openjdk 7u65...  and spark
compilation fails.  i've removed the 2nd JDK (7u71) from jenkins, and
everything is back to normal.


"
shane knapp <sknapp@berkeley.edu>,"Tue, 21 Oct 2014 14:19:25 -0700","Re: short jenkins downtime -- trying to get to the bottom of the git
 fetch timeouts",Davies Liu <davies@databricks.com>,"i've seen a few more builds fail w/timeouts and it appears that we're
definitely NOT hitting any rate limiting.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/22005/console

[jenkins@amp-jenkins-slave-01 ~]$ curl -i -H ""Authorization: token
<REDACTED>"" https://api.github.com | grep Rate
X-RateLimit-Limit: 5000
X-RateLimit-Remaining: 4997
X-RateLimit-Reset: 1413929848
Access-Control-Expose-Headers: ETag, Link, X-GitHub-OTP, X-RateLimit-Limit,
X-RateLimit-Remaining, X-RateLimit-Reset, X-OAuth-Scopes,
X-Accepted-OAuth-Scopes, X-Poll-Interval


:
consoleFull
r
g with the
â€™s
lugin
T
shane knapp<sknapp@berkeley.edu>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
st
 -i
;
total"" |
 -vi
);
total"" |
f
 a
o
't
f
f
e
"
Theodore Si <sjyzhxw@gmail.com>,"Wed, 22 Oct 2014 19:00:08 +0800",Which part of the code deals with communication?,dev@spark.apache.org,"Hi all,

Workers will exchange data in between, right?

What classes are in charge of these actions?

---------------------------------------------------------------------


"
Manoj Awasthi <awasthi.manoj@gmail.com>,"Wed, 22 Oct 2014 18:56:14 +0530",Graphx connectComponents API,dev@spark.apache.org,"Hi Guys,

I am trying something very basic. I am using GraphX to load a graph from an
edge list file which is like this:

*220 224*
*400 401*
*220 221*

So it has following nodes (just for the sake of understanding - bear with
me for drawing):

*220 => 224          400 => 401 *
* ||*

* v 221*

Clearly, there are two ""connected components"" in this graph (please CMIIW).
Following is my code:


*val graph = GraphLoader.edgeListFile(sc, inputFile)*

*val componentLowestVertexGraph = graph.connectedComponents*
*componentLowestVertexGraph.vertices.collect.foreach(x => {*
*      println(x._1) // print node id's*
*  })*

gives me following result:

*224*
*401*
*220*
*221*
*400*

Per the documentation of connectedComponents:

*return a graph with the vertex value containing the lowest vertex id in
the connected component containing that vertex.*

So I was expecting to get two vertices returned for above cases.

Can some one point out if I am missing something?

Manoj
"
Manoj Awasthi <awasthi.manoj@gmail.com>,"Wed, 22 Oct 2014 19:06:17 +0530",Re: Graphx connectComponents API,dev@spark.apache.org,"Well - resolved.

The problem was in my understanding. It returns the graph with vertex
""data"" set to the connected components.

Thanks.


"
Holden Karau <holden@pigscanfly.ca>,"Wed, 22 Oct 2014 09:05:42 -0700",Re: Easy win: SBT plugin config expert to help on SPARK-3359?,Sean Owen <sowen@cloudera.com>,"Hi Sean,

I've pushed a PR for this https://github.com/apache/spark/pull/2893 :)

Cheers,

Holden :)




-- 
Cell : 425-233-8271
"
Bill Bejeck <bbejeck@gmail.com>,"Wed, 22 Oct 2014 12:52:19 -0400",SPARK-3299 jira task question,dev@spark.apache.org,"Since this task involves making changes to some of core functionality, I
figured it's best if I share my intents for completing this task.

This change is a little more involved as it requires modifying the Catalog
trait.
My current plan is to add an abstract method to the Catalog trait and have
any objects extending Catalog override a 'listTables' method.
The SQLContext object uses an instance of SimpleCatalog for any table
operations. Any additional public method defined to list the tables will
delegate to the SimpleCatalog instance. This approach follows the pattern
used for registering tables with the SQLContext.

Any feedback is appreciated.

Thanks,
Bill
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 22 Oct 2014 10:24:11 -0700",Re: Which part of the code deals with communication?,Theodore Si <sjyzhxw@gmail.com>,"The best documentation about communication interfaces is the
SecurityManager doc written by Tom Graves. With this as a starting
point I'd recommend digging through the code for each component.

https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SecurityManager.scala#L59


---------------------------------------------------------------------


"
Ashwin Shankar <ashwinshankar77@gmail.com>,"Wed, 22 Oct 2014 11:47:21 -0700",Multitenancy in Spark - within/across spark context,"dev@spark.apache.org, user@spark.apache.org","Hi Spark devs/users,
suit us for our ETL needs, and one of requirements is multi tenancy.
I did read the official doc
<http://spark.apache.org/docs/latest/job-scheduling.html> and the book, but
I'm still not clear on certain things.

Here are my questions :
1. *Sharing spark context* : How exactly multiple users can share the
cluster using same spark
    context ? UserA wants to run AppA, UserB wants to run AppB. How do they
talk to same
    context ? How exactly are each of their jobs scheduled and run in same
context?
    Is preemption supported in this scenario ? How are user names passed on
to the spark context ?

2. *Different spark context in YARN*: assuming I have a YARN cluster with
queues and preemption
    configured. Are there problems if executors/containers of a spark app
are preempted to allow a
    high priority spark app to execute ? Would the preempted app get stuck
or would it continue to
    make progress? How are user names passed on from spark to yarn(say I'm
using nested user
    queues feature in fair scheduler) ?

3. Sharing RDDs in 1 and 2 above ?

4. Anything else about user/job isolation ?

I know I'm asking a lot of questions. Thanks in advance :) !

-- 
Thanks,
Ashwin
Netflix
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 22 Oct 2014 12:07:44 -0700",Re: Multitenancy in Spark - within/across spark context,Ashwin Shankar <ashwinshankar77@gmail.com>,"Hi Ashwin,

Let me try to answer to the best of my knowledge.


That's not something you might want to do usually. In general, a
SparkContext maps to a user application, so each user would submit
their own job which would create its own SparkContext.

If you want to go outside of Spark, there are project which allow you
to manage SparkContext instances outside of applications and
potentially share them, such as
https://github.com/spark-jobserver/spark-jobserver. But be sure you
actually need it - since you haven't really explained the use case,
it's not very clear.


As far as I understand, this will cause executors to be killed, which
means that Spark will start retrying tasks to rebuild the data that
was held by those executors when needed. Yarn mode does have a
configurable upper limit on the number of executor failures, so if
your jobs keeps getting preempted it will eventually fail (unless you
tweak the settings).

I don't recall whether Yarn has an API to cleanly allow clients to
stop executors when preempted, but even if it does, I don't think
that's supported in Spark at the moment.


Spark will try to run the job as the requesting user; if you're not
using Kerberos, that means the process themselves will be run as
whatever user runs the Yarn daemons, but the Spark app will be run
inside a ""UserGroupInformation.doAs()"" call as the requesting user. So
technically nested queues should work as expected.


I'll assume you don't mean actually sharing RDDs in the same context,
but between different SparkContext instances. You might (big might
here) be able to checkpoint an RDD from one context and load it from
another context; that's actually like some HA-like features for Spark
drivers are being addressed.

The job server I mentioned before, which allows different apps to
share the same Spark context, has a feature to share RDDs by name,
also, without having to resort to checkpointing.

Hope this helps!

-- 
Marcelo

---------------------------------------------------------------------


"
Ashwin Shankar <ashwinshankar77@gmail.com>,"Wed, 22 Oct 2014 14:17:55 -0700",Re: Multitenancy in Spark - within/across spark context,Marcelo Vanzin <vanzin@cloudera.com>,"Thanks Marcelo, that was helpful ! I had some follow up questions :

That's not something you might want to do usually. In general, a

My question was basically this. In this
<http://spark.apache.org/docs/latest/job-scheduling.html> page in the
official doc, under  ""Scheduling within an application"" section, it talks
about multiuser and fair sharing within an app. How does multiuser within
an application work(how users connect to an app,run their stuff) ? When
would I want to use this ?

As far as I understand, this will cause executors to be killed, which

I basically wanted to find out if there were any ""gotchas"" related to
preemption on Spark. Things like say half of an application's executors got
preempted say while doing reduceByKey, will the application progress with
the remaining resources/fair share ?

I'm new to spark, sry if I'm asking something very obvious :).

Thanks,
Ashwin





-- 
Thanks,
Ashwin
"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 22 Oct 2014 17:22:47 -0400",Sharing spark context across multiple spark sql cli initializations,dev@spark.apache.org,"We want to run multiple instances of spark sql cli on our yarn cluster.
Each instance of the cli is to be used by a different user. This would be
non-optimal if each user brings up a different cli given how spark works on
yarn by running executor processes (and hence consuming resources) on
worker nodes for the lifetime of the application. Imagine each user trying
to cache a table in memory when there is limited memory across the
cluster.  The right way seems like to use the same spark context shared
across multiple initializations and running just one spark sql application.
Is my understanding correct about resource usage on yarn for spark-sql? Is
there a way to do the sharing of spark context currently ? Seem like it
needs some kind of thrift interface hooked into the cli driver.

*Apologies if you have already seen this on user group*
"
Sadhan Sood <sadhan.sood@gmail.com>,"Wed, 22 Oct 2014 15:18:26 -0400",Fwd: Sharing spark context across multiple spark sql cli initializations,dev@spark.apache.org,"We want to run multiple instances of spark sql cli on our yarn cluster.
Each instance of the cli is to be used by a different user. This looks
non-optimal if each user brings up a different cli given how spark works on
yarn by running executor processes (and hence consuming resources) on
worker nodes for the lifetime of the application. So, the right way seems
like to use the same spark context shared across multiple initializations
and running just one spark sql application. Is the understanding correct ?
Is there a way to do it currently ? Seem like it needs some kind of thrift
interface hooked into the cli driver.
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 22 Oct 2014 14:36:00 -0700",Re: Multitenancy in Spark - within/across spark context,Ashwin Shankar <ashwinshankar77@gmail.com>,"
I see. The way I read that page is that Spark supports all those
scheduling options; but Spark doesn't give you the means to actually
be able to submit jobs from different users to a running SparkContext
hosted on a different process. For that, you'll need something like
the job server that I referenced before, or write your own framework
for supporting that.

Personally, I'd use the information on that page when dealing with
concurrent jobs in the same SparkContext, but still restricted to the
same user. I'd avoid trying to create any application where a single
SparkContext is trying to be shared by multiple users in any way.


Jobs should still make progress as long as at least one executor is
available. The gotcha would be the one I mentioned, where Spark will
fail your job after ""x"" executors failed, which might be a common
occurrence when preemption is enabled. That being said, it's a
configurable option, so you can set ""x"" to a very large value and your
job should keep on chugging along.

The options you'd want to take a look at are: spark.task.maxFailures
and spark.yarn.max.executor.failures

-- 
Marcelo

---------------------------------------------------------------------


"
catchmonster <skacanski@gmail.com>,"Wed, 22 Oct 2014 16:13:21 -0700 (PDT)",Development testing code,dev@spark.incubator.apache.org,"Hi,
If developing in python, what is preffered way to do unit testing?
Do I use pyunit framework or I need to go with scalaTest?




--

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Wed, 22 Oct 2014 17:54:38 -0700",Re: Development testing code,catchmonster <skacanski@gmail.com>,"Hi,

Many tests in pyspark are implemented as doctests and the python
unittesting framework is also used for additional tests.

Cheers,

Holden :)




-- 
Cell : 425-233-8271
"
Varadharajan Mukundan <srinathsmn@gmail.com>,"Thu, 23 Oct 2014 12:04:43 +0530",Exception while running unit tests that makes use of local-cluster mode,dev@spark.apache.org,"Hi All,

When i try to run unit tests that makes use of local-cluster mode (Ex:
""Accessing HttpBroadcast variables in a local cluster"" in
BroadcastSuite.scala), its failing with the below exception. I'm using
java version ""1.8.0_05"" and scala version  2.10. I tried to look into
the jenkins build report and its passing over there. Please let me
know how i can resolve this issue.

Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times,
most recent failure: Lost task 0.3 in stage 0.0 (TID 6,
192.168.43.112): java.lang.ClassNotFoundException:
org.apache.spark.broadcast.BroadcastSuite$$anonfun$3$$anonfun$19
        java.net.URLClassLoader$1.run(URLClassLoader.java:372)
        java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        java.security.AccessController.doPrivileged(Native Method)
        java.net.URLClassLoader.findClass(URLClassLoader.java:360)
        java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        java.lang.Class.forName0(Native Method)
        java.lang.Class.forName(Class.java:340)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:59)
        java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
        java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
org.apache.spark.SparkException: Job aborted due to stage failure:
Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3
in stage 0.0 (TID 6, 192.168.43.112):
java.lang.ClassNotFoundException:
org.apache.spark.broadcast.BroadcastSuite$$anonfun$3$$anonfun$19
        java.net.URLClassLoader$1.run(URLClassLoader.java:372)
        java.net.URLClassLoader$1.run(URLClassLoader.java:361)
        java.security.AccessController.doPrivileged(Native Method)
        java.net.URLClassLoader.findClass(URLClassLoader.java:360)
        java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        java.lang.Class.forName0(Native Method)
        java.lang.Class.forName(Class.java:340)
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:59)
        java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1613)
        java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1518)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1774)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)
        org.apache.spark.scheduler.Task.run(Task.scala:56)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:181)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        java.lang.Thread.run(Thread.java:745)
Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1191)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1180)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1179)
at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1179)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:694)
at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:694)
at scala.Option.foreach(Option.scala:236)
at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:694)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1397)
at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1352)
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
at akka.actor.ActorCell.invoke(ActorCell.scala:487)
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
at akka.dispatch.Mailbox.run(Mailbox.scala:220)
at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)


Process finished with exit code 0
.

-- 
Thanks,
M. Varadharajan

---------------------------------------------------------------------


"
Jianshi Huang <jianshi.huang@gmail.com>,"Thu, 23 Oct 2014 17:56:39 +0800",Re: Multitenancy in Spark - within/across spark context,Marcelo Vanzin <vanzin@cloudera.com>,"Upvote for the multitanency requirement.

I'm also building a data analytic platform and there'll be multiple users
control of resource size. Users don't really know how much nodes they need,
they always use as much as possible... The result is lots of wasted
resource in our Yarn cluster.

A way to 1) allow multiple spark context to share the same resource or 2)
add dynamic resource management for Yarn mode is very much wanted.

Jianshi




-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
RJ Nowling <rnowling@gmail.com>,"Thu, 23 Oct 2014 05:58:47 -0400",PR for Hierarchical Clustering Needs Review,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

A few months ago, I collected feedback on what the community was looking
for in clustering methods.  A number of the community members requested a
divisive hierarchical clustering method.

Yu Ishikawa has stepped up to implement such a method.  I've been working
with him to communicate what I heard the community request and to review
and improve his code.

You can find the JIRA here:
https://issues.apache.org/jira/browse/SPARK-2429

He has now submitted a PR:
https://github.com/apache/spark/pull/2906

I was hoping Xiangrui, other committers, and community members would be
willing to take a look?  It's quite a large patch so it'll need extra
attention.

Thank you,
RJ

-- 
em rnowling@gmail.com
c 954.496.2314
"
Tom Hubregtsen <thubregtsen@gmail.com>,"Thu, 23 Oct 2014 03:35:39 -0700 (PDT)",Memory,dev@spark.incubator.apache.org,"Hi all,

I would like to validate my understanding of memory regions in Spark. Any
comments on my description below would be appreciated!

Execution is split up into stages, based on wide dependencies between RDDs
and actions such as save. All transformations involving narrow dependencies
before this wide dependency (or action) are pipelined. When Spark uses HDFS,
input data is loaded into memory according to the partitioning used in the
HDFS. As Spark has three regions (general, shuffle and storage), and this
does not yet involve an explicit cache nor a shuffle, I'll assume it goes
into general. During the pipelined execution of transformations with narrow
dependencies, it stays here, using the same partitioning, until we reach a
wide dependency (or an action). It then acquires memory from the shuffle
region, and spills to disk when there is no sufficient amount of memory
available. The result is passed in an iterator (located in the general
space) and the shuffle region is freed.

the storage region. This will guarantee availability for future use, but
also save space from the general region.

Question 1: Is this correct?
Question 2: How big is the general region?
Example: When I tell Spark I have 4GB, but my system actually has 16. I see
that the parameters shuffle and storage are defined (default: 20% and 60%),
but it does not seem that the general area is bounded by this. Will spark
use:
Shuffle: 0.2*4=0.8
Storage: 0.6*4=2.4
General: 1-(0.2+0.6)*4=0.8
Or
Shuffle: max(0.2*4)=max(0.8) //As we acquire from a counter, and not
actually divided memory regions
Storage: max(0.6*4)=max(2.4)
General: 16-actualUsage(shuffle+storage) or 4-actualUsage(shuffle+storage)



--

---------------------------------------------------------------------


"
Varadharajan Mukundan <srinathsmn@gmail.com>,"Thu, 23 Oct 2014 18:17:56 +0530","Re: Exception while running unit tests that makes use of
 local-cluster mode",dev@spark.apache.org,"Hi All,

I just figured it out that it fails whenever its run from Intellij. I
think it relates to the classpath issues mentioned in
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-ScalaTestIssues
.

-- 
Thanks,
M. Varadharajan

---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Thu, 23 Oct 2014 07:37:27 -0700",Re: reading/writing parquet decimal type,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei,

Another thing occurred to me. Will the binary format you're writing sort the data in numeric order? Or would the decimals have to be decoded for comparison?

Cheers,

Michael


though many encodings of int64 can probably do the right thing. We can look into supporting multiple ways to do this -- the spec does say that you should at least be able to read int32s and int64s.
patch and do have a question. It appears you're limiting the precision of decimals written to parquet to those that will fit in a long, yet you're writing the values as a parquet binary type. Why not write them using the int64 parquet type instead?
https://github.com/mateiz/spark/tree/decimal. I'll make some pull requests with these features soon, but meanwhile you can try this branch. See https://github.com/mateiz/spark/compare/decimal for the individual commits that went into it. It has exactly the precision stuff you need, plus some optimizations for working on decimals.
the Parquet Decimal converted type. The first thing I did was update the Spark parquet dependency to version 1.5.0, as this version introduced support for decimals in parquet. However, conversion between the catalyst decimal type and the parquet decimal type is complicated by the fact that the catalyst type does not specify a decimal precision and scale but the parquet type requires them.
scale to the catalyst decimal type? The catalyst decimal type would have unspecified precision and scale by default for backwards compatibility, but users who want to serialize a SchemaRDD with decimal(s) to parquet would have to narrow their decimal type(s) by specifying a precision and scale.
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Thu, 23 Oct 2014 07:43:10 -0700",Receiver/DStream storage level,dev@spark.apache.org,"I'm implementing a custom ReceiverInputDStream and I'm not sure how to initialize the Receiver with the storage level. The storage level is set on the DStream, but there doesn't seem to be a way to pass it to the Receiver. At the same time, setting the storage level separately on the Receiver seems to introduce potential confusion as the storage level of the DStream can be set separately. Is this desired behavior---to have distinct DStream and Receiver storage levels? Perhaps I'm missing something? Also, the storageLevel property of the Receiver[T] class is undocumented.

Cheers,

Michael
---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 23 Oct 2014 10:06:53 -0700",Re: Multitenancy in Spark - within/across spark context,Jianshi Huang <jianshi.huang@gmail.com>,"You may want to take a look at https://issues.apache.org/jira/browse/SPARK-3174.




-- 
Marcelo

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Thu, 23 Oct 2014 14:03:24 -0400",scalastyle annoys me a little bit,"""dev@spark.apache.org"" <dev@spark.apache.org>","100 max width seems very restrictive to me.

even the most restrictive environment i have for development (ssh with
emacs) i get a lot more characters to work with than that.

personally i find the code harder to read, not easier. like i kept
wondering why there are weird newlines in the
middle of constructors and such, only to realise later it was because of
the 100 character limit.

also, i find ""mvn package"" erroring out because of style errors somewhat
excessive. i understand that a pull request needs to conform to ""the style""
before being accepted, but this means i cant even run tests on code that
does not conform to the style guide, which is a bit silly.

i keep going out for coffee while package and tests run, only to come back
for an annoying error that my line is 101 characters and therefore nothing
ran.

is there some maven switch to disable the style checks?

best! koert
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 23 Oct 2014 11:07:09 -0700",Re: scalastyle annoys me a little bit,Koert Kuipers <koert@tresata.com>,"Hey Koert,

I think disabling the style checks in maven package could be a good
idea for the reason you point out. I was sort of mixed on that when it
was proposed for this exact reason. It's just annoying to developers.

In terms of changing the global limit, this is more religion than
anything else, but there are other cases where the current limit is
useful (e.g. if you have many windows open in a large screen).

- Patrick


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 23 Oct 2014 11:08:51 -0700",Re: scalastyle annoys me a little bit,Koert Kuipers <koert@tresata.com>,"I know this is all very subjective, but I find long lines difficult to read.

I also like how 100 characters fit in my editor setup fine (split wide
screen), while a longer line length would mean I can't have two
buffers side-by-side without horizontal scrollbars.

I think it's fine to add a switch to skip the style tests, but then,
you'll still have to fix the issues at some point...





-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 23 Oct 2014 11:14:29 -0700",Re: scalastyle annoys me a little bit,Patrick Wendell <pwendell@gmail.com>,"Koert:
Have you tried adding the following on your commandline ?


Cheers


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 23 Oct 2014 12:06:29 -0700",Spark 1.2 feature freeze on November 1,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just a reminder that as planned [1] we'll go into a feature freeze on
up-or-down call on any patches that go into that branch, along with
individual committers.

It is common for us to receive a very large volume of patches near the
deadline. The highest priority will be fixes and features that are in
review and were submitted earlier in the window. As a heads up, new
feature patches that are submitted in the next week have a good chance
of being pushed after 1.2.

During this coming weeks, I'd like to invite the community to help
with code review, testing patches, helping isolate bugs, our test
infra, etc. In past releases, community participation has helped
increase our ability to merge patches substantially. Individuals
really can make a huge difference here!

[1] https://cwiki.apache.org/confluence/display/SPARK/Wiki+Homepage

- Patrick

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Thu, 23 Oct 2014 15:07:14 -0400",Re: scalastyle annoys me a little bit,Ted Yu <yuzhihong@gmail.com>,"Hey Ted,
i tried:

no luck, still get
[ERROR] Failed to execute goal
org.scalastyle:scalastyle-maven-plugin:0.4.0:check (default) on project
spark-core_2.10: Failed during scalastyle execution: You have 3 Scalastyle
violation(s). -> [Help 1]



"
Xiangrui Meng <mengxr@gmail.com>,"Thu, 23 Oct 2014 12:26:08 -0700",Re: PR for Hierarchical Clustering Needs Review,RJ Nowling <rnowling@gmail.com>,"Hi RJ,

We are close to the v1.2 feature freeze deadline, so I'm busy with the
pipeline feature and couple bugs. I will ask other developers to help
review the PR. Thanks for working with Yu and helping the code review!

Best,
Xiangrui


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 23 Oct 2014 12:55:41 -0700",Re: scalastyle annoys me a little bit,Koert Kuipers <koert@tresata.com>,"Koert:
If you have time, you can try this diff - with which you would be able to
specify the following on the command line:
-Dscalastyle.failonviolation=false

diff --git a/pom.xml b/pom.xml
index 687cc63..108585e 100644
--- a/pom.xml
+++ b/pom.xml
@@ -123,6 +123,7 @@
     <log4j.version>1.2.17</log4j.version>
     <hadoop.version>1.0.4</hadoop.version>
     <protobuf.version>2.4.1</protobuf.version>
+    <scalastyle.failonviolation>true</scalastyle.failonviolation>
     <yarn.version>${hadoop.version}</yarn.version>
     <hbase.version>0.94.6</hbase.version>
     <flume.version>1.4.0</flume.version>
@@ -1071,7 +1072,7 @@
         <version>0.4.0</version>
         <configuration>
           <verbose>false</verbose>
           <includeTestSourceDirectory>false</includeTestSourceDirectory>
           <sourceDirectory>${basedir}/src/main/scala</sourceDirectory>




"
Koert Kuipers <koert@tresata.com>,"Thu, 23 Oct 2014 16:07:24 -0400",Re: scalastyle annoys me a little bit,Ted Yu <yuzhihong@gmail.com>,"great thanks i will do that


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 23 Oct 2014 13:44:44 -0700",Re: scalastyle annoys me a little bit,Koert Kuipers <koert@tresata.com>,"Created SPARK-4066 and attached patch there.


"
Lochana Menikarachchi <lochanac@gmail.com>,"Fri, 24 Oct 2014 07:57:53 +0530",label points with a given index,dev@spark.apache.org,"
     SparkConf conf = new 
SparkConf().setAppName(""LogisticRegression"").setMaster(""local[4]"");
     JavaSparkContext sc = new JavaSparkContext(conf);
     JavaRDD<String> lines = sc.textFile(""some.csv"");
     JavaRDD<LabeledPoint> lPoints = lines.map(new CSVLineParser());

Is there anyway to parse an index to a function.. for example instead of 
hard coding (parts[0]) below is there a way to parse this



public class CSVLineParser implements Function<String, LabeledPoint> {
     private static final Pattern COMMA = Pattern.compile("","");

     @Override
     public LabeledPoint call(String line) {
         String[] parts = COMMA.split(line);
         double y = Double.parseDouble(parts[0]);
         double[] x = new double[parts.length];
         for (int i = 1; i < parts.length; ++i) {
             x[i] = Double.parseDouble(parts[i]);
         }
         return new LabeledPoint(y, Vectors.dense(x));
     }
}

---------------------------------------------------------------------


"
Lochana Menikarachchi <lochanac@gmail.com>,"Fri, 24 Oct 2014 08:52:53 +0530",Re: label points with a given index,dev@spark.apache.org,"Figured constructor can be used for this purpose..


---------------------------------------------------------------------


"
Evan Chan <velvia.github@gmail.com>,"Fri, 24 Oct 2014 00:35:51 -0400",Re: Multitenancy in Spark - within/across spark context,Marcelo Vanzin <vanzin@cloudera.com>,"Ashwin,

I would say the strategies in general are:

1) Have each user submit separate Spark app (each its own Spark
Context), with its own resource settings, and share data through HDFS
or something like Tachyon for speed.

2) Share a single spark context amongst multiple users, using fair
scheduler.  This is sort of like having a Hadoop resource pool.    It
has some obvious HA/SPOF issues, namely that if the context dies then
every user using it is also dead.   Also, sharing RDDs in cached
memory has the same resiliency problems, namely that if any executor
dies then Spark must recompute / rebuild the RDD (it tries to only
rebuild the missing part, but sometimes it must rebuild everything).

Job server can help with 1 or 2, 2 in particular.  If you have any
questions about job server, feel free to ask at the spark-jobserver
google group.   I am the maintainer.

-Evan



---------------------------------------------------------------------


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Fri, 24 Oct 2014 04:55:48 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi,
We are ready with the initial code. Where can I submit it for review ? I
want to get it reviewed before testing 
it at scale.
Also, I see that most of the algorithms take data as RDD[LabeledPoint] . How
should we take input for this since there are no labels.

Can any body help me out with these issues. Here is the JIRA opened for it.
https://issues.apache.org/jira/browse/SPARK-4038

Regards,
Ashutosh 



--

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Fri, 24 Oct 2014 15:59:45 -0400",Re: scalastyle annoys me a little bit,Ted Yu <yuzhihong@gmail.com>,"thanks ted.

apologies for complaining about maven here again, but this is the first
time i seriously use it for development, and i am completely unfamiliar
with it.

a few more issues:

""mvn clean package -DskipTests"" takes about 30 mins for me. thats painful
since its needed for the tests. does anyone know any tricks to speed it up?
(besides getting a better laptop). does zinc help?

does scalastyle overwrite files? after i do ""mvn package"" emacs is
completely confused and for every file it says ""it has been edited"" and i
need to re-open it. not helpful for a development cycle. i think i am
simply going to edit the pom and remove scalacheck for development.

mvn test runs through the projects until one fails. then it skips the rest!
since its very likely that i get a failure in some subproject, this means
its nearly impossible to do a general test run and get a good sense of the
status of the project. for example:

[INFO]
------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO]
[INFO] Spark Project Parent POM .......................... SUCCESS [2.199s]
[INFO] Spark Project Core ................................ SUCCESS
[39:43.028s]
[INFO] Spark Project Bagel ............................... SUCCESS [42.569s]
[INFO] Spark Project GraphX .............................. SUCCESS
[3:22.104s]
[INFO] Spark Project Streaming ........................... SUCCESS
[7:12.592s]
[INFO] Spark Project ML Library .......................... SUCCESS
[10:32.682s]
[INFO] Spark Project Tools ............................... SUCCESS [17.070s]
[INFO] Spark Project Catalyst ............................ SUCCESS
[3:03.470s]
[INFO] Spark Project SQL ................................. SUCCESS
[5:23.993s]
[INFO] Spark Project Hive ................................ FAILURE
[2:08.387s]
[INFO] Spark Project REPL ................................ SKIPPED
[INFO] Spark Project Assembly ............................ SKIPPED
[INFO] Spark Project External Twitter .................... SKIPPED
[INFO] Spark Project External Kafka ...................... SKIPPED
[INFO] Spark Project External Flume Sink ................. SKIPPED
[INFO] Spark Project External Flume ...................... SKIPPED
[INFO] Spark Project External ZeroMQ ..................... SKIPPED
[INFO] Spark Project External MQTT ....................... SKIPPED
[INFO] Spark Project Examples ............................ SKIPPED

in this case i dont care about Hive, but i would have liked to see REPL
run, and Kafka.





"
Koert Kuipers <koert@tresata.com>,"Fri, 24 Oct 2014 16:05:59 -0400",Re: scalastyle annoys me a little bit,Ted Yu <yuzhihong@gmail.com>,"oh i found some stuff about tests and how to continue them, gonna try that
now (-fae switch). should have googled before asking...


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 24 Oct 2014 13:07:35 -0700",Re: scalastyle annoys me a little bit,Koert Kuipers <koert@tresata.com>,"
I noticed this too, and I also noticed some messages about running out
of space for the code cache on the output. In the pom.xml I added this
to the scala compiler options:

              <jvmArg>-XX:ReservedCodeCacheSize=512m</jvmArg>

Although I haven't actually measured if it improves things yet.
Haven't tried zinc either.


You can try ""mvn -fn"" or ""mvn -fae"" (check ""mvn --help"" for what they mean).

-- 
Marcelo

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 24 Oct 2014 21:08:48 +0100",Re: scalastyle annoys me a little bit,Koert Kuipers <koert@tresata.com>,"
Zinc helps by about 50-100%. Worthwhile for sure. brew install zinc
and zinc -start


You can mvn test -pl [module] to test just one module.
It will also indicate to you that after a failure you can mvn test -rf
:[module] to continue where it left off -- you can use this to resume
at the next module.

Or try ""-Dscalatest.testFailureIgnore=true"" if the mvn flags
themselves don't work, for continuing after a test failure.


---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Fri, 24 Oct 2014 16:20:44 -0400",Re: PR for Hierarchical Clustering Needs Review,Xiangrui Meng <mengxr@gmail.com>,"Thanks, Xiangrui!

Might be worth waiting until after the feature freeze to review since it's
a large patch.





-- 
em rnowling@gmail.com
c 954.496.2314
"
Stephen Boesch <javadba@gmail.com>,"Fri, 24 Oct 2014 13:22:45 -0700",Re: scalastyle annoys me a little bit,Sean Owen <sowen@cloudera.com>,"Sean Owen beat me to (strongly) recommending running zinc server.  Using
the -pl option is great too - but be careful to only use it when your work
is restricted to the modules in the (comma separated) list you provide to
-pl.   Also before using -pl you should do a  mvn compile package install
on all modules.  Use the -pl after those steps are done - and then it is
very effective.

2014-10-24 13:08 GMT-07:00 Sean Owen <sowen@cloudera.com>:

"
shane knapp <sknapp@berkeley.edu>,"Fri, 24 Oct 2014 13:32:42 -0700","your weekly git timeout update! TL;DR: i'm now almost certain we're
 not hitting rate limits.","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","so, things look like they've stabilized significantly over the past 10
days, and without any changes on our end:
<snip>
$ /root/tools/get_timeouts.sh 10
timeouts by date:
2014-10-14 -- 2
2014-10-16 -- 1
2014-10-19 -- 1
2014-10-20 -- 2
2014-10-23 -- 5

timeouts by project:
      5 NewSparkPullRequestBuilder
      5 SparkPullRequestBuilder
      1 Tachyon-Pull-Request-Builder
total builds (excepting aborted by a user):
602

total percentage of builds timing out:
01
</snip>

the NewSparkPullRequestBuilder failures are spread over five different days
(10-14 through 10-20), and the SparkPullRequestBuilder failures all
happened yesterday.  there were a LOT of SparkPullRequestBuilder builds
yesterday (60), and the failures happened during these hours (first number
== number of builds failed, second number == hour of the day):
<snip>
$ cat timeouts-102414-130817 | grep SparkPullRequestBuilder | grep
2014-10-23 | awk '{print$3}' | awk -F"":"" '{print$1'} | sort | uniq -c
      1 03
      2 20
      1 22
      1 23
</snip>

however, the number of total SparkPullRequestBuilder builds during these
times don't seem egregious:
<snip>
      4 03
      9 20
      4 22
      9 23
</snip>

nor does the total for ALL builds at those times:
<snip>
      5 03
      9 20
      7 22
     11 23
</snip>

9 builds was the largest number of SparkPullRequestBuilder builds per hour,
but there were other hours with 5, 6 or 7 builds/hour that didn't have a
timeout issue.

in fact, hour 16 (4pm) had the most builds running total yesterday, which
includes 7 SparkPullRequestBuilder builds, and nothing timed out.

most of the pull request builder hits on github are authenticated w/an
oauth token.  this gives us 5000 hits/hour, and unauthed gives us 60/hour.

in conclusion:  there is no way are we hitting github often enough to be
rate limited.  i think i've finally ruled that out completely.  :)
"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Fri, 24 Oct 2014 13:39:13 -0700 (PDT)",Moving PR Builder to mvn,dev@spark.apache.org,"Over the last few months, it seems like we have selected Maven to be the â€œofficialâ€ build system for Spark.Â 


I realize that removing the sbt build may not be easy, but it might be a good idea to start looking into that. We had issues over the past few days where mvn builds were fine, while sbt was failing to resolve dependencies which were test-jars causing compilation of certain tests to fail.


As a first step, I am wondering if it might be a good idea to change the PR builder to mvn and test PRs consistent with the way we test releases. I am not sure how technically feasible it is, but it would be a start to standardizing on one build system.

Thanks,
Hari"
Koert Kuipers <koert@tresata.com>,"Fri, 24 Oct 2014 16:45:53 -0400",Re: scalastyle annoys me a little bit,"""dev@spark.apache.org"" <dev@spark.apache.org>","thanks everyone, very helpful


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Oct 2014 13:46:25 -0700",Re: Moving PR Builder to mvn,Hari Shreedharan <hshreedharan@cloudera.com>,"Overall I think this would be a good idea. The main blocker is just
that I think the Maven build is much slower right now than the SBT
build. However, if we were able to e.g. parallelize the test build on
Jenkins that might make up for it.

I'd actually like to have a trigger where we could tests pull requests
with either one.

- Patrick

""official"" build system for Spark.
good idea to start looking into that. We had issues over the past few days where mvn builds were fine, while sbt was failing to resolve dependencies which were test-jars causing compilation of certain tests to fail.
PR builder to mvn and test PRs consistent with the way we test releases. I am not sure how technically feasible it is, but it would be a start to standardizing on one build system.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Oct 2014 13:50:05 -0700","Re: your weekly git timeout update! TL;DR: i'm now almost certain
 we're not hitting rate limits.",amp-infra@googlegroups.com,"Thanks for the update Shane.

As a point of process, for things like this where we re debugging
specific issues - can we use JIRA instead of notifying everyone on the
spark-dev list?

I'd prefer if ops/infra announcements on the dev list are restricted
to things that are widely applicable to developers (e.g. planned or
unplanned maintenance on jenkins), since this last has hundreds of
people on it.

- Patrick


---------------------------------------------------------------------


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Fri, 24 Oct 2014 13:55:10 -0700 (PDT)",Re: Moving PR Builder to mvn,"""Patrick Wendell"" <pwendell@gmail.com>","I have zinc server running on my mac, and I see maven compilation to be much better than before I had it running. Is the sbt build still faster (sorry, long time since I did a build with sbt).


Thanks,
Hari


 ""official"" build system for Spark.
 good idea to start looking into that. We had issues over the past few days where mvn builds were fine, while sbt was failing to resolve dependencies which were test-jars causing compilation of certain tests to fail.
 PR builder to mvn and test PRs consistent with the way we test releases. I am not sure how technically feasible it is, but it would be a start to standardizing on one build system."
Patrick Wendell <pwendell@gmail.com>,"Fri, 24 Oct 2014 14:06:40 -0700",Re: Moving PR Builder to mvn,Hari Shreedharan <hshreedharan@cloudera.com>,"Does Zinc still help if you are just running a single totally fresh
build? For the pull request builder we purge all state from previous
builds.

- Patrick


---------------------------------------------------------------------


"
"""Hari Shreedharan"" <hshreedharan@cloudera.com>","Fri, 24 Oct 2014 14:35:37 -0700 (PDT)",Re: Moving PR Builder to mvn,"""Stephen Boesch"" <javadba@gmail.com>","+1. From what I can see, it definitely does - though I must say I rarely do full end to end builds though. Maybe worth running as an experiment?


Thanks,
Hari


-
be
on
requests
be
might
few
change
 to"
Stephen Boesch <javadba@gmail.com>,"Fri, 24 Oct 2014 14:34:25 -0700",Re: Moving PR Builder to mvn,Patrick Wendell <pwendell@gmail.com>,"Zinc absolutely helps - feels like makes builds  more than twice as fast -
both on Mac and Linux.   It helps both on fresh and existing builds.

2014-10-24 14:06 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

"
Gary Malouf <malouf.gary@gmail.com>,"Fri, 24 Oct 2014 17:40:12 -0400",Re: Parquet schema migrations,Cody Koeninger <cody@koeninger.org>,"Hi Michael,

Does this affect people who use Hive for their metadata store as well?  I'm
wondering if the issue is as bad as I think it is - namely that if you
build up a year's worth of data, adding a field forces you to have to
migrate that entire year's data.

Gary


"
Sean Owen <sowen@cloudera.com>,"Sat, 25 Oct 2014 00:24:49 +0100",Re: Moving PR Builder to mvn,Patrick Wendell <pwendell@gmail.com>,"Here's a crude benchmark on a Linux box (GCE n1-standard-4). zinc gets
the assembly build in range of SBT's time.

mvn -DskipTests clean package
15:27
(start zinc)
8:18
(rebuild)
7:08

./sbt/sbt -DskipTests clean assembly
5:10
(start zinc)
5:11
(rebuild)
5:06

The dependencies were already downloaded, and the whole build was
cleaned in between.

These are smallish in comparison with time to run tests. I admit I
didn't run them here in the interest of time and because I assumed
zinc doesn't help that.



---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 24 Oct 2014 17:30:35 -0700",Re: Moving PR Builder to mvn,Sean Owen <sowen@cloudera.com>,"Your's are in the same ballpark with mine, where maven builds with zinc
take about 1.4x the time to build with SBT.


"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Sat, 25 Oct 2014 09:16:59 +0800",serialVersionUID incompatible error in class BlockManagerId,dev@spark.apache.org,"Hi,

I update git today and when connecting to spark cluster, I got
the serialVersionUID incompatible error in class BlockManagerId.

Here is  the log,

Shouldn't we better give BlockManagerId a constant serialVersionUID  avoid
this?

Thanks,
Qiuzhuang

scala> val rdd = sc.parparallelize(1 to 100014/10/25 09:10:48 ERROR
Remoting: org.apache.spark.storage.BlockManagerId; local class
incompatible: stream classdesc serialVersionUID = 2439208141545036836,
local class serialVersionUID = 4657685702603429489
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;
local class incompatible: stream classdesc serialVersionUID =
2439208141545036836, local class serialVersionUID = 4657685702603429489
        at
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        at
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)
        at
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)
        at scala.util.Try$.apply(Try.scala:161)
        at
akka.serialization.Serialization.deserialize(Serialization.scala:98)
        at
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)
        at
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)
        at
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/10/25 09:10:48 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 1
0014/10/25 09:11:21 ERROR Remoting:
org.apache.spark.storage.BlockManagerId; local class incompatible: stream
classdesc serialVersionUID = 2439208141545036836, local class
serialVersionUID = 4657685702603429489
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;
local class incompatible: stream classdesc serialVersionUID =
2439208141545036836, local class serialVersionUID = 4657685702603429489
        at
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        at
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)
        at
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)
        at scala.util.Try$.apply(Try.scala:161)
        at
akka.serialization.Serialization.deserialize(Serialization.scala:98)
        at
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)
        at
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)
        at
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/10/25 09:11:21 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 1
14/10/25 09:11:54 INFO SparkDeploySchedulerBackend: Registered executor:
Actor[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50006/user/Executor#-1410691203]
with ID 1
14/10/25 09:11:54 INFO DAGScheduler: Host added was in lost list earlier:
DEV-02.SpringB.GZ
14/10/25 09:11:55 ERROR TaskSchedulerImpl: Lost executor 1 on
DEV-02.SpringB.GZ: remote Akka client disassociated
14/10/25 09:11:55 WARN ReliableDeliverySupervisor: Association with remote
system [akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50006] has failed,
address is now gated for [5000] ms. Reason is: [Association failed with
[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50006]].
14/10/25 09:11:55 INFO DAGScheduler: Executor lost: 1 (epoch 1)
14/10/25 09:11:55 INFO BlockManagerMasterActor: Trying to remove executor 1
from BlockManagerMaster.
14/10/25 09:11:55 INFO BlockManagerMaster: Removed 1 successfully in
removeExecutor
14/10/25 09:11:55 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 1
14/10/25 09:11:55 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 1
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor updated:
app-20141025091012-0002/1 is now EXITED (Command exited with code 1)
14/10/25 09:11:55 INFO SparkDeploySchedulerBackend: Executor
app-20141025091012-0002/1 removed: Command exited with code 1
14/10/25 09:11:55 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 1
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor added:
app-20141025091012-0002/3 on worker-20141025170311-DEV-02.SpringB.GZ-35162
(DEV-02.SpringB.GZ:35162) with 2 cores
14/10/25 09:11:55 INFO SparkDeploySchedulerBackend: Granted executor ID
app-20141025091012-0002/3 on hostPort DEV-02.SpringB.GZ:35162 with 2 cores,
512.0 MB RAM
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor updated:
app-20141025091012-0002/3 is now LOADING
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor updated:
app-20141025091012-0002/3 is now RUNNING
14/10/25 09:11:58 INFO SparkDeploySchedulerBackend: Registered executor:
Actor[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50740/user/Executor#1229699385]
with ID 3
14/10/25 09:11:58 WARN ReliableDeliverySupervisor: Association with remote
system [akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50740] has failed,
address is now gated for [5000] ms. Reason is:
[org.apache.spark.storage.BlockManagerId; local class incompatible: stream
classdesc serialVersionUID = 2439208141545036836, local class
serialVersionUID = 4657685702603429489].
14/10/25 09:11:58 ERROR TaskSchedulerImpl: Lost executor 3 on
DEV-02.SpringB.GZ: remote Akka client disassociated
14/10/25 09:11:58 INFO DAGScheduler: Executor lost: 3 (epoch 2)
14/10/25 09:11:58 INFO BlockManagerMasterActor: Trying to remove executor 3
from BlockManagerMaster.
14/10/25 09:11:58 INFO BlockManagerMaster: Removed 3 successfully in
removeExecutor
14/10/25 09:12:31 ERROR Remoting: org.apache.spark.storage.BlockManagerId;
local class incompatible: stream classdesc serialVersionUID =
2439208141545036836, local class serialVersionUID = 4657685702603429489
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;
local class incompatible: stream classdesc serialVersionUID =
2439208141545036836, local class serialVersionUID = 4657685702603429489
        at
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        at
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)
        at
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)
        at scala.util.Try$.apply(Try.scala:161)
        at
akka.serialization.Serialization.deserialize(Serialization.scala:98)
        at
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)
        at
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)
        at
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/10/25 09:12:31 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 3
14/10/25 09:13:04 ERROR Remoting: org.apache.spark.storage.BlockManagerId;
local class incompatible: stream classdesc serialVersionUID =
2439208141545036836, local class serialVersionUID = 4657685702603429489
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;
local class incompatible: stream classdesc serialVersionUID =
2439208141545036836, local class serialVersionUID = 4657685702603429489
        at
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)
        at
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)
        at
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)
        at
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)
        at
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
        at
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
        at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
        at
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)
        at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)
        at
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)
        at
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)
        at scala.util.Try$.apply(Try.scala:161)
        at
akka.serialization.Serialization.deserialize(Serialization.scala:98)
        at
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)
        at
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)
        at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)
        at
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)
        at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
        at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
        at akka.actor.ActorCell.invoke(ActorCell.scala:487)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
        at akka.dispatch.Mailbox.run(Mailbox.scala:220)
        at
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
        at
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
14/10/25 09:13:04 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 3
14/10/25 09:13:37 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 3
14/10/25 09:13:37 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 3
14/10/25 09:13:37 INFO AppClient$ClientActor: Executor updated:
app-20141025091012-0002/3 is now EXITED (Command exited with code 1)
14/10/25 09:13:37 INFO SparkDeploySchedulerBackend: Executor
app-20141025091012-0002/3 removed: Command exited with code 1
14/10/25 09:13:37 ERROR SparkDeploySchedulerBackend: Asked to remove non
existant executor 3
14/10/25 09:13:37 INFO AppClient$ClientActor: Executor added:
app-20141025091012-0002/4 on worker-20141025170311-DEV-02.SpringB.GZ-35162
(DEV-02.SpringB.GZ:35162) with 2 cores
14/10/25 09:13:37 INFO SparkDeploySchedulerBackend: Granted executor ID
app-20141025091012-0002/4 on hostPort DEV-02.SpringB.GZ:35162 with 2 cores,
512.0 MB RAM
14/10/25 09:13:37 INFO AppClient$ClientActor: Executor updated:
app-20141025091012-0002/4 is now LOADING
14/10/25 09:13:38 INFO AppClient$ClientActor: Executor updated:
app-20141025091012-0002/4 is now RUNNING
14/10/25 09:13:40 INFO SparkDeploySchedulerBackend: Registered executor:
Actor[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:56019/user/Executor#1354626597]
with ID 4
14/10/25 09:13:40 WARN ReliableDeliverySupervisor: Association with remote
system [akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:56019] has failed,
address is now gated for [5000] ms. Reason is:
[org.apache.spark.storage.BlockManagerId; local class incompatible: stream
classdesc serialVersionUID = 2439208141545036836, local class
serialVersionUID = 4657685702603429489].
14/10/25 09:13:40 ERROR TaskSchedulerImpl: Lost executor 4 on
DEV-02.SpringB.GZ: remote Akka client disassociated
14/10/25 09:13:40 INFO DAGScheduler: Executor lost: 4 (epoch 3)
14/10/25 09:13:40 INFO BlockManagerMasterActor: Trying to remove executor 4
from BlockManagerMaster.
14/10/25 09:13:40 INFO BlockManagerMaster: Removed 4 successfully in
removeExecutor
"
Josh Rosen <rosenville@gmail.com>,"Fri, 24 Oct 2014 18:23:06 -0700",Re: serialVersionUID incompatible error in class BlockManagerId,"Qiuzhuang Lian <qiuzhuang.lian@gmail.com>, dev@spark.apache.org","Are all processes (Master, Worker, Executors, Driver) running the same Spark build? Â This error implies that youâ€™re seeing protocol / binary incompatibilities between your Spark driver and cluster.

Spark is API-compatibile across the 1.x series, but we donâ€™t make binary link-level compatibility guarantees:Â https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy. Â This means that your Spark driverâ€™s runtime classpath should use the same version of Spark thatâ€™s installed on your cluster. Â You can compileÂ against a different API-compatible version of Spark, but the runtime versions must match across all components.

To fix this issue, Iâ€™d check that youâ€™ve run the â€œpackageâ€ and â€œassemblyâ€ phases and that your Spark cluster is using this updated version.

- Josh


Hi,  

I update git today and when connecting to spark cluster, I got  
the serialVersionUID incompatible error in class BlockManagerId.  

Here is the log,  

Shouldn't we better give BlockManagerId a constant serialVersionUID avoid  
this?  

Thanks,  
Qiuzhuang  

scala> val rdd = sc.parparallelize(1 to 100014/10/25 09:10:48 ERROR  
Remoting: org.apache.spark.storage.BlockManagerId; local class  
incompatible: stream classdesc serialVersionUID = 2439208141545036836,  
local class serialVersionUID = 4657685702603429489  
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;  
local class incompatible: stream classdesc serialVersionUID =  
2439208141545036836, local class serialVersionUID = 4657685702603429489  
at  
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)  
at  
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)  
at  
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at  
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)  
at  
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)  
at  
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)  
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)  
at  
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)  
at  
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)  
at scala.util.Try$.apply(Try.scala:161)  
at  
akka.serialization.Serialization.deserialize(Serialization.scala:98)  
at  
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)  
at  
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)  
at  
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)  
at akka.actor.Actor$class.aroundReceive(Actor.scala:465)  
at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)  
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)  
at akka.actor.ActorCell.invoke(ActorCell.scala:487)  
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)  
at akka.dispatch.Mailbox.run(Mailbox.scala:220)  
at  
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)  
at  
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)  
at  
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)  
at  
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)  
at  
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  
14/10/25 09:10:48 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 1  
0014/10/25 09:11:21 ERROR Remoting:  
org.apache.spark.storage.BlockManagerId; local class incompatible: stream  
classdesc serialVersionUID = 2439208141545036836, local class  
serialVersionUID = 4657685702603429489  
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;  
local class incompatible: stream classdesc serialVersionUID =  
2439208141545036836, local class serialVersionUID = 4657685702603429489  
at  
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)  
at  
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)  
at  
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at  
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)  
at  
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)  
at  
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)  
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)  
at  
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)  
at  
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)  
at scala.util.Try$.apply(Try.scala:161)  
at  
akka.serialization.Serialization.deserialize(Serialization.scala:98)  
at  
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)  
at  
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)  
at  
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)  
at akka.actor.Actor$class.aroundReceive(Actor.scala:465)  
at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)  
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)  
at akka.actor.ActorCell.invoke(ActorCell.scala:487)  
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)  
at akka.dispatch.Mailbox.run(Mailbox.scala:220)  
at  
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)  
at  
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)  
at  
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)  
at  
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)  
at  
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  
14/10/25 09:11:21 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 1  
14/10/25 09:11:54 INFO SparkDeploySchedulerBackend: Registered executor:  
Actor[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50006/user/Executor#-1410691203]  
with ID 1  
14/10/25 09:11:54 INFO DAGScheduler: Host added was in lost list earlier:  
DEV-02.SpringB.GZ  
14/10/25 09:11:55 ERROR TaskSchedulerImpl: Lost executor 1 on  
DEV-02.SpringB.GZ: remote Akka client disassociated  
14/10/25 09:11:55 WARN ReliableDeliverySupervisor: Association with remote  
system [akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50006] has failed,  
address is now gated for [5000] ms. Reason is: [Association failed with  
[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50006]].  
14/10/25 09:11:55 INFO DAGScheduler: Executor lost: 1 (epoch 1)  
14/10/25 09:11:55 INFO BlockManagerMasterActor: Trying to remove executor 1  
from BlockManagerMaster.  
14/10/25 09:11:55 INFO BlockManagerMaster: Removed 1 successfully in  
removeExecutor  
14/10/25 09:11:55 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 1  
14/10/25 09:11:55 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 1  
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor updated:  
app-20141025091012-0002/1 is now EXITED (Command exited with code 1)  
14/10/25 09:11:55 INFO SparkDeploySchedulerBackend: Executor  
app-20141025091012-0002/1 removed: Command exited with code 1  
14/10/25 09:11:55 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 1  
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor added:  
app-20141025091012-0002/3 on worker-20141025170311-DEV-02.SpringB.GZ-35162  
(DEV-02.SpringB.GZ:35162) with 2 cores  
14/10/25 09:11:55 INFO SparkDeploySchedulerBackend: Granted executor ID  
app-20141025091012-0002/3 on hostPort DEV-02.SpringB.GZ:35162 with 2 cores,  
512.0 MB RAM  
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor updated:  
app-20141025091012-0002/3 is now LOADING  
14/10/25 09:11:55 INFO AppClient$ClientActor: Executor updated:  
app-20141025091012-0002/3 is now RUNNING  
14/10/25 09:11:58 INFO SparkDeploySchedulerBackend: Registered executor:  
Actor[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50740/user/Executor#1229699385]  
with ID 3  
14/10/25 09:11:58 WARN ReliableDeliverySupervisor: Association with remote  
system [akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:50740] has failed,  
address is now gated for [5000] ms. Reason is:  
[org.apache.spark.storage.BlockManagerId; local class incompatible: stream  
classdesc serialVersionUID = 2439208141545036836, local class  
serialVersionUID = 4657685702603429489].  
14/10/25 09:11:58 ERROR TaskSchedulerImpl: Lost executor 3 on  
DEV-02.SpringB.GZ: remote Akka client disassociated  
14/10/25 09:11:58 INFO DAGScheduler: Executor lost: 3 (epoch 2)  
14/10/25 09:11:58 INFO BlockManagerMasterActor: Trying to remove executor 3  
from BlockManagerMaster.  
14/10/25 09:11:58 INFO BlockManagerMaster: Removed 3 successfully in  
removeExecutor  
14/10/25 09:12:31 ERROR Remoting: org.apache.spark.storage.BlockManagerId;  
local class incompatible: stream classdesc serialVersionUID =  
2439208141545036836, local class serialVersionUID = 4657685702603429489  
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;  
local class incompatible: stream classdesc serialVersionUID =  
2439208141545036836, local class serialVersionUID = 4657685702603429489  
at  
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)  
at  
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)  
at  
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at  
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)  
at  
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)  
at  
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)  
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)  
at  
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)  
at  
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)  
at scala.util.Try$.apply(Try.scala:161)  
at  
akka.serialization.Serialization.deserialize(Serialization.scala:98)  
at  
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)  
at  
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)  
at  
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)  
at akka.actor.Actor$class.aroundReceive(Actor.scala:465)  
at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)  
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)  
at akka.actor.ActorCell.invoke(ActorCell.scala:487)  
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)  
at akka.dispatch.Mailbox.run(Mailbox.scala:220)  
at  
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)  
at  
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)  
at  
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)  
at  
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)  
at  
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  
14/10/25 09:12:31 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 3  
14/10/25 09:13:04 ERROR Remoting: org.apache.spark.storage.BlockManagerId;  
local class incompatible: stream classdesc serialVersionUID =  
2439208141545036836, local class serialVersionUID = 4657685702603429489  
java.io.InvalidClassException: org.apache.spark.storage.BlockManagerId;  
local class incompatible: stream classdesc serialVersionUID =  
2439208141545036836, local class serialVersionUID = 4657685702603429489  
at  
java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:617)  
at  
java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1622)  
at  
java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at  
java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1990)  
at  
java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1915)  
at  
java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)  
at  
java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)  
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)  
at  
akka.serialization.JavaSerializer$$anonfun$1.apply(Serializer.scala:136)  
at scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)  
at  
akka.serialization.JavaSerializer.fromBinary(Serializer.scala:136)  
at  
akka.serialization.Serialization$$anonfun$deserialize$1.apply(Serialization.scala:104)  
at scala.util.Try$.apply(Try.scala:161)  
at  
akka.serialization.Serialization.deserialize(Serialization.scala:98)  
at  
akka.remote.MessageSerializer$.deserialize(MessageSerializer.scala:23)  
at  
akka.remote.DefaultMessageDispatcher.payload$lzycompute$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.payload$1(Endpoint.scala:58)  
at akka.remote.DefaultMessageDispatcher.dispatch(Endpoint.scala:76)  
at  
akka.remote.EndpointReader$$anonfun$receive$2.applyOrElse(Endpoint.scala:937)  
at akka.actor.Actor$class.aroundReceive(Actor.scala:465)  
at akka.remote.EndpointActor.aroundReceive(Endpoint.scala:415)  
at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)  
at akka.actor.ActorCell.invoke(ActorCell.scala:487)  
at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)  
at akka.dispatch.Mailbox.run(Mailbox.scala:220)  
at  
akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)  
at  
scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)  
at  
scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)  
at  
scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)  
at  
scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  
14/10/25 09:13:04 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 3  
14/10/25 09:13:37 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 3  
14/10/25 09:13:37 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 3  
14/10/25 09:13:37 INFO AppClient$ClientActor: Executor updated:  
app-20141025091012-0002/3 is now EXITED (Command exited with code 1)  
14/10/25 09:13:37 INFO SparkDeploySchedulerBackend: Executor  
app-20141025091012-0002/3 removed: Command exited with code 1  
14/10/25 09:13:37 ERROR SparkDeploySchedulerBackend: Asked to remove non  
existant executor 3  
14/10/25 09:13:37 INFO AppClient$ClientActor: Executor added:  
app-20141025091012-0002/4 on worker-20141025170311-DEV-02.SpringB.GZ-35162  
(DEV-02.SpringB.GZ:35162) with 2 cores  
14/10/25 09:13:37 INFO SparkDeploySchedulerBackend: Granted executor ID  
app-20141025091012-0002/4 on hostPort DEV-02.SpringB.GZ:35162 with 2 cores,  
512.0 MB RAM  
14/10/25 09:13:37 INFO AppClient$ClientActor: Executor updated:  
app-20141025091012-0002/4 is now LOADING  
14/10/25 09:13:38 INFO AppClient$ClientActor: Executor updated:  
app-20141025091012-0002/4 is now RUNNING  
14/10/25 09:13:40 INFO SparkDeploySchedulerBackend: Registered executor:  
Actor[akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:56019/user/Executor#1354626597]  
with ID 4  
14/10/25 09:13:40 WARN ReliableDeliverySupervisor: Association with remote  
system [akka.tcp://sparkExecutor@DEV-02.SpringB.GZ:56019] has failed,  
address is now gated for [5000] ms. Reason is:  
[org.apache.spark.storage.BlockManagerId; local class incompatible: stream  
classdesc serialVersionUID = 2439208141545036836, local class  
serialVersionUID = 4657685702603429489].  
14/10/25 09:13:40 ERROR TaskSchedulerImpl: Lost executor 4 on  
DEV-02.SpringB.GZ: remote Akka client disassociated  
14/10/25 09:13:40 INFO DAGScheduler: Executor lost: 4 (epoch 3)  
14/10/25 09:13:40 INFO BlockManagerMasterActor: Trying to remove executor 4  
from BlockManagerMaster.  
14/10/25 09:13:40 INFO BlockManagerMaster: Removed 4 successfully in  
removeExecutor  
"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Sat, 25 Oct 2014 09:25:48 +0800",Re: serialVersionUID incompatible error in class BlockManagerId,Josh Rosen <rosenville@gmail.com>,"I update git trunk and build in the two linux machines. I think they should
have the same version. I am going to do a force clean build and then retry.

Thanks.



nary
 binary
.
the same
ile* against
t
œpackageâ€ and â€œassemblyâ€
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
0691203]
e
2
699385]
e
m
;
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
;
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
2
626597]
e
m
"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 24 Oct 2014 21:42:01 -0400",Re: serialVersionUID incompatible error in class BlockManagerId,Josh Rosen <rosenville@gmail.com>,"According to my experience, there are more issues rather than BlockManager when you try to run spark application whose build version is different with your clusterâ€¦.  

I once tried to make jdbc server build with branch-jdbc-1.0 run with a branch-1.0 clusterâ€¦no workaround exitsâ€¦just had to replace cluster jar with branch-jdbc-1.0 jar fileâ€¦..

Best,  

--  
Nan Zhu



Spark build?  This error implies that youâ€™re seeing protocol / binary incompatibilities between your Spark driver and cluster.
ke binary link-level compatibility guarantees: https://cwiki.apache.org/confluence/display/SPARK/Spark+Versioning+Policy.  This means that your Spark driverâ€™s runtime classpath should use the same version of Spark thatâ€™s installed on your cluster.  You can compile against a different API-compatible version of Spark, but the runtime versions must match across all components.
œpackageâ€ and â€œassemblyâ€ phases and that your Spark cluster is using this updated version.
id  
,  
  
89  
  
1)  
90)  
8)  
la:136)  
erialization.scala:104)  
)  
scala:58)  
 
int.scala:937)  
AbstractDispatcher.scala:393)  
)  
Pool.java:1339)  
1979)  
read.java:107)  
n  
am  
  
89  
  
1)  
90)  
8)  
la:136)  
erialization.scala:104)  
)  
scala:58)  
 
int.scala:937)  
AbstractDispatcher.scala:393)  
)  
Pool.java:1339)  
1979)  
read.java:107)  
n  
or:  
tor@DEV-02.SpringB.GZ):50006/user/Executor#-1410691203]  
ier:  
ote  
cutor@DEV-02.SpringB.GZ):50006] has failed,  
d with  
DEV-02.SpringB.GZ):50006]].  
utor 1  
 
n  
n  
n  
162  
ID  
res,  
or:  
tor@DEV-02.SpringB.GZ):50740/user/Executor#1229699385]  
ote  
cutor@DEV-02.SpringB.GZ):50740] has failed,  
tream  
utor 3  
 
Id;  
89  
  
89  
  
1)  
90)  
8)  
la:136)  
erialization.scala:104)  
)  
scala:58)  
 
int.scala:937)  
AbstractDispatcher.scala:393)  
)  
Pool.java:1339)  
1979)  
read.java:107)  
n  
Id;  
89  
  
89  
  
1)  
90)  
8)  
la:136)  
erialization.scala:104)  
)  
scala:58)  
 
int.scala:937)  
AbstractDispatcher.scala:393)  
)  
Pool.java:1339)  
1979)  
read.java:107)  
n  
n  
n  
n  
162  
ID  
res,  
or:  
tor@DEV-02.SpringB.GZ):56019/user/Executor#1354626597]  
ote  
cutor@DEV-02.SpringB.GZ):56019] has failed,  
tream  
utor 4  
 


"
Qiuzhuang Lian <qiuzhuang.lian@gmail.com>,"Sat, 25 Oct 2014 09:31:24 +0800",Re: serialVersionUID incompatible error in class BlockManagerId,Nan Zhu <zhunanmcgill@gmail.com>,"After I do a clean rebuild. It works now.

Thanks,
Qiuzhuang


e cluster jar with
nary
 binary
.
the same
le against
t
œpackageâ€ and â€œassemblyâ€
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
0691203]
e
2
699385]
e
m
;
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
;
on.scala:104)
58)
937)
Dispatcher.scala:393)
a:1339)
ava:107)
2
626597]
e
m
"
salexln <salexln@gmail.com>,"Sat, 25 Oct 2014 06:03:23 -0700 (PDT)",Matix operations in Scala \ Spark,dev@spark.incubator.apache.org,"Hi guys,

I'm working on the implementation of the FuzzyCMeans algorithm (Jira
https://issues.apache.org/jira/browse/SPARK-2344)
and I need to use some operations on Matrices (norm & subtraction)

I could not find any Scala\ Spark Matrix class that will support these
actions.

Should I implement the Matrix as a two dimensional array and make my own
code for the norm & subtraction ?







--

---------------------------------------------------------------------


"
Xuefeng Wu <benewu@gmail.com>,"Sat, 25 Oct 2014 22:11:29 +0800",Re: Matix operations in Scala \ Spark,salexln <salexln@gmail.com>,"how about non/spire or twitter/scalding


Yours, Xuefeng Wu ÎâÑ©·å ¾´ÉÏ

n3.nabble.com/Matix-operations-in-Scala-Spark-tp8959.html
com.

---------------------------------------------------------------------


"
Zongheng Yang <zongheng.y@gmail.com>,"Sat, 25 Oct 2014 16:58:25 +0000",Re: Matix operations in Scala \ Spark,"Xuefeng Wu <benewu@gmail.com>, salexln <salexln@gmail.com>","We recently released a research prototype of a lightweight matrix library
for Spark here: https://github.com/amplab/ml-matrix which does support norm
and subtraction. Feel free to base your implementation on top of it.

Zongheng

n
"
RJ Nowling <rnowling@gmail.com>,"Sat, 25 Oct 2014 22:49:02 -0400",Re: Multitenancy in Spark - within/across spark context,Evan Chan <velvia.github@gmail.com>,"Ashwin,

What is your motivation for needing to share RDDs between jobs? Optimizing
for reusing data across jobs?

If so, you may want to look into Tachyon. My understanding is that Tachyon
acts like a caching layer and you can designate when data will be reused in
multiple jobs so it know to keep that in memory or local disk for faster
access. But my knowledge of tachyon is second hand so forgive me if I have
it wrong :)

RJ



-- 
em rnowling@gmail.com
c 954.496.2314
"
Vibhanshu Prasad <vibhanshugsoc2@gmail.com>,"Sun, 26 Oct 2014 15:13:31 +0530",Potential areas for working,dev@spark.apache.org,"Hello everyone,
I am new to the spark developer community,
I want to know what are the areas where currently programming is in
progress. What areas I can work on as a started.

Vibhanshu
"
Varadharajan Mukundan <srinathsmn@gmail.com>,"Sun, 26 Oct 2014 16:58:39 +0530",Re: Potential areas for working,Vibhanshu Prasad <vibhanshugsoc2@gmail.com>,"+1, I'm interested in contributing to Spark SQL and i'm looking for
starter tasks.




-- 
Thanks,
M. Varadharajan

------------------------------------------------

""Experience is what you get when you didn't get what you wanted""
               -By Prof."
ll <duy.huynh.uiv@gmail.com>,"Sun, 26 Oct 2014 08:07:20 -0700 (PDT)",best IDE for scala + spark development?,dev@spark.incubator.apache.org,"i'm new to both scala and spark.  what IDE / dev environment do you find most
productive for writing code in scala with spark?  is it just vim + sbt?  or
does a full IDE like intellij works out better?  thanks!



--

---------------------------------------------------------------------


"
Subbu M <msubramm@gmail.com>,"Sun, 26 Oct 2014 08:23:10 -0700",Re: best IDE for scala + spark development?,ll <duy.huynh.uiv@gmail.com>,"Eclipse would be a ideal choice for both.
Download and deploy plugins for required things scala+SBT+Spark


"
Stephen Boesch <javadba@gmail.com>,"Sun, 26 Oct 2014 09:27:57 -0700",Re: best IDE for scala + spark development?,ll <duy.huynh.uiv@gmail.com>,"Many of the spark developers use Intellij.   You will in any case probably
want a full IDE (either IJ or eclipse)

2014-10-26 8:07 GMT-07:00 ll <duy.huynh.uiv@gmail.com>:

"
Jay Vyas <jayunit100.apache@gmail.com>,"Sun, 26 Oct 2014 12:32:12 -0400",Re: best IDE for scala + spark development?,ll <duy.huynh.uiv@gmail.com>,"I tried the scala eclipse ide but in scala 2.10 I ran into some weird issues http://stackoverflow.com/questions/24253084/scalaide-and-cryptic-classnotfound-errors ... So I switched to IntelliJ and was much more satisfied...

I've written a post on how I use fedora,sbt, and intellij for spark apps.
http://jayunit100.blogspot.com/2014/07/set-up-spark-application-devleopment.html?m=1

The IntelliJ sbt plugin is imo less buggy then the eclipse scalaIDE stuff.  For example, I found I had to set some special preferences

Finally... given sbts automated recompile option, if you just use tmux, and vim nerdtree, with sbt , you could come pretty close to something like an IDE without all the drama ..

ost
r
n3.nabble.com/best-IDE-for-scala-spark-development-tp8965.html
com.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 26 Oct 2014 14:11:39 -0400",Re: Potential areas for working,Varadharajan Mukundan <srinathsmn@gmail.com>,"Have yâ€™all taken a look at these links?

   -


   https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-StarterTasks
    -


   https://issues.apache.org/jira/browse/SPARK-3740?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)
   <https://issues.apache.org/jira/browse/SPARK-3740?jql=project%20%3D%20SPARK%20AND%20labels%20%3D%20Starter%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened>
   )

Nick
â€‹

m

"
Duy Huynh <duy.huynh.uiv@gmail.com>,"Sun, 26 Oct 2014 18:06:20 -0400",Re: best IDE for scala + spark development?,Jay Vyas <jayunit100.apache@gmail.com>,"i like intellij and eclipse too, but some that they are too heavy.  i would
love to use vim.  are there are good scala plugins for vim?  (i.e code
completion, scala doc, etc)


"
Jianshi Huang <jianshi.huang@gmail.com>,"Mon, 27 Oct 2014 16:47:40 +0800",Build with Hive 0.13.1 doesn't have datanucleus and parquet dependencies.,"""dev@spark.apache.org"" <dev@spark.apache.org>","There's a change in build process lately for Hive 0.13 support and we
should make it obvious. Based on the new pom.xml I tried to enable Hive
0.13.1 support by using option

  -Phive-0.13.1

However, it seems datanucleus and parquet dependencies are not available in
the final build.

Am I missing anything?

Jianshi

-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Mon, 27 Oct 2014 08:57:32 +0000","RE: Build with Hive 0.13.1 doesn't have datanucleus and parquet
 dependencies.","Jianshi Huang <jianshi.huang@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Hive-thriftserver module is not included while specifying the profile hive-0.13.1. 

-----Original Message-----
From: Jianshi Huang [mailto:jianshi.huang@gmail.com] 
Sent: Monday, October 27, 2014 4:48 PM
To: dev@spark.apache.org
Subject: Build with Hive 0.13.1 doesn't have datanucleus and parquet dependencies.

There's a change in build process lately for Hive 0.13 support and we should make it obvious. Based on the new pom.xml I tried to enable Hive
0.13.1 support by using option

  -Phive-0.13.1

However, it seems datanucleus and parquet dependencies are not available in the final build.

Am I missing anything?

Jianshi

--
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Jianshi Huang <jianshi.huang@gmail.com>,"Mon, 27 Oct 2014 17:15:38 +0800",Re: Build with Hive 0.13.1 doesn't have datanucleus and parquet dependencies.,"""Cheng, Hao"" <hao.cheng@intel.com>","Ah I see. Thanks Hao! I'll wait for the fix.

Jianshi





-- 
Jianshi Huang

LinkedIn: jianshi
Twitter: @jshuang
Github & Blog: http://huangjs.github.com/
"
Dean Wampler <deanwampler@gmail.com>,"Mon, 27 Oct 2014 08:15:20 -0500",Re: best IDE for scala + spark development?,Duy Huynh <duy.huynh.uiv@gmail.com>,"For what it's worth, I use Sublime Text + the SBT console for everything. I
can live without the extra IDE features.

However, if you like an IDE, the Eclipse ""Scala IDE"" 4.0 RC1 is a big
improvement over previous releases. For one thing, it can now supports
projects using different versions of Scala, which is convenient for Spark's
current 2.10.4 support and emerging 2.11 support.

http://scala-ide.org/download/milestone.html

Dean


Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
andy petrella <andy.petrella@gmail.com>,"Mon, 27 Oct 2014 14:17:56 +0100",Re: best IDE for scala + spark development?,Dean Wampler <deanwampler@gmail.com>,"I second the S[B]T combo!

I tried ATOM â†’ lack of features and stability (atm)

aâ„•dy â„™etrella
about.me/noootsab
[image: aâ„•dy â„™etrella on about.me]

<http://about.me/noootsab>

:

 I
's
found-errors
nt.html?m=1
x,
ala-spark-development-tp8965.html
--
"
Koert Kuipers <koert@tresata.com>,"Mon, 27 Oct 2014 11:49:32 -0400",Re: best IDE for scala + spark development?,"""dev@spark.incubator.apache.org"" <dev@spark.incubator.apache.org>","editor of your choice + sbt console works + grep great.

if only folks stopped using wildcard imports (it has little benefits in
terms of coding yet requires an IDE with 1G+ of ram to track em down).


e
rd
found-errors
nt.html?m=1
ng
ou
+
ala-spark-development-tp8965.html
t
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 27 Oct 2014 08:54:52 -0700",Re: best IDE for scala + spark development?,Koert Kuipers <koert@tresata.com>,"Also ctags works fine with vim for browsing scala classes

Shivaram

i
de
ird
tfound-errors
k
ent.html?m=1
E
ing
:
you
 +
cala-spark-development-tp8965.html
at

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Oct 2014 10:46:32 -0700",jenkins downtime tomorrow morning ~6am-8am PDT,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","i'll be bringing jenkins down tomorrow morning for some system maintenance
and to get our backups kicked off.

i do expect to have the system back up and running before 8am.

please let me know ASAP if i need to reschedule this.

thanks,

shane
"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Oct 2014 13:24:25 -0700","jenkins emergency restart now, was Re: jenkins downtime tomorrow
 morning ~6am-8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","so, i'm having a race condition between a plugin i installed putting
jenkins in to quiet mode and it failing to perform a backup from this past
weekend.  i'll need to restart the process and get it out of the
constantly-in-to-quiet-mode cycle it's in now.

this will be quick, and i'll restart the jobs i've killed.

this DOES NOT effect the restart/maintenance tomorrow morning.

sorry about the inconvenience,

shane


"
shane knapp <sknapp@berkeley.edu>,"Mon, 27 Oct 2014 13:40:39 -0700","Re: jenkins emergency restart now, was Re: jenkins downtime tomorrow
 morning ~6am-8am PDT","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","ok we're back up and building.  i've retriggered the jobs i killed.


"
Will Benton <willb@redhat.com>,"Mon, 27 Oct 2014 17:14:06 -0400 (EDT)",Re: best IDE for scala + spark development?,ll <duy.huynh.uiv@gmail.com>,"I'll chime in as yet another user who is extremely happy with sbt and a text editor.  (In my experience, running ""ack"" from the command line is usually just as easy and fast as using an IDE's find-in-project facility.)  You can, of course, extend editors with Scala-specific IDE-like functionality (in particular, I am aware of -- but have not used -- ENSIME for emacs or TextMate).

Since you're new to Scala, you may not know that you can run any sbt command preceded by a tilde, which will watch files in your project and run the command when anything changes.  Therefore, running ""~compile"" from the sbt repl will get you most of the continuous syntax-checking functionality you can get from an IDE.

best,
wb

X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id 400A217737
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 27 Oct 2014 22:28:12 +0000 (UTC)
Received: (qmail 51342 invoked by uid 500); 27 Oct 2014 22:28:10 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 51263 invoked by uid 500); 27 Oct 2014 22:28:10 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 51252 invoked by uid 99); 27 Oct 2014 22:28:09 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 27 Oct 2014 22:28:09 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (nike.apache.org: domain of vanzin@cloudera.com designates 209.85.192.46 as permitted sender)
Received: from [209.85.192.46] (HELO mail-qg0-f46.google.com) (209.85.192.46)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 27 Oct 2014 22:27:44 +0000
Received: by mail-qg0-f46.google.com with SMTP id z60so4748303qgd.33
        for <dev@spark.apache.org>; Mon, 27 Oct 2014 15:25:28 -0700 (PDT)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:date:message-id:subject:from:to
         :content-type;
        bh=p8Cxv6KzzttEHGjqWK8Bt+bnSjQF+c5Yi66L501Qa/g=;
        b=mtB3SV5/o6IRvTJN0uvqUdhcUb1eglbtWKyjOo7dXYYhkRtqgonOHx/LgkBHkLxB/2
         4GrpkLPtYRlqT4LdXMmwtCDQHxV8HBEK02B5/ld6zWa40uVoJh6Ts0dKp+1/TEa0eDr8
         2S3/DyOK6LkPw0F0Ys3PayvqVCpB/EKQvAgd+/6OUe9uQVSkS6/GDwbgIimhSqCSvwDb
         +Gb8jZwZWxW+wEZlxFA05gyc0p9XVXn4uOTiI48kUJ0rCc7eVkqJ5zW61aHWvjrzBySJ
         xv/Hvk0V7cJWgUC4j0czV85Uhm4IW3V/BqifrT3zIEaOFxpcWu638v0CtB1Uxs1mmAnZ
         9ZxA==
X-Gm-Message-State: ALoCoQnUJ1Vm1PamW6si18vCHAKMzLmq5NH0NQNPFYtP7aI6yKwyVLBWxXhdXvk4NVhN95pj6tmT
MIME-Version: 1.0
X-Received: by 10.224.65.9 with SMTP id g9mr38143469qai.96.1414448727949; Mon,
 27 Oct 2014 15:25:27 -0700 (PDT)
Received: by 10.229.114.5 with HTTP; Mon, 27 Oct 2014 15:25:27 -0700 (PDT)
Date: Mon, 27 Oct 2014 15:25:27 -0700
Subject: HiveContext bug?
From: Marcelo Vanzin <vanzin@cloudera.com>
To: ""dev@spark.apache.org"" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org

Hey guys,

I've been using the ""HiveFromSpark"" example to test some changes and I
ran into an issue that manifests itself as an NPE inside Hive code
because some configuration object is null.

Tracing back, it seems that `sessionState` being a lazy val in
HiveContext is causing it. That variably is only evaluated in [1],
while the call in [2] causes a Driver to be initialized by [3], which
the tries to use the thread-local session state ([4]) which hasn't
been set yet.

This could be seen as a Hive bug ([3] should probably be calling the
constructor that takes a conf object), but is there a reason why these
fields are lazy in HiveContext? I explicitly called
SessionState.setCurrentSessionState() before the
CommandProcessorFactory call and that seems to fix the issue too.

[1] https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L305
[2] https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala#L289
[3] https://github.com/apache/hive/blob/9c63b2fdc35387d735f4c9d08761203711d4974b/ql/src/java/org/apache/hadoop/hive/ql/processors/CommandProcessorFactory.java#L104
[4] https://github.com/apache/hive/blob/9c63b2fdc35387d735f4c9d08761203711d4974b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java#L286

-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 27 Oct 2014 15:41:05 -0700",Re: HiveContext bug?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Well, looks like a huge coincidence, but this was just sent to github:
https://github.com/apache/spark/pull/2967




-- 
Marcelo

---------------------------------------------------------------------


"
Rahul Singhal <Rahul.Singhal@guavus.com>,"Tue, 28 Oct 2014 05:39:04 +0000",Workaround for python's inability to unzip zip64 spark assembly jar,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

We recently faced the known issue where pyspark does not work when the assembly jar contains more than 65K files. Our build and run time environment are both Java 7 but python fails to unzip the assembly jar as expected (https://issues.apache.org/jira/browse/SPARK-1911).

All nodes in our YARN cluster have spark deployed (at the same local location) on them so we are contemplating the following workaround (apart from using a Java 6 compiled assembly):

Modify PYTHONPATH to give preference to ""$SPARK_HOME/python"" & ""$SPARK_HOME/python/lib/py4j-0.8.1-src.zip"", with this the assembly does not need to be unzipped to access the python files. This worked fine for with my limited testing. And I think, this should work as long as the only reason to unzip the assembly jar is to extract the python files and nothing else (any reason to believe that this may not be the case?).

I would appreciate your opinion on this workaround.

Thanks,
Rahul Singhal
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 27 Oct 2014 23:51:57 -0700",Re: Support Hive 0.13 .1 in Spark SQL,"""Cheng, Hao"" <hao.cheng@intel.com>","Hey Cheng,

Right now we aren't using stable API's to communicate with the Hive
Metastore. We didn't want to drop support for Hive 0.12 so right now
we are using a shim layer to support compiling for 0.12 and 0.13. This
is very costly to maintain.

If Hive has a stable meta-data API for talking to a Metastore, we
should use that (is HCatalog sufficient for this purpose?). Ideally we
would be able to talk to multiple versions of the Hive metastore and
we can keep a single internal version of Hive for our use of Serde's,
etc.

I've created SPARK-4114 for this:
https://issues.apache.org/jira/browse/SPARK-4114

This is a very important issue for Spark SQL, so I'd welcome comments
on that JIRA from anyone who is familiar with Hive/HCatalog internals.

- Patrick


---------------------------------------------------------------------


"
Duy Huynh <duy.huynh.uiv@gmail.com>,"Tue, 28 Oct 2014 04:26:44 -0400",Re: best IDE for scala + spark development?,Will Benton <willb@redhat.com>,"thanks everyone.  i've been using vim and sbt recently, and i really like
it.  it's lightweight, fast.  plus, ack, ctrl-t, nerdtre, etc. in vim do
all the good work.

but, as i'm not familiar with scala/spark api yet, i really wish to have
these two things in vim + sbt.

1.  code completion as in intellij (typing long method / class name in
scala/spark isn't that fun!)

2.  scala doc on the fly in the text editor (just so i don't have to switch
back and forth between the text editor and the scala doc)

did anyone have experience with adding these 2 things to vim?

thanks!







"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 28 Oct 2014 17:29:54 +0800",Re: HiveContext bug?,Marcelo Vanzin <vanzin@cloudera.com>,"Hi Marcelo, yes this is a known Spark SQL bug and we've got PRs to fix it
(2887 & 2967). Not merged yet because newly merged Hive 0.13.1 support
causes some conflicts. Thanks for reporting this :)


"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 28 Oct 2014 17:34:18 +0800",Re: best IDE for scala + spark development?,Duy Huynh <duy.huynh.uiv@gmail.com>,"My two cents for Mac Vim/Emacs users. Fixed a Scala ctags Mac compatibility
bug months ago, and you may want to use the most recent version here
https://github.com/scala/scala-dist/blob/master/tool-support/src/emacs/contrib/dot-ctags




"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Tue, 28 Oct 2014 02:45:01 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi Anant,

Thank you for reviewing and helping us out. Please find the following link
where you can see the initial code.
https://github.com/codeAshu/Outlier-Detection-with-AVF-Spark/blob/master/OutlierWithAVFModel.scala


The input file for the code should be in csv format. We have provided a
dataset there at the link.

We are currently facing the following style issues in the code(code is
working fine though) :

At line no 62 and 79 we have redundant functions and variables
(count_dataPoint, count_trimmedData) for giving  line numbers within the
function trimScores().
 
At line no 144 and 149 if we do not use two separate functions to increment
line numbers we get erroneous results . Is there any alternative way of
handling that?

We think that it because of scala clousers where any local variable which is
not in RDD doesn't get updated in subsequent pairRDDFunctions.


Regards,
Ashutosh & Kaushik 



--

---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 28 Oct 2014 06:35:27 -0700",Re: jenkins downtime tomorrow morning ~6am-8am PDT,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","this is done, and jenkins is up and building again.


"
Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"Tue, 28 Oct 2014 18:18:11 +0100",How to run tests properly?,dev@spark.apache.org,"Hi,

I want to contribute to the MLlib library but I can't get the tests up
working. I've found three ways of running the tests on the commandline.
I just want to execute the MLlib tests.

1. via dev/run-tests script
    This script executes all tests and take several hours to finish.
Some tests failed but I can't say which of them. Should this really take
that long? Can I specify to run only MLlib tests?

2. directly via maven
I did the following described in the docs [0].

export MAVEN_OPTS=""-Xmx2g -XX:MaxPermSize=512M
-XX:ReservedCodeCacheSize=512m""
mvn -Pyarn -Phadoop-2.3 -DskipTests -Phive clean package
mvn -Pyarn -Phadoop-2.3 -Phive test

This also doesn't work.
Why do I have to package spark bevore running the tests?

3. via sbt
I tried the following. I freshly cloned spark and checked out the tag
v1.1.0-rc4.

sbt/sbt ""project mllib"" test

and get the following exception in several cluster tests.

[info] - task size should be small in both training and prediction ***
FAILED ***
[info]   org.apache.spark.SparkException: Job aborted due to stage
failure: Master removed our application: FAILED
[info]   at
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
[info]   at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
[info]   at
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
[info]   at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
[info]   at scala.Option.foreach(Option.scala:236)
[info]   at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)

summary:

[error] Failed: Total 223, Failed 12, Errors 0, Passed 211
[error] Failed tests:
[error]         org.apache.spark.mllib.clustering.KMeansClusterSuite
[error]        
org.apache.spark.mllib.classification.LogisticRegressionClusterSuite
[error]        
org.apache.spark.mllib.optimization.GradientDescentClusterSuite
[error]         org.apache.spark.mllib.classification.SVMClusterSuite
[error]        
org.apache.spark.mllib.linalg.distributed.RowMatrixClusterSuite
[error]        
org.apache.spark.mllib.regression.LinearRegressionClusterSuite
[error]         org.apache.spark.mllib.classification.NaiveBayesClusterSuite
[error]         org.apache.spark.mllib.regression.LassoClusterSuite
[error]        
org.apache.spark.mllib.regression.RidgeRegressionClusterSuite
[error]         org.apache.spark.mllib.optimization.LBFGSClusterSuite
[error] (mllib/test:test) sbt.TestsFailedException: Tests unsuccessful
[error] Total time: 661 s, completed 28.10.2014 17:13:10
sbt/sbt ""project mllib"" test  761,74s user 22,86s system 109% cpu
11:59,57 total

I tried several slightly different ways but I can't get the tests working.
I observed that the tests are running __very__ slow in some
configurations. The cpu nearly idles and the ram usage is low.

Am I doing something fundamental wrong? After many hours of trial and
error I'm stuck.
Long build and test durations are making it difficult to investigate.
Hopefully someone can give me a hint.
Which one is the right way to flexibly run the tests of the different
sub projects.

Thanks,
Niklas


[0] https://spark.apache.org/docs/latest/building-with-maven.html

---------------------------------------------------------------------


"
slcclimber <anant.asty@gmail.com>,"Tue, 28 Oct 2014 10:12:33 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Ashu,
There is one main issue and  a few stylistic/ grammatical things I noticed.
1> You take and rdd or type String which you expect to be comma separated.
This limits usability since the user will have to convert their RDD to that
format only for you to split it on string.
It would make more sense to take an RDD of type (col_num:Int ,
attr_value:Int), frequency:Int) 
You could also use Long instead of Int.

2> the increment functions could be more along the lines of 
    def incr = {count += 1; count}
which is ina a more functional style

3> reset functions could be simply 
    def reset_count = count = 1L

4> in
https://github.com/codeAshu/Outlier-Detection-with-AVF-Spark/blob/master/OutlierWithAVFModel.scala#L108
You have a key of type string which is basically a string of form ""number,
string""
when you could just have a tuple of the form (i:Int, word:String)

5? the lines exceed the style guides 100 character length

Thanks
Anant



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 28 Oct 2014 19:03:20 +0100",Re: How to run tests properly?,Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"
Yes, running all tests takes a long long time. It does print which
tests failed, and you can see the errors in the test output.

Did you read http://spark.apache.org/docs/latest/building-with-maven.html#spark-tests-in-maven
? This shows how to run just one test suite.

In any Maven project you can try things like ""mvn test -pl [module]""
to run just one module's tests.



What doesn't work?
Some tests use the built assembly, which requires packaging.



This just looks like a flaky test failure; I'd try again.

---------------------------------------------------------------------


"
Xuepeng Sun <xs6w@yahoo.com>,"Tue, 28 Oct 2014 12:53:45 -0700 (PDT)",Breeze::DiffFunction not serializable,dev@spark.incubator.apache.org,"Hi,

    I'm trying to call Breeze::LBFGS from the master on each partition but
getting *NonSerializable* error. 
I guess it's well-known that the Breeze DiffFunction is not serializable. 
 
///
import breeze.linalg.{Vector => BV, DenseVector=>BDV, SparseVector=>BSV}

val lbfgs = new breeze.optimize.LBFGS[BDV[Double]]
val wInit: BDV[Double] = Array.fill(numFeatures)(0.0).toBreeze

def localUpdate(d:Array[(Double, BV[Double])], w:BDV[Double]) : BDV[Double]
{
    	 
    def getObj = new DiffFunction[BDV[Double]] {

       def calculate(w: BDV[Double]) : (Double, BDV[Double]) = 
       {
        ...
       }
    }
    lbfgs.minimize(getObj, w)  
}

rdd.mapPartitions{ 

   iter: Iterator[(Double, BV[Double])] => {
            
         val d : Array[(Double, BV[Double])] = iter.toArray			
 	 
         val w : BDV[Double] = localUpdate(d, wInit) 	
         Iterator(w)
} 




The following link talks about using the KyroSerializationWrapper as a
solution: 

http://http://stackoverflow.com/questions/23050067/spark-task-not-serializable-how-to-work-with-complex-map-closures-that-call-o
<http://http://stackoverflow.com/questions/23050067/spark-task-not-serializable-how-to-work-with-complex-map-closures-that-call-o>  

But I didn't have good luck yet. Can some one points to a work-around way to
do the serialization?   
Thanks a lot. 


Xuepeng






--

---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Tue, 28 Oct 2014 19:42:48 -0700",HiveShim not found when building in Intellij,"""dev@spark.apache.org"" <dev@spark.apache.org>","I have run on the command line via maven and it is fine:

 compile package install


But with the latest code Intellij builds do not work. Following is one of
26 similar errors:


Error:(173, 38) not found: value HiveShim
          Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))
                                     ^
"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 28 Oct 2014 19:46:22 -0700",Re: HiveShim not found when building in Intellij,Stephen Boesch <javadba@gmail.com>,"Hi Stephen,

How did you generate your Maven workspace? You need to make sure the Hive profile is enabled for it. For example sbt/sbt -Phive gen-idea.

Matei

-Phadoop-2.3
of
Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))


---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Tue, 28 Oct 2014 19:57:37 -0700",Re: HiveShim not found when building in Intellij,Matei Zaharia <matei.zaharia@gmail.com>,"Hi Matei,
  Until my latest pull from upstream/master it had not been necessary to
add the hive profile: is it now??

I am not using sbt gen-idea. The way to open in intellij has been to Open
the parent directory. IJ recognizes it as a maven project.

There are several steps to do surgery on the yarn-parent / yarn projects ,
then do a full rebuild.  That was working until one week ago.
Intellij/maven is presently broken in  two ways:  this hive shim (which may
yet hopefully be a small/simple fix - let us see) and  (2) the
""NoClassDefFoundError
on ThreadFactoryBuilder"" from my prior emails -and which is quite a serious
 problem .

2014-10-28 19:46 GMT-07:00 Matei Zaharia <matei.zaharia@gmail.com>:

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Oct 2014 20:20:52 -0700",Re: HiveShim not found when building in Intellij,Stephen Boesch <javadba@gmail.com>,"Hey Stephen,

In some cases in the maven build we now have pluggable source
directories based on profiles using the maven build helper plug-in.
This is necessary to support cross building against different Hive
versions, and there will be additional instances of this due to
supporting scala 2.11 and 2.10.

In these cases, you may need to add source locations explicitly to
intellij if you want the entire project to compile there.

Unfortunately as long as we support cross-building like this, it will
be an issue. Intellij's maven support does not correctly detect our
use of the maven-build-plugin to add source directories.

We should come up with a good set of instructions on how to import the
pom files + add the few extra source directories. Off hand I am not
sure exactly what the correct sequence is.

- Patrick


---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Tue, 28 Oct 2014 21:09:37 -0700",Re: HiveShim not found when building in Intellij,Patrick Wendell <pwendell@gmail.com>,"Thanks Patrick for the heads up.

I have not been successful to discover a combination of profiles (i.e.
enabling hive or hive-0.12.0 or hive-13.0) that works in Intellij with
maven. Anyone who knows how to handle this - a quick note here would be
appreciated.



2014-10-28 20:20 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

"
Zhan Zhang <zzhang@hortonworks.com>,"Tue, 28 Oct 2014 21:32:35 -0700",Re: HiveShim not found when building in Intellij,Stephen Boesch <javadba@gmail.com>,"-Phive is to enable hive-0.13.1 and ""-Phive -Phive-0.12.0” is to enable hive-0.12.0. Note that the thrift-server is not supported yet in hive-0.13, but expected to go to upstream soon (Spark-3720).

Thanks.

Zhan Zhang


 

e:
en
s


-- 
CONFIDENTIALITY N"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 29 Oct 2014 12:38:16 +0800",Re: HiveShim not found when building in Intellij,"Zhan Zhang <zzhang@hortonworks.com>, 
 Stephen Boesch <javadba@gmail.com>","Yes, these two combinations work for me.



---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Tue, 28 Oct 2014 21:42:54 -0700",Re: HiveShim not found when building in Intellij,Cheng Lian <lian.cs.zju@gmail.com>,"I am interested specifically in how to build (and hopefully run/debug..)
under Intellij.  Your posts sound like command line maven - which has
always been working already.

Do you have instructions for building in IJ?

2014-10-28 21:38 GMT-07:00 Cheng Lian <lian.cs.zju@gmail.com>:

 enable
13,
ch
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Oct 2014 21:45:26 -0700",Re: HiveShim not found when building in Intellij,Stephen Boesch <javadba@gmail.com>,"Btw - we should have part of the official docs that describes a full
""from scratch"" build in IntelliJ including any gotchas. Then we can
update it if there are build changes that alter it. I created this
JIRA for it:

https://issues.apache.org/jira/browse/SPARK-4128


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 29 Oct 2014 12:54:00 +0800",Re: HiveShim not found when building in Intellij,Stephen Boesch <javadba@gmail.com>,"You may first open the root pom.xml file in IDEA, and then go for menu 
View / Tool Windows / Maven Projects, then choose desired Maven profile 
combination under the ""Profiles"" node (e.g. I usually use hadoop-2.4 + 
hive + hive-0.12.0). IDEA will ask you to re-import the Maven projects, 
confirm, then it should be OK.

I can debug within IDEA with this approach. However, you have to clean 
the whole project before debugging Spark within IDEA if you compiled the 
project outside IDEA. Haven't got time to investigate this annoying issue.

Also, you can remove sub projects unrelated to your tasks to accelerate 
compilation and/or avoid other IDEA build issues (e.g. Avro related 
Spark streaming build failure in IDEA).


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Oct 2014 21:57:49 -0700",Re: HiveShim not found when building in Intellij,Cheng Lian <lian.cs.zju@gmail.com>,"I just started a totally fresh IntelliJ project importing from our
root pom. I used all the default options and I added ""hadoop-2.4,
hive, hive-0.13.1"" profiles. I was able to run spark core tests from
within IntelliJ. Didn't try anything beyond that, but FWIW this
worked.

- Patrick


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 29 Oct 2014 12:57:09 +0800",Re: HiveShim not found when building in Intellij,"Patrick Wendell <pwendell@gmail.com>, 
 Stephen Boesch <javadba@gmail.com>","Hao Cheng had just written such a ""from scratch"" guide for building 
Spark SQL in IDEA. Although it's written in Chinese, I think the 
illustrations are already descriptive enough.

http://www.cnblogs.com/cccchhhh/articles/4058371.html




---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Tue, 28 Oct 2014 22:03:24 -0700",Re: HiveShim not found when building in Intellij,Patrick Wendell <pwendell@gmail.com>,"I have selected the same options as Cheng LIang: hadoop-2.4, hive, hive
0.12.0 .  After  a full Rebuild in IJ I  still see the HiveShim errors.

I really do not know what is different. I had pulled three hours ago from
github upstream master.

Just for kicks i am trying PW's combination which uses 0.13.1 now.. But it
appears there is something else going on here.

Patrick/ Cheng:  did you build on the command line using Maven first?  I do
that since in the past that had been required.

2014-10-28 21:57 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Oct 2014 22:05:30 -0700",Re: HiveShim not found when building in Intellij,Cheng Lian <lian.cs.zju@gmail.com>,"Cheng - to make it recognize the new HiveShim for 0.12 I had to click
on spark-hive under ""packages"" in the left pane, then go to ""Open
Module Settings"" - then explicitly add the v0.12.0/src/main/scala
folder to the sources by navigating to it and then <ctrl>+click to add
it as a source. Did you have to do this?


---------------------------------------------------------------------


"
Stephen Boesch <javadba@gmail.com>,"Tue, 28 Oct 2014 22:14:51 -0700",Re: HiveShim not found when building in Intellij,Patrick Wendell <pwendell@gmail.com>,"Thanks guys - adding the source root for the shim manually was the issue.

For some reason the other issue I  was struggling with
(NoCLassDefFoundError on ThreadFactoryBuilder) also disappeared. I am able
to run tests now inside IJ.  Woot

2014-10-28 22:13 GMT-07:00 Patrick Wendell <pwendell@gmail.com>:

"
Patrick Wendell <pwendell@gmail.com>,"Tue, 28 Oct 2014 22:13:12 -0700",Re: HiveShim not found when building in Intellij,Cheng Lian <lian.cs.zju@gmail.com>,"Oops - I actually should have added v0.13.0 (i.e. to match whatever I
did in the profile).


---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 29 Oct 2014 13:19:53 +0800",Re: HiveShim not found when building in Intellij,Patrick Wendell <pwendell@gmail.com>,"Hm, the shim source folder could be automatically recognized some time 
before, although at a wrong directory level (sql/hive/v0.12.0/src 
instead of sql/hive/v0.12.0/src/main/scala), it compiles.

Just tried against a fresh checkout, indeed need to add shim source 
folder manually. Sorry for the confusion.

Cheng



---------------------------------------------------------------------


"
Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"Wed, 29 Oct 2014 18:02:39 +0100",Re: How to run tests properly?,Sean Owen <sowen@cloudera.com>,"Hi Sean,

thanks for your reply. The tests still don't work. I focused on the
mllib and core tests and made some observations.

The core tests seems to fail because of my german locale. Some tests are
locale dependend like the
UtilsSuite.scala
 - ""string formatting of time durations"" - checks for locale dependend
seperators like ""."" and "",""
 - ""isBindCollision"" - checks for the locale dependend exception message

In the MLlib it seems to be just one source of failure. The same
Exception I described in my first mail appears several times in
different tests.
The reason for all the similar failures is the line 29 in
LocalClusterSparkContext.scala.
When I change the line
.setMaster(""local-cluster[2, 1, 512]"")
to
.setMaster(""local"")
all tests run without a failure. The local-cluster mode seems to be the
reason for the failure. I tried some different configurations like
[1,1,512], [2,1,1024] etc. but couldn't get the tests run without a failure.

Could this be a configuration issue?

Yes I tried that as described below at point 2.
I get the same Exceptions as in every other way.
I don't think so. I tried for several times now in several different ways.

Thanks,
Niklas



---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 29 Oct 2014 11:01:28 -0700",Re: How to run tests properly?,Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"The ""local-cluster"" tests depend on Spark already being packaged.

- Patrick


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 29 Oct 2014 11:23:57 -0700",matrix factorization cross validation,dev <dev@spark.apache.org>,"Hi,

In the current factorization flow, we cross validate on the test dataset
using the RMSE number but there are some other measures which are worth
looking into.

If we consider the problem as a regression problem and the ratings 1-5 are
considered as 5 classes, it is possible to generate a confusion matrix
using MultiClassMetrics.scala

If the ratings are only 0/1 (like from the spotify demo from spark summit)
then it is possible to use Binary Classification Metrices to come up with
the ROC curve...

For topK user/products we should also look into prec@k and pdcg@k as the
metric..

Does it make sense to add the multiclass metric and prec@k, pdcg@k in
examples.MovielensALS along with RMSE ?

Thanks.
Deb
"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 29 Oct 2014 12:14:48 -0700",Re: matrix factorization cross validation,Debasish Das <debasish.das83@gmail.com>,"Let's narrow the context from matrix factorization to recommendation
via ALS. It adds extra complexity if we treat it as a multi-class
classification problem. ALS only outputs a single value for each
prediction, which is hard to convert to probability distribution over
the 5 rating levels. Treating it as a binary classification problem or
a ranking problem does make sense. The RankingMetricc is in master.
Free free to add prec@k and ndcg@k to examples.MovielensALS. ROC
should be good to add as well. -Xiangrui



---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 29 Oct 2014 13:17:31 -0700",Re: matrix factorization cross validation,Xiangrui Meng <mengxr@gmail.com>,"Makes sense for the binary and ranking problem but for example linear
regression for multi-class also optimizes on RMSE but we still measure the
prediction efficiency using some measure on confusion matrix...Is not the
same idea should hold for ALS as well ?



"
Sean Owen <sowen@cloudera.com>,"Wed, 29 Oct 2014 22:37:21 +0100",Re: How to run tests properly?,Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"
Could be! If you can fix them with a few uses of Locale in the code,
that's definitely worth filing a JIRA and opening a PR.


Not sure, what do you mean that it fails?

---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Wed, 29 Oct 2014 20:28:16 -0700",Re: matrix factorization cross validation,Xiangrui Meng <mengxr@gmail.com>,"Is there an example of how to use RankingMetrics ?

Let's take the user, document example...we get user x topic and document x
topic matrices as the model...

Now for each user, we can generate topK document by doing a sort on (1 x
topic)dot(topic x document) and picking topK...

Is it possible to validate such a topK finding algorithm using
RankingMetrics ?



"
Nick Pentreath <nick.pentreath@gmail.com>,"Thu, 30 Oct 2014 10:12:33 +0200",Re: matrix factorization cross validation,Debasish Das <debasish.das83@gmail.com>,"Looking at
https://github.com/apache/spark/blob/814a9cd7fabebf2a06f7e2e5d46b6a2b28b917c2/mllib/src/main/scala/org/apache/spark/mllib/evaluation/RankingMetrics.scala#L82

For each user in test set, you generate an Array of top K predicted item
ids (Int or String probably), and an Array of ground truth item ids (the
known rated or liked items in the test set for that user), and pass that to
precisionAt(k) to compute MAP@k (Actually this method name is a bit
misleading - it should be meanAveragePrecisionAt where the other method
there is without a cutoff at k. However, both compute MAP).

The challenge at scale is actually computing all the top Ks for each user,
as it requires broadcasting all the item factors (unless there is a smarter
way?)

I wonder if it is possible to extend the DIMSUM idea to computing top K
matrix multiply between the user and item factor matrices, as opposed to
all-pairs similarity of one matrix?


"
nm3mon@gmail.com,"Thu, 30 Oct 2014 06:41:55 -0400",Re: best IDE for scala + spark development?,Cheng Lian <lian.cs.zju@gmail.com>,"IntelliJ idea scala plugin comes with an enhanced REPL. It's a pretty decent option too. 

Nabeel

y
trib/dot-ctags
e:

ch

ala-spark-development-tp8965.html

---------------------------------------------------------------------


"
Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"Thu, 30 Oct 2014 12:27:27 +0100",Re: How to run tests properly?,Patrick Wendell <pwendell@gmail.com>,"Can you please briefly explain why packaging is necessary. I thought
packaging would only build the jar and place it in the target folder.
How does that affect the tests? If tests depend on the assembly a ""mvn
install"" would be more sensible to me.
Probably I misunderstand the maven build life-cycle.

Thanks,
Niklas



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 30 Oct 2014 12:36:37 +0100",Re: How to run tests properly?,Niklas Wilcke <1wilcke@informatik.uni-hamburg.de>,"You are right that this is a bit weird compared to the Maven lifecycle
semantics. Maven wants assembly to come after tests but here tests want to
launch the final assembly as part of some tests. Yes you would not normally
have to do this in 2 stages.

"
Stephen Boesch <javadba@gmail.com>,"Thu, 30 Oct 2014 06:39:48 -0700",Re: best IDE for scala + spark development?,nm3mon@gmail.com,"HI Nabeel,
  In what ways is the IJ version of scala repl enhanced?  thx!

2014-10-30 3:41 GMT-07:00 <nm3mon@gmail.com>:

"
Stephen Boesch <javadba@gmail.com>,"Thu, 30 Oct 2014 06:39:48 -0700",Re: best IDE for scala + spark development?,nm3mon@gmail.com,"HI Nabeel,
  In what ways is the IJ version of scala repl enhanced?  thx!

2014-10-30 3:41 GMT-07:00 <nm3mon@gmail.com>:

"
Debasish Das <debasish.das83@gmail.com>,"Thu, 30 Oct 2014 08:13:10 -0700",Re: matrix factorization cross validation,Nick Pentreath <nick.pentreath@gmail.com>,"I thought topK will save us...for each user we have 1xrank...now our movie
factor is a RDD...we pick topK movie factors based on vector norm...with K
= 50, we will have 50 vectors * num_executors in a RDD...with the user
1xrank we do a distributed dot product using RowMatrix APIs...

May be we can't find topK using vector norm on movie factors...


"
nm3mon@gmail.com,"Thu, 30 Oct 2014 14:30:41 -0400",Re: best IDE for scala + spark development?,Stephen Boesch <javadba@gmail.com>,"Multiline support (much shinier than :paste), smart completion and things that an IDE makes easy or better (without any hassle). In particular, fast switching between REPL and editor while staying in the same screen makes me even more productive. 

Nabeel

ent option too.
lity
contrib/dot-ctags
rote:
ike
o
ve

witch
:
 a
s
nd
m
bt?
-scala-spark-development-tp8965.html
-
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 30 Oct 2014 11:35:32 -0700",Re: How to run tests properly?,Sean Owen <sowen@cloudera.com>,"Some of our tests actually require spinning up a small multi-process
spark cluster. These use the normal deployment codepath for Spark
which is that we rely on the spark ""assembly jar"" to be present. That
jar is generated when you run ""mvn package"" via a special sub project
called assembly in our build. This is a bit non-standard. The reason
is that some of our tests are really mini integration tests.

- Patrick


---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Thu, 30 Oct 2014 13:41:30 -0700",Re: matrix factorization cross validation,Nick Pentreath <nick.pentreath@gmail.com>,"I am working on it...I will open up a JIRA once I see some results..

Idea is to come up with a test train set based on users...basically for
each user, we come up with 80% train data and 20% test data...

Now we pick up a K (each user should have a different K based on the movies
he watched so some multiplier) and then we get topK for each user and see
the confusion matrix for each user...

This data will also go to RankingMetrics I think...one is ground truth
array and the other is our prediction...I would like to see the raw
confusions as well..

These measures are necessary to validate any of the topic modeling
algorithms as well...

Is there a better place for it other than mllib examples ?


"
Gerard Maas <gerard.maas@gmail.com>,"Thu, 30 Oct 2014 21:53:35 +0100",Registering custom metrics,"spark users <user@spark.apache.org>, dev@spark.apache.org","vHi,

I've been exploring the metrics exposed by Spark and I'm wondering whether
there's a way to register job-specific metrics that could be exposed
through the existing metrics system.

Would there be an  example somewhere?

BTW, documentation about how the metrics work could be improved. I found
out about the default servlet and the metrics/json/ endpoint on the code. I
could not find any reference to that on the dedicated doc page [1].
Probably something I could contribute if there's nobody on that at the
moment.

-kr, Gerard.

[1]   http://spark.apache.org/docs/1.1.0/monitoring.html#Metrics
"
Sean Owen <sowen@cloudera.com>,"Thu, 30 Oct 2014 22:15:37 +0100",Re: matrix factorization cross validation,Debasish Das <debasish.das83@gmail.com>,"The pretty standard metric for recommenders is mean average precision,
and RankingMetrics will already do that as-is. I don't know that a
confusion matrix for this binary classification does much.



---------------------------------------------------------------------


"
Debasish Das <debasish.das83@gmail.com>,"Thu, 30 Oct 2014 14:21:51 -0700",Re: matrix factorization cross validation,Sean Owen <sowen@cloudera.com>,"Does it make sense to have a user specific K or K is considered same over
all users ?

Intuitively the users who watches more movies should get a higher K than
the others...


"
Sean Owen <sowen@cloudera.com>,"Thu, 30 Oct 2014 22:24:35 +0100",Re: matrix factorization cross validation,Debasish Das <debasish.das83@gmail.com>,"MAP is effectively an average over all k from 1 to min(#
recommendations, # items rated) Getting first recommendations right is
more important than the last.


---------------------------------------------------------------------


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Thu, 30 Oct 2014 21:31:02 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Hi Anant,
sorry for my late reply. Thank you for taking time and reviewing it.
 
I have few comments on first issue.

You are correct on the string (csv) part. But we can not take input of type
you mentioned. We calculate frequency in our function. Otherwise user has to
do all this computation. I realize that taking a RDD[Vector] would be
general enough for all. What do you say?

I agree on rest all the issues. I will correct them soon and post it.
I have a doubt on test cases. Where should I put data while giving test
scripts? or should i generate synthetic data for testing with in the
scripts, how does this work?

Regards,
Ashutosh



--

---------------------------------------------------------------------


"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Thu, 30 Oct 2014 21:38:32 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"?Okay. I'll try it and post it soon with test case. After that I think we can go ahead with the PR.

________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+s1001551n9035h61@n3.nabble.com>
Sent: Friday, October 31, 2014 10:03 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


Ashutosh,
A vector would be a good idea vectors are used very frequently.
Test data is usually stored in the spark/data/mllib folder

Hi Anant,
sorry for my late reply. Thank you for taking time and reviewing it.

I have few comments on first issue.

You are correct on the string (csv) part. But we can not take input of type you mentioned. We calculate frequency in our function. Otherwise user has to do all this computation. I realize that taking a RDD[Vector] would be general enough for all. What do you say?

I agree on rest all the issues. I will correct them soon and post it.
I have a doubt on test cases. Where should I put data while giving test scripts? or should i generate synthetic data for testing with in the scripts, how does this work?

Regards,
Ashutosh

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9034.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9035.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
Ashutosh <ashutosh.trivedi@iiitb.org>,"Thu, 30 Oct 2014 21:41:43 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"A?lready done. Here is the link

 https://issues.apache.org/jira/browse/SPARK-4038

________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+s1001551n9037h23@n3.nabble.com>
Sent: Friday, October 31, 2014 10:09 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


You should create a jira ticket to go with it as well.
Thanks


?Okay. I'll try it and post it soon with test case. After that I think we can go ahead with the PR.

________________________________
From: slcclimber [via Apache Spark Developers List] <ml-node+[hidden email]<http://user/SendEmail.jtp?type=node&node=9036&i=0>>
Sent: Friday, October 31, 2014 10:03 AM
To: Ashutosh Trivedi (MT2013030)
Subject: Re: [MLlib] Contributing Algorithm for Outlier Detection


Ashutosh,
A vector would be a good idea vectors are used very frequently.
Test data is usually stored in the spark/data/mllib folder

Hi Anant,
sorry for my late reply. Thank you for taking time and reviewing it.

I have few comments on first issue.

You are correct on the string (csv) part. But we can not take input of type you mentioned. We calculate frequency in our function. Otherwise user has to do all this computation. I realize that taking a RDD[Vector] would be general enough for all. What do you say?

I agree on rest all the issues. I will correct them soon and post it.
I have a doubt on test cases. Where should I put data while giving test scripts? or should i generate synthetic data for testing with in the scripts, how does this work?

Regards,
Ashutosh

________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9034.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9035.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9036.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>


________________________________
If you reply to this email, your message will be added to the discussion below:
http://apache-spark-developers-list.1001551.n3.nabble.com/MLlib-Contributing-Algorithm-for-Outlier-Detection-tp8880p9037.html
NAML<http://apache-spark-developers-list.1001551.n3.nabble.com/template/NamlServlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 30 Oct 2014 21:45:01 -0700 (PDT)",Re: matrix factorization cross validation,"""Sean Owen"" <sowen@cloudera.com>","Sean, re my point earlier do you know a more efficient way to compute top k for each user, other than to broadcast the item factors?Â 


(I guess one can use the new asymmetric lsh paper perhaps to assist)


â€”
Sent from Mailbox


over
 the"
slcclimber <anant.asty@gmail.com>,"Thu, 30 Oct 2014 21:33:41 -0700 (PDT)",Re: [MLlib] Contributing Algorithm for Outlier Detection,dev@spark.incubator.apache.org,"Ashutosh,
A vector would be a good idea vectors are used very frequently.
Test data is usually stored in the spark/data/mllib folder





--"
Sean Owen <sowen@cloudera.com>,"Fri, 31 Oct 2014 09:26:55 +0100",Re: matrix factorization cross validation,Nick Pentreath <nick.pentreath@gmail.com>,"No, excepting approximate methods like LSH to figure out the
relatively small set of candidates for the users in the partition, and
broadcast or join those.

 k
an

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 31 Oct 2014 13:38:20 -0400",Surprising Spark SQL benchmark,dev <dev@spark.apache.org>,"I know we don't want to be jumping at every benchmark someone posts out
there, but this one surprised me:

http://www.citusdata.com/blog/86-making-postgresql-scale-hadoop-style

This benchmark has Spark SQL failing to complete several queries in the
TPC-H benchmark. I don't understand much about the details of performing
benchmarks, but this was surprising.

Are these results expected?

Related HN discussion here: https://news.ycombinator.com/item?id=8539678

Nick
"
Patrick Wendell <pwendell@gmail.com>,"Fri, 31 Oct 2014 11:09:18 -0700",Re: Surprising Spark SQL benchmark,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nick,

Unfortunately Citus Data didn't contact any of the Spark or Spark SQL
developers when running this. It is really easy to make one system
look better than others when you are running a benchmark yourself
because tuning and sizing can lead to a 10X performance improvement.
This benchmark doesn't share the mechanism in a reproducible way.

There are a bunch of things that aren't clear here:

1. Spark SQL has optimized parquet features, were these turned on?
2. It doesn't mention computing statistics in Spark SQL, but it does
this for Impala and Parquet. Statistics allow Spark SQL to broadcast
small tables which can make a 10X difference in TPC-H.
3. For data larger than memory, Spark SQL often performs better if you
don't call ""cache"", did they try this?

Basically, a self-reported marketing benchmark like this that
*shocker* concludes this vendor's solution is the best, is not
particularly useful.

If Citus data wants to run a credible benchmark, I'd invite them to
directly involve Spark SQL developers in the future. Until then, I
wouldn't give much credence to this or any other similar vendor
benchmark.

- Patrick


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 31 Oct 2014 14:30:40 -0400",Re: Surprising Spark SQL benchmark,Patrick Wendell <pwendell@gmail.com>,"Thanks for the response, Patrick.

I guess the key takeaways are 1) the tuning/config details are everything
(they're not laid out here), 2) the benchmark should be reproducible (it's
not), and 3) reach out to the relevant devs before publishing (didn't
happen).

Probably key takeaways for any kind of benchmark, really...

Nick


2014ë…„ 10ì›” 31ì¼ ê¸ˆìš”ì¼, Patrick Wendell<pwendell@gmail.com>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:

g
678
"
Steve Nunez <snunez@hortonworks.com>,"Fri, 31 Oct 2014 12:13:29 -0700",Re: Surprising Spark SQL benchmark,"Nicholas Chammas <nicholas.chammas@gmail.com>,
	Patrick Wendell <pwendell@gmail.com>","To be fair, we (Spark community) haven¡¯t been any better, for example this
benchmark:

	https://databricks.com/blog/2014/10/10/spark-petabyte-sort.html


For which no details or code have been released to allow others to
reproduce it. I would encourage anyone doing a Spark benchmark in future
to avoid the stigma of vendor reported benchmarks and publish enough
information and code to let others repeat the exercise easily.

	- Steve




gmail.com>´ÔÀÌ ÀÛ¼ºÇÑ ¸Þ½ÃÁö:



-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 31 Oct 2014 15:45:08 -0400",Re: Surprising Spark SQL benchmark,Steve Nunez <snunez@hortonworks.com>,"I believe that benchmark has a pending certification on it. See
http://sortbenchmark.org under ""Process"".

It's true they did not share enough details on the blog for readers to
reproduce the benchmark, but they will have to share enough with the
committee behind the benchmark in order to be certified. Given that this is
a benchmark not many people will be able to reproduce due to size and
complexity, I don't see it as a big negative that the details are not laid
out as long as there is independent certification from a third party.

https://amplab.cs.berkeley.edu/benchmark/

Is has all the details you'd expect, including hosted datasets, to allow
anyone to reproduce the full benchmark, covering a number of systems. I
look forward to the next update to that benchmark (a lot has changed since
Feb). And from what I can tell, it's produced by the same people behind
Spark (Patrick being among them).

So I disagree that the Spark community ""hasn't been any better"" in this
regard.

Nick


2014ë…„ 10ì›” 31ì¼ ê¸ˆìš”ì¼, Steve Nunez<snunez@hortonworks.com>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:

mple this
g
's
ck Wendell<pwendell@gmail.com <javascript:;>>ë‹˜ì´
e
to
at
ly
"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Fri, 31 Oct 2014 12:51:51 -0700",Re: Surprising Spark SQL benchmark,Nicholas Chammas <nicholas.chammas@gmail.com>,"There's been an effort in the AMPLab at Berkeley to set up a shared
codebase that makes it easy to run TPC-DS on SparkSQL, since it's something
we do frequently in the lab to evaluate new research.  Based on this
thread, it sounds like making this more widely-available is something that
would be useful to folks for reproducing the results published by
Databricks / Hortonworks / Cloudera / etc.; we'll share the code on the
list as soon as we're done.

-Kay


is
d
e
Nunez<snunez@hortonworks.com>ë‹˜ì´ ìž‘ì„±í•œ ë©”ì‹œì§€:
xample
e
rick Wendell<pwendell@gmail.com <javascript:;>>ë‹˜ì´
L
ou
s
n
y
er
"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 31 Oct 2014 13:35:04 -0700",Spark consulting,dev@spark.apache.org,"Hello,

Is anyone open to do some consulting work on Spark in San Mateo?

Thanks.

Alex
"
Stephen Boesch <javadba@gmail.com>,"Fri, 31 Oct 2014 13:44:16 -0700",Re: Spark consulting,Alessandro Baretta <alexbaretta@gmail.com>,"May we please refrain from using spark mailing list for job inquiries.
Thanks.

2014-10-31 13:35 GMT-07:00 Alessandro Baretta <alexbaretta@gmail.com>:

"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 31 Oct 2014 13:48:13 -0700",Re: Spark consulting,Stephen Boesch <javadba@gmail.com>,"Stephen,

and the LinkedIn Spark group is a desert.

Alex


"
Gary Malouf <malouf.gary@gmail.com>,"Fri, 31 Oct 2014 16:49:56 -0400",Parquet Migrations,"""dev@spark.apache.org"" <dev@spark.apache.org>","Outside of what is discussed here
<https://issues.apache.org/jira/browse/SPARK-3851> as a future solution, is
there any path for being able to modify a Parquet schema once some data has
been written?  This seems like the kind of thing that should make people
pause when considering whether or not to use Parquet+Spark...
"
Stephen Boesch <javadba@gmail.com>,"Fri, 31 Oct 2014 13:55:04 -0700",Re: Spark consulting,Alessandro Baretta <alexbaretta@gmail.com>,"HI Alessandro,
         It is important to me and probably others as well to be able to
focus on the technical issues and not be distracted that way.
thanks

stephenb

2014-10-31 13:48 GMT-07:00 Alessandro Baretta <alexbaretta@gmail.com>:

"
Michael Armbrust <michael@databricks.com>,"Fri, 31 Oct 2014 14:22:11 -0700",Re: Parquet Migrations,Gary Malouf <malouf.gary@gmail.com>,"You can't change parquet schema without reencoding the data as you need to
recalculate the footer index data.  You can manually do what SPARK-3851
<https://issues.apache.org/jira/browse/SPARK-3851> is going to do today
however.

Consider two schemas:

Old Schema: (a: Int, b: String)
New Schema, where I've dropped and added a column: (a: Int, c: Long)

parquetFile(old).registerTempTable(""old"")
parquetFile(new).registerTempTable(""new"")

sql(""""""
  SELECT a, b, CAST(null AS LONG) AS c  FROM old UNION ALL
  SELECT a, CAST(null AS STRING) AS b, c FROM new
"""""").registerTempTable(""unifiedData"")

Because of filter/column pushdown past UNIONs this should executed as
desired even if you write more complicated queries on top of
""unifiedData"".  Its a little onerous but should work for now.  This can
also support things like column renaming which would be much harder to do
automatically.


"
