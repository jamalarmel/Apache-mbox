Josh Rosen <rosenville@gmail.com>,"Wed, 31 Dec 2014 17:07:56 -0800",Today's Jenkins failures in the Spark Maven builds,dev <dev@spark.apache.org>,"If you've been following AMPLab Jenkins today, you'll notice that there's
been a huge number of Spark test failures in the maintenance branches and
Maven builds.

My best guess as to what's causing this is that I pushed a backport to all
maintenance branches at a moment where Jenkins was otherwise idle, causing
many builds to kick off at almost the exact same time and eventually fail
due to port contention issues in SparkSubmit tests (we didn't disable the
web UI when making external calls to ./spark-submit).  I pushed a hotfix to
address this.  When the first wave of Jenkins builds failed, the next wave
kicked off more-or-less in lockstep since there's only ever one active
build for the master builds and the problem was hit again, this time
failing a DriverSuite test (which has a port contention problem that needs
a separate fix; I'll hotfix this soon).

I believe that this flakiness is due to the lockstep synchronization of the
first wave of builds (e.g. a bunch of builds that ran DriverSuite and
SparkSubmitSuite within a minute or two of each other), and not changes in
recent patches.  If the problem persists after further web UI disabling
hotfixes, then I'll investigate the recent changes in more detail.

Thanks,
Josh
"
Alessandro Baretta <alexbaretta@gmail.com>,"Wed, 31 Dec 2014 17:48:59 -0800",Spark driver main thread hanging after SQL insert,"""dev@spark.apache.org"" <dev@spark.apache.org>","Here's what the console shows:

15/01/01 01:12:29 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 58.0,
whose tasks have all completed, from pool
15/01/01 01:12:29 INFO scheduler.DAGScheduler: Stage 58 (runJob at
ParquetTableOperations.scala:326) finished in 5493.549 s
15/01/01 01:12:29 INFO scheduler.DAGScheduler: Job 41 finished: runJob at
ParquetTableOperations.scala:326, took 5493.747061 s

It is now 01:40:03, so the driver has been hanging for the last 28 minutes.
The web UI on the other hand shows that all tasks completed successfully,
and the output directory has been populated--although the _SUCCESS file is
missing.

It is worth noting that my code started this job as its own thread. The
actual code looks like the following snippet, modulo some simplifications.

  def save_to_parquet(allowExisting : Boolean = false) = {
    val threads = tables.map(table => {
      val thread = new Thread {
        override def run {
          table.insertInto(t.table_name)
        }
      }
      thread.start
      thread
    })
    threads.foreach(_.join)
  }

As far as I can see the insertInto call never returns. Any idea why?

Alex
"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Fri, 2 Jan 2015 06:24:18 +0530",Highly interested in contributing to spark,dev@spark.apache.org,"Hello,

I am Manoj (https://github.com/MechCoder), an undergraduate student highly
interested in Machine Learning. I have contributed to SymPy and
scikit-learn as part of Google Summer of Code projects and my bachelor's
thesis. I have a few quick (non-technical) questions before I dive into the
issue tracker.

Are the ones marked trivial easy to fix ones, that I could try before
attempting slightly more ambitious ones? Also I would like to know if
Apache Spark takes part in Google Summer of Code projects under the Apache
Software Foundation. It would be really great if it does!

Looking forward!

-- 
Godspeed,
Manoj Kumar,
Mech Undergrad
http://manojbits.wordpress.com
"
Reynold Xin <rxin@databricks.com>,"Thu, 1 Jan 2015 20:25:26 -0800",Re: Highly interested in contributing to spark,Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Hi Manoj,

Thanks for the email.

Yes - you should start with the starter task before attempting larger ones.
Last year I signed up as a mentor for GSoC, but no student signed up. I
don't think I'd have time to be a mentor this year, but others might.



"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 01 Jan 2015 22:19:36 -0800 (PST)",Re: Highly interested in contributing to spark,"""Reynold Xin"" <rxin@databricks.com>","I'm sure Spark will sign up for GSoC again this year - and id be surprised if there was not some interest now for projects :)


If I have the time at that point in the year I'd be happy to mentor a project in MLlib but will have to see how my schedule is at that point!




Manoj perhaps some of the locality sensitive hashing stuff you did for scikit-learn could find its way to Spark or spark-projects.


‚Äî
Sent from Mailbox


ones.
com>
highly
bachelor's
the
Apache"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Thu, 01 Jan 2015 22:21:15 -0800 (PST)",Re: Highly interested in contributing to spark,"""Reynold Xin"" <rxin@databricks.com>","Oh actually I was confused with another project, yours was not LSH sorry!






‚Äî
Sent from Mailbox


surprised if there was not some interest now for projects :)
project in MLlib but will have to see how my schedule is at that point!
scikit-learn could find its way to Spark or spark-projects.
ones.
com>
highly
bachelor's
 the
if
Apache"
Patrick Wendell <pwendell@gmail.com>,"Fri, 2 Jan 2015 12:13:38 -0500",Re: Spark driver main thread hanging after SQL insert,Alessandro Baretta <alexbaretta@gmail.com>,"Hi Alessandro,

Can you create a JIRA for this rather than reporting it on the dev
list? That's where we track issues like this. Thanks!.

- Patrick


---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 2 Jan 2015 09:16:53 -0800",Re: Spark driver main thread hanging after SQL insert,Patrick Wendell <pwendell@gmail.com>,"Patrick,

Sure. I was interested in knowing if anyone experienced a similar issue and
whether there was any known workaround. Anyway will report on JIRA.

Alex

"
Manoj Kumar <manojkumarsivaraj334@gmail.com>,"Fri, 2 Jan 2015 23:43:05 +0530",Re: Highly interested in contributing to spark,Nick Pentreath <nick.pentreath@gmail.com>,"Hello,

Thanks for your quick comments and encouragement.

I tried building Spark from source using build/sbt assembly

It however fails at this point

downloading
https://repo1.maven.org/maven2/org/scala-lang/scala-library/2.10.4/scala-library-2.10.4.jar
with SSL certificate errors. I understand that it is due to this problem (
http://apache-spark-user-list.1001560.n3.nabble.com/sbt-sbt-assembly-fails-with-ssl-certificate-error-td3046.html
)

but I'm not sure why it still it uses https when this PR
https://github.com/apache/spark/pull/209 has fixed it. Any help would be
greatful.









-- 
Godspeed,
Manoj Kumar,
Intern, Telecom ParisTech
Mech Undergrad
http://manojbits.wordpress.com
"
"""Ganelin, Ilya"" <Ilya.Ganelin@capitalone.com>","Fri, 2 Jan 2015 14:23:31 -0500",Re: Highly interested in contributing to spark,"Manoj Kumar <manojkumarsivaraj334@gmail.com>, Nick Pentreath
	<nick.pentreath@gmail.com>","I might be seeing a similar error - Iπm trying to build behind a proxy. I
was able to build until recently, but now when I run mvn clean package, I
get the following errors:

I would love to know whatπs going on here.

Exception in thread ""pool-1-thread-1"" Exception in thread ""main""
java.lang.ExceptionInInitializerError
java.lang.ExceptionInInitializerError
	at java.lang.J9VMInternals.ensureError(J9VMInternals.java:186)
	at java.lang.J9VMInternals.ensureError(J9VMInternals.java:186)
	at 
java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:175)
	at 
java.lang.J9VMInternals.recordInitializationFailure(J9VMInternals.java:175)

	at javax.crypto.KeyAgreement.getInstance(Unknown Source)
	at com.ibm.jsse2.lb.h(lb.java:129)
	at javax.crypto.KeyAgreement.getInstance(Unknown Source)
	at com.ibm.jsse2.lb.h(lb.java:129)
	at com.ibm.jsse2.lb.a(lb.java:165)
	at com.ibm.jsse2.l$c_.a(l$c_.java:18)
	at com.ibm.jsse2.lb.a(lb.java:165)
	at com.ibm.jsse2.l$c_.a(l$c_.java:18)	at com.ibm.jsse2.l.a(l.java:172)
	at com.ibm.jsse2.m.a(m.java:38)
	at com.ibm.jsse2.l.a(l.java:172)

	at com.ibm.jsse2.m.a(m.java:38)
	at com.ibm.jsse2.m.h(m.java:21)
	at com.ibm.jsse2.m.h(m.java:21)
	at com.ibm.jsse2.qc.a(qc.java:110)
	at com.ibm.jsse2.qc.<init>(qc.java:822)
	at com.ibm.jsse2.qc.a(qc.java:110)
	at com.ibm.jsse2.qc.<init>(qc.java:822)
	at 
com.ibm.jsse2.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:1
0)	at 
com.ibm.jsse2.SSLSocketFactoryImpl.createSocket(SSLSocketFactoryImpl.java:1
0)

	at 
org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSock
etFactory.createLayeredSocket(SSLConnectionSocketFactory.java:274)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.conn.HttpClientConnec
tionOperator.upgrade(HttpClientConnectionOperator.java:167)
	at 
org.apache.maven.wagon.providers.http.httpclient.conn.ssl.SSLConnectionSock
etFactory.createLayeredSocket(SSLConnectionSocketFactory.java:274)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.conn.HttpClientConnec
tionOperator.upgrade(HttpClientConnectionOperator.java:167)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.conn.PoolingHttpClien
tConnectionManager.upgrade(PoolingHttpClientConnectionManager.java:329)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.establishRoute(MainClientExec.java:392)	at
org.apache.maven.wagon.providers.http.httpclient.impl.conn.PoolingHttpClien
tConnectionManager.upgrade(PoolingHttpClientConnectionManager.java:329)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.establishRoute(MainClientExec.java:392)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.execute(MainClientExec.java:218)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExe
c.execute(ProtocolExec.java:194)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.MainClientE
xec.execute(MainClientExec.java:218)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.ProtocolExe
c.execute(ProtocolExec.java:194)

	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.e
xecute(RetryExec.java:85)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExe
c.execute(RedirectExec.java:108)	at
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RetryExec.e
xecute(RetryExec.java:85)

	at 
org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpCl
ient.doExecute(InternalHttpClient.java:186)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpC
lient.execute(CloseableHttpClient.java:82)
	at 
org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(Abstr
actHttpClientWagon.java:756)	at
org.apache.maven.wagon.providers.http.httpclient.impl.execchain.RedirectExe
c.execute(RedirectExec.java:108)

	at 
org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData
(AbstractHttpClientWagon.java:854)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.client.InternalHttpCl
ient.doExecute(InternalHttpClient.java:186)
	at 
org.apache.maven.wagon.providers.http.httpclient.impl.client.CloseableHttpC
lient.execute(CloseableHttpClient.java:82)
	at 
org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.execute(Abstr
actHttpClientWagon.java:756)	at
org.apache.maven.wagon.StreamWagon.getInputStream(StreamWagon.java:116)
	at org.apache.maven.wagon.StreamWagon.getIfNewer(StreamWagon.java:88)
	at org.apache.maven.wagon.StreamWagon.get(StreamWagon.java:61)
	at 
org.apache.maven.wagon.providers.http.AbstractHttpClientWagon.fillInputData
(AbstractHttpClientWagon.java:854)

	at org.apache.maven.wagon.StreamWagon.getInputStream(StreamWagon.java:116)
	at org.apache.maven.wagon.StreamWagon.getIfNewer(StreamWagon.java:88)	at
org.eclipse.aether.connector.wagon.WagonRepositoryConnector$GetTask.run(Wag
onRepositoryConnector.java:660)

	at 
org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableEr
rorForwarder.java:67)	at
org.apache.maven.wagon.StreamWagon.get(StreamWagon.java:61)
	at 
org.eclipse.aether.connector.wagon.WagonRepositoryConnector$GetTask.run(Wag
onRepositoryConnector.java:660)
	at 
org.eclipse.aether.util.concurrency.RunnableErrorForwarder$1.run(RunnableEr
rorForwarder.java:67)
	at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1
177)
	at 
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1
177)
	at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:
642)

	at java.lang.Thread.run(Thread.java:857)
Caused by: 	at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:
642)
	at java.lang.Thread.run(Thread.java:857)
Caused by: java.lang.SecurityException: Cannot set up certs for trusted CAs
java.lang.SecurityException: Cannot set up certs for trusted CAs
	at javax.crypto.b.<clinit>(Unknown Source)
	at javax.crypto.b.<clinit>(Unknown Source)
	... 30 more
Caused by: java.lang.SecurityException: Cannot locate policy or framework
files!
	... 30 more
	at javax.crypto.b.c(Unknown Source)
	at javax.crypto.b.access$600(Unknown Source)Caused by:
java.lang.SecurityException: Cannot locate policy or framework files!

	at javax.crypto.b$0.run(Unknown Source)
	at javax.crypto.b.c(Unknown Source)	at
java.security.AccessController.doPrivileged(AccessController.java:333)
	... 31 more
	at javax.crypto.b.access$600(Unknown Source)

	at javax.crypto.b$0.run(Unknown Source)
	at java.security.AccessController.doPrivileged(AccessController.java:333)
	... 31 more





________________________________________________________

The information contained in this e-mail is confidential and/or proprietary is intended only for use by the individual or entity to which it is addressed.  If the reader of this message is not the intended recipient, you are hereby notified that any review, retransmission, dissemination, distribution, copying or other use of, or taking of any action in reliance upon this information is strictly prohibited. If you have received this communication in error, please contact the sender and delete the material from your computer.


---------------------------------------------------------------------


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Sat, 3 Jan 2015 17:34:39 +0000 (UTC)",GraphX rmatGraph hangs,"""dev@spark.apache.org"" <dev@spark.apache.org>","The following single line just hangs, when executed in either Spark Shell or standalone:

org.apache.spark.graphx.util.GraphGenerators.rmatGraph(sc, 4, 8)

It just outputs ""0 edges"" and then locks up.
The only other information I've found via Google is:

http://mail-archives.apache.org/mod_mbox/spark-user/201408.mbox/%3C1408617621830-12570.post@n3.nabble.com%3E

---------------------------------------------------------------------


"
xhudik <xhudik@gmail.com>,"Sat, 3 Jan 2015 14:04:23 -0700 (MST)",Re: GraphX rmatGraph hangs,dev@spark.apache.org,"Hi Michael,

yes, I can confirm the behavior.
It get stuck (loop?) and eat all resources, command top gives:
14013 ll     20   0 2998136 489992  19804 S 100.2 12.10  13:29.39 java

You might create an issue/bug in jira
(https://issues.apache.org/jira/browse/SPARK)


best, tomas



--

---------------------------------------------------------------------


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Sun, 4 Jan 2015 12:56:13 +0000 (UTC)",Re: GraphX rmatGraph hangs,"xhudik <xhudik@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Thank you. I created 
https://issues.apache.org/jira/browse/SPARK-5064


Hi Michael,

yes, I can confirm the behavior.
It get stuck (loop?) and eat all resources, command top gives:
14013 ll     20   0 2998136 489992  19804 S 100.2 12.10  13:29.39 java

You might create an issue/bug in jira
(https://issues.apache.org/jira/browse/SPARK)


best, tomas



--


---------------------------------------------------------------------

---------------------------------------------------------------------


"
James <alcaid1801@gmail.com>,"Mon, 5 Jan 2015 10:28:04 +0800",Using graphx to calculate average distance of a big graph,"dev@spark.apache.org, user@spark.apache.org","Recently we want to use spark to calculate the average shortest path
distance between each reachable pair of nodes in a very big graph.

Is there any one ever try this? We hope to discuss about the problem.
"
Gerard Maas <gerard.maas@gmail.com>,"Mon, 5 Jan 2015 11:24:48 +0100",Re: Registering custom metrics,eshioji <eshioji@gmail.com>,"Hi,

Yes, I managed to create a register custom metrics by creating an
 implementation  of org.apache.spark.metrics.source.Source and registering
it to the metrics subsystem.
Source is [Spark] private, so you need to create it under a org.apache.spark
package. In my case, I'm dealing with Spark Streaming metrics, and I
created my CustomStreamingSource under org.apache.spark.streaming as I also
needed access to some [Streaming] private components.

Then, you register your new metric Source on the Spark's metric system,
like so:

SparkEnv.get.metricsSystem.registerSource(customStreamingSource)

And it will get reported to the metrics Sync active on your system. By
default, you can access them through the metric endpoint:
http://<driver-host>:<ui-port>/metrics/json

I hope this helps.

-kr, Gerard.







"
Gerard Maas <gerard.maas@gmail.com>,"Mon, 5 Jan 2015 14:22:47 +0100",Re: Tuning Spark Streaming jobs,Timothy Chen <tnachen@gmail.com>,"Hi Tim,

First of all, let m wish you a happy and fulfilling New Year.
Sorry for the delay in my response. I was out for the xmas break.

I've added my thoughts to the ticket from the perspective of a streaming
Job.
@TD: What do you think?

-kr, Gerard.


"
tgbaggio <gen.tang86@gmail.com>,"Mon, 5 Jan 2015 07:42:56 -0700 (MST)",python converter in HBaseConverter.scala(spark/examples),dev@spark.apache.org,"Hi, 

In  HBaseConverter.scala
<https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/pythonconverters/HBaseConverters.scala> 
, the python converter HBaseResultToStringConverter return only the value of
first column in the result. In my opinion, it limits the utility of this
converter, because it returns only one value per row and moreover it loses
the other information of record, such as column:cell, timestamp. 

Therefore, I would like to propose some modifications about
HBaseResultToStringConverter which will be able to return all records in the
hbase with more complete information: I have already written some code in
pythonConverters.scala
<https://github.com/GenTang/spark_hbase/blob/master/src/main/scala/examples/pythonConverters.scala>  
and it works

Is it OK to modify the code in HBaseConverters.scala, please?
Thanks a lot in advance.

Cheers
Gen




--

---------------------------------------------------------------------


"
Enno Shioji <eshioji@gmail.com>,"Mon, 5 Jan 2015 14:56:02 +0000",Re: Registering custom metrics,Gerard Maas <gerard.maas@gmail.com>,"Hi Gerard,

Thanks for the answer! I had a good look at it, but I couldn't figure out
whether one can use that to emit metrics from your application code.

Suppose I wanted to monitor the rate of bytes I produce, like so:

    stream
        .map { input =>
          val bytes = produce(input)
          // metricRegistry.meter(""some.metrics"").mark(bytes.length)
          bytes
        }
        .saveAsTextFile(""text"")

Is there a way to achieve this with the MetricSystem?


·êß


ark
d
stom-metrics-tp9030p9968.html
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 5 Jan 2015 07:36:37 -0800",Re: python converter in HBaseConverter.scala(spark/examples),tgbaggio <gen.tang86@gmail.com>,"In my opinion this would be useful - there was another thread where returning
only the value of first column in the result was mentioned.

Please create a SPARK JIRA and a pull request.

Cheers


"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Mon, 05 Jan 2015 07:54:39 -0800 (PST)",Re: python converter in HBaseConverter.scala(spark/examples),"""Ted Yu"" <yuzhihong@gmail.com>","Hey¬†


These converters are actually just intended to be examples of how to set up a custom converter for a specific input format. The converter interface is there to provide flexibility where needed, although with the new SparkSQL data store interface the intention is that most common use cases can be handled using that approach rather than custom converters.




The intention is not to have specific converters living in Spark core, which is why these are in the examples project.




Having said that, if you wish to expand the example converter for others reference do feel free to submit a PR.




Ideally though, I would think that various custom converters would be part of external projects that can be listed with¬†http://spark-packages.org/¬†I see your project is already listed there.


‚Äî
Sent from Mailbox


returning
apache/spark/examples/pythonconverters/HBaseConverters.scala
value
this
loses
in
in
ples/pythonConverters.scala
com/python-converter-in-HBaseConverter-scala-spark-examples-tp10001.html"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Mon, 5 Jan 2015 16:22:50 +0000",RE: python converter in HBaseConverter.scala(spark/examples),"Ted Yu <yuzhihong@gmail.com>, tgbaggio <gen.tang86@gmail.com>","We are planning to support HBase as a ""native"" data source to Spark SQL in 1.3 (SPARK-3880). 
More details will come soon.


-----Original Message-----
From: Ted Yu [015 7:37 AM
To: tgbaggio
Cc: dev@spark.apache.org
Subject: Re: python converter in HBaseConverter.scala(spark/examples)

In my opinion this would be useful - there was another thread where returning only the value of first column in the result was mentioned.

Please create a SPARK JIRA and a pull request.

Cheers

On Mon, Jan 5, 2015 at 6:42 AM, tgbaggio <gen.tang86@gmail.com> wrote:

> Hi,
>
> In  HBaseConverter.scala
> <
> https://github.com/apache/spark/blob/master/examples/src/main/scala/or
> g/apache/spark/examples/pythonconverters/HBaseConverters.scala
> >
> , the python converter HBaseResultToStringConverter return only the 
> value of first column in the result. In my opinion, it limits the 
> utility of this converter, because it returns only one value per row 
> and moreover it loses the other information of record, such as 
> column:cell, timestamp.
>
> Therefore, I would like to propose some modifications about 
> HBaseResultToStringConverter which will be able to return all records 
> in the hbase with more complete information: I have already written 
> some code in pythonConverters.scala < 
> https://github.com/GenTang/spark_hbase/blob/master/src/main/scala/exam
> ples/pythonConverters.scala
> >
> and it works
>
> Is it OK to modify the code in HBaseConverters.scala, please?
> Thanks a lot in advance.
>
> Cheers
> Gen
>
>
>
>
> --
> View this message in context:
> http://apache-spark-developers-list.1001551.n3.nabble.com/python-conve
> rter-in-HBaseConverter-scala-spark-examples-tp10001.html
> Sent from the Apache Spark Developers List mailing list archive at 
> Nabble.com.
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> additional commands, e-mail: dev-help@spark.apache.org
>
>
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 5 Jan 2015 09:06:53 -0800",Re: python converter in HBaseConverter.scala(spark/examples),Nick Pentreath <nick.pentreath@gmail.com>,"HBaseConverter is in Spark source tree. Therefore I think it makes sense
for this improvement to be accepted so that the example is more useful.

Cheers


e
t
apache/spark/examples/pythonconverters/HBaseConverters.scala
es/pythonConverters.scala
er-in-HBaseConverter-scala-spark-examples-tp10001.html
"
"""Nick Pentreath"" <nick.pentreath@gmail.com>","Mon, 05 Jan 2015 10:03:58 -0800 (PST)",Re: python converter in HBaseConverter.scala(spark/examples),"""Ted Yu"" <yuzhihong@gmail.com>","Absolutely; as I mentioned by all means submit a PR - I just wanted to point out that any specific converter is not ""officially"" supported, although the interface is of course.


I'm happy to review a PR just ping me when ready.


‚Äî
Sent from Mailbox


com>
set
interface
cases
others
part
I
/apache/spark/examples/pythonconverters/HBaseConverters.scala
records
code
mples/pythonConverters.scala
com/python-converter-in-HBaseConverter-scala-spark-examples-tp10001.html
"
shane knapp <sknapp@berkeley.edu>,"Mon, 5 Jan 2015 10:37:15 -0800","jenkins redirect down (but jenkins is up!), lots of potential","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","UC Berkeley had some major maintenance done this past weekend, and long
story short, not everything came back.  our primary webserver's NFS is down
and that means we're not serving websites, meaning that the redirect to
jenkins is failing.

jenkins is still up, and building some jobs, but we will probably see pull
request builder failures, and other transient issues.  SCM-polling builds
should be fine.

there is no ETA on when this will be fixed, but once our
amplab.cs.berkeley.edu/jenkins redir is working, i will let everyone know.
 i'm trying to get more status updates as they come.

i'm really sorry about the inconvenience.

shane
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 5 Jan 2015 11:36:31 -0800",Fwd: ApacheCon North America 2015 Call For Papers,"user <user@spark.apache.org>,
 dev <dev@spark.apache.org>","FYI, ApacheCon North America call for papers is up.

Matei

ApacheCon North America 2015, and so far the submissions are on the paltry side. Please consider submitting papers for consideration for this event.
http://events.linuxfoundation.org/events/apachecon-north-america
http://events.linuxfoundation.org//events/apachecon-north-america/program/cfp
community on the projects that you're involved in, so that these projects can be represented in Austin.
task of wrangling your community together to create a compelling story about your technology space, please join the comdev mailing list - dev-subscribe@community.apache.org - and speak up there.
you want to discuss this topic further there.)

"
Olivier Toupin <olivier.toupin@gmail.com>,"Mon, 5 Jan 2015 13:25:12 -0700 (MST)",Spark UI history job duration is wrong,dev@spark.apache.org,"Hello,

I'm using Spark 1.2.0 and when running an application, if I go into the UI
and then in the job tab (""/jobs/"") the jobs duration are relevant and the
posted durations looks ok.

However when I open the history (""history/app-<xyz>/jobs/"") for that job,
the duration are wrong showing milliseconds instead of the relevant job
time. The submitted time for each job (except maybe the first) is different
also.

The stage tab is unaffected and show the correct duration for each stages in
both mode.

Should I open a bug?



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 5 Jan 2015 12:37:17 -0800",Re: Spark UI history job duration is wrong,Olivier Toupin <olivier.toupin@gmail.com>,"Thanks for reporting this - it definitely sounds like a bug. Please
open a JIRA for it. My guess is that we define the start or end time
of the job based on the current time instead of looking at data
encoded in the underlying event stream. That would cause it to not
work properly when loading from historical data.

- Patrick


---------------------------------------------------------------------


"
Tathagata Das <tathagata.das1565@gmail.com>,"Mon, 5 Jan 2015 13:57:14 -0800",Re: Spark Streaming Data flow graph,francois.garillot@typesafe.com,"Hey Fran√ßois,

Well, at a high-level here is what I thought about the diagram.

- ReceiverSupervisor handles only one Receiver.
- BlockGenerator is part of ReceiverSupervisor not ReceivedBlockHandler
- The blocks are inserted in BlockManager and if activated,
WriteAheadLogManager in parallel, not through BlockManager as the
diagram seems to imply
- It would be good to have a clean visual separation of what runs in
Executor (better term than Worker) and what is in Driver ... Driver
stuff on left and Executor stuff on right, or vice versa.

More importantly, the word of caution is that all the internal stuff
like ReceiverBlockHandler, Supervisor, etc are subject to change any
time as we keep refactoring stuff. So highlighting these internal
details too much too publicly may lead to future confusion.

TD

gatadassparkmeetup20130617/26
f comments still mention NetworkReceiver).
Ä¶). Data flow up to right before RDD creation is in bold arrows, metadata flow is in normal width arrows.
 share it to ask:
st below) ?
 for it.

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Mon, 5 Jan 2015 14:25:29 -0800","Re: jenkins redirect down (but jenkins is up!), lots of potential",shane knapp <sknapp@berkeley.edu>,"The pull request builder and SCM-polling builds appear to be working fine,
but the links in pull request comments won't work because the AMP Lab
webserver is still down.  In the meantime, though, you can continue to
access Jenkins through https://hadrian.ist.berkeley.edu/jenkins/


"
Andrew Ash <andrew@andrewash.com>,"Mon, 5 Jan 2015 15:40:50 -0800",Maintainer for Mesos,dev <dev@spark.apache.org>,"Hi Spark devs,

I'm interested in having a committer look at a PR [1] for Mesos, but
there's not an entry for Mesos in the maintainers specialties on the wiki
[2].  Which Spark committers have expertise in the Mesos features?

Thanks!
Andrew


[1] https://github.com/apache/spark/pull/3074
[2]
https://cwiki.apache.org/confluence/display/SPARK/Committers#Committers-ReviewProcessandMaintainers
"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 6 Jan 2015 14:08:12 +0530",Guava 11 dependency issue in Spark 1.2.0,"dev@spark.apache.org, user@spark.apache.org","Hi,

I have been running a simple Spark app on a local spark cluster and I came
across this error.

Exception in thread ""main"" java.lang.NoSuchMethodError:
com.google.common.hash.HashFunction.hashInt(I)Lcom/google/common/hash/HashCode;
    at org.apache.spark.util.collection.OpenHashSet.org
$apache$spark$util$collection$OpenHashSet$$hashcode(OpenHashSet.scala:261)
    at
org.apache.spark.util.collection.OpenHashSet$mcI$sp.getPos$mcI$sp(OpenHashSet.scala:165)
    at
org.apache.spark.util.collection.OpenHashSet$mcI$sp.contains$mcI$sp(OpenHashSet.scala:102)
    at
org.apache.spark.util.SizeEstimator$$anonfun$visitArray$2.apply$mcVI$sp(SizeEstimator.scala:214)
    at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
    at
org.apache.spark.util.SizeEstimator$.visitArray(SizeEstimator.scala:210)
    at
org.apache.spark.util.SizeEstimator$.visitSingleObject(SizeEstimator.scala:169)
    at
org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(SizeEstimator.scala:161)
    at
org.apache.spark.util.SizeEstimator$.estimate(SizeEstimator.scala:155)
    at
org.apache.spark.util.collection.SizeTracker$class.takeSample(SizeTracker.scala:78)
    at
org.apache.spark.util.collection.SizeTracker$class.afterUpdate(SizeTracker.scala:70)
    at
org.apache.spark.util.collection.SizeTrackingVector.$plus$eq(SizeTrackingVector.scala:31)
    at
org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:249)
    at
org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:136)
    at
org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:114)
    at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:787)
    at
org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
    at
org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:992)
    at
org.apache.spark.broadcast.TorrentBroadcast.writeBlocks(TorrentBroadcast.scala:98)
    at
org.apache.spark.broadcast.TorrentBroadcast.<init>(TorrentBroadcast.scala:84)
    at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
    at
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:29)
    at
org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:62)
    at org.apache.spark.SparkContext.broadcast(SparkContext.scala:945)
    at org.apache.spark.SparkContext.hadoopFile(SparkContext.scala:695)
    at
com.databricks.spark.avro.AvroRelation.buildScan$lzycompute(AvroRelation.scala:45)
    at
com.databricks.spark.avro.AvroRelation.buildScan(AvroRelation.scala:44)
    at
org.apache.spark.sql.sources.DataSourceStrategy$.apply(DataSourceStrategy.scala:56)
    at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
    at
org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:58)
    at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
    at
org.apache.spark.sql.catalyst.planning.QueryPlanner.apply(QueryPlanner.scala:59)
    at
org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan$lzycompute(SQLContext.scala:418)
    at
org.apache.spark.sql.SQLContext$QueryExecution.sparkPlan(SQLContext.scala:416)
    at
org.apache.spark.sql.SQLContext$QueryExecution.executedPlan$lzycompute(SQLContext.scala:422)
    at
org.apache.spark.sql.SQLContext$QueryExecution.executedPlan(SQLContext.scala:422)
    at org.apache.spark.sql.SchemaRDD.collect(SchemaRDD.scala:444)
    at
org.apache.spark.sql.api.java.JavaSchemaRDD.collect(JavaSchemaRDD.scala:114)


While looking into this I found out that Guava was downgraded to version 11
in this PR.
https://github.com/apache/spark/pull/1610

In this PR OpenHashSet.scala:261 line hashInt has been changed to hashLong.
But when I actually run my app,  ""java.lang.NoSuchMethodError:
com.google.common.hash.HashFunction.hashInt"" error occurs,
which is understandable because hashInt is not available before Guava 12.

So, I''m wondering why this occurs?

Cheers
-- 
Niranda Perera
"
Yi Tian <tianyi.asiainfo@gmail.com>,"Tue, 06 Jan 2015 17:25:26 +0800",[SPARK-5100][SQL] Spark Thrift server monitor page,dev@spark.apache.org,"Hi, all

I have create a JIRA ticket about adding a monitor page for Thrift server.

https://issues.apache.org/jira/browse/SPARK-5100

Anyone could review the design doc, and give some advises?

---------------------------------------------------------------------


"
=?UTF-8?Q?Fran=C3=A7ois_Garillot?= <francois.garillot@typesafe.com>,"Tue, 6 Jan 2015 16:14:08 +0100",Re: Spark Streaming Data flow graph,Tathagata Das <tathagata.das1565@gmail.com>,"Thanks a LOT for your answer ! I've updated the diagram, at the same
address :
https://www.dropbox.com/s/q79taoce2ywdmf1/SparkStreaming.pdf?dl=0

I've addressed your more straightforward remarks directly in the diagram. A
couple questions:

- the location of instances (Executor, Master, Driver) is now marked, I
hope I didn't make too many mistakes there, did I ?

- Given that the communication between instances and their members (e.g.
ReceiverSupervisor / ReceivedBlockHandler) is willingly omitted, have I
forgotten any communication channels ?

- I've represented some queues / buffers using a red trapezoid. I'm thus
starting an inventory of queues or buffers, and I'm interested in adding
the 'implicit' ones as well (e.g. jobSets in JobScheduler, which is indexed
by time in ms). I'd be happy with pointers on where to look : ideally I'm
trying to see any place in the data flow where data is sitting idle for any
length of time, waiting to be chunked somehow (whether it's at the RDD or
block level doesn't really matter to me, I'm interested in all types of
'chunking').

Naturally, this is intended to be a developer document exclusively (hence
in particular why I'm not publicising this on the user ML).



:
gatadassparkmeetup20130617/26
 of
Ä¶).
w
ks for it.
o



-- 
Fran√ßois Garillot
"
shane knapp <sknapp@berkeley.edu>,"Tue, 6 Jan 2015 09:25:43 -0800","Re: jenkins redirect down (but jenkins is up!), lots of potential",Josh Rosen <rosenville@gmail.com>,"the regular url is working now, thanks for your patience.


"
Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"Wed, 7 Jan 2015 09:16:48 +0530",Reading Data  Using TextFileStream,"""Hari Shreedharan (hshreedharan@cloudera.com)"" <hshreedharan@cloudera.com>","
Hi Hari,

Iam trying to read data from a file which is stored in HDFS. Using Flume the data is tailed and stored in HDFS.
Now I want to read this data using TextFileStream. Using the below mentioned code Iam not able to fetch the
Data  from a file which is stored in HDFS. Can anyone help me with this issue.

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;

import com.google.common.collect.Lists;

import java.util.Arrays;
import java.util.List;
import java.util.regex.Pattern;

public final class Test1 {
  public static void main(String[] args) throws Exception {

    SparkConf sparkConf = new SparkConf().setAppName(""JavaWordCount"");
    JavaStreamingContext ssc = new JavaStreamingContext(""local[4]"",""JavaWordCount"",  new Duration(20000));

    JavaDStream<String> textStream = ssc.textFileStream(""user/huser/user/huser/flume"");//Data Directory Path in HDFS


    JavaDStream<String> suspectedStream = textStream.flatMap(new FlatMapFunction<String,String>()
     {
                            public Iterable<String> call(String line) throws Exception {

                            //return Arrays.asList(line.toString().toString());
                           return  Lists.newArrayList(line.toString().toString());
                             }
     });


    suspectedStream.foreach(new Function<JavaRDD<String>,Void>(){

        public Void call(JavaRDD<String> rdd) throws Exception {
        List<String> output = rdd.collect();
        System.out.println(""Sentences Collected from Flume "" + output);
               return  null;
        }
        });

    suspectedStream.print();

    System.out.println(""Welcome TO Flume Streaming"");
    ssc.start();
    ssc.awaitTermination();
  }

}

The command I use is:
./bin/spark-submit --verbose --jars lib/spark-examples-1.1.0-hadoop1.0.4.jar,lib/mysql.jar --master local[*] --deploy-mode client --class xyz.Test1 bin/filestream3.jar





Regards,
Jeniba Johnson


________________________________
The contents of this e-mail and any attachment(s) may contain confidential or privileged information for the intended recipient(s). Unintended recipients are prohibited from taking action on the basis of information in this e-mail and using or disseminating the information, and must notify the sender and delete it from their system. L&T Infotech will not accept responsibility or liability for the accuracy or completeness of, or the presence of any virus or disabling code in this e-mail""
"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 07 Jan 2015 13:57:08 +0800",Re: [SPARK-5100][SQL] Spark Thrift server monitor page,"Yi Tian <tianyi.asiainfo@gmail.com>, dev@spark.apache.org","Talked with Yi offline, personally I think this feature is pretty 
useful, and the design makes sense, and he's already got a running 
prototype.

Yi, would you mind to open a PR for this? Thanks!

Cheng



---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Wed, 7 Jan 2015 12:11:18 +0530",Re: Reading Data Using TextFileStream,Jeniba Johnson <Jeniba.Johnson@lntinfotech.com>,"I think you need to start your streaming job, then put the files there to
get them read. textFileStream doesn't read the existing files i believe.

Also are you sure the path is not the following? (no missing / in the
beginning?)

JavaDStream<String> textStream = ssc.textFileStream(""/user/
huser/user/huser/flume"");


Thanks
Best Regards


"
Jong Wook Kim <jongwook@nyu.edu>,"Wed, 7 Jan 2015 01:27:32 -0700 (MST)",Re: spark-yarn_2.10 1.2.0 artifacts,dev@spark.apache.org,"I don't think that's the case. 

spark-yarn contains `org.apache.spark.deploy.yarn` package, whereas
spark-network-yarn contains `org.apache.spark.network.yarn`, and they do
different things.

The former contains codes for deploying Spark applications to YARN cluster,
and called when running `spark-submit --master yarn...` and the latter
contains a shuffle service that runs in NM process.

While spark-yarn is not a programming interface and normally used only by
Spark toolchain, I need it as a library dependency, for programatically
launching spark application on YARN. 

spark-yarn.jar does appear in '/yarn/stable/target` when building Spark
1.2.0 from source code and I'm manually adding that to my maven repository.
But I'd like to know why it is excluded from the maven repo.



--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 7 Jan 2015 00:31:49 -0800",Re: Hang on Executor classloader lookup for the remote REPL URL classloader,Andrew Ash <andrew@andrewash.com>,"Hey Andrew,

So the executors in Spark will fetch classes from the driver node for
classes defined in the repl from an HTTP server on the driver. Is this
happening in the context of a repl session? Also, is it deterministic
or does it happen only periodically?

The reason all of the other threads are hanging is that there is a
global lock around classloading, so they all queue up.

Could you attach the full stack trace from the driver? Is it possible
that something in the network is blocking the transfer of bytes
between these two processes? Based on the stack trace it looks like it
sent an HTTP request and is waiting on the result back from the
driver.

used for the repl class server is still alive from the vantage point
of both the executor and driver nodes. Another thing to try would be
to temporarily open up any firewalls that are on the nodes or in the
network and see if this makes the problem go away (to isolate it to an
exogenous-to-Spark network issue).

- Patrick


---------------------------------------------------------------------


"
gen tang <gen.tang86@gmail.com>,"Wed, 7 Jan 2015 16:14:10 +0100",Spark on teradata?,"user@spark.apache.org, dev@spark.apache.org","Hi,

I have a stupid question:
Is it possible to use spark on Teradata data warehouse, please? I read some
news on internet which say yes. However, I didn't find any example about
this issue

Thanks in advance.

Cheers
Gen
"
Alessandro Baretta <alexbaretta@gmail.com>,"Wed, 7 Jan 2015 15:04:38 -0800",Missing Catalyst API docs,"""dev@spark.apache.org"" <dev@spark.apache.org>","Gents,

It looks like some of the Catalyst classes' API docs are missing. For
instance, the Expression class, referred to by the SchemaRDD docs seems to
be missing. (See here:
http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SchemaRDD
)

Is this intended or is it due to a failure in the doc creation process?

Alex
"
Reynold Xin <rxin@databricks.com>,"Wed, 7 Jan 2015 15:13:24 -0800",Re: Missing Catalyst API docs,Alessandro Baretta <alexbaretta@gmail.com>,"I'm in the middle of revamping the SchemaRDD public API and in 1.3, we will
have a public, documented version of the expression library. The Catalyst
expression library will remain hidden.

You can track it with this ticket:
https://issues.apache.org/jira/browse/SPARK-5097




"
=?UTF-8?B?5pu56Zuq5p6X?= <xuelincao2014@gmail.com>,"Thu, 8 Jan 2015 10:01:42 +0800","Fwd: When will spark support ""push"" style shuffle?",dev@spark.apache.org,"Hi,

      I've heard a lot of complain about spark's ""pull"" style shuffle. Is
there any plan to support ""push"" style shuffle in the near future?

      Currently, the shuffle phase must be completed before the next stage
starts. While, it is said, in Impala, the shuffled data is ""streamed"" to
the next stage handler, which greatly saves time. Will spark support this
mechanism one day?

Thanks
"
Patrick Wendell <pwendell@gmail.com>,"Wed, 7 Jan 2015 21:06:03 -0800","Re: When will spark support ""push"" style shuffle?",=?UTF-8?B?5pu56Zuq5p6X?= <xuelincao2014@gmail.com>,"This question is conflating a few different concepts. I think the main
question is whether Spark will have a shuffle implementation that
streams data rather than persisting it to disk/cache as a buffer.
Spark currently decouples the shuffle write from the read using
disk/OS cache as a buffer. The two benefits of this approach this are
that it allows intra-query fault tolerance and it makes it easier to
elastically scale and reschedule work within a job. We consider these
to be design requirements (think about jobs that run for several hours
on hundreds of machines). Impala, and similar systems like dremel and
f1, not offer fault tolerance within a query at present. They also
require gang scheduling the entire set of resources that will exist
for the duration of a query.

A secondary question is whether our shuffle should have a barrier or
not. Spark's shuffle currently has a hard barrier between map and
reduce stages. We haven't seen really strong evidence that removing
the barrier is a net win. It can help the performance of a single job
(modestly), but in the a multi-tenant workload, it leads to poor
utilization since you have a lot of reduce tasks that are taking up
slots waiting for mappers to finish. Many large scale users of
Map/Reduce disable this feature in production clusters for that
reason. Thus, we haven't seen compelling evidence for removing the
barrier at this point, given the complexity of doing so.

It is possible that future versions of Spark will support push-based
shuffles, potentially in a mode that remove some of Spark's fault
tolerance properties. But there are many other things we can still
optimize about the shuffle that would likely come before this.

- Patrick

e

---------------------------------------------------------------------


"
Indu Chaube <prakash.indu@gmail.com>,"Wed, 7 Jan 2015 21:20:53 -0800",Need Help to display output of the the command on UI,"user@spark.apache.org, dev@spark.apache.org","Hi Folk,
I need to print output of the below command on Web UI

 val conf=new SparkConf().setMaster(""local"")
    val sc=new SparkContext(conf)
    val file1=sc.textFile(""/var/log/dpkg.log"")

    //Applying filter onto the data
    val data1=file1.filter(line => line.contains(""installed""))
    <b style=""color:red"">{data1.count() } </b>

But it is printing on console. Any Idea how to do it. That will be great
help.


Thanks,
Indu
"
"""Xuelin Cao.2015"" <xuelincao2014@gmail.com>","Wed, 7 Jan 2015 23:25:43 -0700 (MST)","Re: When will spark support ""push"" style shuffle?",dev@spark.apache.org,"Got it. The explain makes sense. Thank you.



l]
o
park-support-push-style-shuffle-tp10028p10029.html
ervlet.jtp?macro=unsubscribe_by_code&node=1&code=eHVlbGluY2FvMjAxNEBnbWFpbC5jb218MXwtOTc3NDY2MzAy>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/Fwd-When-will-spark-support-push-style-shuffle-tp10028p10031.html
om."
"""Jakub Dubovsky"" <spark.dubovsky.jakub@seznam.cz>","Thu, 08 Jan 2015 11:00:08 +0100 (CET)",Spark development with IntelliJ,<dev@spark.apache.org>,"Hi devs,

¬† I'd like to ask if anybody has experience with using intellij 14 to step 
into spark code. Whatever I try I get compilation error:

Error:scalac: bad option: -P:/home/jakub/.m2/repository/org/scalamacros/
paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar

¬† Project is set up by Patrick's instruction [1] and packaged by mvn -
DskipTests clean install. Compilation works fine. Then I just created 
breakpoint in test code and run debug with the error.

¬† Thanks for any hints

¬† Jakub

[1] https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+
Tools#UsefulDeveloperTools-BuildingSparkinIntelliJIDEA
"
Petar Zecevic <petar.zecevic@gmail.com>,"Thu, 08 Jan 2015 11:12:19 +0100",Re: Spark development with IntelliJ,dev@spark.apache.org,"
This helped me:

http://stackoverflow.com/questions/26995023/errorscalac-bad-option-p-intellij-idea




---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 8 Jan 2015 10:26:56 +0000",Re: Spark development with IntelliJ,Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>,"Yeah, I hit this too. IntelliJ picks this up from the build but then
it can't run its own scalac with this plugin added.

Go to Preferences > Build, Execution, Deployment > Scala Compiler and
clear the ""Additional compiler options"" field. It will work then
although the option will come back when the project reimports.

Right now I don't know of a better fix.

There's another recent open question about updating IntelliJ docs:
https://issues.apache.org/jira/browse/SPARK-5136  Should this stuff go
in the site docs, or wiki? I vote for wiki I suppose and make the site
docs point to the wiki. I'd be happy to make wiki edits if I can get
permission, or propose this text along with other new text on the
JIRA.


---------------------------------------------------------------------


"
"""Jakub Dubovsky"" <spark.dubovsky.jakub@seznam.cz>","Thu, 08 Jan 2015 12:33:32 +0100 (CET)",Re: Spark development with IntelliJ,"""Sean Owen"" <sowen@cloudera.com>","Thanks that helped.

I vote for wiki as well. More fine graned documentation should be on wiki 
and linked,

Jakub


---------- P≈Øvodn√≠ zpr√°va ----------
Od: Sean Owen <sowen@cloudera.com>
Komu: Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>
Datum: 8. 1. 2015 11:29:22
P≈ôedmƒõt: Re: Spark development with IntelliJ

""Yeah, I hit this too. IntelliJ picks this up from the build but then
it can't run its own scalac with this plugin added.

Go to Preferences > Build, Execution, Deployment > Scala Compiler and
clear the ""Additional compiler options"" field. It will work then
although the option will come back when the project reimports.

Right now I don't know of a better fix.

There's another recent open question about updating IntelliJ docs:
https://issues.apache.org/jira/browse/SPARK-5136 Should this stuff go
in the site docs, or wiki? I vote for wiki I suppose and make the site
docs point to the wiki. I'd be happy to make wiki edits if I can get
permission, or propose this text along with other new text on the
JIRA.







---------------------------------------------------------------------
"
Tony Reix <tony.reix@bull.net>,"Thu, 8 Jan 2015 13:40:49 +0000",Results of tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I'm checking that Spark works fine on a new environment (PPC64 hardware).
I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can find the results of the tests of Spark, for each version and for the different versions, in order to have a reference to compare my results with. I cannot find them on Spark web-site.
Thx
Tony

"
Ted Yu <yuzhihong@gmail.com>,"Thu, 8 Jan 2015 07:11:54 -0800",Re: Results of tests,Tony Reix <tony.reix@bull.net>,"Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark/


"
Devl Devel <devl.development@gmail.com>,"Thu, 8 Jan 2015 15:58:39 +0000",K-Means And Class Tags,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

I'm trying a simple K-Means example as per the website:

val parsedData = data.map(s => Vectors.dense(s.split(',').map(_.toDouble)))

but I'm trying to write a Java based validation method first so that
missing values are omitted or replaced with 0.

public RDD<Vector> prepareKMeans(JavaRDD<String> data) {
        JavaRDD<Vector> words = data.flatMap(new FlatMapFunction<String,
Vector>() {
            public Iterable<Vector> call(String s) {
                String[] split = s.split("","");
                ArrayList<Vector> add = new ArrayList<Vector>();
                if (split.length != 2) {
                    add.add(Vectors.dense(0, 0));
                } else
                {
                    add.add(Vectors.dense(Double.parseDouble(split[0]),
               Double.parseDouble(split[1])));
                }

                return add;
            }
        });

        return words.rdd();
}

When I then call from scala:

val parsedData=dc.prepareKMeans(data);
val p=parsedData.collect();

I get Exception in thread ""main"" java.lang.ClassCastException:
[Ljava.lang.Object; cannot be cast to
[Lorg.apache.spark.mllib.linalg.Vector;

Why is the class tag is object rather than vector?

1) How do I get this working correctly using the Java validation example
above or
2) How can I modify val parsedData = data.map(s =>
Vectors.dense(s.split(',').map(_.toDouble))) so that when s.split size <2 I
ignore the line? or
3) Is there a better way to do input validation first?

Using spark and mlib:
libraryDependencies += ""org.apache.spark"" % ""spark-core_2.10"" %  ""1.2.0""
libraryDependencies += ""org.apache.spark"" % ""spark-mllib_2.10"" % ""1.2.0""

Many thanks in advance
Dev
"
Tony Reix <tony.reix@bull.net>,"Thu, 8 Jan 2015 16:05:15 +0000",RE:Results of tests,Ted Yu <yuzhihong@gmail.com>,"Thanks !

I've been able to see that there are 3745 tests for version 1.2.0 with profile Hadoop 2.4  .
However, on my side, the maximum tests I've seen are 3485... About 300 tests are missing on my side.
Which Maven option has been used for producing the report file used for building the page:
     https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
  ? (I'm not authorized to look at the ""configuration"" part)

Thx !

Tony

________________________________
De : Ted Yu [yuzhihong@gmail.com]
EnvoyÈ : jeudi 8 janvier 2015 16:11
¿ : Tony Reix
Cc : dev@spark.apache.org
Objet : Re: Results of tests

Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark/

Hi,
I'm checking that Spark works fine on a new environment (PPC64 hardware).
I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can find the results of the tests of Spark, for each version and for the different versions, in order to have a reference to compare my results with. I cannot find them on Spark web-site.
Thx
Tony


"
Yana Kadiyska <yana.kadiyska@gmail.com>,"Thu, 8 Jan 2015 11:13:22 -0500",Re: K-Means And Class Tags,Devl Devel <devl.development@gmail.com>,"How about

data.map(s=>s.split("","")).filter(_.length>1).map(good_entry=>Vectors.dense((Double.parseDouble(good_entry[0]),
Double.parseDouble(good_entry[1]))
‚Äã
(full disclosure, I didn't actually run this). But after the first map you
should have an RDD[Array[String]], then you'd discard everything shorter
than 2, and convert the rest to dense vectors?...In fact if you're
expecting length exactly 2 might want to filter ==2...



ble)))
,
 I
""
""
"
Enno Shioji <eshioji@gmail.com>,"Thu, 8 Jan 2015 16:30:21 +0000",Re: Registering custom metrics,Gerard Maas <gerard.maas@gmail.com>,"FYI I found this approach by Ooyala.

/** Instrumentation for Spark based on accumulators.
  *
  * Usage:
  * val instrumentation = new SparkInstrumentation(""example.metrics"")
  * val numReqs = sc.accumulator(0L)
  * instrumentation.source.registerDailyAccumulator(numReqs, ""numReqs"")
  * instrumentation.register()
  *
  * Will create and report the following metrics:
  * - Gauge with total number of requests (daily)
  * - Meter with rate of requests
  *
  * @param prefix prefix for all metrics that will be reported by this
Instrumentation
  */

https://gist.github.com/ibuenros/9b94736c2bad2f4b8e23
·êß


park
ustom-metrics-tp9030p9968.html
"
xhudik <xhudik@gmail.com>,"Thu, 8 Jan 2015 09:38:42 -0700 (MST)",Re: Spark on teradata?,dev@spark.apache.org,"I don't think this makes sense. TD database is standard RDBMS (even parallel)
while Spark is used for non-relational issues. 
What could make sense is to deploy Spark on Teradata Aster. Aster is a
database cluster that might call external programs via STREAM operator. 
That said Spark/Scala app can be can be called and process some data. The
deployment itself should be easy the potential benefit - hard to say...


hope this helps, Tomas



--

---------------------------------------------------------------------


"
Gerard Maas <gerard.maas@gmail.com>,"Thu, 8 Jan 2015 17:42:54 +0100",Re: Registering custom metrics,Enno Shioji <eshioji@gmail.com>,"Very interesting approach. Thanks for sharing it!


strumentation
t
spark
custom-metrics-tp9030p9968.html
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 8 Jan 2015 08:43:40 -0800",Re: Results of tests,Tony Reix <tony.reix@bull.net>,"Here it is:

[centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/mvn
-DHADOOP_PROFILE=hadoop-2.4 -Dlabel=centos -DskipTests -Phadoop-2.4
-Pyarn -Phive clean package


You can find the above in
https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull


Cheers



h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
/
.
n
"
RJ Nowling <rnowling@gmail.com>,"Thu, 8 Jan 2015 12:46:17 -0500",Re: Maintainer for Mesos,Andrew Ash <andrew@andrewash.com>,"Hi Andrew,

Patrick Wendell and Andrew Or have committed previous patches related to
Mesos. Maybe they would be good committers to look at it?

RJ


"
Reynold Xin <rxin@databricks.com>,"Thu, 8 Jan 2015 10:39:04 -0800",Re: Spark on teradata?,gen tang <gen.tang86@gmail.com>,"Depending on your use cases. If the use case is to extract small amount of
data out of teradata, then you can use the JdbcRDD and soon a jdbc input
source based on the new Spark SQL external data source API.




"
"""devl.development"" <devl.development@gmail.com>","Thu, 8 Jan 2015 13:47:56 -0700 (MST)",Re: K-Means And Class Tags,dev@spark.apache.org,"Thanks for the suggestion, can anyone offer any advice on the ClassCast
Exception going from Java to Scala? Why does going from JavaRDD.rdd() and
then a collect() result in this exception?



--

---------------------------------------------------------------------


"
Devl Devel <devl.development@gmail.com>,"Thu, 8 Jan 2015 20:48:48 +0000",Re: K-Means And Class Tags,yana.kadiyska@gmail.com,"Thanks for the suggestion, can anyone offer any advice on the ClassCast
Exception going from Java to Scala? Why does JavaRDD.rdd() and then a
collect() result in this exception?


dense((Double.parseDouble(good_entry[0]),
u
g,
2
0""
0""
"
Joseph Bradley <joseph@databricks.com>,"Thu, 8 Jan 2015 13:35:47 -0800",Re: K-Means And Class Tags,Devl Devel <devl.development@gmail.com>,"I believe you're running into an erasure issue which we found in
DecisionTree too.  Check out:
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/tree/RandomForest.scala#L134

That retags RDDs which were created from Java to prevent the exception
you're running into.

Hope this helps!
Joseph


dense((Double.parseDouble(good_entry[0]),
r
ing,
,
le
2.0""
2.0""
"
Bill Bejeck <bbejeck@gmail.com>,"Thu, 8 Jan 2015 17:38:30 -0500",Re: Spark development with IntelliJ,Jakub Dubovsky <spark.dubovsky.jakub@seznam.cz>,"I was having the same issue and that helped.  But now I get the following
compilation error when trying to run a test from within Intellij (v 14)

/Users/bbejeck/dev/github_clones/bbejeck-spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala
Error:(308, 109) polymorphic expression cannot be instantiated to expected
type;
 found   : [T(in method
apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method apply)]
 required: org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in
method functionToUdfBuilder)]
  implicit def functionToUdfBuilder[T: TypeTag](func: Function1[_, T]):
ScalaUdfBuilder[T] = ScalaUdfBuilder(func)

Any thoughts?

                                ^


p
/
"
Bill Bejeck <bbejeck@gmail.com>,"Thu, 8 Jan 2015 17:43:21 -0500",PR #3872,dev <dev@spark.apache.org>,"Could one of the admins take a look at PR 3872 (JIRA 3299) submitted on 1/1
"
Sean Owen <sowen@cloudera.com>,"Thu, 8 Jan 2015 23:17:37 +0000",Re: Spark development with IntelliJ,Bill Bejeck <bbejeck@gmail.com>,"I remember seeing this too, but it seemed to be transient. Try
compiling again. In my case I recall that IJ was still reimporting
some modules when I tried to build. I don't see this error in general.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 08 Jan 2015 23:22:40 +0000",Re: Spark development with IntelliJ,"Sean Owen <sowen@cloudera.com>, Bill Bejeck <bbejeck@gmail.com>","Side question: Should this section
<https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IDESetup>
in
the wiki link to Useful Developer Tools
<https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools>?


"
Bill Bejeck <bbejeck@gmail.com>,"Thu, 8 Jan 2015 21:14:32 -0500",Re: Spark development with IntelliJ,Sean Owen <sowen@cloudera.com>,"That worked, thx


"
Nan Zhu <zhunanmcgill@gmail.com>,"Thu, 8 Jan 2015 21:33:48 -0500",missing document of several messages in actor-based receiver?,dev@spark.apache.org,"Hi, TD and other streaming developers,

When I look at the implementation of actor-based receiver (ActorReceiver.scala), I found that there are several messages which are not mentioned in the document  

case props: Props =>
val worker = context.actorOf(props)
logInfo(""Started receiver worker at:"" + worker.path)
sender ! worker

case (props: Props, name: String) =>
val worker = context.actorOf(props, name)
logInfo(""Started receiver worker at:"" + worker.path)
sender ! worker

case _: PossiblyHarmful => hiccups.incrementAndGet()

case _: Statistics =>
val workers = context.children
sender ! Statistics(n.get, workers.size, hiccups.get, workers.mkString(""\n‚Äù))

Is it hided with intention or incomplete document, or I missed something?
And the handler of these messages are ‚Äúbuggy""? e.g. when we start a new worker, we didn‚Äôt increase n (counter of children), and n and hiccups are unnecessarily set to AtomicInteger ?

Best,

--  
Nan Zhu
http://codingcat.me

"
Lewis John Mcgibbney <lewis.mcgibbney@gmail.com>,"Thu, 8 Jan 2015 18:47:49 -0800",[ANNOUNCE] Apache Science and Healthcare Track @ApacheCon NA 2015,"user@ctakes.apache.org, dev@ctakes.apache.org, user@tika.apache.org, 
	user@spark.apache.org, dev@spark.apache.org, user@sis.apache.org, 
	dev@sis.apache.org, user@taverna.incubator.apache.org, 
	dev@taverna.incubator.apache.org, user@airavata.apache.org, 
	Airavata Dev <dev@airavata.apache.org>, user@jena.apache.org, dev@jena.apache.org, 
	user@clerezza.apache.org, dev@clerezza.apache.org, user@marmotta.apache.org, 
	dev@marmotta.apache.org, user@stanbol.apache.org, dev@stanbol.apache.org","Hi Folks,

Apologies for cross posting :(

As some of you may already know, @ApacheCon NA 2015 is happening in Austin,
TX April 13th-16th.

This email is specifically written to attract all folks interested in
Science and Healthcare... this is an official call to arms! I am aware that
there are many Science and Healthcare-type people also lingering in the
Apache Semantic Web communities so this one is for all of you folks as well.

Over a number of years the Science track has been emerging as an attractive
and exciting, at times mind blowing non-traditional track running alongside
the resident HTTP server, Big Data, etc tracks. The Semantic Web Track is
another such emerging track which has proved popular. This year we want to
really get the message out there about how much Apache technology is
actually being used in Science and Healthcare. This is not *only* aimed at
attracting members of the communities below
<http://wiki.apache.org/apachecon/ACNA2015ContentCommittee#Target_Projects>
but also at potentially attracting a brand new breed of conference
participants to ApacheCon <https://wiki.apache.org/apachecon/ApacheCon> and
the Foundation e.g. Scientists who love Apache. We are looking for
exciting, invigorating, obscure, half-baked, funky, academic, practical and
impractical stories, use cases, experiments and down right successes alike
from within the Science domain. The only thing they need to have in common
is that they consume, contribute towards, advocate, disseminate or even
commercialize Apache technology within the Scientific domain and would be
relevant to that audience. It is fully open to interest whether this track
be combined with the proposed *healthcare track*... if there is interest to
do this then we can rename this track to Science and Healthcare. In essence
one could argue that they are one and the same however I digress [image: :)]

What I would like those of you that are interested to do, is to merely
check out the scope and intent of the Apache in Science content curation
which is currently ongoing and to potentially register your interest.

https://wiki.apache.org/apachecon/ACNA2015ContentCommittee#Apache_in_Science

I would love to see the Science and Healthcare track be THE BIGGEST track
@ApacheCon, and although we have some way to go, I'm sure many previous
track participants will tell you this is not to missed.

We are looking for content from a wide variety of Scientific use cases all
related to Apache technology.
Thanks in advance and I look forward to seeing you in Austin.
Lewis

-- 
*Lewis*
"
Patrick Wendell <pwendell@gmail.com>,"Thu, 8 Jan 2015 22:25:20 -0800",Re: Spark development with IntelliJ,Nicholas Chammas <nicholas.chammas@gmail.com>,"Nick - yes. Do you mind moving it? I should have put it in the
""Contributing to Spark"" page.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Thu, 8 Jan 2015 22:58:34 -0800",Re: Spark development with IntelliJ,Nicholas Chammas <nicholas.chammas@gmail.com>,"Actually I went ahead and did it.


---------------------------------------------------------------------


"
Tony Reix <tony.reix@bull.net>,"Fri, 9 Jan 2015 09:15:45 +0000",RE:Results of tests,Ted Yu <yuzhihong@gmail.com>,"Hi Ted

Thanks for the info.
However, I'm still unable to understand how the page:
   https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
has been built.
This page contains details I do not find in the page you indicated to me:
   https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull

As an example, I'm still unable to find these details:
org.apache.spark<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark/>       12 mn   0
        1
        247
        248

org.apache.spark.api.python<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.api.python/> 20 ms   0
        0
        2
        2

org.apache.spark.bagel<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.bagel/>   7.7 s   0
        0
        4
        4

org.apache.spark.broadcast<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.broadcast/>   43 s    0
        0
        17
        17

org.apache.spark.deploy<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy/> 16 s    0
        0
        29
        29

org.apache.spark.deploy.worker<https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy.worker/>   0.55 s  0
        0
        12
        12

........


Moreover, in my Ubuntu/x86_64 environment, I do not find 3745 tests and 0 failures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 ). When using IBM JVM, there are only 2566 tests and 5 failures (in same component: Streaming).

eds of tests.
Is Spark independent of Little/Big-Endian stuff ?

ike on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...

So, I need to learn more about how your Jenkins environment extracts details about the results.
Moreover, which JVM is used ?

Do you plan to use IBM JVM in order to check that Spark and IBM JVM are compatible ? (they already do not look to be compatible 100% ...).

Thanks

Tony

IBM Coop Architect & Technical Leader
Office : +33 (0) 4 76 29 72 67
1 rue de Provence - 38432 …chirolles - France
www.atos.net<http://www.atos.net/>
________________________________
De : Ted Yu [yuzhihong@gmail.com]
EnvoyÈ : jeudi 8 janvier 2015 17:43
¿ : Tony Reix
Cc : dev@spark.apache.org
Objet : Re: Results of tests

Here it is:

[centos] $ /home/jenkins/tools/hudson.tasks.Maven_MavenInstallation/Maven_3.0.5/bin/mvn -DHADOOP_PROFILE=hadoop-2.4 -Dlabel=centos -DskipTests -Phadoop-2.4 -Pyarn -Phive clean package


You can find the above in https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull


Cheers

Thanks !

I've been able to see that there are 3745 tests for version 1.2.0 with profile Hadoop 2.4  .
However, on my side, the maximum tests I've seen are 3485... About 300 tests are missing on my side.
Which Maven option has been used for producing the report file used for building the page:
     https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
  ? (I'm not authorized to look at the ""configuration"" part)

Thx !

Tony

________________________________
De : Ted Yu [yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
EnvoyÈ : jeudi 8 janvier 2015 16:11
¿ : Tony Reix
Cc : dev@spark.apache.org<mailto:dev@spark.apache.org>
Objet : Re: Results of tests

Please take a look at https://amplab.cs.berkeley.edu/jenkins/view/Spark/

Hi,
I'm checking that Spark works fine on a new environment (PPC64 hardware).
I've found some issues, with versions 1.1.0, 1.1.1, and 1.2.0, even when running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can find the results of the tests of Spark, for each version and for the different versions, in order to have a reference to compare my results with. I cannot find them on Spark web-site.
Thx
Tony



"
Tathagata Das <tdas@databricks.com>,"Fri, 9 Jan 2015 01:25:31 -0800",Re: missing document of several messages in actor-based receiver?,Nan Zhu <zhunanmcgill@gmail.com>,"It was not really mean to be hidden. So its essentially the case of the
documentation being insufficient. This code has not gotten much attention
for a while, so it could have a bugs. If you find any and submit a fix for
them, I am happy to take a look!

TD


""\n*‚Äù*))
 a new worker, we didn‚Äôt increase n (counter of children), and n and hiccups are unnecessarily set to AtomicInteger ?
"
Devl Devel <devl.development@gmail.com>,"Fri, 9 Jan 2015 10:41:54 +0000",Re: K-Means And Class Tags,"Joseph Bradley <joseph@databricks.com>, Yana Kadiyska <yana.kadiyska@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Joseph

Thanks for the suggestion, however retag is a private method and when I
call in Scala:

val retaggedInput = parsedData.retag(classOf[Vector])

I get:

Symbol retag is inaccessible from this place

However I can do this from Java, and it works in Scala:

return words.rdd().retag(Vector.class);

Dev




he/spark/mllib/tree/RandomForest.scala#L134
.dense((Double.parseDouble(good_entry[0]),
er
m
),
e
"
Sean Owen <sowen@cloudera.com>,"Fri, 9 Jan 2015 11:52:47 +0000",Re: Results of tests,Tony Reix <tony.reix@bull.net>,"Hey Tony, the number of tests run could vary depending on how the
build is configured. For example, YARN-related tests would only run
when the yarn profile is turned on. Java 8 tests would only run under
Java 8.

Although I don't know that there's any reason to believe the IBM JVM
has a problem with Spark, I see this issue that is potentially related
to endian-ness : https://issues.apache.org/jira/browse/SPARK-2018 I
don't know if that was a Spark issue. Certainly, would be good for you
to investigate if you are interested in resolving it.

The Jenkins output shows you exactly what tests were run and how --
have a look at the logs.

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull

with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
ark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark/>       12 mn   0
park/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.api.python/> 20 ms   0
job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.bagel/>   7.7 s   0
ark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.broadcast/>   43 s    0
/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy/> 16 s    0
w/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy.worker/>   0.55 s  0
 failures, but 3485 tests and 4 failures (when using Oracle JVM 1.7 ). When using IBM JVM, there are only 2566 tests and 5 failures (in same component: Streaming).
dreds of tests.
(like on Ubuntu/x86_64 with IBM JVM), with 6 or 285 failures...
ils about the results.
ompatible ? (they already do not look to be compatible 100% ...).
_3.0.5/bin/mvn -DHADOOP_PROFILE=hadoop-2.4 -Dlabel=centos -DskipTests -Phadoop-2.4 -Pyarn -Phive clean package
rk/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
ofile Hadoop 2.4  .
sts are missing on my side.
uilding the page:
n-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
running on Ubuntu on x86_64 with Oracle JVM. I'd like to know where I can find the results of the tests of Spark, for each version and for the different versions, in order to have a reference to compare my results with. I cannot find them on Spark web-site.

---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 9 Jan 2015 07:38:22 -0500","Re: missing document of several messages in actor-based
 receiver?",Tathagata Das <tdas@databricks.com>,"Thanks, TD,  

I just created 2 JIRAs to track these,  

https://issues.apache.org/jira/browse/SPARK-5174

https://issues.apache.org/jira/browse/SPARK-5175

Can you help to me assign these two JIRAs to me, and I‚Äôd like to submit the PRs

Best,  

--  
Nan Zhu
http://codingcat.me



 documentation being insufficient. This code has not gotten much attention for a while, so it could have a bugs. If you find any and submit a fix for them, I am happy to take a look!
ver.scala), I found that there are several messages which are not mentioned in the document  
ing(""\n‚Äù))
ing?
we start a new worker, we didn‚Äôt increase n (counter of children), and n and hiccups are unnecessarily set to AtomicInteger ?

"
Meethu Mathew <meethu.mathew@flytxt.com>,"Fri, 9 Jan 2015 18:15:28 +0530",Python to Java object conversion of numpy array,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I am trying to send a numpy array as an argument to a function predict() 
in a class in spark/python/pyspark/mllib/clustering.py which is passed 
to the function callMLlibFunc(name, *args)  in 
spark/python/pyspark/mllib/common.py.

Now the value is passed to the function  _py2java(sc, obj) .Here I am 
getting an exception

Py4JJavaError: An error occurred while calling z:org.apache.spark.mllib.api.python.SerDe.loads.
: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct)
	at net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)
	at net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:617)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)


Why common._py2java(sc, obj) is not handling numpy array type?

Please help..


-- 

Regards,

*Meethu Mathew*

*Engineer*

*Flytxt*

www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us 
<http://www.twitter.com/flytxt> | _Connect on Linkedin 
<http://www.linkedin.com/home?trk=hb_tab_home_top>_

"
Michael Armbrust <michael@databricks.com>,"Fri, 9 Jan 2015 08:34:42 -0800",Re: PR #3872,Bill Bejeck <bbejeck@gmail.com>,"I will look at it this weekend.


"
Davies Liu <davies@databricks.com>,"Fri, 9 Jan 2015 10:07:46 -0800",Re: Python to Java object conversion of numpy array,Meethu Mathew <meethu.mathew@flytxt.com>,"Hey Meethu,

The Java API accepts only Vector, so you should convert the numpy array into
pyspark.mllib.linalg.DenseVector.

BTW, which class are you using? the KMeansModel.predict() accept numpy.array,
it will do the conversion for you.

Davies


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 9 Jan 2015 10:47:45 -0800",Re: Results of tests,Sean Owen <sowen@cloudera.com>,"For a build which uses JUnit, we would see a summary such as the following (
https://builds.apache.org/job/HBase-TRUNK/6007/console):

Tests run: 2199, Failures: 0, Errors: 0, Skipped: 25


In https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
, I don't see such statistics.


Looks like scalatest-maven-plugin can be enhanced :-)



h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
e:
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark/>
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.api.python/>
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.bagel/>
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.broadcast/>
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy/>
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy.worker/>
y
mvn
Pyarn
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
/
).
n
"
Josh Rosen <rosenville@gmail.com>,"Fri, 9 Jan 2015 11:46:04 -0800",Re: Results of tests,Ted Yu <yuzhihong@gmail.com>,"The ""Test Result"" pages for Jenkins builds shows some nice statistics for
the test run, including individual test times:

https://amplab.cs.berkeley.edu/jenkins/view/Spark/job/Spark-1.2-Maven-with-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/

Currently this only covers the Java / Scala tests, but we might be able to
integrate the PySpark tests here, too (I think it's just a matter of
getting the Python test runner to generate the correct test result XML
output).


g
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark/
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.api.python/
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.bagel/
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.broadcast/
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy/
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy.worker/
nd
nly
re
mvn
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
h
0
or
h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
an
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 09 Jan 2015 20:15:32 +0000",Re: Results of tests,"Josh Rosen <rosenville@gmail.com>, Ted Yu <yuzhihong@gmail.com>","Just created: ""Integrate Python unit tests into Jenkins""

https://issues.apache.org/jira/browse/SPARK-5178

Nick



=
o
=
d
u
=
=
to
=
=
=
=
=
=
=
).
 2
s
.4
=
=
s
"
Ryan Williams <ryan.blake.williams@gmail.com>,"Fri, 09 Jan 2015 20:16:46 +0000","Present/Future of monitoring spark jobs, ""MetricsSystem"" vs. Web UI, etc.","""dev@spark.apache.org"" <dev@spark.apache.org>","I've long wished the web UI gave me a better sense of how the metrics it
reports are changing over time, so I was intrigued to stumble across the
MetricsSystem
<https://github.com/apache/spark/blob/b6aa557300275b835cce7baa7bc8a80eb5425cbb/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala>
infrastructure the other day.

I've set up a very basic Graphite instance and had dummy Spark jobs report
to it, but that process was a little bumpy (and the docs sparse
<https://spark.apache.org/docs/latest/monitoring.html#metrics>) so I wanted
to come up for air and ask a few questions about the present/future plans
for monitoring Spark jobs.

In rough order of increasing scope:

   - Do most people monitor their Spark jobs in realtime by repeatedly
   refreshing the web UI (cf. SPARK-5106
   <https://issues.apache.org/jira/browse/SPARK-5106>), or is there a
   better way?
   - Does anyone use or rely on the GraphiteSink? Quick googling turned up
   no evidence of anyone using it.
      - Likewise the other Sinks? GangliaSink?
   - Do people have custom Sink subclasses and dashboards that they've
   built to monitor Spark jobs, as was suggested by the appearance of a
   mysterious Ooyala ""DatadogSink"" gist
   <https://gist.github.com/ibuenros/9b94736c2bad2f4b8e23#file-sparkutils-scala-L336>
   in the recent thread on this list about custom metrics
   <http://apache-spark-developers-list.1001551.n3.nabble.com/Registering-custom-metrics-tp9030p10041.html>
   ?
   - What is the longer-term plan for how people should monitor / diagnose
   problems at runtime?
      - Will the official Spark web UI remain the main way that the average
      user will monitor their jobs?
      - Or, will SPARK-3644
      <https://issues.apache.org/jira/browse/SPARK-3644> usher in an era of
      many external implementations of Spark web UIs, so that the average user
      will take one of those ""off the shelf"" that they like best (because its
      graphs are prettier or it emphasizes / pivots around certain metrics that
      others do not)?
      - Is the MetricsSystem infrastructure redundant with the REST API
      discussed in SPARK-3644
      <https://issues.apache.org/jira/browse/SPARK-3644>?
         - Would more robust versions of each start to be redundant in the
         future?
         - I feel like the answers are ""somewhat yes"" and ""yes"", and would
         like to hear other perspectives.

Basically, I want to live in a world where:

   - I can see all of the stats currently exposed on the Web UI,
   - as well as others that aren't there yet,
      - number of records assigned to each task,
      - number of records completed by each task in realtime,
      - gc stats in realtime,
      - # of spill events,
      - size of spill events,
   - and all kinds of derivates of the above,
      - latencies/histograms for everything
         - records per second per task,
         - records per second per executor,
         - top N slowest/worst of any metric,
         - avg spill size,
         - etc.
      - over time,
   - at scale <https://issues.apache.org/jira/browse/SPARK-2017>


Are we going to get to this world by improving the web UI that ships with
Spark? I am pessimistic of that approach:

   - It may be impossible to do in a way that satisfies all stakeholders'
   aesthetic sensibilities and preferences for what stats/views are important.
   - It would be a monumental undertaking relative to the amount of
   attention that seems to have been directed at improving the web UI in the
   last few quarters.

OTOH, if the space of derivative stats and slices thereof that we want to
support is as complex as the outline I gave above suggests it might be,
then Graphite (or some equivalent) could be well suited to the task.
However, this is at odds with the relative obscurity that the MetricsSystem
seems to reside in and my impression that it is not something that core
developers think about or are focused on.

Finally, while the existence of SPARK-3644 (and Josh et al's great work on
it thus far) implies that the REST API / ""let 1000 [web UIs] bloom"" vision
is at least nominally being pursued, it seems like it's still a long way
from fostering a world where my dream use-cases above are realized, and
it's not clear from the outside whether fulfilling that vision is a
priority.

So I'm interested to hear peoples' thoughts on the above questions and what
the plan is / should be going forward. Having learned a lot about how Spark
works, the process of figuring out ""Why My Spark Jobs Are Failing"" still
feels daunting (at best) using the tools I've come across; we need to do a
better job of empowering people to figure these things out.
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 9 Jan 2015 12:18:10 -0800",Re: Results of tests,Josh Rosen <rosenville@gmail.com>,"I noticed that org.apache.spark.sql.hive.execution has a lot of tests
skipped.

Is there plan to enable these tests on Jenkins (so that there is no
regression across releases) ?

Cheers


h-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
o
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
o
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark/
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.api.python/
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.bagel/
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.broadcast/
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy/
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/org.apache.spark.deploy.worker/
.
2
only
/mvn
4
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/consoleFull
:
th
00
th-YARN/lastSuccessfulBuild/HADOOP_PROFILE=hadoop-2.4,label=centos/testReport/
:
"
ogeagla <ogeagla@gmail.com>,"Fri, 9 Jan 2015 13:21:52 -0700 (MST)",Re-use scaling means and variances from StandardScalerModel,dev@spark.apache.org,"Hello,

I would like to re-use the means and variances computed by the fit function
in the StandardScaler, as I persist them and my use case requires consisted
scaling of data based on some initial data set.  The StandardScalerModel's
constructor takes means and variances, but is private[mllib]. 
Forking/compiling Spark or copy/pasting the class into my project are both
options, but  I'd like to stay away from them.  Any chance there is interest
in a PR to allow this re-use via removal of private from the the
constructor?  Or perhaps an alternative solution exists?  

Thanks,
Octavian



--

---------------------------------------------------------------------


"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 9 Jan 2015 16:14:32 -0500","Re: missing document of several messages in actor-based
 receiver?",Tathagata Das <tdas@databricks.com>,"Hi,  

I have created the PR for these two issues

Best,  

--  
Nan Zhu
http://codingcat.me



o submit the PRs
he documentation being insufficient. This code has not gotten much attention for a while, so it could have a bugs. If you find any and submit a fix for them, I am happy to take a look!
eiver.scala), I found that there are several messages which are not mentioned in the document  
tring(""\n‚Äù))
thing?
n we start a new worker, we didn‚Äôt increase n (counter of children), and n and hiccups are unnecessarily set to AtomicInteger ?

"
Xiangrui Meng <mengxr@gmail.com>,"Fri, 9 Jan 2015 22:14:55 -0800",Re: Re-use scaling means and variances from StandardScalerModel,ogeagla <ogeagla@gmail.com>,"Feel free to create a JIRA for this issue. We might need to discuss
what to put in the public constructors. In the meanwhile, you can use
Java serialization to save/load the model:

sc.parallelize(Seq(model), 1).saveAsObjectFile(""/tmp/model"")
val model = sc.objectFile[StandardScalerModel](""/tmp/model"").first()

-Xiangrui


---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Sat, 10 Jan 2015 16:40:43 -0800",Job priority,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is it possible to specify a priority level for a job, such that the active
jobs might be scheduled in order of priority?

Alex
"
Alessandro Baretta <alexbaretta@gmail.com>,"Sun, 11 Jan 2015 07:36:44 -0800",Re: Job priority,Cody Koeninger <cody@koeninger.org>,"Cody,

While I might be able to improve the scheduling of my jobs by using a few
different pools with weights equal to, say, 1, 1e3 and 1e6, effectively
getting a small handful of priority classes. Still, this is really not
quite what I am describing. This is why my original post was on the dev
list. Let me then ask if there is any interest in having priority queue job
scheduling in Spark. This is something I might be able to pull off.

Alex


es
,
 to
l-properties
always
king
"
Mark Hamstra <mark@clearstorydata.com>,"Sun, 11 Jan 2015 10:07:00 -0800",Re: Job priority,Alessandro Baretta <alexbaretta@gmail.com>,"Yes, if you are asking about developing a new priority queue job scheduling
feature and not just about how job scheduling currently works in Spark, the
that's a dev list issue.  The current job scheduling priority is at the
granularity of pools containing jobs, not the jobs themselves; so if you
require strictly job-level priority queuing, that would require a new
development effort -- and one that I expect will involve a lot of tricky
corner cases.

Sorry for misreading the nature of your initial inquiry.


ob
y
,
t
ses
t,
s to
ol-properties
 pool
sking
m
"
Patrick Wendell <pwendell@gmail.com>,"Sun, 11 Jan 2015 20:34:24 -0800",Re: Job priority,Mark Hamstra <mark@clearstorydata.com>,"Priority scheduling isn't something we've supported in Spark and we've
opted to support FIFO and Fair scheduling and asked users to try and
fit these to the needs of their applications.

In practice from what I've seen of priority schedulers, such as the
linux CPU scheduler, is that strict priority scheduling is never used
in practice because of priority starvation and other issues. So you
have this second tier of heuristics that exist to deal with issues
like starvation, priority inversion, etc, and these become very
complex over time.

That said, I looked a this a bit with @kayousterhout and I don't think
it would be very hard to implement a simple priority scheduler in the
current architecture. My main concern would be additional complexity
that would develop over time, based on looking at previous
implementations in the wild.

Alessandro, would you be able to open a JIRA and list some of your
requirements there? That way we could hear whether other people have
similar needs.

- Patrick


---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Sun, 11 Jan 2015 20:35:47 -0800",Re: Job priority,Patrick Wendell <pwendell@gmail.com>,"Ok, will do.

Thanks for providing some context on this topic.

Alex


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Mon, 12 Jan 2015 10:58:14 +0530",Re: Python to Java object conversion of numpy array,Davies Liu <davies@databricks.com>,"Hi,
Thanks Davies .

I added a new class GaussianMixtureModel in clustering.py and the method 
predict in it and trying to pass numpy array from this method.I 
converted it to DenseVector and its solved now.

Similarly I tried passing a List  of more than one dimension to the 
function _py2java , but now the exception is

'list' object has no attribute '_get_object_id'

and when I give a tuple input (Vectors.dense([0.8786, 
-0.7855]),Vectors.dense([-0.1863, 0.7799])) exception is like

'numpy.ndarray' object has no attribute '_get_object_id'

Regards,

*Meethu Mathew*

*Engineer*

*Flytxt*

www.flytxt.com | Visit our blog <http://blog.flytxt.com/> | Follow us 
<http://www.twitter.com/flytxt> | _Connect on Linkedin 
<http://www.linkedin.com/home?trk=hb_tab_home_top>_


"
Davies Liu <davies@databricks.com>,"Sun, 11 Jan 2015 22:16:06 -0800",Re: Python to Java object conversion of numpy array,Meethu Mathew <meethu.mathew@flytxt.com>,"Could you post a piece of code here?


---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Mon, 12 Jan 2015 11:51:19 +0530",Re: Python to Java object conversion of numpy array,Davies Liu <davies@databricks.com>,"Hi,

This is the code I am running.

mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))

membershipMatrix = callMLlibFunc(""findPredict"", 
rdd.map(_convert_to_vector), mu)

Regards,
Meethu

"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Mon, 12 Jan 2015 13:35:07 +0000","Discussion | SparkContext 's setJobGroup and clearJobGroup should
 return a new instance of SparkContext","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi spark committers

I would like to discuss the possibility of changing the signature
of SparkContext 's setJobGroup and clearJobGroup functions to return a
replica of SparkContext with the job group set/unset instead of mutating
the original context. I am building a spark job server and I am assigning
job groups before passing control to user provided logic that uses spark
context to define and execute a job (very much like job-server). The issue
is that I can't reliably know when to clear the job group as user defined
code can use futures to submit multiple tasks in parallel. In fact, I am
even allowing users to return a future from their function on which spark
server can register callbacks to know when the user defined job is
complete. Now, if I set the job group before passing control to user
function and wait on future to complete so that I can clear the job group,
I can no longer use that SparkContext for any other job. This means I will
have to lock on the SparkContext which seems like a bad idea. Therefore, my
proposal would be to return new instance of SparkContext (a replica with
just job group set/unset) that can further be used in concurrent
environment safely. I am also happy mutating the original SparkContext just
not break backward compatibility as long as the returned SparkContext is
not affected by set/unset of job groups on original SparkContext.

Thoughts please?

Thanks,
Aniket
"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Mon, 12 Jan 2015 13:38:17 +0000",YARN | SPARK-5164 | Submitting jobs from windows to linux YARN,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark YARN maintainers

Can anyone please look and comment on SPARK-5164? Basically, this stops
users from submitting jobs (or using spark shell) from a windows machine to
a a YARN cluster running on linux. I should be able to submit a pull
request for this provided the community agrees. This would be a great help
for windows users (like me).

Thanks,
Aniket
"
=?UTF-8?Q?Zsolt_T=C3=B3th?= <toth.zsolt.bme@gmail.com>,"Mon, 12 Jan 2015 15:12:13 +0100",Re: YARN | SPARK-5164 | Submitting jobs from windows to linux YARN,Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Hi Aniket,

I think this is a duplicate of SPARK-1825, isn't it?

Zsolt

2015-01-12 14:38 GMT+01:00 Aniket Bhatnagar <aniket.bhatnagar@gmail.com>:

"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Mon, 12 Jan 2015 14:19:55 +0000",Re: YARN | SPARK-5164 | Submitting jobs from windows to linux YARN,=?UTF-8?Q?Zsolt_T=C3=B3th?= <toth.zsolt.bme@gmail.com>,"Ohh right. It is. I will mark my defect as duplicate and cross check my
notes with the fixes in the pull request. Thanks for pointing out Zsolt :)

ote:

lp
"
preeze <etander@gmail.com>,"Mon, 12 Jan 2015 09:38:21 -0700 (MST)",Apache Spark client high availability,dev@spark.apache.org,"Dear community,

I've been searching the internet for quite a while to find out what is the
best architecture to support HA for a spark client.

We run an application that connects to a standalone Spark cluster and caches
a big chuck of data for subsequent intensive calculations. To achieve HA
we'll need to run several instances of the application on different hosts.

Initially I explored the option to reuse (i.e. share) the same executors set
between SparkContext instances of all running applications. Found it
impossible.

So, every application, which creates an instance of SparkContext, has to
spawn its own executors. Externalizing and sharing executors' memory cache
with Tachyon is a semi-solution since each application's executors will keep
using their own set of CPU cores.

Spark-jobserver is another possibility. It manages SparkContext itself and
accepts job requests from multiple clients for the same context which is
brilliant. However, this becomes a new single point of failure.

Now I am exploring if it's possible to run the Spark cluster in YARN cluster
mode and connect to the driver from multiple clients.

Is there anything I am missing guys?
Any suggestion is highly appreciated!



--

---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 12 Jan 2015 22:22:49 +0530",Re: Apache Spark client high availability,preeze <etander@gmail.com>,"We usually run Spark in HA with the following stack:

-> Apache Mesos
-> Marathon - init/control system for starting, stopping, and maintaining
always-on applications.(Mainly SparkStreaming)
-> Chronos - general-purpose scheduler for Mesos, supports job dependency
graphs.
-> Spark Job Server - primarily for it's ability to reuse shared contexts
with multiple jobs

‚ÄãThis thread has a better discussion
http://apache-spark-user-list.1001560.n3.nabble.com/How-do-you-run-your-spark-app-td7935.html
‚Äã


Thanks
Best Regards


e
.
e
d
ient-high-availability-tp10088.html
"
Erik Erlandson <eje@redhat.com>,"Mon, 12 Jan 2015 12:12:51 -0500 (EST)","Re: Discussion | SparkContext 's setJobGroup and clearJobGroup
 should return a new instance of SparkContext",Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"setJobGroup needs fixing:
https://issues.apache.org/jira/browse/SPARK-4514

I'm interested in any community input on what the semantics or design ""ought"" to be changed to.


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id C0FC417563
	for <apmail-spark-dev-archive@minotaur.apache.org>; Mon, 12 Jan 2015 18:05:49 +0000 (UTC)
Received: (qmail 88837 invoked by uid 500); 12 Jan 2015 18:05:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 88762 invoked by uid 500); 12 Jan 2015 18:05:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 88751 invoked by uid 99); 12 Jan 2015 18:05:49 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 18:05:49 +0000
X-ASF-Spam-Status: No, hits=-0.7 required=10.0
	tests=RCVD_IN_DNSWL_LOW
X-Spam-Check-By: apache.org
Received-SPF: error (athena.apache.org: local policy)
Received: from [209.85.217.180] (HELO mail-lb0-f180.google.com) (209.85.217.180)
    by apache.org (qpsmtpd/0.29) with ESMTP; Mon, 12 Jan 2015 18:05:46 +0000
Received: by mail-lb0-f180.google.com with SMTP id l4so18963435lbv.11
        for <dev@spark.apache.org>; Mon, 12 Jan 2015 10:05:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=KZPHlkDcFZpoEQn7rxfgYyyBVEvQ1eRH5VcqhJQ0Rlg=;
        b=lcp32uOtu+f5MFFqx2JICtKd6Miu8EcgZP6OFNYI0DrdOTidW07h7YKGBY1ZlFC/1O
         zy9/tPv3zd7aFt5qMubFpjTdvYfxstDXkuY92TvZFx/zYKISDP82bXZ6jkI9hLZg5HWg
         lV23TDTqsSoMgwdFDL1wAYTS4z/eqljNLUBPXUTDN9PuX67q3Lv7yGpKFieq8lvt04A3
         8bbqIe/MijE1rE2Pm66Ygwv8I3nj7QjXmCnOJgTe9FULGbXgEIcRT0+6aQXu3zVu1Tet
         CD9ohqw7f1zeGQDP3bWgxyGTo+1YTKaAsvNQs6OoEUTECiMImdO8H901Hf257+C148+c
         MBBw==
X-Gm-Message-State: ALoCoQlWQ7crPN9Ru7o8BFbBJxU/w8UWKin9lFt3oow3czFANje6y9oa+75Qzg799e9DTgNjI8Uy
MIME-Version: 1.0
X-Received: by 10.152.206.108 with SMTP id ln12mr37909116lac.3.1421085903847;
 Mon, 12 Jan 2015 10:05:03 -0800 (PST)
Received: by 10.25.215.136 with HTTP; Mon, 12 Jan 2015 10:05:03 -0800 (PST)
In-Reply-To: <54B367DF.6000807@flytxt.com>
References: <54AFCD68.9010908@flytxt.com>
	<CA+2Pv=gpFV8K0DEAq0=Qx1ynoZ8G9iujSrLUrfKnDuS38-LFWQ@mail.gmail.com>
	<54B35B6E.1000606@flytxt.com>
	<CA+2Pv=gog11B6k8pQ+8e63Zs=4+u0Epd=+Hi3DThqjv+GRAqCA@mail.gmail.com>
	<54B367DF.6000807@flytxt.com>
Date: Mon, 12 Jan 2015 10:05:03 -0800
Message-ID: <CA+2Pv=gUaWx4e=UWZ53DxBBzC7eqGD3JyJgOZ6YiKC1YLOAd2Q@mail.gmail.com>
Subject: Re: Python to Java object conversion of numpy array
From: Davies Liu <davies@databricks.com>
To: Meethu Mathew <meethu.mathew@flytxt.com>
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>
Content-Type: text/plain; charset=UTF-8
X-Virus-Checked: Checked by ClamAV on apache.org


What's the Java API looks like? all the arguments of findPredict
should be converted
into java objects, so what should `mu` be converted to?


---------------------------------------------------------------------


"
Octavian Geagla <ogeagla@gmail.com>,"Mon, 12 Jan 2015 12:19:36 -0700 (MST)",Re: Re-use scaling means and variances from StandardScalerModel,dev@spark.apache.org,"Thanks for the suggestions.  

I've opened this JIRA ticket:
https://issues.apache.org/jira/browse/SPARK-5207
Feel free to modify it, assign it to me, kick off a discussion, etc.  

I'd be more than happy to own this feature and PR.

Thanks,
-Octavian



--

---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Tue, 13 Jan 2015 09:44:47 +0530",Re: Python to Java object conversion of numpy array,Davies Liu <davies@databricks.com>,"Hi,

This is the function defined in PythonMLLibAPI.scala
def findPredict(
       data: JavaRDD[Vector],
       wt: Object,
       mu: Array[Object],
       si: Array[Object]):  RDD[Array[Double]]  = {
}

So the parameter mu should be converted to Array[object].

mu = (Vectors.dense([0.8786, -0.7855]),Vectors.dense([-0.1863, 0.7799]))

def _py2java(sc, obj):

     if isinstance(obj, RDD):
         ...
     elif isinstance(obj, SparkContext):
       ...
     elif isinstance(obj, dict):
        ...
     elif isinstance(obj, (list, tuple)):
         obj = ListConverter().convert(obj, sc._gateway._gateway_client)
     elif isinstance(obj, JavaObject):
         pass
     elif isinstance(obj, (int, long, float, bool, basestring)):
         pass
     else:
         bytes = bytearray(PickleSerializer().dumps(obj))
         obj = sc._jvm.SerDe.loads(bytes)
     return obj

Since its a tuple of Densevectors, in _py2java() its entering the 
isinstance(obj, (list, tuple)) condition and throwing exception(happens 
because the dimension of tuple >1). However the conversion occurs 
correctly if the Pickle conversion is done (last else part).

Hope its clear now.

Regards,
Meethu


"
Niranda Perera <niranda.perera@gmail.com>,"Tue, 13 Jan 2015 14:21:35 +0530",create a SchemaRDD from a custom datasource,dev@spark.apache.org,"Hi,

We have a custom datasources API, which connects to various data sources
and exposes them out as a common API. We are now trying to implement the
Spark datasources API released in 1.2.0 to connect Spark for analytics.

Looking at the sources API, we figured out that we should extend a scan
class (table scan etc). While doing so, we would have to implement the
'schema' and 'buildScan' methods.

say, we can infer the schema of the underlying data and take data out as
Row elements. Is there any way we could create RDD[Row] (needed in the
buildScan method) using these Row elements?

Cheers
-- 
Niranda
"
Reynold Xin <rxin@databricks.com>,"Tue, 13 Jan 2015 00:59:19 -0800",Re: create a SchemaRDD from a custom datasource,Niranda Perera <niranda.perera@gmail.com>,"Depends on what the other side is doing. You can create your own RDD
implementation by subclassing RDD, or it might work if you use
sc.parallelize(1 to n, n).mapPartitionsWithIndex( /* code to read the data
and return an iterator */ ) where n is the number of partitions.


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Tue, 13 Jan 2015 18:23:34 +0530","Use of MapConverter, ListConverter in python to java object conversion","""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

In the python object to java conversion done in the method _py2java in 
spark/python/pyspark/mllib/common.py, why  we are doing individual 
conversion  using MpaConverter,ListConverter? The same can be acheived 
using

bytearray(PickleSerializer().dumps(obj))
obj = sc._jvm.SerDe.loads(bytes)

Is there any performance gain or something in using individual 
converters rather than PickleSerializer?

-- 

Regards,

*Meethu*
"
Zhiwei Chan <z.w.chan.jason@gmail.com>,"Tue, 13 Jan 2015 23:59:03 +0800",Unable to find configuration file at location scalastyle-config.xml,dev@spark.apache.org,"Hi everyone,
  I am newly to spark, and try to package the spark-core for some
modification.  I use IDEA to package the spark-core_2.10 of spark 1.1.1.
When encounter the following error, I  check the website
http://www.scalastyle.org/maven.html, and its suggest configuration is to
modify the spark-parent.pom and add a
 ${basedir}to the <configLocation> and <outputFile> of
scalastyle-maven-plugin, but it doesn't work. I solve this error by cocpy
the config file to core directory, but, is there any other better solutionÔºü

[ERROR] Failed to execute goal
org.scalastyle:scalastyle-maven-plugin:0.4.0:check (default) on project
spark-core_2.10: Failed during scalastyle execution: Unable to find
configuration file at location scalastyle-config.xml -> [Help 1]
[ERROR]
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 13 Jan 2015 08:50:14 -0800",Fwd: [ NOTICE ] Service Downtime Notification - R/W git repos,dev@spark.apache.org,"FYI our git repo may be down for a few hours today.
---------- Forwarded message ----------
From: ""Tony Stevenson"" <pctony@apache.org>
Date: Jan 13, 2015 6:49 AM
Subject: [ NOTICE ] Service Downtime Notification - R/W git repos
To:
Cc:

Folks,

Please note than on Thursday 15th at 20:00 UTC the Infrastructure team
will be taking the read/write git repositories offline.  We expect
that this migration to last about 4 hours.

During the outage the service will be migrated from an old host to a
new one.   We intend to keep the URL the same for access to the repos
after the migration, but an alternate name is already in place in case
DNS updates take too long.   Please be aware it might take some hours
after the completion of the downtime for github to update and reflect
any changes.

The Infrastructure team have been trialling the new host for about a
week now, and [touch wood] have not had any problems with it.

The service is current;y available by accessing repos via:
https://git-wip-us.apache.org

If you have any questions please address them to infrastructure@apache.org




--
Cheers,
Tony


----------------------------------
Tony Stevenson

tony@pc-tony.com
pctony@apache.org

http://www.pc-tony.com

GPG - 1024D/51047D66
----------------------------------
"
Davies Liu <davies@databricks.com>,"Tue, 13 Jan 2015 10:13:25 -0800",Re: Python to Java object conversion of numpy array,Meethu Mathew <meethu.mathew@flytxt.com>,"
I see, we should remove the special case for list and tuple, pickle should work
more reliably for them. I had tried to remove it, it did not break any tests.

Could you do it in your PR or I create a PR for it separately?


---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Tue, 13 Jan 2015 10:17:31 -0800","Re: Use of MapConverter, ListConverter in python to java object conversion",Meethu Mathew <meethu.mathew@flytxt.com>,"It's not necessary, I will create a PR to remove them.

For larger dict/list/tuple, the pickle approach may have less RPC
calls, better performance.

Davies


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 13 Jan 2015 11:09:35 -0800",Re: create a SchemaRDD from a custom datasource,Malith Dhanushka <mmalithh@gmail.com>,"If it is a small collection of them on the driver, you can just use
sc.parallelize to create an RDD.



"
=?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com.INVALID>,"Tue, 13 Jan 2015 21:39:45 +0000 (UTC)",DBSCAN for MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear all,
I think MLlib needs more clustering algorithms and DBSCAN is my first candidate. I am starting to implement it. Any advice?
Muhammad-Ali"
=?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com.INVALID>,"Tue, 13 Jan 2015 21:49:08 +0000 (UTC)",Re: DBSCAN for MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","I have to say, I have created a Jira task for it:
[SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRA

| ¬† |
| ¬† | ¬† | ¬† | ¬† | ¬† |
| [SPARK-5226] Add DBSCAN Clustering Algorithm to MLlib - ASF JIRAMLlib is all k-means now, and I think we should add some new clustering algorithms to it. First candidate is DBSCAN as I think.  |
|  |
| View on issues.apache.org | Preview by Yahoo |
|  |
| ¬† |

 ¬† 

   

 Dear all,
I think MLlib needs more clustering algorithms and DBSCAN is my first candidate. I am starting to implement it. Any advice?
Muhammad-Ali

   "
Madhu <madhu@madhu.com>,"Tue, 13 Jan 2015 15:15:59 -0700 (MST)",VertexId type in GraphX,dev@spark.apache.org,"Are there any plans to generalize the type of VertexId in GraphX?
Our keys are particularly long. We could use the hashCode() trick, but the
chance of collisions is not acceptable. Given our data volume, we have
encountered hashCode() collisions more than once.

I see this Jira, but it is specific to UUIDs:

https://issues.apache.org/jira/browse/SPARK-1153

Looking through the code, it seems doable, but I'm not aware of the
consequences.
Is it entirely a performance issue?

Support for an arbitrary type would be ideal, but arbitrarily long byte
arrays are a reasonable compromise, if that helps.



-----
--
Madhu
https://www.linkedin.com/in/msiddalingaiah
--

---------------------------------------------------------------------


"
Ewan Higgs <ewan.higgs@ugent.be>,"Wed, 14 Jan 2015 14:33:45 +0100",SparkSpark-perf terasort WIP branch,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
I'm trying to build the Spark-perf WIP code but there are some errors to 
do with Hadoop APIs. I presume this is because there is some Hadoop 
version set and it's referring to that. But I can't seem to find it.

The errors are as follows:

[info] Compiling 15 Scala sources and 2 Java sources to 
/home/ehiggs/src/spark-perf/spark-tests/target/scala-2.10/classes...
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraInputFormat.scala:40: 
object task is not a member of package org.apache.hadoop.mapreduce
[error] import org.apache.hadoop.mapreduce.task.TaskAttemptContextImpl
[error]                                    ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraInputFormat.scala:132: 
not found: type TaskAttemptContextImpl
[error]             val context = new TaskAttemptContextImpl(
[error]                               ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraScheduler.scala:37: 
object TTConfig is not a member of package 
org.apache.hadoop.mapreduce.server.tasktracker
[error] import org.apache.hadoop.mapreduce.server.tasktracker.TTConfig
[error]        ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraScheduler.scala:91: 
not found: value TTConfig
[error]   var slotsPerHost : Int = conf.getInt(TTConfig.TT_MAP_SLOTS, 4)
[error]                                        ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraSortAll.scala:7: 
value run is not a member of org.apache.spark.examples.terasort.TeraGen
[error]     tg.run(Array[String](""10M"", ""/tmp/terasort_in""))
[error]        ^
[error] 
/home/ehiggs/src/spark-perf/spark-tests/src/main/scala/spark/perf/terasort/TeraSortAll.scala:9: 
value run is not a member of org.apache.spark.examples.terasort.TeraSort
[error]     ts.run(Array[String](""/tmp/terasort_in"", ""/tmp/terasort_out""))
[error]        ^
[error] 6 errors found
[error] (compile:compile) Compilation failed
[error] Total time: 13 s, completed 05-Jan-2015 12:21:47

I can build the same code if it's in the Spark tree using the following 
command:
mvn -Dhadoop.version=2.5.0 -DskipTests=true install

Is there a way I can convince spark-perf to build this code with the 
appropriate Hadoop library version? I tried to apply the following to 
spark-tests/project/SparkTestsBuild.scala but it didn't seem to work as 
I expected:

$ git diff project/SparkTestsBuild.scala
diff --git a/spark-tests/project/SparkTestsBuild.scala 
b/spark-tests/project/SparkTestsBuild.scala
index 4116326..4ed5f0c 100644
--- a/spark-tests/project/SparkTestsBuild.scala
+++ b/spark-tests/project/SparkTestsBuild.scala
@@ -16,7 +16,9 @@ object SparkTestsBuild extends Build {
          ""org.scalatest"" %% ""scalatest"" % ""2.2.1"" % ""test"",
          ""com.google.guava"" % ""guava"" % ""14.0.1"",
          ""org.apache.spark"" %% ""spark-core"" % ""1.0.0"" % ""provided"",
-        ""org.json4s"" %% ""json4s-native"" % ""3.2.9""
+        ""org.json4s"" %% ""json4s-native"" % ""3.2.9"",
+        ""org.apache.hadoop"" % ""hadoop-common"" % ""2.5.0"",
+        ""org.apache.hadoop"" % ""hadoop-mapreduce"" % ""2.5.0""
        ),
        test in assembly := {},
        outputPath in assembly := 
file(""target/spark-perf-tests-assembly.jar""),
@@ -36,4 +38,4 @@ object SparkTestsBuild extends Build {
          case _ => MergeStrategy.first
        }
      ))
-}
\ No newline at end of file
+}


Yours,
Ewan
"
RJ Nowling <rnowling@gmail.com>,"Wed, 14 Jan 2015 14:26:48 -0500",Incorrect Maven Artifact Names,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

I'm trying to upgrade some Spark RPMs from 1.1.0 to 1.2.0.  As part of the
RPM process, we build Spark with Maven. With Spark 1.2.0, though, the
artifacts are  placed in com/google/guava and there is no org/apache/spark.

I saw that the pom.xml files had been modified to prevent the install
command and that the guava dependency was modified.  Could someone who is
more familiar with the Spark maven files comment on what might be causing
this oddity?

Thanks,
RJ

We build Spark like so:
$ mvn -Phadoop-2.4 -Dmesos.version=0.20.0 -DskipTests clean package

Then build a local Maven repo:

mvn -Phadoop-2.4 \
    -Dmesos.version=0.20.0 \
    -DskipTests install:install-file  \
    -Dfile=assembly/target/scala-2.10/spark-assembly-1.2.0-hadoop2.4.0.jar \
    -DcreateChecksum=true \
    -DgeneratePom=true \
    -DartifactId=spark-assembly_2.1.0 \
    -DlocalRepositoryPath=../maven2
"
Sean Owen <sowen@cloudera.com>,"Wed, 14 Jan 2015 19:33:06 +0000",Re: Incorrect Maven Artifact Names,RJ Nowling <rnowling@gmail.com>,"Guava is shaded, although one class is left in its original package.
This shouldn't have anything to do with Spark's package or namespace
though. What are you saying is in com/google/guava?

You can un-skip the install plugin with -Dmaven.install.skip=false


---------------------------------------------------------------------


"
RJ Nowling <rnowling@gmail.com>,"Wed, 14 Jan 2015 14:59:13 -0500",Re: Incorrect Maven Artifact Names,Sean Owen <sowen@cloudera.com>,"Thanks, Sean.

Yes, Spark is incorrectly copying the spark assembly jar to
com/google/guava in the maven repository.  This is for the 1.2.0 release,
just to clarify.

I reverted the patches that shade Guava and removed the parts disabling the
install plugin and it seemed to fix the issue.

It seems that Spark poms are inheriting something from Guava.

RJ


"
RJ Nowling <rnowling@gmail.com>,"Wed, 14 Jan 2015 16:08:07 -0500",Re: Incorrect Maven Artifact Names,Sean Owen <sowen@cloudera.com>,"Hi Sean,

I confirmed that if I take the Spark 1.2.0 release (a428c446), undo the
guava PR [1], and use -Dmaven.install.skip=false with the workflow above,
the problem is fixed.

RJ


[1]
https://github.com/apache/spark/commit/c9f743957fa963bc1dbed7a44a346ffce1a45cf2#diff-6382f8428b13fa6082fa688178f3dbcc


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 14 Jan 2015 13:16:29 -0800",Re: Incorrect Maven Artifact Names,RJ Nowling <rnowling@gmail.com>,"Hi RJ,

I think I remember noticing in the past that some Guava metadata ends
up overwriting maven-generated metadata in the assembly's manifest.
That's probably something we should fix if that still affects the
build.

That being said, this is probably happening because you're using
""install-file"" instead of ""install"". If you want a workaround that
doesn't require unshading things, you can change assembly.pom to (i)
not skip the install plugin and (ii) have ""jar"" as the packaging,
instead of pom.






-- 
Marcelo

---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Wed, 14 Jan 2015 13:24:57 -0800",Re: K-Means And Class Tags,Devl Devel <devl.development@gmail.com>,"(After asking around,) retag() is private[spark] in Scala, but Java ignores
the ""private[X],"" making retag (unintentionally) public in Java.

Currently, your solution of retagging from Java is the best hack I can
think of.  It may take a bit of engineering to create a proper fix for the
long-term.
Joseph


che/spark/mllib/tree/RandomForest.scala#L134
s.dense((Double.parseDouble(good_entry[0]),
p
"
RJ Nowling <rnowling@gmail.com>,"Wed, 14 Jan 2015 16:40:15 -0500",Re: Incorrect Maven Artifact Names,Marcelo Vanzin <vanzin@cloudera.com>,"Thanks, Marcelo!

I'll look into ""install"" vs ""install-file"".

What is the difference between pom and jar packaging?

guidelines, which makes life a little more interesting. :)  (e.g., RPMs
should resolve against other RPMs instead of external repositories.)


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 14 Jan 2015 13:43:44 -0800",Re: Incorrect Maven Artifact Names,RJ Nowling <rnowling@gmail.com>,"
If you do an install on a ""pom"" packaging module, it will only install
the module's pom file in the target repository.

-- 
Marcelo

---------------------------------------------------------------------


"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 14 Jan 2015 15:55:38 -0800",Re: DBSCAN for MLlib,=?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com>,"Please find my comments on the JRIA page. -Xiangrui

s all k-means now, and I think we should add some new clustering algorithms to it. First candidate is DBSCAN as I think.  |
didate. I am starting to implement it. Any advice?

---------------------------------------------------------------------


"
"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Thu, 15 Jan 2015 02:08:45 +0000",SciSpark: NASA AIST14 proposal,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark Devs,

Just wanted to FYI that I was funded on a 2 year NASA proposal
to build out the concept of a scientific RDD (create by space/time,
and other operations) for use in some neat climate related NASA
use cases.

http://esto.nasa.gov/files/solicitations/AIST_14/ROSES2014_AIST_A41_awards.
html


I will keep everyone posted and plan on interacting with the list
over here to get it done. I expect that we‚Äôll start work in March.
In the meanwhile you guys can scope the abstract at the link provided.
Happy
to chat about it if you have any questions too.

Cheers!

Chris

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Chris Mattmann, Ph.D.
Chief Architect
Instrument Software and Science Data Systems Section (398)
NASA Jet Propulsion Laboratory Pasadena, CA 91109 USA
Office: 168-519, Mailstop: 168-527
Email: chris.a.mattmann@nasa.gov
WWW:  http://sunset.usc.edu/~mattmann/
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Adjunct Associate Professor, Computer Science Department
University of Southern California, Los Angeles, CA 90089 USA
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++




"
Reynold Xin <rxin@databricks.com>,"Wed, 14 Jan 2015 18:18:11 -0800",Re: SciSpark: NASA AIST14 proposal,"""Mattmann, Chris A (3980)"" <chris.a.mattmann@jpl.nasa.gov>","Chris,

This is really cool. Congratulations and thanks for sharing the news.



s.
.
"
Matei Zaharia <matei.zaharia@gmail.com>,"Wed, 14 Jan 2015 18:39:18 -0800",Re: SciSpark: NASA AIST14 proposal,Reynold Xin <rxin@databricks.com>,"Yeah, very cool! You may also want to check out https://issues.apache.org/jira/browse/SPARK-5097 as something to build upon for these operations.

Matei

http://esto.nasa.gov/files/solicitations/AIST_14/ROSES2014_AIST_A41_awards.
March.
provided.


---------------------------------------------------------------------


"
Aniket <aniket.bhatnagar@gmail.com>,"Wed, 14 Jan 2015 21:07:06 -0700 (MST)",Re: SciSpark: NASA AIST14 proposal,dev@spark.apache.org,"Hi Chris

This is super cool. I was wondering if this would be an open source project
so that people can contribute or reuse?

Thanks,
Aniket


s.
.
IST14-proposal-tp10115.html
ervlet.jtp?macro=unsubscribe_by_code&node=1&code=YW5pa2V0LmJoYXRuYWdhckBnbWFpbC5jb218MXwxMzE3NTAzMzQz>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>




--
3.nabble.com/SciSpark-NASA-AIST14-proposal-tp10115p10118.html
om."
RJ Nowling <rnowling@gmail.com>,"Wed, 14 Jan 2015 23:51:30 -0500",Re: SciSpark: NASA AIST14 proposal,"""dev@spark.apache.org"" <dev@spark.apache.org>","Congratulations, Chris!

I created a JIRA for ""dimensional"" RDDs that might be relevant:
https://issues.apache.org/jira/browse/SPARK-4727

Jeremy Freeman pointed me to his lab's work on for neuroscience that have
some related functionality :
http://thefreemanlab.com/thunder/

:

ct
s
ch.
on
IST14-proposal-tp10115.html
rvlet.jtp?macro=unsubscribe_by_code&node=1&code=YW5pa2V0LmJoYXRuYWdhckBnbWFpbC5jb218MXwxMzE3NTAzMzQz
rvlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml
IST14-proposal-tp10115p10118.html
"
Reynold Xin <rxin@databricks.com>,"Wed, 14 Jan 2015 21:45:24 -0800",Spark SQL API changes and stabilization,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Spark devs,

Given the growing number of developers that are building on Spark SQL, we
would like to stabilize the API in 1.3 so users and developers can be
confident to build on it. This also gives us a chance to improve the API.

In particular, we are proposing the following major changes. This should
have no impact for most users (i.e. those running SQL through the JDBC
client or SQLContext.sql method).

1. Everything in sql.catalyst package is private to the project.

2. Redesign SchemaRDD DSL (SPARK-5097): We initially added the DSL for
SchemaRDD and logical plans in order to construct test cases. We have
received feedback from a lot of users that the DSL can be incredibly
powerful. In 1.3, we‚Äôd like to refactor the DSL to make it suitable for not
only constructing test cases, but also in everyday data pipelines. The new
SchemaRDD API is inspired by the data frame concept in Pandas and R.

3. Reconcile Java and Scala APIs (SPARK-5193): We would like to expose one
set of APIs that will work for both Java and Scala. The current Java API
(sql.api.java) does not share any common ancestor with the Scala API. This
led to high maintenance burden for us as Spark developers and for library
developers. We propose to eliminate the Java specific API, and simply work
on the existing Scala API to make it also usable for Java. This will make
Java a first class citizen as Scala. This effectively means that all public
classes should be usable for both Scala and Java, including SQLContext,
HiveContext, SchemaRDD, data types, and the aforementioned DSL.


Again, this should have no impact on most users since the existing DSL is
rarely used by end users. However, library developers might need to change
the import statements because we are moving certain classes around. We will
keep you posted as patches are merged.
"
andy petrella <andy.petrella@gmail.com>,"Thu, 15 Jan 2015 09:39:04 +0000",Re: SciSpark: NASA AIST14 proposal,"RJ Nowling <rnowling@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hey Chris,

This sounds amazing!
You might have to check also with the Geotrellis
<https://github.com/geotrellis/geotrellis> team (Rob and Eugene for
instance) who have already covered quite interesting ground dealing with
tiles as RDD element.
Some algebra operations are there, but also thingies like Shortest Path
(within rasters).

A small, I've a student who is working on a implementation of LU/LC using
Spark (first using CA, then I hope extension using stochastic methods like
local random forest).

If you consider implementing a R-Tree (or perhaps the SD version) for OGIs
operation, I thought that IndexedRDD
<https://github.com/amplab/spark-indexedrdd> could be interesting to
consider (I've been asked to look at options to implement this kind of
distributed and resilient R-Tree, so I'll be happy to see how it'd perform
^^).

cheers and have fun!
andy



rk
arch.
.
-
-
"
preeze <etander@gmail.com>,"Thu, 15 Jan 2015 04:52:04 -0700 (MST)",Spark client reconnect to driver in yarn-cluster deployment mode,dev@spark.apache.org,"(http://spark.apache.org/docs/1.2.0/running-on-yarn.html):

""In yarn-cluster mode, the Spark driver runs inside an application master
process which is managed by YARN on the cluster, and the client can go away
after initiating the application.""

Is there any designed way that the client connects back to the driver (still
running in YARN) for collecting results at a later stage?



--

---------------------------------------------------------------------


"
PierreB <pierre.borckmans@realimpactanalytics.com>,"Thu, 15 Jan 2015 06:04:52 -0700 (MST)",Spark 1.2.0: MissingRequirementError,dev@spark.apache.org,"Hi guys,

A few people seem to have the same problem with Spark 1.2.0 so I figured I
would push it here.

see:
http://apache-spark-user-list.1001560.n3.nabble.com/MissingRequirementError-with-spark-td21149.html

In a nutshell, for sbt test to work, we now need to fork a JVM and also give
more memory to be able to run tests.

See
also:https://github.com/deanwampler/spark-workshop/blob/master/project/Build.scala

This all used to work fine until 1.2.0.

Could u have a look please?
Thanks

P.




--

---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Thu, 15 Jan 2015 07:39:43 -0800",Join implementation in SparkSQL,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello,

Where can I find docs about how joins are implemented in SparkSQL? In
particular, I'd like to know whether they are implemented according to
their relational algebra definition as filters on top of a cartesian
product.

Thanks,

Alex
"
Alessandro Baretta <alexbaretta@gmail.com>,"Thu, 15 Jan 2015 07:53:48 -0800",Re: Spark SQL API changes and stabilization,Reynold Xin <rxin@databricks.com>,"Reynold,

Thanks for the heads up. In general, I strongly oppose the use of ""private""
to restrict access to certain parts of the API, the reason being that I
might find the need to use some of the internals of a library from my own
project. I find that a @DeveloperAPI annotation serves the same purpose as
""private"" without imposing unnecessary restrictions: it discourages people
from using the annotated API and reserves the right for the core developers
to change it suddenly in backwards incompatible ways.

In particular, I would like to express the desire that the APIs to
programmatically construct SchemaRDDs from an RDD[Row] and a StructType
remain public. All the SparkSQL data type objects should be exposed by the
API, and the jekyll build should not hide the docs as it does now.

Thanks.

Alex


le for not
w
e
s
k
ic
e
ll
"
David Robinson <drobin1437@gmail.com>,"Thu, 15 Jan 2015 09:31:35 -0700 (MST)",Re: Implementing TinkerPop on top of GraphX,dev@spark.apache.org,"I am new to Spark and GraphX, however, I use Tinkerpop backed graphs and
think the idea of using Tinkerpop as the API for GraphX is a great idea and
hope you are still headed in that direction.  I noticed that Tinkerpop 3 is
moving into the Apache family:
http://wiki.apache.org/incubator/TinkerPopProposal  which might alleviate
concerns about having an API definition ""outside"" of Spark.

Thanks,




--

---------------------------------------------------------------------


"
"""devl.development"" <devl.development@gmail.com>","Thu, 15 Jan 2015 09:46:41 -0700 (MST)",LinearRegressionWithSGD accuracy,dev@spark.apache.org,"response variable given a feature vector x.

In a simple example I used a perfectly linear dataset such that x=y
y,x
1,1
2,2
...

10000,10000

Using the out-of-box example from the website (with and without scaling):

 val data = sc.textFile(file)

    val parsedData = data.map { line =>
      val parts = line.split(',')
     LabeledPoint(parts(1).toDouble, Vectors.dense(parts(0).toDouble)) //y
and x

    }
    val scaler = new StandardScaler(withMean = true, withStd = true)
      .fit(parsedData.map(x => x.features))
    val scaledData = parsedData
      .map(x =>
      LabeledPoint(x.label,
        scaler.transform(Vectors.dense(x.features.toArray))))

    // Building the model
    val numIterations = 100
    val model = LinearRegressionWithSGD.train(parsedData, numIterations)

    // Evaluate model on training examples and compute training error *
tried using both scaledData and parsedData
    val valuesAndPreds = scaledData.map { point =>
      val prediction = model.predict(point.features)
      (point.label, prediction)
    }
    val MSE = valuesAndPreds.map{case(v, p) => math.pow((v - p), 2)}.mean()
    println(""training Mean Squared Error = "" + MSE)

Both scaled and unscaled attempts give:

training Mean Squared Error = NaN

I've even tried x, y+(sample noise from normal with mean 0 and stddev 1)
still comes up with the same thing.

Is this not supposed to work for x and y or 2 dimensional plots? Is there
something I'm missing or wrong in the code above? Or is there a limitation
in the method?

Thanks for any advice.



--

---------------------------------------------------------------------


"
Robin East <robin.east@xense.co.uk>,"Thu, 15 Jan 2015 17:42:47 +0000",Re: LinearRegressionWithSGD accuracy,"""devl.development"" <devl.development@gmail.com>","-dev, +user

Youíll need to set the gradient descent step size to something small - a bit of trial and error shows that 0.00000001 works.

Youíll need to create a LinearRegressionWithSGD instance and set the step size explicitly:

val lr = new LinearRegre"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Jan 2015 10:33:04 -0800",Re: Spark SQL API changes and stabilization,Alessandro Baretta <alexbaretta@gmail.com>,"Alex,

I didn't communicate properly. By ""private"", I simply meant the expectation
that it is not a public API. The plan is to still omit it from the
scaladoc/javadoc generation, but no language visibility modifier will be
applied on them.

After 1.3, you will likely no longer need to use things in sql.catalyst
package directly. Programmatically construct SchemaRDDs is going to be a
first class public API. Data types have already been moved out of the
sql.catalyst package and now lives in sql.types. They are becoming stable
public APIs. When the ""data frame"" patch is submitted, you will see a
public expression library also. There will be few reason for end users or
library developers to hook into things in sql.catalyst. For the bravest and
the most advanced, they can still use them, with the expectation that it is
subject to change.






g
r
e
e
.
ble for
ew
ne
is
y
rk
e
s
ge
"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Jan 2015 10:36:21 -0800",Re: Join implementation in SparkSQL,Alessandro Baretta <alexbaretta@gmail.com>,"It's a bunch of strategies defined here:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/SparkStrategies.scala

In most common use cases (e.g. inner equi join), filters are pushed below
the join or into the join. Doing a cartesian product followed by a filter
is too expensive.



"
Jay Hutfles <jayhutfles@gmail.com>,"Thu, 15 Jan 2015 16:58:55 +0000",Graphx TripletFields written in Java?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,
  Does anyone know the reasoning behind implementing
org.apache.spark.graphx.TripletFields in Java instead of Scala?  It doesn't
look like there's anything in there that couldn't be done in Scala.
Nothing serious, just curious.  Thanks!
   -Jay
"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Jan 2015 10:40:17 -0800",Re: Graphx TripletFields written in Java?,Jay Hutfles <jayhutfles@gmail.com>,"The static fields - Scala can't express JVM static fields unfortunately.
Those will be important once we provide the Java API.




"
Devl Devel <devl.development@gmail.com>,"Thu, 15 Jan 2015 18:42:49 +0000",Re: LinearRegressionWithSGD accuracy,Robin East <robin.east@xense.co.uk>,"Thanks, that helps a bit at least with the NaN but the MSE is still very
high even with that step size and 10k iterations:

training Mean Squared Error = 3.3322561285919316E7

Does this method need say 100k iterations?







all - a
the step
ean()
n
nWithSGD-accuracy-tp10127.html
"
Corey Nolet <cjnolet@gmail.com>,"Thu, 15 Jan 2015 15:16:43 -0500",Re: Spark SQL API changes and stabilization,Reynold Xin <rxin@databricks.com>,"Reynold,

inferencing logic that creates a Set[(String, StructType)] out of
Map[String,Any]. SPARK-5260 addresses this so that I can use Accumulators
to infer my schema instead of forcing a map/reduce phase to occur on an RDD
in order to get the final schema. Do you (or anyone else) see a path
forward in exposing this to users? A utility class perhaps?


on
nd
is
m
om
s.
ld
table for
PI
,
"
Joseph Bradley <joseph@databricks.com>,"Thu, 15 Jan 2015 12:23:43 -0800",Re: LinearRegressionWithSGD accuracy,Devl Devel <devl.development@gmail.com>,"It looks like you're training on the non-scaled data but testing on the
scaled data.  Have you tried this training & testing on only the scaled
data?


small - a
t the step
):
/y
e)
s)
)
re
nWithSGD-accuracy-tp10127.html
"
Andrew Ash <andrew@andrewash.com>,"Thu, 15 Jan 2015 15:52:08 -0500",Re: Join implementation in SparkSQL,Reynold Xin <rxin@databricks.com>,"What Reynold is describing is a performance optimization in implementation,
but the semantics of the join (cartesian product plus relational algebra
filter) should be the same and produce the same results.


"
Reynold Xin <rxin@databricks.com>,"Thu, 15 Jan 2015 13:25:18 -0800",Re: Spark SQL API changes and stabilization,Corey Nolet <cjnolet@gmail.com>,"We can look into some sort of util class in sql.types for general type
inference. In general many methods in JsonRDD might be useful enough to
extract. Those will probably be marked as DeveloperAPI with less stability
guarantees.


DD
e
r
e
,
C
r
itable for
e
e
t,
L
e
"
Devl Devel <devl.development@gmail.com>,"Thu, 15 Jan 2015 23:30:10 +0000",Re: LinearRegressionWithSGD accuracy,Joseph Bradley <joseph@databricks.com>,"It was a bug in the code, however adding the step parameter got the results
to work.  Mean Squared Error = 2.610379825794694E-5

I've also opened a jira to put the step parameter in the examples so that
people new to mllib have a way to improve the MSE.

https://issues.apache.org/jira/browse/SPARK-5273


 small - a
et the
e
ue)
ns)
1)
onWithSGD-accuracy-tp10127.html
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 16 Jan 2015 01:22:09 +0000",RE: Join implementation in SparkSQL,"Andrew Ash <andrew@andrewash.com>, Reynold Xin <rxin@databricks.com>","Not so sure about your question, but the SparkStrategies.scala and Optimizer.scala is a good start if you want to get details of the join implementation or optimization.

-----Original Message-----
From: Andrew Ash [mailto:andrew@andrewash.com] 
Sent: Friday, January 16, 2015 4:52 AM
To: Reynold Xin
Cc: Alessandro Baretta; dev@spark.apache.org
Subject: Re: Join implementation in SparkSQL

What Reynold is describing is a performance optimization in implementation, but the semantics of the join (cartesian product plus relational algebra
filter) should be the same and produce the same results.

On Thu, Jan 15, 2015 at 1:36 PM, Reynold Xin <rxin@databricks.com> wrote:

> It's a bunch of strategies defined here:
>
> https://github.com/apache/spark/blob/master/sql/core/src/main/scala/or
> g/apache/spark/sql/execution/SparkStrategies.scala
>
> In most common use cases (e.g. inner equi join), filters are pushed 
> below the join or into the join. Doing a cartesian product followed by 
> a filter is too expensive.
>
>
> On Thu, Jan 15, 2015 at 7:39 AM, Alessandro Baretta 
> <alexbaretta@gmail.com
> >
> wrote:
>
> > Hello,
> >
> > Where can I find docs about how joins are implemented in SparkSQL? 
> > In particular, I'd like to know whether they are implemented 
> > according to their relational algebra definition as filters on top 
> > of a cartesian product.
> >
> > Thanks,
> >
> > Alex
> >
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Robin East <robin.east@xense.co.uk>,"Fri, 16 Jan 2015 13:44:53 +0000",Fwd: LinearRegressionWithSGD accuracy,dev@spark.apache.org,"

Sent from my iPhone

Begin forwarded message:

gmail.com>
unning a model where intercept will be 0.00. You need to call setIntercept(true) to include the intercept in the model.
ng the feature values), then the ""true"" model now has an intercept of 5000.5, whereas the original data's ""true"" model has an intercept of 0.  I think that's the issue.
d works out just fine)
e(q._2.toDouble))
ansform(Vectors.dense(x.features.toArray))))
n()
inished. Last 10 stochastic losses 3.3338313007386144E7, 3.333831299679853E7, 3.333831298621632E7, 3.333831297563938E7, 3.3338312965067785E7, 3.3338312954501465E7, 3.333831294394051E7, 3.3338312933384743E7, 3.33383129228344E7, 3.3338312912289333E7
irectly cached, which may hurt performance if its parent RDDs are also uncached.
hts=[0.003567902277776811], intercept=0.0)
ould only converge faster. Non-scaled version produced near perfect results at alpha=0.00000001,numIterations=100
 in the original code block:
hts=[2886.885094323781], intercept=5000.48169121784)
derstands ML well -- I'd appreciate if you guys have any insight on why the small alpha/numIters did so poorly on the scaled data (I've removed the dev list)


m>
ery
ing small - a
d set the step
com>
r the
y
ling):
)) //y
 true)
tions)
r *

ev 1)
 there
ssionWithSGD-accuracy-tp10127.html
--
"
Mick Davies <michael.belldavies@gmail.com>,"Fri, 16 Jan 2015 09:17:07 -0700 (MST)",Optimize encoding/decoding strings when using Parquet,dev@spark.apache.org,"Hi, 

It seems that a reasonably large proportion of query time using Spark SQL
seems to be spent decoding Parquet Binary objects to produce Java Strings.
Has anyone considered trying to optimize these conversions as many are
duplicated.

Details are outlined in the conversation in the user mailing list 
http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SQL-amp-Parquet-data-are-reading-very-very-slow-td21061.html
<http://apache-spark-user-list.1001560.n3.nabble.com/Spark-SQL-amp-Parquet-data-are-reading-very-very-slow-td21061.html> 
, I have copied a bit of that discussion here.

It seems that as Spark processes each row from Parquet it makes a call to
convert the Binary representation for each String column into a Java String.
However in many (probably most) circumstances the underlying Binary instance
from Parquet will have come from a Dictionary, for example when column
cardinality is low. Therefore Spark is converting the same byte array to a
copy of the same Java String over and over again. This is bad due to extra
cpu, extra memory used for these strings, and probably results in more
expensive grouping comparisons. 


I tested a simple hack to cache the last Binary->String conversion per
column in ParquetConverter and this led to a 25% performance improvement for
the queries I used. Admittedly this was over a data set with lots or runs of
the same Strings in the queried columns. 

These costs are quite significant for the type of data that I expect will be
stored in Parquet which will often have denormalized tables and probably
lots of fairly low cardinality string columns 

I think a good way to optimize this would be if changes could be made to
Parquet so that  the encoding/decoding of Objects to Binary is handled on
Parquet side of fence. Parquet could deal with Objects (Strings) as the
client understands them and only use encoding/decoding to store/read from
underlying storage medium. Doing this I think Parquet could ensure that the
encoding/decoding of each Object occurs only once. 

Does anyone have an opinion on this, has it been considered already?

Cheers Mick







--

---------------------------------------------------------------------


"
Ewan Higgs <ewan.higgs@ugent.be>,"Fri, 16 Jan 2015 17:26:27 +0100",RDD order guarantees,dev@spark.apache.org,"Hi all,
Quick one: when reading files, are the orders of partitions guaranteed 
to be preserved? I am finding some weird behaviour where I run 
sortByKeys() on an RDD (which has 16 byte keys) and write it to disk. If 
I open a python shell and run the following:

for part in range(29):
     print map(ord, 
open('/home/ehiggs/data/terasort_out/part-r-000{0:02}'.format(part), 
'r').read(16))

Then each partition is in order based on the first value of each partition.

I can also call TeraValidate.validate from TeraSort and it is happy with 
the results. It seems to be on loading the file that the reordering 
happens. If this is expected, is there a way to ask Spark nicely to give 
me the RDD in the order it was saved?

This is based on trying to fix my TeraValidate code on this branch:
https://github.com/ehiggs/spark/tree/terasort

Thanks,
Ewan

---------------------------------------------------------------------


"
Michel Dufresne <sparkhealthanalytics@gmail.com>,"Fri, 16 Jan 2015 12:56:08 -0500",Setting JVM options to Spark executors in Standalone mode,dev@spark.apache.org,"Hi All,

I'm trying to set some JVM options to the executor processes in a
standalone cluster. Here's what I have in *spark-env.sh*:

jmx_opt=""-Dcom.sun.management.jmxremote""


However the option are showing up on the *daemon* JVM not the *workers*. It
has the same effect as if I was using SPARK_DAEMON_JAVA_OPTS (which should
set it on the daemon process).

Thanks in advance for your help,

Michel
"
Zhan Zhang <zzhang@hortonworks.com>,"Fri, 16 Jan 2015 10:02:47 -0800",Re: Setting JVM options to Spark executors in Standalone mode,Michel Dufresne <sparkhealthanalytics@gmail.com>,"You can try to add it in in conf/spark-defaults.conf

 # spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers=""one two threeî

Thanks.

Zhan Zhang


e""
It
d


-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Michel Dufresne <sparkhealthanalytics@gmail.com>,"Fri, 16 Jan 2015 13:07:31 -0500",Re: Setting JVM options to Spark executors in Standalone mode,Zhan Zhang <zzhang@hortonworks.com>,"Thank for your reply, I've should have mentioned that spark-env.sh is the
only option i found because:

   - I'm passing the public IP address of the slave (which is determined in
   the shell script)
   - I'm creating the SpeakConf/SparkContext from a Play Application
   (therefore I'm not using spark-submit script)

Thanks


lse""
""
.
to
at
ly
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 16 Jan 2015 10:15:42 -0800",Re: Setting JVM options to Spark executors in Standalone mode,Michel Dufresne <sparkhealthanalytics@gmail.com>,"
Then you can set that configuration Zhan mentions directly in your
SparkConf object.

BTW the env variable for what you want is SPARK_EXECUTOR_OPTS, but the
use of env variables to set app configuration is discouraged.


-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 16 Jan 2015 10:41:32 -0800",Re: RDD order guarantees,Ewan Higgs <ewan.higgs@ugent.be>,"You are running on a local file system right? HDFS orders the file based on
names, but local file system often don't. I think that's why the difference.

We might be able to do a sort and order the partitions when we create a RDD
to make this universal though.


"
Michael Armbrust <michael@databricks.com>,"Fri, 16 Jan 2015 10:53:54 -0800",Re: Optimize encoding/decoding strings when using Parquet,Mick Davies <michael.belldavies@gmail.com>,"+1 to adding such an optimization to parquet.  The bytes are tagged
specially as UTF8 in the parquet schema so it seem like it would be
possible to add this.


"
Andrew Musselman <andrew.musselman@gmail.com>,"Fri, 16 Jan 2015 12:04:47 -0800",Spectral clustering,dev@spark.apache.org,"Hi, thinking of picking up this Jira ticket:
https://issues.apache.org/jira/browse/SPARK-4259

Anyone done any work on this to date?  Any thoughts on it before we go too
far in?

Thanks!

Best
Andrew
"
Kushal Datta <kushal.datta@gmail.com>,"Fri, 16 Jan 2015 13:01:33 -0800",Re: Implementing TinkerPop on top of GraphX,David Robinson <drobin1437@gmail.com>,"Hi David,


Yes, we are still headed in that direction.
Please take a look at the repo I sent earlier.
I think that's a good starting point.

Thanks,
-Kushal.


"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 16 Jan 2015 13:54:39 -0800",Re: Join implementation in SparkSQL,Reynold Xin <rxin@databricks.com>,"Reynold,

The source file you are directing me to is a little too terse for me to
understand what exactly is going on. Let me tell you what I'm trying to do
and what problems I'm encountering, so that you might be able to better
direct me investigation of the SparkSQL codebase.

I am computing the join of three tables, sharing the same primary key,
composed of three fields, and having several other fields. My first attempt
at computing this join was in SQL, with a query much like this slightly
simplified one:

         SELECT
          a.key1 key1, a.key2 key2, a.key3 key3,
          a.data1   adata1,    a.data2    adata2,    ...
          b.data1   bdata1,    b.data2    bdata2,    ...
          c.data1   cdata1,    c.data2    cdata2,    ...
        FROM a, b, c
        WHERE
          a.key1 = b.key1 AND a.key2 = b.key2 AND a.key3 = b.key3
          b.key1 = c.key1 AND b.key2 = c.key2 AND b.key3 = c.key3

This code yielded a SparkSQL job containing 40,000 stages, which failed
after filling up all available disk space on the worker nodes.

I then wrote this join as a plain mapreduce join. The code looks roughly
like this:
val a_ = a.map(row => (key(row), (""a"", row))
val b_ = b.map(row => (key(row), (""b"", row))
val c_ = c.map(row => (key(row), (""c"", row""))
val join = UnionRDD(sc, List(a_, b_, c_)).groupByKey

This implementation yields approximately 1600 stages and completes in a few
minutes on a 256 core cluster. The huge difference in scale of the two jobs
makes me think that SparkSQL is implementing my join as cartesian product.
This is they query plan--I'm not sure I can read it, but it does seem to
imply that the filter conditions are not being pushed far down enough:

 'Project [...]
 'Filter (((((('a.key1 = 'b.key1)) && ('a.key2 = b.key2)) && ...)
  'Join Inner, None
   'Join Inner, None

Is maybe SparkSQL unable to push join conditions down from the WHERE clause
into the join itself?

Alex


"
Kyle Ellrott <kellrott@soe.ucsc.edu>,"Fri, 16 Jan 2015 14:11:36 -0800",Re: Implementing TinkerPop on top of GraphX,Kushal Datta <kushal.datta@gmail.com>,"Looking at https://github.com/kdatta/tinkerpop3/compare/graphx-gremlin I
only see a maven build file. Do you have some source code some place else?

I've worked on a spark based implementation (
https://github.com/kellrott/spark-gremlin ), but its not done and I've been
tied up on other projects.
It also look Tinkerpop3 is a bit of a moving target. I had targeted the
work done for gremlin-giraph (
http://www.tinkerpop.com/docs/3.0.0.M5/#giraph-gremlin ) that was part of
the M5 release, as a base model for implementation. But that appears to
have been refactored into gremlin-hadoop (
http://www.tinkerpop.com/docs/3.0.0.M6/#hadoop-gremlin ) in the M6 release.
I need to assess how much this changes the code.

Most of the code that needs to be changes from Giraph to Spark will be
simply replacing classes with spark derived ones. The main place where the
logic will need changed is in the 'GraphComputer' class (
https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/process/computer/giraph/GiraphGraphComputer.java
) which is created by the Graph when the 'compute' method is called (
https://github.com/tinkerpop/tinkerpop3/blob/master/hadoop-gremlin/src/main/java/com/tinkerpop/gremlin/hadoop/structure/HadoopGraph.java#L135
).


Kyle




"
Alessandro Baretta <alexbaretta@gmail.com>,"Fri, 16 Jan 2015 14:11:25 -0800",Re: Spark SQL API changes and stabilization,Reynold Xin <rxin@databricks.com>,"Reynold,

strongly encourage you to work on, is to make sure that the Scaladoc CAN be
generated manually if needed (a ""Use at your own risk"" clause would be
perfectly legitimate here). The reason I say this is that currently even
hacking SparkBuild.scala to include SparkSQL in the unidoc target doesn't
help, as scaladoc itself fails with errors such as these.

[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/dsl/package.scala:359:
polymorphic expression cannot be instantiated to expected type;
[error]  found   : [T(in method
apply)]org.apache.spark.sql.catalyst.dsl.ScalaUdfBuilder[T(in method apply)]
[error]  required:
org.apache.spark.sql.catalyst.dsl.package.ScalaUdfBuilder[T(in method
functionToUdfBuilder)]
[error]   implicit def functionToUdfBuilder[T: TypeTag](func: Function22[_,
_, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, _, T]):
ScalaUdfBuilder[T] = ScalaUdfBuilder(func)
[error]

                            ^
[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala:147:
value q is not a member of StringContext
[error]  Note: implicit class Evaluate2 is not applicable here because it
comes after the application point and it lacks an explicit result type
[error]         q""""""
[error]         ^
[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala:181:
value q is not a member of StringContext
[error]         q""""""
[error]         ^
[error]
/Users/alex/git/spark/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/CodeGenerator.scala:198:
value q is not a member of StringContext

While I understand you desire to discourage users from relying on the
internal ""private"" APIs, there is no reason to prevent people from gaining
a better understanding of how things work by allow them--with some
effort--to get to the docs.

Thanks,

Alex


m
nd
is
m
ng
m
or
.
he
:
we
I.
d
able for
I
ry
ke
is
"
Kushal Datta <kushal.datta@gmail.com>,"Fri, 16 Jan 2015 14:13:26 -0800",Re: Implementing TinkerPop on top of GraphX,Kyle Ellrott <kellrott@soe.ucsc.edu>,"The source code is under a new module named 'graphx'. let me double check.


"
Reynold Xin <rxin@databricks.com>,"Fri, 16 Jan 2015 14:33:36 -0800",Re: Spark SQL API changes and stabilization,Alessandro Baretta <alexbaretta@gmail.com>,"That's a good idea. We didn't intentionally break the doc generation. The
doc generation for Catalyst is broken because we use Scala macros and we
haven't had time to investigate how to fix it yet.

If you have a minute and want to investigate, I can merge it in as soon as
possible.






be
talyst/dsl/package.scala:359:
y)]
talyst/expressions/codegen/CodeGenerator.scala:147:
talyst/expressions/codegen/CodeGenerator.scala:181:
talyst/expressions/codegen/CodeGenerator.scala:198:
g
:
om
l
e
r
and
 is
ing
om
for
s.
the
ld
table for
PI
,
"
Yin Huai <yhuai@databricks.com>,"Fri, 16 Jan 2015 14:47:40 -0800",Re: Join implementation in SparkSQL,Alessandro Baretta <alexbaretta@gmail.com>,"Hi Alex,

Can you attach the output of sql(""explain extended <your
query>"").collect.foreach(println)?

Thanks,

Yin


"
Kushal Datta <kushal.datta@gmail.com>,"Fri, 16 Jan 2015 14:57:37 -0800",Re: Implementing TinkerPop on top of GraphX,Kyle Ellrott <kellrott@soe.ucsc.edu>,"code updated. sorry, wrong branch uploaded before.


"
Ewan Higgs <ewan.higgs@ugent.be>,"Sat, 17 Jan 2015 00:36:55 +0100",Re: RDD order guarantees,Reynold Xin <rxin@databricks.com>,"Yes, I am running on a local file system.

Is there a bug open for this? Mingyu Kim reported the problem last April:
http://apache-spark-user-list.1001560.n3.nabble.com/Spark-reads-partitions-in-a-wrong-order-td4818.html

-Ewan


"
=?UTF-8?Q?Muhammad_Ali_A=27r=C3=A5by?= <angellandros@yahoo.com.INVALID>,"Sat, 17 Jan 2015 02:56:09 +0000 (UTC)",Re: DBSCAN for MLlib,Xiangrui Meng <mengxr@gmail.com>,"Please find my answers on JIRA page.
Muhammad-Ali 

   

 Please find my comments on the JRIA page. -Xiangrui

s all k-means now, and I think we should add some new clustering algorithms to it. First candidate is DBSCAN as I think.¬† |
didate. I am starting to implement it. Any advice?

   "
Akhil Das <akhil@sigmoidanalytics.com>,"Sat, 17 Jan 2015 14:26:53 +0530",Bouncing Mails,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","My mails to the mailing list are getting rejected, have opened a Jira
issue, can someone take a look at it?

https://issues.apache.org/jira/browse/INFRA-9032






Thanks
Best Regards
"
DB Tsai <dbtsai@dbtsai.com>,"Sat, 17 Jan 2015 02:28:17 -0800",Re: LinearRegressionWithSGD accuracy,"""devl.development"" <devl.development@gmail.com>","I'm working on LinearRegressionWithElasticNet using OWLQN now. This
will do the data standardization internally so it's transparent to
users. With OWLQN, you don't have to manually choose stepSize. Will
send out PR soon next week.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai




---------------------------------------------------------------------


"
Chunnan Yao <yaochunnan@gmail.com>,"Sat, 17 Jan 2015 03:29:29 -0700 (MST)",Re: Spark development with IntelliJ,dev@spark.apache.org,"*I followed the procedures instructed by
https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ. 
But problems still occurs which has made me a little bit annoyed. 

My environment settings are:JAVA 1.7.0 Scala: 2.10.4 Spark:1.2.0, Intellij
Idea 14.0.2, Ubuntu 14.04 

Firstly I got the scala plugin correctly installed. 

I choosed maven-3, hadoop-2.4, scala-2.10 as my profiles when importing the
project. 

After importing, I first turned on ""View-Tool Windows-Maven Projects"". I see
the ""hbase-hadoop1"" is selected, but I had not chosen it in the import
process. So I deselected it to leave the hadoop-2.4, maven-3, scala-2.10 to
be the only three selected items in ""Maven Projects-Profiles"".

According to the Wiki, the next step should be ""Generate Sources and Update
Folders For All Projects"". I did so, and waited for some minutes to get the
sub-projects prepared. 

Then I cleared the ""Additional compiler options"" in the
""File-Settings-Build, Execution, Deployment-Compiler-Scala Compiler"". 

Finally I choosed Build-reBuild project. 

However, the compiler failed with ""value q is not a member of stringcontext""
errors. *

(partial screen shot)
-------------------------------------------------------
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateProjection.scala
Error:(42, 21) value q is not a member of StringContext
    val lengthDef = q""final val length = $tupleLength""
                    ^
Error:(54, 7) value q is not a member of StringContext
      q""""""
      ^
Error:(66, 9) value q is not a member of StringContext
        q""""""
        ^
Error:(83, 9) value q is not a member of StringContext
        q""if(isNullAt($iLit)) { null } else { ${newTermName(s""c$i"")} }""
        ^
Error:(85, 7) value q is not a member of StringContext
      q""override def iterator = Iterator[Any](..$allColumns)""
      ^
Error:(88, 27) value q is not a member of StringContext
    val accessorFailure = q""""""scala.sys.error(""Invalid ordinal:"" + i)""""""
                          ^
Error:(95, 9) value q is not a member of StringContext
        q""if(i == $ordinal) { if(isNullAt($i)) return null else return
$elementName }""
        ^
Error:(97, 7) value q is not a member of StringContext
      q""override def apply(i: Int): Any = { ..$cases; $accessorFailure }""
      ^
Error:(106, 9) value q is not a member of StringContext
        q""""""
        ^
Error:(117, 7) value q is not a member of StringContext
      q""override def update(i: Int, value: Any): Unit = { ..$cases;
$accessorFailure }""
      ^
Error:(126, 11) value q is not a member of StringContext
          q""if(i == $i) return $elementName"" :: Nil
          ^
Error:(130, 7) value q is not a member of StringContext
      q""""""
      ^
Error:(143, 11) value q is not a member of StringContext
          q""if(i == $i) { nullBits($i) = false; $elementName = value; return
}"" :: Nil
          ^
Error:(147, 7) value q is not a member of StringContext
      q""""""
      ^
Error:(157, 29) value q is not a member of StringContext
        case BooleanType => q""if ($elementName) 0 else 1""
                            ^
Error:(158, 52) value q is not a member of StringContext
        case ByteType | ShortType | IntegerType => q""$elementName.toInt""
                                                   ^
Error:(159, 26) value q is not a member of StringContext
        case LongType => q""($elementName ^ ($elementName >>> 32)).toInt""
                         ^
Error:(160, 27) value q is not a member of StringContext
        case FloatType => q""java.lang.Float.floatToIntBits($elementName)""
                          ^
Error:(162, 11) value q is not a member of StringContext
          q""{ val b = java.lang.Double.doubleToLongBits($elementName); (b ^
(b >>>32)).toInt }""
          ^
Error:(163, 19) value q is not a member of StringContext
        case _ => q""$elementName.hashCode""
                  ^
Error:(165, 7) value q is not a member of StringContext
      q""if (isNullAt($i)) 0 else $nonNull""
      ^
Error:(168, 54) value q is not a member of StringContext
    val hashUpdates: Seq[Tree] = hashValues.map(v => q""""""result = 37 *
result + $v"""""": Tree)
                                                     ^
Error:(171, 7) value q is not a member of StringContext
      q""""""
      ^
Error:(181, 7) value q is not a member of StringContext
      q""if (this.$elementName != specificType.$elementName) return false""
      ^
Error:(185, 7) value q is not a member of StringContext
      q""""""
      ^
Error:(195, 7) value q is not a member of StringContext
      q""""""
      ^
Error:(210, 16) value q is not a member of StringContext
    val code = q""""""
               ^
---------------------------------------------------------------------
*
Then I tried
http://stackoverflow.com/questions/26995023/errorscalac-bad-option-p-intellij-idea,
which does not clear the ""Additional compiler options"", but change the -P in
to -Xplugin. 
So now my ""Additional Compiler Options"" is like this
""-Xplugin:/home/yaochunnan/.m2/repository/org/scalamacros/paradise_2.10.4/2.0.1/paradise_2.10.4-2.0.1.jar""

Then I reBuild again with the following errors: *
(partial screen shot)
----------------------------------------------------------------------
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveContext.scala
Error:(169, 38) not found: value HiveShim
          Option(tableParameters.get(HiveShim.getStatsSetupConstTotalSize))
                                     ^
Error:(177, 31) not found: value HiveShim
          tableParameters.put(HiveShim.getStatsSetupConstTotalSize,
newTotalSize.toString)
                              ^
Error:(292, 36) not found: value HiveShim
      val proc: CommandProcessor =
HiveShim.getCommandProcessor(Array(tokens(0)), hiveconf)
                                   ^
Error:(304, 25) not found: value HiveShim
          val results = HiveShim.createDriverResultsArray
                        ^
Error:(314, 11) not found: value HiveShim
          HiveShim.processResults(results)
          ^
Error:(418, 7) not found: value HiveShim
      HiveShim.createDecimal(decimal.toBigDecimal.underlying()).toString
      ^
Error:(420, 7) not found: value HiveShim
      HiveShim.createDecimal(decimal.underlying()).toString
      ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveInspectors.scala
Error:(97, 7) not found: value HiveShim
      HiveShim.toCatalystDecimal(
      ^
Error:(123, 46) not found: value HiveShim
    case hdoi: HiveDecimalObjectInspector =>
HiveShim.toCatalystDecimal(hdoi, data)
                                             ^
Error:(156, 19) not found: value HiveShim
      (o: Any) =>
HiveShim.createDecimal(o.asInstanceOf[Decimal].toBigDecimal.underlying())
                  ^
Error:(210, 31) not found: value HiveShim
        case b: BigDecimal => HiveShim.createDecimal(b.underlying())
                              ^
Error:(211, 28) not found: value HiveShim
        case d: Decimal =>
HiveShim.createDecimal(d.toBigDecimal.underlying())
                           ^
Error:(283, 7) not found: value HiveShim
      HiveShim.getStringWritableConstantObjectInspector(value)
      ^
Error:(285, 7) not found: value HiveShim
      HiveShim.getIntWritableConstantObjectInspector(value)
      ^
Error:(287, 7) not found: value HiveShim
      HiveShim.getDoubleWritableConstantObjectInspector(value)
      ^
Error:(289, 7) not found: value HiveShim
      HiveShim.getBooleanWritableConstantObjectInspector(value)
      ^
Error:(291, 7) not found: value HiveShim
      HiveShim.getLongWritableConstantObjectInspector(value)
      ^
Error:(293, 7) not found: value HiveShim
      HiveShim.getFloatWritableConstantObjectInspector(value)
      ^
Error:(295, 7) not found: value HiveShim
      HiveShim.getShortWritableConstantObjectInspector(value)
      ^
Error:(297, 7) not found: value HiveShim
      HiveShim.getByteWritableConstantObjectInspector(value)
      ^
Error:(299, 7) not found: value HiveShim
      HiveShim.getBinaryWritableConstantObjectInspector(value)
      ^
Error:(301, 7) not found: value HiveShim
      HiveShim.getDateWritableConstantObjectInspector(value)
      ^
Error:(303, 7) not found: value HiveShim
      HiveShim.getTimestampWritableConstantObjectInspector(value)
      ^
Error:(305, 7) not found: value HiveShim
      HiveShim.getDecimalWritableConstantObjectInspector(value)
      ^
Error:(307, 7) not found: value HiveShim
      HiveShim.getPrimitiveNullWritableConstantObjectInspector
      ^
Error:(363, 51) not found: value HiveShim
    case w: WritableHiveDecimalObjectInspector =>
HiveShim.decimalTypeInfoToCatalyst(w)
                                                  ^
Error:(364, 47) not found: value HiveShim
    case j: JavaHiveDecimalObjectInspector =>
HiveShim.decimalTypeInfoToCatalyst(j)
                                              ^
Error:(393, 30) not found: value HiveShim
      case d: DecimalType => HiveShim.decimalTypeInfo(d)
                             ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala
Error:(78, 11) not found: value HiveShim
          HiveShim.getAllPartitionsOf(client, table).toSeq
          ^
Error:(205, 7) not found: value HiveShim
      HiveShim.setLocation(tbl, crtTbl)
      ^
Error:(443, 28) not found: value HiveShim
    case d: DecimalType => HiveShim.decimalMetastoreString(d)
                           ^
Error:(472, 53) not found: value HiveShim
      val totalSize =
hiveQlTable.getParameters.get(HiveShim.getStatsSetupConstTotalSize)
                                                    ^
Error:(490, 19) not found: value HiveShim
  val tableDesc = HiveShim.getTableDesc(
                  ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveUdfs.scala
Error:(255, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
Error:(73, 53) not found: type HiveFunctionWrapper
private[hive] case class HiveSimpleUdf(funcWrapper: HiveFunctionWrapper,
children: Seq[Expression])
                                                    ^
Error:(57, 25) not found: type HiveFunctionWrapper
      HiveSimpleUdf(new HiveFunctionWrapper(functionClassName), children)
                        ^
Error:(134, 54) not found: type HiveFunctionWrapper
private[hive] case class HiveGenericUdf(funcWrapper: HiveFunctionWrapper,
children: Seq[Expression])
                                                     ^
Error:(59, 26) not found: type HiveFunctionWrapper
      HiveGenericUdf(new HiveFunctionWrapper(functionClassName), children)
                         ^
Error:(185, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
Error:(62, 27) not found: type HiveFunctionWrapper
      HiveGenericUdaf(new HiveFunctionWrapper(functionClassName), children)
                          ^
Error:(214, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
Error:(64, 20) not found: type HiveFunctionWrapper
      HiveUdaf(new HiveFunctionWrapper(functionClassName), children)
                   ^
Error:(66, 27) not found: type HiveFunctionWrapper
      HiveGenericUdtf(new HiveFunctionWrapper(functionClassName), Nil,
children)
                          ^
Error:(322, 18) not found: type HiveFunctionWrapper
    funcWrapper: HiveFunctionWrapper,
                 ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveQl.scala
Error:(1132, 15) not found: type HiveFunctionWrapper
          new HiveFunctionWrapper(functionName),
              ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/InsertIntoHiveTable.scala
Error:(44, 34) object HiveShim is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.HiveShim._
                                 ^
Error:(43, 8) object ShimFileSinkDesc is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.{ ShimFileSinkDesc => FileSinkDesc}
       ^
Error:(76, 21) not found: type FileSinkDesc
      fileSinkConf: FileSinkDesc,
                    ^
Error:(142, 23) not found: value HiveShim
    val tmpLocation = HiveShim.getExternalTmpPath(hiveContext,
tableLocation)
                      ^
Error:(143, 28) not found: type FileSinkDesc
    val fileSinkConf = new FileSinkDesc(tmpLocation.toString, tableDesc,
false)
                           ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TableReader.scala
Error:(141, 22) not found: value HiveShim
      val partPath = HiveShim.getDataLocationPath(partition)
                     ^
Error:(298, 33) not found: value HiveShim
            row.update(ordinal, HiveShim.toCatalystDecimal(oi, value))
                                ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/TestHive.scala
Error:(384, 3) not found: value HiveShim
  HiveShim.createDefaultDBIfNeeded(this)
  ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/DescribeHiveTableCommand.scala
Error:(29, 8) object HiveShim is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.HiveShim
       ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/HiveTableScan.scala
Error:(89, 5) not found: value HiveShim
    HiveShim.appendReadColumns(hiveConf, neededColumnIDs,
attributes.map(_.name))
    ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/hiveWriterContainers.scala
Error:(38, 34) object HiveShim is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.HiveShim._
                                 ^
Error:(37, 8) object ShimFileSinkDesc is not a member of package
org.apache.spark.sql.hive
import org.apache.spark.sql.hive.{ShimFileSinkDesc => FileSinkDesc}
       ^
Error:(174, 19) not found: type FileSinkDesc
    fileSinkConf: FileSinkDesc,
                  ^
Error:(46, 19) not found: type FileSinkDesc
    fileSinkConf: FileSinkDesc)
                  ^
Error:(220, 33) not found: type FileSinkDesc
      val newFileSinkDesc = new FileSinkDesc(
                                ^
/home/yaochunnan/workspace/spark_source/spark-1.2.0/sql/hive/src/main/scala/org/apache/spark/sql/hive/parquet/FakeParquetSerDe.scala
Warning:(34, 2) @deprecated now takes two arguments; see the scaladoc.
@deprecated(""No code should depend on FakeParquetHiveSerDe as it is only
intended as a "" +
 ^
------------------------------------------------------------------------

*I thought it was the problem from Maven Profiles. So I tried reselecting
hbase-hadoop1 or hive or hbase-hadoop2. The error still occurs. Please help
me. This has annoyed me for a whole afternoon!*




-----
Feel the sparking Spark!
--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Sat, 17 Jan 2015 10:34:42 +0000",Re: Spark development with IntelliJ,"Chunnan Yao <yaochunnan@gmail.com>, Imran Rashid <irashid@cloudera.com>","Yes I've seen that error in the past too, and was just talking to
Imran the other day about it. I thought it only occurred when the hive
module was enabled, which I don't enable.

The problem is that the plugin that causes an error in IntelliJ for
scalac is what parses these values.* I think he got it to work with
this change: http://stackoverflow.com/questions/26788367/quasiquotes-in-intellij-14/26908554#26908554

If that works for you let's put it on the wiki.

* probably an ignorant question but is this feature important enough
to warrant the extra scala compiler plugin? the quasiquotes syntax I
mean.


---------------------------------------------------------------------


"
Chunnan Yao <yaochunnan@gmail.com>,"Sun, 18 Jan 2015 04:16:34 +0800",Re: Spark development with IntelliJ,"Imran Rashid <irashid@cloudera.com>, Sean Owen <sowen@cloudera.com>, dev@spark.apache.org","Followed is the discussion between Imran and me.

2015-01-18 4:12 GMT+08:00 Chunnan Yao <yaochunnan@gmail.com>:

"
Imran Rashid <irashid@cloudera.com>,"Sat, 17 Jan 2015 12:33:58 -0800",Re: Spark development with IntelliJ,Chunnan Yao <yaochunnan@gmail.com>,"btw, I updated the wiki to include instructions on making the
macro-paradise jar a compiler plugin for Intellij:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-IntelliJ


"
Chunnan Yao <yaochunnan@gmail.com>,"Sat, 17 Jan 2015 13:44:35 -0700 (MST)",Re: Spark development with IntelliJ,dev@spark.apache.org,"Nice! 



-----
Feel the sparking Spark!
--

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 17 Jan 2015 15:58:17 -0800",Re: Bouncing Mails,Akhil Das <akhil@sigmoidanalytics.com>,"Akhil,

Those are handled by ASF infrastructure, not anyone in the Spark
project. So this list is not the appropriate place to ask for help.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 17 Jan 2015 17:40:39 -0800",Semantics of LGTM,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just wanted to ping about a minor issue - but one that ends up having
consequence given Spark's volume of reviews and commits. As much as
possible, I think that we should try and gear towards ""Google Style""
LGTM on reviews. What I mean by this is that LGTM has the following
semantics:

""I know this code well, or I've looked at it close enough to feel
confident it should be merged. If there are issues/bugs with this code
later on, I feel confident I can help with them.""

Here is an alternative semantic:

""Based on what I know about this part of the code, I don't see any
show-stopper problems with this patch"".

The issue with the latter is that it ultimately erodes the
significance of LGTM, since subsequent reviewers need to reason about
what the person meant by saying LGTM. In contrast, having strong
semantics around LGTM can help streamline reviews a lot, especially as
reviewers get more experienced and gain trust from the comittership.

There are several easy ways to give a more limited endorsement of a patch:
- ""I'm not familiar with this code, but style, etc look good"" (general
endorsement)
- ""The build changes in this code LGTM, but I haven't reviewed the
rest"" (limited LGTM)

If people are okay with this, I might add a short note on the wiki.
I'm sending this e-mail first, though, to see whether anyone wants to
express agreement or disagreement with this approach.

- Patrick

---------------------------------------------------------------------


"
Reza Zadeh <reza@databricks.com>,"Sat, 17 Jan 2015 18:16:23 -0800",Re: Semantics of LGTM,Patrick Wendell <pwendell@gmail.com>,"LGTM


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 17 Jan 2015 18:19:28 -0800",Re: Semantics of LGTM,Reza Zadeh <reza@databricks.com>,"+1 on this.

code
as
patch:
(general


---------------------------------------------------------------------


"
sandy.ryza@cloudera.com,"Sat, 17 Jan 2015 18:59:39 -0800",Re: Semantics of LGTM,Patrick Wendell <pwendell@gmail.com>,"I think clarifying these semantics is definitely worthwhile. Maybe this complicates the process with additional terminology, but the way I've used these has been:

+1 - I think this is safe to merge and, barring objections from others, would merge it immediately.

LGTM - I have no concerns about this patch, but I don't necessarily feel qualified to make a final call about it.  The TM part acknowledges the judgment as a little more subjective.

I think having some concise way to express both of these is useful.

-Sandy



---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Sun, 18 Jan 2015 08:29:34 +0530",Re: Bouncing Mails,Patrick Wendell <pwendell@gmail.com>,"Yep. They have sorted it out it seems.

"
Aaron Davidson <ilikerps@gmail.com>,"Sat, 17 Jan 2015 19:09:33 -0800",Re: Semantics of LGTM,sandy.ryza@cloudera.com,"I think I've seen something like +2 = ""strong LGTM"" and +1 = ""weak LGTM;
someone else should review"" before. It's nice to have a shortcut which
isn't a sentence when talking about weaker forms of LGTM.


"
Patrick Wendell <pwendell@gmail.com>,"Sat, 17 Jan 2015 19:25:37 -0800",Re: Semantics of LGTM,Aaron Davidson <ilikerps@gmail.com>,"I think the ASF +1 is *slightly* different than Google's LGTM, because
it might convey wanting the patch/feature to be merged but not
necessarily saying you did a thorough review and stand behind it's
technical contents. For instance, I've seen people pile on +1's to try
and indicate support for a feature or patch in some projects, even
though they didn't do a thorough technical review. This +1 is
definitely a useful mechanism.

There is definitely much overlap though in the meaning, though, and
it's largely because Spark had it's own culture around reviews before
it was donated to the ASF, so there is a mix of two styles.

Nonetheless, I'd prefer to stick with the stronger LGTM semantics I
proposed originally (unlike the one Sandy proposed, e.g.). This is
what I've seen every project using the LGTM convention do (Google, and
some open source projects such as Impala) to indicate technical
sign-off.

- Patrick


---------------------------------------------------------------------


"
sandy.ryza@cloudera.com,"Sat, 17 Jan 2015 20:04:39 -0800",Re: Semantics of LGTM,Patrick Wendell <pwendell@gmail.com>,"Yeah, the ASF +1 has become partly overloaded to mean both ""I would like to see this feature"" and ""this patch should be committed"", although, at least in Hadoop, using +1 on JIRA (as opposed to, say, in a release vote) should unambiguously mean the latter unless qualified in some other way.

I don't have any opinion on the specific characters, but I agree with Aaron that it would be nice to have some sort of abbreviation for both the strong and weak forms of approval.

-Sandy

e:
TM;
n't
ed

:

---------------------------------------------------------------------


"
Kay Ousterhout <keo@eecs.berkeley.edu>,"Sat, 17 Jan 2015 20:49:35 -0800",Re: Semantics of LGTM,Sandy Ryza <sandy.ryza@cloudera.com>,"heard the semantics of ""LGTM"" expressed as ""I've looked at this thoroughly
and take as much ownership as if I wrote the patch myself"".  My
understanding is that this is the level of review we expect for all patches
that ultimately go into Spark, so it's important to have a way to concisely
describe when this has been done.

Aaron / Sandy, when have you found the weaker LGTM to be useful?  In the
cases I've seen, if someone else says ""I looked at this very quickly and
didn't see any glaring problems"", it doesn't add any value for subsequent
reviewers (someone still needs to take a thorough look).

-Kay


"
Gil Vernik <GILV@il.ibm.com>,"Sun, 18 Jan 2015 08:26:38 +0200","run time exceptions in Spark 1.2.0 manual build together with OpenStack
 hadoop driver",dev <dev@spark.apache.org>,"Hi,

I took a source code of Spark 1.2.0 and tried to build it together with 
hadoop-openstack.jar ( To allow Spark an access to OpenStack Swift )
I used Hadoop 2.6.0.

The build was fine without problems, however in run time, while trying to 
access ""swift://"" name space i got an exception:
java.lang.NoClassDefFoundError: org/codehaus/jackson/annotate/JsonClass
                 at 
org.codehaus.jackson.map.introspect.JacksonAnnotationIntrospector.findDeserializationType(JacksonAnnotationIntrospector.java:524)
                 at 
org.codehaus.jackson.map.deser.BasicDeserializerFactory.modifyTypeByAnnotation(BasicDeserializerFactory.java:732)
                ...and the long stack trace goes here

Digging into the problem i saw the following:
Jackson versions 1.9.X are not backward compatible, in particular they 
removed JsonClass annotation.
Hadoop 2.6.0 uses jackson-asl version 1.9.13, while Spark has reference to 
older version of jackson.

This is the main  pom.xml of Spark 1.2.0 :

      <dependency>
        <!-- Matches the version of jackson-core-asl pulled in by avro -->
        <groupId>org.codehaus.jackson</groupId>
        <artifactId>jackson-mapper-asl</artifactId>
        <version>1.8.8</version>
      </dependency>

Referencing 1.8.8 version, which is not compatible with Hadoop 2.6.0 .
If we change version to 1.9.13, than all will work fine and there will be 
no run time exceptions while accessing Swift. The following change will 
solve the problem:

      <dependency>
        <!-- Matches the version of jackson-core-asl pulled in by avro -->
        <groupId>org.codehaus.jackson</groupId>
        <artifactId>jackson-mapper-asl</artifactId>
        <version>1.9.13</version>
      </dependency>

I am trying to resolve this somehow so people will not get into this 
issue.
Is there any particular need in Spark for jackson 1.8.8 and not 1.9.13?
Can we remove 1.8.8 and put 1.9.13 for Avro? 
It looks to me that all works fine when Spark build with jackson 1.9.13, 
but i am not an expert and not sure what should be tested.

Thanks,
Gil Vernik.
"
Reynold Xin <rxin@databricks.com>,"Sun, 18 Jan 2015 01:03:51 -0800",Re: Semantics of LGTM,Kay Ousterhout <keo@eecs.berkeley.edu>,"Maybe just to avoid LGTM as a single token when it is not actually
according to Patrick's definition, but anybody can still leave comments
like:

""The direction of the PR looks good to me."" or ""+1 on the direction""

""The build part looks good to me""

...



"
Ted Yu <yuzhihong@gmail.com>,"Sun, 18 Jan 2015 07:41:56 -0800","Re: run time exceptions in Spark 1.2.0 manual build together with
 OpenStack hadoop driver",Gil Vernik <GILV@il.ibm.com>,"Please tale a look at SPARK-4048 and SPARK-5108

Cheers


"
Sean Owen <sowen@cloudera.com>,"Sun, 18 Jan 2015 18:20:59 +0000","Re: run time exceptions in Spark 1.2.0 manual build together with
 OpenStack hadoop driver",Ted Yu <yuzhihong@gmail.com>,"Agree, I think this can / should be fixed with a slightly more
conservative version of https://github.com/apache/spark/pull/3938
related to SPARK-5108.


---------------------------------------------------------------------


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Mon, 19 Jan 2015 04:34:02 +0000 (UTC)",GraphX doc: triangleCount() requirement overstatement?,"""dev@spark.apache.org"" <dev@spark.apache.org>","According to:
https://spark.apache.org/docs/1.2.0/graphx-programming-guide.html#triangle-counting 

""Note that TriangleCount requires the edges to be in canonical orientation (srcId < dstId)""

But isn't this overstating the requirement? Isn't the requirement really that IF there are duplicate edges between two vertices, THEN those edges must all be in the same direction (in order for the groupEdges() at the beginning of triangleCount() to produce the intermediate results that triangleCount() expects)?

If so, should I enter a JIRA ticket to clarify the documentation?

Or is it the case that https://issues.apache.org/jira/browse/SPARK-3650 will make it into Spark 1.3 anyway?

---------------------------------------------------------------------


"
Alessandro Baretta <alexbaretta@gmail.com>,"Sun, 18 Jan 2015 22:06:20 -0800",Memory config issues,"""dev@spark.apache.org"" <dev@spark.apache.org>"," All,

I'm getting out of memory exceptions in SparkSQL GROUP BY queries. I have
plenty of RAM, so I should be able to brute-force my way through, but I
can't quite figure out what memory option affects what process.

My current memory configuration is the following:
export SPARK_WORKER_MEMORY=83971m
export SPARK_DAEMON_MEMORY=15744m

What does each of these config options do exactly?

Also, how come the executors page of the web UI shows no memory usage:

0.0 B / 42.4 GB

And where does 42.4 GB come from?

Alex
"
Reynold Xin <rxin@databricks.com>,"Sun, 18 Jan 2015 22:10:52 -0800",Re: GraphX doc: triangleCount() requirement overstatement?,Michael Malak <michaelmalak@yahoo.com>,"We will merge https://issues.apache.org/jira/browse/SPARK-3650  for 1.3.
Thanks for reminding!



"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 19 Jan 2015 11:59:31 +0530",Re: Memory config issues,Alessandro Baretta <alexbaretta@gmail.com>,"Its the executor memory (spark.executor.memory) which you can set while
creating the spark context. By default it uses 0.6% of the executor memory
for Storage. Now, to show some memory usage, you need to cache (persist)
the RDD. Regarding the OOM Exception, you can increase the level of
parallelism (also you can increase the number of partitions depending on
your data size) and it should be fine.

Thanks
Best Regards


"
Alessandro Baretta <alexbaretta@gmail.com>,"Sun, 18 Jan 2015 22:54:20 -0800",Re: Memory config issues,Akhil Das <akhil@sigmoidanalytics.com>,"Akhil,

Ah, very good point. I guess ""SET spark.sql.shuffle.partitions=1024"" should
do it.

Alex


"
Reynold Xin <rxin@databricks.com>,"Sun, 18 Jan 2015 23:39:13 -0800",Re: RDD order guarantees,Ewan Higgs <ewan.higgs@ugent.be>,"Hi Ewan,

Not sure if there is a JIRA ticket (there are too many that I lose track).

I chatted briefly with Aaron on this. The way we can solve it is to create
a new FileSystem implementation that overrides the listStatus method, and
then in Hadoop Conf set the fs.file.impl to that.

Shouldn't be too hard. Would you be interested in working on it?





"
Yi Tian <tianyi.asiainfo@gmail.com>,"Mon, 19 Jan 2015 17:00:56 +0800","Is there any way to support multiple users executing SQL on thrift
 server?","user@spark.apache.org, dev@spark.apache.org","Is there any way to support multiple users executing SQL on one thrift 
server?

I think there are some problems for spark 1.2.0, for example:

 1. Start thrift server with user A
 2. Connect to thrift server via beeline with user B
 3. Execute ‚Äúinsert into table dest select ‚Ä¶ from table src‚Äù

then we found these items on hdfs:

|drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000
drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary
drwxr-xr-x   - B supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0
drwxr-xr-x   - A supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/_temporary
drwxr-xr-x   - A supergroup          0 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000
-rw-r--r--   3 A supergroup       2671 2015-01-16 16:42 /tmp/hadoop/hive_2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000/part-00000
|

You can see all the temporary path created on driver side (thrift server 
side) is owned by user B (which is what we expected).

But all the output data created on executor side is owned by user A, 
(which is NOT what we expected).
error owner of the output data cause 
|org.apache.hadoop.security.AccessControlException| while the driver 
side moving output data into |dest| table.

Is anyone know how to resolve this problem?

‚Äã
"
Mick Davies <michael.belldavies@gmail.com>,"Mon, 19 Jan 2015 02:04:14 -0700 (MST)",Re: Optimize encoding/decoding strings when using Parquet,dev@spark.apache.org,"Added a JIRA to track
https://issues.apache.org/jira/browse/SPARK-5309



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 19 Jan 2015 09:20:42 +0000",Re: Memory config issues,Akhil Das <akhil@sigmoidanalytics.com>,"
(Uses 0.6 or 60%)

---------------------------------------------------------------------


"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 19 Jan 2015 16:10:21 +0530",Re: Semantics of LGTM,Reynold Xin <rxin@databricks.com>,"Patrick's original proposal LGTM :).  However until now, I have been in the
impression of LGTM with special emphasis on TM part. That said, I will be
okay/happy(or Responsible ) for the patch, if it goes in.

Prashant Sharma




"
Romi Kuntsman <romi@totango.com>,"Mon, 19 Jan 2015 15:59:36 +0200",Re: Spark client reconnect to driver in yarn-cluster deployment mode,preeze <etander@gmail.com>,"""in yarn-client mode it only controls the environment of the executor
launcher""

So you either use yarn-client mode, and then your app keeps running and
controlling the process
Or you use yarn-cluster mode, and then you send a jar to YARN, and that jar
should have code to report the result back to you

*Romi Kuntsman*, *Big Data Engineer*
 http://www.totango.com


"
Mick Davies <michael.belldavies@gmail.com>,"Mon, 19 Jan 2015 07:38:44 -0700 (MST)",Re: Optimize encoding/decoding strings when using Parquet,dev@spark.apache.org,"Here are some timings showing effect of caching last Binary->String
conversion. Query times are reduced significantly and variation in timings
due to reduction in garbage is very significant.

Set of sample queries selecting various columns, applying some filtering and
then aggregating

Spark 1.2.0
Query 1 mean time 8353.3 millis, std deviation 480.91511147441025 millis
Query 2 mean time 8677.6 millis, std deviation 3193.345518417949 millis
Query 3 mean time 11302.5 millis, std deviation 2989.9406998950476 millis
Query 4 mean time 10537.0 millis, std deviation 5166.024024549462 millis
Query 5 mean time 9559.9 millis, std deviation 4141.487667493409 millis
Query 6 mean time 12638.1 millis, std deviation 3639.4505522430477 millis


Spark 1.2.0 - cache last Binary->String conversion
Query 1 mean time 5118.9 millis, std deviation 549.6670608448152 millis
Query 2 mean time 3761.3 millis, std deviation 202.57785883183013 millis
Query 3 mean time 7358.8 millis, std deviation 242.58918176850162 millis
Query 4 mean time 4173.5 millis, std deviation 179.802515122688 millis
Query 5 mean time 3857.0 millis, std deviation 140.71957930579526 millis
Query 6 mean time 7512.0 millis, std deviation 198.32633040858022 millis




--

---------------------------------------------------------------------


"
Ewan Higgs <ewan.higgs@ugent.be>,"Mon, 19 Jan 2015 16:04:28 +0100",Re: RDD order guarantees,Reynold Xin <rxin@databricks.com>,"Hi Reynold.
I'll take a look.

SPARK-5300 is open for this issue.
-Ewan


"
Mick Davies <michael.belldavies@gmail.com>,"Mon, 19 Jan 2015 11:10:01 -0700 (MST)",Re: Optimize encoding/decoding strings when using Parquet,dev@spark.apache.org,"
Looking at Parquet code - it looks like hooks are already in place to
support this.

In particular PrimitiveConverter has methods hasDictionarySupport and
addValueFromDictionary for this purpose. These are not used by
CatalystPrimitiveConverter.

I think that it would be pretty straightforward to add this. Has anyone
considered this? Shall I get a pull request  together for it.

Mick



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 19 Jan 2015 10:31:00 -0800",Re: Optimize encoding/decoding strings when using Parquet,Mick Davies <michael.belldavies@gmail.com>,"Definitely go for a pull request!



"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 Jan 2015 12:00:42 -0800",Re: Semantics of LGTM,Prashant Sharma <scrapcodes@gmail.com>,"Okay - so given all this I was going to put the following on the wiki
tentatively:

## Reviewing Code
Community code review is Spark's fundamental quality assurance
process. When reviewing a patch, your goal should be to help
streamline the committing process by giving committers confidence this
patch has been verified by an additional party. It's encouraged to
(politely) submit technical feedback to the author to identify areas
for improvement or potential bugs.

If you feel a patch is ready for inclusion in Spark, indicate this to
committers with a comment: ""I think this patch looks good"". Spark uses
the LGTM convention for indicating the highest level of technical
sign-off on a patch: simply comment with the word ""LGTM"". An LGTM is a
strong statement, it should be interpreted as the following: ""I've
looked at this thoroughly and take as much ownership as if I wrote the
patch myself"". If you comment LGTM you will be expected to help with
bugs or follow-up issues on the patch. Judicious use of LGTM's is a
great way to gain credibility as a reviewer with the broader
community.

It's also welcome for reviewers to argue against the inclusion of a
feature or patch. Simply indicate this in the comments.

- Patrick


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 19 Jan 2015 12:03:44 -0800",Re: Semantics of LGTM,Prashant Sharma <scrapcodes@gmail.com>,"The wiki does not seem to be operational ATM, but I will do this when
it is back up.


---------------------------------------------------------------------


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Mon, 19 Jan 2015 20:20:14 +0000 (UTC)",GraphX vertex partition/location strategy,"""dev@spark.apache.org"" <dev@spark.apache.org>","Does GraphX make an effort to co-locate vertices onto the same workers as the majority (or even some) of its edges?

---------------------------------------------------------------------


"
Ankur Dave <ankurdave@gmail.com>,"Mon, 19 Jan 2015 13:08:37 -0800",Re: GraphX vertex partition/location strategy,Michael Malak <michaelmalak@yahoo.com>,"No - the vertices are hash-partitioned onto workers independently of the
edges. It would be nice for each vertex to be on the worker with the most
adjacent edges, but we haven't done this yet since it would add a lot of
complexity to avoid load imbalance while reducing the overall communication
by a small factor.

We refer to the number of partitions containing adjacent edges for a
particular vertex as the vertex's replication factor. I think the typical
replication factor for power-law graphs with 100-200 partitions is 10-15,
and placing the vertex at the ideal location would only reduce the
replication factor by 1.

Ankur <http://www.ankurdave.com/>


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Mon, 19 Jan 2015 21:31:16 +0000 (UTC)",Re: GraphX vertex partition/location strategy,Ankur Dave <ankurdave@gmail.com>,"But wouldn't the gain be greater under something similar to EdgePartition1D (but perhaps better load-balanced based on number of edges for each vertex) and an algorithm that primarily follows edges in the forward direction?
      From: Ankur Dave <ankurdave@gmail.com>
 To: Michael Malak <michaelmalak@yahoo.com> 
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Monday, January 19, 2015 2:08 PM
 Subject: Re: GraphX vertex partition/location strategy
   
No - the vertices are hash-partitioned onto workers independently of the edges. It would be nice for each vertex to be on the worker with the most adjacent edges, but we haven't done this yet since it would add a lot of complexity to avoid load imbalance while reducing the overall communication by a small factor.
We refer to the number of partitions containing adjacent edges for a particular vertex as the vertex's replication factor. I think the typical replication factor for power-law graphs with 100-200 partitions is 10-15, and placing the vertex at the ideal location would only reduce the replication factor by 1.

Ankur



Does GraphX make an effort to co-locate vertices onto the same workers as the majority (or even some) of its edges?



   "
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Tue, 20 Jan 2015 00:09:01 +0000 (UTC)",GraphX ShortestPaths backwards?,"""dev@spark.apache.org"" <dev@spark.apache.org>","GraphX ShortestPaths seems to be following edges backwards instead of forwards:

import org.apache.spark.graphx._
val g = Graph(sc.makeRDD(Array((1L,""""), (2L,""""), (3L,""""))), sc.makeRDD(Array(Edge(1L,2L,""""), Edge(2L,3L,""""))))

lib.ShortestPaths.run(g,Array(3)).vertices.collect
res1: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map()), (3,Map(3 -> 0)), (2,Map()))

lib.ShortestPaths.run(g,Array(1)).vertices.collect

res2: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map(1 -> 0)), (3,Map(1 -> 2)), (2,Map(1 -> 1)))

If I am not mistaken about my assessment, then I believe the following changes will make it run ""forward"":

Change one occurrence of ""src"" to ""dst"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L64

Change three occurrences of ""dst"" to ""src"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L65

---------------------------------------------------------------------


"
Xuelin Cao <xuelincao2014@gmail.com>,"Tue, 20 Jan 2015 10:27:08 +0800",Will Spark-SQL support vectorized query engine someday?,dev@spark.apache.org,"Hi,

     Correct me if I were wrong. It looks like, the current version of
Spark-SQL is *tuple-at-a-time* module. Basically, each time the physical
operator produces a tuple by recursively call child->execute .

     There are papers that illustrate the benefits of vectorized query
engine. And Hive-Stinger also embrace this style.

     So, the question is, will Spark-SQL give a support to vectorized query
execution someday?

     Thanks
"
Jeff Wang <jingjingwang929@gmail.com>,"Mon, 19 Jan 2015 21:44:08 -0500",Join the developer community of spark,dev@spark.apache.org,"Hi:

I would like to contribute to the code of spark. Can I join the community?

Thanks,

Jeff
"
Alessandro Baretta <alexbaretta@gmail.com>,"Mon, 19 Jan 2015 18:53:02 -0800",Re: Join the developer community of spark,Jeff Wang <jingjingwang929@gmail.com>,"https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

Enjoy!

Alex


"
Reynold Xin <rxin@databricks.com>,"Mon, 19 Jan 2015 23:55:32 -0800",Re: Will Spark-SQL support vectorized query engine someday?,Xuelin Cao <xuelincao2014@gmail.com>,"It will probably eventually make its way into part of the query engine, one
way or another. Note that there are in general a lot of other lower hanging
fruits before you have to do vectorization.

As far as I know, Hive doesn't really have vectorization because the
vectorization in Hive is simply writing everything in small batches, in
order to avoid the virtual function call overhead, and hoping the JVM can
unroll some of the loops. There is no SIMD involved.

Something that is pretty useful, which isn't exactly from vectorization but
comes from similar lines of research, is being able to push predicates down
into the columnar compression encoding. For example, one can turn string
comparisons into integer comparisons. These will probably give much larger
performance improvements in common queries.



"
Xuelin Cao <xuelincao2014@gmail.com>,"Tue, 20 Jan 2015 16:30:21 +0800",Re: Will Spark-SQL support vectorized query engine someday?,Reynold Xin <rxin@databricks.com>,"Thanks, Reynold

      Regarding the ""lower hanging fruits"", can you give me some example?
Where can I find them in JIRA?



"
Reynold Xin <rxin@databricks.com>,"Tue, 20 Jan 2015 00:34:06 -0800",Re: Will Spark-SQL support vectorized query engine someday?,Xuelin Cao <xuelincao2014@gmail.com>,"I don't know if there is a list, but in general running performance
profiler can identify a lot of things...


"
James <alcaid1801@gmail.com>,"Tue, 20 Jan 2015 20:35:07 +0800",not found: type LocalSparkContext,dev@spark.apache.org,"Hi all,

When I was trying to write a test on my spark application I met

```
Error:(14, 43) not found: type LocalSparkContext
class HyperANFSuite extends FunSuite with LocalSparkContext {
```

At the source code of spark-core I could not found ""LocalSparkContext"",
thus I wonder how to write a test like [this] (
https://github.com/apache/spark/blob/master/graphx/src/test/scala/org/apache/spark/graphx/lib/ConnectedComponentsSuite.scala
)

Alcaid
"
Will Benton <willb@redhat.com>,"Tue, 20 Jan 2015 13:05:01 -0500 (EST)",Re: not found: type LocalSparkContext,James <alcaid1801@gmail.com>,"It's declared here:

  https://github.com/apache/spark/blob/master/core/src/test/scala/org/apache/spark/LocalSparkContext.scala

I assume you're already importing LocalSparkContext, but since the test classes aren't included in Spark packages, you'll also need to package them up in order to use them in your application (viz., outside of Spark).



best,
wb

X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id CB69817743
	for <apmail-spark-dev-archive@minotaur.apache.org>; Tue, 20 Jan 2015 18:50:07 +0000 (UTC)
Received: (qmail 64400 invoked by uid 500); 20 Jan 2015 18:50:06 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 64324 invoked by uid 500); 20 Jan 2015 18:50:06 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 64313 invoked by uid 99); 20 Jan 2015 18:50:06 -0000
Received: from nike.apache.org (HELO nike.apache.org) (192.87.106.230)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 18:50:06 +0000
X-ASF-Spam-Status: No, hits=2.8 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,URI_HEX
X-Spam-Check-By: apache.org
Received-SPF: error (nike.apache.org: local policy)
Received: from [209.85.215.45] (HELO mail-la0-f45.google.com) (209.85.215.45)
    by apache.org (qpsmtpd/0.29) with ESMTP; Tue, 20 Jan 2015 18:49:41 +0000
Received: by mail-la0-f45.google.com with SMTP id gd6so12365995lab.4
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 10:47:04 -0800 (PST)
X-Google-DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=1e100.net; s=20130820;
        h=x-gm-message-state:mime-version:in-reply-to:references:date
         :message-id:subject:from:to:cc:content-type;
        bh=RyIgvEPxdA9Mp6F2oBolhipwgstCS+dWJPm2P5I6upg=;
        b=eHtJ4Bke11aP7SG4u2O8uV9/iz5RrCM38hXLgqvIUfdx9M9+mfESM45w6FR/a8wt0r
         5s4CdHpPD0VtGcskkcpzHJCWVDeO2hdlc7Q6RO7b6USF5Pef0MV9EMQnPJC31Q7oltgl
         j+B9arQZUWOfrYtqw44Aof/GIffbEO3DMuIYEpHEDdFVIckx7O4tPHJDE14Dki6Y5RsF
         ow1DmTDwB9BDD2uh5N+Zw/6xpehN+fo+b9lzcEgjEYtJ3mk/mh8bSynxfaMv1OE14neQ
         L4FAbzP4oc8MRrgTjK6JUbqOsqg2sw7dPyqkryTUApn7QWSYPWS4UB1QXVP/TOXSNk1W
         0+Pg==
X-Gm-Message-State: ALoCoQnE7OsS0lx7p1T36R7HhbLVWu2mbwRvPZ+DNoOCfNvV3QAX4E8U5ATFixyPvlUBDPGMBldP
MIME-Version: 1.0
X-Received: by 10.152.224.161 with SMTP id rd1mr12103219lac.86.1421779624379;
 Tue, 20 Jan 2015 10:47:04 -0800 (PST)
Received: by 10.113.4.10 with HTTP; Tue, 20 Jan 2015 10:47:04 -0800 (PST)
In-Reply-To: <CACNcKkEQLUq90_JzvwDZQcZGqagjWKgZOaTRscApL10RGX+_Aw@mail.gmail.com>
References: <1421322724150-10122.post@n3.nabble.com>
	<CACNcKkEQLUq90_JzvwDZQcZGqagjWKgZOaTRscApL10RGX+_Aw@mail.gmail.com>
Date: Tue, 20 Jan 2015 10:47:04 -0800
Message-ID: <CAMJOb8=1j7oM=dPYPsk6ba9TajWSch0jf-guHYiHx0sCQP5KmA@mail.gmail.com>
Subject: Re: Spark client reconnect to driver in yarn-cluster deployment mode
From: Andrew Or <andrew@databricks.com>
To: Romi Kuntsman <romi@totango.com>
Cc: preeze <etander@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=001a113364f675e07a050d19ddff
X-Virus-Checked: Checked by ClamAV on apache.org

--001a113364f675e07a050d19ddff
Content-Type: text/plain; charset=UTF-8

Hi Preeze,

(still running in YARN) for collecting results at a later stage?

No, there is not support built into Spark for this. For this to happen
seamlessly the driver will have to start a server (pull model) or send the
results to some other server once the jobs complete (push model), both of
which add complexity to the driver. Alternatively, you can just poll on the
output files that your application produces; e.g. you can have your driver
write the results of a count to a file and poll on that file. Something
like that.

-Andrew

2015-01-19 5:59 GMT-08:00 Romi Kuntsman <romi@totango.com>:


--001a113364f675e07a050d19ddff--

"
Cheng Lian <lian.cs.zju@gmail.com>,"Tue, 20 Jan 2015 11:30:04 -0800","Re: Is there any way to support multiple users executing SQL on thrift
 server?","Yi Tian <tianyi.asiainfo@gmail.com>, user@spark.apache.org, 
 dev@spark.apache.org","Hey Yi,

I'm quite unfamiliar with Hadoop/HDFS auth mechanisms for now, but would 
like to investigate this issue later. Would you please open an JIRA for 
it? Thanks!

Cheng


"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 20 Jan 2015 12:56:32 -0800",Re: Spectral clustering,Andrew Musselman <andrew.musselman@gmail.com>,"Fan and Stephen (cc'ed) are working on this feature. They will update
the JIRA page and report progress soon. -Xiangrui


---------------------------------------------------------------------


"
Andrew Musselman <andrew.musselman@gmail.com>,"Tue, 20 Jan 2015 13:03:05 -0800",Re: Spectral clustering,Xiangrui Meng <mengxr@gmail.com>,"Awesome, thanks


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 21 Jan 2015 00:13:31 +0000",Standardized Spark dev environment,Spark dev list <dev@spark.apache.org>,"What do y'all think of creating a standardized Spark development
environment, perhaps encoded as a Vagrantfile, and publishing it under
`dev/`?

The goal would be to make it easier for new developers to get started with
all the right configs and tools pre-installed.

If we use something like Vagrant, we may even be able to make it so that a
single Vagrantfile creates equivalent development environments across OS X,
Linux, and Windows, without having to do much (or any) OS-specific work.

I imagine for committers and regular contributors, this exercise may seem
pointless, since y'all are probably already very comfortable with your
workflow.

I wonder, though, if any of you think this would be worthwhile as a
improvement to the ""new Spark developer"" experience.

Nick
"
shenyan zhen <shenyanls@gmail.com>,"Tue, 20 Jan 2015 19:29:06 -0500",Re: Standardized Spark dev environment,Nicholas Chammas <nicholas.chammas@gmail.com>,"Great suggestion.

"
Ted Yu <yuzhihong@gmail.com>,"Tue, 20 Jan 2015 16:35:16 -0800",Re: Standardized Spark dev environment,Nicholas Chammas <nicholas.chammas@gmail.com>,"How many profiles (hadoop / hive /scala) would this development environment
support ?

Cheers


"
Sean Owen <sowen@cloudera.com>,"Wed, 21 Jan 2015 01:08:38 +0000",Re: Standardized Spark dev environment,Nicholas Chammas <nicholas.chammas@gmail.com>,"My concern would mostly be maintenance. It adds to an already very complex
build. It only assists developers who are a small audience. What does this
provide, concretely?

"
<nate@reactor8.com>,"Tue, 20 Jan 2015 17:12:24 -0800",RE: Standardized Spark dev environment,"""'dev'"" <dev@spark.apache.org>","If there is some interest in more standardization and setup of dev/test environments spark community might be interested in starting to participate in apache bigtop effort:

http://bigtop.apache.org/

While the project had its start and initial focus on packaging, testing, deploying Hadoop/hdfs related stack its looking like we will be targeting ""data engineers"" going forward, thus spark is looking to become bigger central piece to bigtop effort as the project moves towards a ""v1"" release.

We will be doing a bigtop/bigdata workshop late Feb at the SocalLinux Conference:

http://www.socallinuxexpo.org/scale/13x

Right now scoping some content that will be getting started spark related for the event, targeted intro of bigtop/spark puppet powered deployment components going into the event as well.

Also the group will be holding a meetup at Amazon's Palo Alto office on Jan 27th if any folks are interested.

Nate

My concern would mostly be maintenance. It adds to an already very complex build. It only assists developers who are a small audience. What does this provide, concretely?
<nicholas.chammas@gmail.com>


OS-specific work.



---------------------------------------------------------------------


"
Will Benton <willb@redhat.com>,"Tue, 20 Jan 2015 20:22:38 -0500 (EST)",Re: Standardized Spark dev environment,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hey Nick,

I did something similar with a Docker image last summer; I haven't updated the images to cache the dependencies for the current Spark master, but it would be trivial to do so:

http://chapeau.freevariable.com/2014/08/jvm-test-docker.html


best,
wb


X-Original-To: apmail-spark-dev-archive@minotaur.apache.org
Delivered-To: apmail-spark-dev-archive@minotaur.apache.org
Received: from mail.apache.org (hermes.apache.org [140.211.11.3])
	by minotaur.apache.org (Postfix) with SMTP id AA51417B3A
	for <apmail-spark-dev-archive@minotaur.apache.org>; Wed, 21 Jan 2015 03:21:52 +0000 (UTC)
Received: (qmail 23907 invoked by uid 500); 21 Jan 2015 03:21:50 -0000
Delivered-To: apmail-spark-dev-archive@spark.apache.org
Received: (qmail 23832 invoked by uid 500); 21 Jan 2015 03:21:50 -0000
Mailing-List: contact dev-help@spark.apache.org; run by ezmlm
Precedence: bulk
List-Help: <mailto:dev-help@spark.apache.org>
List-Unsubscribe: <mailto:dev-unsubscribe@spark.apache.org>
List-Post: <mailto:dev@spark.apache.org>
List-Id: <dev.spark.apache.org>
Delivered-To: mailing list dev@spark.apache.org
Received: (qmail 23820 invoked by uid 99); 21 Jan 2015 03:21:50 -0000
Received: from athena.apache.org (HELO athena.apache.org) (140.211.11.136)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:21:50 +0000
X-ASF-Spam-Status: No, hits=1.5 required=10.0
	tests=HTML_MESSAGE,RCVD_IN_DNSWL_LOW,SPF_PASS
X-Spam-Check-By: apache.org
Received-SPF: pass (athena.apache.org: domain of nicholas.chammas@gmail.com designates 209.85.213.182 as permitted sender)
Received: from [209.85.213.182] (HELO mail-ig0-f182.google.com) (209.85.213.182)
    by apache.org (qpsmtpd/0.29) with ESMTP; Wed, 21 Jan 2015 03:21:46 +0000
Received: by mail-ig0-f182.google.com with SMTP id z20so10778908igj.3
        for <dev@spark.apache.org>; Tue, 20 Jan 2015 19:21:25 -0800 (PST)
DKIM-Signature: v=1; a=rsa-sha256; c=relaxed/relaxed;
        d=gmail.com; s=20120113;
        h=mime-version:references:from:date:message-id:subject:to:cc
         :content-type;
        bh=rpt+wLvdpYkhaQqUjGCF0cidr3jY9fkM4LX3mVvo4Dg=;
        b=YF0gO/0z4neYU8hOYiKxVScukvi9RhQZ1nUBL2ZCb+/E5LedvtbAeE8mBb3ee80RYO
         wKWEFX03Mv8SL381RmJwDqDe+j/iLkEJ1jR4KY2nEcs9otgrGg4iYJbY8RteZ0dpigb6
         eYkj2xqAHkY7WNqeiXkpTU84Dq8EoudWJF0VKoZzcJS+GrCyq9aeonJlLFXIT8YiQh6z
         xGKPPa+Gx6uWVWVt2iphiYdCNhkhbR4cGYAIfIJo6Xzn1nX035p2lfOkJ3FaHNqOmb28
         jnhxldwmykzkcJR+KplUaaTS2DMtdzzC3GCGyaFP4ec7MxYvozqoy00opzNrAaL/EIhv
         Fd9w==
X-Received: by 10.42.169.197 with SMTP id c5mr37475982icz.72.1421810485475;
 Tue, 20 Jan 2015 19:21:25 -0800 (PST)
MIME-Version: 1.0
References: <CAOhmDzcjE6y8WMaaB0Sn5aO2o6R=KpMrfDNWS8viDOQ5h=-EPQ@mail.gmail.com>
 <243108128.13409171.1421803358312.JavaMail.zimbra@redhat.com>
From: Nicholas Chammas <nicholas.chammas@gmail.com>
Date: Wed, 21 Jan 2015 03:21:24 +0000
Message-ID: <CAOhmDzcVo9Bbieb-AS8pFacLDJSpd--JuaMd=Tsg25ot=SMx5Q@mail.gmail.com>
Subject: Re: Standardized Spark dev environment
To: Will Benton <willb@redhat.com>
Cc: Spark dev list <dev@spark.apache.org>
Content-Type: multipart/alternative; boundary=90e6ba6e8a5cecc0b3050d210c27
X-Virus-Checked: Checked by ClamAV on apache.org

--90e6ba6e8a5cecc0b3050d210c27
Content-Type: text/plain; charset=UTF-8
Content-Transfer-Encoding: quoted-printable

How many profiles (hadoop / hive /scala) would this development environment
support ?

As many as we want. We probably want to cover a good chunk of the build
matrix <https://issues.apache.org/jira/browse/SPARK-2004> that Spark
officially supports.

What does this provide, concretely?

It provides a reliable way to create a =E2=80=9Cgood=E2=80=9D Spark develop=
ment
environment. Roughly speaking, this probably should mean an environment
that matches Jenkins, since that=E2=80=99s where we run =E2=80=9Cofficial=
=E2=80=9D testing and
builds.

For example, Spark has to run on Java 6 and Python 2.6. When devs build and
run Spark locally, we can make sure they=E2=80=99re doing it on these versi=
ons of
the languages with a simple vagrant up.

Nate, could you comment on how something like this would relate to the
Bigtop effort?

http://chapeau.freevariable.com/2014/08/jvm-test-docker.html

Will, that=E2=80=99s pretty sweet. I tried something similar a few months a=
go as an
experiment to try building/testing Spark within a container. Here=E2=80=99s=
 the
shell script I used <https://gist.github.com/nchammas/60b04141f3b9f053faaa>
against the base CentOS Docker image to setup an environment ready to build
and test Spark.

We want to run Spark unit tests within containers on Jenkins, so it might
make sense to develop a single Docker image that can be used as both a =E2=
=80=9Cdev
environment=E2=80=9D as well as execution container on Jenkins.

Perhaps that=E2=80=99s the approach to take instead of looking into Vagrant=
.

Nick


Hey Nick,
d
t
S
.
em
=E2=80=8B

--90e6ba6e8a5cecc0b3050d210c27--

"
James <alcaid1801@gmail.com>,"Wed, 21 Jan 2015 11:27:38 +0800",Re: not found: type LocalSparkContext,Will Benton <willb@redhat.com>,"I could not correctly import org.apache.spark.LocalSparkContext,

I use sbt on Intellij for developing,here is my build sbt.

```
libraryDependencies += ""org.apache.spark"" %% ""spark-core"" % ""1.2.0""

libraryDependencies += ""org.apache.spark"" %% ""spark-graphx"" % ""1.2.0""

libraryDependencies += ""com.clearspring.analytics"" % ""stream"" % ""2.7.0""

libraryDependencies += ""org.scalatest"" % ""scalatest_2.10"" % ""2.0""

resolvers += ""Akka Repository"" at ""http://repo.akka.io/releases/""
```

I think maybe I have make some mistakes on the library setting, as a new
developer of spark application, I wonder what is the standard procedure of
developing a spark application.

Any reply is appreciated.


Alcaid


2015-01-21 2:05 GMT+08:00 Will Benton <willb@redhat.com>:

"
jay vyas <jayunit100.apache@gmail.com>,"Tue, 20 Jan 2015 22:44:34 -0500",Re: Standardized Spark dev environment,Nicholas Chammas <nicholas.chammas@gmail.com>,"I can comment on both...  hi will and nate :)

1) Will's Dockerfile solution is  the most  simple direct solution to the
dev environment question : its a  efficient way to build and develop spark
environments for dev/test..  It would be cool to put that Dockerfile
(and/or maybe a shell script which uses it) in the top level of spark as
the build entry point.  For total platform portability, u could wrap in a
vagrantfile to launch a lightweight vm, so that windows worked equally
well.

2) However, since nate mentioned  vagrant and bigtop, i have to chime in :)
the vagrant recipes in bigtop are a nice reference deployment of how to
deploy spark in a heterogenous hadoop style environment, and tighter
integration testing w/ bigtop for spark releases would be lovely !  The
vagrant stuff use puppet to deploy an n node VM or docker based cluster, in
which users can easily select components (including
spark,yarn,hbase,hadoop,etc...) by simnply editing a YAML file :
https://github.com/apache/bigtop/blob/master/bigtop-deploy/vm/vagrant-puppet/vagrantconfig.yaml....
As nate said, it would be alot of fun to get more cross collaboration
between the spark and bigtop communities.   Input on how we can better
integrate spark (wether its spork, hbase integration, smoke tests aroudn
the mllib stuff, or whatever, is always welcome )







nt
opment
‚Äù testing and
nd
sions of
 ago as an
ôs the
a
ld
‚Äúdev
nt.
it
r
r



-- 
jay vyas
"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Wed, 21 Jan 2015 04:21:13 +0000 (UTC)",Re: GraphX ShortestPaths backwards?,"""dev@spark.apache.org"" <dev@spark.apache.org>","I created https://issues.apache.org/jira/browse/SPARK-5343 for this.


GraphX ShortestPaths seems to be following edges backwards instead of forwards:

import org.apache.spark.graphx._
val g = Graph(sc.makeRDD(Array((1L,""""), (2L,""""), (3L,""""))), sc.makeRDD(Array(Edge(1L,2L,""""), Edge(2L,3L,""""))))

lib.ShortestPaths.run(g,Array(3)).vertices.collect
res1: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map()), (3,Map(3 -> 0)), (2,Map()))

lib.ShortestPaths.run(g,Array(1)).vertices.collect

res2: Array[(org.apache.spark.graphx.VertexId, org.apache.spark.graphx.lib.ShortestPaths.SPMap)] = Array((1,Map(1 -> 0)), (3,Map(1 -> 2)), (2,Map(1 -> 1)))

If I am not mistaken about my assessment, then I believe the following changes will make it run ""forward"":

Change one occurrence of ""src"" to ""dst"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L64

Change three occurrences of ""dst"" to ""src"" in
https://github.com/apache/spark/blob/master/graphx/src/main/scala/org/apache/spark/graphx/lib/ShortestPaths.scala#L65

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 20 Jan 2015 21:42:43 -0800",Re: not found: type LocalSparkContext,James <alcaid1801@gmail.com>,"You don't need the LocalSparkContext. It is only for Spark's own unit test.

You can just create a SparkContext and use it in your unit tests, e.g.

val sc = new SparkContext(""local"", ""my test app"", new SparkConf)


"
"""DEVAN M.S."" <msdevanms@gmail.com>","Wed, 21 Jan 2015 11:25:24 +0530",KNN for large data set,"User <user@spark.apache.org>, dev@spark.apache.org","Hi all,

Please help me to find out best way for K-nearest neighbor using spark for
large data sets.
"
Paolo Platter <paolo.platter@agilelab.it>,"Wed, 21 Jan 2015 06:30:36 +0000",R: Standardized Spark dev environment,"jay vyas <jayunit100.apache@gmail.com>, Nicholas Chammas
	<nicholas.chammas@gmail.com>","Hi all,
I also tried the docker way and it works well.
I suggest to look at sequenceiq/spark dockers, they are very active on that field.

Paolo

Inviata dal mio Windows Phone
________________________________
Da: jay vyas<mailto:jayunit100.apache@gmail.com>
Inviato: ‚Äé21/‚Äé01/‚Äé2015 04:45
A: Nicholas Chammas<mailto:nicholas.cha; Spark dev list<mailto:dev@spark.apache.org>
Oggetto: Re: Standardized Spark dev environment

I can comment on both...  hi will and nate :)

1) Will's Dockerfile solution is  the most  simple direct solution to the
dev environment question : its a  efficient way to build and develop spark
environments for dev/test..  It would be cool to put that Dockerfile
(and/or maybe a shell script which uses it) in the top level of spark as
the build entry point.  For total platform portability, u could wrap in a
vagrantfile to launch a lightweight vm, so that windows worked equally
well.

2) However, since nate mentioned  vagrant and bigtop, i have to chime in :)
the vagrant recipes in bigtop are a nice reference deployment of how to
deploy spark in a heterogenous hadoop style environment, and tighter
integration testing w/ bigtop for spark releases would be lovely !  The
vagrant stuff use puppet to deploy an n node VM or docker based cluster, in
which users can easily select components (including
spark,yarn,hbase,hadoop,etc...) by simnply editing a YAML file :
https://github.com/apache/bigtop/blob/master/bigtop-deploy/vm/vagrant-puppet/vagrantconfig.yaml....
As nate said, it would be alot of fun to get more cross collaboration
between the spark and bigtop communities.   Input on how we can better
integrate spark (wether its spork, hbase integration, smoke tests aroudn
the mllib stuff, or whatever, is always welcome )






On Tue, Jan 20, 2015 at 10:21 PM, Nicholas Chammas <
nicholas.chammas@gmail.com> wrote:

> How many profiles (hadoop / hive /scala) would this development environment
> support ?
>
> As many as we want. We probably want to cover a good chunk of the build
> matrix <https://issues.apache.org/jira/browse/SPARK-2004> that Spark
> officially supports.
>
> What does this provide, concretely?
>
> It provides a reliable way to create a ‚Äúgood‚Äù Spark development
> environment. Roughly speaking, this probably should mean an environment
> that matches Jenkins, since that‚Äôs where we run ‚Äúofficial‚Äù testing and
> builds.
>
> For example, Spark has to run on Java 6 and Python 2.6. When devs build and
> run Spark locally, we can make sure they‚Äôre doing it on these versions of
> the languages with a simple vagrant up.
>
> Nate, could you comment on how something like this would relate to the
> Bigtop effort?
>
> http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
>
> Will, that‚Äôs pretty sweet. I tried something similar a few months ago as an
> experiment to try building/testing Spark within a container. Here‚Äôs the
> shell script I used <https://gist.github.com/nchammas/60b04141f3b9f053faaa
> >
> against the base CentOS Docker image to setup an environment ready to build
> and test Spark.
>
> We want to run Spark unit tests within containers on Jenkins, so it might
> make sense to develop a single Docker image that can be used as both a ‚Äúdev
> environment‚Äù as well as execution container on Jenkins.
>
> Perhaps that‚Äôs the approach to take instead of looking into Vagrant.
>
> Nick
>
> On Tue Jan 20 2015 at 8:22:41 PM Will Benton <willb@redhat.com> wrote:
>
> Hey Nick,
> >
> > I did something similar with a Docker image last summer; I haven't
> updated
> > the images to cache the dependencies for the current Spark master, but it
> > would be trivial to do so:
> >
> > http://chapeau.freevariable.com/2014/08/jvm-test-docker.html
> >
> >
> > best,
> > wb
> >
> >
> > ----- Original Message -----
> > > From: ""Nicholas Chammas"" <nicholas.chammas@gmail.com>
> > > To: ""Spark dev list"" <dev@spark.apache.org>
> > > Sent: Tuesday, January 20, 2015 6:13:31 PM
> > > Subject: Standardized Spark dev environment
> > >
> > > What do y'all think of creating a standardized Spark development
> > > environment, perhaps encoded as a Vagrantfile, and publishing it under
> > > `dev/`?
> > >
> > > The goal would be to make it easier for new developers to get started
> > with
> > > all the right configs and tools pre-installed.
> > >
> > > If we use something like Vagrant, we may even be able to make it so
> that
> > a
> > > single Vagrantfile creates equivalent development environments across
> OS
> > X,
> > > Linux, and Windows, without having to do much (or any) OS-specific
> work.
> > >
> > > I imagine for committers and regular contributors, this exercise may
> seem
> > > pointless, since y'all are probably already very comfortable with your
> > > workflow.
> > >
> > > I wonder, though, if any of you think this would be worthwhile as a
> > > improvement to the ""new Spark developer"" experience.
> > >
> > > Nick
> > >
> >
> ‚Äã
>



--
jay vyas
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 20 Jan 2015 23:28:49 -0800",Re: Standardized Spark dev environment,Paolo Platter <paolo.platter@agilelab.it>,"To respond to the original suggestion by Nick. I always thought it
would be useful to have a Docker image on which we run the tests and
build releases, so that we could have a consistent environment that
other packagers or people trying to exhaustively run Spark tests could
replicate (or at least look at) to understand exactly how we recommend
building Spark. Sean - do you think that is too high of overhead?

In terms of providing images that we encourage as standard deployment
images of Spark and want to make portable across environments, that's
a much larger project and one with higher associated maintenance
overhead. So I'd be interested in seeing that evolve as its own
project (spark-deploy) or something associated with bigtop, etc.

- Patrick


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 21 Jan 2015 07:51:45 +0000",Re: Standardized Spark dev environment,Patrick Wendell <pwendell@gmail.com>,"If the goal is a reproducible test environment then I think that is what
Jenkins is. Granted you can only ask it for a test. But presumably you get
the same result if you start from the same VM image as Jenkins and run the
same steps.

I bet it is not hard to set up and maintain. I bet it is easier than a VM.
But unless Jenkins is using it aren't we just making another different
standard build env in an effort to standardize? If it is not the same then
it loses value as being exactly the same as the reference build env. Has a
problem come up that this solves?

If the goal is just easing developer set up then what does a Docker image
do - what does it set up for me? I don't know of stuff I need set up on OS
X for me beyond the IDE.

"
Patrick Wendell <pwendell@gmail.com>,"Wed, 21 Jan 2015 00:09:35 -0800",Re: Standardized Spark dev environment,Sean Owen <sowen@cloudera.com>,"
But the issue is when users can't reproduce Jenkins failures. We don't
publish anywhere what the exact set of packages and versions is that
is installed on Jenkins. And it can change since it's a shared
infrastructure with other projects. So why not publish this manifest
as a docker file and then have it run on jenkins using that image? My
point is that this ""VM image + steps"" is not public anywhere.


Right now the reference build env is an AMI I created and keep adding
stuff to when Spark gets new dependencies (e.g. the version of ruby we
need to create the docs, new python stats libraries, etc). So if we
had a docker image, then I would use that for making the RC's as well
and it could serve as a definitive reference for people who want to
understand exactly what set of things they need to build Spark.


There are actually a good number of packages you need to do a full
build of Spark including a compliant python version, Java version,
certain python packages, ruby and jekyll stuff for the docs, etc
(mentioned a bit earlier).

- Patrick

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 21 Jan 2015 08:50:55 +0000",Re: Standardized Spark dev environment,Patrick Wendell <pwendell@gmail.com>,"Sure, can Jenkins use this new image too? If not then it doesn't help with
reproducing a Jenkins failure, most of which even Jenkins can't reproduce.
But if it does and it can be used for builds then that does seem like it is
reducing rather than increasing environment configurations which is good.

That's different from developer setup. Surely that is a large number of
permutations to maintain? Win, Linux, OS X at least. Whereas I have not
needed nor probably would want a whole second tool chain on my machine for
Spark... for me it doesn't solve a problem. So just wondering how many
people this will help as devs versus some apparent big maintenance
overhead.

Although if this could replace the scripts that try to fetch sbt and mvn et
al that alone could save enough complexity to make it worthwhile. Would it
do that?

"
Will Benton <willb@redhat.com>,"Wed, 21 Jan 2015 08:42:40 -0500 (EST)",Re: Standardized Spark dev environment,Patrick Wendell <pwendell@gmail.com>,"best,
wb

---------------------------------------------------------------------


"
Stephen Brewin <sbrewin@synsys.com>,"Wed, 21 Jan 2015 14:51:11 +0000","SPARK-5267 : Add a streaming module to ingest Apache Camel Messages
 from a configured endpoints",dev@spark.apache.org,"Hi All

Any thoughts, comments or questions regarding the proposal outlined at 
https://issues.apache.org/jira/browse/SPARK-5267?

Cheers
Steve

- - - - - - - - - - - - - - - - - -

This private and confidential e-mail has been sent to you by Synergy Systems Limited. It may not represent the views of Synergy Systems Limited.

If you are not the intended recipient of this e-mail and have received it in error, please notify the sender by replying with ""received in error"" as the subject and then delete it from your mailbox.

---------------------------------------------------------------------


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Wed, 21 Jan 2015 14:43:34 -0200",Issue with repartition and cache,dev@spark.apache.org,"Hi guys, have anyone find something like this?
I have a training set, and when I repartition it, if I call cache it throw
a classcastexception when I try to execute anything that access it

val rep120 = train.repartition(120)
val cached120 = rep120.cache
cached120.map(f => f(1).asInstanceOf[Int]).sum

Cell Toolbar:
   In [1]:

ClusterSettings.executorMemory=Some(""28g"")

ClusterSettings.maxResultSize = ""20g""

ClusterSettings.resume=true

ClusterSettings.coreInstanceType=""r3.xlarge""

ClusterSettings.coreInstanceCount = 30

ClusterSettings.clusterName=""UberdataContextCluster-Dirceu""

uc.applyDateFormat(""YYMMddHH"")

Searching for existing cluster UberdataContextCluster-Dirceu ...
Spark standalone cluster started at
http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:8080
Found 1 master(s), 30 slaves
Ganglia started at
http://ec2-54-68-91-64.us-west-2.compute.amazonaws.com:5080/ganglia

In [37]:

import org.apache.spark.sql.catalyst.types._

import eleflow.uberdata.util.IntStringImplicitTypeConverter._

import eleflow.uberdata.enums.SupportedAlgorithm._

import eleflow.uberdata.data._

import org.apache.spark.mllib.tree.DecisionTree

import eleflow.uberdata.enums.DateSplitType._

import org.apache.spark.mllib.regression.LabeledPoint

import org.apache.spark.mllib.linalg.Vectors

import org.apache.spark.mllib.classification._

import eleflow.uberdata.model._

import eleflow.uberdata.data.stat.Statistics

import eleflow.uberdata.enums.ValidationMethod._

import org.apache.spark.rdd.RDD

In [5]:

val train = uc.load(uc.toHDFSURI(""/tmp/data/input/train_rev4.csv"")).applyColumnTypes(Seq(DecimalType(),
LongType,TimestampType, StringType,


             StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


             StringType, StringType, StringType, StringType,


              LongType, LongType,StringType, StringType,StringType,


              StringType,StringType))

.formatDateValues(2,DayOfAWeek | Period).slice(excludes = Seq(12,13))

Out[5]:
idclickhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_domain
app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C21
100000941815109427302.03.0100501fbe01fef384576728905ebdecad23867801e8d9
07d7df2244956a241215706320501722035-1791000016934911786371502.03.010050
1fbe01fef384576728905ebdecad23867801e8d907d7df22711ee1201015704320501722035
100084791000037190421511948602.03.0100501fbe01fef384576728905ebdecad2386
7801e8d907d7df228a4875bd1015704320501722035100084791000064072448083837602.0
3.0100501fbe01fef384576728905ebdecad23867801e8d907d7df226332421a101570632050
1722035100084791000067905641704209602.03.010051fe8cc4489166c1610569f928
ecad23867801e8d907d7df22779d90c21018993320502161035-115710000720757801103869
02.03.010050d6137915bb1ef334f028772becad23867801e8d907d7df228a4875bd1016920
32050189904311000771171000072472998854491102.03.0100508fda644b25d4cfcd
f028772becad23867801e8d907d7df22be6db1d71020362320502333039-1157
In [7]:

val test = uc.load(uc.toHDFSURI(""/tmp/data/input/test_rev4.csv"")).applyColumnTypes(Seq(DecimalType(),
TimestampType, StringType,


             StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


              StringType, StringType, StringType, StringType,


             StringType, StringType, StringType, StringType,


              LongType, LongType,StringType, StringType,StringType,


              StringType,StringType)).

formatDateValues(1,DayOfAWeek | Period).slice(excludes =Seq(11,12))

Out[7]:
idhour1hour2C1banner_possite_idsite_domainsite_categoryapp_idapp_domain
app_categorydevice_modeldevice_typedevice_conn_typeC14C15C16C17C18C19C20C21
100001740588092635695.03.010050235ba823f6ebf28ef028772becad23867801e8d9
07d7df220eb711ec10833032050761317510007523100001825269208554285.03.010050
1fbe01fef384576728905ebdecad23867801e8d907d7df22ecb851b21022676320502616035
10008351100005541398292139845.03.0100501fbe01fef384576728905ebdecad2386
7801e8d907d7df221f0bc64f102267632050261603510008351100010946378097988455.0
3.01005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8542422a7101864832050
1092380910015661100013770415586707455.03.01005085f751fdc4e18dd650e219e0
9c13b4192347f47af95efa071f0bc64f1023160320502667047-122110001521204153353724
5.03.01005157fe1b205b626596f028772becad23867801e8d907d7df2268b6db2c106563320
50572239-132100019110567070233785.03.0100501fbe01fef384576728905ebdecad2386
7801e8d907d7df22d4897fef102281332050264723910014823
In [ ]:

val (validationPrediction2, logRegModel2, testDataSet2,
validationDataSet2, trainDataSet2, testPrediction2) =

        eleflow.uberdata.data.Predictor.predict(train,test,excludes=
Seq(6,7,9,10,12,13), iterations = 100, algorithm =
BinaryLogisticRegressionBFGS)

spent time 1943

Out[5]:

MappedRDD[165] at map at Predictor.scala:265

In [ ]:

val corr2 = eleflow.uberdata.data.stat.Statistics.targetCorrelation(validationDataSet2)

In [ ]:

val correlated = corr2.filter(_._1>0.01)

In [ ]:

val correlated2 = correlated.map(_._2)

Out[8]:

Array(11768, 11285, 11278, 11289, 12051, 11279, 42, 11805, 11767, 46,
22, 12063, 20, 8388, 11438, 11783, 8981, 11408, 8386, 11360, 11377,
12059, 11418, 12044, 11771, 11359, 11839, 9118, 9116, 8666, 11986,
8665, 8888, 8887, 18, 12058, 11925, 11468, 11336, 11769, 9254, 9029,
11404, 9028, 71, 11982, 11798, 63, 7401, 8673, 12040, 8664, 4986, 452,
11949, 12050, 76, 11800, 8975, 11189, 11743, 11956, 11801, 12026,
8976, 11784, 2418, 11808, 12054, 11904, 1819, 7, 1840, 11429, 11608,
11983, 11387, 9403, 11495, 11985, 8658, 1020, 11626, 8384, 41, 8387,
11778, 4390, 7067, 11489, 11542, 3, 8381, 9154, 11766, 11479, 9077,
10782, 11680, 11830, 12043, 8926, 8982, 11409, 11391, 11364, 8656,
1274, 5523, 9, 12025, 8279, 1528, 10, 11490, 12046, 6771, 3937, 11450,
11811, 8632, 38, 8898, 11382, 12028, 12053, 4563, 5040, 11330, 1983,
11799, 11327, 11672, 8628, 11342, 11813, 6450, 11825, 8941, 10407,
11806, 11643, 8940, 9405, 11757, 9075, 12056, 11522, 11688, 10406,
11322, 9076, 29, 12064, 8637, 11347, 10831, 11406, 11773, 40, 10560,
11645, 9404, 11789, 11651, 9743, 11835, 11843, 9382, 11971, 11646,
12065, 11984, 8681, 10563, 12039, 9383, 8680, 8391, 3260, 5453, 10120,
8602, 11649, 9385, 4320, 9384, 11210, 11750, 11319, 11787, 11506,
11628, 11415, 11777, 10576, 240, 12017, 0, 10121, 11644, 8929, 11392,
12024, 5602, 9280, 11473, 884, 11812, 10741, 11780, 11503, 8672,
11357, 11966, 12055, 11539, 8644, 11350, 11836, 9058, 11271, 11764,
5094, 7881, 11504, 11698, 11424, 11831, 11587, 11426, 2577, 11610,
8948, 11987, 10744, 9290, 11477, 11497, 11367, 8622, 11969, 12030,
8062, 11664, 11704, 10949, 11508, 10530, 10225, 7655, 4274, 10534,
11394, 8934, 15, 11671, 11845, 12069, 6767, 3713, 8979, 11310, 10670,
8978, 11498, 11281, 11291, 11549, 11840, 10119, 10419, 897, 5875,
11482, 10617, 9331, 10618, 11662, 12060, 11496, 10654, 9742, 11422,
12027, 11545, 6612, 9757, 11881, 19, 11321, 11402, 11256, 8389, 9379,
9741, 11705, 5188, 2780, 8593, 11325, 9452, 11255, 9304, 11990, 8393,
11853, 11619, 9312, 9061, 11425, 8385, 11642, 12023, 9303, 8885,
11375, 6807, 8576, 11528, 11485, 11786, 8518, 11834, 12066, 2257,
11345, 11333, 11903, 9918, 11992, 11257, 11488, 11637, 7215, 10556,
11744, 12018, 12031, 1990, 542, 6099, 9005, 11900, 9739, 11566, 11481,
11314, 12052, 11307, 1828, 12072, 5, 10020, 11413, 10138, 11295, 8959,
8025)

In [ ]:

val trained = trainDataSet2.map{f =>

                                val array = f._2.features.toArray

                                new LabeledPoint(f._2.label,Vectors.dense(

correlated2.map(i => array(i))))}.cache

Out[9]:

MappedRDD[175] at map at <console>:52

In [ ]:

val m = Predictor.binaryLogisticRegressionModelSGD(trained,100)

In [23]:

val validated = validationDataSet2.map{f =>

                                val array = f._2.features.toArray

                                (f._1,new LabeledPoint(f._2.label,Vectors.dense(

correlated2.map(i => array(i)))))}.cache

Out[23]:

MappedRDD[682] at map at <console>:71

In [24]:

val prediction = validated.map {

      case (ids, point) =>

        (ids, m.model.asInstanceOf[LogisticRegressionModel].predict(point.features))

    }

Out[24]:

MappedRDD[683] at map at <console>:79

In [20]:

validated.take(2)

Out[20]:

Array((0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),
(0.0,[0.0,0.0,0.0,1.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]))

In [26]:

val logloss = eleflow.uberdata.data.stat.Statistics.logLoss(prediction)

Out[26]:

5.861273254972684

In [17]:

validationDataSet2.take(3)

Out[17]:

Array((0,(0.0,(12073,[0,1,4,9,18,42,4563,8382,8386,8575,11279,11289,11322,11766,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))),
(0,(0.0,(12073,[0,1,4,9,18,42,3260,8382,8386,8577,11279,11289,11322,11766,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))),
(0,(0.0,(12073,[0,1,4,10,40,42,4729,8382,8386,8672,11279,11289,11357,11768,11805,11852,12051],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))))

In [ ]:

trained.take(4)

In [7]:



import org.apache.spark.mllib.classification._

In [8]:

val steps = Seq(Step(10,2),new Step(7,3), new Step(6,4))

Out[8]:
0Step(10,2)1Step(7,3)2Step(6,4)
In [ ]:

val predictor  =
eleflow.uberdata.data.Predictor.evolutivePredict(train.repartition(240),
test, algorithm = BinaryLogisticRegressionBFGS,

                             validationMethod = LogarithmicLoss, steps
= steps, iterations = 30)

In [ ]:

uc.terminate

In [11]:

train.partitions.size

Out[11]:

94

In [20]:

val rep60 = train.repartition(120)

Out[20]:
idclickhourC1banner_possite_idsite_domainsite_categoryapp_idapp_domain
app_categorydevice_iddevice_ipdevice_modeldevice_typedevice_conn_typeC14C15
C16C17C18C19C20C211769841751868484765301410221210051e8f79e60c4342784f028772b
ecad23867801e8d907d7df22a99f214ab526ff2ce9b8d8d71020634320502374339-123
17703074559452740131141022121005085f751fdc4e18dd650e219e0399477562347f47a
cef3e64932d58615ab5a307674de3ee61221768320502506035-115717708054784542889711
0141022121005085f751fdc4e18dd650e219e051cedd4eaefc06bd0f2161f8a99f214a
d30ecac3542422a7102161132050248032971001116117713001998424865357114102212
1005085f751fdc4e18dd650e219e0bc44c87d7801e8d90f2161f8ad97ca8caa305f51
43836a961020633320502374339-123177175933008005586270141022121005085f751fd
c4e18dd650e219e0f888bf4c5b9c592b0f2161f89a5442e768bc961a1f0bc64f102115332050
2420235-169177224932175731189110141022121005085f751fdc4e18dd650e219e0
e96773f02347f47a0f2161f8a99f214abf741817ef726eae1021767320502506035-1157
17727816327614515164014102212100505bcf81a29d54950bf028772becad23867801e8d9
07d7df22a99f214a5e4ee78bbe87996b1221770320502507035100176157
In [ ]:

val cach60 = rep60.cache

In [28]:

cach60.map(f => f(1).asInstanceOf[Int]).sum

org.apache.spark.SparkException: Job aborted due to stage failure:
Task 53 in stage 53.0 failed 4 times, most recent failure: Lost task
53.3 in stage 53.0 (TID 1322,
ip-172-31-0-62.us-west-2.compute.internal):
java.lang.ClassCastException: java.lang.String cannot be cast to
java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:59)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:59)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
:172)
	at scala.collection.AbstractIterator.reduceLeft(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:853)
	at org.apache.spark.rdd.RDD$$anonfun$18.apply(RDD.scala:851)
	at org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350)
	at org.apache.spark.SparkContext$$anonfun$29.apply(SparkContext.scala:1350)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
    org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202)
    scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
    org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
    org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696)
    scala.Option.foreach(Option.scala:236)
    org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696)
    org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420)
    akka.actor.Actor$class.aroundReceive(Actor.scala:465)
    org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375)
    akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
    akka.actor.ActorCell.invoke(ActorCell.scala:487)
    akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
    akka.dispatch.Mailbox.run(Mailbox.scala:220)
    akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
    scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
    scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
    scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
    scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)



R
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Wed, 21 Jan 2015 16:40:53 -0200",Re: Issue with repartition and cache,Sandy Ryza <sandy.ryza@cloudera.com>,"Hi Sandy, thanks for the reply.

I tried to run this code without the cache and it worked.
Also if I cache before repartition, it also works, the problem seems to be
something related with repartition and caching.
My train is a SchemaRDD, and if I make all my columns as StringType, the
error doesn't happen, but if I have anything else, this exception is thrown.



2015-01-21 16:37 GMT-02:00 Sandy Ryza <sandy.ryza@cloudera.com>:

""
ow
(Seq(DecimalType(),
main
C21
035
2.0
2050
3869
920
Seq(DecimalType(),
C21
0
035
.0
2050
3724
3320
386
t2)
e(
0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),
0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]))
)
2,11766,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))),
6,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))),
68,11805,11852,12051],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))))
C15
772b
a
9711
2
d
2050
d9
wC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:59)
wC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:59)
)
172)
a:1145)
va:615)
eduler.scala:1214)
cheduler.scala:1203)
cheduler.scala:1202)
la:59)
02)
pply(DAGScheduler.scala:696)
pply(DAGScheduler.scala:696)
.scala:696)
e$2.applyOrElse(DAGScheduler.scala:1420)
AGScheduler.scala:1375)
tDispatcher.scala:393)
va:1339)
java:107)
"
Sandy Ryza <sandy.ryza@cloudera.com>,"Wed, 21 Jan 2015 10:37:54 -0800",Re: Issue with repartition and cache,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Hi Dirceu,

Does the issue not show up if you run ""map(f =>
f(1).asInstanceOf[Int]).sum"" on the ""train"" RDD?  It appears that f(1) is
an String, not an Int.  If you're looking to parse and convert it, ""toInt""
should be used instead of ""asInstanceOf"".

-Sandy


w
Seq(DecimalType(),
ain
21
35
.0
050
869
20
eq(DecimalType(),
21
35
0
050
724
320
86
2)
(
.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),
,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,2.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]))
,11766,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))),
,11803,11904,12065],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))),
8,11805,11852,12051],[2.0,3.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]))))
15
72b
711
050
9
C$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:59)
C$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(<console>:59)
72)
:1145)
a:615)
duler.scala:1214)
heduler.scala:1203)
heduler.scala:1202)
a:59)
2)
ply(DAGScheduler.scala:696)
ply(DAGScheduler.scala:696)
scala:696)
$2.applyOrElse(DAGScheduler.scala:1420)
GScheduler.scala:1375)
Dispatcher.scala:393)
a:1339)
ava:107)
"
Xiangrui Meng <mengxr@gmail.com>,"Wed, 21 Jan 2015 10:55:39 -0800",Re: KNN for large data set,"""DEVAN M.S."" <msdevanms@gmail.com>","For large datasets, you need hashing in order to compute k-nearest
neighbors locally. You can start with LSH + k-nearest in Google
scholar: http://scholar.google.com/scholar?q=lsh+k+nearest -Xiangrui


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 21 Jan 2015 12:00:49 -0800",Re: Standardized Spark dev environment,Will Benton <willb@redhat.com>,"Yep,

I think it's only useful (and likely to be maintained) if we actually
use this on Jenkins. So that was my proposal. Basically give people a
docker file so they can understand exactly what versions of everything
we use for our reference build. And if they don't want to use docker
directly, this will at least serve as an up-to-date list of
packages/versions they should try to install locally in whatever
environment they have.

- Patrick

m>, ""Paolo Platter""
com>, ""Will Benton"" <willb@redhat.com>
g to solve.  The other part was teasing out differences between the Fedora Java environment and a more conventional Java environment.  I agree with Sean (and I think this is your suggestion as well, Patrick) that making the environment Jenkins runs a standard image that is available for public consumption would be useful in general.

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 21 Jan 2015 16:02:39 -0800",Upcoming Spark 1.2.1 RC,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

I am planning to cut a 1.2.1 RC soon and wanted to notify people.

There are a handful of important fixes in the 1.2.1 branch
(http://s.apache.org/Mpn) particularly for Spark SQL. There was also
an issue publishing some of our artifacts with 1.2.0 and this release
would fix it for downstream projects.

You can track outstanding 1.2.1 blocker issues here at
http://s.apache.org/2v2 - I'm guessing all remaining blocker issues
will be fixed today.

I think we have a good handle on the remaining outstanding fixes, but
please let me know if you think there are severe outstanding fixes
that need to be backported into this branch or are not tracked above.

Thanks!
- Patrick

---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Thu, 22 Jan 2015 10:39:44 +0530",Test suites in  the python wrapper of kmeans failing,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

The test suites in the Kmeans class in clustering.py is not updated to 
take the seed value and hence it is failing.
Shall I make the changes and submit it along with my PR( Python API for 
Gaussian Mixture Model) or create a JIRA ?

Regards,
Meethu

---------------------------------------------------------------------


"
Meethu Mathew <meethu.mathew@flytxt.com>,"Thu, 22 Jan 2015 12:21:21 +0530",Re: Test suites in  the python wrapper of kmeans failing,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

Sorry it was my mistake. My code was not properly built.

Regards,
Meethu


_<http://www.linkedin.com/home?trk=hb_tab_home_top>_


"
Mick Davies <michael.belldavies@gmail.com>,"Thu, 22 Jan 2015 01:59:02 -0700 (MST)",Are there any plans to run Spark on top of Succinct,dev@spark.apache.org,"
http://succinct.cs.berkeley.edu/wp/wordpress/

Looks like a really interesting piece of work that could dovetail well with
Spark.

I have been trying recently to optimize some queries I have running on Spark
on top of Parquet but the support from Parquet for predicate push down
especially for dictionary based columns is a bit limiting. I am not sure,
but from a cursory view it looks like this format may help in this area.

Mick




--

---------------------------------------------------------------------


"
"""DEVAN M.S."" <msdevanms@gmail.com>","Thu, 22 Jan 2015 18:59:16 +0530",Re: KNN for large data set,Xiangrui Meng <mengxr@gmail.com>,"Thanks Xiangrui Meng will try this.

And, found this https://github.com/kaushikranjan/knnJoin also.
Will this work with double data ? Can we find out z value of
*Vector(10.3,4.5,3,5)* ?







"
Dean Wampler <deanwampler@gmail.com>,"Thu, 22 Jan 2015 09:04:44 -0600",Re: Are there any plans to run Spark on top of Succinct,Mick Davies <michael.belldavies@gmail.com>,"Interesting. I was wondering recently if anyone has explored working with
compressed data directly.

Dean Wampler, Ph.D.
Author: Programming Scala, 2nd Edition
<http://shop.oreilly.com/product/0636920033073.do> (O'Reilly)
Typesafe <http://typesafe.com>
@deanwampler <http://twitter.com/deanwampler>
http://polyglotprogramming.com


"
Nicholas Murphy <halcyonic@gmail.com>,"Thu, 22 Jan 2015 15:19:43 -0800",query planner design doc?,dev@spark.apache.org,"Hi-

Quick question: is there a design doc (or something more than ‚Äúlook at the code‚Äù) for the query planner for Spark SQL (i.e., the component that takes‚Ä¶Catalyst?‚Ä¶operator trees and translates them into SPARK operations)?

Thanks,
Nick
---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Thu, 22 Jan 2015 19:45:24 -0800",Re: query planner design doc?,Nicholas Murphy <halcyonic@gmail.com>,"Here is the initial design document for catalyst :
https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPGU5M1Y/edit

Strategies (many of which are in SparkStragegies.scala) are the part that
creates the physical operators from a catalyst logical plan.  These
operators have execute() methods that actually call RDD operations.


ok at the
hat
PARK operations)?
"
William-Smith <williamsmith.mail@gmail.com>,"Thu, 22 Jan 2015 21:42:48 -0700 (MST)",Re: spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2,dev@spark.apache.org,"I have had the same issue while using HttpClient from AWS EMR Spark Streaming
to post to a nodejs server.

I have found ... using
Classloder.getResource('org/apache/http/client/HttpClient"") .... that the
class 
Is being loaded front the spark-assembly-1.1.0-hadoop2.4.0.jar.

That in itself is not the issue because the version is 4.2.5 .... the same
version I am using on my local machine with success .... using Hadoop cdh 5.



The issue is that HttpClient relies on Httpcore .... and there is an old
commons-httpcore-1.3.jar as well as httpcore-4.5.2 in the spark-assembly
jar.

It looks like the old one is getting loaded first.

So the fix might be to build the Spark jar myself without the httpcore-1.3
and replace it on bootstrap.
I will keep you posted on the outcome.





--

---------------------------------------------------------------------


"
"""Saumitra Shahapure (Vizury)"" <saumitra.shahapure@vizury.com>","Fri, 23 Jan 2015 11:52:21 +0530",Spark performance gains for small queries,dev@spark.apache.org,"Hello,

We were comparing performance of some of our production hive queries
between Hive and Spark. We compared Hive(0.13)+hadoop (1.2.1) against both
Spark 0.9 and 1.1. We could see that the performance gains have been good
in Spark.

We tried a very simple query,
select count(*) from T where col3=123
in both sparkSQL and Hive (with hive.map.aggr=true) and found that Spark
performance had been 2x better than Hive (120sec vs 60sec). Table T is
stored in S3 and contains 600MB single GZIP file.

My question is, why Spark is faster than Hive here? In both of the cases,
the file will be downloaded, uncompressed and lines will be counted by a
single process. For Hive case, reducer will be identity function
since hive.map.aggr is true.

Note that disk spills and network I/O are very less for Hive's case as well,
--
Regards,
Saumitra Shahapure
"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 23 Jan 2015 00:38:22 -0800",Re: Spark performance gains for small queries,"""Saumitra Shahapure (Vizury)"" <saumitra.shahapure@vizury.com>","It's hard to tell without more details, but the start-up latency in Hive can sometimes be high, especially if you are running Hive on MapReduce. MR just takes 20-30 seconds per job to spin up even if the job is doing nothing.

For real use of Spark SQL for short queries by the way, I'd recommend using the JDBC server so that you can have a long-running Spark process. It gets quite a bit faster after the first few queries.

Matei

both
good
Spark
cases,
a
well,


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 23 Jan 2015 10:13:57 +0000",Re: spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2,William-Smith <williamsmith.mail@gmail.com>,"Did you use spark.files.userClassPathFirst = true? it's exactly for
this kind of problem.


---------------------------------------------------------------------


"
"""Saumitra Shahapure (Vizury)"" <saumitra.shahapure@vizury.com>","Fri, 23 Jan 2015 17:19:55 +0530",Re: Spark performance gains for small queries,Matei Zaharia <matei.zaharia@gmail.com>,"Hey Matei,

Thanks for your reply. We would keep in mind to use JDBC server for smaller
queries.

For the mapreduce job start-up, are you pointing towards JVM initialization
latencies in MR? Other than JVM initialization, does Spark do any
optimization (that is not done by mapreduce) to speed up the startup?

--
Regards,
Saumitra Shahapure


"
Nicholas Murphy <halcyonic@gmail.com>,"Fri, 23 Jan 2015 09:44:28 -0800",Re: query planner design doc?,Michael Armbrust <michael@databricks.com>,"Okay, thanks.  The design document mostly details the infrastructure for optimization strategies but doesn‚Äôt detail the strategies themselves.  I take it the set of strategies are basically embodied in SparkStrategies.scala...is there a design doc/roadmap/JIRA issue detailing what strategies exist and which are planned?

Thanks,
Nick

https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPGU5M1Y/edit <https://docs.google.com/document/d/1Hc_Ehtr0G8SQUg69cmViZsMi55_Kf3tISD9GPGU5M1Y/edit>
that creates the physical operators from a catalyst logical plan.  These operators have execute() methods that actually call RDD operations.
ook at the code‚Äù) for the query planner for Spark SQL (i.e., the component that takes‚Ä¶Catalyst?‚Ä¶operator trees and translates them into SPARK operations)?
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 23 Jan 2015 18:00:05 +0000",Maximum size of vector that reduce can handle,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I am trying to measure the Spark reduce performance for big vectors. My motivation is related to machine learning gradient. Gradient is a vector that is computed on each worker and then all results need to be summed up and broadcasted back to workers. For example, present machine learning applications involve very long parameter vectors, for deep neural networks it can be up to 2Billions. So, I want to measure the time that is needed for this operation depending on the size of vector and number of workers. I wrote few lines of code that assume that Spark will distribute partitions among all available workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores, 16GB RAM), each runs 2 Workers.

import org.apache.spark.mllib.rdd.RDDFunctions._
import breeze.linalg._
import org.apache.log4j._
Logger.getRootLogger.setLevel(Level.OFF)
val n = 60000000
val p = 12
val vv = sc.parallelize(0 until p, p).map(i => DenseVector.rand[Double]( n ))
vv.reduce(_ + _)

When executing in shell with 60M vector it crashes after some period of timJava HotSpot(TM) 64-Bit Server VM warning: INFO: os::commit_memory(0x0000000755500000, 2863661056, 0) failed; error='Cannot allocate memory' (errno=12)
#
# There is insufficient memory for the Java Runtime Environment to continue.
# Native memory allocation (malloc) failed to allocate 2863661056 bytes for committing reserved memory.

I run shell with --executor-memory 8G --driver-memory 8G, so handling 60M vector of Double should not be a problem. Are there any big overheads for this? What is the maximum size of vector that reduce can handle? 

Best regards, Alexander

P.S. 

""spark.driver.maxResultSize 0"" needs to set in order to run this code. I also needed to change ""java.io.tmpdir"" and ""spark.local.dir"" folders because my /tmp folder which is default, was too small and Spark swaps heavily into this folder. Without these settings I get either ""no space left on device"" or ""out of memory"" exceptions.

I also submitted a bug https://issues.apache.org/jira/browse/SPARK-5386

---------------------------------------------------------------------


"
Michael Davies <michael.belldavies@gmail.com>,"Fri, 23 Jan 2015 19:35:07 +0000",Re: Optimize encoding/decoding strings when using Parquet,Reynold Xin <rxin@databricks.com>,"Added PR https://github.com/apache/spark/pull/4139 <https://github.com/apache/spark/pull/4139> - I think tests have been re-arranged so merge necessary

Mick


<michael.belldavies@gmail.com <mailto:michael.belldavies@gmail.com>> anyone
http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141p10195.html <http://apache-spark-developers-list.1001551.n3.nabble.com/Optimize-encoding-decoding-strings-when-using-Parquet-tp10141p10195.html>
Nabble.com.
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
DB Tsai <dbtsai@dbtsai.com>,"Fri, 23 Jan 2015 11:53:20 -0800",Re: Maximum size of vector that reduce can handle,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Hi Alexander,

When you use `reduce` to aggregate the vectors, those will actually be
pulled into driver, and merged over there. Obviously, it's not
scaleable given you are doing deep neural networks which have so many
coefficients.

Please try treeReduce instead which is what we do in linear regression
and logistic regression.

See https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/optimization/LBFGS.scala
for example.

val (gradientSum, lossSum) = data.treeAggregate((Vectors.zeros(n), 0.0))(
seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) =>
val l = localGradient.compute(
features, label, bcW.value, grad)
(grad, loss + l)
},
combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) =>
axpy(1.0, grad2, grad1)
(grad1, loss1 + loss2)
})

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



otivation is related to machine learning gradient. Gradient is a vector that is computed on each worker and then all results need to be summed up and broadcasted back to workers. For example, present machine learning applications involve very long parameter vectors, for deep neural networks it can be up to 2Billions. So, I want to measure the time that is needed for this operation depending on the size of vector and number of workers. I wrote few lines of code that assume that Spark will distribute partitions among all available workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores, 16GB RAM), each runs 2 Workers.
e]( n ))
000755500000, 2863661056, 0) failed; error='Cannot allocate memory' (errno=12)
ue.
or committing reserved memory.
 vector of Double should not be a problem. Are there any big overheads for this? What is the maximum size of vector that reduce can handle?
also needed to change ""java.io.tmpdir"" and ""spark.local.dir"" folders because my /tmp folder which is default, was too small and Spark swaps heavily into this folder. Without these settings I get either ""no space left on device"" or ""out of memory"" exceptions.

---------------------------------------------------------------------


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 23 Jan 2015 20:07:28 +0000",RE: Maximum size of vector that reduce can handle,DB Tsai <dbtsai@dbtsai.com>,"Hi DB Tsai,

Thank you for your suggestion. Actually, I've started my experiments with ""treeReduce"". Originally, I had ""vv.treeReduce(_ + _, 2)"" in my script exactly because MLlib optimizers are using it, as you pointed out with LBFGS. However, it leads to the same problems as ""reduce"", but presumably not so directly. As far as I understand, treeReduce limits the number of communications between workers and master forcing workers to partially compute the reduce operation.

Are you sure that driver will first collect all results (or all partial results in treeReduce) and ONLY then perform aggregation? If that is the problem, then how to force it to do aggregation after receiving each portion of data from Workers?

Best regards, Alexander

-----Original Message-----
From: DB Tsai [mailto:dbtsai@dbtsai.com] 
Sent: Friday, January 23, 2015 11:53 AM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Maximum size of vector that reduce can handle

Hi Alexander,

When you use `reduce` to aggregate the vectors, those will actually be pulled into driver, and merged over there. Obviously, it's not scaleable given you are doing deep neural networks which have so many coefficients.

Please try treeReduce instead which is what we do in linear regression and logistic regression.

See https://github.com/apache/spark/blob/branch-1.1/mllib/src/main/scala/org/apache/spark/mllib/optimization/LBFGS.scala
for example.

val (gradientSum, lossSum) = data.treeAggregate((Vectors.zeros(n), 0.0))( seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) => val l = localGradient.compute( features, label, bcW.value, grad) (grad, loss + l) }, combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) => axpy(1.0, grad2, grad1) (grad1, loss1 + loss2)
})

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



On Fri, Jan 23, 2015 at 10:00 AM, Ulanov, Alexander <alexander.ulanov@hp.com> wrote:
> Dear Spark developers,
>
> I am trying to measure the Spark reduce performance for big vectors. My motivation is related to machine learning gradient. Gradient is a vector that is computed on each worker and then all results need to be summed up and broadcasted back to workers. For example, present machine learning applications involve very long parameter vectors, for deep neural networks it can be up to 2Billions. So, I want to measure the time that is needed for this operation depending on the size of vector and number of workers. I wrote few lines of code that assume that Spark will distribute partitions among all available workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores, 16GB RAM), each runs 2 Workers.
>
> import org.apache.spark.mllib.rdd.RDDFunctions._
> import breeze.linalg._
> import org.apache.log4j._
> Logger.getRootLogger.setLevel(Level.OFF)
> val n = 60000000
> val p = 12
> val vv = sc.parallelize(0 until p, p).map(i => 
> DenseVector.rand[Double]( n )) vv.reduce(_ + _)
>
> When executing in shell with 60M vector it crashes after some period of time. One of the node contains the following in stdout:
> Java HotSpot(TM) 64-Bit Server VM warning: INFO: 
> os::commit_memory(0x0000000755500000, 2863661056, 0) failed; 
> error='Cannot allocate memory' (errno=12) # # There is insufficient memory for the Java Runtime Environment to continue.
> # Native memory allocation (malloc) failed to allocate 2863661056 bytes for committing reserved memory.
>
> I run shell with --executor-memory 8G --driver-memory 8G, so handling 60M vector of Double should not be a problem. Are there any big overheads for this? What is the maximum size of vector that reduce can handle?
>
> Best regards, Alexander
>
> P.S.
>
> ""spark.driver.maxResultSize 0"" needs to set in order to run this code. I also needed to change ""java.io.tmpdir"" and ""spark.local.dir"" folders because my /tmp folder which is default, was too small and Spark swaps heavily into this folder. Without these settings I get either ""no space left on device"" or ""out of memory"" exceptions.
>
> I also submitted a bug 
> https://issues.apache.org/jira/browse/SPARK-5386
>
> ---------------------------------------------------------------------
> To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org For 
> additional commands, e-mail: dev-help@spark.apache.org
>

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org
For additional commands, e-mail: dev-help@spark.apache.org
"
Michael Armbrust <michael@databricks.com>,"Fri, 23 Jan 2015 12:25:36 -0800",Re: query planner design doc?,Nicholas Murphy <halcyonic@gmail.com>,"No, are you looking for something in particular?


ves.  I
g
GU5M1Y/edit
ook at
ent that
SPARK operations)?
"
Peter Prettenhofer <peter.prettenhofer@gmail.com>,"Fri, 23 Jan 2015 22:53:04 +0100",Re: Spark 1.2.0: MissingRequirementError,PierreB <pierre.borckmans@realimpactanalytics.com>,"much appreciated if somebody could help fixing this issue -- or at least
give me some hints what might be wrong

thanks,
 Peter

2015-01-15 14:04 GMT+01:00 PierreB <pierre.borckmans@realimpactanalytics.com



-- 
Peter Prettenhofer
"
DB Tsai <dbtsai@dbtsai.com>,"Fri, 23 Jan 2015 14:18:00 -0800",Re: Maximum size of vector that reduce can handle,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>, dev@spark.apache.org","Hi Alexander,

For `reduce`, it's an action that will collect all the data from
mapper to driver, and perform the aggregation in driver. As a result,
if the output from the mapper is very large, and the numbers of
partitions in mapper are large, it might cause a problem.

For `treeReduce`, as the name indicates, the way it works is in the
first layer, it aggregates the output of the mappers two by two
resulting half of the numbers of output. And then, we continuously do
the aggregation layer by layer. The final aggregation will be done in
driver but in this time, the numbers of data are small.

By default, depth 2 is used, so if you have so many partitions of
large vector, this may still cause issue. You can increase the depth
into higher numbers such that in the final reduce in driver, the
number of partitions are very small.

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



 ""treeReduce"". Originally, I had ""vv.treeReduce(_ + _, 2)"" in my script exactly because MLlib optimizers are using it, as you pointed out with LBFGS. However, it leads to the same problems as ""reduce"", but presumably not so directly. As far as I understand, treeReduce limits the number of communications between workers and master forcing workers to partially compute the reduce operation.
esults in treeReduce) and ONLY then perform aggregation? If that is the problem, then how to force it to do aggregation after receiving each portion of data from Workers?
lled into driver, and merged over there. Obviously, it's not scaleable given you are doing deep neural networks which have so many coefficients.
d logistic regression.
org/apache/spark/mllib/optimization/LBFGS.scala
)( seqOp = (c, v) => (c, v) match { case ((grad, loss), (label, features)) => val l = localGradient.compute( features, label, bcW.value, grad) (grad, loss + l) }, combOp = (c1, c2) => (c1, c2) match { case ((grad1, loss1), (grad2, loss2)) => axpy(1.0, grad2, grad1) (grad1, loss1 + loss2)
motivation is related to machine learning gradient. Gradient is a vector that is computed on each worker and then all results need to be summed up and broadcasted back to workers. For example, present machine learning applications involve very long parameter vectors, for deep neural networks it can be up to 2Billions. So, I want to measure the time that is needed for this operation depending on the size of vector and number of workers. I wrote few lines of code that assume that Spark will distribute partitions among all available workers. I have 6-machine cluster (Xeon 3.3GHz 4 cores, 16GB RAM), each runs 2 Workers.
memory for the Java Runtime Environment to continue.
for committing reserved memory.
M vector of Double should not be a problem. Are there any big overheads for this? What is the maximum size of vector that reduce can handle?
 also needed to change ""java.io.tmpdir"" and ""spark.local.dir"" folders because my /tmp folder which is default, was too small and Spark swaps heavily into this folder. Without these settings I get either ""no space left on device"" or ""out of memory"" exceptions.

---------------------------------------------------------------------


"
Rapelly Kartheek <kartheek.mbms@gmail.com>,"Sat, 24 Jan 2015 10:25:36 +0530",Find the two storage Locations of each partition of a replicated rdd.,dev@spark.apache.org,"hi,

I wanna find the storage locations( BlockManagerIds) of each partition when
the rdd is replicated twice. I mean, If a twice replicated rdd has got 5
partitions, I would like to know the first and second storage locations of
each partition. Basically, I am trying to modify the list of nodes selected
for replicating an rdd.

I just want to checkout where exactly does the first and second copies of
each partition gets stored. I tried upon the rdd storage details in the
webUI, but couldn't gain much.

Any help please!!

Thank you
Karthik
"
William-Smith <williamsmith.mail@gmail.com>,"Sat, 24 Jan 2015 08:45:11 -0700 (MST)",Re: spark 1.1.0 (w/ hadoop 2.4) vs aws java sdk 1.7.2,dev@spark.apache.org,"Hey Sean,

I did find that after my post and tried it
....spark.files.userClassPathFirst = true
But it is deemed in the docs as instrumental .... and did not work.
Monitored that additional config via the logs and it did not complain ... no
change though.


So...
Due to my timeline for a Demo....

Used HttpUrl instead for my POST and abandoned using Apache HTTP for now.


My app works but I am sure that this will raise it's head again with other
transitive dependencies.
Surely will repost any findings after Demo and beyond on this subject .. to
benefit the group.

Thx.
Will






--

---------------------------------------------------------------------


"
Octavian Geagla <ogeagla@gmail.com>,"Sat, 24 Jan 2015 09:26:25 -0700 (MST)","Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?",dev@spark.apache.org,"Hello,

I found it useful to implement the  Hadamard Product
<https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29http://>   as
a VectorTransformer.  It can be applied to scale (by a constant) a certain
dimension (column) of the data set.  

Since I've already implemented it and am using it, I thought I'd see if
there's interest in this feature going in as Experimental.  I'm not sold on
the name 'Weighter', either.

Here's the current branch with the work (docs, impl, tests).
<https://github.com/ogeagla/spark/compare/spark-mllib-weighting>  

The implementation was heavily inspired by those of StandardScalerModel and
Normalizer.

Thanks
Octavian



--

---------------------------------------------------------------------


"
shrenik <galashrenik55@gmail.com>,"Sat, 24 Jan 2015 13:36:09 -0700 (MST)",Project ideas for spark,dev@spark.apache.org,"Hello guys, 
I'm a graduate student and have to make a semester long project based on
Data Intensive Computing. I'm interested in using Spark in my project so I
would really appreciate if any body could suggest any project ideas related
to using spark. Thank you in advance.



--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sat, 24 Jan 2015 22:27:23 +0000",Does spark-ec2 support Windows?,Spark dev list <dev@spark.apache.org>,"Is spark-ec2 supposed to run normally from Windows (e.g. to launch a
cluster)?

I ask because I don‚Äôt see mention of Windows anywhere in relation to
spark-ec2, and there is an open PR
<https://github.com/apache/spark/pull/4162/files#diff-ada66bbeb2f1327b508232ef6c3805a5R358>
that checks file permissions, and I‚Äôm not sure if we need to care about how
permission masks show up on Windows vs. Unix systems.

Nick
‚Äã
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sat, 24 Jan 2015 14:34:19 -0800",Re: Does spark-ec2 support Windows?,Nicholas Chammas <nicholas.chammas@gmail.com>,"AFAIK we've never claimed that spark-ec2 is supported on Windows -- for
example in the code, we assume the existence of `ssh` as a binary on PATH
etc. That said I've never tried to run it from Windows, so I am not sure if
it can somehow be used.

Thanks
Shivaram


 to
32ef6c3805a5R358
 about how
"
PierreB <pierre.borckmans@realimpactanalytics.com>,"Mon, 26 Jan 2015 04:15:42 -0700 (MST)",[SQL] Self join with ArrayType columns problems,dev@spark.apache.org,"Using Spark 1.2.0, we are facing some weird behaviour when performing self
join on a table with some ArrayType field.
(potential bug ?)

I have set up a minimal non working example here:
https://gist.github.com/pierre-borckmans/4853cd6d0b2f2388bf4f
<https://gist.github.com/pierre-borckmans/4853cd6d0b2f2388bf4f>  

In a nutshell, if the ArrayType column used for the pivot is created
manually in the StructType definition, everything works as expected.
However, if the ArrayType pivot column is obtained by a sql query (be it by
using a ""array"" wrapper, or using a collect_list operator for instance),
then results are completely off.

Could anyone have a look as this really is a blocking issue.

Thanks!

Cheers

P.



--

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Mon, 26 Jan 2015 12:37:18 -0800",Re: Are there any plans to run Spark on top of Succinct,Rachit Agarwal <ragarwal@berkeley.edu>,"There was work being done at Berkeley on prototyping support for Succinct
in Spark SQL.  Rachit might have more information.


"
Reynold Xin <rxin@databricks.com>,"Mon, 26 Jan 2015 14:18:34 -0800",renaming SchemaRDD -> DataFrame,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

We are considering renaming SchemaRDD -> DataFrame in 1.3, and wanted to
get the community's opinion.

The context is that SchemaRDD is becoming a common data format used for
bringing data into Spark from external systems, and used for various
components of Spark, e.g. MLlib's new pipeline API. We also expect more and
more users to be programming directly against SchemaRDD API rather than the
core RDD API. SchemaRDD, through its less commonly used DSL originally
designed for writing test cases, always has the data-frame like API. In
1.3, we are redesigning the API to make the API usable for end users.


There are two motivations for the renaming:

1. DataFrame seems to be a more self-evident name than SchemaRDD.

2. SchemaRDD/DataFrame is actually not going to be an RDD anymore (even
though it would contain some RDD functions like map, flatMap, etc), and
calling it Schema*RDD* while it is not an RDD is highly confusing. Instead.
DataFrame.rdd will return the underlying RDD for all RDD methods.


My understanding is that very few users program directly against the
SchemaRDD API at the moment, because they are not well documented. However,
oo maintain backward compatibility, we can create a type alias DataFrame
that is still named SchemaRDD. This will maintain source compatibility for
Scala. That said, we will have to update all existing materials to use
DataFrame rather than SchemaRDD.
"
Patrick Wendell <pwendell@gmail.com>,"Mon, 26 Jan 2015 15:01:55 -0800",Re: renaming SchemaRDD -> DataFrame,Reynold Xin <rxin@databricks.com>,"correspondence where you can get an RDD to/from a DataFrame.


---------------------------------------------------------------------


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Mon, 26 Jan 2015 23:11:22 +0000 (UTC)",Re: renaming SchemaRDD -> DataFrame,"Patrick Wendell <pwendell@gmail.com>, Reynold Xin <rxin@databricks.com>","And in the off chance that anyone hasn't seen it yet, the Jan. 13 Bay Area Spark Meetup YouTube contained a wealth of background information on this idea (mostly from Patrick and Reynold :-).

https://www.youtube.com/watch?v=YWppYPWznSQ

________________________________
From: Patrick Wendell <pwendell@gmail.com>
To: Reynold Xin <rxin@databricks.com> 
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org> 
Sent: Monday, January 26, 2015 4:01 PM
Subject: Re: renaming SchemaRDD -> DataFrame


correspondence where you can get an RDD to/from a DataFrame.



---------------------------------------------------------------------

---------------------------------------------------------------------


"
Koert Kuipers <koert@tresata.com>,"Mon, 26 Jan 2015 19:27:22 -0500",Re: renaming SchemaRDD -> DataFrame,Michael Malak <michaelmalak@yahoo.com>,"what i am trying to say is: structured data != sql


"
Koert Kuipers <koert@tresata.com>,"Mon, 26 Jan 2015 19:26:09 -0500",Re: renaming SchemaRDD -> DataFrame,Michael Malak <michaelmalak@yahoo.com>,"""The context is that SchemaRDD is becoming a common data format used for
bringing data into Spark from external systems, and used for various
components of Spark, e.g. MLlib's new pipeline API.""

i agree. this to me also implies it belongs in spark core, not sql


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 26 Jan 2015 17:32:13 -0800",Re: renaming SchemaRDD -> DataFrame,Koert Kuipers <koert@tresata.com>,"(Actually when we designed Spark SQL we thought of giving it another name, like Spark Schema, but we decided to stick with SQL since that was the most obvious use case to many users.)

Matei

long-term, supporting structured data efficiently does require quite a bit of the infrastructure in Spark SQL, such as query planning and columnar storage. The intent of Spark SQL though is to be more than a SQL server -- it's meant to be a library for manipulating structured data. Since this is possible to build over the core API, it's pretty natural to organize it that way, same as Spark Streaming is a library.
for
Bay Area
this
1:1
wanted to
for
various
more
than
originally
API. In
users.
(even
and
the
DataFrame
compatibility
use
---------------------------------------------------------------------
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 26 Jan 2015 17:31:00 -0800",Re: renaming SchemaRDD -> DataFrame,Koert Kuipers <koert@tresata.com>,"While it might be possible to move this concept to Spark Core long-term, supporting structured data efficiently does require quite a bit of the infrastructure in Spark SQL, such as query planning and columnar storage. The intent of Spark SQL though is to be more than a SQL server -- it's meant to be a library for manipulating structured data. Since this is possible to build over the core API, it's pretty natural to organize it that way, same as Spark Streaming is a library.

Matei

for
Area
this
wanted to
for
more
than
originally
In
users.
(even
and
DataFrame
compatibility
use


---------------------------------------------------------------------


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 26 Jan 2015 17:46:52 -0800",Re: renaming SchemaRDD -> DataFrame,Matei Zaharia <matei.zaharia@gmail.com>,"Both SchemaRDD and DataFrame sound fine to me, though I like the former
slightly better because it's more descriptive.

Even if SchemaRDD's needs to rely on Spark SQL under the covers, it would
be more clear from a user-facing perspective to at least choose a package
name for it that omits ""sql"".

I would also be in favor of adding a separate Spark Schema module for Spark
SQL to rely on, but I imagine that might be too large a change at this
point?

-Sandy


"
Kushal Datta <kushal.datta@gmail.com>,"Mon, 26 Jan 2015 18:02:53 -0800",Re: renaming SchemaRDD -> DataFrame,Sandy Ryza <sandy.ryza@cloudera.com>,"I want to address the issue that Matei raised about the heavy lifting
required for a full SQL support. It is amazing that even after 30 years of
research there is not a single good open source columnar database like
Vertica. There is a column store option in MySQL, but it is not nearly as
sophisticated as Vertica or MonetDB. But there's a true need for such a
system. I wonder why so and it's high time to change that.

"
Reynold Xin <rxin@databricks.com>,"Mon, 26 Jan 2015 22:05:05 -0800",talk on interface design,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

In Spark, we have done reasonable well historically in interface and API
design, especially compared with some other Big Data systems. However, we
have also made mistakes along the way. I want to share a talk I gave about
interface design at Databricks' internal retreat.

https://speakerdeck.com/rxin/interface-design-for-spark-community

Interface design is a vital part of Spark becoming a long-term sustainable,
thriving framework. Good interfaces can be the project's biggest asset,
while bad interfaces can be the worst technical debt. As the project scales
bigger and bigger, the community is expanding and we are getting a wider
range of contributors that have not thought about this as their everyday
development experience outside Spark.

It is part-art part-science and in some sense acquired taste. However, I
think there are common issues that can be spotted easily, and common
principles that can address a lot of the low hanging fruits. Through this
presentation, I hope to bring to everybody's attention the issue of
interface design and encourage everybody to think hard about interface
design in their contributions.
"
Andrew Ash <andrew@andrewash.com>,"Tue, 27 Jan 2015 01:23:33 -0500",Re: talk on interface design,Reynold Xin <rxin@databricks.com>,"In addition to the references you have at the end of the presentation,
there's a great set of practical examples based on the learnings from Qt
posted here: http://www21.in.tum.de/~blanchet/api-design.pdf

Chapter 4's way of showing a principle and then an example from Qt is
particularly instructional.


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 26 Jan 2015 23:02:50 -0800",[VOTE] Release Apache Spark 1.2.1 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.2.1!

The tag to be voted on is v1.2.1-rc1 (commit 3e2d7d3):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=3e2d7d310b76c293b9ac787f204e6880f508f6ec

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc1/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1061/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc1-docs/

Please vote on releasing this package as Apache Spark 1.2.1!

The vote is open until Friday, January 30, at 07:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.1
[ ] -1 Do not release this package because ...

For a list of fixes in this release, see http://s.apache.org/Mpn.

To learn more about Apache Spark, please see
http://spark.apache.org/

- Patrick

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Jan 2015 00:04:19 -0800",Re: talk on interface design,Andrew Ash <andrew@andrewash.com>,"Thanks, Andrew. That's great material.



"
angel__ <angel.alvarez.pascua@gmail.com>,"Tue, 27 Jan 2015 04:18:06 -0700 (MST)",Re: Use mvn to build Spark 1.2.0  failed,dev@spark.apache.org,"I had that problem when I tried to build Spark 1.2. I don't exactly know what
is causing it, but I guess it might have something to do with user
permissions.

I could finally fix this by building Spark as ""root"" user (now I'm dealing
with another problem, but ...that's another story...)



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 27 Jan 2015 11:23:43 +0000",Re: Use mvn to build Spark 1.2.0 failed,angel__ <angel.alvarez.pascua@gmail.com>,"You certainly do not need yo build Spark as root. It might clumsily
overcome a permissions problem in your local env but probably causes other
problems.

"
Sean Owen <sowen@cloudera.com>,"Tue, 27 Jan 2015 13:00:36 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Patrick Wendell <pwendell@gmail.com>,"I think there are several signing / hash issues that should be fixed
before this release.

Hashes:

http://issues.apache.org/jira/browse/SPARK-5308
https://github.com/apache/spark/pull/4161

The hashes here are correct, but have two issues:

As noted in the JIRA, the format of the hash file is ""nonstandard"" --
at least, doesn't match what Maven outputs, and apparently which tools
like Leiningen expect, which is just the hash with no file name or
spaces. There are two ways to fix that: different command-line tools
(see PR), or, just ask Maven to generate these hashes (a different,
easy PR).

However, is the script I modified above used to generate these hashes?
It's generating SHA1 sums, but the output in this release candidate
has (correct) SHA512 sums.

This may be more than a nuisance, since last time for some reason
Maven Central did not register the project hashes.

http://search.maven.org/#artifactdetails%7Corg.apache.spark%7Cspark-core_2.10%7C1.2.0%7Cjar
does not show them but they exist:
http://www.us.apache.org/dist/spark/spark-1.2.0/

It may add up to a problem worth rooting out before this release.


Signing:

As noted in https://issues.apache.org/jira/browse/SPARK-5299 there are
two signing keys in
https://people.apache.org/keys/committer/pwendell.asc (9E4FE3AF,
00799F7E) but only one is in http://www.apache.org/dist/spark/KEYS

However, these artifacts seem to be signed by FC8ED089 which isn't in either.

Details details, but I'd say non-binding -1 at the moment.



---------------------------------------------------------------------


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Tue, 27 Jan 2015 12:28:59 -0200",Re: renaming SchemaRDD -> DataFrame,,"Can't the SchemaRDD remain the same, but deprecated, and be removed in the
release 1.5(+/- 1)  for example, and the new code been added to DataFrame?
With this, we don't impact in existing code for the next few releases.



2015-01-27 0:02 GMT-02:00 Kushal Datta <kushal.datta@gmail.com>:

"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 27 Jan 2015 07:54:50 -0800","Re: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?",ogeagla <ogeagla@gmail.com>,"I would call it Scaler. You might want to add it to the spark.ml pipieline
api. Please check the spark.ml.HashingTF implementation. Note that this
should handle sparse vectors efficiently.

Hadamard and FFTs are quite useful. If you are intetested, make sure that
we call an FFT libary that is license-compatible with Apache.

-Xiangrui

"
Xiangrui Meng <mengxr@gmail.com>,"Tue, 27 Jan 2015 08:03:40 -0800",Re: Maximum size of vector that reduce can handle,DB Tsai <dbtsai@dbtsai.com>,"60m-vector costs 480MB memory. You have 12 of them to be reduced to the
driver. So you need ~6GB memory not counting the temp vectors generated
from '_+_'. You need to increase driver memory to make it work. That being
said, ~10^7 hits the limit for the current impl of glm. -Xiangrui

"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 27 Jan 2015 08:51:10 -0800","Re: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?",Xiangrui Meng <mengxr@gmail.com>,"Hmm... Scaler and Scalar are very close together both in terms of
pronunciation and spelling - and I wouldn't want to create confusion
between the two. Further - this operation (elementwise multiplication by a
static vector) is general enough that maybe it should have a more general
name?


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Tue, 27 Jan 2015 08:55:04 -0800",Re: renaming SchemaRDD -> DataFrame,Matei Zaharia <matei.zaharia@gmail.com>,"I'm +1 on this, although a little worried about unknowingly introducing
SparkSQL dependencies every time someone wants to use this. It would be
great if the interface can be abstract and the implementation (in this
case, SparkSQL backend) could be swapped out.

DataFrame seems like a name carried over from pandas (and by extension, R),
and it's never been obvious to me what a ""Frame"" is.




"
Koert Kuipers <koert@tresata.com>,"Tue, 27 Jan 2015 12:08:08 -0500",Re: renaming SchemaRDD -> DataFrame,Matei Zaharia <matei.zaharia@gmail.com>,"hey matei,
i think that stuff such as SchemaRDD, columar storage and perhaps also
query planning can be re-used by many systems that do analysis on
structured data. i can imagine panda-like systems, but also datalog or
scalding-like (which we use at tresata and i might rebase on SchemaRDD at
some point). SchemaRDD should become the interface for all these. and
columnar storage abstractions should be re-used between all these.

currently the sql tie in is way beyond just the (perhaps unfortunate)
naming convention. for example a core part of the SchemaRD abstraction is
Row, which is org.apache.spark.sql.catalyst.expressions.Row, forcing anyone
that want to build on top of SchemaRDD to dig into catalyst, a SQL Parser
(if i understand it correctly, i have not used catalyst, but it looks
neat). i should not need to include a SQL parser just to use structured
data in say a panda-like framework.

best, koert



"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Tue, 27 Jan 2015 18:08:53 +0100",UnknownHostException while running YarnTestSuite,dev@spark.apache.org,"Hi,

I‚Äôm trying to run the Spark test suite on an EC2 instance, but I can‚Äôt get
Yarn tests to pass. The hostname I get on that machine is not resolvable,
but adding a line in /etc/hosts makes the other tests pass, except for Yarn
tests.

Any help is greatly appreciated!

thanks,
iulian

ubuntu@ip-172-30-0-248:~/spark$ cat /etc/hosts
127.0.0.1 localhost
172.30.0.248 ip-172-30-0-248

# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts

and the exception:

[info] - run Spark in yarn-client mode *** FAILED *** (2 seconds, 249
milliseconds)
[info]   java.net.UnknownHostException: Invalid host name: local host
is: (unknown); destination host is: ""ip-172-30-0-248"":57041;
java.net.UnknownHostException; For more details see:
http://wiki.apache.org/hadoop/UnknownHost
[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native
Method)
[info]   at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
[info]   at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
[info]   at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
[info]   at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:783)
[info]   at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:742)
[info]   at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
[info]   at org.apache.hadoop.ipc.Client.getConnection(Client.java:1448)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1377)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1359)
[info]   at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
[info]   at com.sun.proxy.$Proxy69.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:152)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[info]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[info]   at java.lang.reflect.Method.invoke(Method.java:606)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
[info]   at com.sun.proxy.$Proxy70.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:294)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
[info]   at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
[info]   at org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:49)
[info]   at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:90)
[info]   at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)
[info]   at org.apache.spark.SparkContext.<init>(SparkContext.scala:343)
[info]   at org.apache.spark.deploy.yarn.YarnClusterDriver$.main(YarnClusterSuite.scala:175)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply$mcV$sp(YarnClusterSuite.scala:118)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(YarnClusterSuite.scala:116)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(YarnClusterSuite.scala:116)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)
[info]   at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite.org$scalatest$BeforeAndAfterAll$$super$run(YarnClusterSuite.scala:36)
[info]   at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite.run(YarnClusterSuite.scala:36)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
[info]   Cause: java.net.UnknownHostException:
[info]   at org.apache.hadoop.ipc.Client$Connection.<init>(Client.java:400)
[info]   at org.apache.hadoop.ipc.Client.getConnection(Client.java:1448)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1377)
[info]   at org.apache.hadoop.ipc.Client.call(Client.java:1359)
[info]   at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
[info]   at com.sun.proxy.$Proxy69.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterMetrics(ApplicationClientProtocolPBClientImpl.java:152)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[info]   at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
[info]   at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
[info]   at java.lang.reflect.Method.invoke(Method.java:606)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:186)
[info]   at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
[info]   at com.sun.proxy.$Proxy70.getClusterMetrics(Unknown Source)
[info]   at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getYarnClusterMetrics(YarnClientImpl.java:294)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
[info]   at org.apache.spark.deploy.yarn.Client$$anonfun$submitApplication$1.apply(Client.scala:91)
[info]   at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
[info]   at org.apache.spark.deploy.yarn.Client.logInfo(Client.scala:49)
[info]   at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:90)
[info]   at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
[info]   at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)
[info]   at org.apache.spark.SparkContext.<init>(SparkContext.scala:343)
[info]   at org.apache.spark.deploy.yarn.YarnClusterDriver$.main(YarnClusterSuite.scala:175)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply$mcV$sp(YarnClusterSuite.scala:118)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(YarnClusterSuite.scala:116)
[info]   at org.apache.spark.deploy.yarn.YarnClusterSuite$$anonfun$1.apply(YarnClusterSuite.scala:116)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.scalatest.Suite$class.withFixture(Suite.scala:1122)

‚Äã
-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
Mark Hamstra <mark@clearstorydata.com>,"Tue, 27 Jan 2015 09:18:57 -0800",Re: renaming SchemaRDD -> DataFrame,Koert Kuipers <koert@tresata.com>,"In master, Reynold has already taken care of moving Row
into org.apache.spark.sql; so, even though the implementation of Row (and
GenericRow et al.) is in Catalyst (which is more optimizer than parser),
that needn't be of concern to users of the API in its most recent state.


"
Boromir Widas <vcsubsvc@gmail.com>,"Tue, 27 Jan 2015 12:37:50 -0500",Re: Maximum size of vector that reduce can handle,Xiangrui Meng <mengxr@gmail.com>,"I am running into this issue as well, when storing large Arrays as the
value in a kv pair
and then doing a reducebykey.
Can one of the experts please comment if it would make sense to add an
operation to
add values in place like accumulators do - this would essentially merge the
vectors for
a given key in place, avoiding multiple allocations of temp array/vectors.
This should
be faster for datasets with frequently repeated keys.


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 27 Jan 2015 09:55:47 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Sean Owen <sowen@cloudera.com>,"Hey Sean,

The release script generates hashes in two places (take a look a bit
further down in the script), one for the published artifacts and the
other for the binaries. In the case of the binaries we use SHA512
because, AFAIK, the ASF does not require you to use SHA1 and SHA512 is
better. In the case of the published Maven artifacts we use SHA1
because my understanding is this is what Maven requires. However, it
does appear that the format is now one that maven cannot parse.

Anyways, it seems fine to just change the format of the hash per your PR.

- Patrick


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 27 Jan 2015 18:00:40 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Patrick Wendell <pwendell@gmail.com>,"Got it. Ignore the SHA512 issue since these aren't somehow expected by
a policy or Maven to be in a certain format. Just wondered if the
difference was intended.

The Maven way of generated the SHA1 hashes is to set this on the
install plugin, AFAIK, although I'm not sure if the intent was to hash
files that Maven didn't create:

<configuration>
    <createChecksum>true</createChecksum>
</configuration>

As for the key issue, I think it's just a matter of uploading the new
key in both places.

We should all of course test the release anyway.


---------------------------------------------------------------------


"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Tue, 27 Jan 2015 17:59:47 +0000 (UTC)",Re: renaming SchemaRDD -> DataFrame,"""Evan R. Sparks"" <evan.sparks@gmail.com>, 
	Matei Zaharia <matei.zaharia@gmail.com>","I personally have no preference DataFrame vs. DataTable, but only wish to lay out the history and etymology simply because I'm into that sort of thing.

""Frame"" comes from Marvin Minsky's 1970's AI construct: ""slots"" and the data that go in them. The S programming language (precursor to R) adopted this terminology in 1991. R of course became popular with the rise of Data Science around 2012.
http://www.google.com/trends/explore#q=%22data%20science%22%2C%20%22r%20programming%22&cmpt=q&tz=

""DataFrame"" would carry the implication that it comes along with its own metadata, whereas ""DataTable"" might carry the implication that metadata is stored in a central metadata repository.

""DataFrame"" is thus technically more correct for SchemaRDD, but is a less familiar (and thus less accessible) term for those not immersed in data science or AI and thus may have narrower appeal.


I'm +1 on this, although a little worried about unknowingly introducing
SparkSQL dependencies every time someone wants to use this. It would be
great if the interface can be abstract and the implementation (in this
case, SparkSQL backend) could be swapped out.

DataFrame seems like a name carried over from pandas (and by extension, R),
and it's never been obvious to me what a ""Frame"" is.






---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 27 Jan 2015 10:10:25 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Sean Owen <sowen@cloudera.com>,"Yes - the key issue is just due to me creating new keys this time
around. Anyways let's take another stab at this. In the mean time,
please don't hesitate to test the release itself.

- Patrick


---------------------------------------------------------------------


"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Tue, 27 Jan 2015 18:31:44 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Patrick Wendell <pwendell@gmail.com>,"Weíre using spark on scala 2.11 /w hadoop2.4.  Would it be practical / make sense to build a bin version of spark against scala 2.11 for versions other than just hadoop1 at this time?

Cheers,

Sean


1.2.1!
7d310b76c293b9ac787f204e6880f508f6ec


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 27 Jan 2015 10:35:10 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Sean McNamara <Sean.McNamara@webtrends.com>,"Hey Sean,

Right now we don't publish every 2.11 binary to avoid combinatorial
explosion of the number of build artifacts we publish (there are other
parameters such as whether hive is included, etc). We can revisit this
in future feature releases, but .1 releases like this are reserved for
bug fixes.

- Patrick


---------------------------------------------------------------------


"
Sean McNamara <Sean.McNamara@Webtrends.com>,"Tue, 27 Jan 2015 18:43:52 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Patrick Wendell <pwendell@gmail.com>,"Sounds good, that makes sense.

Cheers,

Sean

ake sense to build a bin version of spark against scala 2.11 for versions other than just hadoop1 at this time?
e:
n 1.2.1!
2d7d310b76c293b9ac787f204e6880f508f6ec


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 27 Jan 2015 11:13:58 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Sean McNamara <Sean.McNamara@webtrends.com>,"Okay - we've resolved all issues with the signatures and keys.
However, I'll leave the current vote open for a bit to solicit
additional feedback.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Jan 2015 11:18:39 -0800",Re: renaming SchemaRDD -> DataFrame,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Dirceu,

That is not possible because one cannot overload return types.

SQLContext.parquetFile (and many other methods) needs to return some type,
and that type cannot be both SchemaRDD and DataFrame.

In 1.3, we will create a type alias for DataFrame called SchemaRDD to not
break source compatibility for Scala.



"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Jan 2015 11:19:59 -0800",Re: renaming SchemaRDD -> DataFrame,Koert Kuipers <koert@tresata.com>,"Koert,

As Mark said, I have already refactored the API so that nothing is catalyst
is exposed (and users won't need them anyway). Data types, Row interfaces
are both outside catalyst package and in org.apache.spark.sql.


"
Koert Kuipers <koert@tresata.com>,"Tue, 27 Jan 2015 14:29:59 -0500",Re: renaming SchemaRDD -> DataFrame,Reynold Xin <rxin@databricks.com>,"thats great. guess i was looking at a somewhat stale master branch...


"
Dmitriy Lyubimov <dlieu.7@gmail.com>,"Tue, 27 Jan 2015 12:05:07 -0800",Re: renaming SchemaRDD -> DataFrame,,"It has been pretty evident for some time that's what it is, hasn't it?

Yes that's a better name IMO.


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Tue, 27 Jan 2015 18:10:05 -0200",Re: renaming SchemaRDD -> DataFrame,"""dev@spark.apache.org"" <dev@spark.apache.org>","Reynold,
But with type alias we will have the same problem, right?
If the methods doesn't receive schemardd anymore, we will have to change
our code to migrade from schema to dataframe. Unless we have an implicit
conversion between DataFrame and SchemaRDD



2015-01-27 17:18 GMT-02:00 Reynold Xin <rxin@databricks.com>:

"
Matei Zaharia <matei.zaharia@gmail.com>,"Tue, 27 Jan 2015 12:25:25 -0800",Re: renaming SchemaRDD -> DataFrame,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"The type alias means your methods can specify either type and they will work. It's just another name for the same type. But Scaladocs and such will show DataFrame as the type.

Matei

change
implicit
type,
not
in the
DataFrame?
releases.
lifting
years
like
nearly
such a
it
for
this
another
was
of
this
be
and
format
like
end
alias
---------------------------------------------------------------------
---------------------------------------------------------------------
---------------------------------------------------------------------


---------------------------------------------------------------------


"
Krishna Sankar <ksankar42@gmail.com>,"Tue, 27 Jan 2015 12:35:12 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Patrick Wendell <pwendell@gmail.com>,"+1
1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:55 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0 -Phive -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
1.2.0
2.1. sta"
Patrick Wendell <pwendell@gmail.com>,"Tue, 27 Jan 2015 15:56:57 -0800",Friendly reminder/request to help with reviews!,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Just a reminder, as always around release time we have a very large
volume of patches show up near the deadline.

to have community involvement in performing code reviews. And in
particular, doing a thorough review and signing off on a patch with
LGTM can substantially increase the odds we can merge a patch
confidently.

If you are newer to Spark, finding a single area of the codebase to
focus on can still provide a lot of value to the project in the
reviewing process.

Cheers and good luck with everyone on work for this release.

- Patrick

---------------------------------------------------------------------


"
Gurumurthy Yeleswarapu <guruyvs@yahoo.com.INVALID>,"Wed, 28 Jan 2015 00:02:40 +0000 (UTC)",Re: Friendly reminder/request to help with reviews!,"Patrick Wendell <pwendell@gmail.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Patrick:
I would love to help reviewing in any way I can. Im fairly new here. Can you help with a pointer to get me started.
Thanks
      From: Patrick Wendell <pwendell@gmail.com>
 To: ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Tuesday, January 27, 2015 3:56 PM
 Subject: Friendly reminder/request to help with reviews!
   
Hey All,

Just a reminder, as always around release time we have a very large
volume of patches show up near the deadline.

to have community involvement in performing code reviews. And in
particular, doing a thorough review and signing off on a patch with
LGTM can substantially increase the odds we can merge a patch
confidently.

If you are newer to Spark, finding a single area of the codebase to
focus on can still provide a lot of value to the project in the
reviewing process.

Cheers and good luck with everyone on work for this release.

- Patrick

---------------------------------------------------------------------



   "
Reynold Xin <rxin@databricks.com>,"Tue, 27 Jan 2015 16:20:23 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC1),Krishna Sankar <ksankar42@gmail.com>,"+1

Tested on Mac OS X


"
Reynold Xin <rxin@databricks.com>,"Tue, 27 Jan 2015 16:35:16 -0800",Re: renaming SchemaRDD -> DataFrame,Matei Zaharia <matei.zaharia@gmail.com>,"Alright I have merged the patch ( https://github.com/apache/spark/pull/4173
) since I don't see any strong opinions against it (as a matter of fact
most were for it). We can still change it if somebody lays out a strong
argument.


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 28 Jan 2015 02:02:34 -0800",[RESULT] [VOTE] Release Apache Spark 1.2.1 (RC1),Reynold Xin <rxin@databricks.com>,"This vote is cancelled in favor of RC2.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 28 Jan 2015 02:06:10 -0800",[VOTE] Release Apache Spark 1.2.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version 1.2.1!

The tag to be voted on is v1.2.1-rc1 (commit b77f876):
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc2/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1062/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-1.2.1-rc2-docs/

Changes from rc1:

Please vote on releasing this package as Apache Spark 1.2.1!

The vote is open until  Saturday, January 31, at 10:04 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.2.1
[ ] -1 Do not release this package because ...

For a list of fixes in this release, see http://s.apache.org/Mpn.

To learn more about Apache Spark, please see
http://spark.apache.org/

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 28 Jan 2015 02:08:10 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Minor typo in the above e-mail - the tag is named v1.2.1-rc2 (not v1.2.1-rc1).


---------------------------------------------------------------------


"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Wed, 28 Jan 2015 10:12:03 +0000",Data source API | Support for dynamic schema,"""dev@spark.apache.org"" <dev@spark.apache.org>","I saw the talk on Spark data sources and looking at the interfaces, it
seems that the schema needs to be provided upfront. This works for many
data sources but I have a situation in which I would need to integrate a
system that supports schema evolutions by allowing users to change schema
without affecting existing rows. Basically, each row contains a schema hint
(id and version) and this allows developers to evolve schema over time and
perform migration at will. Since the schema needs to be specified upfront
in the data source API, one possible way would be to build a union of all
schema versions and handle populating row values appropriately. This works
in case columns have been added or deleted in the schema but doesn't work
if types have changed. I was wondering if it is possible to change the API
 to provide schema for each row instead of expecting data source to provide
schema upfront?

Thanks,
Aniket
"
Aniket <aniket.bhatnagar@gmail.com>,"Wed, 28 Jan 2015 03:17:00 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),dev@spark.apache.org,"Hi Patrick,

I am wondering if this version will address issues around certain artifacts
such issue is https://issues.apache.org/jira/browse/SPARK-5144

Thanks,
Aniket






--"
Patrick Wendell <pwendell@gmail.com>,"Wed, 28 Jan 2015 02:20:13 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),Aniket <aniket.bhatnagar@gmail.com>,"Yes - it fixes that issue.

ts
ne
rs
on
f87673d1f9f03d4c83cf583158227c551359b
:
/
n
pache-Spark-1-2-1-RC2-tp10317p10318.html
Servlet.jtp?macro=unsubscribe_by_code&node=1&code=YW5pa2V0LmJoYXRuYWdhckBnbWFpbC5jb218MXwxMzE3NTAzMzQz>
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
.n3.nabble.com/VOTE-Release-Apache-Spark-1-2-1-RC2-tp10317p10320.html
.com.

---------------------------------------------------------------------


"
PierreB <pierre.borckmans@realimpactanalytics.com>,"Wed, 28 Jan 2015 03:41:14 -0700 (MST)",Re: [SQL] Self join with ArrayType columns problems,dev@spark.apache.org,"Should I file a JIRA for this?



--

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 28 Jan 2015 13:17:34 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),Patrick Wendell <pwendell@gmail.com>,"+1 (nonbinding). I verified that all the hash / signing items I
mentioned before are resolved.

The source package compiles on Ubuntu / Java 8. I ran tests and the
passed. Well, actually I see the same failure I've seeing locally on
OS X and on Ubuntu for"
Ye Xianjin <advancedxy@gmail.com>,"Wed, 28 Jan 2015 21:22:01 +0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),Sean Owen <sowen@cloudera.com>,"Sean,  
the MQRRStreamSuite is also failed for me on Mac OS X, Though I don‚Äôt have time to invest that.

--  
Ye Xianjin
Sent with Sparrow (http://www.sparrowmailapp.com/?sig)




gress
.java:423)
ion 1.2.1!
b77f87673d1f9f03d4c83cf583158227c551359b
t:
2/
script.

v-unsubscribe@spark.apache.org)
o:dev-help@spark.apache.org)
unsubscribe@spark.apache.org)
dev-help@spark.apache.org)


"
Sean Owen <sowen@cloudera.com>,"Wed, 28 Jan 2015 13:44:41 +0000",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),Ye Xianjin <advancedxy@gmail.com>,"We had both been using Java 8; Ye reports that it fails on Java 6 too.
We both believe this has been failing for a fair while, so I do not
think it's a regression. I'll make a JIRA though.

ôt have
ess
:423)
87673d1f9f03d4c83cf583158227c551359b
pt.

---------------------------------------------------------------------


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Wed, 28 Jan 2015 12:52:40 -0200",Re: Use mvn to build Spark 1.2.0 failed,,"I was facing the same problem, and I fixed it by adding

<plugin>
<artifactId>maven-assembly-plugin</artifactId>
        <version>2.4.1</version>
        <configuration>
          <descriptors>
            <descriptor>assembly/src/main/assembly/assembly.xml</descriptor>
          </descriptors>
        </configuration>
</plugin>
 in the root pom.xml, following the maven assembly plugin docs
<http://maven.apache.org/plugins-archives/maven-assembly-plugin-2.4.1/examples/multimodule/module-source-inclusion-simple.html>

I can make a PR on this if you consider this an issue.

Now I'm facing this problem, is that what you have now?
[ERROR] Failed to execute goal
org.apache.maven.plugins:maven-assembly-plugin:2.4.1:single (default-cli)
on project spark-network-common_2.10: Failed to create assembly: Error
adding file 'org.apache.spark:spark-network-common_2.10:jar:1.3.0-SNAPSHOT'
to archive:
/home/dirceu/projects/spark/network/common/target/scala-2.10/classes isn't
a file. -> [Help 1]


2015-01-27 9:23 GMT-02:00 Sean Owen <sowen@cloudera.com>:

"
Sean Owen <sowen@cloudera.com>,"Wed, 28 Jan 2015 15:32:55 +0000",Re: Use mvn to build Spark 1.2.0 failed,Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"I don't see how this would relate to the problem in the OP? the
assemblies build fine already as far as I can tell.

Your new error may be introduced by your change.


---------------------------------------------------------------------


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Wed, 28 Jan 2015 14:17:18 -0200",Re: Use mvn to build Spark 1.2.0 failed,"Sean Owen <sowen@cloudera.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Before this I was facing the same problem, and fixed it adding the plugin
at the root pom.xml

Maybe this is related to the release, mine is:
Apache Maven 3.2.3 (33f8c3e1027c3ddde99d3cdebad2656a31e8fdf4;
2014-08-11T17:58:10-03:00)
Java version: 1.8.0_20, vendor: Oracle Corporation
OS name: ""linux"", version: ""3.13.0-24-generic"", arch: ""amd64"", family:
""unix""

Or the command that I'm using:
mvn -Dhadoop.version=2.0.0-mr1-cdh4.2.0 -DskipTests -Phive
-Phive-thriftserver clean compile assembly:single

I'm trying to build using the pr/1290
wyphao.2007 have you figured out how to complete the build?



2015-01-28 13:32 GMT-02:00 Sean Owen <sowen@cloudera.com>:

"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 28 Jan 2015 16:33:43 +0000",Re: Extending Scala style checks,Reynold Xin <rxin@databricks.com>,"FYI: scalastyle just merged in a patch to add support for external rules
<https://github.com/scalastyle/scalastyle/issues/25#ref-commit-9a576c3>.

I forget why I was following the linked issue, but I assume it's related to
this discussion.

Nick



"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 28 Jan 2015 11:26:08 -0800",Re: Data source API | Support for dynamic schema,"Aniket Bhatnagar <aniket.bhatnagar@gmail.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Aniket,

In general the schema of all rows in a single table must be same. This 
is a basic assumption made by Spark SQL. Schema union does make sense, 
and we're planning to support this for Parquet. But as you've mentioned, 
it doesn't help if types of different versions of a column differ from 
each other. Also, you need to reload the data source table after schema 
changes happen.

Cheng



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Jan 2015 11:38:55 -0800",Re: Data source API | Support for dynamic schema,Cheng Lian <lian.cs.zju@gmail.com>,"It's an interesting idea, but there are major challenges with per row
schema.

1. Performance - query optimizer and execution use assumptions about schema
and data to generate optimized query plans. Having to re-reason about
schema for each row can substantially slow down the engine, but due to
optimization and due to the overhead of schema information associated with
each row.

2. Data model: per-row schema is fundamentally a different data model. The
current relational model has gone through 40 years of research and have
very well defined semantics. I don't think there are well defined semantics
of a per-row schema data model. For example, what is the semantics of an
UDF function that is operating on a data cell that has incompatible schema?
Should we also coerce or convert the data type? If yes, will that lead to
conflicting semantics with some other rules? We need to answer questions
like this in order to have a robust data model.






"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 28 Jan 2015 12:51:47 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),Sean Owen <sowen@cloudera.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:22 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0
-Phive -DskipTests
2. Tested pyspark, mlib - running as well as compare results w"
jay vyas <jayunit100.apache@gmail.com>,"Wed, 28 Jan 2015 18:12:01 -0500",spark akka fork : is the source anywhere?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi spark. Where is akka coming from in spark ?

I see the distribution referenced is a spark artifact... but not in the
apache namespace.

     <akka.group>org.spark-project.akka</akka.group>
     <akka.version>2.3.4-spark</akka.version>

Clearly this is a deliberate thought out change (See SPARK-1812), but its
not clear where 2.3.4 spark is coming from and who is maintaining its
release?

-- 
jay vyas

PS

I've had some conversations with will benton as well about this, and its
clear that some modifications to akka are needed, or else a protobug error
occurs, which amount to serialization incompatibilities, hence if one wants
to build spark from sources, the patched akka is required (or else, manual
patching needs to be done)...

15/01/28 22:58:10 ERROR ActorSystemImpl: Uncaught fatal error from thread
[sparkWorker-akka.remote.default-remote-dispatcher-6] shutting down
ActorSystem [sparkWorker] java.lang.VerifyError: class
akka.remote.WireFormats$AkkaControlMessage overrides final method
getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;
"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Jan 2015 15:33:52 -0800",Re: spark akka fork : is the source anywhere?,jay vyas <jayunit100.apache@gmail.com>,"Hopefully problems like this will go away entirely in the next couple of
releases. https://issues.apache.org/jira/browse/SPARK-5293




"
Evan Chan <velvia.github@gmail.com>,"Wed, 28 Jan 2015 16:20:30 -0800",Re: renaming SchemaRDD -> DataFrame,Reynold Xin <rxin@databricks.com>,"Hey guys,

How does this impact the data sources API?  I was planning on using
this for a project.

+1 that many things from spark-sql / DataFrame is universally
desirable and useful.

By the way, one thing that prevents the columnar compression stuff in
Spark SQL from being more useful is, at least from previous talks with
Reynold and Michael et al., that the format was not designed for
persistence.

I have a new project that aims to change that.  It is a
zero-serialisation, high performance binary vector library, designed
from the outset to be a persistent storage friendly.  May be one day
it can replace the Spark SQL columnar compression.

Michael told me this would be a lot of work, and recreates parts of
Parquet, but I think it's worth it.  LMK if you'd like more details.

-Evan


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Jan 2015 16:23:43 -0800",Re: renaming SchemaRDD -> DataFrame,Evan Chan <velvia.github@gmail.com>,"It shouldn't change the data source api at all because data sources create
RDD[Row], and that gets converted into a DataFrame automatically
(previously to SchemaRDD).

https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/sources/interfaces.scala

types. Types were previously defined in sql.catalyst.types, and now moved
to sql.types. After 1.3, sql.catalyst is hidden from users, and all public
APIs have first class classes/objects defined in sql directly.




"
Evan Chan <velvia.github@gmail.com>,"Wed, 28 Jan 2015 16:41:07 -0800",Re: renaming SchemaRDD -> DataFrame,Reynold Xin <rxin@databricks.com>,"I believe that most DataFrame implementations out there, like Pandas,
supports the idea of missing values / NA, and some support the idea of
Not Meaningful as well.

Does Row support anything like that?  That is important for certain
applications.  I thought that Row worked by being a mutable object,
but haven't looked into the details in a while.

-Evan


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Jan 2015 16:42:56 -0800",Re: renaming SchemaRDD -> DataFrame,Evan Chan <velvia.github@gmail.com>,"Isn't that just ""null"" in SQL?


"
Evan Chan <velvia.github@gmail.com>,"Wed, 28 Jan 2015 17:49:01 -0800",Re: renaming SchemaRDD -> DataFrame,Reynold Xin <rxin@databricks.com>,"Yeah, it's ""null"".   I was worried you couldn't represent it in Row
because of primitive types like Int (unless you box the Int, which
would be a performance hit).  Anyways, I'll take another look at the
Row API again  :-p


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 28 Jan 2015 17:55:32 -0800",Re: renaming SchemaRDD -> DataFrame,Evan Chan <velvia.github@gmail.com>,"In particular the performance tricks are in SpecificMutableRow.


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 28 Jan 2015 18:01:22 -0800",Re: spark akka fork : is the source anywhere?,Reynold Xin <rxin@databricks.com>,"It's maintained here:

https://github.com/pwendell/akka/tree/2.2.3-shaded-proto

Over time, this is something that would be great to get rid of, per rxin


---------------------------------------------------------------------


"
"""Evan R. Sparks"" <evan.sparks@gmail.com>","Wed, 28 Jan 2015 18:10:51 -0800",Re: renaming SchemaRDD -> DataFrame,Reynold Xin <rxin@databricks.com>,"You've got to be a little bit careful here. ""NA"" in systems like R or
pandas may have special meaning that is distinct from ""null"".

See, e.g. http://www.r-bloggers.com/r-na-vs-null/




"
shane knapp <sknapp@berkeley.edu>,"Wed, 28 Jan 2015 21:56:48 -0800",emergency jenkins restart soon,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","the spark master builds stopped triggering ~yesterday and the logs don't
show anything.  i'm going to give the current batch of spark pull request
builder jobs a little more time (~30 mins) to finish, then kill whatever is
left and restart jenkins.  anything that was queued or killed will be
retriggered once jenkins is back up.

sorry for the inconvenience, we'll get this sorted asap.

thanks,

shane
"
shane knapp <sknapp@berkeley.edu>,"Wed, 28 Jan 2015 22:29:03 -0800",Re: emergency jenkins restart soon,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","jenkins is back up and all builds have been retriggered...  things are
building and looking good, and i'll keep an eye on the spark master builds
tonite and tomorrow.


"
Reynold Xin <rxin@databricks.com>,"Wed, 28 Jan 2015 22:31:14 -0800",Re: emergency jenkins restart soon,shane knapp <sknapp@berkeley.edu>,"Thanks for doing that, Shane!



"
shane knapp <sknapp@berkeley.edu>,"Wed, 28 Jan 2015 22:32:18 -0800",Re: emergency jenkins restart soon,Reynold Xin <rxin@databricks.com>,"np!  the master builds haven't triggered yet, but let's give the rube
goldberg machine a minute to get it's bearings.


"
DB Tsai <dbtsai@dbtsai.com>,"Wed, 28 Jan 2015 23:18:42 -0800",Re: LinearRegressionWithSGD accuracy,Robin East <robin.east@xense.co.uk>,"Hi Robin,

You can try this PR out. This has built-in features scaling, and has
ElasticNet regularization (L1/L2 mix). This implementation can stably
converge to model from R's glmnet package.

https://github.com/apache/spark/pull/4259

Sincerely,

DB Tsai
-------------------------------------------------------
Blog: https://www.dbtsai.com
LinkedIn: https://www.linkedin.com/in/dbtsai



all - a bit of trial and error shows that 0.00000001 works.
the step size explicitly:
rote:
:
y
)
)
mean()
e
on
1.n3.nabble.com/LinearRegressionWithSGD-accuracy-tp10127.html
e.com.

---------------------------------------------------------------------


"
Aniket Bhatnagar <aniket.bhatnagar@gmail.com>,"Thu, 29 Jan 2015 15:37:34 +0000",Re: Data source API | Support for dynamic schema,"Reynold Xin <rxin@databricks.com>, Cheng Lian <lian.cs.zju@gmail.com>","Thanks Reynold and Cheng. It does seem quiet a bit of heavy lifting to have
schema per row. I will for now settle with having to do a union schema of
all the schema versions and complain any incompatibilities :-)

Looking forward to do great things with the API!

Thanks,
Aniket


"
shane knapp <sknapp@berkeley.edu>,"Thu, 29 Jan 2015 08:32:50 -0800",Re: emergency jenkins restart soon,Reynold Xin <rxin@databricks.com>,"the master builds triggered around ~1am last night (according to the logs),
so it looks like we're back in business.


"
Robert C Senkbeil <rcsenkbe@us.ibm.com>,"Thu, 29 Jan 2015 11:25:03 -0600",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","
+1

I verified that the REPL jars published work fine with the Spark Kernel
project (can build/test against them).

Signed,
Chip Senkbeil



From:	Krishna Sankar <ksankar42@gmail.com>
To:	Sean Owen <sowen@cloudera.com>
Cc:	Patrick Wendell <pwendell@gmail.com>, ""dev@spark.apache.org""
            <dev@spark.apache.org>
Date:	01/28/2015 02:52 PM
Subject:	Re: [VOTE] Release Apache Spark 1.2.1 (RC2)



+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 12:22 min
     mvn clean package -Pyarn -Dyarn.version=2.6.0 -Phadoop-2.4
-Dhadoop.version=2.6.0
-Phive -DskipTests
2. Tested pyspark, mlib - running as well as compare results with 1.1.x &
1.2.0
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
2.3. Decision Tree, Naive Bayes OK
2.4. KMeans OK
       Center And Scale OK
       Fixed : org.apache.spark.SparkException in zip !
2.5. rdd operations OK
      State of the Union Texts - MapReduce, Filter,sortByKey (word count)
2.6. Recommendation (Movielens medium dataset ~1 M ratings) OK
       Model evaluation/optimization (rank, numIter, lmbda) with itertools
OK

Cheers
<k/>


?

(ClientState.java:423)
t
,
version
https://git-wip-us.apache.org/repos/asf?p=spark.git;a=commit;h=b77f87673d1f9f03d4c83cf583158227c551359b

 at:
062/
e
es
--

"
Mohit Jaggi <mohitjaggi@gmail.com>,"Thu, 29 Jan 2015 09:52:00 -0800","Re: RDD.combineBy without intermediate (k,v) pair allocation",francois.garillot@typesafe.com,"Francois,
RDD.aggregate() does not support aggregation by key. But, indeed, that is the kind of implementation I am looking for, one that does not allocate intermediate space for storing (K,V) pairs. When working with large datasets this type of intermediate memory allocation wrecks havoc with garbage collection, not to mention unnecessarily increases the working memory requirement of the program.

I wonder if someone has already noticed this and there is an effort underway to optimize this. If not, I will take a shot at adding this functionality.

Mohit.

‚Äúvalue‚Äù (payload) parts of the RDD elements is uniform (a function), it‚Äôs unclear to me how this would be more efficient that extracting key and value and then using combine, however.
do a combineByKey() operation. I can do that by creating an intermediate RDD of k,v pairs and using PairRDDFunctions.combineByKey(). However, I believe it will be more efficient if I can avoid this intermediate RDD. Is there a way I can do this by passing in a function that extracts the key, like in RDD.groupBy()? [oops, RDD.groupBy seems to create the intermediate RDD anyway, maybe a better implementation is possible for that too?] 


"
Evan Chan <velvia.github@gmail.com>,"Thu, 29 Jan 2015 11:32:53 -0800",Re: renaming SchemaRDD -> DataFrame,"""Evan R. Sparks"" <evan.sparks@gmail.com>","+1.... having proper NA support is much cleaner than using null, at
least the Java null.


---------------------------------------------------------------------


"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Thu, 29 Jan 2015 17:52:34 -0200",TimeoutException on tests,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,
I'm trying to use a local build spark, adding the pr 1290 to the 1.2.0
build and after I do the build, I my tests start to fail.
 should create labeledpoint *** FAILED *** (10 seconds, 50 milliseconds)
[info]   java.util.concurrent.TimeoutException: Futures timed out after
[10000 milliseconds]

It seems that this is related to a netty problem, I've already tried to
change the netty version but it didn't solved my problem (migrated from
3.4.0.Final, to 3.10.0.Final, does anyone here know how to fix it?

Kind Regards,
Dirceu
"
Octavian Geagla <ogeagla@gmail.com>,"Thu, 29 Jan 2015 13:40:38 -0700 (MST)","Re: Any interest in 'weighting' VectorTransformer which does
 component-wise scaling?",dev@spark.apache.org,"Thanks for the responses.  How would something like HadamardProduct or
similar be in order to keep it explicit?  Would still be a VectorTransformer
so the name and trait would hopefully lead to a somewhat self-documenting
class.  

Xiangrui, do you mean Hadamard product or transform?  My initial proposal
was only a vector-vector product, but I can extend this to matrices. The
transform would require a bit more work, which I'm willing to do, but I'm
not sure where FFT comes in, can you elaborate?



--

---------------------------------------------------------------------


"
rtshadow <pastuszka.przemyslaw@gmail.com>,"Thu, 29 Jan 2015 13:45:20 -0700 (MST)",How to speed PySpark to match Scala/Java performance,dev@spark.apache.org,"Hi,

In my company, we've been trying to use PySpark to run ETLs on our data.
Alas, it turned out to be terribly slow compared to Java or Scala API (which
we ended up using to meet performance criteria). 

To be more quantitative, let's consider simple case:
I've generated test file (848MB): /seq 1 100000000 > /tmp/test/

and tried to run simple computation on it, which includes three steps: read
-> multiply each row by 2 -> take max
Code in python: /sc.textFile(""/tmp/test"").map(lambda x: x * 2).max()/
Code in scala: /sc.textFile(""/tmp/test"").map(x => x * 2).max()/

Here are the results of this simple benchmark:
CPython - 59s
PyPy - 26s
Scala version - 7s

I didn't dig into what exactly contributes to execution times of CPython /
PyPy, but it seems that serialization / deserialization, when sending data
to the worker may be the issue. 
I know some guys already have been asking about using Jython
(http://apache-spark-developers-list.1001551.n3.nabble.com/Jython-importing-pyspark-td8654.html#a8658,
http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-Driver-from-Jython-td7142.html),
but it seems, that no one have really done this with Spark.
It looks like performance gain from using jython can be huge - you wouldn't
need to spawn PythonWorkers, all the code would be just executed inside
SparkExecutor JVM, using python code compiled to java bytecode. Do you think
that's possible to achieve? Do you see any obvious obstacles? Of course,
jython doesn't have C extensions, but if one doesn't need them, then it
should fit here nicely.

I'm willing to try to marry Spark with Jython and see how it goes.

What do you think about this?





--

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Thu, 29 Jan 2015 13:08:29 -0800",Re: How to speed PySpark to match Scala/Java performance,rtshadow <pastuszka.przemyslaw@gmail.com>,"Hey,

Without having Python as fast as Scala/Java, I think it's impossible to similar
performance in PySpark as in Scala/Java. Jython is also much slower than
Scala/Java.

With Jython, we can avoid the cost of manage multiple process and RPC,
we may still need to do the data conversion between Java and Python.
Given that fact that Jython is not widely used in production, it may introduce
more troubles than the performance gain.

Spark jobs can be easily speed up by scaling out (by adding more resources).
I think the most advantage of PySpark is that it let you do fast prototype.
pure Python
jobs into Scala to reduce the cost(it's optional).

Now days, engineer time is much more expensive than CPU time, I think we
should be more focus on the former.

That's my 2 cents.

Davies


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 29 Jan 2015 13:12:43 -0800",Re: How to speed PySpark to match Scala/Java performance,rtshadow <pastuszka.przemyslaw@gmail.com>,"Python and get the same performance. It can't express everything, but for
basic things like projection, filter, join, aggregate and simple numeric
computation, it should work pretty well.



"
Koert Kuipers <koert@tresata.com>,"Thu, 29 Jan 2015 16:24:45 -0500",Re: renaming SchemaRDD -> DataFrame,"""dev@spark.apache.org"" <dev@spark.apache.org>","to me the word DataFrame does come with certain expectations. one of them
is that the data is stored columnar. in R data.frame internally uses a list
of sequences i think, but since lists can have labels its more like a
SortedMap[String, Array[_]]. this makes certain operations very cheap (such
as adding a column).

in Spark the closest thing would be a data structure where per Partition
the data is also stored columnar. does spark SQL already use something like
that? Evan mentioned ""Spark SQL columnar compression"", which sounds like
it. where can i find that?

thanks


"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 29 Jan 2015 14:00:35 -0800",Re: renaming SchemaRDD -> DataFrame,"Koert Kuipers <koert@tresata.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Forgot to mention that you can find it here 
<https://github.com/apache/spark/blob/f9e569452e2f0ae69037644170d8aa79ac6b4ccf/sql/core/src/main/scala/org/apache/spark/sql/columnar/InMemoryColumnarTableScan.scala>.


‚Äã
"
Cheng Lian <lian.cs.zju@gmail.com>,"Thu, 29 Jan 2015 13:59:09 -0800",Re: renaming SchemaRDD -> DataFrame,"Koert Kuipers <koert@tresata.com>, 
 ""dev@spark.apache.org"" <dev@spark.apache.org>","Yes, when a DataFrame is cached in memory, it's stored in an efficient 
columnar format. And you can also easily persist it on disk using 
Parquet, which is also columnar.

Cheng



---------------------------------------------------------------------


"
Sasha Kacanski <skacanski@gmail.com>,"Thu, 29 Jan 2015 19:31:41 -0500",Re: How to speed PySpark to match Scala/Java performance,Reynold Xin <rxin@databricks.com>,"Hi Reynold,
In my project I want to use Python API too.
When you mention DF's are we talking about pandas or this is something
internal to spark py api.
If you could elaborate a bit on this or point me to alternate documentation.
Thanks much --sasha





-- 
Aleksandar Kacanski
"
Reynold Xin <rxin@databricks.com>,"Thu, 29 Jan 2015 16:41:35 -0800",Re: How to speed PySpark to match Scala/Java performance,Sasha Kacanski <skacanski@gmail.com>,"It is something like this: https://issues.apache.org/jira/browse/SPARK-5097




"
Sasha Kacanski <skacanski@gmail.com>,"Thu, 29 Jan 2015 19:44:25 -0500",Re: How to speed PySpark to match Scala/Java performance,Reynold Xin <rxin@databricks.com>,"thanks for quick reply, I will check the link.
Hopefully, with conversion to py3, or 3.4 we could take advantage of
asyncio and other cool new stuff ...




-- 
Aleksandar Kacanski
"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 30 Jan 2015 16:06:48 -0800",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)

Ran spark-shell and Scala jobs on top of yarn (using the hadoop-2.4 tarball).

There's a very slight behavioral change in the API. This code now throws an NPE:

  new SparkConf().setIfMissing(""foo"", null)

It worked before. It's probably"
ankits <ankitsoni9@gmail.com>,"Fri, 30 Jan 2015 19:15:55 -0700 (MST)",Get size of rdd in memory,dev@spark.apache.org,"Hi,

I want to benchmark the memory savings by using the in-memory columnar
storage for schemardds (using cacheTable) vs caching the SchemaRDD directly.
It would be really helpful to be able to query this from the spark-shell or
jobs directly. Could a dev point me to the way to do this? From what I
understand i will need a reference to the block manager, or something like
RDDInfo.fromRdd(rdd).memSize.

I could use reflection or whatever to override the private access modifiers.



--

---------------------------------------------------------------------


"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 30 Jan 2015 19:54:25 -0800",Re: Get size of rdd in memory,"ankits <ankitsoni9@gmail.com>, dev@spark.apache.org","Here is a toy |spark-shell| session snippet that can show the memory 
consumption difference:

|import  org.apache.spark.sql.SQLContext
import  sc._

val  sqlContext  =  new  SQLContext(sc)
import  sqlContext._

setConf(""spark.sql.shuffle.partitions"",""1"")

case  class  KV(key:Int, value:String)

parallelize(1  to1024).map(i =>KV(i, i.toString)).toSchemaRDD.cache().count()
parallelize(1  to1024).map(i =>KV(i, i.toString)).cache().count()
|

You may see the result from the storage page of the web UI. It suggests 
the in-memory columnar version uses 11.6KB while the raw RDD version 
uses 76.6KB on my machine.

Not quite sure how to do the comparison programmatically. You can track 
the data source of the ‚ÄúSize in Memory‚Äù field showed in the web UI 
storage tab.

Cheng


‚Äã
"
Anjana Fernando <lafernando@gmail.com>,"Sat, 31 Jan 2015 14:25:24 +0530",Custom Cluster Managers / Standalone Recovery Mode in Spark,dev@spark.apache.org,"Hi everyone,

I've been experimenting, and somewhat of a newbie for Spark. I was
wondering, if there is any way, that I can use a custom cluster manager
implementation with Spark. Basically, as I understood, at the moment, the
inbuilt modes supported are with standalone, Mesos and  Yarn. My
requirement is basically a simple clustering solution with high
availability of the master. I don't want to use a separate Zookeeper
cluster, since this would complicate my deployment, but rather, I would
like to use something like Hazelcast, which has a peer-to-peer cluster
coordination implementation.

I found that, there is already this JIRA [1], which requests for a custom
persistence engine, I guess for storing state information. So basically,
what I would want to do is, use Hazelcast to use for leader election, to
make an existing node the master, and to lookup the state information from
the distributed memory. Appreciate any help on how to archive this. And if
it useful for a wider audience, hopefully I can contribute this back to the
project.

[1] https://issues.apache.org/jira/browse/SPARK-1180

Cheers,
Anjana.
"
nl32 <neilp23@gmail.com>,"Sat, 31 Jan 2015 02:02:10 -0700 (MST)",Disabling eviction warnings when using sbt,dev@spark.apache.org,"I am trying to disabling eviction warnings when using sbt, such as these:
[warn] There may be incompatibilities among your library dependencies.[warn]
Here are some of the libraries that were evicted:[warn] 	*
com.typesafe.sbt:sbt-git:0.6.1 -> 0.6.2[warn] 	*
com.typesafe.sbt:sbt-site:0.7.0 -> 0.7.1[warn] Run 'evicted' to see detailed
eviction warnings
I am using this line to disable eviction warnings:
evictionWarningOptions in update :=
EvictionWarningOptions.default.withWarnTransitiveEvictions(false).withWarnDirectEvictions(false).withWarnScalaVersionEviction(false)
as found here https://github.com/sbt/sbt/issues/1636#issuecomment-57498141.
But I am not sure where it belongs in project/ScalaBuild.scalaI've tried
putting it in SparkBuild.sharedSettings, but it doesn't work. Can anyone
help?



--"
MartinWeindel <martin.weindel@gmail.com>,"Sat, 31 Jan 2015 12:57:14 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.2.1 (RC2),dev@spark.apache.org,"FYI: Spark 1.2.1rc2 does not work on Windows!

machine:
INFO  org.apache.spark.SparkEnv:59 - Registering BlockManagerMaster
ERROR org.apache.spark.util.Utils:75 - Failed to create local root dir in
C:\Users\mweindel\AppData\Local\Temp\. Ignoring this directory.
ERROR org.apache.spark.storage.DiskBlockManager:75 - Failed to create any
local dir.

I have already located the cause. A newly added function chmod700() in
org.apache.util.Utils uses functionality which only works on a Unix file
system.

See also pull request [https://github.com/apache/spark/pull/4299] for my
suggestion how to resolve the issue.

Best regards,

Martin Weindel



--

---------------------------------------------------------------------


"
