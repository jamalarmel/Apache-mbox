Sun Rui <sunrise_win@163.com>,"Wed, 1 Jun 2016 22:42:38 +0800",Re: Windows Rstudio to Linux spakR,Selvam Raman <selmna@gmail.com>,"Selvam,

First, deploy the Spark distribution on your Windows machine, which is of the same version of Spark in your Linux cluster

Second, follow the instructions at https://github.com/apache/spark/tree/master/R#using-sparkr-from-rstudio. Specify the Spark master URL for your Linux Spark cluster when calling sparkR.init(). Don’t know your Spark cluster deployment mode. If it is YARN, you may have to copy YARN conf files from your cluster and set YARN_CONF_DIR environment variable to point to it.

These steps are my personal understanding, I have not tested in this scenario. Please report if you have any problem.

Rstudio(Windows env).
தவிர்த்து நெஞ்சம் நிமிர்த்து""

"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Wed, 1 Jun 2016 21:01:07 +0530",ImportError: No module named numpy,"user <user@spark.apache.org>, dev <dev@spark.apache.org>","If any one please can help me with following error.

 File
""/opt/mapr/spark/spark-1.6.1/python/lib/pyspark.zip/pyspark/mllib/__init__.py"",
line 25, in <module>

ImportError: No module named numpy


Thanks in advance!
"
Holden Karau <holden@pigscanfly.ca>,"Wed, 1 Jun 2016 08:32:27 -0700",Re: ImportError: No module named numpy,Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Generally this means numpy isn't installed on the system or your PYTHONPATH
has somehow gotten pointed somewhere odd,




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Wed, 1 Jun 2016 21:26:45 +0530",Re: ImportError: No module named numpy,Holden Karau <holden@pigscanfly.ca>,"Thanks .
How can this be resolved?


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 01 Jun 2016 15:58:26 +0000",Re: [DISCUSS] Removing or changing maintainer process,"Matei Zaharia <matei.zaharia@gmail.com>, dev <dev@spark.apache.org>","I just heard about mention-bot at PyCon 2016
<https://www.youtube.com/watch?v=gRFHvavxnos>:

https://github.com/facebook/mention-bot

Do you have a GitHub project that is too big for people to subscribe to all
the notifications? The mention bot will automatically mention potential
reviewers on pull requests. It helps getting faster turnaround on pull
requests by involving the right people early on.

mention-bot checks the blame history for the files modified by a PR and
automatically pings the most likely candidates for review. I wonder if it
would work well for us.

Nick

m

I’ve also heard that we should try to keep some other instructions for
e great to see
tch and having
ReviewProcessandMaintainers)
ng
o
 be
nd
"
=?UTF-8?Q?Sergio_Fern=C3=A1ndez?= <wikier@apache.org>,"Wed, 1 Jun 2016 20:09:47 +0200",Re: ImportError: No module named numpy,Bhupendra Mishra <bhupendra.mishra@gmail.com>,"sudo pip install numpy

m

:
t__.py"",


-- 
Sergio Fernández
Partner Technology Manager
Redlink GmbH
m: +43 6602747925
e: sergio.fernandez@redlink.co
w: http://redlink.co
"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Wed, 1 Jun 2016 23:46:22 +0530",Re: ImportError: No module named numpy,=?UTF-8?Q?Sergio_Fern=C3=A1ndez?= <wikier@apache.org>,"I have numpy installed but where I should setup PYTHONPATH?



it__.py"",
"
Julio Antonio Soto de Vicente <julio@esbet.es>,"Wed, 1 Jun 2016 20:34:08 +0200",Re: ImportError: No module named numpy,Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Try adding to spark-env.sh (renaming if you still have it with .template at the end):

PYSPARK_PYTHON=/path/to/your/bin/python

Where your bin/python is your actual Python environment with Numpy installed.


scribió:
te:
PATH has somehow gotten pointed somewhere odd,
b/__init__.py"", line 25, in <module>
"
Sean Owen <sowen@cloudera.com>,"Wed, 1 Jun 2016 13:36:06 -0500",Spark 2.0.0-preview artifacts still not available in Maven,"dev <dev@spark.apache.org>, Reynold Xin <rxin@databricks.com>","Just checked and they are still not published this week. Can these be
published ASAP to complete the 2.0.0-preview release?

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Jun 2016 13:06:35 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"They are here ain't they?

https://repository.apache.org/content/repositories/orgapachespark-1182/

Did you mean publishing them to maven central? My understanding is that
publishing to maven central isn't a required step of doing theses. This
might be a good opportunity to discuss that. My thought is that it is since
Maven central is immutable, and the purposes of the preview releases are to
get people to test it early on in preparation for the actual release, it
might be better to not publish preview releases to maven central. Users
testing with preview releases can just use the temporary repository above.





"
Michael Armbrust <michael@databricks.com>,"Wed, 1 Jun 2016 13:46:46 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Reynold Xin <rxin@databricks.com>,"Yeah, we don't usually publish RCs to central, right?


"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Jun 2016 13:50:41 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Michael Armbrust <michael@databricks.com>,"To play devil's advocate, previews are technically not RCs. They are
actually voted releases.


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 1 Jun 2016 13:53:55 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Reynold Xin <rxin@databricks.com>,"So are RCs, aren't they?

Personally I'm fine with not releasing to maven central. Any extra
effort needed by regular users to use a preview / RC is good with me.




-- 
Marcelo

---------------------------------------------------------------------


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Wed, 01 Jun 2016 21:34:23 +0000",Re: Spark 2.0.0-preview artifacts still not available in Maven,"Marcelo Vanzin <vanzin@cloudera.com>, Reynold Xin <rxin@databricks.com>","I think what Reynold probably means is that previews are releases for which
a vote *passed*.

~ Jonathan


"
Sean Owen <sowen@cloudera.com>,"Wed, 1 Jun 2016 16:51:59 -0500",Re: Spark 2.0.0-preview artifacts still not available in Maven,Marcelo Vanzin <vanzin@cloudera.com>,"An RC is something that gets voted on, and the final one is turned
into a blessed release. I agree that RCs don't get published to Maven
Central, but releases do of course.

This was certainly to be an official release, right? A beta or alpha
can still be an official, published release. The proximate motivation
was to solve a problem of advertising ""Apache Spark 2.0.0 preview"" in
a product, when no such release existed from the ASF. Hence the point
was to produce a full regular release, and I think that needs to
include the usual Maven artifacts.

I'd think we want less effort, not more, to let people test it? for
example, right now I can't easily try my product build against
2.0.0-preview.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 1 Jun 2016 15:00:03 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"
While I understand your point of view, I like the extra effort to get
to these artifacts because it prevents people from easily building
their applications on top of what is known to be an unstable release
(either API-wise or quality wise).

I see this preview release more like a snapshot release that was voted
on for wide testing, instead of a proper release that we want to
encourage people to build on. And like snapshots, I like that to use
it on your application you have to go out of your way and add a
separate repository instead of just changing a version string or
command line argument.

My 2 bits.

-- 
Marcelo

---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 1 Jun 2016 15:02:31 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"

I don't feel super strongly one way or the other, so if we need to publish
it permanently we can.

However, either way you can still test against this release.  You just need
to add a resolver as well (which is how I have always tested packages
is already fairly far behind branch-2.0, so many of the issues that people
might report have already been fixed and that might continue even after the
release is made.  I'd rather be able to force upgrades eventually when we
vote on the final 2.0 release.
"
Sean Owen <sowen@cloudera.com>,"Wed, 1 Jun 2016 17:10:06 -0500",Re: Spark 2.0.0-preview artifacts still not available in Maven,Michael Armbrust <michael@databricks.com>,"I'll be more specific about the issue that I think trumps all this,
which I realize maybe not everyone was aware of.

There was a long and contentious discussion on the PMC about, among
other things, advertising a ""Spark 2.0 preview"" from Databricks, such
as at https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html

That post has already been updated/fixed from an earlier version, but
part of the resolution was to make a full ""2.0.0 preview"" release in
order to continue to be able to advertise it as such. Without it, I
believe the PMC's conclusion remains that this blog post / product
announcement is not allowed by ASF policy. Hence, either the product
announcements need to be taken down and a bunch of wording changed in
the Databricks product, or, this needs to be a normal release.

Obviously, it seems far easier to just finish the release per usual. I
actually didn't realize this had not been offered for download at
http://spark.apache.org/downloads.html either. It needs to be
accessible there too.


We can get back in the weeds about what a ""preview"" release means,
but, normal voted releases can and even should be alpha/beta
(http://www.apache.org/dev/release.html) The culture is, in theory, to
release early and often. I don't buy an argument that it's too old, at
2 weeks, when the alternative is having nothing at all to test
against.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 1 Jun 2016 15:58:09 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"Hi Sean,

(writing this email with my Apache hat on only and not Databricks hat)

The preview release is available here:
http://spark.apache.org/downloads.html (there is an entire section
dedicated to it and also there is a news link to it on the right).

Again, I think this is a good opportunity to define what a release should
contain. Based on
http://www.apache.org/dev/release.html#where-do-releases-go

""In addition to the distribution directory, project that use Maven or a
related build tool sometimes place their releases on repository.apache.org
beside some convenience binaries. The distribution directory is required,
while the repository system is an optional convenience.""

So I'm reading it as that maven publication is not necessary. My
understanding is that the general community (beyond who follows the dev
list) should understand that preview is not a stable release, and we as the
PMC should set expectations accordingly. Developers that can test the
preview releases tend to be more savvy and are comfortable on the bleeding
edge. It is actually fairly easy for them to add a maven repo. Now reading
the page I realized no where on the page did we mention the temporary maven
repo. I will fix that.

I think it'd be pretty bad if preview releases in anyway become ""default
version"", because they are unstable and contain a lot of blocker bugs.

So my concrete proposal is:

1. Separate (officially voted) releases into normal and preview.

releases, and the other listing preview releases.

3. Everywhere we mention preview releases, include the proper disclaimer
e.g. ""This preview is not a stable release in terms of either API or
functionality, but it is meant to give the community early access to try
the code that will become Spark 2.0.""

4. Publish normal releases to maven central, and preview releases only to
the staging maven repo. But of course we should include the temporary maven
repo for preview releases on the download page.







"
Thomas Gerber <thomas.gerber@radius.com>,"Wed, 1 Jun 2016 16:04:55 -0700",OOM while unrolling in Spark 1.5.2,dev@spark.apache.org,"Hello,

I came across a weird non-easily replicable OOM in executor during
unrolling. The standalone cluster uses default memory settings on Spark
1.5.2.

What strikes me is that the OOM happens when Spark tries to allocate a
bytebuffer for the FileChannel when dropping blocks from memory and writing
them on disk because of storage level.

To be fair, the cluster is under lots of memory pressure.

I was thinking decreasing the spark.storage.safetyFraction to give more
breathing room to Spark. Is that the right way to think here?

The stacktrace:

java.lang.OutOfMemoryError
    at sun.misc.Unsafe.allocateMemory(Native Method)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:127)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306)
    at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:174)
    at sun.nio.ch.IOUtil.write(IOUtil.java:58)
    at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:205)
    at
org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply$mcV$sp(DiskStore.scala:50)
    at
org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:49)
    at
org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:49)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206)
    at org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:52)
    at
org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1043)
    at
org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1002)
    at
org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:468)
    at
org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:457)
    at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at
org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:457)
    at
org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:292)
    at
org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:262)
    at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)


Thanks,
*Thomas Gerber*
Director of Data Engineering

<http://radius.com/>
java.lang.OutOfMemoryError
    at sun.misc.Unsafe.allocateMemory(Native Method)
    at java.nio.DirectByteBuffer.<init>(DirectByteBuffer.java:127)
    at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306)
    at sun.nio.ch.Util.getTemporaryDirectBuffer(Util.java:174)
    at sun.nio.ch.IOUtil.write(IOUtil.java:58)
    at sun.nio.ch.FileChannelImpl.write(FileChannelImpl.java:205)
    at org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply$mcV$sp(DiskStore.scala:50)
    at org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:49)
    at org.apache.spark.storage.DiskStore$$anonfun$putBytes$1.apply(DiskStore.scala:49)
    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206)
    at org.apache.spark.storage.DiskStore.putBytes(DiskStore.scala:52)
    at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1043)
    at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1002)
    at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:468)
    at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:457)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:457)
    at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:292)
    at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
    at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:262)
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:300)
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
---------------------------------------------------------------------"
Sean Owen <sowen@cloudera.com>,"Wed, 1 Jun 2016 18:26:53 -0500",Re: Spark 2.0.0-preview artifacts still not available in Maven,Reynold Xin <rxin@databricks.com>,"
Oops, it is indeed down there at the bottom, before nightlies. I
honestly missed it below the fold. I'd advocate for making it a (non
default?) option in the main downloads dropdown, but this then becomes
a minor issue. The core source/binary artifacts _are_ publicly
available.



Agree. The question is what makes this release special? because other
releases have been published to Maven. I think the argument is that
it's a buggy alpha/beta/preview release, but so were 0.x releases.
Reasonable people could make up different policies, so here I'm
appealing to guidance: http://www.apache.org/dev/release.html

""Releases are packages that have been approved for general public
release, with varying degrees of caveat regarding their perceived
quality or potential for change. Releases that are intended for
everyday usage by non-developers are usually referred to as ""stable""
or ""general availability (GA)"" releases. Releases that are believed to
be usable by testers and developers outside the project, but perhaps
not yet stable in terms of features or functionality, are usually
referred to as ""beta"" or ""unstable"". Releases that only represent a
project milestone and are intended only for bleeding-edge developers
working outside the project are called ""alpha"".""

I don't think releases are defined by whether they're stable or buggy,
but by whether they were produced by a sanctioned process that
protects contributors under the ASF umbrella, etc etc. Compare to a
nightly build which we don't want everyone to consume, not so much
because it might be buggier, but because these protections don't
apply.

Certainly, it's vital to communicate how to interpret the stability of
the releases, but -preview releases are still normal releases to the
public.

I don't think bugginess therefore is the question. Any Spark dev knows
that x.y.0 Spark releases have gone out with even Critical and in the
past Blocker issues unresolved, and the world failed to fall apart.
(We're better about this now.) I actually think the -preview release
idea is worth repeating for this reason -- .0-preview is the new .0.
It'd be more accurate IMHO and better for all.



Why would this happen? releases happen ~3 months and could happen
faster if this is a concern. 2.0.0 final is, I'd wager, coming in <1
month.



+1, that puts it above the fold and easily findable to anyone willing
to consume such a thing.



Can't hurt to overcommunicate this for -preview releases in general.



This is the only thing I disagree with. AFAIK other ASF projects
readily publish alpha and beta releases, under varying naming
conventions (alpha, beta, RC1, etc) It's not something that needs to
be hidden like a nightly.

The audience for Maven artifacts are developers, not admins or users.
Compare the risk of a developer somehow not understanding what they're
getting, to the friction caused by making developers add a repo to get
at it.

I get it, that seems minor. But given the recent concern about making
sure ""2.0.0 preview"" is available as an ASF release, I'd advise us to
make sure this release is not any harder to get at than others, to
really put that to bed.

---------------------------------------------------------------------


"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Thu, 2 Jun 2016 13:29:49 +0530",Re: ImportError: No module named numpy,Julio Antonio Soto de Vicente <julio@esbet.es>,"its RHEL

and i have already exported environment variable in spark-env.sh as
follows.. error still there  error: ImportError: No module named numpy

export PYSPARK_PYTHON=/usr/bin/python

thanks


nit__.py"",
"
=?UTF-8?Q?Sergio_Fern=C3=A1ndez?= <wikier@apache.org>,"Thu, 2 Jun 2016 11:31:36 +0200",Re: ImportError: No module named numpy,Bhupendra Mishra <bhupendra.mishra@gmail.com>,"m

According the documentation at
http://spark.apache.org/docs/latest/configuration.html#environment-variables
the PYSPARK_PYTHON environment variable is for poniting to the Python
interpreter binary.

If you check the programming guide
https://spark.apache.org/docs/0.9.0/python-programming-guide.html#installing-and-configuring-pyspark
it says you need to add your custom path to PYTHONPATH (the script
automatically adds the bin/pyspark there).

So typically in Linux you would need to add the following (assuming you
installed numpy there):

export PYTHONPATH=$PYTHONPATH:/usr/lib/python2.7/dist-packages

Hope that helps.




g>
init__.py"",


-- 
Sergio Fernández
Partner Technology Manager
Redlink GmbH
m: +43 6602747925
e: sergio.fernandez@redlink.co
w: http://redlink.co
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 2 Jun 2016 11:35:14 +0200","SPARK_YARN_MODE, yarn-client master URL and SparkILoop",dev <dev@spark.apache.org>,"Hi,

While reviewing where SPARK_YARN_MODE is used and how, I found one
""weird"" place where the ""yarn-client"" is checked against - see
https://github.com/apache/spark/blob/master/repl/scala-2.10/src/main/scala/org/apache/spark/repl/SparkILoop.scala#L946.

Since yarn-client (and yarn-cluster) are no longer in use, I'm pretty
sure it's of no use and could be safely removed. If not, we should do
something with it anyway.

Please guide before I file a JIRA issue. Thanks.


Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Thu, 2 Jun 2016 19:02:57 +0530",Re: ImportError: No module named numpy,=?UTF-8?Q?Sergio_Fern=C3=A1ndez?= <wikier@apache.org>,"did not resolved. :(

rote:

les
ing-and-configuring-pyspark
e
m>
rg>
_init__.py"",
"
Sean Owen <srowen@gmail.com>,"Thu, 2 Jun 2016 08:35:33 -0500",Re: Some minor LICENSE and NOTICE issues with 2.0 preview release,"dev@spark.apache.org, Justin Mclean <justin@classsoftware.com>","+dev

rote:
he ones at [1] do not. Adding “apache” gives you some extra legal protection.

As to why just 'spark' -- I believe it's merely historical. My
understanding of the trademark policy from discussions over the past
month is that software ident"
Sean Owen <sowen@cloudera.com>,"Thu, 2 Jun 2016 08:39:11 -0500","Re: SPARK_YARN_MODE, yarn-client master URL and SparkILoop",Jacek Laskowski <jacek@japila.pl>,"I think they're deprecated, not necessarily entirely unused. I
personally might leave it, but don't feel strongly about it.


---------------------------------------------------------------------


"
nguyen duc tuan <newvalue92@gmail.com>,"Thu, 2 Jun 2016 20:46:09 +0700",Re: ImportError: No module named numpy,Bhupendra Mishra <bhupendra.mishra@gmail.com>,"​​
You should set both PYSPARK_DRIVER_PYTHON and PYSPARK_PYTHON the path to
your python interpreter.

2016-06-02 20:32 GMT+07:00 Bhupendra Mishra <bhupendra.mishra@gmail.com>:

bles
ling-and-configuring-pyspark
org>
__init__.py"",
"
Marcin Tustin <mtustin@handybook.com>,"Thu, 2 Jun 2016 10:28:33 -0400",Re: Some minor LICENSE and NOTICE issues with 2.0 preview release,Sean Owen <srowen@gmail.com>,"Changing the maven co-ordinates is going to cause everyone in the world who
uses a maven-based build system to have update their builds. Given that sbt
uses ivy by default, that's likely to affect almost every spark user.

Unless we can articulate what the extra legal protections are (and frankly
I don't believe that having or not having apache in the maven co-ordinates
or jar filenames makes a jot of difference - I'm happy to be proved wrong)
I'm strongly negative on such a change.

Marcin


 the ones at [1]
tion.
e
s

-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Sean Owen <sowen@cloudera.com>,"Thu, 2 Jun 2016 09:45:26 -0500",Re: Some minor LICENSE and NOTICE issues with 2.0 preview release,Marcin Tustin <mtustin@handybook.com>,"In this case we're just talking about the name of the .tgz archives
that are distributed for download. I agree we would not want to change
the Maven coordinates.

:
ho
bt
y I
or
'm
m the ones at [1]
tection.
be
rce
 be
t is
r

---------------------------------------------------------------------


"
Justin Mclean <justin@classsoftware.com>,"Fri, 3 Jun 2016 01:22:22 +1000",Re: Some minor LICENSE and NOTICE issues with 2.0 preview release,Sean Owen <srowen@gmail.com>,"Hi,

past

Yes it's not required, but given the branding issues it may useful to do.

files have a year range.
yes.

Most NOTICE files would say 201X-2016 but the above seems a valid approach.


For starters permissive licenses are mentioned, NOTICE is for required notices only. I'd suggest you ask on general @incubator mailing list for a review. People there have knowledge and experience of what should and should not be in a NOTICE file.


The source license and notice should be different from the binary licence and notice as the contents of the artefacts are different. [1]


So they should not be mentioned in the source license or notice files. [1]

not-wrong to maintain one file that

Having extra stuff in NOTICE is at worse a documentation error, so t’snot a legal issue as such and at worse a minor issue, but it does impose an extra burden on downstream projects. IMO would be best if this wasn’t the case, Again refer to the guiding principle and also [1]


It’s never necessary to have jar in a source release, they can be compiled as part of the build process. I’s suggest you read Roy’s view on this [2].

And again this is just my option as an outsider, I may be missing info etc etc etc. It’s certainly not legal advice or insistence that anything needs to be changed, that’s up to the PMC to decide.

Also no need for a long discussion on this I think I've provided enough info for the PMC to decide if anything needs to change or not.

Thanks,
Justin

1. http://www.apache.org/dev/licensing-howto.html#binary
2. http://mail-archives.apache.org/mod_mbox/incubator-general/201203.mbox/%3C0F5691A1-97C0-444F-A514-B2E4E8E907DA@gbiv.com%3E
---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Thu, 2 Jun 2016 10:44:38 -0500",Re: Some minor LICENSE and NOTICE issues with 2.0 preview release,Justin Mclean <justin@classsoftware.com>,"rote:
tices only. I'd suggest you ask on general @incubator mailing list for a review. People there have knowledge and experience of what should and should not be in a NOTICE file.

The entries at the beginning are all Category B licenses, and ...
""Software under the following licenses may be included in binary form
within an Apache product if the inclusion is appropriately labeled
(see below):""

This is my understanding of ""appropriate labeling"".


 and notice as the contents of the artefacts are different. [1]

It's a fair point that this would be better. I'll put it on my radar.


 compiled as part of the build process. I’s suggest you read Roy’s view on this [2].

I don't think that's true, if a test needs a jar to load to test jar
loading. That's what this is AFAIK, just like including a JPEG in the
source distro to support a JPEG test. It is not black-box compiled
code that is part of the software product, which is what Roy is
correctly calling out as not OK in a source release.

---------------------------------------------------------------------


"
Sean Owen <srowen@apache.org>,"Thu, 02 Jun 2016 18:14:19 +0000","Re: Fixing up ""Spark"" mentions on CDH docs",dev@spark.apache.org,"Agree, not private really. I thought it might be the smaller audience that
is actually interested but no reason to not share a bit more widely.


all_spark.html
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 2 Jun 2016 19:42:41 +0000 (UTC)",Re: Spark 2.0.0-preview artifacts still not available in Maven,"Sean Owen <sowen@cloudera.com>, Reynold Xin <rxin@databricks.com>","The documentation for the preview release also seem to be missing?
Also what happens if we want to do a second preview release?  The naming doesn't seem to allow then unless we call it preview 2.
Tom 

e:
 

ted

Oops, it is indeed down there at the bottom, before nightlies. I
honestly missed it below the fold. I'd advocate for making it a (non
default?) option in the main downloads dropdown, but this then becomes
a minor issue. The core source/binary artifacts _are_ publicly
available.


g

Agree. The question is what makes this release special? because other
releases have been published to Maven. I think the argument is that
it's a buggy alpha/beta/preview release, but so were 0.x releases.
Reasonable people could make up different policies, so here I'm
appealing to guidance: http://www.apache.org/dev/release.html

""Releases are packages that have been approved for general public
release, with varying degrees of caveat regarding their perceived
quality or potential for change. Releases that are intended for
everyday usage by non-developers are usually referred to as ""stable""
or ""general availability (GA)"" releases. Releases that are believed to
be usable by testers and developers outside the project, but perhaps
not yet stable in terms of features or functionality, are usually
referred to as ""beta"" or ""unstable"". Releases that only represent a
project milestone and are intended only for bleeding-edge developers
working outside the project are called ""alpha"".""

I don't think releases are defined by whether they're stable or buggy,
but by whether they were produced by a sanctioned process that
protects contributors under the ASF umbrella, etc etc. Compare to a
nightly build which we don't want everyone to consume, not so much
because it might be buggier, but because these protections don't
apply.

Certainly, it's vital to communicate how to interpret the stability of
the releases, but -preview releases are still normal releases to the
public.

I don't think bugginess therefore is the question. Any Spark dev knows
that x.y.0 Spark releases have gone out with even Critical and in the
past Blocker issues unresolved, and the world failed to fall apart.
(We're better about this now.) I actually think the -preview release
idea is worth repeating for this reason -- .0-preview is the new .0.
It'd be more accurate IMHO and better for all.



Why would this happen? releases happen ~3 months and could happen
faster if this is a concern. 2.0.0 final is, I'd wager, coming in <1
month.


es,

+1, that puts it above the fold and easily findable to anyone willing
to consume such a thing.


the

Can't hurt to overcommunicate this for -preview releases in general.


en

This is the only thing I disagree with. AFAIK other ASF projects
readily publish alpha and beta releases, under varying naming
conventions (alpha, beta, RC1, etc) It's not something that needs to
be hidden like a nightly.

The audience for Maven artifacts are developers, not admins or users.
Compare the risk of a developer somehow not understanding what they're
getting, to the friction caused by making developers add a repo to get
at it.

I get it, that seems minor. But given the recent concern about making
sure ""2.0.0 preview"" is available as an ASF release, I'd advise us to
make sure this release is not any harder to get at than others, to
really put that to bed.

---------------------------------------------------------------------



  "
Reynold Xin <rxin@databricks.com>,"Thu, 2 Jun 2016 13:01:56 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Tom Graves <tgraves_cs@yahoo.com>,"projects (e.g. Scala).

So we can have Apache Spark 2.1.0-M1, Apache Spark 2.1.0-M2.





"
Justin Mclean <justin@classsoftware.com>,"Fri, 3 Jun 2016 08:47:26 +1000",Re: Some minor LICENSE and NOTICE issues with 2.0 preview release,Sean Owen <sowen@cloudera.com>,"Hi,


Thanks for the consideration. If you need anything reviewed just ask.

Justin

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Fri, 3 Jun 2016 08:53:11 +0000",Re: Spark 2.0.0-preview artifacts still not available in Maven,dev <dev@spark.apache.org>,"
It's been voted on by the project, so can go up on central

There's already some JIRAs being filed against it, this is a metric of success as pre-beta of the artifacts.

The risk of exercising the m2 central option is that people may get expectations that they can point their code at the 2.0.0-preview and then, when a release comes out, simply
update their dependency; this may/may not be the case. But is it harmful if people do start building and testing against the preview? If it finds problems early, it can only be a good thing


review-easier-faster-and-smarter.html
sh
eed
se
le
the
e


---------------------------------------------------------------------


"
=?UTF-8?Q?Jos=c3=a9_Manuel_Abu=c3=adn_Mosquera?= <abuinjm@gmail.com>,"Fri, 3 Jun 2016 13:02:12 +0200","Implementing linear albegra operations in the distributed linalg
 package",dev@spark.apache.org,"Hello,

I would like to add some linear algebra operations to all the 
DistributedMatrix classes that Spark actually handles (CoordinateMatrix, 
BlockMatrix, IndexedRowMatrix and RowMatrix), but first I would like do 
ask if you consider this useful. (For me, it is)

Of course, these operations will be distributed, but they will rely on 
the local implementation of mllib linalg. For example, when multiplying 
an IndexedRowMatrix by a DenseVector, the multiplication of one of the 
matrix rows by the vector will be performed by using the local 
implementation

What is your opinion about it?

Thank you

-- 
José Manuel Abuín Mosquera
Pre-doctoral researcher
Centro de Investigación en Tecnoloxías da Información (CiTIUS)
University of Santiago de Compostela
15782 Santiago de Compostela, Spain

http://citius.usc.es/equipo/investigadores-en-formacion/josemanuel.abuin
http://jmabuin.github.io


---------------------------------------------------------------------


"
=?UTF-8?B?546L5pmT6Zuo?= <g.wangxy@gmail.com>,"Fri, 3 Jun 2016 20:58:15 +0800",Any one can help to merge this pull request about Spark Thrift Server HA,dev@spark.apache.org,"Hi developers!
I submit a pull request for a long time.
This pull request is about Spark Thrift Server HA issue.
https://github.com/apache/spark/pull/9113
Any one can help to merge this pull request?
Thanks!
"
Gerhard Fiedler <gfiedler@algebraixdata.com>,"Fri, 3 Jun 2016 15:01:14 +0000",Where is DataFrame.scala in 2.0?,"""dev@spark.apache.org"" <dev@spark.apache.org>","When I look at the sources in Github, I see DataFrame.scala at https://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala in the 1.6 branch. But when I change the branch to branch-2.0 or master, I get a 404 error. I also can't find the file in the directory listings, for example https://github.com/apache/spark/tree/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql (for branch-2.0).

It seems that quite a few APIs use the DataFrame class, even in 2.0. Can someone please point me to its location, or otherwise explain why it is not there?

Thanks,
Gerhard

"
=?UTF-8?Q?Herman_van_H=C3=B6vell_tot_Westerflier?= <hvanhovell@questtec.nl>,"Fri, 3 Jun 2016 17:04:55 +0200",Re: Where is DataFrame.scala in 2.0?,Gerhard Fiedler <gfiedler@algebraixdata.com>,"Hi Gerhard,

DataFrame and DataSet have been merged in Spark 2.0. A DataFrame is now a
DataSet that contains Row objects. We still maintain a type alias for
DataFrame:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L45

HTH

Kind regards,

Herman van Hövell tot Westerflier

2016-06-03 17:01 GMT+02:00 Gerhard Fiedler <gfiedler@algebraixdata.com>:

rg/apache/spark/sql/DataFrame.scala
I
ings, for
rg/apache/spark/sql
ot
"
Michael Malak <michaelmalak@yahoo.com.INVALID>,"Fri, 3 Jun 2016 15:05:22 +0000 (UTC)",Re: Where is DataFrame.scala in 2.0?,"Gerhard Fiedler <gfiedler@algebraixdata.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","It's been reduced to a single line of code.
http://technicaltidbit.blogspot.com/2016/03/dataframedataset-swap-places-in-spark-20.html




      From: Gerhard Fiedler <gfiedler@algebraixdata.com>
 To: ""dev@spark.apache.org"" <dev@spark.apache.org> 
 Sent: Friday, June 3, 2016 9:01 AM
 Subject: Where is DataFrame.scala in 2.0?
   
 <!--#yiv0106397017 _filtered #yiv0106397017 {font-family:""Cambria Math"";panose-1:2 4 5 3 5 4 6 3 2 4;} _filtered #yiv0106397017 {font-family:Calibri;panose-1:2 15 5 2 2 2 4 3 2 4;} _filtered #yiv0106397017 {font-family:Verdana;panose-1:2 11 6 4 3 5 4 4 2 4;}#yiv0106397017 #yiv0106397017 p.yiv0106397017MsoNormal, #yiv0106397017 li.yiv0106397017MsoNormal, #yiv0106397017 div.yiv0106397017MsoNormal {margin:0cm;margin-bottom:.0001pt;font-size:11.0pt;font-family:""Calibri"", sans-serif;}#yiv0106397017 a:link, #yiv0106397017 span.yiv0106397017MsoHyperlink {color:blue;text-decoration:underline;}#yiv0106397017 a:visited, #yiv0106397017 span.yiv0106397017MsoHyperlinkFollowed {color:purple;text-decoration:underline;}#yiv0106397017 span.yiv0106397017EmailStyle17 {font-family:""Verdana"", sans-serif;color:#006300;font-weight:normal;font-style:normal;}#yiv0106397017 .yiv0106397017MsoChpDefault {font-family:""Calibri"", sans-serif;} _filtered #yiv0106397017 {margin:72.0pt 72.0pt 72.0pt 72.0pt;}#yiv0106397017 div.yiv0106397017WordSection1 {}-->When I look at the sources in Github, I see DataFrame.scala athttps://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala in the 1.6 branch. But when I change the branch to branch-2.0 or master, I get a 404 error. I also can’t find the file in the directory listings, for example https://github.com/apache/spark/tree/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql (for branch-2.0).    It seems that quite a few APIs use the DataFrame class, even in 2.0. Can someone please point me to its location, or otherwise explain why it is not there?    Thanks, Gerhard    

  "
Gerhard Fiedler <gfiedler@algebraixdata.com>,"Fri, 3 Jun 2016 15:10:04 +0000",RE: Where is DataFrame.scala in 2.0?,"=?utf-8?B?SGVybWFuIHZhbiBIw7Z2ZWxsIHRvdCBXZXN0ZXJmbGllcg==?=
	<hvanhovell@questtec.nl>, Michael Malak <michaelmalak@yahoo.com>","Thanks!

From: Herman van Hövell tot Westerflier [mailto:hvanhovell@questtec.nl]
Sent: Fri, Jun 03, 2016 10:05
To: Gerhard Fiedler <gfiedler@algebraixdata.com>
Cc: dev@spark.apache.org
Subject: Re: Where is DataFrame.scala in 2.0?

Hi Gerhard,

DataFrame and DataSet have been merged in Spark 2.0. A DataFrame is now a DataSet that contains Row objects. We still maintain a type alias for DataFrame: https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L45

HTH

Kind regards,

Herman van Hövell tot Westerflier

2016-06-03 17:01 GMT+02:00 Gerhard Fiedler <gfiedler@algebraixdata.com<mailto:gfiedler@algebraixdata.com>>:
When I look at the sources in Github, I see DataFrame.scala at https://github.com/apache/spark/blob/branch-1.6/sql/core/src/main/scala/org/apache/spark/sql/DataFrame.scala in the 1.6 branch. But when I change the branch to branch-2.0 or master, I get a 404 error. I also can’t find the file in the directory listings, for example https://github.com/apache/spark/tree/branch-2.0/sql/core/src/main/scala/org/apache/spark/sql (for branch-2.0).

It seems that quite a few APIs use the DataFrame class, even in 2.0. Can someone please point me to its location, or otherwise explain why it is not there?

Thanks,
Gerhard


"
Mark Hamstra <mark@clearstorydata.com>,"Fri, 3 Jun 2016 08:22:51 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Steve Loughran <stevel@hortonworks.com>,"It's not a question of whether the preview artifacts can be made available
on Maven central, but rather whether they must be or should be.  I've got
no problems leaving these unstable, transitory artifacts out of the more
permanent, canonical repository.


"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 3 Jun 2016 19:48:53 -0700",Welcoming Yanbo Liang as a committer,dev <dev@spark.apache.org>,"Hi all,

The PMC recently voted to add Yanbo Liang as a committer. Yanbo has been a super active contributor in many areas of MLlib. Please join me in welcoming Yanbo!

Matei
---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 3 Jun 2016 19:50:19 -0700",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations, Yanbo.


"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Sat, 4 Jun 2016 11:52:21 +0900",Re: Welcoming Yanbo Liang as a committer,Yanbo Liang <ybliang8@gmail.com>,"congrats!

// maropu




-- 
---
Takeshi Yamamuro
"
Nan Zhu <zhunanmcgill@gmail.com>,"Fri, 3 Jun 2016 22:54:32 -0400",Re: Welcoming Yanbo Liang as a committer,"Ted Yu <yuzhihong@gmail.com>, Matei Zaharia
 <matei.zaharia@gmail.com>","Congratulations !

-- 
Nan Zhu

Congratulations, Yanbo.

Hi all,

The PMC recently voted to add Yanbo Liang as a committer. Yanbo has been a super active contributor in many areas of MLlib. Please join me in welcoming Yanbo!

Matei
---------------------------------------------------------------------
For additional commands, e-mail: dev-help@spark.apache.org


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 3 Jun 2016 20:21:52 -0700",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations Yanbo !

Regards
Mridul


"
Xiao Li <gatorsmile@gmail.com>,"Fri, 3 Jun 2016 20:22:21 -0700",Re: Welcoming Yanbo Liang as a committer,Nan Zhu <zhunanmcgill@gmail.com>,"Congratulations, Yanbo!

2016-06-03 19:54 GMT-07:00 Nan Zhu <zhunanmcgill@gmail.com>:

"
Dongjoon Hyun <dongjoon@apache.org>,"Fri, 3 Jun 2016 20:38:56 -0700",Re: Welcoming Yanbo Liang as a committer,Xiao Li <gatorsmile@gmail.com>,"Wow, Congratulations, Yanbo!

Dongjoon.


"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Sat, 4 Jun 2016 09:33:22 +0530",Re: Welcoming Yanbo Liang as a committer,Dongjoon Hyun <dongjoon@apache.org>,"congratulations Yanbo!



"
Kai Jiang <jiangkai@gmail.com>,"Sat, 04 Jun 2016 05:44:24 +0000",Re: Welcoming Yanbo Liang as a committer,"Bhupendra Mishra <bhupendra.mishra@gmail.com>, Dongjoon Hyun <dongjoon@apache.org>","Congratulations Yanbo!


"
Jacek Laskowski <jacek@japila.pl>,"Sat, 4 Jun 2016 10:03:49 +0200",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Hi,

Congrats Yanbo!

p.s. It should go to user@, too.

Jacek

Hi all,

The PMC recently voted to add Yanbo Liang as a committer. Yanbo has been a
super active contributor in many areas of MLlib. Please join me in
welcoming Yanbo!

Matei
---------------------------------------------------------------------
"
nihed mbarek <nihedmm@gmail.com>,"Sat, 4 Jun 2016 10:07:17 +0200",Re: Welcoming Yanbo Liang as a committer,Jacek Laskowski <jacek@japila.pl>,"Congratulations Yanbo :)

Le samedi 4 juin 2016, Jacek Laskowski <jacek@japila.pl> a écrit :

a

-- 

M'BAREK Med Nihed,
Fedora Ambassador, TUNISIA, Northern Africa
http://www.nihed.com

<http://tn.linkedin.com/in/nihed>
"
Suresh Thalamati <suresh.thalamati@gmail.com>,"Sat, 4 Jun 2016 06:50:57 -0700",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations, Yanbo

been a super active contributor in many areas of MLlib. Please join me in welcoming Yanbo!


---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Sat, 4 Jun 2016 10:05:59 -0700",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations Yanbo !!!




-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Marcin Tustin <mtustin@handybook.com>,"Sat, 4 Jun 2016 13:09:00 -0400",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congrats!



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.com/press> at Handy
Handy just raised $50m 
<http://venturebeat.com/2015/11/02/on-demand-home-service-handy-raises-50m-in-round-led-by-fidelity/> led 
by Fidelity

"
Hortonworks <zzhang@hortonworks.com>,"Sat, 4 Jun 2016 10:16:54 -0700",Re: Welcoming Yanbo Liang as a committer,Dongjoon Hyun <dongjoon@apache.org>,"Congratulations, Yanbo

Zhan Zhang

Sent from my iPhone


-- 
CONFIDENTIALITY NOTICE
NOTICE: This message is intended for the use of the individual or entity to 
which it is addressed and may contain information that is confidential, 
privileged and exempt from disclosure under applicable law. If the reader 
of this message is not the intended recipient, you are hereby notified that 
any printing, copying, dissemination, distribution, disclosure or 
forwarding of this communication is strictly prohibited. If you have 
received this communication in error, please contact the sender immediately 
and delete it from your system. Thank You.

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Sat, 04 Jun 2016 17:21:13 +0000",Re: Welcoming Yanbo Liang as a committer,"Hortonworks <zzhang@hortonworks.com>, Dongjoon Hyun <dongjoon@apache.org>","Congratulations Yanbo and welcome

"
Reynold Xin <rxin@databricks.com>,"Sat, 4 Jun 2016 10:22:36 -0700",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations, Yanbo!


"
Matei Zaharia <matei.zaharia@gmail.com>,"Sat, 4 Jun 2016 15:24:21 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Mark Hamstra <mark@clearstorydata.com>,"Personally I'd just put them on the staging repo and link to that on the downloads page. It will create less confusion for people browsing Maven Central later and wondering which releases are safe to use.

Matei

available on Maven central, but rather whether they must be or should be.  I've got no problems leaving these unstable, transitory artifacts out of the more permanent, canonical repository.
success as pre-beta of the artifacts.
expectations that they can point their code at the 2.0.0-preview and then, when a release comes out, simply
harmful if people do start building and testing against the preview? If it finds problems early, it can only be a good thing
such
https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html <https://databricks.com/blog/2016/05/11/apache-spark-2-0-technical-preview-easier-faster-and-smarter.html>
but
in
I
<http://spark.apache.org/downloads.html> either. It needs to be
<http://www.apache.org/dev/release.html>) The culture is, in theory, to
at
for
publish
just need
packages
release
people
after the
when we
---------------------------------------------------------------------
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>
<mailto:dev-unsubscribe@spark.apache.org>
<mailto:dev-help@spark.apache.org>

"
Sean Owen <sowen@cloudera.com>,"Sat, 4 Jun 2016 23:42:44 +0100",Re: Spark 2.0.0-preview artifacts still not available in Maven,Matei Zaharia <matei.zaharia@gmail.com>,"Artifacts that are not for public consumption shouldn't be in a public
release; this is instead what nightlies are for. However, this was a
normal public release.

I am not even sure why it's viewed as particularly unsafe, but, unsafe
alpha and beta releases are just releases, and their name and
documentation clarify their status for those who care. These are
regularly released by other projects.

That is, the question is not, is this a beta? Everyone agrees it
probably is, and is documented as such.

The question is, can you just not fully release it? I don't think so,
even as a matter of process, and don't see a good reason not to.

To Reynold's quote, I think that's suggesting that not all projects
will release to a repo at all (e.g. OpenOffice?). I don't think it
means you're free to not release some things to Maven, if that's
appropriate and common for the type of project.

Regarding risk, remember that the audience for Maven artifacts are
developers, not admins or end users. I understand that developers can
temporarily change their build to use a different resolver if they
care, but, why? (and, where would someone figure this out?)

Regardless: the 2.0.0-preview docs aren't published to go along with
the source/binary releases. Those need be released to the project
site, though probably under a different /preview/ path or something.
If they are, is it weird that someone wouldn't find the release in the
usual place in Maven then?

Given that the driver of this was concern over wide access to
2.0.0-preview, I think it's best to err on the side openness vs some
theoretical problem.


---------------------------------------------------------------------


"
Hyukjin Kwon <gurwls223@gmail.com>,"Sun, 5 Jun 2016 15:43:48 +0900",Re: Welcoming Yanbo Liang as a committer,Matei Zaharia <matei.zaharia@gmail.com>,"Congratulations!

2016-06-04 11:48 GMT+09:00 Matei Zaharia <matei.zaharia@gmail.com>:

"
Yuhao Yang <hhbyyh@gmail.com>,"Sun, 5 Jun 2016 00:14:37 -0700",Re: Welcoming Yanbo Liang as a committer,Yanbo Liang <ybliang8@gmail.com>,"Congratulations Yanbo

2016-06-04 23:43 GMT-07:00 Hyukjin Kwon <gurwls223@gmail.com>:

"
Linbo Jin <linbojin203@gmail.com>,"Sun, 5 Jun 2016 16:56:32 +0800",Re: Welcoming Yanbo Liang as a committer,Yanbo Liang <ybliang8@gmail.com>,"Congratulations! Yanbo


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Sun, 5 Jun 2016 12:57:36 +0200",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"Hi all

IMHO the preview ‘release’ is good at is is now, so no further changes required.
For me the preview was a trigger to what will be the next Spark 2.0, really appreciate the effort guys made to describe it and market it:)

I’ll appreciate if the Apache Spark team will start a vote for a new alpha-beta release and point the current status of the project. Since the preview was released there are numerous updates.

Best,
Ovidiu 
 
the
Maven
available
got no


---------------------------------------------------------------------


"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Sun, 5 Jun 2016 20:03:12 +0900",Re: Welcoming Yanbo Liang as a committer,Yanbo Liang <ybliang8@gmail.com>,"Congratulations Yanbo!


- Kousuke



---------------------------------------------------------------------


"
Marcin Tustin <mtustin@handybook.com>,"Sun, 5 Jun 2016 15:29:23 -0400",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"+1 agree that right the problem is theoretical esp if the preview label is
in the version coordinates as it should be.



-- 
Want to work at Handy? Check out our culture deck and open roles 
<http://www.handy.com/careers>
Latest news <http://www.handy.co"
Bryan Cutler <cutlerb@gmail.com>,"Sun, 5 Jun 2016 16:07:06 -0700",Re: Welcoming Yanbo Liang as a committer,,"Congratulations Yanbo!

"
Liwei Lin <lwlin7@gmail.com>,"Mon, 6 Jun 2016 09:25:22 +0800",Re: Welcoming Yanbo Liang as a committer,,"Congratulations Yanbo!


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Sun, 5 Jun 2016 19:10:46 -0700",Re: Welcoming Yanbo Liang as a committer,Yanbo Liang <ybliang8@gmail.com>,"Congrats, Yanbo!


"
Prashant Sharma <scrapcodes@gmail.com>,"Mon, 6 Jun 2016 16:21:24 +0530",https://issues.apache.org seems to be down for a while today.,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

https://issues.apache.org seems to be down for a while today. I was not
sure, who looks into this.

http://downorisitjustme.com/res.php?url=issues.apache.org


Thanks,
--Prashant
"
,"Mon, 6 Jun 2016 14:17:28 +0200",Re: https://issues.apache.org seems to be down for a while today.,dev@spark.apache.org,"Yes, Jira was down couple of hours ago, but it's back now.

Regards
JB


-- 
jbonofre@apache.org
http://blog.nanthrax.net
Talend - http://www.talend.com

---------------------------------------------------------------------


"
Shane Curcuru <asf@shanecurcuru.org>,"Mon, 6 Jun 2016 08:34:44 -0400",Re: Spark 2.0.0-preview artifacts still not available in Maven,dev@spark.apache.org,"

...

The mere fact that there continues to be repeated pushback from PMC
members employed by DataBricks to such a reasonable and easy question to
answer and take action on for the benefit of all the project's users
raises red flags for me.

Immaterial of the actual motivations of individual PMC members, this
still gives the *appearance* that DataBricks as an organization
effectively exercises a more than healthy amount of control over how the
project operates in simple, day-to-day manners.

I strongly urge everyone participating in Apache Spark development to
read and take to heart this required policy for Apache projects:

  http://community.apache.org/projectIndependence

- Shane, speaking as an individual

(If I were speaking in other roles I hold, I wouldn't be as polite)

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 6 Jun 2016 08:00:35 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Shane Curcuru <asf@shanecurcuru.org>,"This is not a Databricks vs. The World situation, and the fact that some
persist in forcing every issue into that frame is getting annoying.  There
are good engineering and project-management reasons not to populate the
long-term, canonical repository of Maven artifacts with what are known to
be severely compromised builds of limited usefulness, particularly over
time.  It is a legitimate dispute over whether these preview artifacts
should be deployed to Maven Central, not one that must be seen as
Databricks seeking improper advantage.


"
Gayathri Murali <gayathri.m.softie@gmail.com>,"Mon, 6 Jun 2016 08:11:45 -0700",Re: Welcoming Yanbo Liang as a committer,"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Congratulations Yanbo Liang! Well deserved.



"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 06 Jun 2016 15:23:39 +0000",Re: Spark 2.0.0-preview artifacts still not available in Maven,"Mark Hamstra <mark@clearstorydata.com>, Shane Curcuru <asf@shanecurcuru.org>","+1 to what Mark said. I've been following this discussion and I don't
understand where the sudden ""Databricks vs. everybody else"" narrative came
from.


"
Sean Owen <sowen@cloudera.com>,"Mon, 6 Jun 2016 17:51:30 +0100",Re: Spark 2.0.0-preview artifacts still not available in Maven,Mark Hamstra <mark@clearstorydata.com>,"I still don't know where this ""severely compromised builds of limited
usefulness"" thing comes from? what's so bad? You didn't veto its
release, after all. And rightly so: a release doesn't mean ""definitely
works""; it means it was created the right way. It's OK to say it's
buggy alpha software; this isn't an argument to not really release it.

But aside from that: if it should be used by someone, then who did you
have in mind?

It would be coherent at least to decide not to make alpha-like
release, but, we agreed to, which is why this argument sort of
surprises me.

I share some concerns about piling on Databricks. Nothing here is by
nature about an organization. However, this release really began in
response to a thread (which not everyone here can see) about
Databricks releasing a ""2.0.0 preview"" option in their product before
it existed. I presume employees of that company sort of endorse this,
which has put this same release into the hands of not just developers
or admins but end users -- even with caveats and warnings.

(And I think that's right!)

While I'd like to see your reasons before I'd agree with you Mark,
yours is a feasible position; I'm not as sure how people who work for
Databricks can argue at the same time however that this should be
carefully guarded as an ASF release -- even with caveats and warnings.

We don't need to assume bad faith -- I don't. The appearance alone is
enough to act to make this consistent.

But, I think the resolution is simple: it's not 'dangerous' to release
this and I don't think people who say they think this really do. So
just finish this release normally, and we're done. Even if you think
there's an argument against it, weigh vs the problems above.



---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 6 Jun 2016 10:08:19 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"

I simply mean that it was released with the knowledge that there are still
significant bugs in the preview that definitely would warrant a veto if
this were intended to be on a par with other releases.  There have been
repeated announcements to that effect, but developers finding the preview
artifacts on Maven Central months from now may well not also see those
announcements and related discussion.  The artifacts will be very stale and
no longer useful for their limited testing purpose, but will persist in the
repository.


"
"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","Mon, 6 Jun 2016 13:35:11 -0400",Can't compile 2.0-preview with scala 2.10,dev@spark.apache.org,"Hi,

I've checked out the 2.0-preview and attempted to build it
with ./dev/make-distribution.sh -Pscala-2.10

However i keep getting

[INFO] --- maven-enforcer-plugin:1.4.1:enforce (enforce-versions) @
spark-parent_2.11 ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.BannedDependencies
failed with message:
Found Banned Dependency: org.scala-lang.modules:scala-xml_2.11:jar:1.0.2
Found Banned Dependency: org.scalatest:scalatest_2.11:jar:2.2.6

Is scala 2.10 not being supported going forward ?. If so the profile should
probably be removed from the master pom.xml


Thanks,

Franklyn
"
Ted Yu <yuzhihong@gmail.com>,"Mon, 6 Jun 2016 10:41:37 -0700",Re: Can't compile 2.0-preview with scala 2.10,"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","See the following from
https://amplab.cs.berkeley.edu/jenkins/view/Spark%20QA%20Compile/job/SPARK-master-COMPILE-sbt-SCALA-2.10/1642/consoleFull
:

+ SBT_FLAGS+=('-Dscala-2.10')
+ ./dev/change-scala-version.sh 2.10


FYI



"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 6 Jun 2016 10:48:11 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"

In this case, I would only expect the 2.0.0 preview to be treated as just
any other release, period.


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 6 Jun 2016 10:49:09 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Mark Hamstra <mark@clearstorydata.com>,"
A few months from now, why would a developer choose a preview, alpha, beta
compared to the GA 2.0 release ?

As for the being stale part, this is true for every release anyone put out
there.


-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 6 Jun 2016 11:12:42 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Luciano Resende <luckbr1975@gmail.com>,"Is there any way to remove artifacts from Maven Central? Maybe that would
help clean these things up long-term, though it would create problems for
users who for some reason decide to rely on these previews.

In any case, if people are *really* concerned about this, we should just
put it there. My thought was that it's better for users to do something
special to link to this release (e.g. add a reference to the staging repo)
so that they are more likely to know that it's a special, unstable thing.
Same thing they do to use snapshots.

Matei


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 6 Jun 2016 11:17:17 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Luciano Resende <luckbr1975@gmail.com>,"BTW, same goes with docs -- Sean, if you want to add a /docs/2.0-preview on
the website and link to it, go for it!

Matei


"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 6 Jun 2016 11:18:31 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Matei Zaharia <matei.zaharia@gmail.com>,"
So, consider this thread started on another project :
https://www.mail-archive.com/dev@bahir.apache.org/msg00038.html

What would be your recommendation ?
   - Start a release based on Apache Spark 2.0.0 preview staging repo ? I
would  reject that...
   - Start a release on a set of artifacts that are going to be deleted ? I
would also reject that

To me, if companies are using the release on their products, and other
projects are relying on the release to provide a way for users to test,
this should be considered as any other release, published permanently,
which at some point will become obsolete and users will move on to more
stable releases.

Thanks



-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 6 Jun 2016 11:19:41 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Luciano Resende <luckbr1975@gmail.com>,"Precisely because the naming of the preview artifacts has to fall outside
of the normal versioning, I can easily see incautious Maven users a few
months from now mistaking the preview artifacts as
spark-2.0-something-special instead of spark-2.0-something-stale.


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 6 Jun 2016 20:31:12 +0200",Re: Spark 2.0.0-preview artifacts still not available in Maven,Ovidiu Cristian Marcu <ovidiu-cristian.marcu@inria.fr>,"+1 for moving this discussion to a proactive new (alpha/beta) release of Apache Spark 2.0!

month or the preview will be pushed to maven and considered an alpha?
would help clean these things up long-term, though it would create problems for users who for"
Sean Owen <sowen@cloudera.com>,"Mon, 6 Jun 2016 19:48:14 +0100",Re: Spark 2.0.0-preview artifacts still not available in Maven,"""dev@spark.apache.org"" <dev@spark.apache.org>","Artifacts can't be removed from Maven in any normal circumstance, but,
it's no problem.

The argument that people might keep using it goes for any older
release. Why would anyone use 1.6.0 when 1.6.1 exists? yet we keep
1.6.0 just for the record and to not break builds. It may be that
Foobar 3.0-beta depends on 2.0.0-preview and 3.0 will shortly depend
on 2.0.0, but, killing the -preview artifact breaks that other
historical release/branch.

I agree that ""-alpha-1"" would have been better. But we're talking
about working around pretty bone-headed behavior, to not notice what
version of Spark they build against, or not understand what
2.0.0-preview vs 2.0.0 means in a world of semver.

BTW Maven sorts 2.0.0-preview before 2.0.0, so 2.0.0 would show up as
the latest, when released, in tools like mvn
versions:display-dependency-updates. You could exclude the preview
release by requiring version [2.0.0,).


---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Mon, 6 Jun 2016 12:00:57 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"Fine.  I don't feel strongly enough about it to continue to argue against
putting the artifacts on Maven Central.


"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Jun 2016 12:05:16 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Mark Hamstra <mark@clearstorydata.com>,"The bahir one was a good argument actually. I just clicked the button to
push it into Maven central.



"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 6 Jun 2016 12:13:08 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Reynold Xin <rxin@databricks.com>,"
Thank You !!!
"
thibaut <thibaut.gensollen@gmail.com>,"Mon, 6 Jun 2016 15:13:17 -0400",Mesos and No transport is loaded for protocol,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi there,

I an trying to configure Spark for running on top of Mesos. But every time I send a job, it fails. I can see mesos downloading correctly the spark.tgz but I have this errors at the end :


Any idea ? I did not find anything for solving my issue..  Is it my cluster ? Spark ? both ? Thanking you in advance.
Thibaut

I0606 15:06:35.628329 16520 fetcher.cpp:456] Fetched 'http://d3kbcqa49mib13.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz' to '/tmp/mesos/slaves/c58064f7-88b6-438d-b76f-fc28c6cc51a1-S0/frameworks/c58064f7-88b6-438d-b76f-fc28c6cc51a1-0079/executors/3/runs/23913146-d87f-445c-9f6b-f412ad2cbbd7/spark-1.5.1-bin-hadoop2.6.tgz'
I0606 15:06:35.687414 16527 exec.cpp:143] Version: 0.28.1
I0606 15:06:35.691270 16540 exec.cpp:217] Executor registered on slave c58064f7-88b6-438d-b76f-fc28c6cc51a1-S0
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
16/06/06 15:06:36 INFO CoarseGrainedExecutorBackend: Registered signal handlers for [TERM, HUP, INT]
16/06/06 15:06:36 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
16/06/06 15:06:37 INFO SecurityManager: Changing view acls to: thibautg
16/06/06 15:06:37 INFO SecurityManager: Changing modify acls to: thibautg
16/06/06 15:06:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(thibautg); users with modify permissions: Set(thibautg)
16/06/06 15:06:37 INFO Slf4jLogger: Slf4jLogger started
16/06/06 15:06:38 INFO Remoting: Starting remoting
16/06/06 15:06:38 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://driverPropsFetcher@141.213.4.119:56419]
16/06/06 15:06:38 INFO Utils: Successfully started service 'driverPropsFetcher' on port 56419.
Exception in thread ""main"" akka.remote.RemoteTransportException: No transport is loaded for protocol: [spark], available protocols: [akka.tcp]
	at akka.remote.Remoting$.localAddressForRemote(Remoting.scala:87)
	at akka.remote.Remoting.localAddressForRemote(Remoting.scala:129)
	at akka.remote.RemoteActorRefProvider.rootGuardianAt(RemoteActorRefProvider.scala:338)
	at akka.actor.ActorRefFactory$class.actorSelection(ActorRefProvider.scala:318)
	at akka.actor.ActorSystem.actorSelection(ActorSystem.scala:272)
	at org.apache.spark.rpc.akka.AkkaRpcEnv.asyncSetupEndpointRefByURI(AkkaRpcEnv.scala:216)
	at org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:98)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$$anonfun$run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:162)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:69)
	at org.apache.spark.deploy.SparkHadoopUtil$$anon$1.run(SparkHadoopUtil.scala:68)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1628)
	at org.apache.spark.deploy.SparkHadoopUtil.runAsSparkUser(SparkHadoopUtil.scala:68)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.run(CoarseGrainedExecutorBackend.scala:149)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend$.main(CoarseGrainedExecutorBackend.scala:250)
	at org.apache.spark.executor.CoarseGrainedExecutorBackend.main(CoarseGrainedExecutorBackend.scala)

 "
Imran Rashid <irashid@cloudera.com>,"Mon, 6 Jun 2016 14:59:31 -0500",Re: Spark 2.0.0-preview artifacts still not available in Maven,Luciano Resende <luckbr1975@gmail.com>,"I've been a bit on the fence on this, but I agree that Luciano makes a
compelling reason for why we really should publish things to maven
central.  Sure we slightly increase the risk somebody refers to the preview
release too late, but really that is their own fault.

And I also I agree with comments from Sean and Mark that this is *not* a
""Databricks vs The World"" scenario at all.


"
Imran Rashid <irashid@cloudera.com>,"Mon, 6 Jun 2016 15:49:44 -0500",apologies for flaky BlacklistIntegrationSuite,dev <dev@spark.apache.org>,"Hi all,

just a heads up, I introduced a flaky test, BlacklistIntegrationSuite, a
week ago or so.  I *thought* I had solved the problems, but turns out there
was more flakiness remaining.  for now I've just turned the tests off, so
if you this has led to failures for you, just re-trigger your build.

Opened a jira to fix it for real here if you're interested:
https://issues.apache.org/jira/browse/SPARK-15783

Again apologies for the flakiness in the build.

Imran
"
Timothy Chen <tnachen@gmail.com>,"Mon, 6 Jun 2016 13:58:56 -0700",Re: Mesos and No transport is loaded for protocol,thibaut <thibaut.gensollen@gmail.com>,"Hi,

How did you package the spark.tgz, and are you running the same code that you packaged when you ran spark submit?

And what is your settings for spark look like?

Tim


 I send a job, it fails. I can see mesos downloading correctly the spark.tgz but I have this errors at the end :
r ? Spark ? both ? Thanking you in advance.
3.cloudfront.net/spark-1.5.1-bin-hadoop2.6.tgz' to '/tmp/mesos/slaves/c58064f7-88b6-438d-b76f-fc28c6cc51a1-S0/frameworks/c58064f7-88b6-438d-b76f-fc28c6cc51a1-0079/executors/3/runs/23913146-d87f-445c-9f6b-f412ad2cbbd7/spark-1.5.1-bin-hadoop2.6.tgz'
064f7-88b6-438d-b76f-fc28c6cc51a1-S0
rties
dlers for [TERM, HUP, INT]
ary for your platform... using builtin-java classes where applicable
sabled; ui acls disabled; users with view permissions: Set(thibautg); users with modify permissions: Set(thibautg)
[akka.tcp://driverPropsFetcher@141.213.4.119:56419]
cher' on port 56419.
ort is loaded for protocol: [spark], available protocols: [akka.tcp]
Provider.scala:338)
.scala:318)
AkkaRpcEnv.scala:216)
8)
run$1.apply$mcV$sp(CoarseGrainedExecutorBackend.scala:162)
til.scala:69)
til.scala:68)
formation.java:1628)
opUtil.scala:68)
seGrainedExecutorBackend.scala:149)
rseGrainedExecutorBackend.scala:250)
seGrainedExecutorBackend.scala)
"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Jun 2016 14:07:05 -0700",Re: apologies for flaky BlacklistIntegrationSuite,Imran Rashid <irashid@cloudera.com>,"Thanks for fixing it!



"
"""S. Kai Chen"" <sean.kai.chen@gmail.com>","Mon, 6 Jun 2016 15:40:24 -0700",Add hot-deploy capability in Spark Shell,dev@spark.apache.org,"Hi,

We use spark-shell heavily for ad-hoc data analysis as well as iterative
development of the analytics code. A common workflow consists the following
steps:

   1. Write a small Scala module, assemble the fat jar
   2. Start spark-shell with the assembly jar file
   3. Try out some ideas in the shell, then capture the code back into the
   module
   4. Go back to step 1 and restart the shell

This is very similar to what people do in web-app development. And the pain
point is similar: in web-app development, a lot of time is spent waiting
for new code to be deployed; here, a lot of time is spent waiting for Spark
to restart. Having the ability to hot-deploy code in the REPL would help a
lot, just as being able to hot-deploy in containers like Play, or using
JRebel, has helped boost productivity tremendously.

I do have code that works with the 1.5.2 release.  Is this something that's
interesting enough to be included in Spark proper?  If so, should I create
a Jira ticket or github PR for the master branch?


Cheers,

Kai
"
Reynold Xin <rxin@databricks.com>,"Mon, 6 Jun 2016 16:16:55 -0700",Re: Add hot-deploy capability in Spark Shell,"""S. Kai Chen"" <sean.kai.chen@gmail.com>","Thanks for the email. How do you deal with in-memory state that reference
the classes? This can happen in both streaming and caching in RDD and
temporary view creation in SQL.


"
Kai Chen <sean.kai.chen@gmail.com>,"Mon, 6 Jun 2016 16:24:37 -0700",Re: Add hot-deploy capability in Spark Shell,Reynold Xin <rxin@databricks.com>,"I don't.  The hot-deploy shouldn't happen while there is a job running.  At
least in the REPL it won't make much sense.  It's a development-only
feature to shorten the iterative coding cycle.  In production environment,
this is not enabled ... though there might be situations where it would be
desirable.  But currently I'm not handling that, as it's much more complex.


"
Krot Viacheslav <krot.vyacheslav@gmail.com>,"Tue, 07 Jun 2016 14:11:06 +0000",streaming JobScheduler and error handling confusing behavior,dev@spark.apache.org,"Hi,
I don't know if it is a bug or a feature, but one thing in streaming error
handling seems confusing to me - I create streaming context, start and call
#awaitTermination like this:

try {
  ssc.awaitTermination();
} catch (Exception e) {
  LoggerFactory.getLogger(getClass()).error(""Job failed. Stopping JVM"", e);
  System.exit(-1);
}

I expect that jvm will be terminated as soon as any job fails and no more
jobs are started. But actually this is not true - before exception is
caught another job starts.
This is caused by the design of JobScheduler event loop:

private def processEvent(event: JobSchedulerEvent) {
  try {
    event match {
      case JobStarted(job, startTime) => handleJobStart(job, startTime)
      case JobCompleted(job, completedTime) => handleJobCompletion(job,
completedTime)
      case ErrorReported(m, e) => handleError(m, e)
    }
  } catch {
    case e: Throwable =>
      reportError(""Error in job scheduler"", e)
  }
}

If error happens it calls handleError that wakes up a lock in ContextWaiter
and notifies my main thread. But meanwhile it starts next job, and
sometimes it is enough to complete it! I have several jobs in each batch
and want each of them run only and only if previous completed successfully.

For API user point of view this behavior is confusing and you cannot guess
how it works until looking into the source code.

What do you think about adding another spark configuration parameter
not allow to run next job?
"
Xiangrui Meng <meng@databricks.com>,"Tue, 07 Jun 2016 14:15:21 +0000",Re: Welcoming Yanbo Liang as a committer,"Gayathri Murali <gayathri.m.softie@gmail.com>, 
	""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Congrats!!


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 7 Jun 2016 15:01:07 +0000 (UTC)",Re: Spark 2.0.0-preview artifacts still not available in Maven,"Imran Rashid <irashid@cloudera.com>, 
	Luciano Resende <luckbr1975@gmail.com>","I just checked and I don't see the 2.0 preview release at all anymore on .http://spark.apache.org/downloads.html, is it in transition?    The only place I can see it is at http://spark.apache.org/news/spark-2.0.0-preview.html

I would like to see docs there too.  My opinion is it should be as easy to use/try out as any other spark release.
Tom

 

ote:
 

 I've been a bit on the fence on this, but I agree that Luciano makes a compelling reason for why we really should publish things to maven central.  Sure we slightly increase the risk somebody refers to the preview release too late, but really that is their own fault.
And I also I agree with comments from Sean and Mark that this is *not* a ""Databricks vs The World"" scenario at all.
e:




The bahir one was a good argument actually. I just clicked the button to push it into Maven central.


Thank You !!!
 



  "
Sean Owen <sowen@cloudera.com>,"Tue, 7 Jun 2016 16:04:10 +0100",Re: Spark 2.0.0-preview artifacts still not available in Maven,Tom Graves <tgraves_cs@yahoo.com>,"It's there (refresh maybe?). See the end of the downloads dropdown.

For the moment you can see the docs in the nightly docs build:
https://home.apache.org/~pwendell/spark-nightly/spark-branch-2.0-docs/latest/

I don't know, what's the best way to put this into the main site?
under a /preview root? I am not sure how that process works.


---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 7 Jun 2016 15:10:57 +0000 (UTC)",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"Thanks Sean, you were right, hard refresh made it show up.
Seems like we should at least link to the preview docs from http://spark.apache.org/documentation.html.
Tom 

:
 

 It's there (refresh maybe?). See the end of the downloads dropdown.

For the moment you can see the docs in the nightly docs build:
https://home.apache.org/~pwendell/spark-nightly/spark-branch-2.0-docs/latest/

I don't know, what's the best way to put this into the main site?
under a /preview root? I am not sure how that process works.

 The only
asy to

---------------------------------------------------------------------



  "
Sean Owen <sowen@cloudera.com>,"Tue, 7 Jun 2016 16:14:20 +0100",Re: Spark 2.0.0-preview artifacts still not available in Maven,Tom Graves <tgraves_cs@yahoo.com>,"As a stop-gap, I can edit that page to have a small section about
preview releases and point to the nightly docs.

Not sure who has the power to push 2.0.0-preview to site/docs, but, if
that's done then we can symlink ""preview"" in that dir to it and be
done, and update this section about preview docs accordingly.


---------------------------------------------------------------------


"
ElfoLiNk <gazza.matte@gmail.com>,"Tue, 7 Jun 2016 08:43:46 -0700 (MST)","Standalone Cluster Mode: how does spark allocate
 spark.executor.cores?",dev@spark.apache.org,"Hi,
I'm searching for how and where spark allocates cores per executor in the
source code.
Is it possible to control programmaticaly allocated cores in standalone
cluster mode?

Regards,
Matteo



--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 7 Jun 2016 08:59:59 -0700",Re: Spark 2.0.0-preview artifacts still not available in Maven,Sean Owen <sowen@cloudera.com>,"As far as I know the process is just to copy docs/_site from the build
to the appropriate location in the SVN repo (i.e.
site/docs/2.0.0-preview).

Thanks
Shivaram


---------------------------------------------------------------------


"
"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","Tue, 7 Jun 2016 17:47:46 -0400",Can't use UDFs with Dataframes in spark-2.0-preview scala-2.10,dev@spark.apache.org,"I've built spark-2.0-preview (8f5a04b) with scala-2.10 using the following


and then ran the following code in a pyspark shell

from pyspark.sql import SparkSession


This never returns with a result.
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 7 Jun 2016 15:22:54 -0700",Re: Can't use UDFs with Dataframes in spark-2.0-preview scala-2.10,"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","With commit 200f01c8fb15680b5630fbd122d44f9b1d096e02 using Scala 2.11:

Using Python version 2.7.9 (default, Apr 29 2016 10:48:06)
SparkSession available as 'spark'.
DF').getOrCreate()
[Row(incremented=2), Row(incremented=3)]

Let me build with Scala 2.10 and try again.


"
franklyn <franklyn.dsouza@shopify.com>,"Tue, 7 Jun 2016 15:31:10 -0700 (MST)",Re: Can't use UDFs with Dataframes in spark-2.0-preview scala-2.10,dev@spark.apache.org,"Thanks Ted !.

I'm using
https://github.com/apache/spark/commit/8f5a04b6299e3a47aca13cbb40e72344c0114860
and building with scala-2.10

I can confirm that it works with scala-2.11



--

---------------------------------------------------------------------


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 7 Jun 2016 15:58:47 -0700",Dataset API agg question,dev <dev@spark.apache.org>,"I'm trying to switch from RDD API to Dataset API
My question is about reduceByKey method

e.g. in the following example I'm trying to rewrite

sc.parallelize(Seq(1->2, 1->5, 3->6)).reduceByKey(math.max).take(10)

using DS API. That is what I have so far:

Seq(1->2, 1->5,
3->6).toDS.groupBy(_._1).agg(max($""_2"").as(ExpressionEncoder[Int])).take(10)

Questions:

1. is it possible to avoid typing ""as(ExpressionEncoder[Int])"" or replace
it with smth shorter?

2.  Why I have to use String column name in max function? e.g. $""_2"" or
col(""_2"").  can I use _._2 instead?


Alex
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 7 Jun 2016 16:07:25 -0700",Re: Dataset API agg question,Alexander Pivovarov <apivovarov@gmail.com>,"Have you tried the following ?

Seq(1->2, 1->5, 3->6).toDS(""a"", ""b"")

then you can refer to columns by name.

FYI



"
Ted Yu <yuzhihong@gmail.com>,"Tue, 7 Jun 2016 16:27:14 -0700",Re: Can't use UDFs with Dataframes in spark-2.0-preview scala-2.10,franklyn <franklyn.dsouza@shopify.com>,"I built with Scala 2.10


The above just hung.


"
Alexander Pivovarov <apivovarov@gmail.com>,"Tue, 7 Jun 2016 16:31:34 -0700",Re: Dataset API agg question,Ted Yu <yuzhihong@gmail.com>,"Ted, It does not work like that

you have to .map(toAB).toDS


"
franklyn <franklyn.dsouza@shopify.com>,"Tue, 7 Jun 2016 16:45:10 -0700 (MST)",Re: Can't use UDFs with Dataframes in spark-2.0-preview scala-2.10,dev@spark.apache.org,"Thanks for reproducing it Ted, should i make a Jira Issue?.



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 7 Jun 2016 16:47:27 -0700",Re: Can't use UDFs with Dataframes in spark-2.0-preview scala-2.10,franklyn <franklyn.dsouza@shopify.com>,"Please go ahead.


"
Reynold Xin <rxin@databricks.com>,"Tue, 7 Jun 2016 22:50:40 -0700",Re: Dataset API agg question,Alexander Pivovarov <apivovarov@gmail.com>,"Take a look at the implementation of typed sum/avg:
https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/expressions/scalalang/typed.scala

You can implement a typed max/min.



"
Sean Owen <sowen@cloudera.com>,"Wed, 8 Jun 2016 13:14:56 +0100",Spark 2.0.0 preview docs uploaded,"""dev@spark.apache.org"" <dev@spark.apache.org>","OK, this is done:

http://spark.apache.org/documentation.html
http://spark.apache.org/docs/2.0.0-preview/
http://spark.apache.org/docs/preview/


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 08 Jun 2016 14:24:31 +0000",Re: NegativeArraySizeException / segfault,"Koert Kuipers <koert@tresata.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","I just raised https://issues.apache.org/jira/browse/SPARK-15822 for a
similar looking issue. Analyzing the core dump from the segv with Memory
Analyzer it looks very much like a UTF8String is very corrupt.

Cheers,


"
Koert Kuipers <koert@tresata.com>,"Wed, 8 Jun 2016 12:43:18 -0400",Re: NegativeArraySizeException / segfault,Pete Robbins <robbinspg@gmail.com>,"great!

we weren't able to reproduce it because the unit tests use a broadcast-join
while on the cluster it uses sort-merge-join. so the issue is in
sort-merge-join.

we are now able to reproduce it in tests using
spark.sql.autoBroadcastJoinThreshold=-1
it produces weird looking garbled results in the join.
hoping to get a minimal reproducible example soon.


"
Andres Perez <andres@tresata.com>,"Wed, 8 Jun 2016 13:54:36 -0400",Re: NegativeArraySizeException / segfault,Koert Kuipers <koert@tresata.com>,"We were able to reproduce it with a minimal example. I've opened a jira
issue:

https://issues.apache.org/jira/browse/SPARK-15825


"
Alexander Pivovarov <apivovarov@gmail.com>,"Wed, 8 Jun 2016 16:53:28 -0700",Kryo registration for Tuples?,dev <dev@spark.apache.org>,"if my RDD is RDD[(String, (Long, MyClass))]

Do I need to register

classOf[MyClass]
classOf[(Any, Any)]

or

classOf[MyClass]
classOf[(Long, MyClass)]
classOf[(String, (Long, MyClass))]

?
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 8 Jun 2016 17:39:55 -0700",Re: Kryo registration for Tuples?,Alexander Pivovarov <apivovarov@gmail.com>,"I think the second group (3 classOf's) should be used.

Cheers


"
Reynold Xin <rxin@databricks.com>,"Wed, 8 Jun 2016 17:52:07 -0700",Re: Kryo registration for Tuples?,Ted Yu <yuzhihong@gmail.com>,"Due to type erasure they have no difference, although watch out for Scala
tuple serialization.


"
Alexander Pivovarov <apivovarov@gmail.com>,"Wed, 8 Jun 2016 18:00:01 -0700",Re: Kryo registration for Tuples?,Reynold Xin <rxin@databricks.com>,"Can I just enable spark.kryo.registrationRequired and look at error
messages to get unregistered classes?


"
Reynold Xin <rxin@databricks.com>,"Wed, 8 Jun 2016 18:00:43 -0700",Re: Kryo registration for Tuples?,Alexander Pivovarov <apivovarov@gmail.com>,"Yes you can :)



"
Alexander Pivovarov <apivovarov@gmail.com>,"Wed, 8 Jun 2016 20:42:55 -0700",rdd.distinct with Partitioner,dev <dev@spark.apache.org>,"most of the RDD methods which shuffle data take Partitioner as a parameter

But rdd.distinct does not have such signature

Should I open a PR for that?

/**
 * Return a new RDD containing the distinct elements in this RDD.
 */

def distinct(partitioner: Partitioner)(implicit ord: Ordering[T] =
null): RDD[T] = withScope {
  map(x => (x, null)).reduceByKey(partitioner, (x, y) => x).map(_._1)
}
"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Thu, 09 Jun 2016 12:22:21 +0800",Re: rdd.distinct with Partitioner,Alexander Pivovarov <apivovarov@gmail.com>,"Hi Alexander,

I think it does not guarantee to be right if an arbitrary Partitioner is passed in.

I have created a notebook and you can check it out. (https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7973071962862063/2110745399505739/58107563000366/latest.html <https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7973071962862063/2110745399505739/58107563000366/latest.html>)

Best regards,

Yang


Pivovarov <apivovarov@gmail.com> д
parameter
null): RDD[T] = withScope {
x).map(_._1)

"
Mridul Muralidharan <mridul@gmail.com>,"Wed, 8 Jun 2016 21:26:01 -0700",Re: rdd.distinct with Partitioner,=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>,"The example violates the basic contract of a Partitioner.
It does make sense to take Partitioner as a param to distinct - though it
is fairly trivial to simulate that in user code as well ...

Regards
Mridul


te:

239c93eaaa8714f173bcfc/7973071962862063/2110745399505739/58107563000366/latest.html
42，Alexander Pivovarov <apivovarov@gmail.com
：
r
): RDD[T] = withScope {
"
Alexander Pivovarov <apivovarov@gmail.com>,"Wed, 8 Jun 2016 21:51:43 -0700",Re: rdd.distinct with Partitioner,=?UTF-8?B?5rGq5rSL?= <tiandiwoxin@icloud.com>,"reduceByKey(randomPartitioner, (a, b) => a + b) also gives incorrect result

Why reduceByKey with Partitioner exists then?


239c93eaaa8714f173bcfc/7973071962862063/2110745399505739/58107563000366/latest.html
42，Alexander Pivovarov <apivovarov@gmail.com> 写道：
r
): RDD[T] = withScope {
"
Pranay Tonpay <ptonpay@gmail.com>,"Thu, 9 Jun 2016 10:29:55 +0530",DAG in Pipeline,dev@spark.apache.org,"Hi,
Pipeline as of now seems to be having a series of transformers and
estimators in a serial fashion.
Is it possible to create a DAG sort of thing -
Eg -
Two transformers running in parallel to cleanse data (a custom built
Transformer)  in some way and then their outputs ( two outputs ) used for
some sort of correlation ( another custom built Transformer )

Let me know -

thx
pranay
"
=?gb2312?B?zfTR8w==?= <tiandiwoxin@icloud.com>,"Thu, 09 Jun 2016 13:18:47 +0800",Re: rdd.distinct with Partitioner,Alexander Pivovarov <apivovarov@gmail.com>,"Frankly speaking, I think reduceByKey with Partitioner has the same problem too and it should not be exposed to public user either. Because it is a little hard to fully understand how the partitioner behaves without looking at the actual code.  

And if there exits a basic contract of a Partitioner, maybe it should be stated explicitly in the document if not enforced by code.

However, I dont feel too strong to argue about this issue except stating my concern. It will not cause too much trouble anyway once users learn the semantics. Just a judgement call by the API designer.


Pivovarov <apivovarov@gmail.com> д
result 
is passed in.
(https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7973071962862063/2110745399505739/58107563000366/latest.html <https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/7973071962862063/2110745399505739/58107563000366/latest.html>)
Pivovarov <apivovarov@gmail.com <mailto:apivovarov@gmail.com>> д
parameter
null): RDD[T] = withScope {
x).map(_._1)

"
Pete Robbins <robbinspg@gmail.com>,"Thu, 09 Jun 2016 06:33:16 +0000",Re: Spark 2.0.0 preview docs uploaded,"Sean Owen <sowen@cloudera.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","It would be nice to have a ""what's new in 2.0.0"" equivalent to
https://spark.apache.org/releases/spark-release-1-6-0.html available or am
I just missing it?


"
Sean Owen <sowen@cloudera.com>,"Thu, 9 Jun 2016 10:06:12 +0100",Re: Spark 2.0.0 preview docs uploaded,Pete Robbins <robbinspg@gmail.com>,"Available but mostly as JIRA output:
https://spark.apache.org/news/spark-2.0.0-preview.html


---------------------------------------------------------------------


"
Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Thu, 9 Jun 2016 18:15:25 +0300",Strange exception while reading Parquet files,dev@spark.apache.org,"Hello!

I have faced a very strange exception (stack-trace in the end of this
email) while trying to read Parquet file using Hive Context from Spark
1.3.1, Hive 0.13.

This issue appears only on YARN (standalone and local are working fine) and
only when HiveContext is used (from SqlContext everything works fine).

After researching for the whole week, I was able to find only two mentions
<https://mail-archives.apache.org/mod_mbox/spark-user/201604.mbox/%3C24E8D947D2CC144DA340FBD0F71DD67232B1C912@MAILBOX-HYD.capiqcorp.com%3E>,
second is a on some companies Jira
<https://jira.talendforge.org/browse/TBD-3615>.

The only current workaround I have is upgrading to Spark 1.4.1 but this
isn't a solution.


Does anyone knows how to deal with it?


Thanks in advance,
Rostyslav Sotnychenko



---------- STACK TRACE ------------

16/06/10 00:13:58 ERROR TaskSetManager: Task 0 in stage 2.0 failed 4 times;
aborting job
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0
in stage 2.0 failed 4 times, most recent failure: Lost task 0.3 in stage
2.0 (TID 14, 2-op.cluster): java.io.EOFException
at java.io.DataInputStream.readInt(DataInputStream.java:392)
at parquet.hadoop.ParquetInputSplit.readArray(ParquetInputSplit.java:240)
at parquet.hadoop.ParquetInputSplit.readUTF8(ParquetInputSplit.java:230)
at parquet.hadoop.ParquetInputSplit.readFields(ParquetInputSplit.java:197)
at org.apache.hadoop.io.ObjectWritable.readObject(ObjectWritable.java:285)
at org.apache.hadoop.io.ObjectWritable.readFields(ObjectWritable.java:77)
at
org.apache.spark.SerializableWritable$$anonfun$readObject$1.apply$mcV$sp(SerializableWritable.scala:43)
at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1138)
at
org.apache.spark.SerializableWritable.readObject(SerializableWritable.scala:39)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1058)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1897)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1997)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1921)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1997)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1921)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:68)
at
org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:94)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:185)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
"
Holden Karau <holden@pigscanfly.ca>,"Thu, 9 Jun 2016 10:50:41 -0700",Re: JIRA SPARK-2984,Sunil Kumar <parvat_2000@yahoo.com>,"I think your error could possibly be different - looking at the original
JIRA the issue was happening on HDFS and you seem to be experiencing the
issue on s3n, and while I don't have full view of the problem I could see
this being s3 specific (read-after-write on s3 is trickier than
read-after-write on HDFS).




-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Holden Karau <holden@pigscanfly.ca>,"Thu, 9 Jun 2016 11:07:17 -0700",Re: JIRA SPARK-2984,Sunil Kumar <parvat_2000@yahoo.com>,"I'd do some searching and see if there is a JIRA related to this problem
on s3 and if you don't find one go ahead and make one. Even if it is an
intrinsic problem with s3 (and I'm not super sure since I'm just reading
this on mobile) - it would maybe be a good thing for us to document.



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Weiqing Yang <yangweiqing001@gmail.com>,"Thu, 9 Jun 2016 16:37:44 -0700",Add Caller Context in Spark,dev@spark.apache.org,"Hi,

Hadoop has implemented a feature of log tracing – caller context (Jira:
HDFS-9184 <https://issues.apache.org/jira/browse/HDFS-9184> and YARN-4349
<https://issues.apache.org/jira/browse/YARN-4349>). The motivation is to
better diagnose and understand how specific applications impacting parts of
the Hadoop system and potential problems they may be creating (e.g.
overloading NN). As HDFS mentioned inHDFS-9184
<https://issues.apache.org/jira/browse/HDFS-9184>, for a given HDFS
operation, it's very helpful to track which upper level job issues it. The
upper level callers may be specific Oozie tasks, MR jobs, hive queries,
Spark jobs.

Hadoop ecosystems like MapReduce, Tez (TEZ-2851
<https://issues.apache.org/jira/browse/TEZ-2851>), Hive (HIVE-12249
<https://issues.apache.org/jira/browse/HIVE-12249>, HIVE-12254
<https://issues.apache.org/jira/browse/HIVE-12254>) and Pig(PIG-4714
<https://issues.apache.org/jira/browse/PIG-4714>) have implemented their
caller contexts. Those systems invoke HDFS client API and Yarn client API
to setup caller context, and also expose an API to pass in caller context
into it.

Lots of Spark applications are running on Yarn/HDFS. Spark can also
implement its caller context via invoking HDFS/Yarn API, and also expose an
API to its upstream applications to set up their caller contexts. In the
end, the spark caller context written into Yarn log / HDFS log can
associate with task id, stage id, job id and app id.  That is also very
good for Spark users to identify tasks especially if Spark supports
multi-tenant environment in the future.

e.g.  Run SparkKmeans on Spark.

In HDFS log:
…
2016-05-25 15:36:23,748 INFO FSNamesystem.audit: allowed=true
ugi=yang(auth:SIMPLE)        ip=/127.0.0.1        cmd=getfileinfo
src=/data/mllib/kmeans_data.txt/_spark_metadata       dst=null
perm=null      proto=rpc callerContext=SparkKMeans
application_1464728991691_0009 running on Spark

 2016-05-25 15:36:27,893 INFO FSNamesystem.audit: allowed=true
ugi=yang (auth:SIMPLE)        ip=/127.0.0.1        cmd=open
src=/data/mllib/kmeans_data.txt       dst=null       perm=null
proto=rpc
callerContext=JobID_0_stageID_0_stageAttemptId_0_taskID_0_attemptNumber_0 on
Spark
…

“application_146472899169” is the application id.

I do have code that works with spark master branch. I am going to create a
Jira. Please feel free to let me know if you have any concern or comments.

Thanks,
Qing
"
Reynold Xin <rxin@databricks.com>,"Thu, 9 Jun 2016 16:40:55 -0700",Re: Add Caller Context in Spark,Weiqing Yang <yangweiqing001@gmail.com>,"to make sure is that Spark should still work outside of Hadoop, and also in
older versions of Hadoop.


(Jira:
of
e
an
_0 on
a
.
"
Weiqing Yang <yangweiqing001@gmail.com>,"Thu, 9 Jun 2016 17:42:04 -0700",Re: Add Caller Context in Spark,Reynold Xin <rxin@databricks.com>,"Yes, it is a string. Jira SPARK-15857
<https://issues.apache.org/jira/browse/SPARK-15857> is created.

Thanks,
WQ


 (Jira:
9
 of
he
I
t
 an
r_0 on
"
Takeshi Yamamuro <linguin.m.s@gmail.com>,"Thu, 9 Jun 2016 23:21:35 -0700",Re: Strange exception while reading Parquet files,Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Hi,

Does this issue also occur in v1.6.1 and v2.0-preview?

// maropu




-- 
---
Takeshi Yamamuro
"
Matei Zaharia <matei.zaharia@gmail.com>,"Fri, 10 Jun 2016 13:01:28 -0700",Updated Spark logo,"dev <dev@spark.apache.org>,
 user <user@spark.apache.org>","Hi all, FYI, we've recently updated the Spark logo at https://spark.apache.org/ to say ""Apache Spark"" instead of just ""Spark"". Many ASF projects have been doing this recently to make it clearer that they are associated with the ASF, and indeed the ASF's branding guidelines generally require that projects be referred to as ""Apache X"" in various settings, especially in related commercial or open source products (https://www.apache.org/foundation/marks/). If you have any kind of site or product that uses Spark logo, it would be great to update to this full one.

There are EPS versions of the logo available at https://spark.apache.org/images/spark-logo.eps and https://spark.apache.org/images/spark-logo-reverse.eps; before using these also check https://www.apache.org/foundation/marks/.

Matei
---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Fri, 10 Jun 2016 14:01:53 -0700",Re: Implementing linear albegra operations in the distributed linalg package,=?UTF-8?Q?Jos=C3=A9_Manuel_Abu=C3=ADn_Mosquera?= <abuinjm@gmail.com>,"I agree that more distributed matrix ops would be good to have, but I think
there are a few things which need to happen first:
* Now that the spark.ml package has local linear algebra separate from the
spark.mllib package, we should migrate the distributed linear algebra
implementations over to spark.ml.
* This migration will require a bit of thinking about what the API should
look like.  Should it use Datasets?  If so, are there missing requirements
to fix within Datasets or local linear algebra?

I just created a JIRA; let's discuss more there:
https://issues.apache.org/jira/browse/SPARK-15882

Thanks for bringing this up!
Joseph


sk
e
x
IUS)
"
Joseph Bradley <joseph@databricks.com>,"Sun, 12 Jun 2016 10:33:35 -0700",Re: Shrinking the DataFrame lineage,Hamel Kothari <hamelkothari@gmail.com>,"Sorry for the slow response.  I agree with Hamel on #1.
GraphFrames are mostly wrappers for GraphX algorithms.  There are a few
which are not:
* BFS: This is an iterative DataFrame alg.  Though it has unit tests, I
have not pushed it in scaling to see how far it can go.
* Belief Propagation example: This uses the conversion to and from an RDD.
Not great, but it's really just an example for now.

I definitely want to get this issue fixed ASAP!


"
Joseph Bradley <joseph@databricks.com>,"Sun, 12 Jun 2016 10:45:01 -0700",Re: Welcoming Yanbo Liang as a committer,Xiangrui Meng <meng@databricks.com>,"Congrats & welcome!


"
Joseph Bradley <joseph@databricks.com>,"Sun, 12 Jun 2016 10:47:51 -0700",Re: DAG in Pipeline,Pranay Tonpay <ptonpay@gmail.com>,"Hi Pranay,

Yes, you can do this.  The DAG structure should be specified via the
various Transformers' input and output columns, where a Transformer can
have multiple input and/or output columns.  Most of the classification and
regression Models are good examples of Transformers with multiple input and
output columns.

Hope this helps!
Joseph


"
Joseph Bradley <joseph@databricks.com>,"Sun, 12 Jun 2016 10:49:29 -0700",Re: DAG in Pipeline,Pranay Tonpay <ptonpay@gmail.com>,"in topological order according to the DAG.


"
"""Haopu Wang"" <HWang@qilinsoft.com>","Mon, 13 Jun 2016 09:11:45 +0800","RE: Should I avoid ""state"" in an Spark application?","""Haopu Wang"" <HWang@qilinsoft.com>,
	<user@spark.apache.org>,
	<dev@spark.apache.org>","Can someone look at my questions? Thanks again!

 

________________________________

From: Haopu Wang 
Sent: 2016612 16:40
To: user@spark.apache.org
Subject: Should I avoid ""state"" in an Spark application?

 

I have a Spark application whose structure is below:

 

    var ts: Long = 0L

    dstream1.foreachRDD{

        (x, time) => {

            ts = time

            x.do_something()...

        }

    }

    ......

    process_data(dstream2, ts, ......)

 

I assume foreachRDD function call can update ""ts"" variable which is then used in the Spark tasks of ""process_data"" function.

 

should I concern if switch to YARN?

 

And I saw some articles are recommending to avoid state in Scala programming. Without the state variable, how could that be done?

 

Any comments or suggestions are appreciated.

 

Thanks,

Haopu

"
Jacek Laskowski <jacek@japila.pl>,"Mon, 13 Jun 2016 19:50:46 +0200",[YARN] Small fix for yarn.Client to use buildPath (not Path.SEPARATOR),dev <dev@spark.apache.org>,"Hi,

Just noticed that yarn.Client#populateClasspath uses Path.SEPARATOR
[1] to build a CLASSPATH entry while another similar-looking line uses
buildPath method [2].

Could a pull request with a change to use buildPath at [1] be
accepted? I'm always confused how to fix such small changes.

[1] https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L1298
[2] Path.SEPARATOR

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 13 Jun 2016 20:26:14 +0200",tpcds q1 - java.lang.NegativeArraySizeException,dev <dev@spark.apache.org>,"Hi,

Running the first query of tpcds on a standalone setup (4 nodes, tpcds2 generated for scale 10 and transformed in parquet under hdfs)  it results in one exception [1].
Close to this problem I found this issue https://issues.apache.org/jira/browse/SPARK-12089 <https://issues.apache.org/jira/browse/SPARK-12089> but it seems to be solved.

Running the second query is successful.

OpenJDK 64-Bit Server VM 1.7.0_101-b00 on Linux 3.2.0-4-amd64
Intel(R) Xeon(R) CPU E5-2630 v3 @ 2.40GHz
TPCDS Snappy:                            Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------
q2                                            4512 / 8142          0.0       61769.4       1.0X

Best,
Ovidiu

[1]
WARN TaskSetManager: Lost task 17.0 in stage 80.0 (TID 4469, 172.16.96.70): java.lang.NegativeArraySizeException
	at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:61)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3$$anon$2.hasNext(WholeStageCodegenExec.scala:386)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1362)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

ERROR TaskSetManager: Task 17 in stage 80.0 failed 4 times; aborting job

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:806)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:806)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:806)
	at eduler.scala:1644)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1603)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1592)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1872)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1935)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:974)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:956)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1371)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:357)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1358)
	at org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:128)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2163)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2489)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2162)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2167)
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2167)
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2502)
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2167)
	at org.apache.spark.sql.Dataset.collect(Dataset.scala:2143)
	at org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2$$anonfun$apply$2.apply$mcVI$sp(TPCDSQueryBenchmark.scala:88)
	at org.apache.spark.util.Benchmark$$anonfun$addCase$1.apply(Benchmark.scala:75)
	at org.apache.spark.util.Benchmark$$anonfun$addCase$1.apply(Benchmark.scala:73)
	at org.apache.spark.util.Benchmark.measure(Benchmark.scala:135)
	at org.apache.spark.util.Benchmark$$anonfun$1.apply(Benchmark.scala:104)
	at org.apache.spark.util.Benchmark$$anonfun$1.apply(Benchmark.scala:102)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at org.apache.spark.util.Benchmark.run(Benchmark.scala:102)
	at org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2.apply(TPCDSQueryBenchmark.scala:90)
	at org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2.apply(TPCDSQueryBenchmark.scala:57)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
	at org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$.tpcdsAll(TPCDSQueryBenchmark.scala:57)
	at org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$.main(TPCDSQueryBenchmark.scala:135)
	at org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark.main(TPCDSQueryBenchmark.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.NegativeArraySizeException
	at org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:61)
	at org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3$$anon$2.hasNext(WholeStageCodegenExec.scala:386)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:664)
	at org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1365)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1362)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
	at org.apache.spark.scheduler.Task.run(Task.scala:85)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

"
Sean Owen <sowen@cloudera.com>,"Mon, 13 Jun 2016 20:06:48 +0100",Re: [YARN] Small fix for yarn.Client to use buildPath (not Path.SEPARATOR),Jacek Laskowski <jacek@japila.pl>,"Yeah it does the same thing anyway. It's fine to consistently use the
method. I think there's an instance in ClientSuite that can use it.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 13 Jun 2016 12:25:33 -0700",Re: tpcds q1 - java.lang.NegativeArraySizeException,Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Did you try this on master?



"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Mon, 13 Jun 2016 21:54:23 +0200",Re: tpcds q1 - java.lang.NegativeArraySizeException,Reynold Xin <rxin@databricks.com>,"Yes, commit ad102af 

<ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>> tpcds2 generated for scale 10 and transformed in parquet under hdfs)  it results in one exception [1].
https://issues.apache.org/jira/browse/SPARK-12089 <https://issues.apache.org/jira/browse/SPARK-12089> but it seems to be solved.
Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------
      61769.4       1.0X
172.16.96.70): java.lang.NegativeArraySizeException
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:61)
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3$$anon$2.hasNext(WholeStageCodegenExec.scala:386)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)
org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1365)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1362)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
job
<http://org.apache.spark.scheduler.dagscheduler.org/>$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:806)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:806)
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:806)
eduler.scala:1644)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1603)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1592)
org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:974)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1371)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:128)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2163)
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2489)
<http://org.apache.spark.sql.dataset.org/>$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2162)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2167)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2167)
<http://org.apache.spark.sql.dataset.org/>$apache$spark$sql$Dataset$$collect(Dataset.scala:2167)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2$$anonfun$apply$2.apply$mcVI$sp(TPCDSQueryBenchmark.scala:88)
org.apache.spark.util.Benchmark$$anonfun$addCase$1.apply(Benchmark.scala:75)
org.apache.spark.util.Benchmark$$anonfun$addCase$1.apply(Benchmark.scala:73)
org.apache.spark.util.Benchmark$$anonfun$1.apply(Benchmark.scala:104)
org.apache.spark.util.Benchmark$$anonfun$1.apply(Benchmark.scala:102)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
scala.collection.AbstractTraversable.map(Traversable.scala:104)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2.apply(TPCDSQueryBenchmark.scala:90)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2.apply(TPCDSQueryBenchmark.scala:57)
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$.tpcdsAll(TPCDSQueryBenchmark.scala:57)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$.main(TPCDSQueryBenchmark.scala:135)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark.main(TPCDSQueryBenchmark.scala)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:61)
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3$$anon$2.hasNext(WholeStageCodegenExec.scala:386)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:664)
org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1365)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1362)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

"
Sameer Agarwal <sameer@databricks.com>,"Mon, 13 Jun 2016 15:12:34 -0700",Re: tpcds q1 - java.lang.NegativeArraySizeException,Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"I'm unfortunately not able to reproduce this on master. Does the query
always fail deterministically?




-- 
Sameer Agarwal
Software Engineer | Databricks Inc.
http://cs.berkeley.edu/~sameerag
"
Mingyu Kim <mkim@palantir.com>,"Tue, 14 Jun 2016 00:30:20 +0000",Utilizing YARN AM RPC port field,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

 

YARN provides a way for AppilcationMaster to register a RPC port so that a client outside the YARN cluster can reach the application for any RPCs, but Spark’s YARN AMs simply register a dummy port number of 0. (See https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnRMClient.scala#L74) This is useful for the long-running Spark application usecases where jobs are submitted via a form of RPC to an already started Spark context running in YARN cluster mode. Spark job server (https://github.com/spark-jobserver/spark-jobserver) and Livy (https://github.com/cloudera/hue/tree/master/apps/spark/java) are good open-source examples of these usecases. The current work-around is to have the Spark AM make a call back to a configured URL with the port number of the RPC server for the client to communicate with the AM.

 

Utilizing YARN AM RPC port allows the port number reporting to be done in a secure way (i.e. With AM RPC port field and Kerberized YARN cluster, you don’t need to re-invent a way to verify the authenticity of the port number reporting.) and removes the callback from YARN cluster back to a client, which means you can operate YARN in a low-trust environment and run other client applications behind a firewall.

 

A couple of proposals for utilizing YARN AM RPC port I have are, (Note that you cannot simply pre-configure the port number and pass it to Spark AM via configuration because of potential port conflicts on the YARN node)

 

·         Start-up an empty Jetty server during Spark AM initialization, set the port number when registering AM with RM, and pass a reference to the Jetty server into the Spark application (e.g. through SparkContext) for the application to dynamically add servlet/resources to the Jetty server.

·         Have an optional static method in the main class (e.g. initializeRpcPort()) which optionally sets up a RPC server and returns the RPC port. Spark AM can call this method, register the port number to RM and continue on with invoking the main method. I don’t see this making a good API, though.

 

I’m curious to hear what other people think. Would this be useful for anyone? What do you think about the proposals? Please feel free to suggest other ideas. Thanks!

 

Mingyu

"
Egor Pahomov <pahomov.egor@gmail.com>,"Mon, 13 Jun 2016 17:56:02 -0700",Return binary mode in ThriftServer,"Reynold Xin <rxin@databricks.com>, davies@databricks.com, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","In May due to the SPARK-15095 binary mode was ""removed"" (code is there, but
you can not turn it on) from Spark-2.0. In 1.6.1 binary was default and in
2.0.0-preview it was removed. It's really annoying:

   - I can not use Tableau+Spark anymore
   - I need to change connection URL in SQL client for every analyst in my
   organization. And with Squirrel I experiencing problems with that.
   - We have parts of infrastructure, which connected to data
   infrastructure though ThriftServer. And of course format was binary.

I've created a ticket to get binary back(
https://issues.apache.org/jira/browse/SPARK-15934), but that's not the
point. I've experienced this problem a month ago, but haven't done anything
about it, because I believed, that I'm stupid and doing something wrong.
But documentation was release recently and it contained no information
about this new thing and it made me digging.

Most of what I describe is just annoying, but Tableau+Spark new
incompatibility I believe is big deal. Maybe I'm wrong and there are ways
to make things work, it's just I wouldn't expect move to 2.0.0 to be so
time consuming.

My point: Do we have any guidelines regarding doing such radical things?

-- 


*Sincerely yoursEgor Pakhomov*
"
Reynold Xin <rxin@databricks.com>,"Mon, 13 Jun 2016 18:11:37 -0700",Re: Return binary mode in ThriftServer,Egor Pahomov <pahomov.egor@gmail.com>,"Thanks for the email. Things like this (and bugs) are exactly the reason
the preview releases exist. It seems like enough people have run into
problem with this one that maybe we should just bring it back for backward
compatibility.


"
dvlpr <nandolaprasiddh@gmail.com>,"Tue, 14 Jun 2016 00:29:09 -0700 (MST)",Custom receiver to connect MySQL database,dev@spark.apache.org,"Hi folks,
I have written some codes for custom receiver to get data from MySQL db.
Belowed code:

class CustomReceiver(url: String, username: String, password: String)
extends Receiver[String](StorageLevel.MEMORY_AND_DISK_2) with Logging {

 case class customer(c_sk: Int, c_add_sk: Int, c_first: String)
 
  def onStart() {
    // Start the thread that receives data over a connection
    new Thread(""MySQL Receiver"") {
      override def run() { receive() }
    }.start()
  }

  def onStop() {
   // There is nothing much to do as the thread calling receive()
   // is designed to stop by itself isStopped() returns false
  }

  private def receive() {
    Class.forName(""com.mysql.jdbc.Driver"").newInstance()
    val con = DriverManager.getConnection(url, username, password)
    }

while executing this code i am getting an error: Exception in thread ""main""
java.lang.IllegalArgumentException: requirement failed: No output operations
registered, so nothing to execute

Please help me to solve my problem ? 



--

---------------------------------------------------------------------


"
Adam Roberts <AROBERTS@uk.ibm.com>,"Tue, 14 Jun 2016 12:18:12 +0100",Databricks SparkPerf with Spark 2.0,dev <dev@spark.apache.org>,"Hi, I'm working on having ""SparkPerf"" (
https://github.com/databricks/spark-perf) run with Spark 2.0, noticed a 
few pull requests not yet accepted so concerned this project's been 
abandoned - it's proven very useful in the past for quality assurance as 
we can easily exercise lots of Spark functions with a cluster (perhaps 
exposing problems that don't surface with the Spark unit tests).

I want to use Scala 2.11.8 and Spark 2.0.0 so I'm making my way through 
various files, currently faced with a NoSuchMethod exception

NoSuchMethodError: 
org/apache/spark/SparkContext.rddToPairRDDFunctions(Lorg/apache/spark/rdd/RDD;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;Lscala/math/Ordering;)Lorg/apache/spark/rdd/PairRDDFunctions; 
at spark.perf.AggregateByKey.runTest(KVDataTest.scala:137) 

class AggregateByKey(sc: SparkContext) extends KVDataTest(sc) {
  override def runTest(rdd: RDD[_], reduceTasks: Int) {
    rdd.asInstanceOf[RDD[(String, String)]]
      .map{case (k, v) => (k, v.toInt)}.reduceByKey(_ + _, 
reduceTasks).count()
  }
}

Grepping shows
./spark-tests/target/streams/compile/incCompileSetup/$global/streams/inc_compile_2.10:/home/aroberts/Desktop/spark-perf/spark-tests/src/main/scala/spark/perf/KVDataTest.scala 
-> rddToPairRDDFunctions 

The scheduling-throughput tests complete fine but the problem here is seen 
with agg-by-key (and likely other modules to fix owing to API changes 
between 1.x and 2.x which I guess is the cause of the above problem).

Has anybody already made good progress here? Would like to work together 
and get this available for everyone, I'll be churning through it either 
way. Will be looking at HiBench also.

Next step for me is to use sbt -Dspark.version=2.0.0 (2.0.0-preview?) and 
work from there, although I figured the prep tests stage would do this for 
me (how else is it going to build?).

Cheers,




Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Steve Loughran <stevel@hortonworks.com>,"Tue, 14 Jun 2016 11:44:07 +0000","Re: [YARN] Small fix for yarn.Client to use buildPath (not
 Path.SEPARATOR)",dev <dev@spark.apache.org>,"
if you want to be able to build up CPs on windows to run on a Linux cluster, or vice-versa, you really need to be using the Environment.CLASS_PATH_SEPARATOR field, ""<CPS>"". This is expanded in the cluster, not in the client

Although tagged as @Public, @Unstable, it's been in there sinceYARN-1824 & Hadoop 2.4; things rely on it. If someone wants to fix that by submitting a patch to YARN-5247; I'll review it.

apache/spark/deploy/yarn/Client.scala#L1298


---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 14 Jun 2016 11:55:08 +0000",Re: Utilizing YARN AM RPC port field,Mingyu Kim <mkim@palantir.com>,"
On 14 Jun 2016, at 01:30, Mingyu Kim <mkim@palantir.com<mailto:mkim@palantir.com>> wrote:

Hi all,

YARN provides a way for AppilcationMaster to register a RPC port so that a client outside the YARN cluster can reach the application for any RPCs, but Spark’s YARN AMs simply register a dummy port number of 0. (See https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnRMClient.scala#L74) This is useful for the long-running Spark application usecases where jobs are submitted via a form of RPC to an already started Spark context running in YARN cluster mode. Spark job server (https://github.com/spark-jobserver/spark-jobserver) and Livy (https://github.com/cloudera/hue/tree/master/apps/spark/java) are good open-source examples of these usecases. The current work-around is to have the Spark AM make a call back to a configured URL with the port number of the RPC server for the client to communicate with the AM.

Utilizing YARN AM RPC port allows the port number reporting to be done in a secure way (i.e. With AM RPC port field and Kerberized YARN cluster, you don’t need to re-invent a way to verify the authenticity of the port number reporting.) and removes the callback from YARN cluster back to a client, which means you can operate YARN in a low-trust environment and run other client applications behind a firewall.

A couple of proposals for utilizing YARN AM RPC port I have are, (Note that you cannot simply pre-configure the port number and pass it to Spark AM via configuration because of potential port conflicts on the YARN node)

•         Start-up an empty Jetty server during Spark AM initialization, set the port number when registering AM with RM, and pass a reference to the Jetty server into the Spark application (e.g. through SparkContext) for the application to dynamically add servlet/resources to the Jetty server.
•         Have an optional static method in the main class (e.g. initializeRpcPort()) which optionally sets up a RPC server and returns the RPC port. Spark AM can call this method, register the port number to RM and continue on with invoking the main method. I don’t see this making a good API, though.

I’m curious to hear what other people think. Would this be useful for anyone? What do you think about the proposals? Please feel free to suggest other ideas. Thanks!


It's a recurrent irritation of mine that you can't ever change the HTTP/RPC ports of a YARN AM after launch; it creates a complex startup state where you can't register until your IPC endpoints are up.

Tactics

-Create a socket on an empty port, register it, hand off the port to the RPC setup code as the chosen port. Ideally, support a range to scan, so that systems which only open a specific range of ports, e.g. 6500-6800 can have those ports only scanned. We've done this in other projects.

-serve up the port binding info via a REST API off the AM web; clients hit the (HEAD/GET only RM Proxy), ask for the port, work on it. Nonstandard; could be extensible with other binding information. (TTL of port caching, ....)

-Use the YARN-913 ZK based registry to register/lookup bindings. This is used in various YARN apps to register service endpoints (RPC, Rest); there's work ongoing for DNS support. this would allow you to use DNS against a specific DNS server to get the endpoints. Works really well with containerized deployments where the apps come up with per-container IPaddresses and fixed ports.
Although you couldn't get the latter into the spark-yarn codeitself (needs Hadoop 2.6+), you can plug in support via the extension point implemented in SPARK-11314., I've actually thought of doing that for a while...just been too busy.

-Just fix the bit of the YARN api that forces you to know your endpoints in advance. People will appreciate it, though it will take a while to trickle downstream.




"
Adam Roberts <AROBERTS@uk.ibm.com>,"Tue, 14 Jun 2016 12:56:12 +0100",Re: Databricks SparkPerf with Spark 2.0,dev <dev@spark.apache.org>,"Fixed the below problem, grepped for spark.version, noticed some instances 
of 1.5.2 being declared, changed to 2.0.0-preview in 
spark-tests/project/SparkTestsBuild.scala

Next one to fix is:
16/06/14 12:52:44 INFO ContextCleaner: Cleaned shuffle 9
Exception in thread ""main"" java.lang.NoSuchMethodError: 
org/json4s/jackson/JsonMethods$.render$default$2(Lorg/json4s/JsonAST$JValue;)Lorg/json4s/Formats;

I'm going to log this and further progress under ""Issues"" for the project 
itself (probably need to change org.json4s version in 
SparkTestsBuild.scala, now I know this file is super important), so the 
emails here will at least point people there.

Cheers,







From:   Adam Roberts/UK/IBM@IBMGB
To:     dev <dev@spark.apache.org>
Date:   14/06/2016 12:18
Subject:        Databricks SparkPerf with Spark 2.0



Hi, I'm working on having ""SparkPerf"" (
https://github.com/databricks/spark-perf) run with Spark 2.0, noticed a 
few pull requests not yet accepted so concerned this project's been 
abandoned - it's proven very useful in the past for quality assurance as 
we can easily exercise lots of Spark functions with a cluster (perhaps 
exposing problems that don't surface with the Spark unit tests). 

I want to use Scala 2.11.8 and Spark 2.0.0 so I'm making my way through 
various files, currently faced with a NoSuchMethod exception 

NoSuchMethodError: 
org/apache/spark/SparkContext.rddToPairRDDFunctions(Lorg/apache/spark/rdd/RDD;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;Lscala/math/Ordering;)Lorg/apache/spark/rdd/PairRDDFunctions; 
at spark.perf.AggregateByKey.runTest(KVDataTest.scala:137) 

class AggregateByKey(sc: SparkContext) extends KVDataTest(sc) {
  override def runTest(rdd: RDD[_], reduceTasks: Int) {
    rdd.asInstanceOf[RDD[(String, String)]]
      .map{case (k, v) => (k, v.toInt)}.reduceByKey(_ + _, 
reduceTasks).count()
  } 
}

Grepping shows
./spark-tests/target/streams/compile/incCompileSetup/$global/streams/inc_compile_2.10:/home/aroberts/Desktop/spark-perf/spark-tests/src/main/scala/spark/perf/KVDataTest.scala 
-> rddToPairRDDFunctions 

The scheduling-throughput tests complete fine but the problem here is seen 
with agg-by-key (and likely other modules to fix owing to API changes 
between 1.x and 2.x which I guess is the cause of the above problem). 

Has anybody already made good progress here? Would like to work together 
and get this available for everyone, I'll be churning through it either 
way. Will be looking at HiBench also. 

Next step for me is to use sbt -Dspark.version=2.0.0 (2.0.0-preview?) and 
work from there, although I figured the prep tests stage would do this for 
me (how else is it going to build?). 

Cheers, 




Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU

Unless stated otherwise above:
IBM United Kingdom Limited - Registered in England and Wales with number 
741598. 
Registered office: PO Box 41, North Harbour, Portsmouth, Hampshire PO6 3AU
"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Tue, 14 Jun 2016 14:04:50 +0200",Re: tpcds q1 - java.lang.NegativeArraySizeException,Sameer Agarwal <sameer@databricks.com>,"I confirm the same exception for other queries as well. I was able to reproduce it many times.
Queries 1, 3 and 5 failed with the same exception. Queries 2 and 4 are running ok.

I am using TPCDSQueryBenchmark and I have used the following settings:

spark.conf.set(SQLConf.PARQUET_VECTORIZED_READER_ENABLED.key, ""true"")
spark.conf.set(SQLConf.WHOLESTAGE_CODEGEN_ENABLED.key, ""true”)

 spark.executor.memory              102g
 spark.executor.extraJavaOptions    -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:ObjectAlignmentInBytes=32
 spark.executor.cores               16
 spark.driver.maxResultSize         32g
 spark.default.parallelism          128
 spark.sql.shuffle.partitions       128
 spark.sql.parquet.compression.codec snappy
 spark.sql.optimizer.maxIterations  500
 spark.sql.autoBroadcastJoinThreshold 41943040
 spark.shuffle.file.buffer          64k
 spark.akka.frameSize               128
 spark.shuffle.manager              sort


always fail deterministically?
<ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>> <ovidiu-cristian.marcu@inria.fr <mailto:ovidiu-cristian.marcu@inria.fr>> tpcds2 generated for scale 10 and transformed in parquet under hdfs)  it results in one exception [1].
https://issues.apache.org/jira/browse/SPARK-12089 <https://issues.apache.org/jira/browse/SPARK-12089> but it seems to be solved.
Rate(M/s)   Per Row(ns)   Relative
------------------------------------------------------------------------------------------------
0.0       61769.4       1.0X
172.16.96.70): java.lang.NegativeArraySizeException
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:61)
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3$$anon$2.hasNext(WholeStageCodegenExec.scala:386)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:628)
org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1365)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1362)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
job
<http://org.apache.spark.scheduler.dagscheduler.org/>$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:806)
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:806)
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:806)
eduler.scala:1644)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1603)
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1592)
org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:974)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1371)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:128)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2163)
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2489)
<http://org.apache.spark.sql.dataset.org/>$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2162)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2167)
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$collect$1.apply(Dataset.scala:2167)
<http://org.apache.spark.sql.dataset.org/>$apache$spark$sql$Dataset$$collect(Dataset.scala:2167)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2$$anonfun$apply$2.apply$mcVI$sp(TPCDSQueryBenchmark.scala:88)
org.apache.spark.util.Benchmark$$anonfun$addCase$1.apply(Benchmark.scala:75)
org.apache.spark.util.Benchmark$$anonfun$addCase$1.apply(Benchmark.scala:73)
org.apache.spark.util.Benchmark$$anonfun$1.apply(Benchmark.scala:104)
org.apache.spark.util.Benchmark$$anonfun$1.apply(Benchmark.scala:102)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
scala.collection.AbstractTraversable.map(Traversable.scala:104)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2.apply(TPCDSQueryBenchmark.scala:90)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$$anonfun$tpcdsAll$2.apply(TPCDSQueryBenchmark.scala:57)
scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:35)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$.tpcdsAll(TPCDSQueryBenchmark.scala:57)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark$.main(TPCDSQueryBenchmark.scala:135)
org.apache.spark.sql.execution.benchmark.TPCDSQueryBenchmark.main(TPCDSQueryBenchmark.scala)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:729)
org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:185)
org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:210)
org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:124)
org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder.grow(BufferHolder.java:61)
org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter.write(UnsafeRowWriter.java:214)
org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$doExecute$3$$anon$2.hasNext(WholeStageCodegenExec.scala:386)
scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
org.spark_project.guava.collect.Ordering.leastOf(Ordering.java:664)
org.apache.spark.util.collection.Utils$.takeOrdered(Utils.scala:37)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1365)
org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1$$anonfun$30.apply(RDD.scala:1362)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:757)
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
"
Jacek Laskowski <jacek@japila.pl>,"Tue, 14 Jun 2016 14:58:32 +0200",Re: [YARN] Small fix for yarn.Client to use buildPath (not Path.SEPARATOR),Steve Loughran <stevel@hortonworks.com>,"Hi Steve and Sean,

Didn't expect such a warm welcome from Sean and you! Since I'm with
Spark on YARN these days, let me see what I can do to make it nicer.
Thanks!

I'm going to change Spark to use buildPath first. And then propose
another patch to use Environment.CLASS_PATH_SEPARATOR instead. And
only then I could work on
https://issues.apache.org/jira/browse/YARN-5247. Is this about
changing the annotation(s) only?

Thanks for your support!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Chris Fregly <chris@fregly.com>,"Tue, 14 Jun 2016 09:06:05 -0400",Re: Return binary mode in ThriftServer,Reynold Xin <rxin@databricks.com>,"+1 on bringing it back.  causing all sorts of problems on my end that was not obvious without digging in

I was having problems building spark, as well, with the --hive-thriftserver flag.  also thought I was doing something wrong on my end.

he preview re"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Tue, 14 Jun 2016 13:37:28 +0000",RE: Return binary mode in ThriftServer,"Chris Fregly <chris@fregly.com>, Reynold Xin <rxin@databricks.com>","I experienced something similar using Sprak+Microstretegy
I reformatted the commands file locally and recompiled spark 2.0, for the issue was resolved, but I am not sure I made the change in the correct direction
1.    SPARK-14947<https://issues.apache.org/jira/browse/SPARK-14947>




Raymond Honderdors
Team Lead Analytics BI
Business Intelligence Developer
raymond.honderdors@sizmek.com<mailto:raymond.honderdors@sizmek.com>
T +972.7325.3569
Herzliya

From: Chris Fregly [mailto:chris@fregly.com]
Sent: Tuesday, June 14, 2016 4:06 PM
To: Reynold Xin <rxin@databricks.com>
Cc: Egor Pahomov <pahomov.egor@gmail.com>; davies@databricks.com; dev@spark.apache.org
Subject: Re: Return binary mode in ThriftServer

+1 on bringing it back.  causing all sorts of problems on my end that was not obvious without digging in

I was having problems building spark, as well, with the --hive-thriftserver flag.  also thought I was doing something wrong on my end.

On Jun 13, 2016, at 9:11 PM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:
Thanks for the email. Things like this (and bugs) are exactly the reason the preview releases exist. It seems like enough people have run into problem with this one that maybe we should just bring it back for backward compatibility.

On Monday, June 13, 2016, Egor Pahomov <pahomov.egor@gmail.com<mailto:pahomov.egor@gmail.com>> wrote:
In May due to the SPARK-15095 binary mode was ""removed"" (code is there, but you can not turn it on) from Spark-2.0. In 1.6.1 binary was default and in 2.0.0-preview it was removed. It's really annoying:

  *   I can not use Tableau+Spark anymore
  *   I need to change connection URL in SQL client for every analyst in my organization. And with Squirrel I experiencing problems with that.
  *   We have parts of infrastructure, which connected to data infrastructure though ThriftServer. And of course format was binary.
I've created a ticket to get binary back(https://issues.apache.org/jira/browse/SPARK-15934), but that's not the point. I've experienced this problem a month ago, but haven't done anything about it, because I believed, that I'm stupid and doing something wrong. But documentation was release recently and it contained no information about this new thing and it made me digging.

Most of what I describe is just annoying, but Tableau+Spark new incompatibility I believe is big deal. Maybe I'm wrong and there are ways to make things work, it's just I wouldn't expect move to 2.0.0 to be so time consuming.

My point: Do we have any guidelines regarding doing such radical things?

--
Sincerely yours
Egor Pakhomov

"
lalit sharma <lalitkishore09@gmail.com>,"Tue, 14 Jun 2016 19:09:03 +0530",Re: Return binary mode in ThriftServer,Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"+1 for bringing this back.

 Binary mode needs to be present for working with data visualization tools.

--Regards,
Lalit


"
Michael Armbrust <michael@databricks.com>,"Tue, 14 Jun 2016 10:36:08 -0700",Re: Databricks SparkPerf with Spark 2.0,Adam Roberts <AROBERTS@uk.ibm.com>,"NoSuchMethodError always means that you are compiling against a different
classpath than is available at runtime, so it sounds like you are on the
right track.  The project is not abandoned, we're just busy with the
release.  It would be great if you could open a pull request.


"
Matthias Niehoff <matthias.niehoff@codecentric.de>,"Tue, 14 Jun 2016 20:45:30 +0200",Re: Custom receiver to connect MySQL database,dvlpr <nandolaprasiddh@gmail.com>,"You must add an output operation your normal stream application that uses
the receiver.  Calling print() on the DStream will do the job

2016-06-14 9:29 GMT+02:00 dvlpr <nandolaprasiddh@gmail.com>:

n""
-to-connect-MySQL-database-tp17895.html


-- 
Matthias Niehoff | IT-Consultant | Agile Software Factory  | Consulting
codecentric AG | Zeppelinstr 2 | 76185 Karlsruhe | Deutschland
tel: +49 (0) 721.9595-681 | fax: +49 (0) 721.9595-666 | mobil: +49 (0)
172.1702676
www.codecentric.de | blog.codecentric.de | www.meettheexperts.de |
www.more4fi.de

Sitz der Gesellschaft: Solingen | HRB 25917| Amtsgericht Wuppertal
Vorstand: Michael Hochgürtel . Mirko Novakovic . Rainer Vehns
Aufsichtsrat: Patric Fedlmeier (Vorsitzender) . Klaus Jäger . Jürgen Schütz

Diese E-Mail einschließlich evtl. beigefügter Dateien enthält vertrauliche
und/oder rechtlich geschützte Informationen. Wenn Sie nicht der richtige
Adressat sind oder diese E-Mail irrtümlich erhalten haben, informieren Sie
bitte sofort den Absender und löschen Sie diese E-Mail und evtl.
beigefügter Dateien umgehend. Das unerlaubte Kopieren, Nutzen oder Öffnen
evtl. beigefügter Dateien sowie die unbefugte Weitergabe dieser E-Mail ist
nicht gestattet
"
"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","Tue, 14 Jun 2016 15:12:51 -0400",Spark Assembly jar ?,dev@spark.apache.org,"Just wondering where the spark-assembly jar has gone in 2.0. i've been
reading that its been removed but i'm not sure what the new workflow is .
"
Reynold Xin <rxin@databricks.com>,"Tue, 14 Jun 2016 12:23:32 -0700",Re: Spark Assembly jar ?,"""Franklyn D'souza"" <franklyn.dsouza@shopify.com>","You just need to run normal packaging and all the scripts are now setup to
run without the assembly jars.


"
dvlpr <nandolaprasiddh@gmail.com>,"Tue, 14 Jun 2016 12:24:34 -0700 (MST)",Re: Custom receiver to connect MySQL database,dev@spark.apache.org,"I have tried but it gives me an error. because something is missing in my
code



--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Tue, 14 Jun 2016 13:06:37 -0700",Re: spark-ec2 scripts with spark-2.0.0-preview,Sunil Kumar <parvat_2000@yahoo.com>,"Can you open an issue on https://github.com/amplab/spark-ec2 ?  I
think we should be able to escape the version string and pass the
2.0.0-preview through the scripts

Shivaram


---------------------------------------------------------------------


"
Mingyu Kim <mkim@palantir.com>,"Tue, 14 Jun 2016 21:13:37 +0000",Re: Utilizing YARN AM RPC port field,Steve Loughran <stevel@hortonworks.com>,"Thanks for the pointers, Steve!

 

The first option sounds like a the most light-weight and non-disruptive option among them. So, we can add a configuration that enables socket initialization, Spark AM will create a ServerSocket if the socket init is enabled and set it on SparkContext

 

If there are no objections, I can file a bug and find time to tackle it myself. 

 

Mingyu

 

From: Steve Loughran <stevel@hortonworks.com>
Date: Tuesday, June 14, 2016 at 4:55 AM
To: Mingyu Kim <mkim@palantir.com>
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>, Matt Cheah <mcheah@palantir.com>
Subject: Re: Utilizing YARN AM RPC port field

 

 


 

Hi all,

 

YARN provides a way for AppilcationMaster to register a RPC port so that a client outside the YARN cluster can reach the application for any RPCs, but Spark’s YARN AMs simply register a dummy port number of 0. (See https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnRMClient.scala#L74) This is useful for the long-running Spark application usecases where jobs are submitted via a form of RPC to an already started Spark context running in YARN cluster mode. Spark job server (https://github.com/spark-jobserver/spark-jobserver) and Livy (https://github.com/cloudera/hue/tree/master/apps/spark/java) are good open-source examples of these usecases. The current work-around is to have the Spark AM make a call back to a configured URL with the port number of the RPC server for the client to communicate with the AM.

 

Utilizing YARN AM RPC port allows the port number reporting to be done in a secure way (i.e. With AM RPC port field and Kerberized YARN cluster, you don’t need to re-invent a way to verify the authenticity of the port number reporting.) and removes the callback from YARN cluster back to a client, which means you can operate YARN in a low-trust environment and run other client applications behind a firewall.

 

A couple of proposals for utilizing YARN AM RPC port I have are, (Note that you cannot simply pre-configure the port number and pass it to Spark AM via configuration because of potential port conflicts on the YARN node)

 

·         Start-up an empty Jetty server during Spark AM initialization, set the port number when registering AM with RM, and pass a reference to the Jetty server into the Spark application (e.g. through SparkContext) for the application to dynamically add servlet/resources to the Jetty server.

·         Have an optional static method in the main class (e.g. initializeRpcPort()) which optionally sets up a RPC server and returns the RPC port. Spark AM can call this method, register the port number to RM and continue on with invoking the main method. I don’t see this making a good API, though.

 

I’m curious to hear what other people think. Would this be useful for anyone? What do you think about the proposals? Please feel free to suggest other ideas. Thanks!

 

 

It's a recurrent irritation of mine that you can't ever change the HTTP/RPC ports of a YARN AM after launch; it creates a complex startup state where you can't register until your IPC endpoints are up.

 

Tactics

 

-Create a socket on an empty port, register it, hand off the port to the RPC setup code as the chosen port. Ideally, support a range to scan, so that systems which only open a specific range of ports, e.g. 6500-6800 can have those ports only scanned. We've done this in other projects.

 

-serve up the port binding info via a REST API off the AM web; clients hit the (HEAD/GET only RM Proxy), ask for the port, work on it. Nonstandard; could be extensible with other binding information. (TTL of port caching, ....)

 

-Use the YARN-913 ZK based registry to register/lookup bindings. This is used in various YARN apps to register service endpoints (RPC, Rest); there's work ongoing for DNS support. this would allow you to use DNS against a specific DNS server to get the endpoints. Works really well with containerized deployments where the apps come up with per-container IPaddresses and fixed ports.

Although you couldn't get the latter into the spark-yarn codeitself (needs Hadoop 2.6+), you can plug in support via the extension point implemented in SPARK-11314., I've actually thought of doing that for a while...just been too busy.

 

-Just fix the bit of the YARN api that forces you to know your endpoints in advance. People will appreciate it, though it will take a while to trickle downstream.

 

 

 

 

"
Egor Pahomov <pahomov.egor@gmail.com>,"Tue, 14 Jun 2016 15:21:20 -0700",Re: Spark Assembly jar ?,Reynold Xin <rxin@databricks.com>,"It's strange for me, that having and support fat jar was never a important
thing. We have next scenario - we have big application, where spark is just
another library for data processing. So we can not create small jar and
feed it to spark scripts - we need to call spark from application. And
having fat jar as maven dependency is perfect. We have some spark installed
on cluster(whatever cloudera put there), but often we need to patch spark
for our needs, so we need to bring everything with us. Different
departments use different spark versions - so we can not share jars on
cluster easily. Yep, there are some disadvantages, but flexibility of
changing spark process and deploying overcome these disadvantages.

So we probably would patch pom's as usual to create fat jar.

2016-06-14 12:23 GMT-07:00 Reynold Xin <rxin@databricks.com>:



-- 


*Sincerely yoursEgor Pakhomov*
"
Kun Liu <kl1885@nyu.edu>,"Tue, 14 Jun 2016 19:20:04 -0400",send this email to subscribe,dev@spark.apache.org,"-- 
*  Kun Liu*
  M.S in Computer Science
  New York University
  Phone: (917) 864-1016
"
dvlpr <nandolaprasiddh@gmail.com>,"Tue, 14 Jun 2016 23:05:59 -0700 (MST)",Re: Custom receiver to connect MySQL database,dev@spark.apache.org,"hii

*Prasiddh Nandola*
MSc Computer Science
Technical University Kaiserslautern
+49 176 67817239






--"
dvlpr <nandolaprasiddh@gmail.com>,"Tue, 14 Jun 2016 23:06:09 -0700 (MST)",Re: Custom receiver to connect MySQL database,dev@spark.apache.org,"I have tried but it gives me an error. because something is missing in my
code

*Prasiddh Nandola*
MSc Computer Science
Technical University Kaiserslautern
+49 176 67817239






--"
Steve Loughran <stevel@hortonworks.com>,"Wed, 15 Jun 2016 08:49:59 +0000","Re: [YARN] Small fix for yarn.Client to use buildPath (not
 Path.SEPARATOR)",Jacek Laskowski <jacek@japila.pl>,"

yes; though I should warn that I have evidence that some people have been breaking things tagged a stable; 

https://issues.apache.org/jira/browse/YARN-5130

what that @Stable marker does do is give you ability to complain when things break

ter, or vice-versa, you really need to be using the Environment.CLASS_PATH_SEPARATOR field, ""<CPS>"". This is expanded in the cluster, not in the client
 & Hadoop 2.4; things rely on it. If someone wants to fix that by submitting a patch to YARN-5247; I'll review it.
e:
g/apache/spark/deploy/yarn/Client.scala#L1298


---------------------------------------------------------------------


"
Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Wed, 15 Jun 2016 12:35:39 +0300",Hive-on-Spark with access to Hive from Spark jobs,dev@spark.apache.org,"Hello!

I have a question regarding Hive and Spark.

As far as I know, in order to use Hive-on-Spark one need to compile Spark
without Hive profile, but that means that it won't be possible to access
Hive from normal Spark jobs.

How is community going to address this issue? Making two different
spark-assembly jars or something else?


Thanks,
Rostyslav
"
VG <vlinked@gmail.com>,"Wed, 15 Jun 2016 19:50:50 +0530","ERROR RetryingBlockFetcher: Exception while beginning fetch of 1
 outstanding blocks",dev@spark.apache.org,"I have a very simple driver which loads a textFile and filters a sub-string
from each line in the textfile.
When the collect action is executed , I am getting an exception.   (The
file is only 90 MB - so I am confused what is going on..) I am running on a
local standalone cluster

16/06/15 19:45:22 INFO BlockManagerInfo: Removed broadcast_2_piece0 on
192.168.56.1:56413 in memory (size: 2.5 KB, free: 2.4 GB)
16/06/15 19:45:22 INFO BlockManagerInfo: Removed broadcast_1_piece0 on
192.168.56.1:56413 in memory (size: 1900.0 B, free: 2.4 GB)
16/06/15 19:45:22 INFO BlockManagerInfo: Added rdd_2_1 on disk on
192.168.56.1:56413 (size: 2.7 MB)
16/06/15 19:45:22 INFO MemoryStore: Block taskresult_7 stored as bytes in
memory (estimated size 2.7 MB, free 2.4 GB)
16/06/15 19:45:22 INFO BlockManagerInfo: Added taskresult_7 in memory on
192.168.56.1:56413 (size: 2.7 MB, free: 2.4 GB)
16/06/15 19:45:22 INFO Executor: Finished task 1.0 in stage 2.0 (TID 7).
2823777 bytes result sent via BlockManager)
16/06/15 19:45:22 INFO TaskSetManager: Starting task 2.0 in stage 2.0 (TID
8, localhost, partition 2, PROCESS_LOCAL, 5422 bytes)
16/06/15 19:45:22 INFO Executor: Running task 2.0 in stage 2.0 (TID 8)
16/06/15 19:45:22 INFO HadoopRDD: Input split:
file:/C:/Users/i303551/Downloads/ariba-logs/ssws/access.2016.04.26/access.2016.04.26:67108864+25111592
16/06/15 19:45:22 INFO BlockManagerInfo: Added rdd_2_2 on disk on
192.168.56.1:56413 (size: 2.0 MB)
16/06/15 19:45:22 INFO MemoryStore: Block taskresult_8 stored as bytes in
memory (estimated size 2.0 MB, free 2.4 GB)
16/06/15 19:45:22 INFO BlockManagerInfo: Added taskresult_8 in memory on
192.168.56.1:56413 (size: 2.0 MB, free: 2.4 GB)
16/06/15 19:45:22 INFO Executor: Finished task 2.0 in stage 2.0 (TID 8).
2143771 bytes result sent via BlockManager)
16/06/15 19:45:43 ERROR RetryingBlockFetcher: Exception while beginning
fetch of 1 outstanding blocks
java.io.IOException: Failed to connect to /192.168.56.1:56413
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
at
org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
at
org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:105)
at
org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:92)
at
org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:546)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1793)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)
Caused by: java.net.ConnectException: Connection timed out: no further
information: /192.168.56.1:56413
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
at
io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
at
io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
at
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
... 1 more
16/06/15 19:45:43 INFO RetryingBlockFetcher: Retrying fetch (1/3) for 1
outstanding blocks after 5000 ms
16/06/15 19:46:04 ERROR RetryingBlockFetcher: Exception while beginning
fetch of 1 outstanding blocks
java.io.IOException: Failed to connect to /192.168.56.1:56413
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
at
org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
at
org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:105)
at
org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:92)
at
org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:546)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1793)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)
Caused by: java.net.ConnectException: Connection timed out: no further
information: /192.168.56.1:56413
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
at
io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
at
io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
at
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
... 1 more
16/06/15 19:46:04 INFO RetryingBlockFetcher: Retrying fetch (1/3) for 1
outstanding blocks after 5000 ms
16/06/15 19:46:25 ERROR RetryingBlockFetcher: Exception while beginning
fetch of 1 outstanding blocks
java.io.IOException: Failed to connect to /192.168.56.1:56413
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
at
org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.start(RetryingBlockFetcher.java:120)
at
org.apache.spark.network.netty.NettyBlockTransferService.fetchBlocks(NettyBlockTransferService.scala:105)
at
org.apache.spark.network.BlockTransferService.fetchBlockSync(BlockTransferService.scala:92)
at
org.apache.spark.storage.BlockManager.getRemoteBytes(BlockManager.scala:546)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply$mcV$sp(TaskResultGetter.scala:76)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2$$anonfun$run$1.apply(TaskResultGetter.scala:57)
at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1793)
at
org.apache.spark.scheduler.TaskResultGetter$$anon$2.run(TaskResultGetter.scala:56)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)
Caused by: java.net.ConnectException: Connection timed out: no further
information: /192.168.56.1:56413
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
at
io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
at
io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
at
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
... 1 more
16/06/15 19:46:25 INFO RetryingBlockFetcher: Retrying fetch (1/3) for 1
outstanding blocks after 5000 ms
16/06/15 19:46:46 ERROR RetryingBlockFetcher: Exception while beginning
fetch of 1 outstanding blocks (after 1 retries)
java.io.IOException: Failed to connect to /192.168.56.1:56413
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
at
org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
at java.util.concurrent.FutureTask.run(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)
Caused by: java.net.ConnectException: Connection timed out: no further
information: /192.168.56.1:56413
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
at
io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
at
io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
at
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
... 1 more
16/06/15 19:46:46 INFO RetryingBlockFetcher: Retrying fetch (2/3) for 1
outstanding blocks after 5000 ms
16/06/15 19:47:07 ERROR RetryingBlockFetcher: Exception while beginning
fetch of 1 outstanding blocks (after 1 retries)
java.io.IOException: Failed to connect to /192.168.56.1:56413
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:228)
at
org.apache.spark.network.client.TransportClientFactory.createClient(TransportClientFactory.java:179)
at
org.apache.spark.network.netty.NettyBlockTransferService$$anon$1.createAndStart(NettyBlockTransferService.scala:96)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.fetchAllOutstanding(RetryingBlockFetcher.java:140)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher.access$200(RetryingBlockFetcher.java:43)
at
org.apache.spark.network.shuffle.RetryingBlockFetcher$1.run(RetryingBlockFetcher.java:170)
at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
at java.util.concurrent.FutureTask.run(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
at java.lang.Thread.run(Unknown Source)
Caused by: java.net.ConnectException: Connection timed out: no further
information: /192.168.56.1:56413
at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
at sun.nio.ch.SocketChannelImpl.finishConnect(Unknown Source)
at
io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:224)
at
io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:289)
at
io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:528)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
at
io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
at
io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
... 1 more
16/06/15 19:47:07 INFO RetryingBlockFetcher: Retrying fetch (2/3) for 1
outstanding blocks after 5000 ms


Appreciate some help.

Regards
"
Reynold Xin <rxin@databricks.com>,"Wed, 15 Jun 2016 12:01:17 -0700",cutting 1.6.2 rc and 2.0.0 rc this week?,"""dev@spark.apache.org"" <dev@spark.apache.org>","It's been a while and we have accumulated quite a few bug fixes in
branch-1.6. I'm thinking about cutting 1.6.2 rc this week. Any patches
somebody want to get in last minute?

looked at the 60 unresolved tickets and almost all of them look like they
can be retargeted are are just some doc updates. I'm going to be more
aggressive and pushing individual people about resolving those, in case
this drags on forever.
"
Cody Koeninger <cody@koeninger.org>,"Wed, 15 Jun 2016 14:15:12 -0500",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,Reynold Xin <rxin@databricks.com>,"Any word on Kafka 0.10 support / SPARK-12177

I understand the hesitation, but is having nothing better than having
a standalone subproject marked as experimental?


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 15 Jun 2016 20:21:03 +0100",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,Reynold Xin <rxin@databricks.com>,"1.6.2 RC seems fine to me; I don't know of outstanding issues. Clearly
we need to keep the 1.x line going for a bit, so a bug fix release
sounds good,

Although we've got some work to do before 2.0.0 it does look like it's
within reach. Especially if declaring an RC creates more focus on
resolving the most important blocker issues -- and if we do burn those
down before 2.0.0 -- this sounds like a good step IMHO.


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Wed, 15 Jun 2016 12:23:19 -0700",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,Sean Owen <sowen@cloudera.com>,"+1 to both of these!


"
Reynold Xin <rxin@databricks.com>,"Wed, 15 Jun 2016 13:57:09 -0700",Re: Hive-on-Spark with access to Hive from Spark jobs,Rostyslav Sotnychenko <r.sotnychenko@gmail.com>,"Are you running Spark on YARN, Mesos, Standalone? For all of them you can
make the Hive dependency just part of your application, and then you can
manage this pretty easily.



"
Mingyu Kim <mkim@palantir.com>,"Wed, 15 Jun 2016 21:23:33 +0000",Re: Utilizing YARN AM RPC port field,Steve Loughran <stevel@hortonworks.com>,"FYI, I just filed https://issues.apache.org/jira/browse/SPARK-15974.

 

Mingyu

 

From: Mingyu Kim <mkim@palantir.com>
Date: Tuesday, June 14, 2016 at 2:13 PM
To: Steve Loughran <stevel@hortonworks.com>
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>, Matt Cheah <mcheah@palantir.com>
Subject: Re: Utilizing YARN AM RPC port field

 

Thanks for the pointers, Steve!

 

The first option sounds like a the most light-weight and non-disruptive option among them. So, we can add a configuration that enables socket initialization, Spark AM will create a ServerSocket if the socket init is enabled and set it on SparkContext

 

If there are no objections, I can file a bug and find time to tackle it myself. 

 

Mingyu

 

From: Steve Loughran <stevel@hortonworks.com>
Date: Tuesday, June 14, 2016 at 4:55 AM
To: Mingyu Kim <mkim@palantir.com>
Cc: ""dev@spark.apache.org"" <dev@spark.apache.org>, Matt Cheah <mcheah@palantir.com>
Subject: Re: Utilizing YARN AM RPC port field

 

 


 

Hi all,

 

YARN provides a way for AppilcationMaster to register a RPC port so that a client outside the YARN cluster can reach the application for any RPCs, but Spark’s YARN AMs simply register a dummy port number of 0. (See https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnRMClient.scala#L74) This is useful for the long-running Spark application usecases where jobs are submitted via a form of RPC to an already started Spark context running in YARN cluster mode. Spark job server (https://github.com/spark-jobserver/spark-jobserver) and Livy (https://github.com/cloudera/hue/tree/master/apps/spark/java) are good open-source examples of these usecases. The current work-around is to have the Spark AM make a call back to a configured URL with the port number of the RPC server for the client to communicate with the AM.

 

Utilizing YARN AM RPC port allows the port number reporting to be done in a secure way (i.e. With AM RPC port field and Kerberized YARN cluster, you don’t need to re-invent a way to verify the authenticity of the port number reporting.) and removes the callback from YARN cluster back to a client, which means you can operate YARN in a low-trust environment and run other client applications behind a firewall.

 

A couple of proposals for utilizing YARN AM RPC port I have are, (Note that you cannot simply pre-configure the port number and pass it to Spark AM via configuration because of potential port conflicts on the YARN node)

 

·         Start-up an empty Jetty server during Spark AM initialization, set the port number when registering AM with RM, and pass a reference to the Jetty server into the Spark application (e.g. through SparkContext) for the application to dynamically add servlet/resources to the Jetty server.

·         Have an optional static method in the main class (e.g. initializeRpcPort()) which optionally sets up a RPC server and returns the RPC port. Spark AM can call this method, register the port number to RM and continue on with invoking the main method. I don’t see this making a good API, though.

 

I’m curious to hear what other people think. Would this be useful for anyone? What do you think about the proposals? Please feel free to suggest other ideas. Thanks!

 

 

It's a recurrent irritation of mine that you can't ever change the HTTP/RPC ports of a YARN AM after launch; it creates a complex startup state where you can't register until your IPC endpoints are up.

 

Tactics

 

-Create a socket on an empty port, register it, hand off the port to the RPC setup code as the chosen port. Ideally, support a range to scan, so that systems which only open a specific range of ports, e.g. 6500-6800 can have those ports only scanned. We've done this in other projects.

 

-serve up the port binding info via a REST API off the AM web; clients hit the (HEAD/GET only RM Proxy), ask for the port, work on it. Nonstandard; could be extensible with other binding information. (TTL of port caching, ....)

 

-Use the YARN-913 ZK based registry to register/lookup bindings. This is used in various YARN apps to register service endpoints (RPC, Rest); there's work ongoing for DNS support. this would allow you to use DNS against a specific DNS server to get the endpoints. Works really well with containerized deployments where the apps come up with per-container IPaddresses and fixed ports.

Although you couldn't get the latter into the spark-yarn codeitself (needs Hadoop 2.6+), you can plug in support via the extension point implemented in SPARK-11314., I've actually thought of doing that for a while...just been too busy.

 

-Just fix the bit of the YARN api that forces you to know your endpoints in advance. People will appreciate it, though it will take a while to trickle downstream.

 

 

 

 

"
Shankar Venkataraman <shankarvenkataraman666@gmail.com>,"Wed, 15 Jun 2016 21:56:54 +0000",Yarn Log aggregation for a killed spark streaming job,dev <dev@spark.apache.org>,"Hi!

We are seeing an issue around log aggregation under Yarn with Spark
streaming. The specific case below is a example - we had to kill a spark
streaming job, and would like to see the logs of the consumer so as to find
out what happened before we had to kill it.
Yarn reports the status of a killed Spark streaming job with a ""log
aggregation status"" of N/A. Yarn seems to be doing the right thing for all
other jobs with respect to log aggregation - jobs that  either aborted or
were terminated normally after finish.

Any clues on what may be happening. We are using Spark 1.5.2. Is there a
fix for such behavior in later releases?

$ yarn application -status application_1460521878257_8455

16/06/14 19:24:54 INFO impl.TimelineClientImpl: Timeline service

address: http://abhdp-rm1.marketo.org:8188/ws/v1/timeline/

Application Report :

Application-Id : application_1460521878257_8455

Application-Name : ab-crmstreaming-service

Application-Type : SPARK

User : crmintegration

Queue : crm

Start-Time : 1463694307675

Finish-Time : 1464848682220

Progress : 0%

State : KILLED

Final-State : KILLED

Tracking-URL : N/A

RPC Port : -1

AM Host : N/A

Aggregate Resource Allocation : 0 MB-seconds, 0 vcore-seconds

Log Aggregation Status : N/A

Diagnostics : N/A
"
Raymond Honderdors <Raymond.Honderdors@sizmek.com>,"Thu, 16 Jun 2016 03:05:59 +0000",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,"Michael Armbrust <michael@databricks.com>, Sean Owen <sowen@cloudera.com>","+1 for both

Get Outlook for Android<https://aka.ms/ghei36>




+1 to both of these!

1.6.2 RC seems fine to me; I don't know of outstanding issues. Clearly
we need to keep the 1.x line going for a bit, so a bug fix release
sounds good,

Although we've go"
andy petrella <andy.petrella@gmail.com>,"Thu, 16 Jun 2016 06:02:28 +0000",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,"Raymond Honderdors <Raymond.Honderdors@sizmek.com>, 
	Michael Armbrust <michael@databricks.com>, Sean Owen <sowen@cloudera.com>","+1 both too
(for tomorrow lunchtime? ^^)


andy
"
Amit Sela <amitsela33@gmail.com>,"Thu, 16 Jun 2016 06:11:13 +0000",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,"andy petrella <andy.petrella@gmail.com>, 
	Raymond Honderdors <Raymond.Honderdors@sizmek.com>, Michael Armbrust <michael@databricks.com>, 
	Sean Owen <sowen@cloudera.com>","Should we backport https://github.com/apache/spark/pull/13424 to 1.6.2 ?


"
Avshalom <avshalomman@gmail.com>,"Thu, 16 Jun 2016 01:57:42 -0700 (MST)",Spark SQL Count Distinct,dev@spark.apache.org,"Hi all,

We would like to perform a count distinct query based on a certain filter.
e.g. our data is of the form:

userId, Name, Restaurant name, Restaurant Type
===============================
100,    John,     Pizza Hut,            Pizza
100,    John,     Del Pepe,             Pasta
100,    John,     Hagen Daz,          Ice Cream
100,    John,     Dominos,             Pasta
200,    Mandy,  Del Pepe,             Pasta

And we would like to know the number of distinct Pizza eaters.

The issue is, we have roughly ~200 million entries, so even with a large
cluster, we could still be in a risk of memory overload if the distinct
implementation has to load all of the data into RAM.
The Spark Core implementation which uses reduce to 1 and sum doesn't have
this risk.

I've found this old thread which compares Spark Core and Spark SQL count
distinct performance:

http://apache-spark-developers-list.1001551.n3.nabble.com/DataFrame-distinct-vs-RDD-distinct-td12098.html
<http://apache-spark-developers-list.1001551.n3.nabble.com/DataFrame-distinct-vs-RDD-distinct-td12098.html>  

distinct implementation is not based on a Hash Set anymore, but we're still
concerned that it won't be as safe as the Spark Core implementation.
We don't mind waiting a long time for the computation to end, but we don't
want to reach out of memory errors.

Would highly appreciate any input.

Thanks
Avshalom



--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Thu, 16 Jun 2016 13:17:37 +0200",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,Reynold Xin <rxin@databricks.com>,"That's be awesome to have another 2.0 RC! I know many people who'd
consider it as a call to action to play with 2.0.

+1000

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Thu, 16 Jun 2016 12:48:52 +0000 (UTC)",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,"Reynold Xin <rxin@databricks.com>, 
	""dev@spark.apache.org"" <dev@spark.apache.org>","+1
Tom 

 

 It's been a while and we have accumulated quite a few bug fixes in branch-1.6. I'm thinking about cutting 1.6.2 rc this week. Any patches somebody want to get in last minute?





   "
Richard Marscher <rmarscher@localytics.com>,"Thu, 16 Jun 2016 11:37:40 -0400",Encoder Guide / Option[T] Encoder,Dev <dev@spark.apache.org>,"Are there any user or dev guides for writing Encoders? I'm trying to read
through the source code to figure out how to write a proper Option[T]
encoder, but it's not straightforward without deep spark-sql source
knowledge. Is it unexpected for users to need to write their own Encoders
with the availability of ExpressionEncoder.apply and the bean encoder
method?

As additional color for the Option[T] encoder, I have tried using the
ExpressionEncoder but it does not treat nulls properly and passes them
through. I'm not sure if this is a side-effect of
https://github.com/apache/spark/pull/13425 where a beneficial change was
made to have missing parts of joins continue through as nulls instead of
the default value for the data type (like -1 for ints). But my thought is
that would then also apply for the generic Option encoder generated as it
would see a null column value and skip passing it into the Option.apply

-- 
*Richard Marscher*
Senior Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
mylisttech@gmail.com,"Thu, 16 Jun 2016 22:00:01 +0530",Hello ,SparkDev <dev@spark.apache.org>,"Dear All,


Looking for guidance.

I am Interested in contributing to the Spark MLlib. Could you please take a few minutes to guide me as to what you would consider an ideal path / skill an individual should posses.

I know R / Python / Java / C and C++

I have a firm understanding of algorithms and Machine learning. I do know spark at a ""workable knowledge level"".

Where should I start and what should I try to do first  ( spark internal level ) and then pick up items on JIRA OR new specifications on Spark.

R has a great set of packages - would it be difficult to migrate them to Spark R set. I could try it with your support or if it's desired. 


I wouldn't mind doing testing of some defects etc as an initial learning curve if that would assist the community.

Please, guide.

Regards,
Harmeet



---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 16 Jun 2016 10:39:43 -0700",Re: Spark SQL Count Distinct,Avshalom <avshalomman@gmail.com>,"You should be fine in 1.6 onward. Count distinct doesn't require data to
fit in memory there.



"
Cody Koeninger <cody@koeninger.org>,"Thu, 16 Jun 2016 12:53:47 -0500",Structured streaming use of DataFrame vs Datasource,"""dev@spark.apache.org"" <dev@spark.apache.org>","Is there a principled reason why sql.streaming.* and
sql.execution.streaming.* are making extensive use of DataFrame
instead of Datasource?

Or is that just a holdover from code written before the move / type alias?

---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Thu, 16 Jun 2016 13:18:34 -0500",Re: Structured streaming use of DataFrame vs Datasource,"""dev@spark.apache.org"" <dev@spark.apache.org>","Sorry, meant DataFrame vs Dataset


---------------------------------------------------------------------


"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 16 Jun 2016 11:42:51 -0700",Re: Structured streaming use of DataFrame vs Datasource,Cody Koeninger <cody@koeninger.org>,"DataFrame is a type alias of Dataset[Row], so externally it seems like
Dataset is the main type and DataFrame is a derivative type.
However, internally, since everything is processed as Rows, everything uses
DataFrames, Type classes used in a Dataset is internally converted to rows
for processing. . Therefore internally DataFrame is like ""main"" type that
is used.


"
Ovidiu-Cristian MARCU <ovidiu-cristian.marcu@inria.fr>,"Thu, 16 Jun 2016 20:44:41 +0200",DMTCP and debug a failed stage in spark,dev <dev@spark.apache.org>,"Hi,

I have a TPCDS query that fails in the stage 80 which is a ResultStage (SparkSQL).
Ideally I would like to ‘checkpoint’ a previous stage which was executed successfully and replay the failed stage for debug purposes.
Anyone managed to do something similar that could point some hints?
Maybe someone used a tool like DMTCP [1] and it can be applied in this situation?

[1] http://dmtcp.sourceforge.net/ <http://dmtcp.sourceforge.net/>

Best,
Ovidiu
  "
Michael Armbrust <michael@databricks.com>,"Thu, 16 Jun 2016 12:03:51 -0700",Re: Encoder Guide / Option[T] Encoder,Richard Marscher <rmarscher@localytics.com>,"There is no public API for writing encoders at the moment, though we are
hoping to open this up in Spark 2.1.

What is not working about encoders for options?  Which version of Spark are
you running?  This is working as I would expect?

https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/1023043053387187/1073771007111588/2840265927289860/latest.html


"
Cody Koeninger <cody@koeninger.org>,"Thu, 16 Jun 2016 14:38:29 -0500",Re: Structured streaming use of DataFrame vs Datasource,Tathagata Das <tathagata.das1565@gmail.com>,"Is this really an internal / external distinction?

For a concrete example, Source.getBatch seems to be a public
interface, but returns DataFrame.


---------------------------------------------------------------------


"
Richard Marscher <rmarscher@localytics.com>,"Thu, 16 Jun 2016 15:39:08 -0400",Re: Encoder Guide / Option[T] Encoder,Michael Armbrust <michael@databricks.com>,"Yea WRT Options maybe I'm thinking about it incorrectly or misrepresenting
it as relating to Encoders or to pure Option encoder. The semantics I'm
thinking of are around the deserialization of a type T and lifting it into
Option[T] via the Option.apply function which converts null to None. Tying
back into joinWith where you get nulls on outer-joins. It's possible to map
after joinWith and use the Option.apply in the map function, but there was
a thought it might be able to be done at deserialization. Maybe that's not
possible.




-- 
*Richard Marscher*
Senior Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 16 Jun 2016 12:50:49 -0700",Re: Structured streaming use of DataFrame vs Datasource,Cody Koeninger <cody@koeninger.org>,"There are different ways to view this. If its confusing to think that
Source API returning DataFrames, its equivalent to thinking that you are
returning a Dataset[Row], and DataFrame is just a shorthand.
And DataFrame/Datasetp[Row] is to Dataset[String] is what java
Array[Object] is to Array[String]. DataFrame is more general in a way, as
every Dataset can be boiled down to a DataFrame. So to keep the Source APIs
general (and also source-compatible with 1.x), they return DataFrame.


"
Cody Koeninger <cody@koeninger.org>,"Thu, 16 Jun 2016 16:05:33 -0500",Re: Structured streaming use of DataFrame vs Datasource,Tathagata Das <tathagata.das1565@gmail.com>,"I'm clear on what a type alias is.  My question is more that moving
from e.g. Dataset[T] to Dataset[Row] involves throwing away
information.  Reading through code that uses the Dataframe alias, it's
a little hard for me to know when that's intentional or not.



---------------------------------------------------------------------


"
Tathagata Das <tathagata.das1565@gmail.com>,"Thu, 16 Jun 2016 14:22:56 -0700",Re: Structured streaming use of DataFrame vs Datasource,Cody Koeninger <cody@koeninger.org>,"Its not throwing away any information from the point of view of the SQL
optimizer. The schema preserves all the type information that the catalyst
uses. The type information T in Dataset[T] is only used at the API level to
ensure compilation-time type checks of the user program.


"
Luyi Wang <wangluyi1982@gmail.com>,"Thu, 16 Jun 2016 16:35:48 -0700",Regarding on the dataframe stat frequent,dev@spark.apache.org,"Hey there:

The frequent item in dataframe stat package seems not accurate. In the
documentation,it did mention that it has false positive but still seems
incorrect.

Wondering if this is all known problem or not?


Here is a quick example showing the problem.

val sqlContext = new SQLContext(sc)
import sqlContext.implicits._

val rows = Seq((0,""a""),(1, ""c""),(2, ""a""),(3, ""a""),(4, ""b""),(5,
""d"")).toDF(""id"", ""category"")
val history = rows.toDF(""id"", ""category"")

history.stat.freqItems(Array(""category""),0.5).show
history.stat.freqItems(Array(""category""),0.3).show
history.stat.freqItems(Array(""category""),0.51).show
history.stat.freqItems(Array(""category"")).show


Here is the output

+------------------+
|category_freqItems|
+------------------+
|            [d, a]|
+------------------+

+------------------+
|category_freqItems|
+------------------+
|               [a]|
+------------------+

+------------------+
|category_freqItems|
+------------------+
|                []|
+------------------+

+------------------+
|category_freqItems|
+------------------+
|      [b, d, a, c]|
+------------------+



The problem results from the freqItemCounter class's add function which is
used in the function singlePassFreqItems aggregation stage.

Regarding on the paper, the return size of the frequent set can't be larger
than 1/minimum_support,which we indicated as k hereby, so that  in
singlePassFreqItems
the counterMap is created with this size.

The logic of the add function is following:

To add up the counter of a item, when it already exists in the map,  the
counter is added up.If it doesn't exist and also map size less than k, it
inserts.  if it doesn't exist and also current size just equal to size k,
then it will compare the inserted count with the minimum value. if the
counter of the to be inserted item is larger than or equals to the current
minimum, item is inserted and all items with counter value larger than
current minimum would and smaller and equals to will be removed.  If
counter of the to be inserted item is smaller than the current minimum,
item won't be inserted and counters of all items in the map will be deduct
the inserted counter value.

Problem:

Since it would retain the items larger than the current minimum,  if the
current minimum is just happened to be the count of second most frequent
item. it would be removed if the to be inserted item has the same count. In
this case, possibly a smaller one would be inserted in the map afterward
and returned later.

Given one example here. ""a"" appears 3 times, ""b"" and ""c"" both appears 2
times, ""d"" appears only once, total 8 times, For minimum support 0.5, the
map is initiated with size 2.   The correct answer should return items
appears more than 4 times, which is empty. However it returns ""a"" and ""d"".
The reason it returned two items is because of map size. The reason ""d"" is
returned is because that ""b"" and ""c"" appear the same amount and more than
""d"", but they are cleaned when either one of them already inserted and the
map reach the size limitation. and when ""d"" is to be inserted, size is
smaller and it is inserted.


val rows = Seq((0,""a""),(1, ""b""),(2, ""a""),(3, ""a""),(4, ""b""),(5,
""c""),(6,""c""),(7,""d"")).toDF(""id"", ""category"")
val history = rows.toDF(""id"", ""category"")

history.stat.freqItems(Array(""category""),0.5).show
history.stat.freqItems(Array(""category""),0.3).show
history.stat.freqItems(Array(""category""),0.51).show
history.stat.freqItems(Array(""category"")).show


+------------------+
|category_freqItems|
+------------------+
|            [d, a]|
+------------------+

+------------------+
|category_freqItems|
+------------------+
|         [b, a, c]|
+------------------+

+------------------+
|category_freqItems|
+------------------+
|                []|
+------------------+

+------------------+
|category_freqItems|
+------------------+
|      [b, d, a, c]|
+------------------+


Hope this explains the problem.

Thanks.

-Luyi.
"
Prajwal Tuladhar <praj@infynyxx.com>,"Fri, 17 Jun 2016 00:39:36 +0000",Spark internal Logging trait potential thread unsafe,dev@spark.apache.org,"Hi,

The way log instance inside Logger trait is current being initialized
doesn't seem to be thread safe [1]. Current implementation only guarantees
initializeLogIfNecessary() is initialized in lazy + thread safe way.

Is there a reason why it can't be just: [2]

@transient private lazy val log_ : Logger = {
    initializeLogIfNecessary(false)
    LoggerFactory.getLogger(logName)
  }


And with that initializeLogIfNecessary() can be called without double
locking.

-- 
--
Cheers,
Praj

[1]
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/Logging.scala#L44-L50
[2]
https://github.com/apache/spark/blob/8ef3399aff04bf8b7ab294c0f55bcf195995842b/core/src/main/scala/org/apache/spark/internal/Logging.scala#L35
"
Reynold Xin <rxin@apache.org>,"Thu, 16 Jun 2016 21:49:04 -0700",[VOTE] Release Apache Spark 1.6.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.2!

The vote is open until Sunday, June 19, 2016 at 22:00 PDT and passes if a
majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.2
[ ] -1 Do not release this package because ...


The tag to be voted on is v1.6.2-rc1
(4168d9c94a9564f6b3e62f5d669acde13a7c7cf6)

The release files, including signatures, digests, etc. can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.2-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1184

The documentation corresponding to this release can be found at:
https://home.apache.org/~pwendell/spark-releases/spark-1.6.2-rc1-docs/


=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.1.

================================================
== What justifies a -1 vote for this release? ==
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.1, missing features, or bugs related to new features will not
necessarily block this release.
"
Bhupendra Mishra <bhupendra.mishra@gmail.com>,"Fri, 17 Jun 2016 12:53:09 +0530",Re: ImportError: No module named numpy,Daniel Rodriguez <df.rodriguez143@gmail.com>,"Issue has been fixed after lots of R&D around finally found preety simple
things causing this problem

It was related to permission issue on the python libraries. The user I am
logged in was not having enough permission to read/execute the following
python liabraries.

 /usr/lib/python2.7/site-packages/
/usr/lib64/python2.7/

so above path should have read/execute permission to user executing
python/pyspark program.

Thanks everyone for your help with same. Appreciate!
Regards


m

er-with-anaconda-and-cdh/
en
da
:
g>
y
iables
alling-and-configuring-pyspark
u
e.org>
b/__init__.py"",
gfus,
ekten.
-----------------------------------------------------------------------
029ebe63&id=ff955ef848&e=b789cc1a5f>
29ebe63&id=998e8f655c&e=b789cc1a5f>
29ebe63&id=7ae7d93d42&e=b789cc1a5f>[image:
029ebe63&id=f8c66fb950&e=b789cc1a5f>
"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Jun 2016 00:30:59 -0700",testing the kafka 0.10 connector,"""dev@spark.apache.org"" <dev@spark.apache.org>","Cody has graciously worked on a new connector for dstream for Kafka 0.10.
Can people that use Kafka test this connector out? The patch is at
https://github.com/apache/spark/pull/11863

Although we have stopped merging new features into branch-2.0, this
connector is very decoupled from rest of Spark and we might be able to put
this into 2.0.1 (or 2.0.0 if everything works out).

Thanks.
"
Sean Owen <sowen@cloudera.com>,"Fri, 17 Jun 2016 11:19:59 +0200",Re: Spark internal Logging trait potential thread unsafe,Prajwal Tuladhar <praj@infynyxx.com>,"I think that's OK to change, yes. I don't see why it's necessary to
init log_ the way it is now. initializeLogIfNecessary() has a purpose
though.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Fri, 17 Jun 2016 11:22:10 +0200",Re: Regarding on the dataframe stat frequent,Luyi Wang <wangluyi1982@gmail.com>,"If you have a clean test case demonstrating the desired behavior, and
a change which makes it work that way, yes make a JIRA and PR.


---------------------------------------------------------------------


"
Xinh Huynh <xinh.huynh@gmail.com>,"Fri, 17 Jun 2016 09:46:13 -0700",Re: Hello,mylisttech@gmail.com,"Here are some guidelines about contributing to Spark:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark

There is also a section specific to MLlib:

https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-MLlib-specificContributionGuidelines

-Xinh


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Fri, 17 Jun 2016 17:55:09 +0000",Re: [VOTE] Release Apache Spark 1.6.2 (RC1),"Reynold Xin <rxin@apache.org>, ""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 17 Jun 2016 10:58:09 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","-1 (non-binding)

SPARK-16017 shows a severe perf regression in YARN compared to 1.6.1.




-- 
Marcelo

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 17 Jun 2016 11:04:39 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC1),,"Docker Integration Tests failed on Linux:

http://pastebin.com/Ut51aRV3

Here was the command I used:

mvn clean -Phive -Phive-thriftserver -Pyarn -Phadoop-2.6 -Psparkr
-Dhadoop.version=2.7.0 package

Has anyone seen similar error ?

Thanks


"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Fri, 17 Jun 2016 16:22:22 -0600",Re: Hello,Xinh Huynh <xinh.huynh@gmail.com>,"What is the best way to determine what the library maintainers believe is
important work to be done?

I have looked through the JIRA and its unclear what are priority items one
could do work on. I am guessing this is in part because things are a little
hectic with final work for 2.0, but it would be helpful to know what to
look for or if its better to ask library maintainers directly.

Thanks,
Pedro Rodriguez




-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 909-353-4423
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
Ted Yu <yuzhihong@gmail.com>,"Fri, 17 Jun 2016 15:28:56 -0700",Re: Hello,Pedro Rodriguez <ski.rodriguez@gmail.com>,"You can use a JIRA filter to find JIRAs of the component(s) you're
interested in.
Then sort by Priority.

Maybe comment on the JIRA if you want to work on it.


"
Michael Armbrust <michael@databricks.com>,"Fri, 17 Jun 2016 15:32:29 -0700",Re: Hello,Pedro Rodriguez <ski.rodriguez@gmail.com>,"Another good signal is the ""target version"" (which by convention is only
set by committers).  When I set this for the upcoming version it means I
think its important enough that I will prioritize reviewing a patch for it.


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Sat, 18 Jun 2016 01:36:54 +0000",Spark 2.0 on YARN - Files in config archive not ending up on executor classpath,"user <user@spark.apache.org>, Spark dev list <dev@spark.apache.org>","I'm trying to debug a problem in Spark 2.0.0-SNAPSHOT
(commit bdf5fe4143e5a1a393d97d0030e76d35791ee248) where Spark's
log4j.properties is not getting picked up in the executor classpath (and
driver classpath for yarn-cluster mode), so Hadoop's log4j.properties file
is taking precedence in the YARN containers.

Spark's log4j.properties file is correctly being bundled into the
__spark_conf__.zip file and getting added to the DistributedCache, but it
is not in the classpath of the executor, as evidenced by the following
command, which I ran in spark-shell:

scala> sc.parallelize(Seq(1)).map(_ =>
getClass().getResource(""/log4j.properties"")).first
res3: java.net.URL = file:/etc/hadoop/conf.empty/log4j.properties

I then ran the following in spark-shell to verify the classpath of the
executors:

scala> sc.parallelize(Seq(1)).map(_ =>
System.getProperty(""java.class.path"")).flatMap(_.split(':')).filter(e =>
!e.endsWith("".jar"") && !e.endsWith(""*"")).collect.foreach(println)
...
/mnt/yarn/usercache/hadoop/appcache/application_1466208403287_0003/container_1466208403287_0003_01_000003
/mnt/yarn/usercache/hadoop/appcache/application_1466208403287_0003/container_1466208403287_0003_01_000003/__spark_conf__
/etc/hadoop/conf
...

So the JVM has this nonexistent __spark_conf__ directory in the classpath
when it should really be __spark_conf__.zip (which is actually a symlink to
a directory, despite the .zip filename).

% sudo ls -l
/mnt/yarn/usercache/hadoop/appcache/application_1466208403287_0003/container_1466208403287_0003_01_000003
total 20
-rw-r--r-- 1 yarn yarn   88 Jun 18 01:26 container_tokens
-rwx------ 1 yarn yarn  594 Jun 18 01:26
default_container_executor_session.sh
-rwx------ 1 yarn yarn  648 Jun 18 01:26 default_container_executor.sh
-rwx------ 1 yarn yarn 4419 Jun 18 01:26 launch_container.sh
lrwxrwxrwx 1 yarn yarn   59 Jun 18 01:26 __spark_conf__.zip ->
/mnt1/yarn/usercache/hadoop/filecache/17/__spark_conf__.zip
lrwxrwxrwx 1 yarn yarn   77 Jun 18 01:26 __spark_libs__ ->
/mnt/yarn/usercache/hadoop/filecache/16/__spark_libs__4490748779530764463.zip
drwx--x--- 2 yarn yarn   46 Jun 18 01:26 tmp

Does anybody know why this is happening? Is this a bug in Spark, or is it
the JVM doing this (possibly because the extension is .zip)?

Thanks,
Jonathan
"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Fri, 17 Jun 2016 22:13:42 -0600",Spark 2.0 Dataset Documentation,dev <dev@spark.apache.org>,"Hi All,

At my workplace we are starting to use Datasets in 1.6.1 and even more with
Spark 2.0 in place of Dataframes. I looked at the 1.6.1 documentation then
the 2.0 documentation and it looks like not much time has been spent
writing a Dataset guide/tutorial.

Preview Docs:
https://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/sql-programming-guide.html#creating-datasets
Spark master docs:
https://github.com/apache/spark/blob/master/docs/sql-programming-guide.md

I would like to spend the time to contribute an improvement to those docs
with a more in depth examples of creating and using Datasets (eg using $ to
select columns). Is this of value, and if so what should my next step be to
get this going (create JIRA etc)?

-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
R&D Data Science Intern at Oracle Data Cloud
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 909-353-4423
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Fri, 17 Jun 2016 22:32:49 -0600",Re: Skew data,Selvam Raman <selmna@gmail.com>,"I am going to take a guess that this means that your partitions within an
RDD are not balanced (one or more partitions are much larger than the
rest). This would mean a single core would need to do much more work than
the rest leading to poor performance. In general, the way to fix this is to
spread data across partitions evenly. In most cases calling repartition is
enough to solve the problem. If you have a special case you might need
create your own custom partitioner.

Pedro





-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 909-353-4423
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
"""Kazuaki Ishizaki"" <ISHIZAKI@jp.ibm.com>","Sat, 18 Jun 2016 13:43:48 +0900",Question about equality of o.a.s.sql.Row,dev@spark.apache.org,"Dear all,

I have three questions about equality of org.apache.spark.sql.Row.

(1) If a Row has a complex type (e.g. Array), is the following behavior 
expected?
If two Rows has the same array instance, Row.equals returns true in the 
second assert. If two Rows has different array instances (a1 and a2) that 
have the same array elements, Row.equals returns false in the third 
assert.

val a1 = Array(3, 4)
val a2 = Array(3, 4)
val r1 = Row(a1)
val r2 = Row(a2)
assert(a1.sameElements(a2)) // SUCCESS
assert(Row(a1).equals(Row(a1)))  // SUCCESS
assert(Row(a1).equals(Row(a2)))  // FAILURE

This is because two objects are compared by ""o1 != o2"" instead of 
""o1.equals(o2)"" at 
https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/Row.scala#L408

(2) If (1) is expected, where is this behavior is described or defined? I 
cannot find the description in the API document.
https://spark.apache.org/docs/1.6.1/api/java/org/apache/spark/sql/Row.html
https://home.apache.org/~pwendell/spark-releases/spark-2.0.0-preview-docs/api/scala/index.html#org.apache.spark.sql.Row

(3) If (3) is expected, is there any recommendation to write code of 
equality of two Rows that have an Array or complex types (e.g. Map)?

Best Regards,
Kazuaki Ishizaki, @kiszk

"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 17 Jun 2016 22:12:06 -0700",Re: Spark 2.0 Dataset Documentation,"Pedro Rodriguez <ski.rodriguez@gmail.com>, dev <dev@spark.apache.org>","Hey Pedro,

SQL programming guide is being updated. Here's the PR, but not merged 
yet: https://github.com/apache/spark/pull/13592

Cheng


"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Fri, 17 Jun 2016 23:28:49 -0600",Re: Spark 2.0 Dataset Documentation,Cheng Lian <lian.cs.zju@gmail.com>,"The updates look great!

Looks like many places are updated to the new APIs, but there still isn't a
section for working with Datasets (most of the docs work with Dataframes).
Are you planning on adding more? I am thinking something that would address
common questions like the one I posted on the user email list earlier today.

Should I take discussion to your PR?

Pedro




-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 909-353-4423
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
Cheng Lian <lian.cs.zju@gmail.com>,"Fri, 17 Jun 2016 22:44:58 -0700",Re: Spark 2.0 Dataset Documentation,Pedro Rodriguez <ski.rodriguez@gmail.com>,"As mentioned in the PR description, this is just an initial PR to bring 
existing contents up to date, so that people can add more contents 
incrementally.

We should definitely cover more about Dataset.


Cheng



"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Fri, 17 Jun 2016 23:58:51 -0600",Re: Spark 2.0 Dataset Documentation,Cheng Lian <lian.cs.zju@gmail.com>,"I would be open to working on Dataset documentation if no one else isn't
already working on it. Thoughts?




-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 909-353-4423
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
Reynold Xin <rxin@databricks.com>,"Fri, 17 Jun 2016 23:07:00 -0700",Re: Spark 2.0 Dataset Documentation,Pedro Rodriguez <ski.rodriguez@gmail.com>,"Please go for it!


"
Reynold Xin <rxin@databricks.com>,"Sat, 18 Jun 2016 00:13:44 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC1),Marcelo Vanzin <vanzin@cloudera.com>,"Looks like that's resolved now.

I will wait till Sunday to cut rc2 to give people more time to find issues
with rc1.



"
Jacek Laskowski <jacek@japila.pl>,"Sat, 18 Jun 2016 14:03:50 +0200",Re: Spark 2.0 Dataset Documentation,Pedro Rodriguez <ski.rodriguez@gmail.com>,"

Or even my favourite one - the tick ` :-)

Jacek

---------------------------------------------------------------------


"
Pedro Rodriguez <ski.rodriguez@gmail.com>,"Sat, 18 Jun 2016 07:42:31 -0600",Re: Spark 2.0 Dataset Documentation,Jacek Laskowski <jacek@japila.pl>,"Going to go ahead and starting working on the docs assuming this gets
merged https://github.com/apache/spark/pull/13592. Opened a JIRA
https://issues.apache.org/jira/browse/SPARK-16046

Having some issues building docs. The Java docs fail to build. Output when
it fails is here:
https://gist.github.com/EntilZha/9c585662ef7cda820c311d1c7eb16e42

This might be causing an issue where loading the API docs fails due to some
javascript errors (doesn't seem to switch page correctly). The main one
repeated several times is: main.js:2 Uncaught SyntaxError: Unexpected token
<

Pedro





-- 
Pedro Rodriguez
PhD Student in Distributed Machine Learning | CU Boulder
UC Berkeley AMPLab Alumni

ski.rodriguez@gmail.com | pedrorodriguez.io | 909-353-4423
Github: github.com/EntilZha | LinkedIn:
https://www.linkedin.com/in/pedrorodriguezscience
"
Jacek Laskowski <jacek@japila.pl>,"Sat, 18 Jun 2016 16:10:22 +0200",Re: [VOTE] Release Apache Spark 1.6.2 (RC1),Reynold Xin <rxin@databricks.com>,"+1

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------

"
Yash Sharma <yash360@gmail.com>,"Sun, 19 Jun 2016 01:57:00 +1000",Does dataframe write append mode work with text format,dev@spark.apache.org,"Hi All,
I have been using the parquet append mode for write which works just
fine. Just wanted to check if the same is supported for plain text format.
The below code blows up with error saying the file already exists.



{code}
userEventsDF.write.mode(""append"").partitionBy(""year"", ""month"",
""date"").text(outputDir)
or,
userEventsDF.write.mode(""append"").partitionBy(""year"", ""month"",
""date"").format(""text"").save(outputDir)
{code}
"
Xiao Li <gatorsmile@gmail.com>,"Sat, 18 Jun 2016 18:32:38 -0700",Re: Does dataframe write append mode work with text format,Yash Sharma <yash360@gmail.com>,"Hi, Yash,

It should work.

 val df = spark.range(1, 5)
   .select('id + 1 as 'p1, 'id + 2 as 'p2, 'id + 3 as 'p3, 'id + 4 as 'p4,
'id + 5 as 'p5, 'id as 'b)
   .selectExpr(""p1"", ""p2"", ""p3"", ""p4"", ""p5"", ""CAST(b AS STRING) AS
s"").coalesce(1)

 df.write.partitionBy(""p1"", ""p2"", ""p3"", ""p4"",
""p5"").text(dir.getCanonicalPath)
 val newDF = spark.read.text(dir.getCanonicalPath)
 newDF.show()

 df.write.partitionBy(""p1"", ""p2"", ""p3"", ""p4"", ""p5"")
   .mode(SaveMode.Append).text(dir.getCanonicalPath)
 val newDF2 = spark.read.text(dir.getCanonicalPath)
 newDF2.show()

I tried it. It works well.

Thanks,

Xiao Li

2016-06-18 8:57 GMT-07:00 Yash Sharma <yash360@gmail.com>:

"
Yash Sharma <yash360@gmail.com>,"Sun, 19 Jun 2016 11:36:08 +1000",Re: Does dataframe write append mode work with text format,Xiao Li <gatorsmile@gmail.com>,"Awesome!! will give it a try again.  Thanks!!

- Thanks, via mobile,  excuse brevity.

"
Krishna Sankar <ksankar42@gmail.com>,"Sat, 18 Jun 2016 19:20:19 -0700",Thanks For a Job Well Done !!!,"""dev@spark.apache.org"" <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi all,
   Just wanted to thank all for the dataset API - most of the times we see
only bugs in these lists ;o).

   - Putting some context, this weekend I was updating the SQL chapters of
   my book - it had all the ugliness of SchemaRDD,
   registerTempTable, take(10).foreach(println)
   and take(30).foreach(e=>println(""%15s | %9.2f |"".format(e(0),e(1)))) ;o)
   - I remember Hossein Falaki chiding me about the ugly println statements
      !
      - Took me a little while to grok the dataset, sparksession,
      spark.read.option(""header"",""true"").option(""inferSchema"",""true"").csv(...)
et
      al.
         - I am a big R fan and know the language pretty decent - so the
         constructs are familiar
   ...) it was just beautiful - well done folks !!!
   because one has to think thru the old, the new and the transitional arc
      - I even remember the good old days when we were discussing whether
      Spark would get the dataframes like R at one of Paco's sessions !
      - And now, it looks very decent for data wrangling.

Cheers & keep up the good work
<k/>
P.S: My next chapter is the MLlib - need to convert to ml. Should be
interesting ... I am a glutton for punishment - of the Spark kind, of
course !
"
Reynold Xin <rxin@databricks.com>,"Sat, 18 Jun 2016 22:34:49 -0700",Re: Thanks For a Job Well Done !!!,Krishna Sankar <ksankar42@gmail.com>,"Thanks for the kind words, Krishna! Please keep the feedback coming.


"
Sean Owen <sowen@cloudera.com>,"Sun, 19 Jun 2016 21:59:36 +0100",Re: [VOTE] Release Apache Spark 1.6.2 (RC1),Reynold Xin <rxin@apache.org>,"I consistently get this error in the launcher suite on Ubuntu 16. I
don't know if it's a known issue.

-------------------------------------------------------

 T E S T S

-------------------------------------------------------

OpenJDK 64-Bit Server VM warning: ignoring option MaxPermSize=512m;
support was removed in 8.0

Running org.apache.spark.launcher.SparkSubmitOptionParserSuite

Tests run: 4, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.333
sec - in org.apache.spark.launcher.SparkSubmitOptionParserSuite

Running org.apache.spark.launcher.LauncherServerSuite

Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.376
sec <<< FAILURE! - in org.apache.spark.launcher.LauncherServerSuite

testCommunication(org.apache.spark.launcher.LauncherServerSuite)  Time
elapsed: 0.078 sec  <<< FAILURE!

java.lang.AssertionError: null

at org.junit.Assert.fail(Assert.java:86)

at org.junit.Assert.assertTrue(Assert.java:41)

at org.junit.Assert.assertNotNull(Assert.java:621)

at org.junit.Assert.assertNotNull(Assert.java:631)

at org.apache.spark.launcher.LauncherServerSuite.testCommunication(LauncherServerSuite.java:98)


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 19 Jun 2016 21:24:39 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC1),Sean Owen <sowen@cloudera.com>,"While we wait on the resolution of the test issue, I've created rc2.

This vote is now canceled in favor of rc2.


"
Reynold Xin <rxin@databricks.com>,"Sun, 19 Jun 2016 21:24:53 -0700",[VOTE] Release Apache Spark 1.6.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.6.2. The vote is open until Wednesday, June 22, 2016 at 22:00 PDT and
passes if a majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.6.2
[ ] -1 Do not release this package because ...


The tag to be voted on is v1.6.2-rc2
(54b1121f351f056d6b67d2bb4efe0d553c0f7482)

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.2-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1186/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.6.2-rc2-docs/


=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.6.1.

================================================
== What justifies a -1 vote for this release? ==
================================================
This is a maintenance release in the 1.6.x series.  Bugs already present in
1.6.1, missing features, or bugs related to new features will not
necessarily block this release.
"
Jacek Laskowski <jacek@japila.pl>,"Mon, 20 Jun 2016 08:54:45 +0200",How to explain SchedulerBackend.reviveOffers()?,dev <dev@spark.apache.org>,"Hi,

Whenever I see `backend.reviveOffers()` I'm struggling myself with
properly explaining what it does. My understanding is that it requests
a SchedulerBackend (that's responsible for talking to a cluster
manager) to...that's the moment I'm not sure about.

How would you explain `backend.reviveOffers()`?

p.s. I understand that it's somehow related to how Mesos manages
resources where it offers resources, but can't find anything related
to `reviving offers` in Mesos docs :(

Please guide. Thanks!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Amit Rana <amitranavsr94@gmail.com>,"Mon, 20 Jun 2016 17:53:04 +0530",Spark dev-setup,dev@spark.apache.org,"Hi all,

I am interested  in figuring out how pyspark works at core/internal level.
And  would like to understand the code flow as well.
For that I need to run a simple  example  in debug mode so that I can trace
the data flow for pyspark.
Can anyone please guide me on how do I set up my development environment
for the same in intellij IDEA in Windows 7.

Thanks,
Amit Rana
"
Jonathan Kelly <jonathakamzn@gmail.com>,"Mon, 20 Jun 2016 14:04:47 +0000","Re: Spark 2.0 on YARN - Files in config archive not ending up on
 executor classpath","user <user@spark.apache.org>, Spark dev list <dev@spark.apache.org>","Does anybody have any thoughts on this?

"
Richard Marscher <rmarscher@localytics.com>,"Mon, 20 Jun 2016 11:21:27 -0400",Inconsistent joinWith behavior?,Dev <dev@spark.apache.org>,"I know recently outer join was changed to preserve actual nulls through the
join in https://github.com/apache/spark/pull/13425. I am seeing what seems
like inconsistent behavior though based on how the join is interacted with.
In one case the default datatype values are still used instead of nulls
whereas the other case passes the nulls through. I have a small databricks
notebook showing the case against 2.0 preview:

https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/160347920874755/4268263383756277/673639177603143/latest.html

-- 
*Richard Marscher*
Senior Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
Stephen Hellberg <hellbes@uk.ibm.com>,"Mon, 20 Jun 2016 09:36:57 -0700 (MST)",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,dev@spark.apache.org,"Sean Owen wrote

Is there any perspective on just how long 'a bit' might be?  I'm not sure
I've found any prior description in our community of a (long-term?) support
commitment previously - are we talking months, or years?



--

---------------------------------------------------------------------


"
Matei Zaharia <matei.zaharia@gmail.com>,"Mon, 20 Jun 2016 10:22:35 -0700",Re: How to explain SchedulerBackend.reviveOffers()?,Jacek Laskowski <jacek@japila.pl>,"Hi Jacek,

This applies to all schedulers actually -- it just tells Spark to re-check the available nodes and possibly launch tasks on them, because a new stage was submitted. Then when any node is available, the scheduler will call the TaskSetManager with an ""offer"" for the node.

Matei



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 20 Jun 2016 18:41:35 +0100",Re: cutting 1.6.2 rc and 2.0.0 rc this week?,Stephen Hellberg <hellbes@uk.ibm.com>,"This is only my opinion, but, I really do not expect a 1.7.0. I can
imagine a 1.6.3 bug fix release in a few months, but kind of doubt it
would continue much past that.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 20 Jun 2016 10:42:20 -0700","Re: Spark 2.0 on YARN - Files in config archive not ending up on
 executor classpath",Jonathan Kelly <jonathakamzn@gmail.com>,"I just tried this locally and can see the wrong behavior you mention.
I'm running a somewhat old build of 2.0, but I'll take a look.




-- 
Marcelo

---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Mon, 20 Jun 2016 11:52:45 -0700",Re: Inconsistent joinWith behavior?,Richard Marscher <rmarscher@localytics.com>,"Hello Richard,

Looks like the Dataset is Dataset[(Int, Int)]. I guess for the case of
""ds.joinWith(other, expr, Outer).map({ case (t, u) => (Option(t),
Option(u)) })"". We are trying to use null to create a ""(Int, Int)"" and
somehow it ended up with a tuple2 having default values.

Can you create a jira? We will investigate the issue.

Thanks!

Yin


"
Michael Armbrust <michael@databricks.com>,"Mon, 20 Jun 2016 12:03:20 -0700",Re: Question about equality of o.a.s.sql.Row,Kazuaki Ishizaki <ISHIZAKI@jp.ibm.com>,"

Even equals(...) does not do what you want on the JVM:

scala> Array(1,2).equals(Array(1,2))
res1: Boolean = false




Pull requests for documentation welcome!




Internally for tests, we usually compare the string representation of the
Row.
"
Richard Marscher <rmarscher@localytics.com>,"Mon, 20 Jun 2016 15:04:31 -0400",Re: Inconsistent joinWith behavior?,Yin Huai <yhuai@databricks.com>,"Hi,

thanks for the response. I have created a JIRA ticket:
https://issues.apache.org/jira/browse/SPARK-16076




-- 
*Richard Marscher*
Senior Software Engineer
Localytics
Localytics.com <http://localytics.com/> | Our Blog
<http://localytics.com/blog> | Twitter <http://twitter.com/localytics> |
Facebook <http://facebook.com/localytics> | LinkedIn
<http://www.linkedin.com/company/1148792?trk=tyah>
"
"""Erik O'Shaughnessy"" <erik.oshaughnessy@oracle.com>","Mon, 20 Jun 2016 14:33:52 -0500",Integrating a Native ARchive (NAR)  into Apache Spark,dev@spark.apache.org,"Hello, 

I’ve cobbled together a JNI interface and packaged it with the nar-maven-plugin, but I’m frankly
out of my depth with Java/Scala/Maven and could use some pointers about how to make my new JNI
object available for import in spark-shell.

My NAR project builds a simple Foo object that wraps some native libfoo calls. The unit tests
for the NAR indicate that my C APIs are being called successfully. I installed it into my local .m2
repository cache and was able to include it as a dependency in a simple test project which builds
and calls my shared library.  So far so good.

Wrong.

I’ve tried adding Foo as a dependency in spark/pom.xml and adding nar-maven-plugin to the build
section of the top level POM:

<dependencies>
 <dependency>
  <groupId>com.oracle.sae</groupId>
  <artifactId>Foo</artifactId>
  <version>1.0-SNAPSHOT</version>
  <type>nar</type>
 </dependency>
</dependencies>

<build>
 <plugins>
  <plugin>
    <groupId>com.github.maven-nar</groupId>
    <artifactId>nar-maven-plugin</artifactId>
    <version>3.4.1-SNAPSHOT-SAE</version>
    <extensions>true</extensions>
    <executions>
      <execution>
        <goals>
          <goal>nar-unpack</goal>
        </goals>
      </execution>
    </executions>
  <plugin>
  </plugin>
 </plugins>
</build>

I had to do some hacking on nar-maven-plugin to teach it about my platform/os/linker, which
is the reason for the goofy nar-maven-plugin version number. 

A spark build succeeds, but:

scala> import com.oracle.sae.Foo
<console>:23: error: object Foo is not a member of package com.oracle.sae


I tried adding my Foo maven project as a module in the top-level POM and moving my project
into the Apache Spark tree.  It built everything including my Foo project, however the Foo
object wasn’t available in spark-shell like I had hoped.

I’m a totally neophyte when it comes to the Maven ecosystem, however I’ve come a long way in
the last two weeks. I know it’s a weird request, but I would be most appreciative if someone
could provide some gentle hints about how to make my Foo object available within spark-shell.

Best regards,
Erik


Erik O'Shaughnessy - Principal Software Engineer
Strategic Application Engineering
Austin, TX 512-401-1070
erik.oshaughnessy@oracle.com


---------------------------------------------------------------------


"
dhruve ashar <dhruveashar@gmail.com>,"Mon, 20 Jun 2016 15:19:22 -0500",Re: Question about equality of o.a.s.sql.Row,Michael Armbrust <michael@databricks.com>,"In scala, ""=="" and ""!="" are not operators but methods which are defined here
<http://www.scala-lang.org/api/current/#scala.Any> as :

The expression x == that is equivalent to if (x eq null) that eq null else
x.*equals*(that).
The expression x != that is equivalent to true if !(this == that)

So its recommended that you override equals method but check for equality
using == and !=.








-- 
-Dhruve Ashar
"
Jonathan Kelly <jonathakamzn@gmail.com>,"Mon, 20 Jun 2016 20:20:08 +0000","Re: Spark 2.0 on YARN - Files in config archive not ending up on
 executor classpath",Marcelo Vanzin <vanzin@cloudera.com>,"Thanks for the confirmation! Shall I cut a JIRA issue?


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 20 Jun 2016 13:21:00 -0700","Re: Spark 2.0 on YARN - Files in config archive not ending up on
 executor classpath",Jonathan Kelly <jonathakamzn@gmail.com>,"It doesn't hurt to have a bug tracking it, in case anyone else has
time to look at it before I do.




-- 
Marcelo

---------------------------------------------------------------------


"
Jonathan Kelly <jonathakamzn@gmail.com>,"Mon, 20 Jun 2016 21:14:33 +0000","Re: Spark 2.0 on YARN - Files in config archive not ending up on
 executor classpath",Marcelo Vanzin <vanzin@cloudera.com>,"OK, JIRA created: https://issues.apache.org/jira/browse/SPARK-16080

Also, after looking at the code a bit I think I see the reason. If I'm
correct, it may actually be a very easy fix.


"
Joseph Bradley <joseph@databricks.com>,"Mon, 20 Jun 2016 21:55:03 -0700",Re: Hello,Michael Armbrust <michael@databricks.com>,"Hi Harmeet,

I'll add one more item to the other advice: The community is in the process
of putting together a roadmap JIRA for 2.1 for ML:
https://issues.apache.org/jira/browse/SPARK-15581

This JIRA lists some of the major items and links to a few umbrella JIRAs
with subtasks.  I'd expect this roadmap to change a little more as it is
still being formed, but I hope it provides some guidance.  Feel free to
ping on specific JIRAs to ask about their current importance and to see who
else is working on them.

Thanks!
Joseph


"
Pete Robbins <robbinspg@gmail.com>,"Tue, 21 Jun 2016 09:57:00 +0000",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","The PR (https://github.com/apache/spark/pull/13055) to fix
https://issues.apache.org/jira/browse/SPARK-15262 was applied to 1.6.2
however this fix caused another issue
https://issues.apache.org/jira/browse/SPARK-15606 the fix for which (
https://github.com/apache/spark/pull/13355) has not been backported to the
1.6 branch so I'm now seeing the same failure in 1.6.2

Cheers,


"
Deenar Toraskar <deenar.toraskar@gmail.com>,"Tue, 21 Jun 2016 11:15:59 +0100",Spark 2.0 Aggregator with complex types,dev <dev@spark.apache.org>,"Hi Guys

I have got the Aggregator in Spark 2.0 working for case classes and
primitive types and some complex types like Seq[], but when I use Maps or
multi-dimensional arrays I get an exception at runtime. Is this supported
or I am doing something wrong? Here is a code snippet and a stack trace.
There is a similar JIRA, but that does not fix my issue
https://issues.apache.org/jira/browse/SPARK-15704. Checked on last night's
build.

Regards
Deenar

import org.apache.spark.sql.functions._
import org.apache.spark.sql.TypedColumn
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
import org.apache.spark.sql.expressions.Aggregator
import org.apache.spark.sql.{Encoder,Row}
import sqlContext.implicits._

object CustomSummer extends Aggregator[Valuation, Map[Int, Seq[Double]],
Seq[Seq[Double]]] with Serializable  {
     def zero: Map[Int, Seq[Double]] = Map()
     def reduce(b: Map[Int, Seq[Double]], a:Valuation): Map[Int,
Seq[Double]] = {
       val timeInterval: Int = a.timeInterval
       val currentSum: Seq[Double] = b.get(timeInterval).getOrElse(Nil)
       val currentRow: Seq[Double] = a.pvs
       b.updated(timeInterval, sumArray(currentSum, currentRow))
     }
    def sumArray(a: Seq[Double], b: Seq[Double]): Seq[Double] = Nil
    def merge(b1: Map[Int, Seq[Double]], b2: Map[Int, Seq[Double]]):
Map[Int, Seq[Double]] = {
        b1 ++ b2.map { case (timeInterval, exposures) =>
          timeInterval -> sumArray(exposures, b1.getOrElse(timeInterval,
Nil))
        }
     }
     def finish(exposures: Map[Int, Seq[Double]]): Seq[Seq[Double]] =
      {
        exposures.size match {
          case 0 => null
          case _ => {
            val range = exposures.keySet.max
            (0 to range).map(x => exposures.getOrElse(x, Nil))
          }
        }
      }
  override def bufferEncoder: Encoder[Map[Int,Seq[Double]]] =
ExpressionEncoder()
  override def outputEncoder: Encoder[Seq[Seq[Double]]] =
ExpressionEncoder()
   }

case class Valuation(timeInterval : Int, pvs : Seq[Double])
val valns = sc.parallelize(Seq(Valuation(0, Seq(1.0,2.0,3.0)),
  Valuation(2, Seq(1.0,2.0,3.0)),
  Valuation(1, Seq(1.0,2.0,3.0)),Valuation(2,
Seq(1.0,2.0,3.0)),Valuation(0, Seq(1.0,2.0,3.0))
  )).toDS

val g_c1 =
valns.groupByKey(_.timeInterval).agg(CustomSummer.toColumn).show(false)


============

org.apache.spark.SparkException: Job aborted due to stage failure: Task 1
in stage 10.0 failed 1 times, most recent failure: Lost task 1.0 in stage
10.0 (TID 19, localhost): java.lang.IndexOutOfBoundsException: 0
at
scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)
at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)
at scala.collection.mutable.ArrayBuffer.remove(ArrayBuffer.scala:167)
at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:244)
at
org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
at
org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:214)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:156)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression.semanticHash(Expression.scala:174)
at
org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$Expr.hashCode(EquivalentExpressions.scala:39)
at scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:210)
at
scala.collection.mutable.HashTable$HashUtils$class.elemHashCode(HashTable.scala:398)
at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:39)
at scala.collection.mutable.HashTable$class.findEntry(HashTable.scala:130)
at scala.collection.mutable.HashMap.findEntry(HashMap.scala:39)
at scala.collection.mutable.HashMap.get(HashMap.scala:69)
at
org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExpr(EquivalentExpressions.scala:53)
at
org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:86)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionElimination(CodeGenerator.scala:661)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:718)
at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:59)
at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
at
org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:369)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:93)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:92)
at
org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:178)
at
org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:197)
at
org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:29)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:84)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:75)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:85)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)
at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)
at
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1872)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1885)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1898)
at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)
at
org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)
at
org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2176)
at
org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2525)
at org.apache.spark.sql.Dataset.org
$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2175)
at org.apache.spark.sql.Dataset.org
$apache$spark$sql$Dataset$$collect(Dataset.scala:2182)
at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1918)
at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1917)
at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2555)
at org.apache.spark.sql.Dataset.head(Dataset.scala:1917)
at org.apache.spark.sql.Dataset.take(Dataset.scala:2132)
at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)
at org.apache.spark.sql.Dataset.show(Dataset.scala:526)
at org.apache.spark.sql.Dataset.show(Dataset.scala:506)
Caused by: java.lang.IndexOutOfBoundsException: 0
at
scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)
at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)
at scala.collection.mutable.ArrayBuffer.remove(ArrayBuffer.scala:167)
at
org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:244)
at
org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)
at
org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:214)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:156)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at
scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
at scala.collection.immutable.List.foreach(List.scala:318)
at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
at scala.collection.AbstractTraversable.map(Traversable.scala:105)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)
at
org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)
at
org.apache.spark.sql.catalyst.expressions.Expression.semanticHash(Expression.scala:174)
at
org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$Expr.hashCode(EquivalentExpressions.scala:39)
at scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:210)
at
scala.collection.mutable.HashTable$HashUtils$class.elemHashCode(HashTable.scala:398)
at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:39)
at scala.collection.mutable.HashTable$class.findEntry(HashTable.scala:130)
at scala.collection.mutable.HashMap.findEntry(HashMap.scala:39)
at scala.collection.mutable.HashMap.get(HashMap.scala:69)
at
org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExpr(EquivalentExpressions.scala:53)
at
org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:86)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionElimination(CodeGenerator.scala:661)
at
org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:718)
at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:59)
at
org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)
at
org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:369)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:93)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:92)
at
org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:178)
at
org.apache.spark.sql.execution.aggregate.AggregationIterator.<init>(AggregationIterator.scala:197)
at
org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.<init>(SortBasedAggregationIterator.scala:29)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:84)
at
org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:75)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
at org.apache.spark.scheduler.Task.run(Task.scala:85)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
at java.lang.Thread.run(Thread.java:745)
"
Sachin Aggarwal <different.sachin@gmail.com>,"Tue, 21 Jun 2016 15:49:42 +0530",Structured Streaming partition logic with respect to storage and fileformat,dev@spark.apache.org,"when we use readStream to read data as Stream, how spark decides the no of
RDD and partition within each RDD with respect to storage and file format.

val dsJson = sqlContext.readStream.json(""/Users/sachin/testSpark/inputJson"")

val dsCsv = sqlContext.readStream.option(""header"",""true"").csv(
""/Users/sachin/testSpark/inputCsv"")

val ds = sqlContext.readStream.text(""/Users/sachin/testSpark/inputText"")
val dsText = ds.as[String].map(x =>(x.split("" "")(0),x.split(""
"")(1))).toDF(""name"",""age"")

val dsParquet =
sqlContext.readStream.format(""parquet"").parquet(""/Users/sachin/testSpark/inputParquet"")



-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
Akhil Das <akhld@hacked.work>,"Tue, 21 Jun 2016 18:19:15 +0700",Re: Spark dev-setup,Amit Rana <amitranavsr94@gmail.com>,"You can read this documentation to get started with the setup
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-IntelliJ


There was a pyspark setup discussion on SO over here
http://stackoverflow.com/questions/33478218/write-and-run-pyspark-in-intellij-idea





-- 
Cheers!
"
Simon NANTY <Simon.NANTY@amadeus.com>,"Tue, 21 Jun 2016 11:48:09 +0000",Possible contribution to MLlib,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi all,

In my team, we are currently developing a fork of spark MLlib extending K-means method such that it is possible to set its own distance function. In this implementation, it could be possible to directly pass, in argument of the K-means train function, a distance function whose signature is: (VectorWithNorm, VectorWithNorm) => Double.

We have found the Jira instance SPARK-11665 proposing to support new distances in bisecting K-means. There has also been the Jira instance SPARK-3219 proposing to add Bregman divergences as distance functions, but it has not been added to MLlib. Therefore, we are wondering if such an extension of MLlib K-means algorithm would be appreciated by the community and would have chances to get included in future spark releases.

Regards,

Simon Nanty

"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Tue, 21 Jun 2016 13:06:34 +0200",Re: Structured Streaming partition logic with respect to storage and fileformat,Sachin Aggarwal <different.sachin@gmail.com>,"Based on the underlying Hadoop FileFormat. This one does it mostly based on blocksize. You can change this though.

te:
 RDD and partition within each RDD with respect to storage and file format.
on"")
chin/testSpark/inputCsv"")

toDF(""name"",""age"")
sachin/testSpark/inputParquet"")
"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 21 Jun 2016 21:13:22 +0800",Re: Possible contribution to MLlib,Simon NANTY <Simon.NANTY@amadeus.com>,"I think it is valuable to make the distance function pluggable and also
provide some builtin distance function. This might be also useful for other
algorithms besides KMeans.





-- 
Best Regards

Jeff Zhang
"
"""tesmai4@gmail.com"" <tesmai4@gmail.com>","Tue, 21 Jun 2016 17:01:16 +0100",Jar for Spark developement,dev@spark.apache.org,"Hi,

Beginner in Spark development. Took time to configure Eclipse + Scala. Is
there any tutorial that can help beginners.

Still struggling to find Spark JAR files for development. There is no lib
folder in my Spark distribution (neither in pre-built nor in custom built..)


Regards,
"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 21 Jun 2016 10:40:10 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Pete Robbins <robbinspg@gmail.com>,"Hey Pete,

I didn't backport it to 1.6 because it just affects tests in most cases.
I'm sure we also have other places calling blocking methods in the event
loops, so similar issues are still there even after applying this patch.
Hence, I don't think it's a blocker for 1.6.2.


"
Sean Owen <sowen@cloudera.com>,"Tue, 21 Jun 2016 18:49:08 +0100",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Reynold Xin <rxin@databricks.com>,"I'm getting some errors building on Ubuntu 16 + Java 7. First is one
that may just be down to a Scala bug:

[ERROR] bad symbolic reference. A signature in WebUI.class refers to
term eclipse
in package org which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when
compiling WebUI.class.
[ERROR] bad symbolic reference. A signature in WebUI.class refers to term jetty
in value org.eclipse which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when
compiling WebUI.class.

But I'm seeing some consistent timezone-related failures, from core:

UIUtilsSuite:
- formatBatchTime *** FAILED ***
  ""2015/05/14 [14]:04:40"" did not equal ""2015/05/14 [21]:04:40""
(UIUtilsSuite.scala:73)


and several from Spark SQL, like:


- udf_unix_timestamp *** FAILED ***
  Results do not match for udf_unix_timestamp:
  == Parsed Logical Plan ==
  'Project [unresolvedalias(2009-03-20
11:30:01),unresolvedalias('unix_timestamp(2009-03-20 11:30:01))]
  +- 'UnresolvedRelation `oneline`, None

  == Analyzed Logical Plan ==
  _c0: string, _c1: bigint
  Project [2009-03-20 11:30:01 AS _c0#122914,unixtimestamp(2009-03-20
11:30:01,yyyy-MM-dd HH:mm:ss) AS _c1#122915L]
  +- MetastoreRelation default, oneline, None

  == Optimized Logical Plan ==
  Project [2009-03-20 11:30:01 AS _c0#122914,1237548601 AS _c1#122915L]
  +- MetastoreRelation default, oneline, None

  == Physical Plan ==
  Project [2009-03-20 11:30:01 AS _c0#122914,1237548601 AS _c1#122915L]
  +- HiveTableScan MetastoreRelation default, oneline, None
  _c0 _c1
  !== HIVE - 1 row(s) ==            == CATALYST - 1 row(s) ==
  !2009-03-20 11:30:01 1237573801   2009-03-20 11:30:01 1237548601
(HiveComparisonTest.scala:458)


I'll start looking into them. It could be real, if possibly minor,
bugs because I presume most of the testing happens on machines in a
PDT timezone instead of UTC? that's at least the timezone of the
machine I'm testing on.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 21 Jun 2016 10:53:21 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Sean Owen <sowen@cloudera.com>,"
This is probably https://issues.apache.org/jira/browse/SPARK-13780. It
should only affect incremental builds (""mvn -rf ..."" or ""mvn -pl
...""), not clean builds. Not sure about the other ones.

-- 
Marcelo

---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Tue, 21 Jun 2016 19:05:11 +0100",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"Nice one, yeah indeed I was doing an incremental build. Not a blocker.
I'll have a look into the others, though I suspect they're problems
with tests rather than production code.


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Tue, 21 Jun 2016 20:36:06 +0000",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","It breaks Spark running on machines with less than 3 cores/threads, which
may be rare, and it is maybe an edge case.

Personally, I like to fix known bugs and the fact there are other blocking
methods in event loops actually makes it worse not to fix ones that you
know about.

Probably not a blocker to release though but that's your call.

Cheers,


"
"""Shixiong(Ryan) Zhu"" <shixiong@databricks.com>","Tue, 21 Jun 2016 14:44:42 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Pete Robbins <robbinspg@gmail.com>,"Hey Pete,

I just pushed your PR to branch 1.6. As it's not a blocker, it may or may
not be in 1.6.2, depending on if there will be another RC.


"
Reynold Xin <rxin@databricks.com>,"Tue, 21 Jun 2016 18:26:31 -0700",[VOTE] Release Apache Spark 2.0.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
2.0.0. The vote is open until Friday, June 24, 2016 at 19:00 PDT and passes
if a majority of at least 3+1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 2.0.0
[ ] -1 Do not release this package because ...


The tag to be voted on is v2.0.0-rc1
(0c66ca41afade6db73c9aeddd5aed6e5dcea90df).

This release candidate resolves ~2400 issues:
https://s.apache.org/spark-2.0.0-rc1-jira

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1187/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-docs/


=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions from 1.x.

================================================
== What justifies a -1 vote for this release? ==
================================================
Critical bugs impacting major functionalities.

Bugs already present in 1.x, missing features, or bugs related to new
features will not necessarily block this release. Note that historically
Spark documentation has been published on the website separately from the
main release so we do not need to block the release due to documentation
errors either.
"
BaiRan <lizbai@icloud.com>,"Wed, 22 Jun 2016 10:45:49 +0800",Question about Bloom Filter in Spark 2.0,dev@spark.apache.org,"Hi all,

I have a question about bloom filter implementation in Spark-12818 issue. If I have a ORC file with bloom filter metadata, how can I utilise it by Spark SQL?
Thanks.

Best,
Ran"
Reynold Xin <rxin@databricks.com>,"Tue, 21 Jun 2016 19:48:41 -0700",Re: Question about Bloom Filter in Spark 2.0,BaiRan <lizbai@icloud.com>,"SPARK-12818 is about building a bloom filter on existing data. It has
nothing to do with the ORC bloom filter, which can be used to do predicate
pushdown.



"
Sachin Aggarwal <different.sachin@gmail.com>,"Wed, 22 Jun 2016 11:02:10 +0530",Re: Structured Streaming partition logic with respect to storage and fileformat,=?UTF-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"what will the scenario in case of s3 and  local file system?

ote:

f
.
)
.toDF(""name"",""age"")
/sachin/testSpark/inputParquet"")


-- 

Thanks & Regards

Sachin Aggarwal
7760502772
"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Jun 2016 06:38:21 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Reynold Xin <rxin@databricks.com>,"While I'd officially -1 this while there are still many blockers, this
should certainly be tested as usual, because they're mostly doc and
""audit"" type issues.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Jun 2016 09:02:37 +0100",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Reynold Xin <rxin@databricks.com>,"I'm fairly convinced this error and others that appear timestamp
related are an environment problem. This test and method have been
present for several Spark versions, without change. I reviewed the
logic and it seems sound, explicitly setting the time zone correctly.
I am not sure why it behaves differently on this machine.

I'd give a +1 to this release if nobody else is seeing errors like
this. The sigs, hashes, other tests pass for me.


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Jun 2016 11:14:54 +0100",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Reynold Xin <rxin@databricks.com>,"Oops, one more in the ""does anybody else see this"" department:

- offset recovery *** FAILED ***
  recoveredOffsetRanges.forall(((or: (org.apache.spark.streaming.Time,
Array[org.apache.spark.streaming.kafka.OffsetRange])) =>
earlierOffsetRangesAsSets.contains(scala.Tuple2.apply[org.apache.spark.streaming.Time,
scala.collection.immutable.Set[org.apache.spark.streaming.kafka.OffsetRange]](or._1,
scala.this.Predef.refArrayOps[org.apache.spark.streaming.kafka.OffsetRange](or._2).toSet[org.apache.spark.streaming.kafka.OffsetRange]))))
was false Recovered ranges are not the same as the ones generated
(DirectKafkaStreamSuite.scala:301)

This actually fails consistently for me too in the Kafka integration
code. Not timezone related, I think.


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Wed, 22 Jun 2016 10:38:31 +0000",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"Sean Owen <sowen@cloudera.com>, Reynold Xin <rxin@databricks.com>","This has failed on our 1.6 stream builds regularly. (
https://issues.apache.org/jira/browse/SPARK-6005) looks fixed in 2.0?


"
Priya Ch <learnings.chitturi@gmail.com>,"Wed, 22 Jun 2016 16:39:17 +0530",Spark Task failure with File segment length as negative,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Hi All,

I am running Spark Application with 1.8TB of data (which is stored in Hive
tables format).  I am reading the data using HiveContect and processing it.
The cluster has 5 nodes total, 25 cores per machine and 250Gb per node. I
am launching the application with 25 executors with 5 cores each and 45GB
per executor. Also, specified the property
spark.yarn.executor.memoryOverhead=2024.

During the execution, tasks are lost and ShuffleMapTasks are re-submitted.
I am seeing that tasks are failing with the following message -

*java.lang.IllegalArgumentException: requirement failed: File segment
length cannot be negative (got -27045427)*









* at scala.Predef$.require(Predef.scala:233)*









* at org.apache.spark.storage.FileSegment.<init>(FileSegment.scala:28)*









* at
org.apache.spark.storage.DiskBlockObjectWriter.fileSegment(DiskBlockObjectWriter.scala:220)*









* at
org.apache.spark.shuffle.sort.ShuffleExternalSorter.writeSortedFile(ShuffleExternalSorter.java:184)*









* at
org.apache.spark.shuffle.sort.ShuffleExternalSorter.closeAndGetSpills(ShuffleExternalSorter.java:398)*









* at
org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:206)*









* at
org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:166)*









* at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)*









* at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)*









* at org.apache.spark.scheduler.Task.run(Task.scala:89)*









* at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)*









* at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)*









* at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)*









I understood that its because the shuffle block is > 2G, the Int value is
taking negative and throwing the above exeception.

Can someone throw light on this ? What is the fix for this ?

Thanks,
Padma CH
"
BaiRan <lizbai@icloud.com>,"Wed, 22 Jun 2016 20:01:26 +0800",Re: Question about Bloom Filter in Spark 2.0,Reynold Xin <rxin@databricks.com>,"After building bloom filter on existing data, does spark engine utilise bloom filter during query processing?
Is there any plan about predicate push down by using bloom filter in ORC / Parquet?

Thanks
Ran
nothing to do with the ORC bloom filter, which can be used to do predicate pushdown.
issue. If I have a ORC file with bloom filter metadata, how can I utilise it by Spark SQL?

"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Jun 2016 13:41:49 +0100",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Pete Robbins <robbinspg@gmail.com>,"Good call, probably worth back-porting, I'll try to do that. I don't
think it blocks a release, but would be good to get into a next RC if
any.


---------------------------------------------------------------------


"
Prajwal Tuladhar <praj@infynyxx.com>,"Wed, 22 Jun 2016 08:44:19 -0400",Re: Spark internal Logging trait potential thread unsafe,Sean Owen <sowen@cloudera.com>,"Created a JIRA issue https://issues.apache.org/jira/browse/SPARK-16131 and
PR @ https://github.com/apache/spark/pull/13842





-- 
--
Cheers,
Praj
"
Cody Koeninger <cody@koeninger.org>,"Wed, 22 Jun 2016 09:19:08 -0500",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Sean Owen <sowen@cloudera.com>,"If we're considering backporting changes for the 0.8 kafka
integration, I am sure there are people who would like to get

https://issues.apache.org/jira/browse/SPARK-10963

into 1.6.x as well


---------------------------------------------------------------------


"
Cody Koeninger <cody@koeninger.org>,"Wed, 22 Jun 2016 09:29:24 -0500",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Sean Owen <sowen@cloudera.com>,"I don't have a vote, but I'd just like to reiterate that I think kafka
0.10 support should be added to a 2.0 release candidate; if not now,
then well before release.

- it's a completely standalone jar, so shouldn't break anyone who's
using the existing 0.8 support
- it's like the 5th highest voted open ticket, and has been open for months
- Luciano has said multiple times that he wants to merge that PR into
Bahir if it isn't in a RC for spark 2.0, which I think would confuse
users and cause maintenance problems


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Jun 2016 15:39:44 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Cody Koeninger <cody@koeninger.org>,"I profess ignorance again though I really should know by now, but,
what's opposing that? I personally thought this was going to be in 2.0
and didn't kind of notice it wasn't ...


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Wed, 22 Jun 2016 14:42:16 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"Sean Owen <sowen@cloudera.com>, Cody Koeninger <cody@koeninger.org>","For the clueless (like me):

https://bahir.apache.org/#home

Apache Bahir provides extensions to distributed analytic platforms such as
Apache Spark.

Initially Apache Bahir will contain streaming connectors that were a part
of Apache Spark prior to version 2.0:

   - streaming-akka
   - streaming-mqtt
   - streaming-twitter
   - streaming-zeromq

The Apache Bahir community welcomes the proposal of new extensions.

Nick
​


t:
/
================
================
an
=========================
=========================
"
Cody Koeninger <cody@koeninger.org>,"Wed, 22 Jun 2016 09:46:46 -0500",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Sean Owen <sowen@cloudera.com>,"As far as I know the only thing blocking it at this point is lack of
committer review / approval.

It's technically adding a new feature after spark code-freeze, but it
doesn't change existing code, and the kafka project didn't release
0.10 until the end of may.



---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Wed, 22 Jun 2016 15:51:44 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"Cody Koeninger <cody@koeninger.org>, Imran Rashid <irashid@cloudera.com>, 
	Mark Grover <mgrover@cloudera.com>","Hm, I thought that was to be added for 2.0. Imran I know you may have
been working alongside Mark on it; what do you think?

TD / Reynold would you object to it for 2.0?


---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Wed, 22 Jun 2016 07:55:35 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Cody Koeninger <cody@koeninger.org>,"

To be fair with the Kafka 0.10 PR assessment :

I was expecting somewhat an easy transition from customer using 0.80 to
0.10 connector, but the 0.10 seems to have been treated as a completely new
extension, also, there is no python support, no samples on the pr
demonstrating how to use security capabilities and no documentation
updates.

Thanks

-- 
Luciano Resende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Cody Koeninger <cody@koeninger.org>,"Wed, 22 Jun 2016 10:17:38 -0500",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Luciano Resende <luckbr1975@gmail.com>,"Luciano knows there are publicly available examples of how to use the
0.10 connector, including TLS support, because he asked me about it
and I gave him a link

https://github.com/koeninger/kafka-exactly-once/blob/kafka-0.9/src/main/scala/example/TlsStream.scala

If any committer at any time had said ""I'd accept this PR, if only it
included X"", I'd be happy to provide X.  Documentation updates and
python support for the 0.8 direct stream connector were done after the
original PR.




---------------------------------------------------------------------


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Wed, 22 Jun 2016 15:43:36 +0000","Unable to  increase Active Tasks of a Spark Streaming Process in
 Yarn","""'user@spark.apache.org'"" <user@spark.apache.org>,
	""'dev@spark.apache.org'"" <dev@spark.apache.org>","Hello all,

I am running a spark streaming process where I got a batch of 6000 events.  But when I look at executors only one active task is running.  I tried dynamic allocation and as well as setting number of executors etc.  Even if I have 15 executors only one active task is running at a time.  Can any one please guide me what am I doing wrong here.

[cid:image003.png@01D1CC62.29CA9A60]

[cid:image004.png@01D1CC62.29CA9A60]
"
Chris Fregly <chris@fregly.com>,"Wed, 22 Jun 2016 09:01:59 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Cody Koeninger <cody@koeninger.org>,"+1 for 0.10 support.  this is huge.




-- 
*Chris Fregly*
Research Scientist @ PipelineIO
San Francisco, CA
pipeline.io
advancedspark.com
"
shane knapp <sknapp@berkeley.edu>,"Wed, 22 Jun 2016 10:20:04 -0700","[build system] jenkins process wedged, need to do restart","dev <dev@spark.apache.org>, amp-infra <amp-infra@googlegroups.com>","of course, on my first day back from vacation, i notice that the
jenkins process got wedged immediately upon my visiting the page.

one quick jenkins/httpd restart later and we're back up and building.
sorry for any inconvenience!

shane

---------------------------------------------------------------------


"
Mark Grover <mark@apache.org>,"Wed, 22 Jun 2016 10:32:18 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Chris Fregly <chris@fregly.com>,"Yeah, I am +1 for including Kafka 0.10 integration as well. We had to wait
for Kafka 0.10 because there were incompatibilities between the Kafka 0.9
and 0.10 API. And, yes, the code for 0.8.0 remains unchanged so there
shouldn't be any regression for existing users. It's only new code for 0.10.

The comments about python support lacking are correct but I do think it's
unfair to unblock this particular PR, without a wider policy of blocking
every PR on that.


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Wed, 22 Jun 2016 17:33:29 +0000","RE: Unable to  increase Active Tasks of a Spark Streaming Process
 in Yarn","""'user@spark.apache.org'"" <user@spark.apache.org>,
	""'dev@spark.apache.org'"" <dev@spark.apache.org>","Here are some more details any pointer is really appreciated:

I have configured number of partitions at Kafka level as 40.  Number of repartition of spark streaming as 40.  I have disabled dynamic allocation for spark.

From: Rachana Srivastava
Sent: Wednesday, June 22, 2016 8:44 AM
To: 'user@spark.apache.org'; 'dev@spark.apache.org'
Subject: Unable to increase Active Tasks of a Spark Streaming Process in Yarn

Hello all,

I am running a spark streaming process where I got a batch of 6000 events.  But when I look at executors only one active task is running.  I tried dynamic allocation and as well as setting number of executors etc.  Even if I have 15 executors only one active task is running at a time.  Can any one please guide me what am I doing wrong here.

[cid:image001.png@01D1CC71.839B2A50]

[cid:image002.png@01D1CC71.839B2A50]
"
Tim Hunter <timhunter@databricks.com>,"Wed, 22 Jun 2016 10:40:53 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Cody Koeninger <cody@koeninger.org>,"+1 This release passes all tests on the graphframes and tensorframes
packages.


"
Rachana Srivastava <Rachana.Srivastava@markmonitor.com>,"Wed, 22 Jun 2016 18:01:01 +0000","RE: Unable to  increase Active Tasks of a Spark Streaming Process
 in Yarn","""'user@spark.apache.org'"" <user@spark.apache.org>,
	""'dev@spark.apache.org'"" <dev@spark.apache.org>","Not sure why number of active jobs are always 1 regardless of number of partitions, executors etc.  Can anyone please guide me what drives this Active Job.

[cid:image003.png@01D1CC75.5C4E4B90]

From: Rachana Srivastava
Sent: Wednesday, June 22, 2016 10:33 AM
To: 'user@spark.apache.org'; 'dev@spark.apache.org'
Subject: RE: Unable to increase Active Tasks of a Spark Streaming Process in Yarn

Here are some more details any pointer is really appreciated:

I have configured number of partitions at Kafka level as 40.  Number of repartition of spark streaming as 40.  I have disabled dynamic allocation for spark.

From: Rachana Srivastava
Sent: Wednesday, June 22, 2016 8:44 AM
To: 'user@spark.apache.org'; 'dev@spark.apache.org'
Subject: Unable to increase Active Tasks of a Spark Streaming Process in Yarn

Hello all,

I am running a spark streaming process where I got a batch of 6000 events.  But when I look at executors only one active task is running.  I tried dynamic allocation and as well as setting number of executors etc.  Even if I have 15 executors only one active task is running at a time.  Can any one please guide me what am I doing wrong here.

[cid:image004.png@01D1CC75.5C4E4B90]

[cid:image005.png@01D1CC75.5C4E4B90]
"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Wed, 22 Jun 2016 20:07:18 +0200",Re: Question about Bloom Filter in Spark 2.0,BaiRan <lizbai@icloud.com>,"You should see at it both levels: there is one bloom filter for Orc data and one for data in-memory. 

It is already a good step towards an integration of format and in-memory representation for columnar data. 

oom filter during query processing?
 Parquet?
hing to do with the ORC bloom filter, which can be used to do predicate pushdown.
. If I have a ORC file with bloom filter metadata, how can I utilise it by Spark SQL?
"
Jonathan Kelly <jonathakamzn@gmail.com>,"Wed, 22 Jun 2016 18:33:10 +0000",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"Tim Hunter <timhunter@databricks.com>, Cody Koeninger <cody@koeninger.org>",1
Michael Armbrust <michael@databricks.com>,"Wed, 22 Jun 2016 12:14:25 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Jonathan Kelly <jonathakamzn@gmail.com>,1
Sean McNamara <Sean.McNamara@Webtrends.com>,"Wed, 22 Jun 2016 19:19:08 +0000",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Michael Armbrust <michael@databricks.com>,"+1


+1

+1

+1 This release passes all tests on the graphframes and tensorframes packages.

If we're considering backporting changes for the 0.8 kafka
integration, I am sure there are people who would like to get

https://issues.apache.org/jira/browse/SP"
Reynold Xin <rxin@databricks.com>,"Wed, 22 Jun 2016 12:53:05 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 myself



"
Kousuke Saruta <sarutak@oss.nttdata.co.jp>,"Thu, 23 Jun 2016 05:07:41 +0900",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1 (non-binding)



"
Sameer Agarwal <sameer@databricks.com>,"Wed, 22 Jun 2016 13:30:24 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1




-- 
Sameer Agarwal
Software Engineer | Databricks Inc.
http://cs.berkeley.edu/~sameerag
"
Petar Zecevic <petar.zecevic@gmail.com>,"Wed, 22 Jun 2016 22:32:33 +0200",Re: Jar for Spark developement,dev@spark.apache.org,"
You can check out the Spark in Action book. In my (not so humble) 
opinion, it's very good for beginners.

Petar (author)



"
Krishna Sankar <ksankar42@gmail.com>,"Wed, 22 Jun 2016 15:13:38 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Reynold Xin <rxin@databricks.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 37:11 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib (iPython 4.0)
2.0 Spark version is 1.6.2
2.1. statistics (min,max,mean,Pearson,Spearman)"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Wed, 22 Jun 2016 23:04:38 +0000",RE: [VOTE] Release Apache Spark 2.0.0 (RC1),"Reynold Xin <rxin@databricks.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","-1
Spark Unit tests fail on Windows. Still not resolved, though marked as resolved.
https://issues.apache.org/jiabricks.com]
Sent: Tuesday, June 21, 2016 6:27 PM
To: dev@spark.apache.org
Subject: [VOTE] Release Apache Spark 2.0.0 (RC1)

Please vote"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 22 Jun 2016 16:07:37 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","
To be pedantic, it's marked as a duplicate
(https://issues.apache.org/jira/browse/SPARK-15899), which doesn't
mean necessarily that it's fixed.






-- 
Marcelo

---------------------------------------------------------------------


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 22 Jun 2016 16:07:57 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","SPARK-15893 is resolved as a duplicate of SPARK-15899.  SPARK-15899 is
Unresolved.


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 22 Jun 2016 16:09:10 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Marcelo Vanzin <vanzin@cloudera.com>,"It's also marked as Minor, not Blocker.


"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 23 Jun 2016 00:56:05 +0000",RE: [VOTE] Release Apache Spark 2.0.0 (RC1),"Mark Hamstra <mark@clearstorydata.com>, Marcelo Vanzin
	<vanzin@cloudera.com>","Spark Unit tests fail on Windows in Spark 2.0. It can be considered as blocker since there are people that develop for Spark on Windows. The referenced issue is indeed Minor and has nothing to do with unit tests.

From: Mark Hamstra [mailto:mark@clearstorydata.com]
Sent: Wednesday, June 22, 2016 4:09 PM
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: Ulanov, Alexander <alexander.ulanov@hpe.com>; Reynold Xin <rxin@databricks.com>; dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 2.0.0 (RC1)

It's also marked as Minor, not Blocker.

On Wed, Jun 22, 2016 at 4:07 PM, Marcelo Vanzin <vanzin@cloudera.com<mailto:vanzin@cloudera.com>> wrote:
On Wed, Jun 22, 2016 at 4:04 PM, Ulanov, Alexander
<alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
> -1
>
> Spark Unit tests fail on Windows. Still not resolved, though marked as
> resolved.

To be pedantic, it's marked as a duplicate
(https://issues.apache.org/jira/browse/SPARK-15899), which doesn't
mean necessarily that it's fixed.



> https://issues.apache.org/jira/browse/SPARK-15893
>
> From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
> Sent: Tuesday, June 21, 2016 6:27 PM
> To: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Subject: [VOTE] Release Apache Spark 2.0.0 (RC1)
>
>
>
> Please vote on releasing the following candidate as Apache Spark version
> 2.0.0. The vote is open until Friday, June 24, 2016 at 19:00 PDT and passes
> if a majority of at least 3+1 PMC votes are cast.
>
>
>
> [ ] +1 Release this package as Apache Spark 2.0.0
>
> [ ] -1 Do not release this package because ...
>
>
>
>
>
> The tag to be voted on is v2.0.0-rc1
> (0c66ca41afade6db73c9aeddd5aed6e5dcea90df).
>
>
>
> This release candidate resolves ~2400 issues:
> https://s.apache.org/spark-2.0.0-rc1-jira
>
>
>
> The release files, including signatures, digests, etc. can be found at:
>
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-bin/
>
>
>
> Release artifacts are signed with the following key:
>
> https://people.apache.org/keys/committer/pwendell.asc
>
>
>
> The staging repository for this release can be found at:
>
> https://repository.apache.org/content/repositories/orgapachespark-1187/
>
>
>
> The documentation corresponding to this release can be found at:
>
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-docs/
>
>
>
>
>
> =======================================
>
> == How can I help test this release? ==
>
> =======================================
>
> If you are a Spark user, you can help us test this release by taking an
> existing Spark workload and running on this release candidate, then
> reporting any regressions from 1.x.
>
>
>
> ================================================
>
> == What justifies a -1 vote for this release? ==
>
> ================================================
>
> Critical bugs impacting major functionalities.
>
>
>
> Bugs already present in 1.x, missing features, or bugs related to new
> features will not necessarily block this release. Note that historically
> Spark documentation has been published on the website separately from the
> main release so we do not need to block the release due to documentation
> errors either.
>
>
>
>


--
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>

"
Reynold Xin <rxin@databricks.com>,"Wed, 22 Jun 2016 18:43:11 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Alex - if you have access to a windows box, can you fix the issue? I'm not
sure how many Spark contributors have windows boxes.



"
Daniel Imberman <daniel.imberman@gmail.com>,"Thu, 23 Jun 2016 02:07:59 +0000",Creating a python port for a Scala Spark Projeect,"user <user@spark.apache.org>, dev@spark.apache.org","Hi All,

I've developed a spark module in scala that I would like to add a python
port for. I want to be able to allow users to create a pyspark RDD and send
it to my system. I've been looking into the pyspark source code as well as
py4J and was wondering if there has been anything like this implemented
before.

Thank you
"
Holden Karau <holden@pigscanfly.ca>,"Wed, 22 Jun 2016 19:12:37 -0700",Re: Creating a python port for a Scala Spark Projeect,Daniel Imberman <daniel.imberman@gmail.com>,"PySpark RDDs are (on the Java side) are essentially RDD of pickled objects
and mostly (but not entirely) opaque to the JVM. It is possible (by using
some internals) to pass a PySpark DataFrame to a Scala library (you may or
may not find the talk I gave at Spark Summit useful
https://www.youtube.com/watch?v=V6DkTVvy9vk as well as some of the Python
examples in
https://github.com/high-performance-spark/high-performance-spark-examples
). Good luck! :)





-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Daniel Imberman <daniel.imberman@gmail.com>,"Thu, 23 Jun 2016 02:15:08 +0000",Re: Creating a python port for a Scala Spark Projeect,Holden Karau <holden@pigscanfly.ca>,"Thank you Holden, I look forward to watching your talk!


"
linxi zeng <linxizeng0615@gmail.com>,"Thu, 23 Jun 2016 11:10:59 +0800","why did spark2.0 Disallow ROW FORMAT and STORED AS (parquet | orc |
 avro etc.)","user@spark.apache.org, dev@spark.apache.org","Hi All,
    I have tried the spark sql of Spark branch-2.0 and countered an
unexpected problem:

Operation not allowed: ROW FORMAT DELIMITED is only compatible with
'textfile', not 'orc'(line 1, pos 0)

the sql is like:

CREATE TABLE IF NOT EXISTS test.test_orc
(
 ...
)
PARTITIONED BY (xxx)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t' LINES TERMINATED BY '\n'
stored as orc

I found this JIRA: https://issues.apache.org/jira/browse/SPARK-15279,
but still can't understand why?

and by the way, this sql is work fine on spark1.4.
"
"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","Thu, 23 Jun 2016 03:11:37 +0000",RE: [VOTE] Release Apache Spark 2.0.0 (RC1),Reynold Xin <rxin@databricks.com>,"Here is the fix https://github.com/apache/spark/pull/13868
From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Wednesday, June 22, 2016 6:43 PM
To: Ulanov, Alexander <alexander.ulanov@hpe.com>
Cc: Mark Hamstra <mark@clearstorydata.com>; Marcelo Vanzin <vanzin@cloudera.com>; dev@spark.apache.org
Subject: Re: [VOTE] Release Apache Spark 2.0.0 (RC1)

Alex - if you have access to a windows box, can you fix the issue? I'm not sure how many Spark contributors have windows boxes.


On Wed, Jun 22, 2016 at 5:56 PM, Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
Spark Unit tests fail on Windows in Spark 2.0. It can be considered as blocker since there are people that develop for Spark on Windows. The referenced issue is indeed Minor and has nothing to do with unit tests.

From: Mark Hamstra [mailto:mark@clearstorydata.com<mailto:mark@clearstorydata.com>]
Sent: Wednesday, June 22, 2016 4:09 PM
To: Marcelo Vanzin <vanzin@cloudera.com<mailto:vanzin@cloudera.com>>
Cc: Ulanov, Alexander <alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>>; Reynold Xin <rxin@databricks.com<maiv@spark.apache.org>
Subject: Re: [VOTE] Release Apache Spark 2.0.0 (RC1)

It's also marked as Minor, not Blocker.

On Wed, Jun 22, 2016 at 4:07 PM, Marcelo Vanzin <va Wed, Jun 22, 2016 at 4:04 PM, Ulanov, Alexander
<alexander.ulanov@hpe.com<mailto:alexander.ulanov@hpe.com>> wrote:
> -1
>
> Spark Unit tests fail on Windows. Still not resolved, though marked as
> resolved.

To be pedantic, it's marked as a duplicate
(https://issues.apache.org/jira/browse/SPARK-15899), which doesn't
mean necessarily that it's fixed.



> https://issues.apache.org/jira/browse/SPARK-15893
>
> From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
> Sent: Tuesday, June 21, 2016 6:27 PM
> To: dev@spark.apache.org<mailto:dev@spark.apache.org>
> Subject: [VOTE] Release Apache Spark 2.0.0 (RC1)
>
>
>
> Please vote on releasing the following candidate as Apache Spark version
> 2.0.0. The vote is open until Friday, June 24, 2016 at 19:00 PDT and passes
> if a majority of at least 3+1 PMC votes are cast.
>
>
>
> [ ] +1 Release this package as Apache Spark 2.0.0
>
> [ ] -1 Do not release this package because ...
>
>
>
>
>
> The tag to be voted on is v2.0.0-rc1
> (0c66ca41afade6db73c9aeddd5aed6e5dcea90df).
>
>
>
> This release candidate resolves ~2400 issues:
> https://s.apache.org/spark-2.0.0-rc1-jira
>
>
>
> The release files, including signatures, digests, etc. can be found at:
>
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-bin/
>
>
>
> Release artifacts are signed with the following key:
>
> https://people.apache.org/keys/committer/pwendell.asc
>
>
>
> The staging repository for this release can be found at:
>
> https://repository.apache.org/content/repositories/orgapachespark-1187/
>
>
>
> The documentation corresponding to this release can be found at:
>
> http://people.apache.org/~pwendell/spark-releases/spark-2.0.0-rc1-docs/
>
>
>
>
>
> =======================================
>
> == How can I help test this release? ==
>
> =======================================
>
> If you are a Spark user, you can help us test this release by taking an
> existing Spark workload and running on this release candidate, then
> reporting any regressions from 1.x.
>
>
>
> ================================================
>
> == What justifies a -1 vote for this release? ==
>
> ================================================
>
> Critical bugs impacting major functionalities.
>
>
>
> Bugs already present in 1.x, missing features, or bugs related to new
> features will not necessarily block this release. Note that historically
> Spark documentation has been published on the website separately from the
> main release so we do not need to block the release due to documentation
> errors either.
>
>
>
>

--
Marcelo

---------------------------------------------------------------------
To unsubscribe, e-mail: dev-unsubscribe@spark.apache.org<mailto:dev-unsubscribe@spark.apache.org>
For additional commands, e-mail: dev-help@spark.apache.org<mailto:dev-help@spark.apache.org>


"
Mark Hamstra <mark@clearstorydata.com>,"Wed, 22 Jun 2016 21:27:30 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""Ulanov, Alexander"" <alexander.ulanov@hpe.com>","No, that isn't necessarily enough to be considered a blocker.  A blocker
would be something that would have large negative effects on a significant
number of people trying to run Spark.  Arguably, something that prevents a
minority of Spark developers from running unit tests on one OS does not
qualify.  That's not to say that we shouldn't fix this, but only that it
needn't block a 2.0.0 release.


"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Jun 2016 08:51:37 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","First pass of feedback on the RC: all the sigs, hashes, etc are fine.
Licensing is up to date to the best of my knowledge.

I'm hitting test failures, some of which may be spurious. Just putting
them out there to see if they ring bells. This is Java 8 on Ubuntu 16.


- spilling with compression *** FAILED ***
  java.lang.Exception: Test failed with compression using codec
org.apache.spark.io.SnappyCompressionCodec:
assertion failed: expected cogroup to spill, but did not
  at scala.Predef$.assert(Predef.scala:170)
  at org.apache.spark.TestUtils$.assertSpilled(TestUtils.scala:170)
...

I feel like I've seen this before, and see some possibly relevant
fixes, but they're in 2.0.0 already:
https://github.com/apache/spark/pull/10990
Is this something where a native library needs to be installed or something?


- to UTC timestamp *** FAILED ***
  ""2016-03-13 [02]:00:00.0"" did not equal ""2016-03-13 [10]:00:00.0""
(DateTimeUtilsSuite.scala:506)

I know, we talked about this for the 1.6.2 RC, but I reproduced this
locally too. I will investigate, could still be spurious.


StateStoreSuite:
- maintenance *** FAILED ***
  The code passed to eventually never returned normally. Attempted 627
times over 10.000180116 seconds. Last failure message:
StateStoreSuite.this.fileExists(provider, 1L, false) was true earliest
file not deleted. (StateStoreSuite.scala:395)

No idea.


- offset recovery *** FAILED ***
  The code passed to eventually never returned normally. Attempted 197
times over 10.040864806 seconds. Last failure message:
strings.forall({
    ((x$1: Any) => DirectKafkaStreamSuite.collectedData.contains(x$1))
  }) was false. (DirectKafkaStreamSuite.scala:250)

Also something that was possibly fixed already for 2.0.0 and that I
just back-ported into 1.6. Could be just a very similar failure.


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Thu, 23 Jun 2016 09:25:10 +0100",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Sean Owen <sowen@cloudera.com>,"I'm also seeing some of these same failures:

- spilling with compression *** FAILED ***
I have seen this occassionaly

- to UTC timestamp *** FAILED ***
This was fixed yesterday in branch-2.0 (
https://issues.apache.org/jira/browse/SPARK-16078)

- offset recovery *** FAILED ***
Haven't seen this for a while and thought the flaky test was fixed but it
popped up again in one of our builds.

StateStoreSuite:
- maintenance *** FAILED ***
Just seen this has been failing for last 2 days on one build machine (linux
amd64)


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Thu, 23 Jun 2016 17:51:52 +0530",Spark Thrift Server Concurrency,"Spark dev list <dev@spark.apache.org>, user <user@spark.apache.org>","Hi All,

query execution time for some queries are less than a second and some are
more than 2seconds. The Spark Thrift Server logs shows all 20 queries are
submitted at same time 16/06/23 12:12:01 but the result schema are at
different times.

16/06/23 12:12:01 INFO SparkExecuteStatementOperation: Running query
'select distinct val2 from philips1 where key>=1000 and key<=1500

16/06/23 12:12:*02* INFO SparkExecuteStatementOperation: Result Schema:
ArrayBuffer(val2#2110)
16/06/23 12:12:*03* INFO SparkExecuteStatementOperation: Result Schema:
ArrayBuffer(val2#2182)
16/06/23 12:12:*04* INFO SparkExecuteStatementOperation: Result Schema:
ArrayBuffer(val2#2344)
16/06/23 12:12:*05* INFO SparkExecuteStatementOperation: Result Schema:
ArrayBuffer(val2#2362)

There are sufficient executors running on YARN. The concurrency is affected
by Single Driver. How to improve the concurrency and what are the best
practices.

Thanks,
Prabhu Joseph
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Thu, 23 Jun 2016 14:45:31 +0200",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","-1

I need SPARK-13283 to be solved.

Regards,
Maciek Bryński

2016-06-23 0:13 GMT+02:00 Krishna Sankar <ksankar42@gmail.com>:

"") OK
===============
===============
========================
========================


-- 
Maciek Bryński
"
Sean Owen <sowen@cloudera.com>,"Thu, 23 Jun 2016 13:50:05 +0100",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"I don't think that qualifies as a blocker; not even clear it's a
regression. Even non-binding votes here should focus on whether this
is OK to release as a maintenance update to 1.6.1.

ote:

---------------------------------------------------------------------


"
vaquar khan <vaquar.khan@gmail.com>,"Thu, 23 Jun 2016 08:00:59 -0500",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),Sean Owen <sowen@cloudera.com>,"+1 (non-binding

Regards,
Vaquar khan

"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Jun 2016 09:53:53 -0700",Re: [VOTE] Release Apache Spark 1.6.2 (RC2),=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Maciej let's fix SPARK-13283. It won't block 1.6.2 though.

ote:

)
R
'"") OK
:
n
================
================
=========================
=========================
t
"
Reynold Xin <rxin@databricks.com>,"Thu, 23 Jun 2016 10:00:13 -0700",Re: [VOTE][RESULT] Release Apache Spark 1.6.2 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Vote passed. Please see below. I will work on packaging the release.

+1 (9 votes, 4 binding)
Reynold Xin*
Sean Owen*
Tim Hunter
Michael Armbrust*
Sean McNamara*
Kousuke Saruta
Sameer Agarwal
Krishna Sankar
Vaquar Khan

0
none

-1
Maciej Bryński


* binding votes



===============
===============
========================
========================
"
Michael Segel <msegel_hadoop@hotmail.com>,"Thu, 23 Jun 2016 10:42:25 -0700",Re: Spark Thrift Server Concurrency,Prabhu Joseph <prabhujose.gates@gmail.com>,"Hi, 
There are  a lot of moving parts and a lot of unknowns from your description. 
Besides the version stuff. 

How many executors, how many cores? How much memory? 
Are you persisting (memory and disk) or just caching (memory) 

During the execution… same tables… are  you seeing a lot of shuffling of data for some queries and not others? 

It sounds like an interesting problem… 

the query execution time for some queries are less than a second and some are more than 2seconds. The Spark Thrift Server logs shows all 20 queries are submitted at same time 16/06/23 12:12:01 but the result schema are at different times.
'select distinct val2 from philips1 where key>=1000 and key<=1500
ArrayBuffer(val2#2110)
ArrayBuffer(val2#2182)
ArrayBuffer(val2#2344)
ArrayBuffer(val2#2362)
affected by Single Driver. How to improve the concurrency and what are the best practices.

"
Jacek Laskowski <jacek@japila.pl>,"Thu, 23 Jun 2016 23:41:54 +0200","Does CoarseGrainedSchedulerBackend care about cores only? And
 disregards memory?",dev <dev@spark.apache.org>,"Hi,

After reviewing makeOffer and launchTasks in
CoarseGrainedSchedulerBackend I came to the following conclusion:

Scheduling in Spark relies on cores only (not memory), i.e. the number
of tasks Spark can run on an executor is constrained by the number of
cores available only. When submitting Spark application for execution
both -- memory and cores -- can be specified explicitly.

Would you agree? Do I miss anything important?

I was very surprised when I found it out as I thought that memory
would also have been a limiting factor.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Krishna <research800@gmail.com>,"Thu, 23 Jun 2016 14:59:48 -0700",destroyPythonWorker job in PySpark,"dev@spark.apache.org, user <user@spark.apache.org>","Hi,

I am running a PySpark app with 1000's of cores (partitions is a small
multiple of # of cores) and the overall application performance is fine.
However, I noticed that, at the end of the job, PySpark initiates job
clean-up procedures and as part of this procedure, PySpark executes a job
shown in the Web UI as ""runJob at PythonRDD.scala:361"" for each
executor/core. The pain point is that, this step is running in a sequential
fashion and it has become the bottleneck in our application. Even though
each job takes only 0.5 sec (on average), it adds up when running with
1000's of executors.

Looking into the code for ""destroyPythonWorker"" in ""SparkEnv.scala"", is
this behavior the result of ""stopWorker"" being executed sequentially within
foreach? Let me know if I'm missing something and what can be done to fix
the issue.

  private[spark]



Spark version: 1.5.0-cdh5.5.1

Thanks
"
Yin Huai <yhuai@databricks.com>,"Thu, 23 Jun 2016 21:59:19 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Pete Robbins <robbinspg@gmail.com>,"-1 because of https://issues.apache.org/jira/browse/SPARK-16121.

This jira was resolved after 2.0.0-RC1 was cut. Without the fix, Spark
SQL effectively only uses the driver to list files when loading datasets
and the driver-side file listing is very slow"
Darshan Singh <darshan.meel@gmail.com>,"Fri, 24 Jun 2016 08:11:00 +0100",Re: Partitioning in spark,ayan guha <guha.ayan@gmail.com>,"Thanks but the whole point is not setting it explicitly but it should be
derived from its parent RDDS.

Thanks


"
Simon Scott <Simon.Scott@viavisolutions.com>,"Fri, 24 Jun 2016 08:23:17 +0000",Associating user objects with SparkContext/SparkStreamingContext,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

I am developing a streaming application using checkpointing on Spark 1.5.1

I have just run into a NotSerializableException because some of the state that my streaming functions need cannot be serialized. This state is only used in the driver process, it is the checkpointing that requires the serialization.

So I am considering moving that state into a Scala ""object"" - i.e. global singleton that must be mutable to allow the state to be set at application start.

I would prefer to be able to create immutable state and attach it to either the SparkContext or SparkStreamingContext but I can't find an api for that.

Does anybody else think is a good idea? Is there a better way? Or would such an api be a useful enhancement to Spark?

Thanks in advance
Simon

Research Developer
Viavi Solutions
"
ElfoLiNk <gazza.matte@gmail.com>,"Fri, 24 Jun 2016 03:13:14 -0700 (MST)",HistoryServer: missing information,dev@spark.apache.org,"In Spark 1.6.1 HistoryServer doesn't show coresGranted , coresPerExecutor ,
memoryPerExecutorMB for the applications, is this normal?

Example:

*http://MasterIP:8080/api/v1/applications*
{
  ""id"" : ""app-20160623171554-0006"",
  ""name"" : ""TeraSort"",
  ""coresGranted"" : 20,
  ""maxCores"" : 20,
  ""coresPerExecutor"" : 10,
  ""memoryPerExecutorMB"" : 1024,
  ""attempts"" : [ {
    ""startTime"" : ""2016-06-23T15:15:54.477GMT"",
    ""endTime"" : ""2016-06-23T15:20:42.976GMT"",
    ""sparkUser"" : ""xxx"",
    ""completed"" : true
  } ]

*http://HistoryServerIP:18080/api/v1/applications*
 {
  ""id"" : ""app-20160623171554-0006"",
  ""name"" : ""TeraSort"",
  ""attempts"" : [ {
    ""startTime"" : ""2016-06-23T15:15:49.212GMT"",
    ""endTime"" : ""2016-06-23T15:20:42.863GMT"",
    ""sparkUser"" : ""xxx"",
    ""completed"" : true
  } ]

Notice also the different startTime 





--

---------------------------------------------------------------------


"
Nick Pentreath <nick.pentreath@gmail.com>,"Fri, 24 Jun 2016 11:37:55 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","I'm getting the following when trying to run ./dev/run-tests (not happening
on master) from the extracted source tar. Anyone else seeing this?

error: Could not access 'fc0a1475ef'
**********************************************************************
File ""./dev/run-tests.py"", line 69, in
__main__.identify_changed_files_from_git_commits
Failed example:
    [x.name for x in determine_modules_for_files(
identify_changed_files_from_git_commits(""fc0a1475ef"",
target_ref=""5da21f07""))]
Exception raised:
    Traceback (most recent call last):
      File ""/Users/nick/miniconda2/lib/python2.7/doctest.py"", line 1315, in
__run
        compileflags, 1) in test.globs
      File ""<doctest __main__.identify_changed_files_from_git_commits[0]>"",
line 1, in <module>
        [x.name for x in determine_modules_for_files(
identify_changed_files_from_git_commits(""fc0a1475ef"",
target_ref=""5da21f07""))]
      File ""./dev/run-tests.py"", line 86, in
identify_changed_files_from_git_commits
        universal_newlines=True)
      File ""/Users/nick/miniconda2/lib/python2.7/subprocess.py"", line 573,
in check_output
        raise CalledProcessError(retcode, cmd, output=output)
    CalledProcessError: Command '['git', 'diff', '--name-only',
'fc0a1475ef', '5da21f07']' returned non-zero exit status 1
error: Could not access '50a0496a43'
**********************************************************************
File ""./dev/run-tests.py"", line 71, in
__main__.identify_changed_files_from_git_commits
Failed example:
    'root' in [x.name for x in determine_modules_for_files(
 identify_changed_files_from_git_commits(""50a0496a43"",
target_ref=""6765ef9""))]
Exception raised:
    Traceback (most recent call last):
      File ""/Users/nick/miniconda2/lib/python2.7/doctest.py"", line 1315, in
__run
        compileflags, 1) in test.globs
      File ""<doctest __main__.identify_changed_files_from_git_commits[1]>"",
line 1, in <module>
        'root' in [x.name for x in determine_modules_for_files(
 identify_changed_files_from_git_commits(""50a0496a43"",
target_ref=""6765ef9""))]
      File ""./dev/run-tests.py"", line 86, in
identify_changed_files_from_git_commits
        universal_newlines=True)
      File ""/Users/nick/miniconda2/lib/python2.7/subprocess.py"", line 573,
in check_output
        raise CalledProcessError(retcode, cmd, output=output)
    CalledProcessError: Command '['git', 'diff', '--name-only',
'50a0496a43', '6765ef9']' returned non-zero exit status 1
**********************************************************************
1 items had failures:
   2 of   2 in __main__.identify_changed_files_from_git_commits
***Test Failed*** 2 failures.




"
Evan Sparks <evan.sparks@gmail.com>,"Fri, 24 Jun 2016 08:12:18 -0700",Re: Associating user objects with SparkContext/SparkStreamingContext,Simon Scott <Simon.Scott@viavisolutions.com>,"I would actually think about this the other way around. Move the functions you are passing to the streaming jobs out to their own object if possible. Spark's closure capture rules are necessarily far reaching and serialize the object that contains these methods, which is a common cause of the problem you're seeing. 

Another option is to mark the non-serializable state as ""@transient"" if it is never accessed by the worker processes. 

rote:

hat my streaming functions need cannot be serialized. This state is only used in the driver process, it is the checkpointing that requires the serialization.
 – i.e. global singleton that must be mutable to allow the state to be set at application start.
r the SparkContext or SparkStreamingContext but I can’t find an api for that.
ch an api be a useful enhancement to Spark?
"
joshuata <joshasplund@gmail.com>,"Fri, 24 Jun 2016 10:55:25 -0700 (MST)",Re: Jar for Spark developement,dev@spark.apache.org,"With regards to the Spark JAR files, I have had really good success with the 
sbt plugin <https://github.com/databricks/sbt-spark-package>  . You can set
the desired spark version along with any plugins, and it will automatically
fetch your dependencies and put them on the classpath. 



--

---------------------------------------------------------------------


"
Matt Cheah <mcheah@palantir.com>,"Fri, 24 Jun 2016 18:52:13 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"Nick Pentreath <nick.pentreath@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","-1 because of SPARK-16181 which is a correctness regression from 1.6. Looks like the patch is ready though: https://github.com/apache/spark/pull/13884 – it would be ideal for this patch to make it into the release.

-Matt Cheah

From: Nick Pentreath"
=?UTF-8?B?5q6155+z55+z?= <burness1990@gmail.com>,"Sun, 26 Jun 2016 12:16:57 +0800",MinMaxScaler With features include category variables,dev@spark.apache.org,"Hi all:


    I use the MinMaxScaler for data normalization, but I found the the api
is only for Vector, we must vectorized the features firtst. However, the
feature usually include two parts: one is need to be Normalization, another
should not be normalized such as categorical. I want to add a api with the
DataFrame which aim to normalize the columns which we want to normalize.
And then we can make it to be vector and sent to the ML model api to train.
I think that will be very useful for the developer with machine learning.



Best Regards

Thanks
"
Jacek Laskowski <jacek@japila.pl>,"Sun, 26 Jun 2016 14:58:49 +0200","Using SHUFFLE_SERVICE_ENABLED for MesosCoarseGrainedSchedulerBackend,
 BlockManager, and Utils?",dev <dev@spark.apache.org>,"Hi,

I've just noticed that there is the private[spark] val
SHUFFLE_SERVICE_ENABLED in package object config [1]

[1] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/internal/config/package.scala#L74-L75

However MesosCoarseGrainedSchedulerBackend [2], BlockManager [3] and
Utils [4] are all using their own copies.

Would that be acceptable* to send a pull request to get rid of this redundancy?

[*] I'm staring at @srowen for his nodding in agreement :-)

[2] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/cluster/mesos/MesosCoarseGrainedSchedulerBackend.scala#L71
[3] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/BlockManager.scala#L73-L74
[4] https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/Utils.scala#L748

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Randy Gelhausen <rgelhau@gmail.com>,"Sun, 26 Jun 2016 18:34:22 -0400",Spark 1.6.1: Unexpected partition behavior?,"user@spark.apache.org, dev@spark.apache.org","<code>
val enriched_web_logs = sqlContext.sql(""""""
select web_logs.datetime, web_logs.node as app_host, source_ip, b.node as
source_host, log
from web_logs
left outer join (select distinct node, address from nodes) b on source_ip =
address
"""""")
enriched_web_logs.coalesce(1).write.format(""parquet"").mode(""overwrite"").save(bucket+""derived/enriched_web_logs"")
enriched_web_logs.registerTempTable(""enriched_web_logs"")
sqlContext.cacheTable(""enriched_web_logs"")
</code>

There are only 524 records in the resulting table, and I have explicitly
attempted to coalesce into 1 partition.

Yet my Spark UI shows 200 (mostly empty) partitions:
RDD NameStorage LevelCached PartitionsFraction CachedSize in MemorySize in
ExternalBlockStoreSize on Disk
In-memory table enriched_web_logs
<http://localhost:4040/storage/rdd?id=86> Memory
Deserialized 1x Replicated 200 100% 22.0 KB 0.0 B 0.0 BWhy would there be
200 partitions despite the coalesce call?
"
Randy Gelhausen <rgelhau@gmail.com>,"Sun, 26 Jun 2016 18:38:19 -0400",Re: Spark 1.6.1: Unexpected partition behavior?,"user@spark.apache.org, dev@spark.apache.org","Sorry, please ignore the above.

I now see I called coalesce on a different reference, than I used to
register the table.


"
Prabhu Joseph <prabhujose.gates@gmail.com>,"Mon, 27 Jun 2016 11:30:49 +0530",Re: Spark Thrift Server Concurrency,Michael Segel <msegel_hadoop@hotmail.com>,"Spark Thrift Server is started with

./sbin/start-thriftserver.sh --master yarn-client --hiveconf
hive.server2.thrift.port=10001 --num-executors 4 --executor-cores 2
--executor-memory 4G --conf spark.scheduler.mode=FAIR

20 parallel below queries are executed

select distinct val2 from philips1 where key>=1000 and key<=1500

And there is no issue at the backend Spark Executors, as spark jobs UI
shows all 20 queries are launched and completed with same duration. And all
20 queries are received by Spark Thrift Server at same time. But the Spark
Driver present inside Spark Thrift Sever  looks like overloaded and hence
the queries are not parsed and
submitted to executors at same time and hence seeing the delay in query
execution time from client.






of shuffling of
e
"
Simon Scott <Simon.Scott@viavisolutions.com>,"Mon, 27 Jun 2016 07:48:46 +0000",RE: Associating user objects with SparkContext/SparkStreamingContext,Evan Sparks <evan.sparks@gmail.com>,"“move the functions you are passing” yes this is what I had already done – and what I hope to avoid

Thank you however for the reminder about @transient – with that I am able to create a function value that includes the non-serializable state as a @transient val. Which at least packages the solution closer to the code that causes the problem.

Cheers
Simon

From: Evan Sparks [mailto:evan.sparks@gmail.com]
Sent: 24 June 2016 16:12
To: Simon Scott <Simon.Scott@viavisolutions.com>
Cc: dev@spark.apache.org
Subject: Re: Associating user objects with SparkContext/SparkStreamingContext

I would actually think about this the other way around. Move the functions you are passing to the streaming jobs out to their own object if possible. Spark's closure capture rules are necessarily far reaching and serialize the object that contains these methods, which is a common cause of the problem you're seeing.

Another option is to mark the non-serializable state as ""@transient"" if it is never accessed by the worker processes.

On Jun 24, 2016, at 1:23 AM, Simon Scott <Simon.Scott@viavisolutions.com<mailto:Simon.Scott@viavisolutions.com>> wrote:
Hi,

I am developing a streaming application using checkpointing on Spark 1.5.1

I have just run into a NotSerializableException because some of the state that my streaming functions need cannot be serialized. This state is only used in the driver process, it is the checkpointing that requires the serialization.

So I am considering moving that state into a Scala “object” – i.e. global singleton that must be mutable to allow the state to be set at application start.

I would prefer to be able to create immutable state and attach it to either the SparkContext or SparkStreamingContext but I can’t find an api for that.

Does anybody else think is a good idea? Is there a better way? Or would such an api be a useful enhancement to Spark?

Thanks in advance
Simon

Research Developer
Viavi Solutions
"
Sean Owen <sowen@cloudera.com>,"Mon, 27 Jun 2016 09:19:59 +0100","Re: Using SHUFFLE_SERVICE_ENABLED for MesosCoarseGrainedSchedulerBackend,
 BlockManager, and Utils?",Jacek Laskowski <jacek@japila.pl>,"That seems OK. If it introduces another module dependency we'd have to
think about it. I assume these constants should really be used
consistently everywhere if possible, just because it otherwise means
duplicating the defaults and possibly incorrectly. I think you could
have a look at that more broadly too.


---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 27 Jun 2016 13:33:37 +0000",Please add an unsubscribe link to the footer of user list email,Spark dev list <dev@spark.apache.org>,"Howdy,

It seems like every week we have at least a couple of people emailing the
user list in vain with ""Unsubscribe"" in the subject, the body, or both.

I remember a while back that every email on the user list used to include a
footer with a quick link to unsubscribe. It was removed, I believe, because
someone thought it was unnecessary clutter.

I disagree.

Please add that footer back in. It's a tiny cost to pay for giving users
the convenience of easily unsubscribing; it makes it much less likely that
people will mistakenly spam everybody as they have been; and it follows
modern email list conventions. The link can just be a ""mailto:..."" link
that does the same thing that you have to do today to unsubscribe.

I don't think we need to do the same thing on the dev list since the volume
of email is much lower, and since people on average here are probably more
familiar with the mechanics of subscribing to and unsubscribing from Apache
mailing lists.

Nick
"
linxi zeng <linxizeng0615@gmail.com>,"Mon, 27 Jun 2016 23:30:44 +0800",run spark sql with script transformation faild,"user@spark.apache.org, dev@spark.apache.org","Hi, all:
    Recently, we are trying to compare with spark sql and hive on MR, and I
have tried to run spark (spark1.6 rc2) sql with script transformation, the
spark job faild and get an error message like:

16/06/26 11:01:28 INFO codegen.GenerateUnsafeProjection: Code
generated in 19.054534 ms

16/06/26 11:01:28 ERROR execution.ScriptTransformationWriterThread:
/bin/bash: test.py: command not found



16/06/26 11:01:28 ERROR util.Utils: Uncaught exception in thread
Thread-ScriptTransformation-Feed

java.io.IOException: Stream closed

	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)

	at java.io.OutputStream.write(OutputStream.java:116)

	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)

	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)

	at java.io.DataOutputStream.write(DataOutputStream.java:107)

	at org.apache.hadoop.hive.ql.exec.TextRecordWriter.write(TextRecordWriter.java:53)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(ScriptTransformation.scala:277)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(ScriptTransformation.scala:255)

	at scala.collection.Iterator$class.foreach(Iterator.scala:727)

	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply$mcV$sp(ScriptTransformation.scala:255)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:244)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:244)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread.run(ScriptTransformation.scala:244)

16/06/26 11:01:28 ERROR util.SparkUncaughtExceptionHandler: Uncaught
exception in thread Thread[Thread-ScriptTransformation-Feed,5,main]

java.io.IOException: Stream closed

	at java.lang.ProcessBuilder$NullOutputStream.write(ProcessBuilder.java:434)

	at java.io.OutputStream.write(OutputStream.java:116)

	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)

	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)

	at java.io.DataOutputStream.write(DataOutputStream.java:107)

	at org.apache.hadoop.hive.ql.exec.TextRecordWriter.write(TextRecordWriter.java:53)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(ScriptTransformation.scala:277)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1$$anonfun$apply$mcV$sp$1.apply(ScriptTransformation.scala:255)

	at scala.collection.Iterator$class.foreach(Iterator.scala:727)

	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply$mcV$sp(ScriptTransformation.scala:255)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:244)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread$$anonfun$run$1.apply(ScriptTransformation.scala:244)

	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)

	at org.apache.spark.sql.hive.execution.ScriptTransformationWriterThread.run(ScriptTransformation.scala:244)


cmd is:



the sql and python script is:
transform.sql (which was executed successfully on hive) :


test.py:



And after making two modifications:
(1) chmod +x test.py
(2) transform.sql：using 'test.py'  ->  using './test.py'
the sql executed successfully.
I was wonder that if the spark sql with script transformation should be run
like this way? Any one meet the same problem?
"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jun 2016 10:02:28 -0700",Re: Please add an unsubscribe link to the footer of user list email,Nicholas Chammas <nicholas.chammas@gmail.com>,"Let me look into this...


"
Jacek Laskowski <jacek@japila.pl>,"Mon, 27 Jun 2016 18:14:01 +0100","Re: Using SHUFFLE_SERVICE_ENABLED for MesosCoarseGrainedSchedulerBackend,
 BlockManager, and Utils?",Sean Owen <sowen@cloudera.com>,"Thanks Sean. I'm going to create a JIRA for it and start the work under it.

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
Egor Pahomov <pahomov.egor@gmail.com>,"Mon, 27 Jun 2016 11:13:31 -0700",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),Matt Cheah <mcheah@palantir.com>,"-1 : SPARK-16228 [SQL]  - ""Percentile"" needs explicit cast to double,
otherwise it throws an error. I can not move my existing 100500 quires to
2.0 transparently.

2016-06-24 11:52 GMT-07:00 Matt Cheah <mcheah@palantir.com>:

r this
VbvX3nU3OSDadUnJxjAs&m"
Koert Kuipers <koert@tresata.com>,"Mon, 27 Jun 2016 15:22:25 -0400",SPARK-15982 breaks external DataSources,"""dev@spark.apache.org"" <dev@spark.apache.org>","hey,

since SPARK-15982 was fixed (https://github.com/apache/spark/pull/13727) i
believe all external DataSources that rely on using .load(path) without
being a FileFormat themselves are broken.

i noticed this because our unit tests for the elasticsearch datasource
broke.

i commented on the pullreq with the specific issue i am facing.

best,
koert
"
Luciano Resende <luckbr1975@gmail.com>,"Mon, 27 Jun 2016 16:14:12 -0700",Spark streaming connectors available for testing,dev <dev@spark.apache.org>,"The Apache Bahir project is voting a release based on Spark 2.0.0-preview.
https://www.mail-archive.com/dev@bahir.apache.org/msg00085.html

It currently provides the following Apache Spark Streaming connectors:

    streaming-akka
    streaming-mqtt
    streaming-twitter
    streaming-zeromq

While we are continuing to work towards a release to support Spark 2.0.0,
we appreciate your help around testing the release and the current Spark
Streaming connectors.

To add the connectors to your scala application, the best way is to build
the source of Bahir with 'mvn clean install' which will make the necessary
dependencies available in your local maven repository and will enable you
to reference the connectors in your application and also submit  your
application to a local Spark test environment utilizing --packages.

Build:
mvn clean install

Add repository to your scala application (build.sbt):
resolvers += ""Local Maven Repository"" at ""file://"" +
Path.userHome.absolutePath + ""/.m2/repository""

Submit your application to a local Spark test environment:
bin/spark-submit --master spark://127.0.0.1:7077 --packages
org.apache.bahir:spark-streaming-akka_2.11:2.0.0-preview --class
org.apache.spark.examples.streaming.akka.ActorWordCount
~/opensource/apache/bahir/streaming-akka-examples/target/scala-2.11/streaming-akka-examples_2.11-1.0.jar
localhost 9999


The Bahir community welcomes questions, comments, bug reports and all your
feedback.

http://bahir.apache.org/community/

Thanks
"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jun 2016 16:31:36 -0700",Re: SPARK-15982 breaks external DataSources,Koert Kuipers <koert@tresata.com>,"Yup this is bad. Can you create a JIRA ticket too?



"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jun 2016 18:59:46 -0700",[ANNOUNCE] Announcing Spark 1.6.2,"""dev@spark.apache.org"" <dev@spark.apache.org>, user <user@spark.apache.org>","We are happy to announce the availability of Spark 1.6.2! This maintenance
release includes fixes across several areas of Spark. You can find the list
of changes here: https://s.apache.org/spark-1.6.2

And download the release here: http://spark.apache.org/downloads.html
"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jun 2016 19:04:40 -0700",Re: Please add an unsubscribe link to the footer of user list email,Nicholas Chammas <nicholas.chammas@gmail.com>,"Filed infra ticket: https://issues.apache.org/jira/browse/INFRA-12185




"
Reynold Xin <rxin@databricks.com>,"Mon, 27 Jun 2016 22:13:21 -0700",Re: Please add an unsubscribe link to the footer of user list email,Nicholas Chammas <nicholas.chammas@gmail.com>,"If people want this to happen, please go comment on the INFRA ticket:

https://issues.apache.org/jira/browse/INFRA-12185

Otherwise it will probably be dropped.



"
Nick Pentreath <nick.pentreath@gmail.com>,"Tue, 28 Jun 2016 09:28:52 +0000",Re: [VOTE] Release Apache Spark 2.0.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","I take it there will be another RC due to some blockers and as there were
no +1 votes anyway.

FWIW, I cannot run python tests using ""./python/run-tests"".

I'd be -1 for this reason (see https://github.com/apache/spark/pull/13737 /
http://issues.apache.org/jira/browse/SPARK-15954) - does anyone else
encounter this?

./python/run-tests --python-executables=python2.7
Running PySpark tests. Output is in
/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/unit-tests.log
Will test against the following Python executables: ['python2.7']
Will test the following Python modules: ['pyspark-core', 'pyspark-ml',
'pyspark-mllib', 'pyspark-sql', 'pyspark-streaming']
....Using Spark's default log4j profile:
org/apache/spark/log4j-defaults.properties
Setting default log level to ""WARN"".
To adjust logging level use sc.setLogLevel(newLevel).
======================================================================
ERROR: setUpClass (pyspark.sql.tests.HiveContextSQLTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/pyspark/sql/tests.py"",
line 1620, in setUpClass
    cls.spark = HiveContext._createForTesting(cls.sc)
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/pyspark/sql/context.py"",
line 490, in _createForTesting
    jtestHive =
sparkContext._jvm.org.apache.spark.sql.hive.test.TestHiveContext(jsc)
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/java_gateway.py"",
line 1183, in __call__
    answer, self._gateway_client, None, self._fqn)
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/lib/py4j-0.10.1-src.zip/py4j/protocol.py"",
line 312, in get_return_value
    format(target_id, ""."", name), value)
Py4JJavaError: An error occurred while calling
None.org.apache.spark.sql.hive.test.TestHiveContext.
: java.lang.NullPointerException
at
org.apache.spark.sql.hive.test.TestHiveSparkSession.getHiveFile(TestHive.scala:183)
at
org.apache.spark.sql.hive.test.TestHiveSparkSession.<init>(TestHive.scala:214)
at
org.apache.spark.sql.hive.test.TestHiveSparkSession.<init>(TestHive.scala:122)
at org.apache.spark.sql.hive.test.TestHiveContext.<init>(TestHive.scala:77)
at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
at
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
at
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:240)
at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
at py4j.Gateway.invoke(Gateway.java:236)
at
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
at py4j.GatewayConnection.run(GatewayConnection.java:211)
at java.lang.Thread.run(Thread.java:745)


======================================================================
ERROR: setUpClass (pyspark.sql.tests.SQLTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/pyspark/sql/tests.py"",
line 189, in setUpClass
    ReusedPySparkTestCase.setUpClass()
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/pyspark/tests.py"",
line 344, in setUpClass
    cls.sc = SparkContext('local[4]', cls.__name__)
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/pyspark/context.py"",
line 112, in __init__
    SparkContext._ensure_initialized(self, gateway=gateway)
  File
""/Users/nick/workspace/scala/spark-rcs/spark-2.0.0/python/pyspark/context.py"",
line 261, in _ensure_initialized
    callsite.function, callsite.file, callsite.linenum))
ValueError: Cannot run multiple SparkContexts at once; existing
SparkContext(app=ReusedPySparkTestCase, master=local[4]) created by
<module> at /Users/nick/miniconda2/lib/python2.7/runpy.py:72

----------------------------------------------------------------------
Ran 4 tests in 4.800s

FAILED (errors=2)

Had test failures in pyspark.sql.tests with python2.7; see logs.



or this
hVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
e>
hVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
hVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
e>
hVbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wx5Qjw-efxMVvKXnjUsSkkQcEF6zQHQLQaGtAK9pxIw&e=>
rg_jira_browse_SPARK-2D16121&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=9200NP4SpeJSUNrSrlWWEC7vFvjWSyCHnx5LD7Sj9u4&e=>.
s
org_jira_browse_SPARK-2D16078&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBgK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=SuVdXUNGdAhYgtA2fMLe5vZ2PFrPOaeO3i3cbhYU4tc&e=>)
g
.
nJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=goarAptcJYfLg44f7BAwhbipqJlRFKz9Y6Z36HItiKg&e=>
ache_spark_pull_10990&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ4tFb6oXY8_yzfAosPOqqaV4wtMg6ZPwM&s=dFymYD9NRVHIJ5MKpmzPcH_NYwLjOWcZd7FUuQBpTUU&e=>
7
t
7
1))
d
spark-2D2.0.0-2Drc1-2Djira&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2UXBrZ42KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=ZD_PezvsJ1GyDhv7MhaeUrVba_uhED5mPkqKpfenKEE&e=>
/
org_-7Epwendell_spark-2Dreleases_spark-2D2.0.0-2Drc1-2Dbin_&d=DQMFaQ&c=3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=wSbzZ2LyuDcNKaCijEPdt9rokQ0R9w66tn2jMfjKN2I&e=>
.org_keys_committer_pwendell.asc&d=DQMFaQ&c=izlc9mHr637UR4lpLEZLFFS3Vn2vw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=i1Uxw1NyUf2iuA3CXbyiEODD1RR24rAXUvkc42ut8Ao&e=>
7/
ache.org_content_repositories_orgapachespark-2D1187_&d=DQMFaQ&c=izlc9mHadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=QjsvnxXe6JBQqXwKw6r-fIIHI9E0ugeeICAqjRXRNwc&e=>
s/
org_-7Epwendell_spark-2Dreleases_spark-2D2.0.0-2Drc1-2Ddocs_&d=DQMFaQ&cbvX3nU3OSDadUnJxjAs&m=Y3d-oJvw2gK_2KXYjXY8_yzfAosPOqqaV4wtMg6ZPwM&s=_6IZExLgc8WoxW0kft_weR7AvELgbFXnHZdezQ_IYGk&e=>
=================
=================
g
==========================
==========================
ew
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 28 Jun 2016 12:35:49 +0000",Re: Please add an unsubscribe link to the footer of user list email,Reynold Xin <rxin@databricks.com>,"
Before going ahead I would like to see a much broader consensus of the
changes being asked for. At least 3 +1s from PMC members would be ok.

Can some PMC members chime in there please?

Nick
​


both.
ikely
...""
.
"
pgrandjean <patgrdj@yahoo.fr>,"Tue, 28 Jun 2016 07:49:00 -0700 (MST)",UDTRegistration public,dev@spark.apache.org,"Hi, I have discovered class UDTRegistration on master. Will it be made
public? If yes, when and under which version?

https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/types/UDTRegistration.scala

Thanks!
Patrick.



--

---------------------------------------------------------------------


"
Jacek Laskowski <jacek@japila.pl>,"Tue, 28 Jun 2016 18:54:05 +0100",What's the meaning of Target Version/s in Spark's JIRA?,dev <dev@spark.apache.org>,"Hi,

While reviewing the release notes for 1.6.2 I stumbled upon
https://issues.apache.org/jira/browse/SPARK-13522. It's got Target
Version/s: 2.0.0 with Fix Version/s: 1.6.2, 2.0.0.

What's the meaning of Target Version/s in Spark?

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski

---------------------------------------------------------------------


"
Dongjoon Hyun <dongjoon@apache.org>,"Tue, 28 Jun 2016 10:58:17 -0700",Re: What's the meaning of Target Version/s in Spark's JIRA?,Jacek Laskowski <jacek@japila.pl>,"Hi,

1.6.2 is just the result of back-porting of that patch.

The patch was originally targeted and merged into 2.0.0.

Warmly,
Dongjoon.



"
Jacek Laskowski <jacek@japila.pl>,"Tue, 28 Jun 2016 19:03:04 +0100",Re: What's the meaning of Target Version/s in Spark's JIRA?,Dongjoon Hyun <dongjoon@apache.org>,"Hi,

That makes sense. Thanks Dongjoon for the very prompt response!

Pozdrawiam,
Jacek Laskowski
----
https://medium.com/@jaceklaskowski/
Mastering Apache Spark http://bit.ly/mastering-apache-spark
Follow me at https://twitter.com/jaceklaskowski



---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 28 Jun 2016 11:08:18 -0700","[build system] hanging procs on the jenkins master, emergency reboot","amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>","jenkins got itself in to another state and was killing the master.
while poking around, i noticed lots of sleeping processes that were
using a TON of cpu and had a bunch of log files open, but not writing
to them.  anyways, it looked like it needed a quick restart and that
seems to have fixed the problem.

i'll keep an eye on things for the rest of today, but we're looking better.

ah, java webapps.

shane

---------------------------------------------------------------------


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 28 Jun 2016 12:59:11 -0700","Structured Streaming Sink in 2.0 collect/foreach restrictions added
 in SPARK-16020","""dev@spark.apache.org"" <dev@spark.apache.org>","Looking at the Sink in 2.0 there is a warning (added in SPARK-16020 without
a lot of details) that says ""Note: You cannot apply any operators on `data`
except consuming it (e.g., `collect/foreach`)."" but I'm wondering if this
restriction is perhaps too broadly worded? Provided that we consume the
data in a blocking fashion could we apply some other transformation
beforehand? Or is there a better way to get equivalent foreachRDD
functionality with the structured streaming API?

on Datasets which are not supported for Streaming use (e.g. toJson etc.)?

Cheers,

Holden :)
-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Michael Allman <michael@videoamp.com>,"Tue, 28 Jun 2016 15:27:14 -0700",Spark SQL PR looking for love...,dev@spark.apache.org,"Hello,

Do any Spark SQL committers/experts have bandwidth to review a PR I submitted a week ago, https://github.com/apache/spark/pull/13818 <https://github.com/apache/spark/pull/13818>? The associated Jira ticket is https://issues.apache.org/jira/browse/SPARK-15968 <https://issues.apache.org/jira/browse/SPARK-15968>.

Thank you!

Michael"
Michael Allman <michael@videoamp.com>,"Tue, 28 Jun 2016 15:51:19 -0700",Re: Spark SQL PR looking for love...,dev@spark.apache.org,"I should briefly mention what the PR is about... This is a patch to address a problem where non-empty partitioned Hive metastore tables are never returned in a cache lookup in HiveMetastoreCatalog.getCached.

Thanks,

Michael


submitted a week ago, https://github.com/apache/spark/pull/13818 <https://github.com/apache/spark/pull/13818>? The associated Jira ticket is https://issues.apache.org/jira/browse/SPARK-15968 <https://issues.apache.org/jira/browse/SPARK-15968>.

"
=?UTF-8?B?5qWK6ZaU5a+M?= <tilumi0@gmail.com>,"Wed, 29 Jun 2016 08:00:28 +0800",Does schema merge on keys with different types is allowed?,dev@spark.apache.org,"The issue https://issues.apache.org/jira/browse/SPARK-15516 that is about
schema merging on keys with different types need to be clarified. Does
anyone know the behaviour of schema merging when encountering different
types?
"
Michael Armbrust <michael@databricks.com>,"Tue, 28 Jun 2016 18:14:00 -0700","Re: Structured Streaming Sink in 2.0 collect/foreach restrictions
 added in SPARK-16020",Holden Karau <holden@pigscanfly.ca>,"This is not too broadly worded, and in general I would caution that any
interface in org.apache.spark.sql.catalyst or
org.apache.spark.sql.execution is considered internal and likely to change
in between releases.  We do plan to open a stable source/sink API in a
future release.

The problem here is that the DataFrame is constructed using an
incrementalized physical query plan.  If you call any operations on the
Dataframe that change the logical plan, you will loose prior state and the
DataFrame will return an incorrect result.  Since this was discovered late
in the release process we decided it was better to document the current
behavior, rather than do a large refactoring.


"
Holden Karau <holden@pigscanfly.ca>,"Tue, 28 Jun 2016 18:30:56 -0700","Re: Structured Streaming Sink in 2.0 collect/foreach restrictions
 added in SPARK-16020",Michael Armbrust <michael@databricks.com>,"Ok, that makes sense (the JIRA where the restriction note was added didn't
have a lot of details). So for now, would converting to an RDD inside of a
custom Sink and then doing your operations on that be a reasonable work
around?



-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
"
Michael Armbrust <michael@databricks.com>,"Tue, 28 Jun 2016 18:35:22 -0700","Re: Structured Streaming Sink in 2.0 collect/foreach restrictions
 added in SPARK-16020",Holden Karau <holden@pigscanfly.ca>,"Yeah, turning it into an RDD should preserve the incremental planning.


"
Hyukjin Kwon <gurwls223@gmail.com>,"Wed, 29 Jun 2016 10:50:52 +0900",Re: Does schema merge on keys with different types is allowed?,=?UTF-8?B?5qWK6ZaU5a+M?= <tilumi0@gmail.com>,"I have tested that issue manually and looked into the codes before.

It seems it does not support to find a compatible type.

https://github.com/apache/spark/blob/b914e1930fd5c5f2808f92d4958ec6fbeddf2e30/sql/catalyst/src/main/scala/org/apache/spark/sql/types/StructType.scala#L396-L465


2016-06-29 9:00 GMT+09:00 楊閔富 <tilumi0@gmail.com>:

"
Gav <ipv6guru@gmail.com>,"Wed, 29 Jun 2016 18:35:54 +1000",test,dev@spark.apache.org,"ignore

-- 
Gav...
"
Nishadi Kirielle <ndimeshi@gmail.com>,"Wed, 29 Jun 2016 23:19:44 +0530",Bitmap Indexing to increase OLAP query performance,dev@spark.apache.org,"Hi All,

I am a CSE undergraduate and as for our final year project, we are
expecting to construct a cluster based, bit-oriented analytic platform
(storage engine) to provide fast query performance when used for OLAP with
the use of novel bitmap indexing techniques when and where appropriate.

For that we are expecting to use Spark SQL. We will need to implement a way
to cache the bit map indexes and in-cooperate the use of bitmap indexing at
the catalyst optimizer level when it is possible.

I would highly appreciate your feedback regarding the proposed approach.

Thank you & Regards

Nishadi Kirielle
Department of Computer Science and Engineering
University of Moratuwa
Sri Lanka
"
=?utf-8?Q?J=C3=B6rn_Franke?= <jornfranke@gmail.com>,"Wed, 29 Jun 2016 21:21:44 +0200",Re: Bitmap Indexing to increase OLAP query performance,Nishadi Kirielle <ndimeshi@gmail.com>,"
Is it the traditional bitmap indexing? I would not recommend it for big data. You could use bloom filters and min/max indexes in-memory which look to be more appropriate. However, if you want to use bitmap indexes then you would have to do it as you say. However, bitmap indexes may consume a lot of memory, so I am not sure that simply caching them in-memory is desired. 

ng to construct a cluster based, bit-oriented analytic platform (storage engine) to provide fast query performance when used for OLAP with the use of novel bitmap indexing techniques when and where appropriate. 
y to cache the bit map indexes and in-cooperate the use of bitmap indexing at the catalyst optimizer level when it is possible.

---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 29 Jun 2016 22:48:54 +0200",Spark 2.0 Performance drop,dev@spark.apache.org,"Hi,
Did anyone measure performance of Spark 2.0 vs Spark 1.6 ?

I did some test on parquet file with many nested columns (about 30G in
400 partitions) and Spark 2.0 is sometimes 2x slower.

I tested following queries:
1) select count(*) where id > some_id
In this query we have PPD and performance is similar. (about 1 sec)

2) select count(*) where nested_column.id > some_id
Spark 1.6 -> 1.6 min
Spark 2.0 -> 2.1 min
Is it normal that both version didn't do PPD ?

3) Spark connected with python
df.where('id > some_id').rdd.flatMap(lambda r: [r.id] if not r.id %
100000 else []).collect()
Spark 1.6 -> 2.3 min
Spark 2.0 -> 4.6 min (2x slower)

I used BasicProfiler for this task and cumulative time was:
Spark 1.6 - 4300 sec
Spark 2.0 - 5800 sec

Should I expect such a drop in performance ?

BTW: why in Spark 2.0 Dataframe lost map and flatmap method ?

I don't know how to prepare sample data to show the problem.
Any ideas ? Or public data with many nested columns ?

I'd like to create Jira for it but Apache server is down at the moment.

Regards,
-- 
Maciek Bryński

---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Wed, 29 Jun 2016 14:22:11 -0700",Re: Spark 2.0 Performance drop,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Hi Maciej,

In Spark, projection pushdown is currently limited to top-level columns (StructFields). VideoAmp has very large parquet-based tables (many billions of records accumulated per day) with deeply nested schema (four or five levels), and we've spent a considerable amount of time optimizing query performance on these tables.

We have a patch internally that extends Spark to support projection pushdown for arbitrarily nested fields. This has resulted in a *huge* performance improvement for many of our queries, like 10x to 100x in some cases.

I'm still putting the finishing touches on our port of this patch to Spark master and 2.0. We haven't done any specific benchmarking between versions, but I will do that when our patch is complete. We hope to contribute this functionality to the Spark project at some point in the near future, but it is not ready yet.

I'm sorry I don't have any concrete advice for you, but I hope this helps shed some light on the current support in Spark for projection pushdown.

Michael

moment.


---------------------------------------------------------------------


"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Wed, 29 Jun 2016 23:39:13 +0200",Re: Spark 2.0 Performance drop,Michael Allman <michael@videoamp.com>,"2016-06-29 23:22 GMT+02:00 Michael Allman <michael@videoamp.com>:
 shed some light on the current support in Spark for projection pushdown.

Michael,
Thanks for the answer. This resolves one of my questions.
Which Spark version you have patched ? 1.6 ? Are you planning to
public this patch or just for 2.0 branch ?

I gladly help with some benchmark in my environment.

Regards,
-- 
Maciek Bryński

---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Wed, 29 Jun 2016 15:47:37 -0700",Re: Spark 2.0 Performance drop,=?utf-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"The patch we use in production is for 1.5. We're porting the patch to master (and downstream to 2.0, which is presently very similar) with the intention of submitting a PR ""soon"". We'll push it here when it's ready: https://github.com/VideoAmp/spark-public.

Regarding benchmarking, we have a suite of Spark SQL regression tests which we run to check correctness and performance. I can share our findings when I have them.

Cheers,

Michael

helps shed some light on the current support in Spark for projection pushdown.


---------------------------------------------------------------------


"
Nishadi Kirielle <ndimeshi@gmail.com>,"Thu, 30 Jun 2016 11:57:21 +0530",Re: Bitmap Indexing to increase OLAP query performance,=?UTF-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"Thank you for the response.
Can I please know the reason why bit map indexes are not appropriate for
big data.
Rather than using the traditional bitmap indexing techniques we are
planning to implement a combination of novel bitmap indexing techniques
like bit sliced indexes and projection indexes.
Furthermore, can I please know whether bloom filters have already been
implemented in Spark.

Thank you

rote:

k
ou
.
h
.
"
=?UTF-8?Q?Maciej_Bry=C5=84ski?= <maciek@brynski.pl>,"Thu, 30 Jun 2016 09:28:30 +0200",Re: Spark 2.0 Performance drop,Michael Allman <michael@videoamp.com>,"I filled up 2 Jira.
1) Performance when queries nested column
https://issues.apache.org/jira/browse/SPARK-16320

2) Pyspark performance
https://issues.apache.org/jira/browse/SPARK-16321

I found Jira for:
1) PPD on nested columns
https://issues.apache.org/jira/browse/SPARK-5151

2) Drop of support for df.map etc. in Pyspark
https://issues.apache.org/jira/browse/SPARK-13594

2016-06-30 0:47 GMT+02:00 Michael Allman <michael@videoamp.com>:
ter (and downstream to 2.0, which is presently very similar) with the intention of submitting a PR ""soon"". We'll push it here when it's ready: https://github.com/VideoAmp/spark-public.
ch we run to check correctness and performance. I can share our findings when I have them.
te:
ps shed some light on the current support in Spark for projection pushdown.



-- 
Maciek Bryński

---------------------------------------------------------------------


"
Michael Allman <michael@videoamp.com>,"Thu, 30 Jun 2016 02:58:21 -0700",Re: Bitmap Indexing to increase OLAP query performance,Nishadi Kirielle <ndimeshi@gmail.com>,"Hi Nishadi,

I have not seen bloom filters in Spark. They are mentioned as part of the Orc file format, but I don't know if Spark uses them: https://orc.apache.org/docs/spec-index.html. Parquet has block-level min/max values, null counts, etc for leaf columns in its metadata. I don't believe Spark uses those directly either, though the underlying column reader may. See https://github.com/apache/parquet-mr/tree/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/metadata and https://github.com/apache/parquet-mr/tree/master/parquet-column/src/main/java/org/apache/parquet/column/statistics.

Michael


for big data. 
planning to implement a combination of novel bitmap indexing techniques like bit sliced indexes and projection indexes. 
implemented in Spark.
big data. You could use bloom filters and min/max indexes in-memory which look to be more appropriate. However, if you want to use bitmap indexes then you would have to do it as you say. However, bitmap indexes may consume a lot of memory, so I am not sure that simply caching them in-memory is desired.
expecting to construct a cluster based, bit-oriented analytic platform (storage engine) to provide fast query performance when used for OLAP with the use of novel bitmap indexing techniques when and where appropriate.
implement a way to cache the bit map indexes and in-cooperate the use of bitmap indexing at the catalyst optimizer level when it is possible.
approach.

"
Pete Robbins <robbinspg@gmail.com>,"Thu, 30 Jun 2016 13:37:54 +0000",branch-2.0 build failure,Dev <dev@spark.apache.org>,"Our build on branch-2.0 is failing after the PR for updating kafka to 0.10.
The new kafka pom.xml files are naming the parent version as 2.0.0-SNAPSHOT
but the branch 2.0 poms have been updated to 2.0.1-SNAPSHOT after the rc1
cut. Shouldn't the pom versions remain as 2.0.0-SNAPSHOT until a 2.0.0 has
been released?
"
Sean Owen <sowen@cloudera.com>,"Thu, 30 Jun 2016 14:51:12 +0100",Re: branch-2.0 build failure,Pete Robbins <robbinspg@gmail.com>,"TD has literally just merged the fix.


---------------------------------------------------------------------


"
Pete Robbins <robbinspg@gmail.com>,"Thu, 30 Jun 2016 13:58:31 +0000",Re: branch-2.0 build failure,Sean Owen <sowen@cloudera.com>,"Ok, thanks. I'll await it appearing.


"
cbruegg <mail@cbruegg.com>,"Thu, 30 Jun 2016 11:36:19 -0700 (MST)",Debugging Spark itself in standalone cluster mode,dev@spark.apache.org,"Hello everyone,

I'm a student assistant in research at the University of Paderborn, working
on integrating Spark (v1.6.2) with a new network resource management system.
I have already taken a deep dive into the source code of spark-core w.r.t.
its scheduling systems.

We are running a cluster in standalone mode consisting of a master node and
three slave nodes. Am I right to assume that tasks are scheduled within the
TaskSchedulerImpl using the DAGScheduler in this mode? I need to find a
place where the execution plan (and each stage) for a job is computed and
can be analyzed, so I placed some breakpoints in these two classes.

The remote debugging session within IntelliJ IDEA has been established by
running the following commands on the master node before:

  export SPARK_WORKER_OPTS=""-Xdebug
-Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n""
  export SPARK_MASTER_OPTS=""-Xdebug
-Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n""

Port 4000 has been forwarded to my local machine. Unfortunately, none of my
breakpoints through the class get hit when I invoke a task like
sc.parallelize(1 to 1000).count() in spark-shell on the master node (using
--master spark://...), though when I pause all threads I can see that the
process I am debugging runs some kind of event queue, which means that the
debugger is connected to /something/.

Do I rely on false assumptions or should these breakpoints in fact get hit?
I am not too familiar with Spark, so please bear with me if I got something
wrong. Many thanks in advance for your help.

Best regards,
Christian Brüggemann



--
3.nabble.com/Debugging-Spark-itself-in-standalone-cluster-mode-tp18139.html
om.

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 30 Jun 2016 12:00:18 -0700",Re: Debugging Spark itself in standalone cluster mode,cbruegg <mail@cbruegg.com>,"Yes, scheduling is centralized in the driver.

For debugging, I think you'd want to set the executor JVM, not the worker
JVM flags.



ng
.
nd
he
my
g
e
t?
ng
-itself-in-standalone-cluster-mode-tp18139.html
"
Reynold Xin <rxin@databricks.com>,"Thu, 30 Jun 2016 14:10:57 -0700",Re: Logical Plan,Darshan Singh <darshan.meel@gmail.com>,"Which version are you using here? If the underlying files change,
technically we should go through optimization again.

Perhaps the real ""fix"" is to figure out why is logical plan creation so
slow for 700 columns.



"
Reynold Xin <rxin@databricks.com>,"Thu, 30 Jun 2016 15:02:38 -0700",Re: Logical Plan,"Darshan Singh <darshan.meel@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","drop user@spark and keep only dev@

This is something great to figure out, if you have time. Two things that
would be great to try:

1. See how this works on Spark 2.0.

2. If it is slow, try the following:

org.apache.spark.sql.catalyst.rules.RuleExecutor.resetTime()

// run your query

org.apache.spark.sql.catalyst.rules.RuleExecutor.dumpTimeSpent()


And report back where the time are spent if possible. Thanks!




"
Mich Talebzadeh <mich.talebzadeh@gmail.com>,"Thu, 30 Jun 2016 23:06:23 +0100",Re: Logical Plan,Darshan Singh <darshan.meel@gmail.com>,"I don't think Spark optimizer supports something like statement cache where
plan is cached and bind variables (like RDBMS) are used for different
values, thus saving the parsing.

What you re stating is that the source and tempTable change but the plan
itself remains the same. I have not seen this in 1.6.1 and as I understand
Spark does yet support CBO yet not even in 2.0


HTH



Dr Mich Talebzadeh



LinkedIn * https://www.linkedin.com/profile/view?id=AAEAAAAWh2gBxianrbJd6zP6AcPCCdOABUrV8Pw
<https://www.linkedin.com/profile/view?id=AAEAAAAWh2gBxianrbJd6zP6AcPCCdOABUrV8Pw>*



http://talebzadehmich.wordpress.com


*Disclaimer:* Use it at your own risk. Any and all responsibility for any
loss, damage or destruction of data or any other property which may arise
from relying on this email's technical content is explicitly disclaimed.
The author will in no case be liable for any monetary damages arising from
such loss, damage or destruction.




"
