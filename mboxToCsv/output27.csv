Patrick Wendell <pwendell@gmail.com>,"Fri, 31 Jul 2015 18:50:31 -0700",Re: Should spark-ec2 get its own repo?,Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Hey All,

I've mostly kept quiet since I am not very active in maintaining this
code anymore. However, it is a bit odd that the project is
split-brained with a lot of the code being on github and some in the
Spark repo.

If the consensus is to migrate everything to github, that seems okay
with me. I would vouch for having user continuity, for instance still
have a ""shim"" ec2/spark-ec2 script that could perhaps just download
and unpack the real script from github.

- Patrick


---------------------------------------------------------------------


"
Matt Goodman <meawoppl@gmail.com>,"Sat, 1 Aug 2015 10:08:11 -0700",Re: Should spark-ec2 get its own repo?,Patrick Wendell <pwendell@gmail.com>,"I think that is a good idea, and slated to happen.  At the very least a
README or some such.  Is this a use case for git submodules?  I am
considering porting some of this to a more general spark-cloud launcher,
including google/aliyun/rackspace.  It shouldn't be hard at all given the
current approach for setup/install.

--Matthew Goodman

=====================
Check Out My Website: http://craneium.net
Find me on LinkedIn: http://tinyurl.com/d6wlch


"
Josh Rosen <joshrosen@databricks.com>,"Sat, 1 Aug 2015 11:32:51 -0700",Re: Should spark-ec2 get its own repo?,Matt Goodman <meawoppl@gmail.com>,"I don't think that using git submodules is a good idea here:

   - The extra `git submodule init && git submodule update` step can lead
   to confusing problems in certain workflows.
   - We'd wind up with many commits that serve only to bump the submodule
   SHA; these commits will be hard to review since they won't contain line
   diffs (the author will have to manually provide a link to the diff of code
   changes).



"
Patrick Wendell <pwendell@gmail.com>,"Sat, 1 Aug 2015 14:47:36 -0700",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Bharath Ravi Kumar <reachbach@gmail.com>,"Hey All,

I got it up and running - it was a newly surfaced bug in the build scripts.

- Patrick


---------------------------------------------------------------------


"
Bharath Ravi Kumar <reachbach@gmail.com>,"Sun, 2 Aug 2015 08:49:06 +0530",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Patrick Wendell <pwendell@gmail.com>,"Thanks for fixing it.


"
Burak Yavuz <brkyvz@gmail.com>,"Sat, 1 Aug 2015 21:11:15 -0700",Re: FrequentItems in spark-sql-execution-stat,Koert Kuipers <koert@tresata.com>,"Hi Yucheng,

Thanks for pointing out the issue. You are correct, in the case that the
final map is completely empty after the merge, we do need to add the final
element to the map, with the correct count (decrement the count with the
max count that was already in the map). I'll submit a fix for it.

Best,
Burak


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Sun, 02 Aug 2015 18:34:13 +0000",Re: Should spark-ec2 get its own repo?,"Matt Goodman <meawoppl@gmail.com>, Patrick Wendell <pwendell@gmail.com>","

FWIW, there are already some tools for launching Spark clusters on GCE and
Azure:

http://spark-packages.org/?q=tags%3A%22Deployment%22

Nick
"
Haseeb <11besemjaved@seecs.edu.pk>,"Sun, 2 Aug 2015 13:55:52 -0700 (MST)",What are 'Buckets' referred in Spark Core code,dev@spark.apache.org,"Hi all ,
I am neebie trying to understand spark internals. There some entity referred
to as 'buckets' at many places in Spark Core code but I am having a hard
time what it is as it is just mentioned in code comments but I didn't come
across any data structure that reffered to it or any class for that matter.
I'd be really grateful if someone could shed some light on what exactly
buckets are and what is their functionally with respect to Spark internals.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sun, 2 Aug 2015 14:02:48 -0700",Re: What are 'Buckets' referred in Spark Core code,Haseeb <11besemjaved@seecs.edu.pk>,"There are two usage of buckets used in Spark core.

The first usage is in histogram, used to perform sorting. Basically we
build an approximate histogram of the data in order to decide how to
partition the data in sorting. Each bucket is a range in the histogram.

The 2nd is used in shuffle, where we partition the output of each map task
into different ""buckets"", letting the reduce side fetching the map side
data based on their partition id.



"
cheez <11besemjaved@seecs.edu.pk>,"Sun, 2 Aug 2015 14:13:15 -0700 (MST)",Re: What are 'Buckets' referred in Spark Core code,dev@spark.apache.org,"Do we have a data structure that corresponds to buckets in Shuffle ? That is
of we wanted to explore the 'content' of these buckets in shuffle phase, can
we do that ? If yes, how ?



--

---------------------------------------------------------------------


"
"""Huang, Jie"" <jie.huang@intel.com>","Mon, 3 Aug 2015 01:21:32 +0000",[SparkScore]Performance portal for Apache Spark - WW31,"user <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Performance Portal for Apache Spark
Description
________________________________
Each data point represents each workload runtime percent compared with the previous week. Different lines represents different workloads running on spark yarn-client mode.
Hardware
________________________________
CPU type: Intel(r) Xeon(r) CPU E5-2697 v2 @ 2.70GHz
Memory: 128GB
NIC: 10GbE
Disk(s): 8 x 1TB SATA HDD
Software
________________________________
JAVA version: 1.8.0_25
Hadoop version: 2.5.0-CDH5.3.2
HiBench version: 4.0
Spark on yarn-client mode
Cluster
________________________________
1 node for Master
10 nodes for Slave
Regular
Summary
The lower percent the better performance.
________________________________
Group

ww24

ww25

ww26

ww27

ww28

ww29

ww30

ww31

HiBench

-6.5%

-3.1%

-2.1%

-6.4%

-2.7%

-0.7%

1.6%

2.2%

spark-perf

-4.7%

-4.6%

-5.4%

-4.6%

-12.8%

-12.5%

-11.7%

-12.3%

[http://01org.github.io/sparkscore/image/plaf1.time/overall.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
Detail
________________________________
HiBench
________________________________
JOB

ww24

ww25

ww26

ww27

ww28

ww29

ww30

ww31

commit

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

a803ac3e

fb5d43fb

sleep

-4.1%

12.8%

-5.1%

-4.5%

-3.1%

-0.7%

-0.8%

-0.6%

wordcount

-18.6%

-10.9%

6.9%

-12.9%

-10.0%

-9.2%

-8.5%

-8.0%

kmeans

86.9%

95.8%

123.3%

99.3%

127.9%

102.6%

116.9%

95.3%

scan

-25.5%

-21.0%

-12.4%

-19.8%

-19.7%

-20.5%

-16.3%

8.6%

bayes

-29.7%

-31.3%

-30.9%

-31.1%

-31.0%

-30.1%

-30.4%

-27.6%

aggregation

-15.3%

-15.0%

-37.6%

-37.0%

-37.3%

7.6%

11.4%

12.9%

join

-12.7%

-13.9%

-16.4%

-17.8%

-14.8%

-13.2%

-15.8%

-15.6%

sort

-17.5%

-17.3%

-20.7%

-17.7%

-13.9%

-15.6%

-13.2%

-16.9%

pagerank

-11.4%

-13.0%

-11.4%

-10.1%

-12.0%

-11.7%

-11.3%

-11.0%

terasort

-16.7%

-17.0%

-16.3%

-11.9%

-13.1%

-15.7%

-16.4%

-14.5%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/HiBench_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.
spark-perf
________________________________
JOB

ww24

ww25

ww26

ww27

ww28

ww29

ww30

ww31

commit

db81b9d8

4eb48ed1

32e3cdaa

ec784381

2b820f2a

c472eb17

a803ac3e

fb5d43fb

agg

5.2%

2.5%

1.1%

3.0%

-18.8%

-19.0%

-19.1%

-18.4%

agg-int

4.0%

8.2%

7.0%

7.5%

6.2%

11.4%

9.4%

5.5%

agg-naive

-6.7%

-6.8%

-8.5%

-6.9%

-15.5%

-18.0%

-15.1%

-20.6%

scheduling

-6.4%

-6.5%

-5.7%

-1.8%

-6.0%

-9.7%

-6.1%

-1.0%

count-filter

-10.2%

-10.4%

-9.8%

-10.4%

-18.0%

-17.4%

-17.3%

-19.6%

count

-7.3%

-7.0%

-8.0%

-7.4%

-15.1%

-14.3%

-14.3%

-16.5%

sort

-14.6%

-14.4%

-13.9%

-15.9%

-24.0%

-23.2%

-21.7%

-21.7%

sort-int

-1.5%

-2.2%

-5.3%

-5.0%

-11.3%

-9.6%

-9.5%

-6.4%

Comments: null means no such workload running or workload failed in this time.
[http://01org.github.io/sparkscore/image/plaf1.time/spark-perf_workloads.png]
Y-Axis: normalized completion time; X-Axis: Work Week.
The commit number can be found in the result table.
The performance score for each workload is normalized based on the elapsed time for 1.2 release. The lower the better.

Thank you && Best Regards,
Grace $B!J(BHuang Jie)
---------------------------------------------------------------------
SSG STO Big Data Technology
Intel Asia-Pacific Research & Development Ltd.
No. 880 Zi Xing Road
Shanghai, PRC, 200241
Phone: (86-21) 61166031

"
james <yiazhou@gmail.com>,"Sun, 2 Aug 2015 18:33:19 -0700 (MST)","Re: Came across Spark SQL hang/Error issue with Spark 1.5 Tungsten
 feature",dev@spark.apache.org,"Thank you for your reply!
Do you mean that currently if i want to use this Tungsten feature, we had to
set sort shuffle manager(spark.shuffle.manager=sort) ,right ?  However, I
saw a slide ""Deep Dive into Project Tungsten: Bringing Spark Closer to Bare
Metal"" published in Spark Summit 2015 and it seems to recommend
'tungsten-sort' manager.



--

---------------------------------------------------------------------


"
Josh Rosen <joshrosen@databricks.com>,"Sun, 2 Aug 2015 22:07:15 -0700","Master JIRA ticket for tracking Spark 1.5.0 configuration renames,
 defaults changes, and configuration deprecation",Dev <dev@spark.apache.org>,"To help us track planned / finished configuration renames, defaults
changes, and configuration deprecation for the upcoming 1.5.0 release, I
have created https://issues.apache.org/jira/browse/SPARK-9550.

As you make configuration changes or think of configurations that need to
be audited, please update that ticket by editing it or posting a comment.

This ticket will help us later when it comes time to draft release notes.

Thanks,
Josh
"
james <yiazhou@gmail.com>,"Mon, 3 Aug 2015 04:35:04 -0700 (MST)","Re: Came across Spark SQL hang/Error issue with Spark 1.5 Tungsten
 feature",dev@spark.apache.org,"Based on the latest spark code(commit
608353c8e8e50461fafff91a2c885dca8af3aaa8) and used the same Spark SQL query
to test two group of combined configuration and seemed that currently it
don't work fine in ""tungsten-sort"" shuffle manager from below results:

*Test 1# (PASSED)*
spark.shuffle.manager=sort
spark.sql.codegen=true
spark.sql.unsafe.enabled=true 

*Test 2#(FAILED)*
spark.shuffle.manager=tungsten-sort
spark.sql.codegen=true
spark.sql.unsafe.enabled=true 

15/08/03 16:46:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode4:50313
15/08/03 16:46:02 INFO spark.MapOutputTrackerMaster: Size of output statuses
for shuffle 3 is 586 bytes
15/08/03 16:46:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode2:60490
15/08/03 16:46:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode2:56319
15/08/03 16:46:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode1:58179
15/08/03 16:46:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode1:32816
15/08/03 16:46:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode3:55840
15/08/03 16:46:02 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send
map output locations for shuffle 3 to bignode3:46874
15/08/03 16:46:02 WARN scheduler.TaskSetManager: Lost task 42.0 in stage
158.0 (TID 1548, bignode4): java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at
org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:118)
        at
org.apache.spark.sql.execution.UnsafeRowSerializerInstance$$anon$3$$anon$1.next(UnsafeRowSerializer.scala:107)
        at scala.collection.Iterator$$anon$13.next(Iterator.scala:372)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at
org.apache.spark.util.CompletionIterator.next(CompletionIterator.scala:30)
        at
org.apache.spark.InterruptibleIterator.next(InterruptibleIterator.scala:43)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
        at
org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:167)
        at
org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:140)
        at
org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:120)
        at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at
org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:71)
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:86)
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)




--

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 3 Aug 2015 10:18:55 -0700",Moving spark-ec2 to amplab github organization,dev@mesos.apache.org,"Hi Mesos developers

The Apache Spark project has been hosting using
https://github.com/mesos/spark-ec2 as a supporting repository for some
of our EC2 scripts. This is a remnant from the days when the Spark
project itself was hosted at github.com/mesos/spark. Based on
discussions in the Spark Developer mailing list [1], we plan to move
the repository to github.com/amplab/spark-ec2 to enable a better
development workflow. As these scripts are not used by the Apache
Mesos project I don‚Äôt think any action is required from the Mesos
developers, but please let me know if you have any thoughts about
this.

Thanks
Shivaram

[1] http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Should-spark-ec2-get-its-own-repo-td13151.html

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 3 Aug 2015 10:25:33 -0700",Re: Should spark-ec2 get its own repo?,Nicholas Chammas <nicholas.chammas@gmail.com>,"I sent a note to the Mesos developers and created
https://github.com/apache/spark/pull/7899 to change the repository
pointer. There are 3-4 open PRs right now in the mesos/spark-ec2
repository and I'll work on migrating them to amplab/spark-ec2 later
today.

My thoughts on moving the python script is that we should have a
wrapper shell script that just fetches the latest version of
spark_ec2.py for the corresponding Spark branch. We already have
separate branches in our spark-ec2 repository for different Spark
versions so it can just be a call to `wget
https://github.com/amplab/spark-ec2/tree/<spark-version>/driver/spark_ec2.py`.

Thanks
Shivaram


---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Mon, 3 Aug 2015 10:32:54 -0700","Re: Package Release Annoucement: Spark SQL on HBase ""Astro""","""Bing Xiao (Bing)"" <bing.xiao@huawei.com>","When I tried to compile against hbase 1.1.1, I got:

[ERROR]
/home/hbase/ssoh/src/main/scala/org/apache/spark/sql/hbase/SparkSqlRegionObserver.scala:124:
overloaded method next needs result type
[ERROR]   override def next(result: java.util.List[Cell], limit: Int) =
next(result)

Is there plan to support hbase 1.x ?

Thanks


e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Mon, 3 Aug 2015 17:45:49 +0000","RE: Package Release Annoucement: Spark SQL on HBase ""Astro""","Ted Yu <yuzhihong@gmail.com>, ""Bing Xiao (Bing)"" <bing.xiao@huawei.com>","HBase 1.0 should work fine even though we have not completed full tests yet. Support of 1.1 should be able to be added with a minimal effort.

Thanks,

Yan

From: Te3, 2015 10:33 AM
To: Bing Xiao (Bing)
Cc: dev@spark.apache.org; user@spark.apache.org; Yan Zhou.sc
Subject: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

When I tried to compile against hbase 1.1.1, I got:

[ERROR] /home/hbase/ssoh/src/main/scala/org/apache/spark/sql/hbase/SparkSqlRegionObserver.scala:124: overloaded method next needs result type
[ERROR]   override def next(result: java.util.List[Cell], limit: Int) = next(result)

Is there plan to support hbase 1.x ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team


"
Trevor Grant <trevor.d.grant@gmail.com>,"Mon, 3 Aug 2015 12:52:59 -0500",Unsubscribe,dev@spark.apache.org,"Please drop me from this list

Trevor Grant
Data Scientist
https://github.com/rawkintrevo
http://stackexchange.com/users/3002022/rawkintrevo

*""Fortunate is he, who is able to know the causes of things.""  -Virgil*
"
Sean Owen <sowen@cloudera.com>,"Mon, 3 Aug 2015 19:01:49 +0100",PSA: Maven 3.3.3 now required to build,dev <dev@spark.apache.org>,"If you use build/mvn or are already using Maven 3.3.3 locally (i.e.
via brew on OS X), then this won't affect you, but I wanted to call
attention to https://github.com/apache/spark/pull/7852 which makes
Maven 3.3.3 the minimum required to build Spark. This heads off
problems from some behavior differences that Patrick and I observed
between 3.3 and 3.2 last week, on top of the ""dependency reduced POM""
glitch from the 1.4.1 release window.

Again all you need to do is use build/mvn if you don't already have
the latest Maven installed and all will be well.

Sean

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 03 Aug 2015 18:10:09 +0000",Re: Unsubscribe,"Trevor Grant <trevor.d.grant@gmail.com>, dev@spark.apache.org","The way to do that is to follow the ""Unsubscribe"" link here for dev@spark:

http://spark.apache.org/community.html

We can't drop you. You have to do it yourself.

Nick


"
Reynold Xin <rxin@databricks.com>,"Mon, 3 Aug 2015 11:11:52 -0700",[ANNOUNCE] Spark branch-1.5,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Devs,

Just an announcement that I've cut Spark's branch-1.5 to form the basis of
the 1.5 release. Other than a few stragglers, this represents the end of
active feature development for Spark 1.5. *If committers are merging any
features (outside of alpha modules), please shoot me an email so I can help
coordinate. Any new commits will need to be explicitly merged into
branch-1.5*.

In the next few days, we should come up with testing plans for the release
and create umbrella JIRAs for testing various components and changes. I
plan to cut a preview package for 1.5 towards the end of this week or early
next week.


- R
"
Sean Owen <sowen@cloudera.com>,"Mon, 3 Aug 2015 20:05:49 +0100",Re: [ANNOUNCE] Spark branch-1.5,Reynold Xin <rxin@databricks.com>,"Are these about the right rules of engagement for now until the
release candidate?

- Don't merge new features or improvements into 1.5 unless they're
Important and Have Been Discussed
- Docs and tests are OK to merge into 1.5
- Bug fixes can be merged into 1.5, with increasing conservativeness
as the release candidate approaches

FWIW there are now 331 JIRAs targeted at 1.5.0.

Would it be reasonable to start un-targeting non-bug non-blocker
issues? like, would anyone yell if I started doing that? that would
leave ~100 JIRAs, which still seems like more than can actually go
into the release. And anyone can re-target as desired.

I'm interested with using this to communicate about release planning
so we can actually see how things are moving along and decide if 1.5
has to be pushed back or not; otherwise it seems pretty unpredictable
what's coming, going in, and when the process stops and outputs a
release.



---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Mon, 3 Aug 2015 12:12:58 -0700",Re: [ANNOUNCE] Spark branch-1.5,Sean Owen <sowen@cloudera.com>,"I agree that it's high time to start changing/removing target versions,
especially if component maintainers have a good idea of what is not needed
for 1.5.  I'll start doing that on ML.


"
Guru Medasani <gdmeda@gmail.com>,"Mon, 3 Aug 2015 14:37:35 -0500",Re: PSA: Maven 3.3.3 now required to build,Sean Owen <sowen@cloudera.com>,"Thanks Sean. I noticed this one while building Spark version 1.5.0-SNAPSHOT this morning. 

WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireMavenVersion failed with message:
Detected Maven Version: 3.2.5 is not in the allowed range 3.3.3.

Should we be using maven 3.3.3 locally or build/mvn starting from Spark 1.4.1 or Spark version 1.5?

Guru Medasani
gdmeda@gmail.com




"
Sean Owen <sowen@cloudera.com>,"Mon, 3 Aug 2015 20:38:57 +0100",Re: PSA: Maven 3.3.3 now required to build,Guru Medasani <gdmeda@gmail.com>,"Using ./build/mvn should always be fine. Your local mvn is fine too if
it's 3.3.3 or later (3.3.3 is the latest). That's what any brew users
on OS X out there will have, by the way.


---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Mon, 3 Aug 2015 12:45:10 -0700",Re: PSA: Maven 3.3.3 now required to build,Sean Owen <sowen@cloudera.com>,"Just note that if you have ""mvn"" in your path, you need to use ""build/mvn
--force"".




-- 
Marcelo
"
Michael Armbrust <michael@databricks.com>,"Mon, 3 Aug 2015 12:45:55 -0700",Re: [ANNOUNCE] Spark branch-1.5,Joseph Bradley <joseph@databricks.com>,"I think the maintainers of the various components should take care of
this.  Reynold and I just did a pass over SQL and I think that by Friday
there should only be blocker bugs / documentation remaining.
"
Guru Medasani <gdmeda@gmail.com>,"Mon, 3 Aug 2015 15:12:51 -0500",Re: PSA: Maven 3.3.3 now required to build,Sean Owen <sowen@cloudera.com>,"Thanks Sean. Reason I asked this is, in Building Spark documentation of 1.4.1, I still see this.

https://spark.apache.org/docs/latest/building-spark.html <https://spark.apache.org/docs/latest/building-spark.html>

Building Spark using Maven requires Maven 3.0.4 or newer and Java 6+.

But I noticed the following warnings from the build of Spark version 1.5.0-snapshot. So I was wondering if the changes you mentioned relate to newer versions of Spark or for 1.4.1 version as well.

[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireMavenVersion failed with message:
Detected Maven Version: 3.2.5 is not in the allowed range 3.3.3.

[WARNING] Rule 1: org.apache.maven.plugins.enforcer.RequireJavaVersion failed with message:
Detected JDK Version: 1.6.0-36 is not in the allowed range 1.7.

Guru Medasani
gdmeda@gmail.com

1.5.0-SNAPSHOT
org.apache.maven.plugins.enforcer.RequireMavenVersion
Spark

"
Sean Owen <sowen@cloudera.com>,"Mon, 3 Aug 2015 21:26:06 +0100",Re: PSA: Maven 3.3.3 now required to build,Guru Medasani <gdmeda@gmail.com>,"That statement is true for Spark 1.4.x. But you've reminded me that I
failed to update this doc for 1.5, to say Maven 3.3.3 is required.
Patch coming up.


---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Mon, 3 Aug 2015 14:43:06 -0700",Re: PSA: Maven 3.3.3 now required to build,Sean Owen <sowen@cloudera.com>,"Yeah the best bet is to use ./build/mvn --force (otherwise we'll still
use your system maven).

- Patrick


---------------------------------------------------------------------


"
Eron Wright <ewright@live.com>,"Mon, 3 Aug 2015 16:51:04 -0700",Make ML Developer APIs public (post-1.4),"""dev@spark.apache.org"" <dev@spark.apache.org>","
Hello,

In developing new third-party pipeline components for Spark ML 1.4 (see dl4j-spark-ml), I encountered a few gaps in the earlier effort to make the ML Developer APIs public (SPARK-5995).    I plan to file issues after we discuss on this thread.   The below is a list of types that are presently private but might best be made public.
VectorUDT.    To define a relation with a vector field,  VectorUDT must be instantiated.
SchemaUtils.   Third-party pipeline components have a need for checking column types and appending columns.
Identifiable trait.   The trait generates a unique identifier for the associated pipeline component.  Nice to have a consistent format by reusing the trait.
ProbabilisticClassifier.  Third-party components should leverage the complex logic around computing only selected columns.
Shared Params (HasLabel, HasFeatures).   This is covered in SPARK-7146 but reiterating it here.
Thanks,
Eron Wright

"
Guru Medasani <gdmeda@gmail.com>,"Mon, 3 Aug 2015 22:20:05 -0500","Consistent recommendation for submitting spark apps to YARN, -master yarn --deploy-mode x vs -master yarn-x' ",dev <dev@spark.apache.org>,"Hi,

I was looking at the spark-submit and spark-shell --help  on both (Spark 1.3.1 and Spark 1.5-snapshot) versions and the Spark documentation for submitting Spark applications to YARN. It seems to be there is some mismatch in the preferred syntax and documentation. 

Spark documentation <http://spark.apache.org/docs/latest/submitting-applications.html#master-urls> says that we need to specify either yarn-cluster or yarn-client to connect to a yarn cluster. 


yarn-client	Connect to a YARN¬† <http://spark.apache.org/docs/latest/running-on-yarn.html>cluster in client mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.
yarn-cluster	Connect to a YARN¬† <http://spark.apache.org/docs/latest/running-on-yarn.html>cluster in cluster mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.
In the spark-submit --help it says the following Options: --master yarn --deploy-mode cluster or client.

Usage: spark-submit [options] <app jar | python file> [app arguments]
Usage: spark-submit --kill [submission ID] --master [spark://...]
Usage: spark-submit --status [submission ID] --master [spark://...]

Options:
  --master MASTER_URL         spark://host:port, mesos://host:port, yarn, or local.
  --deploy-mode DEPLOY_MODE   Whether to launch the driver program locally (""client"") or
                              on one of the worker machines inside the cluster (""cluster"")
                              (Default: client).

I want to bring this to your attention as this is a bit confusing for someone running Spark on YARN. For example, they look at the spark-submit help command and start using the syntax, but when they look at online documentation or user-group mailing list, they see different spark-submit syntax. 

From a quick discussion with other engineers at Cloudera it seems like ‚Äîdeploy-mode is preferred as it is more consistent with the way things are done with other cluster managers, i.e. there is no standalone-cluster or standalone-client masters. This applies to Mesos as well.

Either syntax works, but I would like to propose to use ‚Äò-master yarn ‚Äîdeploy-mode x‚Äô instead of ‚Äò-master yarn-cluster or -master yarn-client‚Äô as it is consistent with other cluster managers . This would require updating all Spark pages related to submitting Spark applications to YARN.

So far I‚Äôve identified the following pages.

1) http://spark.apache.org/docs/latest/running-on-yarn.html <http://spark.apache.org/docs/latest/running-on-yarn.html>
2) http://spark.apache.org/docs/latest/submitting-applications.html#master-urls <http://spark.apache.org/docs/latest/submitting-applications.html#master-urls>

There is a JIRA to track the progress on this as well.

https://issues.apache.org/jira/browse/SPARK-9570 <https://issues.apache.org/jira/browse/SPARK-9570>
 
The option we choose dictates whether we update the documentation  or spark-submit and spark-shell help pages.  

Any thoughts which direction we should go? 

Guru Medasani
gdmeda@gmail.com



"
Meihua Wu <rotationsymmetry14@gmail.com>,"Mon, 3 Aug 2015 23:32:54 -0700",How to help for 1.5 release?,dev <dev@spark.apache.org>,"I think the team is preparing for the 1.5 release. Anything to help with
the QA, testing etc?

Thanks,

MW
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 4 Aug 2015 12:19:00 +0530",Re: How to help for 1.5 release?,Meihua Wu <rotationsymmetry14@gmail.com>,"I think you can start from here
https://issues.apache.org/jira/browse/SPARK/fixforversion/12332078/?selectedTab=com.atlassian.jira.jira-projects-plugin:version-summary-panel

Thanks
Best Regards


"
Patrick <petz2000@gmail.com>,"Tue, 4 Aug 2015 00:50:13 -0700 (MST)",Re: Have Friedman's glmnet algo running in Spark,dev@spark.apache.org,"I have a follow up on this: 
I see on JIRA that the idea of having a GLMNET implementation was more or
less abandoned, since a OWLQN implementation was chosen to construct a model
using L1/L2 regularization.   

However, GLMNET has the property of ""returning a multitide of models
(corresponding to different vales of penalty parameters [for the
regularization])"". I think this is not the case in the OWLQN implementation. 
However, this would be really helpful to compare the accuracy of models with
different regParam values. 
As far as I understood, this would avoid to have a costly cross-validation
step over a possibly large set of regParam values. 


Joseph Bradley wrote







--

---------------------------------------------------------------------


"
Priya Ch <learnings.chitturi@gmail.com>,"Tue, 4 Aug 2015 16:33:04 +0530",Fwd: Writing streaming data to cassandra creates duplicates,"""user@spark.apache.org"" <user@spark.apache.org>, dev@spark.apache.org","Yes...union would be one solution. I am not doing any aggregation hence
reduceByKey would not be useful. If I use groupByKey, messages with same
key would be obtained in a partition. But groupByKey is very expensive
operation as it involves shuffle operation. My ultimate goal is to write
the messages to cassandra. if the messages with same key are handled by
different streams...there would be concurrency issues. To resolve this i
can union dstreams and apply hash parttioner so that it would bring all the
same keys to a single partition or do a groupByKey which does the same.

As groupByKey is expensive, is there any work around for this ?


ld
n
or
e 3
rom
e is
,
date
ress
 or
batch
for
to
emantics-of-output-operations
are
re as
either
"
mike@mbowles.com,"Tue, 04 Aug 2015 15:21:28 +0000",Re:  Have Friedman's glmnet algo running in Spark,"""Patrick"" <petz2000@gmail.com>, dev@spark.apache.org"," My friends and I are continuing work on the algorithm. You are right thaf coordinate descent for minimizing penalized regression with an absolute value penalty and the other is managing the regularization parameters. Friedmans algorithm does return the the entire regularization path. We have had to get fairly deep into the mechanics of linear algebra. The tricky part has been arranging the matrix and vector multiplications to minimize the compute times - (e.g. big time differences between multiplying by a submatrix versus mulbiplying by the columns in the submatrix, etc. ) 

All of the versions we've produced generate a multitude of solutions (default = 100) for a range of different values of the regularization parameter. The solutions always cover the most heavily penalized end of the curve. The number of solutions generated depends on how fine the steps are and how close the solutions get to the fully saturated (un-penalized) solution. Default values for these work about 80% of the time. 

Personally, i've always found it useful to have the entire regularization. It's just a question of whether the points on the path are generated by hunting and pecking or done all in one shot systematically. 
mike






T imp entation was more orless abandoned, since a OWLQN implementation was chosen to construct a modelusing L1/L2 regularization. However, GLMNET has the property of ""returning a multitide of models(corresponding to different vales of penalty parameters [for theregularization])"". I think this is not the case in the OWLQN implementation. However, this would be really helpful to compare the accuracy of models withdifferent regParam values. As far as I understood, this would avoid to have a costly cross-validationstep over a possibly large set of regParam values. Joseph Bradley wrote> Some of this discussion seems valuable enough to preserve on the JIRA; can> we move it there (and copy any relevant discussions from previous001551.n3.nabble.com/Have-Friedman-s-glmnet-algo-running-in-Spark-tp10692p13587.htmlSent from the Apache Spark Developers List mailing list archive at Nabble.com.---------------------------------------------------------dditional commands, e-mail: dev-help@spark.apache.org
"
Patrick Wendell <pwendell@gmail.com>,"Tue, 4 Aug 2015 10:29:56 -0700",Re: How to help for 1.5 release?,Akhil Das <akhil@sigmoidanalytics.com>,"Hey Meihua,

If you are a user of Spark, one thing that is really helpful is to run
Spark 1.5 on your workload and report any issues, performance
regressions, etc.

- Patrick


---------------------------------------------------------------------


"
shane knapp <sknapp@berkeley.edu>,"Tue, 4 Aug 2015 11:14:04 -0700",shane will be OOO 8-5-15 through 8-18-15,"amp-infra <amp-infra@googlegroups.com>, dev <dev@spark.apache.org>, 
	Jon Kuroda <jkuroda@eecs.berkeley.edu>, Matthew L Massie <massie@berkeley.edu>, 
	Josh Rosen <joshrosen@databricks.com>, Patrick Wendell <patrick@databricks.com>","so i done gone and got myself hitched, and will be disappearing in to
the rainy island of kol chang in thailand for the next ~2 weeks.  :)

this means i will be completely out of contact, and have to leave
jenkins in the gentle hands of jon kuroda (a sysadmin here at the lab)
and matt massie (my boss).  they've been CCed on this email, and
briefed on the basic operations, so should be able to maintain things
while i'm gone.

i will ask that during these next couple of weeks that we hold off on
any major system changes and package installations, unless it's a
blocker and needed for any releases.  this is mostly my fault, as i've
not finished porting all of the bash setup scripts (of doom) to
ansible and i'd like to minimize feature drift.

anyways, i'll be back in a couple of weeks, so don't break anything.  :)

shane

---------------------------------------------------------------------


"
Sandeep Giri <sandeep@knowbigdata.com>,"Wed, 5 Aug 2015 15:04:14 +0530",Re: New Feature Request,Jonathan Winandy <jonathan.winandy@gmail.com>,"Hi Jonathan,

Does that guarantee a result? I do not see that it is really optimized.

Hi Carsten,


How does the following code work:

data.filter(qualifying_function).take(n).count() >= n


Also, as per my understanding, in both the approaches you mentioned the
qualifying function will be executed on whole dataset even if the value was
already found in the first element of RDD:


   - data.filter(qualifying_function).take(n).count() >= n
      - val contains1MatchingElement = !(data.filter(qualifying_
      function).isEmpty())

Isn't it? Am I missing something?


Regards,
Sandeep Giri,
+1 347 781 4573 (US)
+91-953-899-8962 (IN)

www.KnowBigData.com. <http://KnowBigData.com.>
Phone: +1-253-397-1945 (Office)

[image: linkedin icon] <https://linkedin.com/company/knowbigdata> [image:
other site icon] <http://knowbigdata.com>  [image: facebook icon]
<https://facebook.com/knowbigdata> [image: twitter icon]
<https://twitter.com/IKnowBigData> <https://twitter.com/IKnowBigData>



ld
q6
n
"
Sean Owen <sowen@cloudera.com>,"Wed, 5 Aug 2015 10:39:58 +0100",Re: New Feature Request,Sandeep Giri <sandeep@knowbigdata.com>,"I don't think countApprox is appropriate here unless approximation is OK.
But more generally, counting everything matching a filter requires applying
the filter to the whole data set, which seems like the thing to be avoided
here.

The take approach is better since it would stop after finding n matching
elements (it might do a little extra work given partitioning and
buffering). It would not filter the whole data set.

The only downside there is that it would copy n elements to the driver.


as
e
g
#q6
rn
s
"
Sandeep Giri <sandeep@knowbigdata.com>,"Wed, 5 Aug 2015 21:19:53 +0530",,Sean Owen <sowen@cloudera.com>,"Yes, but in the take() approach we will be bringing the data to the driver
and is no longer distributed.

Also, the take() takes only count as argument which means that every time
we would transferring the redundant elements.


Regards,
Sandeep Giri,
+1 347 781 4573 (US)
+91-953-899-8962 (IN)

www.KnowBigData.com. <http://KnowBigData.com.>
Phone: +1-253-397-1945 (Office)

[image: linkedin icon] <https://linkedin.com/company/knowbigdata> [image:
other site icon] <http://knowbigdata.com>  [image: facebook icon]
<https://facebook.com/knowbigdata> [image: twitter icon]
<https://twitter.com/IKnowBigData> <https://twitter.com/IKnowBigData>



ng
d
was
:
he
ng
f
2#q6
h
es
"
Sean Owen <sowen@cloudera.com>,"Wed, 5 Aug 2015 16:51:22 +0100",Re:,Sandeep Giri <sandeep@knowbigdata.com>,"take only brings n elements to the driver, which is probably still a win if
n is small. I'm not sure what you mean by only taking a count argument --
what else would be an arg to take?


"
Sandeep Giri <sandeep@knowbigdata.com>,"Wed, 5 Aug 2015 21:35:54 +0530",Re:,Sean Owen <sowen@cloudera.com>,"Okay. I think I got it now. Yes take() does not need to be called more than
once. I got the impression that we wanted to bring elements to the driver
node and then run out qualifying_function on driver_node.

Now, I am back to my question which I started with: Could there be an
approach where the qualifying_function() does not get called after an
element has been found?


Regards,
Sandeep Giri,
+1 347 781 4573 (US)
+91-953-899-8962 (IN)

www.KnowBigData.com. <http://KnowBigData.com.>
Phone: +1-253-397-1945 (Office)

[image: linkedin icon] <https://linkedin.com/company/knowbigdata> [image:
other site icon] <http://knowbigdata.com>  [image: facebook icon]
<https://facebook.com/knowbigdata> [image: twitter icon]
<https://twitter.com/IKnowBigData> <https://twitter.com/IKnowBigData>



"
Guru Medasani <gdmeda@gmail.com>,"Wed, 5 Aug 2015 11:28:31 -0500","Re: Consistent recommendation for submitting spark apps to YARN, -master yarn --deploy-mode x vs -master yarn-x' ",dev <dev@spark.apache.org>,"Following up on this thread to see if anyone has some thoughts or opinions on the mentioned approach.


Guru Medasani
gdmeda@gmail.com



(Spark 1.3.1 and Spark 1.5-snapshot) versions and the Spark documentation for submitting Spark applications to YARN. It seems to be there is some mismatch in the preferred syntax and documentation. 
<http://spark.apache.org/docs/latest/submitting-applications.html#master-urls> says that we need to specify either yarn-cluster or yarn-client to connect to a yarn cluster. 
<http://spark.apache.org/docs/latest/running-on-yarn.html>cluster in client mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.
<http://spark.apache.org/docs/latest/running-on-yarn.html>cluster in cluster mode. The cluster location will be found based on the HADOOP_CONF_DIR or YARN_CONF_DIR variable.
yarn --deploy-mode cluster or client.
<spark://...]>
<spark://...]>
mesos://host:port <mesos://host:port>, yarn, or local.
locally (""client"") or
cluster (""cluster"")
someone running Spark on YARN. For example, they look at the spark-submit help command and start using the syntax, but when they look at online documentation or user-group mailing list, they see different spark-submit syntax. 
like ‚Äîdeploy-mode is preferred as it is more consistent with the way things are done with other cluster managers, i.e. there is no standalone-cluster or standalone-client masters. This applies to Mesos as well.
 yarn ‚Äîdeploy-mode x‚Äô instead of ‚Äò-master yarn-cluster or -master yarn-client‚Äô as it is consistent with other cluster managers . This would require updating all Spark pages related to submitting Spark applications to YARN.
<http://spark.apache.org/docs/latest/running-on-yarn.html>
http://spark.apache.org/docs/latest/submitting-applications.html#master-urls <http://spark.apache.org/docs/latest/submitting-applications.html#master-urls>
<https://issues.apache.org/jira/browse/SPARK-9570>
spark-submit and spark-shell help pages.  

"
Feynman Liang <fliang@databricks.com>,"Wed, 5 Aug 2015 09:54:18 -0700",Re:,Sandeep Giri <sandeep@knowbigdata.com>,"qualifying_function() will be executed on each partition in parallel;
stopping all parallel execution after the first instance satisfying
qualifying_function() would mean that you would have to effectively make
the computation sequential.


"
Jonathan Winandy <jonathan.winandy@gmail.com>,"Wed, 5 Aug 2015 19:18:39 +0200",Re:,Feynman Liang <fliang@databricks.com>,"Hello !

You could try something like that :

def exists[T](rdd:RDD[T])(f:T=>Boolean, n:Long):Boolean = {

  val context: SparkContext = rdd.sparkContext
  val grp: String = Random.alphanumeric.take(10).mkString
  context.setJobGroup(grp, ""exist"")
  val count: Accumulator[Long] = context.accumulator(0L)

  val iteratorToInt: (Iterator[T]) => Int = {
    iterator =>
      val i: Int = iterator.count(f)
      count += i
      i
  }

  val t = new Thread {
    override def run {
      while (count.value < n) {}
      context.cancelJobGroup(grp)
    }
  }
  t.start()
  try {
    context.runJob(rdd, iteratorToInt) > n
  } catch  {
    case e:SparkException => {
      count.value > n
    }
  } finally {
    t.stop()
  }

}



It stops the computation if enough elements satisfying the condition are
witnessed.

It is performant if the RDD is well partitioned. If this is a problem, you
could change iteratorToInt to :

val iteratorToInt: (Iterator[T]) => Int = {
  iterator =>
    val i: Int = iterator.count(x => {
      if(f(x)) {
        count += 1
        true
      } else false
    })
    i

}


I am interested in a safer way to perform partial computation in spark.

Cheers,
Jonathan


"
Patrick Wendell <pwendell@gmail.com>,"Wed, 5 Aug 2015 11:24:56 -0700",Avoiding unnecessary build changes until tests are in better shape,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hey All,

Was wondering if people would be willing to avoid merging build
changes until we have put the tests in better shape. The reason is
that build changes are the most likely to cause downstream issues with
the test matrix and it's very difficult to reverse engineer which
patches caused which problems when the tests are not in a stable
state. For instance, the updates to Hive 1.2.1 caused cascading
failures that have lasted several days now and in the mean time a few
other build related patches were also merged - as these pile up it
gets harder for us to have confidence those other patches didn't
introduce problems.

https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Test/

- Patrick

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Wed, 5 Aug 2015 11:41:45 -0700","Re: Avoiding unnecessary build changes until tests are in better
 shape",Patrick Wendell <pwendell@gmail.com>,"+1.  I've been holding off on reviewing / merging patches like the 
run-tests-jenkins Python refactoring for exactly this reason.



---------------------------------------------------------------------


"
Daniel Li <danielli90@gmail.com>,"Thu, 6 Aug 2015 01:27:47 -0700",Why SparkR didn't reuse PythonRDD,dev@spark.apache.org,"
When reading Spark codebase, looks to me PythonRDD.scala is reusable, I
wonder why SparkR choose to implement its own RRDD.scala?

thanks
Daniel
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 6 Aug 2015 02:20:47 -0700",Re: Is there any way to support multiple users executing SQL on thrift server?,Cheng Lian <lian.cs.zju@gmail.com>,"What is the JIRA number if a JIRA has been logged for this ?

Thanks



ike to investigate this issue later. Would you please open an JIRA for it? Thanks!
rver?
Äù
2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000
2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary
2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0
2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/_temporary
2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000
2015-01-16_16-42-48_923_1860943684064616152-3/-ext-10000/_temporary/0/task_201501161642_0022_m_000000/part-00000
ide) is owned by user B (which is what we expected).
ch is NOT what we expected).
trolException while the driver side moving output data into dest table.
"
Jonathan Winandy <jonathan.winandy@gmail.com>,"Thu, 6 Aug 2015 11:32:42 +0200",Re:,Feynman Liang <fliang@databricks.com>,"Hello !

I think I found a performant and nice solution based on take' source code :

def exists[T](rdd: RDD[T])(qualif: T => Boolean, num: Int): Boolean = {
  if (num == 0) {
    true
  } else {
    var count: Int = 0
    val totalParts: Int = rdd.partitions.length
    var partsScanned: Int = 0
    while (count < num && partsScanned < totalParts) {
      var numPartsToTry: Int = 1
      if (partsScanned > 0) {
        if (count == 0) {
          numPartsToTry = partsScanned * 4
        } else {
          numPartsToTry = Math.max((1.5 * num * partsScanned /
count).toInt - partsScanned, 1)
          numPartsToTry = Math.min(numPartsToTry, partsScanned * 4)
        }
      }

      val left: Int = num - count
      val p: Range = partsScanned until math.min(partsScanned +
numPartsToTry, totalParts)
      val res: Array[Int] = rdd.sparkContext.runJob(rdd, (it:
Iterator[T]) => it.filter(qualif).take(left).size, p, allowLocal =
true)

      count = count + res.sum
      partsScanned += numPartsToTry
    }

    count >= num
  }
}

//val all:RDD[Any]
println(exists(all)(_ => {println(""."") ; true}, 10))

It's super fast for small values of n and I think it parallelise
nicely for large values.

Please tell me what you think.


Have a nice day,

Jonathan






"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 6 Aug 2015 09:29:29 -0700",Re: Why SparkR didn't reuse PythonRDD,Daniel Li <danielli90@gmail.com>,"PythonRDD.scala has a number of PySpark specific conventions (for
example worker reuse, exceptions etc.) and PySpark specific protocols
(e.g. for communicating accumulators, broadcasts between the JVM and
Python etc.). While it might be possible to refactor the two classes
to share some more code I don't think its worth making the code more
complex in order to do that.

Thanks
Shivaram


---------------------------------------------------------------------


"
Joseph Bradley <joseph@databricks.com>,"Thu, 6 Aug 2015 13:37:52 -0700",Re: Make ML Developer APIs public (post-1.4),Eron Wright <ewright@live.com>,"Eron,

Thanks for sending out this list!  We can make some of the critical ones
public for 1.5, but they will be marked DeveloperApi since they may require
changes in the future.  Just made the JIRA: [
https://issues.apache.org/jira/browse/SPARK-9704] and I'll send a PR soon.

Joseph


"
cheez <11besemjaved@seecs.edu.pk>,"Thu, 6 Aug 2015 14:47:09 -0700 (MST)",Bucket mappings of map stage output,dev@spark.apache.org,"Hey all. 
I was trying to understand Spark Internals by looking in to (and hacking)
the code. I was basically trying to explore the buckets which are generated
when we partition the output of each map task and then let the reduce side
fetch them on the basis of paritionId. I went into the write() method of
SortShuffleWriter and there is an Iterator by the name of records passed in
to it as an argument. This key-value pair is what I though represented the
buckets. But upon exploring its contents I realized that i was wrong because
pairs with same keys were being shown in different buckets which should not
have been the case. 
I'd really appreciate if someone could help me find where these buckets
originate.



--

---------------------------------------------------------------------


"
Davies Liu <davies@databricks.com>,"Thu, 6 Aug 2015 15:14:51 -0700",Re: PySpark on PyPi,Olivier Girardot <o.girardot@lateral-thoughts.com>,"We could do that after 1.5 released, it will have same release cycle
as Spark in the future.

o
m>
in
ôve done
ySpark jobs
r=‚ÄúX‚Äù)`
are
definitely
gh
ct
 a
ou
 PyPI.
 the
int,
nd
e
ing in
s
just
by
park
ot
e that
are as
7.
n the
 :
r
al
d when
f
a √©crit
s
ad the
ht now,
ample,
f I want
uch nicer
en
-src.zip:$PYTHONPATH
es
, I bet
of the
on part of
ect
operly.

---------------------------------------------------------------------


"
Vikram Kone <vikramkone@gmail.com>,"Thu, 6 Aug 2015 17:53:54 -0700",Workflow manager tool for scheduling spark jobs on cassandra,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
I'm looking for open source workflow tools/engines that allow us to
schedule spark jobs on a cassandra cluster. Since there are tonnes of
alternatives out there like Ozzie, Azkaban, Luigi , Chronos etc, I wanted
to check with people here to see what they are using today.

Some of the requirements of the workflow engine that I'm looking for are

1. First class support for Spark and  Cassandra.
2. Good open source community support and well tested at production scale.
3. Should be easy to write job dependencices using XML or web itnerface .
Ex; job A depends on Job and Job C, so run Job A after B and C are finished.
4. Time based  recurrent scheduling. Run the spark jobs at a given time
every hour or day or week or month.

Thanks for the inputs
"
Reynold Xin <rxin@databricks.com>,"Thu, 6 Aug 2015 19:47:29 -0700",Re: Fixed number of partitions in RangePartitioner,=?UTF-8?Q?Sergio_Ram=C3=ADrez?= <sramirezga@ugr.es>,"Any reason why you need exactly a certain number of partitions?

empty partitions if the number of distinct elements is small. That would
require changing Spark.

If you want a quick work around, you can also append some random value to
your key, before running range partitioning, and then just remove those
random value post range partitioning.


ote:

em
"
Renyi Xiong <renyixiong0@gmail.com>,"Thu, 6 Aug 2015 20:33:34 -0700",SparkR driver side JNI,dev@spark.apache.org,"why SparkR chose to uses inter-process socket solution eventually on driver
side instead of in-process JNI showed in one of its doc's below (about page
20)?

https://spark-summit.org/wp-content/uploads/2014/07/SparkR-Interactive-R-Programs-at-Scale-Shivaram-Vankataraman-Zongheng-Yang.pdf
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 6 Aug 2015 21:51:51 -0700",Re: SparkR driver side JNI,Renyi Xiong <renyixiong0@gmail.com>,"The in-process JNI only works out when the R process comes up first
and we launch a JVM inside it. In many deploy modes like YARN (or
actually in anything using spark-submit) the JVM comes up first and we
launch R after that. Using an inter-process solution helps us cover
both use cases

Thanks
Shivaram


---------------------------------------------------------------------


"
Sandeep Giri <sandeep@knowbigdata.com>,"Fri, 7 Aug 2015 16:35:43 +0530",Re:,Jonathan Winandy <jonathan.winandy@gmail.com>,"Looks good.

Regards,
Sandeep Giri,
+1 347 781 4573 (US)
+91-953-899-8962 (IN)

www.KnowBigData.com. <http://KnowBigData.com.>
Phone: +1-253-397-1945 (Office)

[image: linkedin icon] <https://linkedin.com/company/knowbigdata> [image:
other site icon] <http://knowbigdata.com>  [image: facebook icon]
<https://facebook.com/knowbigdata> [image: twitter icon]
<https://twitter.com/IKnowBigData> <https://twitter.com/IKnowBigData>



"
Bertrand Dechoux <dechouxb@gmail.com>,"Fri, 7 Aug 2015 22:20:55 +0200",[SPARK-9720] spark.ml Identifiable types should have UID in toString methods,dev@spark.apache.org,"That's mostly the case now but it is not enforced.

Should it be? If so, I can work on a pull-request.

Bertrand
"
Gil Vernik <GILV@il.ibm.com>,"Mon, 10 Aug 2015 14:55:36 +0300",possible issues with listing objects in the HadoopFSrelation ,Dev <dev@spark.apache.org>,"Just some thoughts, hope i didn't missed something obvious.

HadoopFSRelation calls directly FileSystem class to list files in the 
path.
It looks like it implements basically the same logic as in the 
FileInputFormat.listStatus method ( located in 
hadoop-map-reduce-client-core)

The point is that HadoopRDD (or similar ) calls getSplits method that 
calls FileInputFormat.listStatus, while HadoopFSRelation calls FileSystem 
directly and both of them try to achieve ""listing"" of objects.

There might be various issues with this, for example this one 
https://issues.apache.org/jira/browse/SPARK-7868 makes sure that 
""_temporary"" is not returned in a result, but the the listing of 
FileInputFormat contains more logic,  it uses hidden PathFilter like this

  private static final PathFilter hiddenFileFilter = new PathFilter(){
      public boolean accept(Path p){
        String name = p.getName(); 
        return !name.startsWith(""_"") && !name.startsWith("".""); 
      }
    }; 

In addition, custom FileOutputCommitter, may use other name than 
""_temporary"" . 

All this may lead that HadoopFSrelation and HadoopRDD will provide 
different lists from the same data source.

My question is: what the roadmap for this listing in HadoopFSrelation. 
Will it implement exactly the same logic like in 
FileInputFormat.listStatus, or may be one day HadoopFSrelation will call 
FileInputFormat.listStatus and provide custom PathFilter or 
MultiPathFilter? This way there will be single  code that list objects.

Thanks,
Gil.


"
Davies Liu <davies@databricks.com>,"Mon, 10 Aug 2015 11:23:51 -0700",Re: PySpark on PyPi,ellisonbg@gmail.com,"I think so, any contributions on this are welcome.

:
 to
com>
e in
y.
,
is
Äôve done
 PySpark jobs
ter=‚ÄúX‚Äù)`
) are
s definitely
ough
om>
fact
in a
 you
on PyPI.
ad the
ylint,
 and
ore
nging in
its
l just
d by
yspark
 not
rce that
tmare as
267.
to
hen the
to :
n
dir
ry
onal
hed when
 if
be
ves
load the
ight now,
example,
 if I want
 much nicer
when
.1-src.zip:$PYTHONPATH
lves
is, I bet
n of the
E
thon part of
oject
ir
properly.

---------------------------------------------------------------------


"
"""Starch, Michael D (398M)"" <Michael.D.Starch@jpl.nasa.gov>","Mon, 10 Aug 2015 18:54:46 +0000",Pushing Spark to 10Gb/s,"""dev@spark.apache.org"" <dev@spark.apache.org>","All,

I am trying to get data moving in and out of spark at 10Gb/s. I currently have a very powerful cluster to work on, offering 40Gb/s inifiniband links so I believe the network pipe should be fast enough.

Has anyone gotten spark operating at high data rates before? Any advice would be appreciated.

-Michael Starch
---------------------------------------------------------------------


"
Matt Goodman <meawoppl@gmail.com>,"Mon, 10 Aug 2015 12:28:03 -0700",Re: PySpark on PyPi,Davies Liu <davies@databricks.com>,"I would tentatively suggest also conda packaging.

http://conda.pydata.org/docs/

--Matthew Goodman

=====================
Check Out My Website: http://craneium.net
Find me on LinkedIn: http://tinyurl.com/d6wlch


k
‚Äôve
un
ôs
f
e
)
re
e
)
s
e
k
,
e
l
g
d
rk
du> a
r
be
s
zip:$PYTHONPATH
m
or
k
"
Jeremy Freeman <freeman.jeremy@gmail.com>,"Mon, 10 Aug 2015 16:57:50 -0400",Re: Should spark-ec2 get its own repo?,shivaram@eecs.berkeley.edu,"Hi all, definitely a +1 to this plan.

Wanted to also share this library for Spark + GCE by a collaborator of mine, Michael Broxton, which seems to expand and improve on the earlier one Nick pointed us to. It‚Äôs pip installable, not yet on spark-packages, but I‚Äôm sure he‚Äôd be game to add it.

https://github.com/broxtronix/spark-gce <https://github.com/broxtronix/spark-gce>

https://github.com/amplab/spark-ec2/tree/<spark-version>/driver/spark_ec2.py`.
at all
GCE and

"
Ted Yu <yuzhihong@gmail.com>,"Mon, 10 Aug 2015 17:54:08 -0700","Re: Package Release Annoucement: Spark SQL on HBase ""Astro""","""Bing Xiao (Bing)"" <bing.xiao@huawei.com>","Yan / Bing:
Mind taking a look at HBASE-14181
<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame
DataSource to HBase-Spark Module' ?

Thanks


e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 11 Aug 2015 06:11:59 +0000","=?utf-8?B?562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1lbnQ6IFNwYXJrIFNR?=
 =?utf-8?Q?L_on_HBase_""Astro""?=","Ted Yu <yuzhihong@gmail.com>, ""Bing Xiao (Bing)"" <bing.xiao@huawei.com>","Ted,

I‚Äôm in China now, and seem to experience difficulty to access Apache Jira. Anyways, it appears to me  that HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> attempts to support Spark DataFrame inside HBase.
If true, one question to me is whether HBase is intended to have a built-in query engine or not. Or it will stick with the current way as
a k-v store with some built-in processing capabilities in the forms of coprocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
built on top of it.

Thanks,

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà11Êó• 8:54
Êî∂‰ª∂‰∫∫: Bing Xiao (Bing)
ÊäÑÈÄÅ: dev@spark.apache.org; user@spark.apache.org; Yan Zhou.sc
‰∏ªÈ¢ò: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan / Bing:
Mind taking a look at HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame DataSource to HBase-Spark Module' ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 11 Aug 2015 00:27:42 -0700","=?utf-8?Q?Re:_=E7=AD=94=E5=A4=8D:_Package_Release_Annoucement:_S?=
 =?utf-8?Q?park_SQL_on_HBase_""Astro""?=","""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","HBase will not have query engine. 

It will provide better support to query engines. 

Cheers



he Jira. Anyways, it appears to me  that HBASE-14181 attempts to support Spark DataFrame inside HBase.
n query engine or not. Or it will stick with the current way as
rocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
:54
sc
stro""
-Spark Module' ?
rote:
elease.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
:
igent scan, based on partial evaluation technique
ssor to support ultra low latency processing
 filter, Primary Key, Bulk load, Update)
k load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
he integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
al team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
 ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 11 Aug 2015 08:07:38 +0000","=?utf-8?B?562U5aSNOiDnrZTlpI06IFBhY2thZ2UgUmVsZWFzZSBBbm5vdWNlbWVudDog?=
 =?utf-8?Q?Spark_SQL_on_HBase_""Astro""?=",Ted Yu <yuzhihong@gmail.com>,"Ok. Then a question will be to define a boundary between a query engine and a built-in processing. If, for instance, the Spark DataFrame functionalities involving shuffling are to be supported inside HBase,
in my opinion, it‚Äôd be hard not to tag it as an query engine. If, on the other hand, only map-side ops from DataFrame are to be supported inside HBase, then Astro‚Äôs coprocessor already has the capabilities.

Again, I still have no full knowledge about HBase-14181 beyond your description in email. So my opinion above might be skewed as result.

Regards,
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà11Êó• 15:28
Êî∂‰ª∂‰∫∫: Yan Zhou.sc
ÊäÑÈÄÅ: Bing Xiao (Bing); dev@spark.apache.org; user@spark.apache.org
‰∏ªÈ¢ò: Re: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

HBase will not have query engine.

It will provide better support to query engines.

Cheers


On Aug 10, 2015, at 11:11 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

I‚Äôm in China now, and seem to experience difficulty to access Apache Jira. Anyways, it appears to me  that HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> attempts to support Spark DataFrame inside HBase.
If true, one question to me is whether HBase is intended to have a built-in query engine or not. Or it will stick with the current way as
a k-v store with some built-in processing capabilities in the forms of coprocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
built on top of it.om]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà11Êó• 8:54
Êî∂‰ª∂‰∫∫: Bing Xiao (Bing)
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>; Yan Zhou.sc
‰∏ªÈ¢ò: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan / Bing:
Mind taking a look at HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame DataSource to HBase-Spark Module' ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team


"
Reynold Xin <rxin@databricks.com>,"Tue, 11 Aug 2015 01:10:05 -0700","Re: [discuss] Removing individual commit messages from the squash
 commit message",Manoj Kumar <manojkumarsivaraj334@gmail.com>,"This is now done with this pull request:
https://github.com/apache/spark/pull/8091


Committers please update the script to get this ""feature"".



"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 11 Aug 2015 14:34:20 +0530",Re: Pushing Spark to 10Gb/s,"""Starch, Michael D (398M)"" <Michael.D.Starch@jpl.nasa.gov>","Hi Starch,

It also depends on the applications behavior, some might not be properly
able to utilize the network. If you are using say Kafka, then one thing
that you should keep in mind is the Size of the individual message and the
number of partitions that you are having. The higher the message size and
higher number of partitions (in kafka) will utilize the network properly.
With this combination, we have operated few pipelines running at 10Gb/s (~
1GB/s ).

Thanks
Best Regards


"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 11 Aug 2015 14:43:50 +0530",Re: Inquery about contributing codes,Hyukjin Kwon <gurwls223@gmail.com>,"You can create a new Issue and send a pull request for the same i think.

+ dev list

Thanks
Best Regards


"
Jeff Zhang <zjffdu@gmail.com>,"Tue, 11 Aug 2015 17:25:24 +0800",Is OutputCommitCoordinator necessary for all the stages ?,dev@spark.apache.org,"As my understanding, OutputCommitCoordinator should only be necessary for
ResultStage (especially for ResultStage with hdfs write), but currently it
is used for all the stages. Is there any reason for that ?

-- 
Best Regards

Jeff Zhang
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 11 Aug 2015 19:29:07 +0530",Spark runs into an Infinite loop even if the tasks are completed successfully,"""user@spark.apache.org"" <user@spark.apache.org>, dev <dev@spark.apache.org>","Hi

My Spark job (running in local[*] with spark 1.4.1) reads data from a
thrift server(Created an RDD, it will compute the partitions in
getPartitions() call and in computes hasNext will return records from these
partitions), count(), foreach() is working fine it returns the correct
number of records. But whenever there is shuffleMap stage (like reduceByKey
etc.) then all the tasks are executing properly but it enters in an
infinite loop saying :


   1. 15/08/11 13:05:54 INFO DAGScheduler: Resubmitting ShuffleMapStage 1 (map
   at FilterMain.scala:59) because some of its tasks had failed: 0, 3


Here's the complete stack-trace http://pastebin.com/hyK7cG8S

What could be the root cause of this problem? I looked up and bumped into
this closed JIRA <https://issues.apache.org/jira/browse/SPARK-583> (which
is very very old)




Thanks
Best Regards
"
gen tang <gen.tang86@gmail.com>,"Tue, 11 Aug 2015 22:11:35 +0800",Potential bug broadcastNestedLoopJoin or default value of spark.sql.autoBroadcastJoinThreshold,dev@spark.apache.org,"Hi,

Recently, I use spark sql to do join on non-equality condition, condition1
or condition2 for example.

Spark will use broadcastNestedLoopJoin to do this. Assume that one of
dataframe(df1) is not created from hive table nor local collection and the
other one is created from hivetable(df2). For df1, spark will use
defaultSizeInBytes * length of df1 to estimate the size of df1 and use
correct size for df2.

As the result, in most cases, spark will think df1 is bigger than df2 even
df2 is really huge. And spark will do df2.collect(), which will cause error
or slowness of program.

Maybe we should just use defaultSizeInBytes for logicalRDD, not
defaultSizeInBytes * length?

Hope this could be helpful
Thanks a lot in advance for your help and input.

Cheers
Gen
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 11 Aug 2015 14:47:02 +0000 (UTC)","Re: Master JIRA ticket for tracking Spark 1.5.0 configuration
 renames, defaults changes, and configuration deprecation","Josh Rosen <joshrosen@databricks.com>, Dev <dev@spark.apache.org>","is there a jira for in compatibilities? ¬†I was just trying spark 1.5 and it appears with dataframe aggregates (like sum) now return columns named sum(columname) where as in spark 1.4 it was SUM(columnname). ¬†note the capital vs lower case.
I wanted to check and make sure this was a known change?
Tom 


   

 To help us track planned / finished configuration renames, defaults changes, and configuration deprecation for the upcoming 1.5.0 release, I have created https://issues.apache.org/jira/browse/SPARK-9550.
As you make configuration changes or think of configurations that need to be audited, please update that ticket by editing it or posting a comment.
This ticket will help us later when it comes time to draft release notes.
Thanks,Josh

  "
Josh Rosen <rosenville@gmail.com>,"Tue, 11 Aug 2015 09:17:14 -0700",Re: Is OutputCommitCoordinator necessary for all the stages ?,dev@spark.apache.org,"Can you clarify what you mean by ""used for all stages""? 
OutputCommitCoordinator RPCs should only be initiated through 
SparkHadoopMapRedUtil.commitTask(), so while the OutputCommitCoordinator 
doesn't make a distinction between ShuffleMapStages and ResultStages 
there still should not be a performance penalty for this because the 
extra rounds of RPCs should only be performed when necessary.



---------------------------------------------------------------------


"
Pala M Muthaia <mchettiar@rocketfuelinc.com>,"Tue, 11 Aug 2015 12:25:25 -0700",Sources/pom for org.spark-project.hive,dev <dev@spark.apache.org>,"Hi,

I am trying to make Spark SQL 1.4 work with our internal fork of Hive. We
have some customizations in Hive (custom authorization, various hooks etc)
that are all part of hive-exec.

Given Spark's hive dependency is through org.spark-project.hive groupId,
looks like i need to modify the definition of hive-exec artifact there to
take dependency on our internal hive (vs org.apache.hive), and then
everything else would flow through.

However, i am unable to find sources for org.spark-project.hive to make
this change. Is it available? Otherwise, how can i proceed in this
situation?


Thanks,
pala
"
Ted Yu <yuzhihong@gmail.com>,"Tue, 11 Aug 2015 12:56:31 -0700",Re: Sources/pom for org.spark-project.hive,Pala M Muthaia <mchettiar@rocketfuelinc.com>,"Have you looked at
https://github.com/pwendell/hive/tree/0.13.1-shaded-protobuf ?

Cheers


"
westurner <wes.turner@gmail.com>,"Tue, 11 Aug 2015 13:55:24 -0700 (MST)",Re: PySpark on PyPi,dev@spark.apache.org,"Matt Goodman wrote

$ conda skeleton pypi pyspark
# update git_tag and git_uri
# add test commands (import pyspark; import pyspark.[...])

Docs for building conda packages for multiple operating systems and
interpreters from PyPi packages:

*
http://www.pydanny.com/building-conda-packages-for-multiple-operating-systems.html
* https://github.com/audreyr/cookiecutter/issues/232


Matt Goodman wrote












run
ôs




d
,


e


.
 be
.zip:$PYTHONPATH




l













--
3.nabble.com/PySpark-on-PyPi-tp12626p13635.html
om.

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Tue, 11 Aug 2015 21:29:45 +0000",Re: Sources/pom for org.spark-project.hive,Pala M Muthaia <mchettiar@rocketfuelinc.com>,"

Hi,

I am trying to make Spark SQL 1.4 work with our internal fork of Hive. We have some customizations in Hive (custom authorization, various hooks etc) that are all part of hive-exec.

Given Spark's hive dependency is through org.spark-project.hive groupId, looks like i need to modify the definition of hive-exec artifact there to take dependency on our internal hive (vs org.apache.hive), and then everything else would flow through.


you can just change the hive group definition in your spark build; that's the easy part
<hive.group>org.spark-project.hive</hive.group>

harder is getting a consistent kryo binding and any other shading/unshading. In SPARK-8064 we've moved Spark 1.5 to using Hive 1.2.1, but even there we had to patch hive to use the same Kryo version, and shade protobuf in hive-exec for everything to work on Hadoop 1.x.


However, i am unable to find sources for org.spark-project.hive to make this change. Is it available? Otherwise, how can i proceed in this situation?

Ted's pointed to the 0.13 code; the 1.2.1 is under https://github.com/pwendell/hive/commits/release-1.2.1-spark

however: do not attempt to change hive versions in a release, things are intertwined at the SparkSQL level your code just won't work.



Thanks,
pala

"
westurner <wes.turner@gmail.com>,"Tue, 11 Aug 2015 14:31:53 -0700 (MST)",Re: PySpark on PyPi,dev@spark.apache.org,"westurner wrote
tems.html

* conda meta.yaml can specify e.g. a test.sh script(s) that should return 0
 
  Docs: http://conda.pydata.org/docs/building/meta-yaml.html#test-section


Wes Turner wrote












e
 run
Äôs




s
H
n


V


d
c.zip:$PYTHONPATH


a
d
s


e
n













--
3.nabble.com/PySpark-on-PyPi-tp12626p13637.html
om.

---------------------------------------------------------------------


"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 11 Aug 2015 22:58:54 +0000","=?utf-8?B?562U5aSNOiDnrZTlpI06IFBhY2thZ2UgUmVsZWFzZSBBbm5vdWNlbWVudDog?=
 =?utf-8?Q?Spark_SQL_on_HBase_""Astro""?=",Ted Yu <yuzhihong@gmail.com>,"Finally I can take a look at HBASE-14181 now. Unfortunately there is no design doc mentioned. Superficially it is very similar to Astro with a difference of
this being part of HBase client library; while Astro works as a Spark package so will evolve and function more closely with Spark SQL/Dataframe instead of HBase.

In terms of architecture, my take is loosely-coupled query engines on top of KV store vs. an array of query engines supported by, and packaged as part of, a KV store.

Functionality-wise the two could be close but Astro also supports Python as a result of tight integration with Spark.
It will be interesting to see performance comparisons when HBase-14181 imail.com]
Sent: Tuesday, August 11, 2015 3:28 PM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org; user@spark.apache.org
Subject: Re: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

HBase will not have query engine.

It will provide better support to query engines.

Cheers


On Aug 10, 2015, at 11:11 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

I‚Äôm in China now, and seem to experience difficulty to access Apache Jira. Anyways, it appears to me  that HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> attempts to support Spark DataFrame inside HBase.
If true, one question to me is whether HBase is intended to have a built-in query engine or not. Or it will stick with the current way as
a k-v store with some built-in processing capabilities in the forms of coprocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
built on top of it.

Thanks,

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà11Êó• 8:54
Êî∂‰ª∂‰∫∫: Bing Xiao (Bing)
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spapache.org>; Yan Zhou.sc
‰∏ªÈ¢ò: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan / Bing:
Mind taking a look at HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame DataSource to HBase-Spark Module' ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team


"
Ted Yu <yuzhihong@gmail.com>,"Tue, 11 Aug 2015 16:01:43 -0700","=?UTF-8?B?UmU6IOetlOWkjTog562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1lbnQ6IFNwYQ==?=
	=?UTF-8?B?cmsgU1FMIG9uIEhCYXNlICJBc3RybyI=?=","""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Yan:
Where can I find performance numbers for Astro (it's close to middle of
August) ?

Cheers

:

 on HBase ""Astro""
che Jira.
pled query
ihong@gmail.com>]
• 8:54
Zhou.sc
se ""Astro""
e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Tue, 11 Aug 2015 23:13:17 +0000","=?utf-8?B?UkU6IOetlOWkjTog562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1l?=
 =?utf-8?Q?nt:_Spark_SQL_on_HBase_""Astro""?=",Ted Yu <yuzhihong@gmail.com>,"We have not ‚Äúformally‚Äù published any numbers yet. A good reference is a slide deck we posted for the meetup in March.
, or better yet for interested parties to run performance comparisons by themselves for now.

As for status quo of Astro, we have been focusing on fixing bugs (UDF-related bug in some coprocessor/custom filter combos), and add support of querying string columns in HBase as integers from Astro.

Thanks,

From: Ted Yu [mailto:yuzhihong@gmail.com]
Sent: Wednesday, August 12, 2015 7:02 AM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org; user@spark.apache.org
Subject: Re: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan:
Where can I find performance numbers for Astro (it's close to middle of August) ?

Cheers

On Tue, Aug 11, 2015 at 3:58 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Finally I can take a look at HBASE-14181 now. Unfortunately there is no design doc mentioned. Superficially it is very similar to Astro with a difference of
this being part of HBase client library; while Astro works as a Spark package so will evolve and function more closely with Spark SQL/Dataframe instead of HBase.

In terms of architecture, my take is loosely-coupled query engines on top of KV store vs. an array of query engines supported by, and packaged as part of, a KV store.

Functionality-wise the two could be close but Astro also supports Python as a result of tight integration with Spark.
It will be interesting to see performance comparisons when HBase-14181 is ready.

Thanks,


From: Ted Yu [mailto:yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
Sent: Tuesday, August 11, 2015 3:28 PM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.ap§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

HBase will not have query engine.

It will provide better support to query engines.

Cheers

On Aug 10, 2015, at 11:11 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

I‚Äôm in China now, and seem to experience difficulty to access Apache Jira. Anyways, it appears to me  that HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> attempts to support Spark DataFrame inside HBase.
If true, one question to me is whether HBase is intended to have a built-in query engine or not. Or it will stick with the current way as
a k-v store with some built-in processing capabilities in the forms of coprocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
built on top of it.

Thanks,

Âèë‰ª∂‰∫∫: Ted Yu [mailto4
Êî∂‰ª∂‰∫∫: Bing Xiao (Bing)
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>; Yan Zhou.sc
‰∏ªÈ¢ò: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan / Bing:
Mind taking a look at HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame DataSource to HBase-Spark Module' ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team



"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 11 Aug 2015 19:28:22 -0400","=?UTF-8?B?UkU6IOetlOWkjTog562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1lbnQ6IFNwYQ==?=
	=?UTF-8?B?cmsgU1FMIG9uIEhCYXNlICJBc3RybyI=?=","""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Hey Yan,

I've been the one building out this spark functionality in hbase so maybe I
can help clarify.

The hbase-spark module is just focused on making spark integration with
hbase easy and out of the box for both spark and spark streaming.

I and I believe the hbase team has no desire to build a sql engine in
hbase.  This jira comes the closest to that line.  The main thing here is
filter push down logic for basic sql operation like =, >
, and <.  User define functions and secondary indexes are not in my scope.

Another main goal of hbase-spark module is to be able to allow a user to
do  anything they did with MR/HBase now with Spark/Hbase.  Things like bulk
load.

Let me know if u have any questions

Ted Malaska

reference is a
rt
noucement: Spark SQL on HBase
 on HBase ""Astro""
che Jira.
pled query
ihong@gmail.com>]
• 8:54
Zhou.sc
se ""Astro""
e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
Jeff Zhang <zjffdu@gmail.com>,"Wed, 12 Aug 2015 07:35:01 +0800",Re: Is OutputCommitCoordinator necessary for all the stages ?,Josh Rosen <rosenville@gmail.com>,"Hi Josh,

I mean on the driver side. OutputCommitCorrdinator.startStage is called in
DAGScheduler#submitMissingTasks for all the stages (cost some memory).
Although it is fine that as long as executor side don't call RPC, there's
no much performance penalty.




-- 
Best Regards

Jeff Zhang
"
Pala M Muthaia <mchettiar@rocketfuelinc.com>,"Tue, 11 Aug 2015 17:29:41 -0700",Re: Sources/pom for org.spark-project.hive,Steve Loughran <stevel@hortonworks.com>,"Thanks for the pointers. Yes, i started with changing the hive.group
property in pom and started seeing various dependency issues.

Initially i thought spark-project.hive was just a pom for uber jars that
pull in hive classes without transitive dependencies like kryo, but looks
like lot more changes are needed, including editing the sources.

We are looking at alternative approaches, since our customizations to Hive
are pretty limited and may not warrant the effort required here.

Thanks.


"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Wed, 12 Aug 2015 00:48:46 +0000","=?utf-8?B?UkU6IOetlOWkjTog562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1l?=
 =?utf-8?Q?nt:_Spark_SQL_on_HBase_""Astro""?=",Ted Malaska <ted.malaska@cloudera.com>,"Ted,

Thanks for pointing out more details of HBase-14181. I am afraid I may still need to learn more before I can make very accurate and pointed comments.

As for filter push down, Astro has a powerful approach to basically break down arbitrarily complex logic expressions comprising of AND/OR/IN/NOT
to generate partition-specific predicates to be pushed down to HBase. This may not be a significant performance improvement if the filter logic is simple and/or the processing is IO-bound,
but could be so for online ad-hoc analysis.

For UDFs, Astro supports it both in and out of HBase custom filter.

For secondary index, Astro do not support it now. With the probable support by HBase in the future(thanks to Ted Yu‚Äôs comments a while ago), we could add this support along with its specific optimizations.

For bulk load, Astro has a much faster way to load the tabular data, we believe.

Right now, Astro‚Äôs filter pushdown is through HBase built-in filters and custom filter.

As for HBase-14181, I see some overlaps with Astro. Both have dependences on Spark SQL, and both supports Spark Dataframe as an access interface, both supports predicate pushdown.
Astro is not designed for MR (or Spark‚Äôs equivalent) access though.

If HBase-14181 is shooting for access to HBase data through a subset of DataFrame functionalities like filter, projection, and other map-side ops, would it be feasible to decouple it from Spark?
My understanding is that 14181 does not run Spark execution engine at all, but will make use of Spark Dataframe semantic and/or logic planning to pass a logic (sub-)plan to the HBase. If true, it might
be desirable to directly support Dataframe in HBase.

Thanks,


From: Ted Malaska [mailto:ted.malaska@cloudera.com]
Sent: Wednesday, August 12, 2015 7:28 AM
To: Yan Zhou.sc
Cc: user; dev@spark.apache.org; Bing Xiao (Bing); Ted Yu
Subject: RE: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""


Hey Yan,

I've been the one building out this spark functionality in hbase so maybe I can help clarify.

The hbase-spark module is just focused on making spark integration with hbase easy and out of the box for both spark and spark streaming.

I and I believe the hbase team has no desire to build a sql engine in hbase.  This jira comes the closest to that line.  The main thing here is filter push down logic for basic sql operation like =, >
, and <.  User define functions and secondary indexes are not in my scope.

Another main goal of hbase-spark module is to be able to allow a user to do  anything they did with MR/HBase now with Spark/Hbase.  Things like bulk load.

Let me know if u have any questions

Ted Malaska
On Aug 11, 2015 7:13 PM, ""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
We have not ‚Äúformally‚Äù published any numbers yet. A good reference is a slide deck we posted for the meetup in March.
, or better yet for interested parties to run performance comparisons by themselves for now.

As for status quo of Astro, we have been focusing on fixing bugs (UDF-related bug in some coprocessor/custom filter combos), and add support of querying string columns in HBase as integers from Astro.
15 7:02 AM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Re: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan:
Where can I find performance numbers for Astro (it's close to middle of August) ?

Cheers

On Tue, Aug 11, 2015 at 3:58 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Finally I can take a look at HBASE-14181 now. Unfortunately there is no design doc mentioned. Superficially it is very similar to Astro with a difference of
this being part of HBase client library; while Astro works as a Spark package so will evolve and function more closely with Spark SQL/Dataframe instead of HBase.

In terms of architecture, my take is loosely-coupled query engines on top of KV store vs. an array of query engines supported by, and packaged as part of, a KV store.

Functionality-wise the two could be close but Astro also supports Python as a result of tight integration with Spark.
It will be interesting to see performance comparisons when HBase-14181 is r 11, 2015 3:28 PM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Re: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

HBase will not have query engine.

It will provide better support to query engines.

Cheers

On Aug 10, 2015, at 11:11 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

I‚Äôm in China now, and seem to experience difficulty to access Apache Jira. Anyways, it appears to me  that HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> attempts to support Spark DataFrame inside HBase.
If true, one question to me is whether HBase is intended to have a built-in query engine or not. Or it will stick with the current way as
a k-v store with some built-in processing capabilities in the forms of coprocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
built on top of it.

Thanks,

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà11Êó• 8:54
Êî∂‰ª∂‰∫∫: Bing Xiao (Bing)
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>; Yan Zhou.sc
‰∏ªÈ¢ò: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan / Bing:
Mind taking a look at HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame DataSource to HBase-Spark Module' ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team



"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 11 Aug 2015 20:55:32 -0400","=?UTF-8?B?UkU6IOetlOWkjTog562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1lbnQ6IFNwYQ==?=
	=?UTF-8?B?cmsgU1FMIG9uIEhCYXNlICJBc3RybyI=?=","""Yan Zhou. sc"" <Yan.Zhou.sc@huawei.com>","The bulk load code is 14150 if u r interested.  Let me know how it can be
made faster.

It's just a spark shuffle and writing hfiles.   Unless astro wrote it's own
shuffle the times should be very close.

s
e ago), we
ers and
h.
,
,
ss
noucement: Spark SQL on HBase
.
lk
reference is a
rt
noucement: Spark SQL on HBase
 on HBase ""Astro""
che Jira.
pled query
ihong@gmail.com>]
• 8:54
Zhou.sc
se ""Astro""
e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Wed, 12 Aug 2015 01:02:39 +0000","=?utf-8?B?UkU6IOetlOWkjTog562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1l?=
 =?utf-8?Q?nt:_Spark_SQL_on_HBase_""Astro""?=",Ted Malaska <ted.malaska@cloudera.com>,"No, Astro bulkloader does not use its own shuffle. But map/reduce-side processing is somewhat different from HBase‚Äôs bulk loader that are used by many HBase apps I believe.

From: Ted Malaska [mailto:ted.malaska@cloudera.com]
Sent: Wednesday, August 12, 2015 8:56 AM
To: Yan Zhou.sc
Cc: dev@spark.apache.org; Ted Yu; Bing Xiao (Bing); user
Subject: RE: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""


The bulk load code is 14150 if u r interested.  Let me know how it can be made faster.

It's just a spark shuffle and writing hfiles.   Unless astro wrote it's own shuffle the times should be very close.
On Aug 11, 2015 8:49 PM, ""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

Thanks for pointing out more details of HBase-14181. I am afraid I may still need to learn more before I can make very accurate and pointed comments.

As for filter push down, Astro has a powerful approach to basically break down arbitrarily complex logic expressions comprising of AND/OR/IN/NOT
to generate partition-specific predicates to be pushed down to HBase. This may not be a significant performance improvement if the filter logic is simple and/or the processing is IO-bound,
but could be so for online ad-hoc analysis.

For UDFs, Astro supports it both in and out of HBase custom filter.

For secondary index, Astro do not support it now. With the probable support by HBase in the future(thanks to Ted Yu‚Äôs comments a while ago), we could add this support along with its specific optimizations.

For bulk load, Astro has a much faster way to load the tabular data, we believe.

Right now, Astro‚Äôs filter pushdown is through HBase built-in filters and custom filter.

As for HBase-14181, I see some overlaps with Astro. Both have dependences on Spark SQL, and both supports Spark Dataframe as an access interface, both supports predicate pushdown.
Astro is not designed for MR (or Spark‚Äôs equivalent) access though.

If HBase-14181 is shooting for access to HBase data through a subset of DataFrame functionalities like filter, projection, and other map-side ops, would it be feasible to decouple it from Spark?
My understanding is that 14181 does not run Spark execution engine at all, but will make use of Spark Dataframe semantic and/or logic planning to pass a logic (sub-)plan to the HBase. If true, it might
be desirable to directly support Dataframe in HBase.

Thanks,


From: Ted Malaska [mailto:ted.malaska@cloudera.com<mailto:ted.malaska@cloudera.com>]
Sent: Wednesday, August 12, 2015 7:28 AM
To: Yan Zhou.sc
Cc: user; dev@spark.apache.org<mailto:dev@spark.apache.org>; Bing Xiao (Bing); Ted Yu
Subject: RE: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""


Hey Yan,

I've been the one building out this spark functionality in hbase so maybe I can help clarify.

The hbase-spark module is just focused on making spark integration with hbase easy and out of the box for both spark and spark streaming.

I and I believe the hbase team has no desire to build a sql engine in hbase.  This jira comes the closest to that line.  The main thing here is filter push down logic for basic sql operation like =, >
, and <.  User define functions and secondary indexes are not in my scope.

Another main goal of hbase-spark module is to be able to allow a user to do  anything they did with MR/HBase now with Spark/Hbase.  Things like bulk load.

Let me know if u have any questions

Ted Malaska
On Aug 11, 2015 7:13 PM, ""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
We have not ‚Äúformally‚Äù published any numbers yet. A good reference is a slide deck we posted for the meetup in March.
, or better yet for interested parties to run performance comparisons by themselves for now.

As for status quo of Astro, we have been focusing on fixing bugs (UDF-related bug in some coprocessor/custom filter combos), and add support of querying string columns in HBase as integers fugust 12, 2015 7:02 AM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Re: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan:
Where can I find performance numbers for Astro (it's close to middle of August) ?

Cheers

On Tue, Aug 11, 2015 at 3:58 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Finally I can take a look at HBASE-14181 now. Unfortunately there is no design doc mentioned. Superficially it is very similar to Astro with a difference of
this being part of HBase client library; while Astro works as a Spark package so will evolve and function more closely with Spark SQL/Dataframe instead of HBase.

In terms of architecture, my take is loosely-coupled query engines on top of KV store vs. an array of query engines supported by, and packaged as part of, a KV store.

Functionality-wise the two could be close but Astro also supports Python as a result of tight integration with Spark.
It will be interesting to see performance comparisons when HBassday, August 11, 2015 3:28 PM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Re: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

HBase will not have query engine.

It will provide better support to query engines.

Cheers

On Aug 10, 2015, at 11:11 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

I‚Äôm in China now, and seem to experience difficulty to access Apache Jira. Anyways, it appears to me  that HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> attempts to support Spark DataFrame inside HBase.
If true, one question to me is whether HBase is intended to have a built-in query engine or not. Or it will stick with the current way as
a k-v store with some built-in processing capabilities in the forms of coprocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
built on top of it.

Thanks,

Âèë‰ª∂‰∫∫: Ted Yu [mailto:yuzhihong@gmail.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà11Êó• 8:54
Êî∂‰ª∂‰∫∫: Bing Xiao (Bing)
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>; Yan Zhou.sc
‰∏ªÈ¢ò: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan / Bing:
Mind taking a look at HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame DataSource to HBase-Spark Module' ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team



"
Ted Malaska <ted.malaska@cloudera.com>,"Tue, 11 Aug 2015 21:13:50 -0400","=?UTF-8?B?UkU6IOetlOWkjTog562U5aSNOiBQYWNrYWdlIFJlbGVhc2UgQW5ub3VjZW1lbnQ6IFNwYQ==?=
	=?UTF-8?B?cmsgU1FMIG9uIEhCYXNlICJBc3RybyI=?=","""Yan Zhou. sc"" <Yan.Zhou.sc@huawei.com>","There a number of ways to bulk load.

There is bulk put, partition bulk put, mr bulk load, and now hbase-14150
which is spark shuffle bulk load.

Let me know if I have missed a bulk loading option.  All these r possible
with the new hbase-spark module.

As for the filter push down discussion in the past email.  U will note in
14181 that the filter push will also limit the scan range or drop scan all
together for gets.

Ted Malaska

e used by
noucement: Spark SQL on HBase
s
e ago), we
ers and
h.
,
,
ss
noucement: Spark SQL on HBase
.
lk
reference is a
rt
noucement: Spark SQL on HBase
 on HBase ""Astro""
che Jira.
pled query
ihong@gmail.com>]
• 8:54
Zhou.sc
se ""Astro""
e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Wed, 12 Aug 2015 02:12:17 +0000","RE: Potential bug broadcastNestedLoopJoin or default value of
 spark.sql.autoBroadcastJoinThreshold","gen tang <gen.tang86@gmail.com>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","Firstly, spark.sql.autoBroadcastJoinThreshold only works for the EQUAL JOIN.

Currently, for the non-equal join, if the join type is the INNER join, then it will be done by CartesianProduct join and BroadcastNestedLoopJoin works for the outer joins.

In the BroadcastnestedLoopJoin, the table with smaller estimate size will be broadcasted, but if the smaller table is also a huge table, I don‚Äôt think Spark SQL can handle that right now (OOM).

So, I am not sure how you created the df1 instance, but we‚Äôd better to reflect the real size for the statistics of it, and let the framework decide what to do, hopefully Spark Sql can support the non-equal join for large tables in the next release.

Hao

From: gen tang [mailto:gen.tang86@gmail.com]
Sent: Tuesday, August 11, 2015 10:12 PM
To: dev@spark.apache.org
Subject: Potential bug broadcastNestedLoopJoin or default value of spark.sql.autoBroadcastJoinThreshold

Hi,

Recently, I use spark sql to do join on non-equality condition, condition1 or condition2 for example.

Spark will use broadcastNestedLoopJoin to do this. Assume that one of dataframe(df1) is not created from hive table nor local collection and the other one is created from hivetable(df2). For df1, spark will use defaultSizeInBytes * length of df1 to estimate the size of df1 and use correct size for df2.

As the result, in most cases, spark will think df1 is bigger than df2 even df2 is really huge. And spark will do df2.collect(), which will cause error or slowness of program.

Maybe we should just use defaultSizeInBytes for logicalRDD, not defaultSizeInBytes * length?

Hope this could be helpful
Thanks a lot in advance for your help and input.

Cheers
Gen

"
"""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Wed, 12 Aug 2015 04:18:31 +0000","=?utf-8?B?562U5aSNOiDnrZTlpI06IOetlOWkjTogUGFja2FnZSBSZWxlYXNlIEFubm91?=
 =?utf-8?Q?cement:_Spark_SQL_on_HBase_""Astro""?=",Ted Malaska <ted.malaska@cloudera.com>,"We are using MR-based bulk loading on Spark.

For filter pushdown, Astro does partition-pruning, scan range pruning, and use Gets as much as possible.

Thanks,


Âèë‰ª∂‰∫∫: Ted Malaska [mailto:ted.malaska@cloudera.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà12Êó• 9:14
Êî∂‰ª∂‰∫∫: Yan Zhou.sc
ÊäÑÈÄÅ: dev@spark.apache.org; Bing Xiao (Bing); Ted Yu; user
‰∏ªÈ¢ò: RE: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""


There a number of ways to bulk load.

There is bulk put, partition bulk put, mr bulk load, and now hbase-14150 which is spark shuffle bulk load.

Let me know if I have missed a bulk loading option.  All these r possible with the new hbase-spark module.

As for the filter push down discussion in the past email.  U will note in 14181 that the filter push will also limit the scan range or drop scan all together for gets.

Ted Malaska
On Aug 11, 2015 9:06 PM, ""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
No, Astro bulkloader does not use its own shuffle. But map/reduce-side processing is somewhat different from HBase‚Äôs bulk loader that are used by many HBase apps I believe.

From: Ted Malaska [mailto:ted.mal: Wednesday, August 12, 2015 8:56 AM
To: Yan Zhou.sc
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>; Ted Yu; Bing Xiao (Bing); user
Subject: RE: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""


The bulk load code is 14150 if u r interested.  Let me know how it can be made faster.

It's just a spark shuffle and writing hfiles.   Unless astro wrote it's own shuffle the times should be very close.
On Aug 11, 2015 8:49 PM, ""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

Thanks for pointing out more details of HBase-14181. I am afraid I may still need to learn more before I can make very accurate and pointed comments.

As for filter push down, Astro has a powerful approach to basically break down arbitrarily complex logic expressions comprising of AND/OR/IN/NOT
to generate partition-specific predicates to be pushed down to HBase. This may not be a significant performance improvement if the filter logic is simple and/or the processing is IO-bound,
but could be so for online ad-hoc analysis.

For UDFs, Astro supports it both in and out of HBase custom filter.

For secondary index, Astro do not support it now. With the probable support by HBase in the future(thanks to Ted Yu‚Äôs comments a while ago), we could add this support along with its specific optimizations.

For bulk load, Astro has a much faster way to load the tabular data, we believe.

Right now, Astro‚Äôs filter pushdown is through HBase built-in filters and custom filter.

As for HBase-14181, I see some overlaps with Astro. Both have dependences on Spark SQL, and both supports Spark Dataframe as an access interface, both supports predicate pushdown.
Astro is not designed for MR (or Spark‚Äôs equivalent) access though.

If HBase-14181 is shooting for access to HBase data through a subset of DataFrame functionalities like filter, projection, and other map-side ops, would it be feasible to decouple it from Spark?
My understanding is that 14181 does not run Spark execution engine at all, but will make use of Spark Dataframe semantic and/or logic planning to pass a logic (sub-)plan to the HBase. If true, it might
be desirable to directly support Dataframe in HBase.

Thanks,


From: Ted Malcloudera.com>]
Sent: Wednesday, August 12, 2015 7:28 AM
To: Yan Zhou.sc
Cc: user; dev@spark.apache.org<mailto:dev@spark.apache.org>; Bing Xiao (Bing); Ted Yu
Subject: RE: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""


Hey Yan,

I've been the one building out this spark functionality in hbase so maybe I can help clarify.

The hbase-spark module is just focused on making spark integration with hbase easy and out of the box for both spark and spark streaming.

I and I believe the hbase team has no desire to build a sql engine in hbase.  This jira comes the closest to that line.  The main thing here is filter push down logic for basic sql operation like =, >
, and <.  User define functions and secondary indexes are not in my scope.

Another main goal of hbase-spark module is to be able to allow a user to do  anything they did with MR/HBase now with Spark/Hbase.  Things like bulk load.

Let me know if u have any questions

Ted Malaska
On Aug 11, 2015 7:13 PM, ""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
We have not ‚Äúformally‚Äù published any numbers yet. A good reference is a slide deck we posted for the meetup in March.
, or better yet for interested parties to run performance comparisons by themselves for now.

As for status quo of Astro, we have been focusing on fixing bugs (UDF-related bug in some coprocessor/custom filter combos), and add support of querying string columns in HBase as integers from Astro.

Thanks,

From: Ted Yu [mailto:yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
Sent: Wednesday, August 12, 2015 7:02 AM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>
Subject: Re: Á≠îÂ§ç: Á≠îÂ§ç: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan:
Where can I find performance numbers for Astro (it's close to middle of August) ?

Cheers

On Tue, Aug 11, 2015 at 3:58 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Finally I can take a look at HBASE-14181 now. Unfortunately there is no design doc mentioned. Superficially it is very similar to Astro with a difference of
this being part of HBase client library; while Astro works as a Spark package so will evolve and function more closely with Spark SQL/Dataframe instead of HBase.

In terms of architecture, my take is loosely-coupled query engines on top of KV store vs. an array of query engines supported by, and packaged as part of, a KV store.

Functionality-wise the two could be close but Astro also supports Python as a result of tight integration with Spark.
It will be interesting to see performance comparisons when HBase-14181 is ready.

Thanks,


From: Ted Yu [mailto:yuzhihong@gmail.com<mailto:yuzhihong@gmail.com>]
Sent: Tuesday, August 11, 2015 3:28 PM
To: Yan Zhou.sc
Cc: Bing Xiao (Bing); dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailtoease Annoucement: Spark SQL on HBase ""Astro""

HBase will not have query engine.

It will provide better support to query engines.

Cheers

On Aug 10, 2015, at 11:11 PM, Yan Zhou.sc <Yan.Zhou.sc@huawei.com<mailto:Yan.Zhou.sc@huawei.com>> wrote:
Ted,

I‚Äôm in China now, and seem to experience difficulty to access Apache Jira. Anyways, it appears to me  that HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> attempts to support Spark DataFrame inside HBase.
If true, one question to me is whether HBase is intended to have a built-in query engine or not. Or it will stick with the current way as
a k-v store with some built-in processing capabilities in the forms of coprocessor, custom filter, ‚Ä¶, etc., which allows for loosely-coupled query engines
built on top of il.com]
ÂèëÈÄÅÊó∂Èó¥: 2015Âπ¥8Êúà11Êó• 8:54
Êî∂‰ª∂‰∫∫: Bing Xiao (Bing)
ÊäÑÈÄÅ: dev@spark.apache.org<mailto:dev@spark.apache.org>; user@spark.apache.org<mailto:user@spark.apache.org>; Yan Zhou.sc
‰∏ªÈ¢ò: Re: Package Release Annoucement: Spark SQL on HBase ""Astro""

Yan / Bing:
Mind taking a look at HBASE-14181<https://issues.apache.org/jira/browse/HBASE-14181> 'Add Spark DataFrame DataSource to HBase-Spark Module' ?

Thanks

On Wed, Jul 22, 2015 at 4:53 PM, Bing Xiao (Bing) <bing.xiao@huawei.com<mailto:bing.xiao@huawei.com>> wrote:
We are happy to announce the availability of the Spark SQL on HBase 1.0.0 release.  http://spark-packages.org/package/Huawei-Spark/Spark-SQL-on-HBase
The main features in this package, dubbed ‚ÄúAstro‚Äù, include:

‚Ä¢         Systematic and powerful handling of data pruning and intelligent scan, based on partial evaluation technique

‚Ä¢         HBase pushdown capabilities like custom filters and coprocessor to support ultra low latency processing

‚Ä¢         SQL, Data Frame support

‚Ä¢         More SQL capabilities made possible (Secondary index, bloom filter, Primary Key, Bulk load, Update)

‚Ä¢         Joins with data from other sources

‚Ä¢         Python/Java/Scala support

‚Ä¢         Support latest Spark 1.4.0 release


The tests by Huawei team and community contributors covered the areas: bulk load; projection pruning; partition pruning; partial evaluation; code generation; coprocessor; customer filtering; DML; complex filtering on keys and non-keys; Join/union with non-Hbase data; Data Frame; multi-column family test.  We will post the test results including performance tests the middle of August.
You are very welcomed to try out or deploy the package, and help improve the integration tests with various combinations of the settings, extensive Data Frame tests, complex join/union test and extensive performance tests.  Please use the ‚ÄúIssues‚Äù ‚ÄúPull Requests‚Äù links at this package homepage, if you want to report bugs, improvement or feature requests.
Special thanks to project owner and technical leader Yan Zhou, Huawei global team, community contributors and Databricks.   Databricks has been providing great assistance from the design to the release.
‚ÄúAstro‚Äù, the Spark SQL on HBase package will be useful for ultra low latency query and analytics of large scale data sets in vertical enterprises. We will continue to work with the community to develop new features and improve code base.  Your comments and suggestions are greatly appreciated.

Yan Zhou / Bing Xiao
Huawei Big Data team



"
gen tang <gen.tang86@gmail.com>,"Wed, 12 Aug 2015 12:47:56 +0800",Re: Potential bug broadcastNestedLoopJoin or default value of spark.sql.autoBroadcastJoinThreshold,"""Cheng, Hao"" <hao.cheng@intel.com>","Hi,

Thanks a lot.

The problem is not do non-equal join for large tables, in fact, one table
is really small and another one is huge.

The problem is that spark can only get the correct size for dataframe
created directly from hive table. Even we create a dataframe from local
collection, it uses defaultSizeInBytes as its size. (Here, I am really
confused: why we don't use LogicalLocalTable in exsitingRDD.scala to
estimate its size. As I understand, this case class is created for this
purpose)

Then if we do some join or unionAll operation on this dataframe, the
estimated size will explode.

For instance, if we do join, val df = df1.join(df2, condition) then
df.queryExecution.analyzed.statistics.sizeInBytes = df1 * df2

In my case, I create df1 instance from an existing rdd.

I find a workaround for this problem:
1. save df1 in hive table
2. read this hive table and create a new dataframe
3. do outer join with this new dataframe

Cheers
Gen


Äôt
er to
1
e
n
or
"
Cheng Lian <lian.cs.zju@gmail.com>,"Wed, 12 Aug 2015 15:51:05 +0800",Re: possible issues with listing objects in the HadoopFSrelation,"Gil Vernik <GILV@il.ibm.com>, Dev <dev@spark.apache.org>","Hi Gil,

Sorry for the late reply and thanks for raising this question. The file 
listing logic in HadoopFsRelation is intentionally made different from 
Hadoop FileInputFormat. Here are the reasons:

1. Efficiency: when computing RDD partitions, 
FileInputFormat.listStatus() is called on the driver side in a 
sequential manner, and can be slow for S3 directories with lots of 
sub-directories, e.g. partitioned tables with thousands or even more 
partitions. This is partly because file metadata operation can be very 
slow on S3. HadoopFsRelation relies on this file listing action to do 
partition discovery, and we've made a distributed parallel version in 
Spark 1.5: we first list input paths on driver side in a sequential 
breadth-first manner, and once we find the number of directories to be 
listed exceeds a threshold (32 by default), we launch a Spark job to do 
file listing. With this mechanism, we've observed 2 orders of magnitude 
performance boost when reading partitioned table with thousands of 
distinct partitions located on S3.

2. Semantics difference: the default hiddenFileFilter doesn't apply in 
every cases. For example, Parquet summary files _metadata and 
_common_metadata plays crucial roles in schema discovery and schema 
merging, and we don't want to exclude them when listing the files. But 
they are removed when reading the actual data. However, we probably 
should allow users to pass in user defined path filters.

Cheng


"
Gil Vernik <GILV@il.ibm.com>,"Wed, 12 Aug 2015 12:33:25 +0300",Re: possible issues with listing objects in the HadoopFSrelation,Cheng Lian <lian.cs.zju@gmail.com>,"Hi Cheng,

Thanks a lot for responding to it.
I still miss some points in the Efficiency and i would be very thankful if 
you will expand it little bit more.
As i see it, both HadoopFSRelation and FileInputFormat.listStatus perform 
lists and eventually both calls to FileSystem.listStatus method.
FileInputFormat.listStatus does not need to be sequential, it also can 
create many threads to list objects from the same data source.
   int numThreads = job
        .getInt(
            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.
LIST_STATUS_NUM_THREADS,
            org.apache.hadoop.mapreduce.lib.input.FileInputFormat.
DEFAULT_LIST_STATUS_NUM_THREADS);

But i guess if you call FileSystem directly without calling intermediate 
FileInputFormat - it may be faster, just because there is less code to 
execute?

I want to make sure i understand your correctly. So HadoopFSRelation will 
create partitions by itself, perform listing, etc.. it will also be in 
parallel in 1.5.0.  But the code inside Spark that use HadoopRDD will 
still rely on the partitions provided from FileInputFormat?
For example if I have 10000 objects inside some bucket and i access this 
via sparkContext.hadoopFile than FileInputFormat will provide all the 
partitions and splits, but if i will access the same bucket from some code 
that relies on HadoopFSRelation than partitions will be created by 
HadoopFSRelation?

Thanks
Gil.


 



From:   Cheng Lian <lian.cs.zju@gmail.com>
To:     Gil Vernik/Haifa/IBM@IBMIL, Dev <dev@spark.apache.org>
Date:   12/08/2015 10:51
Subject:        Re: possible issues with listing objects in the 
HadoopFSrelation



Hi Gil,

Sorry for the late reply and thanks for raising this question. The file 
listing logic in HadoopFsRelation is intentionally made different from 
Hadoop FileInputFormat. Here are the reasons:

1. Efficiency: when computing RDD partitions, FileInputFormat.listStatus() 
is called on the driver side in a sequential manner, and can be slow for 
S3 directories with lots of sub-directories, e.g. partitioned tables with 
thousands or even more partitions. This is partly because file metadata 
operation can be very slow on S3. HadoopFsRelation relies on this file 
listing action to do partition discovery, and we've made a distributed 
parallel version in Spark 1.5: we first list input paths on driver side in 
a sequential breadth-first manner, and once we find the number of 
directories to be listed exceeds a threshold (32 by default), we launch a 
Spark job to do file listing. With this mechanism, we've observed 2 orders 
of magnitude performance boost when reading partitioned table with 
thousands of distinct partitions located on S3.

2. Semantics difference: the default hiddenFileFilter doesn't apply in 
every cases. For example, Parquet summary files _metadata and 
_common_metadata plays crucial roles in schema discovery and schema 
merging, and we don't want to exclude them when listing the files. But 
they are removed when reading the actual data. However, we probably should 
allow users to pass in user defined path filters.

Cheng

Just some thoughts, hope i didn't missed something obvious. 

HadoopFSRelation calls directly FileSystem class to list files in the 
path. 
It looks like it implements basically the same logic as in the 
FileInputFormat.listStatus method ( located in 
hadoop-map-reduce-client-core) 

The point is that HadoopRDD (or similar ) calls getSplits method that 
calls FileInputFormat.listStatus, while HadoopFSRelation calls FileSystem 
directly and both of them try to achieve ""listing"" of objects. 

There might be various issues with this, for example this one 
https://issues.apache.org/jira/browse/SPARK-7868 makes sure that 
""_temporary"" is not returned in a result, but the the listing of 
FileInputFormat contains more logic,  it uses hidden PathFilter like this 

  private static final PathFilter hiddenFileFilter = new PathFilter(){ 
      public boolean accept(Path p){ 
        String name = p.getName(); 
        return !name.startsWith(""_"") && !name.startsWith("".""); 
      } 
    }; 

In addition, custom FileOutputCommitter, may use other name than 
""_temporary"" . 

All this may lead that HadoopFSrelation and HadoopRDD will provide 
different lists from the same data source. 

My question is: what the roadmap for this listing in HadoopFSrelation. 
Will it implement exactly the same logic like in 
FileInputFormat.listStatus, or may be one day HadoopFSrelation will call 
FileInputFormat.listStatus and provide custom PathFilter or 
MultiPathFilter? This way there will be single  code that list objects. 

Thanks, 
Gil. 




"
Eugene Morozov <fathersson@list.ru>,"Wed, 12 Aug 2015 17:06:42 +0300",Does Spark optimization might miss to run transformation?,"user <user@spark.apache.org>,
 Dev <dev@spark.apache.org>","Hi!

Iíd like to complete action (store / print smth) inside of transformation (map or mapPartitions). This approach has some flaws, but there is a question. Might it happen that Spark will optimise (RDD or DataFrame) processing so that my mapPartitions simply wonít happen?

--
Eugene Morozov
fathersson@list.ru




"
java8964 <java8964@hotmail.com>,"Wed, 12 Aug 2015 10:36:35 -0400","Spark 1.2.2 build problem with Hive 0.12, bringing in wrong version
 of avro-mapred","user <user@spark.apache.org>, Dev <dev@spark.apache.org>","Hi, This email is sent to both dev and user list, just want to see if someone familiar with Spark/Maven build procedure can provide any help.
I am building Spark 1.2.2 with the following command:
mvn -Phadoop-2.2 -Dhadoop.version=2.2.0 -Phive -Phive-0.12.0
The spark-assembly-1.2.2-hadoop2.2.0.jar contains the avro and avro-ipc of version 1.7.6, but avro-mapred of version 1.7.1, which caused some wired runtime exception when I tried to read the avro file in the Spark 1.2.2, as following:
NullPointerException	at java.io.StringReader.<init>(StringReader.java:50)	at org.apache.avro.Schema$Parser.parse(Schema.java:943)	at org.apache.avro.Schema.parse(Schema.java:992)	at org.apache.avro.mapred.AvroJob.getInputSchema(AvroJob.java:65)	at org.apache.avro.mapred.AvroRecordReader.<init>(AvroRecordReader.java:43)	at org.apache.avro.mapred.AvroInputFormat.getRecordReader(AvroInputFormat.java:52)	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:233)	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:210)
So I run the following command to understand that avro-mapred 1.7.1 is brought in by Hive 0.12 profile:
mvn -Phadoop-2.2 -Dhadoop.version=2.2.0 -Phive -Phive-0.12.0 dependency:tree -Dverbose -Dincludes=org.apache.avro
[INFO] ------------------------------------------------------------------------[INFO] Building Spark Project Hive 1.2.2[INFO] ------------------------------------------------------------------------[INFO][INFO] --- maven-dependency-plugin:2.4:tree (default-cli) @ spark-hive_2.10 ---[INFO] org.apache.spark:spark-hive_2.10:jar:1.2.2[INFO] +- org.apache.spark:spark-core_2.10:jar:1.2.2:compile[INFO] |  \- org.apache.hadoop:hadoop-client:jar:2.2.0:compile (version managed from 1.0.4)[INFO] |     \- org.apache.hadoop:hadoop-common:jar:2.2.0:compile[INFO] |        \- (org.apache.avro:avro:jar:1.7.6:compile - version managed from 1.7.1; omitted for duplicate)[INFO] +- org.spark-project.hive:hive-serde:jar:0.12.0-protobuf-2.5:compile[INFO] |  +- (org.apache.avro:avro:jar:1.7.6:compile - version managed from 1.7.1; omitted for duplicate)[INFO] |  \- org.apache.avro:avro-mapred:jar:1.7.1:compile[INFO] |     \- (org.apache.avro:avro-ipc:jar:1.7.6:compile - version managed from 1.7.1; omitted for duplicate)[INFO] +- org.apache.avro:avro:jar:1.7.6:compile[INFO] \- org.apache.avro:avro-mapred:jar:hadoop2:1.7.6:compile[INFO]    +- org.apache.avro:avro-ipc:jar:1.7.6:compile[INFO]    |  \- (org.apache.avro:avro:jar:1.7.6:compile - version managed from 1.7.1; omitted for duplicate)[INFO]    \- org.apache.avro:avro-ipc:jar:tests:1.7.6:compile[INFO]       \- (org.apache.avro:avro:jar:1.7.6:compile - version managed from 1.7.1; omitted for duplicate)[INFO]
In this case, I could manually fix all the classes in the final jar, changing from avro-mapred 1.7.1 to 1.7.6, but I wonder if there is any other solution, as this way is very error-prone.
Also, just from the above message, I can see avro-mapred.jar.hadoop2:1.7.6 dependency is there, but looks like it is being omitted. Not sure why Maven choosed the lower version, as I am not a Maven guru.
My question, under the above situation, do I have a easy way to build it with avro-mapred 1.7.6, instead of 1.7.1?
Thanks
Yong 		 	   		  "
Reynold Xin <rxin@databricks.com>,"Wed, 12 Aug 2015 10:14:24 -0700",Re: Intermittent timeout failure org/apache/spark/sql/hive/thriftserver/CliSuite.scala,Tim Preece <tepreece@mail.com>,"Thanks for finding this. Should we just switch to Java's process library
for now?



"
Imran Rashid <irashid@cloudera.com>,"Wed, 12 Aug 2015 12:27:34 -0500",Re: Spark runs into an Infinite loop even if the tasks are completed successfully,Akhil Das <akhil@sigmoidanalytics.com>,"yikes.

Was this a one-time thing?  Or does it happen consistently?  can you turn
on debug logging for o.a.s.scheduler (dunno if it will help, but maybe ...)


"
quasiben <quasiben@gmail.com>,"Wed, 12 Aug 2015 14:12:14 -0700 (MST)",Re: PySpark on PyPi,dev@spark.apache.org,"I've help to build a conda installable spark packages in the past.  You can
an older recipe here:
https://github.com/conda/conda-recipes/tree/master/spark

And I've been updating packages here: 
https://anaconda.org/anaconda-cluster/spark

`conda install -c anaconda-cluster spark` 

The above should work for OSX/Linux-64 and py27/py34 

--Ben 




--

---------------------------------------------------------------------


"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Thu, 13 Aug 2015 10:04:08 +0000",please help with ClassNotFoundException,dev@spark.apache.org,"Hi,
    I am using spark 1.4 when an issue occurs to me.
    I am trying to use the aggregate function:
    JavaRdd<String> rdd = some rdd;
    HashMap<Long, TypeA> zeroValue = new HashMap();
    // add initial key-value pair for zeroValue
    rdd.aggregate(zeroValue,
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>>(){//implementation},
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>(){//implementation})

    here is the stack trace when i run the application:

Caused by: java.lang.ClassNotFoundException: TypeA
at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
at java.security.AccessController.doPrivileged(Native Method)
at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
at java.lang.Class.forName0(Native Method)
at java.lang.Class.forName(Class.java:274)
at
org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)
at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
at java.util.HashMap.readObject(HashMap.java:1180)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at java.lang.reflect.Method.invoke(Method.java:606)
at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
at
org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
at
org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:89)
at org.apache.spark.util.Utils$.clone(Utils.scala:1458)
at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1049)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1047)
at
org.apache.spark.api.java.JavaRDDLike$class.aggregate(JavaRDDLike.scala:413)
at
org.apache.spark.api.java.AbstractJavaRDDLike.aggregate(JavaRDDLike.scala:47)
     *however I have checked that TypeA is in the jar file which is in the
classpath*
*    And when I use an empty HashMap as the zeroValue, the exception has
gone*
*    Does anyone meet the same problem, or can anyone help me with it?*
"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Thu, 13 Aug 2015 18:43:28 +0800","=?gb18030?B?u9i4tKO6cGxlYXNlIGhlbHAgd2l0aCBDbGFzc05v?=
 =?gb18030?B?dEZvdW5kRXhjZXB0aW9u?=","""=?gb18030?B?1tzHp+q7?="" <qhzhou@apache.org>, ""=?gb18030?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?="" <dev@spark.apache.org>","Are you using 1.4.0?  If yes, use 1.4.1




------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""÷‹«ßÍª"";<qhzhou@apache.org>;
∑¢ÀÕ ±º‰: 2015ƒÍ8‘¬13»’(–«∆⁄Àƒ) ÕÌ…œ6:04
 ’º˛»À: ""dev""<dev@spark.apache.org>; 

÷˜Ã‚: please help with ClassNotFoundException



Hi,    I am using spark 1.4 when an issue occurs to me.
    I am trying to use the aggregate function:
    JavaRdd<String> rdd = some rdd;
    HashMap<Long, TypeA> zeroValue = new HashMap();
    // add initial key-value pair for zeroValue
    rdd.aggregate(zeroValue, 
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>>(){//implementation},
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>(){//implementation})


    here is the stack trace when i run the application:


Caused by: java.lang.ClassNotFoundException: TypeA
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:274)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:89)
	at org.apache.spark.util.Utils$.clone(Utils.scala:1458)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1047)
	at org.apache.spark.api.java.JavaRDDLike$class.aggregate(JavaRDDLike.scala:413)
	at org.apache.spark.api.java.AbstractJavaRDDLike.aggregate(JavaRDDLike.scala:47)

     however I have checked that TypeA is in the jar file which is in the classpath
    And when I use an empty HashMap as the zeroValue, the exception has gone
    Does anyone meet the same problem, or can anyone help me with it?"
cheez <11besemjaved@seecs.edu.pk>,"Thu, 13 Aug 2015 02:26:28 -0700 (MST)",Switch from Sort based to Hash based shuffle,dev@spark.apache.org,"I understand that the current master branch of Spark uses Sort based shuffle.
Is there a way to change that to Hash based shuffle, just for experimental
purposes by modifying the source code ?



--

---------------------------------------------------------------------


"
Ranjana Rajendran <ranjana.rajendran@gmail.com>,"Thu, 13 Aug 2015 06:09:03 -0700",Re: Switch from Sort based to Hash based shuffle,cheez <11besemjaved@seecs.edu.pk>,"Hi Cheez,

You can set the parameter spark.shuffle.manager when you submit the Spark
job.

--conf spark.shuffle.manager=hash

Thank you,
Ranjana


"
=?UTF-8?B?5ZGo5Y2D5piK?= <z.qianhao@gmail.com>,"Thu, 13 Aug 2015 13:30:34 +0000",Re: please help with ClassNotFoundException,"Sea <261810726@qq.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi sea
    Is it the same issue as https://issues.apache.org/jira/browse/SPARK-8368

Sea <261810726@qq.com>‰∫é2015Âπ¥8Êúà13Êó•Âë®Âõõ ‰∏ãÂçà6:52ÂÜôÈÅìÔºö

-
he.org>;
•(ÊòüÊúüÂõõ) Êôö‰∏ä6:04
s(JavaSerializer.scala:66)
)
:57)
mpl.java:43)
)
alizer.scala:69)
izer.scala:89)
:148)
:109)
13)
:47)
-- 
Best Regard
ZhouQianhao
"
Akhil Das <akhil@sigmoidanalytics.com>,"Thu, 13 Aug 2015 19:19:25 +0530",Re: Switch from Sort based to Hash based shuffle,cheez <11besemjaved@seecs.edu.pk>,"Have a look at spark.shuffle.manager, You can switch between sort and hash
with this configuration.

spark.shuffle.managersortImplementation to use for shuffling data. There
are two implementations available:sort and hash. Sort-based shuffle is more
memory-efficient and is the default option starting in 1.2.

Thanks
Best Regards


"
Ranjana Rajendran <ranjana.rajendran@gmail.com>,"Thu, 13 Aug 2015 07:37:51 -0700",Graphx - how to add vertices to a HashSet of vertices ?,dev@spark.apache.org,"Hi,

sampledVertices is a HashSet of vertices

      var sampledVertices: HashSet[VertexId] = HashSet()

In each iteration, I am making a list of neighborVertexIds

      val neighborVertexIds = burnEdges.map((e:Edge[Int]) => e.dstId)

I want to add this neighborVertexIds to the sampledVertices Hashset.

What is the best way to do this ?

   Currently, I have

sampledVertices = sampledVertices ++ neighborVertexIds.toArray

I realize, toArray is making this a separate stage.

What will be the most efficient way to achieve this ?

Similarly, I need to add the neighborVertexIds to burnQueue where burnQueue
is is a queue of vertices

var burnQueue: Queue[VertexId] = Queue()

Thank you,

Ranjana
"
freedafeng <freedafeng@yahoo.com>,"Thu, 13 Aug 2015 09:13:19 -0700 (MST)",What does NativeMethodAccessorImpl.java do?,dev@spark.apache.org,"I am running a spark job with only two operations: mapPartition and then
per partition. I saw there is a stage 2 for this job that runs this java
program. I am not a java programmer. Could anyone please let me know what
this java program does? or simply how to get rid of this from running, or at
least get it run faster? The collect() call is not important to me. All the
work was done in mapPartition which sends out data to a k-v store. It's sth
like foreachPartition. But I cannot get foreachPartition() to run somehow.
Spark 1.1.1.

Thanks!



--

---------------------------------------------------------------------


"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Fri, 14 Aug 2015 00:36:14 +0800","=?gb18030?B?UmWjuiBwbGVhc2UgaGVscCB3aXRoIENsYXNzTm90?=
 =?gb18030?B?Rm91bmRFeGNlcHRpb24=?=","""=?gb18030?B?1tzHp+q7?="" <z.qianhao@gmail.com>, ""=?gb18030?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?="" <dev@spark.apache.org>","Yes, I guess so. I see this bug before.




------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""÷‹«ßÍª"";<z.qianhao@gmail.com>;
∑¢ÀÕ ±º‰: 2015ƒÍ8‘¬13»’(–«∆⁄Àƒ) ÕÌ…œ9:30
 ’º˛»À: ""Sea""<261810726@qq.com>; ""dev@spark.apache.org""<dev@spark.apache.org>; 

÷˜Ã‚: Re: please help with ClassNotFoundException



Hi sea    Is it the same issue as https://issues.apache.org/jira/browse/SPARK-8368


Sea <261810726@qq.com>”⁄2015ƒÍ8‘¬13»’÷‹Àƒ œ¬ŒÁ6:52–¥µ¿£∫

Are you using 1.4.0?  If yes, use 1.4.1




------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""÷‹«ßÍª"";<qhzhou@apache.org>;
∑¢ÀÕ ±º‰: 2015ƒÍ8‘¬13»’(–«∆⁄Àƒ) ÕÌ…œ6:04
 ’º˛»À: ""dev""<dev@spark.apache.org>; 

÷˜Ã‚: please help with ClassNotFoundException




Hi,    I am using spark 1.4 when an issue occurs to me.
    I am trying to use the aggregate function:
    JavaRdd<String> rdd = some rdd;
    HashMap<Long, TypeA> zeroValue = new HashMap();
    // add initial key-value pair for zeroValue
    rdd.aggregate(zeroValue, 
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>>(){//implementation},
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>(){//implementation})


    here is the stack trace when i run the application:


Caused by: java.lang.ClassNotFoundException: TypeA
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:274)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:89)
	at org.apache.spark.util.Utils$.clone(Utils.scala:1458)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1047)
	at org.apache.spark.api.java.JavaRDDLike$class.aggregate(JavaRDDLike.scala:413)
	at org.apache.spark.api.java.AbstractJavaRDDLike.aggregate(JavaRDDLike.scala:47)

     however I have checked that TypeA is in the jar file which is in the classpath
    And when I use an empty HashMap as the zeroValue, the exception has gone
    Does anyone meet the same problem, or can anyone help me with it?



-- 

Best RegardZhouQianhao"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 13 Aug 2015 10:40:10 -0700",Re: What does NativeMethodAccessorImpl.java do?,freedafeng <freedafeng@yahoo.com>,"That's not a program, it's just a class in the Java library. Spark looks at
the call stack and uses it to describe the job in the UI. If you look at
the whole stack trace you'll see more things that might tell you what's
really going on in that job.




-- 
Marcelo
"
rfarrjr <rfarrjr@gmail.com>,"Thu, 13 Aug 2015 08:16:54 -0700 (MST)","possible bug: user SparkConf properties not copied to worker
 process",dev@spark.apache.org,"Ran into an issue setting a property on the SparkConf that wasn't made
available on the worker.  After some digging[1] I noticed that only
properties that start with ""spark."" are sent by the schedular.   I'm not
sure if this was intended behavior or not.

Using Spark Streaming 1.4.1 running on Java 8.

~Robert

[1]
https://github.com/apache/spark/blob/v1.4.1/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala#L243




--

---------------------------------------------------------------------


"
Imran Rashid <irashid@cloudera.com>,"Thu, 13 Aug 2015 13:07:15 -0500",Re: Spark runs into an Infinite loop even if the tasks are completed successfully,Akhil Das <akhil@sigmoidanalytics.com>,"oh I see, you are defining your own RDD & Partition types, and you had a
bug where partition.index did not line up with the partitions slot in
rdd.getPartitions.  Is that correct?


"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Aug 2015 11:18:13 -0700",Re: possible bug: user SparkConf properties not copied to worker process,rfarrjr <rfarrjr@gmail.com>,"That was intentional - what's your use case that require configs not
starting with spark?



"
Naga Vij <nvbuchu1@gmail.com>,"Thu, 13 Aug 2015 09:44:55 -0700",subscribe,dev@spark.apache.org,"subscribe
"
Naga Vij <nvbuchu1@gmail.com>,"Thu, 13 Aug 2015 09:47:29 -0700",Fwd: - Spark 1.4.1 - run-example SparkPi - Failure ...,"user@spark.apache.org, dev@spark.apache.org","Has anyone run into this?

---------- Forwarded message ----------
From: Naga Vij <nvbuchu1@gmail.com>
Date: Wed, Aug 12, 2015 at 5:47 PM
Subject: - Spark 1.4.1 - run-example SparkPi - Failure ...
To: user@spark.apache.org


Hi,

I am evaluating Spark 1.4.1

Any idea on why run-example SparkPi fails?

Here's what I am encountering with Spark 1.4.1 on Mac OS X (10.9.5) ...

---------------------------------------------------------------------------------------------------------------

~/spark-1.4.1 $ bin/run-example SparkPi

Using Spark's default log4j profile:
org/apache/spark/log4j-defaults.properties

15/08/12 17:20:20 INFO SparkContext: Running Spark version 1.4.1

15/08/12 17:20:20 WARN NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable

15/08/12 17:20:20 INFO SecurityManager: Changing view acls to: nv

15/08/12 17:20:20 INFO SecurityManager: Changing modify acls to: nv

15/08/12 17:20:20 INFO SecurityManager: SecurityManager: authentication
disabled; ui acls disabled; users with view permissions: Set(nv); users
with modify permissions: Set(nv)

15/08/12 17:20:21 INFO Slf4jLogger: Slf4jLogger started

15/08/12 17:20:21 INFO Remoting: Starting remoting

15/08/12 17:20:21 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://sparkDriver@10.0.0.6:53024]

15/08/12 17:20:21 INFO Utils: Successfully started service 'sparkDriver' on
port 53024.

15/08/12 17:20:21 INFO SparkEnv: Registering MapOutputTracker

15/08/12 17:20:21 INFO SparkEnv: Registering BlockManagerMaster

15/08/12 17:20:21 INFO DiskBlockManager: Created local directory at
/private/var/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-52fc9b2e-52b1-4456-a6e4-36ee2505fa01/blockmgr-1a7c45b7-0839-420a-99db-737414f35bd7

15/08/12 17:20:21 INFO MemoryStore: MemoryStore started with capacity 265.4
MB

15/08/12 17:20:21 INFO HttpFileServer: HTTP File server directory is
/private/var/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-52fc9b2e-52b1-4456-a6e4-36ee2505fa01/httpd-2ef0b6b9-8614-41be-bc73-6ba856694d5e

15/08/12 17:20:21 INFO HttpServer: Starting HTTP Server

15/08/12 17:20:21 INFO Utils: Successfully started service 'HTTP file
server' on port 53025.

15/08/12 17:20:21 INFO SparkEnv: Registering OutputCommitCoordinator

15/08/12 17:20:21 INFO Utils: Successfully started service 'SparkUI' on
port 4040.

15/08/12 17:20:21 INFO SparkUI: Started SparkUI at http://10.0.0.6:4040

15/08/12 17:20:21 INFO SparkContext: Added JAR
file:/Users/nv/spark-1.4.1/examples/target/scala-2.10/spark-examples-1.4.1-hadoop2.6.0.jar
at http://10.0.0.6:53025/jars/spark-examples-1.4.1-hadoop2.6.0.jar with
timestamp 1439425221758

15/08/12 17:20:21 INFO Executor: Starting executor ID driver on host
localhost

15/08/12 17:20:21 INFO Utils: Successfully started service
'org.apache.spark.network.netty.NettyBlockTransferService' on port 53026.

15/08/12 17:20:21 INFO NettyBlockTransferService: Server created on 53026

15/08/12 17:20:21 INFO BlockManagerMaster: Trying to register BlockManager

15/08/12 17:20:21 INFO BlockManagerMasterEndpoint: Registering block
manager localhost:53026 with 265.4 MB RAM, BlockManagerId(driver,
localhost, 53026)

15/08/12 17:20:21 INFO BlockManagerMaster: Registered BlockManager

15/08/12 17:20:22 INFO SparkContext: Starting job: reduce at
SparkPi.scala:35

15/08/12 17:20:22 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:35)
with 2 output partitions (allowLocal=false)

15/08/12 17:20:22 INFO DAGScheduler: Final stage: ResultStage 0(reduce at
SparkPi.scala:35)

15/08/12 17:20:22 INFO DAGScheduler: Parents of final stage: List()

15/08/12 17:20:22 INFO DAGScheduler: Missing parents: List()

15/08/12 17:20:22 INFO DAGScheduler: Submitting ResultStage 0
(MapPartitionsRDD[1] at map at SparkPi.scala:31), which has no missing
parents

15/08/12 17:20:22 INFO MemoryStore: ensureFreeSpace(1888) called with
curMem=0, maxMem=278302556

15/08/12 17:20:22 INFO MemoryStore: Block broadcast_0 stored as values in
memory (estimated size 1888.0 B, free 265.4 MB)

15/08/12 17:20:22 INFO MemoryStore: ensureFreeSpace(1202) called with
curMem=1888, maxMem=278302556

15/08/12 17:20:22 INFO MemoryStore: Block broadcast_0_piece0 stored as
bytes in memory (estimated size 1202.0 B, free 265.4 MB)

15/08/12 17:20:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory
on localhost:53026 (size: 1202.0 B, free: 265.4 MB)

15/08/12 17:20:22 INFO SparkContext: Created broadcast 0 from broadcast at
DAGScheduler.scala:874

15/08/12 17:20:22 INFO DAGScheduler: Submitting 2 missing tasks from
ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:31)

15/08/12 17:20:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks

15/08/12 17:20:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID
0, localhost, PROCESS_LOCAL, 1442 bytes)

15/08/12 17:20:22 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID
1, localhost, PROCESS_LOCAL, 1442 bytes)

15/08/12 17:20:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)

15/08/12 17:20:22 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)

15/08/12 17:20:22 INFO Executor: Fetching
http://10.0.0.6:53025/jars/spark-examples-1.4.1-hadoop2.6.0.jar with
timestamp 1439425221758

15/08/12 17:21:22 INFO Executor: Fetching
http://10.0.0.6:53025/jars/spark-examples-1.4.1-hadoop2.6.0.jar with
timestamp 1439425221758

15/08/12 17:21:22 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)

java.net.SocketTimeoutException: connect timed out

at java.net.PlainSocketImpl.socketConnect(Native Method)

at
java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)

at
java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)

at
java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)

at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)

at java.net.Socket.connect(Socket.java:579)

at sun.net.NetworkClient.doConnect(NetworkClient.java:175)

at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)

at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)

at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)

at sun.net.www.http.HttpClient.New(HttpClient.java:308)

at sun.net.www.http.HttpClient.New(HttpClient.java:326)

at
sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)

at
sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)

at
sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)

at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:639)

at org.apache.spark.util.Utils$.fetchFile(Utils.scala:453)

at
org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:398)

at
org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:390)

at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)

at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)

at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)

at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)

at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)

at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)

at
scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

at org.apache.spark.executor.Executor.org
$apache$spark$executor$Executor$$updateDependencies(Executor.scala:390)

at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:193)

at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

at java.lang.Thread.run(Thread.java:745)
---------------------------------------------------------------------------------------------------------------

Thanks
Naga
"
Ted Yu <yuzhihong@gmail.com>,"Thu, 13 Aug 2015 11:32:36 -0700",Re: subscribe,Naga Vij <nvbuchu1@gmail.com>,"See first section on https://spark.apache.org/community


"
Naga Vij <nvbuchu1@gmail.com>,"Thu, 13 Aug 2015 11:46:05 -0700",Fwd: - Spark 1.4.1 - run-example SparkPi - Failure ...,"user@spark.apache.org, dev@spark.apache.org","Hello,

Any idea on why this is happening?

Thanks
Naga

---------- Forwarded message ----------
From: Naga Vij <nvbuchu1@gmail.com>
Date: Wed, Aug 12, 2015 at 5:47 PM
Subject: - Spark 1.4.1 - run-example SparkPi - Failure ...
To: user@spark.apache.org


Hi,

I am evaluating Spark 1.4.1

Any idea on why run-example SparkPi fails?

Here's what I am encountering with Spark 1.4.1 on Mac OS X (10.9.5) ...

---------------------------------------------------------------------------------------------------------------

~/spark-1.4.1 $ bin/run-example SparkPi

Using Spark's default log4j profile:
org/apache/spark/log4j-defaults.properties

15/08/12 17:20:20 INFO SparkContext: Running Spark version 1.4.1

15/08/12 17:20:20 WARN NativeCodeLoader: Unable to load native-hadoop
library for your platform... using builtin-java classes where applicable

15/08/12 17:20:20 INFO SecurityManager: Changing view acls to: nv

15/08/12 17:20:20 INFO SecurityManager: Changing modify acls to: nv

15/08/12 17:20:20 INFO SecurityManager: SecurityManager: authentication
disabled; ui acls disabled; users with view permissions: Set(nv); users
with modify permissions: Set(nv)

15/08/12 17:20:21 INFO Slf4jLogger: Slf4jLogger started

15/08/12 17:20:21 INFO Remoting: Starting remoting

15/08/12 17:20:21 INFO Remoting: Remoting started; listening on addresses
:[akka.tcp://sparkDriver@10.0.0.6:53024]

15/08/12 17:20:21 INFO Utils: Successfully started service 'sparkDriver' on
port 53024.

15/08/12 17:20:21 INFO SparkEnv: Registering MapOutputTracker

15/08/12 17:20:21 INFO SparkEnv: Registering BlockManagerMaster

15/08/12 17:20:21 INFO DiskBlockManager: Created local directory at
/private/var/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-52fc9b2e-52b1-4456-a6e4-36ee2505fa01/blockmgr-1a7c45b7-0839-420a-99db-737414f35bd7

15/08/12 17:20:21 INFO MemoryStore: MemoryStore started with capacity 265.4
MB

15/08/12 17:20:21 INFO HttpFileServer: HTTP File server directory is
/private/var/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-52fc9b2e-52b1-4456-a6e4-36ee2505fa01/httpd-2ef0b6b9-8614-41be-bc73-6ba856694d5e

15/08/12 17:20:21 INFO HttpServer: Starting HTTP Server

15/08/12 17:20:21 INFO Utils: Successfully started service 'HTTP file
server' on port 53025.

15/08/12 17:20:21 INFO SparkEnv: Registering OutputCommitCoordinator

15/08/12 17:20:21 INFO Utils: Successfully started service 'SparkUI' on
port 4040.

15/08/12 17:20:21 INFO SparkUI: Started SparkUI at http://10.0.0.6:4040

15/08/12 17:20:21 INFO SparkContext: Added JAR
file:/Users/nv/spark-1.4.1/examples/target/scala-2.10/spark-examples-1.4.1-hadoop2.6.0.jar
at http://10.0.0.6:53025/jars/spark-examples-1.4.1-hadoop2.6.0.jar with
timestamp 1439425221758

15/08/12 17:20:21 INFO Executor: Starting executor ID driver on host
localhost

15/08/12 17:20:21 INFO Utils: Successfully started service
'org.apache.spark.network.netty.NettyBlockTransferService' on port 53026.

15/08/12 17:20:21 INFO NettyBlockTransferService: Server created on 53026

15/08/12 17:20:21 INFO BlockManagerMaster: Trying to register BlockManager

15/08/12 17:20:21 INFO BlockManagerMasterEndpoint: Registering block
manager localhost:53026 with 265.4 MB RAM, BlockManagerId(driver,
localhost, 53026)

15/08/12 17:20:21 INFO BlockManagerMaster: Registered BlockManager

15/08/12 17:20:22 INFO SparkContext: Starting job: reduce at
SparkPi.scala:35

15/08/12 17:20:22 INFO DAGScheduler: Got job 0 (reduce at SparkPi.scala:35)
with 2 output partitions (allowLocal=false)

15/08/12 17:20:22 INFO DAGScheduler: Final stage: ResultStage 0(reduce at
SparkPi.scala:35)

15/08/12 17:20:22 INFO DAGScheduler: Parents of final stage: List()

15/08/12 17:20:22 INFO DAGScheduler: Missing parents: List()

15/08/12 17:20:22 INFO DAGScheduler: Submitting ResultStage 0
(MapPartitionsRDD[1] at map at SparkPi.scala:31), which has no missing
parents

15/08/12 17:20:22 INFO MemoryStore: ensureFreeSpace(1888) called with
curMem=0, maxMem=278302556

15/08/12 17:20:22 INFO MemoryStore: Block broadcast_0 stored as values in
memory (estimated size 1888.0 B, free 265.4 MB)

15/08/12 17:20:22 INFO MemoryStore: ensureFreeSpace(1202) called with
curMem=1888, maxMem=278302556

15/08/12 17:20:22 INFO MemoryStore: Block broadcast_0_piece0 stored as
bytes in memory (estimated size 1202.0 B, free 265.4 MB)

15/08/12 17:20:22 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory
on localhost:53026 (size: 1202.0 B, free: 265.4 MB)

15/08/12 17:20:22 INFO SparkContext: Created broadcast 0 from broadcast at
DAGScheduler.scala:874

15/08/12 17:20:22 INFO DAGScheduler: Submitting 2 missing tasks from
ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:31)

15/08/12 17:20:22 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks

15/08/12 17:20:22 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID
0, localhost, PROCESS_LOCAL, 1442 bytes)

15/08/12 17:20:22 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID
1, localhost, PROCESS_LOCAL, 1442 bytes)

15/08/12 17:20:22 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)

15/08/12 17:20:22 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)

15/08/12 17:20:22 INFO Executor: Fetching
http://10.0.0.6:53025/jars/spark-examples-1.4.1-hadoop2.6.0.jar with
timestamp 1439425221758

15/08/12 17:21:22 INFO Executor: Fetching
http://10.0.0.6:53025/jars/spark-examples-1.4.1-hadoop2.6.0.jar with
timestamp 1439425221758

15/08/12 17:21:22 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)

java.net.SocketTimeoutException: connect timed out

at java.net.PlainSocketImpl.socketConnect(Native Method)

at
java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)

at
java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)

at
java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)

at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)

at java.net.Socket.connect(Socket.java:579)

at sun.net.NetworkClient.doConnect(NetworkClient.java:175)

at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)

at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)

at sun.net.www.http.HttpClient.<init>(HttpClient.java:211)

at sun.net.www.http.HttpClient.New(HttpClient.java:308)

at sun.net.www.http.HttpClient.New(HttpClient.java:326)

at
sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)

at
sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)

at
sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)

at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:639)

at org.apache.spark.util.Utils$.fetchFile(Utils.scala:453)

at
org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:398)

at
org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:390)

at
scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)

at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)

at
scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)

at
scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:226)

at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)

at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)

at
scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)

at org.apache.spark.executor.Executor.org
$apache$spark$executor$Executor$$updateDependencies(Executor.scala:390)

at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:193)

at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

at java.lang.Thread.run(Thread.java:745)
---------------------------------------------------------------------------------------------------------------

Thanks
Naga
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Thu, 13 Aug 2015 15:46:04 -0300",Re: - Spark 1.4.1 - run-example SparkPi - Failure ...,Naga Vij <nvbuchu1@gmail.com>,"Hi Naga,
This happened here sometimes when the memory of the spark cluster wasn't
enough, and Java GC enters into an infinite loop trying to free some memory.
To fix this I just added more memory to the Workers of my cluster, or you
can increase the number of partitions of your RDD, using the repartition
method.

Regards,
Dirceu

2015-08-13 13:47 GMT-03:00 Naga Vij <nvbuchu1@gmail.com>:

"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Aug 2015 12:05:03 -0700",Fwd: [ANNOUNCE] Spark 1.5.0-preview package,"""dev@spark.apache.org"" <dev@spark.apache.org>","Retry sending this again ...

---------- Forwarded message ----------
From: Reynold Xin <rxin@databricks.com>
Date: Thu, Aug 13, 2015 at 12:15 AM
Subject: [ANNOUNCE] Spark 1.5.0-preview package
To: ""dev@spark.apache.org"" <dev@spark.apache.org>


In order to facilitate community testing of the 1.5.0 release, I've built a
preview package. This is not a release candidate, so there is no voting
involved. However, it'd be great if community members can start testing
with this preview package.


This preview package contains all the commits to branch-1.5
<https://github.com/apache/spark/tree/branch-1.5> till
commit cedce9bdb72a00cbcbcc81d57f2a550eaf4416e8.

The source files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-preview-20150812-bin/

The artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1133/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-preview-20150812-docs/


*== How can you help? ==*

If you are a Spark user, you can help us test this release by taking a
Spark workload and running on this preview release, then reporting any
regressions.


*== Major changes to help you focus your testing ==*

As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog
<https://issues.apache.org/jira/issues/?jql=fixVersion%20%3D%201.5.0%20AND%20project%20%3D%20SPARK>
.


*RDD/DataFrame/SQL APIs*

- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing

*DataFrame/SQL Backend Execution*

- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans

*Data Sources, Hive, Hadoop, Mesos and Cluster Management*

- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names

*SparkR*

- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like

*Streaming*

- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI

*Machine Learning and Advanced Analytics*

- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
"
Ted Malaska <ted.malaska@cloudera.com>,"Thu, 13 Aug 2015 15:07:39 -0400","=?UTF-8?B?UmU6IOetlOWkjTog562U5aSNOiDnrZTlpI06IFBhY2thZ2UgUmVsZWFzZSBBbm5vdWNlbQ==?=
	=?UTF-8?B?ZW50OiBTcGFyayBTUUwgb24gSEJhc2UgIkFzdHJvIg==?=","""Yan Zhou.sc"" <Yan.Zhou.sc@huawei.com>","Cool seems like the design are very close.

Here is my latest blog on my work with HBase and Spark.  Let me know if you
have any questions.  There should be two more blogs next month talking
about bulk load through spark 14150 which is committed, and SparkSQL 14181
which should be done next week.

http://blog.cloudera.com/blog/2015/08/apache-spark-comes-to-apache-hbase-with-hbase-spark-module/


d
om]
• 9:14
r
 Release Annoucement: Spark SQL on HBase ""Astro""
l
e used by
noucement: Spark SQL on HBase
s
e ago), we
ers and
h.
,
,
ss
noucement: Spark SQL on HBase
.
lk
reference is a
rt
noucement: Spark SQL on HBase
 on HBase ""Astro""
che Jira.
pled query
ong@gmail.com>]
• 8:54
ou.sc
 ""Astro""
e:
essor
m
e
ys
he
e
.
inks at this package homepage, if
r ultra low
"
Dirceu Semighini Filho <dirceu.semighini@gmail.com>,"Thu, 13 Aug 2015 16:08:16 -0300",Re: - Spark 1.4.1 - run-example SparkPi - Failure ...,"Naga Vij <nvbuchu1@gmail.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Naga,
If you are trying to use classes from this jar, you will need to call the
addJar method from the sparkcontext, which will put this jar in the all
workers context.
Even when you execute it in standalone.


2015-08-13 16:02 GMT-03:00 Naga Vij <nvbuchu1@gmail.com>:

"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Aug 2015 12:11:45 -0700",Fwd: [ANNOUNCE] Spark 1.5.0-preview package,"""dev@spark.apache.org"" <dev@spark.apache.org>","(I tried to send this last night but somehow ASF mailing list rejected my
mail)


In order to facilitate community testing of the 1.5.0 release, I've built a
preview package. This is not a release candidate, so there is no voting
involved. However, it'd be great if community members can start testing
with this preview package.


This preview package contains all the commits to branch-1.5 till commit
cedce9bdb72a00cbcbcc81d57f2a550eaf4416e8.

The source files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-preview-20150812-bin/

The artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1133/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-preview-20150812-docs/


== How can you help? ==

If you are a Spark user, you can help us test this release by taking a
Spark workload and running on this preview release, then reporting any
regressions.


== Major changes to help you focus your testing ==

As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.


RDD/DataFrame/SQL APIs

- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing

DataFrame/SQL Backend Execution

- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans

Data Sources, Hive, Hadoop, Mesos and Cluster Management

- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names

SparkR

- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like

Streaming

- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI

Machine Learning and Advanced Analytics

- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
"
freedafeng <freedafeng@yahoo.com>,"Thu, 13 Aug 2015 13:12:14 -0700 (MST)",Re: What does NativeMethodAccessorImpl.java do?,dev@spark.apache.org,"Thanks Marcelo! 

The reason I was asking that question is that I was expecting my spark job
to be a ""map only"" job. In other words, it should finish after the
mapPartitions run for all partitions. This is because the job is only
mapPartitions() plus count() where mapPartitions only yield one integer for
each partition. The first stage running ""count at
/root/workspace/**/mapred/aerospike_calculations.py:35"" completed after
reasonably long time. I was expecting the job to complete right away after
the first stage is complete. To my surprise, the second stage calling
""collect at NativeMethodAccessorImpl.java:-2"" runs super slow, about as slow
as the first stage. 

I want to know what the second stage is doing..

================================UI============================
Spark Stages
Total Duration: 8.2 h
Scheduling Mode: FIFO
Active Stages: 1
Completed Stages: 2
Failed Stages: 0
Active Stages (1)

Stage Id	Description	Submitted	Duration	Tasks: Succeeded/Total	Input	Shuffle
Read	Shuffle Write
2	
(kill) collect at NativeMethodAccessorImpl.java:-2 +details
2015/08/13 16:01:59	4.1 h	
360/2048	375.1 GB		
Completed Stages (2)

Stage Id	Description	Submitted	Duration	Tasks: Succeeded/Total	Input	Shuffle
Read	Shuffle Write
1	
count at /root/workspace/**/aerospike_calculations.py:35
2015/08/13 12:02:40	7.5 h	
2048/2048	1785.6 GB		
0	
first at SerDeUtil.scala:70 +details
2015/08/13 12:02:34	4 s	
1/1	839.0 MB		
Failed Stages (0)

Stage Id	Description	Submitted	Duration	Tasks: Succeeded/Total	Input	Shuffle
Read	Shuffle Write	Failure Reason




--

---------------------------------------------------------------------


"
Thomas Dudziak <tomdzk@gmail.com>,"Thu, 13 Aug 2015 14:01:18 -0700",Developer API & plugins for Hive & Hadoop ?,dev <dev@spark.apache.org>,"Hi,

I have asked this before but didn't receive any comments, but with the
impending release of 1.5 I wanted to bring this up again.
Right now, Spark is very tightly coupled with OSS Hive & Hadoop which
causes me a lot of work every time there is a new version because I don't
run OSS Hive/Hadoop versions (and before you ask, I can't).

My question is, does Spark need to be so tightly coupled with these two ?
Or put differently, would it be possible to introduce a developer API
between Spark (up and including e.g. SqlContext) and Hadoop (for HDFS bits)
and Hive (e.g. HiveContext and beyond) and move the actual Hadoop & Hive
dependencies into plugins (e.g. separate maven modules)?
This would allow me to easily maintain my own Hive/Hadoop-ish integration
with our internal systems without ever having to touch Spark code.
I expect this could also allow for instance Hadoop vendors to provide their
own, more optimized implementations without Spark having to know about them.

cheers,
Tom
"
rfarrjr <rfarrjr@gmail.com>,"Thu, 13 Aug 2015 14:11:48 -0700 (MST)","Re: possible bug: user SparkConf properties not copied to worker
 process",dev@spark.apache.org,"Thanks for the response.

In this particular case we passed a url that would be leveraged when
configuring some serialization support for Kryo.   We are using a schema
registry and leveraging it to efficiently serialize avro objects without the
need to register specific records or schemas up front.

Adding ""spark."" to the property name works just didn't want to conflict with
the core spark properties namespace.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Aug 2015 14:16:55 -0700",Re: possible bug: user SparkConf properties not copied to worker process,rfarrjr <rfarrjr@gmail.com>,"Is this through Java properties? For java properties, you can pass them
using spark.executor.extraJavaOptions.





"
Reynold Xin <rxin@databricks.com>,"Thu, 13 Aug 2015 14:19:28 -0700",Re: Developer API & plugins for Hive & Hadoop ?,Thomas Dudziak <tomdzk@gmail.com>,"I believe for Hive, there is already a client interface that can be used to
build clients for different Hive metastores. That should also work for your
heavily forked one.

For Hadoop, it is definitely a bigger project to refactor. A good way to
start evaluating this is to list what needs to be changed. Maybe you can
start by telling us what you need to change for every upgrade? Feel free to
email me in private if this is sensitive and you don't want to share in a
public list.







"
rfarrjr <rfarrjr@gmail.com>,"Thu, 13 Aug 2015 14:37:34 -0700 (MST)","Re: possible bug: user SparkConf properties not copied to worker
 process",dev@spark.apache.org,"That works.



--

---------------------------------------------------------------------


"
Thomas Dudziak <tomdzk@gmail.com>,"Thu, 13 Aug 2015 16:08:41 -0700",Re: Developer API & plugins for Hive & Hadoop ?,Reynold Xin <rxin@databricks.com>,"Unfortunately it doesn't because our version of Hive has different syntax
elements and thus I need to patch them in (and a few other minor things).
It would be great if there would be a developer api on a somewhat higher
level.


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Fri, 14 Aug 2015 06:57:03 +0700",Re: Developer API & plugins for Hive & Hadoop ?,Thomas Dudziak <tomdzk@gmail.com>,"Hi Tom,

Not sure how much this helps, but are you aware that you can build Spark
with the -Phadoop-provided profile to avoid packaging Hadoop dependencies
in the assembly jar?

-Sandy


"
Josh Rosen <rosenville@gmail.com>,"Thu, 13 Aug 2015 19:21:27 -0700",Re: Automatically deleting pull request comments left by AmplabJenkins,dev <dev@spark.apache.org>,"Prototype is at https://github.com/databricks/spark-pr-dashboard/pull/59


"
Ted Yu <yuzhihong@gmail.com>,"Thu, 13 Aug 2015 19:54:19 -0700",Re: Automatically deleting pull request comments left by AmplabJenkins,Josh Rosen <rosenville@gmail.com>,"Thanks Josh for the initiative.

I think reducing the redundancy in QA bot posts would make discussion on GitHub
UI more focused.

Cheers


"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 14 Aug 2015 02:56:23 +0000","RE: Automatically deleting pull request comments left by
 AmplabJenkins","Josh Rosen <rosenville@gmail.com>, dev <dev@spark.apache.org>","I found the https://spark-prs.appspot.com/ is super slow while open it in a new window recently, not sure just myself or everybody experience the same, is there anyways to speed up?

From: Josh Rosen [mailto:rosenville@gmail.com]
Sent: Friday, August 14, 2015 10:21 AM
To: dev
Subject: Re: Automatically deleting pull request comments left by AmplabJenkins

Prototype is at https://github.com/databricks/spark-pr-dashboard/pull/59

On Wed, Aug 12, 2015 at 7:51 PM, Josh Rosen <rosenville@gmail.com<mailtot if I wrote a script to auto-delete pull request comments from AmplabJenkins?

Currently there are two bots which post Jenkins test result comments to GitHub, AmplabJenkins and SparkQA.

SparkQA is the account which post the detailed Jenkins start and finish messages that contain information on which commit is being tested and which tests have failed. This bot is controlled via the dev/run-tests-jenkins script.

AmplabJenkins is controlled by the Jenkins GitHub Pull Request Builder plugin. This bot posts relatively uninformative comments (""Merge build triggered"", ""Merge build started"", ""Merge build failed"") that do not contain any links or details specific to the tests being run.

It is technically non-trivial prevent these AmplabJenkins comments from being posted in the first place (see https://issues.apache.org/jira/browse/SPARK-4216).

However, as a short-term hack I'd like to deploy a script which automatically deletes these comments as soon as they're posted, with an exemption carved out for the ""Can an admin approve this patch for testing?"" messages. This will help to significantly de-clutter pull request discussions in the GitHub UI.

If nobody objects, I'd like to deploy this script sometime in the next few days.

(From a technical perspective, my script uses the GitHub REST API and AmplabJenkins' own OAuth token to delete the comments.  The final deployment environment will most likely be the backend of http://spark-prs.appspot.com).

- Josh

"
Ted Yu <yuzhihong@gmail.com>,"Thu, 13 Aug 2015 20:04:16 -0700",Re: Automatically deleting pull request comments left by AmplabJenkins,"""Cheng, Hao"" <hao.cheng@intel.com>","I tried accessing just now.
It took several seconds before the page showed up.

FYI


"
=?UTF-8?B?5ZGo5Y2D5piK?= <z.qianhao@gmail.com>,"Fri, 14 Aug 2015 03:14:45 +0000",Re: please help with ClassNotFoundException,"Sea <261810726@qq.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Sea
     I have updated spark to 1.4.1, however the problem still exists, any
idea?

Sea <261810726@qq.com>‰∫é2015Âπ¥8Êúà14Êó•Âë®‰∫î ‰∏äÂçà12:36ÂÜôÈÅìÔºö

-
mail.com>;
•(ÊòüÊúüÂõõ) Êôö‰∏ä9:30
.org""<
®Âõõ ‰∏ãÂçà6:52ÂÜôÈÅìÔºö
--
che.org>;
•(ÊòüÊúüÂõõ) Êôö‰∏ä6:04
ss(JavaSerializer.scala:66)
2)
)
a:57)
Impl.java:43)
7)
)
ializer.scala:69)
lizer.scala:89)
a:148)
a:109)
413)
a:47)
-- 
Best Regard
ZhouQianhao
"
"""Cheng, Hao"" <hao.cheng@intel.com>","Fri, 14 Aug 2015 03:17:06 +0000","RE: Automatically deleting pull request comments left by
 AmplabJenkins",Ted Yu <yuzhihong@gmail.com>,"OK, thanks, probably just myself‚Ä¶

From: Ted Yu [mailto:yuzhihong@gmail.com]
Sent: Friday, August 14, 2015 11:04 AM
To: Cheng, Hao
Cc: Josh Rosen; dev
Subject: Re: Automatically deleting pull request comments left by AmplabJenkins

I tried accessing just now.
It took several seconds before the page showed up.

FYI

On Thu, Aug 13, 2015 at 7:56 PM, Cheng, Hao <hao.cheng@intel.com<mailto:hao.cheng@intel.com>> wrote:
I found the https://spark-prs.appspot.com/ is super slow while open it in a new window recently, not sure just myself or everybody experience the same, is there anyways to speed up?

From: Josh Rosen [mailto:rosenville@gmail.com<mailto:rosenville@gmail.com>]
Sent: Friday, August 14, 2015 10:21 AM
To: dev
Subject: Re: Automatically deleting pull request comments left by AmplabJenkins

Prototype is at https://github.com/databricks/spark-pr-dashboard/pull/59

On Wed, Aug 12, 2015 at 7:51 PM, Josh Rosen <rosenville@gmail.com<mailto:rosenville@gmail.com>> wrote:
TL;DR: would anyone object if I wrote a script to auto-delete pull request comments from AmplabJenkins?

Currently there are two bots which post Jenkins test result comments to GitHub, AmplabJenkins and SparkQA.

SparkQA is the account which post the detailed Jenkins start and finish messages that contain information on which commit is being tested and which tests have failed. This bot is controlled via the dev/run-tests-jenkins script.

AmplabJenkins is controlled by the Jenkins GitHub Pull Request Builder plugin. This bot posts relatively uninformative comments (""Merge build triggered"", ""Merge build started"", ""Merge build failed"") that do not contain any links or details specific to the tests being run.

It is technically non-trivial prevent these AmplabJenkins comments from being posted in the first place (see https://issues.apache.org/jira/browse/SPARK-4216).

However, as a short-term hack I'd like to deploy a script which automatically deletes these comments as soon as they're posted, with an exemption carved out for the ""Can an admin approve this patch for testing?"" messages. This will help to significantly de-clutter pull request discussions in the GitHub UI.

If nobody objects, I'd like to deploy this script sometime in the next few days.

(From a technical perspective, my script uses the GitHub REST API and AmplabJenkins' own OAuth token to delete the comments.  The final deployment environment will most likely be the backend of http://spark-prs.appspot.com).

- Josh


"
"""=?gb18030?B?U2Vh?="" <261810726@qq.com>","Fri, 14 Aug 2015 13:01:50 +0800","=?gb18030?B?UmWjuiBwbGVhc2UgaGVscCB3aXRoIENsYXNzTm90?=
 =?gb18030?B?Rm91bmRFeGNlcHRpb24=?=","""=?gb18030?B?1tzHp+q7?="" <z.qianhao@gmail.com>, ""=?gb18030?B?ZGV2QHNwYXJrLmFwYWNoZS5vcmc=?="" <dev@spark.apache.org>","I have no idea... We use scala. You upgrade to 1.4 so quickly...,  are you using spark in production?  Spark 1.3 is better than spark1.4. 


------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""÷‹«ßÍª"";<z.qianhao@gmail.com>;
∑¢ÀÕ ±º‰: 2015ƒÍ8‘¬14»’(–«∆⁄ŒÂ) ÷–ŒÁ11:14
 ’º˛»À: ""Sea""<261810726@qq.com>; ""dev@spark.apache.org""<dev@spark.apache.org>; 

÷˜Ã‚: Re: please help with ClassNotFoundException



Hi Sea     I have updated spark to 1.4.1, however the problem still exists, any idea?


Sea <261810726@qq.com>”⁄2015ƒÍ8‘¬14»’÷‹ŒÂ …œŒÁ12:36–¥µ¿£∫

Yes, I guess so. I see this bug before.




------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""÷‹«ßÍª"";<z.qianhao@gmail.com>;
∑¢ÀÕ ±º‰: 2015ƒÍ8‘¬13»’(–«∆⁄Àƒ) ÕÌ…œ9:30
 ’º˛»À: ""Sea""<261810726@qq.com>; ""dev@spark.apache.org""<dev@spark.apache.org>; 

÷˜Ã‚: Re: please help with ClassNotFoundException




Hi sea    Is it the same issue as https://issues.apache.org/jira/browse/SPARK-8368


Sea <261810726@qq.com>”⁄2015ƒÍ8‘¬13»’÷‹Àƒ œ¬ŒÁ6:52–¥µ¿£∫

Are you using 1.4.0?  If yes, use 1.4.1




------------------ ‘≠ º” º˛ ------------------
∑¢º˛»À: ""÷‹«ßÍª"";<qhzhou@apache.org>;
∑¢ÀÕ ±º‰: 2015ƒÍ8‘¬13»’(–«∆⁄Àƒ) ÕÌ…œ6:04
 ’º˛»À: ""dev""<dev@spark.apache.org>; 

÷˜Ã‚: please help with ClassNotFoundException




Hi,    I am using spark 1.4 when an issue occurs to me.
    I am trying to use the aggregate function:
    JavaRdd<String> rdd = some rdd;
    HashMap<Long, TypeA> zeroValue = new HashMap();
    // add initial key-value pair for zeroValue
    rdd.aggregate(zeroValue, 
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>>(){//implementation},
                   new Function2<HashMap<Long, TypeA>,
                        String,
                        HashMap<Long, TypeA>(){//implementation})


    here is the stack trace when i run the application:


Caused by: java.lang.ClassNotFoundException: TypeA
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:274)
	at org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:66)
	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1612)
	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1517)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1771)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at java.util.HashMap.readObject(HashMap.java:1180)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1893)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1798)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:69)
	at org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:89)
	at org.apache.spark.util.Utils$.clone(Utils.scala:1458)
	at org.apache.spark.rdd.RDD$$anonfun$aggregate$1.apply(RDD.scala:1049)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.aggregate(RDD.scala:1047)
	at org.apache.spark.api.java.JavaRDDLike$class.aggregate(JavaRDDLike.scala:413)
	at org.apache.spark.api.java.AbstractJavaRDDLike.aggregate(JavaRDDLike.scala:47)

     however I have checked that TypeA is in the jar file which is in the classpath
    And when I use an empty HashMap as the zeroValue, the exception has gone
    Does anyone meet the same problem, or can anyone help me with it?



-- 

Best RegardZhouQianhao



-- 

Best RegardZhouQianhao"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Fri, 14 Aug 2015 06:21:53 +0000",Re: please help with ClassNotFoundException,"Sea <261810726@qq.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","Hi, Sea
     Problem solved, it turn out to be that I have updated spark cluster to
1.4.1, however the client has not been updated.
     Thank you so much.

Sea <261810726@qq.com>‰∫é2015Âπ¥8Êúà14Êó•Âë®‰∫î ‰∏ãÂçà1:01ÂÜôÈÅìÔºö

u
-
mail.com>;
•(ÊòüÊúü‰∫î) ‰∏≠Âçà11:14
.org""<
®‰∫î ‰∏äÂçà12:36ÂÜôÈÅìÔºö
--
gmail.com>;
•(ÊòüÊúüÂõõ) Êôö‰∏ä9:30
e.org""<
®Âõõ ‰∏ãÂçà6:52ÂÜôÈÅìÔºö
---
ache.org>;
•(ÊòüÊúüÂõõ) Êôö‰∏ä6:04
ass(JavaSerializer.scala:66)
1)
va:57)
rImpl.java:43)
)
8)
rializer.scala:69)
alizer.scala:89)
la:148)
la:109)
:413)
la:47)
s
"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 14 Aug 2015 12:12:58 +0530",Re: Spark runs into an Infinite loop even if the tasks are completed successfully,Imran Rashid <irashid@cloudera.com>,"Yep, and it works fine for operations which does not involve any shuffle
(like foreach,, count etc) and those which involves shuffle operations ends
up in an infinite loop. Spark should somehow indicate this instead of going
in an infinite loop.

Thanks
Best Regards


"
Mridul Muralidharan <mridul@gmail.com>,"Fri, 14 Aug 2015 00:34:52 -0700",Re: Spark runs into an Infinite loop even if the tasks are completed successfully,Akhil Das <akhil@sigmoidanalytics.com>,"What I understood from Imran's mail (and what was referenced in his
mail) the RDD mentioned seems to be violating some basic contracts on
how partitions are used in spark [1].
They cannot be arbitrarily numbered,have duplicates, etc.


Extending RDD to add functionality is typically for niche cases; and
requires subclasses to adhere to the explicit (and implicit)
contracts/lifecycles for them.
Using existing RDD's as template would be a good idea for
customizations - one way to look at it is, using RDD is more in api
space but extending them is more in spi space.

Violations would actually not even be detectable by spark-core in general case.


Regards,
Mridul

[1] Ignoring the array out of bounds, etc - I am assuming the intent
is to show overlapping partitions, duplicates. index to partition
mismatch - that sort of thing.



---------------------------------------------------------------------


"
pishen tsai <pishen02@gmail.com>,"Fri, 14 Aug 2015 15:56:03 +0800","Introduce a sbt plugin to deploy and submit jobs to a spark cluster
 on ec2",dev@spark.apache.org,"Hello,

I have written a sbt plugin called spark-deployer, which is able to
deploy a standalone spark cluster on aws ec2 and submit jobs to it.
https://github.com/pishen/spark-deployer

Compared to current spark-ec2 script, this design may have several
benefits (features):
1. All the code are written in Scala.
2. Just add one line in your project/plugins.sbt and you are ready to
go. (You don't have to download the python code and store it at
someplace.)
3. The whole development flow (write code for spark job, compile the
code, launch the cluster, assembly and submit the job to master,
terminate the cluster when the job is finished) can be done in sbt.
4. Support parallel deployment of the worker machines by Scala's Future.
5. Allow dynamically add or remove worker machines to/from the current cluster.
6. All the configurations are stored in a typesafe config file. You
don't need to store it elsewhere and map the settings into spark-ec2's
command line arguments.
7. The core library is separated from sbt plugin, hence it's possible
to execute the deployment from an environment without sbt (only JVM is
required).
8. Support adjustable ec2 root disk size, custom security groups,
custom ami (can run on default Amazon ami), custom spark tarball, and
VPC. (Well, most of these are also supported in spark-ec2 in slightly
different form, just mention it anyway.)

Since this project is still in its early stage, it lacks some features
of spark-ec2 such as self-installed HDFS (we use s3 directly),
stoppable cluster, ganglia, and the copy script.
However, it's already usable for our company and we are trying to move
our production spark projects from spark-ec2 to spark-deployer.

Any suggestion, testing help, or pull request are highly appreicated.

maybe as another choice (suggestion link) alongside spark-ec2 on
Spark's official documentation.
Of course, before that, I have to make this project stable enough
(strange errors just happen on aws api from time to time).
I'm wondering if this kind of contribution is possible and is there
any rule to follow or anyone to contact?
(Maybe the source code will not be merged into spark's main
repository, since I've noticed that spark-ec2 is also planning to move
out.)

Regards,
Pishen Tsai

---------------------------------------------------------------------


"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Fri, 14 Aug 2015 07:59:59 +0000",avoid creating small objects,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,
    All I want to do is that,
    1. read from some source
    2. do some calculation to get some byte array
    3. write the byte array to hdfs
    In hadoop, I can share an ImmutableByteWritable, and do some
System.arrayCopy, it will prevent the application from creating a lot of
small objects which will improve the gc latency.
    *However I was wondering if there is any solution like above in spark
that can avoid creating small objects*
"
Akhil Das <akhil@sigmoidanalytics.com>,"Fri, 14 Aug 2015 13:32:35 +0530",Re: Spark runs into an Infinite loop even if the tasks are completed successfully,Mridul Muralidharan <mridul@gmail.com>,"Thanks for the clarifications Mrithul.

Thanks
Best Regards


"
=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"Fri, 14 Aug 2015 08:07:50 +0000",Re: avoid creating small objects,"""dev@spark.apache.org"" <dev@spark.apache.org>","I am thinking that creating a shared object outside the closure, use this
object to hold the byte array.
will this work?

Âë®ÂçÉÊòä <qhzhou@apache.org>‰∫é2015Âπ¥8Êúà14Êó•Âë®‰∫î ‰∏ãÂçà4:02ÂÜôÈÅìÔºö

"
pishen tsai <pishen02@gmail.com>,"Fri, 14 Aug 2015 16:15:04 +0800","Re: Introduce a sbt plugin to deploy and submit jobs to a spark
 cluster on ec2",dev@spark.apache.org,"Sorry for previous line-breaking format, try to resend the mail again.

I have written a sbt plugin called spark-deployer, which is able to deploy
a standalone spark cluster on aws ec2 and submit jobs to it.
https://github.com/pishen/spark-deployer

Compared to current spark-ec2 script, this design may have several benefits
(features):
1. All the code are written in Scala.
2. Just add one line in your project/plugins.sbt and you are ready to go.
(You don't have to download the python code and store it at someplace.)
3. The whole development flow (write code for spark job, compile the code,
launch the cluster, assembly and submit the job to master, terminate the
cluster when the job is finished) can be done in sbt.
4. Support parallel deployment of the worker machines by Scala's Future.
5. Allow dynamically add or remove worker machines to/from the current
cluster.
6. All the configurations are stored in a typesafe config file. You don't
need to store it elsewhere and map the settings into spark-ec2's command
line arguments.
7. The core library is separated from sbt plugin, hence it's possible to
execute the deployment from an environment without sbt (only JVM is
required).
8. Support adjustable ec2 root disk size, custom security groups, custom
ami (can run on default Amazon ami), custom spark tarball, and VPC. (Well,
most of these are also supported in spark-ec2 in slightly different form,
just mention it anyway.)

Since this project is still in its early stage, it lacks some features of
spark-ec2 such as self-installed HDFS (we use s3 directly), stoppable
cluster, ganglia, and the copy script.
However, it's already usable for our company and we are trying to move our
production spark projects from spark-ec2 to spark-deployer.

Any suggestion, testing help, or pull request are highly appreciated.

another choice (suggestion link) alongside spark-ec2 on Spark's official
documentation.
Of course, before that, I have to make this project stable enough (strange
errors just happen on aws api from time to time).
I'm wondering if this kind of contribution is possible and is there any
rule to follow or anyone to contact?
(Maybe the source code will not be merged into spark's main repository,
since I've noticed that spark-ec2 is also planning to move out.)

Regards,
Pishen Tsai
"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Aug 2015 01:18:43 -0700",Re: avoid creating small objects,=?UTF-8?B?5ZGo5Y2D5piK?= <qhzhou@apache.org>,"You can use mapPartitions to do that.


l','qhzhou@apache.org');>>‰∫é2015Âπ¥8Êúà14Êó•Âë®‰∫î
"
=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"Fri, 14 Aug 2015 10:57:50 +0200",Re: Automatically deleting pull request comments left by AmplabJenkins,Josh Rosen <rosenville@gmail.com>,"
Some of these can be configured. For instance, make sure to disable ""Use
comments to report intermediate phases: triggered et al"", and if you add a
publicly accessible URL in ""Published Jenkins URL"", you will get a link to
the test result in the test result comment. I know these are global
settings, but the Jenkins URL is unique anyway, and intermediate phases are
probably equally annoying to everyone.

You can see the only comment posted for a successful PR build here:
https://github.com/scala-ide/scala-ide/pull/991#issuecomment-128016214

I'd avoid more custom code if possible.

my 2c,
iulian





-- 

--
Iulian Dragos

------
Reactive Apps on the JVM
www.typesafe.com
"
mkhaitman <mark.khaitman@chango.com>,"Fri, 14 Aug 2015 06:37:52 -0700 (MST)",Re: Fwd: [ANNOUNCE] Spark 1.5.0-preview package,dev@spark.apache.org,"Has anyone had success using this preview? We were able to build the preview,
and able to start the spark-master, however, unable to connect any spark
workers to it. 

Kept receiving ""AkkaRpcEnv address in use"" while attempting to connect the
spark-worker to the master. Also confirmed that the worker was indeed
starting and a non-blocking random port as expected, and not dying... just
refusing to connect. :/

Built using maven 3.3.3 with jdk 7, on centos 7. We're running a hadoop
2.7.1 cluster in development (where I attempted to run this), and targeting
hadoop 2.6 worked fine for 1.4.1 at least, so wondering if that is the cause
of anything.



--

---------------------------------------------------------------------


"
Silas Davis <silas@silasdavis.net>,"Fri, 14 Aug 2015 14:56:03 +0000",Re: Writing to multiple outputs in Spark,"Silas Davis <silas@silasdavis.net>, dev@spark.apache.org","Would it be right to assume that the silence on this topic implies others
don't really have this issue/desire?


"
Alex Angelini <alex.angelini@shopify.com>,"Fri, 14 Aug 2015 11:04:28 -0400",Re: Writing to multiple outputs in Spark,Silas Davis <silas@silasdavis.net>,"Speaking about Shopify's deployment, this would be a really nice to have
feature.

We would like to write data to folders with the structure
`<year>/<month>/<day>` but have had to hold off on that because of the lack
of support for MultipleOutputs.


"
"""Abhishek R. Singh"" <abhishsi@tetrationanalytics.com>","Fri, 14 Aug 2015 08:10:49 -0700",Re: Writing to multiple outputs in Spark,Silas Davis <silas@silasdavis.net>,"A workaround would be to have multiple passes on the RDD and each pass write its own output?

Or in a foreachPartition do it in a single pass (open up multiple files per partition to write out)?

-Abhishek-


others don't really have this issue/desire?
output files based on key, but the plain RDD interface doesn't seem to and it should.
It seems like a feature that is somewhat missing from Spark.
different locations depending based on the key. For example in a pair RDD your key may be (language, date, userId) and you would like to write separate files to $someBasePath/$language/$date. Then there would be  a version of saveAsHadoopDataset that would be able to multiple location based on key using the underlying OutputFormat. Perahps it would take a pair RDD with keys ($partitionKey, $realKey), so for example ((language, date), userId).
https://spark.apache.org/docs/1.4.0/api/scala/index.html#org.apache.spark.sql.DataFrameWriter
https://github.com/apache/spark/pull/4895/files. 
MultipleOutputFormat) which is part of the old hadoop1 API. It only works for text but could be generalised for any underlying OutputFormat by using MultipleOutputFormat (but only for hadoop1 - which doesn't support ParquetAvroOutputFormat for example)
https://gist.github.com/mlehman/df9546f6be2e362bbad2 
APIs) and extends saveAsNewHadoopDataset to support multiple outputs. Should work for any underlying OutputFormat. Probably better implemented by extending saveAs[NewAPI]HadoopDataset.
http://docs.cascading.org/cascading/2.5/javadoc/cascading/tap/local/PartitionTap.html to do this
Does Spark provide similar functionality through some other mechanism? How would it be best implemented?
wrapper OutputFormat that writes multiple outputs using hadoop MultipleOutputs but doesn't require modification of the PairRDDFunctions. The principle is similar however. Again it feels slightly hacky to use dummy fields for the ReduceContextImpl, but some of this may be a part of the impedance mismatch between Spark and plain Hadoop... Here is my attempt: https://gist.github.com/silasdavis/d1d1f1f7ab78249af462 
suggestion of how best to achieve it.

"
"""Shkurenko, Alex"" <ashkurenko@enova.com>","Fri, 14 Aug 2015 12:30:00 -0500",SparkR DataFrame fail to return data of Decimal type,dev@spark.apache.org,"Got an issue similar to https://issues.apache.org/jira/browse/SPARK-8897,
but with the Decimal datatype coming from a Postgres DB:

//Set up SparkR

~/Downloads/postgresql-9.4-1201.jdbc4.jar sparkr-shell"")

// Connect to a Postgres DB via JDBC
    CREATE TEMPORARY TABLE mytable
    USING org.apache.spark.sql.jdbc
    OPTIONS (url 'jdbc:postgresql://servername:5432/dbname'
    ,dbtable 'mydbtable'
)
"")

// Try pulling a Decimal column from a table

// The schema shows up fine


DataFrame[a_decimal_column:decimal(10,0)]


StructType
|-name = ""a_decimal_column"", type = ""DecimalType(10,0)"", nullable = TRUE

// ... but pulling data fails:

localDF <- collect(myDataFrame)

Error in as.data.frame.default(x[[i]], optional = TRUE) :
  cannot coerce class """"jobj"""" to a data.frame


-------
Proposed fix:

diff --git a/core/src/main/scala/org/apache/spark/api/r/SerDe.scala
b/core/src/main/scala/org/apache/spark/api/r/SerDe.scala
index d5b4260..b77ae2a 100644
--- a/core/src/main/scala/org/apache/spark/api/r/SerDe.scala
+++ b/core/src/main/scala/org/apache/spark/api/r/SerDe.scala
@@ -219,6 +219,9 @@ private[spark] object SerDe {
         case ""float"" | ""java.lang.Float"" =>
           writeType(dos, ""double"")
           writeDouble(dos, value.asInstanceOf[Float].toDouble)
+        case ""decimal"" | ""java.math.BigDecimal"" =>
+           writeType(dos, ""double"")
+           writeDouble(dos,
scala.math.BigDecimal(value.asInstanceOf[java.math.BigDecimal]).toDouble)
         case ""double"" | ""java.lang.Double"" =>
           writeType(dos, ""double"")
           writeDouble(dos, value.asInstanceOf[Double])

Thanks,
Alex
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 14 Aug 2015 10:43:25 -0700",Re: SparkR DataFrame fail to return data of Decimal type,"""Shkurenko, Alex"" <ashkurenko@enova.com>","Thanks for the catch. Could you send a PR with this diff ?


---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Fri, 14 Aug 2015 10:51:48 -0700",Re: Automatically deleting pull request comments left by AmplabJenkins,=?UTF-8?Q?Iulian_Drago=C8=99?= <iulian.dragos@typesafe.com>,"I think that I'm still going to want some custom code to remove the build
start messages from SparkQA and it's hardly any code, so I'm going to stick
with the custom approach for now. The problem is that I don't want _any_
posts from AmplabJenkins, even if they're improved to be more informative,
since our custom SparkQA provides nicer output.

.com>

:
hich
a
o
re
ng?""
"
"""Shkurenko, Alex"" <ashkurenko@enova.com>","Fri, 14 Aug 2015 13:20:23 -0500",Re: SparkR DataFrame fail to return data of Decimal type,shivaram@eecs.berkeley.edu,"Created https://issues.apache.org/jira/browse/SPARK-9982, working on the PR


"
Pete Robbins <robbinspg@gmail.com>,"Fri, 14 Aug 2015 19:27:33 +0100",Reliance on java.math.BigInteger implementation,dev@spark.apache.org,"ref: https://issues.apache.org/jira/browse/SPARK-9370

The code to handle BigInteger types in

org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters.java

and

org.apache.spark.unsafe.Platform.java

is dependant on the implementation of java.math.BigInteger

eg:

      try {
        signumOffset =
_UNSAFE.objectFieldOffset(BigInteger.class.getDeclaredField(""signum""));
        magOffset =
_UNSAFE.objectFieldOffset(BigInteger.class.getDeclaredField(""mag""));
      } catch (Exception ex) {
        // should not happen
      }

This is relying on there being fields ""int signum"" and ""int[] mag""

These implementaton fields are not part of the Java specification for this
class so can not be relied upon.

We are running Spark on IBM jdks and their spec-compliant implementation
has different internal fields. This causes an abort when running on these
java runtimes. There is also no guarantee that any future implentations of
OpenJDK will maintain these field names.

I think we need to find an implementation of these Spark functions that
only relies on Java compliant classes rather than specific implementations.

Any thoughts?

Cheers,
"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 14 Aug 2015 20:05:59 +0000",Re: Writing to multiple outputs in Spark,"""Abhishek R. Singh"" <abhishsi@tetrationanalytics.com>, Silas Davis <silas@silasdavis.net>","See: https://issues.apache.org/jira/browse/SPARK-3533

Feel free to comment there and make a case if you think the issue should be
reopened.

Nick


"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Aug 2015 13:09:38 -0700",Re: Writing to multiple outputs in Spark,Alex Angelini <alex.angelini@shopify.com>,"This is already supported with the new partitioned data sources in
DataFrame/SQL right?



"
"""Varadhan, Jawahar"" <varadhan@yahoo.com.INVALID>","Fri, 14 Aug 2015 20:15:43 +0000 (UTC)",Setting up Spark/flume/? to Ingest 10TB from FTP,"""dev@spark.apache.org"" <dev@spark.apache.org>","What is the best way to bring such a huge file from a FTP server into Hadoop to persist in HDFS? Since a single jvm process might run out of memory, I was wondering if I can use Spark or Flume to do this. Any help on this matter is appreciated.¬†
I prefer a application/process running inside Hadoop which is doing this transfer
Thanks."
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 14 Aug 2015 13:23:34 -0700",Re: Setting up Spark/flume/? to Ingest 10TB from FTP,"""Varadhan, Jawahar"" <varadhan@yahoo.com>","Why do you need to use Spark or Flume for this?

You can just use curl and hdfs:

  curl ftp://blah | hdfs dfs -put - /blah






-- 
Marcelo
"
=?UTF-8?B?SsO2cm4gRnJhbmtl?= <jornfranke@gmail.com>,"Fri, 14 Aug 2015 21:03:02 +0000",Re: Setting up Spark/flume/? to Ingest 10TB from FTP,"Marcelo Vanzin <vanzin@cloudera.com>, ""Varadhan, Jawahar"" <varadhan@yahoo.com>","Well what do you do in case of failure?
I think one should use a professional ingestion tool that ideally does not
need to reload everything in case of failure and verifies that the file has
been transferred correctly via checksums.
I am not sure if Flume supports ftp, but Ssh,scp should be supported. You
may check also other Flume sources or write your own in case of ftp (taking
into account comments above). I hope your file is compressed

Le ven. 14 ao√ªt 2015 √† 22:23, Marcelo Vanzin <vanzin@cloudera.com
 on
"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Aug 2015 14:03:45 -0700",Re: Fwd: [ANNOUNCE] Spark 1.5.0-preview package,mkhaitman <mark.khaitman@chango.com>,"Is it possible that you have only upgraded some set of nodes but not the
others?

We have ran some performance benchmarks on this so it definitely runs in
some configuration. Could still be buggy in some other configurations
though.



"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Aug 2015 14:58:01 -0700",SPARK-10000 + now,"""dev@spark.apache.org"" <dev@spark.apache.org>","Five month ago we reached 10000 commits on GitHub. Today we reached 10000
JIRA tickets.

https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20created%3E%3D-1w%20ORDER%20BY%20created%20DESC


Hopefully the extra character we have to type doesn't bring our
productivity much.
"
Reynold Xin <rxin@databricks.com>,"Fri, 14 Aug 2015 15:43:41 -0700",Re: Reliance on java.math.BigInteger implementation,Pete Robbins <robbinspg@gmail.com>,"I pinged the IBM team to submit a patch that would work on IBM JVM.



"
Cheolsoo Park <piaozhexiu@gmail.com>,"Fri, 14 Aug 2015 16:11:56 -0700",Jenkins having issues?,Dev <dev@spark.apache.org>,"Hi devs,

Jenkins failed twice in my PR <https://github.com/apache/spark/pull/8216>
for unknown error-

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/40930/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/40931/console

Can you help?

Thank you!
Cheolsoo
"
Josh Rosen <rosenville@gmail.com>,"Fri, 14 Aug 2015 16:24:52 -0700",Re: Automatically deleting pull request comments left by AmplabJenkins,dev <dev@spark.apache.org>,"The updated prototype listed in
https://github.com/databricks/spark-pr-dashboard/pull/59 is now running
live on spark-prs as part of its PR comment update task.


ck
,
fe.com
:
9
o
h
which
s
 a
to
are
m
n
ing?""
"
Muhammad Haseeb Javed <11besemjaved@seecs.edu.pk>,"Sat, 15 Aug 2015 16:44:30 +0500",Re: Switch from Sort based to Hash based shuffle,Akhil Das <akhil@sigmoidanalytics.com>,"Thanks guys, that did it.


"
Pete Robbins <robbinspg@gmail.com>,"Sat, 15 Aug 2015 13:41:55 +0100",Re: Reliance on java.math.BigInteger implementation,Reynold Xin <rxin@databricks.com>,"That would be me then ;-)

I'm working on a patch.

Cheers,


"
Gil Vernik <GILV@il.ibm.com>,"Sun, 16 Aug 2015 19:05:16 +0300",[spark-csv] how to build with Hadoop 2.6.0?,Dev <dev@spark.apache.org>,"I would like to build spark-csv with Hadoop 2.6.0
I noticed that when i build it with sbt/sbt ++2.10.4 package it build it 
with Hadoop 2.2.0 ( at least this is what i saw in the .ivy2 repository).

How to define 2.6.0 during spark-csv build? By the way, is it possible to 
build spark-csv using maven repository?

Thanks,
Gil.
"
Olivier Girardot <o.girardot@lateral-thoughts.com>,"Sun, 16 Aug 2015 19:16:07 +0200",Re: [ANNOUNCE] Nightly maven and package builds for Spark,,"Hi Patrick,
is there any way for the nightly build to include common distributions like
: with/without hive/yarn support, hadoop 2.4, 2.6 etc... ?
For now it seems that the nightly binary package builds actually ships only
the source ?
I can help on that too if you want,

Regards,

Olivier.

2015-08-02 5:19 GMT+02:00 Bharath Ravi Kumar <reachbach@gmail.com>:

m
t
F
spark/
 a


-- 
*Olivier Girardot* | Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Sun, 16 Aug 2015 14:38:07 -0700",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Olivier Girardot <o.girardot@lateral-thoughts.com>,"I just investigated this and this is happening because of a Maven
version requirement not being met. I'll look at modifying the build
scripts to use Maven 3.3.3 (with build/mvn --force ?)

Shivaram

ke
ly
m>
e
it
SF
apache/spark/
g
----

---------------------------------------------------------------------


"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Mon, 17 Aug 2015 01:04:25 -0700",Re: [ANNOUNCE] Nightly maven and package builds for Spark,Olivier Girardot <o.girardot@lateral-thoughts.com>,"This should be fixed now. I just triggered a manual build and the
latest binaries are at
http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/spark-1.5.0-SNAPSHOT-2015_08_17_00_36-3ff81ad-bin/

Thanks
Shivaram

at
ve
on
an
nt
o
rg/apache/spark/
-------

---------------------------------------------------------------------


"
Silas Davis <silas@silasdavis.net>,"Mon, 17 Aug 2015 12:16:25 +0000",Re: Writing to multiple outputs in Spark,"Reynold Xin <rxin@databricks.com>, Alex Angelini <alex.angelini@shopify.com>","@Reynold Xin: not really: it only works for Parquet (see partitionBy:
https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter),
it requires you to have a DataFrame in the first place (for my use case the
spark sql in"
java8964 <java8964@hotmail.com>,"Mon, 17 Aug 2015 10:55:34 -0400",Spark Job Hangs on our production cluster,"""user@spark.apache.org"" <user@spark.apache.org>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","I am comparing the log of Spark line by line between the hanging case (big dataset) and not hanging case (small dataset). 
In the hanging case, the Spark's log looks identical with not hanging case for reading the first block data from the HDFS.
But after that, starting from line 438 in the spark-hang.log, I only see the log generated from Worker, like following in the next 10 minutes:
15/08/14 14:24:19 DEBUG Worker: [actor] received message SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]15/08/14 14:24:19 DEBUG Worker: [actor] handled message (0.121965 ms) SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]...........................................15/08/14 14:33:04 DEBUG Worker: [actor] received message SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]15/08/14 14:33:04 DEBUG Worker: [actor] handled message (0.136146 ms) SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]
until almost 10 minutes I have to kill the job. I know it will hang forever.
But in the good log (spark-finished.log), starting from the line 361, Spark started to read the 2nd split data, I can see all the debug message from ""BlockReaderLocal, BlockManger"".
If I compared between these 2 cases log:
in the good log case from line 478, I can saw this message:15/08/14 14:37:09 DEBUG BlockReaderLocal: putting FileInputStream for ......
But in the hang log case for reading the 2nd split data, I don't see this message any more (It existed for the 1st split). I believe in this case, this log message should show up, as the 2nd split block also existed on this Spark node, as just before it, I can see the following debug message:
15/08/14 14:24:11 DEBUG BlockReaderLocal: Created BlockReaderLocal for file /services/contact2/data/contacts/20150814004805-part-r-00002.avro block BP-834217708-10.20.95.130-1438701195738:blk_1074484553_1099531839081 in datanode 10.20.95.146:5001015/08/14 14:24:11 DEBUG Project: Creating MutableProj: WrappedArray(), inputSchema: ArrayBuffer(account_id#0L, contact_id#1, sequence_id#2, state#3, name#4, kind#5, prefix_name#6, first_name#7, middle_name#8, company_name#9, job_title#10, source_name#11, source_detail    s#12, provider_name#13, provider_details#14, created_at#15L, create_source#16, updated_at#17L, update_source#18, accessed_at#19L, deleted_at#20L, delta#21, birthday_day#22, birthday_month#23, anniversary#24L, contact_fields#25, related_contacts#26, contact_channels#27    , contact_notes#28, contact_service_addresses#29, contact_street_addresses#30), codegen:false
This log is generated on node (10.20.95.146), and Spark created ""BlockReaderLocal"" to read the data from the local node.
Now my question is, can someone give me any idea why ""DEBUG BlockReaderLocal: putting FileInputStream for ...."" doesn't show up any more in this case?
I attached the log files again in this email, and really hope I can get some help from this list.
Thanks
Yong
From: java8964@hotmail.com
To: user@spark.apache.org
Subject: RE: Spark Job Hangs on our production cluster
Date: Fri, 14 Aug 2015 15:14:10 -0400




I still want to check if anyone can provide any help related to the Spark 1.2.2 will hang on our production cluster when reading Big HDFS data (7800 avro blocks), while looks fine for small data (769 avro blocks).
I enable the debug level in the spark log4j, and attached the log file if it helps to trouble shooting in this case.
Summary of our cluster:
IBM BigInsight V3.0.0.2 (running with Hadoop 2.2.0 + Hive 0.12)42 Data nodes, each one is running HDFS data node process + task tracker + spark work running 2nd Name node + JobTracker
The test cases I did are 2, using very simple spark shell to read 2 folders, one is big data with 1T avro files; another one is small data with 160G avro files.
The avro files schema of 2 folders are different, but I don't think that will make any difference here.
The test script is like following:
import org.apache.spark.sql.SQLContextval sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)import com.databricks.spark.avro._val testdata = sqlContext.avroFile(""hdfs://namenode:9000/bigdata_folder"")   // vs sqlContext.avroFile(""hdfs://namenode:9000/smalldata_folder"")testdata.registerTempTable(""testdata"")testdata.count()
Both cases are kicking off as the same following:/opt/spark/bin/spark-shell --jars /opt/ibm/cclib/spark-avro.jar --conf spark.ui.port=4042 --executor-memory 24G --total-executor-cores 42 --conf spark.storage.memoryFraction=0.1 --conf spark.sql.shuffle.partitions=2000 --conf spark.default.parallelism=2000
When the script point to the small data folder, the Spark can finish very fast. Each task of scanning the HDFS block can finish within 30 seconds or less.
When the script point to the big data folder, most of the nodes can finish scan the first block of HDFS within 2 mins (longer than case 1), then the scanning will hang, across all the nodes in the cluster, which means no task can continue any more. The whole job will hang until I have to killed it.
There are logs attached in this email, and here is what I can read from the log files:
1) Spark-finished.log, which is the log generated from Spark in good case.    In this case, it is clear there is a loop to read the data from the HDFS, looping like:    15/08/14 14:38:05 INFO HadoopRDD: Input split:    15/08/14 14:37:40 DEBUG Client: IPC Client (370155726) connection to p2-bigin101/10.20.95.130:9000 from....    15/08/14 14:37:40 DEBUG ProtobufRpcEngine: Call: getBlockLocations took 2ms    15/08/14 14:38:32 INFO HadoopRDD: Input split:
     There are exception in it, like:     java.lang.NoSuchMethodException: org.apache.hadoop.fs.FileSystem$Statistics.getThreadStatistics()        at java.lang.Class.getDeclaredMethod(Class.java:2009)        at org.apache.spark.util.Utils$.invoke(Utils.scala:1827)        at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:179)        at org.apache.spark.deploy.SparkHadoopUtil$$anonfun$getFileSystemThreadStatistics$1.apply(SparkHadoopUtil.scala:179)
But doesn't affect the function and didn't fail the job.
2) Spark-hang.log, which is from the same node generated from Spark in the hang case:    In this case, it looks like Spark can read the data from HDFS first time, as the log looked same as the good case log., but after that, only the following DEBUG message output: 15/08/14 14:24:19 DEBUG Worker: [actor] received message SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]15/08/14 14:24:19 DEBUG Worker: [actor] handled message (0.121965 ms) SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]15/08/14 14:24:34 DEBUG Worker: [actor] received message SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]15/08/14 14:24:34 DEBUG Worker: [actor] handled message (0.135455 ms) SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]15/08/14 14:24:49 DEBUG Worker: [actor] received message SendHeartbeat from Actor[akka://sparkWorker/user/Worker#90699948]
There is no more ""connecting"" to datanode message, until after 10 minus, I have to just kill the executor.
While in this 10 minutes, I did 2 times of ""jstack"" of the Spark java processor, trying to find out what thread is being blocked, attached as ""2698306-1.log"" and ""2698306-2.log"", as 2698306 is the pid.
Can some one give me any hint about what could be the root reason of this? While the spark is hanging to read the big dataset, the HDFS is health, as I can get/put the data in HDFS, and also the MR job running at same time continue without any problems.
I am thinking to generate a 1T text files folder to test Spark in this cluster, as I want to rule out any problem could related to AVRO, but it will take a while for me to generate that. But I am not sure if AVRO format could be the cause.
Thanks for your help.
Yong
From: java8964@hotmail.com
To: user@spark.apache.org
Subject: Spark Job Hangs on our production cluster
Date: Tue, 11 Aug 2015 16:19:05 -0400




Currently we have a IBM BigInsight cluster with 1 namenode + 1 JobTracker + 42 data/task nodes, which runs with BigInsight V3.0.0.2, corresponding with Hadoop 2.2.0 with MR1.
Since IBM BigInsight doesn't come with Spark, so we build Spark 1.2.2 with Hadoop 2.2.0 + Hive 0.12 by ourselves, and deploy it on the same cluster.
The IBM Biginsight comes with IBM jdk 1.7, but during our experience on stage environment, we found out Spark works better with Oracle JVM. So we run spark under Oracle JDK 1.7.0_79.
Now on production, we are facing a issue we never faced, nor can reproduce on our staging cluster. 
We are using Spark Standalone cluster, and here is the basic configurations:
more spark-env.shexport JAVA_HOME=/opt/javaexport PATH=$JAVA_HOME/bin:$PATHexport HADOOP_CONF_DIR=/opt/ibm/biginsights/hadoop-conf/export SPARK_CLASSPATH=/opt/ibm/biginsights/IHC/lib/ibm-compression.jar:/opt/ibm/biginsights/hive/lib/db2jcc4-10.6.jarexport SPARK_LOCAL_DIRS=/data1/spark/local,/data2/spark/local,/data3/spark/localexport SPARK_MASTER_WEBUI_PORT=8081export SPARK_MASTER_IP=host1export SPARK_MASTER_OPTS=""-Dspark.deploy.defaultCores=42""export SPARK_WORKER_MEMORY=24gexport SPARK_WORKER_CORES=6export SPARK_WORKER_DIR=/tmp/spark/workexport SPARK_DRIVER_MEMORY=2gexport SPARK_EXECUTOR_MEMORY=2g
more spark-defaults.confspark.master 			spark://host1:7077spark.eventLog.enabled 		truespark.eventLog.dir		hdfs://host1:9000/spark/eventLogspark.serializer		org.apache.spark.serializer.KryoSerializerspark.executor.extraJavaOptions	-verbose:gc -XX:+PrintGCDetails -XX:+PrintGCTimeStamps
We are using AVRO file format a lot, and we have these 2 datasets, one is about 96G, and the other one is a little over 1T. Since we are using AVRO, so we also built spark-avro of commit ""a788c9fce51b0ec1bb4ce88dc65c1d55aaa675b8"", which is the latest version supporting Spark 1.2.x.
Here is the problem we are facing on our production cluster, even the following simple spark-shell commands will hang in our production cluster:
import org.apache.spark.sql.SQLContextval sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)import com.databricks.spark.avro._val bigData = sqlContext.avroFile(""hdfs://namenode:9000/bigData/"")bigData.registerTempTable(""bigData"")bigData.count()
                                    (44 + 42) / 7800]
no update for more than 30 minutes and longer.
The big dataset with 1T should generate 7800 HDFS block, but Spark's HDFS client looks like having problem to read them. Since we are running Spark on the data nodes, all the Spark tasks are running as ""NODE_LOCAL"" on locality level.
If I go to the data/task node which Spark tasks hang, and use the JStack to dump the thread, I got the following on the top:
015-08-11 15:38:38Full thread dump Java HotSpot(TM) 64-Bit Server VM (24.79-b02 mixed mode):
""Attach Listener"" daemon prio=10 tid=0x00007f0660589000 nid=0x1584d waiting on condition [0x0000000000000000]   java.lang.Thread.State: RUNNABLE
""org.apache.hadoop.hdfs.PeerCache@4a88ec00"" daemon prio=10 tid=0x00007f06508b7800 nid=0x13302 waiting on condition [0x00007f060be94000]   java.lang.Thread.State: TIMED_WAITING (sleeping)        at java.lang.Thread.sleep(Native Method)        at org.apache.hadoop.hdfs.PeerCache.run(PeerCache.java:252)        at org.apache.hadoop.hdfs.PeerCache.access$000(PeerCache.java:39)        at org.apache.hadoop.hdfs.PeerCache$1.run(PeerCache.java:135)        at java.lang.Thread.run(Thread.java:745)
""shuffle-client-1"" daemon prio=10 tid=0x00007f0650687000 nid=0x132fc runnable [0x00007f060d198000]   java.lang.Thread.State: RUNNABLE        at sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)        at sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:269)        at sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:79)        at sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:87)        - locked <0x000000067bf47710> (a io.netty.channel.nio.SelectedSelectionKeySet)        - locked <0x000000067bf374e8> (a java.util.Collections$UnmodifiableSet)        - locked <0x000000067bf373d0> (a sun.nio.ch.EPollSelectorImpl)        at sun.nio.ch.SelectorImpl.select(SelectorImpl.java:98)        at io.netty.channel.nio.NioEventLoop.select(NioEventLoop.java:622)        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:310)        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116)        at java.lang.Thread.run(Thread.java:745)
Meantime, I can confirm our Hadoop/HDFS cluster works fine, as the MapReduce jobs also run without any problem, and ""Hadoop fs"" command works fine in the BigInsight.
I attached the jstack output with this email, but I don't know what could be the root reason.The same Spark shell command works fine, if I point to the small dataset, instead of big dataset. The small dataset will have around 800 HDFS blocks, and Spark finishes without any problem.
Here are some facts I know:
1) Since the BigInsight is running on IBM JDK, so I make the Spark run under the same JDK, same problem for BigData set.2) I even changed ""--total-executor-cores"" to 42, which will make each executor runs with one core (as we have 42 Spark workers), to avoid any multithreads, but still no luck.3) This problem of scanning 1T data hanging is NOT 100% for sure happening. Sometime I didn't see it, but more than 50% I will see it if I try.4) We never met this issue on our stage cluster, but it has only (1 namenode + 1 jobtracker + 3 data/task nodes), and the same dataset is only 160G on it.5) While the Spark java processing hanging, I didn't see any exception or issue on the HDFS data node log. 
Does anyone have any clue about this?
Thanks
Yong

 		 	   		  

---------------------------------------------------------------------
For additional commands, e-mail: user-help@spark.apache.org 		 	   		  

---------------------------------------------------------------------
For additional commands, e-mail: user-help@spark.apache.org 		 	   		  
---------------------------------------------------------------------"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Mon, 17 Aug 2015 15:09:58 +0000",[survey] [spark-ec2] What do you like/dislike about spark-ec2?,"Spark dev list <dev@spark.apache.org>, user <user@spark.apache.org>","Howdy folks!

I‚Äôm interested in hearing about what people think of spark-ec2
<http://spark.apache.org/docs/latest/ec2-scripts.html> outside of the
formal JIRA process. Your answers will all be anonymous and public.

If the embedded form below doesn‚Äôt work for you, you can use this link to
get the same survey:

http://goo.gl/forms/erct2s6KRR

Cheers!
Nick
‚Äã
"
Jerry Lam <chilinglam@gmail.com>,"Mon, 17 Aug 2015 16:07:27 -0400",Re: [survey] [spark-ec2] What do you like/dislike about spark-ec2?,Nicholas Chammas <nicholas.chammas@gmail.com>,"Hi Nick,

I forgot to mention in the survey that ganglia is never installed properly
for some reasons.

I have this exception every time I launched the cluster:

Starting httpd: httpd: Syntax error on line 154 of
/etc/httpd/conf/httpd.conf: Cannot load
/etc/httpd/modules/mod_authz_core.so into server:
/etc/httpd/modules/mod_authz_core.so: cannot open shared object file: No
such file or directory

[FAILED]

Best Regards,

Jerry


 link to
"
shane knapp <sknapp@berkeley.edu>,"Tue, 18 Aug 2015 16:26:45 -0700",Re: Jenkins having issues?,Cheolsoo Park <piaozhexiu@gmail.com>,"hey all...  so this has been happening intermittently and i'm not sure
what's causing it.

sometimes directories under the target/tmp/ dir get created w/o the
owner write bit set, so that they look like this:
dr-xr-xr-x.  2 jenkins jenkins  4096 Aug  9 01:28
/home/jenkins/workspace/SparkPullRequestBuilder/target/tmp/spark-67a08260-3318-42ec-b12d-65c700f8f220

this means that the next build that runs in that directory space fails
when 'git clean -fdx' encounters a directory that it can't remove.

for now, i've added the following lines to the pull request builder
run script (before the git clean command) to fix the target/ dir:

echo ""fixing target dir permissions""
chmod -R +w target/*

other than that, i'm looking around the codebase some older builds and
seeing if i can't find the culprit.
---------- Forwarded message ----------
From: Cheolsoo Park <piaozhexiu@gmail.com>
Date: Fri, Aug 14, 2015 at 4:11 PM
Subject: Jenkins having issues?
To: Dev <dev@spark.apache.org>


Hi devs,

Jenkins failed twice in my PR for unknown error-

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/40930/console
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/40931/console

Can you help?

Thank you!
Cheolsoo

---------------------------------------------------------------------


"
Cheolsoo Park <piaozhexiu@gmail.com>,"Tue, 18 Aug 2015 17:07:36 -0700",Re: Jenkins having issues?,shane knapp <sknapp@berkeley.edu>,"Thank you for looking into it.


"
canan chen <ccnfdu@gmail.com>,"Wed, 19 Aug 2015 16:44:09 +0800",What's the best practice for developing new features for spark ?,"dev@spark.apache.org, spark users <user@spark.apache.org>","I want to work on one jira, but it is not easy to do unit test, because it
involves different components especially UI. spark building is pretty slow,
I don't want to build it each time to test my code change. I am wondering
how other people do ? Is there any experience can share ? Thanks
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 19 Aug 2015 02:50:33 -0700",Re: What's the best practice for developing new features for spark ?,canan chen <ccnfdu@gmail.com>,"See this thread:

http://search-hadoop.com/m/q3RTtdZv0d1btRHl/Spark+build+module&subj=Building+Spark+Building+just+one+module+



 involves different components especially UI. spark building is pretty slow, I don't want to build it each time to test my code change. I am wondering how other people do ? Is there any experience can share ? Thanks

---------------------------------------------------------------------


"
canan chen <ccnfdu@gmail.com>,"Wed, 19 Aug 2015 20:10:40 +0800",Re: What's the best practice for developing new features for spark ?,Ted Yu <yuzhihong@gmail.com>,"Thanks Ted.  I notice another thread about running spark programmatically
(client mode for standalone and yarn). Would it be much easier to debug
spark if is is possible ? Hasn't anyone thought about it ?


"
=?UTF-8?Q?Zolt=C3=A1n_Zvara?= <zoltan.zvara@gmail.com>,"Wed, 19 Aug 2015 12:38:13 +0000",Re: What's the best practice for developing new features for spark ?,"canan chen <ccnfdu@gmail.com>, Ted Yu <yuzhihong@gmail.com>","I personally build with SBT and run Spark on YARN with IntelliJ. You need
to connect to remote JVMs with a remote debugger. You also need to do
similar, if you use Python, because it will launch a JVM on the driver
aswell.


"
Ratika Prasad <rprasad@couponsinc.com>,"Wed, 19 Aug 2015 15:52:14 +0000",Unable to run the spark application in standalone cluster mode,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi ,

We have a simple spark application which is running through when run locally on master node as below

./bin/spark-submit --class com.coupons.salestransactionprocessor.SalesTransactionDataPointCreation --master local sales-transaction-processor-0.0.1-SNAPSHOT-jar-with-dependencies.jar

But however I try to run it in cluster mode [ our spark cluster has two nodes one master and one slave with executer memory of 512MB], the application fails with the below, Pls provide some inputs as to why?

15/08/19 15:37:52 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/8 is now RUNNING
15/08/19 15:37:56 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:11 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:26 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/8 is now EXITED (Command exited with code 1)
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Executor app-20150819153234-0001/8 removed: Command exited with code 1
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor added: app-20150819153234-0001/9 on worker-20150812111932-ip-172-28-161-173.us-west-2.compute.internal-50108 (ip-172-28-161-173.us-west-2.compute.internal:50108) with 1 cores
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20150819153234-0001/9 on hostPort ip-172-28-161-173.us-west-2.compute.internal:50108 with 1 cores, 512.0 MB RAM
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/9 is now RUNNING
15/08/19 15:38:41 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:56 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:39:11 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:39:12 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/9 is now EXITED (Command exited with code 1)
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Executor app-20150819153234-0001/9 removed: Command exited with code 1
15/08/19 15:39:12 ERROR cluster.SparkDeploySchedulerBackend: Application has been killed. Reason: Master removed our application: FAILED
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Failed to run count at SalesTransactionDataPointCreation.java:29
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: FAILED
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
15/08/19 15:39:12 WARN thread.QueuedThreadPool: 8 threads could not be stopped
15/08/19 15:39:12 INFO ui.SparkUI: Stopped Spark web UI at http://172.28.161.131:4040
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Stopping DAGScheduler
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Shutting down all executors
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Asking each executor to shut down

Thanks
R
"
Madhusudanan Kandasamy <madhusudanan@in.ibm.com>,"Wed, 19 Aug 2015 21:31:25 +0530",Re: Unable to run the spark application in standalone cluster mode,Ratika Prasad <rprasad@couponsinc.com>,"
Try Increasing the spark worker memory in conf/spark-env.sh

export SPARK_WORKER_MEMORY=2g

Thanks,
Madhu.


                                                                           
             Ratika Prasad                                                 
             <rprasad@couponsi                                             
             nc.com>                                                    To 
                                       ""dev@spark.apache.org""              
             08/19/2015 09:22          <dev@spark.apache.org>              
             PM                                                         cc 
                                                                           
                                                                   Subject 
                                       Unable to run the spark application 
                                       in standalone cluster mode          
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Hi ,

We have a simple spark application which is running through when run
locally on master node as below

./bin/spark-submit --class
com.coupons.salestransactionprocessor.SalesTransactionDataPointCreation
--master local
sales-transaction-processor-0.0.1-SNAPSHOT-jar-with-dependencies.jar

But however I try to run it in cluster mode [ our spark cluster has two
nodes one master and one slave with executer memory of 512MB], the
application fails with the below, Pls provide some inputs as to why?

15/08/19 15:37:52 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/8 is now RUNNING
15/08/19 15:37:56 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:11 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:26 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/8 is now EXITED (Command exited with code 1)
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Executor
app-20150819153234-0001/8 removed: Command exited with code 1
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor added:
app-20150819153234-0001/9 on
worker-20150812111932-ip-172-28-161-173.us-west-2.compute.internal-50108
(ip-172-28-161-173.us-west-2.compute.internal:50108) with 1 cores
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Granted
executor ID app-20150819153234-0001/9 on hostPort
ip-172-28-161-173.us-west-2.compute.internal:50108 with 1 cores, 512.0 MB
RAM
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/9 is now RUNNING
15/08/19 15:38:41 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:56 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:39:11 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:39:12 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/9 is now EXITED (Command exited with code 1)
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Executor
app-20150819153234-0001/9 removed: Command exited with code 1
15/08/19 15:39:12 ERROR cluster.SparkDeploySchedulerBackend: Application
has been killed. Reason: Master removed our application: FAILED
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0,
whose tasks have all completed, from pool
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/metrics/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/static,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/executors/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/executors,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/environment/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/environment,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/pool,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/stage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages,null}
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Failed to run count at
SalesTransactionDataPointCreation.java:29
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Master removed our application: FAILED
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark
$scheduler$DAGScheduler$$failJobAndIndependentStages
(DAGScheduler.scala:1185)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage
$1.apply(DAGScheduler.scala:1174)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage
$1.apply(DAGScheduler.scala:1173)
        at scala.collection.mutable.ResizableArray$class.foreach
(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach
(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage
(DAGScheduler.scala:1173)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun
$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun
$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed
(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$
$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec
(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec
(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask
(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker
(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run
(ForkJoinWorkerThread.java:107)
15/08/19 15:39:12 WARN thread.QueuedThreadPool: 8 threads could not be
stopped
15/08/19 15:39:12 INFO ui.SparkUI: Stopped Spark web UI at
http://172.28.161.131:4040
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Stopping DAGScheduler
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Shutting down
all executors
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Asking each
executor to shut down

Thanks
R"
Ratika Prasad <rprasad@couponsinc.com>,"Wed, 19 Aug 2015 16:03:49 +0000",RE: Unable to run the spark application in standalone cluster mode,Madhusudanan Kandasamy <madhusudanan@in.ibm.com>,"Should this be done on master or slave node or both ?

From: Madhusudanan Kandasamy [mailto:madhusudanan@in.ibm.com]
Sent: Wednesday, August 19, 2015 9:31 PM
To: Ratika Prasad <rprasad@couponsinc.com>
Cc: dev@spark.apache.org
Subject: Re: Unable to run the spark application in standalone cluster mode


Try Increasing the spark worker memory in conf/spark-env.sh

export SPARK_WORKER_MEMORY=2g

Thanks,
Madhu.

[Inactive hide details for Ratika Prasad ---08/19/2015 09:22:37 PM---Ratika Prasad <rprasad@couponsinc.com>]Ratika Prasad ---08/19/2015 09:22:37 PM---Ratika Prasad <rprasad@couponsinc.com<mailto:rprasad@couponsinc.com>>
Ratika Prasad <rprasad@couponsinc.com<mailto:rprasad@couponsinc.com>>

08/19/2015 09:22 PM


To


""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>


cc




Subject


Unable to run the spark application in standalone cluster mode








Hi ,

We have a simple spark application which is running through when run locally on master node as below

./bin/spark-submit --class com.coupons.salestransactionprocessor.SalesTransactionDataPointCreation --master local sales-transaction-processor-0.0.1-SNAPSHOT-jar-with-dependencies.jar

But however I try to run it in cluster mode [ our spark cluster has two nodes one master and one slave with executer memory of 512MB], the application fails with the below, Pls provide some inputs as to why?

15/08/19 15:37:52 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/8 is now RUNNING
15/08/19 15:37:56 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:11 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:26 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/8 is now EXITED (Command exited with code 1)
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Executor app-20150819153234-0001/8 removed: Command exited with code 1
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor added: app-20150819153234-0001/9 on worker-20150812111932-ip-172-28-161-173.us-west-2.compute.internal-50108 (ip-172-28-161-173.us-west-2.compute.internal:50108) with 1 cores
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Granted executor ID app-20150819153234-0001/9 on hostPort ip-172-28-161-173.us-west-2.compute.internal:50108 with 1 cores, 512.0 MB RAM
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/9 is now RUNNING
15/08/19 15:38:41 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:38:56 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:39:11 WARN scheduler.TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient memory
15/08/19 15:39:12 INFO client.AppClient$ClientActor: Executor updated: app-20150819153234-0001/9 is now EXITED (Command exited with code 1)
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Executor app-20150819153234-0001/9 removed: Command exited with code 1
15/08/19 15:39:12 ERROR cluster.SparkDeploySchedulerBackend: Application has been killed. Reason: Master removed our application: FAILED
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/metrics/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/static,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/executors,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/environment,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/storage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/pool,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/stage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped o.e.j.s.ServletContextHandler{/stages,null}
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Failed to run count at SalesTransactionDataPointCreation.java:29
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due to stage failure: Master removed our application: FAILED
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1185)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1174)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1173)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1173)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
15/08/19 15:39:12 WARN thread.QueuedThreadPool: 8 threads could not be stopped
15/08/19 15:39:12 INFO ui.SparkUI: Stopped Spark web UI at http://172.28.161.131:4040
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Stopping DAGScheduler
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Shutting down all executors
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Asking each executor to shut down

Thanks
R
"
Madhusudanan Kandasamy <madhusudanan@in.ibm.com>,"Wed, 19 Aug 2015 21:40:59 +0530",RE: Unable to run the spark application in standalone cluster mode,Ratika Prasad <rprasad@couponsinc.com>,"
Slave nodes..

Thanks,
Madhu.


                                                                           
             Ratika Prasad                                                 
             <rprasad@couponsi                                             
             nc.com>                                                    To 
                                       Madhusudanan                        
             08/19/2015 09:33          Kandasamy/India/IBM@IBMIN           
             PM                                                         cc 
                                       ""dev@spark.apache.org""              
                                       <dev@spark.apache.org>              
                                                                   Subject 
                                       RE: Unable to run the spark         
                                       application in standalone cluster   
                                       mode                                
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           




Should this be done on master or slave node or both ?

From: Madhusudanan Kandasamy [mailto:madhusudanan@in.ibm.com]
Sent: Wednesday, August 19, 2015 9:31 PM
To: Ratika Prasad <rprasad@couponsinc.com>
Cc: dev@spark.apache.org
Subject: Re: Unable to run the spark application in standalone cluster mode



Try Increasing the spark worker memory in conf/spark-env.sh

export SPARK_WORKER_MEMORY=2g

Thanks,
Madhu.

Inactive hide details for Ratika Prasad ---08/19/2015 09:22:37 PM---Ratika
Prasad <rprasad@couponsinc.com>Ratika Prasad ---08/19/2015 09:22:37
PM---Ratika Prasad <rprasad@couponsinc.com>


                                                                           
       Ratika Prasad <                                                     
       rprasad@couponsinc.com>                                             
                                                                           
                                                                           
                                                                        To 
       08/19/2015 09:22 PM                                                 
                                                     ""dev@spark.apache.org 
                                                     "" <                   
                                                     dev@spark.apache.org> 
                                                                           
                                                                        cc 
                                                                           
                                                                           
                                                                   Subject 
                                                                           
                                                     Unable to run the     
                                                     spark application in  
                                                     standalone cluster    
                                                     mode                  
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           
                                                                           





Hi ,

We have a simple spark application which is running through when run
locally on master node as below

./bin/spark-submit --class
com.coupons.salestransactionprocessor.SalesTransactionDataPointCreation
--master local
sales-transaction-processor-0.0.1-SNAPSHOT-jar-with-dependencies.jar

But however I try to run it in cluster mode [ our spark cluster has two
nodes one master and one slave with executer memory of 512MB], the
application fails with the below, Pls provide some inputs as to why?

15/08/19 15:37:52 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/8 is now RUNNING
15/08/19 15:37:56 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:11 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:26 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/8 is now EXITED (Command exited with code 1)
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Executor
app-20150819153234-0001/8 removed: Command exited with code 1
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor added:
app-20150819153234-0001/9 on
worker-20150812111932-ip-172-28-161-173.us-west-2.compute.internal-50108
(ip-172-28-161-173.us-west-2.compute.internal:50108) with 1 cores
15/08/19 15:38:32 INFO cluster.SparkDeploySchedulerBackend: Granted
executor ID app-20150819153234-0001/9 on hostPort
ip-172-28-161-173.us-west-2.compute.internal:50108 with 1 cores, 512.0 MB
RAM
15/08/19 15:38:32 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/9 is now RUNNING
15/08/19 15:38:41 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:38:56 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:39:11 WARN scheduler.TaskSchedulerImpl: Initial job has not
accepted any resources; check your cluster UI to ensure that workers are
registered and have sufficient memory
15/08/19 15:39:12 INFO client.AppClient$ClientActor: Executor updated:
app-20150819153234-0001/9 is now EXITED (Command exited with code 1)
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Executor
app-20150819153234-0001/9 removed: Command exited with code 1
15/08/19 15:39:12 ERROR cluster.SparkDeploySchedulerBackend: Application
has been killed. Reason: Master removed our application: FAILED
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 0.0,
whose tasks have all completed, from pool
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/metrics/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/stage/kill,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/static,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/executors/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/executors,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/environment/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/environment,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage/rdd/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage/rdd,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/storage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/pool/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/pool,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/stage/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/stage,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages/json,null}
15/08/19 15:39:12 INFO handler.ContextHandler: stopped
o.e.j.s.ServletContextHandler{/stages,null}
15/08/19 15:39:12 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Failed to run count at
SalesTransactionDataPointCreation.java:29
Exception in thread ""main"" org.apache.spark.SparkException: Job aborted due
to stage failure: Master removed our application: FAILED
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark
$scheduler$DAGScheduler$$failJobAndIndependentStages
(DAGScheduler.scala:1185)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage
$1.apply(DAGScheduler.scala:1174)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage
$1.apply(DAGScheduler.scala:1173)
        at scala.collection.mutable.ResizableArray$class.foreach
(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach
(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage
(DAGScheduler.scala:1173)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun
$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun
$handleTaskSetFailed$1.apply(DAGScheduler.scala:688)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed
(DAGScheduler.scala:688)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$
$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1391)
        at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
        at akka.actor.ActorCell.invoke(ActorCell.scala:456)
        at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
        at akka.dispatch.Mailbox.run(Mailbox.scala:219)
        at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec
(AbstractDispatcher.scala:386)
        at scala.concurrent.forkjoin.ForkJoinTask.doExec
(ForkJoinTask.java:260)
        at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask
(ForkJoinPool.java:1339)
        at scala.concurrent.forkjoin.ForkJoinPool.runWorker
(ForkJoinPool.java:1979)
        at scala.concurrent.forkjoin.ForkJoinWorkerThread.run
(ForkJoinWorkerThread.java:107)
15/08/19 15:39:12 WARN thread.QueuedThreadPool: 8 threads could not be
stopped
15/08/19 15:39:12 INFO ui.SparkUI: Stopped Spark web UI at
http://172.28.161.131:4040
15/08/19 15:39:12 INFO scheduler.DAGScheduler: Stopping DAGScheduler
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Shutting down
all executors
15/08/19 15:39:12 INFO cluster.SparkDeploySchedulerBackend: Asking each
executor to shut down

Thanks
R

"
Ratika Prasad <rprasad@couponsinc.com>,"Wed, 19 Aug 2015 16:14:35 +0000",Creating RDD with key and Subkey,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi,

We have a need where we need the RDD with the following format JavaPairRDD<String,HashMap<String,List<String>>>, mostly RDD with a Key and Subkey kind of a structure, how is that doable in Spark ?

Thanks
R
"
Silas Davis <silas@silasdavis.net>,"Wed, 19 Aug 2015 17:04:23 +0000",Re: Creating RDD with key and Subkey,"Ratika Prasad <rprasad@couponsinc.com>, ""dev@spark.apache.org"" <dev@spark.apache.org>","This should be sent to the user mailing list, I think.

It depends what you want to do with the RDD, so yes you could throw around
(String, HashMap<String,List<String>>) tuples or perhaps you'd like to be
able to groupByKey, reduceByKey on the key and sub-key as a composite in
which case JavaPairRDD<Tuple2<String,String>, List<String>> might be more
appropriate. Not really clear what you are asking.



"
Ratika Prasad <rprasad@couponsinc.com>,"Wed, 19 Aug 2015 17:28:39 +0000",Re: Creating RDD with key and Subkey,"Silas Davis <silas@silasdavis.net>, ""dev@spark.apache.org""
	<dev@spark.apache.org>","
We need to create RDDas below

JavaPairRDD<String,List<HashMap<String,List<String>>>>

The idea is we need to do lookup() on Key which will return a list of hash maps kind of structure and then do lookup on subkey which is the key in the HashMap returned



_____________________________
From: Silas Davis <silas@silasdavis.net<mailto:silas@silasdavis.net>>
Sent: Wednesday, August 19, 2015 10:34 pm
Subject: Re: Creating RDD with key and Subkey
To: Ratika Prasad <rprasad@couponsinc.com<mailto:rprasad@couponsinc.com>>, <dev@spark.apache.org<mailto:dev@spark.apache.org>>


This should be sent to the user mailing list, I think.

It depends what you want to do with the RDD, so yes you could throw around (String, HashMap<String,List<String>>) tuples or perhaps you'd like to be able to groupByKey, reduceByKey on the key and sub-key as a composite in which case JavaPairRDD<Tuple2<String,String>, List<String>> might be more appropriate. Not really clear what you are asking.


Hi,

We have a need where we need the RDD with the following format JavaPairRDD<String,HashMap<String,List<String>>>, mostly RDD with a Key and Subkey kind of a structure, how is that doable in Spark ?

Thanks
R


"
Mohit Jaggi <mohitjaggi@gmail.com>,"Wed, 19 Aug 2015 11:47:06 -0700",Re: [spark-csv] how to build with Hadoop 2.6.0?,Gil Vernik <GILV@il.ibm.com>,"spark-csv should not depend on hadoop


"
Ranjana Rajendran <ranjana.rajendran@gmail.com>,"Wed, 19 Aug 2015 13:48:16 -0700",Re: Creating RDD with key and Subkey,Ratika Prasad <rprasad@couponsinc.com>,"Hi Ratika,

I tried the following:

val l = List(""apple"", ""orange"", ""banana"")

var inner = new scala.collection.mutable.HashMap[String, List[String]]

inner.put(""fruits"",l)

var list = new scala.collection.mutable.HashMap[String,
scala.collection.mutable.HashMap[String, List[String]]]

list.put(""food"", inner)

import scala.collection.JavaConverters._

val rdd = sc.parallelize(list.toSeq)

Now, the O(1) look up for value for a key is lost here. See the discussion
below:
http://apache-spark-user-list.1001560.n3.nabble.com/How-to-create-RDD-over-hashmap-td893.html


"
Gil Vernik <GILV@il.ibm.com>,"Thu, 20 Aug 2015 08:53:05 +0300",Re: [spark-csv] how to build with Hadoop 2.6.0?,Mohit Jaggi <mohitjaggi@gmail.com>,"It shouldn't?
This one com.databricks.spark.csv.util.TextFile has hadoop imports. 

I figured out that the answer to my question is just to add 
libraryDependencies += ""org.apache.hadoop"" % ""hadoop-client"" % ""2.6.0"". 
But i still wonder where is this 2.2.0 default comes from.



From:   Mohit Jaggi <mohitjaggi@gmail.com>
To:     Gil Vernik/Haifa/IBM@IBMIL
Cc:     Dev <dev@spark.apache.org>
Date:   19/08/2015 21:47
Subject:        Re: [spark-csv] how to build with Hadoop 2.6.0?



spark-csv should not depend on hadoop

I would like to build spark-csv with Hadoop 2.6.0 
I noticed that when i build it with sbt/sbt ++2.10.4 package it build it 
with Hadoop 2.2.0 ( at least this is what i saw in the .ivy2 repository). 

How to define 2.6.0 during spark-csv build? By the way, is it possible to 
build spark-csv using maven repository? 

Thanks, 
Gil. 


"
Mohit Jaggi <mohitjaggi@gmail.com>,"Thu, 20 Aug 2015 00:41:49 -0700",Re: [spark-csv] how to build with Hadoop 2.6.0?,Gil Vernik <GILV@il.ibm.com>,"2.2.0 is the default version spark uses if a specific version of hadoop is
not specified while building it.
spark-csv uses spark-packages to ""link"" with spark. ideally, it would not
care about any specific hadoop version. also ideally, spark-csv should not
have that hadoop import at all.
your workaround may lead to trouble because spark-csv would then include
hadoop in its assembly. you would then have duplicate hadoop client code
when you use this spark-csv assembly jar in a spark cluster.


"
Brian Granger <ellisonbg@gmail.com>,"Thu, 20 Aug 2015 10:03:06 -0700",Re: PySpark on PyPi,Auberon Lopez <auberon.lopez@gmail.com>,"Auberon, can you also post this to the Jupyter Google Group?

rote:
e:
r
e
te
.
o run
Äôs
g
e
d
ls
TH
k
r
on
p
SV
y.edu>
t
ld
0.8.2.1-src.zip:$PYTHONPATH
va
id
ts
he
on



-- 
Brian E. Granger
Associate Professor of Physics and Data Science
Cal Poly State University, San Luis Obispo
@ellisonbg on Twitter and GitHub
bgranger@calpoly.edu and ellisonbg@gmail.com

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Thu, 20 Aug 2015 18:08:09 +0000",Re: [survey] [spark-ec2] What do you like/dislike about spark-ec2?,"Spark dev list <dev@spark.apache.org>, user <user@spark.apache.org>","I'm planning to close the survey to further responses early next week.

If you haven't chimed in yet, the link to the survey is here:

http://goo.gl/forms/erct2s6KRR

We already have some great responses, which you can view. I'll share a
summary after the survey is closed.

Cheers!

Nick



 link to
"
mkhaitman <mark.khaitman@chango.com>,"Thu, 20 Aug 2015 12:26:05 -0700 (MST)",Re: Fwd: [ANNOUNCE] Spark 1.5.0-preview package,dev@spark.apache.org,"Turns out it was a mix of user-error as well as a bug in the sbt/sbt build
that has since been fixed in the current 1.5 branch (I built from this
commit: b4f4e91c395cb69ced61d9ff1492d1b814f96828)

I've been testing out the dynamic allocation specifically and it's looking
pretty solid! Haven't come across any regressions yet! 

(Hadoop 2.7.1 / CentOS 7) - targeted hadoop 2.6.0 though while building with
sbt.

Mark.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 12:32:45 -0700",Re: [ANNOUNCE] Spark 1.5.0-preview package,mkhaitman <mark.khaitman@chango.com>,"Thanks for reporting back, Mark.

I will soon post a release candidate.


"
Brian Granger <ellisonbg@gmail.com>,"Thu, 20 Aug 2015 13:43:42 -0700",Re: PySpark on PyPi,Auberon Lopez <auberon.lopez@gmail.com>,"I would start with just the plain python package without the JAR and
then see if it makes sense to add the JAR over time.

rote:
e
he
te:
ng
m>
n
ion.
s
e to run
‚Äôs
,
n,
rk
n/
o
to
is
k
 I
s
re
 I
n
Mb
do
4j-0.8.2.1-src.zip:$PYTHONPATH
e
r
y
to
--



-- 
Brian E. Granger
Associate Professor of Physics and Data Science
Cal Poly State University, San Luis Obispo
@ellisonbg on Twitter and GitHub
bgranger@calpoly.edu and ellisonbg@gmail.com

---------------------------------------------------------------------


"
Justin Uang <justin.uang@gmail.com>,"Thu, 20 Aug 2015 21:44:21 +0000",Re: PySpark on PyPi,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Brian Granger <ellisonbg@gmail.com>","I would prefer to just do it without the jar first as well. My hunch is
that to run spark the way it is intended, we need the wrapper scripts, like
spark-submit. Does anyone know authoritatively if that is the case?


PI
n
of
,
e
m
sation.
ible to
e
t
d
he
h
g
ed
on
t
?
at
d
It
.zip:$PYTHONPATH
f
e
a
n
"
Justin Uang <justin.uang@gmail.com>,"Thu, 20 Aug 2015 21:57:00 +0000",Re: PySpark on PyPi,"Olivier Girardot <o.girardot@lateral-thoughts.com>, Brian Granger <ellisonbg@gmail.com>","source distribution to PyPI? If so, is that something that the maintainers
need to add to the process that they use to publish releases?


ke
m>
o
m
k,
se
R
rsation.
sible to
re
y
it
ch
h
ng
d
 ?
ld
g
 It
c.zip:$PYTHONPATH
he
 a
on
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 20 Aug 2015 21:57:49 +0000",Dataframe aggregation with Tungsten unsafe,"""dev@spark.apache.org"" <dev@spark.apache.org>","Dear Spark developers,

I am trying to benchmark the new Dataframe aggregation implemented under the project Tungsten and released with Spark 1.4 (I am using the latest Spark from the repo, i.e. 1.5):
https://github.com/apache/spark/pull/5725
It tells that the aggregation should be faster due to using the unsafe to allocate memory and in-place update. It was also presented on Spark Summit this Summer:
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen
The following enables the new aggregation in spark-config:
spark.sql.unsafe.enabled=true
spark.unsafe.offHeap=true

I wrote a simple code that does aggregation of values by keys. However, the time needed to execute the code does not depend if the new aggregation is on or off. Could you suggest how can I observe the improvement that the aggregation provides? Could you write a code snippet that takes advantage of the new aggregation?

case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.count()
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)


Best regards, Alexander
"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 15:22:56 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","How did you run this? I couldn't run your query with 4G of RAM in 1.4, but
in 1.5 it ran.

Also I recommend just dumping the data to parquet on disk to evaluate,
rather than using the in-memory cache, which is super slow and we are
thinking of removing/replacing with something else.


val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x =>
(util.Random.nextInt(size / repetitions),
util.Random.nextDouble)).toDF(""key"", ""value"")

data.write.parquet(""/scratch/rxin/tmp/alex"")


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex"")
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)




"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 15:31:39 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>"," I didn't wait long enough earlier. Actually it did finish when I raised
memory to 8g.

In 1.5 with Tungsten (which should be the same as 1.4 with your unsafe
flags), the query took 40s with 4G of mem.

In 1.4, it took 195s with 8G of mem.

This is not a scientific benchmark and I only ran it once.




"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 16:07:40 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","BTW one other thing -- don't use the count() to do benchmark, since the
optimizer is smart enough to figure out that you don't actually need to run
the sum.


For the purpose of benchmarking, you can use

df.foreach(i => do nothing)





"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Thu, 20 Aug 2015 23:09:47 +0000",Re: Dataframe aggregation with Tungsten unsafe,Reynold Xin <rxin@databricks.com>,"Hi Reynold,

Thank you for suggestion. This code takes around 30 sec on my setup (5 workers with 32GB). My issue is that I don't see the change in time if I unset the unsafe flags. Could you explain why it might happen?

20 ¡◊«. 2015 «., ◊ 15:32, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> Œ¡–…”¡Ã(¡):

 I didn't wait long enough earlier. Actually it did finish when I raised memory to 8g.

In 1.5 with Tungsten (which should be the same as 1.4 with your unsafe flags), the query took 40s with 4G of mem.

In 1.4, it took 195s with 8G of mem.

This is not a scientific benchmark and I only ran it once.



How did you run this? I couldn't run your query with 4G of RAM in 1.4, but in 1.5 it ran.

Also I recommend just dumping the data to parquet on disk to evaluate, rather than using the in-memory cache, which is super slow and we are thinking of removing/replacing with something else.


val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => (util.Random.nextInt(size / repetitions), util.Random.nextDouble)).toDF(""key"", ""value"")

data.write.parquet(""/scratch/rxin/tmp/alex"")


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex"")
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)



Dear Spark developers,

I am trying to benchmark the new Dataframe aggregation implemented under the project Tungsten and released with Spark 1.4 (I am using the latest Spark from the repo, i.e. 1.5):
https://github.com/apache/spark/pull/5725
It tells that the aggregation should be faster due to using the unsafe to allocate memory and in-place update. It was also presented on Spark Summit this Summer:
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen
The following enables the new aggregation in spark-config:
spark.sql.unsafe.enabled=true
spark.unsafe.offHeap=true

I wrote a simple code that does aggregation of values by keys. However, the time needed to execute the code does not depend if the new aggregation is on or off. Could you suggest how can I observe the improvement that the aggregation provides? Could you write a code snippet that takes advantage of the new aggregation?

case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.count()
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)


Best regards, Alexander



---------------------------------------------------------------------


"
westurner <wes.turner@gmail.com>,"Thu, 20 Aug 2015 16:16:53 -0700 (MST)",Re: PySpark on PyPi,dev@spark.apache.org,"pip-installable source distribution to PyPI? If so, is that something that
the maintainers need to add to the process that they use to publish
releases?

A setup.py, Travis.yml, tox.ini (e.g cookiecutter)?
https://github.com/audreyr/cookiecutter-pypackage

https://wrdrd.com/docs/tools/#python-packages

* scripts=[]
* package_data / MANIFEST.in
* entry_points
   * console_scripts
   *
https://pythonhosted.org/setuptools/setuptools.html#eggsecutable-scripts

https://github.com/audreyr/cookiecutter-pypackage

... https://wrdrd.com/docs/consulting/knowledge-engineering#spark

that to run spark the way it is intended, we need the wrapper scripts, like
spark-submit. Does anyone know authoritatively if that is the case?
SPARK_HOME env variable is pointing to a Spark distribution with a
different version from the pyspark package ?
:
to see
PyPI
in the
n
work of
appreciated.
still being
pyspark,
conda.
email]>
there
package
release
PR to
a lot
python
ersation.
scenarios
ssible
to run
there‚Äôs
require
standards-compliant way
(Pandas,
situation,
actually work
it
python/
need to
the
adding
wants to
which
n
that is
with
compelling
installed
provided
team pick
t
PySpark, I
t
python
specific
distribution's
lack
SparkContext are
automatically, I
distribution
full 300Mb
compromise ?
that
could
booting up
?
It
=
unit-tests do
PYTHONPATH=$SPARK_HOME/python/:$SPARK_HOME/python/lib/py4j-0.8.2.1-src.zip:$PYTHONPATH
Python and
if we
newer
the
deploy
in a
python
tools to
---------------------------------------------------------------------
below:
http://apache-spark-developers-list.1001551.n3.nabble.com/PySpark-on-PyPi-tp12626p13766.html




--
3.nabble.com/PySpark-on-PyPi-tp12626p13772.html
om."
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 16:22:04 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","I think you might need to turn codegen on also in order for the unsafe
stuff to work.



bricks.com<mailto:
t
t
h-rosen
n
e
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 21 Aug 2015 00:01:26 +0000",RE: Dataframe aggregation with Tungsten unsafe,Reynold Xin <rxin@databricks.com>,"When I add the following option:
spark.sql.codegen      true

Spark crashed on the ‚Äúdf.count‚Äù with concurrentException (below). Are you sure that I need to set this flag to get unsafe? It looks like SQL flag, and I don‚Äôt use sql.


java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
         at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
         at org.spark-project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
         at org.spark-project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
         at org.spark-project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
         at org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
         at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
         at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
         at org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:286)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:283)
         at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:180)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:277)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:276)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
         at org.apache.spark.scheduler.Task.run(Task.scala:70)
         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
         at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
         at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
         at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
         at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
         at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
         at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:830)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:814)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
         at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
         at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
         at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
         at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
         at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
         at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
         at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
         at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
         at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:246)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:64)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:32)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:273)
         at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
         ... 28 more
Caused by: java.lang.ClassNotFoundException: Override
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:69)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at java.lang.Class.forName0(Native Method)
         at java.lang.Class.forName(Class.java:270)
         at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
         at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
         ... 66 more
Caused by: java.lang.ClassNotFoundException: Override
         at java.lang.ClassLoader.findClass(ClassLoader.java:531)
         at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:64)
         ... 73 more


From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Thursday, August 20, 2015 4:22 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Dataframe aggregation with Tungsten unsafe

I think you might need to turn codegen on also in order for the unsafe stuff to work.


On Thu, Aug 20, 2015 at 4:09 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Reynold,

Thank you for suggestion. This code takes around 30 sec on my setup (5 workers with 32GB). My issue is that I don't see the change in time if I unset the unsafe flags. Could you explain why it might happen?

20 –∞–≤–≥. 2015 –≥., –≤ 15:32, Reynold Xin <rxin@databricks.com<m't wait long enough earlier. Actually it did finish when I raised memory to 8g.

In 1.5 with Tungsten (which should be the same as 1.4 with your unsafe flags), the query took 40s with 4G of mem.

In 1.4, it took 195s with 8G of mem.

This is not a scientific benchmark and I only ran it once.



On Thu, Aug 20, 2015 at 3:22 PM, Rey<mailto:rxin@databricks.com<mailto:rxin@databricks.com>>> wrote:
How did you run this? I couldn't run your query with 4G of RAM in 1.4, but in 1.5 it ran.

Also I recommend just dumping the data to parquet on disk to evaluate, rather than using the in-memory cache, which is super slow and we are thinking of removing/replacing with something else.


val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => (util.Random.nextInt(size / repetitions), util.Random.nextDouble)).toDF(""key"", ""value"")

data.write.parquet(""/scratch/rxin/tmp/alex"")


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex"")
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)


On Thu, Aug 20, 2015 at 2:57 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am trying to benchmark the new Dataframe aggregation implemented under the project Tungsten and released with Spark 1.4 (I am using the latest Spark from the repo, i.e. 1.5):
https://github.com/apache/spark/pull/5725
It tells that the aggregation should be faster due to using the unsafe to allocate memory and in-place update. It was also presented on Spark Summit this Summer:
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen
The following enables the new aggregation in spark-config:
spark.sql.unsafe.enabled=true
spark.unsafe.offHeap=true

I wrote a simple code that does aggregation of values by keys. However, the time needed to execute the code does not depend if the new aggregation is on or off. Could you suggest how can I observe the improvement that the aggregation provides? Could you write a code snippet that takes advantage of the new aggregation?

case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.count()
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)


Best regards, Alexander


"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 17:25:57 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Yes - DataFrame and SQL are the same thing.

Which version are you running? Spark 1.4 doesn't run Janino --- but you
have a Janino exception?


(below). Are you
ide
ractFuture.java:306)
uture.java:293)
.java:116)
ly(Uninterruptibles.java:135)
ache.java:2410)
:2380)
he.java:2342)
)
.java:4874)
CodeGenerator.scala:286)
CodeGenerator.scala:283)
)
InMemoryColumnarTableScan.scala:277)
InMemoryColumnarTableScan.scala:276)
(RDD.scala:686)
(RDD.scala:686)
7)
7)
7)
)
)
:1145)
a:615)
n
5119)
:814)
:794)
er.java:350)
4)
java:769)
tCompiler.java:347)
139)
4)
va:383)
java:315)
odeGenerator.scala:246)
te(GeneratePredicate.scala:64)
te(GeneratePredicate.scala:32)
oad(CodeGenerator.scala:273)
(LocalCache.java:3599)
:2379)
cala:69)
oader.java:78)
:26)
:34)
:30)
cala:64)
bricks.com<mailto:
t
t
h-rosen
n
e
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 21 Aug 2015 00:35:09 +0000",RE: Dataframe aggregation with Tungsten unsafe,Reynold Xin <rxin@databricks.com>,"I am using Spark 1.5 cloned from master on June 12. (The aggregate unsafe feature was added to Spark on April 29.)

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Thursday, August 20, 2015 5:26 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Dataframe aggregation with Tungsten unsafe

Yes - DataFrame and SQL are the same thing.

Which version are you running? Spark 1.4 doesn't run Janino --- but you have a Janino exception?

On Thu, Aug 20, 2015 at 5:01 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
When I add the following option:
spark.sql.codegen      true

Spark crashed on the ‚Äúdf.count‚Äù with concurrentException (below). Are you sure that I need to set this flag to get unsafe? It looks like SQL flag, and I don‚Äôt use sql.


java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
         at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
         at org.spark-project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
         at org.spark-project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
         at org.spark-project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
         at org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
         at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
         at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
         at org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:286)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:283)
         at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:180)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:277)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:276)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
         at org.apache.spark.scheduler.Task.run(Task.scala:70)
         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
         at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
         at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
         at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
         at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
         at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
         at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:830)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:814)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
         at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
         at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
         at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
         at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
         at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
         at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
         at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
         at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
         at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:246)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:64)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:32)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:273)
         at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
         ... 28 more
Caused by: java.lang.ClassNotFoundException: Override
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:69)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at java.lang.Class.forName0(Native Method)
         at java.lang.Class.forName(Class.java:270)
         at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
         at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
         ... 66 more
Caused by: java.lang.ClassNotFoundException: Override
         at java.lang.ClassLoader.findClass(ClassLoader.java:531)
         at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:64)
         ... 73 more


From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
Sent: Thursday, August 20, 2015 4:22 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Dataframe aggregation with Tungsten unsafe

I think you might need to turn codegen on also in order for the unsafe stuff to work.


On Thu, Aug 20, 2015 at 4:09 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Reynold,

Thank you for suggestion. This code takes around 30 sec on my setup (5 workers with 32GB). My issue is that I don't see the change in time if I unset the unsafe flags. Could you explain why it might happen?

20 –∞–≤–≥. 2015 –≥., –≤ 15:32, Reynold Xin <rxin@databri
 I didn't wait long enough earlier. Actually it did finish when I raised memory to 8g.

In 1.5 with Tungsten (which should be the same as 1.4 with your unsafe flags), the query took 40s with 4G of mem.

In 1.4, it took 195s with 8G of mem.

This is not a scientific benchmark and I only ran it once.



On Thu, Aug 20, 2015 at 3:2icks.com><mailto:rxin@databricks.com<mailto:rxin@databricks.com>>> wrote:
How did you run this? I couldn't run your query with 4G of RAM in 1.4, but in 1.5 it ran.

Also I recommend just dumping the data to parquet on disk to evaluate, rather than using the in-memory cache, which is super slow and we are thinking of removing/replacing with something else.


val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => (util.Random.nextInt(size / repetitions), util.Random.nextDouble)).toDF(""key"", ""value"")

data.write.parquet(""/scratch/rxin/tmp/alex"")


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex"")
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)

On Thu, Aug 20, 2015 at 2:57 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am trying to benchmark the new Dataframe aggregation implemented under the project Tungsten and released with Spark 1.4 (I am using the latest Spark from the repo, i.e. 1.5):
https://github.com/apache/spark/pull/5725
It tells that the aggregation should be faster due to using the unsafe to allocate memory and in-place update. It was also presented on Spark Summit this Summer:
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen
The following enables the new aggregation in spark-config:
spark.sql.unsafe.enabled=true
spark.unsafe.offHeap=true

I wrote a simple code that does aggregation of values by keys. However, the time needed to execute the code does not depend if the new aggregation is on or off. Could you suggest how can I observe the improvement that the aggregation provides? Could you write a code snippet that takes advantage of the new aggregation?

case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.count()
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)


Best regards, Alexander


"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 17:43:18 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Please git pull :)



(below). Are you
ide
ractFuture.java:306)
uture.java:293)
.java:116)
ly(Uninterruptibles.java:135)
ache.java:2410)
:2380)
he.java:2342)
)
.java:4874)
CodeGenerator.scala:286)
CodeGenerator.scala:283)
)
InMemoryColumnarTableScan.scala:277)
InMemoryColumnarTableScan.scala:276)
(RDD.scala:686)
(RDD.scala:686)
7)
7)
7)
)
)
:1145)
a:615)
n
5119)
:814)
:794)
er.java:350)
4)
java:769)
tCompiler.java:347)
139)
4)
va:383)
java:315)
odeGenerator.scala:246)
te(GeneratePredicate.scala:64)
te(GeneratePredicate.scala:32)
oad(CodeGenerator.scala:273)
(LocalCache.java:3599)
:2379)
cala:69)
oader.java:78)
:26)
:34)
:30)
cala:64)
bricks.com<mailto:
t
t
h-rosen
n
e
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 21 Aug 2015 02:00:48 +0000",RE: Dataframe aggregation with Tungsten unsafe,Reynold Xin <rxin@databricks.com>,"Did git pull :)

Now I do get the difference in time between on/off Tungsten unsafe: it is 24-25 seconds (unsafe on) vs 32-26 seconds (unsafe off) for the example below.

Why I am not getting the improvement as advertised on Spark Summit (slide 23)?
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen

My dataset is 100M rows, is it big enough to get the improvement? Do I use aggregate correctly?


case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.foreach { x => {} }
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.foreach { x => {} }
println((System.nanoTime() - t) / 1e9)

Unsafe on:
spark.sql.codegen       true
spark.sql.unsafe.enabled        true
spark.unsafe.offHeap    true

Unsafe off:
spark.sql.codegen       false
spark.sql.unsafe.enabled        false
spark.unsafe.offHeap    false

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Thursday, August 20, 2015 5:43 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Dataframe aggregation with Tungsten unsafe

Please git pull :)


On Thu, Aug 20, 2015 at 5:35 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
I am using Spark 1.5 cloned from master on June 12. (The aggregate unsafe feature was added to Spark on April 29.)

From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
Sent: Thursday, August 20, 2015 5:26 PM

To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Dataframe aggregation with Tungsten unsafe

Yes - DataFrame and SQL are the same thing.

Which version are you running? Spark 1.4 doesn't run Janino --- but you have a Janino exception?

On Thu, Aug 20, 2015 at 5:01 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
When I add the following option:
spark.sql.codegen      true

Spark crashed on the ‚Äúdf.count‚Äù with concurrentException (below). Are you sure that I need to set this flag to get unsafe? It looks like SQL flag, and I don‚Äôt use sql.


java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
         at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
         at org.spark-project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
         at org.spark-project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
         at org.spark-project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
         at org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
         at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
         at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
         at org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:286)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:283)
         at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:180)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:277)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:276)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
         at org.apache.spark.scheduler.Task.run(Task.scala:70)
         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
         at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
         at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
         at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
         at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
         at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
         at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:830)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:814)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
         at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
         at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
         at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
         at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
         at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
         at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
         at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
         at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
         at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:246)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:64)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:32)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:273)
         at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
         ... 28 more
Caused by: java.lang.ClassNotFoundException: Override
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:69)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at java.lang.Class.forName0(Native Method)
         at java.lang.Class.forName(Class.java:270)
         at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
         at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
         ... 66 more
Caused by: java.lang.ClassNotFoundException: Override
         at java.lang.ClassLoader.findClass(ClassLoader.java:531)
         at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:64)
         ... 73 more


From: Reynoldom>]
Sent: Thursday, August 20, 2015 4:22 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Dataframe aggregation with Tungsten unsafe

I think you might need to turn codegen on also in order for the unsafe stuff to work.


On Thu, Aug 20, 2015 at 4:09 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Reynold,

Thank you for suggestion. This code takes around 30 sec on my setup (5 workers with 32GB). My issue is that I don't see the change in time if I unset the unsafe flags. Could you explain why it might happen?

20 –∞–≤–≥. 2015 –≥., –≤ 15:32, Reynold Xin <rxin@databricks.com<maio:rxin@databricks.com>>> –Ω–∞–ø–∏—Å–∞–ª(–∞):

 I didn't wait long enough earlier. Actually it did finish when I raised memory to 8g.

In 1.5 with Tungsten (which should be the same as 1.4 with your unsafe flags), the query took 40s with 4G of mem.

In 1.4, it took 195s with 8G of mem.

This is not a scientific benchmark and I only ran it once.



On Thu, Aug 20, 2015 at 3:22 PM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com><mailto:rxin@databricks.com<mailto:rxin@databricks.com>>> wrote:
How did you run this? I couldn't run your query with 4G of RAM in 1.4, but in 1.5 it ran.

Also I recommend just dumping the data to parquet on disk to evaluate, rather than using the in-memory cache, which is super slow and we are thinking of removing/replacing with something else.


val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => (util.Random.nextInt(size / repetitions), util.Random.nextDouble)).toDF(""key"", ""value"")

data.write.parquet(""/scratch/rxin/tmp/alex"")


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex"")
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)
On Thu, Aug 20, 2015 at 2:57 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am trying to benchmark the new Dataframe aggregation implemented under the project Tungsten and released with Spark 1.4 (I am using the latest Spark from the repo, i.e. 1.5):
https://github.com/apache/spark/pull/5725
It tells that the aggregation should be faster due to using the unsafe to allocate memory and in-place update. It was also presented on Spark Summit this Summer:
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen
The following enables the new aggregation in spark-config:
spark.sql.unsafe.enabled=true
spark.unsafe.offHeap=true

I wrote a simple code that does aggregation of values by keys. However, the time needed to execute the code does not depend if the new aggregation is on or off. Could you suggest how can I observe the improvement that the aggregation provides? Could you write a code snippet that takes advantage of the new aggregation?

case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.count()
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)


Best regards, Alexander



"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 21:24:08 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Not sure what's going on or how you measure the time, but the difference
here is pretty big when I test on my laptop. Maybe you set the wrong config
variables? (spark.sql.* are sql variables that you set in
sqlContext.setConf -- and in 1.5, they are consolidated into a single
flag: spark.sql.tungsten.enabled. See below.


I ran with a 10m dataset (created by calling sample(true, 0.1) on the 100m
dataset), since the 100m one takes too long when tungsten is off on my
laptop so I didn't wait. (40s - 50s with Tungsten on)


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex-10m"")

val t = System.nanoTime()
df.groupBy(""key"").sum(""value"").queryExecution.toRdd.count()
println((System.nanoTime() - t) / 1e9)



5.48951

sqlContext.setConf(""spark.sql.tungsten.enabled"", ""false"")

run it again, and took 25.127962.




It's also possible that the benefit is less when you have infinite amount
of memory (relative to the tiny dataset size) and as a result GC happens
less.



h-rosen
e
(below). Are you
ide
ractFuture.java:306)
uture.java:293)
.java:116)
ly(Uninterruptibles.java:135)
ache.java:2410)
:2380)
he.java:2342)
)
.java:4874)
CodeGenerator.scala:286)
CodeGenerator.scala:283)
)
InMemoryColumnarTableScan.scala:277)
InMemoryColumnarTableScan.scala:276)
(RDD.scala:686)
(RDD.scala:686)
7)
7)
7)
)
)
:1145)
a:615)
n
5119)
:814)
:794)
er.java:350)
4)
java:769)
tCompiler.java:347)
139)
4)
va:383)
java:315)
odeGenerator.scala:246)
te(GeneratePredicate.scala:64)
te(GeneratePredicate.scala:32)
oad(CodeGenerator.scala:273)
(LocalCache.java:3599)
:2379)
cala:69)
oader.java:78)
:26)
:34)
:30)
cala:64)
bricks.com<mailto:
t
t
h-rosen
n
e
"
Reynold Xin <rxin@databricks.com>,"Thu, 20 Aug 2015 21:37:49 -0700",[VOTE] Release Apache Spark 1.5.0 (RC1),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.5.0!

The vote is open until Monday, Aug 17, 2015 at 20:00 UTC and passes if a
majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/


The tag to be voted on is v1.5.0-rc1:
https://github.com/apache/spark/tree/4c56ad772637615cc1f4f88d619fac6c372c8552

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc1-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release can be found at:
https://repository.apache.org/content/repositories/orgapachespark-1137/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc1-docs/


=======================================
== How can I help test this release? ==
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.


================================================
== What justifies a -1 vote for this release? ==
================================================
This vote is happening towards the end of the 1.5 QA period, so -1 votes
should only occur for significant regressions from 1.4. Bugs already
present in 1.4, minor regressions, or bugs related to new features will not
block this release.


===============================================================
== What should happen to JIRA tickets still targeting 1.5.0? ==
===============================================================
1. It is OK for documentation patches to target 1.5.0 and still go into
branch-1.5, since documentations will be packaged separately from the
release.
2. New features for non-alpha-modules should target 1.6+.
3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the target
version.


==================================================
== Major changes to help you focus your testing ==
==================================================
As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.

RDD/DataFrame/SQL APIs

- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing

DataFrame/SQL Backend Execution

- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans

Data Sources, Hive, Hadoop, Mesos and Cluster Management

- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names

SparkR

- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like

Streaming

- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI

Machine Learning and Advanced Analytics

- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Thu, 20 Aug 2015 21:43:52 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Reynold Xin <rxin@databricks.com>,"FYI

The staging repository published as version 1.5.0 is at
https://repository.apache.org/content/repositories/orgapachespark-1136
while the staging repository published as version 1.5.0-rc1 is at
https://repository.apache.org/content/repositories/orgapachespark-1137

Thanks
Shivaram


---------------------------------------------------------------------


"
Eugene Morozov <evgeny.a.morozov@gmail.com>,"Fri, 21 Aug 2015 13:37:28 +0300",DataFrame. SparkPlan / Project serialization issue: ArrayIndexOutOfBounds.,"dev@spark.apache.org, user@spark.apache.org","Hi,

I'm using spark 1.3.1 built against hadoop 1.0.4 and java 1.7 and I'm
trying to save my data frame to parquet.
The issue I'm stuck looks like serialization tries to do pretty weird
thing: tries to write to an empty array.

The last (through stack trace) line of spark code that leads to exception
is in method SerializationDebugger.visitSerializable(o: Object, stack:
List[String]): List[String].
desc.getObjFieldValues(finalObj, objFieldValues)

The reason it does so, is because finalObj is
org.apache.spark.sql.execution.Project and objFieldValues is an empty
array! As a result there are two fields to read from the Project instance
object (happens in java.io.ObjectStreamClass), but there is an empty array
to read into.

A little bit of code with debug info:
private def visitSerializable(o: Object, stack: List[String]): List[String]
= {
val (finalObj, desc) = findObjectAndDescriptor(o) //finalObj: ""Project"",
""desc: ""org.apache.spark.sql.execution.Project""
      val slotDescs = desc.getSlotDescs //java.io.ObjectStreamClass[2] [0:
SparkPlan, 1: Project]
      var i = 0 //i: 0
      while (i < slotDescs.length) {
        val slotDesc = slotDescs(i) //slotDesc:
""org.apache.spark.sql.execution.SparkPlan""
        if (slotDesc.hasWriteObjectMethod) {
          // TODO: Handle classes that specify writeObject method.
        } else {
          val fields: Array[ObjectStreamField] = slotDesc.getFields
//fields: java.io.ObjectStreamField[1] [0: ""Z codegenEnabled""]
          val objFieldValues: Array[Object] = new
Array[Object](slotDesc.getNumObjFields) //objFieldValues:
java.lang.Object[0]
          val numPrims = fields.length - objFieldValues.length //numPrims: 1
          desc.getObjFieldValues(finalObj, objFieldValues) //leads to
exception

So it looks like it gets objFieldValues array from the SparkPlan object,
but uses it to receive values from Project object.

Here is the schema of my data frame
root
 |-- Id: long (nullable = true)
 |-- explodes: struct (nullable = true)
 |    |-- Identifiers: array (nullable = true)
 |    |    |-- element: struct (containsNull = true)
 |    |    |    |-- Type: array (nullable = true)
 |    |    |    |    |-- element: string (containsNull = true)
 |-- Identifiers: struct (nullable = true)
 |    |-- Type: array (nullable = true)
 |    |    |-- element: string (containsNull = true)
 |-- Type2: string (nullable = true)
 |-- Type: string (nullable = true)

Actual stack trace is:
org.apache.spark.SparkException: Task not serializable
at
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:166)
at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:158)
at org.apache.spark.SparkContext.clean(SparkContext.scala:1623)
at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:635)
at org.apache.spark.sql.execution.Project.execute(basicOperators.scala:40)
at
org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:84)
at org.apache.spark.sql.DataFrame.collect(DataFrame.scala:887)
at
com.reltio.analytics.data.application.DataAccessTest.testEntities_NestedAttribute(DataAccessTest.java:199)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at
org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
at
org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
at
org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
at
org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
at
org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
at
org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:27)
at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:271)
at
org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:70)
at
org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
at org.junit.runners.ParentRunner$3.run(ParentRunner.java:238)
at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:63)
at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:236)
at org.junit.runners.ParentRunner.access$000(ParentRunner.java:53)
at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:229)
at org.junit.runners.ParentRunner.run(ParentRunner.java:309)
at org.junit.runner.JUnitCore.run(JUnitCore.java:160)
at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:68)
Caused by: java.lang.reflect.InvocationTargetException
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
at
org.apache.spark.serializer.SerializationDebugger$ObjectStreamClassMethods$.getObjFieldValues$extension(SerializationDebugger.scala:240)
at
org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializable(SerializationDebugger.scala:150)
at
org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visit(SerializationDebugger.scala:99)
at
org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializable(SerializationDebugger.scala:158)
at
org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visit(SerializationDebugger.scala:99)
at
org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visitSerializable(SerializationDebugger.scala:158)
at
org.apache.spark.serializer.SerializationDebugger$SerializationDebugger.visit(SerializationDebugger.scala:99)
at
org.apache.spark.serializer.SerializationDebugger$.find(SerializationDebugger.scala:58)
at
org.apache.spark.serializer.SerializationDebugger$.improveException(SerializationDebugger.scala:39)
at
org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:47)
at
org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:80)
at
org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:164)
... 30 more
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
at
java.io.ObjectStreamClass$FieldReflector.getObjFieldValues(ObjectStreamClass.java:2050)
at java.io.ObjectStreamClass.getObjFieldValues(ObjectStreamClass.java:1252)
... 46 more
--
Be well!
Jean Morozov
"
Marek Kolodziej <mkolod.dev@gmail.com>,"Fri, 21 Aug 2015 05:29:13 -0700",Tungsten and sun.misc.Unsafe,"user@spark.apache.org, dev@spark.apache.org","Hello,

I attended the Tungsten-related presentations at Spark Summit (by Josh
Rosen) and at Big Data Scala (by Matei Zaharia). Needless to say, this
project holds great promise for major performance improvements.

At Josh's talk, I heard about the use of sun.misc.Unsafe as a way of
achieving some of these optimizations (e.g. slides 11-17 of Josh's
presentation:
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen).
I have no problems with the use of Unsafe in the code itself (I've done it
before myself, too), however I think there is a considerable risk
associated with beginning the use of Unsafe now, because Oracle is
determined to limit access to APIs such as Unsafe starting in Java 9.

JEP 260 <http://openjdk.java.net/jeps/260> was filed specifically to limit
access to internal JDK APIs that were ""never intended for external use,
including ""sun.misc.*"" The JEP does say that the functionality of
sun.misc.Unsafe is to remain available even as other internal APIs are
blocked for non-JDK use, however, it also says that ""the functionality of
many methods of this class is now available via *variable handles (JEP 193
<http://openjdk.java.net/jeps/193>).*"" If the direct access to
sun.misc.Unsafe is blocked and only the variable handles access remains,
this may mean more than just a need for code refactoring - functionality
such as doing ""malloc"" from Spark core may be restricted.

JEP 260 has evolved quite a bit over time and the wording available now
(after the Aug. 4, 2015) seems more reasonable than before. Nevertheless,
Hazelcast and other companies whose technologies depend on the availability
of Unsafe started a Google doc here
<https://docs.google.com/document/d/1GDm_cAxYInmoHMor-AkStzWvwE9pw6tnz_CebJQxuUE/edit#heading=h.brct71tr6e13>
.

I doubt that Oracle would want to make life difficult for everyone. In
addition to Spark's code base, projects such as Akka, Cassandra, Hibernate,
Netty, Neo4j and Spring (among many others) depend on Unsafe. Still, there
are tons of posts about this issue in the Java community (e.g. here
<https://jaxenter.com/hazelcast-on-java-unsafe-class-119286.html>'s a
Hazelcast interview, also from Aug. 3, the day before the latest update to
JEP 260). There are tons of concerned posts on the blogosphere, too (e.g.
here
<http://blog.dripstat.com/removal-of-sun-misc-unsafe-a-disaster-in-the-making/>
).

Have the leaders of the Spark community been following these Unsafe-related
developments and if so, what's Spark's plan of handling whatever Oracle
throws our way?

Marek
"
Sean Owen <sowen@cloudera.com>,"Fri, 21 Aug 2015 17:28:59 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Reynold Xin <rxin@databricks.com>,"Signatures, license, etc. look good. I'm getting some fairly
consistent failures using Java 7 + Ubuntu 15 + ""-Pyarn -Phive
-Phive-thriftserver -Phadoop-2.6"" -- does anyone else see these? they
are likely just test problems, but worth asking. Stack traces are at
the end.

There are currently 79 issues targeted for 1.5.0, of which 19 are
bugs, of which 1 is a blocker. (1032 have been resolved for 1.5.0.)
That's significantly better than at the last release. I presume a lot
of what's still targeted is not critical and can now be
untargeted/retargeted.

It occurs to me that the flurry of planning that took place at the
start of the 1.5 QA cycle a few weeks ago was quite helpful, and is
the kind of thing that would be even more useful at the start of a
release cycle. So would be great to do this for 1.6 in a few weeks.
Indeed there are already 267 issues targeted for 1.6.0 -- a decent
roadmap already.


Test failures:

Core

- Unpersisting TorrentBroadcast on executors and driver in distributed
mode *** FAILED ***
  java.util.concurrent.TimeoutException: Can't find 2 executors before
10000 milliseconds elapsed
  at org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561)
  at org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:313)
  at org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:287)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply$mcV$sp(BroadcastSuite.scala:165)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165)
  at org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165)
  at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
  at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
  at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
  at org.scalatest.Transformer.apply(Transformer.scala:22)
  ...

Streaming

- stop slow receiver gracefully *** FAILED ***
  0 was not greater than 0 (StreamingContextSuite.scala:324)

Kafka

- offset recovery *** FAILED ***
  The code passed to eventually never returned normally. Attempted 191
times over 10.043196973 seconds. Last failure message:
strings.forall({
    ((elem: Any) => DirectKafkaStreamSuite.collectedData.contains(elem))
  }) was false. (DirectKafkaStreamSuite.scala:249)


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 21 Aug 2015 09:35:23 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),"""shivaram@eecs.berkeley.edu"" <shivaram@eecs.berkeley.edu>","A mistake in the original email. The vote closes at 20:00 UTC, Aug 24,
rather than Aug 17.


"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Fri, 21 Aug 2015 18:07:56 +0000",RE: Dataframe aggregation with Tungsten unsafe,Reynold Xin <rxin@databricks.com>,"I‚Äôve made few experiments in different settings based on the same code that you used.
1)Created two datasets in hdfs on a cluster of 5 worker nodes and copied them to local fs:
val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => (util.Random.nextInt(size / repetitions), util.Random.nextDouble)).toDF(""key"", ""value"")
data.write.parquet(""hdfs://alex"")
data.write.parquet(‚Äú/home/alex‚Äù)
val sample = data.sample(true, 0.1)
sample.write.parquet(""hdfs://alex-10m"")
sample.write.parquet(‚Äú/home/alex-10m‚Äù)
2) Run the following code in local mode (spark-shell --master local) and cluster mode (5 nodes with 1 worker each)
val df = sqlContext.read.parquet(""data"")
val t = System.nanoTime()
df.groupBy(""key"").sum(""value"").queryExecution.toRdd.count()
println((System.nanoTime() - t) / 1e9)
3) Run the same code in local and cluster mode with persisting the data in memory
val df = sqlContext.read.parquet(""data"")
df.persist
df.foreach { x => {} }
val t = System.nanoTime()
df.groupBy(""key"").sum(""value"").queryExecution.toRdd.count()
println((System.nanoTime() - t) / 1e9)

In the above both cases Tungsten was switched on or off by:
sqlContext.setConf(""spark.sql.tungsten.enabled"", ""true"" or ‚Äùfalse‚Äù).
Each experiment was run in a new shell. Below are the results:

Data size

Mode

Storage

Tungsten disabled

Tungsten enabled

10M

Cluster


Parquet

9.6

7.4

Persist

10.9

5.1

Local

Parquet

57.7

35.8

Persist

61.9

31.4

100M

Cluster

Parquet

25.4

18.8

Persist

48.6

14.8


Hardware: 6x nodes with 2x Xeon  X5650  @ 2.67 32GB RAM, 1 master, 5 workers. Local mode: one node.

It seems that there is a nice improvement with Tungsten enabled given that data is persisted in memory 2x and 3x. However, the improvement is not that nice for parquet, it is 1.5x. What‚Äôs interesting, with Tungsten enabled performance of in-memory data and parquet data aggregation is similar. Could anyone comment on this? It seems counterintuitive to me.

Local performance was not as good as Reynold had. I have around 1.5x, he had 5x. However, local mode is not interesting.


From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Thursday, August 20, 2015 9:24 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Dataframe aggregation with Tungsten unsafe

Not sure what's going on or how you measure the time, but the difference here is pretty big when I test on my laptop. Maybe you set the wrong config variables? (spark.sql.* are sql variables that you set in sqlContext.setConf -- and in 1.5, they are consolidated into a single flag: spark.sql.tungsten.enabled. See below.


I ran with a 10m dataset (created by calling sample(true, 0.1) on the 100m dataset), since the 100m one takes too long when tungsten is off on my laptop so I didn't wait. (40s - 50s with Tungsten on)


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex-10m"")

val t = System.nanoTime()
df.groupBy(""key"").sum(""value"").queryExecution.toRdd.count()
println((System.nanoTime() - t) / 1e9)


On 1.5, with 8g driver memory and 8 cores:

5.48951

sqlContext.setConf(""spark.sql.tungsten.enabled"", ""false"")

run it again, and took 25.127962.


On 1.4, with 8g driver memory and 8 cores: 25.583473


It's also possible that the benefit is less when you have infinite amount of memory (relative to the tiny dataset size) and as a result GC happens less.


On Thu, Aug 20, 2015 at 7:00 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Did git pull :)

Now I do get the difference in time between on/off Tungsten unsafe: it is 24-25 seconds (unsafe on) vs 32-26 seconds (unsafe off) for the example below.

Why I am not getting the improvement as advertised on Spark Summit (slide 23)?
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen

My dataset is 100M rows, is it big enough to get the improvement? Do I use aggregate correctly?


case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.foreach { x => {} }
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.foreach { x => {} }
println((System.nanoTime() - t) / 1e9)

Unsafe on:
spark.sql.codegen       true
spark.sql.unsafe.enabled        true
spark.unsafe.offHeap    true

Unsafe off:
spark.sql.codegen       false
spark.sql.unsafe.enabled        false
spark.unsafe.offHeap    false

From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
Sent: Thursday, August 20, 2015 5:43 PM

To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Dataframe aggregation with Tungsten unsafe

Please git pull :)


On Thu, Aug 20, 2015 at 5:35 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
I am using Spark 1.5 cloned from master on June 12. (The aggregate unsafe feature was added to Spark on April 29.)

From: Reynold Xin [mailto:rxin@databricks.com<mailto:rxin@databricks.com>]
Sent: Thursday, August 20, 2015 5:26 PM

To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Dataframe aggregation with Tungsten unsafe

Yes - DataFrame and SQL are the same thing.

Which version are you running? Spark 1.4 doesn't run Janino --- but you have a Janino exception?

On Thu, Aug 20, 2015 at 5:01 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
When I add the following option:
spark.sql.codegen      true

Spark crashed on the ‚Äúdf.count‚Äù with concurrentException (below). Are you sure that I need to set this flag to get unsafe? It looks like SQL flag, and I don‚Äôt use sql.


java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
         at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
         at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
         at org.spark-project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
         at org.spark-project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
         at org.spark-project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
         at org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
         at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
         at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
         at org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:286)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:283)
         at org.apache.spark.sql.execution.SparkPlan.newPredicate(SparkPlan.scala:180)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:277)
         at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$8.apply(InMemoryColumnarTableScan.scala:276)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
         at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
         at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
         at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
         at org.apache.spark.scheduler.Task.run(Task.scala:70)
         at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
         at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
         at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 14, Column 10: Override
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
         at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
         at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
         at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
         at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
         at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
         at org.codehaus.janino.UnitCompiler.hasAnnotation(UnitCompiler.java:830)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:814)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
         at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
         at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
         at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
         at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
         at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
         at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
         at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
         at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
         at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
         at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
         at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
         at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
         at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
         at org.codehaus.janino.ClassBodyEvaluator.<init>(ClassBodyEvaluator.java:72)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:246)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:64)
         at org.apache.spark.sql.catalyst.expressions.codegen.GeneratePredicate$.create(GeneratePredicate.scala:32)
         at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:273)
         at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
         at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
         ... 28 more
Caused by: java.lang.ClassNotFoundException: Override
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:69)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at java.lang.Class.forName0(Native Method)
         at java.lang.Class.forName(Class.java:270)
         at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
         at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
         at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
         ... 66 more
Caused by: java.lang.ClassNotFoundException: Override
         at java.lang.ClassLoader.findClass(ClassLoader.java:531)
         at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
         at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
         at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
         at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:64)
         ... 73 more


Fromabricks.com>]
Sent: Thursday, August 20, 2015 4:22 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org<mailto:dev@spark.apache.org>
Subject: Re: Dataframe aggregation with Tungsten unsafe

I think you might need to turn codegen on also in order for the unsafe stuff to work.


On Thu, Aug 20, 2015 at 4:09 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Hi Reynold,

Thank you for suggestion. This code takes around 30 sec on my setup (5 workers with 32GB). My issue is that I don't see the change in time if I unset the unsafe flags. Could you explain why it might happen?

20 –∞–≤–≥. 2015 –≥., –≤ 15:32, Reynold Xin <rxin@databrickcom<mailto:rxin@databricks.com>>> –Ω–∞–ø–∏—Å–∞–ª(–∞):

 I didn't wait long enough earlier. Actually it did finish when I raised memory to 8g.

In 1.5 with Tungsten (which should be the same as 1.4 with your unsafe flags), the query took 40s with 4G of mem.

In 1.4, it took 195s with 8G of mem.

This is not a scientific benchmark and I only ran it once.



On Thu, Aug 20, 2015 at 3:22 PM, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com><mailto:rxin@databricks.com<mailto:rxin@databricks.com>>> wrote:
How did you run this? I couldn't run your query with 4G of RAM in 1.4, but in 1.5 it ran.

Also I recommend just dumping the data to parquet on disk to evaluate, rather than using the in-memory cache, which is super slow and we are thinking of removing/replacing with something else.


val size = 100000000
val partitions = 10
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => (util.Random.nextInt(size / repetitions), util.Random.nextDouble)).toDF(""key"", ""value"")

data.write.parquet(""/scratch/rxin/tmp/alex"")


val df = sqlContext.read.parquet(""/scratch/rxin/tmp/alex"")
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)
On Thu, Aug 20, 2015 at 2:57 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:
Dear Spark developers,

I am trying to benchmark the new Dataframe aggregation implemented under the project Tungsten and released with Spark 1.4 (I am using the latest Spark from the repo, i.e. 1.5):
https://github.com/apache/spark/pull/5725
It tells that the aggregation should be faster due to using the unsafe to allocate memory and in-place update. It was also presented on Spark Summit this Summer:
http://www.slideshare.net/SparkSummit/deep-dive-into-project-tungsten-josh-rosen
The following enables the new aggregation in spark-config:
spark.sql.unsafe.enabled=true
spark.unsafe.offHeap=true

I wrote a simple code that does aggregation of values by keys. However, the time needed to execute the code does not depend if the new aggregation is on or off. Could you suggest how can I observe the improvement that the aggregation provides? Could you write a code snippet that takes advantage of the new aggregation?

case class Counter(key: Int, value: Double)
val size = 100000000
val partitions = 5
val repetitions = 5
val data = sc.parallelize(1 to size, partitions).map(x => Counter(util.Random.nextInt(size / repetitions), util.Random.nextDouble))
val df = sqlContext.createDataFrame(data)
df.persist()
df.count()
val t = System.nanoTime()
val res = df.groupBy(""key"").agg(sum(""value""))
res.count()
println((System.nanoTime() - t) / 1e9)


Best regards, Alexander




"
Steve Loughran <stevel@hortonworks.com>,"Fri, 21 Aug 2015 18:11:52 +0000",Re: Tungsten and sun.misc.Unsafe,Marek Kolodziej <mkolod.dev@gmail.com>,"

I doubt that Oracle would want to make life difficult for everyone. In addition to Spark's code base, projects such as Akka, Cassandra, Hibernate, Netty, Neo4j and Spring (among many others) depend on Unsafe. Still, there are tons of posts about this issue in the Java community (e.g. here<https://jaxenter.com/hazelcast-on-java-unsafe-class-119286.html>'s a Hazelcast interview, also from Aug. 3, the day before the latest update to JEP 260). There are tons of concerned posts on the blogosphere, too (e.g. here<http://blog.dripstat.com/removal-of-sun-misc-unsafe-a-disaster-in-the-making/>).

Have the leaders of the Spark community been following these Unsafe-related developments and if so, what's Spark's plan of handling whatever Oracle throws our way?

I don't know about Spark, but I know that Hadoop uses a lot of it, introspecting into sun.security for access to kerberos operations, switching to the ibm. equivalent. Without that kerberos simply doesn't work.

As of now, the project's stance is ""if Oracle want hadoop to run on Oracle Java 9, they'd better have a plan""
"
Reynold Xin <rxin@databricks.com>,"Fri, 21 Aug 2015 11:18:42 -0700",Re: Tungsten and sun.misc.Unsafe,Marek Kolodziej <mkolod.dev@gmail.com>,"I'm actually somewhat involved with the Google Docs you linked to.

I don't think Oracle will remove Unsafe in JVM 9. As you said, JEP 260
already proposes making Unsafe available. Given the widespread use of
Unsafe for performance and advanced functionalities, I don't think Oracle
can just remove it in one release. If they do, there will be strong
backlash and the act would significantly undermine the credibility of the
JVM as a long-term platform.

Note that for Spark itself, we move pretty fast and can replace all the use
of Unsafe with a newer alternative in one release if absolutely necessary
(the actual coding takes only a day or two).




"
Marek Kolodziej <mkolod.dev@gmail.com>,"Fri, 21 Aug 2015 11:39:18 -0700",Re: Tungsten and sun.misc.Unsafe,Reynold Xin <rxin@databricks.com>,"Thanks Reynold, that helps a lot. I'm glad you're involved with that Google
Doc community effort. I think it's because of that doc that the JEP's
wording and scope changed for the better since it originally got
introduced.

Marek


"
Reynold Xin <rxin@databricks.com>,"Fri, 21 Aug 2015 12:14:08 -0700",Re: DataFrame. SparkPlan / Project serialization issue: ArrayIndexOutOfBounds.,Eugene Morozov <evgeny.a.morozov@gmail.com>,"You've probably hit this bug:
https://issues.apache.org/jira/browse/SPARK-7180

It's fixed in Spark 1.4.1+. Try setting spark.serializer.extraDebugInfo to
false and see if it goes away.



"
mkhaitman <mark.khaitman@chango.com>,"Fri, 21 Aug 2015 14:17:33 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),dev@spark.apache.org,"Just a heads up that this RC1 release is still appearing as ""1.5.0-SNAPSHOT""
(Not just me right..?)



--

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Fri, 21 Aug 2015 14:19:47 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),mkhaitman <mark.khaitman@chango.com>,"I pointed hbase-spark module (in HBase project) to 1.5.0-rc1 and was able
to build the module (with proper maven repo).

FYI


"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 21 Aug 2015 14:20:00 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),mkhaitman <mark.khaitman@chango.com>,"The pom files look correct, but this file is not:
https://github.com/apache/spark/blob/4c56ad772637615cc1f4f88d619fac6c372c8552/core/src/main/scala/org/apache/spark/package.scala

So, I guess, -1?





-- 
Marcelo

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Fri, 21 Aug 2015 14:59:04 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Marcelo Vanzin <vanzin@cloudera.com>,"Problem noted. Apparently the release script doesn't automate the
replacement of all version strings yet. I'm going to publish a new RC over
the weekend with the release version properly assigned.

Please continue the testing and report any problems you find. Thanks!



"
Luciano Resende <luckbr1975@gmail.com>,"Sun, 23 Aug 2015 17:27:33 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Sean Owen <sowen@cloudera.com>,"

Hi Sean,

Were you able to resolve this ? I am trying it on Linux (Ubuntu 14.04.3
LTS) and when building with a clean maven repo I am getting issues where it
can't find lib_managed/jar when trying to build the Launcher. But looks
like you went a bit further on this ?

Running org.apache.spark.launcher.SparkSubmitCommandBuilderSuite
Tests run: 7, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 0.018 sec
<<< FAILURE! - in org.apache.spark.launcher.SparkSubmitCommandBuilderSuite
testDriverCmdBuilder(org.apache.spark.launcher.SparkSubmitCommandBuilderSuite)
Time elapsed: 0.005 sec  <<< ERROR!
java.lang.IllegalStateException: Library directory
'/home/lresende/dev/spark/source/releases/spark-1.5.0/lib_managed/jars'
does not exist.
    at
org.apache.spark.launcher.CommandBuilderUtils.checkState(CommandBuilderUtils.java:249)
    at
org.apache.spark.launcher.AbstractCommandBuilder.buildClassPath(AbstractCommandBuilder.java:218)
    at
org.apache.spark.launcher.AbstractCommandBuilder.buildJavaCommand(AbstractCommandBuilder.java:115)
    at
org.apache.spark.launcher.SparkSubmitCommandBuilder.buildSparkSubmitCommand(SparkSubmitCommandBuilder.java:196)
    at
org.apache.spark.launcher.SparkSubmitCommandBuilder.buildCommand(SparkSubmitCommandBuilder.java:121)
    at
org.apache.spark.launcher.SparkSubmitCommandBuilderSuite.testCmdBuilder(SparkSubmitCommandBuilderSuite.java:174)
    at
org.apache.spark.launcher.SparkSubmitCommandBuilderSuite.testDriverCmdBuilder(SparkSubmitCommandBuilderSuite.java:51)


-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Sean Owen <sowen@cloudera.com>,"Mon, 24 Aug 2015 05:51:44 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Luciano Resende <luckbr1975@gmail.com>,"I think this is a symptom of not running ""mvn ... package"" and then
""mvn ... test""?


---------------------------------------------------------------------


"
Sean Owen <sowen@cloudera.com>,"Mon, 24 Aug 2015 15:23:26 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Reynold Xin <rxin@databricks.com>,"PS Shixiong Zhu is correct that this one has to be fixed:
https://issues.apache.org/jira/browse/SPARK-10168

For example you can see assemblies like this are nearly empty:
https://repository.apache.org/content/repositories/orgapachespark-1137/org/apache/spark/spark-streaming-flume-assembly_2.10/1.5.0-rc1/

Just a publishing glitch but worth a few more eyes on.


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Mon, 24 Aug 2015 11:58:41 -0700",Fwd: [jira] [Commented] (INFRA-10191) git pushing for Spark fails,"""dev@spark.apache.org"" <dev@spark.apache.org>","FYI


---------- Forwarded message ----------
From: Geoffrey Corey (JIRA) <jira@apache.org>
Date: Mon, Aug 24, 2015 at 11:54 AM
Subject: [jira] [Commented] (INFRA-10191) git pushing for Spark fails
To: rxin@apache.org



    [
https://issues.apache.org/jira/browse/INFRA-10191?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=14709850#comment-14709850
]

Geoffrey Corey commented on INFRA-10191:
----------------------------------------

We are having some permissions issues with the ldap server that git-wip-us
uses to authenticate. WIll update this ticket once the service is back up.

the following errors:
u'PR_TOOL_MERGE_PR_8373_MASTER:master']' returned non-zero exit status 128
while try to import pandas
https://git-wip-us.apache.org/repos/asf/spark.git/': The requested URL
returned error: 500



--
This message was sent by Atlassian JIRA
(v6.3.4#6332)
"
Reynold Xin <rxin@databricks.com>,"Mon, 24 Aug 2015 13:00:23 -0700",Re: [jira] [Commented] (INFRA-10191) git pushing for Spark fails,"""dev@spark.apache.org"" <dev@spark.apache.org>","This has been resolved.



"
Reynold Xin <rxin@databricks.com>,"Mon, 24 Aug 2015 14:07:21 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Sandy Ryza <sandy.ryza@cloudera.com>,"Nope --- I cut that last Friday but had an error. I will remove it and cut
a new one.



"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 24 Aug 2015 14:08:41 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Reynold Xin <rxin@databricks.com>,"Cool, thanks!


"
Sandy Ryza <sandy.ryza@cloudera.com>,"Mon, 24 Aug 2015 14:06:19 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Sean Owen <sowen@cloudera.com>,"I see that there's an 1.5.0-rc2 tag in github now.  Is that the official
RC2 tag to start trying out?

-Sandy


"
"""dan@lumity.com"" <dan@lumity.com>","Mon, 24 Aug 2015 18:59:02 -0700 (MST)","ExternalSorter: Thread *** spilling in-memory map of 352.6 MB to
 disk (38 times so far)",dev@spark.apache.org,"Hi all,

I posted this on the user list but didn't get any responses to I was hoping
one of you awesome devs could shed some light:


I'm trying to run a spark 1.5 job with: 

 ./spark-shell --driver-java-options ""-Xdebug
-Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1044 -Xms16g
-Xmx48g -Xss128m""   

I get lots of error messages like : 

15/08/24 20:24:33 INFO ExternalSorter: Thread 172 spilling in-memory map of
352.2 MB to disk (40 times so far) 
15/08/24 20:24:33 INFO ExternalSorter: Thread 179 spilling in-memory map of
352.2 MB to disk (39 times so far) 
15/08/24 20:24:34 INFO ExternalSorter: Thread 197 spilling in-memory map of
352.2 MB to disk (39 times so far) 
15/08/24 20:24:34 INFO ExternalSorter: Thread 192 spilling in-memory map of
352.2 MB to disk (39 times so far) 
15/08/24 20:24:36 INFO ShuffleMemoryManager: TID 798 waiting for at least
1/2N of shuffle memory pool to be free 
15/08/24 20:24:36 INFO ExternalSorter: Thread 170 spilling in-memory map of
352.2 MB to disk (39 times so far) 
15/08/24 20:24:36 INFO ExternalSorter: Thread 171 spilling in-memory map of
352.2 MB to disk (40 times so far) 

When I force a stack trace with jstack, I get stack traces like this for
each of my 36 cores: 

""Executor task launch worker-44"" daemon prio=10 tid=0x00007fb9c404d000
nid=0x417f runnable [0x00007fb5e3ffe000] 
   java.lang.Thread.State: RUNNABLE 
        at
scala.collection.mutable.ResizableArray$class.$init$(ResizableArray.scala:32) 
        at scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:49) 
        at scala.collection.mutable.ArrayBuffer.<init>(ArrayBuffer.scala:63) 
        at
org.apache.spark.util.SizeEstimator$SearchState.<init>(SizeEstimator.scala:154) 
        at
org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(SizeEstimator.scala:183) 
        at
org.apache.spark.util.SizeEstimator$$anonfun$sampleArray$1.apply$mcVI$sp(SizeEstimator.scala:262) 
        at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:166) 
        at
org.apache.spark.util.SizeEstimator$.sampleArray(SizeEstimator.scala:254) 
        at
org.apache.spark.util.SizeEstimator$.visitArray(SizeEstimator.scala:238) 
        at
org.apache.spark.util.SizeEstimator$.visitSingleObject(SizeEstimator.scala:194) 
        at
org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(SizeEstimator.scala:186) 
        at
org.apache.spark.util.SizeEstimator$.estimate(SizeEstimator.scala:54) 
        at
org.apache.spark.util.collection.SizeTracker$class.takeSample(SizeTracker.scala:78) 
        at
org.apache.spark.util.collection.SizeTracker$class.afterUpdate(SizeTracker.scala:70) 
        at
org.apache.spark.util.collection.PartitionedPairBuffer.afterUpdate(PartitionedPairBuffer.scala:30) 
        at
org.apache.spark.util.collection.PartitionedPairBuffer.insert(PartitionedPairBuffer.scala:53) 
        at
org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:214) 
        at
org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73) 
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73) 
        at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41) 
        at org.apache.spark.scheduler.Task.run(Task.scala:88) 
        at
org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214) 
        at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) 
        at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) 
        at java.lang.Thread.run(Thread.java:745) 



Jmap Shows: 

root@ip-172-31-28-227:~# jmap -heap 16421 
Attaching to process ID 16421, please wait... 
Debugger attached successfully. 
Server compiler detected. 
JVM version is 24.79-b02 

using thread-local object allocation. 
Parallel GC with 25 thread(s) 

Heap Configuration: 
   MinHeapFreeRatio = 0 
   MaxHeapFreeRatio = 100 
   MaxHeapSize      = 51539607552 (49152.0MB) 
   NewSize          = 1310720 (1.25MB) 
   MaxNewSize       = 17592186044415 MB 
   OldSize          = 5439488 (5.1875MB) 
   NewRatio         = 2 
   SurvivorRatio    = 8 
   PermSize         = 21757952 (20.75MB) 
   MaxPermSize      = 268435456 (256.0MB) 
   G1HeapRegionSize = 0 (0.0MB) 

Heap Usage: 
PS Young Generation 
Eden Space: 
   capacity = 15290335232 (14582.0MB) 
   used     = 11958528232 (11404.541236877441MB) 
   free     = 3331807000 (3177.4587631225586MB) 
   78.20971908433302% used 
   capacity = 869269504 (829.0MB) 
   used     = 868910560 (828.6576843261719MB) 
   free     = 358944 (0.342315673828125MB) 
   99.9587073976082% used 
To Space: 
   capacity = 965738496 (921.0MB) 
   used     = 0 (0.0MB) 
   free     = 965738496 (921.0MB) 
   0.0% used 
PS Old Generation 
   capacity = 11453595648 (10923.0MB) 
   used     = 1423152248 (1357.223747253418MB) 
   free     = 10030443400 (9565.776252746582MB) 
   12.425375329611077% used 
PS Perm Generation 
   capacity = 107479040 (102.5MB) 
   used     = 107107360 (102.14553833007812MB) 
   free     = 371680 (0.354461669921875MB) 
   99.65418373666158% used 

27990 interned Strings occupying 2892496 bytes. 


This seems to indicate that the Threads are dumping extra data into temp
files on the disk, but none of the temp files in /tmp/ are growing as far as
I can tell.  I think this means the spilling isn't actually happening, or I
don't know where to look for it if it is.   
I can't tell what's going on here, does anyone have any suggestions on how
to work through this? 

As an update, when I reran the job with spark.shuffle.spill = false, the job
moves along very very slowly and I get stack traces like:

	
Thread 20826: (state = IN_VM)
 - java.util.IdentityHashMap.resize(int) @bci=54, line=469 (Compiled frame)
 - java.util.IdentityHashMap.put(java.lang.Object, java.lang.Object)
@bci=118, line=445 (Compiled frame)
 - org.apache.spark.util.SizeEstimator$SearchState.enqueue(java.lang.Object)
@bci=21, line=159 (Compiled frame)
 -
org.apache.spark.util.SizeEstimator$$anonfun$visitSingleObject$1.apply(java.lang.reflect.Field)
@bci=12, line=203 (Compiled frame)
 -
org.apache.spark.util.SizeEstimator$$anonfun$visitSingleObject$1.apply(java.lang.Object)
@bci=5, line=202 (Compiled frame)
 - scala.collection.immutable.List.foreach(scala.Function1) @bci=15,
line=381 (Compiled frame)
 - org.apache.spark.util.SizeEstimator$.visitSingleObject(java.lang.Obje
ct, org.apache.spark.util.SizeEstimator$SearchState) @bci=75, li
ne=202 (Compiled frame)
 -
org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(java.lang.Object,
java.util.IdentityHashMap) @bci=
32, line=186 (Compiled frame)
 -
org.apache.spark.util.SizeEstimator$$anonfun$sampleArray$1.apply$mcVI$sp(int)
@bci=71, line=262 (Compiled frame)
 - scala.collection.immutable.Range.foreach$mVc$sp(scala.Function1) @bci=87,
line=166 (Compiled frame)
 - org.apache.spark.util.SizeEstimator$.sampleArray(java.lang.Object,
org.apache.spark.util.SizeEstimator$SearchState, java.util.Random,
 org.apache.spark.util.collection.OpenHashSet, int) @bci=39, line=254
(Compiled frame)
 - org.apache.spark.util.SizeEstimator$.visitArray(java.lang.Object,
java.lang.Class, org.apache.spark.util.SizeEstimator$SearchState) @
bci=185, line=238 (Compiled frame)
 - org.apache.spark.util.SizeEstimator$.visitSingleObject(java.lang.Object,
org.apache.spark.util.SizeEstimator$SearchState) @bci=16, li
ne=194 (Compiled frame)
 -
org.apache.spark.util.SizeEstimator$.org$apache$spark$util$SizeEstimator$$estimate(java.lang.Object,
java.util.IdentityHashMap) @bci=
32, line=186 (Compiled frame)
 - org.apache.spark.util.SizeEstimator$.estimate(java.lang.Object) @bci=9,
line=54 (Compiled frame)
 -
org.apache.spark.util.collection.SizeTracker$class.takeSample(org.apache.spark.util.collection.SizeTracker)
@bci=23, line=78 (Compile
d frame)
 -
org.apache.spark.util.collection.SizeTracker$class.afterUpdate(org.apache.spark.util.collection.SizeTracker)
@bci=31, line=70 (Compil
ed frame)
 - org.apache.spark.util.collection.PartitionedPairBuffer.afterUpdate()
@bci=1, line=30 (Compiled frame)
 - org.apache.spark.util.collection.PartitionedPairBuffer.insert(int,
java.lang.Object, java.lang.Object) @bci=63, line=53 (Compiled fra
me)
 -
org.apache.spark.util.collection.ExternalSorter.insertAll(scala.collection.Iterator)
@bci=200, line=214 (Compiled frame)
 -
org.apache.spark.shuffle.sort.SortShuffleWriter.write(scala.collection.Iterator)
@bci=203, line=73 (Interpreted frame)
 -
org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
@bci=186, line=73 (Interpreted frame)
 -
org.apache.spark.scheduler.ShuffleMapTask.runTask(org.apache.spark.TaskContext)
@bci=2, line=41 (Interpreted frame)
 - org.apache.spark.scheduler.Task.run(long, int,
org.apache.spark.metrics.MetricsSystem) @bci=118, line=88 (Interpreted
frame)
 - org.apache.spark.executor.Executor$TaskRunner.run() @bci=334, line=214
(Interpreted frame)
 -
java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker)
@bci=95, line=1145 (Interpreted fra
me)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=615
(Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)

Would appreciate any advice or suggestions!

Best, 
Dan 




--

---------------------------------------------------------------------


"
<andrew.rowson@thomsonreuters.com>,"Tue, 25 Aug 2015 09:17:57 +0000",Spark builds: allow user override of project version at buildtime,<dev@spark.apache.org>,"I've got an interesting challenge in building Spark. For various reasons we
do a few different builds of spark, typically with a few different profile
options (e.g. against different versions of Hadoop, some with/without Hive
etc.). We mirror the spark repo internally and have a buildserver that
builds and publishes different Spark versions to an artifactory server. The
problem is that the output of each build is published with the version that
is in the pom.xml file - a build of Spark @tags/v1.4.1 always comes out with
an artefact version of '1.4.1'. However, because we may have three different
Spark builds for 1.4.1, it'd be useful to be able to override this version
at build time, so that we can publish 1.4.1, 1.4.1-cdh5.3.3 and maybe
1.4.1-cdh5.3.3-hive as separate artifacts. 

My understanding of maven is that the /project/version value in the pom.xml
isn't overridable. At the moment, I've hacked around this by having a
pre-build task that rewrites the various pom files and adjust the version to
a string that's correct for that particular build. 

Would it be useful to instead populate the version from a maven property,
which could then be overridable on the CLI? Something like:

<project>
    <version>${spark.version}</version>
    <properties>
        <spark.version>1.4.1</version>
    </properties>
</project>

Then, if I wanted to do a build against a specific profile, I could also
pass in a -Dspark.version=1.4.1-custom-string and have the output artifacts
correctly named. The default behaviour should be the same. Child pom files
would need to reference ${spark.version} in their parent section I think.

Any objections to this?

Andrew
"
Akhil Das <akhil@sigmoidanalytics.com>,"Tue, 25 Aug 2015 15:01:45 +0530","Re: Introduce a sbt plugin to deploy and submit jobs to a spark
 cluster on ec2",pishen tsai <pishen02@gmail.com>,"You can add it to the spark packages i guess http://spark-packages.org/

Thanks
Best Regards


"
pishen <pishen02@gmail.com>,"Tue, 25 Aug 2015 09:19:59 -0700 (MST)","Re: Introduce a sbt plugin to deploy and submit jobs to a spark
 cluster on ec2",dev@spark.apache.org,"Thank you for the suggestions, actually this project is already on
spark-packages for 1~2 months.
Then I think what I need is some promotions :P

2015-08-25 23:51 GMT+08:00 saurfang [via Apache Spark Developers List] <
ml-node+s1001551n13809h6@n3.nabble.com>:





--"
"""Varadhan, Jawahar"" <varadhan@yahoo.com.INVALID>","Tue, 25 Aug 2015 16:37:35 +0000 (UTC)","Spark (1.2.0) submit fails with exception saying log directory
 already exists","""\""dev@spark.apache.org\"""" <dev@spark.apache.org>, 
	User <user@spark.apache.org>","Here is the error
yarn.ApplicationMaster: Final app status: FAILED, exitCode: 15, (reason: User class threw exception: Log directory hdfs://Sandbox/user/spark/applicationHistory/application_1438113296105_0302 already exists!)
I am using cloudera 5.3.2 with Spark 1.2.0
Any help is appreciated.
ThanksJay

"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 25 Aug 2015 10:07:09 -0700","Re: Spark (1.2.0) submit fails with exception saying log directory
 already exists","""Varadhan, Jawahar"" <varadhan@yahoo.com>","This probably means your app is failing and the second attempt is
hitting that issue. You may fix the ""directory already exists"" error
by setting
spark.eventLog.overwrite=true in your conf, but most probably that
will just expose the actual error in your app.




-- 
Marcelo

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 25 Aug 2015 10:09:03 -0700",Re: Spark builds: allow user override of project version at buildtime,andrew.rowson@thomsonreuters.com,"
Have you tried it? My understanding is that no project does that
because it doesn't work. To resolve properties you need to read the
parent pom(s), and if there's a variable reference there, well, you
can't do it. Chicken & egg.

-- 
Marcelo

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Tue, 25 Aug 2015 17:31:04 +0000",Re: [survey] [spark-ec2] What do you like/dislike about spark-ec2?,"Spark dev list <dev@spark.apache.org>, user <user@spark.apache.org>","Final chance to fill out the survey!

http://goo.gl/forms/erct2s6KRR

I'm gonna close it to new responses tonight and send out a summary of the
results.

Nick

m>

s link to
"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 25 Aug 2015 18:52:55 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),"Sandy Ryza <sandy.ryza@cloudera.com>, Sean Owen <sowen@cloudera.com>","Is there a jira to update the sql hive docs?Spark SQL and DataFrames - Spark 1.5.0 Documentation

| ¬† |
| ¬† | ¬† | ¬† | ¬† | ¬† |
| Spark SQL and DataFrames - Spark 1.5.0 DocumentationSpark SQL and DataFrame Guide Overview DataFrames Starting Point: SQLContext Creating DataFrames DataFrame Operations Running SQL Queries Programmatically Interoperating with RDDs  |
|  |
| View on people.apache.org | Preview by Yahoo |
|  |
| ¬† |


it still says default is 0.13.1 but pom file builds with hive 1.2.1-spark.
Tom 


   

 I see that there's an 1.5.0-rc2 tag in github now.¬† Is that the official RC2 tag to start trying out?
-Sandy

PS Shixiong Zhu is correct that this one has to be fixed:
https://issues.apache.org/jira/browse/SPARK-10168

For example you can see assemblies like this are nearly empty:
https://repository.apache.org/content/repositories/orgapachespark-1137/org/apache/spark/spark-streaming-flume-assembly_2.10/1.5.0-rc1/

Just a publishing glitch but worth a few more eyes on.

 before
utorsUp(JobProgressListener.scala:561)
adcast(BroadcastSuite.scala:313)
$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:287)
y$mcV$sp(BroadcastSuite.scala:165)
y(BroadcastSuite.scala:165)
y(BroadcastSuite.scala:165)
ansformer.scala:22)
)
ted 191
.contains(elem))
c8552
===============
===============
========================
========================
sent
k
=======================================
=======================================
t
==========================
==========================
s
vity
ata
t
t
l

---------------------------------------------------------------------





  "
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 25 Aug 2015 13:33:41 -0700",Paring down / tagging tests (or some other way to avoid timeouts)?,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hello y'all,

So I've been getting kinda annoyed with how many PR tests have been
timing out. I took one of the logs from one of my PRs and started to
do some crunching on the data from the output, and here's a list of
the 5 slowest suites:

307.14s HiveSparkSubmitSuite
382.641s VersionsSuite
398s CliSuite
410.52s HashJoinCompatibilitySuite
2508.61s HiveCompatibilitySuite

Looking at those, I'm not surprised at all that we see so many
timeouts. Is there any ongoing effort to trim down those tests
(especially HiveCompatibilitySuite) or somehow restrict when they're
run?

Almost 1 hour to run a single test suite that affects a rather
isolated part of the code base looks a little excessive to me.

-- 
Marcelo

---------------------------------------------------------------------


"
Patrick Wendell <pwendell@gmail.com>,"Tue, 25 Aug 2015 13:40:21 -0700",Re: Paring down / tagging tests (or some other way to avoid timeouts)?,Marcelo Vanzin <vanzin@cloudera.com>,"There is already code in place that restricts which tests run
depending on which code is modified. However, changes inside of
Spark's core currently require running all dependent tests. If you
have some ideas about how to improve that heuristic, it would be
great.

- Patrick


---------------------------------------------------------------------


"
Michael Armbrust <michael@databricks.com>,"Tue, 25 Aug 2015 13:48:57 -0700",Re: Paring down / tagging tests (or some other way to avoid timeouts)?,Patrick Wendell <pwendell@gmail.com>,"I'd be okay skipping the HiveCompatibilitySuite for core-only changes.
They do often catch bugs in changes to catalyst or sql though.  Same for
HashJoinCompatibilitySuite/VersionsSuite.

HiveSparkSubmitSuite/CliSuite should probably stay, as they do test things
like addJar that have been broken by core in the past.


"
Tom Graves <tgraves_cs@yahoo.com.INVALID>,"Tue, 25 Aug 2015 20:56:33 +0000 (UTC)",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),"Tom Graves <tgraves_cs@yahoo.com>, Sandy Ryza <sandy.ryza@cloudera.com>, 
	Sean Owen <sowen@cloudera.com>","Anyone using HiveContext with secure Hive with Spark 1.5 and have it working?
We have a non standard version of hive but was pulling our hive jars and its failing to authenticate. ¬†It could be something in our hive version but wondering if spark isn't forwarding credentials properly.
Tom 


   

 Is there a jira to update the sql hive docs?Spark SQL and DataFrames - Spark 1.5.0 Documentation

| ¬† |
| ¬† | ¬† | ¬† | ¬† | ¬† |
| Spark SQL and DataFrames - Spark 1.5.0 DocumentationSpark SQL and DataFrame Guide Overview DataFrames Starting Point: SQLContext Creating DataFrames DataFrame Operations Running SQL Queries Programmatically Interoperating with RDDs  |
|  |
| View on people.apache.org | Preview by Yahoo |
|  |
| ¬† |


it still says default is 0.13.1 but pom file builds with hive 1.2.1-spark.
Tom 


   

 I see that there's an 1.5.0-rc2 tag in github now.¬† Is that the official RC2 tag to start trying out?
-Sandy

PS Shixiong Zhu is correct that this one has to be fixed:
https://issues.apache.org/jira/browse/SPARK-10168

For example you can see assemblies like this are nearly empty:
https://repository.apache.org/content/repositories/orgapachespark-1137/org/apache/spark/spark-streaming-flume-assembly_2.10/1.5.0-rc1/

Just a publishing glitch but worth a few more eyes on.

 before
utorsUp(JobProgressListener.scala:561)
adcast(BroadcastSuite.scala:313)
$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:287)
y$mcV$sp(BroadcastSuite.scala:165)
y(BroadcastSuite.scala:165)
y(BroadcastSuite.scala:165)
ansformer.scala:22)
)
ted 191
.contains(elem))
c8552
===============
===============
========================
========================
sent
k
=======================================
=======================================
t
==========================
==========================
s
vity
ata
t
t
l

---------------------------------------------------------------------





   

  "
Marcelo Vanzin <vanzin@cloudera.com>,"Tue, 25 Aug 2015 14:13:26 -0700",Re: Paring down / tagging tests (or some other way to avoid timeouts)?,Michael Armbrust <michael@databricks.com>,"I chatted with Patrick briefly offline. It would be interesting to
know whether the scripts have some way of saying ""run a smaller
version of certain tests"" (e.g. by setting a system property that the
tests look at to decide what to run). That way, if there are no
changes under sql/, we could still run a small part of
HiveCompatibilitySuite, just not all of it. The reasoning being that
if a core change breaks something in Hive, it will probably break many
tests, not a specific one.




-- 
Marcelo

---------------------------------------------------------------------


"
Doug Balog <doug.sparkdev@dugos.com>,"Tue, 25 Aug 2015 17:31:33 -0400",Re: [VOTE] Release Apache Spark 1.5.0 (RC1),Tom Graves <tgraves_cs@yahoo.com>,"It works for me in cluster mode. 
I‚Äôm running on Hortonworks 2.2.4.12 in secure mode with Hive 0.14
I built with

./make-distribution ‚Äîtgz -Phive -Phive-thriftserver -Phbase-provided -Pyarn -Phadoop-2.6 

Doug



working?
and its failing to authenticate.  It could be something in our hive version but wondering if spark isn't forwarding credentials properly.
SQLContext Creating DataFrames DataFrame Operations Running SQL Queries Programmatically Interoperating with RDDs
1.2.1-spark.
official RC2 tag to start trying out?
https://repository.apache.org/content/repositories/orgapachespark-1137/org/apache/spark/spark-streaming-flume-assembly_2.10/1.5.0-rc1/
they
lot
distributed
before
org.apache.spark.ui.jobs.JobProgressListener.waitUntilExecutorsUp(JobProgressListener.scala:561)
org.apache.spark.broadcast.BroadcastSuite.testUnpersistBroadcast(BroadcastSuite.scala:313)
org.apache.spark.broadcast.BroadcastSuite.org$apache$spark$broadcast$BroadcastSuite$$testUnpersistTorrentBroadcast(BroadcastSuite.scala:287)
org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply$mcV$sp(BroadcastSuite.scala:165)
org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165)
org.apache.spark.broadcast.BroadcastSuite$$anonfun$16.apply(BroadcastSuite.scala:165)
org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
191
DirectKafkaStreamSuite.collectedData.contains(elem))
version
if a
http://spark.apache.org/
https://github.com/apache/spark/tree/4c56ad772637615cc1f4f88d619fac6c372c8552
at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc1-bin/
https://repository.apache.org/content/repositories/orgapachespark-1137/
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc1-docs/
================
================
taking an
=========================
=========================
votes
already present
block
========================================

========================================
into
the
target
===========================
===========================
the
plans
locations)
connectivity
metadata
Parquet
short
Kinesis,
linear
details UI
transformation,
VectorSlicer.
isotonic
sequential
Kolmogorov-Smirnov
models,


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 25 Aug 2015 14:44:58 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","m

t
at
nabled
I think a large part of that is coming from the pressure created by JVM GC.
Putting more data in-memory makes GC worse, unless GC is well tuned.
"
"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","Wed, 26 Aug 2015 00:19:15 +0000",Re: Dataframe aggregation with Tungsten unsafe,Reynold Xin <rxin@databricks.com>,"Thank you for the explanation. The size if the 100M data is ~1.4GB in memory and each worker has 32GB of memory. It seems to be a lot of free memory available. I wonder how Spark can hit GC with such setup?

Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>>



It seems that there is a nice improvement with Tungsten enabled given that data is persisted in memory 2x and 3x. However, the improvement is not that nice for parquet, it is 1.5x. Whatís interesting, with Tungsten enabled performance of in-memory data and parquet data aggregation is similar. Could anyone comment on this? It seems counterintuitive to me.

Local performance was not as good as Reynold had. I have around 1.5x, he had 5x. However, local mode is not interesting.


I think a large part of that is coming from the pressure created by JVM GC. Putting more data in-memory makes GC worse, unless GC is well tuned.




---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Tue, 25 Aug 2015 18:04:57 -0700",Re: Dataframe aggregation with Tungsten unsafe,"""Ulanov, Alexander"" <alexander.ulanov@hp.com>","There are a lot of GC activity due to the non-code-gen path being sloppy
about garbage creation. This is not actually what happens, but just as an
example:

rdd.map { i: Int => i + 1 }

This under the hood becomes a closure that boxes on every input and every
output, creating two extra objects.

The reality is more complicated than this -- but here's a simpler view of
what happens with GC in these cases. You might've heard from other places
that the JVM is very efficient about transient object allocations. That is
true when you look at these allocations in isolation, but unfortunately not
true when you look at them in aggregate.

First, due to the way the iterator interface is constructed, it is hard for
the JIT compiler to on-stack allocate these objects. Then two things happen:

1. They pile up and cause more young gen GCs to happen.
2. After a few young gen GCs, some mid-tenured objects (e.g. an aggregation
map) get copied into the old-gen, and eventually requires a full GC to free
them. Full GCs are much more expensive than young gen GCs (usually involves
copying all the data in the old gen).

So the more garbages that are created -> the more frequently full GC
happens.

The more long lived objects in the old gen (e.g. cache) -> the more
expensive full GC is.




t
at
nabled
"
Michael Armbrust <michael@databricks.com>,"Tue, 25 Aug 2015 19:11:42 -0700",Re: Spark builds: allow user override of project version at buildtime,Marcelo Vanzin <vanzin@cloudera.com>,"This isn't really answering the question, but for what it is worth, I
manage several different branches of Spark and publish custom named
versions regularly to an internal repository, and this is *much* easier
with SBT than with maven.  You can actually link the Spark SBT build into
an external SBT build and write commands that cross publish as needed.

For your case something as simple as build/sbt ""set version in Global :=
'1.4.1-custom-string'"" publish might do the trick.


"
Reynold Xin <rxin@databricks.com>,"Tue, 25 Aug 2015 21:28:14 -0700",[VOTE] Release Apache Spark 1.5.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","Please vote on releasing the following candidate as Apache Spark version
1.5.0. The vote is open until Friday, Aug 29, 2015 at 5:00 UTC and passes
if a majority of at least 3 +1 PMC votes are cast.

[ ] +1 Release this package as Apache Spark 1.5.0
[ ] -1 Do not release this package because ...

To learn more about Apache Spark, please see http://spark.apache.org/


The tag to be voted on is v1.5.0-rc2:
https://github.com/apache/spark/tree/727771352855dbb780008c449a877f5aaa5fc27a

The release files, including signatures, digests, etc. can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc2-bin/

Release artifacts are signed with the following key:
https://people.apache.org/keys/committer/pwendell.asc

The staging repository for this release (published as 1.5.0-rc2) can be
found at:
https://repository.apache.org/content/repositories/orgapachespark-1141/

The staging repository for this release (published as 1.5.0) can be found
at:
https://repository.apache.org/content/repositories/orgapachespark-1140/

The documentation corresponding to this release can be found at:
http://people.apache.org/~pwendell/spark-releases/spark-1.5.0-rc2-docs/


=======================================
How can I help test this release?
=======================================
If you are a Spark user, you can help us test this release by taking an
existing Spark workload and running on this release candidate, then
reporting any regressions.


================================================
What justifies a -1 vote for this release?
================================================
This vote is happening towards the end of the 1.5 QA period, so -1 votes
should only occur for significant regressions from 1.4. Bugs already
present in 1.4, minor regressions, or bugs related to new features will not
block this release.


===============================================================
What should happen to JIRA tickets still targeting 1.5.0?
===============================================================
1. It is OK for documentation patches to target 1.5.0 and still go into
branch-1.5, since documentations will be packaged separately from the
release.
2. New features for non-alpha-modules should target 1.6+.
3. Non-blocker bug fixes should target 1.5.1 or 1.6.0, or drop the target
version.


==================================================
Major changes to help you focus your testing
==================================================

As of today, Spark 1.5 contains more than 1000 commits from 220+
contributors. I've curated a list of important changes for 1.5. For the
complete list, please refer to Apache JIRA changelog.

RDD/DataFrame/SQL APIs

- New UDAF interface
- DataFrame hints for broadcast join
- expr function for turning a SQL expression into DataFrame column
- Improved support for NaN values
- StructType now supports ordering
- TimestampType precision is reduced to 1us
- 100 new built-in expressions, including date/time, string, math
- memory and local disk only checkpointing

DataFrame/SQL Backend Execution

- Code generation on by default
- Improved join, aggregation, shuffle, sorting with cache friendly
algorithms and external algorithms
- Improved window function performance
- Better metrics instrumentation and reporting for DF/SQL execution plans

Data Sources, Hive, Hadoop, Mesos and Cluster Management

- Dynamic allocation support in all resource managers (Mesos, YARN,
Standalone)
- Improved Mesos support (framework authentication, roles, dynamic
allocation, constraints)
- Improved YARN support (dynamic allocation with preferred locations)
- Improved Hive support (metastore partition pruning, metastore
connectivity to 0.13 to 1.2, internal Hive upgrade to 1.2)
- Support persisting data in Hive compatible format in metastore
- Support data partitioning for JSON data sources
- Parquet improvements (upgrade to 1.7, predicate pushdown, faster metadata
discovery and schema merging, support reading non-standard legacy Parquet
files generated by other libraries)
- Faster and more robust dynamic partition insert
- DataSourceRegister interface for external data sources to specify short
names

SparkR

- YARN cluster mode in R
- GLMs with R formula, binomial/Gaussian families, and elastic-net
regularization
- Improved error messages
- Aliases to make DataFrame functions more R-like

Streaming

- Backpressure for handling bursty input streams.
- Improved Python support for streaming sources (Kafka offsets, Kinesis,
MQTT, Flume)
- Improved Python streaming machine learning algorithms (K-Means, linear
regression, logistic regression)
- Native reliable Kinesis stream support
- Input metadata like Kafka offsets made visible in the batch details UI
- Better load balancing and scheduling of receivers across cluster
- Include streaming storage in web UI

Machine Learning and Advanced Analytics

- Feature transformers: CountVectorizer, Discrete Cosine transformation,
MinMaxScaler, NGram, PCA, RFormula, StopWordsRemover, and VectorSlicer.
- Estimators under pipeline APIs: naive Bayes, k-means, and isotonic
regression.
- Algorithms: multilayer perceptron classifier, PrefixSpan for sequential
pattern mining, association rule generation, 1-sample Kolmogorov-Smirnov
test.
- Improvements to existing algorithms: LDA, trees/ensembles, GMMs
- More efficient Pregel API implementation for GraphX
- Model summary for linear and logistic regression.
- Python API: distributed matrices, streaming k-means and linear models,
LDA, power iteration clustering, etc.
- Tuning and evaluation: train-validation split and multiclass
classification evaluator.
- Documentation: document the release version of public API methods
"
"""Wang, Yanping"" <yanping.wang@intel.com>","Wed, 26 Aug 2015 05:00:44 +0000",RE: Dataframe aggregation with Tungsten unsafe,"'Reynold Xin' <rxin@databricks.com>, ""Ulanov, Alexander""
	<alexander.ulanov@hp.com>","Hi, Reynold and others

I agree with your comments on mid-tenured objects and GC. In fact, dealing with mid-tenured objects are the major challenge for all java GC implementations.

I am wondering if anyone has played -XX:+PrintTenuringDistribution flags and see how exactly ages distribution look like when your program runs?
My output with -XX:+PrintGCDetails look like below: (Oracle jdk8 update 60 http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html

Age 1-5 are young guys, 13, 14, 15 are old guys.
The middle guys will have to be copied multiple times before become dead in old regions normally need some major GC to clean them up.

Desired survivor size 2583691264 bytes, new threshold 15 (max 15)
- age   1:   13474960 bytes,   13474960 total
- age   2:    2815592 bytes,   16290552 total
- age   3:     632784 bytes,   16923336 total
- age   4:     428432 bytes,   17351768 total
- age   5:     648696 bytes,   18000464 total
- age   6:     572328 bytes,   18572792 total
- age   7:     549216 bytes,   19122008 total
- age   8:     539544 bytes,   19661552 total
- age   9:     422256 bytes,   20083808 total
- age  10:     552928 bytes,   20636736 total
- age  11:     430464 bytes,   21067200 total
- age  12:     753320 bytes,   21820520 total
- age  13:     230864 bytes,   22051384 total
- age  14:     276288 bytes,   22327672 total
- age  15:     809272 bytes,   23136944 total

I‚Äôd love to see how others‚Äô objects‚Äô age distribution look like. Actually once we know the age distribution for some particular use cases, we can find a ways to avoid Full GC. Full GC is expensive because both CMS and G1 Full GC are single threaded. GC tuning nowadays becomes a task of just trying to avoid Full GC completely.

Thanks
-yanping

From: Reynold Xin [mailto:rxin@databricks.com]
Sent: Tuesday, August 25, 2015 6:05 PM
To: Ulanov, Alexander
Cc: dev@spark.apache.org
Subject: Re: Dataframe aggregation with Tungsten unsafe

There are a lot of GC activity due to the non-code-gen path being sloppy about garbage creation. This is not actually what happens, but just as an example:

rdd.map { i: Int => i + 1 }

This under the hood becomes a closure that boxes on every input and every output, creating two extra objects.

The reality is more complicated than this -- but here's a simpler view of what happens with GC in these cases. You might've heard from other places that the JVM is very efficient about transient object allocations. That is true when you look at these allocations in isolation, but unfortunately not true when you look at them in aggregate.

First, due to the way the iterator interface is constructed, it is hard for the JIT compiler to on-stack allocate these objects. Then two things happen:

1. They pile up and cause more young gen GCs to happen.
2. After a few young gen GCs, some mid-tenured objects (e.g. an aggregation map) get copied into the old-gen, and eventually requires a full GC to free them. Full GCs are much more expensive than young gen GCs (usually involves copying all the data in the old gen).

So the more garbages that are created -> the more frequently full GC happens.

The more long lived objects in the old gen (e.g. cache) -> the more expensive full GC is.



On Tue, Aug 25, 2015 at 5:19 PM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>> wrote:
Thank you for the explanation. The size if the 100M data is ~1.4GB in memory and each worker has 32GB of memory. It seems to be a lot of free memory available. I wonder how Spark can hit GC with such setup?

Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com><maiOn Fri, Aug 21, 2015 at 11:07 AM, Ulanov, Alexander <alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com><mailto:alexander.ulanov@hp.com<mailto:alexander.ulanov@hp.com>>> wrote:

It seems that there is a nice improvement with Tungsten enabled given that data is persisted in memory 2x and 3x. However, the improvement is not that nice for parquet, it is 1.5x. What‚Äôs interesting, with Tungsten enabled performance of in-memory data and parquet data aggregation is similar. Could anyone comment on this? It seems counterintuitive to me.

Local performance was not as good as Reynold had. I have around 1.5x, he had 5x. However, local mode is not interesting.


I think a large part of that is coming from the pressure created by JVM GC. Putting more data in-memory makes GC worse, unless GC is well tuned.



"
rake <randy@randykerber.com>,"Wed, 26 Aug 2015 00:13:46 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),dev@spark.apache.org,"rxin wrote

I was looking for a version of this release based on Hadoop 2.2.  Is there
some reason there isn't one, especially since 2.2.0 is described as the
default Hadoop version?

-- Randy Kerber



--

---------------------------------------------------------------------


"
rake <randy@randykerber.com>,"Wed, 26 Aug 2015 01:04:02 -0700 (MST)","Re: Introduce a sbt plugin to deploy and submit jobs to a spark
 cluster on ec2",dev@spark.apache.org,"This looks promising. I'm trying to use spark-ec2 to launch a cluster with
Spark 1.5.0-SNAPSHOT and failing.

Where should we ask questions, report problems?

I couple of questions I have already after looking through the project:

    - Where does the configuration file /spark-deployer.conf/ go (what
folder)?
    - Should spark-deployer work with the Nightly/latest Builds available at
the 3 links listed here:
         Nightly Builds
<https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools#UsefulDeveloperTools-NightlyBuilds>  

Thanks,
  Randy Kerber



--

---------------------------------------------------------------------


"
pishen tsai <pishen02@gmail.com>,"Wed, 26 Aug 2015 16:40:11 +0800","Re: Introduce a sbt plugin to deploy and submit jobs to a spark
 cluster on ec2",dev@spark.apache.org,"Please ask questions at the gitter channel for now.
https://gitter.im/pishen/spark-deployer

- spark-deployer.conf should be placed in your project's root directory
(beside build.sbt)
- To use the nightly builds, you can replace the value of ""spark-tgz-url""
in spark-deployer.conf to the tgz you want to test. Also, remember to
change the version of Spark in build.sbt. The SNAPSHOT version is not
tested, if you meet any problem, please report it at gitter.

Thanks,
pishen


2015-08-26 16:04 GMT+08:00 rake [via Apache Spark Developers List] <
ml-node+s1001551n13829h26@n3.nabble.com>:

h
#UsefulDeveloperTools-NightlyBuilds>
-plugin-to-deploy-and-submit-jobs-to-a-spark-cluster-on-ec2-tp13703p13829.html
ervlet.jtp?macro=unsubscribe_by_code&node=13703&code=cGlzaGVuMDJAZ21haWwuY29tfDEzNzAzfC0xNjIyODI0MzY2>
ervlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
"
Furkan KAMACI <furkankamaci@gmail.com>,"Wed, 26 Aug 2015 11:50:30 +0300",Spark Cannot Connect to HBaseClusterSingleton,dev@spark.apache.org,"Hi,

I start an Hbase cluster for my test class. I use that helper class:

https://github.com/apache/gora/blob/master/gora-hbase/src/test/java/org/apache/gora/hbase/util/HBaseClusterSingleton.java

and use it as like that:

private static final HBaseClusterSingleton cluster =
HBaseClusterSingleton.build(1);

I retrieve configuration object as follows:

cluster.getConf()

and I use it at Spark as follows:

sparkContext.newAPIHadoopRDD(conf, MyInputFormat.class, clazzK,
    clazzV);

When I run my test there is no need to startup an Hbase cluster because
Spark will connect to my dummy cluster. However when I run my test method
it throws an error:

2015-08-26 01:19:59,558 INFO [Executor task launch
worker-0-SendThread(localhost:2181)] zookeeper.ClientCnxn
(ClientCnxn.java:logStartConnect(966)) - Opening socket connection to
server localhost/127.0.0.1:2181. Will not attempt to authenticate using
SASL (unknown error)

2015-08-26 01:19:59,559 WARN [Executor task launch
worker-0-SendThread(localhost:2181)] zookeeper.ClientCnxn
(ClientCnxn.java:run(1089)) - Session 0x0 for server null, unexpected
error, closing socket connection and attempting reconnect
java.net.ConnectException: Connection refused at
sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at
sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) at
org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350)
at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)
Hbase tests, which do not run on Spark, works well. When I check the logs I
see that cluster and Spark is started up correctly:

2015-08-26 01:35:21,791 INFO [main] hdfs.MiniDFSCluster
(MiniDFSCluster.java:waitActive(2055)) - Cluster is active

2015-08-26 01:35:40,334 INFO [main] util.Utils (Logging.scala:logInfo(59))
- Successfully started service 'sparkDriver' on port 56941.
I realized that when I start up an hbase from command line my test method
for Spark connects to it!

So, does it means that it doesn't care about the conf I passed to it? Any
ideas about how to solve it?
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 26 Aug 2015 02:08:32 -0700",Re: Spark Cannot Connect to HBaseClusterSingleton,Furkan KAMACI <furkankamaci@gmail.com>,"The connection failure was to zookeeper. 

Have you verified that localhost:2181 can serve requests ?
What version of hbase was Gora built against ?

Cheers




ache/gora/hbase/util/HBaseClusterSingleton.java
on.build(1);
ark will connect to my dummy cluster. However when I run my test method it throws an error:
alhost:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(966)) - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
alhost:2181)] zookeeper.ClientCnxn (ClientCnxn.java:run(1089)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect java.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)
 see that cluster and Spark is started up correctly:
va:waitActive(2055)) - Cluster is active
 - Successfully started service 'sparkDriver' on port 56941.
or Spark connects to it!
deas about how to solve it?
"
<andrew.rowson@thomsonreuters.com>,"Wed, 26 Aug 2015 09:18:29 +0000","RE: Spark builds: allow user override of project version at
 buildtime","<michael@databricks.com>, <vanzin@cloudera.com>","So, I actually tried this, and it built without problems, but publishing the artifacts to artifactory ended up with some strangeness in the child poms, where the property wasn‚Äôt resolved. This leads to issues pulling them into other projects of: ‚ÄúCould not find org.apache.spark:spark-parent_2.10:${spark.version}.‚Äù

There's conflicting information out on the web about whether this should or shouldn't work, and whether it is or isn't a good idea. Broad consensus is that this is actually a bit of a hack around Maven, so it's probably not something we should do.

I'll explore whether sbt is more flexible and does what's needed. 

Andrew

From: Michael Armbrust [mailto:michael@databricks.com] 
Sent: 26 August 2015 03:12
To: Marcelo Vanzin <vanzin@cloudera.com>
Cc: Rowson, Andrew G. (Financial&Risk) <andrew.rowson@thomsonreuters.com>; dev@spark.apache.org
Subject: Re: Spark builds: allow user override of project version at buildtime

This isn't really answering the question, but for what it is worth, I manage several different branches of Spark and publish custom named versions regularly to an internal repository, and this is *much* easier with SBT than with maven.  You can actually link the Spark SBT build into an external SBT build and write commands that cross publish as needed.

For your case something as simple as build/sbt ""set version in Global := '1.4.1-custom-string'"" publish might do the trick.

also
artifacts
files
think.

Have you tried it? My understanding is that no project does that
because it doesn't work. To resolve properties you need to read the
parent pom(s), and if there's a variable reference there, well, you
can't do it. Chicken & egg.

--
Marcelo

---------------------------------------------------------------------

"
Furkan KAMACI <furkankamaci@gmail.com>,"Wed, 26 Aug 2015 12:30:12 +0300",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

I'll check Zookeeper connection but another test method which runs on hbase
without Spark works without any error. Hbase version is 0.98.8-hadoop2 and
I use Spark 1.3.1

Kind Regards,
Furkan KAMACI
26 Aƒüu 2015 12:08 tarihinde ""Ted Yu"" <yuzhihong@gmail.com> yazdƒ±:

:
pache/gora/hbase/util/HBaseClusterSingleton.java
java:350)
)
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 26 Aug 2015 02:40:59 -0700",Re: Spark Cannot Connect to HBaseClusterSingleton,Furkan KAMACI <furkankamaci@gmail.com>,"Can you log the contents of the Configuration you pass from Spark ?
The output would give you some clue. 

Cheers




e without Spark works without any error. Hbase version is 0.98.8-hadoop2 and I use Spark 1.3.1
:
e:
apache/gora/hbase/util/HBaseClusterSingleton.java
eton.build(1);
park will connect to my dummy cluster. However when I run my test method it throws an error:
ocalhost:2181)] zookeeper.ClientCnxn (ClientCnxn.java:logStartConnect(966)) - Opening socket connection to server localhost/127.0.0.1:2181. Will not attempt to authenticate using SASL (unknown error)
ocalhost:2181)] zookeeper.ClientCnxn (ClientCnxn.java:run(1089)) - Session 0x0 for server null, unexpected error, closing socket connection and attempting reconnect java.net.ConnectException: Connection refused at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method) at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:739) at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport(ClientCnxnSocketNIO.java:350) at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1068)
s I see that cluster and Spark is started up correctly:
java:waitActive(2055)) - Cluster is active
)) - Successfully started service 'sparkDriver' on port 56941.
d for Spark connects to it!
y ideas about how to solve it?
"
Olivier Girardot <ssaboum@gmail.com>,"Wed, 26 Aug 2015 11:47:55 +0200",ClassCastException using DataFrame only when num-executors > 2 ...,"dev <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","Hi everyone,
I know this ""post title"" doesn't seem very logical and I agree,
we have a very complex computation using ""only"" pyspark dataframes and when
launching the computation on a CDH 5.3 cluster using Spark 1.5.0 rc1
(problem is reproduced with 1.4.x).
If the number of executors is the default 2, the computation is very long
but doesn't fail.
If the number of executors is 3 or more (tested up to 20), then the
computation fails very quickly with the following error :

*Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast
to java.lang.Long*

The complete stracktrace being :

Driver stacktrace:
at org.apache.spark.scheduler.DAGScheduler.org
$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1267)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1255)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1254)
at
scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
at
org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1254)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
at
org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
at scala.Option.foreach(Option.scala:236)
at
org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:684)
at
duler.scala:1480)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1442)
at
org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1431)
at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:554)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1805)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1818)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1831)
at org.apache.spark.SparkContext.runJob(SparkContext.scala:1902)
at org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:905)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
at
org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
at org.apache.spark.rdd.RDD.collect(RDD.scala:904)
at org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:264)
at org.apache.spark.RangePartitioner.<init>(Partitioner.scala:126)
at
org.apache.spark.sql.execution.Exchange$$anonfun$doExecute$1.apply(Exchange.scala:156)
at
org.apache.spark.sql.execution.Exchange$$anonfun$doExecute$1.apply(Exchange.scala:141)
at
org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:48)
... 138 more
*Caused by: java.lang.ClassCastException: java.lang.Double cannot be cast
to java.lang.Long*
at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:110)
at
org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getLong(rows.scala:41)
at
org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getLong(rows.scala:220)
at
org.apache.spark.sql.catalyst.expressions.JoinedRow.getLong(JoinedRow.scala:85)
at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown
Source)
at
org.apache.spark.sql.execution.Window$$anonfun$8$$anon$1.next(Window.scala:325)
at
org.apache.spark.sql.execution.Window$$anonfun$8$$anon$1.next(Window.scala:252)
at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
at
org.apache.spark.sql.execution.Window$$anonfun$8$$anon$1.fetchNextRow(Window.scala:265)
at
org.apache.spark.sql.execution.Window$$anonfun$8$$anon$1.<init>(Window.scala:272)
at org.apache.spark.sql.execution.Window$$anonfun$8.apply(Window.scala:252)
at org.apache.spark.sql.execution.Window$$anonfun$8.apply(Window.scala:251)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
at
org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:46)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:46)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at
org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at
org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:46)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
at org.apache.spark.scheduler.Task.run(Task.scala:88)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
... 1 more

And I've joined the complete (a bit anonymised) log driver side.
The computation is launched using yarn client-mode (some computations are
done on the driver side beforehand ~30 min so timestamps are correct)

Is the number of executors related in any way to the logical plan computed
by the Dataframe ?

The error seems to be related to the new Window operations (I'm using
mainly lag and lead operations) :
org.apache.spark.sql.execution.Window$$anonfun$8$$anon$1.next(Window.scala:325)

Regards,

-- 
*Olivier Girardot* | Associ√©
o.girardot@lateral-thoughts.com
+33 6 24 09 17 94

---------------------------------------------------------------------"
Ted Malaska <ted.malaska@cloudera.com>,"Wed, 26 Aug 2015 08:45:41 -0400",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Yu <yuzhihong@gmail.com>,"I've always used HBaseTestingUtility and never really had much trouble. I
use that for all my unit testing between Spark and HBase.

Here are some code examples if your interested

--Main HBase-Spark Module
https://github.com/apache/hbase/tree/master/hbase-spark

--Unit test that cover all basic connections
https://github.com/apache/hbase/blob/master/hbase-spark/src/test/scala/org/apache/hadoop/hbase/spark/HBaseContextSuite.scala

--If you want to look at the old stuff before it went into HBase

Let me know if that helps


:
±:
apache/gora/hbase/util/HBaseClusterSingleton.java
d
t
.java:350)
s
 on
d
y
"
Furkan KAMACI <furkankamaci@gmail.com>,"Wed, 26 Aug 2015 16:29:58 +0300",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Malaska <ted.malaska@cloudera.com>,"Hi,

Here is the test method I've ignored due to Connection Refused problem
failure:

https://github.com/kamaci/gora/blob/master/gora-hbase/src/test/java/org/apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65

I've implemented a Spark backend for Apache Gora as GSoC project and this
is the latest obstacle that I should solve. If you can help me, you are
welcome.

Kind Regards,
Furkan KAMACI


g/apache/hadoop/hbase/spark/HBaseContextSuite.scala
±:
/apache/gora/hbase/util/HBaseClusterSingleton.java
od
at
O.java:350)
' on
"
Ted Malaska <ted.malaska@cloudera.com>,"Wed, 26 Aug 2015 09:40:51 -0400",Re: Spark Cannot Connect to HBaseClusterSingleton,Furkan KAMACI <furkankamaci@gmail.com>,"Where can I find the code for MapReduceTestUtils.testSparkWordCount?


pache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
rg/apache/hadoop/hbase/spark/HBaseContextSuite.scala
±:
g/apache/gora/hbase/util/HBaseClusterSingleton.java
e
hod
 at
IO.java:350)
)
r' on
"
gsvic <victorasgs@gmail.com>,"Wed, 26 Aug 2015 06:47:12 -0700 (MST)","SQLContext.read.json(""path"") throws java.io.IOException",dev@spark.apache.org,"Hi,

I have the following issue. I am trying to load a 2.5G JSON file from a
10-node Hadoop Cluster. Actually, I am trying to create a DataFrame, using
sqlContext.read.json(""hdfs://master:9000/path/file.json""). 

The JSON file contains a parsed table(relation) from the TPCH benchmark. 

After finishing some tasks, the job fails by throwing several
java.io.IOExceptions. For smaller files (eg 700M it works fine). I am
posting a part of the log and the whole stack trace below: 

15/08/26 16:31:44 INFO TaskSetManager: Starting task 10.1 in stage 1.0 (TID
47, 192.168.5.146, ANY, 1416 bytes)
15/08/26 16:31:44 INFO TaskSetManager: Starting task 11.1 in stage 1.0 (TID
48, 192.168.5.150, ANY, 1416 bytes)
15/08/26 16:31:44 INFO TaskSetManager: Starting task 4.1 in stage 1.0 (TID
49, 192.168.5.149, ANY, 1416 bytes)
15/08/26 16:31:44 INFO TaskSetManager: Starting task 8.1 in stage 1.0 (TID
50, 192.168.5.246, ANY, 1416 bytes)
15/08/26 16:31:53 INFO TaskSetManager: Finished task 10.0 in stage 1.0 (TID
17) in 104681 ms on 192.168.5.243 (27/35)
15/08/26 16:31:53 INFO TaskSetManager: Finished task 8.0 in stage 1.0 (TID
15) in 105541 ms on 192.168.5.193 (28/35)
15/08/26 16:31:55 INFO TaskSetManager: Finished task 11.0 in stage 1.0 (TID
18) in 107122 ms on 192.168.5.167 (29/35)
15/08/26 16:31:57 INFO TaskSetManager: Finished task 5.0 in stage 1.0 (TID
12) in 109583 ms on 192.168.5.245 (30/35)
15/08/26 16:32:08 INFO TaskSetManager: Finished task 4.1 in stage 1.0 (TID
49) in 24135 ms on 192.168.5.149 (31/35)
15/08/26 16:32:13 WARN TaskSetManager: Lost task 2.0 in stage 1.0 (TID 9,
192.168.5.246): java.io.IOException: Too many bytes before newline:
2147483648
	at org.apache.hadoop.util.LineReader.readDefaultLine(LineReader.java:249)
	at org.apache.hadoop.util.LineReader.readLine(LineReader.java:174)
	at
org.apache.hadoop.mapred.LineRecordReader.<init>(LineRecordReader.java:134)
	at
org.apache.hadoop.mapred.TextInputFormat.getRecordReader(TextInputFormat.java:67)
	at org.apache.spark.rdd.HadoopRDD$$anon$1.<init>(HadoopRDD.scala:239)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:216)
	at org.apache.spark.rdd.HadoopRDD.compute(HadoopRDD.scala:101)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at
org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)



--

---------------------------------------------------------------------


"
Furkan KAMACI <furkankamaci@gmail.com>,"Wed, 26 Aug 2015 16:48:54 +0300",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Malaska <ted.malaska@cloudera.com>,"Hi,

Here is the MapReduceTestUtils.testSparkWordCount()


https://github.com/kamaci/gora/blob/master/gora-core/src/test/java/org/apache/gora/mapreduce/MapReduceTestUtils.java#L108

Here is SparkWordCount


https://github.com/kamaci/gora/blob/8f1acc6d4ef6c192e8fc06287558b7bc7c39b040/gora-core/src/examples/java/org/apache/gora/examples/spark/SparkWordCount.java

Lastly, here is GoraSparkEngine:


https://github.com/kamaci/gora/blob/master/gora-core/src/main/java/org/apache/gora/spark/GoraSparkEngine.java

Kind Regards,
Furkan KAMACI


apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
s
org/apache/hadoop/hbase/spark/HBaseContextSuite.scala
ƒ±:
rg/apache/gora/hbase/util/HBaseClusterSingleton.java
 test
) at
NIO.java:350)
8)
er' on
"
Ted Malaska <ted.malaska@cloudera.com>,"Wed, 26 Aug 2015 10:02:00 -0400",Re: Spark Cannot Connect to HBaseClusterSingleton,Furkan KAMACI <furkankamaci@gmail.com>,"Where is the input format class.  When every I use the search on your
github it says ""We couldn‚Äôt find any issues matching 'GoraInputFormat'""




ache/gora/mapreduce/MapReduceTestUtils.java#L108
040/gora-core/src/examples/java/org/apache/gora/examples/spark/SparkWordCount.java
ache/gora/spark/GoraSparkEngine.java
/apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
u
/org/apache/hadoop/hbase/spark/HBaseContextSuite.scala
ƒ±:
org/apache/gora/hbase/util/HBaseClusterSingleton.java
y test
o
d
9) at
tNIO.java:350)
68)
ver' on
?
"
Ted Yu <yuzhihong@gmail.com>,"Wed, 26 Aug 2015 07:13:43 -0700",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Malaska <ted.malaska@cloudera.com>,"I found GORA-386 Gora Spark Backend Support

Should the discussion be continued there ?

Cheers


rmat'""
pache/gora/mapreduce/MapReduceTestUtils.java#L108
b040/gora-core/src/examples/java/org/apache/gora/examples/spark/SparkWordCount.java
pache/gora/spark/GoraSparkEngine.java
g/apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
ou
a/org/apache/hadoop/hbase/spark/HBaseContextSuite.scala
n
ƒ±:
:
/org/apache/gora/hbase/util/HBaseClusterSingleton.java
my test
to
ed
39) at
etNIO.java:350)
068)
e
iver' on
"
Furkan KAMACI <furkankamaci@gmail.com>,"Wed, 26 Aug 2015 18:05:18 +0300",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Yu <yuzhihong@gmail.com>,"I'll send an e-mail to Gora dev list too and also attach my patch into my
GSoC Jira issue you mentioned and then we can continue at there.

Before I do that stuff, I wanted to get Spark dev community's ideas to
solve my problem due to you may have faced such kind of problems before.
26 Aƒüu 2015 17:13 tarihinde ""Ted Yu"" <yuzhihong@gmail.com> yazdƒ±:

ormat'""
apache/gora/mapreduce/MapReduceTestUtils.java#L108
9b040/gora-core/src/examples/java/org/apache/gora/examples/spark/SparkWordCount.java
apache/gora/spark/GoraSparkEngine.java
m
rg/apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
you
m
la/org/apache/hadoop/hbase/spark/HBaseContextSuite.scala
dƒ±:
a/org/apache/gora/hbase/util/HBaseClusterSingleton.java
 my test
 to
ted
739) at
ketNIO.java:350)
1068)
river' on
"
Chris Freeman <cfreeman@alteryx.com>,"Wed, 26 Aug 2015 15:08:08 +0000",Maven issues with 1.5-RC,"""dev@spark.apache.org"" <dev@spark.apache.org>","Currently trying to compile 1.5-RC2 (from https://github.com/apache/spark/commit/727771352855dbb780008c449a877f5aaa5fc27a) and running into issues with the new Maven requirement. I have 3.0.4 installed at the system level, 1.5 requires 3.3.3. As Patrick has pointed out in other places, this should be a non-issue since Spark can download and use its own version of Maven, and you can guarantee this happens by using the ‚Äîforce flag when calling build/mvn. However, this doesn‚Äôt appear to be working as intended (or I just have really bad luck).

When I run build/mvn --force -DskipTests -Psparkr package, the first thing I see is this:

Using `mvn` from path: /home/cloudera/spark/build/apache-maven-3.3.3/bin/mvn

Looks good. However, after initializing the build order and starting on Spark Project Parent POM, I still see this:

[INFO] --- maven-enforcer-plugin:1.4:enforce (enforce-versions) @ spark-parent_2.10 ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireMavenVersion failed with message:
Detected Maven Version: 3.0.4 is not in the allowed range 3.3.3.

And then the build fails. Has anyone else experienced this/anyone have any idea what I‚Äôm missing here? Running on CentOS with Java 7, for what it‚Äôs worth.

--
Chris Freeman
Senior Content Engineer - Alteryx
(657) 900 5462
"
Furkan KAMACI <furkankamaci@gmail.com>,"Wed, 26 Aug 2015 18:08:22 +0300",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Yu <yuzhihong@gmail.com>,"Btw, here is the source code of GoraInputFormat.java :

https://github.com/kamaci/gora/blob/master/gora-core/src/main/java/org/apache/gora/mapreduce/GoraInputFormat.java
26 Aƒüu 2015 18:05 tarihinde ""Furkan KAMACI"" <furkankamaci@gmail.com> yazdƒ±:

±:
Format'""
/apache/gora/mapreduce/MapReduceTestUtils.java#L108
39b040/gora-core/src/examples/java/org/apache/gora/examples/spark/SparkWordCount.java
/apache/gora/spark/GoraSparkEngine.java
m
org/apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
 you
.
ala/org/apache/hadoop/hbase/spark/HBaseContextSuite.scala
:
?
zdƒ±:
m>
va/org/apache/gora/hbase/util/HBaseClusterSingleton.java
n my test
n to
cted
:739) at
cketNIO.java:350)
:1068)
Driver' on
t
"
Sean Owen <sowen@cloudera.com>,"Wed, 26 Aug 2015 16:18:18 +0100",Re: Maven issues with 1.5-RC,Chris Freeman <cfreeman@alteryx.com>,"It sounds like you're doing the right things. I believe the Jenkins
test machines also have 3.0.4, but successfully build by using
build/mvn --force. Not sure what to make of that.

:
5fc27a)
and
ôt appear to be
g I
mvn
y
at it‚Äôs

---------------------------------------------------------------------


"
Ted Yu <yuzhihong@gmail.com>,"Wed, 26 Aug 2015 08:55:34 -0700",Re: Spark Cannot Connect to HBaseClusterSingleton,Furkan KAMACI <furkankamaci@gmail.com>,"My understanding is that people on this mailing list who are interested to
help can log comments on the GORA JIRA.
HBase integration with Spark is proven to work. So the intricacies should
be on Gora side.


ache/gora/mapreduce/GoraInputFormat.java
±:
tFormat'""
g/apache/gora/mapreduce/MapReduceTestUtils.java#L108
c39b040/gora-core/src/examples/java/org/apache/gora/examples/spark/SparkWordCount.java
g/apache/gora/spark/GoraSparkEngine.java
m
/org/apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
d
, you
e.
cala/org/apache/hadoop/hbase/spark/HBaseContextSuite.scala
 ?
m>
s
azdƒ±:
ava/org/apache/gora/hbase/util/HBaseClusterSingleton.java
un my test
on to
ected
a:739) at
ocketNIO.java:350)
a:1068)
kDriver' on
o
"
Luc Bourlier <luc.bourlier@typesafe.com>,"Wed, 26 Aug 2015 18:07:44 +0200",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Reynold Xin <rxin@databricks.com>,"- tested the backpressure/rate controlling in streaming. It works as
expected.
- there is a problem with the Scala 2.11 sbt build:
https://issues.apache.org/jira/browse/SPARK-10227

Luc Bourlier

Luc Bourlier
*Spark Team  - Typesafe, Inc.*
luc.bourlier@ty"
Sean Owen <sowen@cloudera.com>,"Wed, 26 Aug 2015 18:01:53 +0100",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Reynold Xin <rxin@databricks.com>,"My quick take: no blockers at this point, except for one potential
issue. Still some 'critical' bugs worth a look. The release seems to
pass tests but i get a lot of spurious failures; it took about 16
hours of running tests to get everything to pass at least once.


Current score: 56 issues targeted at 1.5.0, of which 14 bugs, of which
no blockers and 8 critical.

This one might be a blocker as it seems to mean that SBT + Scala 2.11
does not compile:
https://issues.apache.org/jira/browse/SPARK-10227

pretty simple issue, but weigh in on the PR:
https://github.com/apache/spark/pull/8433

For reference here are the Critical ones:

Key Component Summary Assignee
SPARK-6484 Spark Core Ganglia metrics xml reporter doesn't escape
correctly Josh Rosen
SPARK-6701 Tests, YARN Flaky test: o.a.s.deploy.yarn.YarnClusterSuite
Python application
SPARK-7420 Tests Flaky test: o.a.s.streaming.JobGeneratorSuite ""Do not
clear received block data too soon"" Tathagata Das
SPARK-8119 Spark Core HeartbeatReceiver should not adjust application
executor resources Andrew Or
SPARK-8414 Spark Core Ensure ContextCleaner actually triggers clean
ups Andrew Or
SPARK-8447 Shuffle Test external shuffle service with all shuffle managers
SPARK-10224 Streaming BlockGenerator may lost data in the last block
SPARK-10287 SQL After processing a query using JSON data, Spark SQL
continuously refreshes metadata of the table
Total: 8 issues


I'm seeing the following tests fail intermittently, with ""-Phive
-Phive-thriftserver -Phadoop-2.6"" on Ubuntu 15 / Java 7:

- security mismatch password *** FAILED ***
  Expected exception java.io.IOException to be thrown, but
java.nio.channels.CancelledKeyException was thrown.
(ConnectionManagerSuite.scala:123)


DAGSchedulerSuite:
...
- misbehaved resultHandler should not crash DAGScheduler and
SparkContext *** FAILED ***
  java.lang.UnsupportedOperationException: taskSucceeded() called on a
finished JobWaiter was not instance of
org.apache.spark.scheduler.DAGSchedulerSuiteDummyException
(DAGSchedulerSuite.scala:861)

HeartbeatReceiverSuite:
...
- normal heartbeat *** FAILED ***
  3 did not equal 2 (HeartbeatReceiverSuite.scala:104)


- Unpersisting HttpBroadcast on executors only in distributed mode ***
FAILED ***
  ...
- Unpersisting HttpBroadcast on executors and driver in distributed
mode *** FAILED ***
  ...
- Unpersisting TorrentBroadcast on executors only in distributed mode
*** FAILED ***
  ...
- Unpersisting TorrentBroadcast on executors and driver in distributed
mode *** FAILED ***


StreamingContextSuite:
...
- stop gracefully *** FAILED ***
  1749735 did not equal 1190429 Received records = 1749735, processed
records = 1190428 (StreamingContextSuite.scala:279)


DirectKafkaStreamSuite:
- offset recovery *** FAILED ***
  The code passed to eventually never returned normally. Attempted 193
times over 10.010808486 seconds. Last failure message:
strings.forall({
    ((elem: Any) => DirectKafkaStreamSuite.collectedData.contains(elem))
  }) was false. (DirectKafkaStreamSuite.scala:249)


---------------------------------------------------------------------


"
Yin Huai <yhuai@databricks.com>,"Wed, 26 Aug 2015 10:19:44 -0700","Re: SQLContext.read.json(""path"") throws java.io.IOException",gsvic <victorasgs@gmail.com>,"The JSON support in Spark SQL handles a file with one JSON object per line
or one JSON array of objects per line. What is the format your file? Does
it only contain a single line?


"
shane knapp <sknapp@berkeley.edu>,"Wed, 26 Aug 2015 11:26:19 -0700",Re: Maven issues with 1.5-RC,Sean Owen <sowen@cloudera.com>,"we build on jenkins w/3.1.1, but also have 3.0.4.

te:
a5fc27a)
d
 and
g
Äôt appear to be
ng I
/mvn
ny
hat it‚Äôs

---------------------------------------------------------------------


"
gsvic <victorasgs@gmail.com>,"Wed, 26 Aug 2015 11:38:29 -0700 (MST)","Re: SQLContext.read.json(""path"") throws java.io.IOException",dev@spark.apache.org,"Yes, it contain one line





-- 
Victor Giannakouris - Salalidis

Researcher
Computing Systems Laboratory (CSLab),
National Technical University of Athens

Software Engineer
isMOOD Data Technology Services

LinkedIn:
http://gr.linkedin.com/pub/victor-giannakouris-salalidis/69/585/b23/




--"
Reynold Xin <rxin@databricks.com>,"Wed, 26 Aug 2015 11:41:13 -0700","Re: SQLContext.read.json(""path"") throws java.io.IOException",gsvic <victorasgs@gmail.com>,"Any reason why you have more than 2G in a single line?

There is a limit of 2G in the Hadoop library we use. Also the JVM doesn't
work when your string is that long.




?
.
9,
7483648
134)
t.java:67)
)
)
)
)
70)
41)
va:1145)
ava:615)
ad-json-path-throws-java-io-IOException-tp13841.html
d-json-path-throws-java-io-IOException-tp13841p13852.html
Servlet.jtp?macro=macro_viewer&id=instant_html%21nabble%3Aemail.naml&base=nabble.naml.namespaces.BasicNamespace-nabble.view.web.template.NabbleNamespace-nabble.view.web.template.NodeNamespace&breadcrumbs=notify_subscribers%21nabble%3Aemail.naml-instant_emails%21nabble%3Aemail.naml-send_instant_email%21nabble%3Aemail.naml>
d-json-path-throws-java-io-IOException-tp13841p13854.html>
"
gsvic <victorasgs@gmail.com>,"Wed, 26 Aug 2015 13:41:51 -0700 (MST)","Re: SQLContext.read.json(""path"") throws java.io.IOException",dev@spark.apache.org,"No, I created the file by appending each JSON record in a loop without
changing line. I've just changed that and now it works fine. Thank you very
much for your support.



--

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Wed, 26 Aug 2015 13:42:00 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","29.



"
Reynold Xin <rxin@databricks.com>,"Wed, 26 Aug 2015 13:42:41 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Sean Owen <sowen@cloudera.com>,"The Scala 2.11 issue should be fixed, but doesn't need to be a blocker,
since Maven builds fine. The sbt build is more aggressive to make sure we
catch warnings.




"
Holden Karau <holden@pigscanfly.ca>,"Wed, 26 Aug 2015 14:23:34 -0700","Building with sbt ""impossible to get artifacts when data has not been loaded""","""dev@spark.apache.org"" <dev@spark.apache.org>","Has anyone else run into ""impossible to get artifacts when data has not
been loaded. IvyNode = org.scala-lang#scala-library;2.10.3"" during
hive/update when building with sbt. Working around it is pretty simple
(just add it as a dependency), but I'm wondering if its impacting anyone
else and I should make a PR for it or if its something funky with my local
build setup.

-- 
Cell : 425-233-8271
Twitter: https://twitter.com/holdenkarau
Linked In: https://www.linkedin.com/in/holdenkarau
"
Marcelo Vanzin <vanzin@cloudera.com>,"Wed, 26 Aug 2015 14:27:37 -0700","Re: Building with sbt ""impossible to get artifacts when data has not
 been loaded""",Holden Karau <holden@pigscanfly.ca>,"I ran into the same error (different dependency) earlier today. In my
case, the maven pom files and the sbt dependencies had a conflict
(different versions of the same artifact) and ivy got confused. Not
sure whether that will help in your case or not...




-- 
Marcelo

---------------------------------------------------------------------


"
Josh Rosen <rosenville@gmail.com>,"Wed, 26 Aug 2015 14:35:41 -0700","Re: Building with sbt ""impossible to get artifacts when data has not
 been loaded""",dev@spark.apache.org,"I ran into a similar problem while working on the spark-redshift library
and was able to fix it by bumping that library's ScalaTest version. I'm
still fighting some mysterious Scala issues while trying to test the
spark-csv library against 1.5.0-RC1, so it's possible that a build or
dependency change in Spark might be responsible for this.



---------------------------------------------------------------------


"
Furkan KAMACI <furkankamaci@gmail.com>,"Thu, 27 Aug 2015 01:51:35 +0300",Re: Spark Cannot Connect to HBaseClusterSingleton,Ted Yu <yuzhihong@gmail.com>,"Hi Ted,

You can check full stack trace log from the attachment at Jira:

https://issues.apache.org/jira/browse/GORA-386

Kind Regards,
Furkan KAMACI


o
pache/gora/mapreduce/GoraInputFormat.java
y
.
±:
utFormat'""
m
rg/apache/gora/mapreduce/MapReduceTestUtils.java#L108
7c39b040/gora-core/src/examples/java/org/apache/gora/examples/spark/SparkWordCount.java
rg/apache/gora/spark/GoraSparkEngine.java
?
a/org/apache/gora/hbase/mapreduce/TestHBaseStoreWordCount.java#L65
lp me,
se.
scala/org/apache/hadoop/hbase/spark/HBaseContextSuite.scala
k
on is
yazdƒ±:
java/org/apache/gora/hbase/util/HBaseClusterSingleton.java
run my test
ion to
pected
va:739) at
SocketNIO.java:350)
va:1068)
k
rkDriver' on
"
Calvin Jia <jia.calvin@gmail.com>,"Wed, 26 Aug 2015 15:56:23 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),"""dev@spark.apache.org"" <dev@spark.apache.org>","+1, tested that 1.5.0-RC2 works with Tachyon 0.7.1 as external block store.
"
David Smith <dasmiq@gmail.com>,"Wed, 26 Aug 2015 18:10:43 -0700 (MST)",Differing performance in self joins,dev@spark.apache.org,"I've noticed that two queries, which return identical results, have very
different performance. I'd be interested in any hints about how avoid
problems like this.

The DataFrame df contains a string field ""series"" and an integer ""eday"", the
number of days since (or before) the 1970-01-01 epoch.

I'm doing some analysis over a sliding date window and, for now, avoiding
UDAFs. I'm therefore using a self join. First, I create 

val laggard = df.withColumnRenamed(""series"",
""p_series"").withColumnRenamed(""eday"", ""p_eday"")

Then, the following query runs in 16s:

df.join(laggard, (df(""series"") === laggard(""p_series"")) && (df(""eday"") ===
(laggard(""p_eday"") + 1))).count

while the following query runs in 4 - 6 minutes:

df.join(laggard, (df(""series"") === laggard(""p_series"")) && ((df(""eday"") -
laggard(""p_eday"")) === 1)).count

It's worth noting that the series term is necessary to keep the query from
doing a complete cartesian product over the data.

Ideally, I'd like to look at lags of more than one day, but the following is
equally slow:

df.join(laggard, (df(""series"") === laggard(""p_series"")) && (df(""eday"") -
laggard(""p_eday"")).between(1,7)).count

Any advice about the general principle at work here would be welcome.

Thanks,
David



--

---------------------------------------------------------------------


"
Feng Tian <ftian@vitessedata.com>,"Wed, 26 Aug 2015 21:40:50 -0700",A TPCH benchmark for Spark,dev@spark.apache.org,"Hi,

We released a package called LLQL, which is a serialization of operators of
relational algebra.  Spark SQL Plan is the first one supported.

More interesting to the spark community probably is our test that
implements TPCH.  We manually rewrote some sql -- mainly pulling subqueries
out and converted them into joins.   From the executor's point of view,
spark seems to work quite well.  However, there are several expression
parsing or algbraization issues, notably Q22, Q6, Q7, Q9.

Q2 will go OOM, and sometimes Q9 as well.   We are excited about Tungsten
project and looking forward to the 1.5 release.

We are running on Spark 1.4.0, prebuilt with Hadoop 2.6.

Links to the github and the tests,

https://github.com/vitesse-ftian/quark

https://github.com/vitesse-ftian/quark/blob/master/examples/src/com/vitessedata/examples/quark/Tpch.scala

Have fun with test and timing :-)

Thanks,
Feng
"
Ashish Rawat <Ashish.Rawat@guavus.com>,"Thu, 27 Aug 2015 07:42:02 +0000",FW: High Availability of Spark Driver,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi Patrick,

As discussed in another thread, we are looking for a solution to the problem of lost state on Spark Driver failure. Can you please share Spark's long term strategy for resolving this problem.

<-- Original Mail Content Below -->

We have come across the problem of Spark Applications (on Yarn) requiring a restart in case of Spark Driver (or application master) going down. This is hugely inconvenient for long running applications which are maintaining a big state in memory. The repopulation of state in itself may require a downtime of many minutes, which is not acceptable for most live systems.

As you would have noticed that Yarn community has acknowledged ""long running services"" as an important class of use cases, and thus identified and removed problems in working with long running services in Yarn.
http://hortonworks.com/blog/support-long-running-services-hadoop-yarn-clusters/

It would be great if Spark, which is the most important processing engine on Yarn, also figures out issues in working with long running Spark applications and publishes recommendations or make framework changes for removing those. The need to keep the application running in case of Driver and Application Master failure, seems to be an important requirement from this perspective. The two most compelling use cases being:

  1.  Huge state of historical data in Spark Streaming, required for stream processing
  2.  Very large cached tables in Spark SQL (very close to our use case where we periodically cache RDDs and query using Spark SQL)

In our analysis, for both of these use cases, a working HA solution can be built by

  1.  Preserving the state of executors (not killing them on driver failures)
  2.  Persisting some meta info required by Spark SQL and Block Manager.
  3.  Restarting and reconnecting the Driver to AM and executors

This would preserve the cached data, enabling the application to come back quickly. This can further be extended for preserving and recovering the Computation State.

I would request you to share your thoughts on this issue and possible future directions.

Regards,
Ashish
"
Ashish Rawat <Ashish.Rawat@guavus.com>,"Thu, 27 Aug 2015 07:44:20 +0000",Re: High Availability of Spark Driver,"""dev@spark.apache.org"" <dev@spark.apache.org>","If anyone else is also facing similar problems and have figured out a good workaround within the current design, then please share.

Regards,
Ashish

From: Ashish Rawat <ashish.rawat@guavus.com<mailto:ashish.rawat@guavus.com>Date: Thursday, 27 August 2015 1:12 pm
To: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: FW: High Availability of Spark Driver

Hi Patrick,

As discussed in another thread, we are looking for a solution to the problem of lost state on Spark Driver failure. Can you please share Spark's long term strategy for resolving this problem.

<-- Original Mail Content Below -->

We have come across the problem of Spark Applications (on Yarn) requiring a restart in case of Spark Driver (or application master) going down. This is hugely inconvenient for long running applications which are maintaining a big state in memory. The repopulation of state in itself may require a downtime of many minutes, which is not acceptable for most live systems.

As you would have noticed that Yarn community has acknowledged ""long running services"" as an important class of use cases, and thus identified and removed problems in working with long running services in Yarn.
http://hortonworks.com/blog/support-long-running-services-hadoop-yarn-clusters/

It would be great if Spark, which is the most important processing engine on Yarn, also figures out issues in working with long running Spark applications and publishes recommendations or make framework changes for removing those. The need to keep the application running in case of Driver and Application Master failure, seems to be an important requirement from this perspective. The two most compelling use cases being:

  1.  Huge state of historical data in Spark Streaming, required for stream processing
  2.  Very large cached tables in Spark SQL (very close to our use case where we periodically cache RDDs and query using Spark SQL)

In our analysis, for both of these use cases, a working HA solution can be built by

  1.  Preserving the state of executors (not killing them on driver failures)
  2.  Persisting some meta info required by Spark SQL and Block Manager.
  3.  Restarting and reconnecting the Driver to AM and executors

This would preserve the cached data, enabling the application to come back quickly. This can further be extended for preserving and recovering the Computation State.

I would request you to share your thoughts on this issue and possible future directions.

Regards,
Ashish
"
Jacek Laskowski <jacek@japila.pl>,"Thu, 27 Aug 2015 12:02:16 +0200","Re: Building with sbt ""impossible to get artifacts when data has not
 been loaded""",Holden Karau <holden@pigscanfly.ca>,"
Hi,

Since it's sbt...

I've just tried it out myself, and didn't face any errors. How do you
run the command that led to the issue? Can you hide ~/.m2/repository
and ~/.ivy2 and ~/.sbt/0.13 by renaming them to some other name and
give it a shot again? There are few shared repos in play that might
get in the way to reproduce it.

BTW you could use the Docker image
https://hub.docker.com/r/jaceklaskowski/docker-builds-sbt/ to work in
a very clean and isolated build environment.

[spark]> hive/update
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] * com.google.code.findbugs:jsr305:1.3.9 -> 2.0.1
[warn] * com.google.guava:guava:11.0.2 -> 14.0.1
[warn] * commons-net:commons-net:2.2 -> 3.1
[warn] Run 'evicted' to see detailed eviction warnings
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] * com.google.guava:guava:11.0.2 -> 14.0.1
[warn] Run 'evicted' to see detailed eviction warnings
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] * com.google.guava:guava:11.0.2 -> 14.0.1
[warn] Run 'evicted' to see detailed eviction warnings
[info] Updating {file:/Users/jacek/dev/oss/spark/}hive...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] Done updating.
[warn] There may be incompatibilities among your library dependencies.
[warn] Here are some of the libraries that were evicted:
[warn] * com.google.code.findbugs:jsr305:1.3.9 -> 2.0.1
[warn] * com.google.guava:guava:11.0.2 -> 14.0.1
[warn] Run 'evicted' to see detailed eviction warnings
[success] Total time: 3 s, completed Aug 27, 2015 11:58:18 AM

Pozdrawiam,
Jacek

--
Jacek Laskowski | http://blog.japila.pl | http://blog.jaceklaskowski.pl
Follow me at https://twitter.com/jaceklaskowski
Upvote at http://stackoverflow.com/users/1305344/jacek-laskowski

---------------------------------------------------------------------


"
Steve Loughran <stevel@hortonworks.com>,"Thu, 27 Aug 2015 10:32:00 +0000",Re: Maven issues with 1.5-RC,Chris Freeman <cfreeman@alteryx.com>,"what happens if you edit your env to remove mvn 3.0.4, and unset any MAVEN_HOME?

On 26 Aug 2015, at 16:08, Chris Freeman <cfreeman@alteryx.com<mailto:cfreeman@alteryx.com>> wrote:

Currently trying to compile 1.5-RC2 (from https://github.com/apache/spark/commit/727771352855dbb780008c449a877f5aaa5fc27a) and running into issues with the new Maven requirement. I have 3.0.4 installed at the system level, 1.5 requires 3.3.3. As Patrick has pointed out in other places, this should be a non-issue since Spark can download and use its own version of Maven, and you can guarantee this happens by using the ‚Äîforce flag when calling build/mvn. However, this doesn‚Äôt appear to be working as intended (or I just have really bad luck).

When I run build/mvn --force -DskipTests -Psparkr package, the first thing I see is this:

Using `mvn` from path: /home/cloudera/spark/build/apache-maven-3.3.3/bin/mvn

Looks good. However, after initializing the build order and starting on Spark Project Parent POM, I still see this:

[INFO] --- maven-enforcer-plugin:1.4:enforce (enforce-versions) @ spark-parent_2.10 ---
[WARNING] Rule 0: org.apache.maven.plugins.enforcer.RequireMavenVersion failed with message:
Detected Maven Version: 3.0.4 is not in the allowed range 3.3.3.

And then the build fails. Has anyone else experienced this/anyone have any idea what I‚Äôm missing here? Running on CentOS with Java 7, for what it‚Äôs worth.

--
Chris Freeman
Senior Content Engineer - Alteryx
(657) 900 5462

"
Steve Loughran <stevel@hortonworks.com>,"Thu, 27 Aug 2015 10:49:57 +0000",Re: High Availability of Spark Driver,Ashish Rawat <Ashish.Rawat@guavus.com>,"

Hi Patrick,

As discussed in another thread, we are looking for a solution to the problem of lost state on Spark Driver failure. Can you please share Sparkís long term strategy for resolving this problem.

<-- Original Mail Content Below -->

We have come across the problem of Spark Applications (on Yarn) requiring a restart in case of Spark Driver (or application master) going down. This is hugely inconvenient for long running applications which are maintaining a big state in memory. The repopulation of state in itself may require a downtime of many minutes, which is not acceptable for most live systems.

As you would have noticed that Yarn community has acknowledged ""long running services"" as an important class of use cases, and thus identified and removed problems in working with long running services in Yarn.
http://hortonworks.com/blog/support-long-running-services-hadoop-yarn-clusters/


Yeah, I spent a lot of time on that, or at least using the features, in other work under YARN-896, summarised in http://www.slideshare.net/steve_l/yarn-services

It would be great if Spark, which is the most important processing engine on Yarn,

I'f you look at the CPU-hours going in to the big hadoop clusters, it's actually MR work and things behind Hive. but: these apps don't attempt HA

Why not? It requires whatever maintains the overall app status (spark: the driver) to persist that state in a way where it can be rebuilt. A restarted AM with the ""retain containers"" feature turned on gets nothing back from YARN except the list of previous allocated containers, and is left to sort itself out.

also figures out issues in working with long running Spark applications and publishes recommendations or make framework changes for removing those. The need to keep the application running in case of Driver and Application Master failure, seems to be an important requirement from this perspective. The two most compelling use cases being:

  1.  Huge state of historical data in Spark Streaming, required for stream processing
  2.  Very large cached tables in Spark SQL (very close to our use case where we periodically cache RDDs and query using Spark SQL)


Generally spark streaming is viewed as the big need here, but yes, long-lived cached data matters.

Bear in mind that before Spark 1.5, you can't run any spark YARN app for longer than the expiry time of your delegation tokens, so in a secure cluster you have a limit of a couple of days anyway. Unless your cluster is particularly unreliable, AM failures are usually pretty unlikely in such a short timespan. Container failure is more likely as 1) you have more of them and 2) if you have pre-emption turned on in the scheduler or are pushing the work out to a label containing spot VMs, the will fail.

In our analysis, for both of these use cases, a working HA solution can be built by

  1.  Preserving the state of executors (not killing them on driver failures)

This is a critical one


  1.  Persisting some meta info required by Spark SQL and Block Manager.

again, needs a failure tolerant storage mechanism. HDFS and ZK can work together here, but your code needs to handle all the corner cases of inconsistency, including the ""AM failure partway through state update"" scenario.

Sometimes you even need to reach for the mathematics, with TLA+ being the language of choice. Start with the ZK proof paper to see if you can get a vague idea about what it's up to -as that gives hints about how its behaviour may not be what you expect.

  1.  Restarting and reconnecting the Driver to AM and executors

I don't know how Akka can recover from this. Existing long-lived YARN services use the Hadoop 2.6+ YARN registry, which was done with this purpose in mind. Example, slider: when the containers lose contact with the AM, they pol the registry to await a new AM entry.

This would preserve the cached data, enabling the application to come back quickly. This can further be extended for preserving and recovering the Computation State.


There's also

  1.  Credential recovery. Restarted AMs get an updated HDFS delegation token by way of YARN, but nothing else.
  2.  Container/AM failure tracking to identify failing clusters. YARN uses a weighted moving average to decide when an AM is unreliable; on long-lived services the service itself should reach the same decisions about containers and nodes.
  3.  Testing. You need to be confident that things are resilient to failure and network partitions. Don't underestimate the effort here -Jepsen shows what is needed ( https://aphyr.com/ ). Saying ""Zookeeper handles it all"" doesn't magically fix things.

I an HA runtime is ultimately a great thing to have óbut don't underestimate the effort.




I would request you to share your thoughts on this issue and possible future directions.

Regards,
Ashish

"
saurfang <forest.fang@outlook.com>,"Thu, 27 Aug 2015 08:17:51 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),dev@spark.apache.org,"Compiled on Windows with YARN and HIVE. However I got exception when
submitting application to YARN due to: 

java.net.URISyntaxException: Illegal character in opaque part at index 2:
D:\TEMP\spark-b32c5b5b-a9fa-4cfd-a233-3977588d4092\__spark_conf__1960856096319316224.zip
        at java.net.URI$Parser.fail(URI.java:2829)
        at java.net.URI$Parser.checkChars(URI.java:3002)
        at java.net.URI$Parser.parse(URI.java:3039)
        at java.net.URI.<init>(URI.java:595)
        at
org.apache.spark.deploy.yarn.Client.org$apache$spark$deploy$yarn$Client$$distribute$1(Client.scala:321)
        at
org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:417)

It looks like either we can do `new File(path).toURI` at here:
https://github.com/apache/spark/blob/v1.5.0-rc2/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L321

Or make sure the file path use '/' separator here:
https://github.com/apache/spark/blob/v1.5.0-rc2/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L417




--

---------------------------------------------------------------------


"
saurfang <forest.fang@outlook.com>,"Thu, 27 Aug 2015 08:50:27 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),dev@spark.apache.org,"Nevermind. It looks like this has been fixed in
https://github.com/apache/spark/pull/8053 but didn't make the cut? Even
though the associated JIRA is targeted for 1.6, I was able to submit to YARN
from Windows without a problem with 1.4. I'm wondering if this fix will be
merged to 1.5 branch. Let me know if someone thinks I'm just not doing the
compile and/or spark-submit right.



--

---------------------------------------------------------------------


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 27 Aug 2015 09:52:03 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),saurfang <forest.fang@outlook.com>,"Are you just submitting from Windows or are you also running YARN on Windows?

If the former, I think the only fix that would be needed is this line
(from that same patch):
https://github.com/apache/spark/blob/master/yarn/src/main/scala/org/apache/spark/deploy/yarn/Client.scala#L434

I don't believe YARN running on Windows worked at all before that
patch (regardless of that individual issue). I'll leave it to Reynold
whether Windows support is critical enough to warrant a new rc.





-- 
Marcelo

---------------------------------------------------------------------


"
Sen Fang <forest.fang@outlook.com>,"Thu, 27 Aug 2015 17:46:15 +0000",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"Agree on the line fix. I'm submitting from Windows to YARN running on
Linux. I imagine that this isn't that uncommon especially for developers
working in corporate setting.


"
Atsu Kakitani <atkakitani@groupon.com>,"Thu, 27 Aug 2015 12:21:36 -0700",Opening up metrics interfaces,dev@spark.apache.org,"Hi,

I was wondering if there are any plans to open up the API for Spark's
metrics system. I want to write custom sources and sinks, but these
interfaces aren't public right now. I saw that there was also an issue open
for this (https://issues.apache.org/jira/browse/SPARK-5630), but it hasn't
been addressed - is there a reason why these interfaces are kept private?

Thanks,
Atsu
"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 27 Aug 2015 16:42:29 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Reynold Xin <rxin@databricks.com>,"+1. I tested the ""without hadoop"" binary package and ran our internal
tests on it with dynamic allocation both on and off.

The Windows issue Sen raised could be considered a regression /
blocker, though, and it's a one line fix. If we feel that's importa"
Reynold Xin <rxin@databricks.com>,"Thu, 27 Aug 2015 16:42:26 -0700",Re: Opening up metrics interfaces,Atsu Kakitani <atkakitani@groupon.com>,"I'd like this to happen, but it hasn't been super high priority on
anybody's mind.

There are a couple things that could be good to do:

1. At the application level: consolidate task metrics and accumulators.
They have substantial overlap, and from high level should just be
consolidated. Maybe there are some differences in semantics w.r.t. retries
or fault-tolerance, but those can be just modes in the consolidated
interface/implementation.

interface to add new metrics.

2. At the process/service monitoring level: expose an internal metrics
interface to make it easier to create new metrics and publish them via a
rest interface. Last time I looked at this (~4 weeks ago), publication of
the current metrics was not as straightforward as I was hoping for. We use
the codahale library only in some places (IIRC just the cluster manager,
but not the actual executors). It'd make sense to create a simple wrapper
for the coda hale library and make it easier to create new metrics.



"
Thomas Dudziak <tomdzk@gmail.com>,"Thu, 27 Aug 2015 16:54:44 -0700",Re: Opening up metrics interfaces,Reynold Xin <rxin@databricks.com>,"+1. I'd love to simply define a timer in my code (maybe metrics-scala ?)
using Spark's metrics registry. Also maybe switch to the newer version
(io.dropwizard.metrics) ?


"
Marcelo Vanzin <vanzin@cloudera.com>,"Thu, 27 Aug 2015 16:56:27 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Reynold Xin <rxin@databricks.com>,"
Looks like Josh just found a blocker, so maybe we can squeeze this in?

---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Thu, 27 Aug 2015 16:58:14 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Marcelo Vanzin <vanzin@cloudera.com>,"Marcelo - please submit a patch anyway. If we don't include it in this
release, it will go into 1.5.1.





"
Krishna Sankar <ksankar42@gmail.com>,"Thu, 27 Aug 2015 20:09:32 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Reynold Xin <rxin@databricks.com>,"+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 42:36 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib
2.1. statistics (min,max,mean,Pearson,Spearman) OK
2.2. Linear/Ridge/Laso Regression OK
"
Manish Amde <manish9ue@gmail.com>,"Thu, 27 Aug 2015 22:03:35 -0700",Re: Feedback: Feature request,"""Murphy, James"" <James.Murphy@disney.com>","Hi James,

It's a good idea. A JSON format is more convenient for visualization though a little inconvenient to read. How about toJson() method? It might make the mllib api inconsistent across models though. 

You should probably create a JIRA for this.

CC: dev list

-Manish

e:
ct rules that could easily facilitate visualization with libraries like D3.
esult =
"
Ashish Rawat <Ashish.Rawat@guavus.com>,"Fri, 28 Aug 2015 07:53:14 +0000",Re: High Availability of Spark Driver,Steve Loughran <stevel@hortonworks.com>,"Thanks Steve. I had not spent many brain cycles on analysing the Yarn pieces, your insights would be extremely useful.

I was also considering Zookeeper and Yarn registry for persisting state and sharing information. But for a basic POC, I used the file system and was able to

  1.  Preserve Executors.
  2.  Reconnect Executors back to Driver by storing the Executor endpoints info into a local file system. When driver restarts, use this info to send update driver message to executor endpoints. Executors can then update all of their Akka endpoints and reconnect.
  3.  Reregister Block Manager and report back blocks. This utilises most of Sparkís existing code, I only had to update the BlockManagerMaster endpoint in executors.

Surprisingly, Spark components took the restart in a much better way than I had anticipated and were easy to accept new work :-)

I am still figuring out other complexities around preserving RDD lineage and computation. From my initial analysis, preserving the whole computation might be complex and may not be required. Perhaps, the lineage of only the cached RDDs can be preserved to recover any lost blocks.

I am definitely not underestimating the effort, both within Spark and around interfacing with Yarn, but just trying to emphasise that a single node leading to full application restart, does not seem right for a long running service. Thoughts?

Regards,
Ashish

From: Steve Loughran <stevel@hortonworks.com<mailto:stevel@hortonworks.com>Date: Thursday, 27 August 2015 4:19 pm
To: Ashish Rawat <ashish.rawat@guavus.com<mailto:ashish.rawat@guavus.com>>
Cc: ""dev@spark.apache.org<mailto:dev@spark.apache.org>"" <dev@spark.apache.org<mailto:dev@spark.apache.org>>
Subject: Re: High Availability of Spark Driver



Hi Patrick,

As discussed in another thread, we are looking for a solution to the problem of lost state on Spark Driver failure. Can you please share Sparkís long term strategy for resolving this problem.

<-- Original Mail Content Below -->

We have come across the problem of Spark Applications (on Yarn) requiring a restart in case of Spark Driver (or application master) going down. This is hugely inconvenient for long running applications which are maintaining a big state in memory. The repopulation of state in itself may require a downtime of many minutes, which is not acceptable for most live systems.

As you would have noticed that Yarn community has acknowledged ""long running services"" as an important class of use cases, and thus identified and removed problems in working with long running services in Yarn.
http://hortonworks.com/blog/support-long-running-services-hadoop-yarn-clusters/


Yeah, I spent a lot of time on that, or at least using the features, in other work under YARN-896, summarised in http://www.slideshare.net/steve_l/yarn-services

It would be great if Spark, which is the most important processing engine on Yarn,

I'f you look at the CPU-hours going in to the big hadoop clusters, it's actually MR work and things behind Hive. but: these apps don't attempt HA

Why not? It requires whatever maintains the overall app status (spark: the driver) to persist that state in a way where it can be rebuilt. A restarted AM with the ""retain containers"" feature turned on gets nothing back from YARN except the list of previous allocated containers, and is left to sort itself out.

also figures out issues in working with long running Spark applications and publishes recommendations or make framework changes for removing those. The need to keep the application running in case of Driver and Application Master failure, seems to be an important requirement from this perspective. The two most compelling use cases being:

  1.  Huge state of historical data in Spark Streaming, required for stream processing
  2.  Very large cached tables in Spark SQL (very close to our use case where we periodically cache RDDs and query using Spark SQL)


Generally spark streaming is viewed as the big need here, but yes, long-lived cached data matters.

Bear in mind that before Spark 1.5, you can't run any spark YARN app for longer than the expiry time of your delegation tokens, so in a secure cluster you have a limit of a couple of days anyway. Unless your cluster is particularly unreliable, AM failures are usually pretty unlikely in such a short timespan. Container failure is more likely as 1) you have more of them and 2) if you have pre-emption turned on in the scheduler or are pushing the work out to a label containing spot VMs, the will fail.

In our analysis, for both of these use cases, a working HA solution can be built by

  1.  Preserving the state of executors (not killing them on driver failures)

This is a critical one


  1.  Persisting some meta info required by Spark SQL and Block Manager.

again, needs a failure tolerant storage mechanism. HDFS and ZK can work together here, but your code needs to handle all the corner cases of inconsistency, including the ""AM failure partway through state update"" scenario.

Sometimes you even need to reach for the mathematics, with TLA+ being the language of choice. Start with the ZK proof paper to see if you can get a vague idea about what it's up to -as that gives hints about how its behaviour may not be what you expect.

  1.  Restarting and reconnecting the Driver to AM and executors

I don't know how Akka can recover from this. Existing long-lived YARN services use the Hadoop 2.6+ YARN registry, which was done with this purpose in mind. Example, slider: when the containers lose contact with the AM, they pol the registry to await a new AM entry.

This would preserve the cached data, enabling the application to come back quickly. This can further be extended for preserving and recovering the Computation State.


There's also

  1.  Credential recovery. Restarted AMs get an updated HDFS delegation token by way of YARN, but nothing else.
  2.  Container/AM failure tracking to identify failing clusters. YARN uses a weighted moving average to decide when an AM is unreliable; on long-lived services the service itself should reach the same decisions about containers and nodes.
  3.  Testing. You need to be confident that things are resilient to failure and network partitions. Don't underestimate the effort here -Jepsen shows what is needed ( https://aphyr.com/ ). Saying ""Zookeeper handles it all"" doesn't magically fix things.

I an HA runtime is ultimately a great thing to have óbut don't underestimate the effort.




I would request you to share your thoughts on this issue and possible future directions.

Regards,
Ashish

"
Cody Koeninger <cody@koeninger.org>,"Fri, 28 Aug 2015 07:56:08 -0500",Re: Feedback: Feature request,Manish Amde <manish9ue@gmail.com>,"I wrote some code for this a while back, pretty sure it didn't need access
to anything private in the decision tree / random forest model.  If people
want it added to the api I can put together a PR.

I think it's important to have separately parseable operators / operands
though.  E.g

""lhs"":0,""op"":""<="",""rhs"":-35.0

"
Manish Amde <manish9ue@gmail.com>,"Fri, 28 Aug 2015 06:20:10 -0700",Re: Feedback: Feature request,Cody Koeninger <cody@koeninger.org>,"Sounds good. It's a request I have seen a few times in the past and have
needed it personally. May be Joseph Bradley has something to add.

I think a JIRA to capture this will be great. We can move this discussion
to the JIRA then.


"
Chester Chen <chester@alpinenow.com>,"Fri, 28 Aug 2015 09:27:30 -0700",Re: High Availability of Spark Driver,Ashish Rawat <Ashish.Rawat@guavus.com>,"Ashish and Steve
     I am also working on the long running Yarn Spark Job. Just start to
focus on failure recovery. This thread of discussion is really helpful.

Chester


s
n
rMaster
on
e
‚Äôs
s
g
sters/
e
ed
t
e.
n
e.
is
a
e
he
k
epsen
"
fsacerdoti <fsacerdoti@jumptrading.com>,"Fri, 28 Aug 2015 12:43:49 -0700 (MST)",IOError on createDataFrame,dev@spark.apache.org,"Hello,

Similar to the thread below [1], when I tried to create an RDD from a 4GB
pandas dataframe I encountered the error

    TypeError: cannot create an RDD from type: <type 'list'>

However looking into the code shows this is raised from a generic ""except
Exception:"" predicate (pyspark/sql/context.py:238 in spark-1.4.1). A
debugging session reveals the true error is SPARK_LOCAL_DIRS ran out of
space:

-> rdd = self._sc.parallelize(data)
(Pdb) 
*IOError: (28, 'No space left on device')*

In this case, creating an RDD from a large matrix (~50mill rows) is required
for us. I'm a bit concerned about spark's process here:

   a. turning the dataframe into records (data.to_records)
   b. writing it to tmp
   c. reading it back again in scala.

Is there a better way? The intention would be to operate on slices of this
large dataframe using numpy operations via spark's transformations and
actions.

Thanks,
FDS
 
1. https://www.mail-archive.com/user@spark.apache.org/msg35139.html





--

---------------------------------------------------------------------


"
Nicholas Chammas <nicholas.chammas@gmail.com>,"Fri, 28 Aug 2015 20:48:09 +0000",Re: [survey] [spark-ec2] What do you like/dislike about spark-ec2?,"Spark dev list <dev@spark.apache.org>, user <user@spark.apache.org>","Hi Everybody!

Thanks for participating in the spark-ec2 survey. The full results are
publicly viewable here:

https://docs.google.com/forms/d/1VC3YEcylbguzJ-YeggqxntL66MbqksQHPwbodPz_RTg/viewanalytics

The gist of the results is as follows:

Most people found spark-ec2 useful as an easy way to get a working Spark
cluster to run a quick experiment or do some benchmarking without having to
do a lot of manual configuration or setup work.

Many people lamented the slow launch times of spark-ec2, problems getting
it to launch clusters within a VPC, and broken Ganglia installs. Some also
mentioned that Hadoop 2 didn't work as expected.

Wish list items for spark-ec2 included faster launches, selectable Hadoop 2
versions, and more configuration options.

If you'd like to add your own feedback to what's already there, I've
decided to leave the survey open for a few more days:

http://goo.gl/forms/erct2s6KRR

As noted before, your results are anonymous and public.

Thanks again for participating! I hope this has been useful to the
community.

Nick

m>

is link
"
Jonathan Bender <jonathan.bender@gmail.com>,"Fri, 28 Aug 2015 14:22:03 -0700 (MST)",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),dev@spark.apache.org,"-1 for regression on PySpark + YARN support

It seems like this JIRA https://issues.apache.org/jira/browse/SPARK-7733
added a requirement for Java 7 in the build process.  Due to some quirks
with the Java archive format changes between Java 6 and 7, using"
Marcelo Vanzin <vanzin@cloudera.com>,"Fri, 28 Aug 2015 14:34:04 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Jonathan Bender <jonathan.bender@gmail.com>,"Hi Jonathan,

Can you be more specific about what problem you're running into?

SPARK-6869 fixed the issue of pyspark vs. assembly jar by shipping the
pyspark archives separately to YARN. With that fix in place, pyspark
doesn't need to get anything from the Spark assembly, so it has no
problems running on YARN. I just downloaded
spark-1.5.0-bin-hadoop2.6.tgz and tried that out, and pyspark works
fine on YARN for me.





-- 
Marcelo

---------------------------------------------------------------------


"
Luciano Resende <luckbr1975@gmail.com>,"Fri, 28 Aug 2015 14:37:39 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Reynold Xin <rxin@databricks.com>,"The binary archives seems to be having some issues, which seems consistent
on few of the different ones (different versions of hadoop) that I tried.

 tar -xvf spark-1.5.0-bin-hadoop2.6.tgz

x spark-1.5.0-bin-hadoop2.6/lib/spark-examples-1.5.0-hadoop2.6.0.jar
x spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar
x spark-1.5.0-bin-hadoop2.6/lib/spark-1.5.0-yarn-shuffle.jar
x spark-1.5.0-bin-hadoop2.6/README.md
tar: copyfile unpack
(spark-1.5.0-bin-hadoop2.6/python/test_support/sql/orc_partitioned/SUCCESS.crc)
failed: No such file or directory

tar tzf spark-1.5.0-bin-hadoop2.3.tgz | grep SUCCESS.crc
spark-1.5.0-bin-hadoop2.3/python/test_support/sql/orc_partitioned/._SUCCESS.crc

This seems similar to a problem Avro release was having recently.





-- 
Luciano Resende
http://people.apache.org/~lresende
http://twitter.com/lresende1975
http://lresende.blogspot.com/
"
Shivaram Venkataraman <shivaram@eecs.berkeley.edu>,"Fri, 28 Aug 2015 15:08:54 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Luciano Resende <luckbr1975@gmail.com>,"I've seen similar tar file warnings and in my case it was because I
was using the default tar on a Macbook. Using gnu-tar from brew made
the warnings go away.

Thanks
Shivaram


---------------------------------------------------------------------


"
Jon Bender <jonathan.bender@gmail.com>,"Fri, 28 Aug 2015 15:18:10 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),,"Marcelo,

Thanks for replying -- after looking at my test again, I misinterpreted
another issue I'm seeing which is unrelated (note I'm not using a pre-built
binary, rather had to build my own with Yarn/Hive support, as I want to use
it on an older cluster (CDH5.1.0)).

I can start up a pyspark app on YARN, so I don't want to block this.  +1

Best,
Jonathan


"
Yin Huai <yhuai@databricks.com>,"Fri, 28 Aug 2015 15:36:56 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Jon Bender <jonathan.bender@gmail.com>,"-1

Found a problem on reading partitioned table. Right now, we may create a
SQL project/filter operator for every partition. When we have thousands of
partitions, there will be a huge number of SQLMetrics (accumulators), which
causes high memory pressure"
vaquar khan <vaquar.khan@gmail.com>,"Sat, 29 Aug 2015 18:20:51 +0530",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Krishna Sankar <ksankar42@gmail.com>,"+1 (1.5.0 RC2)Compiled on Windows with YARN.

Regards,
Vaquar khan
+1 (non-binding, of course)

1. Compiled OSX 10.10 (Yosemite) OK Total time: 42:36 min
     mvn clean package -Pyarn -Phadoop-2.6 -DskipTests
2. Tested pyspark, mllib
2.1. statistics (min,"
=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Sat, 29 Aug 2015 20:52:16 +0300",Research of Spark scalability / performance issues,dev@spark.apache.org,"Hi guys!

I am going to make a contribution to Spark, but I didn't have much
experience using it under high load and will be very appreciated for any
help for pointing out scalability or performance issues that can be
researched and resolved.

I have several ideas:
1. Nodes HA (Seems like this is resolved in spark, but maybe someone knows
existing problems..)
2. Improve data distribution between nodes. (analyze queries and
automatically suggest data distribution to improve performance)
3. To think about Geo distribution. but is it actual?

It will be master degree project. please, help me to select right
improvement.

Thanks in advance!
"
Paul Weiss <paulweiss.dev@gmail.com>,"Sat, 29 Aug 2015 15:17:00 -0400",Tungsten off heap memory access for C++ libraries,dev@spark.apache.org,"Hi,

Would the benefits of project tungsten be available for access by non-JVM
programs directly into the off-heap memory?  Spark using dataframes w/ the
tungsten improvements will definitely help analytics within the JVM world
but accessing outside 3rd party c++ libraries is a challenge especially
when trying to do it with a zero copy.

Ideally the off heap memory would be accessible to a non JVM program and be
invoked in process using JNI per each partition.  The alternatives to this
involve additional costs of starting another process if using pipes as well
as the additional copy all the data.

In addition to read only non-JVM access in process would there be a way to
share the dataframe that is in memory out of process and across spark
contexts.  This way an expensive complicated initial build up of a
dataframe would not have to be replicated as well not having to pay the
penalty of the startup costs on failure.

thanks,

-paul
"
Timothy Chen <tnachen@gmail.com>,"Sat, 29 Aug 2015 12:29:35 -0700",Re: Tungsten off heap memory access for C++ libraries,Paul Weiss <paulweiss.dev@gmail.com>,"I would also like to see data shared off-heap to a 3rd party C++
library with JNI, I think the complications would be how to memory
manage this and make sure the 3rd party libraries also adhere to the
access contracts as well.

Tim


---------------------------------------------------------------------


"
Reynold Xin <rxin@databricks.com>,"Sat, 29 Aug 2015 18:40:46 -0700",Re: Tungsten off heap memory access for C++ libraries,Paul Weiss <paulweiss.dev@gmail.com>,"Supporting non-JVM code without memory copying and serialization is
actually one of the motivations behind Tungsten. We didn't talk much about
it since it is not end-user-facing and it is still too early. There are a
few challenges still:

1. Spark cannot run entirely in off-heap mode (by entirely here I'm
referring to all the data-plane memory, not control-plane such as RPCs
since those don't matter much). There is nothing fundamental. It just takes
a while to make sure all code paths allocate/free memory using the proper
allocators.

2. The memory layout of data is still in flux, since we are only 4 months
into Tungsten. They will change pretty frequently for the foreseeable
future, and as a result, the C++ side of things will have change as well.




"
Reynold Xin <rxin@databricks.com>,"Sat, 29 Aug 2015 18:42:26 -0700",Re: Research of Spark scalability / performance issues,"=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>, 
	Kay Ousterhout <kayousterhout@gmail.com>","Both 2 and 3 are pretty good topics for master's project I think.

You can also look into how one can improve Spark's scheduler throughput.
Couple years ago Kay measured it but things have changed. It would be great
to start with measurement, and then look at where the bottlenecks are, and
see how we can improve it.


õ–∏—Ö–æ–º–∞–Ω <serglihoman@gmail.com>

s
"
Paul Weiss <paulweiss.dev@gmail.com>,"Sun, 30 Aug 2015 08:58:33 -0400",Re: Tungsten off heap memory access for C++ libraries,Reynold Xin <rxin@databricks.com>,"Reynold,

That is great to hear.  Definitely interested in how 2. is being
leveraging the off heap memory is how the data is organized as well as
being able to easily access it from the C++ side.  For example how would
you store a multi dimensional array of doubles and how would you specify
that?  Perhaps Avro or Protobuf could be used for storing complex nested
structures although making that a zero copy could be a challenge.
Regardless of how the internals lays the data out in memory the important
requirements are:

a) ensuring zero copy
b) providing a friendly api on the C++ side so folks don't have to deal
with raw bytes, serialization, and JNI
c) ability to specify a complex (multi type and nested) structure via a
schema for memory storage (compile time generated would be sufficient but
run time dynamically would be extremely flexible)

Perhaps a simple way to accomplish would be to enhance dataframes to have a
C++ api that can access the off-heap memory in a clean way from Spark (in
process and w/ zero copy).

Also, is this work being done on a branch I could look into further and try
out?

thanks,
-paul




"
Sandy Ryza <sandy.ryza@cloudera.com>,"Sun, 30 Aug 2015 11:30:10 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),vaquar khan <vaquar.khan@gmail.com>,"+1 (non-binding)
built from source and ran some jobs against YARN

-Sandy


"") OK
k. But
s
fc27a
d
===============
===============
========================
========================
not
=======================================
=============================="
Patrick Wendell <pwendell@gmail.com>,"Sun, 30 Aug 2015 21:48:29 -0700",[ANNOUNCE] New testing capabilities for pull requests,"""dev@spark.apache.org"" <dev@spark.apache.org>","Hi All,

For pull requests that modify the build, you can now test different
build permutations as part of the pull request builder. To trigger
these, you add a special phrase to the title of the pull request.
Current options are:

[test-maven] - run tests using maven and not sbt
[test-hadoop1.0] - test using older hadoop versions (can use 1.0, 2.0,
2.2, and 2.3).

The relevant source code is here:
https://github.com/apache/spark/blob/master/dev/run-tests-jenkins#L193

This is useful because it allows up-front testing of build changes to
avoid breaks once a patch has already been merged.

I've documented this on the wiki:
https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools

- Patrick

---------------------------------------------------------------------


"
Akhil Das <akhil@sigmoidanalytics.com>,"Mon, 31 Aug 2015 12:28:34 +0530",Re: IOError on createDataFrame,fsacerdoti <fsacerdoti@jumptrading.com>,"Why not attach a bigger hard disk to the machines and point your
SPARK_LOCAL_DIRS to it?

Thanks
Best Regards


"
Reynold Xin <rxin@databricks.com>,"Mon, 31 Aug 2015 01:12:27 -0700",Re: Tungsten off heap memory access for C++ libraries,Paul Weiss <paulweiss.dev@gmail.com>,"
We don't have a branch yet -- because there is no code nor design for this
yet. As I said, it is one of the motivations behind Tungsten, but it is
fairly early and we don't have anything yet. When we start doing it, I will
shoot the dev list an email.
"
Reynold Xin <rxin@databricks.com>,"Mon, 31 Aug 2015 01:13:46 -0700",Re: Tungsten off heap memory access for C++ libraries,Paul Weiss <paulweiss.dev@gmail.com>,"BTW if you are interested in this, we could definitely get some help in
terms of prototyping the feasibility, i.e. how we can have a native (e.g.
C++) API for data access shipped with Spark. There are a lot of questions
(e.g. build, portability) that need to be answered.


"
yash datta <saucam@gmail.com>,"Mon, 31 Aug 2015 16:14:51 +0530",KryoSerializer for closureSerializer in DAGScheduler,dev@spark.apache.org,"Hi devs,

Curently the only supported serializer for serializing tasks in
DAGScheduler.scala is JavaSerializer.


val taskBinaryBytes: Array[Byte] = stage match {
  case stage: ShuffleMapStage =>
    closureSerializer.serialize((stage.rdd, stage.shuffleDep): AnyRef).array()
  case stage: ResultStage =>
    closureSerializer.serialize((stage.rdd,
stage.resultOfJob.get.func) : AnyRef).array()
  }

taskBinary = sc.broadcast(taskBinaryBytes)


Could somebody give me pointers as to what all is involved if we want to
change it to KryoSerializer ?




http://apache-spark-developers-list.1001551.n3.nabble.com/bug-using-kryo-as-closure-serializer-td6473.html

 was to use chill-scala ' s KryoSerializer
for closureSerializer :

private val closureSerializer = SparkEnv.get.closureSerializer.newInstance()



But on digging the code it looks like KryoSerializer being used is from
twitter chill library only.

in KryoSerializer.scala :

val instantiator = new EmptyScalaKryoInstantiator
val kryo = instantiator.newKryo()

----------------------------------------

package com.twitter.chill
class EmptyScalaKryoInstantiator() extends com.twitter.chill.KryoInstantiator {
  override def newKryo() : com.twitter.chill.KryoBase = { /* compiled code */ }
}



I am working on a low latency job and much of the time is spent in
serializing result stage rdd (~140 ms ) and the serialized size is 2.8 mb.
Thoughts ? Is this reasonable ? Wanted to check if shifting to
kryoserializer helps here.

I am serializing a UnionRDD which is created by code like this :


rdds here is a list of schemaRDDs


val condition = 'column === indexValue

val selectFields = UnresolvedAttribute(""ts"") :: fieldClass.selectFields

val sddpp = rdds.par.map(x => x.where(condition).select(selectFields: _*))



val rddpp = sddpp.map(x => new PartitionPruningRDD(x, partitioner.func))


val unioned = new UnionRDD(sqlContext.sparkContext, rddpp.toList)


My partitioner above selects one partition (from 100 partitions) per RDD
from the list of RDDs passed to UnionRDD, and UnionRDD finally created has
127 partitions

Calling unioned.collect leads to serialization of UnionRDD.

I am using spark 1.2.1


Any help regarding this will be highly appreciated.


Best
Yash Datta


-- 
When events unfold with calm and ease
When the winds that blow are merely breeze
Learn from nature, from birds and bees
Live your life in love, and let joy not cease.
"
Steve Loughran <stevel@hortonworks.com>,"Mon, 31 Aug 2015 11:43:15 +0000",Re: Research of Spark scalability / performance issues,Reynold Xin <rxin@databricks.com>,"
If you look at the recurrent issues in datacentre-scale computing systems, two stand out
-resilience to failure: that's algorithms and the layers underneath (storage, work allocation & tracking ...)
-scheduling: maximising resource utilisation while prioritising high-SLA work (interactive things, HBase)  on a mixed-workload cluster

Scheduling is where you get to use phrases like ""APX-hard""  in papers attached to JIRAs and not only not scare people, you may actually get feedback. In large ‚Äîand we are taking 10K+ node clusters these days‚Äî, fractional improvements in cluster utilisation are measurable in a size big enough to show up in spreadsheets that people who sign off on cluster purchases. But for the same reason, the maintainers of the big schedulers (e.g. the YARN ones) are usually pretty reluctant to trust getting patches in. Focusing on scheduling within an app is likely to be more tractable


HA is one of those really-hard-get-right problems. If you like your Lamport papers and enjoy the TLA+ toolchain it's the one to go for. The best tactic here is start with the work of others, which outside of Google means ""Zookeeper""..understanding its proof would be a good start into that work.

Other topic
-work optimisation (e.g. partitioning, placement, ordering of operations)
-self-tuning & cluster operations optimisation: can logs & live monitoring be used to improve system efficiency. The fact that logs are themselves large datasets means you get to use the analysis layer to introspect on past work. Cluster ops don't viewed as a CS problem, more one of those implementation details

Finally, and this isn't in a software stack itself, but something that'd be used to test everything, and again, uses the tooling, is something to radically improve how we test and understand those test results
http://steveloughran.blogspot.co.uk/2015/05/distributed-system-testing-where-now.html

A challenge there would actually be getting your supervisors to recognise the problem and accept that its worth you putting in the effort. Fault injection & failure simulation is something to consider here; it hooks up to HA nicely. Look at the jepsen work as an example https://aphyr.com/posts

-Steve


On 30 Aug 2015, at 02:42, Reynold Xin <rxin@databricks.com<mailto:rxin@databricks.com>> wrote:

Both 2 and 3 are pretty good topics for master's project I think.

You can also look into how one can improve Spark's scheduler throughput. Couple years ago Kay measured it but things have changed. It would be great to start with measurement, and then look at where the bottlenecks are, and see how we can improve it.


On Sat, Aug 29, 2015 at 10:52 AM, –°–µ—Ä–≥–µ–π –õ–∏—Ö–æ–º–∞–Ω <serglihomas!

I am going to make a contribution to Spark, but I didn't have much experience using it under high load and will be very appreciated for any help for pointing out scalability or performance issues that can be researched and resolved.

I have several ideas:
1. Nodes HA (Seems like this is resolved in spark, but maybe someone knows existing problems..)
2. Improve data distribution between nodes. (analyze queries and automatically suggest data distribution to improve performance)
3. To think about Geo distribution. but is it actual?

It will be master degree project. please, help me to select right improvement.

Thanks in advance!


"
=?UTF-8?B?0KHQtdGA0LPQtdC5INCb0LjRhdC+0LzQsNC9?= <serglihoman@gmail.com>,"Mon, 31 Aug 2015 15:22:05 +0300",Re: Research of Spark scalability / performance issues,Steve Loughran <stevel@hortonworks.com>,"I will focus on scheduling to improve throughput. I need some time to read
JIRAs tickets and analyze requirements for future scheduler.  I will update
this thread with doc when it will be done.

Thanks guys!

2015-08-31 14:43 GMT+03:00 Steve Loughran <stevel@hortonworks.com>:

,
ys‚Äî,
ig
s
e
t
g
ere-now.html
at
d
–õ–∏—Ö–æ–º–∞–Ω <serglihoman@gmail.com>
"
Josh Rosen <rosenville@gmail.com>,"Mon, 31 Aug 2015 06:51:40 -0700",Re: KryoSerializer for closureSerializer in DAGScheduler,yash datta <saucam@gmail.com>,"There are currently a few known issues with using KryoSerializer as the
closure serializer, so it's going to require some changes to Spark if we
want to properly support this. See https://github.com/apache/spark/pull/6361
and https://issues.apache.org/jira/browse/SPARK-7708 for some discussion of
the difficulties here.


"
fsacerdoti <fsacerdoti@jumptrading.com>,"Mon, 31 Aug 2015 07:50:39 -0700 (MST)",Re: IOError on createDataFrame,dev@spark.apache.org,"There are two issues here:

1. Suppression of the true reason for failure. The spark runtime reports
""TypeError"" but that is not why the operation failed.

2. The low performance of loading a pandas dataframe.


DISCUSSION

Number (1) is easily fixed, and the primary purpose for my post.
Number (2) is harder, and may lead us to abandon Spark. To answer Akhil, the
process is too slow. Yes it will work, but with large dense datasets, the
line

    data = [r.tolist() for r in data.to_records(index=False)]

is basically a brick wall. It will take longer to load the RDD than to do
all operations on it, by a large margin.

Any help or guidance (should we write some custom loader?) would be
appreciated.

FDS



--

---------------------------------------------------------------------


"
Paul Weiss <paulweiss.dev@gmail.com>,"Mon, 31 Aug 2015 11:00:33 -0400",Re: Tungsten off heap memory access for C++ libraries,Reynold Xin <rxin@databricks.com>,"Sounds good, want me to create a jira and link it to SPARK-9697? Will put
down some ideas to start.

"
yash datta <saucam@gmail.com>,"Mon, 31 Aug 2015 21:11:28 +0530",Re: KryoSerializer for closureSerializer in DAGScheduler,Josh Rosen <rosenville@gmail.com>,"Thanks josh ... i'll take a look

"
Olivier Girardot <ssaboum@gmail.com>,"Mon, 31 Aug 2015 18:31:06 +0200",Re: ClassCastException using DataFrame only when num-executors > 2 ...,"dev <dev@spark.apache.org>, ""user@spark.apache.org"" <user@spark.apache.org>","tested now against Spark 1.5.0 rc2, and same exceptions happen when
num-executors > 2 :

15/08/25 10:31:10 WARN scheduler.TaskSetManager: Lost task 0.1 in stage 5.0
(TID 501, xxxxxxx): java.lang.ClassCastException: java.lang.Double cannot
be cast to java.lang.Long
        at scala.runtime.BoxesRunTime.unboxToLong(BoxesRunTime.java:110)
        at
org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow$class.getLong(rows.scala:41)
        at
org.apache.spark.sql.catalyst.expressions.GenericInternalRow.getLong(rows.scala:220)
        at
org.apache.spark.sql.catalyst.expressions.JoinedRow.getLong(JoinedRow.scala:85)
        at
org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificMutableProjection.apply(Unknown
Source)
        at
org.apache.spark.sql.execution.Window$$anonfun$8$$anon$1.next(Window.scala:325)
        at
org.apache.spark.sql.execution.Window$$anonfun$8$$anon$1.next(Window.scala:252)
        at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)


2015-08-26 11:47 GMT+02:00 Olivier Girardot <ssaboum@gmail.com>:

duler.scala:1267)
heduler.scala:1255)
heduler.scala:1254)
a:59)
4)
ply(DAGScheduler.scala:684)
ply(DAGScheduler.scala:684)
scala:684)
heduler.scala:1480)
duler.scala:1442)
duler.scala:1431)
:147)
:108)
ge.scala:156)
ge.scala:141)
)
tLong(rows.scala:41)
.scala:220)
la:85)
rojection.apply(Unknown
a:325)
a:252)
dow.scala:265)
ala:272)
2)
1)
(RDD.scala:706)
(RDD.scala:706)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
8)
sWithPreparationRDD.scala:46)
sWithPreparationRDD.scala:46)
8)
8)
la:88)
8)
8)
sWithPreparationRDD.scala:46)
8)
)
)
:1145)
a:615)
d
a:325)
"
Reynold Xin <rxin@databricks.com>,"Mon, 31 Aug 2015 15:52:09 -0700",Re: [VOTE] Release Apache Spark 1.5.0 (RC2),Sandy Ryza <sandy.ryza@cloudera.com>,"I'm going to -1 the release myself since the issue @yhuai identified is
pretty serious. It basically OOMs the driver for reading any files with a
large number of partitions. Looks like the patch for that has already been
merged.

I'm going to cut rc3 momentarily.



)
R
'"") OK
rk. But
:
n
es
5fc27a
================
================
=========================
=========================
s
 not
========================================
========================================
===========================
===========================
ns
acy
,
r
I
,
,
"
Philip <branning@gmail.com>,"Mon, 31 Aug 2015 16:25:19 -0700",Re: IOError on createDataFrame,fsacerdoti <fsacerdoti@jumptrading.com>,"Pandas performance is definitely the issue here. You're using Pandas as an
ETL system, and it's more suitable as an endpoint rather than an conduit.
That is, it's great to dump your data there and do your analysis within
Pandas, subject to its constraints, but if you need to ""back out"" and use
something that can spread out into multiple machines' memory space, you'll
need to go back to the original data sources.

Here is some information about performance from the Odo project, which is a
Python ETL tool that supports Pandas and Spark. They report it taking on
the order of minutes to get 1M rows out of Pandas, and >11.5 hours to push
their 33GB test set through it.
http://odo.readthedocs.org/en/latest/perf.html

Can you format your data as CSV or JSON or something that allows use of
faster loading tools?

Philip


"
